===============================
2365192.pbshpc
vsky017.hpc.iitd.ac.in
===============================
/home/cse/phd/anz208849/scratch/data
wait
['so_run_btad_2.py']
Snapshot stored in: ../scratch/saved_models/acdc/dannet/train
                     note : train          
                    model : deeplab        
                    train : 1              
                 multigpu : 0              
                    fixbn : 0              
                 fix_seed : 1              
            learning_rate : 7.5e-05        
                num_steps : 5000           
                   epochs : 1000           
             weight_decay : 0.0005         
                 momentum : 0.9            
                    power : 0.9            
                    round : 6              
               print_freq : 372            
                save_freq : 372            
              tensorboard : 1              
                  neptune : 0              
                   screen : 1              
                      val : 1              
                 val_freq : 5              
                   source : acdc_train_rf  
                   target : acdc_val_rf    
                   worker : 4              
               batch_size : 8              
              num_classes : 2              
                input_src : 720            
                input_tgt : 720            
                 crop_src : 600            
                 crop_tgt : 600            
                   mirror : 1              
                scale_min : 0.5            
                scale_max : 1.5            
                      rec : 0              
              init_weight : ./save/model410_city_deeplabv2.pth
             restore_from : None           
                 snapshot : ../scratch/saved_models/acdc/dannet/train
                   result : ./miou_result/ 
                      log : ./log/         
                   plabel : ./plabel       
                       tb : ./log/train    

Mode --> Train
Lets ride...
epoch =      0/  1000, exp = train
train_iter_loss: 0.6977042555809021
train_iter_loss: 0.7101811170578003
train_iter_loss: 0.697610080242157
train_iter_loss: 0.7467474341392517
train_iter_loss: 0.7084610462188721
train_iter_loss: 0.7067896723747253
train_iter_loss: 0.6840895414352417
train_iter_loss: 0.6879435181617737
train_iter_loss: 0.6956297159194946
train_iter_loss: 0.6637094616889954
train_iter_loss: 0.6796347498893738
train_iter_loss: 0.6545622944831848
train_iter_loss: 0.6943588256835938
train_iter_loss: 0.6920433044433594
train_iter_loss: 0.6552983522415161
train_iter_loss: 0.6730962991714478
train_iter_loss: 0.6428670287132263
train_iter_loss: 0.6730539798736572
train_iter_loss: 0.6474576592445374
train_iter_loss: 0.6278368234634399
train_iter_loss: 0.5953311920166016
train_iter_loss: 0.6155704855918884
train_iter_loss: 0.6458262205123901
train_iter_loss: 0.5921847820281982
train_iter_loss: 0.6087016463279724
train_iter_loss: 0.6584452390670776
train_iter_loss: 0.6176530718803406
train_iter_loss: 0.6081227660179138
train_iter_loss: 0.6236741542816162
train_iter_loss: 0.6521559953689575
train_iter_loss: 0.5918947458267212
train_iter_loss: 0.6321289539337158
train_iter_loss: 0.6327366232872009
train_iter_loss: 0.578768789768219
train_iter_loss: 0.569950520992279
train_iter_loss: 0.6405339241027832
train_iter_loss: 0.559471607208252
train_iter_loss: 0.5392532348632812
train_iter_loss: 0.6292768716812134
train_iter_loss: 0.6480173468589783
train_iter_loss: 0.5371443629264832
train_iter_loss: 0.5979118943214417
train_iter_loss: 0.584892749786377
train_iter_loss: 0.5613080859184265
train_iter_loss: 0.5482013821601868
train_iter_loss: 0.5983165502548218
train_iter_loss: 0.5826829671859741
train_iter_loss: 0.5765261054039001
train_iter_loss: 0.6156854033470154
train_iter_loss: 0.5672258138656616
train_iter_loss: 0.5532483458518982
train_iter_loss: 0.5462421774864197
train_iter_loss: 0.5906916260719299
train_iter_loss: 0.5981197953224182
train_iter_loss: 0.5947068929672241
train_iter_loss: 0.5566479563713074
train_iter_loss: 0.5092087984085083
train_iter_loss: 0.6113081574440002
train_iter_loss: 0.510522723197937
train_iter_loss: 0.5634980797767639
train_iter_loss: 0.5359958410263062
train_iter_loss: 0.5795740485191345
train_iter_loss: 0.5255957841873169
train_iter_loss: 0.5203778743743896
train_iter_loss: 0.5218334794044495
train_iter_loss: 0.5499390959739685
train_iter_loss: 0.5553719401359558
train_iter_loss: 0.5964251160621643
train_iter_loss: 0.5951583385467529
train_iter_loss: 0.5511190891265869
train_iter_loss: 0.6371246576309204
train_iter_loss: 0.551902174949646
train_iter_loss: 0.5361745953559875
train_iter_loss: 0.5663456916809082
train_iter_loss: 0.5932835340499878
train_iter_loss: 0.6305772662162781
train_iter_loss: 0.5728856921195984
train_iter_loss: 0.5434884428977966
train_iter_loss: 0.49850887060165405
train_iter_loss: 0.5148090720176697
train_iter_loss: 0.5407136082649231
train_iter_loss: 0.5434268116950989
train_iter_loss: 0.5561209917068481
train_iter_loss: 0.6051115989685059
train_iter_loss: 0.5628436207771301
train_iter_loss: 0.515658438205719
train_iter_loss: 0.4929012656211853
train_iter_loss: 0.5014128684997559
train_iter_loss: 0.5741674900054932
train_iter_loss: 0.5872781276702881
train_iter_loss: 0.42623600363731384
train_iter_loss: 0.5234844088554382
train_iter_loss: 0.5737903118133545
train_iter_loss: 0.5712611675262451
train_iter_loss: 0.5154457688331604
train_iter_loss: 0.49137428402900696
train_iter_loss: 0.5443944334983826
train_iter_loss: 0.5811905264854431
train_iter_loss: 0.5524476170539856
train_iter_loss: 0.49365028738975525
train loss :0.5921
---------------------
Validation seg loss: 0.5357246480460437 at epoch 0
********************
best_val_epoch_loss:  0.5357246480460437
MODEL UPDATED
epoch =      1/  1000, exp = train
train_iter_loss: 0.5602179765701294
train_iter_loss: 0.5219196081161499
train_iter_loss: 0.5674254298210144
train_iter_loss: 0.5514442324638367
train_iter_loss: 0.5516688823699951
train_iter_loss: 0.5503668189048767
train_iter_loss: 0.5783365368843079
train_iter_loss: 0.580906093120575
train_iter_loss: 0.59218430519104
train_iter_loss: 0.6152463555335999
train_iter_loss: 0.5470330715179443
train_iter_loss: 0.5628265738487244
train_iter_loss: 0.44978752732276917
train_iter_loss: 0.4743751585483551
train_iter_loss: 0.5516907572746277
train_iter_loss: 0.495783269405365
train_iter_loss: 0.4531537592411041
train_iter_loss: 0.5192633271217346
train_iter_loss: 0.5749346613883972
train_iter_loss: 0.547828197479248
train_iter_loss: 0.5202404260635376
train_iter_loss: 0.5283308029174805
train_iter_loss: 0.5055527687072754
train_iter_loss: 0.4643234610557556
train_iter_loss: 0.44752034544944763
train_iter_loss: 0.5178980231285095
train_iter_loss: 0.5450105667114258
train_iter_loss: 0.5289135575294495
train_iter_loss: 0.45930832624435425
train_iter_loss: 0.4475383162498474
train_iter_loss: 0.5489387512207031
train_iter_loss: 0.5554525256156921
train_iter_loss: 0.4132770597934723
train_iter_loss: 0.5485550165176392
train_iter_loss: 0.45445486903190613
train_iter_loss: 0.5491088628768921
train_iter_loss: 0.6292200684547424
train_iter_loss: 0.5950719714164734
train_iter_loss: 0.5219833850860596
train_iter_loss: 0.4855400025844574
train_iter_loss: 0.5126994252204895
train_iter_loss: 0.49238675832748413
train_iter_loss: 0.4487434923648834
train_iter_loss: 0.5161044597625732
train_iter_loss: 0.46881750226020813
train_iter_loss: 0.48541709780693054
train_iter_loss: 0.5547707676887512
train_iter_loss: 0.5254743099212646
train_iter_loss: 0.5122928619384766
train_iter_loss: 0.5116273760795593
train_iter_loss: 0.6105056405067444
train_iter_loss: 0.49851539731025696
train_iter_loss: 0.5767117142677307
train_iter_loss: 0.4585658311843872
train_iter_loss: 0.5316627025604248
train_iter_loss: 0.59174644947052
train_iter_loss: 0.5713303089141846
train_iter_loss: 0.49593737721443176
train_iter_loss: 0.463961660861969
train_iter_loss: 0.4450071454048157
train_iter_loss: 0.5553632974624634
train_iter_loss: 0.4685513973236084
train_iter_loss: 0.4955166280269623
train_iter_loss: 0.5756127238273621
train_iter_loss: 0.4901042580604553
train_iter_loss: 0.467536598443985
train_iter_loss: 0.455705851316452
train_iter_loss: 0.43238136172294617
train_iter_loss: 0.48539984226226807
train_iter_loss: 0.5273321866989136
train_iter_loss: 0.4362194538116455
train_iter_loss: 0.497109979391098
train_iter_loss: 0.3989208936691284
train_iter_loss: 0.5005345940589905
train_iter_loss: 0.46478232741355896
train_iter_loss: 0.5310683250427246
train_iter_loss: 0.4777929484844208
train_iter_loss: 0.4743454158306122
train_iter_loss: 0.4657558798789978
train_iter_loss: 0.4889858365058899
train_iter_loss: 0.4811663031578064
train_iter_loss: 0.4176539182662964
train_iter_loss: 0.5179769396781921
train_iter_loss: 0.451058566570282
train_iter_loss: 0.4567427933216095
train_iter_loss: 0.5033427476882935
train_iter_loss: 0.4921281635761261
train_iter_loss: 0.4108218848705292
train_iter_loss: 0.49581509828567505
train_iter_loss: 0.47956496477127075
train_iter_loss: 0.41992059350013733
train_iter_loss: 0.49745437502861023
train_iter_loss: 0.4969978630542755
train_iter_loss: 0.4471881091594696
train_iter_loss: 0.41588619351387024
train_iter_loss: 0.47318392992019653
train_iter_loss: 0.47497761249542236
train_iter_loss: 0.4854040741920471
train_iter_loss: 0.4843950867652893
train_iter_loss: 0.5663422346115112
train loss :0.5113
---------------------
Validation seg loss: 0.4848123652473936 at epoch 1
********************
best_val_epoch_loss:  0.4848123652473936
MODEL UPDATED
epoch =      2/  1000, exp = train
train_iter_loss: 0.4324815273284912
train_iter_loss: 0.43588340282440186
train_iter_loss: 0.5366098880767822
train_iter_loss: 0.4640902280807495
train_iter_loss: 0.5138813853263855
train_iter_loss: 0.5076632499694824
train_iter_loss: 0.4689757525920868
train_iter_loss: 0.39052051305770874
train_iter_loss: 0.43141984939575195
train_iter_loss: 0.45562243461608887
train_iter_loss: 0.5285911560058594
train_iter_loss: 0.38787931203842163
train_iter_loss: 0.353646844625473
train_iter_loss: 0.39519327878952026
train_iter_loss: 0.49334874749183655
train_iter_loss: 0.43691250681877136
train_iter_loss: 0.4577275812625885
train_iter_loss: 0.5015856027603149
train_iter_loss: 0.46657639741897583
train_iter_loss: 0.4993841052055359
train_iter_loss: 0.44221869111061096
train_iter_loss: 0.4682023823261261
train_iter_loss: 0.4194663465023041
train_iter_loss: 0.6112293004989624
train_iter_loss: 0.3830566108226776
train_iter_loss: 0.47528958320617676
train_iter_loss: 0.381147563457489
train_iter_loss: 0.36273810267448425
train_iter_loss: 0.5329664349555969
train_iter_loss: 0.5181488990783691
train_iter_loss: 0.5017622113227844
train_iter_loss: 0.5148380994796753
train_iter_loss: 0.45712167024612427
train_iter_loss: 0.4807119071483612
train_iter_loss: 0.5116568207740784
train_iter_loss: 0.465553343296051
train_iter_loss: 0.48082035779953003
train_iter_loss: 0.43828538060188293
train_iter_loss: 0.4791136384010315
train_iter_loss: 0.4374101758003235
train_iter_loss: 0.42264682054519653
train_iter_loss: 0.4066670835018158
train_iter_loss: 0.5151939392089844
train_iter_loss: 0.4077318608760834
train_iter_loss: 0.5603727102279663
train_iter_loss: 0.3808527886867523
train_iter_loss: 0.4657239019870758
train_iter_loss: 0.48429855704307556
train_iter_loss: 0.3986940383911133
train_iter_loss: 0.46712133288383484
train_iter_loss: 0.5240217447280884
train_iter_loss: 0.47919413447380066
train_iter_loss: 0.44346243143081665
train_iter_loss: 0.5247857570648193
train_iter_loss: 0.4437659978866577
train_iter_loss: 0.5017455816268921
train_iter_loss: 0.47591152787208557
train_iter_loss: 0.5134028792381287
train_iter_loss: 0.4195230305194855
train_iter_loss: 0.4256249666213989
train_iter_loss: 0.5213293433189392
train_iter_loss: 0.41645389795303345
train_iter_loss: 0.37324848771095276
train_iter_loss: 0.42089322209358215
train_iter_loss: 0.3532593250274658
train_iter_loss: 0.5675245523452759
train_iter_loss: 0.40019893646240234
train_iter_loss: 0.4081137478351593
train_iter_loss: 0.4452643096446991
train_iter_loss: 0.4750025272369385
train_iter_loss: 0.4542609751224518
train_iter_loss: 0.48399069905281067
train_iter_loss: 0.45647871494293213
train_iter_loss: 0.37745946645736694
train_iter_loss: 0.450043648481369
train_iter_loss: 0.45729050040245056
train_iter_loss: 0.50406813621521
train_iter_loss: 0.44506892561912537
train_iter_loss: 0.4359399080276489
train_iter_loss: 0.4625685513019562
train_iter_loss: 0.5131585597991943
train_iter_loss: 0.385379821062088
train_iter_loss: 0.4543248414993286
train_iter_loss: 0.4164513051509857
train_iter_loss: 0.4352554380893707
train_iter_loss: 0.5011060833930969
train_iter_loss: 0.40042319893836975
train_iter_loss: 0.48972517251968384
train_iter_loss: 0.4861913025379181
train_iter_loss: 0.47249913215637207
train_iter_loss: 0.382581502199173
train_iter_loss: 0.6004046201705933
train_iter_loss: 0.46120694279670715
train_iter_loss: 0.4165087938308716
train_iter_loss: 0.5060300827026367
train_iter_loss: 0.36034682393074036
train_iter_loss: 0.584716260433197
train_iter_loss: 0.44151732325553894
train_iter_loss: 0.5248764753341675
train_iter_loss: 0.42042216658592224
train loss :0.4645
---------------------
Validation seg loss: 0.4601012382586047 at epoch 2
********************
best_val_epoch_loss:  0.4601012382586047
MODEL UPDATED
epoch =      3/  1000, exp = train
train_iter_loss: 0.3426453173160553
train_iter_loss: 0.4335162341594696
train_iter_loss: 0.5210276246070862
train_iter_loss: 0.4053482711315155
train_iter_loss: 0.39799508452415466
train_iter_loss: 0.47956159710884094
train_iter_loss: 0.37521132826805115
train_iter_loss: 0.4100658893585205
train_iter_loss: 0.501918375492096
train_iter_loss: 0.5524739027023315
train_iter_loss: 0.4917963743209839
train_iter_loss: 0.41632577776908875
train_iter_loss: 0.49422720074653625
train_iter_loss: 0.38550832867622375
train_iter_loss: 0.4392664134502411
train_iter_loss: 0.36887767910957336
train_iter_loss: 0.45508822798728943
train_iter_loss: 0.456316202878952
train_iter_loss: 0.4312373697757721
train_iter_loss: 0.4169606864452362
train_iter_loss: 0.39765405654907227
train_iter_loss: 0.43006080389022827
train_iter_loss: 0.4327378571033478
train_iter_loss: 0.5033990740776062
train_iter_loss: 0.5081056952476501
train_iter_loss: 0.4035596251487732
train_iter_loss: 0.47760939598083496
train_iter_loss: 0.5443928241729736
train_iter_loss: 0.3320156931877136
train_iter_loss: 0.48659005761146545
train_iter_loss: 0.40619441866874695
train_iter_loss: 0.5491126775741577
train_iter_loss: 0.3811517357826233
train_iter_loss: 0.40590375661849976
train_iter_loss: 0.48983457684516907
train_iter_loss: 0.43031659722328186
train_iter_loss: 0.4232182204723358
train_iter_loss: 0.3549111485481262
train_iter_loss: 0.3940929174423218
train_iter_loss: 0.49635863304138184
train_iter_loss: 0.4205343723297119
train_iter_loss: 0.39312267303466797
train_iter_loss: 0.3771170973777771
train_iter_loss: 0.5072161555290222
train_iter_loss: 0.48661378026008606
train_iter_loss: 0.47187215089797974
train_iter_loss: 0.437648206949234
train_iter_loss: 0.4149678647518158
train_iter_loss: 0.5232555866241455
train_iter_loss: 0.4276089072227478
train_iter_loss: 0.46830010414123535
train_iter_loss: 0.40445688366889954
train_iter_loss: 0.45578089356422424
train_iter_loss: 0.4471728503704071
train_iter_loss: 0.44827327132225037
train_iter_loss: 0.4297132194042206
train_iter_loss: 0.5259994268417358
train_iter_loss: 0.5454682111740112
train_iter_loss: 0.40211018919944763
train_iter_loss: 0.36811214685440063
train_iter_loss: 0.33155158162117004
train_iter_loss: 0.3666076362133026
train_iter_loss: 0.36606693267822266
train_iter_loss: 0.3635880947113037
train_iter_loss: 0.466506689786911
train_iter_loss: 0.5127356052398682
train_iter_loss: 0.47629761695861816
train_iter_loss: 0.2940506041049957
train_iter_loss: 0.3347577154636383
train_iter_loss: 0.39935117959976196
train_iter_loss: 0.4657275974750519
train_iter_loss: 0.44504314661026
train_iter_loss: 0.4528454542160034
train_iter_loss: 0.44573086500167847
train_iter_loss: 0.3992674648761749
train_iter_loss: 0.48710304498672485
train_iter_loss: 0.45718511939048767
train_iter_loss: 0.3598305284976959
train_iter_loss: 0.39039528369903564
train_iter_loss: 0.4424169361591339
train_iter_loss: 0.48086774349212646
train_iter_loss: 0.4320404529571533
train_iter_loss: 0.41407352685928345
train_iter_loss: 0.3174090087413788
train_iter_loss: 0.4920118451118469
train_iter_loss: 0.5127307176589966
train_iter_loss: 0.4911232888698578
train_iter_loss: 0.3137577474117279
train_iter_loss: 0.42621663212776184
train_iter_loss: 0.43633872270584106
train_iter_loss: 0.5339663028717041
train_iter_loss: 0.38339662551879883
train_iter_loss: 0.30890601873397827
train_iter_loss: 0.49084725975990295
train_iter_loss: 0.4254690408706665
train_iter_loss: 0.34327054023742676
train_iter_loss: 0.3539853096008301
train_iter_loss: 0.4409891664981842
train_iter_loss: 0.4482177495956421
train_iter_loss: 0.3274082541465759
train loss :0.4367
---------------------
Validation seg loss: 0.39986187810043117 at epoch 3
********************
best_val_epoch_loss:  0.39986187810043117
MODEL UPDATED
epoch =      4/  1000, exp = train
train_iter_loss: 0.3931848406791687
train_iter_loss: 0.43587684631347656
train_iter_loss: 0.3290499746799469
train_iter_loss: 0.46328917145729065
train_iter_loss: 0.3993256688117981
train_iter_loss: 0.358853816986084
train_iter_loss: 0.3864753544330597
train_iter_loss: 0.42954060435295105
train_iter_loss: 0.398204505443573
train_iter_loss: 0.38919681310653687
train_iter_loss: 0.40562206506729126
train_iter_loss: 0.4426451325416565
train_iter_loss: 0.43390730023384094
train_iter_loss: 0.38151368498802185
train_iter_loss: 0.34930944442749023
train_iter_loss: 0.49001091718673706
train_iter_loss: 0.4357408881187439
train_iter_loss: 0.3861251771450043
train_iter_loss: 0.3729330003261566
train_iter_loss: 0.5020894408226013
train_iter_loss: 0.4088408648967743
train_iter_loss: 0.45300453901290894
train_iter_loss: 0.35277438163757324
train_iter_loss: 0.4275434911251068
train_iter_loss: 0.4054766297340393
train_iter_loss: 0.4579112231731415
train_iter_loss: 0.4038354456424713
train_iter_loss: 0.4649697542190552
train_iter_loss: 0.47179311513900757
train_iter_loss: 0.4314296245574951
train_iter_loss: 0.4194152057170868
train_iter_loss: 0.38735005259513855
train_iter_loss: 0.304289311170578
train_iter_loss: 0.3191826343536377
train_iter_loss: 0.4405258595943451
train_iter_loss: 0.3860943615436554
train_iter_loss: 0.4110351502895355
train_iter_loss: 0.4285561442375183
train_iter_loss: 0.6467543244361877
train_iter_loss: 0.3620363473892212
train_iter_loss: 0.4927944242954254
train_iter_loss: 0.40088215470314026
train_iter_loss: 0.2924652397632599
train_iter_loss: 0.5311768651008606
train_iter_loss: 0.41797637939453125
train_iter_loss: 0.3506926894187927
train_iter_loss: 0.4098055362701416
train_iter_loss: 0.38247376680374146
train_iter_loss: 0.49890777468681335
train_iter_loss: 0.4591854512691498
train_iter_loss: 0.44503024220466614
train_iter_loss: 0.35686248540878296
train_iter_loss: 0.3634377419948578
train_iter_loss: 0.3902135491371155
train_iter_loss: 0.34476375579833984
train_iter_loss: 0.3895722031593323
train_iter_loss: 0.2901882231235504
train_iter_loss: 0.39679139852523804
train_iter_loss: 0.40605491399765015
train_iter_loss: 0.37280720472335815
train_iter_loss: 0.3695187270641327
train_iter_loss: 0.3953045606613159
train_iter_loss: 0.30426013469696045
train_iter_loss: 0.36297133564949036
train_iter_loss: 0.45038896799087524
train_iter_loss: 0.4291541576385498
train_iter_loss: 0.4459773600101471
train_iter_loss: 0.4182257354259491
train_iter_loss: 0.4103151261806488
train_iter_loss: 0.5277079343795776
train_iter_loss: 0.4388497769832611
train_iter_loss: 0.2946506142616272
train_iter_loss: 0.38289356231689453
train_iter_loss: 0.3468002676963806
train_iter_loss: 0.40113624930381775
train_iter_loss: 0.47513121366500854
train_iter_loss: 0.3030325472354889
train_iter_loss: 0.3802781105041504
train_iter_loss: 0.40956389904022217
train_iter_loss: 0.34491828083992004
train_iter_loss: 0.34575217962265015
train_iter_loss: 0.38243788480758667
train_iter_loss: 0.4384317398071289
train_iter_loss: 0.35366570949554443
train_iter_loss: 0.4054429531097412
train_iter_loss: 0.33480456471443176
train_iter_loss: 0.4207685887813568
train_iter_loss: 0.3298283815383911
train_iter_loss: 0.3297217786312103
train_iter_loss: 0.53328937292099
train_iter_loss: 0.4154362380504608
train_iter_loss: 0.4089542329311371
train_iter_loss: 0.335845947265625
train_iter_loss: 0.36249348521232605
train_iter_loss: 0.3209199607372284
train_iter_loss: 0.29123085737228394
train_iter_loss: 0.4071425795555115
train_iter_loss: 0.366271048784256
train_iter_loss: 0.3993699550628662
train_iter_loss: 0.3891482651233673
train loss :0.4046
---------------------
Validation seg loss: 0.3908542484607337 at epoch 4
********************
best_val_epoch_loss:  0.3908542484607337
MODEL UPDATED
epoch =      5/  1000, exp = train
train_iter_loss: 0.5143985748291016
train_iter_loss: 0.43007439374923706
train_iter_loss: 0.4476029872894287
train_iter_loss: 0.3056362569332123
train_iter_loss: 0.3780767619609833
train_iter_loss: 0.434078186750412
train_iter_loss: 0.35012590885162354
train_iter_loss: 0.4314654469490051
train_iter_loss: 0.3975156247615814
train_iter_loss: 0.3997661769390106
train_iter_loss: 0.34929221868515015
train_iter_loss: 0.35903382301330566
train_iter_loss: 0.44193607568740845
train_iter_loss: 0.44361862540245056
train_iter_loss: 0.3263132870197296
train_iter_loss: 0.450069397687912
train_iter_loss: 0.3652663826942444
train_iter_loss: 0.4957841634750366
train_iter_loss: 0.3005366325378418
train_iter_loss: 0.37610071897506714
train_iter_loss: 0.32742756605148315
train_iter_loss: 0.31685134768486023
train_iter_loss: 0.3399249315261841
train_iter_loss: 0.3736765384674072
train_iter_loss: 0.4020250141620636
train_iter_loss: 0.4431932270526886
train_iter_loss: 0.4250671863555908
train_iter_loss: 0.3334108293056488
train_iter_loss: 0.4082300066947937
train_iter_loss: 0.37140190601348877
train_iter_loss: 0.4663082957267761
train_iter_loss: 0.2835795283317566
train_iter_loss: 0.36805135011672974
train_iter_loss: 0.4373701512813568
train_iter_loss: 0.40616753697395325
train_iter_loss: 0.32834410667419434
train_iter_loss: 0.36585915088653564
train_iter_loss: 0.35898348689079285
train_iter_loss: 0.31898242235183716
train_iter_loss: 0.30187487602233887
train_iter_loss: 0.36252254247665405
train_iter_loss: 0.3705139756202698
train_iter_loss: 0.47251567244529724
train_iter_loss: 0.33419474959373474
train_iter_loss: 0.41318947076797485
train_iter_loss: 0.465695321559906
train_iter_loss: 0.3034725487232208
train_iter_loss: 0.4903869926929474
train_iter_loss: 0.4075130522251129
train_iter_loss: 0.3391036093235016
train_iter_loss: 0.39302390813827515
train_iter_loss: 0.37297379970550537
train_iter_loss: 0.37032201886177063
train_iter_loss: 0.3467501103878021
train_iter_loss: 0.36804959177970886
train_iter_loss: 0.43747639656066895
train_iter_loss: 0.3162974715232849
train_iter_loss: 0.4074951112270355
train_iter_loss: 0.35260698199272156
train_iter_loss: 0.44849514961242676
train_iter_loss: 0.36235857009887695
train_iter_loss: 0.3984779715538025
train_iter_loss: 0.41370266675949097
train_iter_loss: 0.44728243350982666
train_iter_loss: 0.35146209597587585
train_iter_loss: 0.32898351550102234
train_iter_loss: 0.3641728162765503
train_iter_loss: 0.39004889130592346
train_iter_loss: 0.4867834448814392
train_iter_loss: 0.4380006790161133
train_iter_loss: 0.3961241543292999
train_iter_loss: 0.3684592843055725
train_iter_loss: 0.46199703216552734
train_iter_loss: 0.3840799033641815
train_iter_loss: 0.4164324402809143
train_iter_loss: 0.3587813079357147
train_iter_loss: 0.3734362721443176
train_iter_loss: 0.36166736483573914
train_iter_loss: 0.29073211550712585
train_iter_loss: 0.3602510392665863
train_iter_loss: 0.1957699954509735
train_iter_loss: 0.379027396440506
train_iter_loss: 0.3431435525417328
train_iter_loss: 0.44326746463775635
train_iter_loss: 0.47242438793182373
train_iter_loss: 0.3986399173736572
train_iter_loss: 0.39731791615486145
train_iter_loss: 0.4340556561946869
train_iter_loss: 0.46204036474227905
train_iter_loss: 0.37046128511428833
train_iter_loss: 0.3308960497379303
train_iter_loss: 0.37400996685028076
train_iter_loss: 0.31054407358169556
train_iter_loss: 0.3760320842266083
train_iter_loss: 0.35623323917388916
train_iter_loss: 0.3796660304069519
train_iter_loss: 0.48263049125671387
train_iter_loss: 0.37495094537734985
train_iter_loss: 0.2975079119205475
train_iter_loss: 0.2740119695663452
train loss :0.3876
---------------------
Validation seg loss: 0.3608274979973739 at epoch 5
********************
best_val_epoch_loss:  0.3608274979973739
MODEL UPDATED
epoch =      6/  1000, exp = train
train_iter_loss: 0.3167739808559418
train_iter_loss: 0.2598499655723572
train_iter_loss: 0.3893270492553711
train_iter_loss: 0.25293463468551636
train_iter_loss: 0.23213373124599457
train_iter_loss: 0.30729469656944275
train_iter_loss: 0.3913944661617279
train_iter_loss: 0.304856538772583
train_iter_loss: 0.3255598545074463
train_iter_loss: 0.4605393707752228
train_iter_loss: 0.39956173300743103
train_iter_loss: 0.4321306347846985
train_iter_loss: 0.431987464427948
train_iter_loss: 0.33356228470802307
train_iter_loss: 0.39271825551986694
train_iter_loss: 0.32853588461875916
train_iter_loss: 0.31280943751335144
train_iter_loss: 0.3132663071155548
train_iter_loss: 0.32231593132019043
train_iter_loss: 0.39707857370376587
train_iter_loss: 0.32516399025917053
train_iter_loss: 0.4243856370449066
train_iter_loss: 0.4191504120826721
train_iter_loss: 0.3775101602077484
train_iter_loss: 0.34591540694236755
train_iter_loss: 0.3986656069755554
train_iter_loss: 0.31104299426078796
train_iter_loss: 0.2957805097103119
train_iter_loss: 0.35393041372299194
train_iter_loss: 0.3617528975009918
train_iter_loss: 0.4623967111110687
train_iter_loss: 0.35973361134529114
train_iter_loss: 0.3971625864505768
train_iter_loss: 0.3898664712905884
train_iter_loss: 0.33317700028419495
train_iter_loss: 0.4202762246131897
train_iter_loss: 0.31317368149757385
train_iter_loss: 0.3691016435623169
train_iter_loss: 0.32636338472366333
train_iter_loss: 0.31414058804512024
train_iter_loss: 0.348934531211853
train_iter_loss: 0.46850937604904175
train_iter_loss: 0.418961763381958
train_iter_loss: 0.5231258273124695
train_iter_loss: 0.33022841811180115
train_iter_loss: 0.4027918875217438
train_iter_loss: 0.39801618456840515
train_iter_loss: 0.3851443827152252
train_iter_loss: 0.4021556079387665
train_iter_loss: 0.31499239802360535
train_iter_loss: 0.40512216091156006
train_iter_loss: 0.42940473556518555
train_iter_loss: 0.43017762899398804
train_iter_loss: 0.3765335977077484
train_iter_loss: 0.4049778878688812
train_iter_loss: 0.3771609365940094
train_iter_loss: 0.36774736642837524
train_iter_loss: 0.38655927777290344
train_iter_loss: 0.35903212428092957
train_iter_loss: 0.34941092133522034
train_iter_loss: 0.31969112157821655
train_iter_loss: 0.40843701362609863
train_iter_loss: 0.5171094536781311
train_iter_loss: 0.35495680570602417
train_iter_loss: 0.4313649833202362
train_iter_loss: 0.41789790987968445
train_iter_loss: 0.3672165870666504
train_iter_loss: 0.40772590041160583
train_iter_loss: 0.293277382850647
train_iter_loss: 0.416044145822525
train_iter_loss: 0.33591949939727783
train_iter_loss: 0.37095344066619873
train_iter_loss: 0.3452310264110565
train_iter_loss: 0.22614061832427979
train_iter_loss: 0.29240766167640686
train_iter_loss: 0.385809987783432
train_iter_loss: 0.40656566619873047
train_iter_loss: 0.27794796228408813
train_iter_loss: 0.2977389097213745
train_iter_loss: 0.3576894700527191
train_iter_loss: 0.39736366271972656
train_iter_loss: 0.4011370539665222
train_iter_loss: 0.45883187651634216
train_iter_loss: 0.3153870701789856
train_iter_loss: 0.37372469902038574
train_iter_loss: 0.433456152677536
train_iter_loss: 0.36572566628456116
train_iter_loss: 0.3897128701210022
train_iter_loss: 0.35092395544052124
train_iter_loss: 0.4521910548210144
train_iter_loss: 0.3014855682849884
train_iter_loss: 0.31357768177986145
train_iter_loss: 0.23803246021270752
train_iter_loss: 0.24267928302288055
train_iter_loss: 0.29468294978141785
train_iter_loss: 0.36837074160575867
train_iter_loss: 0.32013416290283203
train_iter_loss: 0.32521486282348633
train_iter_loss: 0.45289868116378784
train_iter_loss: 0.32838210463523865
train loss :0.3682
---------------------
Validation seg loss: 0.3528641589548228 at epoch 6
********************
best_val_epoch_loss:  0.3528641589548228
MODEL UPDATED
epoch =      7/  1000, exp = train
train_iter_loss: 0.6385400891304016
train_iter_loss: 0.4076448976993561
train_iter_loss: 0.337216317653656
train_iter_loss: 0.2744159996509552
train_iter_loss: 0.4117104411125183
train_iter_loss: 0.3786100447177887
train_iter_loss: 0.35369759798049927
train_iter_loss: 0.2782150208950043
train_iter_loss: 0.2658645212650299
train_iter_loss: 0.3599497079849243
train_iter_loss: 0.3342481851577759
train_iter_loss: 0.3694591820240021
train_iter_loss: 0.3708018362522125
train_iter_loss: 0.3802286982536316
train_iter_loss: 0.35441818833351135
train_iter_loss: 0.3090892434120178
train_iter_loss: 0.3197212815284729
train_iter_loss: 0.33564671874046326
train_iter_loss: 0.28111690282821655
train_iter_loss: 0.2628951668739319
train_iter_loss: 0.3901330232620239
train_iter_loss: 0.42420604825019836
train_iter_loss: 0.4660700261592865
train_iter_loss: 0.2958475947380066
train_iter_loss: 0.25441551208496094
train_iter_loss: 0.23354116082191467
train_iter_loss: 0.48332804441452026
train_iter_loss: 0.3676198124885559
train_iter_loss: 0.37688133120536804
train_iter_loss: 0.4734303653240204
train_iter_loss: 0.3420308828353882
train_iter_loss: 0.3569653034210205
train_iter_loss: 0.2871178984642029
train_iter_loss: 0.28187549114227295
train_iter_loss: 0.4444791078567505
train_iter_loss: 0.31486693024635315
train_iter_loss: 0.40679308772087097
train_iter_loss: 0.39289945363998413
train_iter_loss: 0.2950688302516937
train_iter_loss: 0.4081016480922699
train_iter_loss: 0.3614223599433899
train_iter_loss: 0.3768567740917206
train_iter_loss: 0.4451802372932434
train_iter_loss: 0.49838346242904663
train_iter_loss: 0.34814152121543884
train_iter_loss: 0.43211331963539124
train_iter_loss: 0.3252772390842438
train_iter_loss: 0.35722434520721436
train_iter_loss: 0.41944172978401184
train_iter_loss: 0.3491981625556946
train_iter_loss: 0.3161419630050659
train_iter_loss: 0.3929559290409088
train_iter_loss: 0.3832544684410095
train_iter_loss: 0.4110332429409027
train_iter_loss: 0.3796732723712921
train_iter_loss: 0.3221525549888611
train_iter_loss: 0.2715946137905121
train_iter_loss: 0.22204355895519257
train_iter_loss: 0.30408746004104614
train_iter_loss: 0.34554436802864075
train_iter_loss: 0.2597008943557739
train_iter_loss: 0.272929847240448
train_iter_loss: 0.42860546708106995
train_iter_loss: 0.30703824758529663
train_iter_loss: 0.4288153648376465
train_iter_loss: 0.37316447496414185
train_iter_loss: 0.2987883388996124
train_iter_loss: 0.38885459303855896
train_iter_loss: 0.31866076588630676
train_iter_loss: 0.29534614086151123
train_iter_loss: 0.28371983766555786
train_iter_loss: 0.3189340829849243
train_iter_loss: 0.33560681343078613
train_iter_loss: 0.2478920817375183
train_iter_loss: 0.2615145146846771
train_iter_loss: 0.4083331227302551
train_iter_loss: 0.37347015738487244
train_iter_loss: 0.3146584928035736
train_iter_loss: 0.33012625575065613
train_iter_loss: 0.368338406085968
train_iter_loss: 0.371016263961792
train_iter_loss: 0.4602225124835968
train_iter_loss: 0.3549773395061493
train_iter_loss: 0.2420848309993744
train_iter_loss: 0.4001096785068512
train_iter_loss: 0.32547977566719055
train_iter_loss: 0.3490779995918274
train_iter_loss: 0.37547701597213745
train_iter_loss: 0.34594476222991943
train_iter_loss: 0.3689793646335602
train_iter_loss: 0.3290680944919586
train_iter_loss: 0.2901470363140106
train_iter_loss: 0.38855865597724915
train_iter_loss: 0.29001307487487793
train_iter_loss: 0.2906316816806793
train_iter_loss: 0.33897870779037476
train_iter_loss: 0.4649759531021118
train_iter_loss: 0.3536304235458374
train_iter_loss: 0.3504638075828552
train_iter_loss: 0.2878638505935669
train loss :0.3554
---------------------
Validation seg loss: 0.3495946084412764 at epoch 7
********************
best_val_epoch_loss:  0.3495946084412764
MODEL UPDATED
epoch =      8/  1000, exp = train
train_iter_loss: 0.34947675466537476
train_iter_loss: 0.23310253024101257
train_iter_loss: 0.20129390060901642
train_iter_loss: 0.3466336727142334
train_iter_loss: 0.31618303060531616
train_iter_loss: 0.30807337164878845
train_iter_loss: 0.3879929780960083
train_iter_loss: 0.3947592079639435
train_iter_loss: 0.38877806067466736
train_iter_loss: 0.23705117404460907
train_iter_loss: 0.265186071395874
train_iter_loss: 0.4013339877128601
train_iter_loss: 0.4152545630931854
train_iter_loss: 0.29139986634254456
train_iter_loss: 0.2976098358631134
train_iter_loss: 0.371820330619812
train_iter_loss: 0.23629052937030792
train_iter_loss: 0.34613609313964844
train_iter_loss: 0.32515814900398254
train_iter_loss: 0.40052545070648193
train_iter_loss: 0.48899516463279724
train_iter_loss: 0.35256966948509216
train_iter_loss: 0.37920963764190674
train_iter_loss: 0.39406201243400574
train_iter_loss: 0.2464381605386734
train_iter_loss: 0.25198593735694885
train_iter_loss: 0.37350529432296753
train_iter_loss: 0.36838701367378235
train_iter_loss: 0.27501678466796875
train_iter_loss: 0.2799932658672333
train_iter_loss: 0.24809201061725616
train_iter_loss: 0.37467697262763977
train_iter_loss: 0.34629330039024353
train_iter_loss: 0.3075297772884369
train_iter_loss: 0.3049940764904022
train_iter_loss: 0.30387255549430847
train_iter_loss: 0.32521164417266846
train_iter_loss: 0.39024618268013
train_iter_loss: 0.37963852286338806
train_iter_loss: 0.2162364274263382
train_iter_loss: 0.3226456344127655
train_iter_loss: 0.32145655155181885
train_iter_loss: 0.33856090903282166
train_iter_loss: 0.2545469403266907
train_iter_loss: 0.4656065106391907
train_iter_loss: 0.2992906868457794
train_iter_loss: 0.28002026677131653
train_iter_loss: 0.35032734274864197
train_iter_loss: 0.3566906452178955
train_iter_loss: 0.2470424622297287
train_iter_loss: 0.3023684024810791
train_iter_loss: 0.32918035984039307
train_iter_loss: 0.34322813153266907
train_iter_loss: 0.3968033790588379
train_iter_loss: 0.38194480538368225
train_iter_loss: 0.331159383058548
train_iter_loss: 0.41490453481674194
train_iter_loss: 0.3358483910560608
train_iter_loss: 0.3335254192352295
train_iter_loss: 0.2936325669288635
train_iter_loss: 0.318999320268631
train_iter_loss: 0.33176419138908386
train_iter_loss: 0.2873370051383972
train_iter_loss: 0.34479987621307373
train_iter_loss: 0.40375375747680664
train_iter_loss: 0.30803659558296204
train_iter_loss: 0.32961151003837585
train_iter_loss: 0.2828267812728882
train_iter_loss: 0.2627422511577606
train_iter_loss: 0.28008797764778137
train_iter_loss: 0.3248213827610016
train_iter_loss: 0.35219982266426086
train_iter_loss: 0.39831873774528503
train_iter_loss: 0.3181922733783722
train_iter_loss: 0.28902214765548706
train_iter_loss: 0.408231645822525
train_iter_loss: 0.24709348380565643
train_iter_loss: 0.36993885040283203
train_iter_loss: 0.3422548770904541
train_iter_loss: 0.3311198949813843
train_iter_loss: 0.4361098110675812
train_iter_loss: 0.27474725246429443
train_iter_loss: 0.35385507345199585
train_iter_loss: 0.34560444951057434
train_iter_loss: 0.37327924370765686
train_iter_loss: 0.3397676646709442
train_iter_loss: 0.26605117321014404
train_iter_loss: 0.3546285629272461
train_iter_loss: 0.3056846261024475
train_iter_loss: 0.3515854477882385
train_iter_loss: 0.2715874910354614
train_iter_loss: 0.3108080327510834
train_iter_loss: 0.27229827642440796
train_iter_loss: 0.344816118478775
train_iter_loss: 0.27044329047203064
train_iter_loss: 0.3130701184272766
train_iter_loss: 0.25980904698371887
train_iter_loss: 0.3381858170032501
train_iter_loss: 0.35798558592796326
train_iter_loss: 0.37389081716537476
train loss :0.3322
---------------------
Validation seg loss: 0.33522564102456254 at epoch 8
********************
best_val_epoch_loss:  0.33522564102456254
MODEL UPDATED
epoch =      9/  1000, exp = train
train_iter_loss: 0.3456636071205139
train_iter_loss: 0.31139063835144043
train_iter_loss: 0.3682803213596344
train_iter_loss: 0.27905985713005066
train_iter_loss: 0.3167378902435303
train_iter_loss: 0.3460714519023895
train_iter_loss: 0.38114526867866516
train_iter_loss: 0.284017413854599
train_iter_loss: 0.32714900374412537
train_iter_loss: 0.3343821167945862
train_iter_loss: 0.27194744348526
train_iter_loss: 0.3194415867328644
train_iter_loss: 0.2844865918159485
train_iter_loss: 0.347331166267395
train_iter_loss: 0.3431498408317566
train_iter_loss: 0.30630284547805786
train_iter_loss: 0.24678485095500946
train_iter_loss: 0.3574279844760895
train_iter_loss: 0.3215276300907135
train_iter_loss: 0.2544978857040405
train_iter_loss: 0.339857816696167
train_iter_loss: 0.2899750769138336
train_iter_loss: 0.29001089930534363
train_iter_loss: 0.3396024703979492
train_iter_loss: 0.4462423622608185
train_iter_loss: 0.463491290807724
train_iter_loss: 0.2573220729827881
train_iter_loss: 0.1769481748342514
train_iter_loss: 0.2913943827152252
train_iter_loss: 0.18370838463306427
train_iter_loss: 0.25558850169181824
train_iter_loss: 0.2616061568260193
train_iter_loss: 0.38779589533805847
train_iter_loss: 0.6008453369140625
train_iter_loss: 0.24087801575660706
train_iter_loss: 0.25756311416625977
train_iter_loss: 0.2747496962547302
train_iter_loss: 0.32816773653030396
train_iter_loss: 0.3138459324836731
train_iter_loss: 0.30300429463386536
train_iter_loss: 0.33917200565338135
train_iter_loss: 0.34738266468048096
train_iter_loss: 0.28971776366233826
train_iter_loss: 0.3217434883117676
train_iter_loss: 0.3459925651550293
train_iter_loss: 0.20755936205387115
train_iter_loss: 0.22321008145809174
train_iter_loss: 0.2895166873931885
train_iter_loss: 0.3811928331851959
train_iter_loss: 0.389525443315506
train_iter_loss: 0.29752206802368164
train_iter_loss: 0.3079865872859955
train_iter_loss: 0.3937930762767792
train_iter_loss: 0.26371631026268005
train_iter_loss: 0.3171328008174896
train_iter_loss: 0.3453391492366791
train_iter_loss: 0.3714679181575775
train_iter_loss: 0.2681167721748352
train_iter_loss: 0.4185437560081482
train_iter_loss: 0.329995334148407
train_iter_loss: 0.34224480390548706
train_iter_loss: 0.3110622465610504
train_iter_loss: 0.38504430651664734
train_iter_loss: 0.38397812843322754
train_iter_loss: 0.3072793185710907
train_iter_loss: 0.4110875427722931
train_iter_loss: 0.36200764775276184
train_iter_loss: 0.4020354449748993
train_iter_loss: 0.26997557282447815
train_iter_loss: 0.31273648142814636
train_iter_loss: 0.28366342186927795
train_iter_loss: 0.3738907277584076
train_iter_loss: 0.4196155071258545
train_iter_loss: 0.36651766300201416
train_iter_loss: 0.28663522005081177
train_iter_loss: 0.36699000000953674
train_iter_loss: 0.2817067503929138
train_iter_loss: 0.33327534794807434
train_iter_loss: 0.3312326967716217
train_iter_loss: 0.3050825595855713
train_iter_loss: 0.3178456723690033
train_iter_loss: 0.31369978189468384
train_iter_loss: 0.31800583004951477
train_iter_loss: 0.26623499393463135
train_iter_loss: 0.32270145416259766
train_iter_loss: 0.3115653097629547
train_iter_loss: 0.28931209444999695
train_iter_loss: 0.2963266968727112
train_iter_loss: 0.3091983497142792
train_iter_loss: 0.37999778985977173
train_iter_loss: 0.18249432742595673
train_iter_loss: 0.31805557012557983
train_iter_loss: 0.35812076926231384
train_iter_loss: 0.3565393090248108
train_iter_loss: 0.31233519315719604
train_iter_loss: 0.33432409167289734
train_iter_loss: 0.3124980032444
train_iter_loss: 0.26733171939849854
train_iter_loss: 0.22595015168190002
train_iter_loss: 0.32887691259384155
train loss :0.3239
---------------------
Validation seg loss: 0.32195481559577976 at epoch 9
********************
best_val_epoch_loss:  0.32195481559577976
MODEL UPDATED
epoch =     10/  1000, exp = train
train_iter_loss: 0.3252182900905609
train_iter_loss: 0.32228565216064453
train_iter_loss: 0.4156673550605774
train_iter_loss: 0.2792421579360962
train_iter_loss: 0.33130544424057007
train_iter_loss: 0.22025685012340546
train_iter_loss: 0.3803262412548065
train_iter_loss: 0.2448430210351944
train_iter_loss: 0.3400426208972931
train_iter_loss: 0.2559733986854553
train_iter_loss: 0.22351598739624023
train_iter_loss: 0.24579323828220367
train_iter_loss: 0.2595268487930298
train_iter_loss: 0.27693337202072144
train_iter_loss: 0.24867455661296844
train_iter_loss: 0.30396705865859985
train_iter_loss: 0.4396167993545532
train_iter_loss: 0.2921915352344513
train_iter_loss: 0.34874024987220764
train_iter_loss: 0.2864674925804138
train_iter_loss: 0.3081338107585907
train_iter_loss: 0.365164577960968
train_iter_loss: 0.43399980664253235
train_iter_loss: 0.3354978561401367
train_iter_loss: 0.33769071102142334
train_iter_loss: 0.502109706401825
train_iter_loss: 0.4138401746749878
train_iter_loss: 0.25294217467308044
train_iter_loss: 0.26446208357810974
train_iter_loss: 0.3106112480163574
train_iter_loss: 0.5143482089042664
train_iter_loss: 0.2612232565879822
train_iter_loss: 0.2658158838748932
train_iter_loss: 0.3103463351726532
train_iter_loss: 0.2583298683166504
train_iter_loss: 0.41236886382102966
train_iter_loss: 0.46161434054374695
train_iter_loss: 0.24315078556537628
train_iter_loss: 0.25311464071273804
train_iter_loss: 0.31014588475227356
train_iter_loss: 0.29850202798843384
train_iter_loss: 0.2982054650783539
train_iter_loss: 0.27076709270477295
train_iter_loss: 0.3312034606933594
train_iter_loss: 0.3322623372077942
train_iter_loss: 0.4029999077320099
train_iter_loss: 0.3851945698261261
train_iter_loss: 0.33801761269569397
train_iter_loss: 0.26023903489112854
train_iter_loss: 0.2873481512069702
train_iter_loss: 0.2671692371368408
train_iter_loss: 0.2957310378551483
train_iter_loss: 0.2591514587402344
train_iter_loss: 0.21584166586399078
train_iter_loss: 0.28055986762046814
train_iter_loss: 0.3174903392791748
train_iter_loss: 0.32901883125305176
train_iter_loss: 0.3702555000782013
train_iter_loss: 0.31769800186157227
train_iter_loss: 0.3497026264667511
train_iter_loss: 0.2879570722579956
train_iter_loss: 0.3537156879901886
train_iter_loss: 0.32405611872673035
train_iter_loss: 0.3587692975997925
train_iter_loss: 0.2790010869503021
train_iter_loss: 0.24498672783374786
train_iter_loss: 0.24554170668125153
train_iter_loss: 0.19514940679073334
train_iter_loss: 0.44081512093544006
train_iter_loss: 0.2617220878601074
train_iter_loss: 0.322699636220932
train_iter_loss: 0.38538074493408203
train_iter_loss: 0.272666335105896
train_iter_loss: 0.36315983533859253
train_iter_loss: 0.2618386447429657
train_iter_loss: 0.21970494091510773
train_iter_loss: 0.19694353640079498
train_iter_loss: 0.29204225540161133
train_iter_loss: 0.34502941370010376
train_iter_loss: 0.14493612945079803
train_iter_loss: 0.21393540501594543
train_iter_loss: 0.46128594875335693
train_iter_loss: 0.3108748495578766
train_iter_loss: 0.3498048186302185
train_iter_loss: 0.33813318610191345
train_iter_loss: 0.34661078453063965
train_iter_loss: 0.3353408873081207
train_iter_loss: 0.3123045563697815
train_iter_loss: 0.3177926242351532
train_iter_loss: 0.265910804271698
train_iter_loss: 0.277117520570755
train_iter_loss: 0.1959417760372162
train_iter_loss: 0.3195854127407074
train_iter_loss: 0.382037878036499
train_iter_loss: 0.31903526186943054
train_iter_loss: 0.3620584309101105
train_iter_loss: 0.17848467826843262
train_iter_loss: 0.2979801893234253
train_iter_loss: 0.37537190318107605
train_iter_loss: 0.2945987582206726
train loss :0.3144
---------------------
Validation seg loss: 0.3110349061635305 at epoch 10
********************
best_val_epoch_loss:  0.3110349061635305
MODEL UPDATED
epoch =     11/  1000, exp = train
train_iter_loss: 0.3833216726779938
train_iter_loss: 0.29617369174957275
train_iter_loss: 0.24223391711711884
train_iter_loss: 0.29460012912750244
train_iter_loss: 0.38331204652786255
train_iter_loss: 0.35235312581062317
train_iter_loss: 0.22466179728507996
train_iter_loss: 0.27611246705055237
train_iter_loss: 0.3025915324687958
train_iter_loss: 0.26557987928390503
train_iter_loss: 0.37156563997268677
train_iter_loss: 0.24892021715641022
train_iter_loss: 0.363943487405777
train_iter_loss: 0.3606562316417694
train_iter_loss: 0.1791975200176239
train_iter_loss: 0.19964735209941864
train_iter_loss: 0.3628610074520111
train_iter_loss: 0.23498409986495972
train_iter_loss: 0.28416600823402405
train_iter_loss: 0.300887793302536
train_iter_loss: 0.2708452045917511
train_iter_loss: 0.31747519969940186
train_iter_loss: 0.29378172755241394
train_iter_loss: 0.38036292791366577
train_iter_loss: 0.3569137156009674
train_iter_loss: 0.26513513922691345
train_iter_loss: 0.29521358013153076
train_iter_loss: 0.5018825531005859
train_iter_loss: 0.3288353383541107
train_iter_loss: 0.3725707530975342
train_iter_loss: 0.3294644355773926
train_iter_loss: 0.3014093041419983
train_iter_loss: 0.37657278776168823
train_iter_loss: 0.3278719186782837
train_iter_loss: 0.3237918019294739
train_iter_loss: 0.23941916227340698
train_iter_loss: 0.2803148925304413
train_iter_loss: 0.3932822048664093
train_iter_loss: 0.3339221775531769
train_iter_loss: 0.296049028635025
train_iter_loss: 0.2480439841747284
train_iter_loss: 0.25085023045539856
train_iter_loss: 0.26941660046577454
train_iter_loss: 0.365325003862381
train_iter_loss: 0.34296920895576477
train_iter_loss: 0.3214750289916992
train_iter_loss: 0.22414563596248627
train_iter_loss: 0.22464464604854584
train_iter_loss: 0.23898670077323914
train_iter_loss: 0.2826300859451294
train_iter_loss: 0.3216699957847595
train_iter_loss: 0.3005087375640869
train_iter_loss: 0.3148667812347412
train_iter_loss: 0.30109384655952454
train_iter_loss: 0.31677383184432983
train_iter_loss: 0.2000301629304886
train_iter_loss: 0.31068944931030273
train_iter_loss: 0.30543753504753113
train_iter_loss: 0.36442434787750244
train_iter_loss: 0.2983076870441437
train_iter_loss: 0.300778865814209
train_iter_loss: 0.1782708466053009
train_iter_loss: 0.3116813004016876
train_iter_loss: 0.2964165210723877
train_iter_loss: 0.2566404342651367
train_iter_loss: 0.16930687427520752
train_iter_loss: 0.33540135622024536
train_iter_loss: 0.16667693853378296
train_iter_loss: 0.3027605712413788
train_iter_loss: 0.24874675273895264
train_iter_loss: 0.2625334858894348
train_iter_loss: 0.30308371782302856
train_iter_loss: 0.34223833680152893
train_iter_loss: 0.3184782862663269
train_iter_loss: 0.29027000069618225
train_iter_loss: 0.3010083734989166
train_iter_loss: 0.2883082926273346
train_iter_loss: 0.3179640769958496
train_iter_loss: 0.2731752395629883
train_iter_loss: 0.2199224829673767
train_iter_loss: 0.33228814601898193
train_iter_loss: 0.30509495735168457
train_iter_loss: 0.2808161973953247
train_iter_loss: 0.37484413385391235
train_iter_loss: 0.24013230204582214
train_iter_loss: 0.19052866101264954
train_iter_loss: 0.30994099378585815
train_iter_loss: 0.2933754026889801
train_iter_loss: 0.24911966919898987
train_iter_loss: 0.3590565025806427
train_iter_loss: 0.2857487201690674
train_iter_loss: 0.24901798367500305
train_iter_loss: 0.1939939558506012
train_iter_loss: 0.26516294479370117
train_iter_loss: 0.34559738636016846
train_iter_loss: 0.2965084910392761
train_iter_loss: 0.2770283818244934
train_iter_loss: 0.40671610832214355
train_iter_loss: 0.2942950129508972
train_iter_loss: 0.2671944499015808
train loss :0.2993
---------------------
Validation seg loss: 0.31087200336580006 at epoch 11
********************
best_val_epoch_loss:  0.31087200336580006
MODEL UPDATED
epoch =     12/  1000, exp = train
train_iter_loss: 0.35498276352882385
train_iter_loss: 0.36319905519485474
train_iter_loss: 0.2481236308813095
train_iter_loss: 0.38358432054519653
train_iter_loss: 0.27225461602211
train_iter_loss: 0.45581430196762085
train_iter_loss: 0.3229624927043915
train_iter_loss: 0.3908957242965698
train_iter_loss: 0.24984654784202576
train_iter_loss: 0.276932954788208
train_iter_loss: 0.24561594426631927
train_iter_loss: 0.2056945413351059
train_iter_loss: 0.2269015908241272
train_iter_loss: 0.3246860206127167
train_iter_loss: 0.35438457131385803
train_iter_loss: 0.4181790351867676
train_iter_loss: 0.24548271298408508
train_iter_loss: 0.1848544180393219
train_iter_loss: 0.26992762088775635
train_iter_loss: 0.32557424902915955
train_iter_loss: 0.28582367300987244
train_iter_loss: 0.3305521607398987
train_iter_loss: 0.25824490189552307
train_iter_loss: 0.3457443118095398
train_iter_loss: 0.28049731254577637
train_iter_loss: 0.36966440081596375
train_iter_loss: 0.29908034205436707
train_iter_loss: 0.35564133524894714
train_iter_loss: 0.34523260593414307
train_iter_loss: 0.20558536052703857
train_iter_loss: 0.24426934123039246
train_iter_loss: 0.1988455206155777
train_iter_loss: 0.2969802916049957
train_iter_loss: 0.26001453399658203
train_iter_loss: 0.3199009299278259
train_iter_loss: 0.1716618835926056
train_iter_loss: 0.265505850315094
train_iter_loss: 0.3087635338306427
train_iter_loss: 0.18904563784599304
train_iter_loss: 0.3649638891220093
train_iter_loss: 0.3293379843235016
train_iter_loss: 0.3674232065677643
train_iter_loss: 0.3012605607509613
train_iter_loss: 0.3552214503288269
train_iter_loss: 0.3253820538520813
train_iter_loss: 0.36214184761047363
train_iter_loss: 0.3428206443786621
train_iter_loss: 0.26091185212135315
train_iter_loss: 0.3252330422401428
train_iter_loss: 0.30583563446998596
train_iter_loss: 0.3296464681625366
train_iter_loss: 0.2741478979587555
train_iter_loss: 0.26643359661102295
train_iter_loss: 0.4336857795715332
train_iter_loss: 0.4193347692489624
train_iter_loss: 0.38241127133369446
train_iter_loss: 0.20920062065124512
train_iter_loss: 0.2882792055606842
train_iter_loss: 0.3210204839706421
train_iter_loss: 0.28090694546699524
train_iter_loss: 0.3097675144672394
train_iter_loss: 0.35586991906166077
train_iter_loss: 0.25442764163017273
train_iter_loss: 0.29363754391670227
train_iter_loss: 0.22686277329921722
train_iter_loss: 0.4109826683998108
train_iter_loss: 0.2508009970188141
train_iter_loss: 0.2964937388896942
train_iter_loss: 0.36259135603904724
train_iter_loss: 0.35498467087745667
train_iter_loss: 0.3735980689525604
train_iter_loss: 0.19041003286838531
train_iter_loss: 0.40505796670913696
train_iter_loss: 0.2888800799846649
train_iter_loss: 0.228683203458786
train_iter_loss: 0.3572293519973755
train_iter_loss: 0.3393676280975342
train_iter_loss: 0.35061609745025635
train_iter_loss: 0.32614701986312866
train_iter_loss: 0.44430941343307495
train_iter_loss: 0.5430534482002258
train_iter_loss: 0.24666650593280792
train_iter_loss: 0.37174785137176514
train_iter_loss: 0.33768430352211
train_iter_loss: 0.3471558392047882
train_iter_loss: 0.2595708966255188
train_iter_loss: 0.277500718832016
train_iter_loss: 0.2691892087459564
train_iter_loss: 0.21171671152114868
train_iter_loss: 0.2560749053955078
train_iter_loss: 0.21275703608989716
train_iter_loss: 0.3450266122817993
train_iter_loss: 0.12938839197158813
train_iter_loss: 0.2996473014354706
train_iter_loss: 0.2591359317302704
train_iter_loss: 0.3282592296600342
train_iter_loss: 0.24330554902553558
train_iter_loss: 0.30365368723869324
train_iter_loss: 0.23519332706928253
train_iter_loss: 0.2973243296146393
train loss :0.3079
---------------------
Validation seg loss: 0.2897000826614083 at epoch 12
********************
best_val_epoch_loss:  0.2897000826614083
MODEL UPDATED
epoch =     13/  1000, exp = train
train_iter_loss: 0.22655898332595825
train_iter_loss: 0.31695979833602905
train_iter_loss: 0.28030890226364136
train_iter_loss: 0.35884812474250793
train_iter_loss: 0.35532596707344055
train_iter_loss: 0.2864498794078827
train_iter_loss: 0.16604049503803253
train_iter_loss: 0.1833711862564087
train_iter_loss: 0.3804928958415985
train_iter_loss: 0.23315568268299103
train_iter_loss: 0.3653239607810974
train_iter_loss: 0.37067320942878723
train_iter_loss: 0.2986009418964386
train_iter_loss: 0.2980564832687378
train_iter_loss: 0.25402820110321045
train_iter_loss: 0.29333099722862244
train_iter_loss: 0.2316822111606598
train_iter_loss: 0.3370925188064575
train_iter_loss: 0.34997111558914185
train_iter_loss: 0.2816067039966583
train_iter_loss: 0.33032864332199097
train_iter_loss: 0.2681258022785187
train_iter_loss: 0.330116331577301
train_iter_loss: 0.33928555250167847
train_iter_loss: 0.32160523533821106
train_iter_loss: 0.3204527199268341
train_iter_loss: 0.2697053849697113
train_iter_loss: 0.31976649165153503
train_iter_loss: 0.25073134899139404
train_iter_loss: 0.2442307323217392
train_iter_loss: 0.33638104796409607
train_iter_loss: 0.3066096901893616
train_iter_loss: 0.1812722086906433
train_iter_loss: 0.23317669332027435
train_iter_loss: 0.33747926354408264
train_iter_loss: 0.2892940044403076
train_iter_loss: 0.17976178228855133
train_iter_loss: 0.2636583149433136
train_iter_loss: 0.2933736741542816
train_iter_loss: 0.31406351923942566
train_iter_loss: 0.3035690486431122
train_iter_loss: 0.32815486192703247
train_iter_loss: 0.3042527139186859
train_iter_loss: 0.22882138192653656
train_iter_loss: 0.28226837515830994
train_iter_loss: 0.32900843024253845
train_iter_loss: 0.29075250029563904
train_iter_loss: 0.35985496640205383
train_iter_loss: 0.2524340748786926
train_iter_loss: 0.3238043189048767
train_iter_loss: 0.37766969203948975
train_iter_loss: 0.28231486678123474
train_iter_loss: 0.23543666303157806
train_iter_loss: 0.32934024930000305
train_iter_loss: 0.27455154061317444
train_iter_loss: 0.2921690344810486
train_iter_loss: 0.3137228786945343
train_iter_loss: 0.33065956830978394
train_iter_loss: 0.2933765947818756
train_iter_loss: 0.3749524652957916
train_iter_loss: 0.22900225222110748
train_iter_loss: 0.31134679913520813
train_iter_loss: 0.28904861211776733
train_iter_loss: 0.23448120057582855
train_iter_loss: 0.32741066813468933
train_iter_loss: 0.26132845878601074
train_iter_loss: 0.3450326919555664
train_iter_loss: 0.31547245383262634
train_iter_loss: 0.3628722131252289
train_iter_loss: 0.2782881557941437
train_iter_loss: 0.16317887604236603
train_iter_loss: 0.21970833837985992
train_iter_loss: 0.27599114179611206
train_iter_loss: 0.2532186210155487
train_iter_loss: 0.24321025609970093
train_iter_loss: 0.26544782519340515
train_iter_loss: 0.25823861360549927
train_iter_loss: 0.2725876569747925
train_iter_loss: 0.27090245485305786
train_iter_loss: 0.33741721510887146
train_iter_loss: 0.25429481267929077
train_iter_loss: 0.2220963090658188
train_iter_loss: 0.2623600959777832
train_iter_loss: 0.31172624230384827
train_iter_loss: 0.4203704297542572
train_iter_loss: 0.30164647102355957
train_iter_loss: 0.27446168661117554
train_iter_loss: 0.2877022624015808
train_iter_loss: 0.27137401700019836
train_iter_loss: 0.2411341369152069
train_iter_loss: 0.28473690152168274
train_iter_loss: 0.3101467490196228
train_iter_loss: 0.45605039596557617
train_iter_loss: 0.238310769200325
train_iter_loss: 0.1740674525499344
train_iter_loss: 0.31339630484580994
train_iter_loss: 0.2404344230890274
train_iter_loss: 0.19763368368148804
train_iter_loss: 0.1591338813304901
train_iter_loss: 0.25537464022636414
train loss :0.2900
---------------------
Validation seg loss: 0.28503382972107744 at epoch 13
********************
best_val_epoch_loss:  0.28503382972107744
MODEL UPDATED
epoch =     14/  1000, exp = train
train_iter_loss: 0.23816940188407898
train_iter_loss: 0.10719681531190872
train_iter_loss: 0.23998388648033142
train_iter_loss: 0.24344474077224731
train_iter_loss: 0.3925948143005371
train_iter_loss: 0.32081878185272217
train_iter_loss: 0.1654164046049118
train_iter_loss: 0.3646053671836853
train_iter_loss: 0.3815476596355438
train_iter_loss: 0.20177000761032104
train_iter_loss: 0.31159934401512146
train_iter_loss: 0.34630540013313293
train_iter_loss: 0.23018969595432281
train_iter_loss: 0.3577723503112793
train_iter_loss: 0.3605363667011261
train_iter_loss: 0.3693445920944214
train_iter_loss: 0.19398212432861328
train_iter_loss: 0.2450157254934311
train_iter_loss: 0.2323601096868515
train_iter_loss: 0.3319368362426758
train_iter_loss: 0.28595808148384094
train_iter_loss: 0.21829137206077576
train_iter_loss: 0.28952017426490784
train_iter_loss: 0.2965722680091858
train_iter_loss: 0.3564676344394684
train_iter_loss: 0.3189254105091095
train_iter_loss: 0.1459691971540451
train_iter_loss: 0.20601923763751984
train_iter_loss: 0.2496880739927292
train_iter_loss: 0.2352045625448227
train_iter_loss: 0.2536074221134186
train_iter_loss: 0.2519274055957794
train_iter_loss: 0.33531454205513
train_iter_loss: 0.3306930959224701
train_iter_loss: 0.20228086411952972
train_iter_loss: 0.25777944922447205
train_iter_loss: 0.38577476143836975
train_iter_loss: 0.17221929132938385
train_iter_loss: 0.43054214119911194
train_iter_loss: 0.2970185875892639
train_iter_loss: 0.3243013322353363
train_iter_loss: 0.2191321700811386
train_iter_loss: 0.22489193081855774
train_iter_loss: 0.2627720832824707
train_iter_loss: 0.2741813361644745
train_iter_loss: 0.2636266350746155
train_iter_loss: 0.3026871085166931
train_iter_loss: 0.3008197844028473
train_iter_loss: 0.2882588803768158
train_iter_loss: 0.27459296584129333
train_iter_loss: 0.33849063515663147
train_iter_loss: 0.3162181079387665
train_iter_loss: 0.2557455003261566
train_iter_loss: 0.30784595012664795
train_iter_loss: 0.293072372674942
train_iter_loss: 0.27908429503440857
train_iter_loss: 0.27147695422172546
train_iter_loss: 0.24456529319286346
train_iter_loss: 0.27086806297302246
train_iter_loss: 0.2673269212245941
train_iter_loss: 0.23417800664901733
train_iter_loss: 0.2698603570461273
train_iter_loss: 0.29725903272628784
train_iter_loss: 0.2699529826641083
train_iter_loss: 0.2382393777370453
train_iter_loss: 0.1948658525943756
train_iter_loss: 0.290647029876709
train_iter_loss: 0.2637159526348114
train_iter_loss: 0.277774453163147
train_iter_loss: 0.2339828908443451
train_iter_loss: 0.3332996368408203
train_iter_loss: 0.23004016280174255
train_iter_loss: 0.3849845230579376
train_iter_loss: 0.25480255484580994
train_iter_loss: 0.25091999769210815
train_iter_loss: 0.2477264106273651
train_iter_loss: 0.23736277222633362
train_iter_loss: 0.26503098011016846
train_iter_loss: 0.3836115300655365
train_iter_loss: 0.2745928168296814
train_iter_loss: 0.24553649127483368
train_iter_loss: 0.16210971772670746
train_iter_loss: 0.2218095362186432
train_iter_loss: 0.24782143533229828
train_iter_loss: 0.32915306091308594
train_iter_loss: 0.22043795883655548
train_iter_loss: 0.2339237481355667
train_iter_loss: 0.3236515522003174
train_iter_loss: 0.3370899558067322
train_iter_loss: 0.384823739528656
train_iter_loss: 0.243903249502182
train_iter_loss: 0.24927616119384766
train_iter_loss: 0.30949822068214417
train_iter_loss: 0.30866143107414246
train_iter_loss: 0.23064112663269043
train_iter_loss: 0.3289851248264313
train_iter_loss: 0.24314045906066895
train_iter_loss: 0.3505963683128357
train_iter_loss: 0.32125216722488403
train_iter_loss: 0.28687945008277893
train loss :0.2804
---------------------
Validation seg loss: 0.294852808750463 at epoch 14
epoch =     15/  1000, exp = train
train_iter_loss: 0.3511137068271637
train_iter_loss: 0.4705542027950287
train_iter_loss: 0.27667105197906494
train_iter_loss: 0.19702638685703278
train_iter_loss: 0.31770560145378113
train_iter_loss: 0.2949844300746918
train_iter_loss: 0.2895776927471161
train_iter_loss: 0.25416645407676697
train_iter_loss: 0.22553026676177979
train_iter_loss: 0.2976330518722534
train_iter_loss: 0.2910839915275574
train_iter_loss: 0.2611282169818878
train_iter_loss: 0.19744379818439484
train_iter_loss: 0.36151930689811707
train_iter_loss: 0.21026338636875153
train_iter_loss: 0.23641039431095123
train_iter_loss: 0.199050173163414
train_iter_loss: 0.23441389203071594
train_iter_loss: 0.27645617723464966
train_iter_loss: 0.24220623075962067
train_iter_loss: 0.36263859272003174
train_iter_loss: 0.17553390562534332
train_iter_loss: 0.39049386978149414
train_iter_loss: 0.28367966413497925
train_iter_loss: 0.21739926934242249
train_iter_loss: 0.22545915842056274
train_iter_loss: 0.28357329964637756
train_iter_loss: 0.2073766589164734
train_iter_loss: 0.2545582950115204
train_iter_loss: 0.2805725336074829
train_iter_loss: 0.2563428282737732
train_iter_loss: 0.36627113819122314
train_iter_loss: 0.1286340057849884
train_iter_loss: 0.3480154573917389
train_iter_loss: 0.23014996945858002
train_iter_loss: 0.28461751341819763
train_iter_loss: 0.16661755740642548
train_iter_loss: 0.27891796827316284
train_iter_loss: 0.24947085976600647
train_iter_loss: 0.18758153915405273
train_iter_loss: 0.23304340243339539
train_iter_loss: 0.2539142668247223
train_iter_loss: 0.31372126936912537
train_iter_loss: 0.35524505376815796
train_iter_loss: 0.2976486384868622
train_iter_loss: 0.21099767088890076
train_iter_loss: 0.27453818917274475
train_iter_loss: 0.3357744514942169
train_iter_loss: 0.2767799496650696
train_iter_loss: 0.28359103202819824
train_iter_loss: 0.22439363598823547
train_iter_loss: 0.18872138857841492
train_iter_loss: 0.28154534101486206
train_iter_loss: 0.15728643536567688
train_iter_loss: 0.2614568769931793
train_iter_loss: 0.2023516297340393
train_iter_loss: 0.1883188784122467
train_iter_loss: 0.22379079461097717
train_iter_loss: 0.28047284483909607
train_iter_loss: 0.23168791830539703
train_iter_loss: 0.25657835602760315
train_iter_loss: 0.1922861933708191
train_iter_loss: 0.317147433757782
train_iter_loss: 0.30472028255462646
train_iter_loss: 0.20388741791248322
train_iter_loss: 0.22489574551582336
train_iter_loss: 0.4606609344482422
train_iter_loss: 0.258352667093277
train_iter_loss: 0.33269140124320984
train_iter_loss: 0.38872525095939636
train_iter_loss: 0.44434016942977905
train_iter_loss: 0.24485576152801514
train_iter_loss: 0.310177206993103
train_iter_loss: 0.23966702818870544
train_iter_loss: 0.184825599193573
train_iter_loss: 0.2500030994415283
train_iter_loss: 0.3088786005973816
train_iter_loss: 0.3177209496498108
train_iter_loss: 0.2606678903102875
train_iter_loss: 0.25984159111976624
train_iter_loss: 0.34654107689857483
train_iter_loss: 0.32075613737106323
train_iter_loss: 0.27100181579589844
train_iter_loss: 0.23618920147418976
train_iter_loss: 0.28991174697875977
train_iter_loss: 0.25855493545532227
train_iter_loss: 0.29427865147590637
train_iter_loss: 0.28174522519111633
train_iter_loss: 0.27726101875305176
train_iter_loss: 0.3354494571685791
train_iter_loss: 0.3474743962287903
train_iter_loss: 0.35625356435775757
train_iter_loss: 0.29743635654449463
train_iter_loss: 0.2427925318479538
train_iter_loss: 0.2505984902381897
train_iter_loss: 0.1730283796787262
train_iter_loss: 0.2564100921154022
train_iter_loss: 0.3017638921737671
train_iter_loss: 0.27199146151542664
train_iter_loss: 0.23506715893745422
train loss :0.2742
---------------------
Validation seg loss: 0.2788843927940108 at epoch 15
********************
best_val_epoch_loss:  0.2788843927940108
MODEL UPDATED
epoch =     16/  1000, exp = train
train_iter_loss: 0.30169248580932617
train_iter_loss: 0.24337232112884521
train_iter_loss: 0.22435703873634338
train_iter_loss: 0.19618430733680725
train_iter_loss: 0.318683922290802
train_iter_loss: 0.2779647409915924
train_iter_loss: 0.2939979135990143
train_iter_loss: 0.16685804724693298
train_iter_loss: 0.23621763288974762
train_iter_loss: 0.1926354616880417
train_iter_loss: 0.28619664907455444
train_iter_loss: 0.25303760170936584
train_iter_loss: 0.18709897994995117
train_iter_loss: 0.27834388613700867
train_iter_loss: 0.29074111580848694
train_iter_loss: 0.45670390129089355
train_iter_loss: 0.3115077316761017
train_iter_loss: 0.2816012501716614
train_iter_loss: 0.3717806339263916
train_iter_loss: 0.22885876893997192
train_iter_loss: 0.35645273327827454
train_iter_loss: 0.2321566641330719
train_iter_loss: 0.3407634496688843
train_iter_loss: 0.22459615767002106
train_iter_loss: 0.18522916734218597
train_iter_loss: 0.1796540766954422
train_iter_loss: 0.27567175030708313
train_iter_loss: 0.2950712740421295
train_iter_loss: 0.39005517959594727
train_iter_loss: 0.2773556411266327
train_iter_loss: 0.27106356620788574
train_iter_loss: 0.34673231840133667
train_iter_loss: 0.3171001970767975
train_iter_loss: 0.3236157298088074
train_iter_loss: 0.32973921298980713
train_iter_loss: 0.14483140408992767
train_iter_loss: 0.24271374940872192
train_iter_loss: 0.27596303820610046
train_iter_loss: 0.20779936015605927
train_iter_loss: 0.2440890371799469
train_iter_loss: 0.2358238846063614
train_iter_loss: 0.3568236231803894
train_iter_loss: 0.14502520859241486
train_iter_loss: 0.3487934470176697
train_iter_loss: 0.24156931042671204
train_iter_loss: 0.28365883231163025
train_iter_loss: 0.15872569382190704
train_iter_loss: 0.3510453701019287
train_iter_loss: 0.3280964493751526
train_iter_loss: 0.24077534675598145
train_iter_loss: 0.26546338200569153
train_iter_loss: 0.23756325244903564
train_iter_loss: 0.37835603952407837
train_iter_loss: 0.2098931223154068
train_iter_loss: 0.30994129180908203
train_iter_loss: 0.3208054006099701
train_iter_loss: 0.22468771040439606
train_iter_loss: 0.25656941533088684
train_iter_loss: 0.2660978138446808
train_iter_loss: 0.27049246430397034
train_iter_loss: 0.25079014897346497
train_iter_loss: 0.24340391159057617
train_iter_loss: 0.3538576364517212
train_iter_loss: 0.3427405059337616
train_iter_loss: 0.20194607973098755
train_iter_loss: 0.1720387488603592
train_iter_loss: 0.09793779999017715
train_iter_loss: 0.3611224591732025
train_iter_loss: 0.23887045681476593
train_iter_loss: 0.23287345468997955
train_iter_loss: 0.17122790217399597
train_iter_loss: 0.3465611934661865
train_iter_loss: 0.3551925718784332
train_iter_loss: 0.31202206015586853
train_iter_loss: 0.22237823903560638
train_iter_loss: 0.28479015827178955
train_iter_loss: 0.19893507659435272
train_iter_loss: 0.2700749933719635
train_iter_loss: 0.2674885392189026
train_iter_loss: 0.17462295293807983
train_iter_loss: 0.31999239325523376
train_iter_loss: 0.22532398998737335
train_iter_loss: 0.20827887952327728
train_iter_loss: 0.19131016731262207
train_iter_loss: 0.19177773594856262
train_iter_loss: 0.28989148139953613
train_iter_loss: 0.2832341492176056
train_iter_loss: 0.2958696782588959
train_iter_loss: 0.32191842794418335
train_iter_loss: 0.2954120635986328
train_iter_loss: 0.2861206829547882
train_iter_loss: 0.2819165885448456
train_iter_loss: 0.24013882875442505
train_iter_loss: 0.2621304392814636
train_iter_loss: 0.26460006833076477
train_iter_loss: 0.26987841725349426
train_iter_loss: 0.23571279644966125
train_iter_loss: 0.39873501658439636
train_iter_loss: 0.2967853844165802
train_iter_loss: 0.26117366552352905
train loss :0.2708
---------------------
Validation seg loss: 0.2850728529101273 at epoch 16
epoch =     17/  1000, exp = train
train_iter_loss: 0.2718649208545685
train_iter_loss: 0.35089898109436035
train_iter_loss: 0.2797197699546814
train_iter_loss: 0.202367901802063
train_iter_loss: 0.3080895245075226
train_iter_loss: 0.27649688720703125
train_iter_loss: 0.3168683648109436
train_iter_loss: 0.2839006185531616
train_iter_loss: 0.3064131438732147
train_iter_loss: 0.20140443742275238
train_iter_loss: 0.2589905858039856
train_iter_loss: 0.25463536381721497
train_iter_loss: 0.27606305480003357
train_iter_loss: 0.43879300355911255
train_iter_loss: 0.30395641922950745
train_iter_loss: 0.26024875044822693
train_iter_loss: 0.3116532266139984
train_iter_loss: 0.28237560391426086
train_iter_loss: 0.31556767225265503
train_iter_loss: 0.20019477605819702
train_iter_loss: 0.25886696577072144
train_iter_loss: 0.28485414385795593
train_iter_loss: 0.12762436270713806
train_iter_loss: 0.25063610076904297
train_iter_loss: 0.27648109197616577
train_iter_loss: 0.3010203540325165
train_iter_loss: 0.3348701298236847
train_iter_loss: 0.35228434205055237
train_iter_loss: 0.2688876688480377
train_iter_loss: 0.2866782248020172
train_iter_loss: 0.1732560247182846
train_iter_loss: 0.17520691454410553
train_iter_loss: 0.26871588826179504
train_iter_loss: 0.2482193410396576
train_iter_loss: 0.26471033692359924
train_iter_loss: 0.54398113489151
train_iter_loss: 0.26722872257232666
train_iter_loss: 0.3148512542247772
train_iter_loss: 0.33816683292388916
train_iter_loss: 0.19708363711833954
train_iter_loss: 0.26036307215690613
train_iter_loss: 0.2729771137237549
train_iter_loss: 0.18272113800048828
train_iter_loss: 0.3111742436885834
train_iter_loss: 0.22827494144439697
train_iter_loss: 0.24814946949481964
train_iter_loss: 0.1901635229587555
train_iter_loss: 0.21582245826721191
train_iter_loss: 0.23112936317920685
train_iter_loss: 0.3300752341747284
train_iter_loss: 0.25158920884132385
train_iter_loss: 0.28882402181625366
train_iter_loss: 0.3509881794452667
train_iter_loss: 0.306664377450943
train_iter_loss: 0.1727992743253708
train_iter_loss: 0.3244399130344391
train_iter_loss: 0.20256364345550537
train_iter_loss: 0.11239707469940186
train_iter_loss: 0.2565098702907562
train_iter_loss: 0.32332706451416016
train_iter_loss: 0.32281944155693054
train_iter_loss: 0.29367631673812866
train_iter_loss: 0.24460873007774353
train_iter_loss: 0.24327155947685242
train_iter_loss: 0.39676886796951294
train_iter_loss: 0.33263713121414185
train_iter_loss: 0.23925825953483582
train_iter_loss: 0.30972710251808167
train_iter_loss: 0.23263360559940338
train_iter_loss: 0.21050015091896057
train_iter_loss: 0.20405475795269012
train_iter_loss: 0.3467496335506439
train_iter_loss: 0.21448464691638947
train_iter_loss: 0.19471235573291779
train_iter_loss: 0.2465566247701645
train_iter_loss: 0.28050553798675537
train_iter_loss: 0.24352331459522247
train_iter_loss: 0.2736808955669403
train_iter_loss: 0.24060198664665222
train_iter_loss: 0.2154673933982849
train_iter_loss: 0.18830448389053345
train_iter_loss: 0.21746671199798584
train_iter_loss: 0.348023921251297
train_iter_loss: 0.28540167212486267
train_iter_loss: 0.3117564618587494
train_iter_loss: 0.18614377081394196
train_iter_loss: 0.17739669978618622
train_iter_loss: 0.16204491257667542
train_iter_loss: 0.30036213994026184
train_iter_loss: 0.17933988571166992
train_iter_loss: 0.35370293259620667
train_iter_loss: 0.2437768280506134
train_iter_loss: 0.31662654876708984
train_iter_loss: 0.2699505686759949
train_iter_loss: 0.2172696590423584
train_iter_loss: 0.20589444041252136
train_iter_loss: 0.25818026065826416
train_iter_loss: 0.2529601454734802
train_iter_loss: 0.30727630853652954
train_iter_loss: 0.23584094643592834
train loss :0.2687
---------------------
Validation seg loss: 0.2712025612873851 at epoch 17
********************
best_val_epoch_loss:  0.2712025612873851
MODEL UPDATED
epoch =     18/  1000, exp = train
train_iter_loss: 0.21891838312149048
train_iter_loss: 0.2444295585155487
train_iter_loss: 0.28094059228897095
train_iter_loss: 0.30768442153930664
train_iter_loss: 0.2824547290802002
train_iter_loss: 0.17167261242866516
train_iter_loss: 0.20484447479248047
train_iter_loss: 0.26883968710899353
train_iter_loss: 0.22825542092323303
train_iter_loss: 0.22174835205078125
train_iter_loss: 0.19192473590373993
train_iter_loss: 0.3389776945114136
train_iter_loss: 0.30138662457466125
train_iter_loss: 0.30715274810791016
train_iter_loss: 0.20176555216312408
train_iter_loss: 0.23808972537517548
train_iter_loss: 0.33030736446380615
train_iter_loss: 0.2964232265949249
train_iter_loss: 0.19519609212875366
train_iter_loss: 0.2257816046476364
train_iter_loss: 0.20365333557128906
train_iter_loss: 0.3315274715423584
train_iter_loss: 0.19603510200977325
train_iter_loss: 0.2646186053752899
train_iter_loss: 0.32181647419929504
train_iter_loss: 0.25651684403419495
train_iter_loss: 0.23803119361400604
train_iter_loss: 0.2456178069114685
train_iter_loss: 0.18029063940048218
train_iter_loss: 0.33023080229759216
train_iter_loss: 0.322031706571579
train_iter_loss: 0.2776878774166107
train_iter_loss: 0.2928304672241211
train_iter_loss: 0.2472795844078064
train_iter_loss: 0.3329976499080658
train_iter_loss: 0.2470608651638031
train_iter_loss: 0.2227432280778885
train_iter_loss: 0.2503124177455902
train_iter_loss: 0.3236140012741089
train_iter_loss: 0.1702401340007782
train_iter_loss: 0.3063075840473175
train_iter_loss: 0.286722332239151
train_iter_loss: 0.20844900608062744
train_iter_loss: 0.2817044258117676
train_iter_loss: 0.2889348268508911
train_iter_loss: 0.2936602234840393
train_iter_loss: 0.1308961808681488
train_iter_loss: 0.19031740725040436
train_iter_loss: 0.2500430941581726
train_iter_loss: 0.21166127920150757
train_iter_loss: 0.2275746762752533
train_iter_loss: 0.2113441675901413
train_iter_loss: 0.3341420888900757
train_iter_loss: 0.22242973744869232
train_iter_loss: 0.32559871673583984
train_iter_loss: 0.32856568694114685
train_iter_loss: 0.2188962697982788
train_iter_loss: 0.29339689016342163
train_iter_loss: 0.1955159604549408
train_iter_loss: 0.18345597386360168
train_iter_loss: 0.17340531945228577
train_iter_loss: 0.2803193926811218
train_iter_loss: 0.22711071372032166
train_iter_loss: 0.2650182247161865
train_iter_loss: 0.21158729493618011
train_iter_loss: 0.39325228333473206
train_iter_loss: 0.2987979054450989
train_iter_loss: 0.2614102065563202
train_iter_loss: 0.2196280062198639
train_iter_loss: 0.19844110310077667
train_iter_loss: 0.1800675392150879
train_iter_loss: 0.2130892425775528
train_iter_loss: 0.32747897505760193
train_iter_loss: 0.18569707870483398
train_iter_loss: 0.17878571152687073
train_iter_loss: 0.31353995203971863
train_iter_loss: 0.22411444783210754
train_iter_loss: 0.19788794219493866
train_iter_loss: 0.2628463804721832
train_iter_loss: 0.20261122286319733
train_iter_loss: 0.2354198545217514
train_iter_loss: 0.2954421639442444
train_iter_loss: 0.22907176613807678
train_iter_loss: 0.2783457636833191
train_iter_loss: 0.24348010122776031
train_iter_loss: 0.24788816273212433
train_iter_loss: 0.32642608880996704
train_iter_loss: 0.38828977942466736
train_iter_loss: 0.17234721779823303
train_iter_loss: 0.17563819885253906
train_iter_loss: 0.28573498129844666
train_iter_loss: 0.22800257802009583
train_iter_loss: 0.2562018632888794
train_iter_loss: 0.3147110939025879
train_iter_loss: 0.26272451877593994
train_iter_loss: 0.248518168926239
train_iter_loss: 0.23094825446605682
train_iter_loss: 0.21328775584697723
train_iter_loss: 0.29296875
train_iter_loss: 0.24775919318199158
train loss :0.2556
---------------------
Validation seg loss: 0.2630736672512765 at epoch 18
********************
best_val_epoch_loss:  0.2630736672512765
MODEL UPDATED
epoch =     19/  1000, exp = train
train_iter_loss: 0.19386686384677887
train_iter_loss: 0.21647100150585175
train_iter_loss: 0.26301586627960205
train_iter_loss: 0.2951049208641052
train_iter_loss: 0.24397821724414825
train_iter_loss: 0.258301317691803
train_iter_loss: 0.253612756729126
train_iter_loss: 0.21423189342021942
train_iter_loss: 0.23000264167785645
train_iter_loss: 0.1429261416196823
train_iter_loss: 0.25853773951530457
train_iter_loss: 0.2093430757522583
train_iter_loss: 0.28113195300102234
train_iter_loss: 0.26615506410598755
train_iter_loss: 0.31058529019355774
train_iter_loss: 0.22288592159748077
train_iter_loss: 0.23045875132083893
train_iter_loss: 0.25943705439567566
train_iter_loss: 0.27131524682044983
train_iter_loss: 0.2338331937789917
train_iter_loss: 0.2509150207042694
train_iter_loss: 0.24189557135105133
train_iter_loss: 0.2632125914096832
train_iter_loss: 0.2062487155199051
train_iter_loss: 0.23271751403808594
train_iter_loss: 0.30556923151016235
train_iter_loss: 0.2657104730606079
train_iter_loss: 0.2979007661342621
train_iter_loss: 0.2759920656681061
train_iter_loss: 0.1912960410118103
train_iter_loss: 0.28839367628097534
train_iter_loss: 0.4546678066253662
train_iter_loss: 0.3647749125957489
train_iter_loss: 0.23621933162212372
train_iter_loss: 0.19472132623195648
train_iter_loss: 0.17281050980091095
train_iter_loss: 0.21735474467277527
train_iter_loss: 0.2048751562833786
train_iter_loss: 0.42696213722229004
train_iter_loss: 0.21702395379543304
train_iter_loss: 0.19540414214134216
train_iter_loss: 0.28649842739105225
train_iter_loss: 0.19011007249355316
train_iter_loss: 0.2875259816646576
train_iter_loss: 0.21616725623607635
train_iter_loss: 0.2265738695859909
train_iter_loss: 0.30817270278930664
train_iter_loss: 0.30925485491752625
train_iter_loss: 0.17394834756851196
train_iter_loss: 0.33531853556632996
train_iter_loss: 0.2848716676235199
train_iter_loss: 0.18497712910175323
train_iter_loss: 0.248935267329216
train_iter_loss: 0.12929224967956543
train_iter_loss: 0.2610462009906769
train_iter_loss: 0.23720793426036835
train_iter_loss: 0.28306612372398376
train_iter_loss: 0.33045339584350586
train_iter_loss: 0.18325799703598022
train_iter_loss: 0.2001703530550003
train_iter_loss: 0.2262817621231079
train_iter_loss: 0.28856733441352844
train_iter_loss: 0.3193260133266449
train_iter_loss: 0.25398561358451843
train_iter_loss: 0.190912663936615
train_iter_loss: 0.3263150155544281
train_iter_loss: 0.22836793959140778
train_iter_loss: 0.29618677496910095
train_iter_loss: 0.1844305843114853
train_iter_loss: 0.16878128051757812
train_iter_loss: 0.3018735945224762
train_iter_loss: 0.17340800166130066
train_iter_loss: 0.25293245911598206
train_iter_loss: 0.28203433752059937
train_iter_loss: 0.27771076560020447
train_iter_loss: 0.3234337270259857
train_iter_loss: 0.12087476998567581
train_iter_loss: 0.34804433584213257
train_iter_loss: 0.3082904815673828
train_iter_loss: 0.28268906474113464
train_iter_loss: 0.1811497062444687
train_iter_loss: 0.2532551884651184
train_iter_loss: 0.2717640995979309
train_iter_loss: 0.27913013100624084
train_iter_loss: 0.24773123860359192
train_iter_loss: 0.3095649182796478
train_iter_loss: 0.24490481615066528
train_iter_loss: 0.21685263514518738
train_iter_loss: 0.19250518083572388
train_iter_loss: 0.22527293860912323
train_iter_loss: 0.24179713428020477
train_iter_loss: 0.3035185933113098
train_iter_loss: 0.14062939584255219
train_iter_loss: 0.24929699301719666
train_iter_loss: 0.29183152318000793
train_iter_loss: 0.2576848268508911
train_iter_loss: 0.23818372189998627
train_iter_loss: 0.14626023173332214
train_iter_loss: 0.3250989615917206
train_iter_loss: 0.1841469705104828
train loss :0.2525
---------------------
Validation seg loss: 0.26243810932028966 at epoch 19
********************
best_val_epoch_loss:  0.26243810932028966
MODEL UPDATED
epoch =     20/  1000, exp = train
train_iter_loss: 0.19384925067424774
train_iter_loss: 0.21631357073783875
train_iter_loss: 0.22173532843589783
train_iter_loss: 0.21702897548675537
train_iter_loss: 0.18497858941555023
train_iter_loss: 0.18419086933135986
train_iter_loss: 0.32775622606277466
train_iter_loss: 0.16266372799873352
train_iter_loss: 0.32739853858947754
train_iter_loss: 0.23829962313175201
train_iter_loss: 0.20506761968135834
train_iter_loss: 0.2637978196144104
train_iter_loss: 0.2929186224937439
train_iter_loss: 0.34332624077796936
train_iter_loss: 0.3478374481201172
train_iter_loss: 0.25008127093315125
train_iter_loss: 0.1920027732849121
train_iter_loss: 0.21187418699264526
train_iter_loss: 0.12937940657138824
train_iter_loss: 0.2924916446208954
train_iter_loss: 0.1976185292005539
train_iter_loss: 0.23315197229385376
train_iter_loss: 0.33359917998313904
train_iter_loss: 0.24969501793384552
train_iter_loss: 0.2885790765285492
train_iter_loss: 0.22429220378398895
train_iter_loss: 0.31163808703422546
train_iter_loss: 0.3252508342266083
train_iter_loss: 0.16798916459083557
train_iter_loss: 0.19887329638004303
train_iter_loss: 0.21708321571350098
train_iter_loss: 0.19774502515792847
train_iter_loss: 0.23053812980651855
train_iter_loss: 0.25193724036216736
train_iter_loss: 0.22857332229614258
train_iter_loss: 0.2043042778968811
train_iter_loss: 0.20474474132061005
train_iter_loss: 0.26300281286239624
train_iter_loss: 0.2453719675540924
train_iter_loss: 0.26979711651802063
train_iter_loss: 0.2121550291776657
train_iter_loss: 0.1748957335948944
train_iter_loss: 0.32629626989364624
train_iter_loss: 0.2643927335739136
train_iter_loss: 0.21397355198860168
train_iter_loss: 0.2858092188835144
train_iter_loss: 0.2728450298309326
train_iter_loss: 0.1851048469543457
train_iter_loss: 0.2036105841398239
train_iter_loss: 0.17952781915664673
train_iter_loss: 0.369667649269104
train_iter_loss: 0.19602426886558533
train_iter_loss: 0.21002063155174255
train_iter_loss: 0.1829700469970703
train_iter_loss: 0.4153909981250763
train_iter_loss: 0.2800156772136688
train_iter_loss: 0.2388465255498886
train_iter_loss: 0.2013610303401947
train_iter_loss: 0.2789531946182251
train_iter_loss: 0.2172689437866211
train_iter_loss: 0.28620097041130066
train_iter_loss: 0.23447485268115997
train_iter_loss: 0.3133416771888733
train_iter_loss: 0.23793616890907288
train_iter_loss: 0.28838464617729187
train_iter_loss: 0.3023007810115814
train_iter_loss: 0.34504228830337524
train_iter_loss: 0.3049081563949585
train_iter_loss: 0.24628691375255585
train_iter_loss: 0.20258747041225433
train_iter_loss: 0.1630442589521408
train_iter_loss: 0.1995992362499237
train_iter_loss: 0.16960841417312622
train_iter_loss: 0.2836361527442932
train_iter_loss: 0.2318107932806015
train_iter_loss: 0.3041580319404602
train_iter_loss: 0.24286772310733795
train_iter_loss: 0.24764922261238098
train_iter_loss: 0.2851610481739044
train_iter_loss: 0.2422354817390442
train_iter_loss: 0.3400248885154724
train_iter_loss: 0.20607736706733704
train_iter_loss: 0.23100610077381134
train_iter_loss: 0.2817709445953369
train_iter_loss: 0.29654011130332947
train_iter_loss: 0.2103910595178604
train_iter_loss: 0.29338037967681885
train_iter_loss: 0.2557063698768616
train_iter_loss: 0.20493891835212708
train_iter_loss: 0.22630557417869568
train_iter_loss: 0.20846787095069885
train_iter_loss: 0.2684834897518158
train_iter_loss: 0.2284623086452484
train_iter_loss: 0.2727898061275482
train_iter_loss: 0.22438377141952515
train_iter_loss: 0.251198410987854
train_iter_loss: 0.28205618262290955
train_iter_loss: 0.27375537157058716
train_iter_loss: 0.22664612531661987
train_iter_loss: 0.29036465287208557
train loss :0.2501
---------------------
Validation seg loss: 0.2742275985063247 at epoch 20
epoch =     21/  1000, exp = train
train_iter_loss: 0.2418983280658722
train_iter_loss: 0.22559042274951935
train_iter_loss: 0.22980348765850067
train_iter_loss: 0.16745777428150177
train_iter_loss: 0.28635847568511963
train_iter_loss: 0.5319986939430237
train_iter_loss: 0.2942827045917511
train_iter_loss: 0.2697341740131378
train_iter_loss: 0.34701141715049744
train_iter_loss: 0.23404255509376526
train_iter_loss: 0.2289395034313202
train_iter_loss: 0.17915381491184235
train_iter_loss: 0.30731990933418274
train_iter_loss: 0.23721174895763397
train_iter_loss: 0.23218883574008942
train_iter_loss: 0.2115553617477417
train_iter_loss: 0.32642272114753723
train_iter_loss: 0.2406821995973587
train_iter_loss: 0.27732083201408386
train_iter_loss: 0.2216249257326126
train_iter_loss: 0.432068794965744
train_iter_loss: 0.24543166160583496
train_iter_loss: 0.2242661565542221
train_iter_loss: 0.22523561120033264
train_iter_loss: 0.20235547423362732
train_iter_loss: 0.2346964031457901
train_iter_loss: 0.24213014543056488
train_iter_loss: 0.22179634869098663
train_iter_loss: 0.2278810441493988
train_iter_loss: 0.21689318120479584
train_iter_loss: 0.2603953778743744
train_iter_loss: 0.24101199209690094
train_iter_loss: 0.27690866589546204
train_iter_loss: 0.22402913868427277
train_iter_loss: 0.1313912570476532
train_iter_loss: 0.2755677402019501
train_iter_loss: 0.33043617010116577
train_iter_loss: 0.3586682081222534
train_iter_loss: 0.29692789912223816
train_iter_loss: 0.14234788715839386
train_iter_loss: 0.16237562894821167
train_iter_loss: 0.2848788797855377
train_iter_loss: 0.31350886821746826
train_iter_loss: 0.218919575214386
train_iter_loss: 0.22223812341690063
train_iter_loss: 0.2542775571346283
train_iter_loss: 0.2127126306295395
train_iter_loss: 0.272990882396698
train_iter_loss: 0.29482272267341614
train_iter_loss: 0.29677215218544006
train_iter_loss: 0.3782421350479126
train_iter_loss: 0.24678713083267212
train_iter_loss: 0.3145516812801361
train_iter_loss: 0.28392788767814636
train_iter_loss: 0.28526315093040466
train_iter_loss: 0.2945261597633362
train_iter_loss: 0.19652865827083588
train_iter_loss: 0.15805229544639587
train_iter_loss: 0.21759547293186188
train_iter_loss: 0.3014504313468933
train_iter_loss: 0.27179470658302307
train_iter_loss: 0.25598710775375366
train_iter_loss: 0.3027673661708832
train_iter_loss: 0.26869842410087585
train_iter_loss: 0.28675371408462524
train_iter_loss: 0.22615361213684082
train_iter_loss: 0.11943043768405914
train_iter_loss: 0.3089980483055115
train_iter_loss: 0.3007335364818573
train_iter_loss: 0.22478070855140686
train_iter_loss: 0.3267427682876587
train_iter_loss: 0.17489950358867645
train_iter_loss: 0.23289380967617035
train_iter_loss: 0.3465166389942169
train_iter_loss: 0.27762705087661743
train_iter_loss: 0.27444159984588623
train_iter_loss: 0.1837189644575119
train_iter_loss: 0.207328200340271
train_iter_loss: 0.15856117010116577
train_iter_loss: 0.14529624581336975
train_iter_loss: 0.23056286573410034
train_iter_loss: 0.187853142619133
train_iter_loss: 0.24991533160209656
train_iter_loss: 0.17613063752651215
train_iter_loss: 0.12927255034446716
train_iter_loss: 0.3396213948726654
train_iter_loss: 0.2046271413564682
train_iter_loss: 0.2990904152393341
train_iter_loss: 0.28337377309799194
train_iter_loss: 0.11556351184844971
train_iter_loss: 0.22491905093193054
train_iter_loss: 0.22664597630500793
train_iter_loss: 0.29080820083618164
train_iter_loss: 0.22979697585105896
train_iter_loss: 0.24354854226112366
train_iter_loss: 0.18357034027576447
train_iter_loss: 0.21026131510734558
train_iter_loss: 0.27198293805122375
train_iter_loss: 0.2674593925476074
train_iter_loss: 0.2413058578968048
train loss :0.2526
---------------------
Validation seg loss: 0.260344729460073 at epoch 21
********************
best_val_epoch_loss:  0.260344729460073
MODEL UPDATED
epoch =     22/  1000, exp = train
train_iter_loss: 0.22962850332260132
train_iter_loss: 0.18974348902702332
train_iter_loss: 0.3658767342567444
train_iter_loss: 0.23796740174293518
train_iter_loss: 0.18518421053886414
train_iter_loss: 0.24452073872089386
train_iter_loss: 0.2590494453907013
train_iter_loss: 0.20032523572444916
train_iter_loss: 0.19643330574035645
train_iter_loss: 0.2530321180820465
train_iter_loss: 0.31199243664741516
train_iter_loss: 0.22508949041366577
train_iter_loss: 0.25690069794654846
train_iter_loss: 0.2396397441625595
train_iter_loss: 0.20884066820144653
train_iter_loss: 0.14493921399116516
train_iter_loss: 0.21854908764362335
train_iter_loss: 0.12143244594335556
train_iter_loss: 0.18491671979427338
train_iter_loss: 0.2778598368167877
train_iter_loss: 0.24510569870471954
train_iter_loss: 0.19109821319580078
train_iter_loss: 0.3415781259536743
train_iter_loss: 0.22285878658294678
train_iter_loss: 0.24334833025932312
train_iter_loss: 0.20254138112068176
train_iter_loss: 0.2017543613910675
train_iter_loss: 0.1495499610900879
train_iter_loss: 0.18926239013671875
train_iter_loss: 0.21206098794937134
train_iter_loss: 0.20696969330310822
train_iter_loss: 0.19324293732643127
train_iter_loss: 0.19485674798488617
train_iter_loss: 0.2418683022260666
train_iter_loss: 0.32224899530410767
train_iter_loss: 0.26810747385025024
train_iter_loss: 0.2564130425453186
train_iter_loss: 0.25509026646614075
train_iter_loss: 0.1970905363559723
train_iter_loss: 0.22495365142822266
train_iter_loss: 0.20893193781375885
train_iter_loss: 0.3497120141983032
train_iter_loss: 0.2440011203289032
train_iter_loss: 0.25573012232780457
train_iter_loss: 0.3334406316280365
train_iter_loss: 0.3115323483943939
train_iter_loss: 0.24605992436408997
train_iter_loss: 0.16248103976249695
train_iter_loss: 0.27390655875205994
train_iter_loss: 0.21164093911647797
train_iter_loss: 0.20394495129585266
train_iter_loss: 0.18495403230190277
train_iter_loss: 0.2525736689567566
train_iter_loss: 0.23910027742385864
train_iter_loss: 0.2246253341436386
train_iter_loss: 0.16848443448543549
train_iter_loss: 0.300388365983963
train_iter_loss: 0.26138925552368164
train_iter_loss: 0.2948867678642273
train_iter_loss: 0.12870827317237854
train_iter_loss: 0.2049320638179779
train_iter_loss: 0.27358704805374146
train_iter_loss: 0.23856930434703827
train_iter_loss: 0.22934357821941376
train_iter_loss: 0.3433220088481903
train_iter_loss: 0.20704035460948944
train_iter_loss: 0.3226374387741089
train_iter_loss: 0.2194991409778595
train_iter_loss: 0.06547483801841736
train_iter_loss: 0.1895059198141098
train_iter_loss: 0.37619510293006897
train_iter_loss: 0.23850910365581512
train_iter_loss: 0.21264222264289856
train_iter_loss: 0.2681955397129059
train_iter_loss: 0.20785865187644958
train_iter_loss: 0.21303223073482513
train_iter_loss: 0.23715929687023163
train_iter_loss: 0.2549530863761902
train_iter_loss: 0.1283898502588272
train_iter_loss: 0.21461094915866852
train_iter_loss: 0.21187517046928406
train_iter_loss: 0.21965868771076202
train_iter_loss: 0.25296467542648315
train_iter_loss: 0.4333093762397766
train_iter_loss: 0.30688172578811646
train_iter_loss: 0.22952356934547424
train_iter_loss: 0.25202885270118713
train_iter_loss: 0.23552334308624268
train_iter_loss: 0.2092895805835724
train_iter_loss: 0.2620405852794647
train_iter_loss: 0.32460904121398926
train_iter_loss: 0.2569323182106018
train_iter_loss: 0.2712578773498535
train_iter_loss: 0.3239215016365051
train_iter_loss: 0.28593674302101135
train_iter_loss: 0.184199258685112
train_iter_loss: 0.21966461837291718
train_iter_loss: 0.2795916795730591
train_iter_loss: 0.24611389636993408
train_iter_loss: 0.24336524307727814
train loss :0.2412
---------------------
Validation seg loss: 0.2636858658548796 at epoch 22
epoch =     23/  1000, exp = train
train_iter_loss: 0.1962229311466217
train_iter_loss: 0.20488333702087402
train_iter_loss: 0.2668648660182953
train_iter_loss: 0.26522672176361084
train_iter_loss: 0.21137817203998566
train_iter_loss: 0.2459067702293396
train_iter_loss: 0.23656593263149261
train_iter_loss: 0.24341145157814026
train_iter_loss: 0.28393155336380005
train_iter_loss: 0.14945660531520844
train_iter_loss: 0.2399662584066391
train_iter_loss: 0.153998464345932
train_iter_loss: 0.2771928906440735
train_iter_loss: 0.2748362720012665
train_iter_loss: 0.1570999026298523
train_iter_loss: 0.22903139889240265
train_iter_loss: 0.20510950684547424
train_iter_loss: 0.34101685881614685
train_iter_loss: 0.13486675918102264
train_iter_loss: 0.31131505966186523
train_iter_loss: 0.26546594500541687
train_iter_loss: 0.14382939040660858
train_iter_loss: 0.4318302571773529
train_iter_loss: 0.24553605914115906
train_iter_loss: 0.2797110378742218
train_iter_loss: 0.189772367477417
train_iter_loss: 0.23576290905475616
train_iter_loss: 0.21950270235538483
train_iter_loss: 0.1980234533548355
train_iter_loss: 0.216869056224823
train_iter_loss: 0.35180577635765076
train_iter_loss: 0.3429991602897644
train_iter_loss: 0.1926751434803009
train_iter_loss: 0.18264618515968323
train_iter_loss: 0.22669696807861328
train_iter_loss: 0.19581539928913116
train_iter_loss: 0.2608935236930847
train_iter_loss: 0.16960282623767853
train_iter_loss: 0.28901946544647217
train_iter_loss: 0.2404210865497589
train_iter_loss: 0.43430641293525696
train_iter_loss: 0.2904820144176483
train_iter_loss: 0.22812923789024353
train_iter_loss: 0.29012128710746765
train_iter_loss: 0.23997990787029266
train_iter_loss: 0.162028968334198
train_iter_loss: 0.18967333436012268
train_iter_loss: 0.20845720171928406
train_iter_loss: 0.29410043358802795
train_iter_loss: 0.29060837626457214
train_iter_loss: 0.09595382958650589
train_iter_loss: 0.24253831803798676
train_iter_loss: 0.16668333113193512
train_iter_loss: 0.12167198210954666
train_iter_loss: 0.24263425171375275
train_iter_loss: 0.2201976478099823
train_iter_loss: 0.3253445625305176
train_iter_loss: 0.24989792704582214
train_iter_loss: 0.21570532023906708
train_iter_loss: 0.2032613307237625
train_iter_loss: 0.2815454602241516
train_iter_loss: 0.19221198558807373
train_iter_loss: 0.281157523393631
train_iter_loss: 0.24738462269306183
train_iter_loss: 0.20500855147838593
train_iter_loss: 0.13324198126792908
train_iter_loss: 0.20729631185531616
train_iter_loss: 0.22817285358905792
train_iter_loss: 0.15391962230205536
train_iter_loss: 0.24829716980457306
train_iter_loss: 0.3064323365688324
train_iter_loss: 0.15588706731796265
train_iter_loss: 0.2486031949520111
train_iter_loss: 0.1775325983762741
train_iter_loss: 0.27829301357269287
train_iter_loss: 0.31300675868988037
train_iter_loss: 0.259822815656662
train_iter_loss: 0.17150916159152985
train_iter_loss: 0.2564408779144287
train_iter_loss: 0.2490810751914978
train_iter_loss: 0.1822003871202469
train_iter_loss: 0.2675515413284302
train_iter_loss: 0.2157658040523529
train_iter_loss: 0.2074376493692398
train_iter_loss: 0.18713931739330292
train_iter_loss: 0.16478709876537323
train_iter_loss: 0.22444865107536316
train_iter_loss: 0.29628002643585205
train_iter_loss: 0.24454765021800995
train_iter_loss: 0.27280524373054504
train_iter_loss: 0.16524159908294678
train_iter_loss: 0.20765291154384613
train_iter_loss: 0.23639754951000214
train_iter_loss: 0.18958529829978943
train_iter_loss: 0.20660415291786194
train_iter_loss: 0.3351118564605713
train_iter_loss: 0.19698841869831085
train_iter_loss: 0.20964990556240082
train_iter_loss: 0.23127740621566772
train_iter_loss: 0.40950900316238403
train loss :0.2371
---------------------
Validation seg loss: 0.25822507930954675 at epoch 23
********************
best_val_epoch_loss:  0.25822507930954675
MODEL UPDATED
epoch =     24/  1000, exp = train
train_iter_loss: 0.14858846366405487
train_iter_loss: 0.2135903537273407
train_iter_loss: 0.2243359237909317
train_iter_loss: 0.15224997699260712
train_iter_loss: 0.19560876488685608
train_iter_loss: 0.22703823447227478
train_iter_loss: 0.27159440517425537
train_iter_loss: 0.25591787695884705
train_iter_loss: 0.24627366662025452
train_iter_loss: 0.21319934725761414
train_iter_loss: 0.12929485738277435
train_iter_loss: 0.24555297195911407
train_iter_loss: 0.267386257648468
train_iter_loss: 0.27264493703842163
train_iter_loss: 0.28582340478897095
train_iter_loss: 0.29289519786834717
train_iter_loss: 0.23737940192222595
train_iter_loss: 0.19871282577514648
train_iter_loss: 0.28362131118774414
train_iter_loss: 0.25366267561912537
train_iter_loss: 0.2619941234588623
train_iter_loss: 0.21672756969928741
train_iter_loss: 0.19976450502872467
train_iter_loss: 0.1850392073392868
train_iter_loss: 0.17197969555854797
train_iter_loss: 0.2027251124382019
train_iter_loss: 0.2762467861175537
train_iter_loss: 0.21004971861839294
train_iter_loss: 0.24478447437286377
train_iter_loss: 0.20117305219173431
train_iter_loss: 0.2494632452726364
train_iter_loss: 0.34685614705085754
train_iter_loss: 0.31402507424354553
train_iter_loss: 0.16405752301216125
train_iter_loss: 0.20999565720558167
train_iter_loss: 0.21937976777553558
train_iter_loss: 0.2619376480579376
train_iter_loss: 0.251107782125473
train_iter_loss: 0.17997701466083527
train_iter_loss: 0.30986258387565613
train_iter_loss: 0.2469809353351593
train_iter_loss: 0.13568571209907532
train_iter_loss: 0.24592715501785278
train_iter_loss: 0.26734885573387146
train_iter_loss: 0.17748543620109558
train_iter_loss: 0.1857190579175949
train_iter_loss: 0.20889219641685486
train_iter_loss: 0.26436930894851685
train_iter_loss: 0.2527763843536377
train_iter_loss: 0.16503691673278809
train_iter_loss: 0.17675091326236725
train_iter_loss: 0.22728443145751953
train_iter_loss: 0.23939380049705505
train_iter_loss: 0.20728586614131927
train_iter_loss: 0.19813738763332367
train_iter_loss: 0.23280157148838043
train_iter_loss: 0.2425190657377243
train_iter_loss: 0.22129696607589722
train_iter_loss: 0.23261021077632904
train_iter_loss: 0.1962188482284546
train_iter_loss: 0.2660551369190216
train_iter_loss: 0.18723148107528687
train_iter_loss: 0.18641957640647888
train_iter_loss: 0.15917016565799713
train_iter_loss: 0.26448744535446167
train_iter_loss: 0.1483505666255951
train_iter_loss: 0.20269174873828888
train_iter_loss: 0.21124720573425293
train_iter_loss: 0.24334943294525146
train_iter_loss: 0.3186909258365631
train_iter_loss: 0.2539793848991394
train_iter_loss: 0.2245437353849411
train_iter_loss: 0.25783252716064453
train_iter_loss: 0.21566592156887054
train_iter_loss: 0.19223366677761078
train_iter_loss: 0.26015201210975647
train_iter_loss: 0.2729719281196594
train_iter_loss: 0.25455066561698914
train_iter_loss: 0.2517756223678589
train_iter_loss: 0.31560295820236206
train_iter_loss: 0.16892996430397034
train_iter_loss: 0.31540343165397644
train_iter_loss: 0.25420325994491577
train_iter_loss: 0.34238380193710327
train_iter_loss: 0.17459146678447723
train_iter_loss: 0.25638720393180847
train_iter_loss: 0.20519569516181946
train_iter_loss: 0.14899833500385284
train_iter_loss: 0.2017442286014557
train_iter_loss: 0.20457205176353455
train_iter_loss: 0.2638857662677765
train_iter_loss: 0.1888696700334549
train_iter_loss: 0.21584589779376984
train_iter_loss: 0.2956107556819916
train_iter_loss: 0.19093568623065948
train_iter_loss: 0.21808958053588867
train_iter_loss: 0.20677506923675537
train_iter_loss: 0.34259840846061707
train_iter_loss: 0.25813472270965576
train_iter_loss: 0.2622870206832886
train loss :0.2326
---------------------
Validation seg loss: 0.2579302711869186 at epoch 24
********************
best_val_epoch_loss:  0.2579302711869186
MODEL UPDATED
epoch =     25/  1000, exp = train
train_iter_loss: 0.2779677212238312
train_iter_loss: 0.18698124587535858
train_iter_loss: 0.17868240177631378
train_iter_loss: 0.2214857041835785
train_iter_loss: 0.16897456347942352
train_iter_loss: 0.18567249178886414
train_iter_loss: 0.188684344291687
train_iter_loss: 0.2053236961364746
train_iter_loss: 0.18108800053596497
train_iter_loss: 0.18001514673233032
train_iter_loss: 0.25200310349464417
train_iter_loss: 0.17960824072360992
train_iter_loss: 0.2907836139202118
train_iter_loss: 0.27323195338249207
train_iter_loss: 0.19796037673950195
train_iter_loss: 0.2167249172925949
train_iter_loss: 0.1890692412853241
train_iter_loss: 0.24247363209724426
train_iter_loss: 0.12845806777477264
train_iter_loss: 0.23607724905014038
train_iter_loss: 0.20512312650680542
train_iter_loss: 0.20042204856872559
train_iter_loss: 0.32226812839508057
train_iter_loss: 0.3384304940700531
train_iter_loss: 0.190810889005661
train_iter_loss: 0.23666875064373016
train_iter_loss: 0.17782169580459595
train_iter_loss: 0.16005845367908478
train_iter_loss: 0.23044641315937042
train_iter_loss: 0.18043503165245056
train_iter_loss: 0.3108898997306824
train_iter_loss: 0.18661612272262573
train_iter_loss: 0.24610741436481476
train_iter_loss: 0.22280164062976837
train_iter_loss: 0.19689419865608215
train_iter_loss: 0.2972210943698883
train_iter_loss: 0.20402096211910248
train_iter_loss: 0.20375099778175354
train_iter_loss: 0.3433138132095337
train_iter_loss: 0.2833016514778137
train_iter_loss: 0.19748996198177338
train_iter_loss: 0.15159574151039124
train_iter_loss: 0.40959659218788147
train_iter_loss: 0.24810734391212463
train_iter_loss: 0.20049838721752167
train_iter_loss: 0.2719164490699768
train_iter_loss: 0.28415489196777344
train_iter_loss: 0.31875699758529663
train_iter_loss: 0.2049027979373932
train_iter_loss: 0.21780997514724731
train_iter_loss: 0.1362122893333435
train_iter_loss: 0.25360196828842163
train_iter_loss: 0.25661468505859375
train_iter_loss: 0.32418155670166016
train_iter_loss: 0.29002225399017334
train_iter_loss: 0.30823567509651184
train_iter_loss: 0.17964135110378265
train_iter_loss: 0.19862490892410278
train_iter_loss: 0.16649919748306274
train_iter_loss: 0.1856689304113388
train_iter_loss: 0.318359911441803
train_iter_loss: 0.20462946593761444
train_iter_loss: 0.24855799973011017
train_iter_loss: 0.2027008831501007
train_iter_loss: 0.2551524043083191
train_iter_loss: 0.18386244773864746
train_iter_loss: 0.18459424376487732
train_iter_loss: 0.30788183212280273
train_iter_loss: 0.31379982829093933
train_iter_loss: 0.16935871541500092
train_iter_loss: 0.21947717666625977
train_iter_loss: 0.21616533398628235
train_iter_loss: 0.18882815539836884
train_iter_loss: 0.11544957756996155
train_iter_loss: 0.20992343127727509
train_iter_loss: 0.17682674527168274
train_iter_loss: 0.2217286378145218
train_iter_loss: 0.19402015209197998
train_iter_loss: 0.22360996901988983
train_iter_loss: 0.17755375802516937
train_iter_loss: 0.1497514843940735
train_iter_loss: 0.2605609595775604
train_iter_loss: 0.22507557272911072
train_iter_loss: 0.30506202578544617
train_iter_loss: 0.22259780764579773
train_iter_loss: 0.22305819392204285
train_iter_loss: 0.31579115986824036
train_iter_loss: 0.28588157892227173
train_iter_loss: 0.28028932213783264
train_iter_loss: 0.22130775451660156
train_iter_loss: 0.22469790279865265
train_iter_loss: 0.2315191775560379
train_iter_loss: 0.2237575501203537
train_iter_loss: 0.27625614404678345
train_iter_loss: 0.2686064839363098
train_iter_loss: 0.21492666006088257
train_iter_loss: 0.2056657075881958
train_iter_loss: 0.424677312374115
train_iter_loss: 0.32993245124816895
train_iter_loss: 0.18395699560642242
train loss :0.2336
---------------------
Validation seg loss: 0.2555771502754036 at epoch 25
********************
best_val_epoch_loss:  0.2555771502754036
MODEL UPDATED
epoch =     26/  1000, exp = train
train_iter_loss: 0.27282118797302246
train_iter_loss: 0.19549675285816193
train_iter_loss: 0.3605876863002777
train_iter_loss: 0.2809493839740753
train_iter_loss: 0.1355152428150177
train_iter_loss: 0.18852443993091583
train_iter_loss: 0.2284925878047943
train_iter_loss: 0.2136612981557846
train_iter_loss: 0.3386746048927307
train_iter_loss: 0.16809050738811493
train_iter_loss: 0.20380711555480957
train_iter_loss: 0.2679307162761688
train_iter_loss: 0.22780855000019073
train_iter_loss: 0.31247952580451965
train_iter_loss: 0.2660492956638336
train_iter_loss: 0.28174808621406555
train_iter_loss: 0.16267503798007965
train_iter_loss: 0.15984724462032318
train_iter_loss: 0.18655529618263245
train_iter_loss: 0.2809167802333832
train_iter_loss: 0.17645885050296783
train_iter_loss: 0.18148848414421082
train_iter_loss: 0.21300742030143738
train_iter_loss: 0.10679128021001816
train_iter_loss: 0.2881278991699219
train_iter_loss: 0.25802692770957947
train_iter_loss: 0.2216080278158188
train_iter_loss: 0.2301848828792572
train_iter_loss: 0.21882173418998718
train_iter_loss: 0.180784210562706
train_iter_loss: 0.2186218649148941
train_iter_loss: 0.17385292053222656
train_iter_loss: 0.18860363960266113
train_iter_loss: 0.16668039560317993
train_iter_loss: 0.2675676941871643
train_iter_loss: 0.193906769156456
train_iter_loss: 0.2953052818775177
train_iter_loss: 0.36908724904060364
train_iter_loss: 0.3078213930130005
train_iter_loss: 0.26190584897994995
train_iter_loss: 0.13253121078014374
train_iter_loss: 0.24450024962425232
train_iter_loss: 0.21466107666492462
train_iter_loss: 0.24828635156154633
train_iter_loss: 0.2504137456417084
train_iter_loss: 0.2076158970594406
train_iter_loss: 0.05272603780031204
train_iter_loss: 0.2121323198080063
train_iter_loss: 0.2921162247657776
train_iter_loss: 0.2446548491716385
train_iter_loss: 0.19699010252952576
train_iter_loss: 0.2633858919143677
train_iter_loss: 0.2808360755443573
train_iter_loss: 0.13696697354316711
train_iter_loss: 0.2419857233762741
train_iter_loss: 0.4179503917694092
train_iter_loss: 0.22646145522594452
train_iter_loss: 0.21111136674880981
train_iter_loss: 0.23196004331111908
train_iter_loss: 0.33663398027420044
train_iter_loss: 0.18383121490478516
train_iter_loss: 0.32553577423095703
train_iter_loss: 0.2373783439397812
train_iter_loss: 0.08561000227928162
train_iter_loss: 0.20083099603652954
train_iter_loss: 0.210786372423172
train_iter_loss: 0.2680990695953369
train_iter_loss: 0.18897241353988647
train_iter_loss: 0.13396811485290527
train_iter_loss: 0.3807808458805084
train_iter_loss: 0.1864059567451477
train_iter_loss: 0.2249491959810257
train_iter_loss: 0.3414298892021179
train_iter_loss: 0.31309953331947327
train_iter_loss: 0.25446978211402893
train_iter_loss: 0.2905883193016052
train_iter_loss: 0.14510560035705566
train_iter_loss: 0.2526150345802307
train_iter_loss: 0.20412178337574005
train_iter_loss: 0.1876373440027237
train_iter_loss: 0.33115354180336
train_iter_loss: 0.3232748806476593
train_iter_loss: 0.2404761165380478
train_iter_loss: 0.2076626569032669
train_iter_loss: 0.2302933931350708
train_iter_loss: 0.28802186250686646
train_iter_loss: 0.14572426676750183
train_iter_loss: 0.2310589998960495
train_iter_loss: 0.10772868990898132
train_iter_loss: 0.15909120440483093
train_iter_loss: 0.24515019357204437
train_iter_loss: 0.12849211692810059
train_iter_loss: 0.15602615475654602
train_iter_loss: 0.30033451318740845
train_iter_loss: 0.21139846742153168
train_iter_loss: 0.22567082941532135
train_iter_loss: 0.24266771972179413
train_iter_loss: 0.19195203483104706
train_iter_loss: 0.2522033154964447
train_iter_loss: 0.22967682778835297
train loss :0.2319
---------------------
Validation seg loss: 0.251254232846341 at epoch 26
********************
best_val_epoch_loss:  0.251254232846341
MODEL UPDATED
epoch =     27/  1000, exp = train
train_iter_loss: 0.30077216029167175
train_iter_loss: 0.21854418516159058
train_iter_loss: 0.178923100233078
train_iter_loss: 0.27372387051582336
train_iter_loss: 0.23771700263023376
train_iter_loss: 0.2078694999217987
train_iter_loss: 0.2387201488018036
train_iter_loss: 0.21748368442058563
train_iter_loss: 0.33336570858955383
train_iter_loss: 0.24824467301368713
train_iter_loss: 0.22669781744480133
train_iter_loss: 0.15504774451255798
train_iter_loss: 0.1919575184583664
train_iter_loss: 0.22036638855934143
train_iter_loss: 0.149404376745224
train_iter_loss: 0.17953066527843475
train_iter_loss: 0.20917795598506927
train_iter_loss: 0.1374732255935669
train_iter_loss: 0.25171253085136414
train_iter_loss: 0.2771163582801819
train_iter_loss: 0.28506535291671753
train_iter_loss: 0.33723944425582886
train_iter_loss: 0.18819749355316162
train_iter_loss: 0.1854037493467331
train_iter_loss: 0.26802173256874084
train_iter_loss: 0.23199297487735748
train_iter_loss: 0.22476749122142792
train_iter_loss: 0.26589787006378174
train_iter_loss: 0.18393836915493011
train_iter_loss: 0.2569412589073181
train_iter_loss: 0.20942635834217072
train_iter_loss: 0.3062852621078491
train_iter_loss: 0.21881020069122314
train_iter_loss: 0.30405864119529724
train_iter_loss: 0.223274365067482
train_iter_loss: 0.18425939977169037
train_iter_loss: 0.20723387598991394
train_iter_loss: 0.21803858876228333
train_iter_loss: 0.1903921216726303
train_iter_loss: 0.16845029592514038
train_iter_loss: 0.18368366360664368
train_iter_loss: 0.26136136054992676
train_iter_loss: 0.2347007840871811
train_iter_loss: 0.23247836530208588
train_iter_loss: 0.286937415599823
train_iter_loss: 0.16490602493286133
train_iter_loss: 0.17421965301036835
train_iter_loss: 0.14160524308681488
train_iter_loss: 0.23770880699157715
train_iter_loss: 0.21398231387138367
train_iter_loss: 0.287700891494751
train_iter_loss: 0.19924047589302063
train_iter_loss: 0.26738446950912476
train_iter_loss: 0.2304757982492447
train_iter_loss: 0.17255312204360962
train_iter_loss: 0.1195674017071724
train_iter_loss: 0.16160176694393158
train_iter_loss: 0.23290987312793732
train_iter_loss: 0.28551244735717773
train_iter_loss: 0.20453926920890808
train_iter_loss: 0.31770485639572144
train_iter_loss: 0.22160851955413818
train_iter_loss: 0.2069627344608307
train_iter_loss: 0.19622063636779785
train_iter_loss: 0.2552446722984314
train_iter_loss: 0.16904395818710327
train_iter_loss: 0.263782799243927
train_iter_loss: 0.17550134658813477
train_iter_loss: 0.37686821818351746
train_iter_loss: 0.2383156269788742
train_iter_loss: 0.23201477527618408
train_iter_loss: 0.2997344434261322
train_iter_loss: 0.13514967262744904
train_iter_loss: 0.24147944152355194
train_iter_loss: 0.3418337106704712
train_iter_loss: 0.2548443675041199
train_iter_loss: 0.18221019208431244
train_iter_loss: 0.2806251049041748
train_iter_loss: 0.15316222608089447
train_iter_loss: 0.24775607883930206
train_iter_loss: 0.1108257994055748
train_iter_loss: 0.2696450650691986
train_iter_loss: 0.17855769395828247
train_iter_loss: 0.27341434359550476
train_iter_loss: 0.22438238561153412
train_iter_loss: 0.2446579933166504
train_iter_loss: 0.23161596059799194
train_iter_loss: 0.32940900325775146
train_iter_loss: 0.26937413215637207
train_iter_loss: 0.12825722992420197
train_iter_loss: 0.29104575514793396
train_iter_loss: 0.1580747663974762
train_iter_loss: 0.228315070271492
train_iter_loss: 0.19624459743499756
train_iter_loss: 0.22598110139369965
train_iter_loss: 0.2158714085817337
train_iter_loss: 0.17909106612205505
train_iter_loss: 0.16488967835903168
train_iter_loss: 0.2599083185195923
train_iter_loss: 0.25511056184768677
train loss :0.2286
---------------------
Validation seg loss: 0.25938445934147203 at epoch 27
epoch =     28/  1000, exp = train
train_iter_loss: 0.181223064661026
train_iter_loss: 0.09534133225679398
train_iter_loss: 0.2460782676935196
train_iter_loss: 0.17333996295928955
train_iter_loss: 0.16083015501499176
train_iter_loss: 0.16740261018276215
train_iter_loss: 0.2605961859226227
train_iter_loss: 0.2599864900112152
train_iter_loss: 0.22928622364997864
train_iter_loss: 0.24762175977230072
train_iter_loss: 0.19711415469646454
train_iter_loss: 0.23860177397727966
train_iter_loss: 0.2532126307487488
train_iter_loss: 0.2972643971443176
train_iter_loss: 0.1671966016292572
train_iter_loss: 0.1866074800491333
train_iter_loss: 0.25967201590538025
train_iter_loss: 0.19824230670928955
train_iter_loss: 0.11114505678415298
train_iter_loss: 0.18713822960853577
train_iter_loss: 0.19456662237644196
train_iter_loss: 0.1749979555606842
train_iter_loss: 0.2019578367471695
train_iter_loss: 0.24880798161029816
train_iter_loss: 0.17930805683135986
train_iter_loss: 0.2435082346200943
train_iter_loss: 0.16699163615703583
train_iter_loss: 0.1567382514476776
train_iter_loss: 0.1618712693452835
train_iter_loss: 0.267726331949234
train_iter_loss: 0.19804167747497559
train_iter_loss: 0.27292242646217346
train_iter_loss: 0.1739843785762787
train_iter_loss: 0.29677656292915344
train_iter_loss: 0.2920304834842682
train_iter_loss: 0.3086196482181549
train_iter_loss: 0.2598857283592224
train_iter_loss: 0.195623517036438
train_iter_loss: 0.17717766761779785
train_iter_loss: 0.19560393691062927
train_iter_loss: 0.25180014967918396
train_iter_loss: 0.2513471841812134
train_iter_loss: 0.2459200769662857
train_iter_loss: 0.20970316231250763
train_iter_loss: 0.22932225465774536
train_iter_loss: 0.24093595147132874
train_iter_loss: 0.19709506630897522
train_iter_loss: 0.278827965259552
train_iter_loss: 0.18888509273529053
train_iter_loss: 0.2366393804550171
train_iter_loss: 0.1684841811656952
train_iter_loss: 0.28039801120758057
train_iter_loss: 0.14485757052898407
train_iter_loss: 0.30630481243133545
train_iter_loss: 0.27104514837265015
train_iter_loss: 0.22534699738025665
train_iter_loss: 0.24319550395011902
train_iter_loss: 0.22852113842964172
train_iter_loss: 0.3074861466884613
train_iter_loss: 0.2745016813278198
train_iter_loss: 0.2360505312681198
train_iter_loss: 0.2112502157688141
train_iter_loss: 0.21552841365337372
train_iter_loss: 0.22440843284130096
train_iter_loss: 0.21486741304397583
train_iter_loss: 0.33536064624786377
train_iter_loss: 0.17229928076267242
train_iter_loss: 0.18870873749256134
train_iter_loss: 0.2541505694389343
train_iter_loss: 0.1925804764032364
train_iter_loss: 0.15381605923175812
train_iter_loss: 0.14823240041732788
train_iter_loss: 0.29124659299850464
train_iter_loss: 0.16845180094242096
train_iter_loss: 0.1939527988433838
train_iter_loss: 0.17654605209827423
train_iter_loss: 0.23502154648303986
train_iter_loss: 0.251613050699234
train_iter_loss: 0.20047788321971893
train_iter_loss: 0.3417881727218628
train_iter_loss: 0.14873163402080536
train_iter_loss: 0.155790314078331
train_iter_loss: 0.17335490882396698
train_iter_loss: 0.25027090311050415
train_iter_loss: 0.26660093665122986
train_iter_loss: 0.19383692741394043
train_iter_loss: 0.2083248794078827
train_iter_loss: 0.24628394842147827
train_iter_loss: 0.2313387393951416
train_iter_loss: 0.2411542385816574
train_iter_loss: 0.29078593850135803
train_iter_loss: 0.3037525713443756
train_iter_loss: 0.16837871074676514
train_iter_loss: 0.2053999900817871
train_iter_loss: 0.2395378053188324
train_iter_loss: 0.20664013922214508
train_iter_loss: 0.2349337637424469
train_iter_loss: 0.19741854071617126
train_iter_loss: 0.2928604781627655
train_iter_loss: 0.33798277378082275
train loss :0.2249
---------------------
Validation seg loss: 0.25414952252692774 at epoch 28
epoch =     29/  1000, exp = train
train_iter_loss: 0.21948054432868958
train_iter_loss: 0.11595074087381363
train_iter_loss: 0.18465887010097504
train_iter_loss: 0.21507030725479126
train_iter_loss: 0.2269688844680786
train_iter_loss: 0.2489401251077652
train_iter_loss: 0.13750013709068298
train_iter_loss: 0.22106002271175385
train_iter_loss: 0.15078629553318024
train_iter_loss: 0.3596608638763428
train_iter_loss: 0.17243613302707672
train_iter_loss: 0.2553551197052002
train_iter_loss: 0.18429115414619446
train_iter_loss: 0.12577100098133087
train_iter_loss: 0.22824282944202423
train_iter_loss: 0.12338147312402725
train_iter_loss: 0.21537336707115173
train_iter_loss: 0.28097763657569885
train_iter_loss: 0.25615394115448
train_iter_loss: 0.1785767674446106
train_iter_loss: 0.20172053575515747
train_iter_loss: 0.2343715876340866
train_iter_loss: 0.2389676570892334
train_iter_loss: 0.16326633095741272
train_iter_loss: 0.22257064282894135
train_iter_loss: 0.22238406538963318
train_iter_loss: 0.21802382171154022
train_iter_loss: 0.19502606987953186
train_iter_loss: 0.16658681631088257
train_iter_loss: 0.31632182002067566
train_iter_loss: 0.27813494205474854
train_iter_loss: 0.1759372502565384
train_iter_loss: 0.2643435299396515
train_iter_loss: 0.2522420585155487
train_iter_loss: 0.23340509831905365
train_iter_loss: 0.16867168247699738
train_iter_loss: 0.2144467979669571
train_iter_loss: 0.2567208409309387
train_iter_loss: 0.2103503793478012
train_iter_loss: 0.26938533782958984
train_iter_loss: 0.2254735231399536
train_iter_loss: 0.1851540356874466
train_iter_loss: 0.24751214683055878
train_iter_loss: 0.22395102679729462
train_iter_loss: 0.18590795993804932
train_iter_loss: 0.20651769638061523
train_iter_loss: 0.3933100700378418
train_iter_loss: 0.21219389140605927
train_iter_loss: 0.19953159987926483
train_iter_loss: 0.2379283308982849
train_iter_loss: 0.31152400374412537
train_iter_loss: 0.25669065117836
train_iter_loss: 0.1480187177658081
train_iter_loss: 0.25697869062423706
train_iter_loss: 0.19718506932258606
train_iter_loss: 0.23131270706653595
train_iter_loss: 0.25919488072395325
train_iter_loss: 0.2852886915206909
train_iter_loss: 0.1613660603761673
train_iter_loss: 0.288286417722702
train_iter_loss: 0.36632364988327026
train_iter_loss: 0.13295231759548187
train_iter_loss: 0.18289558589458466
train_iter_loss: 0.2324429154396057
train_iter_loss: 0.18061037361621857
train_iter_loss: 0.19877971708774567
train_iter_loss: 0.2068794071674347
train_iter_loss: 0.3109639883041382
train_iter_loss: 0.35955554246902466
train_iter_loss: 0.17607183754444122
train_iter_loss: 0.3418828845024109
train_iter_loss: 0.24956782162189484
train_iter_loss: 0.25076180696487427
train_iter_loss: 0.3796742856502533
train_iter_loss: 0.3137665390968323
train_iter_loss: 0.2601107656955719
train_iter_loss: 0.13802596926689148
train_iter_loss: 0.5082828998565674
train_iter_loss: 0.21115972101688385
train_iter_loss: 0.28944218158721924
train_iter_loss: 0.2373999059200287
train_iter_loss: 0.20208902657032013
train_iter_loss: 0.3246648609638214
train_iter_loss: 0.2586405873298645
train_iter_loss: 0.36112406849861145
train_iter_loss: 0.1582096368074417
train_iter_loss: 0.2354605495929718
train_iter_loss: 0.2552437484264374
train_iter_loss: 0.25141552090644836
train_iter_loss: 0.21880249679088593
train_iter_loss: 0.1654735505580902
train_iter_loss: 0.1816602200269699
train_iter_loss: 0.18325436115264893
train_iter_loss: 0.22056595981121063
train_iter_loss: 0.18632838129997253
train_iter_loss: 0.20347529649734497
train_iter_loss: 0.2914623022079468
train_iter_loss: 0.22017984092235565
train_iter_loss: 0.15213856101036072
train_iter_loss: 0.23333348333835602
train loss :0.2337
---------------------
Validation seg loss: 0.25115638943213336 at epoch 29
********************
best_val_epoch_loss:  0.25115638943213336
MODEL UPDATED
epoch =     30/  1000, exp = train
train_iter_loss: 0.20618487894535065
train_iter_loss: 0.278138130903244
train_iter_loss: 0.1980396956205368
train_iter_loss: 0.164234921336174
train_iter_loss: 0.1697358787059784
train_iter_loss: 0.2110440731048584
train_iter_loss: 0.18780262768268585
train_iter_loss: 0.22493976354599
train_iter_loss: 0.17211690545082092
train_iter_loss: 0.15139272809028625
train_iter_loss: 0.16682294011116028
train_iter_loss: 0.1781223714351654
train_iter_loss: 0.2968696653842926
train_iter_loss: 0.17418888211250305
train_iter_loss: 0.19898980855941772
train_iter_loss: 0.18547506630420685
train_iter_loss: 0.32357704639434814
train_iter_loss: 0.24510058760643005
train_iter_loss: 0.11585554480552673
train_iter_loss: 0.2168601155281067
train_iter_loss: 0.2196768969297409
train_iter_loss: 0.13821068406105042
train_iter_loss: 0.2295393943786621
train_iter_loss: 0.20574338734149933
train_iter_loss: 0.23660263419151306
train_iter_loss: 0.11875490844249725
train_iter_loss: 0.31031879782676697
train_iter_loss: 0.23292821645736694
train_iter_loss: 0.19903165102005005
train_iter_loss: 0.14189234375953674
train_iter_loss: 0.1536790430545807
train_iter_loss: 0.2552352845668793
train_iter_loss: 0.24824579060077667
train_iter_loss: 0.34222012758255005
train_iter_loss: 0.2785838544368744
train_iter_loss: 0.28497734665870667
train_iter_loss: 0.17283408343791962
train_iter_loss: 0.31523019075393677
train_iter_loss: 0.2551908493041992
train_iter_loss: 0.20610949397087097
train_iter_loss: 0.3505149781703949
train_iter_loss: 0.2270313799381256
train_iter_loss: 0.2964898645877838
train_iter_loss: 0.2828691303730011
train_iter_loss: 0.19197490811347961
train_iter_loss: 0.22628752887248993
train_iter_loss: 0.17980551719665527
train_iter_loss: 0.2658490836620331
train_iter_loss: 0.1875162571668625
train_iter_loss: 0.24242055416107178
train_iter_loss: 0.2023623287677765
train_iter_loss: 0.23643045127391815
train_iter_loss: 0.1925896406173706
train_iter_loss: 0.24487312138080597
train_iter_loss: 0.28166675567626953
train_iter_loss: 0.2582116425037384
train_iter_loss: 0.2075948268175125
train_iter_loss: 0.12912951409816742
train_iter_loss: 0.13962402939796448
train_iter_loss: 0.16441479325294495
train_iter_loss: 0.17979921400547028
train_iter_loss: 0.21555890142917633
train_iter_loss: 0.34833747148513794
train_iter_loss: 0.2442295402288437
train_iter_loss: 0.2606143653392792
train_iter_loss: 0.13705523312091827
train_iter_loss: 0.11865092068910599
train_iter_loss: 0.25612667202949524
train_iter_loss: 0.21145452558994293
train_iter_loss: 0.26078322529792786
train_iter_loss: 0.15533462166786194
train_iter_loss: 0.21690426766872406
train_iter_loss: 0.19699303805828094
train_iter_loss: 0.2167895883321762
train_iter_loss: 0.1491219699382782
train_iter_loss: 0.2121717929840088
train_iter_loss: 0.2342371791601181
train_iter_loss: 0.22714798152446747
train_iter_loss: 0.1753539741039276
train_iter_loss: 0.25917142629623413
train_iter_loss: 0.03790769353508949
train_iter_loss: 0.14620396494865417
train_iter_loss: 0.19440077245235443
train_iter_loss: 0.31038326025009155
train_iter_loss: 0.20293155312538147
train_iter_loss: 0.29127031564712524
train_iter_loss: 0.33906930685043335
train_iter_loss: 0.19324888288974762
train_iter_loss: 0.19914506375789642
train_iter_loss: 0.2061878740787506
train_iter_loss: 0.23560240864753723
train_iter_loss: 0.363953173160553
train_iter_loss: 0.20595106482505798
train_iter_loss: 0.22614529728889465
train_iter_loss: 0.2565770447254181
train_iter_loss: 0.18073110282421112
train_iter_loss: 0.2655313313007355
train_iter_loss: 0.29540979862213135
train_iter_loss: 0.28793567419052124
train_iter_loss: 0.20082618296146393
train loss :0.2227
---------------------
Validation seg loss: 0.243695324075953 at epoch 30
********************
best_val_epoch_loss:  0.243695324075953
MODEL UPDATED
epoch =     31/  1000, exp = train
train_iter_loss: 0.20338383316993713
train_iter_loss: 0.1709449738264084
train_iter_loss: 0.38994014263153076
train_iter_loss: 0.1866387128829956
train_iter_loss: 0.2731442451477051
train_iter_loss: 0.1371329426765442
train_iter_loss: 0.22677142918109894
train_iter_loss: 0.17761382460594177
train_iter_loss: 0.20099394023418427
train_iter_loss: 0.15870846807956696
train_iter_loss: 0.21128228306770325
train_iter_loss: 0.17399223148822784
train_iter_loss: 0.20512309670448303
train_iter_loss: 0.18123607337474823
train_iter_loss: 0.29431402683258057
train_iter_loss: 0.17841392755508423
train_iter_loss: 0.2428329735994339
train_iter_loss: 0.27588286995887756
train_iter_loss: 0.3010109066963196
train_iter_loss: 0.29375162720680237
train_iter_loss: 0.27411890029907227
train_iter_loss: 0.22299160063266754
train_iter_loss: 0.1705927699804306
train_iter_loss: 0.22283369302749634
train_iter_loss: 0.12322364002466202
train_iter_loss: 0.11144448816776276
train_iter_loss: 0.32655322551727295
train_iter_loss: 0.25540831685066223
train_iter_loss: 0.26521605253219604
train_iter_loss: 0.2184847742319107
train_iter_loss: 0.24362702667713165
train_iter_loss: 0.23313012719154358
train_iter_loss: 0.22847424447536469
train_iter_loss: 0.1336461901664734
train_iter_loss: 0.2052893191576004
train_iter_loss: 0.1805291473865509
train_iter_loss: 0.14306724071502686
train_iter_loss: 0.13793334364891052
train_iter_loss: 0.21699151396751404
train_iter_loss: 0.35132962465286255
train_iter_loss: 0.28988853096961975
train_iter_loss: 0.20433691143989563
train_iter_loss: 0.16865000128746033
train_iter_loss: 0.32056567072868347
train_iter_loss: 0.21868450939655304
train_iter_loss: 0.2214333862066269
train_iter_loss: 0.14862552285194397
train_iter_loss: 0.2852172255516052
train_iter_loss: 0.2570398449897766
train_iter_loss: 0.18421877920627594
train_iter_loss: 0.2604863941669464
train_iter_loss: 0.3371569514274597
train_iter_loss: 0.24505142867565155
train_iter_loss: 0.21425941586494446
train_iter_loss: 0.23779290914535522
train_iter_loss: 0.14510086178779602
train_iter_loss: 0.2620616853237152
train_iter_loss: 0.2684176564216614
train_iter_loss: 0.2786702513694763
train_iter_loss: 0.1772168129682541
train_iter_loss: 0.23376640677452087
train_iter_loss: 0.2451290786266327
train_iter_loss: 0.14410904049873352
train_iter_loss: 0.19091741740703583
train_iter_loss: 0.38591480255126953
train_iter_loss: 0.1894649863243103
train_iter_loss: 0.2074114978313446
train_iter_loss: 0.14560702443122864
train_iter_loss: 0.2048056423664093
train_iter_loss: 0.16438136994838715
train_iter_loss: 0.18036703765392303
train_iter_loss: 0.24681523442268372
train_iter_loss: 0.26660412549972534
train_iter_loss: 0.27039775252342224
train_iter_loss: 0.29367995262145996
train_iter_loss: 0.19247795641422272
train_iter_loss: 0.24471302330493927
train_iter_loss: 0.18582820892333984
train_iter_loss: 0.2846369445323944
train_iter_loss: 0.3518109619617462
train_iter_loss: 0.3066011369228363
train_iter_loss: 0.2955513596534729
train_iter_loss: 0.312903493642807
train_iter_loss: 0.2839451730251312
train_iter_loss: 0.2246958166360855
train_iter_loss: 0.29589366912841797
train_iter_loss: 0.19827218353748322
train_iter_loss: 0.21811896562576294
train_iter_loss: 0.24725450575351715
train_iter_loss: 0.1633000522851944
train_iter_loss: 0.20895600318908691
train_iter_loss: 0.17702317237854004
train_iter_loss: 0.1692044734954834
train_iter_loss: 0.1884758025407791
train_iter_loss: 0.19379037618637085
train_iter_loss: 0.13133129477500916
train_iter_loss: 0.24460457265377045
train_iter_loss: 0.21684274077415466
train_iter_loss: 0.20290416479110718
train_iter_loss: 0.2102096825838089
train loss :0.2281
---------------------
Validation seg loss: 0.2446895448034102 at epoch 31
epoch =     32/  1000, exp = train
train_iter_loss: 0.18617326021194458
train_iter_loss: 0.19459421932697296
train_iter_loss: 0.1460835039615631
train_iter_loss: 0.1819070279598236
train_iter_loss: 0.21215806901454926
train_iter_loss: 0.2236277163028717
train_iter_loss: 0.2029937356710434
train_iter_loss: 0.19445237517356873
train_iter_loss: 0.21357868611812592
train_iter_loss: 0.1422831118106842
train_iter_loss: 0.13466323912143707
train_iter_loss: 0.2267376333475113
train_iter_loss: 0.16713978350162506
train_iter_loss: 0.18023061752319336
train_iter_loss: 0.31615158915519714
train_iter_loss: 0.1396082043647766
train_iter_loss: 0.2300741970539093
train_iter_loss: 0.2645874321460724
train_iter_loss: 0.2711271047592163
train_iter_loss: 0.18996784090995789
train_iter_loss: 0.15103945136070251
train_iter_loss: 0.1976994425058365
train_iter_loss: 0.21673624217510223
train_iter_loss: 0.17492716014385223
train_iter_loss: 0.21677814424037933
train_iter_loss: 0.08616085350513458
train_iter_loss: 0.18111108243465424
train_iter_loss: 0.26352033019065857
train_iter_loss: 0.2676331102848053
train_iter_loss: 0.2518431544303894
train_iter_loss: 0.17188501358032227
train_iter_loss: 0.30235257744789124
train_iter_loss: 0.23222459852695465
train_iter_loss: 0.25980809330940247
train_iter_loss: 0.21252432465553284
train_iter_loss: 0.30321434140205383
train_iter_loss: 0.33331286907196045
train_iter_loss: 0.23496773838996887
train_iter_loss: 0.18885105848312378
train_iter_loss: 0.19398102164268494
train_iter_loss: 0.17407147586345673
train_iter_loss: 0.26819899678230286
train_iter_loss: 0.20019540190696716
train_iter_loss: 0.24582654237747192
train_iter_loss: 0.26594090461730957
train_iter_loss: 0.20184406638145447
train_iter_loss: 0.3584860861301422
train_iter_loss: 0.14827972650527954
train_iter_loss: 0.19887880980968475
train_iter_loss: 0.28856393694877625
train_iter_loss: 0.1293831318616867
train_iter_loss: 0.2283421754837036
train_iter_loss: 0.15943673253059387
train_iter_loss: 0.282089501619339
train_iter_loss: 0.392713338136673
train_iter_loss: 0.19894428551197052
train_iter_loss: 0.19195930659770966
train_iter_loss: 0.12849637866020203
train_iter_loss: 0.20118622481822968
train_iter_loss: 0.4750685691833496
train_iter_loss: 0.14813143014907837
train_iter_loss: 0.27793246507644653
train_iter_loss: 0.21914273500442505
train_iter_loss: 0.15747451782226562
train_iter_loss: 0.2185286432504654
train_iter_loss: 0.16692788898944855
train_iter_loss: 0.14431361854076385
train_iter_loss: 0.19196996092796326
train_iter_loss: 0.1801622211933136
train_iter_loss: 0.2549997568130493
train_iter_loss: 0.20759376883506775
train_iter_loss: 0.3667418360710144
train_iter_loss: 0.21969729661941528
train_iter_loss: 0.14424262940883636
train_iter_loss: 0.2298697829246521
train_iter_loss: 0.3039546608924866
train_iter_loss: 0.11986157298088074
train_iter_loss: 0.2994610667228699
train_iter_loss: 0.14878670871257782
train_iter_loss: 0.21584457159042358
train_iter_loss: 0.22645322978496552
train_iter_loss: 0.21155120432376862
train_iter_loss: 0.130650594830513
train_iter_loss: 0.1759289801120758
train_iter_loss: 0.28781458735466003
train_iter_loss: 0.2626541256904602
train_iter_loss: 0.1664920449256897
train_iter_loss: 0.2583264410495758
train_iter_loss: 0.18432927131652832
train_iter_loss: 0.21038170158863068
train_iter_loss: 0.22193941473960876
train_iter_loss: 0.17455051839351654
train_iter_loss: 0.24953660368919373
train_iter_loss: 0.153880774974823
train_iter_loss: 0.3069438934326172
train_iter_loss: 0.25560539960861206
train_iter_loss: 0.21093660593032837
train_iter_loss: 0.1882660686969757
train_iter_loss: 0.14961926639080048
train_iter_loss: 0.2894386351108551
train loss :0.2196
---------------------
Validation seg loss: 0.2435355552314025 at epoch 32
********************
best_val_epoch_loss:  0.2435355552314025
MODEL UPDATED
epoch =     33/  1000, exp = train
train_iter_loss: 0.23178495466709137
train_iter_loss: 0.2200143188238144
train_iter_loss: 0.21118807792663574
train_iter_loss: 0.2489558607339859
train_iter_loss: 0.23444949090480804
train_iter_loss: 0.23186101019382477
train_iter_loss: 0.1696583479642868
train_iter_loss: 0.17822249233722687
train_iter_loss: 0.21487705409526825
train_iter_loss: 0.2416931837797165
train_iter_loss: 0.22060228884220123
train_iter_loss: 0.26236602663993835
train_iter_loss: 0.2768935561180115
train_iter_loss: 0.20449426770210266
train_iter_loss: 0.12119419127702713
train_iter_loss: 0.2901478111743927
train_iter_loss: 0.3081449568271637
train_iter_loss: 0.20272576808929443
train_iter_loss: 0.22108303010463715
train_iter_loss: 0.21694958209991455
train_iter_loss: 0.22464007139205933
train_iter_loss: 0.18585701286792755
train_iter_loss: 0.21600496768951416
train_iter_loss: 0.21207663416862488
train_iter_loss: 0.2931317090988159
train_iter_loss: 0.18248818814754486
train_iter_loss: 0.19482065737247467
train_iter_loss: 0.24670438468456268
train_iter_loss: 0.1515030413866043
train_iter_loss: 0.10498616099357605
train_iter_loss: 0.2978650629520416
train_iter_loss: 0.34765323996543884
train_iter_loss: 0.20514318346977234
train_iter_loss: 0.21256987750530243
train_iter_loss: 0.27047058939933777
train_iter_loss: 0.1759035289287567
train_iter_loss: 0.2002277374267578
train_iter_loss: 0.20688393712043762
train_iter_loss: 0.15595270693302155
train_iter_loss: 0.1879030168056488
train_iter_loss: 0.1631590873003006
train_iter_loss: 0.26595064997673035
train_iter_loss: 0.2705870270729065
train_iter_loss: 0.18275311589241028
train_iter_loss: 0.31623169779777527
train_iter_loss: 0.21197102963924408
train_iter_loss: 0.22714079916477203
train_iter_loss: 0.23308296501636505
train_iter_loss: 0.14721550047397614
train_iter_loss: 0.2468845397233963
train_iter_loss: 0.22942504286766052
train_iter_loss: 0.11267326027154922
train_iter_loss: 0.16239391267299652
train_iter_loss: 0.2838118076324463
train_iter_loss: 0.23093412816524506
train_iter_loss: 0.19634053111076355
train_iter_loss: 0.25123825669288635
train_iter_loss: 0.20563486218452454
train_iter_loss: 0.18557988107204437
train_iter_loss: 0.27272936701774597
train_iter_loss: 0.2784503400325775
train_iter_loss: 0.21232156455516815
train_iter_loss: 0.09854985773563385
train_iter_loss: 0.310891330242157
train_iter_loss: 0.21637484431266785
train_iter_loss: 0.1869756132364273
train_iter_loss: 0.1358392834663391
train_iter_loss: 0.1889405995607376
train_iter_loss: 0.32884350419044495
train_iter_loss: 0.3129516541957855
train_iter_loss: 0.16522887349128723
train_iter_loss: 0.1962282806634903
train_iter_loss: 0.2818743586540222
train_iter_loss: 0.1823883205652237
train_iter_loss: 0.11389466375112534
train_iter_loss: 0.18698249757289886
train_iter_loss: 0.24967703223228455
train_iter_loss: 0.19941715896129608
train_iter_loss: 0.23930427432060242
train_iter_loss: 0.22347715497016907
train_iter_loss: 0.1619451344013214
train_iter_loss: 0.09686658531427383
train_iter_loss: 0.2093394696712494
train_iter_loss: 0.17090067267417908
train_iter_loss: 0.23085416853427887
train_iter_loss: 0.1527157425880432
train_iter_loss: 0.20303496718406677
train_iter_loss: 0.19408029317855835
train_iter_loss: 0.15328872203826904
train_iter_loss: 0.19840185344219208
train_iter_loss: 0.23663803935050964
train_iter_loss: 0.18070346117019653
train_iter_loss: 0.16581428050994873
train_iter_loss: 0.18621109426021576
train_iter_loss: 0.17514294385910034
train_iter_loss: 0.1327231377363205
train_iter_loss: 0.253458172082901
train_iter_loss: 0.2262040227651596
train_iter_loss: 0.22309787571430206
train_iter_loss: 0.23947198688983917
train loss :0.2149
---------------------
Validation seg loss: 0.24764818511903286 at epoch 33
epoch =     34/  1000, exp = train
train_iter_loss: 0.24358850717544556
train_iter_loss: 0.20744244754314423
train_iter_loss: 0.1366858035326004
train_iter_loss: 0.24541649222373962
train_iter_loss: 0.22301819920539856
train_iter_loss: 0.25110530853271484
train_iter_loss: 0.21889175474643707
train_iter_loss: 0.26678338646888733
train_iter_loss: 0.24892717599868774
train_iter_loss: 0.18656480312347412
train_iter_loss: 0.36418214440345764
train_iter_loss: 0.19227169454097748
train_iter_loss: 0.1630534827709198
train_iter_loss: 0.18123620748519897
train_iter_loss: 0.16575156152248383
train_iter_loss: 0.2115076184272766
train_iter_loss: 0.11316079646348953
train_iter_loss: 0.24424861371517181
train_iter_loss: 0.22752214968204498
train_iter_loss: 0.14965105056762695
train_iter_loss: 0.13328367471694946
train_iter_loss: 0.16018685698509216
train_iter_loss: 0.19963164627552032
train_iter_loss: 0.14414849877357483
train_iter_loss: 0.31028202176094055
train_iter_loss: 0.23627698421478271
train_iter_loss: 0.24951684474945068
train_iter_loss: 0.14910359680652618
train_iter_loss: 0.12062884867191315
train_iter_loss: 0.143655464053154
train_iter_loss: 0.3077341616153717
train_iter_loss: 0.22736485302448273
train_iter_loss: 0.19145506620407104
train_iter_loss: 0.1698075383901596
train_iter_loss: 0.23119549453258514
train_iter_loss: 0.11495015770196915
train_iter_loss: 0.2644900381565094
train_iter_loss: 0.26436853408813477
train_iter_loss: 0.22995935380458832
train_iter_loss: 0.15106633305549622
train_iter_loss: 0.2149112969636917
train_iter_loss: 0.1399192363023758
train_iter_loss: 0.11504405736923218
train_iter_loss: 0.34114721417427063
train_iter_loss: 0.26536375284194946
train_iter_loss: 0.23358072340488434
train_iter_loss: 0.1415840983390808
train_iter_loss: 0.18219055235385895
train_iter_loss: 0.19418571889400482
train_iter_loss: 0.1383947730064392
train_iter_loss: 0.37297749519348145
train_iter_loss: 0.182302787899971
train_iter_loss: 0.21412047743797302
train_iter_loss: 0.20792803168296814
train_iter_loss: 0.19219447672367096
train_iter_loss: 0.22764761745929718
train_iter_loss: 0.1708698570728302
train_iter_loss: 0.3124782145023346
train_iter_loss: 0.193206787109375
train_iter_loss: 0.2938655912876129
train_iter_loss: 0.16487731039524078
train_iter_loss: 0.22190861403942108
train_iter_loss: 0.3135077953338623
train_iter_loss: 0.2031756341457367
train_iter_loss: 0.21267658472061157
train_iter_loss: 0.19872024655342102
train_iter_loss: 0.20395728945732117
train_iter_loss: 0.30355584621429443
train_iter_loss: 0.23516111075878143
train_iter_loss: 0.20625194907188416
train_iter_loss: 0.13882553577423096
train_iter_loss: 0.21046030521392822
train_iter_loss: 0.2014853060245514
train_iter_loss: 0.21836738288402557
train_iter_loss: 0.35706064105033875
train_iter_loss: 0.20741234719753265
train_iter_loss: 0.15815424919128418
train_iter_loss: 0.15238021314144135
train_iter_loss: 0.13074712455272675
train_iter_loss: 0.24609264731407166
train_iter_loss: 0.13625076413154602
train_iter_loss: 0.274138480424881
train_iter_loss: 0.23580409586429596
train_iter_loss: 0.20364099740982056
train_iter_loss: 0.2328140139579773
train_iter_loss: 0.4386872351169586
train_iter_loss: 0.15166586637496948
train_iter_loss: 0.2687237858772278
train_iter_loss: 0.05157661810517311
train_iter_loss: 0.22513580322265625
train_iter_loss: 0.1554284542798996
train_iter_loss: 0.3038286566734314
train_iter_loss: 0.3290557861328125
train_iter_loss: 0.21876735985279083
train_iter_loss: 0.2305467426776886
train_iter_loss: 0.21727339923381805
train_iter_loss: 0.19366174936294556
train_iter_loss: 0.12765978276729584
train_iter_loss: 0.3430211544036865
train_iter_loss: 0.16877536475658417
train loss :0.2158
---------------------
Validation seg loss: 0.2410385437169165 at epoch 34
********************
best_val_epoch_loss:  0.2410385437169165
MODEL UPDATED
epoch =     35/  1000, exp = train
train_iter_loss: 0.20608699321746826
train_iter_loss: 0.17345599830150604
train_iter_loss: 0.143589049577713
train_iter_loss: 0.14323563873767853
train_iter_loss: 0.25536513328552246
train_iter_loss: 0.20748494565486908
train_iter_loss: 0.2031015008687973
train_iter_loss: 0.26770472526550293
train_iter_loss: 0.2000858336687088
train_iter_loss: 0.12850229442119598
train_iter_loss: 0.19726458191871643
train_iter_loss: 0.2351296991109848
train_iter_loss: 0.20486125349998474
train_iter_loss: 0.18456895649433136
train_iter_loss: 0.27592530846595764
train_iter_loss: 0.16235435009002686
train_iter_loss: 0.22311267256736755
train_iter_loss: 0.18427620828151703
train_iter_loss: 0.1892506629228592
train_iter_loss: 0.17896345257759094
train_iter_loss: 0.08393286168575287
train_iter_loss: 0.146417036652565
train_iter_loss: 0.21677210927009583
train_iter_loss: 0.2115081399679184
train_iter_loss: 0.36563944816589355
train_iter_loss: 0.18535970151424408
train_iter_loss: 0.2017884999513626
train_iter_loss: 0.22681985795497894
train_iter_loss: 0.16641919314861298
train_iter_loss: 0.18744990229606628
train_iter_loss: 0.1434761881828308
train_iter_loss: 0.23990249633789062
train_iter_loss: 0.19845329225063324
train_iter_loss: 0.20094500482082367
train_iter_loss: 0.2292405068874359
train_iter_loss: 0.26422247290611267
train_iter_loss: 0.26048755645751953
train_iter_loss: 0.19980581104755402
train_iter_loss: 0.27450597286224365
train_iter_loss: 0.1042742058634758
train_iter_loss: 0.1840948909521103
train_iter_loss: 0.20276126265525818
train_iter_loss: 0.15400074422359467
train_iter_loss: 0.29670166969299316
train_iter_loss: 0.23773084580898285
train_iter_loss: 0.35613805055618286
train_iter_loss: 0.2836308479309082
train_iter_loss: 0.1626843810081482
train_iter_loss: 0.3053855001926422
train_iter_loss: 0.2510329782962799
train_iter_loss: 0.1780649572610855
train_iter_loss: 0.22238148748874664
train_iter_loss: 0.2632370591163635
train_iter_loss: 0.20158369839191437
train_iter_loss: 0.1449473649263382
train_iter_loss: 0.18751052021980286
train_iter_loss: 0.1475633680820465
train_iter_loss: 0.24184934794902802
train_iter_loss: 0.27222707867622375
train_iter_loss: 0.1842135637998581
train_iter_loss: 0.29698067903518677
train_iter_loss: 0.20569084584712982
train_iter_loss: 0.29430365562438965
train_iter_loss: 0.16543732583522797
train_iter_loss: 0.20555157959461212
train_iter_loss: 0.2111276537179947
train_iter_loss: 0.11062662303447723
train_iter_loss: 0.11579566448926926
train_iter_loss: 0.1644921749830246
train_iter_loss: 0.1908874213695526
train_iter_loss: 0.15260133147239685
train_iter_loss: 0.14775480329990387
train_iter_loss: 0.2306414693593979
train_iter_loss: 0.19160714745521545
train_iter_loss: 0.22143645584583282
train_iter_loss: 0.27637219429016113
train_iter_loss: 0.24709975719451904
train_iter_loss: 0.2914835512638092
train_iter_loss: 0.19921904802322388
train_iter_loss: 0.13296166062355042
train_iter_loss: 0.2116125375032425
train_iter_loss: 0.21499432623386383
train_iter_loss: 0.13114812970161438
train_iter_loss: 0.3310694396495819
train_iter_loss: 0.3532901108264923
train_iter_loss: 0.28190597891807556
train_iter_loss: 0.2700192928314209
train_iter_loss: 0.2945091724395752
train_iter_loss: 0.29006335139274597
train_iter_loss: 0.18549282848834991
train_iter_loss: 0.231461301445961
train_iter_loss: 0.15463200211524963
train_iter_loss: 0.14867253601551056
train_iter_loss: 0.34773552417755127
train_iter_loss: 0.1566459983587265
train_iter_loss: 0.20300143957138062
train_iter_loss: 0.21382692456245422
train_iter_loss: 0.32908305525779724
train_iter_loss: 0.1602933704853058
train_iter_loss: 0.10190101712942123
train loss :0.2143
---------------------
Validation seg loss: 0.23810271658706214 at epoch 35
********************
best_val_epoch_loss:  0.23810271658706214
MODEL UPDATED
epoch =     36/  1000, exp = train
train_iter_loss: 0.28678128123283386
train_iter_loss: 0.2327190786600113
train_iter_loss: 0.18591263890266418
train_iter_loss: 0.15742194652557373
train_iter_loss: 0.21123534440994263
train_iter_loss: 0.32764938473701477
train_iter_loss: 0.21642103791236877
train_iter_loss: 0.23874180018901825
train_iter_loss: 0.2175447940826416
train_iter_loss: 0.22155211865901947
train_iter_loss: 0.21775178611278534
train_iter_loss: 0.24547705054283142
train_iter_loss: 0.17861489951610565
train_iter_loss: 0.3339878022670746
train_iter_loss: 0.22095733880996704
train_iter_loss: 0.24653668701648712
train_iter_loss: 0.1967121958732605
train_iter_loss: 0.24179978668689728
train_iter_loss: 0.22692401707172394
train_iter_loss: 0.24622222781181335
train_iter_loss: 0.19210006296634674
train_iter_loss: 0.2576860785484314
train_iter_loss: 0.3198579251766205
train_iter_loss: 0.11877801269292831
train_iter_loss: 0.19640836119651794
train_iter_loss: 0.1360122561454773
train_iter_loss: 0.2133229821920395
train_iter_loss: 0.22609084844589233
train_iter_loss: 0.16658388078212738
train_iter_loss: 0.16554100811481476
train_iter_loss: 0.1651812046766281
train_iter_loss: 0.19521564245224
train_iter_loss: 0.22830133140087128
train_iter_loss: 0.1348249465227127
train_iter_loss: 0.1767447292804718
train_iter_loss: 0.19442957639694214
train_iter_loss: 0.19448228180408478
train_iter_loss: 0.19323299825191498
train_iter_loss: 0.08532485365867615
train_iter_loss: 0.1862562596797943
train_iter_loss: 0.21013237535953522
train_iter_loss: 0.2610727548599243
train_iter_loss: 0.1597139686346054
train_iter_loss: 0.18950852751731873
train_iter_loss: 0.13430124521255493
train_iter_loss: 0.33596286177635193
train_iter_loss: 0.1570136398077011
train_iter_loss: 0.16385704278945923
train_iter_loss: 0.21572786569595337
train_iter_loss: 0.1944645494222641
train_iter_loss: 0.33623918890953064
train_iter_loss: 0.1392877697944641
train_iter_loss: 0.28453823924064636
train_iter_loss: 0.19566568732261658
train_iter_loss: 0.22946669161319733
train_iter_loss: 0.21169747412204742
train_iter_loss: 0.13993598520755768
train_iter_loss: 0.21937495470046997
train_iter_loss: 0.32163771986961365
train_iter_loss: 0.1770528256893158
train_iter_loss: 0.30354511737823486
train_iter_loss: 0.2921595573425293
train_iter_loss: 0.1860763281583786
train_iter_loss: 0.20416592061519623
train_iter_loss: 0.30831262469291687
train_iter_loss: 0.12635840475559235
train_iter_loss: 0.17090904712677002
train_iter_loss: 0.20794132351875305
train_iter_loss: 0.23671618103981018
train_iter_loss: 0.1733504682779312
train_iter_loss: 0.20517303049564362
train_iter_loss: 0.20435690879821777
train_iter_loss: 0.2465146780014038
train_iter_loss: 0.22912681102752686
train_iter_loss: 0.1523946076631546
train_iter_loss: 0.19010421633720398
train_iter_loss: 0.26735395193099976
train_iter_loss: 0.341178297996521
train_iter_loss: 0.17653153836727142
train_iter_loss: 0.215812548995018
train_iter_loss: 0.1408298760652542
train_iter_loss: 0.20614898204803467
train_iter_loss: 0.12852884829044342
train_iter_loss: 0.3084334135055542
train_iter_loss: 0.20843897759914398
train_iter_loss: 0.15775710344314575
train_iter_loss: 0.2030652016401291
train_iter_loss: 0.20156791806221008
train_iter_loss: 0.24534645676612854
train_iter_loss: 0.1591598391532898
train_iter_loss: 0.22055722773075104
train_iter_loss: 0.16517964005470276
train_iter_loss: 0.18877573311328888
train_iter_loss: 0.29463279247283936
train_iter_loss: 0.20142407715320587
train_iter_loss: 0.2228448987007141
train_iter_loss: 0.2750755548477173
train_iter_loss: 0.3614787757396698
train_iter_loss: 0.3443792164325714
train_iter_loss: 0.18208196759223938
train loss :0.2174
---------------------
Validation seg loss: 0.23804611659977795 at epoch 36
********************
best_val_epoch_loss:  0.23804611659977795
MODEL UPDATED
epoch =     37/  1000, exp = train
train_iter_loss: 0.24200381338596344
train_iter_loss: 0.16844694316387177
train_iter_loss: 0.30262574553489685
train_iter_loss: 0.11534927785396576
train_iter_loss: 0.33596736192703247
train_iter_loss: 0.18795666098594666
train_iter_loss: 0.17559871077537537
train_iter_loss: 0.5394721031188965
train_iter_loss: 0.24904917180538177
train_iter_loss: 0.2328835278749466
train_iter_loss: 0.21754160523414612
train_iter_loss: 0.18300111591815948
train_iter_loss: 0.21586143970489502
train_iter_loss: 0.16844041645526886
train_iter_loss: 0.3111813962459564
train_iter_loss: 0.19248619675636292
train_iter_loss: 0.12380469590425491
train_iter_loss: 0.26582178473472595
train_iter_loss: 0.3794151544570923
train_iter_loss: 0.2629239559173584
train_iter_loss: 0.14525745809078217
train_iter_loss: 0.24181610345840454
train_iter_loss: 0.19846272468566895
train_iter_loss: 0.12905964255332947
train_iter_loss: 0.17238390445709229
train_iter_loss: 0.18476280570030212
train_iter_loss: 0.13423368334770203
train_iter_loss: 0.2520332634449005
train_iter_loss: 0.15433354675769806
train_iter_loss: 0.19046546518802643
train_iter_loss: 0.12352738529443741
train_iter_loss: 0.21844476461410522
train_iter_loss: 0.31767648458480835
train_iter_loss: 0.20716066658496857
train_iter_loss: 0.15603646636009216
train_iter_loss: 0.2997000813484192
train_iter_loss: 0.2688745856285095
train_iter_loss: 0.23832495510578156
train_iter_loss: 0.26179563999176025
train_iter_loss: 0.2729257047176361
train_iter_loss: 0.39342787861824036
train_iter_loss: 0.24415595829486847
train_iter_loss: 0.12117064744234085
train_iter_loss: 0.08379191160202026
train_iter_loss: 0.13766247034072876
train_iter_loss: 0.15365758538246155
train_iter_loss: 0.19256742298603058
train_iter_loss: 0.2812449634075165
train_iter_loss: 0.24264660477638245
train_iter_loss: 0.22065532207489014
train_iter_loss: 0.23626698553562164
train_iter_loss: 0.18066343665122986
train_iter_loss: 0.20277979969978333
train_iter_loss: 0.20446829497814178
train_iter_loss: 0.2305058091878891
train_iter_loss: 0.16038401424884796
train_iter_loss: 0.1277616173028946
train_iter_loss: 0.30818673968315125
train_iter_loss: 0.24475227296352386
train_iter_loss: 0.17137600481510162
train_iter_loss: 0.29339370131492615
train_iter_loss: 0.29108959436416626
train_iter_loss: 0.2372116893529892
train_iter_loss: 0.19867046177387238
train_iter_loss: 0.19832636415958405
train_iter_loss: 0.20867154002189636
train_iter_loss: 0.282557874917984
train_iter_loss: 0.22402246296405792
train_iter_loss: 0.13621510565280914
train_iter_loss: 0.12583644688129425
train_iter_loss: 0.1548248678445816
train_iter_loss: 0.36866021156311035
train_iter_loss: 0.1590765416622162
train_iter_loss: 0.24085238575935364
train_iter_loss: 0.1691121608018875
train_iter_loss: 0.12639214098453522
train_iter_loss: 0.14050066471099854
train_iter_loss: 0.08169493824243546
train_iter_loss: 0.17890788614749908
train_iter_loss: 0.27620741724967957
train_iter_loss: 0.3056451976299286
train_iter_loss: 0.2342478632926941
train_iter_loss: 0.18367218971252441
train_iter_loss: 0.19462518393993378
train_iter_loss: 0.18136954307556152
train_iter_loss: 0.17816852033138275
train_iter_loss: 0.24086624383926392
train_iter_loss: 0.17044757306575775
train_iter_loss: 0.2154332995414734
train_iter_loss: 0.137690469622612
train_iter_loss: 0.1645343154668808
train_iter_loss: 0.10694396495819092
train_iter_loss: 0.23591464757919312
train_iter_loss: 0.1793793886899948
train_iter_loss: 0.20556029677391052
train_iter_loss: 0.20007692277431488
train_iter_loss: 0.23915183544158936
train_iter_loss: 0.2001093477010727
train_iter_loss: 0.15754273533821106
train_iter_loss: 0.11161619424819946
train loss :0.2133
---------------------
Validation seg loss: 0.23999335697658783 at epoch 37
epoch =     38/  1000, exp = train
train_iter_loss: 0.20476597547531128
train_iter_loss: 0.11946415901184082
train_iter_loss: 0.19359903037548065
train_iter_loss: 0.24751947820186615
train_iter_loss: 0.19618035852909088
train_iter_loss: 0.17744889855384827
train_iter_loss: 0.17306523025035858
train_iter_loss: 0.22077280282974243
train_iter_loss: 0.3390912413597107
train_iter_loss: 0.15822075307369232
train_iter_loss: 0.1792612373828888
train_iter_loss: 0.20585936307907104
train_iter_loss: 0.21816033124923706
train_iter_loss: 0.29213792085647583
train_iter_loss: 0.2945323586463928
train_iter_loss: 0.14832107722759247
train_iter_loss: 0.1094495877623558
train_iter_loss: 0.1239146962761879
train_iter_loss: 0.20764341950416565
train_iter_loss: 0.22024495899677277
train_iter_loss: 0.14998175203800201
train_iter_loss: 0.12797626852989197
train_iter_loss: 0.25436902046203613
train_iter_loss: 0.12003424763679504
train_iter_loss: 0.26743727922439575
train_iter_loss: 0.31161847710609436
train_iter_loss: 0.21772664785385132
train_iter_loss: 0.29500362277030945
train_iter_loss: 0.19184142351150513
train_iter_loss: 0.13244594633579254
train_iter_loss: 0.1419433355331421
train_iter_loss: 0.47770199179649353
train_iter_loss: 0.23704952001571655
train_iter_loss: 0.20485632121562958
train_iter_loss: 0.25987690687179565
train_iter_loss: 0.19092801213264465
train_iter_loss: 0.2638559639453888
train_iter_loss: 0.18540018796920776
train_iter_loss: 0.27566269040107727
train_iter_loss: 0.20767681300640106
train_iter_loss: 0.21289989352226257
train_iter_loss: 0.3228381276130676
train_iter_loss: 0.191547229886055
train_iter_loss: 0.23726777732372284
train_iter_loss: 0.29940366744995117
train_iter_loss: 0.22661831974983215
train_iter_loss: 0.18510408699512482
train_iter_loss: 0.22683225572109222
train_iter_loss: 0.17843498289585114
train_iter_loss: 0.23980511724948883
train_iter_loss: 0.20050416886806488
train_iter_loss: 0.20450866222381592
train_iter_loss: 0.2704428434371948
train_iter_loss: 0.21788644790649414
train_iter_loss: 0.23691785335540771
train_iter_loss: 0.22596889734268188
train_iter_loss: 0.13593147695064545
train_iter_loss: 0.18968676030635834
train_iter_loss: 0.20402489602565765
train_iter_loss: 0.15379945933818817
train_iter_loss: 0.3147680163383484
train_iter_loss: 0.21211031079292297
train_iter_loss: 0.3193630576133728
train_iter_loss: 0.1900429129600525
train_iter_loss: 0.16116425395011902
train_iter_loss: 0.1564488559961319
train_iter_loss: 0.1270616203546524
train_iter_loss: 0.2234492301940918
train_iter_loss: 0.24399538338184357
train_iter_loss: 0.21415959298610687
train_iter_loss: 0.10514988005161285
train_iter_loss: 0.22429239749908447
train_iter_loss: 0.2605128288269043
train_iter_loss: 0.21218930184841156
train_iter_loss: 0.15406474471092224
train_iter_loss: 0.21099314093589783
train_iter_loss: 0.23273682594299316
train_iter_loss: 0.15236760675907135
train_iter_loss: 0.18711765110492706
train_iter_loss: 0.21675778925418854
train_iter_loss: 0.11273238062858582
train_iter_loss: 0.48252779245376587
train_iter_loss: 0.1563623547554016
train_iter_loss: 0.23666152358055115
train_iter_loss: 0.1845403015613556
train_iter_loss: 0.16298244893550873
train_iter_loss: 0.10652519017457962
train_iter_loss: 0.11096175760030746
train_iter_loss: 0.2453298270702362
train_iter_loss: 0.16481390595436096
train_iter_loss: 0.30510783195495605
train_iter_loss: 0.15421265363693237
train_iter_loss: 0.25742825865745544
train_iter_loss: 0.21863040328025818
train_iter_loss: 0.21998357772827148
train_iter_loss: 0.19154231250286102
train_iter_loss: 0.2749175727367401
train_iter_loss: 0.27047020196914673
train_iter_loss: 0.31812840700149536
train_iter_loss: 0.11057834327220917
train loss :0.2152
---------------------
Validation seg loss: 0.23735442195298537 at epoch 38
********************
best_val_epoch_loss:  0.23735442195298537
MODEL UPDATED
epoch =     39/  1000, exp = train
train_iter_loss: 0.18295451998710632
train_iter_loss: 0.10007788985967636
train_iter_loss: 0.17799484729766846
train_iter_loss: 0.1658884435892105
train_iter_loss: 0.239286869764328
train_iter_loss: 0.25220534205436707
train_iter_loss: 0.1888648420572281
train_iter_loss: 0.1958998143672943
train_iter_loss: 0.20460231602191925
train_iter_loss: 0.2299489974975586
train_iter_loss: 0.1915096789598465
train_iter_loss: 0.17726993560791016
train_iter_loss: 0.20776784420013428
train_iter_loss: 0.11000233888626099
train_iter_loss: 0.18808645009994507
train_iter_loss: 0.17999786138534546
train_iter_loss: 0.25658413767814636
train_iter_loss: 0.2606269419193268
train_iter_loss: 0.1828874796628952
train_iter_loss: 0.13979804515838623
train_iter_loss: 0.21499395370483398
train_iter_loss: 0.211142435669899
train_iter_loss: 0.17123223841190338
train_iter_loss: 0.13165409862995148
train_iter_loss: 0.29873210191726685
train_iter_loss: 0.25342753529548645
train_iter_loss: 0.1372775286436081
train_iter_loss: 0.22055672109127045
train_iter_loss: 0.16791905462741852
train_iter_loss: 0.1683150976896286
train_iter_loss: 0.23203293979167938
train_iter_loss: 0.25414782762527466
train_iter_loss: 0.1957155168056488
train_iter_loss: 0.2299225926399231
train_iter_loss: 0.10816995799541473
train_iter_loss: 0.1442665308713913
train_iter_loss: 0.17031215131282806
train_iter_loss: 0.23910924792289734
train_iter_loss: 0.12035052478313446
train_iter_loss: 0.21018977463245392
train_iter_loss: 0.2644879221916199
train_iter_loss: 0.21957851946353912
train_iter_loss: 0.18987832963466644
train_iter_loss: 0.28094086050987244
train_iter_loss: 0.19500060379505157
train_iter_loss: 0.22162622213363647
train_iter_loss: 0.2879348397254944
train_iter_loss: 0.1992872655391693
train_iter_loss: 0.2411109060049057
train_iter_loss: 0.1664479821920395
train_iter_loss: 0.16644416749477386
train_iter_loss: 0.159402534365654
train_iter_loss: 0.1332077980041504
train_iter_loss: 0.18861548602581024
train_iter_loss: 0.3334529399871826
train_iter_loss: 0.1176530197262764
train_iter_loss: 0.16215036809444427
train_iter_loss: 0.20380552113056183
train_iter_loss: 0.2379496693611145
train_iter_loss: 0.208781898021698
train_iter_loss: 0.15370571613311768
train_iter_loss: 0.16338662803173065
train_iter_loss: 0.23082950711250305
train_iter_loss: 0.261873334646225
train_iter_loss: 0.1509597897529602
train_iter_loss: 0.19063855707645416
train_iter_loss: 0.28211092948913574
train_iter_loss: 0.35011935234069824
train_iter_loss: 0.2862778902053833
train_iter_loss: 0.12357582896947861
train_iter_loss: 0.2124778926372528
train_iter_loss: 0.2111235111951828
train_iter_loss: 0.1334894448518753
train_iter_loss: 0.16671396791934967
train_iter_loss: 0.26996514201164246
train_iter_loss: 0.21961790323257446
train_iter_loss: 0.28023242950439453
train_iter_loss: 0.15613937377929688
train_iter_loss: 0.15827180445194244
train_iter_loss: 0.17062512040138245
train_iter_loss: 0.24464650452136993
train_iter_loss: 0.0806063860654831
train_iter_loss: 0.2629210650920868
train_iter_loss: 0.22985617816448212
train_iter_loss: 0.2865186333656311
train_iter_loss: 0.2419346272945404
train_iter_loss: 0.2571468651294708
train_iter_loss: 0.16549068689346313
train_iter_loss: 0.20900534093379974
train_iter_loss: 0.30433857440948486
train_iter_loss: 0.1654854714870453
train_iter_loss: 0.1703343689441681
train_iter_loss: 0.20366676151752472
train_iter_loss: 0.2952457368373871
train_iter_loss: 0.28928524255752563
train_iter_loss: 0.19405879080295563
train_iter_loss: 0.23460137844085693
train_iter_loss: 0.2565500736236572
train_iter_loss: 0.2636750042438507
train_iter_loss: 0.2498394399881363
train loss :0.2088
---------------------
Validation seg loss: 0.23653869306282052 at epoch 39
********************
best_val_epoch_loss:  0.23653869306282052
MODEL UPDATED
epoch =     40/  1000, exp = train
train_iter_loss: 0.199473574757576
train_iter_loss: 0.16336871683597565
train_iter_loss: 0.14840394258499146
train_iter_loss: 0.2563870847225189
train_iter_loss: 0.15211798250675201
train_iter_loss: 0.17820151150226593
train_iter_loss: 0.17938272655010223
train_iter_loss: 0.19186539947986603
train_iter_loss: 0.12971806526184082
train_iter_loss: 0.13037371635437012
train_iter_loss: 0.22385621070861816
train_iter_loss: 0.21170572936534882
train_iter_loss: 0.2500491142272949
train_iter_loss: 0.1888132393360138
train_iter_loss: 0.19790557026863098
train_iter_loss: 0.17589811980724335
train_iter_loss: 0.3854023516178131
train_iter_loss: 0.1388317197561264
train_iter_loss: 0.15624715387821198
train_iter_loss: 0.29791128635406494
train_iter_loss: 0.2077707201242447
train_iter_loss: 0.21503770351409912
train_iter_loss: 0.2948285937309265
train_iter_loss: 0.23256511986255646
train_iter_loss: 0.26760393381118774
train_iter_loss: 0.13562601804733276
train_iter_loss: 0.20850105583667755
train_iter_loss: 0.15885768830776215
train_iter_loss: 0.1602560579776764
train_iter_loss: 0.20473048090934753
train_iter_loss: 0.12023256719112396
train_iter_loss: 0.14603133499622345
train_iter_loss: 0.17714627087116241
train_iter_loss: 0.17831994593143463
train_iter_loss: 0.20701558887958527
train_iter_loss: 0.26572027802467346
train_iter_loss: 0.18130049109458923
train_iter_loss: 0.1432710438966751
train_iter_loss: 0.18439923226833344
train_iter_loss: 0.20542927086353302
train_iter_loss: 0.16874423623085022
train_iter_loss: 0.2359866499900818
train_iter_loss: 0.1898413598537445
train_iter_loss: 0.1775321662425995
train_iter_loss: 0.20506872236728668
train_iter_loss: 0.22296735644340515
train_iter_loss: 0.22985728085041046
train_iter_loss: 0.2730424404144287
train_iter_loss: 0.13643619418144226
train_iter_loss: 0.29979708790779114
train_iter_loss: 0.2414115071296692
train_iter_loss: 0.3387398421764374
train_iter_loss: 0.17620907723903656
train_iter_loss: 0.2885916531085968
train_iter_loss: 0.1344793289899826
train_iter_loss: 0.250307559967041
train_iter_loss: 0.25165891647338867
train_iter_loss: 0.22227881848812103
train_iter_loss: 0.26665621995925903
train_iter_loss: 0.28417810797691345
train_iter_loss: 0.2501385807991028
train_iter_loss: 0.16743357479572296
train_iter_loss: 0.2327374517917633
train_iter_loss: 0.17184200882911682
train_iter_loss: 0.2236579954624176
train_iter_loss: 0.2586461901664734
train_iter_loss: 0.32811465859413147
train_iter_loss: 0.1942601501941681
train_iter_loss: 0.2677059769630432
train_iter_loss: 0.13574041426181793
train_iter_loss: 0.3103836476802826
train_iter_loss: 0.2531578242778778
train_iter_loss: 0.2613632082939148
train_iter_loss: 0.2510841190814972
train_iter_loss: 0.23865455389022827
train_iter_loss: 0.1712060421705246
train_iter_loss: 0.21267279982566833
train_iter_loss: 0.18304868042469025
train_iter_loss: 0.18019844591617584
train_iter_loss: 0.3112899661064148
train_iter_loss: 0.10101819038391113
train_iter_loss: 0.2538994550704956
train_iter_loss: 0.20657481253147125
train_iter_loss: 0.17443808913230896
train_iter_loss: 0.061191629618406296
train_iter_loss: 0.13772723078727722
train_iter_loss: 0.25749751925468445
train_iter_loss: 0.2314334362745285
train_iter_loss: 0.2004529982805252
train_iter_loss: 0.2450670599937439
train_iter_loss: 0.22045357525348663
train_iter_loss: 0.15631535649299622
train_iter_loss: 0.1742628812789917
train_iter_loss: 0.2416166514158249
train_iter_loss: 0.2148081660270691
train_iter_loss: 0.2882324159145355
train_iter_loss: 0.1276446431875229
train_iter_loss: 0.2635655105113983
train_iter_loss: 0.23741111159324646
train_iter_loss: 0.20983077585697174
train loss :0.2126
---------------------
Validation seg loss: 0.24350752164873313 at epoch 40
epoch =     41/  1000, exp = train
train_iter_loss: 0.3037092089653015
train_iter_loss: 0.1406370848417282
train_iter_loss: 0.15134404599666595
train_iter_loss: 0.17235009372234344
train_iter_loss: 0.20681209862232208
train_iter_loss: 0.24416927993297577
train_iter_loss: 0.23078322410583496
train_iter_loss: 0.13569119572639465
train_iter_loss: 0.20886297523975372
train_iter_loss: 0.1363096833229065
train_iter_loss: 0.18424293398857117
train_iter_loss: 0.197470024228096
train_iter_loss: 0.13344356417655945
train_iter_loss: 0.2796136736869812
train_iter_loss: 0.15616893768310547
train_iter_loss: 0.1924571394920349
train_iter_loss: 0.1334080994129181
train_iter_loss: 0.15149492025375366
train_iter_loss: 0.10848650336265564
train_iter_loss: 0.25047603249549866
train_iter_loss: 0.1582786738872528
train_iter_loss: 0.21295952796936035
train_iter_loss: 0.25190380215644836
train_iter_loss: 0.303638756275177
train_iter_loss: 0.10544338077306747
train_iter_loss: 0.2332201898097992
train_iter_loss: 0.2229229360818863
train_iter_loss: 0.20907196402549744
train_iter_loss: 0.1555892378091812
train_iter_loss: 0.23517033457756042
train_iter_loss: 0.17641718685626984
train_iter_loss: 0.15606191754341125
train_iter_loss: 0.2541445195674896
train_iter_loss: 0.27824702858924866
train_iter_loss: 0.12758281826972961
train_iter_loss: 0.16396719217300415
train_iter_loss: 0.18539977073669434
train_iter_loss: 0.17849960923194885
train_iter_loss: 0.15833330154418945
train_iter_loss: 0.2261483371257782
train_iter_loss: 0.18287132680416107
train_iter_loss: 0.17718923091888428
train_iter_loss: 0.24103209376335144
train_iter_loss: 0.17783601582050323
train_iter_loss: 0.20263542234897614
train_iter_loss: 0.14225280284881592
train_iter_loss: 0.12954629957675934
train_iter_loss: 0.12819568812847137
train_iter_loss: 0.21934518218040466
train_iter_loss: 0.21726861596107483
train_iter_loss: 0.19564521312713623
train_iter_loss: 0.11424235254526138
train_iter_loss: 0.11785999685525894
train_iter_loss: 0.18035846948623657
train_iter_loss: 0.2025967538356781
train_iter_loss: 0.17099224030971527
train_iter_loss: 0.1859152764081955
train_iter_loss: 0.20358410477638245
train_iter_loss: 0.3140406310558319
train_iter_loss: 0.17124313116073608
train_iter_loss: 0.1378650665283203
train_iter_loss: 0.18931828439235687
train_iter_loss: 0.222089946269989
train_iter_loss: 0.17118841409683228
train_iter_loss: 0.13679440319538116
train_iter_loss: 0.12218886613845825
train_iter_loss: 0.24913479387760162
train_iter_loss: 0.30297040939331055
train_iter_loss: 0.17741189897060394
train_iter_loss: 0.20966516435146332
train_iter_loss: 0.23682275414466858
train_iter_loss: 0.21494488418102264
train_iter_loss: 0.1697554886341095
train_iter_loss: 0.26745399832725525
train_iter_loss: 0.28404343128204346
train_iter_loss: 0.23872338235378265
train_iter_loss: 0.2955854833126068
train_iter_loss: 0.11953822523355484
train_iter_loss: 0.19662129878997803
train_iter_loss: 0.24337154626846313
train_iter_loss: 0.2253793627023697
train_iter_loss: 0.21633999049663544
train_iter_loss: 0.25567948818206787
train_iter_loss: 0.35043343901634216
train_iter_loss: 0.25812891125679016
train_iter_loss: 0.17983072996139526
train_iter_loss: 0.20419928431510925
train_iter_loss: 0.24212175607681274
train_iter_loss: 0.16278910636901855
train_iter_loss: 0.18712709844112396
train_iter_loss: 0.12016067653894424
train_iter_loss: 0.24954067170619965
train_iter_loss: 0.18289674818515778
train_iter_loss: 0.1694294512271881
train_iter_loss: 0.2353406846523285
train_iter_loss: 0.30258890986442566
train_iter_loss: 0.3365916609764099
train_iter_loss: 0.28572189807891846
train_iter_loss: 0.14402508735656738
train_iter_loss: 0.1735754907131195
train loss :0.2027
---------------------
Validation seg loss: 0.23564463741374467 at epoch 41
********************
best_val_epoch_loss:  0.23564463741374467
MODEL UPDATED
epoch =     42/  1000, exp = train
train_iter_loss: 0.1297929584980011
train_iter_loss: 0.21842975914478302
train_iter_loss: 0.25265541672706604
train_iter_loss: 0.1524369865655899
train_iter_loss: 0.15931569039821625
train_iter_loss: 0.11435883492231369
train_iter_loss: 0.2801588475704193
train_iter_loss: 0.17368008196353912
train_iter_loss: 0.1498735398054123
train_iter_loss: 0.27109163999557495
train_iter_loss: 0.24241143465042114
train_iter_loss: 0.1844274252653122
train_iter_loss: 0.21751658618450165
train_iter_loss: 0.2172287106513977
train_iter_loss: 0.20967546105384827
train_iter_loss: 0.18179412186145782
train_iter_loss: 0.2824386954307556
train_iter_loss: 0.2016623169183731
train_iter_loss: 0.16950297355651855
train_iter_loss: 0.1599893718957901
train_iter_loss: 0.21317069232463837
train_iter_loss: 0.1744149625301361
train_iter_loss: 0.08165083080530167
train_iter_loss: 0.2874188721179962
train_iter_loss: 0.1920035034418106
train_iter_loss: 0.19305524230003357
train_iter_loss: 0.12297249585390091
train_iter_loss: 0.17831729352474213
train_iter_loss: 0.25100451707839966
train_iter_loss: 0.24261891841888428
train_iter_loss: 0.15915514528751373
train_iter_loss: 0.21394824981689453
train_iter_loss: 0.2016361504793167
train_iter_loss: 0.15448220074176788
train_iter_loss: 0.18247917294502258
train_iter_loss: 0.14669868350028992
train_iter_loss: 0.5299085378646851
train_iter_loss: 0.35419803857803345
train_iter_loss: 0.18349403142929077
train_iter_loss: 0.13683727383613586
train_iter_loss: 0.16079875826835632
train_iter_loss: 0.21855592727661133
train_iter_loss: 0.17089809477329254
train_iter_loss: 0.17897416651248932
train_iter_loss: 0.11633829027414322
train_iter_loss: 0.2304626852273941
train_iter_loss: 0.19331291317939758
train_iter_loss: 0.2606804072856903
train_iter_loss: 0.20682714879512787
train_iter_loss: 0.29874372482299805
train_iter_loss: 0.2823258936405182
train_iter_loss: 0.11704281717538834
train_iter_loss: 0.10818129777908325
train_iter_loss: 0.2631685137748718
train_iter_loss: 0.26699793338775635
train_iter_loss: 0.22912387549877167
train_iter_loss: 0.1726679652929306
train_iter_loss: 0.12574875354766846
train_iter_loss: 0.17078980803489685
train_iter_loss: 0.26956459879875183
train_iter_loss: 0.21148104965686798
train_iter_loss: 0.16690129041671753
train_iter_loss: 0.09365145117044449
train_iter_loss: 0.18917256593704224
train_iter_loss: 0.2018115371465683
train_iter_loss: 0.40520167350769043
train_iter_loss: 0.22584395110607147
train_iter_loss: 0.19379295408725739
train_iter_loss: 0.2916041910648346
train_iter_loss: 0.21582265198230743
train_iter_loss: 0.17403659224510193
train_iter_loss: 0.2222016304731369
train_iter_loss: 0.1307685673236847
train_iter_loss: 0.1660122275352478
train_iter_loss: 0.25152936577796936
train_iter_loss: 0.22719377279281616
train_iter_loss: 0.23342761397361755
train_iter_loss: 0.19140954315662384
train_iter_loss: 0.30953267216682434
train_iter_loss: 0.18131139874458313
train_iter_loss: 0.234529048204422
train_iter_loss: 0.27684369683265686
train_iter_loss: 0.26153764128685
train_iter_loss: 0.27664729952812195
train_iter_loss: 0.2681822180747986
train_iter_loss: 0.2786359190940857
train_iter_loss: 0.2673470973968506
train_iter_loss: 0.2122596651315689
train_iter_loss: 0.14494991302490234
train_iter_loss: 0.15558119118213654
train_iter_loss: 0.1162666603922844
train_iter_loss: 0.27524077892303467
train_iter_loss: 0.1446801871061325
train_iter_loss: 0.2850538492202759
train_iter_loss: 0.20901569724082947
train_iter_loss: 0.10464727133512497
train_iter_loss: 0.24185141921043396
train_iter_loss: 0.2380434274673462
train_iter_loss: 0.1880866140127182
train_iter_loss: 0.18232282996177673
train loss :0.2106
---------------------
Validation seg loss: 0.23381915867750375 at epoch 42
********************
best_val_epoch_loss:  0.23381915867750375
MODEL UPDATED
epoch =     43/  1000, exp = train
train_iter_loss: 0.14327016472816467
train_iter_loss: 0.07510664314031601
train_iter_loss: 0.18674097955226898
train_iter_loss: 0.19918417930603027
train_iter_loss: 0.11608661711215973
train_iter_loss: 0.23552876710891724
train_iter_loss: 0.17291247844696045
train_iter_loss: 0.2281719446182251
train_iter_loss: 0.1615391969680786
train_iter_loss: 0.2728351652622223
train_iter_loss: 0.24224968254566193
train_iter_loss: 0.21370261907577515
train_iter_loss: 0.16946610808372498
train_iter_loss: 0.21716901659965515
train_iter_loss: 0.20756162703037262
train_iter_loss: 0.16933484375476837
train_iter_loss: 0.32074809074401855
train_iter_loss: 0.13685265183448792
train_iter_loss: 0.3320596516132355
train_iter_loss: 0.31713196635246277
train_iter_loss: 0.1896236389875412
train_iter_loss: 0.14833229780197144
train_iter_loss: 0.2581903636455536
train_iter_loss: 0.15455763041973114
train_iter_loss: 0.24875564873218536
train_iter_loss: 0.1789758950471878
train_iter_loss: 0.23000122606754303
train_iter_loss: 0.16793283820152283
train_iter_loss: 0.22015202045440674
train_iter_loss: 0.08017213642597198
train_iter_loss: 0.21989957988262177
train_iter_loss: 0.17471033334732056
train_iter_loss: 0.2587478756904602
train_iter_loss: 0.16735918819904327
train_iter_loss: 0.2865296006202698
train_iter_loss: 0.24019397795200348
train_iter_loss: 0.21361678838729858
train_iter_loss: 0.20229579508304596
train_iter_loss: 0.2622445523738861
train_iter_loss: 0.23203106224536896
train_iter_loss: 0.25729095935821533
train_iter_loss: 0.1416986882686615
train_iter_loss: 0.21758896112442017
train_iter_loss: 0.16672834753990173
train_iter_loss: 0.3960900902748108
train_iter_loss: 0.19113926589488983
train_iter_loss: 0.1748133897781372
train_iter_loss: 0.20944295823574066
train_iter_loss: 0.2375115305185318
train_iter_loss: 0.10033607482910156
train_iter_loss: 0.28089383244514465
train_iter_loss: 0.14252468943595886
train_iter_loss: 0.1556931436061859
train_iter_loss: 0.18930213153362274
train_iter_loss: 0.2346654236316681
train_iter_loss: 0.20498991012573242
train_iter_loss: 0.23092348873615265
train_iter_loss: 0.22597052156925201
train_iter_loss: 0.49288448691368103
train_iter_loss: 0.1692323237657547
train_iter_loss: 0.19459883868694305
train_iter_loss: 0.23740120232105255
train_iter_loss: 0.2156614363193512
train_iter_loss: 0.21862785518169403
train_iter_loss: 0.3166554570198059
train_iter_loss: 0.16410663723945618
train_iter_loss: 0.19449953734874725
train_iter_loss: 0.17728255689144135
train_iter_loss: 0.1616043895483017
train_iter_loss: 0.18506135046482086
train_iter_loss: 0.1965978890657425
train_iter_loss: 0.23476988077163696
train_iter_loss: 0.2004726529121399
train_iter_loss: 0.18369156122207642
train_iter_loss: 0.13545408844947815
train_iter_loss: 0.2686940133571625
train_iter_loss: 0.0923963263630867
train_iter_loss: 0.18633635342121124
train_iter_loss: 0.13893596827983856
train_iter_loss: 0.21211887896060944
train_iter_loss: 0.219185009598732
train_iter_loss: 0.21414396166801453
train_iter_loss: 0.18611805140972137
train_iter_loss: 0.1448974311351776
train_iter_loss: 0.15264447033405304
train_iter_loss: 0.181950181722641
train_iter_loss: 0.1747119277715683
train_iter_loss: 0.13758522272109985
train_iter_loss: 0.3376050889492035
train_iter_loss: 0.18588724732398987
train_iter_loss: 0.23130010068416595
train_iter_loss: 0.17486824095249176
train_iter_loss: 0.4147186279296875
train_iter_loss: 0.17128299176692963
train_iter_loss: 0.2425422966480255
train_iter_loss: 0.14091581106185913
train_iter_loss: 0.10973550379276276
train_iter_loss: 0.23431220650672913
train_iter_loss: 0.21189534664154053
train_iter_loss: 0.1908225268125534
train loss :0.2085
---------------------
Validation seg loss: 0.23375697209025328 at epoch 43
********************
best_val_epoch_loss:  0.23375697209025328
MODEL UPDATED
epoch =     44/  1000, exp = train
train_iter_loss: 0.18706835806369781
train_iter_loss: 0.16537164151668549
train_iter_loss: 0.21480542421340942
train_iter_loss: 0.21014991402626038
train_iter_loss: 0.18507607281208038
train_iter_loss: 0.2143561691045761
train_iter_loss: 0.23725678026676178
train_iter_loss: 0.2681497037410736
train_iter_loss: 0.1869853138923645
train_iter_loss: 0.1764840930700302
train_iter_loss: 0.22579585015773773
train_iter_loss: 0.2357291579246521
train_iter_loss: 0.1292591094970703
train_iter_loss: 0.22061845660209656
train_iter_loss: 0.2192503958940506
train_iter_loss: 0.23401829600334167
train_iter_loss: 0.24578902125358582
train_iter_loss: 0.3153982162475586
train_iter_loss: 0.0784752368927002
train_iter_loss: 0.094789519906044
train_iter_loss: 0.1556173861026764
train_iter_loss: 0.13956624269485474
train_iter_loss: 0.15440227091312408
train_iter_loss: 0.173993781208992
train_iter_loss: 0.3098827004432678
train_iter_loss: 0.21256837248802185
train_iter_loss: 0.19988037645816803
train_iter_loss: 0.21181443333625793
train_iter_loss: 0.2770298719406128
train_iter_loss: 0.2062017321586609
train_iter_loss: 0.2602947950363159
train_iter_loss: 0.14024454355239868
train_iter_loss: 0.17243410646915436
train_iter_loss: 0.113054558634758
train_iter_loss: 0.12547820806503296
train_iter_loss: 0.2029220014810562
train_iter_loss: 0.26234859228134155
train_iter_loss: 0.25218984484672546
train_iter_loss: 0.09780894219875336
train_iter_loss: 0.19689390063285828
train_iter_loss: 0.16650676727294922
train_iter_loss: 0.3379632830619812
train_iter_loss: 0.18599845468997955
train_iter_loss: 0.41054773330688477
train_iter_loss: 0.2055657058954239
train_iter_loss: 0.15265291929244995
train_iter_loss: 0.21011464297771454
train_iter_loss: 0.13088960945606232
train_iter_loss: 0.2235289067029953
train_iter_loss: 0.2292444407939911
train_iter_loss: 0.19565647840499878
train_iter_loss: 0.20953507721424103
train_iter_loss: 0.10591921955347061
train_iter_loss: 0.1525564193725586
train_iter_loss: 0.20525594055652618
train_iter_loss: 0.10258571803569794
train_iter_loss: 0.2717205286026001
train_iter_loss: 0.16934208571910858
train_iter_loss: 0.12171211838722229
train_iter_loss: 0.1796901971101761
train_iter_loss: 0.22105705738067627
train_iter_loss: 0.2497558295726776
train_iter_loss: 0.17696760594844818
train_iter_loss: 0.36467283964157104
train_iter_loss: 0.22090275585651398
train_iter_loss: 0.24427862465381622
train_iter_loss: 0.21653425693511963
train_iter_loss: 0.19818115234375
train_iter_loss: 0.16569219529628754
train_iter_loss: 0.19289237260818481
train_iter_loss: 0.13732297718524933
train_iter_loss: 0.20401090383529663
train_iter_loss: 0.14205402135849
train_iter_loss: 0.17063944041728973
train_iter_loss: 0.25464749336242676
train_iter_loss: 0.14199115335941315
train_iter_loss: 0.19922788441181183
train_iter_loss: 0.13883253931999207
train_iter_loss: 0.26850050687789917
train_iter_loss: 0.2579237222671509
train_iter_loss: 0.24898013472557068
train_iter_loss: 0.23949222266674042
train_iter_loss: 0.15435156226158142
train_iter_loss: 0.24064292013645172
train_iter_loss: 0.22821207344532013
train_iter_loss: 0.1816299706697464
train_iter_loss: 0.1764838695526123
train_iter_loss: 0.1495121866464615
train_iter_loss: 0.1907765120267868
train_iter_loss: 0.1940845251083374
train_iter_loss: 0.21490724384784698
train_iter_loss: 0.1336652636528015
train_iter_loss: 0.2595060169696808
train_iter_loss: 0.2658381462097168
train_iter_loss: 0.24383366107940674
train_iter_loss: 0.16389131546020508
train_iter_loss: 0.3408842980861664
train_iter_loss: 0.22641342878341675
train_iter_loss: 0.1445877104997635
train_iter_loss: 0.175278440117836
train loss :0.2042
---------------------
Validation seg loss: 0.2357534141683916 at epoch 44
epoch =     45/  1000, exp = train
train_iter_loss: 0.1753573715686798
train_iter_loss: 0.2853233814239502
train_iter_loss: 0.1982811838388443
train_iter_loss: 0.1376602053642273
train_iter_loss: 0.23118701577186584
train_iter_loss: 0.32992780208587646
train_iter_loss: 0.21855394542217255
train_iter_loss: 0.27531805634498596
train_iter_loss: 0.13460272550582886
train_iter_loss: 0.21990862488746643
train_iter_loss: 0.16347412765026093
train_iter_loss: 0.21822094917297363
train_iter_loss: 0.12103191018104553
train_iter_loss: 0.15637846291065216
train_iter_loss: 0.16756059229373932
train_iter_loss: 0.12272070348262787
train_iter_loss: 0.15377631783485413
train_iter_loss: 0.2342270016670227
train_iter_loss: 0.15589112043380737
train_iter_loss: 0.15770664811134338
train_iter_loss: 0.14176858961582184
train_iter_loss: 0.24656152725219727
train_iter_loss: 0.18112987279891968
train_iter_loss: 0.09991955757141113
train_iter_loss: 0.126261368393898
train_iter_loss: 0.17657412588596344
train_iter_loss: 0.22121880948543549
train_iter_loss: 0.22360217571258545
train_iter_loss: 0.18422996997833252
train_iter_loss: 0.1854436695575714
train_iter_loss: 0.20519714057445526
train_iter_loss: 0.20680484175682068
train_iter_loss: 0.14887404441833496
train_iter_loss: 0.1820216029882431
train_iter_loss: 0.20637917518615723
train_iter_loss: 0.17202922701835632
train_iter_loss: 0.1880398839712143
train_iter_loss: 0.1922154724597931
train_iter_loss: 0.152430921792984
train_iter_loss: 0.1976490467786789
train_iter_loss: 0.20733384788036346
train_iter_loss: 0.18468652665615082
train_iter_loss: 0.22886411845684052
train_iter_loss: 0.1597781926393509
train_iter_loss: 0.2437816709280014
train_iter_loss: 0.31739822030067444
train_iter_loss: 0.1278548687696457
train_iter_loss: 0.29254046082496643
train_iter_loss: 0.2683006823062897
train_iter_loss: 0.09394411742687225
train_iter_loss: 0.22264614701271057
train_iter_loss: 0.2817983329296112
train_iter_loss: 0.14716455340385437
train_iter_loss: 0.19795328378677368
train_iter_loss: 0.3518601953983307
train_iter_loss: 0.19627992808818817
train_iter_loss: 0.13720740377902985
train_iter_loss: 0.2170131504535675
train_iter_loss: 0.1737794429063797
train_iter_loss: 0.2344750314950943
train_iter_loss: 0.13411228358745575
train_iter_loss: 0.2838691771030426
train_iter_loss: 0.18637768924236298
train_iter_loss: 0.1275283694267273
train_iter_loss: 0.2269429862499237
train_iter_loss: 0.19171546399593353
train_iter_loss: 0.15544502437114716
train_iter_loss: 0.20673885941505432
train_iter_loss: 0.17792561650276184
train_iter_loss: 0.19097185134887695
train_iter_loss: 0.21430252492427826
train_iter_loss: 0.15799826383590698
train_iter_loss: 0.2647806406021118
train_iter_loss: 0.212929368019104
train_iter_loss: 0.20954321324825287
train_iter_loss: 0.2154097706079483
train_iter_loss: 0.10667751729488373
train_iter_loss: 0.20606206357479095
train_iter_loss: 0.18038152158260345
train_iter_loss: 0.24453800916671753
train_iter_loss: 0.16927924752235413
train_iter_loss: 0.2463248372077942
train_iter_loss: 0.3475285768508911
train_iter_loss: 0.17534896731376648
train_iter_loss: 0.2142900824546814
train_iter_loss: 0.19128084182739258
train_iter_loss: 0.14751611649990082
train_iter_loss: 0.20049510896205902
train_iter_loss: 0.1377255916595459
train_iter_loss: 0.18415528535842896
train_iter_loss: 0.18282873928546906
train_iter_loss: 0.2172878235578537
train_iter_loss: 0.09483447670936584
train_iter_loss: 0.22840510308742523
train_iter_loss: 0.17255523800849915
train_iter_loss: 0.25117775797843933
train_iter_loss: 0.20106013119220734
train_iter_loss: 0.24158385396003723
train_iter_loss: 0.10492071509361267
train_iter_loss: 0.20840111374855042
train loss :0.1980
---------------------
Validation seg loss: 0.2327407156732285 at epoch 45
********************
best_val_epoch_loss:  0.2327407156732285
MODEL UPDATED
epoch =     46/  1000, exp = train
train_iter_loss: 0.19565774500370026
train_iter_loss: 0.15718400478363037
train_iter_loss: 0.26739954948425293
train_iter_loss: 0.11946988105773926
train_iter_loss: 0.13416647911071777
train_iter_loss: 0.12916700541973114
train_iter_loss: 0.11486120522022247
train_iter_loss: 0.21685965359210968
train_iter_loss: 0.22510161995887756
train_iter_loss: 0.2613235414028168
train_iter_loss: 0.112498939037323
train_iter_loss: 0.3177184760570526
train_iter_loss: 0.14498496055603027
train_iter_loss: 0.19968020915985107
train_iter_loss: 0.2937701940536499
train_iter_loss: 0.1764676421880722
train_iter_loss: 0.1629491150379181
train_iter_loss: 0.2984737753868103
train_iter_loss: 0.11358267813920975
train_iter_loss: 0.1922675222158432
train_iter_loss: 0.1618400365114212
train_iter_loss: 0.07167845219373703
train_iter_loss: 0.11762990057468414
train_iter_loss: 0.31023770570755005
train_iter_loss: 0.4522971510887146
train_iter_loss: 0.16024571657180786
train_iter_loss: 0.23452331125736237
train_iter_loss: 0.20670683681964874
train_iter_loss: 0.3476468324661255
train_iter_loss: 0.19960197806358337
train_iter_loss: 0.20804394781589508
train_iter_loss: 0.13787530362606049
train_iter_loss: 0.18569011986255646
train_iter_loss: 0.13389305770397186
train_iter_loss: 0.11093371361494064
train_iter_loss: 0.20014162361621857
train_iter_loss: 0.3726101219654083
train_iter_loss: 0.2238776683807373
train_iter_loss: 0.27513232827186584
train_iter_loss: 0.19439178705215454
train_iter_loss: 0.15122994780540466
train_iter_loss: 0.1523880958557129
train_iter_loss: 0.18102408945560455
train_iter_loss: 0.2743346393108368
train_iter_loss: 0.17299193143844604
train_iter_loss: 0.27373719215393066
train_iter_loss: 0.16596616804599762
train_iter_loss: 0.226606085896492
train_iter_loss: 0.21568803489208221
train_iter_loss: 0.1861211210489273
train_iter_loss: 0.19696788489818573
train_iter_loss: 0.2521486282348633
train_iter_loss: 0.1404222846031189
train_iter_loss: 0.17983250319957733
train_iter_loss: 0.16692189872264862
train_iter_loss: 0.19503934681415558
train_iter_loss: 0.22661630809307098
train_iter_loss: 0.1759171485900879
train_iter_loss: 0.39900752902030945
train_iter_loss: 0.16333425045013428
train_iter_loss: 0.19041545689105988
train_iter_loss: 0.2768772542476654
train_iter_loss: 0.2930120527744293
train_iter_loss: 0.17512275278568268
train_iter_loss: 0.1278572380542755
train_iter_loss: 0.2408035695552826
train_iter_loss: 0.20478814840316772
train_iter_loss: 0.1579951047897339
train_iter_loss: 0.1626402586698532
train_iter_loss: 0.23584488034248352
train_iter_loss: 0.18981558084487915
train_iter_loss: 0.17720845341682434
train_iter_loss: 0.2728569209575653
train_iter_loss: 0.1787458062171936
train_iter_loss: 0.14235205948352814
train_iter_loss: 0.16023355722427368
train_iter_loss: 0.23501792550086975
train_iter_loss: 0.23170234262943268
train_iter_loss: 0.22809003293514252
train_iter_loss: 0.18737515807151794
train_iter_loss: 0.22154374420642853
train_iter_loss: 0.1707056164741516
train_iter_loss: 0.24165460467338562
train_iter_loss: 0.23441557586193085
train_iter_loss: 0.19360628724098206
train_iter_loss: 0.13685393333435059
train_iter_loss: 0.2557404935359955
train_iter_loss: 0.08716083317995071
train_iter_loss: 0.20943765342235565
train_iter_loss: 0.24732156097888947
train_iter_loss: 0.22889910638332367
train_iter_loss: 0.16160108149051666
train_iter_loss: 0.20552951097488403
train_iter_loss: 0.12826819717884064
train_iter_loss: 0.16463376581668854
train_iter_loss: 0.21197442710399628
train_iter_loss: 0.21812371909618378
train_iter_loss: 0.15227782726287842
train_iter_loss: 0.20625199377536774
train_iter_loss: 0.15479861199855804
train loss :0.2033
---------------------
Validation seg loss: 0.23740789327629894 at epoch 46
epoch =     47/  1000, exp = train
train_iter_loss: 0.3171622157096863
train_iter_loss: 0.2402530461549759
train_iter_loss: 0.21337634325027466
train_iter_loss: 0.14087677001953125
train_iter_loss: 0.1289803385734558
train_iter_loss: 0.1600838601589203
train_iter_loss: 0.2875191271305084
train_iter_loss: 0.26270824670791626
train_iter_loss: 0.17100268602371216
train_iter_loss: 0.15664656460285187
train_iter_loss: 0.2631007730960846
train_iter_loss: 0.1458105593919754
train_iter_loss: 0.11619602888822556
train_iter_loss: 0.3681996762752533
train_iter_loss: 0.1520126461982727
train_iter_loss: 0.29489418864250183
train_iter_loss: 0.17329879105091095
train_iter_loss: 0.11131623387336731
train_iter_loss: 0.14658302068710327
train_iter_loss: 0.17214371263980865
train_iter_loss: 0.13608694076538086
train_iter_loss: 0.07084820419549942
train_iter_loss: 0.24035625159740448
train_iter_loss: 0.18566440045833588
train_iter_loss: 0.2662896513938904
train_iter_loss: 0.14128896594047546
train_iter_loss: 0.2722207009792328
train_iter_loss: 0.1523457169532776
train_iter_loss: 0.12015779316425323
train_iter_loss: 0.2380724996328354
train_iter_loss: 0.3086177408695221
train_iter_loss: 0.23567485809326172
train_iter_loss: 0.25893354415893555
train_iter_loss: 0.24110063910484314
train_iter_loss: 0.21550242602825165
train_iter_loss: 0.2608736455440521
train_iter_loss: 0.26086997985839844
train_iter_loss: 0.3346581757068634
train_iter_loss: 0.22139130532741547
train_iter_loss: 0.09666413068771362
train_iter_loss: 0.23177796602249146
train_iter_loss: 0.35812920331954956
train_iter_loss: 0.11820172518491745
train_iter_loss: 0.19946005940437317
train_iter_loss: 0.11879484355449677
train_iter_loss: 0.16768878698349
train_iter_loss: 0.20172880589962006
train_iter_loss: 0.12203795462846756
train_iter_loss: 0.1917644441127777
train_iter_loss: 0.22961120307445526
train_iter_loss: 0.3269777297973633
train_iter_loss: 0.29555878043174744
train_iter_loss: 0.17969100177288055
train_iter_loss: 0.1893583983182907
train_iter_loss: 0.21650736033916473
train_iter_loss: 0.3747130036354065
train_iter_loss: 0.2596934139728546
train_iter_loss: 0.24156703054904938
train_iter_loss: 0.17926125228405
train_iter_loss: 0.3167305290699005
train_iter_loss: 0.23737789690494537
train_iter_loss: 0.14517706632614136
train_iter_loss: 0.15271230041980743
train_iter_loss: 0.13126224279403687
train_iter_loss: 0.3322790265083313
train_iter_loss: 0.28186914324760437
train_iter_loss: 0.18222576379776
train_iter_loss: 0.1925102025270462
train_iter_loss: 0.18540766835212708
train_iter_loss: 0.1356787085533142
train_iter_loss: 0.15098455548286438
train_iter_loss: 0.18771855533123016
train_iter_loss: 0.17255647480487823
train_iter_loss: 0.09044066071510315
train_iter_loss: 0.21952016651630402
train_iter_loss: 0.193946972489357
train_iter_loss: 0.18056637048721313
train_iter_loss: 0.5546829700469971
train_iter_loss: 0.22663156688213348
train_iter_loss: 0.26412567496299744
train_iter_loss: 0.12775462865829468
train_iter_loss: 0.15473340451717377
train_iter_loss: 0.16791650652885437
train_iter_loss: 0.20303061604499817
train_iter_loss: 0.16984350979328156
train_iter_loss: 0.1699434369802475
train_iter_loss: 0.14144517481327057
train_iter_loss: 0.2653210759162903
train_iter_loss: 0.18856942653656006
train_iter_loss: 0.1725829392671585
train_iter_loss: 0.1559474915266037
train_iter_loss: 0.2444533258676529
train_iter_loss: 0.11644411087036133
train_iter_loss: 0.2786298394203186
train_iter_loss: 0.2516838610172272
train_iter_loss: 0.2073037326335907
train_iter_loss: 0.16122303903102875
train_iter_loss: 0.15624433755874634
train_iter_loss: 0.18063238263130188
train_iter_loss: 0.14909499883651733
train loss :0.2088
---------------------
Validation seg loss: 0.23407239572338337 at epoch 47
epoch =     48/  1000, exp = train
train_iter_loss: 0.12123048305511475
train_iter_loss: 0.19193153083324432
train_iter_loss: 0.197859525680542
train_iter_loss: 0.24311943352222443
train_iter_loss: 0.23820769786834717
train_iter_loss: 0.3087889552116394
train_iter_loss: 0.19348444044589996
train_iter_loss: 0.23534277081489563
train_iter_loss: 0.16567744314670563
train_iter_loss: 0.22301848232746124
train_iter_loss: 0.1376502364873886
train_iter_loss: 0.16681985557079315
train_iter_loss: 0.2949742078781128
train_iter_loss: 0.1734772026538849
train_iter_loss: 0.1982438564300537
train_iter_loss: 0.25104019045829773
train_iter_loss: 0.15103890001773834
train_iter_loss: 0.1859544962644577
train_iter_loss: 0.2365722358226776
train_iter_loss: 0.2184779793024063
train_iter_loss: 0.10773105919361115
train_iter_loss: 0.1530000865459442
train_iter_loss: 0.21019208431243896
train_iter_loss: 0.20344723761081696
train_iter_loss: 0.12554068863391876
train_iter_loss: 0.1819847822189331
train_iter_loss: 0.11348500102758408
train_iter_loss: 0.11786315590143204
train_iter_loss: 0.34401071071624756
train_iter_loss: 0.20951063930988312
train_iter_loss: 0.17946617305278778
train_iter_loss: 0.1337650865316391
train_iter_loss: 0.1954353153705597
train_iter_loss: 0.24603508412837982
train_iter_loss: 0.12058888375759125
train_iter_loss: 0.1908966451883316
train_iter_loss: 0.27580949664115906
train_iter_loss: 0.26485368609428406
train_iter_loss: 0.11621354520320892
train_iter_loss: 0.19889505207538605
train_iter_loss: 0.22456353902816772
train_iter_loss: 0.11670256406068802
train_iter_loss: 0.1729460209608078
train_iter_loss: 0.16034741699695587
train_iter_loss: 0.15771150588989258
train_iter_loss: 0.22896210849285126
train_iter_loss: 0.28622663021087646
train_iter_loss: 0.10269282013177872
train_iter_loss: 0.18464316427707672
train_iter_loss: 0.1557268500328064
train_iter_loss: 0.16595041751861572
train_iter_loss: 0.11262117326259613
train_iter_loss: 0.1471492201089859
train_iter_loss: 0.2676689326763153
train_iter_loss: 0.28722983598709106
train_iter_loss: 0.2076970785856247
train_iter_loss: 0.14348825812339783
train_iter_loss: 0.1476580947637558
train_iter_loss: 0.30220603942871094
train_iter_loss: 0.23365359008312225
train_iter_loss: 0.18703733384609222
train_iter_loss: 0.26219725608825684
train_iter_loss: 0.2590551972389221
train_iter_loss: 0.14829620718955994
train_iter_loss: 0.2410120815038681
train_iter_loss: 0.19223745167255402
train_iter_loss: 0.2647187411785126
train_iter_loss: 0.20869110524654388
train_iter_loss: 0.17014864087104797
train_iter_loss: 0.3723832964897156
train_iter_loss: 0.2590366005897522
train_iter_loss: 0.216090127825737
train_iter_loss: 0.17748847603797913
train_iter_loss: 0.24082884192466736
train_iter_loss: 0.2721695601940155
train_iter_loss: 0.18342852592468262
train_iter_loss: 0.10977496951818466
train_iter_loss: 0.4231416583061218
train_iter_loss: 0.10373108088970184
train_iter_loss: 0.19763295352458954
train_iter_loss: 0.18615958094596863
train_iter_loss: 0.11504079401493073
train_iter_loss: 0.17847201228141785
train_iter_loss: 0.18624797463417053
train_iter_loss: 0.18693500757217407
train_iter_loss: 0.21248416602611542
train_iter_loss: 0.09054693579673767
train_iter_loss: 0.17695459723472595
train_iter_loss: 0.17629772424697876
train_iter_loss: 0.18694786727428436
train_iter_loss: 0.24122220277786255
train_iter_loss: 0.16713938117027283
train_iter_loss: 0.1260012835264206
train_iter_loss: 0.17384293675422668
train_iter_loss: 0.25606831908226013
train_iter_loss: 0.5433635711669922
train_iter_loss: 0.20875993371009827
train_iter_loss: 0.17850720882415771
train_iter_loss: 0.15733252465724945
train_iter_loss: 0.14488783478736877
train loss :0.2022
---------------------
Validation seg loss: 0.23311373248766615 at epoch 48
epoch =     49/  1000, exp = train
train_iter_loss: 0.2166443169116974
train_iter_loss: 0.1670709103345871
train_iter_loss: 0.2185438871383667
train_iter_loss: 0.18752533197402954
train_iter_loss: 0.22047406435012817
train_iter_loss: 0.23251433670520782
train_iter_loss: 0.22940823435783386
train_iter_loss: 0.19140763580799103
train_iter_loss: 0.24086838960647583
train_iter_loss: 0.25747254490852356
train_iter_loss: 0.1709611713886261
train_iter_loss: 0.09824832528829575
train_iter_loss: 0.15027984976768494
train_iter_loss: 0.23431652784347534
train_iter_loss: 0.21880391240119934
train_iter_loss: 0.16804830729961395
train_iter_loss: 0.16064539551734924
train_iter_loss: 0.14374150335788727
train_iter_loss: 0.1910039335489273
train_iter_loss: 0.09460272639989853
train_iter_loss: 0.2028914839029312
train_iter_loss: 0.15277613699436188
train_iter_loss: 0.1686670482158661
train_iter_loss: 0.19875282049179077
train_iter_loss: 0.11622852087020874
train_iter_loss: 0.3073325753211975
train_iter_loss: 0.20026424527168274
train_iter_loss: 0.08697900921106339
train_iter_loss: 0.26263126730918884
train_iter_loss: 0.2341989427804947
train_iter_loss: 0.22549986839294434
train_iter_loss: 0.1445111632347107
train_iter_loss: 0.28951042890548706
train_iter_loss: 0.16256490349769592
train_iter_loss: 0.17134620249271393
train_iter_loss: 0.11613845080137253
train_iter_loss: 0.27563589811325073
train_iter_loss: 0.29120898246765137
train_iter_loss: 0.22524775564670563
train_iter_loss: 0.2224515825510025
train_iter_loss: 0.1478235274553299
train_iter_loss: 0.11933375149965286
train_iter_loss: 0.30267563462257385
train_iter_loss: 0.2467261701822281
train_iter_loss: 0.2867993712425232
train_iter_loss: 0.23491230607032776
train_iter_loss: 0.3595140874385834
train_iter_loss: 0.18222211301326752
train_iter_loss: 0.18125317990779877
train_iter_loss: 0.10995861142873764
train_iter_loss: 0.17665956914424896
train_iter_loss: 0.21955332159996033
train_iter_loss: 0.26579633355140686
train_iter_loss: 0.5516106486320496
train_iter_loss: 0.18120282888412476
train_iter_loss: 0.14924220740795135
train_iter_loss: 0.2524915039539337
train_iter_loss: 0.17580421268939972
train_iter_loss: 0.16722168028354645
train_iter_loss: 0.21356438100337982
train_iter_loss: 0.13259027898311615
train_iter_loss: 0.30786168575286865
train_iter_loss: 0.15929202735424042
train_iter_loss: 0.2504947781562805
train_iter_loss: 0.2280913144350052
train_iter_loss: 0.2275204211473465
train_iter_loss: 0.17179086804389954
train_iter_loss: 0.27277809381484985
train_iter_loss: 0.17638075351715088
train_iter_loss: 0.16260598599910736
train_iter_loss: 0.15596453845500946
train_iter_loss: 0.18372538685798645
train_iter_loss: 0.18236999213695526
train_iter_loss: 0.25571054220199585
train_iter_loss: 0.12314312905073166
train_iter_loss: 0.1810082495212555
train_iter_loss: 0.1149715781211853
train_iter_loss: 0.29603564739227295
train_iter_loss: 0.11437749862670898
train_iter_loss: 0.23669156432151794
train_iter_loss: 0.10569391399621964
train_iter_loss: 0.2066020667552948
train_iter_loss: 0.17277397215366364
train_iter_loss: 0.17070765793323517
train_iter_loss: 0.12738898396492004
train_iter_loss: 0.17731209099292755
train_iter_loss: 0.21037869155406952
train_iter_loss: 0.20813798904418945
train_iter_loss: 0.2784830331802368
train_iter_loss: 0.17023393511772156
train_iter_loss: 0.0705253854393959
train_iter_loss: 0.1702546775341034
train_iter_loss: 0.24779567122459412
train_iter_loss: 0.19293972849845886
train_iter_loss: 0.2393248826265335
train_iter_loss: 0.12664707005023956
train_iter_loss: 0.15902970731258392
train_iter_loss: 0.2928568720817566
train_iter_loss: 0.18728221952915192
train_iter_loss: 0.24936993420124054
train loss :0.2027
---------------------
Validation seg loss: 0.23343123453405668 at epoch 49
epoch =     50/  1000, exp = train
train_iter_loss: 0.3092716336250305
train_iter_loss: 0.13801679015159607
train_iter_loss: 0.2301461398601532
train_iter_loss: 0.2234610766172409
train_iter_loss: 0.17909669876098633
train_iter_loss: 0.21910372376441956
train_iter_loss: 0.269051730632782
train_iter_loss: 0.21272407472133636
train_iter_loss: 0.11184637248516083
train_iter_loss: 0.1554895043373108
train_iter_loss: 0.13553617894649506
train_iter_loss: 0.1074691042304039
train_iter_loss: 0.15764173865318298
train_iter_loss: 0.17224767804145813
train_iter_loss: 0.192640483379364
train_iter_loss: 0.2408556342124939
train_iter_loss: 0.15438947081565857
train_iter_loss: 0.19101017713546753
train_iter_loss: 0.19836142659187317
train_iter_loss: 0.2289976328611374
train_iter_loss: 0.12843725085258484
train_iter_loss: 0.26112911105155945
train_iter_loss: 0.1734364777803421
train_iter_loss: 0.23339666426181793
train_iter_loss: 0.16914507746696472
train_iter_loss: 0.16400842368602753
train_iter_loss: 0.17682527005672455
train_iter_loss: 0.09889746457338333
train_iter_loss: 0.2509157061576843
train_iter_loss: 0.22413891553878784
train_iter_loss: 0.11656621843576431
train_iter_loss: 0.24158695340156555
train_iter_loss: 0.22103041410446167
train_iter_loss: 0.16080480813980103
train_iter_loss: 0.29974305629730225
train_iter_loss: 0.23103997111320496
train_iter_loss: 0.15791168808937073
train_iter_loss: 0.1755315661430359
train_iter_loss: 0.2592575252056122
train_iter_loss: 0.23038282990455627
train_iter_loss: 0.17275865375995636
train_iter_loss: 0.12853077054023743
train_iter_loss: 0.09595119208097458
train_iter_loss: 0.18587613105773926
train_iter_loss: 0.17726346850395203
train_iter_loss: 0.175765261054039
train_iter_loss: 0.22988958656787872
train_iter_loss: 0.11191454529762268
train_iter_loss: 0.3108118772506714
train_iter_loss: 0.21942584216594696
train_iter_loss: 0.2614535093307495
train_iter_loss: 0.29190823435783386
train_iter_loss: 0.1409197449684143
train_iter_loss: 0.1498255878686905
train_iter_loss: 0.17867326736450195
train_iter_loss: 0.23498044908046722
train_iter_loss: 0.22537373006343842
train_iter_loss: 0.210942342877388
train_iter_loss: 0.19915889203548431
train_iter_loss: 0.1927717924118042
train_iter_loss: 0.1789768636226654
train_iter_loss: 0.20295056700706482
train_iter_loss: 0.20850533246994019
train_iter_loss: 0.20669575035572052
train_iter_loss: 0.25771936774253845
train_iter_loss: 0.10777657479047775
train_iter_loss: 0.19954878091812134
train_iter_loss: 0.14502395689487457
train_iter_loss: 0.18442994356155396
train_iter_loss: 0.21585743129253387
train_iter_loss: 0.1839035153388977
train_iter_loss: 0.21029087901115417
train_iter_loss: 0.15731628239154816
train_iter_loss: 0.17959627509117126
train_iter_loss: 0.20914269983768463
train_iter_loss: 0.34641242027282715
train_iter_loss: 0.1269925832748413
train_iter_loss: 0.12191734462976456
train_iter_loss: 0.14804857969284058
train_iter_loss: 0.14204493165016174
train_iter_loss: 0.19402310252189636
train_iter_loss: 0.2597355544567108
train_iter_loss: 0.1951950043439865
train_iter_loss: 0.18836311995983124
train_iter_loss: 0.27944615483283997
train_iter_loss: 0.26739317178726196
train_iter_loss: 0.15994028747081757
train_iter_loss: 0.17069050669670105
train_iter_loss: 0.2573857009410858
train_iter_loss: 0.20269908010959625
train_iter_loss: 0.3405452370643616
train_iter_loss: 0.13773100078105927
train_iter_loss: 0.19229310750961304
train_iter_loss: 0.1732095330953598
train_iter_loss: 0.15233995020389557
train_iter_loss: 0.22406329214572906
train_iter_loss: 0.31318357586860657
train_iter_loss: 0.1404699981212616
train_iter_loss: 0.1406351774930954
train_iter_loss: 0.2135491669178009
train loss :0.1983
---------------------
Validation seg loss: 0.2342552614732171 at epoch 50
epoch =     51/  1000, exp = train
train_iter_loss: 0.08796776086091995
train_iter_loss: 0.20790979266166687
train_iter_loss: 0.197723850607872
train_iter_loss: 0.20105667412281036
train_iter_loss: 0.2295101433992386
train_iter_loss: 0.20565170049667358
train_iter_loss: 0.26295140385627747
train_iter_loss: 0.13484539091587067
train_iter_loss: 0.34033238887786865
train_iter_loss: 0.16345703601837158
train_iter_loss: 0.13335126638412476
train_iter_loss: 0.14904038608074188
train_iter_loss: 0.3106289803981781
train_iter_loss: 0.20715975761413574
train_iter_loss: 0.2720493674278259
train_iter_loss: 0.27776584029197693
train_iter_loss: 0.062009137123823166
train_iter_loss: 0.14164941012859344
train_iter_loss: 0.1773436963558197
train_iter_loss: 0.14805801212787628
train_iter_loss: 0.09926614165306091
train_iter_loss: 0.10582514107227325
train_iter_loss: 0.16977033019065857
train_iter_loss: 0.23760534822940826
train_iter_loss: 0.15585559606552124
train_iter_loss: 0.1395185887813568
train_iter_loss: 0.20024965703487396
train_iter_loss: 0.2331361323595047
train_iter_loss: 0.16331037878990173
train_iter_loss: 0.2149820774793625
train_iter_loss: 0.28108271956443787
train_iter_loss: 0.27784785628318787
train_iter_loss: 0.30163827538490295
train_iter_loss: 0.17411257326602936
train_iter_loss: 0.1767842322587967
train_iter_loss: 0.1378220170736313
train_iter_loss: 0.22263473272323608
train_iter_loss: 0.1921519637107849
train_iter_loss: 0.23149672150611877
train_iter_loss: 0.19052307307720184
train_iter_loss: 0.17256899178028107
train_iter_loss: 0.13030081987380981
train_iter_loss: 0.18531927466392517
train_iter_loss: 0.0475848950445652
train_iter_loss: 0.13247010111808777
train_iter_loss: 0.1777592897415161
train_iter_loss: 0.16489847004413605
train_iter_loss: 0.2443048655986786
train_iter_loss: 0.17230574786663055
train_iter_loss: 0.1990765929222107
train_iter_loss: 0.12472309917211533
train_iter_loss: 0.2551508843898773
train_iter_loss: 0.14866191148757935
train_iter_loss: 0.16600559651851654
train_iter_loss: 0.3018874526023865
train_iter_loss: 0.15895269811153412
train_iter_loss: 0.27487069368362427
train_iter_loss: 0.17075124382972717
train_iter_loss: 0.23498524725437164
train_iter_loss: 0.17461076378822327
train_iter_loss: 0.1738426834344864
train_iter_loss: 0.33061373233795166
train_iter_loss: 0.1795428842306137
train_iter_loss: 0.20585250854492188
train_iter_loss: 0.353749543428421
train_iter_loss: 0.27416545152664185
train_iter_loss: 0.3087499737739563
train_iter_loss: 0.12991823256015778
train_iter_loss: 0.1981876939535141
train_iter_loss: 0.2086765170097351
train_iter_loss: 0.3406168222427368
train_iter_loss: 0.17564323544502258
train_iter_loss: 0.2820945382118225
train_iter_loss: 0.24407194554805756
train_iter_loss: 0.18818874657154083
train_iter_loss: 0.19715727865695953
train_iter_loss: 0.18176844716072083
train_iter_loss: 0.21149899065494537
train_iter_loss: 0.22227904200553894
train_iter_loss: 0.18893907964229584
train_iter_loss: 0.18037287890911102
train_iter_loss: 0.12332765012979507
train_iter_loss: 0.24194002151489258
train_iter_loss: 0.15710510313510895
train_iter_loss: 0.25597479939460754
train_iter_loss: 0.19997459650039673
train_iter_loss: 0.20862053334712982
train_iter_loss: 0.1036786288022995
train_iter_loss: 0.18164333701133728
train_iter_loss: 0.21293246746063232
train_iter_loss: 0.26636630296707153
train_iter_loss: 0.13607953488826752
train_iter_loss: 0.2836816906929016
train_iter_loss: 0.19083943963050842
train_iter_loss: 0.17655357718467712
train_iter_loss: 0.06529933959245682
train_iter_loss: 0.1648441106081009
train_iter_loss: 0.17153912782669067
train_iter_loss: 0.2047877311706543
train_iter_loss: 0.18838201463222504
train loss :0.1996
---------------------
Validation seg loss: 0.2341387311594104 at epoch 51
epoch =     52/  1000, exp = train
train_iter_loss: 0.19380716979503632
train_iter_loss: 0.2259005308151245
train_iter_loss: 0.128720223903656
train_iter_loss: 0.16640298068523407
train_iter_loss: 0.12225249409675598
train_iter_loss: 0.20761121809482574
train_iter_loss: 0.22210653126239777
train_iter_loss: 0.16278818249702454
train_iter_loss: 0.2372276932001114
train_iter_loss: 0.18151722848415375
train_iter_loss: 0.34143152832984924
train_iter_loss: 0.22207222878932953
train_iter_loss: 0.11448764055967331
train_iter_loss: 0.11088061332702637
train_iter_loss: 0.15449801087379456
train_iter_loss: 0.16564619541168213
train_iter_loss: 0.10348856449127197
train_iter_loss: 0.24967412650585175
train_iter_loss: 0.13879476487636566
train_iter_loss: 0.2314116209745407
train_iter_loss: 0.18393249809741974
train_iter_loss: 0.1522008329629898
train_iter_loss: 0.20575977861881256
train_iter_loss: 0.16131453216075897
train_iter_loss: 0.20401211082935333
train_iter_loss: 0.19591136276721954
train_iter_loss: 0.15830400586128235
train_iter_loss: 0.09322149306535721
train_iter_loss: 0.25527846813201904
train_iter_loss: 0.13562744855880737
train_iter_loss: 0.1479925513267517
train_iter_loss: 0.14608755707740784
train_iter_loss: 0.2622526288032532
train_iter_loss: 0.1086907759308815
train_iter_loss: 0.1562027931213379
train_iter_loss: 0.19233039021492004
train_iter_loss: 0.19657768309116364
train_iter_loss: 0.27895674109458923
train_iter_loss: 0.18701890110969543
train_iter_loss: 0.15046603977680206
train_iter_loss: 0.2390165776014328
train_iter_loss: 0.09314748644828796
train_iter_loss: 0.26292428374290466
train_iter_loss: 0.22385714948177338
train_iter_loss: 0.13874393701553345
train_iter_loss: 0.22463688254356384
train_iter_loss: 0.24166764318943024
train_iter_loss: 0.18478694558143616
train_iter_loss: 0.24201805889606476
train_iter_loss: 0.1711132824420929
train_iter_loss: 0.23959970474243164
train_iter_loss: 0.15994048118591309
train_iter_loss: 0.14317235350608826
train_iter_loss: 0.22161032259464264
train_iter_loss: 0.21294225752353668
train_iter_loss: 0.18112657964229584
train_iter_loss: 0.26020747423171997
train_iter_loss: 0.2296620011329651
train_iter_loss: 0.22527436912059784
train_iter_loss: 0.2228849083185196
train_iter_loss: 0.1541360467672348
train_iter_loss: 0.14124031364917755
train_iter_loss: 0.1488119512796402
train_iter_loss: 0.16626480221748352
train_iter_loss: 0.2853727340698242
train_iter_loss: 0.2382386177778244
train_iter_loss: 0.16324135661125183
train_iter_loss: 0.1719382107257843
train_iter_loss: 0.2416316717863083
train_iter_loss: 0.13347135484218597
train_iter_loss: 0.15840809047222137
train_iter_loss: 0.11552078276872635
train_iter_loss: 0.2412804663181305
train_iter_loss: 0.22249186038970947
train_iter_loss: 0.21843115985393524
train_iter_loss: 0.17953555285930634
train_iter_loss: 0.11752981692552567
train_iter_loss: 0.183732271194458
train_iter_loss: 0.20775291323661804
train_iter_loss: 0.13353832066059113
train_iter_loss: 0.17293120920658112
train_iter_loss: 0.25676751136779785
train_iter_loss: 0.25106579065322876
train_iter_loss: 0.23381038010120392
train_iter_loss: 0.2893676161766052
train_iter_loss: 0.22871485352516174
train_iter_loss: 0.1585908830165863
train_iter_loss: 0.20397944748401642
train_iter_loss: 0.2362731248140335
train_iter_loss: 0.2770497798919678
train_iter_loss: 0.16333600878715515
train_iter_loss: 0.21081775426864624
train_iter_loss: 0.1262843757867813
train_iter_loss: 0.23260439932346344
train_iter_loss: 0.11990808695554733
train_iter_loss: 0.1739044487476349
train_iter_loss: 0.13620150089263916
train_iter_loss: 0.20635968446731567
train_iter_loss: 0.2126067727804184
train_iter_loss: 0.23379100859165192
train loss :0.1932
---------------------
Validation seg loss: 0.23564265928459618 at epoch 52
epoch =     53/  1000, exp = train
train_iter_loss: 0.16958391666412354
train_iter_loss: 0.16103176772594452
train_iter_loss: 0.23279757797718048
train_iter_loss: 0.16369333863258362
train_iter_loss: 0.30083781480789185
train_iter_loss: 0.21455657482147217
train_iter_loss: 0.1097043976187706
train_iter_loss: 0.1722254455089569
train_iter_loss: 0.2272704839706421
train_iter_loss: 0.19983342289924622
train_iter_loss: 0.1488138735294342
train_iter_loss: 0.17652034759521484
train_iter_loss: 0.18608371913433075
train_iter_loss: 0.16988420486450195
train_iter_loss: 0.16810806095600128
train_iter_loss: 0.29875868558883667
train_iter_loss: 0.23278912901878357
train_iter_loss: 0.2321936935186386
train_iter_loss: 0.18082021176815033
train_iter_loss: 0.2460547834634781
train_iter_loss: 0.251325398683548
train_iter_loss: 0.25490322709083557
train_iter_loss: 0.18509089946746826
train_iter_loss: 0.1838267296552658
train_iter_loss: 0.2838716506958008
train_iter_loss: 0.22994397580623627
train_iter_loss: 0.16091154515743256
train_iter_loss: 0.1789698451757431
train_iter_loss: 0.2594975531101227
train_iter_loss: 0.14773805439472198
train_iter_loss: 0.12443535029888153
train_iter_loss: 0.19592660665512085
train_iter_loss: 0.10580367594957352
train_iter_loss: 0.20133532583713531
train_iter_loss: 0.17661646008491516
train_iter_loss: 0.2671545147895813
train_iter_loss: 0.10322611033916473
train_iter_loss: 0.17574350535869598
train_iter_loss: 0.1087070107460022
train_iter_loss: 0.2077757716178894
train_iter_loss: 0.2486392706632614
train_iter_loss: 0.2736073434352875
train_iter_loss: 0.15122078359127045
train_iter_loss: 0.1855926215648651
train_iter_loss: 0.14729180932044983
train_iter_loss: 0.22318261861801147
train_iter_loss: 0.05924344062805176
train_iter_loss: 0.09131663292646408
train_iter_loss: 0.1291714608669281
train_iter_loss: 0.21113704144954681
train_iter_loss: 0.17222502827644348
train_iter_loss: 0.13620391488075256
train_iter_loss: 0.19548675417900085
train_iter_loss: 0.1580117791891098
train_iter_loss: 0.20368587970733643
train_iter_loss: 0.3048345744609833
train_iter_loss: 0.16074663400650024
train_iter_loss: 0.22716177999973297
train_iter_loss: 0.20241139829158783
train_iter_loss: 0.17596828937530518
train_iter_loss: 0.12410373985767365
train_iter_loss: 0.15237286686897278
train_iter_loss: 0.16947083175182343
train_iter_loss: 0.2053166776895523
train_iter_loss: 0.1323053240776062
train_iter_loss: 0.28556615114212036
train_iter_loss: 0.2429766058921814
train_iter_loss: 0.19639737904071808
train_iter_loss: 0.16466011106967926
train_iter_loss: 0.14660444855690002
train_iter_loss: 0.144052654504776
train_iter_loss: 0.2072974443435669
train_iter_loss: 0.2666454315185547
train_iter_loss: 0.18480665981769562
train_iter_loss: 0.19095298647880554
train_iter_loss: 0.22295992076396942
train_iter_loss: 0.18045370280742645
train_iter_loss: 0.2747950553894043
train_iter_loss: 0.15651343762874603
train_iter_loss: 0.23794379830360413
train_iter_loss: 0.17460235953330994
train_iter_loss: 0.27979665994644165
train_iter_loss: 0.17285312712192535
train_iter_loss: 0.2852998971939087
train_iter_loss: 0.15278029441833496
train_iter_loss: 0.21386483311653137
train_iter_loss: 0.18412569165229797
train_iter_loss: 0.2683509290218353
train_iter_loss: 0.24992311000823975
train_iter_loss: 0.18369941413402557
train_iter_loss: 0.2586412727832794
train_iter_loss: 0.21712931990623474
train_iter_loss: 0.18837732076644897
train_iter_loss: 0.20687666535377502
train_iter_loss: 0.2398526519536972
train_iter_loss: 0.13321754336357117
train_iter_loss: 0.15838496387004852
train_iter_loss: 0.11223708093166351
train_iter_loss: 0.22185254096984863
train_iter_loss: 0.21868997812271118
train loss :0.1965
---------------------
Validation seg loss: 0.23069781831131791 at epoch 53
********************
best_val_epoch_loss:  0.23069781831131791
MODEL UPDATED
epoch =     54/  1000, exp = train
train_iter_loss: 0.1292615681886673
train_iter_loss: 0.2432994246482849
train_iter_loss: 0.11532287299633026
train_iter_loss: 0.21576054394245148
train_iter_loss: 0.28630101680755615
train_iter_loss: 0.19439642131328583
train_iter_loss: 0.2259422242641449
train_iter_loss: 0.12976227700710297
train_iter_loss: 0.1504313349723816
train_iter_loss: 0.27042442560195923
train_iter_loss: 0.12226533144712448
train_iter_loss: 0.21245412528514862
train_iter_loss: 0.2541687488555908
train_iter_loss: 0.2908276915550232
train_iter_loss: 0.2190382331609726
train_iter_loss: 0.15114836394786835
train_iter_loss: 0.19076292216777802
train_iter_loss: 0.23091483116149902
train_iter_loss: 0.17346657812595367
train_iter_loss: 0.12773758172988892
train_iter_loss: 0.19694288074970245
train_iter_loss: 0.3421863615512848
train_iter_loss: 0.12320296466350555
train_iter_loss: 0.2519261837005615
train_iter_loss: 0.22994959354400635
train_iter_loss: 0.19843098521232605
train_iter_loss: 0.18797419965267181
train_iter_loss: 0.1283101886510849
train_iter_loss: 0.14337994158267975
train_iter_loss: 0.14073359966278076
train_iter_loss: 0.20763440430164337
train_iter_loss: 0.04643011838197708
train_iter_loss: 0.19143953919410706
train_iter_loss: 0.3612760603427887
train_iter_loss: 0.3145012855529785
train_iter_loss: 0.1347847878932953
train_iter_loss: 0.25162217020988464
train_iter_loss: 0.08149348944425583
train_iter_loss: 0.19053053855895996
train_iter_loss: 0.27396076917648315
train_iter_loss: 0.24152474105358124
train_iter_loss: 0.26768600940704346
train_iter_loss: 0.12391036003828049
train_iter_loss: 0.24684259295463562
train_iter_loss: 0.25891876220703125
train_iter_loss: 0.1747124344110489
train_iter_loss: 0.20824599266052246
train_iter_loss: 0.2881404161453247
train_iter_loss: 0.22934989631175995
train_iter_loss: 0.2034962773323059
train_iter_loss: 0.22355571389198303
train_iter_loss: 0.2869066894054413
train_iter_loss: 0.1957324743270874
train_iter_loss: 0.11637800186872482
train_iter_loss: 0.25371211767196655
train_iter_loss: 0.07814843207597733
train_iter_loss: 0.11964575201272964
train_iter_loss: 0.2584690749645233
train_iter_loss: 0.20522263646125793
train_iter_loss: 0.23674558103084564
train_iter_loss: 0.17707206308841705
train_iter_loss: 0.24071066081523895
train_iter_loss: 0.24475392699241638
train_iter_loss: 0.13640345633029938
train_iter_loss: 0.1182459220290184
train_iter_loss: 0.11934836953878403
train_iter_loss: 0.12121573090553284
train_iter_loss: 0.16135159134864807
train_iter_loss: 0.08703883737325668
train_iter_loss: 0.15910585224628448
train_iter_loss: 0.15578339993953705
train_iter_loss: 0.16787073016166687
train_iter_loss: 0.21181638538837433
train_iter_loss: 0.2220405489206314
train_iter_loss: 0.37840595841407776
train_iter_loss: 0.11887921392917633
train_iter_loss: 0.25813350081443787
train_iter_loss: 0.10627267509698868
train_iter_loss: 0.20390476286411285
train_iter_loss: 0.09608351439237595
train_iter_loss: 0.23462623357772827
train_iter_loss: 0.12673766911029816
train_iter_loss: 0.14364783465862274
train_iter_loss: 0.1852046251296997
train_iter_loss: 0.17055600881576538
train_iter_loss: 0.2375791072845459
train_iter_loss: 0.1320486217737198
train_iter_loss: 0.2053285837173462
train_iter_loss: 0.16004301607608795
train_iter_loss: 0.16329437494277954
train_iter_loss: 0.21371695399284363
train_iter_loss: 0.19867266714572906
train_iter_loss: 0.2199300080537796
train_iter_loss: 0.2830263078212738
train_iter_loss: 0.20285677909851074
train_iter_loss: 0.16298125684261322
train_iter_loss: 0.298037052154541
train_iter_loss: 0.24455122649669647
train_iter_loss: 0.24505816400051117
train_iter_loss: 0.15151402354240417
train loss :0.1981
---------------------
Validation seg loss: 0.2301410001017294 at epoch 54
********************
best_val_epoch_loss:  0.2301410001017294
MODEL UPDATED
epoch =     55/  1000, exp = train
train_iter_loss: 0.05227169394493103
train_iter_loss: 0.1831464171409607
train_iter_loss: 0.18559964001178741
train_iter_loss: 0.20764827728271484
train_iter_loss: 0.16431930661201477
train_iter_loss: 0.35640451312065125
train_iter_loss: 0.24289865791797638
train_iter_loss: 0.18003442883491516
train_iter_loss: 0.21749891340732574
train_iter_loss: 0.18557007610797882
train_iter_loss: 0.20720528066158295
train_iter_loss: 0.16045518219470978
train_iter_loss: 0.26092350482940674
train_iter_loss: 0.15162333846092224
train_iter_loss: 0.15196579694747925
train_iter_loss: 0.1670156717300415
train_iter_loss: 0.17295651137828827
train_iter_loss: 0.13955073058605194
train_iter_loss: 0.14097736775875092
train_iter_loss: 0.22570857405662537
train_iter_loss: 0.3410497307777405
train_iter_loss: 0.21336479485034943
train_iter_loss: 0.22982411086559296
train_iter_loss: 0.2718171775341034
train_iter_loss: 0.2079516500234604
train_iter_loss: 0.17884038388729095
train_iter_loss: 0.15807506442070007
train_iter_loss: 0.058422841131687164
train_iter_loss: 0.21234938502311707
train_iter_loss: 0.16572849452495575
train_iter_loss: 0.1455787569284439
train_iter_loss: 0.250638484954834
train_iter_loss: 0.18631184101104736
train_iter_loss: 0.2496933490037918
train_iter_loss: 0.11017116904258728
train_iter_loss: 0.19748669862747192
train_iter_loss: 0.19869619607925415
train_iter_loss: 0.18875983357429504
train_iter_loss: 0.07434194535017014
train_iter_loss: 0.2445351928472519
train_iter_loss: 0.2683306336402893
train_iter_loss: 0.14861272275447845
train_iter_loss: 0.15007628500461578
train_iter_loss: 0.1802804172039032
train_iter_loss: 0.29226359724998474
train_iter_loss: 0.1268482357263565
train_iter_loss: 0.20877453684806824
train_iter_loss: 0.3413279950618744
train_iter_loss: 0.1889180988073349
train_iter_loss: 0.17228379845619202
train_iter_loss: 0.16348235309123993
train_iter_loss: 0.13182756304740906
train_iter_loss: 0.36038973927497864
train_iter_loss: 0.25460085272789
train_iter_loss: 0.19407223165035248
train_iter_loss: 0.122466541826725
train_iter_loss: 0.19143301248550415
train_iter_loss: 0.22741560637950897
train_iter_loss: 0.17068660259246826
train_iter_loss: 0.2048061341047287
train_iter_loss: 0.17304223775863647
train_iter_loss: 0.14575496315956116
train_iter_loss: 0.21111740171909332
train_iter_loss: 0.21209727227687836
train_iter_loss: 0.0870724469423294
train_iter_loss: 0.1025480180978775
train_iter_loss: 0.17792633175849915
train_iter_loss: 0.16334274411201477
train_iter_loss: 0.30680495500564575
train_iter_loss: 0.11481934040784836
train_iter_loss: 0.25030717253685
train_iter_loss: 0.2286314219236374
train_iter_loss: 0.1210101842880249
train_iter_loss: 0.2713232636451721
train_iter_loss: 0.17042316496372223
train_iter_loss: 0.1704893708229065
train_iter_loss: 0.10555221885442734
train_iter_loss: 0.19211821258068085
train_iter_loss: 0.18512682616710663
train_iter_loss: 0.1335340440273285
train_iter_loss: 0.19068661332130432
train_iter_loss: 0.2707545757293701
train_iter_loss: 0.16353368759155273
train_iter_loss: 0.31915363669395447
train_iter_loss: 0.23577922582626343
train_iter_loss: 0.2423611879348755
train_iter_loss: 0.27884289622306824
train_iter_loss: 0.18050691485404968
train_iter_loss: 0.28303492069244385
train_iter_loss: 0.15171611309051514
train_iter_loss: 0.26772814989089966
train_iter_loss: 0.17111951112747192
train_iter_loss: 0.40663495659828186
train_iter_loss: 0.23978152871131897
train_iter_loss: 0.30076315999031067
train_iter_loss: 0.28803545236587524
train_iter_loss: 0.17601527273654938
train_iter_loss: 0.1687081754207611
train_iter_loss: 0.21668246388435364
train_iter_loss: 0.17042328417301178
train loss :0.2018
---------------------
Validation seg loss: 0.2322766401295392 at epoch 55
epoch =     56/  1000, exp = train
train_iter_loss: 0.1580754816532135
train_iter_loss: 0.19413329660892487
train_iter_loss: 0.20599807798862457
train_iter_loss: 0.1897394359111786
train_iter_loss: 0.26776590943336487
train_iter_loss: 0.1582166701555252
train_iter_loss: 0.10559237748384476
train_iter_loss: 0.16967108845710754
train_iter_loss: 0.2160661369562149
train_iter_loss: 0.11239363253116608
train_iter_loss: 0.21836788952350616
train_iter_loss: 0.15398281812667847
train_iter_loss: 0.31179079413414
train_iter_loss: 0.11376018822193146
train_iter_loss: 0.3247680068016052
train_iter_loss: 0.13917407393455505
train_iter_loss: 0.21755841374397278
train_iter_loss: 0.22241073846817017
train_iter_loss: 0.06826232373714447
train_iter_loss: 0.13678714632987976
train_iter_loss: 0.21533013880252838
train_iter_loss: 0.23202122747898102
train_iter_loss: 0.13023242354393005
train_iter_loss: 0.16500160098075867
train_iter_loss: 0.3178607225418091
train_iter_loss: 0.27792471647262573
train_iter_loss: 0.14175471663475037
train_iter_loss: 0.07736789435148239
train_iter_loss: 0.19333882629871368
train_iter_loss: 0.18446089327335358
train_iter_loss: 0.28637197613716125
train_iter_loss: 0.21451517939567566
train_iter_loss: 0.13574939966201782
train_iter_loss: 0.2518928647041321
train_iter_loss: 0.18156233429908752
train_iter_loss: 0.13472846150398254
train_iter_loss: 0.2540947198867798
train_iter_loss: 0.22283433377742767
train_iter_loss: 0.2889929711818695
train_iter_loss: 0.18194401264190674
train_iter_loss: 0.2800755500793457
train_iter_loss: 0.13493682444095612
train_iter_loss: 0.18356402218341827
train_iter_loss: 0.17965082824230194
train_iter_loss: 0.11972814053297043
train_iter_loss: 0.2885081171989441
train_iter_loss: 0.26025471091270447
train_iter_loss: 0.26672711968421936
train_iter_loss: 0.17208079993724823
train_iter_loss: 0.1949753314256668
train_iter_loss: 0.24033334851264954
train_iter_loss: 0.20012527704238892
train_iter_loss: 0.27097299695014954
train_iter_loss: 0.15159764885902405
train_iter_loss: 0.1653924137353897
train_iter_loss: 0.24384725093841553
train_iter_loss: 0.2790631055831909
train_iter_loss: 0.14110711216926575
train_iter_loss: 0.19805808365345
train_iter_loss: 0.19595752656459808
train_iter_loss: 0.23589736223220825
train_iter_loss: 0.1616581529378891
train_iter_loss: 0.1251291036605835
train_iter_loss: 0.16165876388549805
train_iter_loss: 0.6469907164573669
train_iter_loss: 0.20189285278320312
train_iter_loss: 0.13480809330940247
train_iter_loss: 0.10525093972682953
train_iter_loss: 0.20481805503368378
train_iter_loss: 0.20870837569236755
train_iter_loss: 0.20049557089805603
train_iter_loss: 0.17093436419963837
train_iter_loss: 0.2564483880996704
train_iter_loss: 0.19356973469257355
train_iter_loss: 0.27117112278938293
train_iter_loss: 0.2025422602891922
train_iter_loss: 0.11288483440876007
train_iter_loss: 0.15635043382644653
train_iter_loss: 0.18984180688858032
train_iter_loss: 0.1714775562286377
train_iter_loss: 0.09767016023397446
train_iter_loss: 0.15010519325733185
train_iter_loss: 0.41789302229881287
train_iter_loss: 0.35788244009017944
train_iter_loss: 0.22905680537223816
train_iter_loss: 0.17675071954727173
train_iter_loss: 0.12828263640403748
train_iter_loss: 0.18085967004299164
train_iter_loss: 0.13380469381809235
train_iter_loss: 0.19878719747066498
train_iter_loss: 0.18052662909030914
train_iter_loss: 0.21273310482501984
train_iter_loss: 0.2076842486858368
train_iter_loss: 0.12639468908309937
train_iter_loss: 0.13270203769207
train_iter_loss: 0.21024209260940552
train_iter_loss: 0.14987383782863617
train_iter_loss: 0.25620201230049133
train_iter_loss: 0.140662282705307
train_iter_loss: 0.15452134609222412
train loss :0.2010
---------------------
Validation seg loss: 0.2305600792169571 at epoch 56
epoch =     57/  1000, exp = train
train_iter_loss: 0.1640007048845291
train_iter_loss: 0.30230069160461426
train_iter_loss: 0.14678612351417542
train_iter_loss: 0.21469776332378387
train_iter_loss: 0.2807858884334564
train_iter_loss: 0.2603755295276642
train_iter_loss: 0.2521573007106781
train_iter_loss: 0.22919560968875885
train_iter_loss: 0.14712324738502502
train_iter_loss: 0.10629907250404358
train_iter_loss: 0.17410148680210114
train_iter_loss: 0.18102727830410004
train_iter_loss: 0.18312522768974304
train_iter_loss: 0.15040268003940582
train_iter_loss: 0.14768728613853455
train_iter_loss: 0.19608518481254578
train_iter_loss: 0.19659462571144104
train_iter_loss: 0.24483731389045715
train_iter_loss: 0.17565001547336578
train_iter_loss: 0.2213222086429596
train_iter_loss: 0.2277669906616211
train_iter_loss: 0.2142620086669922
train_iter_loss: 0.1659989207983017
train_iter_loss: 0.2666213810443878
train_iter_loss: 0.2068370282649994
train_iter_loss: 0.13738451898097992
train_iter_loss: 0.2479153573513031
train_iter_loss: 0.15266476571559906
train_iter_loss: 0.197661891579628
train_iter_loss: 0.1366901844739914
train_iter_loss: 0.138119176030159
train_iter_loss: 0.1524818390607834
train_iter_loss: 0.20577001571655273
train_iter_loss: 0.2854827344417572
train_iter_loss: 0.14739841222763062
train_iter_loss: 0.2215559333562851
train_iter_loss: 0.12757088243961334
train_iter_loss: 0.18925543129444122
train_iter_loss: 0.09340276569128036
train_iter_loss: 0.16996999084949493
train_iter_loss: 0.1969500482082367
train_iter_loss: 0.23236927390098572
train_iter_loss: 0.23365923762321472
train_iter_loss: 0.22653920948505402
train_iter_loss: 0.18173405528068542
train_iter_loss: 0.24754688143730164
train_iter_loss: 0.2781922221183777
train_iter_loss: 0.17150619626045227
train_iter_loss: 0.31407663226127625
train_iter_loss: 0.11186528205871582
train_iter_loss: 0.1696029156446457
train_iter_loss: 0.1793934404850006
train_iter_loss: 0.16522745788097382
train_iter_loss: 0.31387245655059814
train_iter_loss: 0.3671487271785736
train_iter_loss: 0.1254720538854599
train_iter_loss: 0.172776997089386
train_iter_loss: 0.18525832891464233
train_iter_loss: 0.1415366232395172
train_iter_loss: 0.17831557989120483
train_iter_loss: 0.21999934315681458
train_iter_loss: 0.16830040514469147
train_iter_loss: 0.11273680627346039
train_iter_loss: 0.22218185663223267
train_iter_loss: 0.16571182012557983
train_iter_loss: 0.138115793466568
train_iter_loss: 0.1588277816772461
train_iter_loss: 0.26670461893081665
train_iter_loss: 0.2108803242444992
train_iter_loss: 0.2745985984802246
train_iter_loss: 0.17792020738124847
train_iter_loss: 0.3095749318599701
train_iter_loss: 0.1531788408756256
train_iter_loss: 0.200638547539711
train_iter_loss: 0.08056263625621796
train_iter_loss: 0.19098348915576935
train_iter_loss: 0.26044443249702454
train_iter_loss: 0.17462442815303802
train_iter_loss: 0.10187723487615585
train_iter_loss: 0.19057293236255646
train_iter_loss: 0.1657414585351944
train_iter_loss: 0.26670995354652405
train_iter_loss: 0.15923546254634857
train_iter_loss: 0.1787365823984146
train_iter_loss: 0.17626258730888367
train_iter_loss: 0.13157403469085693
train_iter_loss: 0.21349522471427917
train_iter_loss: 0.2856068015098572
train_iter_loss: 0.33267125487327576
train_iter_loss: 0.2433597594499588
train_iter_loss: 0.15078411996364594
train_iter_loss: 0.1724480539560318
train_iter_loss: 0.24447520077228546
train_iter_loss: 0.2450975775718689
train_iter_loss: 0.12597103416919708
train_iter_loss: 0.12759065628051758
train_iter_loss: 0.15850220620632172
train_iter_loss: 0.15794870257377625
train_iter_loss: 0.27914854884147644
train_iter_loss: 0.20156991481781006
train loss :0.1985
---------------------
Validation seg loss: 0.23264107557962527 at epoch 57
epoch =     58/  1000, exp = train
train_iter_loss: 0.1553356945514679
train_iter_loss: 0.2733072340488434
train_iter_loss: 0.18590311706066132
train_iter_loss: 0.23547857999801636
train_iter_loss: 0.20322772860527039
train_iter_loss: 0.1945972889661789
train_iter_loss: 0.22586210072040558
train_iter_loss: 0.17834894359111786
train_iter_loss: 0.15100300312042236
train_iter_loss: 0.16452203691005707
train_iter_loss: 0.14332863688468933
train_iter_loss: 0.3121383786201477
train_iter_loss: 0.09527803212404251
train_iter_loss: 0.25983747839927673
train_iter_loss: 0.13307996094226837
train_iter_loss: 0.15552903711795807
train_iter_loss: 0.08234183490276337
train_iter_loss: 0.15340475738048553
train_iter_loss: 0.21216636896133423
train_iter_loss: 0.14574705064296722
train_iter_loss: 0.16662517189979553
train_iter_loss: 0.16852019727230072
train_iter_loss: 0.24911467730998993
train_iter_loss: 0.16074790060520172
train_iter_loss: 0.22751206159591675
train_iter_loss: 0.29701918363571167
train_iter_loss: 0.14031384885311127
train_iter_loss: 0.13541747629642487
train_iter_loss: 0.26703500747680664
train_iter_loss: 0.10726413875818253
train_iter_loss: 0.1747543215751648
train_iter_loss: 0.1530442237854004
train_iter_loss: 0.16334518790245056
train_iter_loss: 0.16915418207645416
train_iter_loss: 0.18785884976387024
train_iter_loss: 0.19958148896694183
train_iter_loss: 0.2092399001121521
train_iter_loss: 0.1864449381828308
train_iter_loss: 0.207387313246727
train_iter_loss: 0.09837650507688522
train_iter_loss: 0.21780544519424438
train_iter_loss: 0.13947738707065582
train_iter_loss: 0.15122899413108826
train_iter_loss: 0.2500590682029724
train_iter_loss: 0.11897270381450653
train_iter_loss: 0.30065688490867615
train_iter_loss: 0.16694235801696777
train_iter_loss: 0.07701057195663452
train_iter_loss: 0.1786048859357834
train_iter_loss: 0.27843862771987915
train_iter_loss: 0.28904274106025696
train_iter_loss: 0.236839160323143
train_iter_loss: 0.24405644834041595
train_iter_loss: 0.13052915036678314
train_iter_loss: 0.22016063332557678
train_iter_loss: 0.30935579538345337
train_iter_loss: 0.12349502742290497
train_iter_loss: 0.24894939363002777
train_iter_loss: 0.15930858254432678
train_iter_loss: 0.18684057891368866
train_iter_loss: 0.18780535459518433
train_iter_loss: 0.17420829832553864
train_iter_loss: 0.1646745502948761
train_iter_loss: 0.17898772656917572
train_iter_loss: 0.14421966671943665
train_iter_loss: 0.27070048451423645
train_iter_loss: 0.2818010151386261
train_iter_loss: 0.08198798447847366
train_iter_loss: 0.1925967037677765
train_iter_loss: 0.12055061012506485
train_iter_loss: 0.2174963504076004
train_iter_loss: 0.11418109387159348
train_iter_loss: 0.14698167145252228
train_iter_loss: 0.23119843006134033
train_iter_loss: 0.22456085681915283
train_iter_loss: 0.10828418284654617
train_iter_loss: 0.19088169932365417
train_iter_loss: 0.16020536422729492
train_iter_loss: 0.1644151657819748
train_iter_loss: 0.29435786604881287
train_iter_loss: 0.1632322371006012
train_iter_loss: 0.24901507794857025
train_iter_loss: 0.1575721800327301
train_iter_loss: 0.19815237820148468
train_iter_loss: 0.11848609149456024
train_iter_loss: 0.24012967944145203
train_iter_loss: 0.15366587042808533
train_iter_loss: 0.15716712176799774
train_iter_loss: 0.18913321197032928
train_iter_loss: 0.15470829606056213
train_iter_loss: 0.1371271163225174
train_iter_loss: 0.17138054966926575
train_iter_loss: 0.24257388710975647
train_iter_loss: 0.11423518508672714
train_iter_loss: 0.14191894233226776
train_iter_loss: 0.18171516060829163
train_iter_loss: 0.10013779252767563
train_iter_loss: 0.2394772619009018
train_iter_loss: 0.1938803344964981
train_iter_loss: 0.18395762145519257
train loss :0.1869
---------------------
Validation seg loss: 0.2315732156682127 at epoch 58
epoch =     59/  1000, exp = train
train_iter_loss: 0.2206815928220749
train_iter_loss: 0.20379748940467834
train_iter_loss: 0.24351359903812408
train_iter_loss: 0.15750713646411896
train_iter_loss: 0.21764886379241943
train_iter_loss: 0.19901859760284424
train_iter_loss: 0.1593790054321289
train_iter_loss: 0.1763334721326828
train_iter_loss: 0.0719093456864357
train_iter_loss: 0.13575425744056702
train_iter_loss: 0.09575749188661575
train_iter_loss: 0.09305773675441742
train_iter_loss: 0.14524072408676147
train_iter_loss: 0.19258379936218262
train_iter_loss: 0.2226737141609192
train_iter_loss: 0.2049265205860138
train_iter_loss: 0.19475823640823364
train_iter_loss: 0.23111972212791443
train_iter_loss: 0.15920810401439667
train_iter_loss: 0.23585867881774902
train_iter_loss: 0.1640947461128235
train_iter_loss: 0.17639850080013275
train_iter_loss: 0.12289594858884811
train_iter_loss: 0.34569913148880005
train_iter_loss: 0.29789960384368896
train_iter_loss: 0.06333988904953003
train_iter_loss: 0.1325414925813675
train_iter_loss: 0.16406746208667755
train_iter_loss: 0.2062581330537796
train_iter_loss: 0.13256387412548065
train_iter_loss: 0.22175750136375427
train_iter_loss: 0.22945460677146912
train_iter_loss: 0.23815324902534485
train_iter_loss: 0.10030776262283325
train_iter_loss: 0.1448546200990677
train_iter_loss: 0.2599852383136749
train_iter_loss: 0.31659603118896484
train_iter_loss: 0.23375366628170013
train_iter_loss: 0.1616145223379135
train_iter_loss: 0.21292345225811005
train_iter_loss: 0.1919400691986084
train_iter_loss: 0.17581745982170105
train_iter_loss: 0.1777319610118866
train_iter_loss: 0.19500359892845154
train_iter_loss: 0.32311132550239563
train_iter_loss: 0.15007248520851135
train_iter_loss: 0.2193971574306488
train_iter_loss: 0.20458492636680603
train_iter_loss: 0.1447339504957199
train_iter_loss: 0.33039796352386475
train_iter_loss: 0.18315139412879944
train_iter_loss: 0.26788222789764404
train_iter_loss: 0.24378710985183716
train_iter_loss: 0.1689446121454239
train_iter_loss: 0.10112468898296356
train_iter_loss: 0.2287714183330536
train_iter_loss: 0.23682191967964172
train_iter_loss: 0.10588828474283218
train_iter_loss: 0.22251465916633606
train_iter_loss: 0.13402938842773438
train_iter_loss: 0.18310387432575226
train_iter_loss: 0.17328770458698273
train_iter_loss: 0.12447488307952881
train_iter_loss: 0.1486591249704361
train_iter_loss: 0.21971853077411652
train_iter_loss: 0.12303702533245087
train_iter_loss: 0.22757777571678162
train_iter_loss: 0.15104924142360687
train_iter_loss: 0.30349963903427124
train_iter_loss: 0.27070823311805725
train_iter_loss: 0.18712127208709717
train_iter_loss: 0.23764440417289734
train_iter_loss: 0.17952603101730347
train_iter_loss: 0.11220471560955048
train_iter_loss: 0.11985019594430923
train_iter_loss: 0.2924738824367523
train_iter_loss: 0.19591476023197174
train_iter_loss: 0.16230225563049316
train_iter_loss: 0.22067084908485413
train_iter_loss: 0.3223162591457367
train_iter_loss: 0.11179730296134949
train_iter_loss: 0.22594408690929413
train_iter_loss: 0.2333165854215622
train_iter_loss: 0.1851615160703659
train_iter_loss: 0.20802487432956696
train_iter_loss: 0.09246033430099487
train_iter_loss: 0.24477310478687286
train_iter_loss: 0.12054614722728729
train_iter_loss: 0.20690850913524628
train_iter_loss: 0.13889265060424805
train_iter_loss: 0.22190743684768677
train_iter_loss: 0.16401945054531097
train_iter_loss: 0.38542917370796204
train_iter_loss: 0.2059861570596695
train_iter_loss: 0.10483241081237793
train_iter_loss: 0.23437966406345367
train_iter_loss: 0.15027637779712677
train_iter_loss: 0.1859341263771057
train_iter_loss: 0.159433513879776
train_iter_loss: 0.11008057743310928
train loss :0.1932
---------------------
Validation seg loss: 0.23151037053046924 at epoch 59
epoch =     60/  1000, exp = train
train_iter_loss: 0.1291622668504715
train_iter_loss: 0.21275390684604645
train_iter_loss: 0.26680994033813477
train_iter_loss: 0.14184534549713135
train_iter_loss: 0.18724007904529572
train_iter_loss: 0.24443931877613068
train_iter_loss: 0.13996939361095428
train_iter_loss: 0.16687604784965515
train_iter_loss: 0.1293419897556305
train_iter_loss: 0.1496322602033615
train_iter_loss: 0.15621165931224823
train_iter_loss: 0.3888547122478485
train_iter_loss: 0.13497285544872284
train_iter_loss: 0.1751893013715744
train_iter_loss: 0.20774123072624207
train_iter_loss: 0.17334480583667755
train_iter_loss: 0.26131656765937805
train_iter_loss: 0.17006921768188477
train_iter_loss: 0.1275019347667694
train_iter_loss: 0.20215396583080292
train_iter_loss: 0.18504109978675842
train_iter_loss: 0.13156519830226898
train_iter_loss: 0.1156577542424202
train_iter_loss: 0.28352728486061096
train_iter_loss: 0.15671315789222717
train_iter_loss: 0.07810024917125702
train_iter_loss: 0.0672699511051178
train_iter_loss: 0.2518342435359955
train_iter_loss: 0.13229520618915558
train_iter_loss: 0.11748165637254715
train_iter_loss: 0.28198114037513733
train_iter_loss: 0.18734723329544067
train_iter_loss: 0.10323688387870789
train_iter_loss: 0.17949962615966797
train_iter_loss: 0.11472399532794952
train_iter_loss: 0.22507159411907196
train_iter_loss: 0.08859269320964813
train_iter_loss: 0.16478146612644196
train_iter_loss: 0.5203245878219604
train_iter_loss: 0.2513645589351654
train_iter_loss: 0.23021234571933746
train_iter_loss: 0.2024461328983307
train_iter_loss: 0.1452217549085617
train_iter_loss: 0.2449411302804947
train_iter_loss: 0.08691561222076416
train_iter_loss: 0.2705961763858795
train_iter_loss: 0.14003247022628784
train_iter_loss: 0.06821992248296738
train_iter_loss: 0.21672804653644562
train_iter_loss: 0.17263232171535492
train_iter_loss: 0.21879449486732483
train_iter_loss: 0.5716332197189331
train_iter_loss: 0.16477571427822113
train_iter_loss: 0.14400935173034668
train_iter_loss: 0.18183115124702454
train_iter_loss: 0.2479369193315506
train_iter_loss: 0.1796872764825821
train_iter_loss: 0.22293536365032196
train_iter_loss: 0.1564020961523056
train_iter_loss: 0.06268950551748276
train_iter_loss: 0.1296607106924057
train_iter_loss: 0.16095934808254242
train_iter_loss: 0.19118763506412506
train_iter_loss: 0.2400749921798706
train_iter_loss: 0.1958749145269394
train_iter_loss: 0.1602158397436142
train_iter_loss: 0.31885361671447754
train_iter_loss: 0.15422415733337402
train_iter_loss: 0.22933311760425568
train_iter_loss: 0.1832515448331833
train_iter_loss: 0.26484689116477966
train_iter_loss: 0.22238150238990784
train_iter_loss: 0.1791665256023407
train_iter_loss: 0.04523675516247749
train_iter_loss: 0.19046778976917267
train_iter_loss: 0.212029829621315
train_iter_loss: 0.2586331367492676
train_iter_loss: 0.2738149166107178
train_iter_loss: 0.07338088005781174
train_iter_loss: 0.19205722212791443
train_iter_loss: 0.15584462881088257
train_iter_loss: 0.22489631175994873
train_iter_loss: 0.1818419247865677
train_iter_loss: 0.24730722606182098
train_iter_loss: 0.3259719908237457
train_iter_loss: 0.14693592488765717
train_iter_loss: 0.1390097737312317
train_iter_loss: 0.1888095736503601
train_iter_loss: 0.17439933121204376
train_iter_loss: 0.12725935876369476
train_iter_loss: 0.26188820600509644
train_iter_loss: 0.15689809620380402
train_iter_loss: 0.1807919591665268
train_iter_loss: 0.21087096631526947
train_iter_loss: 0.15267038345336914
train_iter_loss: 0.2511003017425537
train_iter_loss: 0.1792212575674057
train_iter_loss: 0.2835448384284973
train_iter_loss: 0.2581917941570282
train_iter_loss: 0.1583661288022995
train loss :0.1941
---------------------
Validation seg loss: 0.23082264672683658 at epoch 60
epoch =     61/  1000, exp = train
train_iter_loss: 0.19642643630504608
train_iter_loss: 0.2142745703458786
train_iter_loss: 0.16914476454257965
train_iter_loss: 0.15915626287460327
train_iter_loss: 0.25183409452438354
train_iter_loss: 0.192742720246315
train_iter_loss: 0.11434024572372437
train_iter_loss: 0.2476147711277008
train_iter_loss: 0.1413140743970871
train_iter_loss: 0.16063997149467468
train_iter_loss: 0.1906580626964569
train_iter_loss: 0.16612783074378967
train_iter_loss: 0.22435031831264496
train_iter_loss: 0.11108800768852234
train_iter_loss: 0.15363816916942596
train_iter_loss: 0.1313028782606125
train_iter_loss: 0.17924657464027405
train_iter_loss: 0.15615643560886383
train_iter_loss: 0.18995214998722076
train_iter_loss: 0.19302970170974731
train_iter_loss: 0.21313072741031647
train_iter_loss: 0.3053012192249298
train_iter_loss: 0.26283344626426697
train_iter_loss: 0.14811864495277405
train_iter_loss: 0.2818947732448578
train_iter_loss: 0.18425242602825165
train_iter_loss: 0.1787785440683365
train_iter_loss: 0.20110425353050232
train_iter_loss: 0.2637339234352112
train_iter_loss: 0.25266721844673157
train_iter_loss: 0.24282360076904297
train_iter_loss: 0.18850097060203552
train_iter_loss: 0.22082599997520447
train_iter_loss: 0.2716144323348999
train_iter_loss: 0.14002586901187897
train_iter_loss: 0.1985570192337036
train_iter_loss: 0.2460223287343979
train_iter_loss: 0.333035409450531
train_iter_loss: 0.16280677914619446
train_iter_loss: 0.17220309376716614
train_iter_loss: 0.18368829786777496
train_iter_loss: 0.28723281621932983
train_iter_loss: 0.10210875421762466
train_iter_loss: 0.2482600063085556
train_iter_loss: 0.32641732692718506
train_iter_loss: 0.18395809829235077
train_iter_loss: 0.17777179181575775
train_iter_loss: 0.1617521047592163
train_iter_loss: 0.18474724888801575
train_iter_loss: 0.29185986518859863
train_iter_loss: 0.20664498209953308
train_iter_loss: 0.30601420998573303
train_iter_loss: 0.1981453150510788
train_iter_loss: 0.1942363828420639
train_iter_loss: 0.18486247956752777
train_iter_loss: 0.29099753499031067
train_iter_loss: 0.1158176138997078
train_iter_loss: 0.16991493105888367
train_iter_loss: 0.1565047800540924
train_iter_loss: 0.12943607568740845
train_iter_loss: 0.1528581827878952
train_iter_loss: 0.2807403802871704
train_iter_loss: 0.25390729308128357
train_iter_loss: 0.21239589154720306
train_iter_loss: 0.21607960760593414
train_iter_loss: 0.20079922676086426
train_iter_loss: 0.18654035031795502
train_iter_loss: 0.2525711953639984
train_iter_loss: 0.11922582983970642
train_iter_loss: 0.2889845669269562
train_iter_loss: 0.15845541656017303
train_iter_loss: 0.2145766317844391
train_iter_loss: 0.21758389472961426
train_iter_loss: 0.21461541950702667
train_iter_loss: 0.06212862953543663
train_iter_loss: 0.21236580610275269
train_iter_loss: 0.19690801203250885
train_iter_loss: 0.12987613677978516
train_iter_loss: 0.15373952686786652
train_iter_loss: 0.17016373574733734
train_iter_loss: 0.24339869618415833
train_iter_loss: 0.14583013951778412
train_iter_loss: 0.28828054666519165
train_iter_loss: 0.07298514246940613
train_iter_loss: 0.1701725721359253
train_iter_loss: 0.09226411581039429
train_iter_loss: 0.1455245316028595
train_iter_loss: 0.2594994008541107
train_iter_loss: 0.23645396530628204
train_iter_loss: 0.13456018269062042
train_iter_loss: 0.1995043009519577
train_iter_loss: 0.14799775183200836
train_iter_loss: 0.1503410041332245
train_iter_loss: 0.14838819205760956
train_iter_loss: 0.22141940891742706
train_iter_loss: 0.10896361619234085
train_iter_loss: 0.18539245426654816
train_iter_loss: 0.16331619024276733
train_iter_loss: 0.24250108003616333
train_iter_loss: 0.2338932603597641
train loss :0.1979
---------------------
Validation seg loss: 0.2295996006578207 at epoch 61
********************
best_val_epoch_loss:  0.2295996006578207
MODEL UPDATED
epoch =     62/  1000, exp = train
train_iter_loss: 0.26623186469078064
train_iter_loss: 0.10128790140151978
train_iter_loss: 0.2524223029613495
train_iter_loss: 0.2773266136646271
train_iter_loss: 0.19786937534809113
train_iter_loss: 0.13228407502174377
train_iter_loss: 0.37354642152786255
train_iter_loss: 0.17467333376407623
train_iter_loss: 0.2791822552680969
train_iter_loss: 0.13118335604667664
train_iter_loss: 0.1527685821056366
train_iter_loss: 0.1705292910337448
train_iter_loss: 0.22390832006931305
train_iter_loss: 0.17700530588626862
train_iter_loss: 0.18753217160701752
train_iter_loss: 0.18497703969478607
train_iter_loss: 0.3872157037258148
train_iter_loss: 0.11028589308261871
train_iter_loss: 0.19504301249980927
train_iter_loss: 0.06424447149038315
train_iter_loss: 0.09649301320314407
train_iter_loss: 0.24274709820747375
train_iter_loss: 0.16501836478710175
train_iter_loss: 0.18401698768138885
train_iter_loss: 0.21203278005123138
train_iter_loss: 0.15052160620689392
train_iter_loss: 0.16393089294433594
train_iter_loss: 0.15232418477535248
train_iter_loss: 0.09924191981554031
train_iter_loss: 0.11871245503425598
train_iter_loss: 0.3029155731201172
train_iter_loss: 0.21145150065422058
train_iter_loss: 0.2147093564271927
train_iter_loss: 0.3440950810909271
train_iter_loss: 0.2008058726787567
train_iter_loss: 0.2177569717168808
train_iter_loss: 0.18448777496814728
train_iter_loss: 0.1908099204301834
train_iter_loss: 0.16399917006492615
train_iter_loss: 0.19178056716918945
train_iter_loss: 0.13426093757152557
train_iter_loss: 0.19157801568508148
train_iter_loss: 0.2026674598455429
train_iter_loss: 0.23312027752399445
train_iter_loss: 0.21725007891654968
train_iter_loss: 0.10204360634088516
train_iter_loss: 0.17557932436466217
train_iter_loss: 0.1844707727432251
train_iter_loss: 0.2143818736076355
train_iter_loss: 0.119861900806427
train_iter_loss: 0.1741424798965454
train_iter_loss: 0.123295858502388
train_iter_loss: 0.16446183621883392
train_iter_loss: 0.21576541662216187
train_iter_loss: 0.2485865205526352
train_iter_loss: 0.12774215638637543
train_iter_loss: 0.16020095348358154
train_iter_loss: 0.137939453125
train_iter_loss: 0.2089012861251831
train_iter_loss: 0.22615709900856018
train_iter_loss: 0.18650312721729279
train_iter_loss: 0.2630603611469269
train_iter_loss: 0.3107583820819855
train_iter_loss: 0.25732898712158203
train_iter_loss: 0.12486621737480164
train_iter_loss: 0.25738000869750977
train_iter_loss: 0.1431826502084732
train_iter_loss: 0.3436189293861389
train_iter_loss: 0.20731396973133087
train_iter_loss: 0.1830727756023407
train_iter_loss: 0.33081960678100586
train_iter_loss: 0.30422860383987427
train_iter_loss: 0.19208689033985138
train_iter_loss: 0.1817394495010376
train_iter_loss: 0.10287821292877197
train_iter_loss: 0.1882992535829544
train_iter_loss: 0.23500731587409973
train_iter_loss: 0.09338320046663284
train_iter_loss: 0.18673120439052582
train_iter_loss: 0.10712222754955292
train_iter_loss: 0.11319596320390701
train_iter_loss: 0.0967961922287941
train_iter_loss: 0.14238637685775757
train_iter_loss: 0.17437659204006195
train_iter_loss: 0.2555881142616272
train_iter_loss: 0.2824387848377228
train_iter_loss: 0.1545220911502838
train_iter_loss: 0.33407193422317505
train_iter_loss: 0.08471953123807907
train_iter_loss: 0.20455428957939148
train_iter_loss: 0.14080435037612915
train_iter_loss: 0.12481274455785751
train_iter_loss: 0.14438849687576294
train_iter_loss: 0.23668934404850006
train_iter_loss: 0.22015438973903656
train_iter_loss: 0.21964916586875916
train_iter_loss: 0.11068856716156006
train_iter_loss: 0.10122819989919662
train_iter_loss: 0.09060271829366684
train_iter_loss: 0.12206020951271057
train loss :0.1916
---------------------
Validation seg loss: 0.23045332141150282 at epoch 62
epoch =     63/  1000, exp = train
train_iter_loss: 0.08188474178314209
train_iter_loss: 0.14204147458076477
train_iter_loss: 0.17700599133968353
train_iter_loss: 0.1568480283021927
train_iter_loss: 0.11065550148487091
train_iter_loss: 0.13477811217308044
train_iter_loss: 0.18492186069488525
train_iter_loss: 0.1673072874546051
train_iter_loss: 0.1696007251739502
train_iter_loss: 0.17223897576332092
train_iter_loss: 0.2161768674850464
train_iter_loss: 0.18061897158622742
train_iter_loss: 0.24114090204238892
train_iter_loss: 0.23222070932388306
train_iter_loss: 0.22429701685905457
train_iter_loss: 0.22460593283176422
train_iter_loss: 0.23364907503128052
train_iter_loss: 0.288796067237854
train_iter_loss: 0.20612604916095734
train_iter_loss: 0.07362736016511917
train_iter_loss: 0.23424923419952393
train_iter_loss: 0.32960182428359985
train_iter_loss: 0.1272999495267868
train_iter_loss: 0.13792769610881805
train_iter_loss: 0.17851293087005615
train_iter_loss: 0.2528969943523407
train_iter_loss: 0.12643490731716156
train_iter_loss: 0.16388188302516937
train_iter_loss: 0.12882541120052338
train_iter_loss: 0.2491437941789627
train_iter_loss: 0.08342080563306808
train_iter_loss: 0.24007779359817505
train_iter_loss: 0.1866309642791748
train_iter_loss: 0.13630199432373047
train_iter_loss: 0.16783709824085236
train_iter_loss: 0.32890841364860535
train_iter_loss: 0.17855818569660187
train_iter_loss: 0.1800723373889923
train_iter_loss: 0.2738383412361145
train_iter_loss: 0.23825308680534363
train_iter_loss: 0.2026277482509613
train_iter_loss: 0.18715746700763702
train_iter_loss: 0.1969575583934784
train_iter_loss: 0.09382141381502151
train_iter_loss: 0.22293399274349213
train_iter_loss: 0.198333740234375
train_iter_loss: 0.12087884545326233
train_iter_loss: 0.15874825417995453
train_iter_loss: 0.2035229355096817
train_iter_loss: 0.10581976175308228
train_iter_loss: 0.13854601979255676
train_iter_loss: 0.28887149691581726
train_iter_loss: 0.1468336582183838
train_iter_loss: 0.1637152135372162
train_iter_loss: 0.21973401308059692
train_iter_loss: 0.2175666242837906
train_iter_loss: 0.14894595742225647
train_iter_loss: 0.17467555403709412
train_iter_loss: 0.2051028609275818
train_iter_loss: 0.3389838635921478
train_iter_loss: 0.2322666049003601
train_iter_loss: 0.11672688275575638
train_iter_loss: 0.07331803441047668
train_iter_loss: 0.26037076115608215
train_iter_loss: 0.20933014154434204
train_iter_loss: 0.05560795217752457
train_iter_loss: 0.1500820517539978
train_iter_loss: 0.1391327828168869
train_iter_loss: 0.1734052151441574
train_iter_loss: 0.2585046887397766
train_iter_loss: 0.17133603990077972
train_iter_loss: 0.14917275309562683
train_iter_loss: 0.198367640376091
train_iter_loss: 0.18254071474075317
train_iter_loss: 0.1337822526693344
train_iter_loss: 0.23120951652526855
train_iter_loss: 0.28048402070999146
train_iter_loss: 0.17530986666679382
train_iter_loss: 0.2630258798599243
train_iter_loss: 0.16930216550827026
train_iter_loss: 0.17583099007606506
train_iter_loss: 0.16725127398967743
train_iter_loss: 0.16571812331676483
train_iter_loss: 0.19011908769607544
train_iter_loss: 0.2959071397781372
train_iter_loss: 0.16700081527233124
train_iter_loss: 0.20714159309864044
train_iter_loss: 0.27383044362068176
train_iter_loss: 0.1834646463394165
train_iter_loss: 0.226433664560318
train_iter_loss: 0.1286049336194992
train_iter_loss: 0.17916437983512878
train_iter_loss: 0.2765113115310669
train_iter_loss: 0.40410616993904114
train_iter_loss: 0.19434240460395813
train_iter_loss: 0.22914868593215942
train_iter_loss: 0.17769740521907806
train_iter_loss: 0.3057287037372589
train_iter_loss: 0.1983763873577118
train_iter_loss: 0.198832705616951
train loss :0.1946
---------------------
Validation seg loss: 0.2326782719530868 at epoch 63
epoch =     64/  1000, exp = train
train_iter_loss: 0.12904754281044006
train_iter_loss: 0.11619170010089874
train_iter_loss: 0.19544826447963715
train_iter_loss: 0.16627992689609528
train_iter_loss: 0.17079704999923706
train_iter_loss: 0.10234982520341873
train_iter_loss: 0.13916102051734924
train_iter_loss: 0.13700491189956665
train_iter_loss: 0.19667933881282806
train_iter_loss: 0.1466720551252365
train_iter_loss: 0.1693241000175476
train_iter_loss: 0.2680334448814392
train_iter_loss: 0.2374727427959442
train_iter_loss: 0.21506235003471375
train_iter_loss: 0.17864175140857697
train_iter_loss: 0.08620236814022064
train_iter_loss: 0.14301106333732605
train_iter_loss: 0.23858170211315155
train_iter_loss: 0.22720690071582794
train_iter_loss: 0.28004735708236694
train_iter_loss: 0.17579862475395203
train_iter_loss: 0.21387732028961182
train_iter_loss: 0.11502667516469955
train_iter_loss: 0.13903653621673584
train_iter_loss: 0.20052750408649445
train_iter_loss: 0.12656515836715698
train_iter_loss: 0.13254013657569885
train_iter_loss: 0.1396305412054062
train_iter_loss: 0.12205523997545242
train_iter_loss: 0.33316051959991455
train_iter_loss: 0.19618092477321625
train_iter_loss: 0.16970819234848022
train_iter_loss: 0.16468746960163116
train_iter_loss: 0.09120020270347595
train_iter_loss: 0.1449417918920517
train_iter_loss: 0.16293159127235413
train_iter_loss: 0.1956138014793396
train_iter_loss: 0.19992579519748688
train_iter_loss: 0.19316570460796356
train_iter_loss: 0.23310039937496185
train_iter_loss: 0.15813463926315308
train_iter_loss: 0.2961908280849457
train_iter_loss: 0.21576863527297974
train_iter_loss: 0.2867066562175751
train_iter_loss: 0.08812376856803894
train_iter_loss: 0.18076717853546143
train_iter_loss: 0.12529055774211884
train_iter_loss: 0.23411211371421814
train_iter_loss: 0.12021030485630035
train_iter_loss: 0.1782395839691162
train_iter_loss: 0.21354584395885468
train_iter_loss: 0.18456098437309265
train_iter_loss: 0.15047061443328857
train_iter_loss: 0.2961076498031616
train_iter_loss: 0.12841205298900604
train_iter_loss: 0.1500832587480545
train_iter_loss: 0.21719668805599213
train_iter_loss: 0.1771966516971588
train_iter_loss: 0.1975766122341156
train_iter_loss: 0.26929330825805664
train_iter_loss: 0.1722605973482132
train_iter_loss: 0.13621516525745392
train_iter_loss: 0.1322544664144516
train_iter_loss: 0.2231334000825882
train_iter_loss: 0.1997501254081726
train_iter_loss: 0.09743192791938782
train_iter_loss: 0.1770445704460144
train_iter_loss: 0.5804685354232788
train_iter_loss: 0.1806093007326126
train_iter_loss: 0.17465931177139282
train_iter_loss: 0.1500912755727768
train_iter_loss: 0.13364502787590027
train_iter_loss: 0.17747355997562408
train_iter_loss: 0.1973501443862915
train_iter_loss: 0.3725196421146393
train_iter_loss: 0.24070915579795837
train_iter_loss: 0.22259478271007538
train_iter_loss: 0.20676659047603607
train_iter_loss: 0.20147539675235748
train_iter_loss: 0.2386534959077835
train_iter_loss: 0.18009456992149353
train_iter_loss: 0.21686317026615143
train_iter_loss: 0.21479135751724243
train_iter_loss: 0.23553459346294403
train_iter_loss: 0.17840874195098877
train_iter_loss: 0.15381833910942078
train_iter_loss: 0.17917710542678833
train_iter_loss: 0.16915109753608704
train_iter_loss: 0.24144838750362396
train_iter_loss: 0.16874700784683228
train_iter_loss: 0.22795704007148743
train_iter_loss: 0.18327954411506653
train_iter_loss: 0.15557938814163208
train_iter_loss: 0.1546444147825241
train_iter_loss: 0.17944404482841492
train_iter_loss: 0.1955893486738205
train_iter_loss: 0.1891922950744629
train_iter_loss: 0.25055307149887085
train_iter_loss: 0.23810510337352753
train_iter_loss: 0.17877033352851868
train loss :0.1916
---------------------
Validation seg loss: 0.229114715045072 at epoch 64
********************
best_val_epoch_loss:  0.229114715045072
MODEL UPDATED
epoch =     65/  1000, exp = train
train_iter_loss: 0.16148576140403748
train_iter_loss: 0.1936594396829605
train_iter_loss: 0.12084488570690155
train_iter_loss: 0.2340303659439087
train_iter_loss: 0.17258834838867188
train_iter_loss: 0.1803124099969864
train_iter_loss: 0.19544783234596252
train_iter_loss: 0.2217356115579605
train_iter_loss: 0.2159816324710846
train_iter_loss: 0.18995355069637299
train_iter_loss: 0.19221976399421692
train_iter_loss: 0.11679671704769135
train_iter_loss: 0.1468844711780548
train_iter_loss: 0.14660711586475372
train_iter_loss: 0.1382642239332199
train_iter_loss: 0.2878444194793701
train_iter_loss: 0.13989734649658203
train_iter_loss: 0.20792336761951447
train_iter_loss: 0.2566909193992615
train_iter_loss: 0.1301223337650299
train_iter_loss: 0.23091119527816772
train_iter_loss: 0.1334192305803299
train_iter_loss: 0.2755669057369232
train_iter_loss: 0.13306811451911926
train_iter_loss: 0.18602287769317627
train_iter_loss: 0.1800883412361145
train_iter_loss: 0.2657966613769531
train_iter_loss: 0.17268282175064087
train_iter_loss: 0.25757476687431335
train_iter_loss: 0.10686279833316803
train_iter_loss: 0.1645279973745346
train_iter_loss: 0.18552404642105103
train_iter_loss: 0.18971718847751617
train_iter_loss: 0.20479781925678253
train_iter_loss: 0.40442532300949097
train_iter_loss: 0.157144695520401
train_iter_loss: 0.2433968186378479
train_iter_loss: 0.21671657264232635
train_iter_loss: 0.22846974432468414
train_iter_loss: 0.3416319787502289
train_iter_loss: 0.25738343596458435
train_iter_loss: 0.26423946022987366
train_iter_loss: 0.1087879166007042
train_iter_loss: 0.16194820404052734
train_iter_loss: 0.12457993626594543
train_iter_loss: 0.1681721955537796
train_iter_loss: 0.2647112309932709
train_iter_loss: 0.18399854004383087
train_iter_loss: 0.24742567539215088
train_iter_loss: 0.10056164115667343
train_iter_loss: 0.19624219834804535
train_iter_loss: 0.15388348698616028
train_iter_loss: 0.15309976041316986
train_iter_loss: 0.14628447592258453
train_iter_loss: 0.32187047600746155
train_iter_loss: 0.16239462792873383
train_iter_loss: 0.13293960690498352
train_iter_loss: 0.18994978070259094
train_iter_loss: 0.22561539709568024
train_iter_loss: 0.2053195685148239
train_iter_loss: 0.046279992908239365
train_iter_loss: 0.13095206022262573
train_iter_loss: 0.14917725324630737
train_iter_loss: 0.20109893381595612
train_iter_loss: 0.25616371631622314
train_iter_loss: 0.28793689608573914
train_iter_loss: 0.1752162128686905
train_iter_loss: 0.15912018716335297
train_iter_loss: 0.1485482156276703
train_iter_loss: 0.14491870999336243
train_iter_loss: 0.13298499584197998
train_iter_loss: 0.18633462488651276
train_iter_loss: 0.11470694094896317
train_iter_loss: 0.29768773913383484
train_iter_loss: 0.2598282992839813
train_iter_loss: 0.2413967251777649
train_iter_loss: 0.1586003303527832
train_iter_loss: 0.1698637157678604
train_iter_loss: 0.11780384927988052
train_iter_loss: 0.31670281291007996
train_iter_loss: 0.23666100203990936
train_iter_loss: 0.1853507161140442
train_iter_loss: 0.11581414937973022
train_iter_loss: 0.10658298432826996
train_iter_loss: 0.1448129564523697
train_iter_loss: 0.18309226632118225
train_iter_loss: 0.19939714670181274
train_iter_loss: 0.2326650768518448
train_iter_loss: 0.21021950244903564
train_iter_loss: 0.1861971914768219
train_iter_loss: 0.16865013539791107
train_iter_loss: 0.16044308245182037
train_iter_loss: 0.21497711539268494
train_iter_loss: 0.13887295126914978
train_iter_loss: 0.15699568390846252
train_iter_loss: 0.2052658647298813
train_iter_loss: 0.16957132518291473
train_iter_loss: 0.09368965774774551
train_iter_loss: 0.18398956954479218
train_iter_loss: 0.18550065159797668
train loss :0.1906
---------------------
Validation seg loss: 0.23188640358242787 at epoch 65
epoch =     66/  1000, exp = train
train_iter_loss: 0.14062495529651642
train_iter_loss: 0.14985999464988708
train_iter_loss: 0.10106098651885986
train_iter_loss: 0.23930080235004425
train_iter_loss: 0.10532750189304352
train_iter_loss: 0.14937293529510498
train_iter_loss: 0.1214495524764061
train_iter_loss: 0.14322972297668457
train_iter_loss: 0.21641385555267334
train_iter_loss: 0.23223266005516052
train_iter_loss: 0.2600109875202179
train_iter_loss: 0.22196722030639648
train_iter_loss: 0.1761372834444046
train_iter_loss: 0.15814636647701263
train_iter_loss: 0.24753323197364807
train_iter_loss: 0.12953481078147888
train_iter_loss: 0.19782409071922302
train_iter_loss: 0.21270766854286194
train_iter_loss: 0.15317606925964355
train_iter_loss: 0.2276848554611206
train_iter_loss: 0.10025966167449951
train_iter_loss: 0.19641968607902527
train_iter_loss: 0.213094100356102
train_iter_loss: 0.10328967869281769
train_iter_loss: 0.26679834723472595
train_iter_loss: 0.10980571806430817
train_iter_loss: 0.20263512432575226
train_iter_loss: 0.19811974465847015
train_iter_loss: 0.14387843012809753
train_iter_loss: 0.23787666857242584
train_iter_loss: 0.21595880389213562
train_iter_loss: 0.3040006160736084
train_iter_loss: 0.22456640005111694
train_iter_loss: 0.14049124717712402
train_iter_loss: 0.19870685040950775
train_iter_loss: 0.29088348150253296
train_iter_loss: 0.13333263993263245
train_iter_loss: 0.17432807385921478
train_iter_loss: 0.23116575181484222
train_iter_loss: 0.21451880037784576
train_iter_loss: 0.22218748927116394
train_iter_loss: 0.14904020726680756
train_iter_loss: 0.11802385747432709
train_iter_loss: 0.2023228108882904
train_iter_loss: 0.22967229783535004
train_iter_loss: 0.11800181120634079
train_iter_loss: 0.1539563685655594
train_iter_loss: 0.15024255216121674
train_iter_loss: 0.18190409243106842
train_iter_loss: 0.2740069329738617
train_iter_loss: 0.1460161954164505
train_iter_loss: 0.23285873234272003
train_iter_loss: 0.3179752826690674
train_iter_loss: 0.21652400493621826
train_iter_loss: 0.16591140627861023
train_iter_loss: 0.0823683887720108
train_iter_loss: 0.18231241405010223
train_iter_loss: 0.27257347106933594
train_iter_loss: 0.163697749376297
train_iter_loss: 0.12480194121599197
train_iter_loss: 0.2784663438796997
train_iter_loss: 0.16733215749263763
train_iter_loss: 0.08472490310668945
train_iter_loss: 0.1395997554063797
train_iter_loss: 0.1686561554670334
train_iter_loss: 0.33504658937454224
train_iter_loss: 0.17364034056663513
train_iter_loss: 0.16480962932109833
train_iter_loss: 0.15713350474834442
train_iter_loss: 0.1346694529056549
train_iter_loss: 0.2112894058227539
train_iter_loss: 0.1978244036436081
train_iter_loss: 0.14315073192119598
train_iter_loss: 0.12233778089284897
train_iter_loss: 0.22647108137607574
train_iter_loss: 0.21800942718982697
train_iter_loss: 0.13933251798152924
train_iter_loss: 0.2419564425945282
train_iter_loss: 0.18760725855827332
train_iter_loss: 0.24190934002399445
train_iter_loss: 0.1808573603630066
train_iter_loss: 0.12372805923223495
train_iter_loss: 0.25383585691452026
train_iter_loss: 0.1885918527841568
train_iter_loss: 0.06361936032772064
train_iter_loss: 0.07928761839866638
train_iter_loss: 0.20893330872058868
train_iter_loss: 0.3142683804035187
train_iter_loss: 0.1519993543624878
train_iter_loss: 0.14132921397686005
train_iter_loss: 0.16703017055988312
train_iter_loss: 0.16986270248889923
train_iter_loss: 0.15350712835788727
train_iter_loss: 0.20185182988643646
train_iter_loss: 0.2710975706577301
train_iter_loss: 0.20163021981716156
train_iter_loss: 0.1426251381635666
train_iter_loss: 0.285513311624527
train_iter_loss: 0.08037185668945312
train_iter_loss: 0.24043117463588715
train loss :0.1873
---------------------
Validation seg loss: 0.23044443350144714 at epoch 66
epoch =     67/  1000, exp = train
train_iter_loss: 0.06922249495983124
train_iter_loss: 0.21369746327400208
train_iter_loss: 0.18976905941963196
train_iter_loss: 0.1485406458377838
train_iter_loss: 0.21755477786064148
train_iter_loss: 0.3524271845817566
train_iter_loss: 0.17150504887104034
train_iter_loss: 0.12596940994262695
train_iter_loss: 0.13963080942630768
train_iter_loss: 0.1587095856666565
train_iter_loss: 0.17355966567993164
train_iter_loss: 0.12138036638498306
train_iter_loss: 0.16261422634124756
train_iter_loss: 0.06689949333667755
train_iter_loss: 0.24787001311779022
train_iter_loss: 0.2394176423549652
train_iter_loss: 0.16152434051036835
train_iter_loss: 0.22552752494812012
train_iter_loss: 0.21029581129550934
train_iter_loss: 0.09132687002420425
train_iter_loss: 0.18086226284503937
train_iter_loss: 0.16628849506378174
train_iter_loss: 0.21989335119724274
train_iter_loss: 0.3118843734264374
train_iter_loss: 0.17146000266075134
train_iter_loss: 0.1939517855644226
train_iter_loss: 0.19631673395633698
train_iter_loss: 0.09604411572217941
train_iter_loss: 0.1830073446035385
train_iter_loss: 0.19198817014694214
train_iter_loss: 0.11099734902381897
train_iter_loss: 0.16371339559555054
train_iter_loss: 0.18126583099365234
train_iter_loss: 0.10596398264169693
train_iter_loss: 0.19339066743850708
train_iter_loss: 0.19656458497047424
train_iter_loss: 0.2275257110595703
train_iter_loss: 0.21815940737724304
train_iter_loss: 0.30869314074516296
train_iter_loss: 0.18126235902309418
train_iter_loss: 0.12148965895175934
train_iter_loss: 0.20645596086978912
train_iter_loss: 0.3086913228034973
train_iter_loss: 0.2156202495098114
train_iter_loss: 0.29514724016189575
train_iter_loss: 0.2525181174278259
train_iter_loss: 0.21581146121025085
train_iter_loss: 0.4123780429363251
train_iter_loss: 0.13909156620502472
train_iter_loss: 0.16996675729751587
train_iter_loss: 0.21895085275173187
train_iter_loss: 0.2854647934436798
train_iter_loss: 0.20471598207950592
train_iter_loss: 0.14470629394054413
train_iter_loss: 0.15488693118095398
train_iter_loss: 0.2463357001543045
train_iter_loss: 0.23902527987957
train_iter_loss: 0.2236449122428894
train_iter_loss: 0.19550423324108124
train_iter_loss: 0.14738419651985168
train_iter_loss: 0.11066287010908127
train_iter_loss: 0.138761505484581
train_iter_loss: 0.2577894628047943
train_iter_loss: 0.2258857935667038
train_iter_loss: 0.19839298725128174
train_iter_loss: 0.2007022500038147
train_iter_loss: 0.20796675980091095
train_iter_loss: 0.20572225749492645
train_iter_loss: 0.10802645981311798
train_iter_loss: 0.2302442491054535
train_iter_loss: 0.22332988679409027
train_iter_loss: 0.20117919147014618
train_iter_loss: 0.12490170449018478
train_iter_loss: 0.12371888011693954
train_iter_loss: 0.1937696635723114
train_iter_loss: 0.20474882423877716
train_iter_loss: 0.10553476214408875
train_iter_loss: 0.192168727517128
train_iter_loss: 0.13865065574645996
train_iter_loss: 0.144484743475914
train_iter_loss: 0.23897267878055573
train_iter_loss: 0.053898606449365616
train_iter_loss: 0.16978013515472412
train_iter_loss: 0.14988963305950165
train_iter_loss: 0.15398822724819183
train_iter_loss: 0.2060961127281189
train_iter_loss: 0.17808006703853607
train_iter_loss: 0.23572398722171783
train_iter_loss: 0.1557401716709137
train_iter_loss: 0.26268693804740906
train_iter_loss: 0.24043108522891998
train_iter_loss: 0.18719683587551117
train_iter_loss: 0.1778831034898758
train_iter_loss: 0.1813971847295761
train_iter_loss: 0.2687211036682129
train_iter_loss: 0.1052359789609909
train_iter_loss: 0.11749518662691116
train_iter_loss: 0.40077707171440125
train_iter_loss: 0.14326387643814087
train_iter_loss: 0.1548532396554947
train loss :0.1919
---------------------
Validation seg loss: 0.23026068796807864 at epoch 67
epoch =     68/  1000, exp = train
train_iter_loss: 0.14051103591918945
train_iter_loss: 0.1446758359670639
train_iter_loss: 0.1531965434551239
train_iter_loss: 0.1533513069152832
train_iter_loss: 0.17571453750133514
train_iter_loss: 0.15032877027988434
train_iter_loss: 0.10088610649108887
train_iter_loss: 0.22738014161586761
train_iter_loss: 0.16588714718818665
train_iter_loss: 0.14152328670024872
train_iter_loss: 0.12530149519443512
train_iter_loss: 0.31376391649246216
train_iter_loss: 0.2599082291126251
train_iter_loss: 0.14401249587535858
train_iter_loss: 0.18718785047531128
train_iter_loss: 0.25543686747550964
train_iter_loss: 0.25704675912857056
train_iter_loss: 0.2960147261619568
train_iter_loss: 0.1656617969274521
train_iter_loss: 0.12452605366706848
train_iter_loss: 0.274883896112442
train_iter_loss: 0.15986678004264832
train_iter_loss: 0.0978330597281456
train_iter_loss: 0.0806574672460556
train_iter_loss: 0.3285561800003052
train_iter_loss: 0.1836012750864029
train_iter_loss: 0.08903863281011581
train_iter_loss: 0.38407063484191895
train_iter_loss: 0.46639567613601685
train_iter_loss: 0.10665897279977798
train_iter_loss: 0.1640995740890503
train_iter_loss: 0.23515965044498444
train_iter_loss: 0.19364993274211884
train_iter_loss: 0.147373765707016
train_iter_loss: 0.13096657395362854
train_iter_loss: 0.2031850814819336
train_iter_loss: 0.21837621927261353
train_iter_loss: 0.12963493168354034
train_iter_loss: 0.1702008992433548
train_iter_loss: 0.18096120655536652
train_iter_loss: 0.2871975302696228
train_iter_loss: 0.14047956466674805
train_iter_loss: 0.17303353548049927
train_iter_loss: 0.13173630833625793
train_iter_loss: 0.11929970234632492
train_iter_loss: 0.195585235953331
train_iter_loss: 0.23766936361789703
train_iter_loss: 0.30995872616767883
train_iter_loss: 0.21353046596050262
train_iter_loss: 0.28400662541389465
train_iter_loss: 0.19718098640441895
train_iter_loss: 0.14769043028354645
train_iter_loss: 0.1667383313179016
train_iter_loss: 0.2835354208946228
train_iter_loss: 0.1599540412425995
train_iter_loss: 0.20939776301383972
train_iter_loss: 0.2446814328432083
train_iter_loss: 0.09600193053483963
train_iter_loss: 0.21106721460819244
train_iter_loss: 0.18122532963752747
train_iter_loss: 0.1833355873823166
train_iter_loss: 0.1043928861618042
train_iter_loss: 0.10056295245885849
train_iter_loss: 0.17546772956848145
train_iter_loss: 0.18144673109054565
train_iter_loss: 0.206094428896904
train_iter_loss: 0.16460514068603516
train_iter_loss: 0.16137470304965973
train_iter_loss: 0.17701172828674316
train_iter_loss: 0.20286856591701508
train_iter_loss: 0.19662462174892426
train_iter_loss: 0.15276232361793518
train_iter_loss: 0.21758192777633667
train_iter_loss: 0.21667654812335968
train_iter_loss: 0.13385891914367676
train_iter_loss: 0.14972932636737823
train_iter_loss: 0.09496937692165375
train_iter_loss: 0.1462879180908203
train_iter_loss: 0.13348668813705444
train_iter_loss: 0.20333482325077057
train_iter_loss: 0.19917482137680054
train_iter_loss: 0.13503201305866241
train_iter_loss: 0.1512046754360199
train_iter_loss: 0.1679079532623291
train_iter_loss: 0.10359704494476318
train_iter_loss: 0.1317717432975769
train_iter_loss: 0.243249773979187
train_iter_loss: 0.26752427220344543
train_iter_loss: 0.08044575899839401
train_iter_loss: 0.14937666058540344
train_iter_loss: 0.2443084418773651
train_iter_loss: 0.19478395581245422
train_iter_loss: 0.19853201508522034
train_iter_loss: 0.20015786588191986
train_iter_loss: 0.07792246341705322
train_iter_loss: 0.4998568892478943
train_iter_loss: 0.12174849957227707
train_iter_loss: 0.12576456367969513
train_iter_loss: 0.139074444770813
train_iter_loss: 0.3096800744533539
train loss :0.1885
---------------------
Validation seg loss: 0.22708036794485348 at epoch 68
********************
best_val_epoch_loss:  0.22708036794485348
MODEL UPDATED
epoch =     69/  1000, exp = train
train_iter_loss: 0.24194179475307465
train_iter_loss: 0.1724788248538971
train_iter_loss: 0.15458375215530396
train_iter_loss: 0.24470700323581696
train_iter_loss: 0.11947719752788544
train_iter_loss: 0.16655096411705017
train_iter_loss: 0.2346028834581375
train_iter_loss: 0.20256762206554413
train_iter_loss: 0.25099149346351624
train_iter_loss: 0.2412821352481842
train_iter_loss: 0.1752299964427948
train_iter_loss: 0.09385689347982407
train_iter_loss: 0.1598503291606903
train_iter_loss: 0.09454386681318283
train_iter_loss: 0.23115046322345734
train_iter_loss: 0.12899507582187653
train_iter_loss: 0.2106069028377533
train_iter_loss: 0.11450227349996567
train_iter_loss: 0.147390678524971
train_iter_loss: 0.21281090378761292
train_iter_loss: 0.17781591415405273
train_iter_loss: 0.24131594598293304
train_iter_loss: 0.1744038313627243
train_iter_loss: 0.23536132276058197
train_iter_loss: 0.11845183372497559
train_iter_loss: 0.19083838164806366
train_iter_loss: 0.16814973950386047
train_iter_loss: 0.1900179386138916
train_iter_loss: 0.2989015579223633
train_iter_loss: 0.2461831122636795
train_iter_loss: 0.16457176208496094
train_iter_loss: 0.19277018308639526
train_iter_loss: 0.22546344995498657
train_iter_loss: 0.2634648084640503
train_iter_loss: 0.13009656965732574
train_iter_loss: 0.07426376640796661
train_iter_loss: 0.29216018319129944
train_iter_loss: 0.18984940648078918
train_iter_loss: 0.20173001289367676
train_iter_loss: 0.3463861346244812
train_iter_loss: 0.06829044222831726
train_iter_loss: 0.13902530074119568
train_iter_loss: 0.38564226031303406
train_iter_loss: 0.1428339183330536
train_iter_loss: 0.20548057556152344
train_iter_loss: 0.20507711172103882
train_iter_loss: 0.09205999970436096
train_iter_loss: 0.13377386331558228
train_iter_loss: 0.30941832065582275
train_iter_loss: 0.06848874688148499
train_iter_loss: 0.31045806407928467
train_iter_loss: 0.22280320525169373
train_iter_loss: 0.09958121180534363
train_iter_loss: 0.40274956822395325
train_iter_loss: 0.12855258584022522
train_iter_loss: 0.2000093013048172
train_iter_loss: 0.16915573179721832
train_iter_loss: 0.06705201417207718
train_iter_loss: 0.14849531650543213
train_iter_loss: 0.21285562217235565
train_iter_loss: 0.09697091579437256
train_iter_loss: 0.13053162395954132
train_iter_loss: 0.20027007162570953
train_iter_loss: 0.23726509511470795
train_iter_loss: 0.18542525172233582
train_iter_loss: 0.1769254356622696
train_iter_loss: 0.24680891633033752
train_iter_loss: 0.2025594413280487
train_iter_loss: 0.262495756149292
train_iter_loss: 0.2853334844112396
train_iter_loss: 0.2055526226758957
train_iter_loss: 0.27341628074645996
train_iter_loss: 0.11884674429893494
train_iter_loss: 0.17792633175849915
train_iter_loss: 0.18349497020244598
train_iter_loss: 0.34812214970588684
train_iter_loss: 0.17999136447906494
train_iter_loss: 0.19692189991474152
train_iter_loss: 0.21185921132564545
train_iter_loss: 0.317491739988327
train_iter_loss: 0.19852635264396667
train_iter_loss: 0.12288934737443924
train_iter_loss: 0.3512231707572937
train_iter_loss: 0.19049617648124695
train_iter_loss: 0.2172500491142273
train_iter_loss: 0.18261167407035828
train_iter_loss: 0.23540598154067993
train_iter_loss: 0.30841225385665894
train_iter_loss: 0.26050665974617004
train_iter_loss: 0.16916540265083313
train_iter_loss: 0.08668363839387894
train_iter_loss: 0.17027167975902557
train_iter_loss: 0.19254069030284882
train_iter_loss: 0.1506669521331787
train_iter_loss: 0.23491507768630981
train_iter_loss: 0.2410118579864502
train_iter_loss: 0.22641608119010925
train_iter_loss: 0.20272159576416016
train_iter_loss: 0.20678165555000305
train_iter_loss: 0.16875314712524414
train loss :0.1998
---------------------
Validation seg loss: 0.22911311939077558 at epoch 69
epoch =     70/  1000, exp = train
train_iter_loss: 0.24922119081020355
train_iter_loss: 0.17897552251815796
train_iter_loss: 0.30048298835754395
train_iter_loss: 0.21355123817920685
train_iter_loss: 0.15654848515987396
train_iter_loss: 0.1695653647184372
train_iter_loss: 0.18578940629959106
train_iter_loss: 0.16746097803115845
train_iter_loss: 0.21260856091976166
train_iter_loss: 0.2420026659965515
train_iter_loss: 0.16742797195911407
train_iter_loss: 0.19426031410694122
train_iter_loss: 0.19329772889614105
train_iter_loss: 0.24267977476119995
train_iter_loss: 0.16100789606571198
train_iter_loss: 0.16955240070819855
train_iter_loss: 0.3029939830303192
train_iter_loss: 0.24800051748752594
train_iter_loss: 0.17439289391040802
train_iter_loss: 0.25603556632995605
train_iter_loss: 0.10008501261472702
train_iter_loss: 0.2341839224100113
train_iter_loss: 0.18918219208717346
train_iter_loss: 0.18379759788513184
train_iter_loss: 0.19607460498809814
train_iter_loss: 0.12015917897224426
train_iter_loss: 0.3433874249458313
train_iter_loss: 0.25774839520454407
train_iter_loss: 0.14010275900363922
train_iter_loss: 0.1686086505651474
train_iter_loss: 0.29561036825180054
train_iter_loss: 0.16699866950511932
train_iter_loss: 0.14670203626155853
train_iter_loss: 0.18061606585979462
train_iter_loss: 0.2426026314496994
train_iter_loss: 0.19816721975803375
train_iter_loss: 0.1360333263874054
train_iter_loss: 0.18045902252197266
train_iter_loss: 0.11692753434181213
train_iter_loss: 0.2003210186958313
train_iter_loss: 0.29857704043388367
train_iter_loss: 0.2513074278831482
train_iter_loss: 0.1121988371014595
train_iter_loss: 0.18401172757148743
train_iter_loss: 0.24019157886505127
train_iter_loss: 0.17083418369293213
train_iter_loss: 0.12986283004283905
train_iter_loss: 0.17444948852062225
train_iter_loss: 0.1966993808746338
train_iter_loss: 0.19737015664577484
train_iter_loss: 0.313523530960083
train_iter_loss: 0.18038804829120636
train_iter_loss: 0.15686675906181335
train_iter_loss: 0.23172101378440857
train_iter_loss: 0.16189679503440857
train_iter_loss: 0.17789241671562195
train_iter_loss: 0.2176903337240219
train_iter_loss: 0.13990500569343567
train_iter_loss: 0.23398840427398682
train_iter_loss: 0.143968865275383
train_iter_loss: 0.2805413603782654
train_iter_loss: 0.20689864456653595
train_iter_loss: 0.14091728627681732
train_iter_loss: 0.09933321177959442
train_iter_loss: 0.12802372872829437
train_iter_loss: 0.16589251160621643
train_iter_loss: 0.17883573472499847
train_iter_loss: 0.12661556899547577
train_iter_loss: 0.260032594203949
train_iter_loss: 0.15559910237789154
train_iter_loss: 0.1651972234249115
train_iter_loss: 0.24692697823047638
train_iter_loss: 0.19955407083034515
train_iter_loss: 0.10892415791749954
train_iter_loss: 0.15425346791744232
train_iter_loss: 0.21595104038715363
train_iter_loss: 0.16789548099040985
train_iter_loss: 0.1448822021484375
train_iter_loss: 0.1736159324645996
train_iter_loss: 0.22759389877319336
train_iter_loss: 0.09412893652915955
train_iter_loss: 0.20421984791755676
train_iter_loss: 0.17264802753925323
train_iter_loss: 0.10111671686172485
train_iter_loss: 0.14678020775318146
train_iter_loss: 0.3948099613189697
train_iter_loss: 0.1447681486606598
train_iter_loss: 0.16604086756706238
train_iter_loss: 0.26038065552711487
train_iter_loss: 0.24112878739833832
train_iter_loss: 0.22324587404727936
train_iter_loss: 0.16518840193748474
train_iter_loss: 0.15879610180854797
train_iter_loss: 0.18510906398296356
train_iter_loss: 0.17239999771118164
train_iter_loss: 0.18987488746643066
train_iter_loss: 0.20505300164222717
train_iter_loss: 0.16450434923171997
train_iter_loss: 0.19002848863601685
train_iter_loss: 0.2461746782064438
train loss :0.1947
---------------------
Validation seg loss: 0.22965234180666366 at epoch 70
epoch =     71/  1000, exp = train
train_iter_loss: 0.08862181752920151
train_iter_loss: 0.18360461294651031
train_iter_loss: 0.20854267477989197
train_iter_loss: 0.20981591939926147
train_iter_loss: 0.15531541407108307
train_iter_loss: 0.06319444626569748
train_iter_loss: 0.16126558184623718
train_iter_loss: 0.21323943138122559
train_iter_loss: 0.17869889736175537
train_iter_loss: 0.16135427355766296
train_iter_loss: 0.2757936120033264
train_iter_loss: 0.1648150086402893
train_iter_loss: 0.18084841966629028
train_iter_loss: 0.2626287043094635
train_iter_loss: 0.13271665573120117
train_iter_loss: 0.21620817482471466
train_iter_loss: 0.22084113955497742
train_iter_loss: 0.279136061668396
train_iter_loss: 0.20825077593326569
train_iter_loss: 0.15094350278377533
train_iter_loss: 0.16116692125797272
train_iter_loss: 0.1295870542526245
train_iter_loss: 0.21855118870735168
train_iter_loss: 0.2844395935535431
train_iter_loss: 0.13995468616485596
train_iter_loss: 0.13580136001110077
train_iter_loss: 0.19116035103797913
train_iter_loss: 0.17114131152629852
train_iter_loss: 0.16255292296409607
train_iter_loss: 0.0916299894452095
train_iter_loss: 0.17164938151836395
train_iter_loss: 0.14636445045471191
train_iter_loss: 0.25201287865638733
train_iter_loss: 0.21651051938533783
train_iter_loss: 0.2267458736896515
train_iter_loss: 0.08868230879306793
train_iter_loss: 0.1705344319343567
train_iter_loss: 0.14142896234989166
train_iter_loss: 0.28576549887657166
train_iter_loss: 0.13355596363544464
train_iter_loss: 0.22330299019813538
train_iter_loss: 0.24931979179382324
train_iter_loss: 0.16722118854522705
train_iter_loss: 0.1261749118566513
train_iter_loss: 0.0809032991528511
train_iter_loss: 0.24152399599552155
train_iter_loss: 0.16021513938903809
train_iter_loss: 0.1981714814901352
train_iter_loss: 0.24886800348758698
train_iter_loss: 0.25408023595809937
train_iter_loss: 0.24792993068695068
train_iter_loss: 0.24560017883777618
train_iter_loss: 0.1896848827600479
train_iter_loss: 0.18382246792316437
train_iter_loss: 0.19746866822242737
train_iter_loss: 0.1273508071899414
train_iter_loss: 0.11218178272247314
train_iter_loss: 0.20117545127868652
train_iter_loss: 0.29133957624435425
train_iter_loss: 0.06963828951120377
train_iter_loss: 0.23665346205234528
train_iter_loss: 0.14236263930797577
train_iter_loss: 0.10140106827020645
train_iter_loss: 0.19091066718101501
train_iter_loss: 0.20212876796722412
train_iter_loss: 0.2747132182121277
train_iter_loss: 0.2586172819137573
train_iter_loss: 0.29082486033439636
train_iter_loss: 0.1920616626739502
train_iter_loss: 0.19240424036979675
train_iter_loss: 0.1536615937948227
train_iter_loss: 0.1868017464876175
train_iter_loss: 0.4509759247303009
train_iter_loss: 0.21506452560424805
train_iter_loss: 0.15727785229682922
train_iter_loss: 0.2943611145019531
train_iter_loss: 0.1256229430437088
train_iter_loss: 0.05482436344027519
train_iter_loss: 0.17391566932201385
train_iter_loss: 0.1761661022901535
train_iter_loss: 0.1887081116437912
train_iter_loss: 0.09773027896881104
train_iter_loss: 0.17692947387695312
train_iter_loss: 0.20295751094818115
train_iter_loss: 0.2147660106420517
train_iter_loss: 0.17312008142471313
train_iter_loss: 0.21465575695037842
train_iter_loss: 0.2051486372947693
train_iter_loss: 0.2207905501127243
train_iter_loss: 0.29833298921585083
train_iter_loss: 0.1166519969701767
train_iter_loss: 0.09317340701818466
train_iter_loss: 0.0888141319155693
train_iter_loss: 0.19308319687843323
train_iter_loss: 0.23732124269008636
train_iter_loss: 0.21783091127872467
train_iter_loss: 0.23447319865226746
train_iter_loss: 0.17097730934619904
train_iter_loss: 0.22440530359745026
train_iter_loss: 0.098223015666008
train loss :0.1898
---------------------
Validation seg loss: 0.23050107527524233 at epoch 71
epoch =     72/  1000, exp = train
train_iter_loss: 0.16239750385284424
train_iter_loss: 0.12850655615329742
train_iter_loss: 0.18771034479141235
train_iter_loss: 0.13176803290843964
train_iter_loss: 0.18107539415359497
train_iter_loss: 0.18950480222702026
train_iter_loss: 0.20247243344783783
train_iter_loss: 0.12469106167554855
train_iter_loss: 0.24834057688713074
train_iter_loss: 0.11651313304901123
train_iter_loss: 0.13160492479801178
train_iter_loss: 0.09619596600532532
train_iter_loss: 0.16164156794548035
train_iter_loss: 0.12273354828357697
train_iter_loss: 0.13333608210086823
train_iter_loss: 0.17435768246650696
train_iter_loss: 0.2939065098762512
train_iter_loss: 0.1483384221792221
train_iter_loss: 0.1756613552570343
train_iter_loss: 0.19114495813846588
train_iter_loss: 0.2896117866039276
train_iter_loss: 0.24655240774154663
train_iter_loss: 0.11401380598545074
train_iter_loss: 0.13475841283798218
train_iter_loss: 0.2338426560163498
train_iter_loss: 0.12767089903354645
train_iter_loss: 0.12585167586803436
train_iter_loss: 0.1734314113855362
train_iter_loss: 0.1537710279226303
train_iter_loss: 0.16101709008216858
train_iter_loss: 0.24058672785758972
train_iter_loss: 0.18998365104198456
train_iter_loss: 0.15557993948459625
train_iter_loss: 0.2876051664352417
train_iter_loss: 0.2094053477048874
train_iter_loss: 0.09922455251216888
train_iter_loss: 0.16165608167648315
train_iter_loss: 0.16141830384731293
train_iter_loss: 0.19011762738227844
train_iter_loss: 0.1314857453107834
train_iter_loss: 0.23019421100616455
train_iter_loss: 0.13055899739265442
train_iter_loss: 0.173750638961792
train_iter_loss: 0.09607964754104614
train_iter_loss: 0.21914124488830566
train_iter_loss: 0.1408015489578247
train_iter_loss: 0.20013900101184845
train_iter_loss: 0.2294497936964035
train_iter_loss: 0.27842292189598083
train_iter_loss: 0.16652938723564148
train_iter_loss: 0.30848950147628784
train_iter_loss: 0.21227848529815674
train_iter_loss: 0.21636703610420227
train_iter_loss: 0.10875213146209717
train_iter_loss: 0.2179492712020874
train_iter_loss: 0.19394844770431519
train_iter_loss: 0.303326815366745
train_iter_loss: 0.22977758944034576
train_iter_loss: 0.2088555246591568
train_iter_loss: 0.234289288520813
train_iter_loss: 0.20673072338104248
train_iter_loss: 0.09279400110244751
train_iter_loss: 0.2021445631980896
train_iter_loss: 0.17847731709480286
train_iter_loss: 0.134559765458107
train_iter_loss: 0.17348161339759827
train_iter_loss: 0.1906910091638565
train_iter_loss: 0.1540570855140686
train_iter_loss: 0.2392263114452362
train_iter_loss: 0.16701221466064453
train_iter_loss: 0.11073961108922958
train_iter_loss: 0.2084447145462036
train_iter_loss: 0.27979737520217896
train_iter_loss: 0.08922312408685684
train_iter_loss: 0.14360353350639343
train_iter_loss: 0.13574539124965668
train_iter_loss: 0.18501727283000946
train_iter_loss: 0.1729763299226761
train_iter_loss: 0.219072163105011
train_iter_loss: 0.23378267884254456
train_iter_loss: 0.13337184488773346
train_iter_loss: 0.17987044155597687
train_iter_loss: 0.24778346717357635
train_iter_loss: 0.10312097519636154
train_iter_loss: 0.26149433851242065
train_iter_loss: 0.22306761145591736
train_iter_loss: 0.1891237199306488
train_iter_loss: 0.1982123851776123
train_iter_loss: 0.18555361032485962
train_iter_loss: 0.14156389236450195
train_iter_loss: 0.20949703454971313
train_iter_loss: 0.13806362450122833
train_iter_loss: 0.27905723452568054
train_iter_loss: 0.10098135471343994
train_iter_loss: 0.2881210446357727
train_iter_loss: 0.14277780055999756
train_iter_loss: 0.19185307621955872
train_iter_loss: 0.20828542113304138
train_iter_loss: 0.21805430948734283
train_iter_loss: 0.19331592321395874
train loss :0.1853
---------------------
Validation seg loss: 0.228826953537481 at epoch 72
epoch =     73/  1000, exp = train
train_iter_loss: 0.13554780185222626
train_iter_loss: 0.23651614785194397
train_iter_loss: 0.32091841101646423
train_iter_loss: 0.22562070190906525
train_iter_loss: 0.21745912730693817
train_iter_loss: 0.20973347127437592
train_iter_loss: 0.08345162123441696
train_iter_loss: 0.12024969607591629
train_iter_loss: 0.20188835263252258
train_iter_loss: 0.1800239235162735
train_iter_loss: 0.1656092405319214
train_iter_loss: 0.22327493131160736
train_iter_loss: 0.2135416567325592
train_iter_loss: 0.09224855154752731
train_iter_loss: 0.2694680094718933
train_iter_loss: 0.23906628787517548
train_iter_loss: 0.1929657906293869
train_iter_loss: 0.1455155611038208
train_iter_loss: 0.22115278244018555
train_iter_loss: 0.30262911319732666
train_iter_loss: 0.21425707638263702
train_iter_loss: 0.1354512870311737
train_iter_loss: 0.24558942019939423
train_iter_loss: 0.2880442142486572
train_iter_loss: 0.21384945511817932
train_iter_loss: 0.10442394018173218
train_iter_loss: 0.14585286378860474
train_iter_loss: 0.17928335070610046
train_iter_loss: 0.17495575547218323
train_iter_loss: 0.14042043685913086
train_iter_loss: 0.16589823365211487
train_iter_loss: 0.18215379118919373
train_iter_loss: 0.14669938385486603
train_iter_loss: 0.14043369889259338
train_iter_loss: 0.14922289550304413
train_iter_loss: 0.14271560311317444
train_iter_loss: 0.19451524317264557
train_iter_loss: 0.15935924649238586
train_iter_loss: 0.12685146927833557
train_iter_loss: 0.26168909668922424
train_iter_loss: 0.2575199604034424
train_iter_loss: 0.12491948902606964
train_iter_loss: 0.19330386817455292
train_iter_loss: 0.2042878419160843
train_iter_loss: 0.18689250946044922
train_iter_loss: 0.42572447657585144
train_iter_loss: 0.05078234523534775
train_iter_loss: 0.20549792051315308
train_iter_loss: 0.2117445319890976
train_iter_loss: 0.23598486185073853
train_iter_loss: 0.24217449128627777
train_iter_loss: 0.20623667538166046
train_iter_loss: 0.30507782101631165
train_iter_loss: 0.2690090537071228
train_iter_loss: 0.18717631697654724
train_iter_loss: 0.15082348883152008
train_iter_loss: 0.1670467108488083
train_iter_loss: 0.1693585067987442
train_iter_loss: 0.245536670088768
train_iter_loss: 0.10186073184013367
train_iter_loss: 0.1890929639339447
train_iter_loss: 0.13729755580425262
train_iter_loss: 0.19378916919231415
train_iter_loss: 0.16434800624847412
train_iter_loss: 0.1469338983297348
train_iter_loss: 0.19413749873638153
train_iter_loss: 0.14814837276935577
train_iter_loss: 0.12853698432445526
train_iter_loss: 0.19290122389793396
train_iter_loss: 0.13377167284488678
train_iter_loss: 0.221774622797966
train_iter_loss: 0.22697213292121887
train_iter_loss: 0.2137746661901474
train_iter_loss: 0.14482803642749786
train_iter_loss: 0.07054518163204193
train_iter_loss: 0.32528364658355713
train_iter_loss: 0.15221469104290009
train_iter_loss: 0.3322182297706604
train_iter_loss: 0.1509065181016922
train_iter_loss: 0.17442163825035095
train_iter_loss: 0.17921452224254608
train_iter_loss: 0.13301631808280945
train_iter_loss: 0.10680774599313736
train_iter_loss: 0.06199096888303757
train_iter_loss: 0.2568306028842926
train_iter_loss: 0.08159168809652328
train_iter_loss: 0.2088794708251953
train_iter_loss: 0.2943911552429199
train_iter_loss: 0.18251679837703705
train_iter_loss: 0.18702422082424164
train_iter_loss: 0.15397979319095612
train_iter_loss: 0.2321462482213974
train_iter_loss: 0.28594696521759033
train_iter_loss: 0.23572702705860138
train_iter_loss: 0.161520317196846
train_iter_loss: 0.28795453906059265
train_iter_loss: 0.20399267971515656
train_iter_loss: 0.1726336032152176
train_iter_loss: 0.27215588092803955
train_iter_loss: 0.22628624737262726
train loss :0.1940
---------------------
Validation seg loss: 0.22855101402778671 at epoch 73
epoch =     74/  1000, exp = train
train_iter_loss: 0.1798989474773407
train_iter_loss: 0.11469896882772446
train_iter_loss: 0.18457408249378204
train_iter_loss: 0.19135932624340057
train_iter_loss: 0.23510555922985077
train_iter_loss: 0.30458468198776245
train_iter_loss: 0.17741505801677704
train_iter_loss: 0.13492871820926666
train_iter_loss: 0.1512606143951416
train_iter_loss: 0.340911865234375
train_iter_loss: 0.10618000477552414
train_iter_loss: 0.11148621886968613
train_iter_loss: 0.1298532336950302
train_iter_loss: 0.1715589463710785
train_iter_loss: 0.2310331165790558
train_iter_loss: 0.32444244623184204
train_iter_loss: 0.19038844108581543
train_iter_loss: 0.2234811931848526
train_iter_loss: 0.15089085698127747
train_iter_loss: 0.1571192741394043
train_iter_loss: 0.19099003076553345
train_iter_loss: 0.09470119327306747
train_iter_loss: 0.17632322013378143
train_iter_loss: 0.20273937284946442
train_iter_loss: 0.1917247325181961
train_iter_loss: 0.21597960591316223
train_iter_loss: 0.10968606919050217
train_iter_loss: 0.24570472538471222
train_iter_loss: 0.12105373293161392
train_iter_loss: 0.177906334400177
train_iter_loss: 0.1996767371892929
train_iter_loss: 0.18520689010620117
train_iter_loss: 0.22597883641719818
train_iter_loss: 0.27931728959083557
train_iter_loss: 0.13187702000141144
train_iter_loss: 0.20398342609405518
train_iter_loss: 0.1768832802772522
train_iter_loss: 0.17147065699100494
train_iter_loss: 0.18845424056053162
train_iter_loss: 0.14190953969955444
train_iter_loss: 0.2837134301662445
train_iter_loss: 0.14181090891361237
train_iter_loss: 0.2021363228559494
train_iter_loss: 0.12976087629795074
train_iter_loss: 0.20992688834667206
train_iter_loss: 0.24233531951904297
train_iter_loss: 0.14922557771205902
train_iter_loss: 0.20812475681304932
train_iter_loss: 0.13414987921714783
train_iter_loss: 0.12480079382658005
train_iter_loss: 0.12550409138202667
train_iter_loss: 0.1941770613193512
train_iter_loss: 0.20188307762145996
train_iter_loss: 0.24569551646709442
train_iter_loss: 0.14101046323776245
train_iter_loss: 0.14079922437667847
train_iter_loss: 0.158815398812294
train_iter_loss: 0.21109339594841003
train_iter_loss: 0.18226312100887299
train_iter_loss: 0.3140057623386383
train_iter_loss: 0.1778898388147354
train_iter_loss: 0.213957279920578
train_iter_loss: 0.18204547464847565
train_iter_loss: 0.20997369289398193
train_iter_loss: 0.22852450609207153
train_iter_loss: 0.1861342042684555
train_iter_loss: 0.24554051458835602
train_iter_loss: 0.19612906873226166
train_iter_loss: 0.1650104969739914
train_iter_loss: 0.15305180847644806
train_iter_loss: 0.16068869829177856
train_iter_loss: 0.16338062286376953
train_iter_loss: 0.20273807644844055
train_iter_loss: 0.22028377652168274
train_iter_loss: 0.14970314502716064
train_iter_loss: 0.1826515644788742
train_iter_loss: 0.03259342163801193
train_iter_loss: 0.1812930554151535
train_iter_loss: 0.23450592160224915
train_iter_loss: 0.16157878935337067
train_iter_loss: 0.1831461489200592
train_iter_loss: 0.14858685433864594
train_iter_loss: 0.15934622287750244
train_iter_loss: 0.16095557808876038
train_iter_loss: 0.18474389612674713
train_iter_loss: 0.20374451577663422
train_iter_loss: 0.18961943686008453
train_iter_loss: 0.20100954174995422
train_iter_loss: 0.11690171808004379
train_iter_loss: 0.16265729069709778
train_iter_loss: 0.22541095316410065
train_iter_loss: 0.23487506806850433
train_iter_loss: 0.13787195086479187
train_iter_loss: 0.2756502330303192
train_iter_loss: 0.17074759304523468
train_iter_loss: 0.16032755374908447
train_iter_loss: 0.15644404292106628
train_iter_loss: 0.21616670489311218
train_iter_loss: 0.14585721492767334
train_iter_loss: 0.2378135323524475
train loss :0.1870
---------------------
Validation seg loss: 0.22860851261835052 at epoch 74
epoch =     75/  1000, exp = train
train_iter_loss: 0.17820747196674347
train_iter_loss: 0.06395810097455978
train_iter_loss: 0.10853305459022522
train_iter_loss: 0.24387232959270477
train_iter_loss: 0.14684484899044037
train_iter_loss: 0.2501710057258606
train_iter_loss: 0.2841542363166809
train_iter_loss: 0.24242877960205078
train_iter_loss: 0.20126067101955414
train_iter_loss: 0.10075866430997849
train_iter_loss: 0.15418171882629395
train_iter_loss: 0.11268618702888489
train_iter_loss: 0.24279364943504333
train_iter_loss: 0.161118283867836
train_iter_loss: 0.22479680180549622
train_iter_loss: 0.2331608533859253
train_iter_loss: 0.22733943164348602
train_iter_loss: 0.18567240238189697
train_iter_loss: 0.17559939622879028
train_iter_loss: 0.1732935905456543
train_iter_loss: 0.20470914244651794
train_iter_loss: 0.1480218768119812
train_iter_loss: 0.16726252436637878
train_iter_loss: 0.18715114891529083
train_iter_loss: 0.23383165895938873
train_iter_loss: 0.08497443050146103
train_iter_loss: 0.29607000946998596
train_iter_loss: 0.20798516273498535
train_iter_loss: 0.22579243779182434
train_iter_loss: 0.21403591334819794
train_iter_loss: 0.23796305060386658
train_iter_loss: 0.2276347577571869
train_iter_loss: 0.2193337231874466
train_iter_loss: 0.29211586713790894
train_iter_loss: 0.17353567481040955
train_iter_loss: 0.19739685952663422
train_iter_loss: 0.21514752507209778
train_iter_loss: 0.14013540744781494
train_iter_loss: 0.1978558450937271
train_iter_loss: 0.1653863936662674
train_iter_loss: 0.23959019780158997
train_iter_loss: 0.16865913569927216
train_iter_loss: 0.26049527525901794
train_iter_loss: 0.13747470080852509
train_iter_loss: 0.09953287988901138
train_iter_loss: 0.2224135547876358
train_iter_loss: 0.2052009403705597
train_iter_loss: 0.11886781454086304
train_iter_loss: 0.13787002861499786
train_iter_loss: 0.23836465179920197
train_iter_loss: 0.10160425305366516
train_iter_loss: 0.1619807928800583
train_iter_loss: 0.15125367045402527
train_iter_loss: 0.18224191665649414
train_iter_loss: 0.13310503959655762
train_iter_loss: 0.1773548573255539
train_iter_loss: 0.14178761839866638
train_iter_loss: 0.1899067759513855
train_iter_loss: 0.08179908245801926
train_iter_loss: 0.18971195816993713
train_iter_loss: 0.18910974264144897
train_iter_loss: 0.19891458749771118
train_iter_loss: 0.10635785758495331
train_iter_loss: 0.22337563335895538
train_iter_loss: 0.13564874231815338
train_iter_loss: 0.11375982314348221
train_iter_loss: 0.1555289477109909
train_iter_loss: 0.19151344895362854
train_iter_loss: 0.1917533576488495
train_iter_loss: 0.14852076768875122
train_iter_loss: 0.17588628828525543
train_iter_loss: 0.2527809739112854
train_iter_loss: 0.23953714966773987
train_iter_loss: 0.11010300368070602
train_iter_loss: 0.2640511691570282
train_iter_loss: 0.1479872614145279
train_iter_loss: 0.13445773720741272
train_iter_loss: 0.20747482776641846
train_iter_loss: 0.21053680777549744
train_iter_loss: 0.20281343162059784
train_iter_loss: 0.08556673675775528
train_iter_loss: 0.18265999853610992
train_iter_loss: 0.1198577880859375
train_iter_loss: 0.17183269560337067
train_iter_loss: 0.14689815044403076
train_iter_loss: 0.2063388079404831
train_iter_loss: 0.25760161876678467
train_iter_loss: 0.27649450302124023
train_iter_loss: 0.23110835254192352
train_iter_loss: 0.20894518494606018
train_iter_loss: 0.11257569491863251
train_iter_loss: 0.2663453221321106
train_iter_loss: 0.14927460253238678
train_iter_loss: 0.20994524657726288
train_iter_loss: 0.14333641529083252
train_iter_loss: 0.2993921935558319
train_iter_loss: 0.1484750658273697
train_iter_loss: 0.21722717583179474
train_iter_loss: 0.18869128823280334
train_iter_loss: 0.1666218340396881
train loss :0.1866
---------------------
Validation seg loss: 0.22715179413065034 at epoch 75
epoch =     76/  1000, exp = train
train_iter_loss: 0.1676519364118576
train_iter_loss: 0.16264383494853973
train_iter_loss: 0.15640157461166382
train_iter_loss: 0.16168442368507385
train_iter_loss: 0.21437104046344757
train_iter_loss: 0.1696680337190628
train_iter_loss: 0.16892461478710175
train_iter_loss: 0.20659255981445312
train_iter_loss: 0.1400214582681656
train_iter_loss: 0.12131280452013016
train_iter_loss: 0.34620481729507446
train_iter_loss: 0.18567128479480743
train_iter_loss: 0.17883707582950592
train_iter_loss: 0.1097734123468399
train_iter_loss: 0.24342338740825653
train_iter_loss: 0.2919471859931946
train_iter_loss: 0.28750792145729065
train_iter_loss: 0.2415672093629837
train_iter_loss: 0.16472727060317993
train_iter_loss: 0.26495516300201416
train_iter_loss: 0.19191385805606842
train_iter_loss: 0.15580153465270996
train_iter_loss: 0.21408990025520325
train_iter_loss: 0.12424379587173462
train_iter_loss: 0.3722239136695862
train_iter_loss: 0.12301862239837646
train_iter_loss: 0.2274617701768875
train_iter_loss: 0.1556296944618225
train_iter_loss: 0.16620907187461853
train_iter_loss: 0.13578738272190094
train_iter_loss: 0.17023853957653046
train_iter_loss: 0.13946020603179932
train_iter_loss: 0.17669038474559784
train_iter_loss: 0.21614554524421692
train_iter_loss: 0.1441524624824524
train_iter_loss: 0.4617179334163666
train_iter_loss: 0.1944894939661026
train_iter_loss: 0.18393120169639587
train_iter_loss: 0.23016808927059174
train_iter_loss: 0.1618298441171646
train_iter_loss: 0.26808446645736694
train_iter_loss: 0.1608358770608902
train_iter_loss: 0.1843528002500534
train_iter_loss: 0.17602472007274628
train_iter_loss: 0.06224105507135391
train_iter_loss: 0.11623921245336533
train_iter_loss: 0.12296224385499954
train_iter_loss: 0.19155330955982208
train_iter_loss: 0.15104016661643982
train_iter_loss: 0.21026070415973663
train_iter_loss: 0.26085126399993896
train_iter_loss: 0.17096197605133057
train_iter_loss: 0.2610349953174591
train_iter_loss: 0.16580557823181152
train_iter_loss: 0.18034842610359192
train_iter_loss: 0.2072063386440277
train_iter_loss: 0.130062535405159
train_iter_loss: 0.14003287255764008
train_iter_loss: 0.09881448745727539
train_iter_loss: 0.11306377500295639
train_iter_loss: 0.20486058294773102
train_iter_loss: 0.14765380322933197
train_iter_loss: 0.1252528429031372
train_iter_loss: 0.2308684140443802
train_iter_loss: 0.19302761554718018
train_iter_loss: 0.11494211852550507
train_iter_loss: 0.13707762956619263
train_iter_loss: 0.15985852479934692
train_iter_loss: 0.20510132610797882
train_iter_loss: 0.15479528903961182
train_iter_loss: 0.15204471349716187
train_iter_loss: 0.36075931787490845
train_iter_loss: 0.1918417364358902
train_iter_loss: 0.12055975943803787
train_iter_loss: 0.14609748125076294
train_iter_loss: 0.17714357376098633
train_iter_loss: 0.11751866340637207
train_iter_loss: 0.11951980739831924
train_iter_loss: 0.22353701293468475
train_iter_loss: 0.1401856690645218
train_iter_loss: 0.37122398614883423
train_iter_loss: 0.22581332921981812
train_iter_loss: 0.15481816232204437
train_iter_loss: 0.10184729099273682
train_iter_loss: 0.20050592720508575
train_iter_loss: 0.2113594263792038
train_iter_loss: 0.14315788447856903
train_iter_loss: 0.21801568567752838
train_iter_loss: 0.2168690264225006
train_iter_loss: 0.16449899971485138
train_iter_loss: 0.3343171775341034
train_iter_loss: 0.23275180160999298
train_iter_loss: 0.15094996988773346
train_iter_loss: 0.2577895522117615
train_iter_loss: 0.10027384757995605
train_iter_loss: 0.18954549729824066
train_iter_loss: 0.13630440831184387
train_iter_loss: 0.17491282522678375
train_iter_loss: 0.19589585065841675
train_iter_loss: 0.138026624917984
train loss :0.1883
---------------------
Validation seg loss: 0.22694217134267092 at epoch 76
********************
best_val_epoch_loss:  0.22694217134267092
MODEL UPDATED
epoch =     77/  1000, exp = train
train_iter_loss: 0.18829266726970673
train_iter_loss: 0.10372183471918106
train_iter_loss: 0.11448327451944351
train_iter_loss: 0.18871471285820007
train_iter_loss: 0.265124648809433
train_iter_loss: 0.26897671818733215
train_iter_loss: 0.31560251116752625
train_iter_loss: 0.13015399873256683
train_iter_loss: 0.13733524084091187
train_iter_loss: 0.05580807104706764
train_iter_loss: 0.16686701774597168
train_iter_loss: 0.16638538241386414
train_iter_loss: 0.15149420499801636
train_iter_loss: 0.2565079927444458
train_iter_loss: 0.1561993956565857
train_iter_loss: 0.2435569167137146
train_iter_loss: 0.18845337629318237
train_iter_loss: 0.22873476147651672
train_iter_loss: 0.19250373542308807
train_iter_loss: 0.19445644319057465
train_iter_loss: 0.11248547583818436
train_iter_loss: 0.205118328332901
train_iter_loss: 0.16764242947101593
train_iter_loss: 0.18672826886177063
train_iter_loss: 0.15622840821743011
train_iter_loss: 0.14672215282917023
train_iter_loss: 0.2613781988620758
train_iter_loss: 0.2270698845386505
train_iter_loss: 0.208333358168602
train_iter_loss: 0.3326728641986847
train_iter_loss: 0.3163251578807831
train_iter_loss: 0.1932675838470459
train_iter_loss: 0.07342847436666489
train_iter_loss: 0.20678219199180603
train_iter_loss: 0.1545281708240509
train_iter_loss: 0.26038238406181335
train_iter_loss: 0.18769721686840057
train_iter_loss: 0.08907020092010498
train_iter_loss: 0.06098309904336929
train_iter_loss: 0.25171852111816406
train_iter_loss: 0.26762107014656067
train_iter_loss: 0.09871137142181396
train_iter_loss: 0.1654054820537567
train_iter_loss: 0.21027180552482605
train_iter_loss: 0.1846296638250351
train_iter_loss: 0.11931049823760986
train_iter_loss: 0.2048121690750122
train_iter_loss: 0.298087477684021
train_iter_loss: 0.23741522431373596
train_iter_loss: 0.21066921949386597
train_iter_loss: 0.2176191210746765
train_iter_loss: 0.14433929324150085
train_iter_loss: 0.12433633953332901
train_iter_loss: 0.08935786038637161
train_iter_loss: 0.19974565505981445
train_iter_loss: 0.20546025037765503
train_iter_loss: 0.20844046771526337
train_iter_loss: 0.21042890846729279
train_iter_loss: 0.08805105835199356
train_iter_loss: 0.17032308876514435
train_iter_loss: 0.14227372407913208
train_iter_loss: 0.16715720295906067
train_iter_loss: 0.16744861006736755
train_iter_loss: 0.2682506740093231
train_iter_loss: 0.1403372436761856
train_iter_loss: 0.24806444346904755
train_iter_loss: 0.13107505440711975
train_iter_loss: 0.21077017486095428
train_iter_loss: 0.17127199470996857
train_iter_loss: 0.14452847838401794
train_iter_loss: 0.27308937907218933
train_iter_loss: 0.24148112535476685
train_iter_loss: 0.14904509484767914
train_iter_loss: 0.11925122886896133
train_iter_loss: 0.22075103223323822
train_iter_loss: 0.18432456254959106
train_iter_loss: 0.11538656800985336
train_iter_loss: 0.11896928399801254
train_iter_loss: 0.19720430672168732
train_iter_loss: 0.1769365817308426
train_iter_loss: 0.2155584990978241
train_iter_loss: 0.12743407487869263
train_iter_loss: 0.17220120131969452
train_iter_loss: 0.12466561794281006
train_iter_loss: 0.3069075345993042
train_iter_loss: 0.2791140079498291
train_iter_loss: 0.16003713011741638
train_iter_loss: 0.245128333568573
train_iter_loss: 0.22509221732616425
train_iter_loss: 0.2467029094696045
train_iter_loss: 0.2714368402957916
train_iter_loss: 0.20131225883960724
train_iter_loss: 0.1143539696931839
train_iter_loss: 0.13006778061389923
train_iter_loss: 0.13393975794315338
train_iter_loss: 0.12134552747011185
train_iter_loss: 0.18966825306415558
train_iter_loss: 0.1362578123807907
train_iter_loss: 0.23593869805335999
train_iter_loss: 0.18816348910331726
train loss :0.1877
---------------------
Validation seg loss: 0.22718927311658296 at epoch 77
epoch =     78/  1000, exp = train
train_iter_loss: 0.19767995178699493
train_iter_loss: 0.1952487826347351
train_iter_loss: 0.11610759794712067
train_iter_loss: 0.20110411942005157
train_iter_loss: 0.11556754261255264
train_iter_loss: 0.1767140030860901
train_iter_loss: 0.2420027107000351
train_iter_loss: 0.14034931361675262
train_iter_loss: 0.22070544958114624
train_iter_loss: 0.1873294711112976
train_iter_loss: 0.05481976643204689
train_iter_loss: 0.19245873391628265
train_iter_loss: 0.2190188765525818
train_iter_loss: 0.16366197168827057
train_iter_loss: 0.2907489538192749
train_iter_loss: 0.08119844645261765
train_iter_loss: 0.09497281163930893
train_iter_loss: 0.12660489976406097
train_iter_loss: 0.1680070012807846
train_iter_loss: 0.1935545802116394
train_iter_loss: 0.13691000640392303
train_iter_loss: 0.24641099572181702
train_iter_loss: 0.20764140784740448
train_iter_loss: 0.12733151018619537
train_iter_loss: 0.23914475739002228
train_iter_loss: 0.18759460747241974
train_iter_loss: 0.26748982071876526
train_iter_loss: 0.2591984272003174
train_iter_loss: 0.15213024616241455
train_iter_loss: 0.23559093475341797
train_iter_loss: 0.30881354212760925
train_iter_loss: 0.09099224954843521
train_iter_loss: 0.19049113988876343
train_iter_loss: 0.24006325006484985
train_iter_loss: 0.1194838359951973
train_iter_loss: 0.11551618576049805
train_iter_loss: 0.1707010567188263
train_iter_loss: 0.12480298429727554
train_iter_loss: 0.1712389588356018
train_iter_loss: 0.1050703227519989
train_iter_loss: 0.3562508821487427
train_iter_loss: 0.22745229303836823
train_iter_loss: 0.10815199464559555
train_iter_loss: 0.13059477508068085
train_iter_loss: 0.11755911260843277
train_iter_loss: 0.19914290308952332
train_iter_loss: 0.2812652289867401
train_iter_loss: 0.13136166334152222
train_iter_loss: 0.1913759559392929
train_iter_loss: 0.12935708463191986
train_iter_loss: 0.283499538898468
train_iter_loss: 0.13132071495056152
train_iter_loss: 0.20887693762779236
train_iter_loss: 0.13391992449760437
train_iter_loss: 0.17261043190956116
train_iter_loss: 0.1241166815161705
train_iter_loss: 0.16520927846431732
train_iter_loss: 0.18968376517295837
train_iter_loss: 0.2006845772266388
train_iter_loss: 0.09375422447919846
train_iter_loss: 0.20988355576992035
train_iter_loss: 0.12252938747406006
train_iter_loss: 0.2112765908241272
train_iter_loss: 0.21477796137332916
train_iter_loss: 0.264490008354187
train_iter_loss: 0.09027136862277985
train_iter_loss: 0.1017904132604599
train_iter_loss: 0.2096826285123825
train_iter_loss: 0.21423302590847015
train_iter_loss: 0.11730143427848816
train_iter_loss: 0.142484650015831
train_iter_loss: 0.20188210904598236
train_iter_loss: 0.10489524900913239
train_iter_loss: 0.16728883981704712
train_iter_loss: 0.1341283917427063
train_iter_loss: 0.37307867407798767
train_iter_loss: 0.2139386534690857
train_iter_loss: 0.20512191951274872
train_iter_loss: 0.16653181612491608
train_iter_loss: 0.19343173503875732
train_iter_loss: 0.1909121870994568
train_iter_loss: 0.29167836904525757
train_iter_loss: 0.20239423215389252
train_iter_loss: 0.23321203887462616
train_iter_loss: 0.21010908484458923
train_iter_loss: 0.09594044834375381
train_iter_loss: 0.23642829060554504
train_iter_loss: 0.18846869468688965
train_iter_loss: 0.2168062925338745
train_iter_loss: 0.17875070869922638
train_iter_loss: 0.21021445095539093
train_iter_loss: 0.3115614950656891
train_iter_loss: 0.17850063741207123
train_iter_loss: 0.2061576545238495
train_iter_loss: 0.16575025022029877
train_iter_loss: 0.16925586760044098
train_iter_loss: 0.31702134013175964
train_iter_loss: 0.2316754162311554
train_iter_loss: 0.1795687973499298
train_iter_loss: 0.20838913321495056
train loss :0.1872
---------------------
Validation seg loss: 0.2364489347761813 at epoch 78
epoch =     79/  1000, exp = train
train_iter_loss: 0.15562355518341064
train_iter_loss: 0.19215606153011322
train_iter_loss: 0.144463911652565
train_iter_loss: 0.205527663230896
train_iter_loss: 0.15716072916984558
train_iter_loss: 0.18154945969581604
train_iter_loss: 0.30356526374816895
train_iter_loss: 0.16565221548080444
train_iter_loss: 0.14003358781337738
train_iter_loss: 0.23898474872112274
train_iter_loss: 0.21702799201011658
train_iter_loss: 0.2533024847507477
train_iter_loss: 0.10020209103822708
train_iter_loss: 0.1519560068845749
train_iter_loss: 0.18177546560764313
train_iter_loss: 0.27382832765579224
train_iter_loss: 0.06742832064628601
train_iter_loss: 0.180675208568573
train_iter_loss: 0.3486967086791992
train_iter_loss: 0.15295526385307312
train_iter_loss: 0.1983211487531662
train_iter_loss: 0.2215253859758377
train_iter_loss: 0.2066025286912918
train_iter_loss: 0.058090630918741226
train_iter_loss: 0.1632491946220398
train_iter_loss: 0.12470933794975281
train_iter_loss: 0.19199876487255096
train_iter_loss: 0.13416717946529388
train_iter_loss: 0.20908260345458984
train_iter_loss: 0.26425909996032715
train_iter_loss: 0.2124047875404358
train_iter_loss: 0.17762057483196259
train_iter_loss: 0.1983456164598465
train_iter_loss: 0.18555261194705963
train_iter_loss: 0.13651393353939056
train_iter_loss: 0.281047523021698
train_iter_loss: 0.19251088798046112
train_iter_loss: 0.13878922164440155
train_iter_loss: 0.16033925116062164
train_iter_loss: 0.18257510662078857
train_iter_loss: 0.21309544146060944
train_iter_loss: 0.12911754846572876
train_iter_loss: 0.29487380385398865
train_iter_loss: 0.156072735786438
train_iter_loss: 0.15220221877098083
train_iter_loss: 0.35407984256744385
train_iter_loss: 0.10904345661401749
train_iter_loss: 0.22974497079849243
train_iter_loss: 0.16508032381534576
train_iter_loss: 0.3257598578929901
train_iter_loss: 0.18863904476165771
train_iter_loss: 0.14159993827342987
train_iter_loss: 0.18163305521011353
train_iter_loss: 0.12946416437625885
train_iter_loss: 0.19883906841278076
train_iter_loss: 0.13720212876796722
train_iter_loss: 0.19025932252407074
train_iter_loss: 0.2191987782716751
train_iter_loss: 0.40201255679130554
train_iter_loss: 0.2073994129896164
train_iter_loss: 0.17890435457229614
train_iter_loss: 0.19587036967277527
train_iter_loss: 0.12106603384017944
train_iter_loss: 0.12166280299425125
train_iter_loss: 0.14809638261795044
train_iter_loss: 0.17113609611988068
train_iter_loss: 0.1164795383810997
train_iter_loss: 0.2608065605163574
train_iter_loss: 0.15169046819210052
train_iter_loss: 0.09878247231245041
train_iter_loss: 0.2795991897583008
train_iter_loss: 0.4419466555118561
train_iter_loss: 0.3993038237094879
train_iter_loss: 0.2793659567832947
train_iter_loss: 0.23339225351810455
train_iter_loss: 0.12027663737535477
train_iter_loss: 0.11696796119213104
train_iter_loss: 0.09184731543064117
train_iter_loss: 0.251837819814682
train_iter_loss: 0.2331152707338333
train_iter_loss: 0.39468511939048767
train_iter_loss: 0.2909736931324005
train_iter_loss: 0.10259706526994705
train_iter_loss: 0.10840928554534912
train_iter_loss: 0.2501976788043976
train_iter_loss: 0.14980778098106384
train_iter_loss: 0.1338878571987152
train_iter_loss: 0.12852755188941956
train_iter_loss: 0.22691920399665833
train_iter_loss: 0.21602974832057953
train_iter_loss: 0.1597897708415985
train_iter_loss: 0.0989752933382988
train_iter_loss: 0.2147870808839798
train_iter_loss: 0.16542379558086395
train_iter_loss: 0.13908347487449646
train_iter_loss: 0.2592387795448303
train_iter_loss: 0.13636359572410583
train_iter_loss: 0.06798873841762543
train_iter_loss: 0.18998433649539948
train_iter_loss: 0.15664075314998627
train loss :0.1937
---------------------
Validation seg loss: 0.22828450626781527 at epoch 79
epoch =     80/  1000, exp = train
train_iter_loss: 0.1625046730041504
train_iter_loss: 0.15198293328285217
train_iter_loss: 0.18173587322235107
train_iter_loss: 0.41636425256729126
train_iter_loss: 0.20490165054798126
train_iter_loss: 0.3114749789237976
train_iter_loss: 0.11231406778097153
train_iter_loss: 0.17016693949699402
train_iter_loss: 0.09707286208868027
train_iter_loss: 0.11984983086585999
train_iter_loss: 0.14792561531066895
train_iter_loss: 0.15696528553962708
train_iter_loss: 0.13505545258522034
train_iter_loss: 0.11014972627162933
train_iter_loss: 0.21025465428829193
train_iter_loss: 0.22388191521167755
train_iter_loss: 0.1938837617635727
train_iter_loss: 0.15699881315231323
train_iter_loss: 0.24491684138774872
train_iter_loss: 0.14564865827560425
train_iter_loss: 0.16192413866519928
train_iter_loss: 0.1603856086730957
train_iter_loss: 0.22902998328208923
train_iter_loss: 0.2462899535894394
train_iter_loss: 0.1254020482301712
train_iter_loss: 0.1986110359430313
train_iter_loss: 0.2592464089393616
train_iter_loss: 0.1403544843196869
train_iter_loss: 0.2242107391357422
train_iter_loss: 0.14073756337165833
train_iter_loss: 0.19773177802562714
train_iter_loss: 0.18941889703273773
train_iter_loss: 0.1992052048444748
train_iter_loss: 0.18390490114688873
train_iter_loss: 0.16453668475151062
train_iter_loss: 0.19156943261623383
train_iter_loss: 0.09955327212810516
train_iter_loss: 0.187054842710495
train_iter_loss: 0.18801818788051605
train_iter_loss: 0.23825755715370178
train_iter_loss: 0.2598446309566498
train_iter_loss: 0.15273058414459229
train_iter_loss: 0.24357715249061584
train_iter_loss: 0.06666576117277145
train_iter_loss: 0.20798805356025696
train_iter_loss: 0.22373299300670624
train_iter_loss: 0.11073965579271317
train_iter_loss: 0.17622260749340057
train_iter_loss: 0.13540193438529968
train_iter_loss: 0.213408425450325
train_iter_loss: 0.08615483343601227
train_iter_loss: 0.06932397931814194
train_iter_loss: 0.1357988715171814
train_iter_loss: 0.3384809195995331
train_iter_loss: 0.14658306539058685
train_iter_loss: 0.15044695138931274
train_iter_loss: 0.14877605438232422
train_iter_loss: 0.1911284476518631
train_iter_loss: 0.20899055898189545
train_iter_loss: 0.20832578837871552
train_iter_loss: 0.28933489322662354
train_iter_loss: 0.11383135616779327
train_iter_loss: 0.2239115685224533
train_iter_loss: 0.21759696304798126
train_iter_loss: 0.14790114760398865
train_iter_loss: 0.22993893921375275
train_iter_loss: 0.13904470205307007
train_iter_loss: 0.19844286143779755
train_iter_loss: 0.2537305951118469
train_iter_loss: 0.12878616154193878
train_iter_loss: 0.1523824781179428
train_iter_loss: 0.08324260264635086
train_iter_loss: 0.21591070294380188
train_iter_loss: 0.25956442952156067
train_iter_loss: 0.2052725851535797
train_iter_loss: 0.2809983789920807
train_iter_loss: 0.10115806758403778
train_iter_loss: 0.22448480129241943
train_iter_loss: 0.315920352935791
train_iter_loss: 0.2250758856534958
train_iter_loss: 0.11515074223279953
train_iter_loss: 0.13476911187171936
train_iter_loss: 0.24108876287937164
train_iter_loss: 0.1882689893245697
train_iter_loss: 0.24065075814723969
train_iter_loss: 0.12537822127342224
train_iter_loss: 0.1496189832687378
train_iter_loss: 0.10129160434007645
train_iter_loss: 0.19965283572673798
train_iter_loss: 0.20239800214767456
train_iter_loss: 0.2750503122806549
train_iter_loss: 0.3227112293243408
train_iter_loss: 0.118719182908535
train_iter_loss: 0.20792575180530548
train_iter_loss: 0.1772473007440567
train_iter_loss: 0.1272139698266983
train_iter_loss: 0.14592090249061584
train_iter_loss: 0.15015186369419098
train_iter_loss: 0.227122500538826
train_iter_loss: 0.2544805109500885
train loss :0.1876
---------------------
Validation seg loss: 0.2269983680672803 at epoch 80
epoch =     81/  1000, exp = train
train_iter_loss: 0.27599790692329407
train_iter_loss: 0.22993148863315582
train_iter_loss: 0.23087984323501587
train_iter_loss: 0.33519476652145386
train_iter_loss: 0.1819130778312683
train_iter_loss: 0.2070479393005371
train_iter_loss: 0.12361888587474823
train_iter_loss: 0.263460636138916
train_iter_loss: 0.0906638503074646
train_iter_loss: 0.0979723110795021
train_iter_loss: 0.1306585967540741
train_iter_loss: 0.33860936760902405
train_iter_loss: 0.12078407406806946
train_iter_loss: 0.16261211037635803
train_iter_loss: 0.2098894864320755
train_iter_loss: 0.38815125823020935
train_iter_loss: 0.2097460776567459
train_iter_loss: 0.17329150438308716
train_iter_loss: 0.1732921153306961
train_iter_loss: 0.1176963523030281
train_iter_loss: 0.08715374767780304
train_iter_loss: 0.2109944075345993
train_iter_loss: 0.21853545308113098
train_iter_loss: 0.15631701052188873
train_iter_loss: 0.19797733426094055
train_iter_loss: 0.14085055887699127
train_iter_loss: 0.15497422218322754
train_iter_loss: 0.1315152943134308
train_iter_loss: 0.23006311058998108
train_iter_loss: 0.34405517578125
train_iter_loss: 0.17223313450813293
train_iter_loss: 0.135212704539299
train_iter_loss: 0.20091497898101807
train_iter_loss: 0.15531764924526215
train_iter_loss: 0.1184932067990303
train_iter_loss: 0.0864432230591774
train_iter_loss: 0.16081586480140686
train_iter_loss: 0.16345225274562836
train_iter_loss: 0.22269605100154877
train_iter_loss: 0.21767964959144592
train_iter_loss: 0.1921466588973999
train_iter_loss: 0.14037033915519714
train_iter_loss: 0.15776190161705017
train_iter_loss: 0.1816277652978897
train_iter_loss: 0.1340922862291336
train_iter_loss: 0.15821944177150726
train_iter_loss: 0.3036549389362335
train_iter_loss: 0.2996765077114105
train_iter_loss: 0.21954184770584106
train_iter_loss: 0.09021739661693573
train_iter_loss: 0.15263143181800842
train_iter_loss: 0.18479159474372864
train_iter_loss: 0.1478261500597
train_iter_loss: 0.25511014461517334
train_iter_loss: 0.1367006152868271
train_iter_loss: 0.20494569838047028
train_iter_loss: 0.18335780501365662
train_iter_loss: 0.17613965272903442
train_iter_loss: 0.12664136290550232
train_iter_loss: 0.230588898062706
train_iter_loss: 0.13308876752853394
train_iter_loss: 0.2510635256767273
train_iter_loss: 0.16164273023605347
train_iter_loss: 0.1343912035226822
train_iter_loss: 0.2084696739912033
train_iter_loss: 0.14485543966293335
train_iter_loss: 0.1602349579334259
train_iter_loss: 0.25744345784187317
train_iter_loss: 0.09515812993049622
train_iter_loss: 0.23828858137130737
train_iter_loss: 0.15417036414146423
train_iter_loss: 0.16911132633686066
train_iter_loss: 0.12079975008964539
train_iter_loss: 0.15825702250003815
train_iter_loss: 0.23346982896327972
train_iter_loss: 0.2218930721282959
train_iter_loss: 0.11858683824539185
train_iter_loss: 0.13551948964595795
train_iter_loss: 0.18708747625350952
train_iter_loss: 0.16058184206485748
train_iter_loss: 0.4649690091609955
train_iter_loss: 0.07194875925779343
train_iter_loss: 0.10891871899366379
train_iter_loss: 0.22536730766296387
train_iter_loss: 0.15200933814048767
train_iter_loss: 0.21773727238178253
train_iter_loss: 0.2376413643360138
train_iter_loss: 0.2606186866760254
train_iter_loss: 0.20551685988903046
train_iter_loss: 0.2218913733959198
train_iter_loss: 0.2756061255931854
train_iter_loss: 0.20828108489513397
train_iter_loss: 0.2111213058233261
train_iter_loss: 0.11250928789377213
train_iter_loss: 0.1559814214706421
train_iter_loss: 0.18336202204227448
train_iter_loss: 0.18510232865810394
train_iter_loss: 0.12877647578716278
train_iter_loss: 0.23301972448825836
train_iter_loss: 0.1718875616788864
train loss :0.1891
---------------------
Validation seg loss: 0.22925487912769588 at epoch 81
epoch =     82/  1000, exp = train
train_iter_loss: 0.09111452847719193
train_iter_loss: 0.20169025659561157
train_iter_loss: 0.23096033930778503
train_iter_loss: 0.16516359150409698
train_iter_loss: 0.20097734034061432
train_iter_loss: 0.16824327409267426
train_iter_loss: 0.2463962584733963
train_iter_loss: 0.16148480772972107
train_iter_loss: 0.35763445496559143
train_iter_loss: 0.16584628820419312
train_iter_loss: 0.10742518305778503
train_iter_loss: 0.19713078439235687
train_iter_loss: 0.24888058006763458
train_iter_loss: 0.18634893000125885
train_iter_loss: 0.19995169341564178
train_iter_loss: 0.15266947448253632
train_iter_loss: 0.27641382813453674
train_iter_loss: 0.18580691516399384
train_iter_loss: 0.21281586587429047
train_iter_loss: 0.18574202060699463
train_iter_loss: 0.2096630334854126
train_iter_loss: 0.15929172933101654
train_iter_loss: 0.16902752220630646
train_iter_loss: 0.3627852201461792
train_iter_loss: 0.20645402371883392
train_iter_loss: 0.2457032948732376
train_iter_loss: 0.17413009703159332
train_iter_loss: 0.3171420395374298
train_iter_loss: 0.07408060878515244
train_iter_loss: 0.19525985419750214
train_iter_loss: 0.14792394638061523
train_iter_loss: 0.27995985746383667
train_iter_loss: 0.1695965826511383
train_iter_loss: 0.14123839139938354
train_iter_loss: 0.04470808058977127
train_iter_loss: 0.1954292207956314
train_iter_loss: 0.12082621455192566
train_iter_loss: 0.23488622903823853
train_iter_loss: 0.16986867785453796
train_iter_loss: 0.29539015889167786
train_iter_loss: 0.38676345348358154
train_iter_loss: 0.16847653687000275
train_iter_loss: 0.15553231537342072
train_iter_loss: 0.09603937715291977
train_iter_loss: 0.1437879502773285
train_iter_loss: 0.22762846946716309
train_iter_loss: 0.15448491275310516
train_iter_loss: 0.11747931689023972
train_iter_loss: 0.1567322313785553
train_iter_loss: 0.22151176631450653
train_iter_loss: 0.2968980669975281
train_iter_loss: 0.19295328855514526
train_iter_loss: 0.16038157045841217
train_iter_loss: 0.16977153718471527
train_iter_loss: 0.1401529759168625
train_iter_loss: 0.23827670514583588
train_iter_loss: 0.12704291939735413
train_iter_loss: 0.09322149306535721
train_iter_loss: 0.13006313145160675
train_iter_loss: 0.11926295608282089
train_iter_loss: 0.2568107545375824
train_iter_loss: 0.16518931090831757
train_iter_loss: 0.10314056277275085
train_iter_loss: 0.16890409588813782
train_iter_loss: 0.13870805501937866
train_iter_loss: 0.12616686522960663
train_iter_loss: 0.14523984491825104
train_iter_loss: 0.17735739052295685
train_iter_loss: 0.15160097181797028
train_iter_loss: 0.30713337659835815
train_iter_loss: 0.20570409297943115
train_iter_loss: 0.1895914524793625
train_iter_loss: 0.2631353437900543
train_iter_loss: 0.2060365378856659
train_iter_loss: 0.16263657808303833
train_iter_loss: 0.1631532609462738
train_iter_loss: 0.18636555969715118
train_iter_loss: 0.12370352447032928
train_iter_loss: 0.16845113039016724
train_iter_loss: 0.1633583903312683
train_iter_loss: 0.1471465528011322
train_iter_loss: 0.08937452733516693
train_iter_loss: 0.07175691425800323
train_iter_loss: 0.2957051992416382
train_iter_loss: 0.21247535943984985
train_iter_loss: 0.22781412303447723
train_iter_loss: 0.1849074512720108
train_iter_loss: 0.37425124645233154
train_iter_loss: 0.18528054654598236
train_iter_loss: 0.10163478553295135
train_iter_loss: 0.1835089921951294
train_iter_loss: 0.2636420428752899
train_iter_loss: 0.19557693600654602
train_iter_loss: 0.21546930074691772
train_iter_loss: 0.0904487818479538
train_iter_loss: 0.25309082865715027
train_iter_loss: 0.06103413924574852
train_iter_loss: 0.1744057685136795
train_iter_loss: 0.1261126846075058
train_iter_loss: 0.09282117336988449
train loss :0.1866
---------------------
Validation seg loss: 0.2271235695741368 at epoch 82
epoch =     83/  1000, exp = train
train_iter_loss: 0.22492457926273346
train_iter_loss: 0.2441142350435257
train_iter_loss: 0.24540746212005615
train_iter_loss: 0.1271287053823471
train_iter_loss: 0.07601957768201828
train_iter_loss: 0.24169431626796722
train_iter_loss: 0.16250602900981903
train_iter_loss: 0.20981042087078094
train_iter_loss: 0.15934553742408752
train_iter_loss: 0.12270999699831009
train_iter_loss: 0.15015004575252533
train_iter_loss: 0.22682732343673706
train_iter_loss: 0.243689626455307
train_iter_loss: 0.19059014320373535
train_iter_loss: 0.20533806085586548
train_iter_loss: 0.13679088652133942
train_iter_loss: 0.19207912683486938
train_iter_loss: 0.17488959431648254
train_iter_loss: 0.13218657672405243
train_iter_loss: 0.12311781942844391
train_iter_loss: 0.2026648074388504
train_iter_loss: 0.1863372027873993
train_iter_loss: 0.24258169531822205
train_iter_loss: 0.18578873574733734
train_iter_loss: 0.20803402364253998
train_iter_loss: 0.18144990503787994
train_iter_loss: 0.22830964624881744
train_iter_loss: 0.25143662095069885
train_iter_loss: 0.19221679866313934
train_iter_loss: 0.22282622754573822
train_iter_loss: 0.1532169133424759
train_iter_loss: 0.1597352921962738
train_iter_loss: 0.16177012026309967
train_iter_loss: 0.1385257989168167
train_iter_loss: 0.2944047152996063
train_iter_loss: 0.19735419750213623
train_iter_loss: 0.12442819029092789
train_iter_loss: 0.16309334337711334
train_iter_loss: 0.2787192463874817
train_iter_loss: 0.13918879628181458
train_iter_loss: 0.159505695104599
train_iter_loss: 0.13898122310638428
train_iter_loss: 0.24559102952480316
train_iter_loss: 0.16668082773685455
train_iter_loss: 0.15798157453536987
train_iter_loss: 0.1650751829147339
train_iter_loss: 0.06478701531887054
train_iter_loss: 0.23858223855495453
train_iter_loss: 0.08499570190906525
train_iter_loss: 0.19304917752742767
train_iter_loss: 0.2290373146533966
train_iter_loss: 0.0884891152381897
train_iter_loss: 0.2283671796321869
train_iter_loss: 0.15131217241287231
train_iter_loss: 0.2832399010658264
train_iter_loss: 0.11041908711194992
train_iter_loss: 0.17742885649204254
train_iter_loss: 0.22104181349277496
train_iter_loss: 0.20896199345588684
train_iter_loss: 0.18646329641342163
train_iter_loss: 0.1956213414669037
train_iter_loss: 0.20986872911453247
train_iter_loss: 0.1500595510005951
train_iter_loss: 0.24238809943199158
train_iter_loss: 0.14763425290584564
train_iter_loss: 0.2442760020494461
train_iter_loss: 0.24435709416866302
train_iter_loss: 0.1280471831560135
train_iter_loss: 0.24220962822437286
train_iter_loss: 0.17364831268787384
train_iter_loss: 0.15458841621875763
train_iter_loss: 0.22426725924015045
train_iter_loss: 0.13053369522094727
train_iter_loss: 0.24883851408958435
train_iter_loss: 0.03539067134261131
train_iter_loss: 0.15062372386455536
train_iter_loss: 0.23907476663589478
train_iter_loss: 0.20972685515880585
train_iter_loss: 0.13743749260902405
train_iter_loss: 0.21276484429836273
train_iter_loss: 0.14390940964221954
train_iter_loss: 0.21760758757591248
train_iter_loss: 0.25565603375434875
train_iter_loss: 0.13599893450737
train_iter_loss: 0.14936797320842743
train_iter_loss: 0.24823930859565735
train_iter_loss: 0.15544193983078003
train_iter_loss: 0.13017655909061432
train_iter_loss: 0.20223480463027954
train_iter_loss: 0.11003834009170532
train_iter_loss: 0.1436312347650528
train_iter_loss: 0.12370627373456955
train_iter_loss: 0.15462395548820496
train_iter_loss: 0.0727238804101944
train_iter_loss: 0.2417515367269516
train_iter_loss: 0.2743428349494934
train_iter_loss: 0.1890770047903061
train_iter_loss: 0.12809668481349945
train_iter_loss: 0.13013781607151031
train_iter_loss: 0.17022746801376343
train loss :0.1829
---------------------
Validation seg loss: 0.2281970999572637 at epoch 83
epoch =     84/  1000, exp = train
train_iter_loss: 0.16869744658470154
train_iter_loss: 0.07456251978874207
train_iter_loss: 0.14097219705581665
train_iter_loss: 0.15068787336349487
train_iter_loss: 0.245650053024292
train_iter_loss: 0.18017986416816711
train_iter_loss: 0.1796555519104004
train_iter_loss: 0.15983469784259796
train_iter_loss: 0.20309868454933167
train_iter_loss: 0.11842971295118332
train_iter_loss: 0.19311314821243286
train_iter_loss: 0.0592799037694931
train_iter_loss: 0.2416919618844986
train_iter_loss: 0.13917841017246246
train_iter_loss: 0.1474386304616928
train_iter_loss: 0.18755345046520233
train_iter_loss: 0.16091583669185638
train_iter_loss: 0.14548932015895844
train_iter_loss: 0.14927245676517487
train_iter_loss: 0.16254468262195587
train_iter_loss: 0.1472267359495163
train_iter_loss: 0.21939696371555328
train_iter_loss: 0.13985425233840942
train_iter_loss: 0.16082458198070526
train_iter_loss: 0.27139103412628174
train_iter_loss: 0.1771060973405838
train_iter_loss: 0.16571342945098877
train_iter_loss: 0.19257092475891113
train_iter_loss: 0.15897780656814575
train_iter_loss: 0.17825061082839966
train_iter_loss: 0.21474409103393555
train_iter_loss: 0.1949545443058014
train_iter_loss: 0.19295629858970642
train_iter_loss: 0.11298529803752899
train_iter_loss: 0.19065868854522705
train_iter_loss: 0.24025648832321167
train_iter_loss: 0.18194353580474854
train_iter_loss: 0.13975180685520172
train_iter_loss: 0.12933076918125153
train_iter_loss: 0.1085941269993782
train_iter_loss: 0.15182800590991974
train_iter_loss: 0.22179225087165833
train_iter_loss: 0.27288249135017395
train_iter_loss: 0.2815721929073334
train_iter_loss: 0.12335485965013504
train_iter_loss: 0.0789240151643753
train_iter_loss: 0.17282302677631378
train_iter_loss: 0.09813651442527771
train_iter_loss: 0.1332692950963974
train_iter_loss: 0.1638941913843155
train_iter_loss: 0.29343846440315247
train_iter_loss: 0.22749042510986328
train_iter_loss: 0.11103270202875137
train_iter_loss: 0.27742719650268555
train_iter_loss: 0.08503039926290512
train_iter_loss: 0.1293821632862091
train_iter_loss: 0.3025214970111847
train_iter_loss: 0.2364799529314041
train_iter_loss: 0.2303149402141571
train_iter_loss: 0.18866166472434998
train_iter_loss: 0.39722850918769836
train_iter_loss: 0.2796975076198578
train_iter_loss: 0.2772473096847534
train_iter_loss: 0.1871626079082489
train_iter_loss: 0.13687312602996826
train_iter_loss: 0.2904796600341797
train_iter_loss: 0.13317161798477173
train_iter_loss: 0.21708442270755768
train_iter_loss: 0.12881220877170563
train_iter_loss: 0.13578663766384125
train_iter_loss: 0.20750147104263306
train_iter_loss: 0.23649848997592926
train_iter_loss: 0.1973441243171692
train_iter_loss: 0.27990081906318665
train_iter_loss: 0.13060659170150757
train_iter_loss: 0.23129911720752716
train_iter_loss: 0.21404887735843658
train_iter_loss: 0.16374248266220093
train_iter_loss: 0.18291229009628296
train_iter_loss: 0.1831652671098709
train_iter_loss: 0.21194963157176971
train_iter_loss: 0.12009218335151672
train_iter_loss: 0.15854386985301971
train_iter_loss: 0.19854094088077545
train_iter_loss: 0.13947691023349762
train_iter_loss: 0.12074775993824005
train_iter_loss: 0.13855774700641632
train_iter_loss: 0.3230372369289398
train_iter_loss: 0.18560634553432465
train_iter_loss: 0.1602303832769394
train_iter_loss: 0.18642741441726685
train_iter_loss: 0.18287143111228943
train_iter_loss: 0.18954282999038696
train_iter_loss: 0.1425590217113495
train_iter_loss: 0.25909698009490967
train_iter_loss: 0.13453087210655212
train_iter_loss: 0.1519392430782318
train_iter_loss: 0.1553225815296173
train_iter_loss: 0.24667097628116608
train_iter_loss: 0.1407126486301422
train loss :0.1845
---------------------
Validation seg loss: 0.2271104425121591 at epoch 84
epoch =     85/  1000, exp = train
train_iter_loss: 0.22854763269424438
train_iter_loss: 0.264871746301651
train_iter_loss: 0.1470940113067627
train_iter_loss: 0.2247176319360733
train_iter_loss: 0.1717986762523651
train_iter_loss: 0.2335403710603714
train_iter_loss: 0.11188319325447083
train_iter_loss: 0.26150622963905334
train_iter_loss: 0.18223577737808228
train_iter_loss: 0.29109302163124084
train_iter_loss: 0.25388291478157043
train_iter_loss: 0.15852589905261993
train_iter_loss: 0.09700272977352142
train_iter_loss: 0.12935256958007812
train_iter_loss: 0.14997409284114838
train_iter_loss: 0.20276665687561035
train_iter_loss: 0.1494273841381073
train_iter_loss: 0.28148353099823
train_iter_loss: 0.14473718404769897
train_iter_loss: 0.2092124968767166
train_iter_loss: 0.15136024355888367
train_iter_loss: 0.24935148656368256
train_iter_loss: 0.20687328279018402
train_iter_loss: 0.19651801884174347
train_iter_loss: 0.12474042177200317
train_iter_loss: 0.14956341683864594
train_iter_loss: 0.17440488934516907
train_iter_loss: 0.19404146075248718
train_iter_loss: 0.329261839389801
train_iter_loss: 0.17320482432842255
train_iter_loss: 0.10572972893714905
train_iter_loss: 0.15177763998508453
train_iter_loss: 0.10174274444580078
train_iter_loss: 0.17674021422863007
train_iter_loss: 0.21484601497650146
train_iter_loss: 0.1393328607082367
train_iter_loss: 0.1753404289484024
train_iter_loss: 0.11990408599376678
train_iter_loss: 0.23377399146556854
train_iter_loss: 0.3554810881614685
train_iter_loss: 0.27368807792663574
train_iter_loss: 0.25513726472854614
train_iter_loss: 0.20480512082576752
train_iter_loss: 0.1604539006948471
train_iter_loss: 0.22328782081604004
train_iter_loss: 0.2143324315547943
train_iter_loss: 0.24726201593875885
train_iter_loss: 0.13202854990959167
train_iter_loss: 0.08869562298059464
train_iter_loss: 0.11682163178920746
train_iter_loss: 0.15448246896266937
train_iter_loss: 0.18870291113853455
train_iter_loss: 0.12378805875778198
train_iter_loss: 0.20425401628017426
train_iter_loss: 0.15687744319438934
train_iter_loss: 0.13881370425224304
train_iter_loss: 0.2770138084888458
train_iter_loss: 0.15185695886611938
train_iter_loss: 0.11930374801158905
train_iter_loss: 0.20056018233299255
train_iter_loss: 0.1627008616924286
train_iter_loss: 0.2961724102497101
train_iter_loss: 0.22858712077140808
train_iter_loss: 0.206887349486351
train_iter_loss: 0.2227923423051834
train_iter_loss: 0.09861765801906586
train_iter_loss: 0.22999906539916992
train_iter_loss: 0.21475476026535034
train_iter_loss: 0.20044711232185364
train_iter_loss: 0.19337958097457886
train_iter_loss: 0.24262972176074982
train_iter_loss: 0.1490841954946518
train_iter_loss: 0.10687431693077087
train_iter_loss: 0.18587726354599
train_iter_loss: 0.2363082766532898
train_iter_loss: 0.1205606758594513
train_iter_loss: 0.16556541621685028
train_iter_loss: 0.14399433135986328
train_iter_loss: 0.12036368250846863
train_iter_loss: 0.19640855491161346
train_iter_loss: 0.11998093128204346
train_iter_loss: 0.3041215240955353
train_iter_loss: 0.09069394320249557
train_iter_loss: 0.09906663745641708
train_iter_loss: 0.1576828509569168
train_iter_loss: 0.287243515253067
train_iter_loss: 0.17873691022396088
train_iter_loss: 0.2688583433628082
train_iter_loss: 0.22093430161476135
train_iter_loss: 0.10048917680978775
train_iter_loss: 0.12460000813007355
train_iter_loss: 0.1515272855758667
train_iter_loss: 0.11492174118757248
train_iter_loss: 0.20086592435836792
train_iter_loss: 0.14334753155708313
train_iter_loss: 0.19715046882629395
train_iter_loss: 0.38832950592041016
train_iter_loss: 0.27299612760543823
train_iter_loss: 0.20471641421318054
train_iter_loss: 0.15241794288158417
train loss :0.1893
---------------------
Validation seg loss: 0.22792555205523968 at epoch 85
epoch =     86/  1000, exp = train
train_iter_loss: 0.29976093769073486
train_iter_loss: 0.1978677213191986
train_iter_loss: 0.1412520408630371
train_iter_loss: 0.170939102768898
train_iter_loss: 0.18765610456466675
train_iter_loss: 0.16077694296836853
train_iter_loss: 0.2559318244457245
train_iter_loss: 0.22286085784435272
train_iter_loss: 0.11970258504152298
train_iter_loss: 0.2487107515335083
train_iter_loss: 0.18785789608955383
train_iter_loss: 0.22252556681632996
train_iter_loss: 0.18681222200393677
train_iter_loss: 0.1510491967201233
train_iter_loss: 0.23398840427398682
train_iter_loss: 0.24082578718662262
train_iter_loss: 0.10552556067705154
train_iter_loss: 0.11754702776670456
train_iter_loss: 0.3296622633934021
train_iter_loss: 0.18999512493610382
train_iter_loss: 0.13734139502048492
train_iter_loss: 0.17159859836101532
train_iter_loss: 0.2236352115869522
train_iter_loss: 0.18181262910366058
train_iter_loss: 0.2844177186489105
train_iter_loss: 0.17172369360923767
train_iter_loss: 0.2234458029270172
train_iter_loss: 0.2713225781917572
train_iter_loss: 0.15000104904174805
train_iter_loss: 0.3360297977924347
train_iter_loss: 0.17739428579807281
train_iter_loss: 0.1426975429058075
train_iter_loss: 0.17879250645637512
train_iter_loss: 0.13190023601055145
train_iter_loss: 0.1525663286447525
train_iter_loss: 0.1572146713733673
train_iter_loss: 0.13944651186466217
train_iter_loss: 0.27441442012786865
train_iter_loss: 0.17494289577007294
train_iter_loss: 0.11035320162773132
train_iter_loss: 0.35734808444976807
train_iter_loss: 0.15913403034210205
train_iter_loss: 0.10841208696365356
train_iter_loss: 0.16257889568805695
train_iter_loss: 0.19474564492702484
train_iter_loss: 0.1278056502342224
train_iter_loss: 0.14083586633205414
train_iter_loss: 0.21965400874614716
train_iter_loss: 0.2678159177303314
train_iter_loss: 0.1291293054819107
train_iter_loss: 0.21533280611038208
train_iter_loss: 0.15117667615413666
train_iter_loss: 0.204945370554924
train_iter_loss: 0.23149412870407104
train_iter_loss: 0.1513626128435135
train_iter_loss: 0.07652086019515991
train_iter_loss: 0.1380700021982193
train_iter_loss: 0.11995718628168106
train_iter_loss: 0.13709910213947296
train_iter_loss: 0.12093522399663925
train_iter_loss: 0.23136474192142487
train_iter_loss: 0.16615694761276245
train_iter_loss: 0.19398178160190582
train_iter_loss: 0.07993798702955246
train_iter_loss: 0.11835606396198273
train_iter_loss: 0.19142024219036102
train_iter_loss: 0.14657197892665863
train_iter_loss: 0.24872297048568726
train_iter_loss: 0.15397144854068756
train_iter_loss: 0.13210274279117584
train_iter_loss: 0.12629683315753937
train_iter_loss: 0.14623624086380005
train_iter_loss: 0.13843251764774323
train_iter_loss: 0.18442626297473907
train_iter_loss: 0.15955226123332977
train_iter_loss: 0.14251509308815002
train_iter_loss: 0.17557424306869507
train_iter_loss: 0.2822985053062439
train_iter_loss: 0.14218980073928833
train_iter_loss: 0.17327740788459778
train_iter_loss: 0.1145923063158989
train_iter_loss: 0.16244879364967346
train_iter_loss: 0.17486141622066498
train_iter_loss: 0.2361995428800583
train_iter_loss: 0.27656665444374084
train_iter_loss: 0.2925903797149658
train_iter_loss: 0.23588496446609497
train_iter_loss: 0.14544270932674408
train_iter_loss: 0.22967121005058289
train_iter_loss: 0.13200856745243073
train_iter_loss: 0.1655537635087967
train_iter_loss: 0.08868245780467987
train_iter_loss: 0.11432424187660217
train_iter_loss: 0.2043379694223404
train_iter_loss: 0.20129749178886414
train_iter_loss: 0.15456314384937286
train_iter_loss: 0.2165505588054657
train_iter_loss: 0.3648987412452698
train_iter_loss: 0.2932974696159363
train_iter_loss: 0.1636744886636734
train loss :0.1867
---------------------
Validation seg loss: 0.22713083949572635 at epoch 86
epoch =     87/  1000, exp = train
train_iter_loss: 0.09465571492910385
train_iter_loss: 0.2638283371925354
train_iter_loss: 0.2500292956829071
train_iter_loss: 0.10264843702316284
train_iter_loss: 0.1953437775373459
train_iter_loss: 0.13276585936546326
train_iter_loss: 0.19530196487903595
train_iter_loss: 0.1658186912536621
train_iter_loss: 0.23801128566265106
train_iter_loss: 0.13138766586780548
train_iter_loss: 0.12699899077415466
train_iter_loss: 0.11384432017803192
train_iter_loss: 0.16915494203567505
train_iter_loss: 0.2414160668849945
train_iter_loss: 0.2644850015640259
train_iter_loss: 0.17716121673583984
train_iter_loss: 0.14446412026882172
train_iter_loss: 0.1611466109752655
train_iter_loss: 0.19943580031394958
train_iter_loss: 0.18351954221725464
train_iter_loss: 0.11794980615377426
train_iter_loss: 0.19877637922763824
train_iter_loss: 0.15628327429294586
train_iter_loss: 0.17033062875270844
train_iter_loss: 0.1087743416428566
train_iter_loss: 0.13460512459278107
train_iter_loss: 0.39850857853889465
train_iter_loss: 0.13947553932666779
train_iter_loss: 0.17510494589805603
train_iter_loss: 0.18669314682483673
train_iter_loss: 0.34897321462631226
train_iter_loss: 0.21096645295619965
train_iter_loss: 0.19009897112846375
train_iter_loss: 0.36934423446655273
train_iter_loss: 0.23759756982326508
train_iter_loss: 0.17066732048988342
train_iter_loss: 0.3267615735530853
train_iter_loss: 0.16285823285579681
train_iter_loss: 0.12543097138404846
train_iter_loss: 0.21042104065418243
train_iter_loss: 0.1456146091222763
train_iter_loss: 0.20173636078834534
train_iter_loss: 0.18400126695632935
train_iter_loss: 0.28076890110969543
train_iter_loss: 0.46163833141326904
train_iter_loss: 0.12131804972887039
train_iter_loss: 0.16006971895694733
train_iter_loss: 0.27911484241485596
train_iter_loss: 0.14797160029411316
train_iter_loss: 0.2018648386001587
train_iter_loss: 0.14119389653205872
train_iter_loss: 0.09379207342863083
train_iter_loss: 0.08638493716716766
train_iter_loss: 0.11210361868143082
train_iter_loss: 0.1896587759256363
train_iter_loss: 0.17977289855480194
train_iter_loss: 0.19155099987983704
train_iter_loss: 0.2173798829317093
train_iter_loss: 0.20632304251194
train_iter_loss: 0.16568873822689056
train_iter_loss: 0.1242280825972557
train_iter_loss: 0.15069925785064697
train_iter_loss: 0.20245657861232758
train_iter_loss: 0.18647217750549316
train_iter_loss: 0.16948901116847992
train_iter_loss: 0.3099427819252014
train_iter_loss: 0.22902123630046844
train_iter_loss: 0.20705898106098175
train_iter_loss: 0.20758430659770966
train_iter_loss: 0.17311760783195496
train_iter_loss: 0.13739219307899475
train_iter_loss: 0.1975080370903015
train_iter_loss: 0.14401625096797943
train_iter_loss: 0.12184825539588928
train_iter_loss: 0.16971455514431
train_iter_loss: 0.08625555783510208
train_iter_loss: 0.14389267563819885
train_iter_loss: 0.09271817654371262
train_iter_loss: 0.2370387315750122
train_iter_loss: 0.1865849643945694
train_iter_loss: 0.32800865173339844
train_iter_loss: 0.11845368146896362
train_iter_loss: 0.279600590467453
train_iter_loss: 0.17729529738426208
train_iter_loss: 0.21320223808288574
train_iter_loss: 0.24530231952667236
train_iter_loss: 0.13796496391296387
train_iter_loss: 0.22676979005336761
train_iter_loss: 0.15123063325881958
train_iter_loss: 0.32460951805114746
train_iter_loss: 0.19690242409706116
train_iter_loss: 0.16831950843334198
train_iter_loss: 0.18871963024139404
train_iter_loss: 0.20519328117370605
train_iter_loss: 0.2434130609035492
train_iter_loss: 0.15027815103530884
train_iter_loss: 0.20623281598091125
train_iter_loss: 0.10341107845306396
train_iter_loss: 0.1949920356273651
train_iter_loss: 0.18176156282424927
train loss :0.1916
---------------------
Validation seg loss: 0.2272737069617746 at epoch 87
epoch =     88/  1000, exp = train
train_iter_loss: 0.1056460440158844
train_iter_loss: 0.1929333657026291
train_iter_loss: 0.2208348512649536
train_iter_loss: 0.18193690478801727
train_iter_loss: 0.18261130154132843
train_iter_loss: 0.14719901978969574
train_iter_loss: 0.1478004902601242
train_iter_loss: 0.22529561817646027
train_iter_loss: 0.24168747663497925
train_iter_loss: 0.24176612496376038
train_iter_loss: 0.16971752047538757
train_iter_loss: 0.2287774682044983
train_iter_loss: 0.23346425592899323
train_iter_loss: 0.20409122109413147
train_iter_loss: 0.17732630670070648
train_iter_loss: 0.1365349441766739
train_iter_loss: 0.10549580305814743
train_iter_loss: 0.13050317764282227
train_iter_loss: 0.19381316006183624
train_iter_loss: 0.04190969094634056
train_iter_loss: 0.09536226838827133
train_iter_loss: 0.1421046108007431
train_iter_loss: 0.21609841287136078
train_iter_loss: 0.14135436713695526
train_iter_loss: 0.20949408411979675
train_iter_loss: 0.14631006121635437
train_iter_loss: 0.28919875621795654
train_iter_loss: 0.04756810516119003
train_iter_loss: 0.19840502738952637
train_iter_loss: 0.26047757267951965
train_iter_loss: 0.2026546150445938
train_iter_loss: 0.2241227924823761
train_iter_loss: 0.11151021718978882
train_iter_loss: 0.1305992603302002
train_iter_loss: 0.1582363098859787
train_iter_loss: 0.13417862355709076
train_iter_loss: 0.16015681624412537
train_iter_loss: 0.2733133137226105
train_iter_loss: 0.2643651068210602
train_iter_loss: 0.20704606175422668
train_iter_loss: 0.25498855113983154
train_iter_loss: 0.15761984884738922
train_iter_loss: 0.13952012360095978
train_iter_loss: 0.09300622344017029
train_iter_loss: 0.16822373867034912
train_iter_loss: 0.1303722858428955
train_iter_loss: 0.19478562474250793
train_iter_loss: 0.10747962445020676
train_iter_loss: 0.22634446620941162
train_iter_loss: 0.20705172419548035
train_iter_loss: 0.12928259372711182
train_iter_loss: 0.18493378162384033
train_iter_loss: 0.10601992905139923
train_iter_loss: 0.15479013323783875
train_iter_loss: 0.16890282928943634
train_iter_loss: 0.11083512753248215
train_iter_loss: 0.16509567201137543
train_iter_loss: 0.16219699382781982
train_iter_loss: 0.22791346907615662
train_iter_loss: 0.13638019561767578
train_iter_loss: 0.28552281856536865
train_iter_loss: 0.19729718565940857
train_iter_loss: 0.17410843074321747
train_iter_loss: 0.22408506274223328
train_iter_loss: 0.21138598024845123
train_iter_loss: 0.16714294254779816
train_iter_loss: 0.0695793405175209
train_iter_loss: 0.19932164251804352
train_iter_loss: 0.14079931378364563
train_iter_loss: 0.18110862374305725
train_iter_loss: 0.15832901000976562
train_iter_loss: 0.48730048537254333
train_iter_loss: 0.13696198165416718
train_iter_loss: 0.25064733624458313
train_iter_loss: 0.09013991057872772
train_iter_loss: 0.19247260689735413
train_iter_loss: 0.18476803600788116
train_iter_loss: 0.2476341426372528
train_iter_loss: 0.13640955090522766
train_iter_loss: 0.14646941423416138
train_iter_loss: 0.19994482398033142
train_iter_loss: 0.12318223714828491
train_iter_loss: 0.1779910773038864
train_iter_loss: 0.26310500502586365
train_iter_loss: 0.1674392819404602
train_iter_loss: 0.15552304685115814
train_iter_loss: 0.23778638243675232
train_iter_loss: 0.21776694059371948
train_iter_loss: 0.2396337389945984
train_iter_loss: 0.18983888626098633
train_iter_loss: 0.2036614716053009
train_iter_loss: 0.15231522917747498
train_iter_loss: 0.13501596450805664
train_iter_loss: 0.1264050453901291
train_iter_loss: 0.10541947931051254
train_iter_loss: 0.2727505564689636
train_iter_loss: 0.30635738372802734
train_iter_loss: 0.20928727090358734
train_iter_loss: 0.30780553817749023
train_iter_loss: 0.23985037207603455
train loss :0.1845
---------------------
Validation seg loss: 0.2272096343408778 at epoch 88
epoch =     89/  1000, exp = train
train_iter_loss: 0.16812948882579803
train_iter_loss: 0.10994861274957657
train_iter_loss: 0.2578386962413788
train_iter_loss: 0.07582031190395355
train_iter_loss: 0.11983990669250488
train_iter_loss: 0.10326534509658813
train_iter_loss: 0.2554534673690796
train_iter_loss: 0.13750150799751282
train_iter_loss: 0.1399214118719101
train_iter_loss: 0.11534316837787628
train_iter_loss: 0.19875086843967438
train_iter_loss: 0.33683422207832336
train_iter_loss: 0.2700662612915039
train_iter_loss: 0.25687772035598755
train_iter_loss: 0.22432130575180054
train_iter_loss: 0.19662369787693024
train_iter_loss: 0.19260813295841217
train_iter_loss: 0.11161746084690094
train_iter_loss: 0.20378947257995605
train_iter_loss: 0.3522908687591553
train_iter_loss: 0.164786234498024
train_iter_loss: 0.25905418395996094
train_iter_loss: 0.19273392856121063
train_iter_loss: 0.15830282866954803
train_iter_loss: 0.10362503677606583
train_iter_loss: 0.2447793185710907
train_iter_loss: 0.2543579339981079
train_iter_loss: 0.12157563865184784
train_iter_loss: 0.21148642897605896
train_iter_loss: 0.1223960742354393
train_iter_loss: 0.13356579840183258
train_iter_loss: 0.22366000711917877
train_iter_loss: 0.3069823086261749
train_iter_loss: 0.14429719746112823
train_iter_loss: 0.2387934774160385
train_iter_loss: 0.11230258643627167
train_iter_loss: 0.341532438993454
train_iter_loss: 0.18923917412757874
train_iter_loss: 0.19102828204631805
train_iter_loss: 0.11868458241224289
train_iter_loss: 0.20980067551136017
train_iter_loss: 0.13000871241092682
train_iter_loss: 0.2635810077190399
train_iter_loss: 0.18875645101070404
train_iter_loss: 0.16006261110305786
train_iter_loss: 0.24829451739788055
train_iter_loss: 0.29940319061279297
train_iter_loss: 0.12908490002155304
train_iter_loss: 0.06136155501008034
train_iter_loss: 0.18340937793254852
train_iter_loss: 0.15377384424209595
train_iter_loss: 0.23976822197437286
train_iter_loss: 0.16734296083450317
train_iter_loss: 0.11863542348146439
train_iter_loss: 0.14366383850574493
train_iter_loss: 0.14875046908855438
train_iter_loss: 0.14966478943824768
train_iter_loss: 0.1630576252937317
train_iter_loss: 0.1925475299358368
train_iter_loss: 0.22638344764709473
train_iter_loss: 0.25257202982902527
train_iter_loss: 0.14943209290504456
train_iter_loss: 0.2893158197402954
train_iter_loss: 0.23756344616413116
train_iter_loss: 0.10815524309873581
train_iter_loss: 0.07437986135482788
train_iter_loss: 0.19387483596801758
train_iter_loss: 0.16111432015895844
train_iter_loss: 0.10956761240959167
train_iter_loss: 0.34152480959892273
train_iter_loss: 0.19619041681289673
train_iter_loss: 0.16168832778930664
train_iter_loss: 0.13319018483161926
train_iter_loss: 0.1788334846496582
train_iter_loss: 0.19196699559688568
train_iter_loss: 0.14017324149608612
train_iter_loss: 0.11048910021781921
train_iter_loss: 0.1295483410358429
train_iter_loss: 0.19649280607700348
train_iter_loss: 0.23538468778133392
train_iter_loss: 0.2649555206298828
train_iter_loss: 0.14067603647708893
train_iter_loss: 0.15175533294677734
train_iter_loss: 0.2679106891155243
train_iter_loss: 0.15601685643196106
train_iter_loss: 0.2828323543071747
train_iter_loss: 0.12526041269302368
train_iter_loss: 0.10643793642520905
train_iter_loss: 0.20532381534576416
train_iter_loss: 0.14713607728481293
train_iter_loss: 0.13992957770824432
train_iter_loss: 0.24429777264595032
train_iter_loss: 0.1945798397064209
train_iter_loss: 0.2127406895160675
train_iter_loss: 0.2160039097070694
train_iter_loss: 0.27048981189727783
train_iter_loss: 0.15331409871578217
train_iter_loss: 0.20741502940654755
train_iter_loss: 0.15958614647388458
train_iter_loss: 0.06540726870298386
train loss :0.1873
---------------------
Validation seg loss: 0.23062360272654947 at epoch 89
epoch =     90/  1000, exp = train
train_iter_loss: 0.11164975166320801
train_iter_loss: 0.22201965749263763
train_iter_loss: 0.1533278524875641
train_iter_loss: 0.15443335473537445
train_iter_loss: 0.2277716100215912
train_iter_loss: 0.09850532561540604
train_iter_loss: 0.2518269717693329
train_iter_loss: 0.2328113615512848
train_iter_loss: 0.11867110431194305
train_iter_loss: 0.1325099617242813
train_iter_loss: 0.12642763555049896
train_iter_loss: 0.16111618280410767
train_iter_loss: 0.19169850647449493
train_iter_loss: 0.18223626911640167
train_iter_loss: 0.15006214380264282
train_iter_loss: 0.21689492464065552
train_iter_loss: 0.1702888309955597
train_iter_loss: 0.23661720752716064
train_iter_loss: 0.15618975460529327
train_iter_loss: 0.14349231123924255
train_iter_loss: 0.14743253588676453
train_iter_loss: 0.21695180237293243
train_iter_loss: 0.23020067811012268
train_iter_loss: 0.1488206833600998
train_iter_loss: 0.14177417755126953
train_iter_loss: 0.1671498864889145
train_iter_loss: 0.11504371464252472
train_iter_loss: 0.15496118366718292
train_iter_loss: 0.11500540375709534
train_iter_loss: 0.19560661911964417
train_iter_loss: 0.17246732115745544
train_iter_loss: 0.23602049052715302
train_iter_loss: 0.17401625216007233
train_iter_loss: 0.18222413957118988
train_iter_loss: 0.17988221347332
train_iter_loss: 0.2449280172586441
train_iter_loss: 0.10454921424388885
train_iter_loss: 0.1838885247707367
train_iter_loss: 0.17407533526420593
train_iter_loss: 0.16767378151416779
train_iter_loss: 0.2921951413154602
train_iter_loss: 0.14438356459140778
train_iter_loss: 0.179465651512146
train_iter_loss: 0.3395909070968628
train_iter_loss: 0.14758490025997162
train_iter_loss: 0.16430003941059113
train_iter_loss: 0.1330702304840088
train_iter_loss: 0.1866111010313034
train_iter_loss: 0.06455035507678986
train_iter_loss: 0.1534978747367859
train_iter_loss: 0.15501369535923004
train_iter_loss: 0.16097491979599
train_iter_loss: 0.15394999086856842
train_iter_loss: 0.20444828271865845
train_iter_loss: 0.2397124469280243
train_iter_loss: 0.1634417176246643
train_iter_loss: 0.18164993822574615
train_iter_loss: 0.16621384024620056
train_iter_loss: 0.18003085255622864
train_iter_loss: 0.272595077753067
train_iter_loss: 0.13445356488227844
train_iter_loss: 0.12276864051818848
train_iter_loss: 0.1548992395401001
train_iter_loss: 0.32339373230934143
train_iter_loss: 0.21054822206497192
train_iter_loss: 0.18004024028778076
train_iter_loss: 0.12458886951208115
train_iter_loss: 0.17332430183887482
train_iter_loss: 0.20518158376216888
train_iter_loss: 0.17596788704395294
train_iter_loss: 0.17287881672382355
train_iter_loss: 0.10596300661563873
train_iter_loss: 0.14298313856124878
train_iter_loss: 0.10261397808790207
train_iter_loss: 0.3164677321910858
train_iter_loss: 0.20263276994228363
train_iter_loss: 0.30908462405204773
train_iter_loss: 0.11783638596534729
train_iter_loss: 0.25244492292404175
train_iter_loss: 0.22689561545848846
train_iter_loss: 0.19738925993442535
train_iter_loss: 0.13189806044101715
train_iter_loss: 0.1066528931260109
train_iter_loss: 0.06862515956163406
train_iter_loss: 0.12978963553905487
train_iter_loss: 0.27976861596107483
train_iter_loss: 0.11958581954240799
train_iter_loss: 0.18035109341144562
train_iter_loss: 0.31003326177597046
train_iter_loss: 0.23060354590415955
train_iter_loss: 0.1696513146162033
train_iter_loss: 0.26196378469467163
train_iter_loss: 0.18996413052082062
train_iter_loss: 0.24734532833099365
train_iter_loss: 0.3945690095424652
train_iter_loss: 0.369119793176651
train_iter_loss: 0.15133674442768097
train_iter_loss: 0.1419958770275116
train_iter_loss: 0.18668459355831146
train_iter_loss: 0.2124345302581787
train loss :0.1860
---------------------
Validation seg loss: 0.22625259129014216 at epoch 90
********************
best_val_epoch_loss:  0.22625259129014216
MODEL UPDATED
epoch =     91/  1000, exp = train
train_iter_loss: 0.22035495936870575
train_iter_loss: 0.12527745962142944
train_iter_loss: 0.14138631522655487
train_iter_loss: 0.2750159502029419
train_iter_loss: 0.14098261296749115
train_iter_loss: 0.09313859045505524
train_iter_loss: 0.2674463093280792
train_iter_loss: 0.23384183645248413
train_iter_loss: 0.15687933564186096
train_iter_loss: 0.14978431165218353
train_iter_loss: 0.22568626701831818
train_iter_loss: 0.33931878209114075
train_iter_loss: 0.17618417739868164
train_iter_loss: 0.11787846684455872
train_iter_loss: 0.1440906971693039
train_iter_loss: 0.23992154002189636
train_iter_loss: 0.17779536545276642
train_iter_loss: 0.17160983383655548
train_iter_loss: 0.25131914019584656
train_iter_loss: 0.1243201419711113
train_iter_loss: 0.13819748163223267
train_iter_loss: 0.25123459100723267
train_iter_loss: 0.2515093982219696
train_iter_loss: 0.19070707261562347
train_iter_loss: 0.12704870104789734
train_iter_loss: 0.10279859602451324
train_iter_loss: 0.21732641756534576
train_iter_loss: 0.23083235323429108
train_iter_loss: 0.2599886953830719
train_iter_loss: 0.1760379821062088
train_iter_loss: 0.1252184510231018
train_iter_loss: 0.16139623522758484
train_iter_loss: 0.22851088643074036
train_iter_loss: 0.27418389916419983
train_iter_loss: 0.1693202555179596
train_iter_loss: 0.1411542147397995
train_iter_loss: 0.33112531900405884
train_iter_loss: 0.12120146304368973
train_iter_loss: 0.12308689206838608
train_iter_loss: 0.20637479424476624
train_iter_loss: 0.21703429520130157
train_iter_loss: 0.23620536923408508
train_iter_loss: 0.18959036469459534
train_iter_loss: 0.10173046588897705
train_iter_loss: 0.1359543800354004
train_iter_loss: 0.11014977842569351
train_iter_loss: 0.0628124326467514
train_iter_loss: 0.2579568922519684
train_iter_loss: 0.16013163328170776
train_iter_loss: 0.1593514084815979
train_iter_loss: 0.18559277057647705
train_iter_loss: 0.23112092912197113
train_iter_loss: 0.10589539259672165
train_iter_loss: 0.11628539115190506
train_iter_loss: 0.1733192354440689
train_iter_loss: 0.11634407192468643
train_iter_loss: 0.160114124417305
train_iter_loss: 0.2017398327589035
train_iter_loss: 0.21154733002185822
train_iter_loss: 0.217667356133461
train_iter_loss: 0.12437903136014938
train_iter_loss: 0.16473257541656494
train_iter_loss: 0.1209234893321991
train_iter_loss: 0.22879624366760254
train_iter_loss: 0.13462282717227936
train_iter_loss: 0.30608150362968445
train_iter_loss: 0.1631144881248474
train_iter_loss: 0.131039559841156
train_iter_loss: 0.1222483366727829
train_iter_loss: 0.13574115931987762
train_iter_loss: 0.22779376804828644
train_iter_loss: 0.1123180165886879
train_iter_loss: 0.18101517856121063
train_iter_loss: 0.15231196582317352
train_iter_loss: 0.15974320471286774
train_iter_loss: 0.10820674896240234
train_iter_loss: 0.24680902063846588
train_iter_loss: 0.2548360824584961
train_iter_loss: 0.13963760435581207
train_iter_loss: 0.19575759768486023
train_iter_loss: 0.3346872925758362
train_iter_loss: 0.2772190272808075
train_iter_loss: 0.20992785692214966
train_iter_loss: 0.19179989397525787
train_iter_loss: 0.11538732796907425
train_iter_loss: 0.11727958172559738
train_iter_loss: 0.14941874146461487
train_iter_loss: 0.10197166353464127
train_iter_loss: 0.14490258693695068
train_iter_loss: 0.17161807417869568
train_iter_loss: 0.15666614472866058
train_iter_loss: 0.2885609269142151
train_iter_loss: 0.17609082162380219
train_iter_loss: 0.1129758432507515
train_iter_loss: 0.1997464895248413
train_iter_loss: 0.15047545731067657
train_iter_loss: 0.16466763615608215
train_iter_loss: 0.1987186074256897
train_iter_loss: 0.3048943877220154
train_iter_loss: 0.1503991335630417
train loss :0.1826
---------------------
Validation seg loss: 0.23209375332830087 at epoch 91
epoch =     92/  1000, exp = train
train_iter_loss: 0.16051943600177765
train_iter_loss: 0.145978644490242
train_iter_loss: 0.2183620184659958
train_iter_loss: 0.1380058228969574
train_iter_loss: 0.12912043929100037
train_iter_loss: 0.19463910162448883
train_iter_loss: 0.18121571838855743
train_iter_loss: 0.17582717537879944
train_iter_loss: 0.16006821393966675
train_iter_loss: 0.2134474217891693
train_iter_loss: 0.29186710715293884
train_iter_loss: 0.15678207576274872
train_iter_loss: 0.2230614423751831
train_iter_loss: 0.10778085142374039
train_iter_loss: 0.10238835215568542
train_iter_loss: 0.1575765609741211
train_iter_loss: 0.15021821856498718
train_iter_loss: 0.21241289377212524
train_iter_loss: 0.14667530357837677
train_iter_loss: 0.186881422996521
train_iter_loss: 0.13225214183330536
train_iter_loss: 0.2446989268064499
train_iter_loss: 0.19803842902183533
train_iter_loss: 0.20636169612407684
train_iter_loss: 0.13587647676467896
train_iter_loss: 0.17569351196289062
train_iter_loss: 0.23395472764968872
train_iter_loss: 0.2745017409324646
train_iter_loss: 0.23202869296073914
train_iter_loss: 0.14393875002861023
train_iter_loss: 0.11506088078022003
train_iter_loss: 0.06951922178268433
train_iter_loss: 0.1870788335800171
train_iter_loss: 0.2191905826330185
train_iter_loss: 0.08724670112133026
train_iter_loss: 0.1787177473306656
train_iter_loss: 0.21811266243457794
train_iter_loss: 0.12465378642082214
train_iter_loss: 0.13676325976848602
train_iter_loss: 0.1646515280008316
train_iter_loss: 0.30147698521614075
train_iter_loss: 0.12868043780326843
train_iter_loss: 0.31759560108184814
train_iter_loss: 0.11966990679502487
train_iter_loss: 0.17168985307216644
train_iter_loss: 0.250998318195343
train_iter_loss: 0.1411914974451065
train_iter_loss: 0.1500493586063385
train_iter_loss: 0.1990235447883606
train_iter_loss: 0.208018496632576
train_iter_loss: 0.17786811292171478
train_iter_loss: 0.31394776701927185
train_iter_loss: 0.10494040697813034
train_iter_loss: 0.2669760584831238
train_iter_loss: 0.14221449196338654
train_iter_loss: 0.15729361772537231
train_iter_loss: 0.2562429904937744
train_iter_loss: 0.20801615715026855
train_iter_loss: 0.29048457741737366
train_iter_loss: 0.2046121209859848
train_iter_loss: 0.17215123772621155
train_iter_loss: 0.19174763560295105
train_iter_loss: 0.28290271759033203
train_iter_loss: 0.13334256410598755
train_iter_loss: 0.1269269436597824
train_iter_loss: 0.12660135328769684
train_iter_loss: 0.09208153188228607
train_iter_loss: 0.11985305696725845
train_iter_loss: 0.17690689861774445
train_iter_loss: 0.16007362306118011
train_iter_loss: 0.1518716812133789
train_iter_loss: 0.13946856558322906
train_iter_loss: 0.15187397599220276
train_iter_loss: 0.17278043925762177
train_iter_loss: 0.17855685949325562
train_iter_loss: 0.103624626994133
train_iter_loss: 0.23822516202926636
train_iter_loss: 0.07826767861843109
train_iter_loss: 0.1505419760942459
train_iter_loss: 0.19327524304389954
train_iter_loss: 0.23952548205852509
train_iter_loss: 0.1594858169555664
train_iter_loss: 0.1970086544752121
train_iter_loss: 0.187178835272789
train_iter_loss: 0.2732824981212616
train_iter_loss: 0.187102809548378
train_iter_loss: 0.17850527167320251
train_iter_loss: 0.16692854464054108
train_iter_loss: 0.19595953822135925
train_iter_loss: 0.186968594789505
train_iter_loss: 0.25638166069984436
train_iter_loss: 0.1301880031824112
train_iter_loss: 0.07203689962625504
train_iter_loss: 0.14572623372077942
train_iter_loss: 0.1956050992012024
train_iter_loss: 0.15087245404720306
train_iter_loss: 0.21914681792259216
train_iter_loss: 0.26610705256462097
train_iter_loss: 0.16028569638729095
train_iter_loss: 0.2630232870578766
train loss :0.1820
---------------------
Validation seg loss: 0.2270994942529865 at epoch 92
epoch =     93/  1000, exp = train
train_iter_loss: 0.2267138808965683
train_iter_loss: 0.12509199976921082
train_iter_loss: 0.23795552551746368
train_iter_loss: 0.14432555437088013
train_iter_loss: 0.17052564024925232
train_iter_loss: 0.08719875663518906
train_iter_loss: 0.11701580882072449
train_iter_loss: 0.16416847705841064
train_iter_loss: 0.4150063097476959
train_iter_loss: 0.14430278539657593
train_iter_loss: 0.24988283216953278
train_iter_loss: 0.17732906341552734
train_iter_loss: 0.1532290130853653
train_iter_loss: 0.3488527238368988
train_iter_loss: 0.14878061413764954
train_iter_loss: 0.12811297178268433
train_iter_loss: 0.2274896204471588
train_iter_loss: 0.16261513531208038
train_iter_loss: 0.2808004319667816
train_iter_loss: 0.15230801701545715
train_iter_loss: 0.18307220935821533
train_iter_loss: 0.12234342843294144
train_iter_loss: 0.1896151900291443
train_iter_loss: 0.18863661587238312
train_iter_loss: 0.14808213710784912
train_iter_loss: 0.21549156308174133
train_iter_loss: 0.06423764675855637
train_iter_loss: 0.17472995817661285
train_iter_loss: 0.3027801215648651
train_iter_loss: 0.12421216815710068
train_iter_loss: 0.13305440545082092
train_iter_loss: 0.19017523527145386
train_iter_loss: 0.13024547696113586
train_iter_loss: 0.1293112188577652
train_iter_loss: 0.1811104565858841
train_iter_loss: 0.23035168647766113
train_iter_loss: 0.15309157967567444
train_iter_loss: 0.17938733100891113
train_iter_loss: 0.19710412621498108
train_iter_loss: 0.1560036987066269
train_iter_loss: 0.15247182548046112
train_iter_loss: 0.27986854314804077
train_iter_loss: 0.23272478580474854
train_iter_loss: 0.1601933389902115
train_iter_loss: 0.1958921104669571
train_iter_loss: 0.2663840651512146
train_iter_loss: 0.23411498963832855
train_iter_loss: 0.35300031304359436
train_iter_loss: 0.21663576364517212
train_iter_loss: 0.18106381595134735
train_iter_loss: 0.24492622911930084
train_iter_loss: 0.09825491160154343
train_iter_loss: 0.19397389888763428
train_iter_loss: 0.14055199921131134
train_iter_loss: 0.19848822057247162
train_iter_loss: 0.17883466184139252
train_iter_loss: 0.08328945934772491
train_iter_loss: 0.08592405915260315
train_iter_loss: 0.18830327689647675
train_iter_loss: 0.19380585849285126
train_iter_loss: 0.1950569450855255
train_iter_loss: 0.19448165595531464
train_iter_loss: 0.2126627117395401
train_iter_loss: 0.17892499268054962
train_iter_loss: 0.17066746950149536
train_iter_loss: 0.20827564597129822
train_iter_loss: 0.19718867540359497
train_iter_loss: 0.11592921614646912
train_iter_loss: 0.3305038809776306
train_iter_loss: 0.2516475021839142
train_iter_loss: 0.17438070476055145
train_iter_loss: 0.1745087206363678
train_iter_loss: 0.1658945083618164
train_iter_loss: 0.22385025024414062
train_iter_loss: 0.19646351039409637
train_iter_loss: 0.18291231989860535
train_iter_loss: 0.12417206913232803
train_iter_loss: 0.20370811223983765
train_iter_loss: 0.15176880359649658
train_iter_loss: 0.18638503551483154
train_iter_loss: 0.15606725215911865
train_iter_loss: 0.34932562708854675
train_iter_loss: 0.17614924907684326
train_iter_loss: 0.25024840235710144
train_iter_loss: 0.12501680850982666
train_iter_loss: 0.03978859633207321
train_iter_loss: 0.07662378996610641
train_iter_loss: 0.13837409019470215
train_iter_loss: 0.15658222138881683
train_iter_loss: 0.16047972440719604
train_iter_loss: 0.11764205992221832
train_iter_loss: 0.12995977699756622
train_iter_loss: 0.11023412644863129
train_iter_loss: 0.22209833562374115
train_iter_loss: 0.1256229728460312
train_iter_loss: 0.07880010455846786
train_iter_loss: 0.328953355550766
train_iter_loss: 0.19883058965206146
train_iter_loss: 0.2204851657152176
train_iter_loss: 0.05657306686043739
train loss :0.1837
---------------------
Validation seg loss: 0.2251640114195223 at epoch 93
********************
best_val_epoch_loss:  0.2251640114195223
MODEL UPDATED
epoch =     94/  1000, exp = train
train_iter_loss: 0.127567857503891
train_iter_loss: 0.20221835374832153
train_iter_loss: 0.0922350212931633
train_iter_loss: 0.13901355862617493
train_iter_loss: 0.27484241127967834
train_iter_loss: 0.16943961381912231
train_iter_loss: 0.1904832273721695
train_iter_loss: 0.10914729535579681
train_iter_loss: 0.1812446564435959
train_iter_loss: 0.24446161091327667
train_iter_loss: 0.13190755248069763
train_iter_loss: 0.2475079894065857
train_iter_loss: 0.1338745653629303
train_iter_loss: 0.22149619460105896
train_iter_loss: 0.18148623406887054
train_iter_loss: 0.18253470957279205
train_iter_loss: 0.14758558571338654
train_iter_loss: 0.10028555244207382
train_iter_loss: 0.1678035408258438
train_iter_loss: 0.05917052552103996
train_iter_loss: 0.14932195842266083
train_iter_loss: 0.17131558060646057
train_iter_loss: 0.1803435981273651
train_iter_loss: 0.14975392818450928
train_iter_loss: 0.14736583828926086
train_iter_loss: 0.12944722175598145
train_iter_loss: 0.16330452263355255
train_iter_loss: 0.3196913003921509
train_iter_loss: 0.17873579263687134
train_iter_loss: 0.21895268559455872
train_iter_loss: 0.36078348755836487
train_iter_loss: 0.22107817232608795
train_iter_loss: 0.10703259706497192
train_iter_loss: 0.19475257396697998
train_iter_loss: 0.2516632378101349
train_iter_loss: 0.17906196415424347
train_iter_loss: 0.21945451200008392
train_iter_loss: 0.15135392546653748
train_iter_loss: 0.21492162346839905
train_iter_loss: 0.19639362394809723
train_iter_loss: 0.1915537714958191
train_iter_loss: 0.1530856192111969
train_iter_loss: 0.37211161851882935
train_iter_loss: 0.15982922911643982
train_iter_loss: 0.1314111202955246
train_iter_loss: 0.22133739292621613
train_iter_loss: 0.09484723210334778
train_iter_loss: 0.1543506532907486
train_iter_loss: 0.10826417803764343
train_iter_loss: 0.1364952176809311
train_iter_loss: 0.2235317975282669
train_iter_loss: 0.2692793011665344
train_iter_loss: 0.28585144877433777
train_iter_loss: 0.28149524331092834
train_iter_loss: 0.19167208671569824
train_iter_loss: 0.14141537249088287
train_iter_loss: 0.19034895300865173
train_iter_loss: 0.1773660033941269
train_iter_loss: 0.15549348294734955
train_iter_loss: 0.1198432669043541
train_iter_loss: 0.1072467714548111
train_iter_loss: 0.10350209474563599
train_iter_loss: 0.19886356592178345
train_iter_loss: 0.15916892886161804
train_iter_loss: 0.25016260147094727
train_iter_loss: 0.07884091138839722
train_iter_loss: 0.10513568669557571
train_iter_loss: 0.22682131826877594
train_iter_loss: 0.19695726037025452
train_iter_loss: 0.17635303735733032
train_iter_loss: 0.2668227553367615
train_iter_loss: 0.14159883558750153
train_iter_loss: 0.07733074575662613
train_iter_loss: 0.23282238841056824
train_iter_loss: 0.1799677014350891
train_iter_loss: 0.4211623966693878
train_iter_loss: 0.068393275141716
train_iter_loss: 0.27535203099250793
train_iter_loss: 0.2129991203546524
train_iter_loss: 0.23118433356285095
train_iter_loss: 0.22121690213680267
train_iter_loss: 0.21616137027740479
train_iter_loss: 0.23284298181533813
train_iter_loss: 0.16864348948001862
train_iter_loss: 0.1416342407464981
train_iter_loss: 0.1549627184867859
train_iter_loss: 0.1630055010318756
train_iter_loss: 0.128716841340065
train_iter_loss: 0.12971079349517822
train_iter_loss: 0.08173427730798721
train_iter_loss: 0.09871940314769745
train_iter_loss: 0.19716177880764008
train_iter_loss: 0.2702977955341339
train_iter_loss: 0.15021705627441406
train_iter_loss: 0.17918382585048676
train_iter_loss: 0.12007493525743484
train_iter_loss: 0.1225060224533081
train_iter_loss: 0.20039701461791992
train_iter_loss: 0.1801002323627472
train_iter_loss: 0.24865147471427917
train loss :0.1827
---------------------
Validation seg loss: 0.22700442871043705 at epoch 94
epoch =     95/  1000, exp = train
train_iter_loss: 0.1448851376771927
train_iter_loss: 0.1252952218055725
train_iter_loss: 0.24318954348564148
train_iter_loss: 0.18521040678024292
train_iter_loss: 0.09674923121929169
train_iter_loss: 0.11198076605796814
train_iter_loss: 0.242782860994339
train_iter_loss: 0.17972862720489502
train_iter_loss: 0.157229945063591
train_iter_loss: 0.1271325945854187
train_iter_loss: 0.18144892156124115
train_iter_loss: 0.19103504717350006
train_iter_loss: 0.12033559381961823
train_iter_loss: 0.1316535472869873
train_iter_loss: 0.17378072440624237
train_iter_loss: 0.177192822098732
train_iter_loss: 0.16881632804870605
train_iter_loss: 0.1674138605594635
train_iter_loss: 0.17498862743377686
train_iter_loss: 0.23826393485069275
train_iter_loss: 0.2551957964897156
train_iter_loss: 0.15514428913593292
train_iter_loss: 0.34147292375564575
train_iter_loss: 0.13980063796043396
train_iter_loss: 0.18077661097049713
train_iter_loss: 0.14359959959983826
train_iter_loss: 0.21963220834732056
train_iter_loss: 0.24114122986793518
train_iter_loss: 0.13081666827201843
train_iter_loss: 0.22728398442268372
train_iter_loss: 0.16132187843322754
train_iter_loss: 0.14480240643024445
train_iter_loss: 0.20759370923042297
train_iter_loss: 0.4291808009147644
train_iter_loss: 0.18885889649391174
train_iter_loss: 0.16181567311286926
train_iter_loss: 0.17108115553855896
train_iter_loss: 0.13032624125480652
train_iter_loss: 0.0984719917178154
train_iter_loss: 0.14613711833953857
train_iter_loss: 0.19007401168346405
train_iter_loss: 0.19350630044937134
train_iter_loss: 0.16399827599525452
train_iter_loss: 0.2594539523124695
train_iter_loss: 0.0908670425415039
train_iter_loss: 0.12725241482257843
train_iter_loss: 0.2144668698310852
train_iter_loss: 0.15122006833553314
train_iter_loss: 0.16627071797847748
train_iter_loss: 0.14945288002490997
train_iter_loss: 0.18539206683635712
train_iter_loss: 0.1729520708322525
train_iter_loss: 0.21269087493419647
train_iter_loss: 0.09499906003475189
train_iter_loss: 0.14713357388973236
train_iter_loss: 0.1997469812631607
train_iter_loss: 0.11476641893386841
train_iter_loss: 0.19326889514923096
train_iter_loss: 0.285738468170166
train_iter_loss: 0.1974521428346634
train_iter_loss: 0.11392385512590408
train_iter_loss: 0.10942261666059494
train_iter_loss: 0.22815002501010895
train_iter_loss: 0.15587560832500458
train_iter_loss: 0.07647022604942322
train_iter_loss: 0.29296669363975525
train_iter_loss: 0.11817599087953568
train_iter_loss: 0.21935278177261353
train_iter_loss: 0.21060766279697418
train_iter_loss: 0.3048326075077057
train_iter_loss: 0.21562238037586212
train_iter_loss: 0.15169769525527954
train_iter_loss: 0.17722703516483307
train_iter_loss: 0.2249138504266739
train_iter_loss: 0.2269822061061859
train_iter_loss: 0.13258157670497894
train_iter_loss: 0.1933564990758896
train_iter_loss: 0.12963663041591644
train_iter_loss: 0.1609296202659607
train_iter_loss: 0.14040225744247437
train_iter_loss: 0.1531454175710678
train_iter_loss: 0.15610744059085846
train_iter_loss: 0.11600037664175034
train_iter_loss: 0.18053187429904938
train_iter_loss: 0.37065568566322327
train_iter_loss: 0.1103903204202652
train_iter_loss: 0.20153722167015076
train_iter_loss: 0.14531300961971283
train_iter_loss: 0.1619202345609665
train_iter_loss: 0.375047504901886
train_iter_loss: 0.2056470811367035
train_iter_loss: 0.2759757339954376
train_iter_loss: 0.18464688956737518
train_iter_loss: 0.14689786732196808
train_iter_loss: 0.17256537079811096
train_iter_loss: 0.1695304960012436
train_iter_loss: 0.11528384685516357
train_iter_loss: 0.1318637877702713
train_iter_loss: 0.31859463453292847
train_iter_loss: 0.19669875502586365
train loss :0.1838
---------------------
Validation seg loss: 0.2276824971414962 at epoch 95
epoch =     96/  1000, exp = train
train_iter_loss: 0.15692876279354095
train_iter_loss: 0.11119946092367172
train_iter_loss: 0.15874521434307098
train_iter_loss: 0.18627066910266876
train_iter_loss: 0.2615400552749634
train_iter_loss: 0.16312448680400848
train_iter_loss: 0.15561950206756592
train_iter_loss: 0.16428081691265106
train_iter_loss: 0.13706780970096588
train_iter_loss: 0.15349963307380676
train_iter_loss: 0.10259370505809784
train_iter_loss: 0.23469525575637817
train_iter_loss: 0.1498212367296219
train_iter_loss: 0.2062080055475235
train_iter_loss: 0.19874002039432526
train_iter_loss: 0.19653098285198212
train_iter_loss: 0.2078419029712677
train_iter_loss: 0.44784948229789734
train_iter_loss: 0.21692661941051483
train_iter_loss: 0.22248484194278717
train_iter_loss: 0.18770769238471985
train_iter_loss: 0.2494918555021286
train_iter_loss: 0.29971063137054443
train_iter_loss: 0.1725480705499649
train_iter_loss: 0.13772845268249512
train_iter_loss: 0.1322501301765442
train_iter_loss: 0.14854685962200165
train_iter_loss: 0.21987225115299225
train_iter_loss: 0.12503841519355774
train_iter_loss: 0.15546081960201263
train_iter_loss: 0.19279244542121887
train_iter_loss: 0.29230043292045593
train_iter_loss: 0.09187354892492294
train_iter_loss: 0.29862916469573975
train_iter_loss: 0.2426522970199585
train_iter_loss: 0.12222430855035782
train_iter_loss: 0.10868790000677109
train_iter_loss: 0.30141156911849976
train_iter_loss: 0.20593607425689697
train_iter_loss: 0.1585630476474762
train_iter_loss: 0.15393424034118652
train_iter_loss: 0.16537941992282867
train_iter_loss: 0.23732593655586243
train_iter_loss: 0.13620585203170776
train_iter_loss: 0.2246398776769638
train_iter_loss: 0.11929905414581299
train_iter_loss: 0.14195504784584045
train_iter_loss: 0.2231721431016922
train_iter_loss: 0.2640722393989563
train_iter_loss: 0.07017125189304352
train_iter_loss: 0.21989381313323975
train_iter_loss: 0.2325405329465866
train_iter_loss: 0.16068241000175476
train_iter_loss: 0.2438066601753235
train_iter_loss: 0.12490079551935196
train_iter_loss: 0.16211432218551636
train_iter_loss: 0.2257360965013504
train_iter_loss: 0.16612020134925842
train_iter_loss: 0.15643411874771118
train_iter_loss: 0.18926170468330383
train_iter_loss: 0.22582173347473145
train_iter_loss: 0.21504513919353485
train_iter_loss: 0.21288329362869263
train_iter_loss: 0.2508123517036438
train_iter_loss: 0.19888520240783691
train_iter_loss: 0.2834705710411072
train_iter_loss: 0.15001912415027618
train_iter_loss: 0.28396934270858765
train_iter_loss: 0.10741604119539261
train_iter_loss: 0.15422040224075317
train_iter_loss: 0.09876187145709991
train_iter_loss: 0.08006434887647629
train_iter_loss: 0.12975898385047913
train_iter_loss: 0.24161046743392944
train_iter_loss: 0.2761412262916565
train_iter_loss: 0.12167655676603317
train_iter_loss: 0.17592854797840118
train_iter_loss: 0.1673945188522339
train_iter_loss: 0.04452253505587578
train_iter_loss: 0.14587071537971497
train_iter_loss: 0.19477586448192596
train_iter_loss: 0.30891141295433044
train_iter_loss: 0.241494283080101
train_iter_loss: 0.11865580826997757
train_iter_loss: 0.2086041122674942
train_iter_loss: 0.19061191380023956
train_iter_loss: 0.14897412061691284
train_iter_loss: 0.20434939861297607
train_iter_loss: 0.12451589852571487
train_iter_loss: 0.20286495983600616
train_iter_loss: 0.12632720172405243
train_iter_loss: 0.10605472326278687
train_iter_loss: 0.21650536358356476
train_iter_loss: 0.1668805032968521
train_iter_loss: 0.18400786817073822
train_iter_loss: 0.2148449867963791
train_iter_loss: 0.2198348492383957
train_iter_loss: 0.128601536154747
train_iter_loss: 0.18630501627922058
train_iter_loss: 0.25625526905059814
train loss :0.1879
---------------------
Validation seg loss: 0.22683466206533168 at epoch 96
epoch =     97/  1000, exp = train
train_iter_loss: 0.12673352658748627
train_iter_loss: 0.18523840606212616
train_iter_loss: 0.25242775678634644
train_iter_loss: 0.20899748802185059
train_iter_loss: 0.15700966119766235
train_iter_loss: 0.18109752237796783
train_iter_loss: 0.23631055653095245
train_iter_loss: 0.19567401707172394
train_iter_loss: 0.2599523961544037
train_iter_loss: 0.0947561040520668
train_iter_loss: 0.1908792406320572
train_iter_loss: 0.18423278629779816
train_iter_loss: 0.28403082489967346
train_iter_loss: 0.06856115907430649
train_iter_loss: 0.09952021390199661
train_iter_loss: 0.2260926365852356
train_iter_loss: 0.27828702330589294
train_iter_loss: 0.1613033264875412
train_iter_loss: 0.07725600153207779
train_iter_loss: 0.26284900307655334
train_iter_loss: 0.09721232950687408
train_iter_loss: 0.25305822491645813
train_iter_loss: 0.15820342302322388
train_iter_loss: 0.3000306189060211
train_iter_loss: 0.0837327241897583
train_iter_loss: 0.279209166765213
train_iter_loss: 0.12968167662620544
train_iter_loss: 0.08733511716127396
train_iter_loss: 0.16283512115478516
train_iter_loss: 0.23915758728981018
train_iter_loss: 0.194439098238945
train_iter_loss: 0.23745551705360413
train_iter_loss: 0.21477316319942474
train_iter_loss: 0.2141965925693512
train_iter_loss: 0.12544892728328705
train_iter_loss: 0.16004864871501923
train_iter_loss: 0.1777990311384201
train_iter_loss: 0.16601736843585968
train_iter_loss: 0.13701266050338745
train_iter_loss: 0.12315667420625687
train_iter_loss: 0.19162128865718842
train_iter_loss: 0.10392892360687256
train_iter_loss: 0.2999815046787262
train_iter_loss: 0.10764504224061966
train_iter_loss: 0.10533244162797928
train_iter_loss: 0.2917138934135437
train_iter_loss: 0.21338537335395813
train_iter_loss: 0.2893574833869934
train_iter_loss: 0.20728886127471924
train_iter_loss: 0.23130418360233307
train_iter_loss: 0.25674039125442505
train_iter_loss: 0.2333441823720932
train_iter_loss: 0.2391006201505661
train_iter_loss: 0.10960619151592255
train_iter_loss: 0.27113720774650574
train_iter_loss: 0.31122738122940063
train_iter_loss: 0.20922140777111053
train_iter_loss: 0.20449909567832947
train_iter_loss: 0.18012866377830505
train_iter_loss: 0.13542279601097107
train_iter_loss: 0.20436692237854004
train_iter_loss: 0.20206300914287567
train_iter_loss: 0.16206753253936768
train_iter_loss: 0.15262950956821442
train_iter_loss: 0.11600717902183533
train_iter_loss: 0.12632662057876587
train_iter_loss: 0.1315857321023941
train_iter_loss: 0.12246282398700714
train_iter_loss: 0.2064647227525711
train_iter_loss: 0.11267809569835663
train_iter_loss: 0.24574224650859833
train_iter_loss: 0.12973067164421082
train_iter_loss: 0.31089648604393005
train_iter_loss: 0.12810225784778595
train_iter_loss: 0.13892598450183868
train_iter_loss: 0.16558830440044403
train_iter_loss: 0.12888868153095245
train_iter_loss: 0.25623342394828796
train_iter_loss: 0.05504243075847626
train_iter_loss: 0.05342560261487961
train_iter_loss: 0.12366317957639694
train_iter_loss: 0.2381965070962906
train_iter_loss: 0.2632306218147278
train_iter_loss: 0.142011359333992
train_iter_loss: 0.26450207829475403
train_iter_loss: 0.16663731634616852
train_iter_loss: 0.23506322503089905
train_iter_loss: 0.13141128420829773
train_iter_loss: 0.16871902346611023
train_iter_loss: 0.4319600760936737
train_iter_loss: 0.18873634934425354
train_iter_loss: 0.14580288529396057
train_iter_loss: 0.13147391378879547
train_iter_loss: 0.20163388550281525
train_iter_loss: 0.09793513268232346
train_iter_loss: 0.18362918496131897
train_iter_loss: 0.19322770833969116
train_iter_loss: 0.2750283181667328
train_iter_loss: 0.0886082798242569
train_iter_loss: 0.1666308045387268
train loss :0.1864
---------------------
Validation seg loss: 0.22649046773407258 at epoch 97
epoch =     98/  1000, exp = train
train_iter_loss: 0.19700659811496735
train_iter_loss: 0.24207951128482819
train_iter_loss: 0.14384034276008606
train_iter_loss: 0.08200196176767349
train_iter_loss: 0.27604085206985474
train_iter_loss: 0.17530907690525055
train_iter_loss: 0.10288718342781067
train_iter_loss: 0.09911660850048065
train_iter_loss: 0.1770370751619339
train_iter_loss: 0.2006119042634964
train_iter_loss: 0.286470502614975
train_iter_loss: 0.3607103228569031
train_iter_loss: 0.21910546720027924
train_iter_loss: 0.212772399187088
train_iter_loss: 0.17554374039173126
train_iter_loss: 0.13533911108970642
train_iter_loss: 0.13112770020961761
train_iter_loss: 0.160260409116745
train_iter_loss: 0.38750699162483215
train_iter_loss: 0.16443414986133575
train_iter_loss: 0.18148910999298096
train_iter_loss: 0.08787549287080765
train_iter_loss: 0.21228757500648499
train_iter_loss: 0.2067284882068634
train_iter_loss: 0.1796826869249344
train_iter_loss: 0.13280534744262695
train_iter_loss: 0.15759673714637756
train_iter_loss: 0.2955218255519867
train_iter_loss: 0.1128898411989212
train_iter_loss: 0.17942681908607483
train_iter_loss: 0.14198380708694458
train_iter_loss: 0.2885672152042389
train_iter_loss: 0.14304064214229584
train_iter_loss: 0.22940285503864288
train_iter_loss: 0.05410642921924591
train_iter_loss: 0.16775263845920563
train_iter_loss: 0.2426030933856964
train_iter_loss: 0.2088712453842163
train_iter_loss: 0.20080794394016266
train_iter_loss: 0.11804704368114471
train_iter_loss: 0.08688914775848389
train_iter_loss: 0.4190579354763031
train_iter_loss: 0.2470701038837433
train_iter_loss: 0.17370450496673584
train_iter_loss: 0.21726106107234955
train_iter_loss: 0.14474090933799744
train_iter_loss: 0.143474280834198
train_iter_loss: 0.19486881792545319
train_iter_loss: 0.21228747069835663
train_iter_loss: 0.15695162117481232
train_iter_loss: 0.12301471829414368
train_iter_loss: 0.1803779900074005
train_iter_loss: 0.12456363439559937
train_iter_loss: 0.16701921820640564
train_iter_loss: 0.27165937423706055
train_iter_loss: 0.17085768282413483
train_iter_loss: 0.2859036922454834
train_iter_loss: 0.23251143097877502
train_iter_loss: 0.1408914476633072
train_iter_loss: 0.1872098445892334
train_iter_loss: 0.19360722601413727
train_iter_loss: 0.11260081827640533
train_iter_loss: 0.21000149846076965
train_iter_loss: 0.23849181830883026
train_iter_loss: 0.1941782683134079
train_iter_loss: 0.08783087879419327
train_iter_loss: 0.08019310235977173
train_iter_loss: 0.09699172526597977
train_iter_loss: 0.16919857263565063
train_iter_loss: 0.08888255804777145
train_iter_loss: 0.1895114630460739
train_iter_loss: 0.16969269514083862
train_iter_loss: 0.17491589486598969
train_iter_loss: 0.1739203780889511
train_iter_loss: 0.18042105436325073
train_iter_loss: 0.23442214727401733
train_iter_loss: 0.1463039666414261
train_iter_loss: 0.14755721390247345
train_iter_loss: 0.23547904193401337
train_iter_loss: 0.23069718480110168
train_iter_loss: 0.43162161111831665
train_iter_loss: 0.16124173998832703
train_iter_loss: 0.21842136979103088
train_iter_loss: 0.12215246260166168
train_iter_loss: 0.17545165121555328
train_iter_loss: 0.1827060878276825
train_iter_loss: 0.2081248015165329
train_iter_loss: 0.22725333273410797
train_iter_loss: 0.14529642462730408
train_iter_loss: 0.19934089481830597
train_iter_loss: 0.1095767393708229
train_iter_loss: 0.23598729074001312
train_iter_loss: 0.21906805038452148
train_iter_loss: 0.10036643594503403
train_iter_loss: 0.1343420296907425
train_iter_loss: 0.1323746293783188
train_iter_loss: 0.19171492755413055
train_iter_loss: 0.11250019818544388
train_iter_loss: 0.14291156828403473
train_iter_loss: 0.1633426994085312
train loss :0.1851
---------------------
Validation seg loss: 0.2288507342619716 at epoch 98
epoch =     99/  1000, exp = train
train_iter_loss: 0.1678001582622528
train_iter_loss: 0.2866589426994324
train_iter_loss: 0.17470721900463104
train_iter_loss: 0.11547566950321198
train_iter_loss: 0.2429080307483673
train_iter_loss: 0.19922474026679993
train_iter_loss: 0.1301136612892151
train_iter_loss: 0.2509590685367584
train_iter_loss: 0.16830837726593018
train_iter_loss: 0.25797250866889954
train_iter_loss: 0.11947000026702881
train_iter_loss: 0.1674545705318451
train_iter_loss: 0.2731322646141052
train_iter_loss: 0.1251814067363739
train_iter_loss: 0.1155063807964325
train_iter_loss: 0.17086727917194366
train_iter_loss: 0.15075449645519257
train_iter_loss: 0.18960291147232056
train_iter_loss: 0.16564051806926727
train_iter_loss: 0.08742328733205795
train_iter_loss: 0.1709415465593338
train_iter_loss: 0.12127579748630524
train_iter_loss: 0.13900014758110046
train_iter_loss: 0.09152203053236008
train_iter_loss: 0.20962642133235931
train_iter_loss: 0.15366920828819275
train_iter_loss: 0.18702912330627441
train_iter_loss: 0.11956371366977692
train_iter_loss: 0.2235701084136963
train_iter_loss: 0.12298757582902908
train_iter_loss: 0.15159571170806885
train_iter_loss: 0.2052002251148224
train_iter_loss: 0.3346709907054901
train_iter_loss: 0.12468084692955017
train_iter_loss: 0.15355557203292847
train_iter_loss: 0.24855509400367737
train_iter_loss: 0.1317664235830307
train_iter_loss: 0.19062110781669617
train_iter_loss: 0.1670394241809845
train_iter_loss: 0.13726474344730377
train_iter_loss: 0.3091447055339813
train_iter_loss: 0.22391587495803833
train_iter_loss: 0.20188243687152863
train_iter_loss: 0.09865498542785645
train_iter_loss: 0.16946332156658173
train_iter_loss: 0.12814661860466003
train_iter_loss: 0.24584051966667175
train_iter_loss: 0.15559104084968567
train_iter_loss: 0.1436499059200287
train_iter_loss: 0.2100539207458496
train_iter_loss: 0.13294433057308197
train_iter_loss: 0.21248354017734528
train_iter_loss: 0.16349712014198303
train_iter_loss: 0.197919562458992
train_iter_loss: 0.3029022514820099
train_iter_loss: 0.06883975863456726
train_iter_loss: 0.16746528446674347
train_iter_loss: 0.2239927053451538
train_iter_loss: 0.12296848744153976
train_iter_loss: 0.1055084764957428
train_iter_loss: 0.12667769193649292
train_iter_loss: 0.10282415896654129
train_iter_loss: 0.20573799312114716
train_iter_loss: 0.1686692088842392
train_iter_loss: 0.1483338475227356
train_iter_loss: 0.19429603219032288
train_iter_loss: 0.3188056945800781
train_iter_loss: 0.1152489110827446
train_iter_loss: 0.16305573284626007
train_iter_loss: 0.1726262867450714
train_iter_loss: 0.13536213338375092
train_iter_loss: 0.15273652970790863
train_iter_loss: 0.2372552454471588
train_iter_loss: 0.16516828536987305
train_iter_loss: 0.20673085749149323
train_iter_loss: 0.17236678302288055
train_iter_loss: 0.10144661366939545
train_iter_loss: 0.18191568553447723
train_iter_loss: 0.1704818457365036
train_iter_loss: 0.12919825315475464
train_iter_loss: 0.13806015253067017
train_iter_loss: 0.235132098197937
train_iter_loss: 0.19665507972240448
train_iter_loss: 0.1886497586965561
train_iter_loss: 0.4112904369831085
train_iter_loss: 0.10594042390584946
train_iter_loss: 0.2086062729358673
train_iter_loss: 0.23285681009292603
train_iter_loss: 0.2834121286869049
train_iter_loss: 0.21700260043144226
train_iter_loss: 0.2510528862476349
train_iter_loss: 0.15151406824588776
train_iter_loss: 0.1008201390504837
train_iter_loss: 0.2526393234729767
train_iter_loss: 0.14572808146476746
train_iter_loss: 0.1436479687690735
train_iter_loss: 0.1799393743276596
train_iter_loss: 0.11594488471746445
train_iter_loss: 0.17449216544628143
train_iter_loss: 0.16739821434020996
train loss :0.1798
---------------------
Validation seg loss: 0.22723183394321855 at epoch 99
epoch =    100/  1000, exp = train
train_iter_loss: 0.20521977543830872
train_iter_loss: 0.3496825695037842
train_iter_loss: 0.2726212441921234
train_iter_loss: 0.2139974683523178
train_iter_loss: 0.16053880751132965
train_iter_loss: 0.08957652002573013
train_iter_loss: 0.18508292734622955
train_iter_loss: 0.13554984331130981
train_iter_loss: 0.1884148269891739
train_iter_loss: 0.24777628481388092
train_iter_loss: 0.11130259931087494
train_iter_loss: 0.16366428136825562
train_iter_loss: 0.05274387076497078
train_iter_loss: 0.15899096429347992
train_iter_loss: 0.27514442801475525
train_iter_loss: 0.15341971814632416
train_iter_loss: 0.2972700595855713
train_iter_loss: 0.1567266881465912
train_iter_loss: 0.10472307354211807
train_iter_loss: 0.25313684344291687
train_iter_loss: 0.10227387398481369
train_iter_loss: 0.23962876200675964
train_iter_loss: 0.207685187458992
train_iter_loss: 0.1713535189628601
train_iter_loss: 0.17278032004833221
train_iter_loss: 0.13621437549591064
train_iter_loss: 0.13672062754631042
train_iter_loss: 0.16337284445762634
train_iter_loss: 0.09541165083646774
train_iter_loss: 0.12963815033435822
train_iter_loss: 0.34029874205589294
train_iter_loss: 0.14054584503173828
train_iter_loss: 0.1490638107061386
train_iter_loss: 0.10770411789417267
train_iter_loss: 0.20225517451763153
train_iter_loss: 0.1265350729227066
train_iter_loss: 0.19487087428569794
train_iter_loss: 0.1330634504556656
train_iter_loss: 0.17082694172859192
train_iter_loss: 0.19709235429763794
train_iter_loss: 0.2704993188381195
train_iter_loss: 0.22053305804729462
train_iter_loss: 0.19366395473480225
train_iter_loss: 0.13389576971530914
train_iter_loss: 0.24454376101493835
train_iter_loss: 0.16507889330387115
train_iter_loss: 0.14086273312568665
train_iter_loss: 0.3486475646495819
train_iter_loss: 0.1259552389383316
train_iter_loss: 0.17583341896533966
train_iter_loss: 0.24931490421295166
train_iter_loss: 0.1286076009273529
train_iter_loss: 0.17651061713695526
train_iter_loss: 0.2399926483631134
train_iter_loss: 0.1144292950630188
train_iter_loss: 0.10159898549318314
train_iter_loss: 0.1496180146932602
train_iter_loss: 0.20929698646068573
train_iter_loss: 0.22096964716911316
train_iter_loss: 0.14241205155849457
train_iter_loss: 0.13946370780467987
train_iter_loss: 0.1629660278558731
train_iter_loss: 0.21083039045333862
train_iter_loss: 0.21329288184642792
train_iter_loss: 0.22501757740974426
train_iter_loss: 0.25774553418159485
train_iter_loss: 0.1598566174507141
train_iter_loss: 0.24165655672550201
train_iter_loss: 0.178185373544693
train_iter_loss: 0.21414539217948914
train_iter_loss: 0.20787721872329712
train_iter_loss: 0.09025724232196808
train_iter_loss: 0.21353188157081604
train_iter_loss: 0.24763639271259308
train_iter_loss: 0.15744255483150482
train_iter_loss: 0.23720838129520416
train_iter_loss: 0.2295343577861786
train_iter_loss: 0.17729966342449188
train_iter_loss: 0.16732540726661682
train_iter_loss: 0.17273952066898346
train_iter_loss: 0.11771765351295471
train_iter_loss: 0.10364530980587006
train_iter_loss: 0.17179487645626068
train_iter_loss: 0.14542339742183685
train_iter_loss: 0.2648452818393707
train_iter_loss: 0.11745157092809677
train_iter_loss: 0.20482861995697021
train_iter_loss: 0.0802779346704483
train_iter_loss: 0.16342447698116302
train_iter_loss: 0.2730375826358795
train_iter_loss: 0.09921006858348846
train_iter_loss: 0.15616430342197418
train_iter_loss: 0.14198127388954163
train_iter_loss: 0.17441292107105255
train_iter_loss: 0.16994944214820862
train_iter_loss: 0.08003480732440948
train_iter_loss: 0.13002239167690277
train_iter_loss: 0.1872231364250183
train_iter_loss: 0.24366606771945953
train_iter_loss: 0.37542712688446045
train loss :0.1833
---------------------
Validation seg loss: 0.22612039152673394 at epoch 100
epoch =    101/  1000, exp = train
train_iter_loss: 0.15328383445739746
train_iter_loss: 0.38950344920158386
train_iter_loss: 0.2019800990819931
train_iter_loss: 0.22277812659740448
train_iter_loss: 0.16970814764499664
train_iter_loss: 0.1519407033920288
train_iter_loss: 0.10110282152891159
train_iter_loss: 0.13459430634975433
train_iter_loss: 0.16337239742279053
train_iter_loss: 0.20004813373088837
train_iter_loss: 0.23874814808368683
train_iter_loss: 0.21654373407363892
train_iter_loss: 0.15230262279510498
train_iter_loss: 0.23720569908618927
train_iter_loss: 0.1678234189748764
train_iter_loss: 0.07018507272005081
train_iter_loss: 0.06534742563962936
train_iter_loss: 0.22291579842567444
train_iter_loss: 0.2429293990135193
train_iter_loss: 0.2001774162054062
train_iter_loss: 0.26087361574172974
train_iter_loss: 0.16734132170677185
train_iter_loss: 0.2061769813299179
train_iter_loss: 0.18984590470790863
train_iter_loss: 0.1301584392786026
train_iter_loss: 0.16092991828918457
train_iter_loss: 0.22929365932941437
train_iter_loss: 0.19354748725891113
train_iter_loss: 0.24008087813854218
train_iter_loss: 0.12330606579780579
train_iter_loss: 0.10627187043428421
train_iter_loss: 0.23928876221179962
train_iter_loss: 0.08663278818130493
train_iter_loss: 0.23314157128334045
train_iter_loss: 0.12799295783042908
train_iter_loss: 0.16499356925487518
train_iter_loss: 0.1863180696964264
train_iter_loss: 0.16024284064769745
train_iter_loss: 0.19234369695186615
train_iter_loss: 0.2910938858985901
train_iter_loss: 0.19585192203521729
train_iter_loss: 0.18006953597068787
train_iter_loss: 0.15817242860794067
train_iter_loss: 0.16185979545116425
train_iter_loss: 0.12735474109649658
train_iter_loss: 0.12546725571155548
train_iter_loss: 0.1788702756166458
train_iter_loss: 0.15485438704490662
train_iter_loss: 0.1994028091430664
train_iter_loss: 0.13179408013820648
train_iter_loss: 0.15196292102336884
train_iter_loss: 0.17301441729068756
train_iter_loss: 0.11322925984859467
train_iter_loss: 0.1328805685043335
train_iter_loss: 0.12168435007333755
train_iter_loss: 0.15287896990776062
train_iter_loss: 0.3314704895019531
train_iter_loss: 0.15013739466667175
train_iter_loss: 0.1855083703994751
train_iter_loss: 0.23506005108356476
train_iter_loss: 0.12880556285381317
train_iter_loss: 0.21406243741512299
train_iter_loss: 0.1909661889076233
train_iter_loss: 0.1787397414445877
train_iter_loss: 0.49989524483680725
train_iter_loss: 0.23445384204387665
train_iter_loss: 0.1806524693965912
train_iter_loss: 0.12334688752889633
train_iter_loss: 0.06813942641019821
train_iter_loss: 0.15768571197986603
train_iter_loss: 0.09500853717327118
train_iter_loss: 0.14353452622890472
train_iter_loss: 0.11449920386075974
train_iter_loss: 0.207907572388649
train_iter_loss: 0.2301223874092102
train_iter_loss: 0.19038435816764832
train_iter_loss: 0.16867181658744812
train_iter_loss: 0.2190731018781662
train_iter_loss: 0.15733233094215393
train_iter_loss: 0.17361855506896973
train_iter_loss: 0.13926182687282562
train_iter_loss: 0.31526505947113037
train_iter_loss: 0.27988502383232117
train_iter_loss: 0.168986514210701
train_iter_loss: 0.20849068462848663
train_iter_loss: 0.1103137731552124
train_iter_loss: 0.15918995440006256
train_iter_loss: 0.31289342045783997
train_iter_loss: 0.12757329642772675
train_iter_loss: 0.13549797236919403
train_iter_loss: 0.2687602639198303
train_iter_loss: 0.15026280283927917
train_iter_loss: 0.12034769356250763
train_iter_loss: 0.13720951974391937
train_iter_loss: 0.27471479773521423
train_iter_loss: 0.2951684296131134
train_iter_loss: 0.07449280470609665
train_iter_loss: 0.19236940145492554
train_iter_loss: 0.2184736728668213
train_iter_loss: 0.20372939109802246
train loss :0.1851
---------------------
Validation seg loss: 0.23892974220919158 at epoch 101
epoch =    102/  1000, exp = train
train_iter_loss: 0.07931669801473618
train_iter_loss: 0.17684057354927063
train_iter_loss: 0.07555652409791946
train_iter_loss: 0.16604633629322052
train_iter_loss: 0.2232360541820526
train_iter_loss: 0.28424516320228577
train_iter_loss: 0.20513467490673065
train_iter_loss: 0.15898096561431885
train_iter_loss: 0.12325265258550644
train_iter_loss: 0.442447692155838
train_iter_loss: 0.2661084830760956
train_iter_loss: 0.20493999123573303
train_iter_loss: 0.17316944897174835
train_iter_loss: 0.05725712701678276
train_iter_loss: 0.09424449503421783
train_iter_loss: 0.1297713965177536
train_iter_loss: 0.15876956284046173
train_iter_loss: 0.22451771795749664
train_iter_loss: 0.163767471909523
train_iter_loss: 0.17085765302181244
train_iter_loss: 0.2329346388578415
train_iter_loss: 0.24198852479457855
train_iter_loss: 0.22688904404640198
train_iter_loss: 0.19181758165359497
train_iter_loss: 0.22032445669174194
train_iter_loss: 0.3220842182636261
train_iter_loss: 0.2738535702228546
train_iter_loss: 0.14195701479911804
train_iter_loss: 0.13526204228401184
train_iter_loss: 0.14033813774585724
train_iter_loss: 0.24628174304962158
train_iter_loss: 0.33234667778015137
train_iter_loss: 0.07261853665113449
train_iter_loss: 0.15055543184280396
train_iter_loss: 0.2613510191440582
train_iter_loss: 0.28971707820892334
train_iter_loss: 0.15442509949207306
train_iter_loss: 0.25558799505233765
train_iter_loss: 0.14534498751163483
train_iter_loss: 0.17658323049545288
train_iter_loss: 0.2139260321855545
train_iter_loss: 0.1961718052625656
train_iter_loss: 0.1491822451353073
train_iter_loss: 0.2077002227306366
train_iter_loss: 0.18518948554992676
train_iter_loss: 0.126545250415802
train_iter_loss: 0.07075884193181992
train_iter_loss: 0.2447914332151413
train_iter_loss: 0.24390821158885956
train_iter_loss: 0.11568450927734375
train_iter_loss: 0.17823977768421173
train_iter_loss: 0.14116401970386505
train_iter_loss: 0.15246054530143738
train_iter_loss: 0.23303809762001038
train_iter_loss: 0.16190457344055176
train_iter_loss: 0.1680946797132492
train_iter_loss: 0.20640887320041656
train_iter_loss: 0.4065125286579132
train_iter_loss: 0.08686982095241547
train_iter_loss: 0.1896204799413681
train_iter_loss: 0.15469929575920105
train_iter_loss: 0.14611518383026123
train_iter_loss: 0.23556429147720337
train_iter_loss: 0.1199050024151802
train_iter_loss: 0.2141394019126892
train_iter_loss: 0.180037721991539
train_iter_loss: 0.08181798458099365
train_iter_loss: 0.1760547161102295
train_iter_loss: 0.18445557355880737
train_iter_loss: 0.15452472865581512
train_iter_loss: 0.07131770998239517
train_iter_loss: 0.18972907960414886
train_iter_loss: 0.25173869729042053
train_iter_loss: 0.10234668850898743
train_iter_loss: 0.134515181183815
train_iter_loss: 0.2041584849357605
train_iter_loss: 0.18543873727321625
train_iter_loss: 0.1344929188489914
train_iter_loss: 0.06609418988227844
train_iter_loss: 0.5936365127563477
train_iter_loss: 0.15291297435760498
train_iter_loss: 0.2760425806045532
train_iter_loss: 0.1826729029417038
train_iter_loss: 0.26984572410583496
train_iter_loss: 0.17839056253433228
train_iter_loss: 0.09376030415296555
train_iter_loss: 0.221348837018013
train_iter_loss: 0.20760370790958405
train_iter_loss: 0.2508777678012848
train_iter_loss: 0.1230439767241478
train_iter_loss: 0.09793888032436371
train_iter_loss: 0.08893811702728271
train_iter_loss: 0.13593478500843048
train_iter_loss: 0.3897573947906494
train_iter_loss: 0.1089453175663948
train_iter_loss: 0.14559485018253326
train_iter_loss: 0.1646667867898941
train_iter_loss: 0.16299650073051453
train_iter_loss: 0.20807009935379028
train_iter_loss: 0.09204379469156265
train loss :0.1879
---------------------
Validation seg loss: 0.2270306684779671 at epoch 102
epoch =    103/  1000, exp = train
train_iter_loss: 0.2953275144100189
train_iter_loss: 0.17141632735729218
train_iter_loss: 0.15594448149204254
train_iter_loss: 0.2503783404827118
train_iter_loss: 0.20329409837722778
train_iter_loss: 0.180550217628479
train_iter_loss: 0.1880723536014557
train_iter_loss: 0.18337595462799072
train_iter_loss: 0.2169906049966812
train_iter_loss: 0.1822887361049652
train_iter_loss: 0.23088210821151733
train_iter_loss: 0.08748021721839905
train_iter_loss: 0.12470430135726929
train_iter_loss: 0.13253116607666016
train_iter_loss: 0.17471395432949066
train_iter_loss: 0.18960556387901306
train_iter_loss: 0.22349607944488525
train_iter_loss: 0.1457114964723587
train_iter_loss: 0.13954539597034454
train_iter_loss: 0.1382683366537094
train_iter_loss: 0.11774858832359314
train_iter_loss: 0.1475512534379959
train_iter_loss: 0.08172187954187393
train_iter_loss: 0.1299494504928589
train_iter_loss: 0.16486062109470367
train_iter_loss: 0.19802606105804443
train_iter_loss: 0.1954965442419052
train_iter_loss: 0.18341097235679626
train_iter_loss: 0.25491857528686523
train_iter_loss: 0.1453939825296402
train_iter_loss: 0.3254043161869049
train_iter_loss: 0.04628276824951172
train_iter_loss: 0.2980904281139374
train_iter_loss: 0.2398565411567688
train_iter_loss: 0.3134308159351349
train_iter_loss: 0.1948002278804779
train_iter_loss: 0.2784438133239746
train_iter_loss: 0.10987887531518936
train_iter_loss: 0.2757723033428192
train_iter_loss: 0.2266727089881897
train_iter_loss: 0.12847813963890076
train_iter_loss: 0.169645294547081
train_iter_loss: 0.22754529118537903
train_iter_loss: 0.128004789352417
train_iter_loss: 0.2489919364452362
train_iter_loss: 0.20880749821662903
train_iter_loss: 0.18833239376544952
train_iter_loss: 0.1711794137954712
train_iter_loss: 0.18949037790298462
train_iter_loss: 0.2506920397281647
train_iter_loss: 0.15846486389636993
train_iter_loss: 0.16670803725719452
train_iter_loss: 0.18811556696891785
train_iter_loss: 0.22867639362812042
train_iter_loss: 0.13146308064460754
train_iter_loss: 0.22378186881542206
train_iter_loss: 0.10581569373607635
train_iter_loss: 0.1546924114227295
train_iter_loss: 0.22861334681510925
train_iter_loss: 0.16016660630702972
train_iter_loss: 0.23842386901378632
train_iter_loss: 0.1414688378572464
train_iter_loss: 0.11411184072494507
train_iter_loss: 0.23468071222305298
train_iter_loss: 0.09295604377985
train_iter_loss: 0.14314399659633636
train_iter_loss: 0.12402614951133728
train_iter_loss: 0.18060258030891418
train_iter_loss: 0.1238386258482933
train_iter_loss: 0.19688662886619568
train_iter_loss: 0.22352372109889984
train_iter_loss: 0.24295005202293396
train_iter_loss: 0.1657092124223709
train_iter_loss: 0.22210972011089325
train_iter_loss: 0.18147572875022888
train_iter_loss: 0.15511001646518707
train_iter_loss: 0.27928829193115234
train_iter_loss: 0.16573084890842438
train_iter_loss: 0.12449077516794205
train_iter_loss: 0.07123029232025146
train_iter_loss: 0.14756730198860168
train_iter_loss: 0.14000439643859863
train_iter_loss: 0.23487195372581482
train_iter_loss: 0.16302484273910522
train_iter_loss: 0.26779597997665405
train_iter_loss: 0.22689872980117798
train_iter_loss: 0.17217756807804108
train_iter_loss: 0.18256619572639465
train_iter_loss: 0.194339320063591
train_iter_loss: 0.13004079461097717
train_iter_loss: 0.13354024291038513
train_iter_loss: 0.1656394898891449
train_iter_loss: 0.18376480042934418
train_iter_loss: 0.13921689987182617
train_iter_loss: 0.1535564661026001
train_iter_loss: 0.14727157354354858
train_iter_loss: 0.1453603059053421
train_iter_loss: 0.19510997831821442
train_iter_loss: 0.13761280477046967
train_iter_loss: 0.29205015301704407
train loss :0.1836
---------------------
Validation seg loss: 0.2263905480504036 at epoch 103
epoch =    104/  1000, exp = train
train_iter_loss: 0.21472389996051788
train_iter_loss: 0.18244069814682007
train_iter_loss: 0.072364442050457
train_iter_loss: 0.16075800359249115
train_iter_loss: 0.1290159821510315
train_iter_loss: 0.2824808359146118
train_iter_loss: 0.19279800355434418
train_iter_loss: 0.19724898040294647
train_iter_loss: 0.1549765020608902
train_iter_loss: 0.1546975076198578
train_iter_loss: 0.1662394404411316
train_iter_loss: 0.34811267256736755
train_iter_loss: 0.31024169921875
train_iter_loss: 0.17491956055164337
train_iter_loss: 0.15671047568321228
train_iter_loss: 0.16489504277706146
train_iter_loss: 0.23281267285346985
train_iter_loss: 0.2911187708377838
train_iter_loss: 0.22173377871513367
train_iter_loss: 0.22193823754787445
train_iter_loss: 0.10560251772403717
train_iter_loss: 0.14589476585388184
train_iter_loss: 0.19771838188171387
train_iter_loss: 0.2866169214248657
train_iter_loss: 0.2166326344013214
train_iter_loss: 0.1384662240743637
train_iter_loss: 0.1477697491645813
train_iter_loss: 0.10873521119356155
train_iter_loss: 0.170729398727417
train_iter_loss: 0.1722402721643448
train_iter_loss: 0.1379924863576889
train_iter_loss: 0.18883287906646729
train_iter_loss: 0.1561041921377182
train_iter_loss: 0.1614597737789154
train_iter_loss: 0.18357308208942413
train_iter_loss: 0.2459011971950531
train_iter_loss: 0.1986813247203827
train_iter_loss: 0.23436523973941803
train_iter_loss: 0.18471668660640717
train_iter_loss: 0.1083301305770874
train_iter_loss: 0.22081974148750305
train_iter_loss: 0.2191251665353775
train_iter_loss: 0.17889001965522766
train_iter_loss: 0.1683884561061859
train_iter_loss: 0.12346151471138
train_iter_loss: 0.17672109603881836
train_iter_loss: 0.12516897916793823
train_iter_loss: 0.17844480276107788
train_iter_loss: 0.2017326056957245
train_iter_loss: 0.43255892395973206
train_iter_loss: 0.13817910850048065
train_iter_loss: 0.11683200299739838
train_iter_loss: 0.33518582582473755
train_iter_loss: 0.1851382553577423
train_iter_loss: 0.27003395557403564
train_iter_loss: 0.21126902103424072
train_iter_loss: 0.14847874641418457
train_iter_loss: 0.22566966712474823
train_iter_loss: 0.1016550287604332
train_iter_loss: 0.1831967532634735
train_iter_loss: 0.3282875418663025
train_iter_loss: 0.13146497309207916
train_iter_loss: 0.12449212372303009
train_iter_loss: 0.1284143626689911
train_iter_loss: 0.16009409725666046
train_iter_loss: 0.24596716463565826
train_iter_loss: 0.19879524409770966
train_iter_loss: 0.20167949795722961
train_iter_loss: 0.15909801423549652
train_iter_loss: 0.3445836901664734
train_iter_loss: 0.11631119251251221
train_iter_loss: 0.13438862562179565
train_iter_loss: 0.19110268354415894
train_iter_loss: 0.1256146878004074
train_iter_loss: 0.16942794620990753
train_iter_loss: 0.10769824683666229
train_iter_loss: 0.10221976041793823
train_iter_loss: 0.13149797916412354
train_iter_loss: 0.25988495349884033
train_iter_loss: 0.27836373448371887
train_iter_loss: 0.15959350764751434
train_iter_loss: 0.2010854333639145
train_iter_loss: 0.15130679309368134
train_iter_loss: 0.26305973529815674
train_iter_loss: 0.16587333381175995
train_iter_loss: 0.11648677289485931
train_iter_loss: 0.12074051797389984
train_iter_loss: 0.13265295326709747
train_iter_loss: 0.13366350531578064
train_iter_loss: 0.24863263964653015
train_iter_loss: 0.16871652007102966
train_iter_loss: 0.19702135026454926
train_iter_loss: 0.212834432721138
train_iter_loss: 0.25230979919433594
train_iter_loss: 0.0818093866109848
train_iter_loss: 0.2217417061328888
train_iter_loss: 0.1313873827457428
train_iter_loss: 0.2035236805677414
train_iter_loss: 0.11477679014205933
train_iter_loss: 0.16431701183319092
train loss :0.1873
---------------------
Validation seg loss: 0.22697107021187274 at epoch 104
epoch =    105/  1000, exp = train
train_iter_loss: 0.17489096522331238
train_iter_loss: 0.12058722972869873
train_iter_loss: 0.13868655264377594
train_iter_loss: 0.1349019706249237
train_iter_loss: 0.2965478301048279
train_iter_loss: 0.18262505531311035
train_iter_loss: 0.127414733171463
train_iter_loss: 0.14841561019420624
train_iter_loss: 0.1575952023267746
train_iter_loss: 0.05627725273370743
train_iter_loss: 0.2877057194709778
train_iter_loss: 0.27114176750183105
train_iter_loss: 0.18536709249019623
train_iter_loss: 0.14572545886039734
train_iter_loss: 0.22228842973709106
train_iter_loss: 0.12590482831001282
train_iter_loss: 0.19906790554523468
train_iter_loss: 0.2006903737783432
train_iter_loss: 0.15722979605197906
train_iter_loss: 0.18866997957229614
train_iter_loss: 0.20361286401748657
train_iter_loss: 0.2582653760910034
train_iter_loss: 0.22553043067455292
train_iter_loss: 0.16909071803092957
train_iter_loss: 0.09598258882761002
train_iter_loss: 0.26896968483924866
train_iter_loss: 0.22438783943653107
train_iter_loss: 0.17165614664554596
train_iter_loss: 0.26430603861808777
train_iter_loss: 0.07586213946342468
train_iter_loss: 0.19346392154693604
train_iter_loss: 0.22046004235744476
train_iter_loss: 0.24616821110248566
train_iter_loss: 0.23741960525512695
train_iter_loss: 0.12681534886360168
train_iter_loss: 0.13882668316364288
train_iter_loss: 0.16358211636543274
train_iter_loss: 0.20335787534713745
train_iter_loss: 0.19788523018360138
train_iter_loss: 0.2167973518371582
train_iter_loss: 0.20039205253124237
train_iter_loss: 0.17010365426540375
train_iter_loss: 0.10635633766651154
train_iter_loss: 0.08533943444490433
train_iter_loss: 0.18079400062561035
train_iter_loss: 0.09617538750171661
train_iter_loss: 0.10343044996261597
train_iter_loss: 0.1151827722787857
train_iter_loss: 0.20306359231472015
train_iter_loss: 0.175521582365036
train_iter_loss: 0.25579822063446045
train_iter_loss: 0.1384279727935791
train_iter_loss: 0.19311217963695526
train_iter_loss: 0.14929187297821045
train_iter_loss: 0.13408827781677246
train_iter_loss: 0.05468008667230606
train_iter_loss: 0.1283704787492752
train_iter_loss: 0.23414792120456696
train_iter_loss: 0.37974753975868225
train_iter_loss: 0.03029400296509266
train_iter_loss: 0.17536233365535736
train_iter_loss: 0.18057510256767273
train_iter_loss: 0.20795515179634094
train_iter_loss: 0.21256598830223083
train_iter_loss: 0.23916368186473846
train_iter_loss: 0.2227691411972046
train_iter_loss: 0.20609311759471893
train_iter_loss: 0.13219141960144043
train_iter_loss: 0.17542262375354767
train_iter_loss: 0.16629526019096375
train_iter_loss: 0.4176241159439087
train_iter_loss: 0.17438797652721405
train_iter_loss: 0.25573381781578064
train_iter_loss: 0.19511137902736664
train_iter_loss: 0.21781663596630096
train_iter_loss: 0.2588677704334259
train_iter_loss: 0.1618851125240326
train_iter_loss: 0.10764262080192566
train_iter_loss: 0.09764310717582703
train_iter_loss: 0.19353966414928436
train_iter_loss: 0.20710746943950653
train_iter_loss: 0.18091700971126556
train_iter_loss: 0.18541386723518372
train_iter_loss: 0.2201973795890808
train_iter_loss: 0.09454580396413803
train_iter_loss: 0.17882488667964935
train_iter_loss: 0.31602153182029724
train_iter_loss: 0.13423530757427216
train_iter_loss: 0.1456584334373474
train_iter_loss: 0.1984870582818985
train_iter_loss: 0.18238858878612518
train_iter_loss: 0.15081630647182465
train_iter_loss: 0.23352190852165222
train_iter_loss: 0.2087915688753128
train_iter_loss: 0.24846011400222778
train_iter_loss: 0.2021433562040329
train_iter_loss: 0.2587933838367462
train_iter_loss: 0.18132297694683075
train_iter_loss: 0.14709003269672394
train_iter_loss: 0.19511282444000244
train loss :0.1861
---------------------
Validation seg loss: 0.23112608664581236 at epoch 105
epoch =    106/  1000, exp = train
train_iter_loss: 0.16615839302539825
train_iter_loss: 0.12161018699407578
train_iter_loss: 0.23071342706680298
train_iter_loss: 0.15878941118717194
train_iter_loss: 0.20168519020080566
train_iter_loss: 0.31293049454689026
train_iter_loss: 0.23675808310508728
train_iter_loss: 0.15202945470809937
train_iter_loss: 0.20938195288181305
train_iter_loss: 0.2512294054031372
train_iter_loss: 0.1382468342781067
train_iter_loss: 0.1109694316983223
train_iter_loss: 0.28759539127349854
train_iter_loss: 0.2773251533508301
train_iter_loss: 0.17128971219062805
train_iter_loss: 0.10684637725353241
train_iter_loss: 0.08455163985490799
train_iter_loss: 0.19223257899284363
train_iter_loss: 0.3073408007621765
train_iter_loss: 0.15491843223571777
train_iter_loss: 0.12336989492177963
train_iter_loss: 0.16799214482307434
train_iter_loss: 0.1458790898323059
train_iter_loss: 0.14407366514205933
train_iter_loss: 0.1727103590965271
train_iter_loss: 0.08344373852014542
train_iter_loss: 0.11839596927165985
train_iter_loss: 0.2018844485282898
train_iter_loss: 0.16469065845012665
train_iter_loss: 0.16283614933490753
train_iter_loss: 0.1035517230629921
train_iter_loss: 0.19892635941505432
train_iter_loss: 0.38629478216171265
train_iter_loss: 0.22671405971050262
train_iter_loss: 0.18474049866199493
train_iter_loss: 0.22132602334022522
train_iter_loss: 0.09795766323804855
train_iter_loss: 0.14539022743701935
train_iter_loss: 0.15395160019397736
train_iter_loss: 0.1068146750330925
train_iter_loss: 0.346097856760025
train_iter_loss: 0.16510528326034546
train_iter_loss: 0.13098077476024628
train_iter_loss: 0.23373176157474518
train_iter_loss: 0.1275358349084854
train_iter_loss: 0.1569078266620636
train_iter_loss: 0.15136054158210754
train_iter_loss: 0.15011641383171082
train_iter_loss: 0.15434464812278748
train_iter_loss: 0.08428732305765152
train_iter_loss: 0.11364640295505524
train_iter_loss: 0.1602993756532669
train_iter_loss: 0.15121591091156006
train_iter_loss: 0.14978890120983124
train_iter_loss: 0.34178707003593445
train_iter_loss: 0.31936514377593994
train_iter_loss: 0.11578226834535599
train_iter_loss: 0.3152221143245697
train_iter_loss: 0.17159384489059448
train_iter_loss: 0.09227875620126724
train_iter_loss: 0.0823625773191452
train_iter_loss: 0.13358081877231598
train_iter_loss: 0.1433100402355194
train_iter_loss: 0.15809589624404907
train_iter_loss: 0.14069607853889465
train_iter_loss: 0.16384217143058777
train_iter_loss: 0.12770229578018188
train_iter_loss: 0.10804606974124908
train_iter_loss: 0.16236712038516998
train_iter_loss: 0.14764142036437988
train_iter_loss: 0.2609981894493103
train_iter_loss: 0.2042003720998764
train_iter_loss: 0.1632397174835205
train_iter_loss: 0.22443248331546783
train_iter_loss: 0.2149815559387207
train_iter_loss: 0.1659303456544876
train_iter_loss: 0.34553858637809753
train_iter_loss: 0.20178283751010895
train_iter_loss: 0.20403796434402466
train_iter_loss: 0.1263958215713501
train_iter_loss: 0.20849640667438507
train_iter_loss: 0.1680729240179062
train_iter_loss: 0.2385769784450531
train_iter_loss: 0.18329501152038574
train_iter_loss: 0.2209084928035736
train_iter_loss: 0.15511226654052734
train_iter_loss: 0.12305966019630432
train_iter_loss: 0.3186494708061218
train_iter_loss: 0.2267051488161087
train_iter_loss: 0.2216401845216751
train_iter_loss: 0.26615193486213684
train_iter_loss: 0.18391916155815125
train_iter_loss: 0.21676570177078247
train_iter_loss: 0.20769116282463074
train_iter_loss: 0.1653222143650055
train_iter_loss: 0.19097544252872467
train_iter_loss: 0.21498143672943115
train_iter_loss: 0.09861577302217484
train_iter_loss: 0.09400029480457306
train_iter_loss: 0.1338980346918106
train loss :0.1832
---------------------
Validation seg loss: 0.2248419147898566 at epoch 106
********************
best_val_epoch_loss:  0.2248419147898566
MODEL UPDATED
epoch =    107/  1000, exp = train
train_iter_loss: 0.182393416762352
train_iter_loss: 0.1586691439151764
train_iter_loss: 0.1984790414571762
train_iter_loss: 0.1824633926153183
train_iter_loss: 0.2286379635334015
train_iter_loss: 0.1940215528011322
train_iter_loss: 0.17832708358764648
train_iter_loss: 0.22536736726760864
train_iter_loss: 0.1291801780462265
train_iter_loss: 0.28974035382270813
train_iter_loss: 0.205813929438591
train_iter_loss: 0.18290431797504425
train_iter_loss: 0.25003960728645325
train_iter_loss: 0.25127577781677246
train_iter_loss: 0.18722590804100037
train_iter_loss: 0.2684997022151947
train_iter_loss: 0.22004477679729462
train_iter_loss: 0.19153156876564026
train_iter_loss: 0.13162772357463837
train_iter_loss: 0.15525579452514648
train_iter_loss: 0.2247929871082306
train_iter_loss: 0.1819404512643814
train_iter_loss: 0.2633447051048279
train_iter_loss: 0.1454247683286667
train_iter_loss: 0.100779227912426
train_iter_loss: 0.2916327118873596
train_iter_loss: 0.1497746706008911
train_iter_loss: 0.13340602815151215
train_iter_loss: 0.20674937963485718
train_iter_loss: 0.13325604796409607
train_iter_loss: 0.09670901298522949
train_iter_loss: 0.12367893010377884
train_iter_loss: 0.15631096065044403
train_iter_loss: 0.1302141547203064
train_iter_loss: 0.1081317588686943
train_iter_loss: 0.08995633572340012
train_iter_loss: 0.18903326988220215
train_iter_loss: 0.2257792204618454
train_iter_loss: 0.22100335359573364
train_iter_loss: 0.17483091354370117
train_iter_loss: 0.20085033774375916
train_iter_loss: 0.13179829716682434
train_iter_loss: 0.11478401720523834
train_iter_loss: 0.13229826092720032
train_iter_loss: 0.1579974740743637
train_iter_loss: 0.21612240374088287
train_iter_loss: 0.16147750616073608
train_iter_loss: 0.19219468533992767
train_iter_loss: 0.2696385085582733
train_iter_loss: 0.15983109176158905
train_iter_loss: 0.201792374253273
train_iter_loss: 0.18293607234954834
train_iter_loss: 0.07921598106622696
train_iter_loss: 0.12788623571395874
train_iter_loss: 0.21724706888198853
train_iter_loss: 0.1665797084569931
train_iter_loss: 0.1712583303451538
train_iter_loss: 0.211868017911911
train_iter_loss: 0.1403580904006958
train_iter_loss: 0.2429315447807312
train_iter_loss: 0.15745909512043
train_iter_loss: 0.14129000902175903
train_iter_loss: 0.18528161942958832
train_iter_loss: 0.22150053083896637
train_iter_loss: 0.2438831627368927
train_iter_loss: 0.08622409403324127
train_iter_loss: 0.14499196410179138
train_iter_loss: 0.16660800576210022
train_iter_loss: 0.15006278455257416
train_iter_loss: 0.17330008745193481
train_iter_loss: 0.3538297414779663
train_iter_loss: 0.17778322100639343
train_iter_loss: 0.2575247287750244
train_iter_loss: 0.10974302142858505
train_iter_loss: 0.2781201899051666
train_iter_loss: 0.20868922770023346
train_iter_loss: 0.1445520669221878
train_iter_loss: 0.12196984142065048
train_iter_loss: 0.23580306768417358
train_iter_loss: 0.24863973259925842
train_iter_loss: 0.14983221888542175
train_iter_loss: 0.11931462585926056
train_iter_loss: 0.07914656400680542
train_iter_loss: 0.27260902523994446
train_iter_loss: 0.18412111699581146
train_iter_loss: 0.16219407320022583
train_iter_loss: 0.1471286416053772
train_iter_loss: 0.12727373838424683
train_iter_loss: 0.14857596158981323
train_iter_loss: 0.14345262944698334
train_iter_loss: 0.20912396907806396
train_iter_loss: 0.21015197038650513
train_iter_loss: 0.12495903670787811
train_iter_loss: 0.08558434247970581
train_iter_loss: 0.22039751708507538
train_iter_loss: 0.2725263237953186
train_iter_loss: 0.20899100601673126
train_iter_loss: 0.0893816277384758
train_iter_loss: 0.1289123296737671
train_iter_loss: 0.11139214038848877
train loss :0.1802
---------------------
Validation seg loss: 0.22539402426765212 at epoch 107
epoch =    108/  1000, exp = train
train_iter_loss: 0.18084031343460083
train_iter_loss: 0.14524833858013153
train_iter_loss: 0.20413048565387726
train_iter_loss: 0.19618305563926697
train_iter_loss: 0.19932228326797485
train_iter_loss: 0.1513020247220993
train_iter_loss: 0.12221870571374893
train_iter_loss: 0.17511902749538422
train_iter_loss: 0.18543165922164917
train_iter_loss: 0.2484658658504486
train_iter_loss: 0.0530698224902153
train_iter_loss: 0.19580532610416412
train_iter_loss: 0.16148944199085236
train_iter_loss: 0.4396357834339142
train_iter_loss: 0.1481747329235077
train_iter_loss: 0.1932196319103241
train_iter_loss: 0.23654785752296448
train_iter_loss: 0.09498108923435211
train_iter_loss: 0.17750416696071625
train_iter_loss: 0.38799047470092773
train_iter_loss: 0.10654031485319138
train_iter_loss: 0.11165542900562286
train_iter_loss: 0.10311869531869888
train_iter_loss: 0.16880957782268524
train_iter_loss: 0.24487844109535217
train_iter_loss: 0.22286617755889893
train_iter_loss: 0.1310860961675644
train_iter_loss: 0.12386053055524826
train_iter_loss: 0.18445232510566711
train_iter_loss: 0.1684153527021408
train_iter_loss: 0.29245421290397644
train_iter_loss: 0.16964776813983917
train_iter_loss: 0.21799972653388977
train_iter_loss: 0.1349988877773285
train_iter_loss: 0.17071034014225006
train_iter_loss: 0.19456838071346283
train_iter_loss: 0.16405583918094635
train_iter_loss: 0.19247537851333618
train_iter_loss: 0.4810450077056885
train_iter_loss: 0.12834806740283966
train_iter_loss: 0.054064247757196426
train_iter_loss: 0.11072057485580444
train_iter_loss: 0.1731530725955963
train_iter_loss: 0.2895248532295227
train_iter_loss: 0.25095272064208984
train_iter_loss: 0.1727781742811203
train_iter_loss: 0.16506393253803253
train_iter_loss: 0.11489004641771317
train_iter_loss: 0.15635369718074799
train_iter_loss: 0.07972437143325806
train_iter_loss: 0.08846713602542877
train_iter_loss: 0.22411806881427765
train_iter_loss: 0.22217676043510437
train_iter_loss: 0.24240249395370483
train_iter_loss: 0.2985829710960388
train_iter_loss: 0.2443682998418808
train_iter_loss: 0.1910592019557953
train_iter_loss: 0.13013696670532227
train_iter_loss: 0.19069215655326843
train_iter_loss: 0.2843801975250244
train_iter_loss: 0.22497741878032684
train_iter_loss: 0.35443204641342163
train_iter_loss: 0.13511689007282257
train_iter_loss: 0.13354577124118805
train_iter_loss: 0.283781498670578
train_iter_loss: 0.3155473470687866
train_iter_loss: 0.18238432705402374
train_iter_loss: 0.15085157752037048
train_iter_loss: 0.2462327629327774
train_iter_loss: 0.1684650033712387
train_iter_loss: 0.11754626780748367
train_iter_loss: 0.26295164227485657
train_iter_loss: 0.10795416682958603
train_iter_loss: 0.27139291167259216
train_iter_loss: 0.25514841079711914
train_iter_loss: 0.1749436855316162
train_iter_loss: 0.05706220120191574
train_iter_loss: 0.13842439651489258
train_iter_loss: 0.14315004646778107
train_iter_loss: 0.14310406148433685
train_iter_loss: 0.12072092294692993
train_iter_loss: 0.14448969066143036
train_iter_loss: 0.16188277304172516
train_iter_loss: 0.203968808054924
train_iter_loss: 0.2912144362926483
train_iter_loss: 0.14339061081409454
train_iter_loss: 0.12192068994045258
train_iter_loss: 0.11984635144472122
train_iter_loss: 0.13117747008800507
train_iter_loss: 0.21743768453598022
train_iter_loss: 0.14337313175201416
train_iter_loss: 0.18405769765377045
train_iter_loss: 0.14291054010391235
train_iter_loss: 0.285587877035141
train_iter_loss: 0.22680027782917023
train_iter_loss: 0.07946673780679703
train_iter_loss: 0.14319510757923126
train_iter_loss: 0.22023195028305054
train_iter_loss: 0.14704962074756622
train_iter_loss: 0.07954366505146027
train loss :0.1865
---------------------
Validation seg loss: 0.22642690832463075 at epoch 108
epoch =    109/  1000, exp = train
train_iter_loss: 0.2785367965698242
train_iter_loss: 0.1897980272769928
train_iter_loss: 0.15468484163284302
train_iter_loss: 0.15331241488456726
train_iter_loss: 0.2065349817276001
train_iter_loss: 0.20084847509860992
train_iter_loss: 0.17722821235656738
train_iter_loss: 0.18243111670017242
train_iter_loss: 0.17766083776950836
train_iter_loss: 0.17170947790145874
train_iter_loss: 0.25616228580474854
train_iter_loss: 0.2400786429643631
train_iter_loss: 0.08624059706926346
train_iter_loss: 0.1493232399225235
train_iter_loss: 0.21517574787139893
train_iter_loss: 0.12236971408128738
train_iter_loss: 0.10660793632268906
train_iter_loss: 0.13381513953208923
train_iter_loss: 0.2244744747877121
train_iter_loss: 0.21348878741264343
train_iter_loss: 0.1680399775505066
train_iter_loss: 0.2503093481063843
train_iter_loss: 0.16470731794834137
train_iter_loss: 0.18749968707561493
train_iter_loss: 0.18680523335933685
train_iter_loss: 0.23274637758731842
train_iter_loss: 0.1947917938232422
train_iter_loss: 0.21778644621372223
train_iter_loss: 0.31507930159568787
train_iter_loss: 0.25796961784362793
train_iter_loss: 0.33473289012908936
train_iter_loss: 0.19690392911434174
train_iter_loss: 0.07747605443000793
train_iter_loss: 0.22403815388679504
train_iter_loss: 0.21504151821136475
train_iter_loss: 0.10869665443897247
train_iter_loss: 0.3715848922729492
train_iter_loss: 0.10175788402557373
train_iter_loss: 0.3946428596973419
train_iter_loss: 0.1353696882724762
train_iter_loss: 0.12559853494167328
train_iter_loss: 0.16590525209903717
train_iter_loss: 0.14910505712032318
train_iter_loss: 0.21905042231082916
train_iter_loss: 0.08922255039215088
train_iter_loss: 0.19514618813991547
train_iter_loss: 0.16380541026592255
train_iter_loss: 0.08289048820734024
train_iter_loss: 0.10295524448156357
train_iter_loss: 0.14854024350643158
train_iter_loss: 0.29605722427368164
train_iter_loss: 0.16251511871814728
train_iter_loss: 0.16561013460159302
train_iter_loss: 0.19195465743541718
train_iter_loss: 0.21192964911460876
train_iter_loss: 0.1137375459074974
train_iter_loss: 0.1866808980703354
train_iter_loss: 0.1951623111963272
train_iter_loss: 0.11557517200708389
train_iter_loss: 0.14314621686935425
train_iter_loss: 0.0921459048986435
train_iter_loss: 0.3268493711948395
train_iter_loss: 0.20071005821228027
train_iter_loss: 0.14235569536685944
train_iter_loss: 0.1692032516002655
train_iter_loss: 0.26284316182136536
train_iter_loss: 0.12532082200050354
train_iter_loss: 0.10705377161502838
train_iter_loss: 0.22731360793113708
train_iter_loss: 0.14865730702877045
train_iter_loss: 0.13653455674648285
train_iter_loss: 0.11644843220710754
train_iter_loss: 0.253348171710968
train_iter_loss: 0.12284111976623535
train_iter_loss: 0.08750134706497192
train_iter_loss: 0.14593523740768433
train_iter_loss: 0.12120427936315536
train_iter_loss: 0.16167961061000824
train_iter_loss: 0.0888008400797844
train_iter_loss: 0.10987885296344757
train_iter_loss: 0.1724243462085724
train_iter_loss: 0.09023202955722809
train_iter_loss: 0.19013923406600952
train_iter_loss: 0.1273171603679657
train_iter_loss: 0.14013950526714325
train_iter_loss: 0.3286394774913788
train_iter_loss: 0.12535595893859863
train_iter_loss: 0.2386959046125412
train_iter_loss: 0.18523132801055908
train_iter_loss: 0.1339094191789627
train_iter_loss: 0.08874350786209106
train_iter_loss: 0.168767049908638
train_iter_loss: 0.2908514142036438
train_iter_loss: 0.1990913301706314
train_iter_loss: 0.16778309643268585
train_iter_loss: 0.14292310178279877
train_iter_loss: 0.22347761690616608
train_iter_loss: 0.14027352631092072
train_iter_loss: 0.24952559173107147
train_iter_loss: 0.1460193544626236
train loss :0.1808
---------------------
Validation seg loss: 0.22512695868060273 at epoch 109
epoch =    110/  1000, exp = train
train_iter_loss: 0.1276901811361313
train_iter_loss: 0.14370159804821014
train_iter_loss: 0.24032168090343475
train_iter_loss: 0.40314731001853943
train_iter_loss: 0.27446267008781433
train_iter_loss: 0.13700132071971893
train_iter_loss: 0.2732972502708435
train_iter_loss: 0.2525104284286499
train_iter_loss: 0.09219103306531906
train_iter_loss: 0.2782342731952667
train_iter_loss: 0.13129323720932007
train_iter_loss: 0.3135344982147217
train_iter_loss: 0.2435561716556549
train_iter_loss: 0.14280609786510468
train_iter_loss: 0.1713811457157135
train_iter_loss: 0.1983502358198166
train_iter_loss: 0.2022363543510437
train_iter_loss: 0.17365166544914246
train_iter_loss: 0.25425708293914795
train_iter_loss: 0.201193705201149
train_iter_loss: 0.15919508039951324
train_iter_loss: 0.23883472383022308
train_iter_loss: 0.15017740428447723
train_iter_loss: 0.26407718658447266
train_iter_loss: 0.08756230771541595
train_iter_loss: 0.4211759865283966
train_iter_loss: 0.1593056470155716
train_iter_loss: 0.1385677307844162
train_iter_loss: 0.16503873467445374
train_iter_loss: 0.1741865873336792
train_iter_loss: 0.18338994681835175
train_iter_loss: 0.14700430631637573
train_iter_loss: 0.18454097211360931
train_iter_loss: 0.2530250549316406
train_iter_loss: 0.2915361523628235
train_iter_loss: 0.1146981492638588
train_iter_loss: 0.2881324589252472
train_iter_loss: 0.14043237268924713
train_iter_loss: 0.19945146143436432
train_iter_loss: 0.19824126362800598
train_iter_loss: 0.2553560733795166
train_iter_loss: 0.18844996392726898
train_iter_loss: 0.168039932847023
train_iter_loss: 0.13049349188804626
train_iter_loss: 0.15195345878601074
train_iter_loss: 0.22017143666744232
train_iter_loss: 0.1319441944360733
train_iter_loss: 0.15015743672847748
train_iter_loss: 0.15769067406654358
train_iter_loss: 0.12748898565769196
train_iter_loss: 0.16372936964035034
train_iter_loss: 0.21209673583507538
train_iter_loss: 0.16723224520683289
train_iter_loss: 0.17234809696674347
train_iter_loss: 0.12026144564151764
train_iter_loss: 0.13841983675956726
train_iter_loss: 0.168304905295372
train_iter_loss: 0.15745235979557037
train_iter_loss: 0.21990059316158295
train_iter_loss: 0.19437341392040253
train_iter_loss: 0.08057186007499695
train_iter_loss: 0.10142059624195099
train_iter_loss: 0.13680429756641388
train_iter_loss: 0.17336340248584747
train_iter_loss: 0.22303920984268188
train_iter_loss: 0.14270417392253876
train_iter_loss: 0.20448260009288788
train_iter_loss: 0.11254081130027771
train_iter_loss: 0.15706288814544678
train_iter_loss: 0.15341690182685852
train_iter_loss: 0.16039657592773438
train_iter_loss: 0.12857234477996826
train_iter_loss: 0.17316195368766785
train_iter_loss: 0.1762467473745346
train_iter_loss: 0.12829361855983734
train_iter_loss: 0.21394488215446472
train_iter_loss: 0.18856896460056305
train_iter_loss: 0.17279644310474396
train_iter_loss: 0.22105246782302856
train_iter_loss: 0.4224216639995575
train_iter_loss: 0.10948631167411804
train_iter_loss: 0.16547249257564545
train_iter_loss: 0.1818496435880661
train_iter_loss: 0.050055526196956635
train_iter_loss: 0.35537758469581604
train_iter_loss: 0.1878841370344162
train_iter_loss: 0.07785335928201675
train_iter_loss: 0.10828527063131332
train_iter_loss: 0.20152972638607025
train_iter_loss: 0.15034453570842743
train_iter_loss: 0.2256578952074051
train_iter_loss: 0.11882303655147552
train_iter_loss: 0.10263556987047195
train_iter_loss: 0.15482808649539948
train_iter_loss: 0.15745815634727478
train_iter_loss: 0.2135816067457199
train_iter_loss: 0.16916018724441528
train_iter_loss: 0.28496262431144714
train_iter_loss: 0.2948838174343109
train_iter_loss: 0.09707300364971161
train loss :0.1867
---------------------
Validation seg loss: 0.22581530098785768 at epoch 110
epoch =    111/  1000, exp = train
train_iter_loss: 0.21309195458889008
train_iter_loss: 0.1330520361661911
train_iter_loss: 0.13937078416347504
train_iter_loss: 0.15205997228622437
train_iter_loss: 0.26775291562080383
train_iter_loss: 0.13822242617607117
train_iter_loss: 0.12372883409261703
train_iter_loss: 0.22803817689418793
train_iter_loss: 0.22327232360839844
train_iter_loss: 0.13712146878242493
train_iter_loss: 0.2304295003414154
train_iter_loss: 0.15517917275428772
train_iter_loss: 0.18922947347164154
train_iter_loss: 0.21668361127376556
train_iter_loss: 0.1511681079864502
train_iter_loss: 0.11543961614370346
train_iter_loss: 0.1868664026260376
train_iter_loss: 0.16620343923568726
train_iter_loss: 0.2019951343536377
train_iter_loss: 0.12384792417287827
train_iter_loss: 0.14530059695243835
train_iter_loss: 0.21120363473892212
train_iter_loss: 0.25579553842544556
train_iter_loss: 0.21320702135562897
train_iter_loss: 0.10251586884260178
train_iter_loss: 0.10255902260541916
train_iter_loss: 0.15721161663532257
train_iter_loss: 0.18053293228149414
train_iter_loss: 0.1580112725496292
train_iter_loss: 0.14086954295635223
train_iter_loss: 0.3545692265033722
train_iter_loss: 0.1868894249200821
train_iter_loss: 0.15717466175556183
train_iter_loss: 0.2139580398797989
train_iter_loss: 0.17267504334449768
train_iter_loss: 0.22482630610466003
train_iter_loss: 0.17620190978050232
train_iter_loss: 0.21099358797073364
train_iter_loss: 0.14952309429645538
train_iter_loss: 0.2629525661468506
train_iter_loss: 0.14852356910705566
train_iter_loss: 0.15966998040676117
train_iter_loss: 0.19836163520812988
train_iter_loss: 0.22223664820194244
train_iter_loss: 0.26907220482826233
train_iter_loss: 0.12469953298568726
train_iter_loss: 0.1278059035539627
train_iter_loss: 0.18266421556472778
train_iter_loss: 0.15445859730243683
train_iter_loss: 0.27553731203079224
train_iter_loss: 0.23755720257759094
train_iter_loss: 0.12168825417757034
train_iter_loss: 0.282871276140213
train_iter_loss: 0.19979804754257202
train_iter_loss: 0.2036198377609253
train_iter_loss: 0.14590999484062195
train_iter_loss: 0.19553755223751068
train_iter_loss: 0.1512884646654129
train_iter_loss: 0.17310228943824768
train_iter_loss: 0.16322152316570282
train_iter_loss: 0.14823777973651886
train_iter_loss: 0.06744173914194107
train_iter_loss: 0.11988479644060135
train_iter_loss: 0.11989621818065643
train_iter_loss: 0.18334171175956726
train_iter_loss: 0.195016011595726
train_iter_loss: 0.14091205596923828
train_iter_loss: 0.2185872197151184
train_iter_loss: 0.07393679767847061
train_iter_loss: 0.14817537367343903
train_iter_loss: 0.1931162327528
train_iter_loss: 0.12751762568950653
train_iter_loss: 0.1365559697151184
train_iter_loss: 0.12396787106990814
train_iter_loss: 0.18531551957130432
train_iter_loss: 0.22441749274730682
train_iter_loss: 0.26334646344184875
train_iter_loss: 0.21997667849063873
train_iter_loss: 0.20506349205970764
train_iter_loss: 0.13349823653697968
train_iter_loss: 0.18947313725948334
train_iter_loss: 0.16469813883304596
train_iter_loss: 0.1567056030035019
train_iter_loss: 0.13127632439136505
train_iter_loss: 0.1556883007287979
train_iter_loss: 0.18662603199481964
train_iter_loss: 0.14865407347679138
train_iter_loss: 0.18955382704734802
train_iter_loss: 0.14919675886631012
train_iter_loss: 0.05574340745806694
train_iter_loss: 0.11629387736320496
train_iter_loss: 0.3385407328605652
train_iter_loss: 0.19575083255767822
train_iter_loss: 0.12267669290304184
train_iter_loss: 0.23112092912197113
train_iter_loss: 0.3060349225997925
train_iter_loss: 0.1170118972659111
train_iter_loss: 0.19565251469612122
train_iter_loss: 0.11335062235593796
train_iter_loss: 0.2436358481645584
train loss :0.1793
---------------------
Validation seg loss: 0.2273140094541716 at epoch 111
epoch =    112/  1000, exp = train
train_iter_loss: 0.22098955512046814
train_iter_loss: 0.21490906178951263
train_iter_loss: 0.2206152230501175
train_iter_loss: 0.19779780507087708
train_iter_loss: 0.1548738032579422
train_iter_loss: 0.276231586933136
train_iter_loss: 0.2050236016511917
train_iter_loss: 0.22414162755012512
train_iter_loss: 0.12070928514003754
train_iter_loss: 0.1428481787443161
train_iter_loss: 0.12109995633363724
train_iter_loss: 0.22297139465808868
train_iter_loss: 0.15040892362594604
train_iter_loss: 0.09259263426065445
train_iter_loss: 0.1538156419992447
train_iter_loss: 0.18897511065006256
train_iter_loss: 0.41559845209121704
train_iter_loss: 0.24703383445739746
train_iter_loss: 0.1572244018316269
train_iter_loss: 0.055256687104701996
train_iter_loss: 0.11012902855873108
train_iter_loss: 0.15366800129413605
train_iter_loss: 0.11398857831954956
train_iter_loss: 0.2520720958709717
train_iter_loss: 0.14630435407161713
train_iter_loss: 0.16189999878406525
train_iter_loss: 0.14598363637924194
train_iter_loss: 0.2815980911254883
train_iter_loss: 0.16872669756412506
train_iter_loss: 0.105843685567379
train_iter_loss: 0.19519047439098358
train_iter_loss: 0.18462717533111572
train_iter_loss: 0.10189396142959595
train_iter_loss: 0.1430620700120926
train_iter_loss: 0.10112789273262024
train_iter_loss: 0.09478657692670822
train_iter_loss: 0.21465402841567993
train_iter_loss: 0.2664547264575958
train_iter_loss: 0.28701478242874146
train_iter_loss: 0.19414867460727692
train_iter_loss: 0.2031741440296173
train_iter_loss: 0.10469632595777512
train_iter_loss: 0.1635270118713379
train_iter_loss: 0.27545589208602905
train_iter_loss: 0.1951751857995987
train_iter_loss: 0.15923301875591278
train_iter_loss: 0.14289744198322296
train_iter_loss: 0.28089913725852966
train_iter_loss: 0.18030813336372375
train_iter_loss: 0.1262352019548416
train_iter_loss: 0.17695745825767517
train_iter_loss: 0.25686922669410706
train_iter_loss: 0.19907312095165253
train_iter_loss: 0.18631869554519653
train_iter_loss: 0.24298417568206787
train_iter_loss: 0.10693221539258957
train_iter_loss: 0.1407078057527542
train_iter_loss: 0.15102273225784302
train_iter_loss: 0.14548663794994354
train_iter_loss: 0.16732345521450043
train_iter_loss: 0.16058914363384247
train_iter_loss: 0.17596150934696198
train_iter_loss: 0.1877415031194687
train_iter_loss: 0.1519617736339569
train_iter_loss: 0.21333539485931396
train_iter_loss: 0.17007242143154144
train_iter_loss: 0.16267025470733643
train_iter_loss: 0.24432776868343353
train_iter_loss: 0.13525423407554626
train_iter_loss: 0.2965979278087616
train_iter_loss: 0.10957998782396317
train_iter_loss: 0.11943995207548141
train_iter_loss: 0.22024747729301453
train_iter_loss: 0.35682806372642517
train_iter_loss: 0.17050153017044067
train_iter_loss: 0.12447302043437958
train_iter_loss: 0.13458000123500824
train_iter_loss: 0.08408298343420029
train_iter_loss: 0.2997315227985382
train_iter_loss: 0.10673093795776367
train_iter_loss: 0.12027376145124435
train_iter_loss: 0.07462141662836075
train_iter_loss: 0.24276037514209747
train_iter_loss: 0.17678116261959076
train_iter_loss: 0.1507101058959961
train_iter_loss: 0.19254671037197113
train_iter_loss: 0.15741020441055298
train_iter_loss: 0.22138585150241852
train_iter_loss: 0.19280798733234406
train_iter_loss: 0.12634122371673584
train_iter_loss: 0.2365763783454895
train_iter_loss: 0.20811225473880768
train_iter_loss: 0.22016817331314087
train_iter_loss: 0.17239151895046234
train_iter_loss: 0.28915005922317505
train_iter_loss: 0.14798064529895782
train_iter_loss: 0.10377634316682816
train_iter_loss: 0.19132879376411438
train_iter_loss: 0.12750008702278137
train_iter_loss: 0.08079639822244644
train loss :0.1805
---------------------
Validation seg loss: 0.22719441431592097 at epoch 112
epoch =    113/  1000, exp = train
train_iter_loss: 0.19012394547462463
train_iter_loss: 0.035657551139593124
train_iter_loss: 0.12742644548416138
train_iter_loss: 0.1919635385274887
train_iter_loss: 0.1920335292816162
train_iter_loss: 0.08253981918096542
train_iter_loss: 0.22323188185691833
train_iter_loss: 0.26753419637680054
train_iter_loss: 0.07152322679758072
train_iter_loss: 0.16612717509269714
train_iter_loss: 0.2843930423259735
train_iter_loss: 0.22960156202316284
train_iter_loss: 0.15969201922416687
train_iter_loss: 0.3126962184906006
train_iter_loss: 0.08622007817029953
train_iter_loss: 0.22117632627487183
train_iter_loss: 0.12430142611265182
train_iter_loss: 0.24098601937294006
train_iter_loss: 0.2571832835674286
train_iter_loss: 0.21662567555904388
train_iter_loss: 0.13703757524490356
train_iter_loss: 0.2206115573644638
train_iter_loss: 0.2953030467033386
train_iter_loss: 0.14871814846992493
train_iter_loss: 0.12981818616390228
train_iter_loss: 0.18550990521907806
train_iter_loss: 0.13823534548282623
train_iter_loss: 0.16433566808700562
train_iter_loss: 0.2199554145336151
train_iter_loss: 0.08610294759273529
train_iter_loss: 0.20362317562103271
train_iter_loss: 0.15580734610557556
train_iter_loss: 0.14035648107528687
train_iter_loss: 0.11602737009525299
train_iter_loss: 0.11528734862804413
train_iter_loss: 0.14999490976333618
train_iter_loss: 0.2572062611579895
train_iter_loss: 0.11460454016923904
train_iter_loss: 0.06397829204797745
train_iter_loss: 0.1326638013124466
train_iter_loss: 0.19348306953907013
train_iter_loss: 0.1826276332139969
train_iter_loss: 0.17408008873462677
train_iter_loss: 0.11402830481529236
train_iter_loss: 0.4285069406032562
train_iter_loss: 0.2776682674884796
train_iter_loss: 0.25889214873313904
train_iter_loss: 0.18408118188381195
train_iter_loss: 0.22516952455043793
train_iter_loss: 0.1710696667432785
train_iter_loss: 0.16978536546230316
train_iter_loss: 0.23979102075099945
train_iter_loss: 0.09953338652849197
train_iter_loss: 0.13185903429985046
train_iter_loss: 0.0952884778380394
train_iter_loss: 0.3055609464645386
train_iter_loss: 0.15955539047718048
train_iter_loss: 0.20716512203216553
train_iter_loss: 0.15868626534938812
train_iter_loss: 0.14688245952129364
train_iter_loss: 0.16304875910282135
train_iter_loss: 0.11861918121576309
train_iter_loss: 0.1746375560760498
train_iter_loss: 0.13120761513710022
train_iter_loss: 0.14859002828598022
train_iter_loss: 0.28430235385894775
train_iter_loss: 0.38501212000846863
train_iter_loss: 0.31566697359085083
train_iter_loss: 0.19150623679161072
train_iter_loss: 0.1847216635942459
train_iter_loss: 0.2719215154647827
train_iter_loss: 0.2187480330467224
train_iter_loss: 0.24040846526622772
train_iter_loss: 0.24060523509979248
train_iter_loss: 0.2705916166305542
train_iter_loss: 0.1308818757534027
train_iter_loss: 0.10747803747653961
train_iter_loss: 0.20411381125450134
train_iter_loss: 0.134223073720932
train_iter_loss: 0.14505848288536072
train_iter_loss: 0.18424688279628754
train_iter_loss: 0.20881009101867676
train_iter_loss: 0.14982932806015015
train_iter_loss: 0.09551380574703217
train_iter_loss: 0.24848580360412598
train_iter_loss: 0.10935744643211365
train_iter_loss: 0.10928334295749664
train_iter_loss: 0.2702542543411255
train_iter_loss: 0.24346566200256348
train_iter_loss: 0.059794832020998
train_iter_loss: 0.14232373237609863
train_iter_loss: 0.10889159142971039
train_iter_loss: 0.18105830252170563
train_iter_loss: 0.129139244556427
train_iter_loss: 0.35129547119140625
train_iter_loss: 0.173390194773674
train_iter_loss: 0.18148654699325562
train_iter_loss: 0.11545558273792267
train_iter_loss: 0.17713004350662231
train_iter_loss: 0.20608416199684143
train loss :0.1847
---------------------
Validation seg loss: 0.22655320733363898 at epoch 113
epoch =    114/  1000, exp = train
train_iter_loss: 0.14742983877658844
train_iter_loss: 0.1242583841085434
train_iter_loss: 0.26512211561203003
train_iter_loss: 0.11584814637899399
train_iter_loss: 0.137678325176239
train_iter_loss: 0.24813297390937805
train_iter_loss: 0.10288434475660324
train_iter_loss: 0.06730308383703232
train_iter_loss: 0.30775243043899536
train_iter_loss: 0.24633343517780304
train_iter_loss: 0.24007639288902283
train_iter_loss: 0.2151428461074829
train_iter_loss: 0.2519489526748657
train_iter_loss: 0.2156786024570465
train_iter_loss: 0.0991358831524849
train_iter_loss: 0.19038470089435577
train_iter_loss: 0.11272867023944855
train_iter_loss: 0.12173420935869217
train_iter_loss: 0.3436650037765503
train_iter_loss: 0.20239490270614624
train_iter_loss: 0.10750511288642883
train_iter_loss: 0.20711356401443481
train_iter_loss: 0.21724769473075867
train_iter_loss: 0.18807202577590942
train_iter_loss: 0.17236195504665375
train_iter_loss: 0.20446759462356567
train_iter_loss: 0.17946456372737885
train_iter_loss: 0.12339376658201218
train_iter_loss: 0.1139652281999588
train_iter_loss: 0.1877858191728592
train_iter_loss: 0.1595144420862198
train_iter_loss: 0.20033887028694153
train_iter_loss: 0.2594263255596161
train_iter_loss: 0.16025462746620178
train_iter_loss: 0.18956078588962555
train_iter_loss: 0.2584289610385895
train_iter_loss: 0.23617152869701385
train_iter_loss: 0.12113127112388611
train_iter_loss: 0.11014784872531891
train_iter_loss: 0.3800550103187561
train_iter_loss: 0.1626262217760086
train_iter_loss: 0.15338031947612762
train_iter_loss: 0.20875497162342072
train_iter_loss: 0.1668010801076889
train_iter_loss: 0.213478222489357
train_iter_loss: 0.10281828045845032
train_iter_loss: 0.17618583142757416
train_iter_loss: 0.22553123533725739
train_iter_loss: 0.21437707543373108
train_iter_loss: 0.09192487597465515
train_iter_loss: 0.18686561286449432
train_iter_loss: 0.15668082237243652
train_iter_loss: 0.2378133237361908
train_iter_loss: 0.13642114400863647
train_iter_loss: 0.0863548293709755
train_iter_loss: 0.17585037648677826
train_iter_loss: 0.2479659616947174
train_iter_loss: 0.23149025440216064
train_iter_loss: 0.16185782849788666
train_iter_loss: 0.1636277586221695
train_iter_loss: 0.20378103852272034
train_iter_loss: 0.15333548188209534
train_iter_loss: 0.15149252116680145
train_iter_loss: 0.19756121933460236
train_iter_loss: 0.10327610373497009
train_iter_loss: 0.20972183346748352
train_iter_loss: 0.24811138212680817
train_iter_loss: 0.14263400435447693
train_iter_loss: 0.13979600369930267
train_iter_loss: 0.16546104848384857
train_iter_loss: 0.09139913320541382
train_iter_loss: 0.19193820655345917
train_iter_loss: 0.18202972412109375
train_iter_loss: 0.17105887830257416
train_iter_loss: 0.2326369732618332
train_iter_loss: 0.1799454540014267
train_iter_loss: 0.12945307791233063
train_iter_loss: 0.10851139575242996
train_iter_loss: 0.16593478620052338
train_iter_loss: 0.12638835608959198
train_iter_loss: 0.1889663189649582
train_iter_loss: 0.15403829514980316
train_iter_loss: 0.23670680820941925
train_iter_loss: 0.09684517234563828
train_iter_loss: 0.16238325834274292
train_iter_loss: 0.23932653665542603
train_iter_loss: 0.18254335224628448
train_iter_loss: 0.1911240518093109
train_iter_loss: 0.09506995230913162
train_iter_loss: 0.12491709738969803
train_iter_loss: 0.20381270349025726
train_iter_loss: 0.10123546421527863
train_iter_loss: 0.1341591775417328
train_iter_loss: 0.3087061047554016
train_iter_loss: 0.14658808708190918
train_iter_loss: 0.10774313658475876
train_iter_loss: 0.24365442991256714
train_iter_loss: 0.1448136568069458
train_iter_loss: 0.18093617260456085
train_iter_loss: 0.18568284809589386
train loss :0.1794
---------------------
Validation seg loss: 0.2284013573607465 at epoch 114
epoch =    115/  1000, exp = train
train_iter_loss: 0.22438856959342957
train_iter_loss: 0.13340860605239868
train_iter_loss: 0.21938051283359528
train_iter_loss: 0.15984266996383667
train_iter_loss: 0.14141611754894257
train_iter_loss: 0.26525193452835083
train_iter_loss: 0.19294922053813934
train_iter_loss: 0.18628662824630737
train_iter_loss: 0.2317243069410324
train_iter_loss: 0.12878797948360443
train_iter_loss: 0.17604322731494904
train_iter_loss: 0.12517791986465454
train_iter_loss: 0.17154665291309357
train_iter_loss: 0.18413600325584412
train_iter_loss: 0.2036939561367035
train_iter_loss: 0.14625833928585052
train_iter_loss: 0.1983189731836319
train_iter_loss: 0.14217282831668854
train_iter_loss: 0.29783424735069275
train_iter_loss: 0.15371045470237732
train_iter_loss: 0.2713730037212372
train_iter_loss: 0.06622450798749924
train_iter_loss: 0.16531217098236084
train_iter_loss: 0.154694065451622
train_iter_loss: 0.13732914626598358
train_iter_loss: 0.14992207288742065
train_iter_loss: 0.1430077999830246
train_iter_loss: 0.11276433616876602
train_iter_loss: 0.17960214614868164
train_iter_loss: 0.2596893012523651
train_iter_loss: 0.17296181619167328
train_iter_loss: 0.12284990400075912
train_iter_loss: 0.24394771456718445
train_iter_loss: 0.22249941527843475
train_iter_loss: 0.11213564872741699
train_iter_loss: 0.17451520264148712
train_iter_loss: 0.15130490064620972
train_iter_loss: 0.259905070066452
train_iter_loss: 0.11084264516830444
train_iter_loss: 0.12211550772190094
train_iter_loss: 0.08797790855169296
train_iter_loss: 0.2577228546142578
train_iter_loss: 0.15217898786067963
train_iter_loss: 0.07113548368215561
train_iter_loss: 0.2591058909893036
train_iter_loss: 0.31238308548927307
train_iter_loss: 0.2493758648633957
train_iter_loss: 0.11976121366024017
train_iter_loss: 0.1778661012649536
train_iter_loss: 0.16402722895145416
train_iter_loss: 0.15040256083011627
train_iter_loss: 0.2351541370153427
train_iter_loss: 0.16335134208202362
train_iter_loss: 0.16817019879817963
train_iter_loss: 0.22102536261081696
train_iter_loss: 0.10881143808364868
train_iter_loss: 0.1648826003074646
train_iter_loss: 0.16110937297344208
train_iter_loss: 0.15036988258361816
train_iter_loss: 0.2522905766963959
train_iter_loss: 0.12822160124778748
train_iter_loss: 0.23192928731441498
train_iter_loss: 0.3163016438484192
train_iter_loss: 0.1975790560245514
train_iter_loss: 0.23650191724300385
train_iter_loss: 0.1964302510023117
train_iter_loss: 0.15428616106510162
train_iter_loss: 0.1190684512257576
train_iter_loss: 0.13932611048221588
train_iter_loss: 0.19134214520454407
train_iter_loss: 0.21620173752307892
train_iter_loss: 0.25712358951568604
train_iter_loss: 0.12232135981321335
train_iter_loss: 0.20111329853534698
train_iter_loss: 0.13089905679225922
train_iter_loss: 0.1406717449426651
train_iter_loss: 0.20256903767585754
train_iter_loss: 0.22065190970897675
train_iter_loss: 0.14846649765968323
train_iter_loss: 0.17031612992286682
train_iter_loss: 0.3803960382938385
train_iter_loss: 0.13721144199371338
train_iter_loss: 0.13338467478752136
train_iter_loss: 0.21593041718006134
train_iter_loss: 0.16785840690135956
train_iter_loss: 0.169102743268013
train_iter_loss: 0.11541742086410522
train_iter_loss: 0.1061767041683197
train_iter_loss: 0.2639003098011017
train_iter_loss: 0.1537666916847229
train_iter_loss: 0.13781952857971191
train_iter_loss: 0.20281243324279785
train_iter_loss: 0.07862351834774017
train_iter_loss: 0.195136159658432
train_iter_loss: 0.14099130034446716
train_iter_loss: 0.2293306142091751
train_iter_loss: 0.09100179374217987
train_iter_loss: 0.12911230325698853
train_iter_loss: 0.24792954325675964
train_iter_loss: 0.17884190380573273
train loss :0.1802
---------------------
Validation seg loss: 0.22569044556398438 at epoch 115
epoch =    116/  1000, exp = train
train_iter_loss: 0.2188228964805603
train_iter_loss: 0.28372713923454285
train_iter_loss: 0.17939478158950806
train_iter_loss: 0.20391613245010376
train_iter_loss: 0.1207108348608017
train_iter_loss: 0.2708493769168854
train_iter_loss: 0.22705212235450745
train_iter_loss: 0.20439483225345612
train_iter_loss: 0.1625271588563919
train_iter_loss: 0.17558108270168304
train_iter_loss: 0.21934060752391815
train_iter_loss: 0.1652216613292694
train_iter_loss: 0.20454977452754974
train_iter_loss: 0.13181135058403015
train_iter_loss: 0.1793636977672577
train_iter_loss: 0.15623827278614044
train_iter_loss: 0.13156874477863312
train_iter_loss: 0.18723762035369873
train_iter_loss: 0.24392789602279663
train_iter_loss: 0.16032738983631134
train_iter_loss: 0.10523314774036407
train_iter_loss: 0.15752193331718445
train_iter_loss: 0.1782127320766449
train_iter_loss: 0.2925848960876465
train_iter_loss: 0.15026074647903442
train_iter_loss: 0.07354886084794998
train_iter_loss: 0.20385253429412842
train_iter_loss: 0.15669801831245422
train_iter_loss: 0.19328638911247253
train_iter_loss: 0.18677672743797302
train_iter_loss: 0.1656849980354309
train_iter_loss: 0.32539650797843933
train_iter_loss: 0.2260332554578781
train_iter_loss: 0.26335880160331726
train_iter_loss: 0.10658901184797287
train_iter_loss: 0.14868417382240295
train_iter_loss: 0.1783904731273651
train_iter_loss: 0.07015933841466904
train_iter_loss: 0.14031654596328735
train_iter_loss: 0.26085370779037476
train_iter_loss: 0.14350207149982452
train_iter_loss: 0.24692504107952118
train_iter_loss: 0.19915975630283356
train_iter_loss: 0.18933013081550598
train_iter_loss: 0.1851000189781189
train_iter_loss: 0.159736767411232
train_iter_loss: 0.20676513016223907
train_iter_loss: 0.19259551167488098
train_iter_loss: 0.2384341061115265
train_iter_loss: 0.21373341977596283
train_iter_loss: 0.2334497570991516
train_iter_loss: 0.23858597874641418
train_iter_loss: 0.10412244498729706
train_iter_loss: 0.11303266882896423
train_iter_loss: 0.21163344383239746
train_iter_loss: 0.18102411925792694
train_iter_loss: 0.09822060167789459
train_iter_loss: 0.192783921957016
train_iter_loss: 0.13705497980117798
train_iter_loss: 0.20959672331809998
train_iter_loss: 0.16089774668216705
train_iter_loss: 0.21756844222545624
train_iter_loss: 0.20512491464614868
train_iter_loss: 0.08211427181959152
train_iter_loss: 0.08272355794906616
train_iter_loss: 0.23064789175987244
train_iter_loss: 0.179727241396904
train_iter_loss: 0.11025170236825943
train_iter_loss: 0.05770448222756386
train_iter_loss: 0.2283688187599182
train_iter_loss: 0.17353013157844543
train_iter_loss: 0.20262771844863892
train_iter_loss: 0.22509315609931946
train_iter_loss: 0.148381307721138
train_iter_loss: 0.29825615882873535
train_iter_loss: 0.4415660798549652
train_iter_loss: 0.16102878749370575
train_iter_loss: 0.16938841342926025
train_iter_loss: 0.1717180460691452
train_iter_loss: 0.14809389412403107
train_iter_loss: 0.2516998052597046
train_iter_loss: 0.08702292293310165
train_iter_loss: 0.1302131563425064
train_iter_loss: 0.11381851136684418
train_iter_loss: 0.13565854728221893
train_iter_loss: 0.1963667869567871
train_iter_loss: 0.13363809883594513
train_iter_loss: 0.19572538137435913
train_iter_loss: 0.13409171998500824
train_iter_loss: 0.13289642333984375
train_iter_loss: 0.1501120626926422
train_iter_loss: 0.12862947583198547
train_iter_loss: 0.21550104022026062
train_iter_loss: 0.21814067661762238
train_iter_loss: 0.1467025727033615
train_iter_loss: 0.1962169110774994
train_iter_loss: 0.281761109828949
train_iter_loss: 0.19294066727161407
train_iter_loss: 0.23551695048809052
train_iter_loss: 0.085056833922863
train loss :0.1834
---------------------
Validation seg loss: 0.22372350047500628 at epoch 116
********************
best_val_epoch_loss:  0.22372350047500628
MODEL UPDATED
epoch =    117/  1000, exp = train
train_iter_loss: 0.13988937437534332
train_iter_loss: 0.10504955053329468
train_iter_loss: 0.1461140662431717
train_iter_loss: 0.19550277292728424
train_iter_loss: 0.21378231048583984
train_iter_loss: 0.31684428453445435
train_iter_loss: 0.14950306713581085
train_iter_loss: 0.08744150400161743
train_iter_loss: 0.1225108876824379
train_iter_loss: 0.09605860710144043
train_iter_loss: 0.2711263597011566
train_iter_loss: 0.15968751907348633
train_iter_loss: 0.09476223587989807
train_iter_loss: 0.24829678237438202
train_iter_loss: 0.1880360245704651
train_iter_loss: 0.09648367017507553
train_iter_loss: 0.11225958913564682
train_iter_loss: 0.15347599983215332
train_iter_loss: 0.21433791518211365
train_iter_loss: 0.21031342446804047
train_iter_loss: 0.17660318315029144
train_iter_loss: 0.1064026802778244
train_iter_loss: 0.26123690605163574
train_iter_loss: 0.11653590947389603
train_iter_loss: 0.16819949448108673
train_iter_loss: 0.1759497970342636
train_iter_loss: 0.2567351460456848
train_iter_loss: 0.11352971941232681
train_iter_loss: 0.14724837243556976
train_iter_loss: 0.09939001500606537
train_iter_loss: 0.10953754186630249
train_iter_loss: 0.14287123084068298
train_iter_loss: 0.1733551025390625
train_iter_loss: 0.17026261985301971
train_iter_loss: 0.2119818776845932
train_iter_loss: 0.10127018392086029
train_iter_loss: 0.22454772889614105
train_iter_loss: 0.12717798352241516
train_iter_loss: 0.2002735584974289
train_iter_loss: 0.1911177784204483
train_iter_loss: 0.16647829115390778
train_iter_loss: 0.09296213090419769
train_iter_loss: 0.19165819883346558
train_iter_loss: 0.23798596858978271
train_iter_loss: 0.2997770607471466
train_iter_loss: 0.19467155635356903
train_iter_loss: 0.17644934356212616
train_iter_loss: 0.2544139325618744
train_iter_loss: 0.20601041615009308
train_iter_loss: 0.19105930626392365
train_iter_loss: 0.20696410536766052
train_iter_loss: 0.09087108075618744
train_iter_loss: 0.19315952062606812
train_iter_loss: 0.11790066212415695
train_iter_loss: 0.1969398856163025
train_iter_loss: 0.21988853812217712
train_iter_loss: 0.1476682722568512
train_iter_loss: 0.2498037964105606
train_iter_loss: 0.1685754954814911
train_iter_loss: 0.17841017246246338
train_iter_loss: 0.2878649830818176
train_iter_loss: 0.07583914697170258
train_iter_loss: 0.17226991057395935
train_iter_loss: 0.16589882969856262
train_iter_loss: 0.17886941134929657
train_iter_loss: 0.20143088698387146
train_iter_loss: 0.2730901837348938
train_iter_loss: 0.09335481375455856
train_iter_loss: 0.18380069732666016
train_iter_loss: 0.1512048840522766
train_iter_loss: 0.12694254517555237
train_iter_loss: 0.1805742383003235
train_iter_loss: 0.1798776537179947
train_iter_loss: 0.23771144449710846
train_iter_loss: 0.12924420833587646
train_iter_loss: 0.17324954271316528
train_iter_loss: 0.16872763633728027
train_iter_loss: 0.18654225766658783
train_iter_loss: 0.13719969987869263
train_iter_loss: 0.16884304583072662
train_iter_loss: 0.23815381526947021
train_iter_loss: 0.2264995574951172
train_iter_loss: 0.2796093821525574
train_iter_loss: 0.12001355737447739
train_iter_loss: 0.20490914583206177
train_iter_loss: 0.21639524400234222
train_iter_loss: 0.13874758780002594
train_iter_loss: 0.1321336328983307
train_iter_loss: 0.19565217196941376
train_iter_loss: 0.17455975711345673
train_iter_loss: 0.15118402242660522
train_iter_loss: 0.15926679968833923
train_iter_loss: 0.1888982057571411
train_iter_loss: 0.14843732118606567
train_iter_loss: 0.12200163304805756
train_iter_loss: 0.1691516637802124
train_iter_loss: 0.13537217676639557
train_iter_loss: 0.24254456162452698
train_iter_loss: 0.11960980296134949
train_iter_loss: 0.250810444355011
train loss :0.1765
---------------------
Validation seg loss: 0.2258968183349045 at epoch 117
epoch =    118/  1000, exp = train
train_iter_loss: 0.13512563705444336
train_iter_loss: 0.20426274836063385
train_iter_loss: 0.13824467360973358
train_iter_loss: 0.1382981538772583
train_iter_loss: 0.11695794761180878
train_iter_loss: 0.13052766025066376
train_iter_loss: 0.12144125998020172
train_iter_loss: 0.2568982243537903
train_iter_loss: 0.08209950476884842
train_iter_loss: 0.2842090129852295
train_iter_loss: 0.147030770778656
train_iter_loss: 0.16239824891090393
train_iter_loss: 0.13409537076950073
train_iter_loss: 0.14817337691783905
train_iter_loss: 0.16311118006706238
train_iter_loss: 0.1472388058900833
train_iter_loss: 0.17989571392536163
train_iter_loss: 0.13300463557243347
train_iter_loss: 0.12620769441127777
train_iter_loss: 0.14120817184448242
train_iter_loss: 0.30857527256011963
train_iter_loss: 0.20892639458179474
train_iter_loss: 0.1228310838341713
train_iter_loss: 0.10661128163337708
train_iter_loss: 0.22948843240737915
train_iter_loss: 0.15172889828681946
train_iter_loss: 0.053446102887392044
train_iter_loss: 0.1383458375930786
train_iter_loss: 0.1760111302137375
train_iter_loss: 0.1258556991815567
train_iter_loss: 0.10401684045791626
train_iter_loss: 0.1183948814868927
train_iter_loss: 0.0933358371257782
train_iter_loss: 0.2335442155599594
train_iter_loss: 0.14159345626831055
train_iter_loss: 0.3976525366306305
train_iter_loss: 0.2502819001674652
train_iter_loss: 0.14102594554424286
train_iter_loss: 0.177406907081604
train_iter_loss: 0.16293461620807648
train_iter_loss: 0.14270758628845215
train_iter_loss: 0.14504680037498474
train_iter_loss: 0.28042545914649963
train_iter_loss: 0.17402835190296173
train_iter_loss: 0.22814513742923737
train_iter_loss: 0.13532023131847382
train_iter_loss: 0.21368169784545898
train_iter_loss: 0.24675384163856506
train_iter_loss: 0.1493033915758133
train_iter_loss: 0.14274542033672333
train_iter_loss: 0.14821499586105347
train_iter_loss: 0.13854056596755981
train_iter_loss: 0.17742657661437988
train_iter_loss: 0.14768454432487488
train_iter_loss: 0.08644374459981918
train_iter_loss: 0.08714978396892548
train_iter_loss: 0.16214589774608612
train_iter_loss: 0.07345792651176453
train_iter_loss: 0.11372886598110199
train_iter_loss: 0.19422543048858643
train_iter_loss: 0.1822490096092224
train_iter_loss: 0.2122974842786789
train_iter_loss: 0.17724908888339996
train_iter_loss: 0.14794683456420898
train_iter_loss: 0.22093729674816132
train_iter_loss: 0.13786756992340088
train_iter_loss: 0.23819658160209656
train_iter_loss: 0.14699894189834595
train_iter_loss: 0.24462883174419403
train_iter_loss: 0.20676738023757935
train_iter_loss: 0.25173282623291016
train_iter_loss: 0.23872080445289612
train_iter_loss: 0.16239504516124725
train_iter_loss: 0.25987136363983154
train_iter_loss: 0.12200876325368881
train_iter_loss: 0.14344216883182526
train_iter_loss: 0.19187480211257935
train_iter_loss: 0.07791805267333984
train_iter_loss: 0.10329826176166534
train_iter_loss: 0.20639605820178986
train_iter_loss: 0.2362237572669983
train_iter_loss: 0.12034377455711365
train_iter_loss: 0.1663636714220047
train_iter_loss: 0.21785496175289154
train_iter_loss: 0.20248720049858093
train_iter_loss: 0.22580771148204803
train_iter_loss: 0.19984237849712372
train_iter_loss: 0.1953030377626419
train_iter_loss: 0.16073042154312134
train_iter_loss: 0.09378477931022644
train_iter_loss: 0.2120714634656906
train_iter_loss: 0.27020373940467834
train_iter_loss: 0.17507627606391907
train_iter_loss: 0.2014113962650299
train_iter_loss: 0.15162180364131927
train_iter_loss: 0.2202630639076233
train_iter_loss: 0.13146597146987915
train_iter_loss: 0.23311841487884521
train_iter_loss: 0.19213250279426575
train_iter_loss: 0.18823224306106567
train loss :0.1736
---------------------
Validation seg loss: 0.2258782187336177 at epoch 118
epoch =    119/  1000, exp = train
train_iter_loss: 0.07789956033229828
train_iter_loss: 0.2036527544260025
train_iter_loss: 0.17560863494873047
train_iter_loss: 0.2064526230096817
train_iter_loss: 0.12992531061172485
train_iter_loss: 0.26740148663520813
train_iter_loss: 0.10266261547803879
train_iter_loss: 0.32775649428367615
train_iter_loss: 0.18071827292442322
train_iter_loss: 0.15333868563175201
train_iter_loss: 0.1664477437734604
train_iter_loss: 0.23372918367385864
train_iter_loss: 0.16902057826519012
train_iter_loss: 0.14177222549915314
train_iter_loss: 0.1731569916009903
train_iter_loss: 0.18612080812454224
train_iter_loss: 0.29155290126800537
train_iter_loss: 0.1668294072151184
train_iter_loss: 0.1875498741865158
train_iter_loss: 0.18329304456710815
train_iter_loss: 0.20179344713687897
train_iter_loss: 0.2416369467973709
train_iter_loss: 0.22529062628746033
train_iter_loss: 0.10641660541296005
train_iter_loss: 0.17496661841869354
train_iter_loss: 0.21672211587429047
train_iter_loss: 0.22969435155391693
train_iter_loss: 0.15218298137187958
train_iter_loss: 0.18400071561336517
train_iter_loss: 0.13919083774089813
train_iter_loss: 0.1426333487033844
train_iter_loss: 0.30215921998023987
train_iter_loss: 0.2755737900733948
train_iter_loss: 0.23951952159404755
train_iter_loss: 0.13155792653560638
train_iter_loss: 0.2220182865858078
train_iter_loss: 0.16112162172794342
train_iter_loss: 0.1263703852891922
train_iter_loss: 0.1804637312889099
train_iter_loss: 0.23935866355895996
train_iter_loss: 0.1983177661895752
train_iter_loss: 0.11875128000974655
train_iter_loss: 0.1230195164680481
train_iter_loss: 0.19151362776756287
train_iter_loss: 0.15618309378623962
train_iter_loss: 0.16212700307369232
train_iter_loss: 0.15052881836891174
train_iter_loss: 0.1945905238389969
train_iter_loss: 0.11090987920761108
train_iter_loss: 0.11534427851438522
train_iter_loss: 0.12736713886260986
train_iter_loss: 0.19381019473075867
train_iter_loss: 0.13677191734313965
train_iter_loss: 0.12075275927782059
train_iter_loss: 0.1661599576473236
train_iter_loss: 0.12544049322605133
train_iter_loss: 0.12775148451328278
train_iter_loss: 0.2227468490600586
train_iter_loss: 0.1862328201532364
train_iter_loss: 0.0905364602804184
train_iter_loss: 0.20952828228473663
train_iter_loss: 0.13796980679035187
train_iter_loss: 0.27708062529563904
train_iter_loss: 0.16464929282665253
train_iter_loss: 0.13949067890644073
train_iter_loss: 0.12183572351932526
train_iter_loss: 0.18681098520755768
train_iter_loss: 0.12182828038930893
train_iter_loss: 0.10463051497936249
train_iter_loss: 0.0928923487663269
train_iter_loss: 0.15816982090473175
train_iter_loss: 0.18360774219036102
train_iter_loss: 0.158962681889534
train_iter_loss: 0.2706458866596222
train_iter_loss: 0.14614222943782806
train_iter_loss: 0.23584797978401184
train_iter_loss: 0.1414334923028946
train_iter_loss: 0.24571168422698975
train_iter_loss: 0.06252502650022507
train_iter_loss: 0.1620471179485321
train_iter_loss: 0.0943186804652214
train_iter_loss: 0.12360717356204987
train_iter_loss: 0.09813836216926575
train_iter_loss: 0.1525728851556778
train_iter_loss: 0.17261838912963867
train_iter_loss: 0.14847229421138763
train_iter_loss: 0.2017989158630371
train_iter_loss: 0.16888360679149628
train_iter_loss: 0.04513496160507202
train_iter_loss: 0.1728333830833435
train_iter_loss: 0.1395908147096634
train_iter_loss: 0.13039936125278473
train_iter_loss: 0.24721476435661316
train_iter_loss: 0.18547025322914124
train_iter_loss: 0.14453162252902985
train_iter_loss: 0.15244074165821075
train_iter_loss: 0.27199265360832214
train_iter_loss: 0.1455460786819458
train_iter_loss: 0.13706928491592407
train_iter_loss: 0.19899870455265045
train loss :0.1726
---------------------
Validation seg loss: 0.22438659759695237 at epoch 119
epoch =    120/  1000, exp = train
train_iter_loss: 0.11574143916368484
train_iter_loss: 0.11954494565725327
train_iter_loss: 0.1383642852306366
train_iter_loss: 0.3055270314216614
train_iter_loss: 0.18182460963726044
train_iter_loss: 0.23797079920768738
train_iter_loss: 0.16829966008663177
train_iter_loss: 0.05502927303314209
train_iter_loss: 0.18795783817768097
train_iter_loss: 0.1347791850566864
train_iter_loss: 0.10199657082557678
train_iter_loss: 0.1556001752614975
train_iter_loss: 0.17194220423698425
train_iter_loss: 0.17664116621017456
train_iter_loss: 0.16858601570129395
train_iter_loss: 0.10642162710428238
train_iter_loss: 0.14734719693660736
train_iter_loss: 0.11437185853719711
train_iter_loss: 0.15941599011421204
train_iter_loss: 0.12217967957258224
train_iter_loss: 0.2532972991466522
train_iter_loss: 0.19949741661548615
train_iter_loss: 0.20585839450359344
train_iter_loss: 0.09632670879364014
train_iter_loss: 0.14536049962043762
train_iter_loss: 0.1520024985074997
train_iter_loss: 0.14966115355491638
train_iter_loss: 0.1306522935628891
train_iter_loss: 0.16420890390872955
train_iter_loss: 0.2014293372631073
train_iter_loss: 0.23383738100528717
train_iter_loss: 0.21759647130966187
train_iter_loss: 0.06138436496257782
train_iter_loss: 0.13980039954185486
train_iter_loss: 0.3253757059574127
train_iter_loss: 0.13706879317760468
train_iter_loss: 0.18748553097248077
train_iter_loss: 0.22817692160606384
train_iter_loss: 0.21280840039253235
train_iter_loss: 0.24994465708732605
train_iter_loss: 0.16476115584373474
train_iter_loss: 0.1684851199388504
train_iter_loss: 0.24197430908679962
train_iter_loss: 0.21520520746707916
train_iter_loss: 0.23322045803070068
train_iter_loss: 0.15949499607086182
train_iter_loss: 0.1348932981491089
train_iter_loss: 0.08523426204919815
train_iter_loss: 0.38439345359802246
train_iter_loss: 0.19597569108009338
train_iter_loss: 0.26696643233299255
train_iter_loss: 0.1741001158952713
train_iter_loss: 0.18263548612594604
train_iter_loss: 0.18415901064872742
train_iter_loss: 0.16815297305583954
train_iter_loss: 0.1525416225194931
train_iter_loss: 0.10705685615539551
train_iter_loss: 0.20976436138153076
train_iter_loss: 0.15064674615859985
train_iter_loss: 0.10973826050758362
train_iter_loss: 0.053727976977825165
train_iter_loss: 0.1891227513551712
train_iter_loss: 0.22080542147159576
train_iter_loss: 0.20307113230228424
train_iter_loss: 0.11134536564350128
train_iter_loss: 0.23017118871212006
train_iter_loss: 0.16421957314014435
train_iter_loss: 0.11747018992900848
train_iter_loss: 0.14042964577674866
train_iter_loss: 0.1587049812078476
train_iter_loss: 0.11561653763055801
train_iter_loss: 0.12552882730960846
train_iter_loss: 0.11333867907524109
train_iter_loss: 0.25442075729370117
train_iter_loss: 0.20185068249702454
train_iter_loss: 0.14596529304981232
train_iter_loss: 0.1034650206565857
train_iter_loss: 0.13397665321826935
train_iter_loss: 0.1291520893573761
train_iter_loss: 0.18820731341838837
train_iter_loss: 0.1490129828453064
train_iter_loss: 0.13207589089870453
train_iter_loss: 0.2799771726131439
train_iter_loss: 0.2921430468559265
train_iter_loss: 0.2440071403980255
train_iter_loss: 0.17020216584205627
train_iter_loss: 0.099644735455513
train_iter_loss: 0.15882998704910278
train_iter_loss: 0.30134937167167664
train_iter_loss: 0.17932815849781036
train_iter_loss: 0.22319947183132172
train_iter_loss: 0.1407054215669632
train_iter_loss: 0.23435699939727783
train_iter_loss: 0.30890241265296936
train_iter_loss: 0.24177081882953644
train_iter_loss: 0.22091352939605713
train_iter_loss: 0.122747503221035
train_iter_loss: 0.11311724036931992
train_iter_loss: 0.2971533536911011
train_iter_loss: 0.17868199944496155
train loss :0.1781
---------------------
Validation seg loss: 0.2253096890723649 at epoch 120
epoch =    121/  1000, exp = train
train_iter_loss: 0.1406210958957672
train_iter_loss: 0.14484034478664398
train_iter_loss: 0.15231364965438843
train_iter_loss: 0.17458485066890717
train_iter_loss: 0.25872230529785156
train_iter_loss: 0.13287578523159027
train_iter_loss: 0.12028519064188004
train_iter_loss: 0.12673747539520264
train_iter_loss: 0.1779310405254364
train_iter_loss: 0.2058621346950531
train_iter_loss: 0.2024620622396469
train_iter_loss: 0.22409912943840027
train_iter_loss: 0.23681418597698212
train_iter_loss: 0.229851633310318
train_iter_loss: 0.10822439938783646
train_iter_loss: 0.1511688530445099
train_iter_loss: 0.07093888521194458
train_iter_loss: 0.225413978099823
train_iter_loss: 0.1995731145143509
train_iter_loss: 0.24629181623458862
train_iter_loss: 0.18381889164447784
train_iter_loss: 0.06642524898052216
train_iter_loss: 0.23713143169879913
train_iter_loss: 0.17297369241714478
train_iter_loss: 0.184787780046463
train_iter_loss: 0.13588891923427582
train_iter_loss: 0.24936942756175995
train_iter_loss: 0.36717575788497925
train_iter_loss: 0.5798819065093994
train_iter_loss: 0.1407674252986908
train_iter_loss: 0.20719142258167267
train_iter_loss: 0.16387709975242615
train_iter_loss: 0.11326371878385544
train_iter_loss: 0.1840190589427948
train_iter_loss: 0.1739533245563507
train_iter_loss: 0.1549752652645111
train_iter_loss: 0.1886976957321167
train_iter_loss: 0.401837557554245
train_iter_loss: 0.18492619693279266
train_iter_loss: 0.14023339748382568
train_iter_loss: 0.23111118376255035
train_iter_loss: 0.18704649806022644
train_iter_loss: 0.2357412427663803
train_iter_loss: 0.14745132625102997
train_iter_loss: 0.21956926584243774
train_iter_loss: 0.23015883564949036
train_iter_loss: 0.17199371755123138
train_iter_loss: 0.07712361961603165
train_iter_loss: 0.22221821546554565
train_iter_loss: 0.09661879390478134
train_iter_loss: 0.18104605376720428
train_iter_loss: 0.2568322718143463
train_iter_loss: 0.2806542217731476
train_iter_loss: 0.1700473427772522
train_iter_loss: 0.2071014791727066
train_iter_loss: 0.2294377088546753
train_iter_loss: 0.07304351031780243
train_iter_loss: 0.1821848750114441
train_iter_loss: 0.15823687613010406
train_iter_loss: 0.16243568062782288
train_iter_loss: 0.08967918157577515
train_iter_loss: 0.2578367590904236
train_iter_loss: 0.13484209775924683
train_iter_loss: 0.14794111251831055
train_iter_loss: 0.06226811558008194
train_iter_loss: 0.14600342512130737
train_iter_loss: 0.10837207734584808
train_iter_loss: 0.08842384070158005
train_iter_loss: 0.1965489685535431
train_iter_loss: 0.17551331222057343
train_iter_loss: 0.25860685110092163
train_iter_loss: 0.13948658108711243
train_iter_loss: 0.12009795010089874
train_iter_loss: 0.1467205435037613
train_iter_loss: 0.045043252408504486
train_iter_loss: 0.2212456613779068
train_iter_loss: 0.12789012491703033
train_iter_loss: 0.2055225819349289
train_iter_loss: 0.1380842626094818
train_iter_loss: 0.12596873939037323
train_iter_loss: 0.08204608410596848
train_iter_loss: 0.18944481015205383
train_iter_loss: 0.12655816972255707
train_iter_loss: 0.10145886987447739
train_iter_loss: 0.17648126184940338
train_iter_loss: 0.16817951202392578
train_iter_loss: 0.071645088493824
train_iter_loss: 0.16214950382709503
train_iter_loss: 0.343242883682251
train_iter_loss: 0.1535235196352005
train_iter_loss: 0.2977772653102875
train_iter_loss: 0.12311159074306488
train_iter_loss: 0.13621962070465088
train_iter_loss: 0.19084039330482483
train_iter_loss: 0.18036743998527527
train_iter_loss: 0.19581948220729828
train_iter_loss: 0.2420680969953537
train_iter_loss: 0.08253263682126999
train_iter_loss: 0.24726761877536774
train_iter_loss: 0.1448703557252884
train loss :0.1801
---------------------
Validation seg loss: 0.22710883239600457 at epoch 121
epoch =    122/  1000, exp = train
train_iter_loss: 0.15299205482006073
train_iter_loss: 0.1385730355978012
train_iter_loss: 0.1872720569372177
train_iter_loss: 0.17985427379608154
train_iter_loss: 0.15978869795799255
train_iter_loss: 0.1873440444469452
train_iter_loss: 0.1434279829263687
train_iter_loss: 0.2263757735490799
train_iter_loss: 0.323831170797348
train_iter_loss: 0.13829466700553894
train_iter_loss: 0.1787143498659134
train_iter_loss: 0.2183922529220581
train_iter_loss: 0.05514673888683319
train_iter_loss: 0.11390233784914017
train_iter_loss: 0.18035413324832916
train_iter_loss: 0.24088138341903687
train_iter_loss: 0.20283259451389313
train_iter_loss: 0.14485101401805878
train_iter_loss: 0.17445962131023407
train_iter_loss: 0.09191641956567764
train_iter_loss: 0.11372323334217072
train_iter_loss: 0.18798547983169556
train_iter_loss: 0.13725711405277252
train_iter_loss: 0.20034052431583405
train_iter_loss: 0.16224978864192963
train_iter_loss: 0.12050817906856537
train_iter_loss: 0.20368865132331848
train_iter_loss: 0.2679195702075958
train_iter_loss: 0.2022390067577362
train_iter_loss: 0.22448185086250305
train_iter_loss: 0.2141803354024887
train_iter_loss: 0.2039615362882614
train_iter_loss: 0.24411845207214355
train_iter_loss: 0.1625298410654068
train_iter_loss: 0.1828659474849701
train_iter_loss: 0.2955763339996338
train_iter_loss: 0.1405881941318512
train_iter_loss: 0.31662866473197937
train_iter_loss: 0.22054742276668549
train_iter_loss: 0.27042996883392334
train_iter_loss: 0.08488810062408447
train_iter_loss: 0.16321025788784027
train_iter_loss: 0.11288204044103622
train_iter_loss: 0.1746731549501419
train_iter_loss: 0.17128218710422516
train_iter_loss: 0.11179523169994354
train_iter_loss: 0.16055046021938324
train_iter_loss: 0.17318184673786163
train_iter_loss: 0.13498233258724213
train_iter_loss: 0.16484186053276062
train_iter_loss: 0.1491142213344574
train_iter_loss: 0.18617592751979828
train_iter_loss: 0.09751338511705399
train_iter_loss: 0.1782618761062622
train_iter_loss: 0.24937006831169128
train_iter_loss: 0.18911190330982208
train_iter_loss: 0.24906206130981445
train_iter_loss: 0.1460283249616623
train_iter_loss: 0.1038353219628334
train_iter_loss: 0.20304270088672638
train_iter_loss: 0.08950331807136536
train_iter_loss: 0.24344007670879364
train_iter_loss: 0.16567619144916534
train_iter_loss: 0.10931489616632462
train_iter_loss: 0.11639197915792465
train_iter_loss: 0.25510355830192566
train_iter_loss: 0.2946392297744751
train_iter_loss: 0.3156883418560028
train_iter_loss: 0.15929189324378967
train_iter_loss: 0.1779266893863678
train_iter_loss: 0.11225707083940506
train_iter_loss: 0.15119655430316925
train_iter_loss: 0.17675285041332245
train_iter_loss: 0.10334929823875427
train_iter_loss: 0.1083579957485199
train_iter_loss: 0.10669763386249542
train_iter_loss: 0.16536040604114532
train_iter_loss: 0.264031320810318
train_iter_loss: 0.17231498658657074
train_iter_loss: 0.1849956065416336
train_iter_loss: 0.20830100774765015
train_iter_loss: 0.07047367095947266
train_iter_loss: 0.22888463735580444
train_iter_loss: 0.2570796608924866
train_iter_loss: 0.1771073043346405
train_iter_loss: 0.24449415504932404
train_iter_loss: 0.09998735040426254
train_iter_loss: 0.1413855254650116
train_iter_loss: 0.28416135907173157
train_iter_loss: 0.1540820300579071
train_iter_loss: 0.15816281735897064
train_iter_loss: 0.16408878564834595
train_iter_loss: 0.1418490707874298
train_iter_loss: 0.24797573685646057
train_iter_loss: 0.13117803633213043
train_iter_loss: 0.1433432251214981
train_iter_loss: 0.20502068102359772
train_iter_loss: 0.18642643094062805
train_iter_loss: 0.1481674611568451
train_iter_loss: 0.23835420608520508
train loss :0.1800
---------------------
Validation seg loss: 0.2264683185461557 at epoch 122
epoch =    123/  1000, exp = train
train_iter_loss: 0.20740263164043427
train_iter_loss: 0.3008725643157959
train_iter_loss: 0.08988441526889801
train_iter_loss: 0.1276293396949768
train_iter_loss: 0.24281322956085205
train_iter_loss: 0.23883184790611267
train_iter_loss: 0.2036062479019165
train_iter_loss: 0.21744437515735626
train_iter_loss: 0.1041780486702919
train_iter_loss: 0.16007786989212036
train_iter_loss: 0.14318908751010895
train_iter_loss: 0.18564842641353607
train_iter_loss: 0.18124271929264069
train_iter_loss: 0.1923256814479828
train_iter_loss: 0.1850709617137909
train_iter_loss: 0.11575409770011902
train_iter_loss: 0.24337947368621826
train_iter_loss: 0.15416648983955383
train_iter_loss: 0.15502677857875824
train_iter_loss: 0.1494494378566742
train_iter_loss: 0.07275984436273575
train_iter_loss: 0.17684681713581085
train_iter_loss: 0.19029085338115692
train_iter_loss: 0.2069062441587448
train_iter_loss: 0.14221180975437164
train_iter_loss: 0.20056463778018951
train_iter_loss: 0.11386825144290924
train_iter_loss: 0.1900700181722641
train_iter_loss: 0.21309195458889008
train_iter_loss: 0.21913836896419525
train_iter_loss: 0.17285138368606567
train_iter_loss: 0.2330775260925293
train_iter_loss: 0.20010153949260712
train_iter_loss: 0.1163254976272583
train_iter_loss: 0.18200430274009705
train_iter_loss: 0.07608508318662643
train_iter_loss: 0.04079329967498779
train_iter_loss: 0.14999271929264069
train_iter_loss: 0.14376936852931976
train_iter_loss: 0.1274806261062622
train_iter_loss: 0.1718294322490692
train_iter_loss: 0.23564422130584717
train_iter_loss: 0.20326177775859833
train_iter_loss: 0.09692945331335068
train_iter_loss: 0.2083187997341156
train_iter_loss: 0.17385755479335785
train_iter_loss: 0.19740909337997437
train_iter_loss: 0.16783085465431213
train_iter_loss: 0.08449812978506088
train_iter_loss: 0.24645541608333588
train_iter_loss: 0.13692936301231384
train_iter_loss: 0.14178819954395294
train_iter_loss: 0.24165049195289612
train_iter_loss: 0.13558539748191833
train_iter_loss: 0.1921934187412262
train_iter_loss: 0.28612634539604187
train_iter_loss: 0.19454117119312286
train_iter_loss: 0.13603714108467102
train_iter_loss: 0.39086633920669556
train_iter_loss: 0.1484372764825821
train_iter_loss: 0.12774798274040222
train_iter_loss: 0.13929997384548187
train_iter_loss: 0.12785331904888153
train_iter_loss: 0.18330985307693481
train_iter_loss: 0.1501678228378296
train_iter_loss: 0.22413219511508942
train_iter_loss: 0.14709940552711487
train_iter_loss: 0.18153168261051178
train_iter_loss: 0.238226518034935
train_iter_loss: 0.09370233863592148
train_iter_loss: 0.253740519285202
train_iter_loss: 0.1391623467206955
train_iter_loss: 0.11537839472293854
train_iter_loss: 0.25345781445503235
train_iter_loss: 0.09987596422433853
train_iter_loss: 0.16088370978832245
train_iter_loss: 0.10786771774291992
train_iter_loss: 0.26120397448539734
train_iter_loss: 0.14801694452762604
train_iter_loss: 0.11137726157903671
train_iter_loss: 0.18379762768745422
train_iter_loss: 0.20284110307693481
train_iter_loss: 0.18554247915744781
train_iter_loss: 0.11450985819101334
train_iter_loss: 0.2451515793800354
train_iter_loss: 0.2014540135860443
train_iter_loss: 0.24086131155490875
train_iter_loss: 0.1236007884144783
train_iter_loss: 0.16128532588481903
train_iter_loss: 0.13217008113861084
train_iter_loss: 0.13001836836338043
train_iter_loss: 0.17989659309387207
train_iter_loss: 0.20540887117385864
train_iter_loss: 0.34014782309532166
train_iter_loss: 0.08341796696186066
train_iter_loss: 0.16391406953334808
train_iter_loss: 0.20755718648433685
train_iter_loss: 0.23533034324645996
train_iter_loss: 0.22472646832466125
train_iter_loss: 0.2279413789510727
train loss :0.1779
---------------------
Validation seg loss: 0.22938690425933533 at epoch 123
epoch =    124/  1000, exp = train
train_iter_loss: 0.12472575157880783
train_iter_loss: 0.22407959401607513
train_iter_loss: 0.10603449493646622
train_iter_loss: 0.08262501657009125
train_iter_loss: 0.15414729714393616
train_iter_loss: 0.1264585703611374
train_iter_loss: 0.18258966505527496
train_iter_loss: 0.1444118767976761
train_iter_loss: 0.18774113059043884
train_iter_loss: 0.17938028275966644
train_iter_loss: 0.10207833349704742
train_iter_loss: 0.10307639837265015
train_iter_loss: 0.14876945316791534
train_iter_loss: 0.095564104616642
train_iter_loss: 0.19915872812271118
train_iter_loss: 0.21528778970241547
train_iter_loss: 0.14869405329227448
train_iter_loss: 0.32050901651382446
train_iter_loss: 0.11841702461242676
train_iter_loss: 0.10983777046203613
train_iter_loss: 0.14062125980854034
train_iter_loss: 0.07835521548986435
train_iter_loss: 0.13843867182731628
train_iter_loss: 0.33644920587539673
train_iter_loss: 0.1903177946805954
train_iter_loss: 0.0920344889163971
train_iter_loss: 0.11132334917783737
train_iter_loss: 0.22475451231002808
train_iter_loss: 0.23752188682556152
train_iter_loss: 0.2815350294113159
train_iter_loss: 0.222134530544281
train_iter_loss: 0.23263536393642426
train_iter_loss: 0.1408534198999405
train_iter_loss: 0.177961528301239
train_iter_loss: 0.21372751891613007
train_iter_loss: 0.1450417935848236
train_iter_loss: 0.16859084367752075
train_iter_loss: 0.22756105661392212
train_iter_loss: 0.0731590986251831
train_iter_loss: 0.2306436002254486
train_iter_loss: 0.24955783784389496
train_iter_loss: 0.0650535523891449
train_iter_loss: 0.26906147599220276
train_iter_loss: 0.16250485181808472
train_iter_loss: 0.13606198132038116
train_iter_loss: 0.2609032690525055
train_iter_loss: 0.14386925101280212
train_iter_loss: 0.2376059591770172
train_iter_loss: 0.19698689877986908
train_iter_loss: 0.09941819310188293
train_iter_loss: 0.146006777882576
train_iter_loss: 0.2583519518375397
train_iter_loss: 0.2572706639766693
train_iter_loss: 0.2740195095539093
train_iter_loss: 0.12993884086608887
train_iter_loss: 0.20238097012043
train_iter_loss: 0.10559031367301941
train_iter_loss: 0.16241677105426788
train_iter_loss: 0.07494881004095078
train_iter_loss: 0.24807758629322052
train_iter_loss: 0.16822588443756104
train_iter_loss: 0.17066705226898193
train_iter_loss: 0.12423478066921234
train_iter_loss: 0.14668264985084534
train_iter_loss: 0.1694760024547577
train_iter_loss: 0.13853193819522858
train_iter_loss: 0.22590041160583496
train_iter_loss: 0.13510330021381378
train_iter_loss: 0.1480174958705902
train_iter_loss: 0.16315853595733643
train_iter_loss: 0.12520556151866913
train_iter_loss: 0.23079636693000793
train_iter_loss: 0.19554834067821503
train_iter_loss: 0.21597489714622498
train_iter_loss: 0.12447994947433472
train_iter_loss: 0.12962223589420319
train_iter_loss: 0.12390481680631638
train_iter_loss: 0.27317938208580017
train_iter_loss: 0.13108643889427185
train_iter_loss: 0.18513944745063782
train_iter_loss: 0.12252163887023926
train_iter_loss: 0.10990571230649948
train_iter_loss: 0.19537782669067383
train_iter_loss: 0.24759194254875183
train_iter_loss: 0.11727271974086761
train_iter_loss: 0.3088478147983551
train_iter_loss: 0.18935240805149078
train_iter_loss: 0.21582172811031342
train_iter_loss: 0.31167954206466675
train_iter_loss: 0.18824870884418488
train_iter_loss: 0.160197451710701
train_iter_loss: 0.18895748257637024
train_iter_loss: 0.1806522160768509
train_iter_loss: 0.13190893828868866
train_iter_loss: 0.18776623904705048
train_iter_loss: 0.21225407719612122
train_iter_loss: 0.12773920595645905
train_iter_loss: 0.12392351031303406
train_iter_loss: 0.17725782096385956
train_iter_loss: 0.09924685955047607
train loss :0.1752
---------------------
Validation seg loss: 0.226083601789795 at epoch 124
epoch =    125/  1000, exp = train
train_iter_loss: 0.124315045773983
train_iter_loss: 0.12413238734006882
train_iter_loss: 0.26970213651657104
train_iter_loss: 0.156492680311203
train_iter_loss: 0.07965198904275894
train_iter_loss: 0.10265656560659409
train_iter_loss: 0.13680489361286163
train_iter_loss: 0.14508414268493652
train_iter_loss: 0.13495367765426636
train_iter_loss: 0.16043426096439362
train_iter_loss: 0.14533478021621704
train_iter_loss: 0.14818082749843597
train_iter_loss: 0.15232758224010468
train_iter_loss: 0.11197495460510254
train_iter_loss: 0.1265227347612381
train_iter_loss: 0.16594131290912628
train_iter_loss: 0.10587400197982788
train_iter_loss: 0.18456193804740906
train_iter_loss: 0.1970386803150177
train_iter_loss: 0.1628066897392273
train_iter_loss: 0.10127360373735428
train_iter_loss: 0.12499672174453735
train_iter_loss: 0.22416329383850098
train_iter_loss: 0.23895561695098877
train_iter_loss: 0.09022270888090134
train_iter_loss: 0.25456658005714417
train_iter_loss: 0.1240052580833435
train_iter_loss: 0.271744042634964
train_iter_loss: 0.1860751360654831
train_iter_loss: 0.13922357559204102
train_iter_loss: 0.18386372923851013
train_iter_loss: 0.1808956414461136
train_iter_loss: 0.18696017563343048
train_iter_loss: 0.14988316595554352
train_iter_loss: 0.36497393250465393
train_iter_loss: 0.1656598448753357
train_iter_loss: 0.13389728963375092
train_iter_loss: 0.13754943013191223
train_iter_loss: 0.09053894132375717
train_iter_loss: 0.16148681938648224
train_iter_loss: 0.2659013867378235
train_iter_loss: 0.15235894918441772
train_iter_loss: 0.2045704424381256
train_iter_loss: 0.27368712425231934
train_iter_loss: 0.27008968591690063
train_iter_loss: 0.12800732254981995
train_iter_loss: 0.08509751409292221
train_iter_loss: 0.2110789269208908
train_iter_loss: 0.31619757413864136
train_iter_loss: 0.21591892838478088
train_iter_loss: 0.16425704956054688
train_iter_loss: 0.1387811154127121
train_iter_loss: 0.09647516161203384
train_iter_loss: 0.2182004153728485
train_iter_loss: 0.1656389832496643
train_iter_loss: 0.16657468676567078
train_iter_loss: 0.24007780849933624
train_iter_loss: 0.14648091793060303
train_iter_loss: 0.1639377474784851
train_iter_loss: 0.15199923515319824
train_iter_loss: 0.20304924249649048
train_iter_loss: 0.1270419806241989
train_iter_loss: 0.1252087652683258
train_iter_loss: 0.18316948413848877
train_iter_loss: 0.1593644618988037
train_iter_loss: 0.07188975065946579
train_iter_loss: 0.1681932955980301
train_iter_loss: 0.08727899193763733
train_iter_loss: 0.19879043102264404
train_iter_loss: 0.1010991707444191
train_iter_loss: 0.15169763565063477
train_iter_loss: 0.1769997924566269
train_iter_loss: 0.18522222340106964
train_iter_loss: 0.1302199512720108
train_iter_loss: 0.24301157891750336
train_iter_loss: 0.17123904824256897
train_iter_loss: 0.159062460064888
train_iter_loss: 0.10546264797449112
train_iter_loss: 0.2341994047164917
train_iter_loss: 0.13004525005817413
train_iter_loss: 0.2376861870288849
train_iter_loss: 0.15433867275714874
train_iter_loss: 0.1459118127822876
train_iter_loss: 0.18174917995929718
train_iter_loss: 0.21934325993061066
train_iter_loss: 0.13738365471363068
train_iter_loss: 0.1995624303817749
train_iter_loss: 0.1449030190706253
train_iter_loss: 0.18367592990398407
train_iter_loss: 0.21786761283874512
train_iter_loss: 0.1312011331319809
train_iter_loss: 0.29696449637413025
train_iter_loss: 0.15094025433063507
train_iter_loss: 0.11532339453697205
train_iter_loss: 0.1111818477511406
train_iter_loss: 0.19304566085338593
train_iter_loss: 0.28449347615242004
train_iter_loss: 0.23903751373291016
train_iter_loss: 0.24028094112873077
train_iter_loss: 0.0499434620141983
train loss :0.1714
---------------------
Validation seg loss: 0.22528632331358375 at epoch 125
epoch =    126/  1000, exp = train
train_iter_loss: 0.1350129395723343
train_iter_loss: 0.21718843281269073
train_iter_loss: 0.11976095288991928
train_iter_loss: 0.10694015026092529
train_iter_loss: 0.2186180204153061
train_iter_loss: 0.2659333050251007
train_iter_loss: 0.1602863073348999
train_iter_loss: 0.11353594809770584
train_iter_loss: 0.20355454087257385
train_iter_loss: 0.17476004362106323
train_iter_loss: 0.16748923063278198
train_iter_loss: 0.24332793056964874
train_iter_loss: 0.17587779462337494
train_iter_loss: 0.16754764318466187
train_iter_loss: 0.15128538012504578
train_iter_loss: 0.18918994069099426
train_iter_loss: 0.17330780625343323
train_iter_loss: 0.18814146518707275
train_iter_loss: 0.21101374924182892
train_iter_loss: 0.2410452663898468
train_iter_loss: 0.22641953825950623
train_iter_loss: 0.1934434175491333
train_iter_loss: 0.34301188588142395
train_iter_loss: 0.1412966251373291
train_iter_loss: 0.184541255235672
train_iter_loss: 0.07377845793962479
train_iter_loss: 0.2594800591468811
train_iter_loss: 0.16233587265014648
train_iter_loss: 0.13548444211483002
train_iter_loss: 0.2027941644191742
train_iter_loss: 0.16841618716716766
train_iter_loss: 0.14397397637367249
train_iter_loss: 0.1527334451675415
train_iter_loss: 0.11332809925079346
train_iter_loss: 0.2590438723564148
train_iter_loss: 0.25096383690834045
train_iter_loss: 0.1554206907749176
train_iter_loss: 0.1918753683567047
train_iter_loss: 0.18764759600162506
train_iter_loss: 0.11735773831605911
train_iter_loss: 0.22025467455387115
train_iter_loss: 0.2009531408548355
train_iter_loss: 0.16675712168216705
train_iter_loss: 0.09493473172187805
train_iter_loss: 0.2583402991294861
train_iter_loss: 0.11847240477800369
train_iter_loss: 0.10749969631433487
train_iter_loss: 0.21581651270389557
train_iter_loss: 0.17687925696372986
train_iter_loss: 0.09380978345870972
train_iter_loss: 0.10715760290622711
train_iter_loss: 0.14597812294960022
train_iter_loss: 0.13449950516223907
train_iter_loss: 0.17532996833324432
train_iter_loss: 0.19705797731876373
train_iter_loss: 0.15786653757095337
train_iter_loss: 0.14180073142051697
train_iter_loss: 0.16830717027187347
train_iter_loss: 0.18907436728477478
train_iter_loss: 0.16401349008083344
train_iter_loss: 0.15923011302947998
train_iter_loss: 0.10722430050373077
train_iter_loss: 0.12237997353076935
train_iter_loss: 0.199493408203125
train_iter_loss: 0.15867479145526886
train_iter_loss: 0.15878619253635406
train_iter_loss: 0.24938999116420746
train_iter_loss: 0.22717376053333282
train_iter_loss: 0.19032840430736542
train_iter_loss: 0.1486457735300064
train_iter_loss: 0.15381862223148346
train_iter_loss: 0.2126619815826416
train_iter_loss: 0.13058920204639435
train_iter_loss: 0.184560626745224
train_iter_loss: 0.11319342255592346
train_iter_loss: 0.19506724178791046
train_iter_loss: 0.1070646122097969
train_iter_loss: 0.19520264863967896
train_iter_loss: 0.25867748260498047
train_iter_loss: 0.14187981188297272
train_iter_loss: 0.21201293170452118
train_iter_loss: 0.11417223513126373
train_iter_loss: 0.2500346899032593
train_iter_loss: 0.1836392730474472
train_iter_loss: 0.14265678822994232
train_iter_loss: 0.16913308203220367
train_iter_loss: 0.1318790763616562
train_iter_loss: 0.1548076570034027
train_iter_loss: 0.20924131572246552
train_iter_loss: 0.1756749451160431
train_iter_loss: 0.17462339997291565
train_iter_loss: 0.14732985198497772
train_iter_loss: 0.11703477799892426
train_iter_loss: 0.1284758746623993
train_iter_loss: 0.12091104686260223
train_iter_loss: 0.1473795473575592
train_iter_loss: 0.17574992775917053
train_iter_loss: 0.1603517234325409
train_iter_loss: 0.18114395439624786
train_iter_loss: 0.26833388209342957
train loss :0.1747
---------------------
Validation seg loss: 0.22450386193352487 at epoch 126
epoch =    127/  1000, exp = train
train_iter_loss: 0.15431348979473114
train_iter_loss: 0.1279660165309906
train_iter_loss: 0.1529887169599533
train_iter_loss: 0.22636404633522034
train_iter_loss: 0.19181568920612335
train_iter_loss: 0.1258290559053421
train_iter_loss: 0.27453261613845825
train_iter_loss: 0.19103358685970306
train_iter_loss: 0.1730479598045349
train_iter_loss: 0.057930465787649155
train_iter_loss: 0.24088463187217712
train_iter_loss: 0.21186518669128418
train_iter_loss: 0.19055530428886414
train_iter_loss: 0.1502898633480072
train_iter_loss: 0.17905248701572418
train_iter_loss: 0.13184888660907745
train_iter_loss: 0.13766983151435852
train_iter_loss: 0.13307154178619385
train_iter_loss: 0.1662190705537796
train_iter_loss: 0.23093420267105103
train_iter_loss: 0.14772184193134308
train_iter_loss: 0.12171823531389236
train_iter_loss: 0.12930069863796234
train_iter_loss: 0.24122074246406555
train_iter_loss: 0.16816343367099762
train_iter_loss: 0.16322612762451172
train_iter_loss: 0.12442278116941452
train_iter_loss: 0.23260435461997986
train_iter_loss: 0.22461454570293427
train_iter_loss: 0.2533580958843231
train_iter_loss: 0.16307321190834045
train_iter_loss: 0.18595348298549652
train_iter_loss: 0.1474566012620926
train_iter_loss: 0.10130699723958969
train_iter_loss: 0.2179478257894516
train_iter_loss: 0.18247586488723755
train_iter_loss: 0.1797327846288681
train_iter_loss: 0.2347753643989563
train_iter_loss: 0.17262139916419983
train_iter_loss: 0.2216847836971283
train_iter_loss: 0.19470836222171783
train_iter_loss: 0.26138484477996826
train_iter_loss: 0.09988348931074142
train_iter_loss: 0.17256660759449005
train_iter_loss: 0.18158479034900665
train_iter_loss: 0.0639495849609375
train_iter_loss: 0.1500800997018814
train_iter_loss: 0.16809913516044617
train_iter_loss: 0.24077439308166504
train_iter_loss: 0.18718746304512024
train_iter_loss: 0.11343047767877579
train_iter_loss: 0.1752687245607376
train_iter_loss: 0.17220190167427063
train_iter_loss: 0.2907886505126953
train_iter_loss: 0.18136559426784515
train_iter_loss: 0.11913245171308517
train_iter_loss: 0.16896557807922363
train_iter_loss: 0.31971606612205505
train_iter_loss: 0.1825987696647644
train_iter_loss: 0.10742050409317017
train_iter_loss: 0.08651547878980637
train_iter_loss: 0.08051545172929764
train_iter_loss: 0.19211828708648682
train_iter_loss: 0.11319336295127869
train_iter_loss: 0.11388717591762543
train_iter_loss: 0.16652439534664154
train_iter_loss: 0.18590523302555084
train_iter_loss: 0.18115007877349854
train_iter_loss: 0.2467682659626007
train_iter_loss: 0.1978897750377655
train_iter_loss: 0.09678611159324646
train_iter_loss: 0.2226080596446991
train_iter_loss: 0.18821492791175842
train_iter_loss: 0.12896625697612762
train_iter_loss: 0.1849084049463272
train_iter_loss: 0.06734994053840637
train_iter_loss: 0.25968772172927856
train_iter_loss: 0.21869301795959473
train_iter_loss: 0.10146404802799225
train_iter_loss: 0.21773876249790192
train_iter_loss: 0.1700892448425293
train_iter_loss: 0.09066381305456161
train_iter_loss: 0.3149327039718628
train_iter_loss: 0.1681806743144989
train_iter_loss: 0.15865182876586914
train_iter_loss: 0.16003598272800446
train_iter_loss: 0.18979723751544952
train_iter_loss: 0.09632927179336548
train_iter_loss: 0.16718046367168427
train_iter_loss: 0.2387007474899292
train_iter_loss: 0.18795499205589294
train_iter_loss: 0.10258415341377258
train_iter_loss: 0.2901657521724701
train_iter_loss: 0.27415135502815247
train_iter_loss: 0.11851402372121811
train_iter_loss: 0.18826892971992493
train_iter_loss: 0.14879848062992096
train_iter_loss: 0.21646440029144287
train_iter_loss: 0.15458142757415771
train_iter_loss: 0.11616955697536469
train loss :0.1759
---------------------
Validation seg loss: 0.2239109975689987 at epoch 127
epoch =    128/  1000, exp = train
train_iter_loss: 0.1652114987373352
train_iter_loss: 0.1370047777891159
train_iter_loss: 0.21615520119667053
train_iter_loss: 0.21390464901924133
train_iter_loss: 0.16767346858978271
train_iter_loss: 0.14780788123607635
train_iter_loss: 0.18712641298770905
train_iter_loss: 0.25000670552253723
train_iter_loss: 0.16108036041259766
train_iter_loss: 0.17180444300174713
train_iter_loss: 0.20367920398712158
train_iter_loss: 0.11849191784858704
train_iter_loss: 0.11356130987405777
train_iter_loss: 0.35992667078971863
train_iter_loss: 0.26667532324790955
train_iter_loss: 0.18674485385417938
train_iter_loss: 0.16927611827850342
train_iter_loss: 0.2720852792263031
train_iter_loss: 0.11648861318826675
train_iter_loss: 0.2728310227394104
train_iter_loss: 0.18597036600112915
train_iter_loss: 0.14645883440971375
train_iter_loss: 0.06671266257762909
train_iter_loss: 0.19749632477760315
train_iter_loss: 0.1743181198835373
train_iter_loss: 0.14296306669712067
train_iter_loss: 0.21371415257453918
train_iter_loss: 0.07408145070075989
train_iter_loss: 0.14149805903434753
train_iter_loss: 0.17687872052192688
train_iter_loss: 0.1345493346452713
train_iter_loss: 0.12669917941093445
train_iter_loss: 0.17028817534446716
train_iter_loss: 0.1444878727197647
train_iter_loss: 0.18858659267425537
train_iter_loss: 0.18228092789649963
train_iter_loss: 0.1738121211528778
train_iter_loss: 0.21785344183444977
train_iter_loss: 0.22929556667804718
train_iter_loss: 0.30053845047950745
train_iter_loss: 0.22431331872940063
train_iter_loss: 0.203340545296669
train_iter_loss: 0.25094473361968994
train_iter_loss: 0.22163069248199463
train_iter_loss: 0.23433053493499756
train_iter_loss: 0.2342372089624405
train_iter_loss: 0.16723132133483887
train_iter_loss: 0.19105832278728485
train_iter_loss: 0.16224455833435059
train_iter_loss: 0.2234000265598297
train_iter_loss: 0.12443964183330536
train_iter_loss: 0.16027334332466125
train_iter_loss: 0.1159205213189125
train_iter_loss: 0.19829779863357544
train_iter_loss: 0.17136922478675842
train_iter_loss: 0.13479353487491608
train_iter_loss: 0.2012980729341507
train_iter_loss: 0.08299919217824936
train_iter_loss: 0.21023276448249817
train_iter_loss: 0.10850831866264343
train_iter_loss: 0.12303495407104492
train_iter_loss: 0.23419338464736938
train_iter_loss: 0.15776528418064117
train_iter_loss: 0.10540488362312317
train_iter_loss: 0.04928519204258919
train_iter_loss: 0.11036954075098038
train_iter_loss: 0.202296182513237
train_iter_loss: 0.16691192984580994
train_iter_loss: 0.16976548731327057
train_iter_loss: 0.16731704771518707
train_iter_loss: 0.15183810889720917
train_iter_loss: 0.15800638496875763
train_iter_loss: 0.24076972901821136
train_iter_loss: 0.10635485500097275
train_iter_loss: 0.19937483966350555
train_iter_loss: 0.2559741735458374
train_iter_loss: 0.1047784611582756
train_iter_loss: 0.15580697357654572
train_iter_loss: 0.29252272844314575
train_iter_loss: 0.26573488116264343
train_iter_loss: 0.16766823828220367
train_iter_loss: 0.2556864619255066
train_iter_loss: 0.190713569521904
train_iter_loss: 0.12113041430711746
train_iter_loss: 0.18502862751483917
train_iter_loss: 0.1682705134153366
train_iter_loss: 0.11945705115795135
train_iter_loss: 0.08938351273536682
train_iter_loss: 0.18275271356105804
train_iter_loss: 0.1557944118976593
train_iter_loss: 0.1377495676279068
train_iter_loss: 0.11062806099653244
train_iter_loss: 0.17905189096927643
train_iter_loss: 0.2657509744167328
train_iter_loss: 0.1929628998041153
train_iter_loss: 0.16956253349781036
train_iter_loss: 0.2104707956314087
train_iter_loss: 0.14741185307502747
train_iter_loss: 0.13870659470558167
train_iter_loss: 0.13313144445419312
train loss :0.1782
---------------------
Validation seg loss: 0.22581039787322846 at epoch 128
epoch =    129/  1000, exp = train
train_iter_loss: 0.21302689611911774
train_iter_loss: 0.12362327426671982
train_iter_loss: 0.08164606988430023
train_iter_loss: 0.13641357421875
train_iter_loss: 0.09434519708156586
train_iter_loss: 0.1771201342344284
train_iter_loss: 0.1179361492395401
train_iter_loss: 0.17830568552017212
train_iter_loss: 0.2137434333562851
train_iter_loss: 0.16885265707969666
train_iter_loss: 0.10486152768135071
train_iter_loss: 0.12934640049934387
train_iter_loss: 0.15191002190113068
train_iter_loss: 0.3689466416835785
train_iter_loss: 0.25239646434783936
train_iter_loss: 0.15553371608257294
train_iter_loss: 0.11858760565519333
train_iter_loss: 0.1845318228006363
train_iter_loss: 0.38111764192581177
train_iter_loss: 0.10169991105794907
train_iter_loss: 0.17390847206115723
train_iter_loss: 0.16243185102939606
train_iter_loss: 0.13138356804847717
train_iter_loss: 0.11160583049058914
train_iter_loss: 0.1483544111251831
train_iter_loss: 0.1354166567325592
train_iter_loss: 0.1314750462770462
train_iter_loss: 0.2726591229438782
train_iter_loss: 0.155582457780838
train_iter_loss: 0.09238597005605698
train_iter_loss: 0.07801299542188644
train_iter_loss: 0.13511617481708527
train_iter_loss: 0.12951627373695374
train_iter_loss: 0.1436132937669754
train_iter_loss: 0.3535129427909851
train_iter_loss: 0.19272513687610626
train_iter_loss: 0.08202269673347473
train_iter_loss: 0.2428327202796936
train_iter_loss: 0.2302042692899704
train_iter_loss: 0.3876076638698578
train_iter_loss: 0.14729435741901398
train_iter_loss: 0.1960316300392151
train_iter_loss: 0.13772358000278473
train_iter_loss: 0.24686147272586823
train_iter_loss: 0.1641484647989273
train_iter_loss: 0.12944447994232178
train_iter_loss: 0.18005400896072388
train_iter_loss: 0.17808260023593903
train_iter_loss: 0.2587350010871887
train_iter_loss: 0.26159369945526123
train_iter_loss: 0.19471991062164307
train_iter_loss: 0.11091472953557968
train_iter_loss: 0.0785849466919899
train_iter_loss: 0.20048053562641144
train_iter_loss: 0.12479675561189651
train_iter_loss: 0.17737604677677155
train_iter_loss: 0.13321344554424286
train_iter_loss: 0.09345186501741409
train_iter_loss: 0.191462442278862
train_iter_loss: 0.08401233702898026
train_iter_loss: 0.17996981739997864
train_iter_loss: 0.19345569610595703
train_iter_loss: 0.15019051730632782
train_iter_loss: 0.10691208392381668
train_iter_loss: 0.26192009449005127
train_iter_loss: 0.15054816007614136
train_iter_loss: 0.18878430128097534
train_iter_loss: 0.2571754455566406
train_iter_loss: 0.1998179405927658
train_iter_loss: 0.1303059458732605
train_iter_loss: 0.14560119807720184
train_iter_loss: 0.07204995304346085
train_iter_loss: 0.1854601800441742
train_iter_loss: 0.14791761338710785
train_iter_loss: 0.16667979955673218
train_iter_loss: 0.16445237398147583
train_iter_loss: 0.18638664484024048
train_iter_loss: 0.13148380815982819
train_iter_loss: 0.39271607995033264
train_iter_loss: 0.16768038272857666
train_iter_loss: 0.0943920835852623
train_iter_loss: 0.14181169867515564
train_iter_loss: 0.15228255093097687
train_iter_loss: 0.08596138656139374
train_iter_loss: 0.166193887591362
train_iter_loss: 0.2541831135749817
train_iter_loss: 0.2278323620557785
train_iter_loss: 0.1985786110162735
train_iter_loss: 0.2434421330690384
train_iter_loss: 0.1201961487531662
train_iter_loss: 0.1901092827320099
train_iter_loss: 0.24715009331703186
train_iter_loss: 0.13191400468349457
train_iter_loss: 0.12288867682218552
train_iter_loss: 0.2351781576871872
train_iter_loss: 0.23688192665576935
train_iter_loss: 0.21569301187992096
train_iter_loss: 0.1353837102651596
train_iter_loss: 0.08782527595758438
train_iter_loss: 0.11030010133981705
train loss :0.1739
---------------------
Validation seg loss: 0.22430735443420005 at epoch 129
epoch =    130/  1000, exp = train
train_iter_loss: 0.16112849116325378
train_iter_loss: 0.22258029878139496
train_iter_loss: 0.23681844770908356
train_iter_loss: 0.22007612884044647
train_iter_loss: 0.23276656866073608
train_iter_loss: 0.08082554489374161
train_iter_loss: 0.16072183847427368
train_iter_loss: 0.15818944573402405
train_iter_loss: 0.09831884503364563
train_iter_loss: 0.21653497219085693
train_iter_loss: 0.14013290405273438
train_iter_loss: 0.17471849918365479
train_iter_loss: 0.2967393696308136
train_iter_loss: 0.22112761437892914
train_iter_loss: 0.24096673727035522
train_iter_loss: 0.14102885127067566
train_iter_loss: 0.23651443421840668
train_iter_loss: 0.10458078235387802
train_iter_loss: 0.10380987077951431
train_iter_loss: 0.1997864991426468
train_iter_loss: 0.18471933901309967
train_iter_loss: 0.16119611263275146
train_iter_loss: 0.1791904866695404
train_iter_loss: 0.20035086572170258
train_iter_loss: 0.09896013140678406
train_iter_loss: 0.14134901762008667
train_iter_loss: 0.36614036560058594
train_iter_loss: 0.088068388402462
train_iter_loss: 0.21072623133659363
train_iter_loss: 0.11604150384664536
train_iter_loss: 0.21626482903957367
train_iter_loss: 0.20653177797794342
train_iter_loss: 0.19809284806251526
train_iter_loss: 0.11911407113075256
train_iter_loss: 0.14552967250347137
train_iter_loss: 0.06520634144544601
train_iter_loss: 0.19285309314727783
train_iter_loss: 0.24770388007164001
train_iter_loss: 0.3517302870750427
train_iter_loss: 0.1289321482181549
train_iter_loss: 0.2595251202583313
train_iter_loss: 0.11807321012020111
train_iter_loss: 0.2391795516014099
train_iter_loss: 0.14900614321231842
train_iter_loss: 0.2269963026046753
train_iter_loss: 0.10666020214557648
train_iter_loss: 0.15435130894184113
train_iter_loss: 0.17866797745227814
train_iter_loss: 0.15021254122257233
train_iter_loss: 0.1937214434146881
train_iter_loss: 0.17716360092163086
train_iter_loss: 0.1667405515909195
train_iter_loss: 0.14897501468658447
train_iter_loss: 0.23260213434696198
train_iter_loss: 0.1245872750878334
train_iter_loss: 0.031233029440045357
train_iter_loss: 0.19276657700538635
train_iter_loss: 0.07562271505594254
train_iter_loss: 0.1922350525856018
train_iter_loss: 0.19416497647762299
train_iter_loss: 0.24094855785369873
train_iter_loss: 0.22109724581241608
train_iter_loss: 0.18159222602844238
train_iter_loss: 0.1026124432682991
train_iter_loss: 0.09331454336643219
train_iter_loss: 0.08272933214902878
train_iter_loss: 0.3108726441860199
train_iter_loss: 0.28448593616485596
train_iter_loss: 0.19496406614780426
train_iter_loss: 0.16926616430282593
train_iter_loss: 0.26419925689697266
train_iter_loss: 0.17276008427143097
train_iter_loss: 0.22359053790569305
train_iter_loss: 0.14107251167297363
train_iter_loss: 0.17190538346767426
train_iter_loss: 0.1484091877937317
train_iter_loss: 0.13956032693386078
train_iter_loss: 0.25002890825271606
train_iter_loss: 0.2156122773885727
train_iter_loss: 0.2900446951389313
train_iter_loss: 0.3684711456298828
train_iter_loss: 0.223801851272583
train_iter_loss: 0.08156255632638931
train_iter_loss: 0.17858819663524628
train_iter_loss: 0.21234197914600372
train_iter_loss: 0.14448413252830505
train_iter_loss: 0.15703833103179932
train_iter_loss: 0.18080058693885803
train_iter_loss: 0.1792118400335312
train_iter_loss: 0.10147758573293686
train_iter_loss: 0.1228702962398529
train_iter_loss: 0.25600025057792664
train_iter_loss: 0.1278790682554245
train_iter_loss: 0.10293799638748169
train_iter_loss: 0.1940285712480545
train_iter_loss: 0.1310546100139618
train_iter_loss: 0.06295284628868103
train_iter_loss: 0.2152509242296219
train_iter_loss: 0.07861488312482834
train_iter_loss: 0.29692983627319336
train loss :0.1807
---------------------
Validation seg loss: 0.2242390221019961 at epoch 130
epoch =    131/  1000, exp = train
train_iter_loss: 0.11279569566249847
train_iter_loss: 0.17239204049110413
train_iter_loss: 0.09824702143669128
train_iter_loss: 0.12872520089149475
train_iter_loss: 0.06183982640504837
train_iter_loss: 0.2579576373100281
train_iter_loss: 0.19065330922603607
train_iter_loss: 0.19408991932868958
train_iter_loss: 0.09017248451709747
train_iter_loss: 0.16793449223041534
train_iter_loss: 0.12750938534736633
train_iter_loss: 0.14702914655208588
train_iter_loss: 0.13400967419147491
train_iter_loss: 0.17490147054195404
train_iter_loss: 0.31704822182655334
train_iter_loss: 0.11329618096351624
train_iter_loss: 0.17262843251228333
train_iter_loss: 0.0800696387887001
train_iter_loss: 0.07698699831962585
train_iter_loss: 0.12436093389987946
train_iter_loss: 0.13293389976024628
train_iter_loss: 0.1320478320121765
train_iter_loss: 0.16574044525623322
train_iter_loss: 0.29849058389663696
train_iter_loss: 0.2524317800998688
train_iter_loss: 0.31141698360443115
train_iter_loss: 0.09901249408721924
train_iter_loss: 0.23220106959342957
train_iter_loss: 0.09006590396165848
train_iter_loss: 0.10716729611158371
train_iter_loss: 0.17096126079559326
train_iter_loss: 0.25884681940078735
train_iter_loss: 0.1628407984972
train_iter_loss: 0.10512451827526093
train_iter_loss: 0.21359986066818237
train_iter_loss: 0.1872454434633255
train_iter_loss: 0.18200688064098358
train_iter_loss: 0.17746195197105408
train_iter_loss: 0.22150327265262604
train_iter_loss: 0.1853397786617279
train_iter_loss: 0.15617527067661285
train_iter_loss: 0.15189418196678162
train_iter_loss: 0.1477002650499344
train_iter_loss: 0.04495623707771301
train_iter_loss: 0.1861904263496399
train_iter_loss: 0.15865643322467804
train_iter_loss: 0.18787512183189392
train_iter_loss: 0.17204445600509644
train_iter_loss: 0.20396873354911804
train_iter_loss: 0.12816640734672546
train_iter_loss: 0.1882871836423874
train_iter_loss: 0.12651076912879944
train_iter_loss: 0.15493246912956238
train_iter_loss: 0.27867642045021057
train_iter_loss: 0.16295304894447327
train_iter_loss: 0.1806676834821701
train_iter_loss: 0.07963065803050995
train_iter_loss: 0.12009304761886597
train_iter_loss: 0.20666266977787018
train_iter_loss: 0.1491086781024933
train_iter_loss: 0.150715634226799
train_iter_loss: 0.2506197690963745
train_iter_loss: 0.12309666723012924
train_iter_loss: 0.2390306442975998
train_iter_loss: 0.22050417959690094
train_iter_loss: 0.2537198066711426
train_iter_loss: 0.1473107486963272
train_iter_loss: 0.16247710585594177
train_iter_loss: 0.10855790972709656
train_iter_loss: 0.2172098308801651
train_iter_loss: 0.18098273873329163
train_iter_loss: 0.19937632977962494
train_iter_loss: 0.245004341006279
train_iter_loss: 0.11481916159391403
train_iter_loss: 0.17909595370292664
train_iter_loss: 0.20502589643001556
train_iter_loss: 0.14747227728366852
train_iter_loss: 0.11657468974590302
train_iter_loss: 0.5060644745826721
train_iter_loss: 0.15389786660671234
train_iter_loss: 0.14263030886650085
train_iter_loss: 0.2368641048669815
train_iter_loss: 0.1865955889225006
train_iter_loss: 0.17155799269676208
train_iter_loss: 0.18879620730876923
train_iter_loss: 0.12475552409887314
train_iter_loss: 0.18451139330863953
train_iter_loss: 0.16783221065998077
train_iter_loss: 0.28931018710136414
train_iter_loss: 0.20583011209964752
train_iter_loss: 0.19688265025615692
train_iter_loss: 0.19046659767627716
train_iter_loss: 0.1416504681110382
train_iter_loss: 0.09478498250246048
train_iter_loss: 0.27163004875183105
train_iter_loss: 0.15284492075443268
train_iter_loss: 0.11304415017366409
train_iter_loss: 0.26530537009239197
train_iter_loss: 0.1742064505815506
train_iter_loss: 0.15299110114574432
train loss :0.1757
---------------------
Validation seg loss: 0.22831631075801714 at epoch 131
epoch =    132/  1000, exp = train
train_iter_loss: 0.34383392333984375
train_iter_loss: 0.31735387444496155
train_iter_loss: 0.14378507435321808
train_iter_loss: 0.2153214067220688
train_iter_loss: 0.0724981278181076
train_iter_loss: 0.10740122199058533
train_iter_loss: 0.18843476474285126
train_iter_loss: 0.11506888270378113
train_iter_loss: 0.13632690906524658
train_iter_loss: 0.24380236864089966
train_iter_loss: 0.17649522423744202
train_iter_loss: 0.17624375224113464
train_iter_loss: 0.1195477694272995
train_iter_loss: 0.11273980140686035
train_iter_loss: 0.13813431560993195
train_iter_loss: 0.18501576781272888
train_iter_loss: 0.19066530466079712
train_iter_loss: 0.24349604547023773
train_iter_loss: 0.04241150990128517
train_iter_loss: 0.22234973311424255
train_iter_loss: 0.17898178100585938
train_iter_loss: 0.15794029831886292
train_iter_loss: 0.11774972826242447
train_iter_loss: 0.1859620362520218
train_iter_loss: 0.3235105276107788
train_iter_loss: 0.1339423954486847
train_iter_loss: 0.12151297181844711
train_iter_loss: 0.17280347645282745
train_iter_loss: 0.25948482751846313
train_iter_loss: 0.21281500160694122
train_iter_loss: 0.21920868754386902
train_iter_loss: 0.0955284982919693
train_iter_loss: 0.2752939462661743
train_iter_loss: 0.3530251383781433
train_iter_loss: 0.06775055825710297
train_iter_loss: 0.15225067734718323
train_iter_loss: 0.16987109184265137
train_iter_loss: 0.180251806974411
train_iter_loss: 0.07248584926128387
train_iter_loss: 0.13239429891109467
train_iter_loss: 0.16196058690547943
train_iter_loss: 0.25765109062194824
train_iter_loss: 0.13912783563137054
train_iter_loss: 0.349203884601593
train_iter_loss: 0.1452154666185379
train_iter_loss: 0.19136996567249298
train_iter_loss: 0.16808854043483734
train_iter_loss: 0.18631455302238464
train_iter_loss: 0.13866302371025085
train_iter_loss: 0.11581909656524658
train_iter_loss: 0.23322641849517822
train_iter_loss: 0.14684541523456573
train_iter_loss: 0.12474342435598373
train_iter_loss: 0.11021184921264648
train_iter_loss: 0.2327459454536438
train_iter_loss: 0.1853250116109848
train_iter_loss: 0.15383371710777283
train_iter_loss: 0.18453003466129303
train_iter_loss: 0.154887855052948
train_iter_loss: 0.15192586183547974
train_iter_loss: 0.17977268993854523
train_iter_loss: 0.25685176253318787
train_iter_loss: 0.22228898108005524
train_iter_loss: 0.12361492216587067
train_iter_loss: 0.14107373356819153
train_iter_loss: 0.2074432671070099
train_iter_loss: 0.2003183811903
train_iter_loss: 0.12519700825214386
train_iter_loss: 0.17744988203048706
train_iter_loss: 0.20887057483196259
train_iter_loss: 0.2557554841041565
train_iter_loss: 0.21604198217391968
train_iter_loss: 0.157764732837677
train_iter_loss: 0.1564914584159851
train_iter_loss: 0.18180838227272034
train_iter_loss: 0.14583440124988556
train_iter_loss: 0.18415994942188263
train_iter_loss: 0.15496957302093506
train_iter_loss: 0.25200119614601135
train_iter_loss: 0.1907210797071457
train_iter_loss: 0.18073022365570068
train_iter_loss: 0.13625602424144745
train_iter_loss: 0.13186891376972198
train_iter_loss: 0.11881017684936523
train_iter_loss: 0.12846136093139648
train_iter_loss: 0.15313170850276947
train_iter_loss: 0.2008827030658722
train_iter_loss: 0.18039081990718842
train_iter_loss: 0.2058153599500656
train_iter_loss: 0.10885585099458694
train_iter_loss: 0.2676212191581726
train_iter_loss: 0.21893729269504547
train_iter_loss: 0.15565072000026703
train_iter_loss: 0.16649523377418518
train_iter_loss: 0.1959783434867859
train_iter_loss: 0.12853577733039856
train_iter_loss: 0.11371064186096191
train_iter_loss: 0.18493624031543732
train_iter_loss: 0.16926798224449158
train_iter_loss: 0.10706594586372375
train loss :0.1777
---------------------
Validation seg loss: 0.22741920747003466 at epoch 132
epoch =    133/  1000, exp = train
train_iter_loss: 0.079871267080307
train_iter_loss: 0.24943135678768158
train_iter_loss: 0.23261961340904236
train_iter_loss: 0.144101083278656
train_iter_loss: 0.14500343799591064
train_iter_loss: 0.10258381813764572
train_iter_loss: 0.2176782339811325
train_iter_loss: 0.22926968336105347
train_iter_loss: 0.1962243914604187
train_iter_loss: 0.21498341858386993
train_iter_loss: 0.296221524477005
train_iter_loss: 0.22954821586608887
train_iter_loss: 0.13208222389221191
train_iter_loss: 0.1328294724225998
train_iter_loss: 0.110118068754673
train_iter_loss: 0.1908799707889557
train_iter_loss: 0.19477158784866333
train_iter_loss: 0.2108582854270935
train_iter_loss: 0.26134568452835083
train_iter_loss: 0.1677003651857376
train_iter_loss: 0.19428865611553192
train_iter_loss: 0.25164228677749634
train_iter_loss: 0.131440207362175
train_iter_loss: 0.14115843176841736
train_iter_loss: 0.27127665281295776
train_iter_loss: 0.1531619429588318
train_iter_loss: 0.28056132793426514
train_iter_loss: 0.113334521651268
train_iter_loss: 0.28602510690689087
train_iter_loss: 0.11279121786355972
train_iter_loss: 0.16057156026363373
train_iter_loss: 0.2237461805343628
train_iter_loss: 0.22835859656333923
train_iter_loss: 0.1676359325647354
train_iter_loss: 0.12557749450206757
train_iter_loss: 0.14439228177070618
train_iter_loss: 0.1702042818069458
train_iter_loss: 0.1540532261133194
train_iter_loss: 0.09693137556314468
train_iter_loss: 0.13348212838172913
train_iter_loss: 0.1936105340719223
train_iter_loss: 0.10952845960855484
train_iter_loss: 0.1773269921541214
train_iter_loss: 0.18653088808059692
train_iter_loss: 0.21094653010368347
train_iter_loss: 0.17507649958133698
train_iter_loss: 0.2958352565765381
train_iter_loss: 0.10961128771305084
train_iter_loss: 0.2509767413139343
train_iter_loss: 0.2128978818655014
train_iter_loss: 0.13701868057250977
train_iter_loss: 0.12264502048492432
train_iter_loss: 0.13567164540290833
train_iter_loss: 0.11004652827978134
train_iter_loss: 0.17418256402015686
train_iter_loss: 0.0785195529460907
train_iter_loss: 0.11945001035928726
train_iter_loss: 0.23182813823223114
train_iter_loss: 0.12176229059696198
train_iter_loss: 0.19724206626415253
train_iter_loss: 0.11603983491659164
train_iter_loss: 0.14492742717266083
train_iter_loss: 0.1883922517299652
train_iter_loss: 0.16575290262699127
train_iter_loss: 0.1741674691438675
train_iter_loss: 0.3237750828266144
train_iter_loss: 0.21796591579914093
train_iter_loss: 0.20826928317546844
train_iter_loss: 0.09276212751865387
train_iter_loss: 0.12734590470790863
train_iter_loss: 0.11703381687402725
train_iter_loss: 0.34209954738616943
train_iter_loss: 0.1064455583691597
train_iter_loss: 0.19799301028251648
train_iter_loss: 0.14082159101963043
train_iter_loss: 0.22944200038909912
train_iter_loss: 0.11182976514101028
train_iter_loss: 0.17048944532871246
train_iter_loss: 0.23519407212734222
train_iter_loss: 0.156378835439682
train_iter_loss: 0.1807277351617813
train_iter_loss: 0.0977327898144722
train_iter_loss: 0.22219963371753693
train_iter_loss: 0.14064006507396698
train_iter_loss: 0.2974756062030792
train_iter_loss: 0.1036326065659523
train_iter_loss: 0.15516270697116852
train_iter_loss: 0.11542747169733047
train_iter_loss: 0.15059448778629303
train_iter_loss: 0.1893111765384674
train_iter_loss: 0.10736727714538574
train_iter_loss: 0.18141084909439087
train_iter_loss: 0.1862880438566208
train_iter_loss: 0.14307066798210144
train_iter_loss: 0.1580522060394287
train_iter_loss: 0.13209329545497894
train_iter_loss: 0.19468234479427338
train_iter_loss: 0.20481516420841217
train_iter_loss: 0.24961255490779877
train_iter_loss: 0.18125087022781372
train loss :0.1777
---------------------
Validation seg loss: 0.22448712847424004 at epoch 133
epoch =    134/  1000, exp = train
train_iter_loss: 0.2101764976978302
train_iter_loss: 0.1501094102859497
train_iter_loss: 0.10952845960855484
train_iter_loss: 0.13576006889343262
train_iter_loss: 0.2062777876853943
train_iter_loss: 0.1173577755689621
train_iter_loss: 0.16392533481121063
train_iter_loss: 0.20365378260612488
train_iter_loss: 0.2610835134983063
train_iter_loss: 0.07025401294231415
train_iter_loss: 0.05015987530350685
train_iter_loss: 0.22762981057167053
train_iter_loss: 0.11379749327898026
train_iter_loss: 0.1289941370487213
train_iter_loss: 0.1868935525417328
train_iter_loss: 0.20150379836559296
train_iter_loss: 0.23694251477718353
train_iter_loss: 0.18501496315002441
train_iter_loss: 0.16842347383499146
train_iter_loss: 0.23094923794269562
train_iter_loss: 0.2257622331380844
train_iter_loss: 0.4174025356769562
train_iter_loss: 0.16303066909313202
train_iter_loss: 0.13678056001663208
train_iter_loss: 0.1453075259923935
train_iter_loss: 0.22412227094173431
train_iter_loss: 0.4939427375793457
train_iter_loss: 0.17161545157432556
train_iter_loss: 0.21184414625167847
train_iter_loss: 0.1781347692012787
train_iter_loss: 0.14637696743011475
train_iter_loss: 0.1182447001338005
train_iter_loss: 0.20674604177474976
train_iter_loss: 0.16340704262256622
train_iter_loss: 0.10604514926671982
train_iter_loss: 0.23434355854988098
train_iter_loss: 0.2664868235588074
train_iter_loss: 0.23155762255191803
train_iter_loss: 0.243904709815979
train_iter_loss: 0.1293969303369522
train_iter_loss: 0.07444576919078827
train_iter_loss: 0.24584628641605377
train_iter_loss: 0.15758489072322845
train_iter_loss: 0.11689122021198273
train_iter_loss: 0.19857895374298096
train_iter_loss: 0.16251835227012634
train_iter_loss: 0.23262684047222137
train_iter_loss: 0.22686713933944702
train_iter_loss: 0.17924287915229797
train_iter_loss: 0.1406664103269577
train_iter_loss: 0.20768947899341583
train_iter_loss: 0.09153715521097183
train_iter_loss: 0.12258275598287582
train_iter_loss: 0.10955800861120224
train_iter_loss: 0.11836984008550644
train_iter_loss: 0.2508193850517273
train_iter_loss: 0.21087202429771423
train_iter_loss: 0.16622614860534668
train_iter_loss: 0.13574795424938202
train_iter_loss: 0.136285662651062
train_iter_loss: 0.15360534191131592
train_iter_loss: 0.102014921605587
train_iter_loss: 0.11097585409879684
train_iter_loss: 0.14439421892166138
train_iter_loss: 0.1771126240491867
train_iter_loss: 0.11547614634037018
train_iter_loss: 0.1138857752084732
train_iter_loss: 0.21288521587848663
train_iter_loss: 0.1438029557466507
train_iter_loss: 0.20204125344753265
train_iter_loss: 0.23805704712867737
train_iter_loss: 0.20534305274486542
train_iter_loss: 0.16156511008739471
train_iter_loss: 0.10348028689622879
train_iter_loss: 0.15657129883766174
train_iter_loss: 0.14306600391864777
train_iter_loss: 0.2413182407617569
train_iter_loss: 0.07716097682714462
train_iter_loss: 0.24754197895526886
train_iter_loss: 0.1655350923538208
train_iter_loss: 0.19450920820236206
train_iter_loss: 0.17032521963119507
train_iter_loss: 0.2760114073753357
train_iter_loss: 0.19672919809818268
train_iter_loss: 0.24814340472221375
train_iter_loss: 0.2854165732860565
train_iter_loss: 0.12156452983617783
train_iter_loss: 0.14874154329299927
train_iter_loss: 0.17799468338489532
train_iter_loss: 0.12321596592664719
train_iter_loss: 0.18299998342990875
train_iter_loss: 0.23531673848628998
train_iter_loss: 0.13314934074878693
train_iter_loss: 0.2613433003425598
train_iter_loss: 0.10951994359493256
train_iter_loss: 0.1303965151309967
train_iter_loss: 0.058248236775398254
train_iter_loss: 0.23281604051589966
train_iter_loss: 0.18025796115398407
train_iter_loss: 0.2505900263786316
train loss :0.1797
---------------------
Validation seg loss: 0.2250696393784206 at epoch 134
epoch =    135/  1000, exp = train
train_iter_loss: 0.1982385367155075
train_iter_loss: 0.12270763516426086
train_iter_loss: 0.225569948554039
train_iter_loss: 0.1415843963623047
train_iter_loss: 0.16410821676254272
train_iter_loss: 0.10511337220668793
train_iter_loss: 0.13654173910617828
train_iter_loss: 0.23183020949363708
train_iter_loss: 0.13472647964954376
train_iter_loss: 0.22058112919330597
train_iter_loss: 0.14309588074684143
train_iter_loss: 0.19400478899478912
train_iter_loss: 0.2979649603366852
train_iter_loss: 0.11124216765165329
train_iter_loss: 0.10055667906999588
train_iter_loss: 0.20416651666164398
train_iter_loss: 0.07510741800069809
train_iter_loss: 0.18882779777050018
train_iter_loss: 0.19725868105888367
train_iter_loss: 0.13220886886119843
train_iter_loss: 0.09326691925525665
train_iter_loss: 0.23329351842403412
train_iter_loss: 0.23799726366996765
train_iter_loss: 0.19981718063354492
train_iter_loss: 0.1283571571111679
train_iter_loss: 0.3691135048866272
train_iter_loss: 0.1900513917207718
train_iter_loss: 0.19733799993991852
train_iter_loss: 0.2121543288230896
train_iter_loss: 0.19401481747627258
train_iter_loss: 0.0582074411213398
train_iter_loss: 0.2435682862997055
train_iter_loss: 0.18243828415870667
train_iter_loss: 0.08017507940530777
train_iter_loss: 0.17223267257213593
train_iter_loss: 0.10792309790849686
train_iter_loss: 0.22509576380252838
train_iter_loss: 0.22308464348316193
train_iter_loss: 0.17775015532970428
train_iter_loss: 0.10925013571977615
train_iter_loss: 0.24814389646053314
train_iter_loss: 0.1916862577199936
train_iter_loss: 0.16625334322452545
train_iter_loss: 0.09196507930755615
train_iter_loss: 0.04668669402599335
train_iter_loss: 0.09133332967758179
train_iter_loss: 0.12828947603702545
train_iter_loss: 0.269878625869751
train_iter_loss: 0.30250468850135803
train_iter_loss: 0.05249161645770073
train_iter_loss: 0.16758178174495697
train_iter_loss: 0.194678395986557
train_iter_loss: 0.07928411662578583
train_iter_loss: 0.28811055421829224
train_iter_loss: 0.14610159397125244
train_iter_loss: 0.10304029285907745
train_iter_loss: 0.18622244894504547
train_iter_loss: 0.1098189726471901
train_iter_loss: 0.16439230740070343
train_iter_loss: 0.154032900929451
train_iter_loss: 0.16857460141181946
train_iter_loss: 0.28171440958976746
train_iter_loss: 0.3239642381668091
train_iter_loss: 0.22542895376682281
train_iter_loss: 0.1549350917339325
train_iter_loss: 0.11678825318813324
train_iter_loss: 0.18134067952632904
train_iter_loss: 0.17865785956382751
train_iter_loss: 0.1213514655828476
train_iter_loss: 0.1581079363822937
train_iter_loss: 0.22508902847766876
train_iter_loss: 0.19560229778289795
train_iter_loss: 0.1406528204679489
train_iter_loss: 0.18615078926086426
train_iter_loss: 0.14989328384399414
train_iter_loss: 0.1981004923582077
train_iter_loss: 0.16463306546211243
train_iter_loss: 0.2932546138763428
train_iter_loss: 0.20441867411136627
train_iter_loss: 0.17542007565498352
train_iter_loss: 0.23682329058647156
train_iter_loss: 0.15131758153438568
train_iter_loss: 0.13762368261814117
train_iter_loss: 0.1599532514810562
train_iter_loss: 0.234823077917099
train_iter_loss: 0.16527974605560303
train_iter_loss: 0.3253366947174072
train_iter_loss: 0.16158060729503632
train_iter_loss: 0.12963806092739105
train_iter_loss: 0.22517606616020203
train_iter_loss: 0.16057676076889038
train_iter_loss: 0.11998265981674194
train_iter_loss: 0.19782617688179016
train_iter_loss: 0.19142165780067444
train_iter_loss: 0.2352718710899353
train_iter_loss: 0.13230502605438232
train_iter_loss: 0.14124958217144012
train_iter_loss: 0.08000215888023376
train_iter_loss: 0.1876024454832077
train_iter_loss: 0.27460744976997375
train loss :0.1782
---------------------
Validation seg loss: 0.2258262294081022 at epoch 135
epoch =    136/  1000, exp = train
train_iter_loss: 0.1198270246386528
train_iter_loss: 0.08428402990102768
train_iter_loss: 0.1949128359556198
train_iter_loss: 0.1652713567018509
train_iter_loss: 0.17596441507339478
train_iter_loss: 0.21575412154197693
train_iter_loss: 0.11516360193490982
train_iter_loss: 0.16842825710773468
train_iter_loss: 0.301669180393219
train_iter_loss: 0.10952870547771454
train_iter_loss: 0.10664679110050201
train_iter_loss: 0.1372300535440445
train_iter_loss: 0.11461416631937027
train_iter_loss: 0.11191930621862411
train_iter_loss: 0.3415200710296631
train_iter_loss: 0.21408507227897644
train_iter_loss: 0.1555827111005783
train_iter_loss: 0.13467374444007874
train_iter_loss: 0.29673150181770325
train_iter_loss: 0.18258006870746613
train_iter_loss: 0.09772986173629761
train_iter_loss: 0.19035905599594116
train_iter_loss: 0.2017478346824646
train_iter_loss: 0.09808910638093948
train_iter_loss: 0.18508461117744446
train_iter_loss: 0.17435069382190704
train_iter_loss: 0.11704561859369278
train_iter_loss: 0.11152254790067673
train_iter_loss: 0.1807308793067932
train_iter_loss: 0.13594453036785126
train_iter_loss: 0.18008840084075928
train_iter_loss: 0.15456682443618774
train_iter_loss: 0.22492346167564392
train_iter_loss: 0.1257767677307129
train_iter_loss: 0.17185243964195251
train_iter_loss: 0.15380561351776123
train_iter_loss: 0.17436635494232178
train_iter_loss: 0.254299134016037
train_iter_loss: 0.07450692355632782
train_iter_loss: 0.12224654853343964
train_iter_loss: 0.14841730892658234
train_iter_loss: 0.2517344653606415
train_iter_loss: 0.22401082515716553
train_iter_loss: 0.2945373058319092
train_iter_loss: 0.21219080686569214
train_iter_loss: 0.20469143986701965
train_iter_loss: 0.20144881308078766
train_iter_loss: 0.2196110635995865
train_iter_loss: 0.18239696323871613
train_iter_loss: 0.16066870093345642
train_iter_loss: 0.22159992158412933
train_iter_loss: 0.16838137805461884
train_iter_loss: 0.3718103766441345
train_iter_loss: 0.19208042323589325
train_iter_loss: 0.07881483435630798
train_iter_loss: 0.18893027305603027
train_iter_loss: 0.1642531454563141
train_iter_loss: 0.13214650750160217
train_iter_loss: 0.34706807136535645
train_iter_loss: 0.21277515590190887
train_iter_loss: 0.15580683946609497
train_iter_loss: 0.2754819691181183
train_iter_loss: 0.1050201877951622
train_iter_loss: 0.27823370695114136
train_iter_loss: 0.19441625475883484
train_iter_loss: 0.12085647881031036
train_iter_loss: 0.07760661840438843
train_iter_loss: 0.1803033947944641
train_iter_loss: 0.25262415409088135
train_iter_loss: 0.22052313387393951
train_iter_loss: 0.18798260390758514
train_iter_loss: 0.09459494054317474
train_iter_loss: 0.16346806287765503
train_iter_loss: 0.22785227000713348
train_iter_loss: 0.31337448954582214
train_iter_loss: 0.19410328567028046
train_iter_loss: 0.25860780477523804
train_iter_loss: 0.12046657502651215
train_iter_loss: 0.265251487493515
train_iter_loss: 0.16927865147590637
train_iter_loss: 0.09984555095434189
train_iter_loss: 0.16498889029026031
train_iter_loss: 0.13175758719444275
train_iter_loss: 0.12438571453094482
train_iter_loss: 0.14981038868427277
train_iter_loss: 0.17150765657424927
train_iter_loss: 0.3274349272251129
train_iter_loss: 0.16030791401863098
train_iter_loss: 0.23099353909492493
train_iter_loss: 0.1927482932806015
train_iter_loss: 0.22317130863666534
train_iter_loss: 0.08581540733575821
train_iter_loss: 0.15593327581882477
train_iter_loss: 0.14243797957897186
train_iter_loss: 0.23542465269565582
train_iter_loss: 0.11701645702123642
train_iter_loss: 0.1367873102426529
train_iter_loss: 0.1760522574186325
train_iter_loss: 0.14094644784927368
train_iter_loss: 0.10578324645757675
train loss :0.1806
---------------------
Validation seg loss: 0.22565304746253873 at epoch 136
epoch =    137/  1000, exp = train
train_iter_loss: 0.1784440577030182
train_iter_loss: 0.12537944316864014
train_iter_loss: 0.17651976644992828
train_iter_loss: 0.2192911058664322
train_iter_loss: 0.1884550005197525
train_iter_loss: 0.1691346913576126
train_iter_loss: 0.1587519347667694
train_iter_loss: 0.14166806638240814
train_iter_loss: 0.11081694066524506
train_iter_loss: 0.1714339256286621
train_iter_loss: 0.17929399013519287
train_iter_loss: 0.16483280062675476
train_iter_loss: 0.1433127522468567
train_iter_loss: 0.09384050220251083
train_iter_loss: 0.20501263439655304
train_iter_loss: 0.23058314621448517
train_iter_loss: 0.1668974757194519
train_iter_loss: 0.22867706418037415
train_iter_loss: 0.17367996275424957
train_iter_loss: 0.14189253747463226
train_iter_loss: 0.18419568240642548
train_iter_loss: 0.11001820117235184
train_iter_loss: 0.21652153134346008
train_iter_loss: 0.19737042486667633
train_iter_loss: 0.28665250539779663
train_iter_loss: 0.19249014556407928
train_iter_loss: 0.1862722784280777
train_iter_loss: 0.18895839154720306
train_iter_loss: 0.11933600902557373
train_iter_loss: 0.12239488959312439
train_iter_loss: 0.1943839192390442
train_iter_loss: 0.15841545164585114
train_iter_loss: 0.3068530559539795
train_iter_loss: 0.23887386918067932
train_iter_loss: 0.28374049067497253
train_iter_loss: 0.08748810738325119
train_iter_loss: 0.1306302547454834
train_iter_loss: 0.05801134184002876
train_iter_loss: 0.116173654794693
train_iter_loss: 0.18069694936275482
train_iter_loss: 0.1299574077129364
train_iter_loss: 0.22202952206134796
train_iter_loss: 0.10975316166877747
train_iter_loss: 0.14793775975704193
train_iter_loss: 0.11670586466789246
train_iter_loss: 0.11088613420724869
train_iter_loss: 0.0960053950548172
train_iter_loss: 0.1667177379131317
train_iter_loss: 0.1733884960412979
train_iter_loss: 0.27791526913642883
train_iter_loss: 0.29584792256355286
train_iter_loss: 0.0998254343867302
train_iter_loss: 0.202047660946846
train_iter_loss: 0.10949766635894775
train_iter_loss: 0.22765439748764038
train_iter_loss: 0.2309129387140274
train_iter_loss: 0.1014113798737526
train_iter_loss: 0.1500084102153778
train_iter_loss: 0.09430987387895584
train_iter_loss: 0.15401577949523926
train_iter_loss: 0.11937178671360016
train_iter_loss: 0.1787758469581604
train_iter_loss: 0.1473263055086136
train_iter_loss: 0.29588672518730164
train_iter_loss: 0.25398197770118713
train_iter_loss: 0.2058931291103363
train_iter_loss: 0.21127252280712128
train_iter_loss: 0.1334676295518875
train_iter_loss: 0.21517060697078705
train_iter_loss: 0.12524254620075226
train_iter_loss: 0.17448730766773224
train_iter_loss: 0.20741350948810577
train_iter_loss: 0.16637691855430603
train_iter_loss: 0.16571082174777985
train_iter_loss: 0.21470294892787933
train_iter_loss: 0.1784849315881729
train_iter_loss: 0.16130708158016205
train_iter_loss: 0.15368077158927917
train_iter_loss: 0.09930914640426636
train_iter_loss: 0.23296241462230682
train_iter_loss: 0.15065331757068634
train_iter_loss: 0.2834133803844452
train_iter_loss: 0.1606781780719757
train_iter_loss: 0.19797028601169586
train_iter_loss: 0.14398786425590515
train_iter_loss: 0.1371079385280609
train_iter_loss: 0.10305433720350266
train_iter_loss: 0.17151887714862823
train_iter_loss: 0.21285758912563324
train_iter_loss: 0.1257951408624649
train_iter_loss: 0.18031460046768188
train_iter_loss: 0.07176192849874496
train_iter_loss: 0.2805093228816986
train_iter_loss: 0.3045465052127838
train_iter_loss: 0.17795270681381226
train_iter_loss: 0.1805405169725418
train_iter_loss: 0.1424209177494049
train_iter_loss: 0.19813549518585205
train_iter_loss: 0.17073582112789154
train_iter_loss: 0.17247894406318665
train loss :0.1753
---------------------
Validation seg loss: 0.2244712423954932 at epoch 137
epoch =    138/  1000, exp = train
train_iter_loss: 0.24184294044971466
train_iter_loss: 0.1596580445766449
train_iter_loss: 0.29148492217063904
train_iter_loss: 0.19841070473194122
train_iter_loss: 0.24810314178466797
train_iter_loss: 0.1632792353630066
train_iter_loss: 0.09587161988019943
train_iter_loss: 0.17528611421585083
train_iter_loss: 0.24596531689167023
train_iter_loss: 0.13106250762939453
train_iter_loss: 0.0940064936876297
train_iter_loss: 0.10782553255558014
train_iter_loss: 0.16699282824993134
train_iter_loss: 0.1466744989156723
train_iter_loss: 0.16995231807231903
train_iter_loss: 0.12334291636943817
train_iter_loss: 0.1021723523736
train_iter_loss: 0.07081288844347
train_iter_loss: 0.06837873160839081
train_iter_loss: 0.2810225188732147
train_iter_loss: 0.1869666874408722
train_iter_loss: 0.1988266557455063
train_iter_loss: 0.10808422416448593
train_iter_loss: 0.17668159306049347
train_iter_loss: 0.1486845463514328
train_iter_loss: 0.22833706438541412
train_iter_loss: 0.19823937118053436
train_iter_loss: 0.2974153459072113
train_iter_loss: 0.17923785746097565
train_iter_loss: 0.1622912883758545
train_iter_loss: 0.20290376245975494
train_iter_loss: 0.22409935295581818
train_iter_loss: 0.07200043648481369
train_iter_loss: 0.0968809723854065
train_iter_loss: 0.11207583546638489
train_iter_loss: 0.1847352832555771
train_iter_loss: 0.16098153591156006
train_iter_loss: 0.1959550678730011
train_iter_loss: 0.2711091637611389
train_iter_loss: 0.3514573276042938
train_iter_loss: 0.14878137409687042
train_iter_loss: 0.14786072075366974
train_iter_loss: 0.18970634043216705
train_iter_loss: 0.2669069468975067
train_iter_loss: 0.07248076796531677
train_iter_loss: 0.1704791784286499
train_iter_loss: 0.1791967749595642
train_iter_loss: 0.1213563084602356
train_iter_loss: 0.09364984184503555
train_iter_loss: 0.25258126854896545
train_iter_loss: 0.21100440621376038
train_iter_loss: 0.1441597193479538
train_iter_loss: 0.13713501393795013
train_iter_loss: 0.19512708485126495
train_iter_loss: 0.2506606876850128
train_iter_loss: 0.19749446213245392
train_iter_loss: 0.2908364236354828
train_iter_loss: 0.23201420903205872
train_iter_loss: 0.13330857455730438
train_iter_loss: 0.23534362018108368
train_iter_loss: 0.31323328614234924
train_iter_loss: 0.20203183591365814
train_iter_loss: 0.21651577949523926
train_iter_loss: 0.13771173357963562
train_iter_loss: 0.11998913437128067
train_iter_loss: 0.15793107450008392
train_iter_loss: 0.062004562467336655
train_iter_loss: 0.24970881640911102
train_iter_loss: 0.24520662426948547
train_iter_loss: 0.12132494896650314
train_iter_loss: 0.11874996870756149
train_iter_loss: 0.15579837560653687
train_iter_loss: 0.19751910865306854
train_iter_loss: 0.14324580132961273
train_iter_loss: 0.28198865056037903
train_iter_loss: 0.049215368926525116
train_iter_loss: 0.1663823127746582
train_iter_loss: 0.12435010820627213
train_iter_loss: 0.22487980127334595
train_iter_loss: 0.11280947923660278
train_iter_loss: 0.1641668677330017
train_iter_loss: 0.17400021851062775
train_iter_loss: 0.24152430891990662
train_iter_loss: 0.17560283839702606
train_iter_loss: 0.1737423539161682
train_iter_loss: 0.19492150843143463
train_iter_loss: 0.08644270896911621
train_iter_loss: 0.18719123303890228
train_iter_loss: 0.24573229253292084
train_iter_loss: 0.23531781136989594
train_iter_loss: 0.16777187585830688
train_iter_loss: 0.22249455749988556
train_iter_loss: 0.23315118253231049
train_iter_loss: 0.1201457679271698
train_iter_loss: 0.25080764293670654
train_iter_loss: 0.16653990745544434
train_iter_loss: 0.14992259442806244
train_iter_loss: 0.17382031679153442
train_iter_loss: 0.10675554722547531
train_iter_loss: 0.1786060333251953
train loss :0.1793
---------------------
Validation seg loss: 0.22700333760453845 at epoch 138
epoch =    139/  1000, exp = train
train_iter_loss: 0.1680254340171814
train_iter_loss: 0.19338171184062958
train_iter_loss: 0.20068597793579102
train_iter_loss: 0.14570514857769012
train_iter_loss: 0.14423277974128723
train_iter_loss: 0.15651263296604156
train_iter_loss: 0.07372163236141205
train_iter_loss: 0.09967881441116333
train_iter_loss: 0.32047605514526367
train_iter_loss: 0.24266134202480316
train_iter_loss: 0.14576593041419983
train_iter_loss: 0.20329129695892334
train_iter_loss: 0.21561861038208008
train_iter_loss: 0.11520244926214218
train_iter_loss: 0.17435725033283234
train_iter_loss: 0.04290778189897537
train_iter_loss: 0.15400733053684235
train_iter_loss: 0.2686136066913605
train_iter_loss: 0.27562838792800903
train_iter_loss: 0.1405985951423645
train_iter_loss: 0.11929452419281006
train_iter_loss: 0.26672229170799255
train_iter_loss: 0.08613750338554382
train_iter_loss: 0.19916541874408722
train_iter_loss: 0.170741006731987
train_iter_loss: 0.2299550622701645
train_iter_loss: 0.1605064570903778
train_iter_loss: 0.19349107146263123
train_iter_loss: 0.16698674857616425
train_iter_loss: 0.18236254155635834
train_iter_loss: 0.1284535676240921
train_iter_loss: 0.12364315241575241
train_iter_loss: 0.20208972692489624
train_iter_loss: 0.23098880052566528
train_iter_loss: 0.11262676119804382
train_iter_loss: 0.2570890188217163
train_iter_loss: 0.11919239908456802
train_iter_loss: 0.11659389734268188
train_iter_loss: 0.2001638263463974
train_iter_loss: 0.2640199363231659
train_iter_loss: 0.13436588644981384
train_iter_loss: 0.19412563741207123
train_iter_loss: 0.16095538437366486
train_iter_loss: 0.22743727266788483
train_iter_loss: 0.15138618648052216
train_iter_loss: 0.23211660981178284
train_iter_loss: 0.19776704907417297
train_iter_loss: 0.17132654786109924
train_iter_loss: 0.21986475586891174
train_iter_loss: 0.08663284778594971
train_iter_loss: 0.1483139842748642
train_iter_loss: 0.05201198533177376
train_iter_loss: 0.11397559940814972
train_iter_loss: 0.22362494468688965
train_iter_loss: 0.18698541820049286
train_iter_loss: 0.23527613282203674
train_iter_loss: 0.13347689807415009
train_iter_loss: 0.16948240995407104
train_iter_loss: 0.15035448968410492
train_iter_loss: 0.1470961570739746
train_iter_loss: 0.24417269229888916
train_iter_loss: 0.1838548630475998
train_iter_loss: 0.14023567736148834
train_iter_loss: 0.27537083625793457
train_iter_loss: 0.15448209643363953
train_iter_loss: 0.21332603693008423
train_iter_loss: 0.1319194883108139
train_iter_loss: 0.18204626441001892
train_iter_loss: 0.3081379234790802
train_iter_loss: 0.2568518817424774
train_iter_loss: 0.12947720289230347
train_iter_loss: 0.18721185624599457
train_iter_loss: 0.118132583796978
train_iter_loss: 0.12655770778656006
train_iter_loss: 0.2594176232814789
train_iter_loss: 0.3902358412742615
train_iter_loss: 0.15523657202720642
train_iter_loss: 0.20925767719745636
train_iter_loss: 0.08382389694452286
train_iter_loss: 0.23672282695770264
train_iter_loss: 0.29327574372291565
train_iter_loss: 0.11452855914831161
train_iter_loss: 0.17879335582256317
train_iter_loss: 0.2404451221227646
train_iter_loss: 0.13431844115257263
train_iter_loss: 0.18964353203773499
train_iter_loss: 0.19678042829036713
train_iter_loss: 0.16294558346271515
train_iter_loss: 0.13352279365062714
train_iter_loss: 0.15482236444950104
train_iter_loss: 0.044606201350688934
train_iter_loss: 0.13722240924835205
train_iter_loss: 0.035510171204805374
train_iter_loss: 0.2306174337863922
train_iter_loss: 0.1118643507361412
train_iter_loss: 0.2009182870388031
train_iter_loss: 0.14863279461860657
train_iter_loss: 0.2743268311023712
train_iter_loss: 0.11331939697265625
train_iter_loss: 0.24915936589241028
train loss :0.1786
---------------------
Validation seg loss: 0.23221343390221866 at epoch 139
epoch =    140/  1000, exp = train
train_iter_loss: 0.19874370098114014
train_iter_loss: 0.15498992800712585
train_iter_loss: 0.3029683530330658
train_iter_loss: 0.16956141591072083
train_iter_loss: 0.1945728212594986
train_iter_loss: 0.15078091621398926
train_iter_loss: 0.2545603811740875
train_iter_loss: 0.12092215567827225
train_iter_loss: 0.23355503380298615
train_iter_loss: 0.13431765139102936
train_iter_loss: 0.27001717686653137
train_iter_loss: 0.1299506276845932
train_iter_loss: 0.1721104234457016
train_iter_loss: 0.13396281003952026
train_iter_loss: 0.20220643281936646
train_iter_loss: 0.17152976989746094
train_iter_loss: 0.0764794573187828
train_iter_loss: 0.17197154462337494
train_iter_loss: 0.2115953266620636
train_iter_loss: 0.2640160918235779
train_iter_loss: 0.19482791423797607
train_iter_loss: 0.12643024325370789
train_iter_loss: 0.11966190487146378
train_iter_loss: 0.32487428188323975
train_iter_loss: 0.13894322514533997
train_iter_loss: 0.09085138887166977
train_iter_loss: 0.1574106365442276
train_iter_loss: 0.07390312850475311
train_iter_loss: 0.10407058149576187
train_iter_loss: 0.18048036098480225
train_iter_loss: 0.07235638052225113
train_iter_loss: 0.16871701180934906
train_iter_loss: 0.13165324926376343
train_iter_loss: 0.16582846641540527
train_iter_loss: 0.12832413613796234
train_iter_loss: 0.34973812103271484
train_iter_loss: 0.26429280638694763
train_iter_loss: 0.1168748140335083
train_iter_loss: 0.15870149433612823
train_iter_loss: 0.2114134579896927
train_iter_loss: 0.1740495264530182
train_iter_loss: 0.36937201023101807
train_iter_loss: 0.22574089467525482
train_iter_loss: 0.11283178627490997
train_iter_loss: 0.09862542897462845
train_iter_loss: 0.17398899793624878
train_iter_loss: 0.2029409408569336
train_iter_loss: 0.24126483500003815
train_iter_loss: 0.14458806812763214
train_iter_loss: 0.25783756375312805
train_iter_loss: 0.11232803761959076
train_iter_loss: 0.22044573724269867
train_iter_loss: 0.0996752381324768
train_iter_loss: 0.19213233888149261
train_iter_loss: 0.16082672774791718
train_iter_loss: 0.17023278772830963
train_iter_loss: 0.1290184110403061
train_iter_loss: 0.21227294206619263
train_iter_loss: 0.10356879979372025
train_iter_loss: 0.21838736534118652
train_iter_loss: 0.23700961470603943
train_iter_loss: 0.11713095009326935
train_iter_loss: 0.22149451076984406
train_iter_loss: 0.1217632070183754
train_iter_loss: 0.13471077382564545
train_iter_loss: 0.07714007794857025
train_iter_loss: 0.26789388060569763
train_iter_loss: 0.19408011436462402
train_iter_loss: 0.29706960916519165
train_iter_loss: 0.16865824162960052
train_iter_loss: 0.18209277093410492
train_iter_loss: 0.19114522635936737
train_iter_loss: 0.14433373510837555
train_iter_loss: 0.17087587714195251
train_iter_loss: 0.16226084530353546
train_iter_loss: 0.19086897373199463
train_iter_loss: 0.09839104861021042
train_iter_loss: 0.25821247696876526
train_iter_loss: 0.16989897191524506
train_iter_loss: 0.25947079062461853
train_iter_loss: 0.16854417324066162
train_iter_loss: 0.12489542365074158
train_iter_loss: 0.2572684586048126
train_iter_loss: 0.10340821743011475
train_iter_loss: 0.1804119497537613
train_iter_loss: 0.247749462723732
train_iter_loss: 0.16591209173202515
train_iter_loss: 0.22261035442352295
train_iter_loss: 0.10364635288715363
train_iter_loss: 0.21903963387012482
train_iter_loss: 0.19887137413024902
train_iter_loss: 0.22620393335819244
train_iter_loss: 0.1870942860841751
train_iter_loss: 0.20346906781196594
train_iter_loss: 0.21712352335453033
train_iter_loss: 0.14347060024738312
train_iter_loss: 0.17671902477741241
train_iter_loss: 0.1269393414258957
train_iter_loss: 0.05945727601647377
train_iter_loss: 0.3354419767856598
train loss :0.1813
---------------------
Validation seg loss: 0.22298231726196013 at epoch 140
********************
best_val_epoch_loss:  0.22298231726196013
MODEL UPDATED
epoch =    141/  1000, exp = train
train_iter_loss: 0.1611565798521042
train_iter_loss: 0.2011660486459732
train_iter_loss: 0.03384799882769585
train_iter_loss: 0.29879269003868103
train_iter_loss: 0.10240308940410614
train_iter_loss: 0.15683218836784363
train_iter_loss: 0.11190199851989746
train_iter_loss: 0.10327093303203583
train_iter_loss: 0.21041947603225708
train_iter_loss: 0.09058213233947754
train_iter_loss: 0.17914268374443054
train_iter_loss: 0.15257395803928375
train_iter_loss: 0.14736245572566986
train_iter_loss: 0.11594773828983307
train_iter_loss: 0.14027786254882812
train_iter_loss: 0.22498683631420135
train_iter_loss: 0.25412943959236145
train_iter_loss: 0.19567877054214478
train_iter_loss: 0.2997162342071533
train_iter_loss: 0.25607436895370483
train_iter_loss: 0.0784241333603859
train_iter_loss: 0.10357445478439331
train_iter_loss: 0.21137966215610504
train_iter_loss: 0.17820638418197632
train_iter_loss: 0.15251898765563965
train_iter_loss: 0.25661104917526245
train_iter_loss: 0.18668289482593536
train_iter_loss: 0.2249555140733719
train_iter_loss: 0.2612862288951874
train_iter_loss: 0.1372801512479782
train_iter_loss: 0.18206360936164856
train_iter_loss: 0.14319442212581635
train_iter_loss: 0.14639632403850555
train_iter_loss: 0.11983386427164078
train_iter_loss: 0.23361241817474365
train_iter_loss: 0.1654171347618103
train_iter_loss: 0.12417609989643097
train_iter_loss: 0.10430767387151718
train_iter_loss: 0.27892982959747314
train_iter_loss: 0.16371780633926392
train_iter_loss: 0.2055901437997818
train_iter_loss: 0.30837327241897583
train_iter_loss: 0.2630464434623718
train_iter_loss: 0.1339685171842575
train_iter_loss: 0.15380187332630157
train_iter_loss: 0.13666251301765442
train_iter_loss: 0.17691385746002197
train_iter_loss: 0.24326853454113007
train_iter_loss: 0.15425002574920654
train_iter_loss: 0.16853581368923187
train_iter_loss: 0.1483866423368454
train_iter_loss: 0.19949281215667725
train_iter_loss: 0.1309739053249359
train_iter_loss: 0.1348385065793991
train_iter_loss: 0.22209316492080688
train_iter_loss: 0.24952822923660278
train_iter_loss: 0.20660974085330963
train_iter_loss: 0.1730244755744934
train_iter_loss: 0.13307756185531616
train_iter_loss: 0.2532273828983307
train_iter_loss: 0.21189573407173157
train_iter_loss: 0.13277657330036163
train_iter_loss: 0.33847513794898987
train_iter_loss: 0.1664070338010788
train_iter_loss: 0.21004584431648254
train_iter_loss: 0.13133209943771362
train_iter_loss: 0.1934790462255478
train_iter_loss: 0.199478879570961
train_iter_loss: 0.1859080195426941
train_iter_loss: 0.19056813418865204
train_iter_loss: 0.18567931652069092
train_iter_loss: 0.08650712668895721
train_iter_loss: 0.22637653350830078
train_iter_loss: 0.06704208999872208
train_iter_loss: 0.0734434500336647
train_iter_loss: 0.18361209332942963
train_iter_loss: 0.09921689331531525
train_iter_loss: 0.17319880425930023
train_iter_loss: 0.17703405022621155
train_iter_loss: 0.20857501029968262
train_iter_loss: 0.043020155280828476
train_iter_loss: 0.15083053708076477
train_iter_loss: 0.21121717989444733
train_iter_loss: 0.13085417449474335
train_iter_loss: 0.257062166929245
train_iter_loss: 0.18945641815662384
train_iter_loss: 0.19802743196487427
train_iter_loss: 0.11569834500551224
train_iter_loss: 0.19420190155506134
train_iter_loss: 0.2447388768196106
train_iter_loss: 0.12010268121957779
train_iter_loss: 0.20058216154575348
train_iter_loss: 0.08894302695989609
train_iter_loss: 0.08728515356779099
train_iter_loss: 0.10760639607906342
train_iter_loss: 0.15408465266227722
train_iter_loss: 0.26058533787727356
train_iter_loss: 0.15087564289569855
train_iter_loss: 0.21337953209877014
train_iter_loss: 0.15055428445339203
train loss :0.1758
---------------------
Validation seg loss: 0.22350242057428607 at epoch 141
epoch =    142/  1000, exp = train
train_iter_loss: 0.2913829982280731
train_iter_loss: 0.2157299667596817
train_iter_loss: 0.12361413240432739
train_iter_loss: 0.14332269132137299
train_iter_loss: 0.17015515267848969
train_iter_loss: 0.15925151109695435
train_iter_loss: 0.14746959507465363
train_iter_loss: 0.10022500157356262
train_iter_loss: 0.22550466656684875
train_iter_loss: 0.1478395313024521
train_iter_loss: 0.183694526553154
train_iter_loss: 0.22549061477184296
train_iter_loss: 0.2059420347213745
train_iter_loss: 0.20648618042469025
train_iter_loss: 0.23605045676231384
train_iter_loss: 0.1539376825094223
train_iter_loss: 0.09309615939855576
train_iter_loss: 0.22893460094928741
train_iter_loss: 0.15552474558353424
train_iter_loss: 0.11267407238483429
train_iter_loss: 0.14310066401958466
train_iter_loss: 0.11251997947692871
train_iter_loss: 0.09675594419240952
train_iter_loss: 0.16582857072353363
train_iter_loss: 0.17719478905200958
train_iter_loss: 0.1997624635696411
train_iter_loss: 0.1923348605632782
train_iter_loss: 0.1756727248430252
train_iter_loss: 0.22186651825904846
train_iter_loss: 0.09041308611631393
train_iter_loss: 0.22647801041603088
train_iter_loss: 0.12091372162103653
train_iter_loss: 0.31950387358665466
train_iter_loss: 0.08572065830230713
train_iter_loss: 0.14342643320560455
train_iter_loss: 0.1784970462322235
train_iter_loss: 0.14676368236541748
train_iter_loss: 0.17286664247512817
train_iter_loss: 0.20462942123413086
train_iter_loss: 0.19403409957885742
train_iter_loss: 0.13728123903274536
train_iter_loss: 0.19578547775745392
train_iter_loss: 0.12122995406389236
train_iter_loss: 0.2709612250328064
train_iter_loss: 0.18887042999267578
train_iter_loss: 0.1341182142496109
train_iter_loss: 0.213545560836792
train_iter_loss: 0.15305718779563904
train_iter_loss: 0.15016815066337585
train_iter_loss: 0.12313631176948547
train_iter_loss: 0.18428368866443634
train_iter_loss: 0.02925035171210766
train_iter_loss: 0.28669944405555725
train_iter_loss: 0.16284790635108948
train_iter_loss: 0.18382103741168976
train_iter_loss: 0.13522499799728394
train_iter_loss: 0.12236833572387695
train_iter_loss: 0.20521704852581024
train_iter_loss: 0.20271164178848267
train_iter_loss: 0.26038873195648193
train_iter_loss: 0.12193852663040161
train_iter_loss: 0.19502587616443634
train_iter_loss: 0.1105637177824974
train_iter_loss: 0.178331196308136
train_iter_loss: 0.28729385137557983
train_iter_loss: 0.20555685460567474
train_iter_loss: 0.12892849743366241
train_iter_loss: 0.17352551221847534
train_iter_loss: 0.12543043494224548
train_iter_loss: 0.1927226185798645
train_iter_loss: 0.20132146775722504
train_iter_loss: 0.20705418288707733
train_iter_loss: 0.2751731276512146
train_iter_loss: 0.08699136227369308
train_iter_loss: 0.15713466703891754
train_iter_loss: 0.17074301838874817
train_iter_loss: 0.2659528851509094
train_iter_loss: 0.04641692712903023
train_iter_loss: 0.310565710067749
train_iter_loss: 0.21905770897865295
train_iter_loss: 0.1032457947731018
train_iter_loss: 0.0975712388753891
train_iter_loss: 0.3170255720615387
train_iter_loss: 0.24752435088157654
train_iter_loss: 0.16497930884361267
train_iter_loss: 0.11278654634952545
train_iter_loss: 0.18600627779960632
train_iter_loss: 0.12676073610782623
train_iter_loss: 0.20425812900066376
train_iter_loss: 0.14394763112068176
train_iter_loss: 0.1802506446838379
train_iter_loss: 0.15774722397327423
train_iter_loss: 0.12768086791038513
train_iter_loss: 0.11067208647727966
train_iter_loss: 0.17569150030612946
train_iter_loss: 0.15846914052963257
train_iter_loss: 0.2743241786956787
train_iter_loss: 0.19697462022304535
train_iter_loss: 0.20044179260730743
train_iter_loss: 0.13737113773822784
train loss :0.1761
---------------------
Validation seg loss: 0.2245947109746202 at epoch 142
epoch =    143/  1000, exp = train
train_iter_loss: 0.23691008985042572
train_iter_loss: 0.14807568490505219
train_iter_loss: 0.22622670233249664
train_iter_loss: 0.07363425940275192
train_iter_loss: 0.2716067433357239
train_iter_loss: 0.2842544913291931
train_iter_loss: 0.12197741121053696
train_iter_loss: 0.07651091367006302
train_iter_loss: 0.15543949604034424
train_iter_loss: 0.19069203734397888
train_iter_loss: 0.27008891105651855
train_iter_loss: 0.16136722266674042
train_iter_loss: 0.09800267964601517
train_iter_loss: 0.13765849173069
train_iter_loss: 0.1262611746788025
train_iter_loss: 0.24745647609233856
train_iter_loss: 0.1864427775144577
train_iter_loss: 0.12208275496959686
train_iter_loss: 0.136310413479805
train_iter_loss: 0.21407347917556763
train_iter_loss: 0.15328702330589294
train_iter_loss: 0.14712966978549957
train_iter_loss: 0.16603456437587738
train_iter_loss: 0.08138156682252884
train_iter_loss: 0.18399681150913239
train_iter_loss: 0.19285114109516144
train_iter_loss: 0.13265594840049744
train_iter_loss: 0.20782646536827087
train_iter_loss: 0.26070842146873474
train_iter_loss: 0.1251470148563385
train_iter_loss: 0.13846860826015472
train_iter_loss: 0.15799710154533386
train_iter_loss: 0.23827862739562988
train_iter_loss: 0.07697263360023499
train_iter_loss: 0.22815074026584625
train_iter_loss: 0.1365833729505539
train_iter_loss: 0.23913192749023438
train_iter_loss: 0.38619011640548706
train_iter_loss: 0.153670072555542
train_iter_loss: 0.20244614779949188
train_iter_loss: 0.2011646032333374
train_iter_loss: 0.08108581602573395
train_iter_loss: 0.17705047130584717
train_iter_loss: 0.15006990730762482
train_iter_loss: 0.1588016301393509
train_iter_loss: 0.3180304169654846
train_iter_loss: 0.1896379142999649
train_iter_loss: 0.1425291895866394
train_iter_loss: 0.13981327414512634
train_iter_loss: 0.18467269837856293
train_iter_loss: 0.20023857057094574
train_iter_loss: 0.11224686354398727
train_iter_loss: 0.1062140166759491
train_iter_loss: 0.1467975527048111
train_iter_loss: 0.16114583611488342
train_iter_loss: 0.14095166325569153
train_iter_loss: 0.12520061433315277
train_iter_loss: 0.2199527621269226
train_iter_loss: 0.22586800158023834
train_iter_loss: 0.20292924344539642
train_iter_loss: 0.11981204152107239
train_iter_loss: 0.13799560070037842
train_iter_loss: 0.1173056811094284
train_iter_loss: 0.13826210796833038
train_iter_loss: 0.20589295029640198
train_iter_loss: 0.07070531696081161
train_iter_loss: 0.2383541315793991
train_iter_loss: 0.16730748116970062
train_iter_loss: 0.15215227007865906
train_iter_loss: 0.1547871232032776
train_iter_loss: 0.19324693083763123
train_iter_loss: 0.2584976553916931
train_iter_loss: 0.16583110392093658
train_iter_loss: 0.06776777654886246
train_iter_loss: 0.12679056823253632
train_iter_loss: 0.09022145718336105
train_iter_loss: 0.18410977721214294
train_iter_loss: 0.1917584389448166
train_iter_loss: 0.15362073481082916
train_iter_loss: 0.1494649350643158
train_iter_loss: 0.2069501131772995
train_iter_loss: 0.11117936670780182
train_iter_loss: 0.07110069692134857
train_iter_loss: 0.15206103026866913
train_iter_loss: 0.2480618804693222
train_iter_loss: 0.2929105758666992
train_iter_loss: 0.20241354405879974
train_iter_loss: 0.3001866638660431
train_iter_loss: 0.07422533631324768
train_iter_loss: 0.2349146157503128
train_iter_loss: 0.39719027280807495
train_iter_loss: 0.34638240933418274
train_iter_loss: 0.11667367070913315
train_iter_loss: 0.15785548090934753
train_iter_loss: 0.45157817006111145
train_iter_loss: 0.1251908838748932
train_iter_loss: 0.1605912297964096
train_iter_loss: 0.14236198365688324
train_iter_loss: 0.2314571887254715
train_iter_loss: 0.18258754909038544
train loss :0.1794
---------------------
Validation seg loss: 0.22533870995361008 at epoch 143
epoch =    144/  1000, exp = train
train_iter_loss: 0.13038919866085052
train_iter_loss: 0.14287984371185303
train_iter_loss: 0.26870980858802795
train_iter_loss: 0.15833227336406708
train_iter_loss: 0.10652535408735275
train_iter_loss: 0.15555152297019958
train_iter_loss: 0.13505420088768005
train_iter_loss: 0.14295430481433868
train_iter_loss: 0.1764911413192749
train_iter_loss: 0.1915067434310913
train_iter_loss: 0.34923043847084045
train_iter_loss: 0.2050853669643402
train_iter_loss: 0.18920381367206573
train_iter_loss: 0.08845585584640503
train_iter_loss: 0.15987041592597961
train_iter_loss: 0.20096059143543243
train_iter_loss: 0.19400912523269653
train_iter_loss: 0.09475945681333542
train_iter_loss: 0.14079567790031433
train_iter_loss: 0.16585655510425568
train_iter_loss: 0.16193076968193054
train_iter_loss: 0.1668035238981247
train_iter_loss: 0.09422937780618668
train_iter_loss: 0.28017711639404297
train_iter_loss: 0.19885092973709106
train_iter_loss: 0.15652212500572205
train_iter_loss: 0.20797936618328094
train_iter_loss: 0.3072705566883087
train_iter_loss: 0.12350652366876602
train_iter_loss: 0.09848353266716003
train_iter_loss: 0.11302373558282852
train_iter_loss: 0.18473747372627258
train_iter_loss: 0.25328150391578674
train_iter_loss: 0.0707801803946495
train_iter_loss: 0.11490091681480408
train_iter_loss: 0.14375066757202148
train_iter_loss: 0.06461790204048157
train_iter_loss: 0.18944993615150452
train_iter_loss: 0.20143885910511017
train_iter_loss: 0.07467010617256165
train_iter_loss: 0.18056447803974152
train_iter_loss: 0.11790575087070465
train_iter_loss: 0.16606219112873077
train_iter_loss: 0.1352543830871582
train_iter_loss: 0.2028999626636505
train_iter_loss: 0.15951690077781677
train_iter_loss: 0.24643665552139282
train_iter_loss: 0.15631107985973358
train_iter_loss: 0.08417089283466339
train_iter_loss: 0.14158935844898224
train_iter_loss: 0.1552092730998993
train_iter_loss: 0.5265719294548035
train_iter_loss: 0.17269673943519592
train_iter_loss: 0.18762148916721344
train_iter_loss: 0.10064394772052765
train_iter_loss: 0.09187968820333481
train_iter_loss: 0.2902553975582123
train_iter_loss: 0.2237859070301056
train_iter_loss: 0.18435326218605042
train_iter_loss: 0.19440123438835144
train_iter_loss: 0.19411729276180267
train_iter_loss: 0.21395230293273926
train_iter_loss: 0.1864549070596695
train_iter_loss: 0.1377340853214264
train_iter_loss: 0.08415704220533371
train_iter_loss: 0.08693613857030869
train_iter_loss: 0.13660916686058044
train_iter_loss: 0.24459172785282135
train_iter_loss: 0.12391599267721176
train_iter_loss: 0.17012211680412292
train_iter_loss: 0.22917744517326355
train_iter_loss: 0.14082306623458862
train_iter_loss: 0.08066495507955551
train_iter_loss: 0.3270106911659241
train_iter_loss: 0.14485682547092438
train_iter_loss: 0.223642960190773
train_iter_loss: 0.09688260406255722
train_iter_loss: 0.195354625582695
train_iter_loss: 0.18144351243972778
train_iter_loss: 0.2365051954984665
train_iter_loss: 0.3049238324165344
train_iter_loss: 0.07339785248041153
train_iter_loss: 0.1576647013425827
train_iter_loss: 0.29487738013267517
train_iter_loss: 0.15084801614284515
train_iter_loss: 0.2664009630680084
train_iter_loss: 0.1950141340494156
train_iter_loss: 0.18150752782821655
train_iter_loss: 0.2218291312456131
train_iter_loss: 0.19870418310165405
train_iter_loss: 0.15527212619781494
train_iter_loss: 0.2972414195537567
train_iter_loss: 0.070127472281456
train_iter_loss: 0.19587397575378418
train_iter_loss: 0.19451715052127838
train_iter_loss: 0.19292597472667694
train_iter_loss: 0.30886590480804443
train_iter_loss: 0.21175110340118408
train_iter_loss: 0.31538015604019165
train_iter_loss: 0.2636735439300537
train loss :0.1819
---------------------
Validation seg loss: 0.2280878504949077 at epoch 144
epoch =    145/  1000, exp = train
train_iter_loss: 0.2628635764122009
train_iter_loss: 0.11368922889232635
train_iter_loss: 0.1713038980960846
train_iter_loss: 0.11657636612653732
train_iter_loss: 0.11883281916379929
train_iter_loss: 0.2597709000110626
train_iter_loss: 0.23929549753665924
train_iter_loss: 0.1502896547317505
train_iter_loss: 0.12732315063476562
train_iter_loss: 0.1877918243408203
train_iter_loss: 0.14977902173995972
train_iter_loss: 0.1691330075263977
train_iter_loss: 0.18011710047721863
train_iter_loss: 0.2716223895549774
train_iter_loss: 0.09130684286355972
train_iter_loss: 0.26738297939300537
train_iter_loss: 0.13846074044704437
train_iter_loss: 0.28838256001472473
train_iter_loss: 0.05718951299786568
train_iter_loss: 0.12375856190919876
train_iter_loss: 0.13136281073093414
train_iter_loss: 0.1964772492647171
train_iter_loss: 0.17774206399917603
train_iter_loss: 0.19677765667438507
train_iter_loss: 0.15690946578979492
train_iter_loss: 0.3110898435115814
train_iter_loss: 0.1930711269378662
train_iter_loss: 0.19764108955860138
train_iter_loss: 0.12304693460464478
train_iter_loss: 0.22476522624492645
train_iter_loss: 0.19211024045944214
train_iter_loss: 0.16497673094272614
train_iter_loss: 0.16650131344795227
train_iter_loss: 0.15571534633636475
train_iter_loss: 0.23028016090393066
train_iter_loss: 0.09072080999612808
train_iter_loss: 0.25714343786239624
train_iter_loss: 0.1353052258491516
train_iter_loss: 0.06885874271392822
train_iter_loss: 0.1557319611310959
train_iter_loss: 0.18298573791980743
train_iter_loss: 0.4127134084701538
train_iter_loss: 0.19866444170475006
train_iter_loss: 0.18387392163276672
train_iter_loss: 0.27184048295021057
train_iter_loss: 0.12677666544914246
train_iter_loss: 0.16597965359687805
train_iter_loss: 0.26155316829681396
train_iter_loss: 0.1320602148771286
train_iter_loss: 0.3240640461444855
train_iter_loss: 0.15523743629455566
train_iter_loss: 0.20150324702262878
train_iter_loss: 0.2391785979270935
train_iter_loss: 0.11537792533636093
train_iter_loss: 0.18038512766361237
train_iter_loss: 0.21920153498649597
train_iter_loss: 0.17875945568084717
train_iter_loss: 0.29437801241874695
train_iter_loss: 0.12284111976623535
train_iter_loss: 0.22563911974430084
train_iter_loss: 0.07307706028223038
train_iter_loss: 0.23976893723011017
train_iter_loss: 0.1769537776708603
train_iter_loss: 0.1675485372543335
train_iter_loss: 0.2411251813173294
train_iter_loss: 0.14359401166439056
train_iter_loss: 0.11654552072286606
train_iter_loss: 0.13814552128314972
train_iter_loss: 0.08353272825479507
train_iter_loss: 0.12544912099838257
train_iter_loss: 0.20480884611606598
train_iter_loss: 0.18627341091632843
train_iter_loss: 0.11561008542776108
train_iter_loss: 0.23879559338092804
train_iter_loss: 0.08124639838933945
train_iter_loss: 0.13691195845603943
train_iter_loss: 0.29563483595848083
train_iter_loss: 0.15172173082828522
train_iter_loss: 0.2609627842903137
train_iter_loss: 0.13943372666835785
train_iter_loss: 0.10618684440851212
train_iter_loss: 0.09100130200386047
train_iter_loss: 0.1608750969171524
train_iter_loss: 0.23590296506881714
train_iter_loss: 0.1818433403968811
train_iter_loss: 0.17069818079471588
train_iter_loss: 0.17354971170425415
train_iter_loss: 0.2735382616519928
train_iter_loss: 0.214699387550354
train_iter_loss: 0.06855093687772751
train_iter_loss: 0.25794869661331177
train_iter_loss: 0.2001412808895111
train_iter_loss: 0.1565527766942978
train_iter_loss: 0.11160881072282791
train_iter_loss: 0.17813025414943695
train_iter_loss: 0.07134708017110825
train_iter_loss: 0.1690744012594223
train_iter_loss: 0.18039239943027496
train_iter_loss: 0.1280006617307663
train_iter_loss: 0.10634804517030716
train loss :0.1794
---------------------
Validation seg loss: 0.2255456900259234 at epoch 145
epoch =    146/  1000, exp = train
train_iter_loss: 0.13175377249717712
train_iter_loss: 0.21904686093330383
train_iter_loss: 0.14801602065563202
train_iter_loss: 0.18031544983386993
train_iter_loss: 0.1325242817401886
train_iter_loss: 0.14090588688850403
train_iter_loss: 0.15810275077819824
train_iter_loss: 0.13264715671539307
train_iter_loss: 0.2059195339679718
train_iter_loss: 0.14120155572891235
train_iter_loss: 0.14671862125396729
train_iter_loss: 0.3387628495693207
train_iter_loss: 0.23613642156124115
train_iter_loss: 0.07707983255386353
train_iter_loss: 0.16849319636821747
train_iter_loss: 0.32055842876434326
train_iter_loss: 0.21291445195674896
train_iter_loss: 0.11665888130664825
train_iter_loss: 0.15012800693511963
train_iter_loss: 0.19847238063812256
train_iter_loss: 0.1656237095594406
train_iter_loss: 0.36361339688301086
train_iter_loss: 0.10619521886110306
train_iter_loss: 0.19743506610393524
train_iter_loss: 0.12257137894630432
train_iter_loss: 0.17397673428058624
train_iter_loss: 0.16616618633270264
train_iter_loss: 0.174766406416893
train_iter_loss: 0.16375233232975006
train_iter_loss: 0.17832861840724945
train_iter_loss: 0.21319280564785004
train_iter_loss: 0.19032342731952667
train_iter_loss: 0.08294451981782913
train_iter_loss: 0.20324184000492096
train_iter_loss: 0.22295652329921722
train_iter_loss: 0.22569197416305542
train_iter_loss: 0.20122449100017548
train_iter_loss: 0.18327677249908447
train_iter_loss: 0.14927493035793304
train_iter_loss: 0.14226384460926056
train_iter_loss: 0.20674312114715576
train_iter_loss: 0.11158853769302368
train_iter_loss: 0.08356592804193497
train_iter_loss: 0.10401614755392075
train_iter_loss: 0.1208634003996849
train_iter_loss: 0.2588089108467102
train_iter_loss: 0.16203755140304565
train_iter_loss: 0.05607302859425545
train_iter_loss: 0.16094054281711578
train_iter_loss: 0.1105484664440155
train_iter_loss: 0.13240991532802582
train_iter_loss: 0.2528292238712311
train_iter_loss: 0.2513105869293213
train_iter_loss: 0.191229447722435
train_iter_loss: 0.1292695552110672
train_iter_loss: 0.08657141774892807
train_iter_loss: 0.13085700571537018
train_iter_loss: 0.14654241502285004
train_iter_loss: 0.4424418807029724
train_iter_loss: 0.195625439286232
train_iter_loss: 0.3429833650588989
train_iter_loss: 0.1403883844614029
train_iter_loss: 0.2412627786397934
train_iter_loss: 0.22351637482643127
train_iter_loss: 0.22972245514392853
train_iter_loss: 0.2368783950805664
train_iter_loss: 0.18688035011291504
train_iter_loss: 0.11721646785736084
train_iter_loss: 0.13404113054275513
train_iter_loss: 0.20480521023273468
train_iter_loss: 0.11967690289020538
train_iter_loss: 0.15176522731781006
train_iter_loss: 0.1770041137933731
train_iter_loss: 0.18455733358860016
train_iter_loss: 0.1504945307970047
train_iter_loss: 0.12280480563640594
train_iter_loss: 0.16678966581821442
train_iter_loss: 0.17488247156143188
train_iter_loss: 0.18763378262519836
train_iter_loss: 0.1490066945552826
train_iter_loss: 0.18181264400482178
train_iter_loss: 0.2893143892288208
train_iter_loss: 0.14309073984622955
train_iter_loss: 0.18991684913635254
train_iter_loss: 0.13239245116710663
train_iter_loss: 0.3571711778640747
train_iter_loss: 0.16987603902816772
train_iter_loss: 0.21665282547473907
train_iter_loss: 0.13374967873096466
train_iter_loss: 0.13807551562786102
train_iter_loss: 0.1308625191450119
train_iter_loss: 0.14799869060516357
train_iter_loss: 0.20888331532478333
train_iter_loss: 0.17382341623306274
train_iter_loss: 0.15839405357837677
train_iter_loss: 0.2506079375743866
train_iter_loss: 0.15386836230754852
train_iter_loss: 0.2159377783536911
train_iter_loss: 0.15499436855316162
train_iter_loss: 0.11644645780324936
train loss :0.1800
---------------------
Validation seg loss: 0.22330297900469237 at epoch 146
epoch =    147/  1000, exp = train
train_iter_loss: 0.15631601214408875
train_iter_loss: 0.13430637121200562
train_iter_loss: 0.15889859199523926
train_iter_loss: 0.06447350978851318
train_iter_loss: 0.19096538424491882
train_iter_loss: 0.28136739134788513
train_iter_loss: 0.2764994502067566
train_iter_loss: 0.17033043503761292
train_iter_loss: 0.0786120742559433
train_iter_loss: 0.19337202608585358
train_iter_loss: 0.1627810299396515
train_iter_loss: 0.21093817055225372
train_iter_loss: 0.24250194430351257
train_iter_loss: 0.21903616189956665
train_iter_loss: 0.10162698477506638
train_iter_loss: 0.14971844851970673
train_iter_loss: 0.16436558961868286
train_iter_loss: 0.05878898501396179
train_iter_loss: 0.1449243426322937
train_iter_loss: 0.1869303584098816
train_iter_loss: 0.17885996401309967
train_iter_loss: 0.2825104594230652
train_iter_loss: 0.2060031294822693
train_iter_loss: 0.16727139055728912
train_iter_loss: 0.19184406101703644
train_iter_loss: 0.15708579123020172
train_iter_loss: 0.20497630536556244
train_iter_loss: 0.2088579386472702
train_iter_loss: 0.1247275248169899
train_iter_loss: 0.24633972346782684
train_iter_loss: 0.13086384534835815
train_iter_loss: 0.1797873079776764
train_iter_loss: 0.24728989601135254
train_iter_loss: 0.16637012362480164
train_iter_loss: 0.0878007784485817
train_iter_loss: 0.1096448227763176
train_iter_loss: 0.21886838972568512
train_iter_loss: 0.17023281753063202
train_iter_loss: 0.10294188559055328
train_iter_loss: 0.14369529485702515
train_iter_loss: 0.21184998750686646
train_iter_loss: 0.12695465981960297
train_iter_loss: 0.17389839887619019
train_iter_loss: 0.2548860013484955
train_iter_loss: 0.18510881066322327
train_iter_loss: 0.14677080512046814
train_iter_loss: 0.0859583392739296
train_iter_loss: 0.21432389318943024
train_iter_loss: 0.18637493252754211
train_iter_loss: 0.13920630514621735
train_iter_loss: 0.3091101348400116
train_iter_loss: 0.10067963600158691
train_iter_loss: 0.3904671370983124
train_iter_loss: 0.22456949949264526
train_iter_loss: 0.13895396888256073
train_iter_loss: 0.30397287011146545
train_iter_loss: 0.09923379123210907
train_iter_loss: 0.11706031858921051
train_iter_loss: 0.10096318274736404
train_iter_loss: 0.14182160794734955
train_iter_loss: 0.28616487979888916
train_iter_loss: 0.28551560640335083
train_iter_loss: 0.3373571038246155
train_iter_loss: 0.15959399938583374
train_iter_loss: 0.10417339205741882
train_iter_loss: 0.13269230723381042
train_iter_loss: 0.15129023790359497
train_iter_loss: 0.11773446202278137
train_iter_loss: 0.17011785507202148
train_iter_loss: 0.09076518565416336
train_iter_loss: 0.10933628678321838
train_iter_loss: 0.12402050942182541
train_iter_loss: 0.1337999403476715
train_iter_loss: 0.16688299179077148
train_iter_loss: 0.20274010300636292
train_iter_loss: 0.12627233564853668
train_iter_loss: 0.19720223546028137
train_iter_loss: 0.2730540931224823
train_iter_loss: 0.14950624108314514
train_iter_loss: 0.1474614143371582
train_iter_loss: 0.17736805975437164
train_iter_loss: 0.18883873522281647
train_iter_loss: 0.1534615010023117
train_iter_loss: 0.19675716757774353
train_iter_loss: 0.21348142623901367
train_iter_loss: 0.18562228977680206
train_iter_loss: 0.14527490735054016
train_iter_loss: 0.1779894232749939
train_iter_loss: 0.18582357466220856
train_iter_loss: 0.18193168938159943
train_iter_loss: 0.13486386835575104
train_iter_loss: 0.1046380028128624
train_iter_loss: 0.150621235370636
train_iter_loss: 0.12733936309814453
train_iter_loss: 0.25212258100509644
train_iter_loss: 0.21281811594963074
train_iter_loss: 0.13752034306526184
train_iter_loss: 0.14996708929538727
train_iter_loss: 0.24087990820407867
train_iter_loss: 0.19335518777370453
train loss :0.1768
---------------------
Validation seg loss: 0.22770100030696616 at epoch 147
epoch =    148/  1000, exp = train
train_iter_loss: 0.12344858050346375
train_iter_loss: 0.16732342541217804
train_iter_loss: 0.18232962489128113
train_iter_loss: 0.1268128752708435
train_iter_loss: 0.20579087734222412
train_iter_loss: 0.18243372440338135
train_iter_loss: 0.18821734189987183
train_iter_loss: 0.15308912098407745
train_iter_loss: 0.11947961151599884
train_iter_loss: 0.2029896229505539
train_iter_loss: 0.18355080485343933
train_iter_loss: 0.3016086518764496
train_iter_loss: 0.21246635913848877
train_iter_loss: 0.14525869488716125
train_iter_loss: 0.11898598074913025
train_iter_loss: 0.1523851603269577
train_iter_loss: 0.14749759435653687
train_iter_loss: 0.23723797500133514
train_iter_loss: 0.15677036345005035
train_iter_loss: 0.12432756274938583
train_iter_loss: 0.24705946445465088
train_iter_loss: 0.20121562480926514
train_iter_loss: 0.17495988309383392
train_iter_loss: 0.2511792480945587
train_iter_loss: 0.19853806495666504
train_iter_loss: 0.12478817254304886
train_iter_loss: 0.3328731060028076
train_iter_loss: 0.1026298999786377
train_iter_loss: 0.1986200511455536
train_iter_loss: 0.11299162358045578
train_iter_loss: 0.2320612221956253
train_iter_loss: 0.0975886806845665
train_iter_loss: 0.28883108496665955
train_iter_loss: 0.16961318254470825
train_iter_loss: 0.15747666358947754
train_iter_loss: 0.29146063327789307
train_iter_loss: 0.2160715013742447
train_iter_loss: 0.13283269107341766
train_iter_loss: 0.14851489663124084
train_iter_loss: 0.20480112731456757
train_iter_loss: 0.13107989728450775
train_iter_loss: 0.20195114612579346
train_iter_loss: 0.14709177613258362
train_iter_loss: 0.1585754007101059
train_iter_loss: 0.1653488427400589
train_iter_loss: 0.10903065651655197
train_iter_loss: 0.11196113377809525
train_iter_loss: 0.15662704408168793
train_iter_loss: 0.18231037259101868
train_iter_loss: 0.13509418070316315
train_iter_loss: 0.1744401752948761
train_iter_loss: 0.2690378427505493
train_iter_loss: 0.17854788899421692
train_iter_loss: 0.16615913808345795
train_iter_loss: 0.177741140127182
train_iter_loss: 0.251955509185791
train_iter_loss: 0.1127435639500618
train_iter_loss: 0.18383294343948364
train_iter_loss: 0.10292589664459229
train_iter_loss: 0.145308256149292
train_iter_loss: 0.2638385593891144
train_iter_loss: 0.1891256421804428
train_iter_loss: 0.20099316537380219
train_iter_loss: 0.18104766309261322
train_iter_loss: 0.0964544266462326
train_iter_loss: 0.11323878169059753
train_iter_loss: 0.1697215586900711
train_iter_loss: 0.2205571085214615
train_iter_loss: 0.18054157495498657
train_iter_loss: 0.12963028252124786
train_iter_loss: 0.20682403445243835
train_iter_loss: 0.187627911567688
train_iter_loss: 0.27676108479499817
train_iter_loss: 0.06878495961427689
train_iter_loss: 0.13993708789348602
train_iter_loss: 0.1514570415019989
train_iter_loss: 0.12735091149806976
train_iter_loss: 0.19505634903907776
train_iter_loss: 0.201154425740242
train_iter_loss: 0.20129399001598358
train_iter_loss: 0.30216100811958313
train_iter_loss: 0.10376619547605515
train_iter_loss: 0.2247672975063324
train_iter_loss: 0.13776005804538727
train_iter_loss: 0.13205339014530182
train_iter_loss: 0.14873279631137848
train_iter_loss: 0.1900690644979477
train_iter_loss: 0.14133596420288086
train_iter_loss: 0.18161362409591675
train_iter_loss: 0.20774908363819122
train_iter_loss: 0.20651963353157043
train_iter_loss: 0.18516135215759277
train_iter_loss: 0.2117874026298523
train_iter_loss: 0.22880838811397552
train_iter_loss: 0.1882139891386032
train_iter_loss: 0.17693597078323364
train_iter_loss: 0.21625390648841858
train_iter_loss: 0.1421455591917038
train_iter_loss: 0.20248867571353912
train_iter_loss: 0.12068326771259308
train loss :0.1791
---------------------
Validation seg loss: 0.22342594169994007 at epoch 148
epoch =    149/  1000, exp = train
train_iter_loss: 0.1745317429304123
train_iter_loss: 0.12493537366390228
train_iter_loss: 0.06985096633434296
train_iter_loss: 0.27947646379470825
train_iter_loss: 0.12050517648458481
train_iter_loss: 0.17792238295078278
train_iter_loss: 0.22471222281455994
train_iter_loss: 0.26176172494888306
train_iter_loss: 0.2170620560646057
train_iter_loss: 0.21654364466667175
train_iter_loss: 0.06939840316772461
train_iter_loss: 0.18383313715457916
train_iter_loss: 0.19684651494026184
train_iter_loss: 0.07859709113836288
train_iter_loss: 0.10858268290758133
train_iter_loss: 0.10059995204210281
train_iter_loss: 0.16201594471931458
train_iter_loss: 0.12736542522907257
train_iter_loss: 0.2008655071258545
train_iter_loss: 0.1089191809296608
train_iter_loss: 0.1798405796289444
train_iter_loss: 0.1389039307832718
train_iter_loss: 0.15986491739749908
train_iter_loss: 0.18443165719509125
train_iter_loss: 0.20748960971832275
train_iter_loss: 0.20727279782295227
train_iter_loss: 0.3046668469905853
train_iter_loss: 0.1235104575753212
train_iter_loss: 0.12758289277553558
train_iter_loss: 0.06647812575101852
train_iter_loss: 0.11147209256887436
train_iter_loss: 0.11237591505050659
train_iter_loss: 0.33592769503593445
train_iter_loss: 0.2914487421512604
train_iter_loss: 0.17023910582065582
train_iter_loss: 0.10272543132305145
train_iter_loss: 0.27495861053466797
train_iter_loss: 0.16352491080760956
train_iter_loss: 0.17993749678134918
train_iter_loss: 0.1438126266002655
train_iter_loss: 0.08783496171236038
train_iter_loss: 0.28327250480651855
train_iter_loss: 0.3253647983074188
train_iter_loss: 0.1984768956899643
train_iter_loss: 0.4655408263206482
train_iter_loss: 0.1642293781042099
train_iter_loss: 0.24573402106761932
train_iter_loss: 0.23938067257404327
train_iter_loss: 0.13814908266067505
train_iter_loss: 0.15357866883277893
train_iter_loss: 0.11441569775342941
train_iter_loss: 0.13024292886257172
train_iter_loss: 0.13461564481258392
train_iter_loss: 0.19343389570713043
train_iter_loss: 0.2391020804643631
train_iter_loss: 0.7529322504997253
train_iter_loss: 0.15124112367630005
train_iter_loss: 0.13087788224220276
train_iter_loss: 0.20571961998939514
train_iter_loss: 0.21246269345283508
train_iter_loss: 0.1825997531414032
train_iter_loss: 0.10920117795467377
train_iter_loss: 0.15051552653312683
train_iter_loss: 0.16301080584526062
train_iter_loss: 0.1694784164428711
train_iter_loss: 0.16011177003383636
train_iter_loss: 0.14879187941551208
train_iter_loss: 0.19119498133659363
train_iter_loss: 0.2730124890804291
train_iter_loss: 0.17941878736019135
train_iter_loss: 0.23174649477005005
train_iter_loss: 0.15028510987758636
train_iter_loss: 0.25323742628097534
train_iter_loss: 0.14619916677474976
train_iter_loss: 0.10385850071907043
train_iter_loss: 0.12249299883842468
train_iter_loss: 0.2026529461145401
train_iter_loss: 0.2518440783023834
train_iter_loss: 0.2316240817308426
train_iter_loss: 0.15509434044361115
train_iter_loss: 0.2751595973968506
train_iter_loss: 0.17685435712337494
train_iter_loss: 0.19544005393981934
train_iter_loss: 0.10619755834341049
train_iter_loss: 0.163296177983284
train_iter_loss: 0.20545624196529388
train_iter_loss: 0.03203234821557999
train_iter_loss: 0.21326427161693573
train_iter_loss: 0.17048823833465576
train_iter_loss: 0.25491201877593994
train_iter_loss: 0.13416047394275665
train_iter_loss: 0.23099735379219055
train_iter_loss: 0.1781197190284729
train_iter_loss: 0.24809323251247406
train_iter_loss: 0.10955724865198135
train_iter_loss: 0.12273886799812317
train_iter_loss: 0.12447629123926163
train_iter_loss: 0.2539736330509186
train_iter_loss: 0.15093845129013062
train_iter_loss: 0.11162497103214264
train loss :0.1857
---------------------
Validation seg loss: 0.22487622682216032 at epoch 149
epoch =    150/  1000, exp = train
train_iter_loss: 0.2245607227087021
train_iter_loss: 0.30879515409469604
train_iter_loss: 0.10717356204986572
train_iter_loss: 0.13294477760791779
train_iter_loss: 0.1723819077014923
train_iter_loss: 0.2044910192489624
train_iter_loss: 0.1943480521440506
train_iter_loss: 0.20956718921661377
train_iter_loss: 0.20116379857063293
train_iter_loss: 0.13078276813030243
train_iter_loss: 0.304764986038208
train_iter_loss: 0.24374207854270935
train_iter_loss: 0.11460936814546585
train_iter_loss: 0.21568159759044647
train_iter_loss: 0.1117628887295723
train_iter_loss: 0.319688618183136
train_iter_loss: 0.12768444418907166
train_iter_loss: 0.20278343558311462
train_iter_loss: 0.14275537431240082
train_iter_loss: 0.18036571145057678
train_iter_loss: 0.11918304115533829
train_iter_loss: 0.1837548315525055
train_iter_loss: 0.21451696753501892
train_iter_loss: 0.33674541115760803
train_iter_loss: 0.2067214548587799
train_iter_loss: 0.17602284252643585
train_iter_loss: 0.25148090720176697
train_iter_loss: 0.16355814039707184
train_iter_loss: 0.14903250336647034
train_iter_loss: 0.14891214668750763
train_iter_loss: 0.16646017134189606
train_iter_loss: 0.13596537709236145
train_iter_loss: 0.23115931451320648
train_iter_loss: 0.16702167689800262
train_iter_loss: 0.14420853555202484
train_iter_loss: 0.12856753170490265
train_iter_loss: 0.1286868005990982
train_iter_loss: 0.15605053305625916
train_iter_loss: 0.2999955415725708
train_iter_loss: 0.16407597064971924
train_iter_loss: 0.1131884828209877
train_iter_loss: 0.18786537647247314
train_iter_loss: 0.10441292822360992
train_iter_loss: 0.16487149894237518
train_iter_loss: 0.16887684166431427
train_iter_loss: 0.18259455263614655
train_iter_loss: 0.21088233590126038
train_iter_loss: 0.10681641846895218
train_iter_loss: 0.25228822231292725
train_iter_loss: 0.09652914851903915
train_iter_loss: 0.15375733375549316
train_iter_loss: 0.2101992517709732
train_iter_loss: 0.10573256015777588
train_iter_loss: 0.25630345940589905
train_iter_loss: 0.18118321895599365
train_iter_loss: 0.19789858162403107
train_iter_loss: 0.3457208275794983
train_iter_loss: 0.15520676970481873
train_iter_loss: 0.09904919564723969
train_iter_loss: 0.1930548995733261
train_iter_loss: 0.17225179076194763
train_iter_loss: 0.14197081327438354
train_iter_loss: 0.07993580400943756
train_iter_loss: 0.148164764046669
train_iter_loss: 0.15784217417240143
train_iter_loss: 0.1117340549826622
train_iter_loss: 0.18595975637435913
train_iter_loss: 0.06368859112262726
train_iter_loss: 0.2821889817714691
train_iter_loss: 0.2638758420944214
train_iter_loss: 0.1315496861934662
train_iter_loss: 0.17908625304698944
train_iter_loss: 0.1858546882867813
train_iter_loss: 0.41619110107421875
train_iter_loss: 0.1285790503025055
train_iter_loss: 0.14084210991859436
train_iter_loss: 0.17126430571079254
train_iter_loss: 0.1735394299030304
train_iter_loss: 0.18416661024093628
train_iter_loss: 0.11966043710708618
train_iter_loss: 0.22732961177825928
train_iter_loss: 0.12008789926767349
train_iter_loss: 0.1423143595457077
train_iter_loss: 0.13689127564430237
train_iter_loss: 0.11742587387561798
train_iter_loss: 0.12999433279037476
train_iter_loss: 0.1812663972377777
train_iter_loss: 0.14679470658302307
train_iter_loss: 0.2091766744852066
train_iter_loss: 0.18189959228038788
train_iter_loss: 0.14574706554412842
train_iter_loss: 0.1401975303888321
train_iter_loss: 0.25224608182907104
train_iter_loss: 0.13602010905742645
train_iter_loss: 0.17087189853191376
train_iter_loss: 0.16904541850090027
train_iter_loss: 0.20989739894866943
train_iter_loss: 0.11696227639913559
train_iter_loss: 0.1486120969057083
train_iter_loss: 0.19072425365447998
train loss :0.1791
---------------------
Validation seg loss: 0.22403848926835465 at epoch 150
epoch =    151/  1000, exp = train
train_iter_loss: 0.10942227393388748
train_iter_loss: 0.1910974532365799
train_iter_loss: 0.3364621698856354
train_iter_loss: 0.10740625858306885
train_iter_loss: 0.20309500396251678
train_iter_loss: 0.10735930502414703
train_iter_loss: 0.2120048701763153
train_iter_loss: 0.09763579815626144
train_iter_loss: 0.2628137767314911
train_iter_loss: 0.2322850227355957
train_iter_loss: 0.14459054172039032
train_iter_loss: 0.19686055183410645
train_iter_loss: 0.25298234820365906
train_iter_loss: 0.2410506308078766
train_iter_loss: 0.1358160674571991
train_iter_loss: 0.1847737729549408
train_iter_loss: 0.12004540860652924
train_iter_loss: 0.2064129263162613
train_iter_loss: 0.14107444882392883
train_iter_loss: 0.19362875819206238
train_iter_loss: 0.19270452857017517
train_iter_loss: 0.17314942181110382
train_iter_loss: 0.13366828858852386
train_iter_loss: 0.16760846972465515
train_iter_loss: 0.16216595470905304
train_iter_loss: 0.09006457030773163
train_iter_loss: 0.2155584692955017
train_iter_loss: 0.16199812293052673
train_iter_loss: 0.14613907039165497
train_iter_loss: 0.19373783469200134
train_iter_loss: 0.0745592936873436
train_iter_loss: 0.0821133702993393
train_iter_loss: 0.2628917098045349
train_iter_loss: 0.2540687620639801
train_iter_loss: 0.3462175130844116
train_iter_loss: 0.13936275243759155
train_iter_loss: 0.21081292629241943
train_iter_loss: 0.14182625710964203
train_iter_loss: 0.21368388831615448
train_iter_loss: 0.13814681768417358
train_iter_loss: 0.22913986444473267
train_iter_loss: 0.1261216700077057
train_iter_loss: 0.15508641302585602
train_iter_loss: 0.1790698766708374
train_iter_loss: 0.13033895194530487
train_iter_loss: 0.1784043163061142
train_iter_loss: 0.09683793783187866
train_iter_loss: 0.17994636297225952
train_iter_loss: 0.11253050714731216
train_iter_loss: 0.18543846905231476
train_iter_loss: 0.07559481263160706
train_iter_loss: 0.13565082848072052
train_iter_loss: 0.19365473091602325
train_iter_loss: 0.21438193321228027
train_iter_loss: 0.14132164418697357
train_iter_loss: 0.08908730000257492
train_iter_loss: 0.10096070170402527
train_iter_loss: 0.2984957993030548
train_iter_loss: 0.2217624932527542
train_iter_loss: 0.17866268754005432
train_iter_loss: 0.17520444095134735
train_iter_loss: 0.17291921377182007
train_iter_loss: 0.1903589367866516
train_iter_loss: 0.07200764864683151
train_iter_loss: 0.18079069256782532
train_iter_loss: 0.11301037669181824
train_iter_loss: 0.11459054052829742
train_iter_loss: 0.1677127182483673
train_iter_loss: 0.1870356649160385
train_iter_loss: 0.08698249608278275
train_iter_loss: 0.300105482339859
train_iter_loss: 0.1722688227891922
train_iter_loss: 0.17772610485553741
train_iter_loss: 0.21975421905517578
train_iter_loss: 0.2853873372077942
train_iter_loss: 0.1871403604745865
train_iter_loss: 0.2551638185977936
train_iter_loss: 0.20809507369995117
train_iter_loss: 0.179190993309021
train_iter_loss: 0.18149492144584656
train_iter_loss: 0.18939568102359772
train_iter_loss: 0.25653520226478577
train_iter_loss: 0.20062193274497986
train_iter_loss: 0.19320113956928253
train_iter_loss: 0.21756896376609802
train_iter_loss: 0.12589885294437408
train_iter_loss: 0.147555410861969
train_iter_loss: 0.13541240990161896
train_iter_loss: 0.1835722029209137
train_iter_loss: 0.08428660035133362
train_iter_loss: 0.14366985857486725
train_iter_loss: 0.22650927305221558
train_iter_loss: 0.10326262563467026
train_iter_loss: 0.1537523716688156
train_iter_loss: 0.15958309173583984
train_iter_loss: 0.29823553562164307
train_iter_loss: 0.07473018020391464
train_iter_loss: 0.1300303041934967
train_iter_loss: 0.18148328363895416
train_iter_loss: 0.10940980166196823
train loss :0.1752
---------------------
Validation seg loss: 0.2235358942229793 at epoch 151
epoch =    152/  1000, exp = train
train_iter_loss: 0.09125114977359772
train_iter_loss: 0.117940753698349
train_iter_loss: 0.15338461101055145
train_iter_loss: 0.17015166580677032
train_iter_loss: 0.1747860610485077
train_iter_loss: 0.20271816849708557
train_iter_loss: 0.17272457480430603
train_iter_loss: 0.11140879988670349
train_iter_loss: 0.11846335232257843
train_iter_loss: 0.15470951795578003
train_iter_loss: 0.20456963777542114
train_iter_loss: 0.14005406200885773
train_iter_loss: 0.15238109230995178
train_iter_loss: 0.17447784543037415
train_iter_loss: 0.31444182991981506
train_iter_loss: 0.14911878108978271
train_iter_loss: 0.14482760429382324
train_iter_loss: 0.10956268012523651
train_iter_loss: 0.2250855267047882
train_iter_loss: 0.29033660888671875
train_iter_loss: 0.19206197559833527
train_iter_loss: 0.20440813899040222
train_iter_loss: 0.20818519592285156
train_iter_loss: 0.12869912385940552
train_iter_loss: 0.19114115834236145
train_iter_loss: 0.1388895958662033
train_iter_loss: 0.12555813789367676
train_iter_loss: 0.21317268908023834
train_iter_loss: 0.17569687962532043
train_iter_loss: 0.1763419359922409
train_iter_loss: 0.09290697425603867
train_iter_loss: 0.09844467043876648
train_iter_loss: 0.12506930530071259
train_iter_loss: 0.11853986978530884
train_iter_loss: 0.0853159949183464
train_iter_loss: 0.17144209146499634
train_iter_loss: 0.18911506235599518
train_iter_loss: 0.11357928812503815
train_iter_loss: 0.03641541302204132
train_iter_loss: 0.17416924238204956
train_iter_loss: 0.26642268896102905
train_iter_loss: 0.16247093677520752
train_iter_loss: 0.1761251837015152
train_iter_loss: 0.12338075786828995
train_iter_loss: 0.12216191738843918
train_iter_loss: 0.21711261570453644
train_iter_loss: 0.150277778506279
train_iter_loss: 0.13065916299819946
train_iter_loss: 0.2333323359489441
train_iter_loss: 0.18743488192558289
train_iter_loss: 0.20594605803489685
train_iter_loss: 0.17998073995113373
train_iter_loss: 0.22683607041835785
train_iter_loss: 0.10473156720399857
train_iter_loss: 0.1012200340628624
train_iter_loss: 0.2351749986410141
train_iter_loss: 0.22332000732421875
train_iter_loss: 0.27315595746040344
train_iter_loss: 0.1380452960729599
train_iter_loss: 0.08726496249437332
train_iter_loss: 0.107064388692379
train_iter_loss: 0.18994063138961792
train_iter_loss: 0.1940319687128067
train_iter_loss: 0.134984090924263
train_iter_loss: 0.2613174617290497
train_iter_loss: 0.18308021128177643
train_iter_loss: 0.1596367508172989
train_iter_loss: 0.26454436779022217
train_iter_loss: 0.15099039673805237
train_iter_loss: 0.23868893086910248
train_iter_loss: 0.25010696053504944
train_iter_loss: 0.18671630322933197
train_iter_loss: 0.15841056406497955
train_iter_loss: 0.10457317531108856
train_iter_loss: 0.12848317623138428
train_iter_loss: 0.16378702223300934
train_iter_loss: 0.12828952074050903
train_iter_loss: 0.25200241804122925
train_iter_loss: 0.27821025252342224
train_iter_loss: 0.08283305913209915
train_iter_loss: 0.19530753791332245
train_iter_loss: 0.18969127535820007
train_iter_loss: 0.14158585667610168
train_iter_loss: 0.21751292049884796
train_iter_loss: 0.21422739326953888
train_iter_loss: 0.14806902408599854
train_iter_loss: 0.08964300900697708
train_iter_loss: 0.19270828366279602
train_iter_loss: 0.14568652212619781
train_iter_loss: 0.21337474882602692
train_iter_loss: 0.2831152081489563
train_iter_loss: 0.17388856410980225
train_iter_loss: 0.33486253023147583
train_iter_loss: 0.21003533899784088
train_iter_loss: 0.10099445283412933
train_iter_loss: 0.13382387161254883
train_iter_loss: 0.10256554186344147
train_iter_loss: 0.1554223746061325
train_iter_loss: 0.19567762315273285
train_iter_loss: 0.1537095457315445
train loss :0.1726
---------------------
Validation seg loss: 0.22655799441074706 at epoch 152
epoch =    153/  1000, exp = train
train_iter_loss: 0.20661959052085876
train_iter_loss: 0.13082310557365417
train_iter_loss: 0.15733511745929718
train_iter_loss: 0.3457682430744171
train_iter_loss: 0.24457509815692902
train_iter_loss: 0.1840338110923767
train_iter_loss: 0.25417211651802063
train_iter_loss: 0.1917048841714859
train_iter_loss: 0.20014256238937378
train_iter_loss: 0.11851372569799423
train_iter_loss: 0.2686317265033722
train_iter_loss: 0.18375112116336823
train_iter_loss: 0.14083781838417053
train_iter_loss: 0.1792251318693161
train_iter_loss: 0.1955321580171585
train_iter_loss: 0.15701979398727417
train_iter_loss: 0.1529436707496643
train_iter_loss: 0.09690570831298828
train_iter_loss: 0.25745105743408203
train_iter_loss: 0.07178182899951935
train_iter_loss: 0.11228861659765244
train_iter_loss: 0.1620754599571228
train_iter_loss: 0.1764279305934906
train_iter_loss: 0.10420093685388565
train_iter_loss: 0.15000711381435394
train_iter_loss: 0.2683367431163788
train_iter_loss: 0.2439134418964386
train_iter_loss: 0.1799156367778778
train_iter_loss: 0.0767265111207962
train_iter_loss: 0.14071375131607056
train_iter_loss: 0.18862038850784302
train_iter_loss: 0.15405192971229553
train_iter_loss: 0.2624088525772095
train_iter_loss: 0.150946706533432
train_iter_loss: 0.24676695466041565
train_iter_loss: 0.07985201478004456
train_iter_loss: 0.23343811929225922
train_iter_loss: 0.1972026377916336
train_iter_loss: 0.14425739645957947
train_iter_loss: 0.1281982809305191
train_iter_loss: 0.195197194814682
train_iter_loss: 0.24736104905605316
train_iter_loss: 0.14314572513103485
train_iter_loss: 0.19490525126457214
train_iter_loss: 0.13991835713386536
train_iter_loss: 0.11572936922311783
train_iter_loss: 0.12412551790475845
train_iter_loss: 0.1527223140001297
train_iter_loss: 0.09536623954772949
train_iter_loss: 0.07149473577737808
train_iter_loss: 0.19853395223617554
train_iter_loss: 0.24775651097297668
train_iter_loss: 0.19603700935840607
train_iter_loss: 0.2051343470811844
train_iter_loss: 0.10968208312988281
train_iter_loss: 0.04873381182551384
train_iter_loss: 0.11013884842395782
train_iter_loss: 0.20768064260482788
train_iter_loss: 0.20103639364242554
train_iter_loss: 0.13702750205993652
train_iter_loss: 0.17796337604522705
train_iter_loss: 0.1379409283399582
train_iter_loss: 0.20048193633556366
train_iter_loss: 0.14900249242782593
train_iter_loss: 0.14200687408447266
train_iter_loss: 0.17704109847545624
train_iter_loss: 0.22753486037254333
train_iter_loss: 0.10452380776405334
train_iter_loss: 0.18136635422706604
train_iter_loss: 0.14311468601226807
train_iter_loss: 0.2028205841779709
train_iter_loss: 0.1280784010887146
train_iter_loss: 0.22864562273025513
train_iter_loss: 0.13824279606342316
train_iter_loss: 0.23088338971138
train_iter_loss: 0.12635840475559235
train_iter_loss: 0.15283191204071045
train_iter_loss: 0.16725336015224457
train_iter_loss: 0.31934094429016113
train_iter_loss: 0.23677609860897064
train_iter_loss: 0.2812861502170563
train_iter_loss: 0.08096330612897873
train_iter_loss: 0.11065283417701721
train_iter_loss: 0.20979364216327667
train_iter_loss: 0.09943066537380219
train_iter_loss: 0.14614515006542206
train_iter_loss: 0.2897014915943146
train_iter_loss: 0.168114572763443
train_iter_loss: 0.2913571298122406
train_iter_loss: 0.10727199912071228
train_iter_loss: 0.22568294405937195
train_iter_loss: 0.12436460703611374
train_iter_loss: 0.1053909957408905
train_iter_loss: 0.08591398596763611
train_iter_loss: 0.21942754089832306
train_iter_loss: 0.11502529680728912
train_iter_loss: 0.17591701447963715
train_iter_loss: 0.24162477254867554
train_iter_loss: 0.23406846821308136
train_iter_loss: 0.2069227248430252
train loss :0.1754
---------------------
Validation seg loss: 0.22610102112422575 at epoch 153
epoch =    154/  1000, exp = train
train_iter_loss: 0.17263181507587433
train_iter_loss: 0.16849921643733978
train_iter_loss: 0.24890942871570587
train_iter_loss: 0.21788497269153595
train_iter_loss: 0.15434035658836365
train_iter_loss: 0.3656109571456909
train_iter_loss: 0.10874412953853607
train_iter_loss: 0.09981866180896759
train_iter_loss: 0.2606026530265808
train_iter_loss: 0.13122203946113586
train_iter_loss: 0.09697146713733673
train_iter_loss: 0.11913369596004486
train_iter_loss: 0.14700648188591003
train_iter_loss: 0.1147715151309967
train_iter_loss: 0.1365380436182022
train_iter_loss: 0.21291889250278473
train_iter_loss: 0.14752303063869476
train_iter_loss: 0.3760276734828949
train_iter_loss: 0.16645629703998566
train_iter_loss: 0.28337711095809937
train_iter_loss: 0.11493849754333496
train_iter_loss: 0.1944950819015503
train_iter_loss: 0.17014716565608978
train_iter_loss: 0.12358452379703522
train_iter_loss: 0.1738802194595337
train_iter_loss: 0.15375852584838867
train_iter_loss: 0.25938746333122253
train_iter_loss: 0.12518607079982758
train_iter_loss: 0.18694494664669037
train_iter_loss: 0.12427358329296112
train_iter_loss: 0.1591218262910843
train_iter_loss: 0.16829915344715118
train_iter_loss: 0.07245150208473206
train_iter_loss: 0.10696236789226532
train_iter_loss: 0.2120257169008255
train_iter_loss: 0.18077783286571503
train_iter_loss: 0.1859847903251648
train_iter_loss: 0.24559806287288666
train_iter_loss: 0.15618237853050232
train_iter_loss: 0.20781520009040833
train_iter_loss: 0.1920108050107956
train_iter_loss: 0.21833549439907074
train_iter_loss: 0.18044564127922058
train_iter_loss: 0.11853060871362686
train_iter_loss: 0.15515106916427612
train_iter_loss: 0.200492262840271
train_iter_loss: 0.23588396608829498
train_iter_loss: 0.19401858747005463
train_iter_loss: 0.2001928836107254
train_iter_loss: 0.1395432949066162
train_iter_loss: 0.16762161254882812
train_iter_loss: 0.14931462705135345
train_iter_loss: 0.1565748155117035
train_iter_loss: 0.16024085879325867
train_iter_loss: 0.18283089995384216
train_iter_loss: 0.21996967494487762
train_iter_loss: 0.1499401479959488
train_iter_loss: 0.10940079391002655
train_iter_loss: 0.03300463408231735
train_iter_loss: 0.1782817393541336
train_iter_loss: 0.12598450481891632
train_iter_loss: 0.2719494700431824
train_iter_loss: 0.21695083379745483
train_iter_loss: 0.2137650102376938
train_iter_loss: 0.27511709928512573
train_iter_loss: 0.13896919786930084
train_iter_loss: 0.2488299012184143
train_iter_loss: 0.23025447130203247
train_iter_loss: 0.14594270288944244
train_iter_loss: 0.12986499071121216
train_iter_loss: 0.1417245715856552
train_iter_loss: 0.13324101269245148
train_iter_loss: 0.14364124834537506
train_iter_loss: 0.24322578310966492
train_iter_loss: 0.24298590421676636
train_iter_loss: 0.12912733852863312
train_iter_loss: 0.2460447996854782
train_iter_loss: 0.17423535883426666
train_iter_loss: 0.19079872965812683
train_iter_loss: 0.2591208219528198
train_iter_loss: 0.21422404050827026
train_iter_loss: 0.19035902619361877
train_iter_loss: 0.13187386095523834
train_iter_loss: 0.15678343176841736
train_iter_loss: 0.10567907243967056
train_iter_loss: 0.12939581274986267
train_iter_loss: 0.15606030821800232
train_iter_loss: 0.1673022359609604
train_iter_loss: 0.21342721581459045
train_iter_loss: 0.21184946596622467
train_iter_loss: 0.14616335928440094
train_iter_loss: 0.23279507458209991
train_iter_loss: 0.20739008486270905
train_iter_loss: 0.15590578317642212
train_iter_loss: 0.12288378179073334
train_iter_loss: 0.19695638120174408
train_iter_loss: 0.07633563876152039
train_iter_loss: 0.13751569390296936
train_iter_loss: 0.16543807089328766
train_iter_loss: 0.15461982786655426
train loss :0.1774
---------------------
Validation seg loss: 0.22673165525819333 at epoch 154
epoch =    155/  1000, exp = train
train_iter_loss: 0.18963080644607544
train_iter_loss: 0.23240040242671967
train_iter_loss: 0.1319185197353363
train_iter_loss: 0.1282699704170227
train_iter_loss: 0.20257103443145752
train_iter_loss: 0.0928952768445015
train_iter_loss: 0.11251427233219147
train_iter_loss: 0.1242813989520073
train_iter_loss: 0.2603527903556824
train_iter_loss: 0.17011570930480957
train_iter_loss: 0.12650510668754578
train_iter_loss: 0.13091474771499634
train_iter_loss: 0.21677494049072266
train_iter_loss: 0.31334272027015686
train_iter_loss: 0.2589305341243744
train_iter_loss: 0.16464775800704956
train_iter_loss: 0.3477672040462494
train_iter_loss: 0.12075749784708023
train_iter_loss: 0.2242094725370407
train_iter_loss: 0.11382041126489639
train_iter_loss: 0.14720815420150757
train_iter_loss: 0.1829225867986679
train_iter_loss: 0.20466357469558716
train_iter_loss: 0.24684898555278778
train_iter_loss: 0.2048516571521759
train_iter_loss: 0.10765553265810013
train_iter_loss: 0.2135208249092102
train_iter_loss: 0.17143277823925018
train_iter_loss: 0.17476223409175873
train_iter_loss: 0.14725778996944427
train_iter_loss: 0.1827206313610077
train_iter_loss: 0.10592710971832275
train_iter_loss: 0.16574417054653168
train_iter_loss: 0.17090320587158203
train_iter_loss: 0.1985897421836853
train_iter_loss: 0.13879744708538055
train_iter_loss: 0.07400044798851013
train_iter_loss: 0.11117712408304214
train_iter_loss: 0.07793649286031723
train_iter_loss: 0.0685051754117012
train_iter_loss: 0.1671321839094162
train_iter_loss: 0.08447138220071793
train_iter_loss: 0.20958885550498962
train_iter_loss: 0.21917980909347534
train_iter_loss: 0.33164578676223755
train_iter_loss: 0.09550879150629044
train_iter_loss: 0.2270871102809906
train_iter_loss: 0.16242067515850067
train_iter_loss: 0.14667491614818573
train_iter_loss: 0.2133801430463791
train_iter_loss: 0.1112385019659996
train_iter_loss: 0.1741771548986435
train_iter_loss: 0.14979378879070282
train_iter_loss: 0.17142073810100555
train_iter_loss: 0.2070981115102768
train_iter_loss: 0.1983543187379837
train_iter_loss: 0.14568066596984863
train_iter_loss: 0.12067222595214844
train_iter_loss: 0.12185724079608917
train_iter_loss: 0.17488843202590942
train_iter_loss: 0.17155179381370544
train_iter_loss: 0.14736801385879517
train_iter_loss: 0.2468317747116089
train_iter_loss: 0.14885160326957703
train_iter_loss: 0.1465977132320404
train_iter_loss: 0.22536148130893707
train_iter_loss: 0.20380833745002747
train_iter_loss: 0.16538073122501373
train_iter_loss: 0.13707564771175385
train_iter_loss: 0.15130576491355896
train_iter_loss: 0.18434429168701172
train_iter_loss: 0.19547078013420105
train_iter_loss: 0.13516078889369965
train_iter_loss: 0.20472638309001923
train_iter_loss: 0.2290872037410736
train_iter_loss: 0.14366723597049713
train_iter_loss: 0.22821174561977386
train_iter_loss: 0.17910145223140717
train_iter_loss: 0.11434435099363327
train_iter_loss: 0.17800816893577576
train_iter_loss: 0.1549796760082245
train_iter_loss: 0.2304406315088272
train_iter_loss: 0.26218369603157043
train_iter_loss: 0.09590552747249603
train_iter_loss: 0.14854347705841064
train_iter_loss: 0.1965940147638321
train_iter_loss: 0.14893756806850433
train_iter_loss: 0.15181852877140045
train_iter_loss: 0.3488894999027252
train_iter_loss: 0.10180135071277618
train_iter_loss: 0.1702333688735962
train_iter_loss: 0.12966769933700562
train_iter_loss: 0.287447452545166
train_iter_loss: 0.10684799402952194
train_iter_loss: 0.21623550355434418
train_iter_loss: 0.18833085894584656
train_iter_loss: 0.16612805426120758
train_iter_loss: 0.1335560381412506
train_iter_loss: 0.227489173412323
train_iter_loss: 0.17507189512252808
train loss :0.1755
---------------------
Validation seg loss: 0.22502684633616568 at epoch 155
epoch =    156/  1000, exp = train
train_iter_loss: 0.43969765305519104
train_iter_loss: 0.14124402403831482
train_iter_loss: 0.10073042660951614
train_iter_loss: 0.254353791475296
train_iter_loss: 0.1466355323791504
train_iter_loss: 0.18472832441329956
train_iter_loss: 0.18625503778457642
train_iter_loss: 0.18679408729076385
train_iter_loss: 0.23144224286079407
train_iter_loss: 0.25604164600372314
train_iter_loss: 0.09679432958364487
train_iter_loss: 0.18175755441188812
train_iter_loss: 0.2702693045139313
train_iter_loss: 0.2553400993347168
train_iter_loss: 0.4420471787452698
train_iter_loss: 0.15250983834266663
train_iter_loss: 0.11455581337213516
train_iter_loss: 0.19679191708564758
train_iter_loss: 0.17056575417518616
train_iter_loss: 0.18759463727474213
train_iter_loss: 0.36464810371398926
train_iter_loss: 0.13808214664459229
train_iter_loss: 0.19945161044597626
train_iter_loss: 0.10431665927171707
train_iter_loss: 0.08981941640377045
train_iter_loss: 0.13230955600738525
train_iter_loss: 0.20758593082427979
train_iter_loss: 0.1588863581418991
train_iter_loss: 0.13610434532165527
train_iter_loss: 0.03891007602214813
train_iter_loss: 0.12118574231863022
train_iter_loss: 0.15855807065963745
train_iter_loss: 0.16186043620109558
train_iter_loss: 0.15559768676757812
train_iter_loss: 0.29904356598854065
train_iter_loss: 0.14313004910945892
train_iter_loss: 0.17654713988304138
train_iter_loss: 0.1637595295906067
train_iter_loss: 0.10401387512683868
train_iter_loss: 0.14894753694534302
train_iter_loss: 0.11609165370464325
train_iter_loss: 0.24155591428279877
train_iter_loss: 0.12946060299873352
train_iter_loss: 0.28423261642456055
train_iter_loss: 0.14366374909877777
train_iter_loss: 0.22171245515346527
train_iter_loss: 0.15731434524059296
train_iter_loss: 0.15305346250534058
train_iter_loss: 0.23428332805633545
train_iter_loss: 0.1680213212966919
train_iter_loss: 0.1263682246208191
train_iter_loss: 0.1768648773431778
train_iter_loss: 0.14536333084106445
train_iter_loss: 0.12226944416761398
train_iter_loss: 0.3664149343967438
train_iter_loss: 0.2622343897819519
train_iter_loss: 0.11245818436145782
train_iter_loss: 0.19667410850524902
train_iter_loss: 0.13300515711307526
train_iter_loss: 0.10723968595266342
train_iter_loss: 0.2833153009414673
train_iter_loss: 0.14578603208065033
train_iter_loss: 0.18281711637973785
train_iter_loss: 0.10667615383863449
train_iter_loss: 0.23433013260364532
train_iter_loss: 0.09431859105825424
train_iter_loss: 0.058640215545892715
train_iter_loss: 0.10858934372663498
train_iter_loss: 0.2357778549194336
train_iter_loss: 0.15124230086803436
train_iter_loss: 0.3073733448982239
train_iter_loss: 0.16510993242263794
train_iter_loss: 0.07655998319387436
train_iter_loss: 0.14044544100761414
train_iter_loss: 0.17439422011375427
train_iter_loss: 0.2267322689294815
train_iter_loss: 0.1399466097354889
train_iter_loss: 0.08393452316522598
train_iter_loss: 0.11425197869539261
train_iter_loss: 0.15901361405849457
train_iter_loss: 0.17253248393535614
train_iter_loss: 0.04401600733399391
train_iter_loss: 0.11067300289869308
train_iter_loss: 0.19128546118736267
train_iter_loss: 0.1719508171081543
train_iter_loss: 0.16513235867023468
train_iter_loss: 0.20363347232341766
train_iter_loss: 0.22574074566364288
train_iter_loss: 0.1506582349538803
train_iter_loss: 0.16508950293064117
train_iter_loss: 0.18293102085590363
train_iter_loss: 0.05471564456820488
train_iter_loss: 0.1451890915632248
train_iter_loss: 0.16254723072052002
train_iter_loss: 0.22882799804210663
train_iter_loss: 0.31900754570961
train_iter_loss: 0.2382383793592453
train_iter_loss: 0.07992468774318695
train_iter_loss: 0.19060824811458588
train_iter_loss: 0.09048273414373398
train loss :0.1766
---------------------
Validation seg loss: 0.226090125678072 at epoch 156
epoch =    157/  1000, exp = train
train_iter_loss: 0.13387267291545868
train_iter_loss: 0.21459293365478516
train_iter_loss: 0.16997754573822021
train_iter_loss: 0.24067309498786926
train_iter_loss: 0.3644084334373474
train_iter_loss: 0.1408931314945221
train_iter_loss: 0.16275273263454437
train_iter_loss: 0.14375033974647522
train_iter_loss: 0.14734956622123718
train_iter_loss: 0.1919858753681183
train_iter_loss: 0.2157944142818451
train_iter_loss: 0.4001912474632263
train_iter_loss: 0.08050870150327682
train_iter_loss: 0.19753634929656982
train_iter_loss: 0.1645829826593399
train_iter_loss: 0.07191035151481628
train_iter_loss: 0.08416907489299774
train_iter_loss: 0.2218250185251236
train_iter_loss: 0.18721747398376465
train_iter_loss: 0.1510184109210968
train_iter_loss: 0.11414915323257446
train_iter_loss: 0.1751376837491989
train_iter_loss: 0.16709360480308533
train_iter_loss: 0.14153270423412323
train_iter_loss: 0.1765269786119461
train_iter_loss: 0.1441197693347931
train_iter_loss: 0.16485971212387085
train_iter_loss: 0.163667693734169
train_iter_loss: 0.1878369301557541
train_iter_loss: 0.14413510262966156
train_iter_loss: 0.07595053315162659
train_iter_loss: 0.2542586922645569
train_iter_loss: 0.19623734056949615
train_iter_loss: 0.16282948851585388
train_iter_loss: 0.1820523589849472
train_iter_loss: 0.17052064836025238
train_iter_loss: 0.15984152257442474
train_iter_loss: 0.12718994915485382
train_iter_loss: 0.16686223447322845
train_iter_loss: 0.1976439356803894
train_iter_loss: 0.25271522998809814
train_iter_loss: 0.13020674884319305
train_iter_loss: 0.2029264271259308
train_iter_loss: 0.33698758482933044
train_iter_loss: 0.14787371456623077
train_iter_loss: 0.19716650247573853
train_iter_loss: 0.10037140548229218
train_iter_loss: 0.20410682260990143
train_iter_loss: 0.270542711019516
train_iter_loss: 0.31445908546447754
train_iter_loss: 0.1963336169719696
train_iter_loss: 0.2666596472263336
train_iter_loss: 0.3228241503238678
train_iter_loss: 0.20452077686786652
train_iter_loss: 0.2997235357761383
train_iter_loss: 0.19041219353675842
train_iter_loss: 0.12735208868980408
train_iter_loss: 0.14246472716331482
train_iter_loss: 0.18706373870372772
train_iter_loss: 0.04721304029226303
train_iter_loss: 0.18673940002918243
train_iter_loss: 0.29052096605300903
train_iter_loss: 0.11254555732011795
train_iter_loss: 0.19840075075626373
train_iter_loss: 0.12371458858251572
train_iter_loss: 0.07501649856567383
train_iter_loss: 0.18021820485591888
train_iter_loss: 0.1898009032011032
train_iter_loss: 0.16963601112365723
train_iter_loss: 0.17804640531539917
train_iter_loss: 0.10118668526411057
train_iter_loss: 0.2110557109117508
train_iter_loss: 0.2425016313791275
train_iter_loss: 0.18958532810211182
train_iter_loss: 0.06046037748456001
train_iter_loss: 0.1251373291015625
train_iter_loss: 0.1925026923418045
train_iter_loss: 0.11651584506034851
train_iter_loss: 0.1594400852918625
train_iter_loss: 0.0939754769206047
train_iter_loss: 0.152676522731781
train_iter_loss: 0.2264537811279297
train_iter_loss: 0.2323494255542755
train_iter_loss: 0.14135633409023285
train_iter_loss: 0.15440082550048828
train_iter_loss: 0.2000938206911087
train_iter_loss: 0.05492943525314331
train_iter_loss: 0.06045468896627426
train_iter_loss: 0.07376248389482498
train_iter_loss: 0.14701488614082336
train_iter_loss: 0.28204697370529175
train_iter_loss: 0.12589670717716217
train_iter_loss: 0.11826083809137344
train_iter_loss: 0.1536087989807129
train_iter_loss: 0.10206151008605957
train_iter_loss: 0.13499495387077332
train_iter_loss: 0.18904723227024078
train_iter_loss: 0.18466831743717194
train_iter_loss: 0.2514752447605133
train_iter_loss: 0.12341127544641495
train loss :0.1758
---------------------
Validation seg loss: 0.2237357321845473 at epoch 157
epoch =    158/  1000, exp = train
train_iter_loss: 0.18667243421077728
train_iter_loss: 0.21775168180465698
train_iter_loss: 0.22674725949764252
train_iter_loss: 0.03417051583528519
train_iter_loss: 0.18893225491046906
train_iter_loss: 0.1819825917482376
train_iter_loss: 0.1773746907711029
train_iter_loss: 0.221530482172966
train_iter_loss: 0.1776185929775238
train_iter_loss: 0.3071790635585785
train_iter_loss: 0.24224478006362915
train_iter_loss: 0.14019204676151276
train_iter_loss: 0.2279970794916153
train_iter_loss: 0.1862216740846634
train_iter_loss: 0.10290724784135818
train_iter_loss: 0.15802401304244995
train_iter_loss: 0.11517997086048126
train_iter_loss: 0.23135024309158325
train_iter_loss: 0.19603180885314941
train_iter_loss: 0.23200276494026184
train_iter_loss: 0.12854446470737457
train_iter_loss: 0.14080213010311127
train_iter_loss: 0.16539877653121948
train_iter_loss: 0.22883129119873047
train_iter_loss: 0.17957326769828796
train_iter_loss: 0.10401170700788498
train_iter_loss: 0.18965022265911102
train_iter_loss: 0.21158070862293243
train_iter_loss: 0.13663779199123383
train_iter_loss: 0.09905463457107544
train_iter_loss: 0.09913448989391327
train_iter_loss: 0.05596610903739929
train_iter_loss: 0.15401539206504822
train_iter_loss: 0.1100974753499031
train_iter_loss: 0.1833781898021698
train_iter_loss: 0.13795311748981476
train_iter_loss: 0.15060195326805115
train_iter_loss: 0.14656604826450348
train_iter_loss: 0.11439579725265503
train_iter_loss: 0.14509108662605286
train_iter_loss: 0.22086165845394135
train_iter_loss: 0.12498852610588074
train_iter_loss: 0.14167512953281403
train_iter_loss: 0.037566035985946655
train_iter_loss: 0.19395603239536285
train_iter_loss: 0.17715869843959808
train_iter_loss: 0.33934199810028076
train_iter_loss: 0.18233196437358856
train_iter_loss: 0.2008579522371292
train_iter_loss: 0.13093507289886475
train_iter_loss: 0.12179818749427795
train_iter_loss: 0.32325756549835205
train_iter_loss: 0.10800247639417648
train_iter_loss: 0.4478782117366791
train_iter_loss: 0.21250778436660767
train_iter_loss: 0.3248509168624878
train_iter_loss: 0.24677567183971405
train_iter_loss: 0.15681317448616028
train_iter_loss: 0.13445651531219482
train_iter_loss: 0.25138360261917114
train_iter_loss: 0.13056842982769012
train_iter_loss: 0.1276526004076004
train_iter_loss: 0.19702070951461792
train_iter_loss: 0.128939688205719
train_iter_loss: 0.20193016529083252
train_iter_loss: 0.16751259565353394
train_iter_loss: 0.2575315237045288
train_iter_loss: 0.11538496613502502
train_iter_loss: 0.24356406927108765
train_iter_loss: 0.13061967492103577
train_iter_loss: 0.22027134895324707
train_iter_loss: 0.20708182454109192
train_iter_loss: 0.12185588479042053
train_iter_loss: 0.16030322015285492
train_iter_loss: 0.16831845045089722
train_iter_loss: 0.286902517080307
train_iter_loss: 0.1891513168811798
train_iter_loss: 0.1504656821489334
train_iter_loss: 0.182591512799263
train_iter_loss: 0.2360803186893463
train_iter_loss: 0.1331484317779541
train_iter_loss: 0.21180441975593567
train_iter_loss: 0.1613427847623825
train_iter_loss: 0.0707630142569542
train_iter_loss: 0.23549722135066986
train_iter_loss: 0.15688422322273254
train_iter_loss: 0.1563584804534912
train_iter_loss: 0.16805559396743774
train_iter_loss: 0.11725844442844391
train_iter_loss: 0.1431557834148407
train_iter_loss: 0.20723052322864532
train_iter_loss: 0.19025559723377228
train_iter_loss: 0.17416846752166748
train_iter_loss: 0.0993323102593422
train_iter_loss: 0.224533811211586
train_iter_loss: 0.17148014903068542
train_iter_loss: 0.18468184769153595
train_iter_loss: 0.26586806774139404
train_iter_loss: 0.16333645582199097
train_iter_loss: 0.13537032902240753
train loss :0.1788
---------------------
Validation seg loss: 0.23016813821774326 at epoch 158
epoch =    159/  1000, exp = train
train_iter_loss: 0.2204771190881729
train_iter_loss: 0.1964593529701233
train_iter_loss: 0.25974810123443604
train_iter_loss: 0.24444735050201416
train_iter_loss: 0.20913128554821014
train_iter_loss: 0.161860853433609
train_iter_loss: 0.11532710492610931
train_iter_loss: 0.13923291862010956
train_iter_loss: 0.2965526282787323
train_iter_loss: 0.13755875825881958
train_iter_loss: 0.13516353070735931
train_iter_loss: 0.28490591049194336
train_iter_loss: 0.14746959507465363
train_iter_loss: 0.19793932139873505
train_iter_loss: 0.16470524668693542
train_iter_loss: 0.1640252023935318
train_iter_loss: 0.15561090409755707
train_iter_loss: 0.2588645815849304
train_iter_loss: 0.10790082067251205
train_iter_loss: 0.23444104194641113
train_iter_loss: 0.18030880391597748
train_iter_loss: 0.1792697012424469
train_iter_loss: 0.17210347950458527
train_iter_loss: 0.07640288025140762
train_iter_loss: 0.1966778039932251
train_iter_loss: 0.08969845622777939
train_iter_loss: 0.17113067209720612
train_iter_loss: 0.13643673062324524
train_iter_loss: 0.17738768458366394
train_iter_loss: 0.22112224996089935
train_iter_loss: 0.2607784867286682
train_iter_loss: 0.15928007662296295
train_iter_loss: 0.14659449458122253
train_iter_loss: 0.19478189945220947
train_iter_loss: 0.15975318849086761
train_iter_loss: 0.1725596785545349
train_iter_loss: 0.19127734005451202
train_iter_loss: 0.15497949719429016
train_iter_loss: 0.09964755177497864
train_iter_loss: 0.16842608153820038
train_iter_loss: 0.10903867334127426
train_iter_loss: 0.12008187919855118
train_iter_loss: 0.14033542573451996
train_iter_loss: 0.24933241307735443
train_iter_loss: 0.20878632366657257
train_iter_loss: 0.13215942680835724
train_iter_loss: 0.12388456612825394
train_iter_loss: 0.18064050376415253
train_iter_loss: 0.28094154596328735
train_iter_loss: 0.1991921365261078
train_iter_loss: 0.198946014046669
train_iter_loss: 0.25984421372413635
train_iter_loss: 0.2375437468290329
train_iter_loss: 0.20378464460372925
train_iter_loss: 0.11153236031532288
train_iter_loss: 0.06438706815242767
train_iter_loss: 0.22193901240825653
train_iter_loss: 0.10837021470069885
train_iter_loss: 0.19169855117797852
train_iter_loss: 0.23988641798496246
train_iter_loss: 0.10296289622783661
train_iter_loss: 0.2851015329360962
train_iter_loss: 0.08129572868347168
train_iter_loss: 0.10526280105113983
train_iter_loss: 0.18732012808322906
train_iter_loss: 0.1568569391965866
train_iter_loss: 0.044977933168411255
train_iter_loss: 0.17143987119197845
train_iter_loss: 0.17475523054599762
train_iter_loss: 0.30419468879699707
train_iter_loss: 0.0927560105919838
train_iter_loss: 0.20482316613197327
train_iter_loss: 0.23371927440166473
train_iter_loss: 0.25825235247612
train_iter_loss: 0.22452040016651154
train_iter_loss: 0.052886947989463806
train_iter_loss: 0.1853557974100113
train_iter_loss: 0.11972762644290924
train_iter_loss: 0.21855872869491577
train_iter_loss: 0.20349910855293274
train_iter_loss: 0.08990390598773956
train_iter_loss: 0.22560010850429535
train_iter_loss: 0.14812567830085754
train_iter_loss: 0.08569438755512238
train_iter_loss: 0.17776142060756683
train_iter_loss: 0.1862439513206482
train_iter_loss: 0.13181860744953156
train_iter_loss: 0.17212960124015808
train_iter_loss: 0.09633335471153259
train_iter_loss: 0.07890579849481583
train_iter_loss: 0.1371465027332306
train_iter_loss: 0.07543107122182846
train_iter_loss: 0.24610158801078796
train_iter_loss: 0.1152050718665123
train_iter_loss: 0.09684958308935165
train_iter_loss: 0.19815322756767273
train_iter_loss: 0.08074887841939926
train_iter_loss: 0.11689862608909607
train_iter_loss: 0.14977824687957764
train_iter_loss: 0.15728886425495148
train loss :0.1708
---------------------
Validation seg loss: 0.22279935048998528 at epoch 159
********************
best_val_epoch_loss:  0.22279935048998528
MODEL UPDATED
epoch =    160/  1000, exp = train
train_iter_loss: 0.2666178345680237
train_iter_loss: 0.2218443751335144
train_iter_loss: 0.11221247911453247
train_iter_loss: 0.26039382815361023
train_iter_loss: 0.19750791788101196
train_iter_loss: 0.14098182320594788
train_iter_loss: 0.21103152632713318
train_iter_loss: 0.20103591680526733
train_iter_loss: 0.1443759948015213
train_iter_loss: 0.19317403435707092
train_iter_loss: 0.24681062996387482
train_iter_loss: 0.16199269890785217
train_iter_loss: 0.10502176731824875
train_iter_loss: 0.17670004069805145
train_iter_loss: 0.198533833026886
train_iter_loss: 0.22950974106788635
train_iter_loss: 0.12403655797243118
train_iter_loss: 0.17743751406669617
train_iter_loss: 0.13152214884757996
train_iter_loss: 0.1216086745262146
train_iter_loss: 0.20379571616649628
train_iter_loss: 0.13101710379123688
train_iter_loss: 0.12644930183887482
train_iter_loss: 0.2328001856803894
train_iter_loss: 0.2091696858406067
train_iter_loss: 0.14000217616558075
train_iter_loss: 0.08188686519861221
train_iter_loss: 0.22678697109222412
train_iter_loss: 0.14365646243095398
train_iter_loss: 0.20031017065048218
train_iter_loss: 0.19102545082569122
train_iter_loss: 0.15925942361354828
train_iter_loss: 0.26266711950302124
train_iter_loss: 0.18377244472503662
train_iter_loss: 0.20151884853839874
train_iter_loss: 0.2242344468832016
train_iter_loss: 0.21904116868972778
train_iter_loss: 0.1495131403207779
train_iter_loss: 0.15289172530174255
train_iter_loss: 0.12191841006278992
train_iter_loss: 0.2058326154947281
train_iter_loss: 0.12817956507205963
train_iter_loss: 0.19332419335842133
train_iter_loss: 0.16033990681171417
train_iter_loss: 0.09169138222932816
train_iter_loss: 0.10961926728487015
train_iter_loss: 0.14767037332057953
train_iter_loss: 0.39129844307899475
train_iter_loss: 0.11587130278348923
train_iter_loss: 0.1992083340883255
train_iter_loss: 0.141574889421463
train_iter_loss: 0.22891317307949066
train_iter_loss: 0.07086355239152908
train_iter_loss: 0.15980225801467896
train_iter_loss: 0.22361759841442108
train_iter_loss: 0.17366506159305573
train_iter_loss: 0.14844739437103271
train_iter_loss: 0.20655913650989532
train_iter_loss: 0.11095158755779266
train_iter_loss: 0.14496353268623352
train_iter_loss: 0.22603468596935272
train_iter_loss: 0.14367298781871796
train_iter_loss: 0.07128879427909851
train_iter_loss: 0.19594943523406982
train_iter_loss: 0.17663483321666718
train_iter_loss: 0.13488107919692993
train_iter_loss: 0.1263893097639084
train_iter_loss: 0.20422986149787903
train_iter_loss: 0.10949599742889404
train_iter_loss: 0.07805871218442917
train_iter_loss: 0.23196375370025635
train_iter_loss: 0.1639912873506546
train_iter_loss: 0.22986391186714172
train_iter_loss: 0.11807963997125626
train_iter_loss: 0.28312739729881287
train_iter_loss: 0.31117117404937744
train_iter_loss: 0.14922204613685608
train_iter_loss: 0.1489638388156891
train_iter_loss: 0.22811679542064667
train_iter_loss: 0.36980140209198
train_iter_loss: 0.14058108627796173
train_iter_loss: 0.13349571824073792
train_iter_loss: 0.10908707231283188
train_iter_loss: 0.06781122088432312
train_iter_loss: 0.08049789071083069
train_iter_loss: 0.22348469495773315
train_iter_loss: 0.10384779423475266
train_iter_loss: 0.14834681153297424
train_iter_loss: 0.16667024791240692
train_iter_loss: 0.22914692759513855
train_iter_loss: 0.12744931876659393
train_iter_loss: 0.3533879816532135
train_iter_loss: 0.16264158487319946
train_iter_loss: 0.12453820556402206
train_iter_loss: 0.21927601099014282
train_iter_loss: 0.06160302460193634
train_iter_loss: 0.24349206686019897
train_iter_loss: 0.19404152035713196
train_iter_loss: 0.11788675934076309
train_iter_loss: 0.17960114777088165
train loss :0.1760
---------------------
Validation seg loss: 0.22404836420461816 at epoch 160
epoch =    161/  1000, exp = train
train_iter_loss: 0.1318303495645523
train_iter_loss: 0.15058927237987518
train_iter_loss: 0.2025335282087326
train_iter_loss: 0.15556064248085022
train_iter_loss: 0.12409745901823044
train_iter_loss: 0.20422635972499847
train_iter_loss: 0.11039260029792786
train_iter_loss: 0.11022359877824783
train_iter_loss: 0.28013157844543457
train_iter_loss: 0.1375996321439743
train_iter_loss: 0.13985231518745422
train_iter_loss: 0.3339497745037079
train_iter_loss: 0.132780984044075
train_iter_loss: 0.13794663548469543
train_iter_loss: 0.12014074623584747
train_iter_loss: 0.14937283098697662
train_iter_loss: 0.1341947764158249
train_iter_loss: 0.08505180478096008
train_iter_loss: 0.14842957258224487
train_iter_loss: 0.20421598851680756
train_iter_loss: 0.15585093200206757
train_iter_loss: 0.12926805019378662
train_iter_loss: 0.2521802484989166
train_iter_loss: 0.21518227458000183
train_iter_loss: 0.15267615020275116
train_iter_loss: 0.14668817818164825
train_iter_loss: 0.09200996160507202
train_iter_loss: 0.10609998553991318
train_iter_loss: 0.17323139309883118
train_iter_loss: 0.20854274928569794
train_iter_loss: 0.15044376254081726
train_iter_loss: 0.30148565769195557
train_iter_loss: 0.13627870380878448
train_iter_loss: 0.17322885990142822
train_iter_loss: 0.1828094720840454
train_iter_loss: 0.1781701296567917
train_iter_loss: 0.1529851257801056
train_iter_loss: 0.16130492091178894
train_iter_loss: 0.15882237255573273
train_iter_loss: 0.24495084583759308
train_iter_loss: 0.11158523708581924
train_iter_loss: 0.14330880343914032
train_iter_loss: 0.13574691116809845
train_iter_loss: 0.24209363758563995
train_iter_loss: 0.10959109663963318
train_iter_loss: 0.17581471800804138
train_iter_loss: 0.11216055601835251
train_iter_loss: 0.2702631950378418
train_iter_loss: 0.12550465762615204
train_iter_loss: 0.1388404667377472
train_iter_loss: 0.19511836767196655
train_iter_loss: 0.15239183604717255
train_iter_loss: 0.14655065536499023
train_iter_loss: 0.21294710040092468
train_iter_loss: 0.26770949363708496
train_iter_loss: 0.18665887415409088
train_iter_loss: 0.2396259903907776
train_iter_loss: 0.10235358774662018
train_iter_loss: 0.11232747882604599
train_iter_loss: 0.23425254225730896
train_iter_loss: 0.25865283608436584
train_iter_loss: 0.11417561769485474
train_iter_loss: 0.31759607791900635
train_iter_loss: 0.23324406147003174
train_iter_loss: 0.08755609393119812
train_iter_loss: 0.25164446234703064
train_iter_loss: 0.13326098024845123
train_iter_loss: 0.20795094966888428
train_iter_loss: 0.1659603863954544
train_iter_loss: 0.20397141575813293
train_iter_loss: 0.10708928108215332
train_iter_loss: 0.14733146131038666
train_iter_loss: 0.17547282576560974
train_iter_loss: 0.1546393185853958
train_iter_loss: 0.17436128854751587
train_iter_loss: 0.26676154136657715
train_iter_loss: 0.10011083632707596
train_iter_loss: 0.20368178188800812
train_iter_loss: 0.15242576599121094
train_iter_loss: 0.05653945729136467
train_iter_loss: 0.19459331035614014
train_iter_loss: 0.10754100233316422
train_iter_loss: 0.22111278772354126
train_iter_loss: 0.1904209703207016
train_iter_loss: 0.11787132918834686
train_iter_loss: 0.13556650280952454
train_iter_loss: 0.08883202075958252
train_iter_loss: 0.19812393188476562
train_iter_loss: 0.1540909707546234
train_iter_loss: 0.14787693321704865
train_iter_loss: 0.1710325926542282
train_iter_loss: 0.25558972358703613
train_iter_loss: 0.1489754170179367
train_iter_loss: 0.05387750640511513
train_iter_loss: 0.1361985206604004
train_iter_loss: 0.22655145823955536
train_iter_loss: 0.18665051460266113
train_iter_loss: 0.17394080758094788
train_iter_loss: 0.20223522186279297
train_iter_loss: 0.11402542889118195
train loss :0.1699
---------------------
Validation seg loss: 0.22522273807311957 at epoch 161
epoch =    162/  1000, exp = train
train_iter_loss: 0.08976278454065323
train_iter_loss: 0.11866825073957443
train_iter_loss: 0.15379118919372559
train_iter_loss: 0.1505562961101532
train_iter_loss: 0.09982694685459137
train_iter_loss: 0.208628311753273
train_iter_loss: 0.21871522068977356
train_iter_loss: 0.1494053304195404
train_iter_loss: 0.06121787056326866
train_iter_loss: 0.14728906750679016
train_iter_loss: 0.303849995136261
train_iter_loss: 0.14584100246429443
train_iter_loss: 0.0666830763220787
train_iter_loss: 0.15462026000022888
train_iter_loss: 0.2386048436164856
train_iter_loss: 0.25142884254455566
train_iter_loss: 0.15499979257583618
train_iter_loss: 0.13478174805641174
train_iter_loss: 0.10506905615329742
train_iter_loss: 0.30574560165405273
train_iter_loss: 0.2592945992946625
train_iter_loss: 0.2879410684108734
train_iter_loss: 0.14094816148281097
train_iter_loss: 0.20250679552555084
train_iter_loss: 0.1232440397143364
train_iter_loss: 0.18893496692180634
train_iter_loss: 0.2736675441265106
train_iter_loss: 0.1410946249961853
train_iter_loss: 0.32355913519859314
train_iter_loss: 0.0921446830034256
train_iter_loss: 0.19711747765541077
train_iter_loss: 0.4119702875614166
train_iter_loss: 0.11705222725868225
train_iter_loss: 0.15074098110198975
train_iter_loss: 0.07405844330787659
train_iter_loss: 0.18646551668643951
train_iter_loss: 0.3183193504810333
train_iter_loss: 0.2753247320652008
train_iter_loss: 0.19649867713451385
train_iter_loss: 0.18105506896972656
train_iter_loss: 0.16579510271549225
train_iter_loss: 0.14203952252864838
train_iter_loss: 0.06302324682474136
train_iter_loss: 0.35900962352752686
train_iter_loss: 0.16820739209651947
train_iter_loss: 0.15493544936180115
train_iter_loss: 0.10328526049852371
train_iter_loss: 0.2666501998901367
train_iter_loss: 0.31113937497138977
train_iter_loss: 0.13523927330970764
train_iter_loss: 0.19749106466770172
train_iter_loss: 0.1443217396736145
train_iter_loss: 0.2077038288116455
train_iter_loss: 0.17631429433822632
train_iter_loss: 0.13257165253162384
train_iter_loss: 0.25108441710472107
train_iter_loss: 0.15754374861717224
train_iter_loss: 0.2125643938779831
train_iter_loss: 0.19834287464618683
train_iter_loss: 0.0905904620885849
train_iter_loss: 0.16144360601902008
train_iter_loss: 0.12186668813228607
train_iter_loss: 0.14528357982635498
train_iter_loss: 0.11210053414106369
train_iter_loss: 0.11317265778779984
train_iter_loss: 0.11223835498094559
train_iter_loss: 0.1432424634695053
train_iter_loss: 0.18562531471252441
train_iter_loss: 0.06564560532569885
train_iter_loss: 0.16019770503044128
train_iter_loss: 0.17596518993377686
train_iter_loss: 0.12101039290428162
train_iter_loss: 0.2358393520116806
train_iter_loss: 0.2545228600502014
train_iter_loss: 0.12619343400001526
train_iter_loss: 0.1449267417192459
train_iter_loss: 0.17610573768615723
train_iter_loss: 0.12659485638141632
train_iter_loss: 0.35788145661354065
train_iter_loss: 0.1063300222158432
train_iter_loss: 0.2443520575761795
train_iter_loss: 0.1302718222141266
train_iter_loss: 0.12263162434101105
train_iter_loss: 0.09019247442483902
train_iter_loss: 0.1862553507089615
train_iter_loss: 0.17451435327529907
train_iter_loss: 0.1410820484161377
train_iter_loss: 0.189989373087883
train_iter_loss: 0.17567838728427887
train_iter_loss: 0.28615692257881165
train_iter_loss: 0.3276817202568054
train_iter_loss: 0.16942235827445984
train_iter_loss: 0.2625648081302643
train_iter_loss: 0.22503359615802765
train_iter_loss: 0.15555253624916077
train_iter_loss: 0.12264690548181534
train_iter_loss: 0.13617311418056488
train_iter_loss: 0.12000348418951035
train_iter_loss: 0.21365667879581451
train_iter_loss: 0.22230662405490875
train loss :0.1805
---------------------
Validation seg loss: 0.22489644160037334 at epoch 162
epoch =    163/  1000, exp = train
train_iter_loss: 0.14452531933784485
train_iter_loss: 0.18957145512104034
train_iter_loss: 0.16238366067409515
train_iter_loss: 0.06135860085487366
train_iter_loss: 0.24421830475330353
train_iter_loss: 0.14899469912052155
train_iter_loss: 0.18283021450042725
train_iter_loss: 0.19326671957969666
train_iter_loss: 0.21987730264663696
train_iter_loss: 0.14737574756145477
train_iter_loss: 0.050730250775814056
train_iter_loss: 0.17736394703388214
train_iter_loss: 0.1861470341682434
train_iter_loss: 0.14287440478801727
train_iter_loss: 0.194607213139534
train_iter_loss: 0.1437586098909378
train_iter_loss: 0.14234758913516998
train_iter_loss: 0.1729746162891388
train_iter_loss: 0.14434660971164703
train_iter_loss: 0.1960446685552597
train_iter_loss: 0.2270389348268509
train_iter_loss: 0.14835241436958313
train_iter_loss: 0.23130449652671814
train_iter_loss: 0.20860277116298676
train_iter_loss: 0.0842369943857193
train_iter_loss: 0.1872168779373169
train_iter_loss: 0.09418343752622604
train_iter_loss: 0.172511026263237
train_iter_loss: 0.09254487603902817
train_iter_loss: 0.162459596991539
train_iter_loss: 0.2552943527698517
train_iter_loss: 0.22961050271987915
train_iter_loss: 0.29538121819496155
train_iter_loss: 0.13880693912506104
train_iter_loss: 0.2483617216348648
train_iter_loss: 0.2680355906486511
train_iter_loss: 0.08000293374061584
train_iter_loss: 0.30535224080085754
train_iter_loss: 0.18253394961357117
train_iter_loss: 0.22445519268512726
train_iter_loss: 0.11029264330863953
train_iter_loss: 0.2446407973766327
train_iter_loss: 0.14845024049282074
train_iter_loss: 0.1386766880750656
train_iter_loss: 0.22726106643676758
train_iter_loss: 0.16076353192329407
train_iter_loss: 0.05553874373435974
train_iter_loss: 0.17926618456840515
train_iter_loss: 0.19166052341461182
train_iter_loss: 0.16974741220474243
train_iter_loss: 0.047912538051605225
train_iter_loss: 0.1297852098941803
train_iter_loss: 0.1332089602947235
train_iter_loss: 0.18718625605106354
train_iter_loss: 0.15037113428115845
train_iter_loss: 0.06721816956996918
train_iter_loss: 0.12441783398389816
train_iter_loss: 0.08935797214508057
train_iter_loss: 0.2802234888076782
train_iter_loss: 0.30677077174186707
train_iter_loss: 0.11636506021022797
train_iter_loss: 0.19956651329994202
train_iter_loss: 0.20645025372505188
train_iter_loss: 0.0880471020936966
train_iter_loss: 0.13620270788669586
train_iter_loss: 0.13001097738742828
train_iter_loss: 0.20589423179626465
train_iter_loss: 0.13738656044006348
train_iter_loss: 0.1278994232416153
train_iter_loss: 0.1650419980287552
train_iter_loss: 0.2131584733724594
train_iter_loss: 0.13298168778419495
train_iter_loss: 0.20663829147815704
train_iter_loss: 0.1550304889678955
train_iter_loss: 0.168423131108284
train_iter_loss: 0.15664497017860413
train_iter_loss: 0.19418378174304962
train_iter_loss: 0.183026522397995
train_iter_loss: 0.1425948590040207
train_iter_loss: 0.17844818532466888
train_iter_loss: 0.09194818139076233
train_iter_loss: 0.133578360080719
train_iter_loss: 0.20370952785015106
train_iter_loss: 0.23810990154743195
train_iter_loss: 0.2169335037469864
train_iter_loss: 0.11770788580179214
train_iter_loss: 0.27067551016807556
train_iter_loss: 0.14667102694511414
train_iter_loss: 0.16737021505832672
train_iter_loss: 0.16733650863170624
train_iter_loss: 0.053822316229343414
train_iter_loss: 0.2239726334810257
train_iter_loss: 0.19268189370632172
train_iter_loss: 0.19101029634475708
train_iter_loss: 0.0929400846362114
train_iter_loss: 0.16722795367240906
train_iter_loss: 0.18058843910694122
train_iter_loss: 0.1558697670698166
train_iter_loss: 0.20037522912025452
train_iter_loss: 0.18892447650432587
train loss :0.1705
---------------------
Validation seg loss: 0.22477817550336696 at epoch 163
epoch =    164/  1000, exp = train
train_iter_loss: 0.2610991597175598
train_iter_loss: 0.14399200677871704
train_iter_loss: 0.10346903651952744
train_iter_loss: 0.28437134623527527
train_iter_loss: 0.19519339501857758
train_iter_loss: 0.17777512967586517
train_iter_loss: 0.16923151910305023
train_iter_loss: 0.18545201420783997
train_iter_loss: 0.10868191719055176
train_iter_loss: 0.11639794707298279
train_iter_loss: 0.17363490164279938
train_iter_loss: 0.19695298373699188
train_iter_loss: 0.0982457622885704
train_iter_loss: 0.2626301348209381
train_iter_loss: 0.10353869944810867
train_iter_loss: 0.35408440232276917
train_iter_loss: 0.10932161659002304
train_iter_loss: 0.25860917568206787
train_iter_loss: 0.09782657027244568
train_iter_loss: 0.18324275314807892
train_iter_loss: 0.13978376984596252
train_iter_loss: 0.16411766409873962
train_iter_loss: 0.18377931416034698
train_iter_loss: 0.19900962710380554
train_iter_loss: 0.07234357297420502
train_iter_loss: 0.4588201642036438
train_iter_loss: 0.158024862408638
train_iter_loss: 0.2940780818462372
train_iter_loss: 0.2600920796394348
train_iter_loss: 0.20138677954673767
train_iter_loss: 0.14229662716388702
train_iter_loss: 0.12653003633022308
train_iter_loss: 0.15374429523944855
train_iter_loss: 0.12541188299655914
train_iter_loss: 0.15439988672733307
train_iter_loss: 0.11743921786546707
train_iter_loss: 0.14094293117523193
train_iter_loss: 0.21137985587120056
train_iter_loss: 0.10987289994955063
train_iter_loss: 0.31666845083236694
train_iter_loss: 0.14768223464488983
train_iter_loss: 0.27101820707321167
train_iter_loss: 0.07864568382501602
train_iter_loss: 0.1258554905653
train_iter_loss: 0.11196664720773697
train_iter_loss: 0.20549054443836212
train_iter_loss: 0.26544684171676636
train_iter_loss: 0.16234925389289856
train_iter_loss: 0.10675154626369476
train_iter_loss: 0.1038721427321434
train_iter_loss: 0.2212025225162506
train_iter_loss: 0.08284865319728851
train_iter_loss: 0.19256845116615295
train_iter_loss: 0.13030646741390228
train_iter_loss: 0.124158576130867
train_iter_loss: 0.22046028077602386
train_iter_loss: 0.18411070108413696
train_iter_loss: 0.09833329916000366
train_iter_loss: 0.19584286212921143
train_iter_loss: 0.1493406891822815
train_iter_loss: 0.19414947926998138
train_iter_loss: 0.1812768429517746
train_iter_loss: 0.23188285529613495
train_iter_loss: 0.14611054956912994
train_iter_loss: 0.14242935180664062
train_iter_loss: 0.10617081820964813
train_iter_loss: 0.12064475566148758
train_iter_loss: 0.4208552837371826
train_iter_loss: 0.140701025724411
train_iter_loss: 0.1355873942375183
train_iter_loss: 0.1113748550415039
train_iter_loss: 0.13966020941734314
train_iter_loss: 0.15178310871124268
train_iter_loss: 0.12244654446840286
train_iter_loss: 0.16877315938472748
train_iter_loss: 0.16526639461517334
train_iter_loss: 0.22893771529197693
train_iter_loss: 0.12628577649593353
train_iter_loss: 0.20869463682174683
train_iter_loss: 0.21232537925243378
train_iter_loss: 0.19231681525707245
train_iter_loss: 0.1821405589580536
train_iter_loss: 0.08016126602888107
train_iter_loss: 0.10268823057413101
train_iter_loss: 0.13703477382659912
train_iter_loss: 0.1131887435913086
train_iter_loss: 0.10869993269443512
train_iter_loss: 0.08999386429786682
train_iter_loss: 0.1338299661874771
train_iter_loss: 0.27193284034729004
train_iter_loss: 0.12063267081975937
train_iter_loss: 0.11328895390033722
train_iter_loss: 0.16304630041122437
train_iter_loss: 0.13300418853759766
train_iter_loss: 0.27092403173446655
train_iter_loss: 0.11976498365402222
train_iter_loss: 0.16043590009212494
train_iter_loss: 0.1996389925479889
train_iter_loss: 0.1487586349248886
train_iter_loss: 0.14104367792606354
train loss :0.1710
---------------------
Validation seg loss: 0.222366341011915 at epoch 164
********************
best_val_epoch_loss:  0.222366341011915
MODEL UPDATED
epoch =    165/  1000, exp = train
train_iter_loss: 0.18198321759700775
train_iter_loss: 0.14109598100185394
train_iter_loss: 0.10937812924385071
train_iter_loss: 0.1389102041721344
train_iter_loss: 0.14754509925842285
train_iter_loss: 0.2225695699453354
train_iter_loss: 0.2876485288143158
train_iter_loss: 0.14005246758460999
train_iter_loss: 0.18085096776485443
train_iter_loss: 0.11591510474681854
train_iter_loss: 0.20230276882648468
train_iter_loss: 0.168686643242836
train_iter_loss: 0.17056457698345184
train_iter_loss: 0.17270469665527344
train_iter_loss: 0.18956853449344635
train_iter_loss: 0.13737764954566956
train_iter_loss: 0.09999951720237732
train_iter_loss: 0.23642553389072418
train_iter_loss: 0.12266127020120621
train_iter_loss: 0.14739841222763062
train_iter_loss: 0.14294691383838654
train_iter_loss: 0.12191997468471527
train_iter_loss: 0.173312246799469
train_iter_loss: 0.19533494114875793
train_iter_loss: 0.13882699608802795
train_iter_loss: 0.2639223039150238
train_iter_loss: 0.16646815836429596
train_iter_loss: 0.21351096034049988
train_iter_loss: 0.11737174540758133
train_iter_loss: 0.19627131521701813
train_iter_loss: 0.18660776317119598
train_iter_loss: 0.12429258972406387
train_iter_loss: 0.11042681336402893
train_iter_loss: 0.12866705656051636
train_iter_loss: 0.059609733521938324
train_iter_loss: 0.14234419167041779
train_iter_loss: 0.16822728514671326
train_iter_loss: 0.2238965630531311
train_iter_loss: 0.13999976217746735
train_iter_loss: 0.14551609754562378
train_iter_loss: 0.24079371988773346
train_iter_loss: 0.17508918046951294
train_iter_loss: 0.29621291160583496
train_iter_loss: 0.21629977226257324
train_iter_loss: 0.14840105175971985
train_iter_loss: 0.1253320723772049
train_iter_loss: 0.07493393123149872
train_iter_loss: 0.12666913866996765
train_iter_loss: 0.1969151347875595
train_iter_loss: 0.20726552605628967
train_iter_loss: 0.10118146240711212
train_iter_loss: 0.11437644809484482
train_iter_loss: 0.12053687125444412
train_iter_loss: 0.17336708307266235
train_iter_loss: 0.09107235819101334
train_iter_loss: 0.12462018430233002
train_iter_loss: 0.12173446267843246
train_iter_loss: 0.09632544964551926
train_iter_loss: 0.1713722199201584
train_iter_loss: 0.30401620268821716
train_iter_loss: 0.17109322547912598
train_iter_loss: 0.10019604861736298
train_iter_loss: 0.15763528645038605
train_iter_loss: 0.20657753944396973
train_iter_loss: 0.1014249250292778
train_iter_loss: 0.39517635107040405
train_iter_loss: 0.13324923813343048
train_iter_loss: 0.11996094137430191
train_iter_loss: 0.26911261677742004
train_iter_loss: 0.12575064599514008
train_iter_loss: 0.13831250369548798
train_iter_loss: 0.29707422852516174
train_iter_loss: 0.18123407661914825
train_iter_loss: 0.17261989414691925
train_iter_loss: 0.11599431186914444
train_iter_loss: 0.2696273922920227
train_iter_loss: 0.15223240852355957
train_iter_loss: 0.18163426220417023
train_iter_loss: 0.16960369050502777
train_iter_loss: 0.22157351672649384
train_iter_loss: 0.19536800682544708
train_iter_loss: 0.1368415504693985
train_iter_loss: 0.13935595750808716
train_iter_loss: 0.22119837999343872
train_iter_loss: 0.2904752492904663
train_iter_loss: 0.24364930391311646
train_iter_loss: 0.23531803488731384
train_iter_loss: 0.18967317044734955
train_iter_loss: 0.22600015997886658
train_iter_loss: 0.15792743861675262
train_iter_loss: 0.17882852256298065
train_iter_loss: 0.1048404797911644
train_iter_loss: 0.2056502252817154
train_iter_loss: 0.13524241745471954
train_iter_loss: 0.20906101167201996
train_iter_loss: 0.17765717208385468
train_iter_loss: 0.21340908110141754
train_iter_loss: 0.14736568927764893
train_iter_loss: 0.1414455622434616
train_iter_loss: 0.2027280032634735
train loss :0.1730
---------------------
Validation seg loss: 0.22648656307631787 at epoch 165
epoch =    166/  1000, exp = train
train_iter_loss: 0.16891583800315857
train_iter_loss: 0.15358425676822662
train_iter_loss: 0.09832381457090378
train_iter_loss: 0.42076948285102844
train_iter_loss: 0.2025461494922638
train_iter_loss: 0.17975731194019318
train_iter_loss: 0.11305325478315353
train_iter_loss: 0.37979331612586975
train_iter_loss: 0.21475310623645782
train_iter_loss: 0.13720685243606567
train_iter_loss: 0.21515706181526184
train_iter_loss: 0.11959901452064514
train_iter_loss: 0.13928541541099548
train_iter_loss: 0.14617209136486053
train_iter_loss: 0.21572613716125488
train_iter_loss: 0.1752876490354538
train_iter_loss: 0.15799447894096375
train_iter_loss: 0.12914155423641205
train_iter_loss: 0.11282633990049362
train_iter_loss: 0.21099600195884705
train_iter_loss: 0.1467234492301941
train_iter_loss: 0.16073793172836304
train_iter_loss: 0.1815258413553238
train_iter_loss: 0.14445564150810242
train_iter_loss: 0.15330182015895844
train_iter_loss: 0.1842518001794815
train_iter_loss: 0.07772157341241837
train_iter_loss: 0.13103681802749634
train_iter_loss: 0.17735017836093903
train_iter_loss: 0.1529078185558319
train_iter_loss: 0.16616632044315338
train_iter_loss: 0.24838806688785553
train_iter_loss: 0.28320232033729553
train_iter_loss: 0.17880241572856903
train_iter_loss: 0.17615166306495667
train_iter_loss: 0.3129170536994934
train_iter_loss: 0.18851381540298462
train_iter_loss: 0.1231970340013504
train_iter_loss: 0.09655239433050156
train_iter_loss: 0.11736483126878738
train_iter_loss: 0.0999889001250267
train_iter_loss: 0.20572036504745483
train_iter_loss: 0.1922549307346344
train_iter_loss: 0.27328652143478394
train_iter_loss: 0.07569684833288193
train_iter_loss: 0.16309964656829834
train_iter_loss: 0.09415673464536667
train_iter_loss: 0.20120243728160858
train_iter_loss: 0.15040834248065948
train_iter_loss: 0.16309665143489838
train_iter_loss: 0.1300496757030487
train_iter_loss: 0.1847590059041977
train_iter_loss: 0.20432248711585999
train_iter_loss: 0.1910323053598404
train_iter_loss: 0.2584761083126068
train_iter_loss: 0.17584086954593658
train_iter_loss: 0.3099345862865448
train_iter_loss: 0.15105143189430237
train_iter_loss: 0.06467977911233902
train_iter_loss: 0.16082288324832916
train_iter_loss: 0.1736333966255188
train_iter_loss: 0.12145856022834778
train_iter_loss: 0.11245938390493393
train_iter_loss: 0.11707417666912079
train_iter_loss: 0.15277113020420074
train_iter_loss: 0.21627020835876465
train_iter_loss: 0.3096249997615814
train_iter_loss: 0.15695394575595856
train_iter_loss: 0.10396748036146164
train_iter_loss: 0.1658105105161667
train_iter_loss: 0.1528431922197342
train_iter_loss: 0.1792004257440567
train_iter_loss: 0.14315126836299896
train_iter_loss: 0.19261528551578522
train_iter_loss: 0.17995718121528625
train_iter_loss: 0.23535820841789246
train_iter_loss: 0.18191151320934296
train_iter_loss: 0.13205203413963318
train_iter_loss: 0.18827727437019348
train_iter_loss: 0.10694430768489838
train_iter_loss: 0.14740000665187836
train_iter_loss: 0.3012266159057617
train_iter_loss: 0.15439310669898987
train_iter_loss: 0.18768681585788727
train_iter_loss: 0.22272136807441711
train_iter_loss: 0.2405644953250885
train_iter_loss: 0.09083191305398941
train_iter_loss: 0.20058341324329376
train_iter_loss: 0.1437436044216156
train_iter_loss: 0.16118831932544708
train_iter_loss: 0.1664392501115799
train_iter_loss: 0.10228931903839111
train_iter_loss: 0.0747404396533966
train_iter_loss: 0.23036901652812958
train_iter_loss: 0.1513810157775879
train_iter_loss: 0.1465441733598709
train_iter_loss: 0.23787058889865875
train_iter_loss: 0.09454448521137238
train_iter_loss: 0.21054548025131226
train_iter_loss: 0.1645924150943756
train loss :0.1747
---------------------
Validation seg loss: 0.22460966750557693 at epoch 166
epoch =    167/  1000, exp = train
train_iter_loss: 0.10255648195743561
train_iter_loss: 0.11380859464406967
train_iter_loss: 0.12949877977371216
train_iter_loss: 0.19528213143348694
train_iter_loss: 0.1675257682800293
train_iter_loss: 0.1451795995235443
train_iter_loss: 0.20226024091243744
train_iter_loss: 0.09920253604650497
train_iter_loss: 0.1328612118959427
train_iter_loss: 0.18634222447872162
train_iter_loss: 0.22391723096370697
train_iter_loss: 0.17928019165992737
train_iter_loss: 0.19158585369586945
train_iter_loss: 0.153616264462471
train_iter_loss: 0.15357372164726257
train_iter_loss: 0.14040708541870117
train_iter_loss: 0.130900576710701
train_iter_loss: 0.0572674423456192
train_iter_loss: 0.14505773782730103
train_iter_loss: 0.17383065819740295
train_iter_loss: 0.20477740466594696
train_iter_loss: 0.10469550639390945
train_iter_loss: 0.11106562614440918
train_iter_loss: 0.1037629172205925
train_iter_loss: 0.17063334584236145
train_iter_loss: 0.295093297958374
train_iter_loss: 0.21839198470115662
train_iter_loss: 0.23282276093959808
train_iter_loss: 0.07504025846719742
train_iter_loss: 0.0668800100684166
train_iter_loss: 0.2566099762916565
train_iter_loss: 0.22611136734485626
train_iter_loss: 0.25884121656417847
train_iter_loss: 0.2118343561887741
train_iter_loss: 0.17466825246810913
train_iter_loss: 0.2605648636817932
train_iter_loss: 0.057039666920900345
train_iter_loss: 0.21097949147224426
train_iter_loss: 0.27787691354751587
train_iter_loss: 0.10485786199569702
train_iter_loss: 0.20723766088485718
train_iter_loss: 0.2378016859292984
train_iter_loss: 0.17246298491954803
train_iter_loss: 0.16394133865833282
train_iter_loss: 0.15332813560962677
train_iter_loss: 0.05475451424717903
train_iter_loss: 0.28578561544418335
train_iter_loss: 0.1749294251203537
train_iter_loss: 0.14153842628002167
train_iter_loss: 0.18105922639369965
train_iter_loss: 0.15418939292430878
train_iter_loss: 0.2689615488052368
train_iter_loss: 0.22770723700523376
train_iter_loss: 0.2415638417005539
train_iter_loss: 0.2775465250015259
train_iter_loss: 0.31560930609703064
train_iter_loss: 0.18445482850074768
train_iter_loss: 0.1565561294555664
train_iter_loss: 0.24968814849853516
train_iter_loss: 0.2088683396577835
train_iter_loss: 0.22092758119106293
train_iter_loss: 0.15511097013950348
train_iter_loss: 0.14501647651195526
train_iter_loss: 0.155423104763031
train_iter_loss: 0.18390510976314545
train_iter_loss: 0.16842956840991974
train_iter_loss: 0.13846714794635773
train_iter_loss: 0.14499865472316742
train_iter_loss: 0.1812254637479782
train_iter_loss: 0.1859956979751587
train_iter_loss: 0.0684540644288063
train_iter_loss: 0.08501347154378891
train_iter_loss: 0.2294091433286667
train_iter_loss: 0.15162429213523865
train_iter_loss: 0.16559642553329468
train_iter_loss: 0.16073961555957794
train_iter_loss: 0.2195093035697937
train_iter_loss: 0.17850716412067413
train_iter_loss: 0.0864984393119812
train_iter_loss: 0.06667863577604294
train_iter_loss: 0.11706838756799698
train_iter_loss: 0.08915188908576965
train_iter_loss: 0.24026164412498474
train_iter_loss: 0.3075527548789978
train_iter_loss: 0.18221504986286163
train_iter_loss: 0.11287590116262436
train_iter_loss: 0.07149352133274078
train_iter_loss: 0.2189192920923233
train_iter_loss: 0.1428230106830597
train_iter_loss: 0.12783965468406677
train_iter_loss: 0.2074345201253891
train_iter_loss: 0.2375374436378479
train_iter_loss: 0.11494912207126617
train_iter_loss: 0.18681985139846802
train_iter_loss: 0.11728689819574356
train_iter_loss: 0.23897701501846313
train_iter_loss: 0.16355130076408386
train_iter_loss: 0.13464055955410004
train_iter_loss: 0.14294734597206116
train_iter_loss: 0.15731635689735413
train loss :0.1728
---------------------
Validation seg loss: 0.22651075086784814 at epoch 167
epoch =    168/  1000, exp = train
train_iter_loss: 0.10793771594762802
train_iter_loss: 0.16416965425014496
train_iter_loss: 0.1552688479423523
train_iter_loss: 0.1387387365102768
train_iter_loss: 0.11994051188230515
train_iter_loss: 0.1284223049879074
train_iter_loss: 0.0678682029247284
train_iter_loss: 0.19068999588489532
train_iter_loss: 0.20630790293216705
train_iter_loss: 0.13946370780467987
train_iter_loss: 0.22129027545452118
train_iter_loss: 0.13633711636066437
train_iter_loss: 0.21531987190246582
train_iter_loss: 0.17957113683223724
train_iter_loss: 0.2649401128292084
train_iter_loss: 0.22578591108322144
train_iter_loss: 0.18702279031276703
train_iter_loss: 0.14074672758579254
train_iter_loss: 0.12399059534072876
train_iter_loss: 0.10706394165754318
train_iter_loss: 0.2202536016702652
train_iter_loss: 0.2900601923465729
train_iter_loss: 0.15337131917476654
train_iter_loss: 0.2082015573978424
train_iter_loss: 0.2643106281757355
train_iter_loss: 0.15268002450466156
train_iter_loss: 0.30490022897720337
train_iter_loss: 0.16460567712783813
train_iter_loss: 0.16660892963409424
train_iter_loss: 0.1779571771621704
train_iter_loss: 0.14451901614665985
train_iter_loss: 0.17116113007068634
train_iter_loss: 0.08116359263658524
train_iter_loss: 0.04831662401556969
train_iter_loss: 0.09235094487667084
train_iter_loss: 0.10895124077796936
train_iter_loss: 0.06818901747465134
train_iter_loss: 0.1831895411014557
train_iter_loss: 0.09133917093276978
train_iter_loss: 0.035892024636268616
train_iter_loss: 0.13034941256046295
train_iter_loss: 0.17302952706813812
train_iter_loss: 0.17081698775291443
train_iter_loss: 0.13242563605308533
train_iter_loss: 0.25568950176239014
train_iter_loss: 0.16381777822971344
train_iter_loss: 0.16573044657707214
train_iter_loss: 0.19383330643177032
train_iter_loss: 0.10648040473461151
train_iter_loss: 0.11751619726419449
train_iter_loss: 0.16139884293079376
train_iter_loss: 0.20722077786922455
train_iter_loss: 0.09613548219203949
train_iter_loss: 0.14114032685756683
train_iter_loss: 0.17940860986709595
train_iter_loss: 0.16821794211864471
train_iter_loss: 0.16786183416843414
train_iter_loss: 0.21849551796913147
train_iter_loss: 0.1422152817249298
train_iter_loss: 0.15484891831874847
train_iter_loss: 0.20645421743392944
train_iter_loss: 0.25780874490737915
train_iter_loss: 0.15925897657871246
train_iter_loss: 0.20818209648132324
train_iter_loss: 0.07490727305412292
train_iter_loss: 0.19280406832695007
train_iter_loss: 0.08653733879327774
train_iter_loss: 0.14055918157100677
train_iter_loss: 0.149994358420372
train_iter_loss: 0.12200050801038742
train_iter_loss: 0.13183732330799103
train_iter_loss: 0.07220375537872314
train_iter_loss: 0.18059414625167847
train_iter_loss: 0.16363652050495148
train_iter_loss: 0.19559751451015472
train_iter_loss: 0.14387959241867065
train_iter_loss: 0.18096648156642914
train_iter_loss: 0.22265088558197021
train_iter_loss: 0.054943133145570755
train_iter_loss: 0.07703758031129837
train_iter_loss: 0.18358539044857025
train_iter_loss: 0.112603560090065
train_iter_loss: 0.22524043917655945
train_iter_loss: 0.11863575875759125
train_iter_loss: 0.21938100457191467
train_iter_loss: 0.17129209637641907
train_iter_loss: 0.2407083511352539
train_iter_loss: 0.18213334679603577
train_iter_loss: 0.1738058626651764
train_iter_loss: 0.20807823538780212
train_iter_loss: 0.213811457157135
train_iter_loss: 0.17893043160438538
train_iter_loss: 0.335163950920105
train_iter_loss: 0.23915356397628784
train_iter_loss: 0.2795294523239136
train_iter_loss: 0.16229979693889618
train_iter_loss: 0.2009185254573822
train_iter_loss: 0.21982264518737793
train_iter_loss: 0.1323324739933014
train_iter_loss: 0.18708066642284393
train loss :0.1677
---------------------
Validation seg loss: 0.22278997631532685 at epoch 168
epoch =    169/  1000, exp = train
train_iter_loss: 0.1522759348154068
train_iter_loss: 0.1631210595369339
train_iter_loss: 0.1944340020418167
train_iter_loss: 0.11771693080663681
train_iter_loss: 0.230667382478714
train_iter_loss: 0.16082948446273804
train_iter_loss: 0.18912185728549957
train_iter_loss: 0.18831633031368256
train_iter_loss: 0.20772363245487213
train_iter_loss: 0.14347800612449646
train_iter_loss: 0.2903243601322174
train_iter_loss: 0.3533512055873871
train_iter_loss: 0.2003951370716095
train_iter_loss: 0.2110520899295807
train_iter_loss: 0.10442794859409332
train_iter_loss: 0.25305676460266113
train_iter_loss: 0.10573133826255798
train_iter_loss: 0.1656980663537979
train_iter_loss: 0.21515801548957825
train_iter_loss: 0.19338789582252502
train_iter_loss: 0.13886858522891998
train_iter_loss: 0.25961583852767944
train_iter_loss: 0.18077905476093292
train_iter_loss: 0.13009585440158844
train_iter_loss: 0.17967379093170166
train_iter_loss: 0.1109616681933403
train_iter_loss: 0.22467797994613647
train_iter_loss: 0.1482783406972885
train_iter_loss: 0.3160279095172882
train_iter_loss: 0.11924263834953308
train_iter_loss: 0.17852364480495453
train_iter_loss: 0.056702569127082825
train_iter_loss: 0.12585803866386414
train_iter_loss: 0.15282262861728668
train_iter_loss: 0.14576123654842377
train_iter_loss: 0.23683315515518188
train_iter_loss: 0.23144377768039703
train_iter_loss: 0.2132193148136139
train_iter_loss: 0.1854252815246582
train_iter_loss: 0.11738431453704834
train_iter_loss: 0.04442700743675232
train_iter_loss: 0.07461767643690109
train_iter_loss: 0.32192087173461914
train_iter_loss: 0.21832937002182007
train_iter_loss: 0.22801242768764496
train_iter_loss: 0.2854921519756317
train_iter_loss: 0.1843165159225464
train_iter_loss: 0.34355589747428894
train_iter_loss: 0.14441917836666107
train_iter_loss: 0.1637849360704422
train_iter_loss: 0.17320500314235687
train_iter_loss: 0.2128477841615677
train_iter_loss: 0.08905476331710815
train_iter_loss: 0.24727743864059448
train_iter_loss: 0.1330832988023758
train_iter_loss: 0.15057255327701569
train_iter_loss: 0.2831782400608063
train_iter_loss: 0.05927786976099014
train_iter_loss: 0.18149840831756592
train_iter_loss: 0.19406788051128387
train_iter_loss: 0.14138156175613403
train_iter_loss: 0.2569311559200287
train_iter_loss: 0.11067641526460648
train_iter_loss: 0.20173341035842896
train_iter_loss: 0.1638474017381668
train_iter_loss: 0.11726626008749008
train_iter_loss: 0.17439304292201996
train_iter_loss: 0.11547258496284485
train_iter_loss: 0.13235138356685638
train_iter_loss: 0.16981558501720428
train_iter_loss: 0.12821418046951294
train_iter_loss: 0.140254944562912
train_iter_loss: 0.13072733581066132
train_iter_loss: 0.15091370046138763
train_iter_loss: 0.1852869987487793
train_iter_loss: 0.11925256997346878
train_iter_loss: 0.13762712478637695
train_iter_loss: 0.14405488967895508
train_iter_loss: 0.365039199590683
train_iter_loss: 0.1342056393623352
train_iter_loss: 0.16806477308273315
train_iter_loss: 0.038114033639431
train_iter_loss: 0.2106289118528366
train_iter_loss: 0.23595459759235382
train_iter_loss: 0.18489377200603485
train_iter_loss: 0.24534541368484497
train_iter_loss: 0.09649895876646042
train_iter_loss: 0.17921298742294312
train_iter_loss: 0.2426118403673172
train_iter_loss: 0.18252809345722198
train_iter_loss: 0.22171445190906525
train_iter_loss: 0.19105736911296844
train_iter_loss: 0.26230400800704956
train_iter_loss: 0.11713176965713501
train_iter_loss: 0.17166128754615784
train_iter_loss: 0.10039073973894119
train_iter_loss: 0.15546487271785736
train_iter_loss: 0.17547152936458588
train_iter_loss: 0.14279428124427795
train_iter_loss: 0.3727053701877594
train loss :0.1811
---------------------
Validation seg loss: 0.22371573457142935 at epoch 169
epoch =    170/  1000, exp = train
train_iter_loss: 0.13655038177967072
train_iter_loss: 0.2047455608844757
train_iter_loss: 0.1527024656534195
train_iter_loss: 0.22402642667293549
train_iter_loss: 0.2455299198627472
train_iter_loss: 0.21711641550064087
train_iter_loss: 0.1371961236000061
train_iter_loss: 0.2964724004268646
train_iter_loss: 0.15050484240055084
train_iter_loss: 0.17941424250602722
train_iter_loss: 0.2120123952627182
train_iter_loss: 0.2040075957775116
train_iter_loss: 0.1556941717863083
train_iter_loss: 0.14232242107391357
train_iter_loss: 0.13498973846435547
train_iter_loss: 0.14831587672233582
train_iter_loss: 0.15298891067504883
train_iter_loss: 0.24278521537780762
train_iter_loss: 0.2209932953119278
train_iter_loss: 0.12571994960308075
train_iter_loss: 0.14889909327030182
train_iter_loss: 0.15677198767662048
train_iter_loss: 0.12182562053203583
train_iter_loss: 0.1917211413383484
train_iter_loss: 0.16813412308692932
train_iter_loss: 0.29951855540275574
train_iter_loss: 0.0960325077176094
train_iter_loss: 0.16250935196876526
train_iter_loss: 0.41868072748184204
train_iter_loss: 0.069032683968544
train_iter_loss: 0.1343216598033905
train_iter_loss: 0.14939512312412262
train_iter_loss: 0.136082261800766
train_iter_loss: 0.17425762116909027
train_iter_loss: 0.19719760119915009
train_iter_loss: 0.0773143470287323
train_iter_loss: 0.16822922229766846
train_iter_loss: 0.2455359250307083
train_iter_loss: 0.1247711181640625
train_iter_loss: 0.16022302210330963
train_iter_loss: 0.1821368932723999
train_iter_loss: 0.23602761328220367
train_iter_loss: 0.12707938253879547
train_iter_loss: 0.20413997769355774
train_iter_loss: 0.2120310515165329
train_iter_loss: 0.2098691761493683
train_iter_loss: 0.19305388629436493
train_iter_loss: 0.19451190531253815
train_iter_loss: 0.28646165132522583
train_iter_loss: 0.14530707895755768
train_iter_loss: 0.1667550951242447
train_iter_loss: 0.1512000858783722
train_iter_loss: 0.4063166081905365
train_iter_loss: 0.12855839729309082
train_iter_loss: 0.1832941174507141
train_iter_loss: 0.14261102676391602
train_iter_loss: 0.17386239767074585
train_iter_loss: 0.1615346372127533
train_iter_loss: 0.15164560079574585
train_iter_loss: 0.15470059216022491
train_iter_loss: 0.1935015171766281
train_iter_loss: 0.1538151353597641
train_iter_loss: 0.09490939229726791
train_iter_loss: 0.22874963283538818
train_iter_loss: 0.2128426879644394
train_iter_loss: 0.1489858776330948
train_iter_loss: 0.1400708705186844
train_iter_loss: 0.17297856509685516
train_iter_loss: 0.2550690174102783
train_iter_loss: 0.1136683002114296
train_iter_loss: 0.15268367528915405
train_iter_loss: 0.09236618131399155
train_iter_loss: 0.1323447972536087
train_iter_loss: 0.1003015786409378
train_iter_loss: 0.11696553975343704
train_iter_loss: 0.1239042654633522
train_iter_loss: 0.13556502759456635
train_iter_loss: 0.17466546595096588
train_iter_loss: 0.30254408717155457
train_iter_loss: 0.19173093140125275
train_iter_loss: 0.14294150471687317
train_iter_loss: 0.08930296450853348
train_iter_loss: 0.36178839206695557
train_iter_loss: 0.2597314715385437
train_iter_loss: 0.09830362349748611
train_iter_loss: 0.2029888778924942
train_iter_loss: 0.16222532093524933
train_iter_loss: 0.2713375985622406
train_iter_loss: 0.13243526220321655
train_iter_loss: 0.1560296267271042
train_iter_loss: 0.09591509401798248
train_iter_loss: 0.14390116930007935
train_iter_loss: 0.1982894241809845
train_iter_loss: 0.16436484456062317
train_iter_loss: 0.17777372896671295
train_iter_loss: 0.2381734848022461
train_iter_loss: 0.22625471651554108
train_iter_loss: 0.1684003621339798
train_iter_loss: 0.10237804055213928
train_iter_loss: 0.23709005117416382
train loss :0.1794
---------------------
Validation seg loss: 0.22361022728618304 at epoch 170
epoch =    171/  1000, exp = train
train_iter_loss: 0.1469876617193222
train_iter_loss: 0.07779813557863235
train_iter_loss: 0.17868594825267792
train_iter_loss: 0.11600533872842789
train_iter_loss: 0.23322972655296326
train_iter_loss: 0.20165607333183289
train_iter_loss: 0.12402284890413284
train_iter_loss: 0.08220239728689194
train_iter_loss: 0.2948322892189026
train_iter_loss: 0.10673484206199646
train_iter_loss: 0.3064994513988495
train_iter_loss: 0.14036861062049866
train_iter_loss: 0.25784778594970703
train_iter_loss: 0.1951749175786972
train_iter_loss: 0.3322252631187439
train_iter_loss: 0.13705791532993317
train_iter_loss: 0.19655565917491913
train_iter_loss: 0.11446663737297058
train_iter_loss: 0.3080332279205322
train_iter_loss: 0.18193738162517548
train_iter_loss: 0.13734452426433563
train_iter_loss: 0.13048605620861053
train_iter_loss: 0.24705228209495544
train_iter_loss: 0.1882418692111969
train_iter_loss: 0.19355060160160065
train_iter_loss: 0.13969776034355164
train_iter_loss: 0.22577480971813202
train_iter_loss: 0.19897028803825378
train_iter_loss: 0.0828431099653244
train_iter_loss: 0.23658080399036407
train_iter_loss: 0.15879228711128235
train_iter_loss: 0.2214907854795456
train_iter_loss: 0.05803193897008896
train_iter_loss: 0.15230223536491394
train_iter_loss: 0.16026879847049713
train_iter_loss: 0.0928599163889885
train_iter_loss: 0.3077831268310547
train_iter_loss: 0.15635967254638672
train_iter_loss: 0.1291782259941101
train_iter_loss: 0.18097907304763794
train_iter_loss: 0.1349318027496338
train_iter_loss: 0.2720353603363037
train_iter_loss: 0.07304379343986511
train_iter_loss: 0.0963754653930664
train_iter_loss: 0.07947596907615662
train_iter_loss: 0.054909076541662216
train_iter_loss: 0.19784708321094513
train_iter_loss: 0.23038090765476227
train_iter_loss: 0.15231430530548096
train_iter_loss: 0.14232364296913147
train_iter_loss: 0.15598748624324799
train_iter_loss: 0.20844678580760956
train_iter_loss: 0.1427052617073059
train_iter_loss: 0.18842406570911407
train_iter_loss: 0.26289504766464233
train_iter_loss: 0.18129202723503113
train_iter_loss: 0.16516028344631195
train_iter_loss: 0.13107900321483612
train_iter_loss: 0.14506374299526215
train_iter_loss: 0.12523317337036133
train_iter_loss: 0.07851878553628922
train_iter_loss: 0.12238044291734695
train_iter_loss: 0.2380615770816803
train_iter_loss: 0.1488804966211319
train_iter_loss: 0.180062934756279
train_iter_loss: 0.16127841174602509
train_iter_loss: 0.24076159298419952
train_iter_loss: 0.16675369441509247
train_iter_loss: 0.12900452315807343
train_iter_loss: 0.17082832753658295
train_iter_loss: 0.25906163454055786
train_iter_loss: 0.17955470085144043
train_iter_loss: 0.21459990739822388
train_iter_loss: 0.12554024159908295
train_iter_loss: 0.1462547779083252
train_iter_loss: 0.12499493360519409
train_iter_loss: 0.22569787502288818
train_iter_loss: 0.10722922533750534
train_iter_loss: 0.12864156067371368
train_iter_loss: 0.12150513380765915
train_iter_loss: 0.31621092557907104
train_iter_loss: 0.22353492677211761
train_iter_loss: 0.2156769335269928
train_iter_loss: 0.12269043922424316
train_iter_loss: 0.1112479567527771
train_iter_loss: 0.2907143235206604
train_iter_loss: 0.14698749780654907
train_iter_loss: 0.19908811151981354
train_iter_loss: 0.11113645136356354
train_iter_loss: 0.2757371664047241
train_iter_loss: 0.08521880209445953
train_iter_loss: 0.1638534963130951
train_iter_loss: 0.12040337920188904
train_iter_loss: 0.3424769937992096
train_iter_loss: 0.09761212766170502
train_iter_loss: 0.12103545665740967
train_iter_loss: 0.11298812925815582
train_iter_loss: 0.17635053396224976
train_iter_loss: 0.20815517008304596
train_iter_loss: 0.21202385425567627
train loss :0.1737
---------------------
Validation seg loss: 0.22438834301846208 at epoch 171
epoch =    172/  1000, exp = train
train_iter_loss: 0.18684634566307068
train_iter_loss: 0.2929159700870514
train_iter_loss: 0.15343144536018372
train_iter_loss: 0.23725339770317078
train_iter_loss: 0.1869882494211197
train_iter_loss: 0.23405060172080994
train_iter_loss: 0.15723368525505066
train_iter_loss: 0.3695564866065979
train_iter_loss: 0.17013728618621826
train_iter_loss: 0.1794409453868866
train_iter_loss: 0.09819744527339935
train_iter_loss: 0.1589026004076004
train_iter_loss: 0.1252840906381607
train_iter_loss: 0.19445739686489105
train_iter_loss: 0.1784326285123825
train_iter_loss: 0.12945131957530975
train_iter_loss: 0.1536494940519333
train_iter_loss: 0.1628194898366928
train_iter_loss: 0.13578739762306213
train_iter_loss: 0.14501872658729553
train_iter_loss: 0.08056879043579102
train_iter_loss: 0.23238834738731384
train_iter_loss: 0.1666165143251419
train_iter_loss: 0.18479277193546295
train_iter_loss: 0.09441065043210983
train_iter_loss: 0.11853320896625519
train_iter_loss: 0.18972396850585938
train_iter_loss: 0.10866500437259674
train_iter_loss: 0.20745451748371124
train_iter_loss: 0.13325926661491394
train_iter_loss: 0.10975845903158188
train_iter_loss: 0.21887554228305817
train_iter_loss: 0.13175423443317413
train_iter_loss: 0.17377997934818268
train_iter_loss: 0.22059820592403412
train_iter_loss: 0.1402653604745865
train_iter_loss: 0.1666375994682312
train_iter_loss: 0.12722660601139069
train_iter_loss: 0.16850101947784424
train_iter_loss: 0.19721761345863342
train_iter_loss: 0.20001593232154846
train_iter_loss: 0.11287019401788712
train_iter_loss: 0.24714721739292145
train_iter_loss: 0.12198520451784134
train_iter_loss: 0.14202606678009033
train_iter_loss: 0.23031765222549438
train_iter_loss: 0.17112286388874054
train_iter_loss: 0.2407304346561432
train_iter_loss: 0.1269865483045578
train_iter_loss: 0.13793815672397614
train_iter_loss: 0.13945525884628296
train_iter_loss: 0.1425432413816452
train_iter_loss: 0.24652071297168732
train_iter_loss: 0.10273413360118866
train_iter_loss: 0.13649392127990723
train_iter_loss: 0.13487417995929718
train_iter_loss: 0.1206439658999443
train_iter_loss: 0.12737005949020386
train_iter_loss: 0.2753066420555115
train_iter_loss: 0.17422634363174438
train_iter_loss: 0.19617879390716553
train_iter_loss: 0.29260584712028503
train_iter_loss: 0.17550510168075562
train_iter_loss: 0.13632415235042572
train_iter_loss: 0.19588036835193634
train_iter_loss: 0.0810643658041954
train_iter_loss: 0.14089059829711914
train_iter_loss: 0.16735370457172394
train_iter_loss: 0.13178880512714386
train_iter_loss: 0.2034110724925995
train_iter_loss: 0.03258993849158287
train_iter_loss: 0.1145114079117775
train_iter_loss: 0.14452090859413147
train_iter_loss: 0.1742009073495865
train_iter_loss: 0.1796264499425888
train_iter_loss: 0.19051645696163177
train_iter_loss: 0.1909376084804535
train_iter_loss: 0.11799316108226776
train_iter_loss: 0.14093898236751556
train_iter_loss: 0.1903509646654129
train_iter_loss: 0.10234473645687103
train_iter_loss: 0.21381539106369019
train_iter_loss: 0.14258326590061188
train_iter_loss: 0.11223174631595612
train_iter_loss: 0.14997625350952148
train_iter_loss: 0.14208680391311646
train_iter_loss: 0.21333588659763336
train_iter_loss: 0.14518935978412628
train_iter_loss: 0.23087726533412933
train_iter_loss: 0.14727836847305298
train_iter_loss: 0.1866386979818344
train_iter_loss: 0.07010914385318756
train_iter_loss: 0.1422780156135559
train_iter_loss: 0.12863369286060333
train_iter_loss: 0.14958436787128448
train_iter_loss: 0.2669607102870941
train_iter_loss: 0.13986225426197052
train_iter_loss: 0.11018293350934982
train_iter_loss: 0.15975935757160187
train_iter_loss: 0.11949680745601654
train loss :0.1660
---------------------
Validation seg loss: 0.22173783945728023 at epoch 172
********************
best_val_epoch_loss:  0.22173783945728023
MODEL UPDATED
epoch =    173/  1000, exp = train
train_iter_loss: 0.19984346628189087
train_iter_loss: 0.16971303522586823
train_iter_loss: 0.1949535608291626
train_iter_loss: 0.2052583396434784
train_iter_loss: 0.1904139220714569
train_iter_loss: 0.1246589794754982
train_iter_loss: 0.11779213696718216
train_iter_loss: 0.20982736349105835
train_iter_loss: 0.15460985898971558
train_iter_loss: 0.19872364401817322
train_iter_loss: 0.1022493913769722
train_iter_loss: 0.18444320559501648
train_iter_loss: 0.15655417740345
train_iter_loss: 0.2001381367444992
train_iter_loss: 0.19748176634311676
train_iter_loss: 0.28879743814468384
train_iter_loss: 0.09647223353385925
train_iter_loss: 0.14596253633499146
train_iter_loss: 0.14452238380908966
train_iter_loss: 0.22101393342018127
train_iter_loss: 0.18551741540431976
train_iter_loss: 0.12785592675209045
train_iter_loss: 0.1858268678188324
train_iter_loss: 0.1556311994791031
train_iter_loss: 0.28060078620910645
train_iter_loss: 0.15703287720680237
train_iter_loss: 0.11350272595882416
train_iter_loss: 0.11367930471897125
train_iter_loss: 0.17773005366325378
train_iter_loss: 0.1915789544582367
train_iter_loss: 0.12625111639499664
train_iter_loss: 0.11067833751440048
train_iter_loss: 0.19549670815467834
train_iter_loss: 0.24994155764579773
train_iter_loss: 0.22265207767486572
train_iter_loss: 0.1783132404088974
train_iter_loss: 0.31280389428138733
train_iter_loss: 0.062337763607501984
train_iter_loss: 0.17228394746780396
train_iter_loss: 0.21303579211235046
train_iter_loss: 0.22236423194408417
train_iter_loss: 0.1024806872010231
train_iter_loss: 0.19022732973098755
train_iter_loss: 0.07798207551240921
train_iter_loss: 0.16046945750713348
train_iter_loss: 0.18273231387138367
train_iter_loss: 0.057483188807964325
train_iter_loss: 0.2103366255760193
train_iter_loss: 0.24133242666721344
train_iter_loss: 0.18228749930858612
train_iter_loss: 0.26306384801864624
train_iter_loss: 0.194789856672287
train_iter_loss: 0.10802766680717468
train_iter_loss: 0.30795818567276
train_iter_loss: 0.40249329805374146
train_iter_loss: 0.10911665111780167
train_iter_loss: 0.20483288168907166
train_iter_loss: 0.10109219700098038
train_iter_loss: 0.3556084632873535
train_iter_loss: 0.21946628391742706
train_iter_loss: 0.21785299479961395
train_iter_loss: 0.1193302795290947
train_iter_loss: 0.10974699258804321
train_iter_loss: 0.13289931416511536
train_iter_loss: 0.13798627257347107
train_iter_loss: 0.04974920675158501
train_iter_loss: 0.1948150396347046
train_iter_loss: 0.2308293581008911
train_iter_loss: 0.07046676427125931
train_iter_loss: 0.09714289754629135
train_iter_loss: 0.08667460083961487
train_iter_loss: 0.18097053468227386
train_iter_loss: 0.1653846949338913
train_iter_loss: 0.11149002611637115
train_iter_loss: 0.15816529095172882
train_iter_loss: 0.11495678126811981
train_iter_loss: 0.06448393315076828
train_iter_loss: 0.194217711687088
train_iter_loss: 0.13763274252414703
train_iter_loss: 0.3569657802581787
train_iter_loss: 0.180220365524292
train_iter_loss: 0.1171056255698204
train_iter_loss: 0.23250292241573334
train_iter_loss: 0.1674882024526596
train_iter_loss: 0.13757357001304626
train_iter_loss: 0.08228959143161774
train_iter_loss: 0.27375349402427673
train_iter_loss: 0.21138688921928406
train_iter_loss: 0.18355315923690796
train_iter_loss: 0.1973625123500824
train_iter_loss: 0.1904038041830063
train_iter_loss: 0.1437070071697235
train_iter_loss: 0.21083372831344604
train_iter_loss: 0.16163477301597595
train_iter_loss: 0.13609525561332703
train_iter_loss: 0.12115701287984848
train_iter_loss: 0.1311291754245758
train_iter_loss: 0.18923893570899963
train_iter_loss: 0.10987783968448639
train_iter_loss: 0.06004834920167923
train loss :0.1727
---------------------
Validation seg loss: 0.2232621051019655 at epoch 173
epoch =    174/  1000, exp = train
train_iter_loss: 0.1440199911594391
train_iter_loss: 0.14802652597427368
train_iter_loss: 0.340456485748291
train_iter_loss: 0.125573068857193
train_iter_loss: 0.10499611496925354
train_iter_loss: 0.16551923751831055
train_iter_loss: 0.16943612694740295
train_iter_loss: 0.11071250587701797
train_iter_loss: 0.18556901812553406
train_iter_loss: 0.19895566999912262
train_iter_loss: 0.1239377036690712
train_iter_loss: 0.14610503613948822
train_iter_loss: 0.13399404287338257
train_iter_loss: 0.16794779896736145
train_iter_loss: 0.17901793122291565
train_iter_loss: 0.22741112112998962
train_iter_loss: 0.24806267023086548
train_iter_loss: 0.19685280323028564
train_iter_loss: 0.1770509034395218
train_iter_loss: 0.24701373279094696
train_iter_loss: 0.05993577465415001
train_iter_loss: 0.1704263836145401
train_iter_loss: 0.13800153136253357
train_iter_loss: 0.14820733666419983
train_iter_loss: 0.23486807942390442
train_iter_loss: 0.1345512866973877
train_iter_loss: 0.1459474265575409
train_iter_loss: 0.12430115789175034
train_iter_loss: 0.15640385448932648
train_iter_loss: 0.1993453949689865
train_iter_loss: 0.21454501152038574
train_iter_loss: 0.07798049598932266
train_iter_loss: 0.2401915043592453
train_iter_loss: 0.16273854672908783
train_iter_loss: 0.20294159650802612
train_iter_loss: 0.15989628434181213
train_iter_loss: 0.09838608652353287
train_iter_loss: 0.1296813040971756
train_iter_loss: 0.08066766709089279
train_iter_loss: 0.1601203829050064
train_iter_loss: 0.08111944794654846
train_iter_loss: 0.19447164237499237
train_iter_loss: 0.21118436753749847
train_iter_loss: 0.14502666890621185
train_iter_loss: 0.14326223731040955
train_iter_loss: 0.15745121240615845
train_iter_loss: 0.20348869264125824
train_iter_loss: 0.165747731924057
train_iter_loss: 0.10021897405385971
train_iter_loss: 0.16478991508483887
train_iter_loss: 0.19783641397953033
train_iter_loss: 0.12872862815856934
train_iter_loss: 0.14602777361869812
train_iter_loss: 0.21807657182216644
train_iter_loss: 0.1355678290128708
train_iter_loss: 0.31797656416893005
train_iter_loss: 0.11966998130083084
train_iter_loss: 0.13371433317661285
train_iter_loss: 0.15526671707630157
train_iter_loss: 0.23374488949775696
train_iter_loss: 0.13104389607906342
train_iter_loss: 0.243354931473732
train_iter_loss: 0.21898715198040009
train_iter_loss: 0.2694603502750397
train_iter_loss: 0.1938282996416092
train_iter_loss: 0.14886538684368134
train_iter_loss: 0.17767333984375
train_iter_loss: 0.13574442267417908
train_iter_loss: 0.11095969378948212
train_iter_loss: 0.08446358144283295
train_iter_loss: 0.1098230853676796
train_iter_loss: 0.14456284046173096
train_iter_loss: 0.1562163531780243
train_iter_loss: 0.14935711026191711
train_iter_loss: 0.18983763456344604
train_iter_loss: 0.3580479025840759
train_iter_loss: 0.15529030561447144
train_iter_loss: 0.20948269963264465
train_iter_loss: 0.18747761845588684
train_iter_loss: 0.1111832931637764
train_iter_loss: 0.33245712518692017
train_iter_loss: 0.12051738053560257
train_iter_loss: 0.09814255684614182
train_iter_loss: 0.18896135687828064
train_iter_loss: 0.1540350466966629
train_iter_loss: 0.17637373507022858
train_iter_loss: 0.09524674713611603
train_iter_loss: 0.2171470820903778
train_iter_loss: 0.08444468677043915
train_iter_loss: 0.2776276469230652
train_iter_loss: 0.169004887342453
train_iter_loss: 0.26963311433792114
train_iter_loss: 0.29256054759025574
train_iter_loss: 0.08516950905323029
train_iter_loss: 0.2440314143896103
train_iter_loss: 0.2474624514579773
train_iter_loss: 0.17373281717300415
train_iter_loss: 0.14015358686447144
train_iter_loss: 0.2116968184709549
train_iter_loss: 0.11184588074684143
train loss :0.1728
---------------------
Validation seg loss: 0.2279144468418551 at epoch 174
epoch =    175/  1000, exp = train
train_iter_loss: 0.23558534681797028
train_iter_loss: 0.17598634958267212
train_iter_loss: 0.11342503130435944
train_iter_loss: 0.16330170631408691
train_iter_loss: 0.15570800006389618
train_iter_loss: 0.12859414517879486
train_iter_loss: 0.2744658887386322
train_iter_loss: 0.12378451973199844
train_iter_loss: 0.1970454752445221
train_iter_loss: 0.24684110283851624
train_iter_loss: 0.12514881789684296
train_iter_loss: 0.16025593876838684
train_iter_loss: 0.22763517498970032
train_iter_loss: 0.21891823410987854
train_iter_loss: 0.2411048263311386
train_iter_loss: 0.22116830945014954
train_iter_loss: 0.19550684094429016
train_iter_loss: 0.11870148032903671
train_iter_loss: 0.19726915657520294
train_iter_loss: 0.19320572912693024
train_iter_loss: 0.15021397173404694
train_iter_loss: 0.09828057885169983
train_iter_loss: 0.12951117753982544
train_iter_loss: 0.1687200367450714
train_iter_loss: 0.14147964119911194
train_iter_loss: 0.14057199656963348
train_iter_loss: 0.19667506217956543
train_iter_loss: 0.23367668688297272
train_iter_loss: 0.15137140452861786
train_iter_loss: 0.14621217548847198
train_iter_loss: 0.20807811617851257
train_iter_loss: 0.17206542193889618
train_iter_loss: 0.17523542046546936
train_iter_loss: 0.16230729222297668
train_iter_loss: 0.14931973814964294
train_iter_loss: 0.1430629938840866
train_iter_loss: 0.18337099254131317
train_iter_loss: 0.17107558250427246
train_iter_loss: 0.15913602709770203
train_iter_loss: 0.15088552236557007
train_iter_loss: 0.20669616758823395
train_iter_loss: 0.12922444939613342
train_iter_loss: 0.10692555457353592
train_iter_loss: 0.21244272589683533
train_iter_loss: 0.23436744511127472
train_iter_loss: 0.2267177253961563
train_iter_loss: 0.28024354577064514
train_iter_loss: 0.21606890857219696
train_iter_loss: 0.18578054010868073
train_iter_loss: 0.13842153549194336
train_iter_loss: 0.0854542925953865
train_iter_loss: 0.17263668775558472
train_iter_loss: 0.1901102215051651
train_iter_loss: 0.15479625761508942
train_iter_loss: 0.15259625017642975
train_iter_loss: 0.1528744399547577
train_iter_loss: 0.2759407162666321
train_iter_loss: 0.13108065724372864
train_iter_loss: 0.11057272553443909
train_iter_loss: 0.19101528823375702
train_iter_loss: 0.2155841737985611
train_iter_loss: 0.22542652487754822
train_iter_loss: 0.18022148311138153
train_iter_loss: 0.19286389648914337
train_iter_loss: 0.1188681423664093
train_iter_loss: 0.08472500741481781
train_iter_loss: 0.12504951655864716
train_iter_loss: 0.24130165576934814
train_iter_loss: 0.09829307347536087
train_iter_loss: 0.2439100295305252
train_iter_loss: 0.3496825695037842
train_iter_loss: 0.10582302510738373
train_iter_loss: 0.2106882780790329
train_iter_loss: 0.16940155625343323
train_iter_loss: 0.14351218938827515
train_iter_loss: 0.050451137125492096
train_iter_loss: 0.11951874196529388
train_iter_loss: 0.03860658407211304
train_iter_loss: 0.20422977209091187
train_iter_loss: 0.1755359023809433
train_iter_loss: 0.2145199328660965
train_iter_loss: 0.21767514944076538
train_iter_loss: 0.08968216180801392
train_iter_loss: 0.10335595160722733
train_iter_loss: 0.30305901169776917
train_iter_loss: 0.10917294025421143
train_iter_loss: 0.12966321408748627
train_iter_loss: 0.16659532487392426
train_iter_loss: 0.1050291582942009
train_iter_loss: 0.2994501292705536
train_iter_loss: 0.19342739880084991
train_iter_loss: 0.11137019842863083
train_iter_loss: 0.2609846591949463
train_iter_loss: 0.33000895380973816
train_iter_loss: 0.15060776472091675
train_iter_loss: 0.21346254646778107
train_iter_loss: 0.24700289964675903
train_iter_loss: 0.1975185126066208
train_iter_loss: 0.12051316350698471
train_iter_loss: 0.09067351371049881
train loss :0.1762
---------------------
Validation seg loss: 0.22868341247920157 at epoch 175
epoch =    176/  1000, exp = train
train_iter_loss: 0.271369606256485
train_iter_loss: 0.1821736991405487
train_iter_loss: 0.21734914183616638
train_iter_loss: 0.097122423350811
train_iter_loss: 0.23705251514911652
train_iter_loss: 0.14594578742980957
train_iter_loss: 0.18131984770298004
train_iter_loss: 0.1003909558057785
train_iter_loss: 0.1527494192123413
train_iter_loss: 0.129765585064888
train_iter_loss: 0.15713633596897125
train_iter_loss: 0.17451784014701843
train_iter_loss: 0.12306948751211166
train_iter_loss: 0.3042210638523102
train_iter_loss: 0.1847015917301178
train_iter_loss: 0.08688349276781082
train_iter_loss: 0.1640823632478714
train_iter_loss: 0.21712343394756317
train_iter_loss: 0.12624762952327728
train_iter_loss: 0.18486535549163818
train_iter_loss: 0.24272218346595764
train_iter_loss: 0.12151549011468887
train_iter_loss: 0.13442103564739227
train_iter_loss: 0.21982713043689728
train_iter_loss: 0.24560429155826569
train_iter_loss: 0.15397687256336212
train_iter_loss: 0.15644611418247223
train_iter_loss: 0.1289578378200531
train_iter_loss: 0.1912107616662979
train_iter_loss: 0.17836302518844604
train_iter_loss: 0.19530797004699707
train_iter_loss: 0.18676836788654327
train_iter_loss: 0.10179930925369263
train_iter_loss: 0.22699300944805145
train_iter_loss: 0.2203163355588913
train_iter_loss: 0.17669391632080078
train_iter_loss: 0.14096508920192719
train_iter_loss: 0.13638634979724884
train_iter_loss: 0.12933339178562164
train_iter_loss: 0.04883182793855667
train_iter_loss: 0.2024778425693512
train_iter_loss: 0.04630674049258232
train_iter_loss: 0.10517770051956177
train_iter_loss: 0.14250105619430542
train_iter_loss: 0.20042668282985687
train_iter_loss: 0.19374416768550873
train_iter_loss: 0.10471336543560028
train_iter_loss: 0.18560276925563812
train_iter_loss: 0.1591683179140091
train_iter_loss: 0.12886209785938263
train_iter_loss: 0.14259706437587738
train_iter_loss: 0.25119897723197937
train_iter_loss: 0.17452451586723328
train_iter_loss: 0.18879972398281097
train_iter_loss: 0.21306538581848145
train_iter_loss: 0.2596279978752136
train_iter_loss: 0.12932169437408447
train_iter_loss: 0.10075408965349197
train_iter_loss: 0.22842267155647278
train_iter_loss: 0.21133607625961304
train_iter_loss: 0.13072018325328827
train_iter_loss: 0.15040190517902374
train_iter_loss: 0.2138790339231491
train_iter_loss: 0.16201773285865784
train_iter_loss: 0.13815665245056152
train_iter_loss: 0.09163414686918259
train_iter_loss: 0.08302976191043854
train_iter_loss: 0.26789867877960205
train_iter_loss: 0.12373511493206024
train_iter_loss: 0.12053035199642181
train_iter_loss: 0.09688867628574371
train_iter_loss: 0.24532301723957062
train_iter_loss: 0.15016251802444458
train_iter_loss: 0.0893540233373642
train_iter_loss: 0.0495188869535923
train_iter_loss: 0.2554923892021179
train_iter_loss: 0.1258801817893982
train_iter_loss: 0.13364149630069733
train_iter_loss: 0.08060862869024277
train_iter_loss: 0.183474600315094
train_iter_loss: 0.12930995225906372
train_iter_loss: 0.2597784399986267
train_iter_loss: 0.17375798523426056
train_iter_loss: 0.1545432060956955
train_iter_loss: 0.1497623324394226
train_iter_loss: 0.17789919674396515
train_iter_loss: 0.06359881907701492
train_iter_loss: 0.28138723969459534
train_iter_loss: 0.13098712265491486
train_iter_loss: 0.29514676332473755
train_iter_loss: 0.1950574815273285
train_iter_loss: 0.1406538486480713
train_iter_loss: 0.3187589645385742
train_iter_loss: 0.1843796670436859
train_iter_loss: 0.24199363589286804
train_iter_loss: 0.30498936772346497
train_iter_loss: 0.16913031041622162
train_iter_loss: 0.2700073719024658
train_iter_loss: 0.18890859186649323
train_iter_loss: 0.20122358202934265
train loss :0.1724
---------------------
Validation seg loss: 0.22712539561655162 at epoch 176
epoch =    177/  1000, exp = train
train_iter_loss: 0.23362942039966583
train_iter_loss: 0.17971117794513702
train_iter_loss: 0.1587061583995819
train_iter_loss: 0.14931154251098633
train_iter_loss: 0.13888445496559143
train_iter_loss: 0.24265360832214355
train_iter_loss: 0.2628687620162964
train_iter_loss: 0.2513963580131531
train_iter_loss: 0.14567376673221588
train_iter_loss: 0.11315662413835526
train_iter_loss: 0.2004120796918869
train_iter_loss: 0.15137979388237
train_iter_loss: 0.19150730967521667
train_iter_loss: 0.17639896273612976
train_iter_loss: 0.15671756863594055
train_iter_loss: 0.11043783277273178
train_iter_loss: 0.20994330942630768
train_iter_loss: 0.10773855447769165
train_iter_loss: 0.2568129599094391
train_iter_loss: 0.08563648164272308
train_iter_loss: 0.19384895265102386
train_iter_loss: 0.05594013258814812
train_iter_loss: 0.13603146374225616
train_iter_loss: 0.17538385093212128
train_iter_loss: 0.07331651449203491
train_iter_loss: 0.09896817058324814
train_iter_loss: 0.24386398494243622
train_iter_loss: 0.2574823796749115
train_iter_loss: 0.10012615472078323
train_iter_loss: 0.2950734794139862
train_iter_loss: 0.08817422389984131
train_iter_loss: 0.10256659984588623
train_iter_loss: 0.08655209839344025
train_iter_loss: 0.3193954825401306
train_iter_loss: 0.14506840705871582
train_iter_loss: 0.12336386740207672
train_iter_loss: 0.1985156238079071
train_iter_loss: 0.10739301890134811
train_iter_loss: 0.1538129448890686
train_iter_loss: 0.1777087301015854
train_iter_loss: 0.1635315716266632
train_iter_loss: 0.11991214007139206
train_iter_loss: 0.11453208327293396
train_iter_loss: 0.1966623067855835
train_iter_loss: 0.19237437844276428
train_iter_loss: 0.09232757240533829
train_iter_loss: 0.09205980598926544
train_iter_loss: 0.1693773865699768
train_iter_loss: 0.24144130945205688
train_iter_loss: 0.13969430327415466
train_iter_loss: 0.1509483903646469
train_iter_loss: 0.15657955408096313
train_iter_loss: 0.27642300724983215
train_iter_loss: 0.2981191873550415
train_iter_loss: 0.13286055624485016
train_iter_loss: 0.1603630632162094
train_iter_loss: 0.2391425371170044
train_iter_loss: 0.22816748917102814
train_iter_loss: 0.16989895701408386
train_iter_loss: 0.2285032421350479
train_iter_loss: 0.2600022554397583
train_iter_loss: 0.27175241708755493
train_iter_loss: 0.17175640165805817
train_iter_loss: 0.15219421684741974
train_iter_loss: 0.24037136137485504
train_iter_loss: 0.22453932464122772
train_iter_loss: 0.07949233055114746
train_iter_loss: 0.09918380528688431
train_iter_loss: 0.26246631145477295
train_iter_loss: 0.12498556077480316
train_iter_loss: 0.14310237765312195
train_iter_loss: 0.13557520508766174
train_iter_loss: 0.14009986817836761
train_iter_loss: 0.17898809909820557
train_iter_loss: 0.10785537958145142
train_iter_loss: 0.11052097380161285
train_iter_loss: 0.15576229989528656
train_iter_loss: 0.139984592795372
train_iter_loss: 0.23139232397079468
train_iter_loss: 0.1995069980621338
train_iter_loss: 0.2794574797153473
train_iter_loss: 0.20725873112678528
train_iter_loss: 0.19664527475833893
train_iter_loss: 0.36605724692344666
train_iter_loss: 0.18153035640716553
train_iter_loss: 0.18085362017154694
train_iter_loss: 0.24792777001857758
train_iter_loss: 0.14972110092639923
train_iter_loss: 0.14626529812812805
train_iter_loss: 0.11652550101280212
train_iter_loss: 0.29347550868988037
train_iter_loss: 0.12134024500846863
train_iter_loss: 0.08246374875307083
train_iter_loss: 0.1368670016527176
train_iter_loss: 0.16434967517852783
train_iter_loss: 0.136762335896492
train_iter_loss: 0.1405509114265442
train_iter_loss: 0.17364536225795746
train_iter_loss: 0.18074797093868256
train_iter_loss: 0.31089481711387634
train loss :0.1763
---------------------
Validation seg loss: 0.22154421236295746 at epoch 177
********************
best_val_epoch_loss:  0.22154421236295746
MODEL UPDATED
epoch =    178/  1000, exp = train
train_iter_loss: 0.21925519406795502
train_iter_loss: 0.1669464409351349
train_iter_loss: 0.3208028972148895
train_iter_loss: 0.33574292063713074
train_iter_loss: 0.11009801924228668
train_iter_loss: 0.13402917981147766
train_iter_loss: 0.1232033222913742
train_iter_loss: 0.23689033091068268
train_iter_loss: 0.22610080242156982
train_iter_loss: 0.20925460755825043
train_iter_loss: 0.08904227614402771
train_iter_loss: 0.24653314054012299
train_iter_loss: 0.1805201917886734
train_iter_loss: 0.2195655256509781
train_iter_loss: 0.1865905225276947
train_iter_loss: 0.11066840589046478
train_iter_loss: 0.18190889060497284
train_iter_loss: 0.15281547605991364
train_iter_loss: 0.12347505241632462
train_iter_loss: 0.12720689177513123
train_iter_loss: 0.14755024015903473
train_iter_loss: 0.2504432797431946
train_iter_loss: 0.10300091654062271
train_iter_loss: 0.10898389667272568
train_iter_loss: 0.12999649345874786
train_iter_loss: 0.2705777585506439
train_iter_loss: 0.2578261196613312
train_iter_loss: 0.08967673033475876
train_iter_loss: 0.12480782717466354
train_iter_loss: 0.231201633810997
train_iter_loss: 0.1965566724538803
train_iter_loss: 0.1349671632051468
train_iter_loss: 0.17938168346881866
train_iter_loss: 0.2375047653913498
train_iter_loss: 0.1478853076696396
train_iter_loss: 0.23979224264621735
train_iter_loss: 0.1538296788930893
train_iter_loss: 0.15886911749839783
train_iter_loss: 0.29143813252449036
train_iter_loss: 0.2466774731874466
train_iter_loss: 0.14236831665039062
train_iter_loss: 0.1135282814502716
train_iter_loss: 0.061040785163640976
train_iter_loss: 0.2443951964378357
train_iter_loss: 0.12278421968221664
train_iter_loss: 0.15121108293533325
train_iter_loss: 0.16133305430412292
train_iter_loss: 0.12551818788051605
train_iter_loss: 0.20158427953720093
train_iter_loss: 0.19881312549114227
train_iter_loss: 0.1923552304506302
train_iter_loss: 0.09125367552042007
train_iter_loss: 0.08365346491336823
train_iter_loss: 0.15043625235557556
train_iter_loss: 0.16710872948169708
train_iter_loss: 0.5424728393554688
train_iter_loss: 0.2214958518743515
train_iter_loss: 0.15210551023483276
train_iter_loss: 0.13457033038139343
train_iter_loss: 0.08505916595458984
train_iter_loss: 0.28981879353523254
train_iter_loss: 0.30205288529396057
train_iter_loss: 0.18789909780025482
train_iter_loss: 0.1705106496810913
train_iter_loss: 0.2778889238834381
train_iter_loss: 0.20614269375801086
train_iter_loss: 0.09243848919868469
train_iter_loss: 0.04812362417578697
train_iter_loss: 0.13029401004314423
train_iter_loss: 0.14891417324543
train_iter_loss: 0.11579715460538864
train_iter_loss: 0.19398607313632965
train_iter_loss: 0.22108426690101624
train_iter_loss: 0.13445447385311127
train_iter_loss: 0.15430045127868652
train_iter_loss: 0.18751496076583862
train_iter_loss: 0.15771180391311646
train_iter_loss: 0.13042974472045898
train_iter_loss: 0.15069670975208282
train_iter_loss: 0.28857356309890747
train_iter_loss: 0.16995206475257874
train_iter_loss: 0.16631321609020233
train_iter_loss: 0.1455181986093521
train_iter_loss: 0.2047690749168396
train_iter_loss: 0.338942289352417
train_iter_loss: 0.21181562542915344
train_iter_loss: 0.09988938271999359
train_iter_loss: 0.10296926647424698
train_iter_loss: 0.18966645002365112
train_iter_loss: 0.13367706537246704
train_iter_loss: 0.2097514122724533
train_iter_loss: 0.14250551164150238
train_iter_loss: 0.13431675732135773
train_iter_loss: 0.09399296343326569
train_iter_loss: 0.09281078726053238
train_iter_loss: 0.19473496079444885
train_iter_loss: 0.15535901486873627
train_iter_loss: 0.13834692537784576
train_iter_loss: 0.21686944365501404
train_iter_loss: 0.18349826335906982
train loss :0.1784
---------------------
Validation seg loss: 0.22451866878512897 at epoch 178
epoch =    179/  1000, exp = train
train_iter_loss: 0.10230343788862228
train_iter_loss: 0.15285541117191315
train_iter_loss: 0.1515606939792633
train_iter_loss: 0.20389939844608307
train_iter_loss: 0.17195087671279907
train_iter_loss: 0.2537960708141327
train_iter_loss: 0.0666019394993782
train_iter_loss: 0.23488764464855194
train_iter_loss: 0.17617402970790863
train_iter_loss: 0.14896520972251892
train_iter_loss: 0.23545445501804352
train_iter_loss: 0.16952909529209137
train_iter_loss: 0.2857024073600769
train_iter_loss: 0.1729419082403183
train_iter_loss: 0.17029212415218353
train_iter_loss: 0.24054746329784393
train_iter_loss: 0.07587955892086029
train_iter_loss: 0.13268250226974487
train_iter_loss: 0.1888197511434555
train_iter_loss: 0.1083654910326004
train_iter_loss: 0.16995903849601746
train_iter_loss: 0.13473846018314362
train_iter_loss: 0.062437355518341064
train_iter_loss: 0.17287284135818481
train_iter_loss: 0.25786781311035156
train_iter_loss: 0.13980162143707275
train_iter_loss: 0.19012150168418884
train_iter_loss: 0.13660863041877747
train_iter_loss: 0.1262660026550293
train_iter_loss: 0.08715535700321198
train_iter_loss: 0.18852229416370392
train_iter_loss: 0.27493998408317566
train_iter_loss: 0.17535799741744995
train_iter_loss: 0.19845272600650787
train_iter_loss: 0.09860481321811676
train_iter_loss: 0.22364932298660278
train_iter_loss: 0.19834235310554504
train_iter_loss: 0.14143335819244385
train_iter_loss: 0.11224661767482758
train_iter_loss: 0.23042291402816772
train_iter_loss: 0.0848350003361702
train_iter_loss: 0.16607435047626495
train_iter_loss: 0.1838306486606598
train_iter_loss: 0.14171215891838074
train_iter_loss: 0.2337689995765686
train_iter_loss: 0.13135352730751038
train_iter_loss: 0.18771885335445404
train_iter_loss: 0.19461162388324738
train_iter_loss: 0.18354003131389618
train_iter_loss: 0.11579284071922302
train_iter_loss: 0.1669968217611313
train_iter_loss: 0.16109414398670197
train_iter_loss: 0.12976212799549103
train_iter_loss: 0.2432381510734558
train_iter_loss: 0.197800412774086
train_iter_loss: 0.11888548731803894
train_iter_loss: 0.11431565135717392
train_iter_loss: 0.1397877186536789
train_iter_loss: 0.19047001004219055
train_iter_loss: 0.12143157422542572
train_iter_loss: 0.07155168056488037
train_iter_loss: 0.1558542400598526
train_iter_loss: 0.1460835486650467
train_iter_loss: 0.2564491927623749
train_iter_loss: 0.2898977994918823
train_iter_loss: 0.17298556864261627
train_iter_loss: 0.2059897631406784
train_iter_loss: 0.15783776342868805
train_iter_loss: 0.2510492503643036
train_iter_loss: 0.06921400874853134
train_iter_loss: 0.18606501817703247
train_iter_loss: 0.17425164580345154
train_iter_loss: 0.11829201132059097
train_iter_loss: 0.20416571199893951
train_iter_loss: 0.19070184230804443
train_iter_loss: 0.1867612898349762
train_iter_loss: 0.15800605714321136
train_iter_loss: 0.0854623019695282
train_iter_loss: 0.13551634550094604
train_iter_loss: 0.22128427028656006
train_iter_loss: 0.13859392702579498
train_iter_loss: 0.13936924934387207
train_iter_loss: 0.14924190938472748
train_iter_loss: 0.20454072952270508
train_iter_loss: 0.17116273939609528
train_iter_loss: 0.12293911725282669
train_iter_loss: 0.18725265562534332
train_iter_loss: 0.09633815288543701
train_iter_loss: 0.1452694684267044
train_iter_loss: 0.13930892944335938
train_iter_loss: 0.200604647397995
train_iter_loss: 0.10973375290632248
train_iter_loss: 0.12664897739887238
train_iter_loss: 0.10374138504266739
train_iter_loss: 0.09080377221107483
train_iter_loss: 0.1856602430343628
train_iter_loss: 0.19251975417137146
train_iter_loss: 0.15792936086654663
train_iter_loss: 0.14928708970546722
train_iter_loss: 0.16130684316158295
train loss :0.1656
---------------------
Validation seg loss: 0.22108281834297022 at epoch 179
********************
best_val_epoch_loss:  0.22108281834297022
MODEL UPDATED
epoch =    180/  1000, exp = train
train_iter_loss: 0.13736577332019806
train_iter_loss: 0.21885034441947937
train_iter_loss: 0.21448034048080444
train_iter_loss: 0.19763140380382538
train_iter_loss: 0.23283012211322784
train_iter_loss: 0.14966535568237305
train_iter_loss: 0.09023422002792358
train_iter_loss: 0.14301685988903046
train_iter_loss: 0.1686997264623642
train_iter_loss: 0.1746719926595688
train_iter_loss: 0.08723995834589005
train_iter_loss: 0.13369448482990265
train_iter_loss: 0.12830552458763123
train_iter_loss: 0.06521283090114594
train_iter_loss: 0.1904521882534027
train_iter_loss: 0.19267454743385315
train_iter_loss: 0.31765201687812805
train_iter_loss: 0.21299651265144348
train_iter_loss: 0.2790447473526001
train_iter_loss: 0.15040816366672516
train_iter_loss: 0.2515728771686554
train_iter_loss: 0.12069103866815567
train_iter_loss: 0.16362416744232178
train_iter_loss: 0.24522821605205536
train_iter_loss: 0.1746777892112732
train_iter_loss: 0.18596726655960083
train_iter_loss: 0.1315823644399643
train_iter_loss: 0.14307384192943573
train_iter_loss: 0.16126564145088196
train_iter_loss: 0.17321495711803436
train_iter_loss: 0.17345863580703735
train_iter_loss: 0.18472342193126678
train_iter_loss: 0.08063910901546478
train_iter_loss: 0.11983160674571991
train_iter_loss: 0.11133044213056564
train_iter_loss: 0.1997891515493393
train_iter_loss: 0.06338232010602951
train_iter_loss: 0.24942952394485474
train_iter_loss: 0.2034122198820114
train_iter_loss: 0.4425554573535919
train_iter_loss: 0.09190855175256729
train_iter_loss: 0.15679214894771576
train_iter_loss: 0.1977449506521225
train_iter_loss: 0.17268642783164978
train_iter_loss: 0.19524218142032623
train_iter_loss: 0.20272567868232727
train_iter_loss: 0.09153279662132263
train_iter_loss: 0.1762966811656952
train_iter_loss: 0.09589002281427383
train_iter_loss: 0.13780540227890015
train_iter_loss: 0.17385411262512207
train_iter_loss: 0.2336340993642807
train_iter_loss: 0.12228402495384216
train_iter_loss: 0.11768358945846558
train_iter_loss: 0.11962012946605682
train_iter_loss: 0.14083822071552277
train_iter_loss: 0.13558077812194824
train_iter_loss: 0.05363042652606964
train_iter_loss: 0.2043657749891281
train_iter_loss: 0.14855177700519562
train_iter_loss: 0.28714656829833984
train_iter_loss: 0.14650385081768036
train_iter_loss: 0.10731431096792221
train_iter_loss: 0.25680217146873474
train_iter_loss: 0.15639030933380127
train_iter_loss: 0.17104311287403107
train_iter_loss: 0.21158228814601898
train_iter_loss: 0.2414892464876175
train_iter_loss: 0.09212609380483627
train_iter_loss: 0.09707828611135483
train_iter_loss: 0.11717134714126587
train_iter_loss: 0.21172314882278442
train_iter_loss: 0.12265970557928085
train_iter_loss: 0.2567293345928192
train_iter_loss: 0.1537405550479889
train_iter_loss: 0.10550049692392349
train_iter_loss: 0.12142235040664673
train_iter_loss: 0.07652387768030167
train_iter_loss: 0.1448180228471756
train_iter_loss: 0.21813800930976868
train_iter_loss: 0.22565536201000214
train_iter_loss: 0.1273072361946106
train_iter_loss: 0.14219455420970917
train_iter_loss: 0.17582181096076965
train_iter_loss: 0.42472752928733826
train_iter_loss: 0.1681128889322281
train_iter_loss: 0.09990125149488449
train_iter_loss: 0.13283638656139374
train_iter_loss: 0.2679028809070587
train_iter_loss: 0.14440251886844635
train_iter_loss: 0.10588224232196808
train_iter_loss: 0.18590770661830902
train_iter_loss: 0.22006657719612122
train_iter_loss: 0.349285751581192
train_iter_loss: 0.3250526487827301
train_iter_loss: 0.31643250584602356
train_iter_loss: 0.36225467920303345
train_iter_loss: 0.16300588846206665
train_iter_loss: 0.1894744336605072
train_iter_loss: 0.17132730782032013
train loss :0.1779
---------------------
Validation seg loss: 0.22404387973705833 at epoch 180
epoch =    181/  1000, exp = train
train_iter_loss: 0.1693815141916275
train_iter_loss: 0.16162097454071045
train_iter_loss: 0.21861332654953003
train_iter_loss: 0.1572786271572113
train_iter_loss: 0.14566899836063385
train_iter_loss: 0.10302140563726425
train_iter_loss: 0.07262812554836273
train_iter_loss: 0.1377410888671875
train_iter_loss: 0.16445079445838928
train_iter_loss: 0.1852910816669464
train_iter_loss: 0.04701618850231171
train_iter_loss: 0.16975097358226776
train_iter_loss: 0.14791597425937653
train_iter_loss: 0.1760987639427185
train_iter_loss: 0.16390985250473022
train_iter_loss: 0.1123010516166687
train_iter_loss: 0.25744518637657166
train_iter_loss: 0.20423048734664917
train_iter_loss: 0.12605378031730652
train_iter_loss: 0.2512924373149872
train_iter_loss: 0.1187230721116066
train_iter_loss: 0.17699123919010162
train_iter_loss: 0.17999812960624695
train_iter_loss: 0.20443065464496613
train_iter_loss: 0.16284126043319702
train_iter_loss: 0.1471707969903946
train_iter_loss: 0.30694156885147095
train_iter_loss: 0.19946157932281494
train_iter_loss: 0.11690988391637802
train_iter_loss: 0.1954970359802246
train_iter_loss: 0.11071857064962387
train_iter_loss: 0.12839411199092865
train_iter_loss: 0.2064591348171234
train_iter_loss: 0.06925525516271591
train_iter_loss: 0.24686169624328613
train_iter_loss: 0.21264317631721497
train_iter_loss: 0.22235126793384552
train_iter_loss: 0.1562202423810959
train_iter_loss: 0.2676233947277069
train_iter_loss: 0.1859932392835617
train_iter_loss: 0.18114493787288666
train_iter_loss: 0.22032736241817474
train_iter_loss: 0.06622055917978287
train_iter_loss: 0.13633933663368225
train_iter_loss: 0.21190491318702698
train_iter_loss: 0.19194048643112183
train_iter_loss: 0.10675328224897385
train_iter_loss: 0.12262658029794693
train_iter_loss: 0.11363200843334198
train_iter_loss: 0.09537554532289505
train_iter_loss: 0.08016977459192276
train_iter_loss: 0.212793231010437
train_iter_loss: 0.2067297250032425
train_iter_loss: 0.17571748793125153
train_iter_loss: 0.16630686819553375
train_iter_loss: 0.12022180110216141
train_iter_loss: 0.10555581003427505
train_iter_loss: 0.16938339173793793
train_iter_loss: 0.17237472534179688
train_iter_loss: 0.2370757758617401
train_iter_loss: 0.10907477885484695
train_iter_loss: 0.147271528840065
train_iter_loss: 0.210499569773674
train_iter_loss: 0.22386422753334045
train_iter_loss: 0.1323947161436081
train_iter_loss: 0.22430601716041565
train_iter_loss: 0.19070744514465332
train_iter_loss: 0.20651520788669586
train_iter_loss: 0.2812916934490204
train_iter_loss: 0.1378287971019745
train_iter_loss: 0.17810754477977753
train_iter_loss: 0.31407564878463745
train_iter_loss: 0.182205930352211
train_iter_loss: 0.1190805435180664
train_iter_loss: 0.14977267384529114
train_iter_loss: 0.22518055140972137
train_iter_loss: 0.11869900673627853
train_iter_loss: 0.1678605079650879
train_iter_loss: 0.057749126106500626
train_iter_loss: 0.14639334380626678
train_iter_loss: 0.15085645020008087
train_iter_loss: 0.20078963041305542
train_iter_loss: 0.13325135409832
train_iter_loss: 0.20008744299411774
train_iter_loss: 0.1356218457221985
train_iter_loss: 0.1554163694381714
train_iter_loss: 0.1826581507921219
train_iter_loss: 0.10692798346281052
train_iter_loss: 0.21031202375888824
train_iter_loss: 0.11165857315063477
train_iter_loss: 0.15273718535900116
train_iter_loss: 0.17589828372001648
train_iter_loss: 0.10109730809926987
train_iter_loss: 0.168225958943367
train_iter_loss: 0.23507028818130493
train_iter_loss: 0.33948370814323425
train_iter_loss: 0.1802191287279129
train_iter_loss: 0.12638305127620697
train_iter_loss: 0.07092077285051346
train_iter_loss: 0.0774126946926117
train loss :0.1680
---------------------
Validation seg loss: 0.22283263759300956 at epoch 181
epoch =    182/  1000, exp = train
train_iter_loss: 0.1138443574309349
train_iter_loss: 0.14553627371788025
train_iter_loss: 0.16010628640651703
train_iter_loss: 0.14217409491539001
train_iter_loss: 0.11618974804878235
train_iter_loss: 0.32073983550071716
train_iter_loss: 0.1922326385974884
train_iter_loss: 0.15189670026302338
train_iter_loss: 0.1262224018573761
train_iter_loss: 0.2937988042831421
train_iter_loss: 0.1096448004245758
train_iter_loss: 0.14920596778392792
train_iter_loss: 0.20110394060611725
train_iter_loss: 0.14667151868343353
train_iter_loss: 0.18563812971115112
train_iter_loss: 0.22116366028785706
train_iter_loss: 0.14289666712284088
train_iter_loss: 0.22681961953639984
train_iter_loss: 0.21398569643497467
train_iter_loss: 0.1629369854927063
train_iter_loss: 0.127144455909729
train_iter_loss: 0.15276038646697998
train_iter_loss: 0.1255934238433838
train_iter_loss: 0.18017493188381195
train_iter_loss: 0.23845304548740387
train_iter_loss: 0.16063250601291656
train_iter_loss: 0.11785387992858887
train_iter_loss: 0.1306508481502533
train_iter_loss: 0.22261351346969604
train_iter_loss: 0.11185207217931747
train_iter_loss: 0.11342893540859222
train_iter_loss: 0.18437452614307404
train_iter_loss: 0.2006637156009674
train_iter_loss: 0.20836366713047028
train_iter_loss: 0.13659629225730896
train_iter_loss: 0.09343978762626648
train_iter_loss: 0.13469582796096802
train_iter_loss: 0.15359452366828918
train_iter_loss: 0.12098821252584457
train_iter_loss: 0.197300985455513
train_iter_loss: 0.2003227323293686
train_iter_loss: 0.10826680809259415
train_iter_loss: 0.1554371416568756
train_iter_loss: 0.15159709751605988
train_iter_loss: 0.3726482689380646
train_iter_loss: 0.19052532315254211
train_iter_loss: 0.23848053812980652
train_iter_loss: 0.34203606843948364
train_iter_loss: 0.1289300173521042
train_iter_loss: 0.2546590268611908
train_iter_loss: 0.14169517159461975
train_iter_loss: 0.12309239059686661
train_iter_loss: 0.1931484490633011
train_iter_loss: 0.13631246984004974
train_iter_loss: 0.370962530374527
train_iter_loss: 0.13280053436756134
train_iter_loss: 0.12708638608455658
train_iter_loss: 0.18867318332195282
train_iter_loss: 0.17986233532428741
train_iter_loss: 0.2848023474216461
train_iter_loss: 0.1381884068250656
train_iter_loss: 0.11902308464050293
train_iter_loss: 0.11200827360153198
train_iter_loss: 0.09696930646896362
train_iter_loss: 0.24946004152297974
train_iter_loss: 0.13442057371139526
train_iter_loss: 0.24274244904518127
train_iter_loss: 0.26255425810813904
train_iter_loss: 0.17172382771968842
train_iter_loss: 0.09227731823921204
train_iter_loss: 0.1496555060148239
train_iter_loss: 0.15681599080562592
train_iter_loss: 0.1655692607164383
train_iter_loss: 0.12263541668653488
train_iter_loss: 0.12092430889606476
train_iter_loss: 0.34323635697364807
train_iter_loss: 0.1279781013727188
train_iter_loss: 0.18710534274578094
train_iter_loss: 0.1534387469291687
train_iter_loss: 0.08284012973308563
train_iter_loss: 0.14100190997123718
train_iter_loss: 0.10248720645904541
train_iter_loss: 0.13043704628944397
train_iter_loss: 0.3487200140953064
train_iter_loss: 0.08580870181322098
train_iter_loss: 0.15410979092121124
train_iter_loss: 0.12269486486911774
train_iter_loss: 0.24438264966011047
train_iter_loss: 0.34447115659713745
train_iter_loss: 0.18476048111915588
train_iter_loss: 0.2208574265241623
train_iter_loss: 0.3043261468410492
train_iter_loss: 0.14970195293426514
train_iter_loss: 0.0948001965880394
train_iter_loss: 0.18322014808654785
train_iter_loss: 0.055688343942165375
train_iter_loss: 0.17951732873916626
train_iter_loss: 0.1362810879945755
train_iter_loss: 0.1613987237215042
train_iter_loss: 0.094589002430439
train loss :0.1749
---------------------
Validation seg loss: 0.22174350290893102 at epoch 182
epoch =    183/  1000, exp = train
train_iter_loss: 0.17162589728832245
train_iter_loss: 0.16132862865924835
train_iter_loss: 0.11424503475427628
train_iter_loss: 0.184663787484169
train_iter_loss: 0.34059110283851624
train_iter_loss: 0.16095174849033356
train_iter_loss: 0.18688105046749115
train_iter_loss: 0.18335945904254913
train_iter_loss: 0.21524731814861298
train_iter_loss: 0.2276601791381836
train_iter_loss: 0.22634707391262054
train_iter_loss: 0.08863507211208344
train_iter_loss: 0.12681439518928528
train_iter_loss: 0.1832619458436966
train_iter_loss: 0.18104414641857147
train_iter_loss: 0.2640926241874695
train_iter_loss: 0.12510573863983154
train_iter_loss: 0.08048540353775024
train_iter_loss: 0.14154313504695892
train_iter_loss: 0.11004546284675598
train_iter_loss: 0.15275844931602478
train_iter_loss: 0.19044192135334015
train_iter_loss: 0.11906698346138
train_iter_loss: 0.2411416620016098
train_iter_loss: 0.20979955792427063
train_iter_loss: 0.0950746238231659
train_iter_loss: 0.21414634585380554
train_iter_loss: 0.11066332459449768
train_iter_loss: 0.18260034918785095
train_iter_loss: 0.13363422453403473
train_iter_loss: 0.1700693666934967
train_iter_loss: 0.17086215317249298
train_iter_loss: 0.2342606484889984
train_iter_loss: 0.1578388661146164
train_iter_loss: 0.11209513992071152
train_iter_loss: 0.26776620745658875
train_iter_loss: 0.06586794555187225
train_iter_loss: 0.09883520752191544
train_iter_loss: 0.19094102084636688
train_iter_loss: 0.2757369577884674
train_iter_loss: 0.07771383970975876
train_iter_loss: 0.3442980647087097
train_iter_loss: 0.24264807999134064
train_iter_loss: 0.10603403300046921
train_iter_loss: 0.10660966485738754
train_iter_loss: 0.16494838893413544
train_iter_loss: 0.0986892357468605
train_iter_loss: 0.24647989869117737
train_iter_loss: 0.24402330815792084
train_iter_loss: 0.2219434678554535
train_iter_loss: 0.15728385746479034
train_iter_loss: 0.12719891965389252
train_iter_loss: 0.1798657476902008
train_iter_loss: 0.13555072247982025
train_iter_loss: 0.1706705391407013
train_iter_loss: 0.09636856615543365
train_iter_loss: 0.3427065312862396
train_iter_loss: 0.21012131869792938
train_iter_loss: 0.18933606147766113
train_iter_loss: 0.11808162927627563
train_iter_loss: 0.19308920204639435
train_iter_loss: 0.13353148102760315
train_iter_loss: 0.15687017142772675
train_iter_loss: 0.4044192433357239
train_iter_loss: 0.23313894867897034
train_iter_loss: 0.10057273507118225
train_iter_loss: 0.10753808170557022
train_iter_loss: 0.0977730005979538
train_iter_loss: 0.22265076637268066
train_iter_loss: 0.18065626919269562
train_iter_loss: 0.15729953348636627
train_iter_loss: 0.2053590565919876
train_iter_loss: 0.15012136101722717
train_iter_loss: 0.17441539466381073
train_iter_loss: 0.16397078335285187
train_iter_loss: 0.14760516583919525
train_iter_loss: 0.09915166348218918
train_iter_loss: 0.20566311478614807
train_iter_loss: 0.12879355251789093
train_iter_loss: 0.1575530767440796
train_iter_loss: 0.21356531977653503
train_iter_loss: 0.1704748421907425
train_iter_loss: 0.14044830203056335
train_iter_loss: 0.17161984741687775
train_iter_loss: 0.21613484621047974
train_iter_loss: 0.13525208830833435
train_iter_loss: 0.16439850628376007
train_iter_loss: 0.09940832108259201
train_iter_loss: 0.12932433187961578
train_iter_loss: 0.16030825674533844
train_iter_loss: 0.1464410275220871
train_iter_loss: 0.15866200625896454
train_iter_loss: 0.21458542346954346
train_iter_loss: 0.19371581077575684
train_iter_loss: 0.09767468273639679
train_iter_loss: 0.07952383160591125
train_iter_loss: 0.16000433266162872
train_iter_loss: 0.2103121131658554
train_iter_loss: 0.2408827841281891
train_iter_loss: 0.14301154017448425
train loss :0.1729
---------------------
Validation seg loss: 0.2234181548416052 at epoch 183
epoch =    184/  1000, exp = train
train_iter_loss: 0.15612323582172394
train_iter_loss: 0.12075848132371902
train_iter_loss: 0.18181516230106354
train_iter_loss: 0.18632018566131592
train_iter_loss: 0.19072572886943817
train_iter_loss: 0.19300204515457153
train_iter_loss: 0.20726127922534943
train_iter_loss: 0.3459002375602722
train_iter_loss: 0.13567745685577393
train_iter_loss: 0.15665602684020996
train_iter_loss: 0.21600006520748138
train_iter_loss: 0.1115652546286583
train_iter_loss: 0.21972516179084778
train_iter_loss: 0.09675577282905579
train_iter_loss: 0.1501682549715042
train_iter_loss: 0.21259061992168427
train_iter_loss: 0.10029703378677368
train_iter_loss: 0.12577693164348602
train_iter_loss: 0.15071335434913635
train_iter_loss: 0.19248874485492706
train_iter_loss: 0.1594528704881668
train_iter_loss: 0.15463528037071228
train_iter_loss: 0.13871316611766815
train_iter_loss: 0.0932910367846489
train_iter_loss: 0.09305740892887115
train_iter_loss: 0.19902324676513672
train_iter_loss: 0.1803334504365921
train_iter_loss: 0.19830608367919922
train_iter_loss: 0.07648318260908127
train_iter_loss: 0.0917544811964035
train_iter_loss: 0.09544704109430313
train_iter_loss: 0.2026437222957611
train_iter_loss: 0.17074590921401978
train_iter_loss: 0.23662501573562622
train_iter_loss: 0.17946074903011322
train_iter_loss: 0.12633274495601654
train_iter_loss: 0.09718693792819977
train_iter_loss: 0.17568957805633545
train_iter_loss: 0.16286881268024445
train_iter_loss: 0.08965915441513062
train_iter_loss: 0.10798665881156921
train_iter_loss: 0.21452729403972626
train_iter_loss: 0.27860432863235474
train_iter_loss: 0.2134173959493637
train_iter_loss: 0.1600641906261444
train_iter_loss: 0.2072189301252365
train_iter_loss: 0.05872951075434685
train_iter_loss: 0.11997028440237045
train_iter_loss: 0.15133163332939148
train_iter_loss: 0.13565406203269958
train_iter_loss: 0.22440209984779358
train_iter_loss: 0.2089294195175171
train_iter_loss: 0.25706514716148376
train_iter_loss: 0.1257351189851761
train_iter_loss: 0.06651119887828827
train_iter_loss: 0.15920084714889526
train_iter_loss: 0.1483006775379181
train_iter_loss: 0.1629965454339981
train_iter_loss: 0.1725447177886963
train_iter_loss: 0.19772791862487793
train_iter_loss: 0.14534518122673035
train_iter_loss: 0.14600376784801483
train_iter_loss: 0.15609405934810638
train_iter_loss: 0.19668430089950562
train_iter_loss: 0.1148083284497261
train_iter_loss: 0.10709887742996216
train_iter_loss: 0.24959413707256317
train_iter_loss: 0.25602051615715027
train_iter_loss: 0.10571073740720749
train_iter_loss: 0.20060181617736816
train_iter_loss: 0.2503037452697754
train_iter_loss: 0.3267650008201599
train_iter_loss: 0.2992095649242401
train_iter_loss: 0.20253057777881622
train_iter_loss: 0.1359729766845703
train_iter_loss: 0.16677922010421753
train_iter_loss: 0.137254998087883
train_iter_loss: 0.16690024733543396
train_iter_loss: 0.31654468178749084
train_iter_loss: 0.15250584483146667
train_iter_loss: 0.2218339592218399
train_iter_loss: 0.16781307756900787
train_iter_loss: 0.22948431968688965
train_iter_loss: 0.09466652572154999
train_iter_loss: 0.1455240398645401
train_iter_loss: 0.14926329255104065
train_iter_loss: 0.1592877060174942
train_iter_loss: 0.17440833151340485
train_iter_loss: 0.2543598413467407
train_iter_loss: 0.1602364182472229
train_iter_loss: 0.0978841707110405
train_iter_loss: 0.23056954145431519
train_iter_loss: 0.15033826231956482
train_iter_loss: 0.07542227953672409
train_iter_loss: 0.17472894489765167
train_iter_loss: 0.19692280888557434
train_iter_loss: 0.18020781874656677
train_iter_loss: 0.0880904495716095
train_iter_loss: 0.12578622996807098
train_iter_loss: 0.1434868425130844
train loss :0.1697
---------------------
Validation seg loss: 0.22392573439568844 at epoch 184
epoch =    185/  1000, exp = train
train_iter_loss: 0.21196623146533966
train_iter_loss: 0.25091513991355896
train_iter_loss: 0.2152324616909027
train_iter_loss: 0.5131926536560059
train_iter_loss: 0.12582530081272125
train_iter_loss: 0.21917824447155
train_iter_loss: 0.21503494679927826
train_iter_loss: 0.1515260934829712
train_iter_loss: 0.1298137903213501
train_iter_loss: 0.17143315076828003
train_iter_loss: 0.18939411640167236
train_iter_loss: 0.11740144342184067
train_iter_loss: 0.23769208788871765
train_iter_loss: 0.13989345729351044
train_iter_loss: 0.16713488101959229
train_iter_loss: 0.1710757315158844
train_iter_loss: 0.2747214734554291
train_iter_loss: 0.10502579063177109
train_iter_loss: 0.08706321567296982
train_iter_loss: 0.1660987138748169
train_iter_loss: 0.18262429535388947
train_iter_loss: 0.0766485184431076
train_iter_loss: 0.13367187976837158
train_iter_loss: 0.1868855357170105
train_iter_loss: 0.15360039472579956
train_iter_loss: 0.11868397891521454
train_iter_loss: 0.14108900725841522
train_iter_loss: 0.12591014802455902
train_iter_loss: 0.12341386079788208
train_iter_loss: 0.15690895915031433
train_iter_loss: 0.1434943825006485
train_iter_loss: 0.10847017168998718
train_iter_loss: 0.14850999414920807
train_iter_loss: 0.14239849150180817
train_iter_loss: 0.11209353059530258
train_iter_loss: 0.22736388444900513
train_iter_loss: 0.1855260729789734
train_iter_loss: 0.28948551416397095
train_iter_loss: 0.20095784962177277
train_iter_loss: 0.23372004926204681
train_iter_loss: 0.13296781480312347
train_iter_loss: 0.15083378553390503
train_iter_loss: 0.2381194531917572
train_iter_loss: 0.36435627937316895
train_iter_loss: 0.11332739144563675
train_iter_loss: 0.23860466480255127
train_iter_loss: 0.20743884146213531
train_iter_loss: 0.13708695769309998
train_iter_loss: 0.14539484679698944
train_iter_loss: 0.20706850290298462
train_iter_loss: 0.12785041332244873
train_iter_loss: 0.18257802724838257
train_iter_loss: 0.10982176661491394
train_iter_loss: 0.10194207727909088
train_iter_loss: 0.13714981079101562
train_iter_loss: 0.08694805204868317
train_iter_loss: 0.13704635202884674
train_iter_loss: 0.1817360520362854
train_iter_loss: 0.17575211822986603
train_iter_loss: 0.19288372993469238
train_iter_loss: 0.06718286871910095
train_iter_loss: 0.19437922537326813
train_iter_loss: 0.2197771817445755
train_iter_loss: 0.20839743316173553
train_iter_loss: 0.34401756525039673
train_iter_loss: 0.18986444175243378
train_iter_loss: 0.08398739993572235
train_iter_loss: 0.1929919272661209
train_iter_loss: 0.1289135217666626
train_iter_loss: 0.2117568850517273
train_iter_loss: 0.07194569706916809
train_iter_loss: 0.10154970735311508
train_iter_loss: 0.12318240851163864
train_iter_loss: 0.19244737923145294
train_iter_loss: 0.10329954326152802
train_iter_loss: 0.15592893958091736
train_iter_loss: 0.16977770626544952
train_iter_loss: 0.11957502365112305
train_iter_loss: 0.1775384396314621
train_iter_loss: 0.2994913160800934
train_iter_loss: 0.122846819460392
train_iter_loss: 0.16220885515213013
train_iter_loss: 0.1484324187040329
train_iter_loss: 0.18084804713726044
train_iter_loss: 0.1839817315340042
train_iter_loss: 0.10934092104434967
train_iter_loss: 0.06564870476722717
train_iter_loss: 0.1713973879814148
train_iter_loss: 0.18908025324344635
train_iter_loss: 0.2022913545370102
train_iter_loss: 0.1393873691558838
train_iter_loss: 0.36072760820388794
train_iter_loss: 0.2390611171722412
train_iter_loss: 0.19355051219463348
train_iter_loss: 0.14201323688030243
train_iter_loss: 0.13933122158050537
train_iter_loss: 0.13613943755626678
train_iter_loss: 0.11249285191297531
train_iter_loss: 0.18787755072116852
train_iter_loss: 0.1396659016609192
train loss :0.1727
---------------------
Validation seg loss: 0.22442667261820357 at epoch 185
epoch =    186/  1000, exp = train
train_iter_loss: 0.3075026273727417
train_iter_loss: 0.1350737065076828
train_iter_loss: 0.19700469076633453
train_iter_loss: 0.24462158977985382
train_iter_loss: 0.26217538118362427
train_iter_loss: 0.11476581543684006
train_iter_loss: 0.14906351268291473
train_iter_loss: 0.10566052794456482
train_iter_loss: 0.1430983692407608
train_iter_loss: 0.17270034551620483
train_iter_loss: 0.12669450044631958
train_iter_loss: 0.21887262165546417
train_iter_loss: 0.1154390200972557
train_iter_loss: 0.2968844175338745
train_iter_loss: 0.14431874454021454
train_iter_loss: 0.2958807051181793
train_iter_loss: 0.11215969920158386
train_iter_loss: 0.1785961091518402
train_iter_loss: 0.14600269496440887
train_iter_loss: 0.12828277051448822
train_iter_loss: 0.1276240348815918
train_iter_loss: 0.2906331419944763
train_iter_loss: 0.27792471647262573
train_iter_loss: 0.13848638534545898
train_iter_loss: 0.22253431379795074
train_iter_loss: 0.17886072397232056
train_iter_loss: 0.11331567913293839
train_iter_loss: 0.2835918962955475
train_iter_loss: 0.1915518045425415
train_iter_loss: 0.16036279499530792
train_iter_loss: 0.13294345140457153
train_iter_loss: 0.050172992050647736
train_iter_loss: 0.14565272629261017
train_iter_loss: 0.3215075731277466
train_iter_loss: 0.0849771499633789
train_iter_loss: 0.23030845820903778
train_iter_loss: 0.08868242055177689
train_iter_loss: 0.11303981393575668
train_iter_loss: 0.08083692938089371
train_iter_loss: 0.1260620802640915
train_iter_loss: 0.1259358674287796
train_iter_loss: 0.21035559475421906
train_iter_loss: 0.16834180057048798
train_iter_loss: 0.17568691074848175
train_iter_loss: 0.24841056764125824
train_iter_loss: 0.17013846337795258
train_iter_loss: 0.10727812349796295
train_iter_loss: 0.14699015021324158
train_iter_loss: 0.19997091591358185
train_iter_loss: 0.08802372962236404
train_iter_loss: 0.412160187959671
train_iter_loss: 0.19111309945583344
train_iter_loss: 0.1236385852098465
train_iter_loss: 0.1685560792684555
train_iter_loss: 0.1417139768600464
train_iter_loss: 0.2068939208984375
train_iter_loss: 0.2520384192466736
train_iter_loss: 0.24073809385299683
train_iter_loss: 0.16623757779598236
train_iter_loss: 0.17812035977840424
train_iter_loss: 0.13680973649024963
train_iter_loss: 0.2654033303260803
train_iter_loss: 0.16592594981193542
train_iter_loss: 0.12846162915229797
train_iter_loss: 0.15776722133159637
train_iter_loss: 0.31035715341567993
train_iter_loss: 0.18154440820217133
train_iter_loss: 0.12804412841796875
train_iter_loss: 0.23030687868595123
train_iter_loss: 0.20203150808811188
train_iter_loss: 0.15804323554039001
train_iter_loss: 0.14058895409107208
train_iter_loss: 0.10703188925981522
train_iter_loss: 0.09929656982421875
train_iter_loss: 0.13745835423469543
train_iter_loss: 0.07743509113788605
train_iter_loss: 0.15349483489990234
train_iter_loss: 0.23037025332450867
train_iter_loss: 0.18629814684391022
train_iter_loss: 0.236966073513031
train_iter_loss: 0.20464617013931274
train_iter_loss: 0.17192032933235168
train_iter_loss: 0.1112237200140953
train_iter_loss: 0.13856518268585205
train_iter_loss: 0.1862160563468933
train_iter_loss: 0.08131060004234314
train_iter_loss: 0.26817041635513306
train_iter_loss: 0.1373622864484787
train_iter_loss: 0.11397520452737808
train_iter_loss: 0.20718279480934143
train_iter_loss: 0.22210142016410828
train_iter_loss: 0.06390654295682907
train_iter_loss: 0.13592371344566345
train_iter_loss: 0.19423577189445496
train_iter_loss: 0.20364098250865936
train_iter_loss: 0.1962422877550125
train_iter_loss: 0.23664490878582
train_iter_loss: 0.1791706681251526
train_iter_loss: 0.15527763962745667
train_iter_loss: 0.13433799147605896
train loss :0.1762
---------------------
Validation seg loss: 0.22058730765755447 at epoch 186
********************
best_val_epoch_loss:  0.22058730765755447
MODEL UPDATED
epoch =    187/  1000, exp = train
train_iter_loss: 0.09548717737197876
train_iter_loss: 0.11811228096485138
train_iter_loss: 0.18093229830265045
train_iter_loss: 0.3048489987850189
train_iter_loss: 0.16666419804096222
train_iter_loss: 0.12205594033002853
train_iter_loss: 0.3094688057899475
train_iter_loss: 0.22396476566791534
train_iter_loss: 0.14374032616615295
train_iter_loss: 0.12880919873714447
train_iter_loss: 0.21675920486450195
train_iter_loss: 0.17797906696796417
train_iter_loss: 0.1512766182422638
train_iter_loss: 0.16831474006175995
train_iter_loss: 0.15287663042545319
train_iter_loss: 0.06201278418302536
train_iter_loss: 0.11730398237705231
train_iter_loss: 0.07919690757989883
train_iter_loss: 0.12522540986537933
train_iter_loss: 0.1486414223909378
train_iter_loss: 0.1935279369354248
train_iter_loss: 0.11363522708415985
train_iter_loss: 0.143734410405159
train_iter_loss: 0.07022811472415924
train_iter_loss: 0.265460729598999
train_iter_loss: 0.12396331876516342
train_iter_loss: 0.25679585337638855
train_iter_loss: 0.10417905449867249
train_iter_loss: 0.1350204348564148
train_iter_loss: 0.2763952612876892
train_iter_loss: 0.15138979256153107
train_iter_loss: 0.3815948963165283
train_iter_loss: 0.22176310420036316
train_iter_loss: 0.25195252895355225
train_iter_loss: 0.17326031625270844
train_iter_loss: 0.1428598016500473
train_iter_loss: 0.14285703003406525
train_iter_loss: 0.22261396050453186
train_iter_loss: 0.1578121930360794
train_iter_loss: 0.2041267305612564
train_iter_loss: 0.1414330005645752
train_iter_loss: 0.12237059324979782
train_iter_loss: 0.2203318178653717
train_iter_loss: 0.1478491723537445
train_iter_loss: 0.1889600306749344
train_iter_loss: 0.10178624093532562
train_iter_loss: 0.1917261779308319
train_iter_loss: 0.15182791650295258
train_iter_loss: 0.19215355813503265
train_iter_loss: 0.08937972038984299
train_iter_loss: 0.16513749957084656
train_iter_loss: 0.12365517020225525
train_iter_loss: 0.33222725987434387
train_iter_loss: 0.3949177861213684
train_iter_loss: 0.21018601953983307
train_iter_loss: 0.3871660828590393
train_iter_loss: 0.18067479133605957
train_iter_loss: 0.20031996071338654
train_iter_loss: 0.09058961272239685
train_iter_loss: 0.16819186508655548
train_iter_loss: 0.1007438525557518
train_iter_loss: 0.16198864579200745
train_iter_loss: 0.12372872233390808
train_iter_loss: 0.037070196121931076
train_iter_loss: 0.3614576458930969
train_iter_loss: 0.19317574799060822
train_iter_loss: 0.11482939124107361
train_iter_loss: 0.16761477291584015
train_iter_loss: 0.2078453004360199
train_iter_loss: 0.13399004936218262
train_iter_loss: 0.17651741206645966
train_iter_loss: 0.22205302119255066
train_iter_loss: 0.19707991182804108
train_iter_loss: 0.09957873821258545
train_iter_loss: 0.1707431972026825
train_iter_loss: 0.17767520248889923
train_iter_loss: 0.0860585942864418
train_iter_loss: 0.1663583368062973
train_iter_loss: 0.13831506669521332
train_iter_loss: 0.15064559876918793
train_iter_loss: 0.29310405254364014
train_iter_loss: 0.1059885248541832
train_iter_loss: 0.21385528147220612
train_iter_loss: 0.17655602097511292
train_iter_loss: 0.20320941507816315
train_iter_loss: 0.10868214070796967
train_iter_loss: 0.15579190850257874
train_iter_loss: 0.275812566280365
train_iter_loss: 0.21272540092468262
train_iter_loss: 0.2060600221157074
train_iter_loss: 0.15210336446762085
train_iter_loss: 0.19671697914600372
train_iter_loss: 0.3491506278514862
train_iter_loss: 0.136049285531044
train_iter_loss: 0.2373938113451004
train_iter_loss: 0.2297288179397583
train_iter_loss: 0.23727667331695557
train_iter_loss: 0.15808206796646118
train_iter_loss: 0.1785280555486679
train_iter_loss: 0.15026648342609406
train loss :0.1807
---------------------
Validation seg loss: 0.22576107394020511 at epoch 187
epoch =    188/  1000, exp = train
train_iter_loss: 0.10611573606729507
train_iter_loss: 0.07536478340625763
train_iter_loss: 0.09931822866201401
train_iter_loss: 0.16343584656715393
train_iter_loss: 0.43555793166160583
train_iter_loss: 0.1942308396100998
train_iter_loss: 0.22548450529575348
train_iter_loss: 0.13687974214553833
train_iter_loss: 0.16128452122211456
train_iter_loss: 0.17400778830051422
train_iter_loss: 0.16431480646133423
train_iter_loss: 0.12006745487451553
train_iter_loss: 0.1990567147731781
train_iter_loss: 0.16106580197811127
train_iter_loss: 0.25674813985824585
train_iter_loss: 0.1413947194814682
train_iter_loss: 0.1493670791387558
train_iter_loss: 0.1571127474308014
train_iter_loss: 0.11961137503385544
train_iter_loss: 0.2138320952653885
train_iter_loss: 0.10877328366041183
train_iter_loss: 0.1629897654056549
train_iter_loss: 0.06914421916007996
train_iter_loss: 0.25092339515686035
train_iter_loss: 0.13799400627613068
train_iter_loss: 0.1911083310842514
train_iter_loss: 0.10212070494890213
train_iter_loss: 0.18273387849330902
train_iter_loss: 0.13417614996433258
train_iter_loss: 0.11434859037399292
train_iter_loss: 0.1690577119588852
train_iter_loss: 0.07918895035982132
train_iter_loss: 0.15774570405483246
train_iter_loss: 0.11924935132265091
train_iter_loss: 0.0943373441696167
train_iter_loss: 0.23405258357524872
train_iter_loss: 0.14130420982837677
train_iter_loss: 0.08707628399133682
train_iter_loss: 0.08873727172613144
train_iter_loss: 0.25586357712745667
train_iter_loss: 0.12137406319379807
train_iter_loss: 0.13642717897891998
train_iter_loss: 0.3038126528263092
train_iter_loss: 0.16661956906318665
train_iter_loss: 0.17425398528575897
train_iter_loss: 0.11227019876241684
train_iter_loss: 0.13156753778457642
train_iter_loss: 0.11213571578264236
train_iter_loss: 0.13212832808494568
train_iter_loss: 0.25605133175849915
train_iter_loss: 0.26039624214172363
train_iter_loss: 0.12549953162670135
train_iter_loss: 0.3298393189907074
train_iter_loss: 0.11327040195465088
train_iter_loss: 0.15606531500816345
train_iter_loss: 0.12097694724798203
train_iter_loss: 0.19305939972400665
train_iter_loss: 0.18664225935935974
train_iter_loss: 0.21076273918151855
train_iter_loss: 0.13683289289474487
train_iter_loss: 0.1993594765663147
train_iter_loss: 0.11264853179454803
train_iter_loss: 0.06324063986539841
train_iter_loss: 0.13347448408603668
train_iter_loss: 0.09705176949501038
train_iter_loss: 0.1180737242102623
train_iter_loss: 0.14123834669589996
train_iter_loss: 0.3195542097091675
train_iter_loss: 0.09337925165891647
train_iter_loss: 0.19458670914173126
train_iter_loss: 0.14159031212329865
train_iter_loss: 0.19614280760288239
train_iter_loss: 0.14712759852409363
train_iter_loss: 0.10026774555444717
train_iter_loss: 0.198415607213974
train_iter_loss: 0.1787128448486328
train_iter_loss: 0.17506060004234314
train_iter_loss: 0.1186697855591774
train_iter_loss: 0.15405376255512238
train_iter_loss: 0.3384608030319214
train_iter_loss: 0.1487254500389099
train_iter_loss: 0.24378637969493866
train_iter_loss: 0.17793673276901245
train_iter_loss: 0.21637478470802307
train_iter_loss: 0.1901678889989853
train_iter_loss: 0.22501789033412933
train_iter_loss: 0.11299975961446762
train_iter_loss: 0.19023357331752777
train_iter_loss: 0.1991160809993744
train_iter_loss: 0.19966885447502136
train_iter_loss: 0.26760464906692505
train_iter_loss: 0.18689273297786713
train_iter_loss: 0.25589028000831604
train_iter_loss: 0.10875917226076126
train_iter_loss: 0.25132614374160767
train_iter_loss: 0.1885402500629425
train_iter_loss: 0.25037142634391785
train_iter_loss: 0.07280673831701279
train_iter_loss: 0.24660015106201172
train_iter_loss: 0.20536531507968903
train loss :0.1716
---------------------
Validation seg loss: 0.22625733297845385 at epoch 188
epoch =    189/  1000, exp = train
train_iter_loss: 0.19932633638381958
train_iter_loss: 0.13254573941230774
train_iter_loss: 0.24197107553482056
train_iter_loss: 0.12686099112033844
train_iter_loss: 0.23777545988559723
train_iter_loss: 0.20115721225738525
train_iter_loss: 0.05846712365746498
train_iter_loss: 0.2652186453342438
train_iter_loss: 0.1899467259645462
train_iter_loss: 0.20856411755084991
train_iter_loss: 0.22583872079849243
train_iter_loss: 0.21828998625278473
train_iter_loss: 0.22594119608402252
train_iter_loss: 0.10626915842294693
train_iter_loss: 0.06734766811132431
train_iter_loss: 0.140821635723114
train_iter_loss: 0.10713806003332138
train_iter_loss: 0.10750921815633774
train_iter_loss: 0.19177204370498657
train_iter_loss: 0.14498098194599152
train_iter_loss: 0.09618718177080154
train_iter_loss: 0.051185205578804016
train_iter_loss: 0.13217608630657196
train_iter_loss: 0.21655523777008057
train_iter_loss: 0.2163618505001068
train_iter_loss: 0.1741521805524826
train_iter_loss: 0.197111576795578
train_iter_loss: 0.23189231753349304
train_iter_loss: 0.15725275874137878
train_iter_loss: 0.25560832023620605
train_iter_loss: 0.09772313386201859
train_iter_loss: 0.11757116764783859
train_iter_loss: 0.17889146506786346
train_iter_loss: 0.1557578146457672
train_iter_loss: 0.17776955664157867
train_iter_loss: 0.1315506398677826
train_iter_loss: 0.11624158173799515
train_iter_loss: 0.13968151807785034
train_iter_loss: 0.1665651798248291
train_iter_loss: 0.09893077611923218
train_iter_loss: 0.1666218489408493
train_iter_loss: 0.21640028059482574
train_iter_loss: 0.16222921013832092
train_iter_loss: 0.15123209357261658
train_iter_loss: 0.17282186448574066
train_iter_loss: 0.20874406397342682
train_iter_loss: 0.3889012634754181
train_iter_loss: 0.23151512444019318
train_iter_loss: 0.19070230424404144
train_iter_loss: 0.19204777479171753
train_iter_loss: 0.15587960183620453
train_iter_loss: 0.19231899082660675
train_iter_loss: 0.1699472814798355
train_iter_loss: 0.30064597725868225
train_iter_loss: 0.3468243479728699
train_iter_loss: 0.20371653139591217
train_iter_loss: 0.12577082216739655
train_iter_loss: 0.17171582579612732
train_iter_loss: 0.1339111030101776
train_iter_loss: 0.17315396666526794
train_iter_loss: 0.22412708401679993
train_iter_loss: 0.2123493254184723
train_iter_loss: 0.10518264025449753
train_iter_loss: 0.17028388381004333
train_iter_loss: 0.1784704029560089
train_iter_loss: 0.20033124089241028
train_iter_loss: 0.22325636446475983
train_iter_loss: 0.1992601603269577
train_iter_loss: 0.08972161263227463
train_iter_loss: 0.10904088616371155
train_iter_loss: 0.11530552804470062
train_iter_loss: 0.11903590708971024
train_iter_loss: 0.17885859310626984
train_iter_loss: 0.19024160504341125
train_iter_loss: 0.18521973490715027
train_iter_loss: 0.2203248292207718
train_iter_loss: 0.17568691074848175
train_iter_loss: 0.10369725525379181
train_iter_loss: 0.23841722309589386
train_iter_loss: 0.1898694932460785
train_iter_loss: 0.22712619602680206
train_iter_loss: 0.13854563236236572
train_iter_loss: 0.17489084601402283
train_iter_loss: 0.23095263540744781
train_iter_loss: 0.11396746337413788
train_iter_loss: 0.11959754675626755
train_iter_loss: 0.11605554074048996
train_iter_loss: 0.25055912137031555
train_iter_loss: 0.13889917731285095
train_iter_loss: 0.15237796306610107
train_iter_loss: 0.17724627256393433
train_iter_loss: 0.3969263732433319
train_iter_loss: 0.102395199239254
train_iter_loss: 0.1355866938829422
train_iter_loss: 0.1755376011133194
train_iter_loss: 0.27064672112464905
train_iter_loss: 0.14957016706466675
train_iter_loss: 0.3813444674015045
train_iter_loss: 0.09162847697734833
train_iter_loss: 0.06480714678764343
train loss :0.1777
---------------------
Validation seg loss: 0.22223535682654605 at epoch 189
epoch =    190/  1000, exp = train
train_iter_loss: 0.24573329091072083
train_iter_loss: 0.10072431713342667
train_iter_loss: 0.20330190658569336
train_iter_loss: 0.07102169096469879
train_iter_loss: 0.17911787331104279
train_iter_loss: 0.24331872165203094
train_iter_loss: 0.2575990557670593
train_iter_loss: 0.16169792413711548
train_iter_loss: 0.2574314475059509
train_iter_loss: 0.24066832661628723
train_iter_loss: 0.2284102439880371
train_iter_loss: 0.11349745094776154
train_iter_loss: 0.20269474387168884
train_iter_loss: 0.2999507784843445
train_iter_loss: 0.10243470221757889
train_iter_loss: 0.142693430185318
train_iter_loss: 0.08621136844158173
train_iter_loss: 0.16554304957389832
train_iter_loss: 0.07335279136896133
train_iter_loss: 0.23698586225509644
train_iter_loss: 0.07488355040550232
train_iter_loss: 0.10737597197294235
train_iter_loss: 0.10890298336744308
train_iter_loss: 0.1667805314064026
train_iter_loss: 0.1761145442724228
train_iter_loss: 0.23481608927249908
train_iter_loss: 0.12631802260875702
train_iter_loss: 0.1641463041305542
train_iter_loss: 0.2452983558177948
train_iter_loss: 0.2425851672887802
train_iter_loss: 0.15993453562259674
train_iter_loss: 0.12335272133350372
train_iter_loss: 0.27245333790779114
train_iter_loss: 0.20702536404132843
train_iter_loss: 0.1889098435640335
train_iter_loss: 0.17253905534744263
train_iter_loss: 0.15548336505889893
train_iter_loss: 0.13111041486263275
train_iter_loss: 0.15669411420822144
train_iter_loss: 0.12494634836912155
train_iter_loss: 0.10045694559812546
train_iter_loss: 0.1419176161289215
train_iter_loss: 0.27152395248413086
train_iter_loss: 0.24060723185539246
train_iter_loss: 0.08437705039978027
train_iter_loss: 0.17182542383670807
train_iter_loss: 0.1358901709318161
train_iter_loss: 0.23447050154209137
train_iter_loss: 0.14450863003730774
train_iter_loss: 0.36027199029922485
train_iter_loss: 0.19433380663394928
train_iter_loss: 0.186727836728096
train_iter_loss: 0.2680528461933136
train_iter_loss: 0.13944405317306519
train_iter_loss: 0.2116691768169403
train_iter_loss: 0.13265104591846466
train_iter_loss: 0.0917104110121727
train_iter_loss: 0.13273508846759796
train_iter_loss: 0.23416678607463837
train_iter_loss: 0.11217764765024185
train_iter_loss: 0.1484026312828064
train_iter_loss: 0.16817530989646912
train_iter_loss: 0.17166513204574585
train_iter_loss: 0.11032576113939285
train_iter_loss: 0.14888253808021545
train_iter_loss: 0.1538514494895935
train_iter_loss: 0.1298072636127472
train_iter_loss: 0.22023794054985046
train_iter_loss: 0.11088944971561432
train_iter_loss: 0.1932678073644638
train_iter_loss: 0.13147982954978943
train_iter_loss: 0.06540132313966751
train_iter_loss: 0.13994142413139343
train_iter_loss: 0.20919421315193176
train_iter_loss: 0.26121339201927185
train_iter_loss: 0.23393870890140533
train_iter_loss: 0.16498829424381256
train_iter_loss: 0.06328938156366348
train_iter_loss: 0.21094346046447754
train_iter_loss: 0.044139061123132706
train_iter_loss: 0.12179756909608841
train_iter_loss: 0.14241506159305573
train_iter_loss: 0.2876664698123932
train_iter_loss: 0.08318759500980377
train_iter_loss: 0.29629719257354736
train_iter_loss: 0.055849723517894745
train_iter_loss: 0.15106500685214996
train_iter_loss: 0.14391054213047028
train_iter_loss: 0.11480601131916046
train_iter_loss: 0.09721701592206955
train_iter_loss: 0.24193887412548065
train_iter_loss: 0.16173620522022247
train_iter_loss: 0.08641353994607925
train_iter_loss: 0.2972153127193451
train_iter_loss: 0.14404508471488953
train_iter_loss: 0.2884810268878937
train_iter_loss: 0.16427548229694366
train_iter_loss: 0.21446678042411804
train_iter_loss: 0.18651965260505676
train_iter_loss: 0.28040453791618347
train loss :0.1736
---------------------
Validation seg loss: 0.2235961909810046 at epoch 190
epoch =    191/  1000, exp = train
train_iter_loss: 0.24961429834365845
train_iter_loss: 0.18719938397407532
train_iter_loss: 0.10520867258310318
train_iter_loss: 0.185712531208992
train_iter_loss: 0.10128510743379593
train_iter_loss: 0.25212880969047546
train_iter_loss: 0.20587459206581116
train_iter_loss: 0.05195065587759018
train_iter_loss: 0.16394145786762238
train_iter_loss: 0.07991425693035126
train_iter_loss: 0.17460688948631287
train_iter_loss: 0.11767831444740295
train_iter_loss: 0.08640479296445847
train_iter_loss: 0.24370509386062622
train_iter_loss: 0.278697669506073
train_iter_loss: 0.11390558630228043
train_iter_loss: 0.13879840075969696
train_iter_loss: 0.13729797303676605
train_iter_loss: 0.11917982250452042
train_iter_loss: 0.17726127803325653
train_iter_loss: 0.09827413409948349
train_iter_loss: 0.1379140168428421
train_iter_loss: 0.18176120519638062
train_iter_loss: 0.23658281564712524
train_iter_loss: 0.11241279542446136
train_iter_loss: 0.1491040736436844
train_iter_loss: 0.19851844012737274
train_iter_loss: 0.1559489369392395
train_iter_loss: 0.19703838229179382
train_iter_loss: 0.04223548248410225
train_iter_loss: 0.21204523742198944
train_iter_loss: 0.19573204219341278
train_iter_loss: 0.20429028570652008
train_iter_loss: 0.2894185781478882
train_iter_loss: 0.16802218556404114
train_iter_loss: 0.17781631648540497
train_iter_loss: 0.1393027901649475
train_iter_loss: 0.09318552166223526
train_iter_loss: 0.1397491842508316
train_iter_loss: 0.15209819376468658
train_iter_loss: 0.14502361416816711
train_iter_loss: 0.27185535430908203
train_iter_loss: 0.2264889031648636
train_iter_loss: 0.14630860090255737
train_iter_loss: 0.11668290197849274
train_iter_loss: 0.2735612690448761
train_iter_loss: 0.12399480491876602
train_iter_loss: 0.20862820744514465
train_iter_loss: 0.127223938703537
train_iter_loss: 0.20219267904758453
train_iter_loss: 0.26197758316993713
train_iter_loss: 0.16030500829219818
train_iter_loss: 0.17881086468696594
train_iter_loss: 0.1362026035785675
train_iter_loss: 0.1296776682138443
train_iter_loss: 0.19231915473937988
train_iter_loss: 0.09399775415658951
train_iter_loss: 0.3054691255092621
train_iter_loss: 0.13271114230155945
train_iter_loss: 0.17776045203208923
train_iter_loss: 0.15121644735336304
train_iter_loss: 0.2579164505004883
train_iter_loss: 0.20201270282268524
train_iter_loss: 0.25968557596206665
train_iter_loss: 0.13922074437141418
train_iter_loss: 0.2002764493227005
train_iter_loss: 0.13456131517887115
train_iter_loss: 0.10600948333740234
train_iter_loss: 0.22802168130874634
train_iter_loss: 0.23352353274822235
train_iter_loss: 0.1310838758945465
train_iter_loss: 0.12048067152500153
train_iter_loss: 0.17449241876602173
train_iter_loss: 0.13494794070720673
train_iter_loss: 0.1616264432668686
train_iter_loss: 0.1762392818927765
train_iter_loss: 0.15507027506828308
train_iter_loss: 0.16185395419597626
train_iter_loss: 0.15462364256381989
train_iter_loss: 0.1534879207611084
train_iter_loss: 0.1088506355881691
train_iter_loss: 0.09990046173334122
train_iter_loss: 0.12264470756053925
train_iter_loss: 0.127877339720726
train_iter_loss: 0.1561446189880371
train_iter_loss: 0.228293776512146
train_iter_loss: 0.28543609380722046
train_iter_loss: 0.17605511844158173
train_iter_loss: 0.1638418734073639
train_iter_loss: 0.21142753958702087
train_iter_loss: 0.1116785854101181
train_iter_loss: 0.13802850246429443
train_iter_loss: 0.21433192491531372
train_iter_loss: 0.25511106848716736
train_iter_loss: 0.1965772658586502
train_iter_loss: 0.15192678570747375
train_iter_loss: 0.16089332103729248
train_iter_loss: 0.22871822118759155
train_iter_loss: 0.08218732476234436
train_iter_loss: 0.14925584197044373
train loss :0.1704
---------------------
Validation seg loss: 0.22570414551235032 at epoch 191
epoch =    192/  1000, exp = train
train_iter_loss: 0.25261881947517395
train_iter_loss: 0.2353316843509674
train_iter_loss: 0.15590061247348785
train_iter_loss: 0.2757555842399597
train_iter_loss: 0.17574381828308105
train_iter_loss: 0.10217969864606857
train_iter_loss: 0.20015449821949005
train_iter_loss: 0.17726708948612213
train_iter_loss: 0.06620445102453232
train_iter_loss: 0.09982072561979294
train_iter_loss: 0.11423349380493164
train_iter_loss: 0.06378626078367233
train_iter_loss: 0.15767842531204224
train_iter_loss: 0.18507055938243866
train_iter_loss: 0.12427107244729996
train_iter_loss: 0.1575591117143631
train_iter_loss: 0.23321329057216644
train_iter_loss: 0.18088220059871674
train_iter_loss: 0.1453029215335846
train_iter_loss: 0.1977941244840622
train_iter_loss: 0.13637277483940125
train_iter_loss: 0.28224462270736694
train_iter_loss: 0.11873109638690948
train_iter_loss: 0.15754124522209167
train_iter_loss: 0.13161730766296387
train_iter_loss: 0.17374248802661896
train_iter_loss: 0.16929727792739868
train_iter_loss: 0.15197153389453888
train_iter_loss: 0.10972953587770462
train_iter_loss: 0.23897713422775269
train_iter_loss: 0.2630871534347534
train_iter_loss: 0.12140125036239624
train_iter_loss: 0.2690806984901428
train_iter_loss: 0.11303859949111938
train_iter_loss: 0.15167202055454254
train_iter_loss: 0.11825194209814072
train_iter_loss: 0.2335079461336136
train_iter_loss: 0.1467723846435547
train_iter_loss: 0.1456560641527176
train_iter_loss: 0.16799595952033997
train_iter_loss: 0.1886446475982666
train_iter_loss: 0.2307780236005783
train_iter_loss: 0.14913375675678253
train_iter_loss: 0.18592007458209991
train_iter_loss: 0.11121553182601929
train_iter_loss: 0.07763097435235977
train_iter_loss: 0.2331719696521759
train_iter_loss: 0.09351843595504761
train_iter_loss: 0.23074379563331604
train_iter_loss: 0.14864513278007507
train_iter_loss: 0.1917743980884552
train_iter_loss: 0.22976595163345337
train_iter_loss: 0.12838469445705414
train_iter_loss: 0.23649704456329346
train_iter_loss: 0.2403835505247116
train_iter_loss: 0.09092509001493454
train_iter_loss: 0.09719868749380112
train_iter_loss: 0.07672001421451569
train_iter_loss: 0.20118634402751923
train_iter_loss: 0.12408410757780075
train_iter_loss: 0.0549631230533123
train_iter_loss: 0.2597278654575348
train_iter_loss: 0.16895386576652527
train_iter_loss: 0.11805959790945053
train_iter_loss: 0.12556590139865875
train_iter_loss: 0.21761906147003174
train_iter_loss: 0.15390907227993011
train_iter_loss: 0.15566858649253845
train_iter_loss: 0.12553226947784424
train_iter_loss: 0.13703109323978424
train_iter_loss: 0.0426919162273407
train_iter_loss: 0.13922211527824402
train_iter_loss: 0.09417909383773804
train_iter_loss: 0.13786578178405762
train_iter_loss: 0.1826677769422531
train_iter_loss: 0.17115901410579681
train_iter_loss: 0.28167951107025146
train_iter_loss: 0.1628272980451584
train_iter_loss: 0.24392502009868622
train_iter_loss: 0.20432820916175842
train_iter_loss: 0.13884206116199493
train_iter_loss: 0.11995163559913635
train_iter_loss: 0.23518827557563782
train_iter_loss: 0.08667617291212082
train_iter_loss: 0.3062032163143158
train_iter_loss: 0.24597232043743134
train_iter_loss: 0.18209202587604523
train_iter_loss: 0.18058954179286957
train_iter_loss: 0.09961634129285812
train_iter_loss: 0.14602932333946228
train_iter_loss: 0.07256845384836197
train_iter_loss: 0.1456296592950821
train_iter_loss: 0.19243648648262024
train_iter_loss: 0.22489801049232483
train_iter_loss: 0.2998267114162445
train_iter_loss: 0.1981954723596573
train_iter_loss: 0.1631961166858673
train_iter_loss: 0.23268680274486542
train_iter_loss: 0.2521670460700989
train_iter_loss: 0.1686295121908188
train loss :0.1701
---------------------
Validation seg loss: 0.22610374023470114 at epoch 192
epoch =    193/  1000, exp = train
train_iter_loss: 0.1529659926891327
train_iter_loss: 0.26326122879981995
train_iter_loss: 0.13850101828575134
train_iter_loss: 0.13512076437473297
train_iter_loss: 0.13526342809200287
train_iter_loss: 0.13167470693588257
train_iter_loss: 0.15439201891422272
train_iter_loss: 0.16693393886089325
train_iter_loss: 0.3870225250720978
train_iter_loss: 0.152687206864357
train_iter_loss: 0.1439812034368515
train_iter_loss: 0.14614737033843994
train_iter_loss: 0.24447785317897797
train_iter_loss: 0.09023740887641907
train_iter_loss: 0.2382737249135971
train_iter_loss: 0.15695728361606598
train_iter_loss: 0.10918348282575607
train_iter_loss: 0.15836864709854126
train_iter_loss: 0.18309134244918823
train_iter_loss: 0.2151048332452774
train_iter_loss: 0.1690777838230133
train_iter_loss: 0.1610397845506668
train_iter_loss: 0.17193365097045898
train_iter_loss: 0.22435341775417328
train_iter_loss: 0.13803668320178986
train_iter_loss: 0.09768686443567276
train_iter_loss: 0.18560479581356049
train_iter_loss: 0.17031507194042206
train_iter_loss: 0.16948075592517853
train_iter_loss: 0.26566144824028015
train_iter_loss: 0.07399030029773712
train_iter_loss: 0.1961069107055664
train_iter_loss: 0.13172325491905212
train_iter_loss: 0.15615767240524292
train_iter_loss: 0.19742310047149658
train_iter_loss: 0.2652466595172882
train_iter_loss: 0.11716911941766739
train_iter_loss: 0.0905340239405632
train_iter_loss: 0.2925897538661957
train_iter_loss: 0.2735467851161957
train_iter_loss: 0.15886841714382172
train_iter_loss: 0.1754017025232315
train_iter_loss: 0.19396263360977173
train_iter_loss: 0.11956730484962463
train_iter_loss: 0.18929274380207062
train_iter_loss: 0.20787997543811798
train_iter_loss: 0.16923440992832184
train_iter_loss: 0.04773269593715668
train_iter_loss: 0.11788348108530045
train_iter_loss: 0.12577147781848907
train_iter_loss: 0.2837555408477783
train_iter_loss: 0.11440198123455048
train_iter_loss: 0.11298663169145584
train_iter_loss: 0.11906367540359497
train_iter_loss: 0.12970539927482605
train_iter_loss: 0.12489267438650131
train_iter_loss: 0.21780776977539062
train_iter_loss: 0.22444580495357513
train_iter_loss: 0.04337894171476364
train_iter_loss: 0.09260659664869308
train_iter_loss: 0.2327806055545807
train_iter_loss: 0.1705644428730011
train_iter_loss: 0.29606205224990845
train_iter_loss: 0.1730874627828598
train_iter_loss: 0.2874496877193451
train_iter_loss: 0.22875478863716125
train_iter_loss: 0.10239620506763458
train_iter_loss: 0.23724403977394104
train_iter_loss: 0.23340852558612823
train_iter_loss: 0.13500526547431946
train_iter_loss: 0.10686025023460388
train_iter_loss: 0.16281506419181824
train_iter_loss: 0.18712466955184937
train_iter_loss: 0.09983199089765549
train_iter_loss: 0.09003207087516785
train_iter_loss: 0.09652663767337799
train_iter_loss: 0.1575177013874054
train_iter_loss: 0.20281264185905457
train_iter_loss: 0.09439627081155777
train_iter_loss: 0.2599581480026245
train_iter_loss: 0.1802508533000946
train_iter_loss: 0.09319202601909637
train_iter_loss: 0.12398591637611389
train_iter_loss: 0.1940416395664215
train_iter_loss: 0.09801909327507019
train_iter_loss: 0.13957294821739197
train_iter_loss: 0.22521261870861053
train_iter_loss: 0.1865360587835312
train_iter_loss: 0.2161421924829483
train_iter_loss: 0.13468685746192932
train_iter_loss: 0.22234445810317993
train_iter_loss: 0.3128228187561035
train_iter_loss: 0.19578497111797333
train_iter_loss: 0.21198052167892456
train_iter_loss: 0.121157206594944
train_iter_loss: 0.14505136013031006
train_iter_loss: 0.23436971008777618
train_iter_loss: 0.11289970576763153
train_iter_loss: 0.21709796786308289
train_iter_loss: 0.09918690472841263
train loss :0.1721
---------------------
Validation seg loss: 0.2219547793216441 at epoch 193
epoch =    194/  1000, exp = train
train_iter_loss: 0.14346075057983398
train_iter_loss: 0.153547465801239
train_iter_loss: 0.22489866614341736
train_iter_loss: 0.15858331322669983
train_iter_loss: 0.1512531340122223
train_iter_loss: 0.19729800522327423
train_iter_loss: 0.20675097405910492
train_iter_loss: 0.02662833034992218
train_iter_loss: 0.128236785531044
train_iter_loss: 0.22078406810760498
train_iter_loss: 0.13049612939357758
train_iter_loss: 0.17353183031082153
train_iter_loss: 0.1377292275428772
train_iter_loss: 0.17577114701271057
train_iter_loss: 0.2054780125617981
train_iter_loss: 0.1304972916841507
train_iter_loss: 0.06065281108021736
train_iter_loss: 0.14931093156337738
train_iter_loss: 0.1903642863035202
train_iter_loss: 0.24823857843875885
train_iter_loss: 0.1758767068386078
train_iter_loss: 0.29336753487586975
train_iter_loss: 0.16371601819992065
train_iter_loss: 0.1655225306749344
train_iter_loss: 0.05764257162809372
train_iter_loss: 0.22847215831279755
train_iter_loss: 0.0873221755027771
train_iter_loss: 0.21479645371437073
train_iter_loss: 0.14830100536346436
train_iter_loss: 0.091178759932518
train_iter_loss: 0.1089213490486145
train_iter_loss: 0.14870291948318481
train_iter_loss: 0.21733443439006805
train_iter_loss: 0.09367477148771286
train_iter_loss: 0.19875748455524445
train_iter_loss: 0.15688490867614746
train_iter_loss: 0.15613660216331482
train_iter_loss: 0.0931888222694397
train_iter_loss: 0.15941698849201202
train_iter_loss: 0.15276743471622467
train_iter_loss: 0.14658969640731812
train_iter_loss: 0.14368382096290588
train_iter_loss: 0.12130951136350632
train_iter_loss: 0.12127605080604553
train_iter_loss: 0.12146586179733276
train_iter_loss: 0.26374632120132446
train_iter_loss: 0.22520916163921356
train_iter_loss: 0.13576295971870422
train_iter_loss: 0.19990411400794983
train_iter_loss: 0.1685066670179367
train_iter_loss: 0.06140384078025818
train_iter_loss: 0.14390131831169128
train_iter_loss: 0.16070638597011566
train_iter_loss: 0.17431889474391937
train_iter_loss: 0.04368358477950096
train_iter_loss: 0.2890860438346863
train_iter_loss: 0.10297298431396484
train_iter_loss: 0.18754352629184723
train_iter_loss: 0.17524287104606628
train_iter_loss: 0.34542790055274963
train_iter_loss: 0.15466876327991486
train_iter_loss: 0.2801045775413513
train_iter_loss: 0.2079022377729416
train_iter_loss: 0.1492164582014084
train_iter_loss: 0.18362194299697876
train_iter_loss: 0.20142313838005066
train_iter_loss: 0.16779141128063202
train_iter_loss: 0.22050337493419647
train_iter_loss: 0.1770792305469513
train_iter_loss: 0.12192346900701523
train_iter_loss: 0.07392358779907227
train_iter_loss: 0.1821950078010559
train_iter_loss: 0.32358041405677795
train_iter_loss: 0.19232013821601868
train_iter_loss: 0.12964119017124176
train_iter_loss: 0.15377426147460938
train_iter_loss: 0.14375734329223633
train_iter_loss: 0.3151503801345825
train_iter_loss: 0.2154201865196228
train_iter_loss: 0.12048551440238953
train_iter_loss: 0.11708860844373703
train_iter_loss: 0.31266283988952637
train_iter_loss: 0.11045372486114502
train_iter_loss: 0.15351875126361847
train_iter_loss: 0.2468995749950409
train_iter_loss: 0.2731335163116455
train_iter_loss: 0.1349044144153595
train_iter_loss: 0.08503202348947525
train_iter_loss: 0.2061709314584732
train_iter_loss: 0.10260265320539474
train_iter_loss: 0.18897080421447754
train_iter_loss: 0.1940087527036667
train_iter_loss: 0.13057966530323029
train_iter_loss: 0.09920845180749893
train_iter_loss: 0.1863863170146942
train_iter_loss: 0.04373947158455849
train_iter_loss: 0.10373528301715851
train_iter_loss: 0.12015525996685028
train_iter_loss: 0.08369812369346619
train_iter_loss: 0.21576954424381256
train loss :0.1665
---------------------
Validation seg loss: 0.22173685105166063 at epoch 194
epoch =    195/  1000, exp = train
train_iter_loss: 0.30946141481399536
train_iter_loss: 0.1196458637714386
train_iter_loss: 0.16108283400535583
train_iter_loss: 0.3247982859611511
train_iter_loss: 0.10873416066169739
train_iter_loss: 0.22725161910057068
train_iter_loss: 0.16786174476146698
train_iter_loss: 0.12872563302516937
train_iter_loss: 0.22451238334178925
train_iter_loss: 0.09783424437046051
train_iter_loss: 0.1473688781261444
train_iter_loss: 0.30045297741889954
train_iter_loss: 0.23984964191913605
train_iter_loss: 0.14928628504276276
train_iter_loss: 0.10253797471523285
train_iter_loss: 0.20993423461914062
train_iter_loss: 0.12309958785772324
train_iter_loss: 0.13119931519031525
train_iter_loss: 0.13492706418037415
train_iter_loss: 0.191520094871521
train_iter_loss: 0.1489362269639969
train_iter_loss: 0.12323833256959915
train_iter_loss: 0.1883607804775238
train_iter_loss: 0.1651776134967804
train_iter_loss: 0.1265898197889328
train_iter_loss: 0.13354216516017914
train_iter_loss: 0.20678402483463287
train_iter_loss: 0.18962909281253815
train_iter_loss: 0.15523067116737366
train_iter_loss: 0.1926068216562271
train_iter_loss: 0.37157413363456726
train_iter_loss: 0.2624889314174652
train_iter_loss: 0.23533622920513153
train_iter_loss: 0.25600606203079224
train_iter_loss: 0.1522519737482071
train_iter_loss: 0.10053958743810654
train_iter_loss: 0.1475803554058075
train_iter_loss: 0.14063727855682373
train_iter_loss: 0.11861317604780197
train_iter_loss: 0.175131693482399
train_iter_loss: 0.15282192826271057
train_iter_loss: 0.12921930849552155
train_iter_loss: 0.169759601354599
train_iter_loss: 0.16351920366287231
train_iter_loss: 0.16927793622016907
train_iter_loss: 0.2756013870239258
train_iter_loss: 0.1881140023469925
train_iter_loss: 0.09005226194858551
train_iter_loss: 0.16971643269062042
train_iter_loss: 0.09347520768642426
train_iter_loss: 0.1863013505935669
train_iter_loss: 0.13905903697013855
train_iter_loss: 0.23774804174900055
train_iter_loss: 0.14572784304618835
train_iter_loss: 0.23841898143291473
train_iter_loss: 0.16316217184066772
train_iter_loss: 0.08623260259628296
train_iter_loss: 0.14029863476753235
train_iter_loss: 0.23958320915699005
train_iter_loss: 0.10336374491453171
train_iter_loss: 0.23909775912761688
train_iter_loss: 0.15786701440811157
train_iter_loss: 0.12176255881786346
train_iter_loss: 0.06235247850418091
train_iter_loss: 0.1796506941318512
train_iter_loss: 0.1949302852153778
train_iter_loss: 0.17078882455825806
train_iter_loss: 0.1361783742904663
train_iter_loss: 0.10197504609823227
train_iter_loss: 0.25479456782341003
train_iter_loss: 0.16388417780399323
train_iter_loss: 0.13071739673614502
train_iter_loss: 0.112022265791893
train_iter_loss: 0.18265746533870697
train_iter_loss: 0.20023947954177856
train_iter_loss: 0.14259345829486847
train_iter_loss: 0.11212312430143356
train_iter_loss: 0.2061396986246109
train_iter_loss: 0.222408264875412
train_iter_loss: 0.29456374049186707
train_iter_loss: 0.25838783383369446
train_iter_loss: 0.11505100876092911
train_iter_loss: 0.19184452295303345
train_iter_loss: 0.1395638883113861
train_iter_loss: 0.24436263740062714
train_iter_loss: 0.10970287770032883
train_iter_loss: 0.11217964440584183
train_iter_loss: 0.11286287009716034
train_iter_loss: 0.1631843000650406
train_iter_loss: 0.14327283203601837
train_iter_loss: 0.2344297617673874
train_iter_loss: 0.10890822112560272
train_iter_loss: 0.13963812589645386
train_iter_loss: 0.13857883214950562
train_iter_loss: 0.185250386595726
train_iter_loss: 0.15451206266880035
train_iter_loss: 0.23314955830574036
train_iter_loss: 0.16290532052516937
train_iter_loss: 0.19265376031398773
train_iter_loss: 0.20205147564411163
train loss :0.1737
---------------------
Validation seg loss: 0.2244634919641715 at epoch 195
epoch =    196/  1000, exp = train
train_iter_loss: 0.28134530782699585
train_iter_loss: 0.20843084156513214
train_iter_loss: 0.33096617460250854
train_iter_loss: 0.1351110190153122
train_iter_loss: 0.15258553624153137
train_iter_loss: 0.10797017067670822
train_iter_loss: 0.11239413917064667
train_iter_loss: 0.09410647302865982
train_iter_loss: 0.11200766265392303
train_iter_loss: 0.13980238139629364
train_iter_loss: 0.18979358673095703
train_iter_loss: 0.12224942445755005
train_iter_loss: 0.19648906588554382
train_iter_loss: 0.17295688390731812
train_iter_loss: 0.0774575024843216
train_iter_loss: 0.12579725682735443
train_iter_loss: 0.1454870104789734
train_iter_loss: 0.13556131720542908
train_iter_loss: 0.09572925418615341
train_iter_loss: 0.15338478982448578
train_iter_loss: 0.2908218801021576
train_iter_loss: 0.21251966059207916
train_iter_loss: 0.12749972939491272
train_iter_loss: 0.1885545700788498
train_iter_loss: 0.08339641243219376
train_iter_loss: 0.22697314620018005
train_iter_loss: 0.13018102943897247
train_iter_loss: 0.13467608392238617
train_iter_loss: 0.1126604825258255
train_iter_loss: 0.15746276080608368
train_iter_loss: 0.18164823949337006
train_iter_loss: 0.08134506642818451
train_iter_loss: 0.19416400790214539
train_iter_loss: 0.09445799142122269
train_iter_loss: 0.4453713893890381
train_iter_loss: 0.05701260641217232
train_iter_loss: 0.13843993842601776
train_iter_loss: 0.20424358546733856
train_iter_loss: 0.08651100844144821
train_iter_loss: 0.1388675719499588
train_iter_loss: 0.22573477029800415
train_iter_loss: 0.12478149682283401
train_iter_loss: 0.21751323342323303
train_iter_loss: 0.1259271651506424
train_iter_loss: 0.10163325816392899
train_iter_loss: 0.21686699986457825
train_iter_loss: 0.30497369170188904
train_iter_loss: 0.27289506793022156
train_iter_loss: 0.1290893852710724
train_iter_loss: 0.16992169618606567
train_iter_loss: 0.05597074329853058
train_iter_loss: 0.19396091997623444
train_iter_loss: 0.18989335000514984
train_iter_loss: 0.1702796369791031
train_iter_loss: 0.1438487321138382
train_iter_loss: 0.1332743912935257
train_iter_loss: 0.2044777274131775
train_iter_loss: 0.19394196569919586
train_iter_loss: 0.20376940071582794
train_iter_loss: 0.11211542040109634
train_iter_loss: 0.2293115258216858
train_iter_loss: 0.07503066211938858
train_iter_loss: 0.20143122971057892
train_iter_loss: 0.19011840224266052
train_iter_loss: 0.12175628542900085
train_iter_loss: 0.08601690828800201
train_iter_loss: 0.12213422358036041
train_iter_loss: 0.22972960770130157
train_iter_loss: 0.14158326387405396
train_iter_loss: 0.1578393280506134
train_iter_loss: 0.22601662576198578
train_iter_loss: 0.1586950421333313
train_iter_loss: 0.09926685690879822
train_iter_loss: 0.10319302976131439
train_iter_loss: 0.14218509197235107
train_iter_loss: 0.1364225149154663
train_iter_loss: 0.19258052110671997
train_iter_loss: 0.09303262829780579
train_iter_loss: 0.1527373194694519
train_iter_loss: 0.12113529443740845
train_iter_loss: 0.13085587322711945
train_iter_loss: 0.17960968613624573
train_iter_loss: 0.26635223627090454
train_iter_loss: 0.3759065866470337
train_iter_loss: 0.29976704716682434
train_iter_loss: 0.13174846768379211
train_iter_loss: 0.12152061611413956
train_iter_loss: 0.15989382565021515
train_iter_loss: 0.20331181585788727
train_iter_loss: 0.13906852900981903
train_iter_loss: 0.37077903747558594
train_iter_loss: 0.18643882870674133
train_iter_loss: 0.19348138570785522
train_iter_loss: 0.18954651057720184
train_iter_loss: 0.1874811351299286
train_iter_loss: 0.2671937346458435
train_iter_loss: 0.15298642218112946
train_iter_loss: 0.05588546395301819
train_iter_loss: 0.1674908995628357
train_iter_loss: 0.09516552090644836
train loss :0.1690
---------------------
Validation seg loss: 0.22402625618818797 at epoch 196
epoch =    197/  1000, exp = train
train_iter_loss: 0.19185544550418854
train_iter_loss: 0.26987215876579285
train_iter_loss: 0.16327166557312012
train_iter_loss: 0.13008710741996765
train_iter_loss: 0.21519988775253296
train_iter_loss: 0.1560606062412262
train_iter_loss: 0.1191783994436264
train_iter_loss: 0.15949904918670654
train_iter_loss: 0.14364655315876007
train_iter_loss: 0.1830778568983078
train_iter_loss: 0.14776304364204407
train_iter_loss: 0.08434024453163147
train_iter_loss: 0.22241926193237305
train_iter_loss: 0.11504225432872772
train_iter_loss: 0.20209500193595886
train_iter_loss: 0.08900780975818634
train_iter_loss: 0.18816353380680084
train_iter_loss: 0.3224599063396454
train_iter_loss: 0.2299448847770691
train_iter_loss: 0.17794005572795868
train_iter_loss: 0.09815462678670883
train_iter_loss: 0.1730804741382599
train_iter_loss: 0.15402089059352875
train_iter_loss: 0.28724223375320435
train_iter_loss: 0.23579834401607513
train_iter_loss: 0.09639106690883636
train_iter_loss: 0.07211344689130783
train_iter_loss: 0.16743208467960358
train_iter_loss: 0.11014517396688461
train_iter_loss: 0.15611101686954498
train_iter_loss: 0.15536029636859894
train_iter_loss: 0.15575440227985382
train_iter_loss: 0.11292222887277603
train_iter_loss: 0.14124144613742828
train_iter_loss: 0.22929780185222626
train_iter_loss: 0.08303818106651306
train_iter_loss: 0.20910055935382843
train_iter_loss: 0.1696011871099472
train_iter_loss: 0.14074242115020752
train_iter_loss: 0.2506410777568817
train_iter_loss: 0.16275565326213837
train_iter_loss: 0.23102299869060516
train_iter_loss: 0.1805153340101242
train_iter_loss: 0.11924566328525543
train_iter_loss: 0.12274623662233353
train_iter_loss: 0.15599697828292847
train_iter_loss: 0.1524045169353485
train_iter_loss: 0.23511375486850739
train_iter_loss: 0.11494379490613937
train_iter_loss: 0.10070968419313431
train_iter_loss: 0.15427058935165405
train_iter_loss: 0.1378340870141983
train_iter_loss: 0.1714267134666443
train_iter_loss: 0.23257441818714142
train_iter_loss: 0.08373214304447174
train_iter_loss: 0.1986801028251648
train_iter_loss: 0.11791440844535828
train_iter_loss: 0.2773900628089905
train_iter_loss: 0.11792579293251038
train_iter_loss: 0.15733809769153595
train_iter_loss: 0.12258846312761307
train_iter_loss: 0.14225831627845764
train_iter_loss: 0.0968691036105156
train_iter_loss: 0.17560678720474243
train_iter_loss: 0.15315648913383484
train_iter_loss: 0.255692720413208
train_iter_loss: 0.26053938269615173
train_iter_loss: 0.08979351818561554
train_iter_loss: 0.09013137221336365
train_iter_loss: 0.20617890357971191
train_iter_loss: 0.1190963014960289
train_iter_loss: 0.14295005798339844
train_iter_loss: 0.15811967849731445
train_iter_loss: 0.17222435772418976
train_iter_loss: 0.15042643249034882
train_iter_loss: 0.1056252047419548
train_iter_loss: 0.1609935760498047
train_iter_loss: 0.1481681615114212
train_iter_loss: 0.14066830277442932
train_iter_loss: 0.05531204864382744
train_iter_loss: 0.2649798095226288
train_iter_loss: 0.1444714218378067
train_iter_loss: 0.14033479988574982
train_iter_loss: 0.20774516463279724
train_iter_loss: 0.12155710905790329
train_iter_loss: 0.125467911362648
train_iter_loss: 0.1590883731842041
train_iter_loss: 0.24355235695838928
train_iter_loss: 0.12431498616933823
train_iter_loss: 0.18969528377056122
train_iter_loss: 0.07246197015047073
train_iter_loss: 0.16371451318264008
train_iter_loss: 0.10267220437526703
train_iter_loss: 0.31406599283218384
train_iter_loss: 0.16421779990196228
train_iter_loss: 0.13122031092643738
train_iter_loss: 0.2865329086780548
train_iter_loss: 0.13933239877223969
train_iter_loss: 0.2578243613243103
train_iter_loss: 0.4315098822116852
train loss :0.1683
---------------------
Validation seg loss: 0.22219847169054566 at epoch 197
epoch =    198/  1000, exp = train
train_iter_loss: 0.0749080628156662
train_iter_loss: 0.13713319599628448
train_iter_loss: 0.272472083568573
train_iter_loss: 0.1335972249507904
train_iter_loss: 0.15801599621772766
train_iter_loss: 0.16131103038787842
train_iter_loss: 0.17407821118831635
train_iter_loss: 0.17628014087677002
train_iter_loss: 0.24167385697364807
train_iter_loss: 0.13951021432876587
train_iter_loss: 0.1315174102783203
train_iter_loss: 0.16825667023658752
train_iter_loss: 0.14329740405082703
train_iter_loss: 0.122116319835186
train_iter_loss: 0.22166575491428375
train_iter_loss: 0.09740595519542694
train_iter_loss: 0.08373089879751205
train_iter_loss: 0.20952492952346802
train_iter_loss: 0.20077499747276306
train_iter_loss: 0.24742071330547333
train_iter_loss: 0.2590737044811249
train_iter_loss: 0.21026358008384705
train_iter_loss: 0.16642887890338898
train_iter_loss: 0.07256019860506058
train_iter_loss: 0.15179148316383362
train_iter_loss: 0.0943245142698288
train_iter_loss: 0.24413621425628662
train_iter_loss: 0.23390032351016998
train_iter_loss: 0.23132337629795074
train_iter_loss: 0.12962132692337036
train_iter_loss: 0.18187707662582397
train_iter_loss: 0.17750652134418488
train_iter_loss: 0.10970250517129898
train_iter_loss: 0.21000836789608002
train_iter_loss: 0.0932193323969841
train_iter_loss: 0.13366563618183136
train_iter_loss: 0.09630906581878662
train_iter_loss: 0.11971849948167801
train_iter_loss: 0.22387729585170746
train_iter_loss: 0.17243947088718414
train_iter_loss: 0.10172741115093231
train_iter_loss: 0.0770488977432251
train_iter_loss: 0.19332046806812286
train_iter_loss: 0.18069715797901154
train_iter_loss: 0.15606234967708588
train_iter_loss: 0.23032179474830627
train_iter_loss: 0.13661329448223114
train_iter_loss: 0.0775243267416954
train_iter_loss: 0.2844383120536804
train_iter_loss: 0.08752763271331787
train_iter_loss: 0.153354212641716
train_iter_loss: 0.2158021479845047
train_iter_loss: 0.11114116758108139
train_iter_loss: 0.10100392997264862
train_iter_loss: 0.2908000946044922
train_iter_loss: 0.11975615471601486
train_iter_loss: 0.1754748672246933
train_iter_loss: 0.10213909298181534
train_iter_loss: 0.13838356733322144
train_iter_loss: 0.25784170627593994
train_iter_loss: 0.13352365791797638
train_iter_loss: 0.1333426982164383
train_iter_loss: 0.16986966133117676
train_iter_loss: 0.1923365443944931
train_iter_loss: 0.1871172934770584
train_iter_loss: 0.16096843779087067
train_iter_loss: 0.19526948034763336
train_iter_loss: 0.26881375908851624
train_iter_loss: 0.12272536754608154
train_iter_loss: 0.19367243349552155
train_iter_loss: 0.10854121297597885
train_iter_loss: 0.16978026926517487
train_iter_loss: 0.2084183692932129
train_iter_loss: 0.17245735228061676
train_iter_loss: 0.1491159200668335
train_iter_loss: 0.2080671489238739
train_iter_loss: 0.21086861193180084
train_iter_loss: 0.20124919712543488
train_iter_loss: 0.15054951608181
train_iter_loss: 0.11139069497585297
train_iter_loss: 0.16877636313438416
train_iter_loss: 0.12177415192127228
train_iter_loss: 0.21575474739074707
train_iter_loss: 0.2968111038208008
train_iter_loss: 0.20232118666172028
train_iter_loss: 0.13184316456317902
train_iter_loss: 0.22675858438014984
train_iter_loss: 0.14515039324760437
train_iter_loss: 0.08256177604198456
train_iter_loss: 0.4149145185947418
train_iter_loss: 0.1710297167301178
train_iter_loss: 0.16549775004386902
train_iter_loss: 0.09573045372962952
train_iter_loss: 0.1200091689825058
train_iter_loss: 0.14537368714809418
train_iter_loss: 0.18639452755451202
train_iter_loss: 0.11847508698701859
train_iter_loss: 0.16136884689331055
train_iter_loss: 0.12449762970209122
train_iter_loss: 0.14001832902431488
train loss :0.1685
---------------------
Validation seg loss: 0.21965732808523583 at epoch 198
********************
best_val_epoch_loss:  0.21965732808523583
MODEL UPDATED
epoch =    199/  1000, exp = train
train_iter_loss: 0.25002187490463257
train_iter_loss: 0.045418091118335724
train_iter_loss: 0.17231564223766327
train_iter_loss: 0.15987800061702728
train_iter_loss: 0.22483192384243011
train_iter_loss: 0.09523361176252365
train_iter_loss: 0.1444113403558731
train_iter_loss: 0.24393078684806824
train_iter_loss: 0.19230437278747559
train_iter_loss: 0.2452286034822464
train_iter_loss: 0.18969157338142395
train_iter_loss: 0.09733613580465317
train_iter_loss: 0.14554184675216675
train_iter_loss: 0.12374259531497955
train_iter_loss: 0.17209215462207794
train_iter_loss: 0.15186545252799988
train_iter_loss: 0.181509867310524
train_iter_loss: 0.1343187540769577
train_iter_loss: 0.159003347158432
train_iter_loss: 0.2254471629858017
train_iter_loss: 0.16669897735118866
train_iter_loss: 0.13039791584014893
train_iter_loss: 0.27188023924827576
train_iter_loss: 0.15496104955673218
train_iter_loss: 0.09367212653160095
train_iter_loss: 0.14863435924053192
train_iter_loss: 0.2562389373779297
train_iter_loss: 0.17521882057189941
train_iter_loss: 0.12513698637485504
train_iter_loss: 0.13231834769248962
train_iter_loss: 0.11658621579408646
train_iter_loss: 0.26735594868659973
train_iter_loss: 0.1747463494539261
train_iter_loss: 0.23479141294956207
train_iter_loss: 0.2543042302131653
train_iter_loss: 0.14583590626716614
train_iter_loss: 0.11998032033443451
train_iter_loss: 0.13552606105804443
train_iter_loss: 0.13069912791252136
train_iter_loss: 0.16255107522010803
train_iter_loss: 0.08281505852937698
train_iter_loss: 0.17915043234825134
train_iter_loss: 0.12476377189159393
train_iter_loss: 0.24322523176670074
train_iter_loss: 0.17251650989055634
train_iter_loss: 0.1259315013885498
train_iter_loss: 0.16156308352947235
train_iter_loss: 0.3267349302768707
train_iter_loss: 0.07540992647409439
train_iter_loss: 0.26449477672576904
train_iter_loss: 0.2004343867301941
train_iter_loss: 0.20497991144657135
train_iter_loss: 0.05995708331465721
train_iter_loss: 0.13043446838855743
train_iter_loss: 0.2366851568222046
train_iter_loss: 0.21536307036876678
train_iter_loss: 0.16864441335201263
train_iter_loss: 0.14432241022586823
train_iter_loss: 0.1284540295600891
train_iter_loss: 0.1758120208978653
train_iter_loss: 0.132895827293396
train_iter_loss: 0.2567063868045807
train_iter_loss: 0.1981978863477707
train_iter_loss: 0.13094225525856018
train_iter_loss: 0.139145165681839
train_iter_loss: 0.06298881769180298
train_iter_loss: 0.15583640336990356
train_iter_loss: 0.16745224595069885
train_iter_loss: 0.08203088492155075
train_iter_loss: 0.16970661282539368
train_iter_loss: 0.12236342579126358
train_iter_loss: 0.3288402855396271
train_iter_loss: 0.17039500176906586
train_iter_loss: 0.1638510376214981
train_iter_loss: 0.24407140910625458
train_iter_loss: 0.11128394305706024
train_iter_loss: 0.2024463266134262
train_iter_loss: 0.19335295259952545
train_iter_loss: 0.09738554060459137
train_iter_loss: 0.19209109246730804
train_iter_loss: 0.16189590096473694
train_iter_loss: 0.1380079686641693
train_iter_loss: 0.21735800802707672
train_iter_loss: 0.17184029519557953
train_iter_loss: 0.1766747385263443
train_iter_loss: 0.15529853105545044
train_iter_loss: 0.11883309483528137
train_iter_loss: 0.19027823209762573
train_iter_loss: 0.16578656435012817
train_iter_loss: 0.14748601615428925
train_iter_loss: 0.11007699370384216
train_iter_loss: 0.18945057690143585
train_iter_loss: 0.2338840514421463
train_iter_loss: 0.1639513373374939
train_iter_loss: 0.09536952525377274
train_iter_loss: 0.179576113820076
train_iter_loss: 0.18031233549118042
train_iter_loss: 0.13426393270492554
train_iter_loss: 0.0939076691865921
train_iter_loss: 0.2670336663722992
train loss :0.1696
---------------------
Validation seg loss: 0.2195352355439989 at epoch 199
********************
best_val_epoch_loss:  0.2195352355439989
MODEL UPDATED
epoch =    200/  1000, exp = train
train_iter_loss: 0.17841428518295288
train_iter_loss: 0.14972653985023499
train_iter_loss: 0.16026613116264343
train_iter_loss: 0.15408746898174286
train_iter_loss: 0.12382087856531143
train_iter_loss: 0.2590312659740448
train_iter_loss: 0.08879324048757553
train_iter_loss: 0.14070500433444977
train_iter_loss: 0.12308374047279358
train_iter_loss: 0.2277161180973053
train_iter_loss: 0.33442622423171997
train_iter_loss: 0.17823705077171326
train_iter_loss: 0.1571463644504547
train_iter_loss: 0.2823981046676636
train_iter_loss: 0.0912293866276741
train_iter_loss: 0.24676395952701569
train_iter_loss: 0.15173794329166412
train_iter_loss: 0.06736091524362564
train_iter_loss: 0.14997054636478424
train_iter_loss: 0.19438791275024414
train_iter_loss: 0.33855438232421875
train_iter_loss: 0.15001890063285828
train_iter_loss: 0.1363305002450943
train_iter_loss: 0.2936231791973114
train_iter_loss: 0.2762864828109741
train_iter_loss: 0.19678841531276703
train_iter_loss: 0.29708054661750793
train_iter_loss: 0.1940566450357437
train_iter_loss: 0.32797569036483765
train_iter_loss: 0.329118937253952
train_iter_loss: 0.07743614912033081
train_iter_loss: 0.11432348191738129
train_iter_loss: 0.11894529312849045
train_iter_loss: 0.21023374795913696
train_iter_loss: 0.10203925520181656
train_iter_loss: 0.24656662344932556
train_iter_loss: 0.17559018731117249
train_iter_loss: 0.19960333406925201
train_iter_loss: 0.1424971967935562
train_iter_loss: 0.11810915172100067
train_iter_loss: 0.1621711403131485
train_iter_loss: 0.15676110982894897
train_iter_loss: 0.21757060289382935
train_iter_loss: 0.11985757201910019
train_iter_loss: 0.16036337614059448
train_iter_loss: 0.17178218066692352
train_iter_loss: 0.1538093388080597
train_iter_loss: 0.2252988964319229
train_iter_loss: 0.15247038006782532
train_iter_loss: 0.09925909340381622
train_iter_loss: 0.11375653743743896
train_iter_loss: 0.11131571978330612
train_iter_loss: 0.08786077052354813
train_iter_loss: 0.16488851606845856
train_iter_loss: 0.13968242704868317
train_iter_loss: 0.16777507960796356
train_iter_loss: 0.1053403988480568
train_iter_loss: 0.14580221474170685
train_iter_loss: 0.11625673621892929
train_iter_loss: 0.15915198624134064
train_iter_loss: 0.12588630616664886
train_iter_loss: 0.28254517912864685
train_iter_loss: 0.13062436878681183
train_iter_loss: 0.15008409321308136
train_iter_loss: 0.09051915258169174
train_iter_loss: 0.08627624064683914
train_iter_loss: 0.21778802573680878
train_iter_loss: 0.1779020130634308
train_iter_loss: 0.16407322883605957
train_iter_loss: 0.1604187935590744
train_iter_loss: 0.08050781488418579
train_iter_loss: 0.2186404913663864
train_iter_loss: 0.2682238519191742
train_iter_loss: 0.1350696086883545
train_iter_loss: 0.10578592866659164
train_iter_loss: 0.09706752747297287
train_iter_loss: 0.22612525522708893
train_iter_loss: 0.1240314468741417
train_iter_loss: 0.27788448333740234
train_iter_loss: 0.1385323852300644
train_iter_loss: 0.3233807682991028
train_iter_loss: 0.17678211629390717
train_iter_loss: 0.1864149123430252
train_iter_loss: 0.19598041474819183
train_iter_loss: 0.2490774542093277
train_iter_loss: 0.1794300526380539
train_iter_loss: 0.146843820810318
train_iter_loss: 0.16186922788619995
train_iter_loss: 0.08133088797330856
train_iter_loss: 0.20471696555614471
train_iter_loss: 0.17820872366428375
train_iter_loss: 0.21982280910015106
train_iter_loss: 0.24632859230041504
train_iter_loss: 0.15327513217926025
train_iter_loss: 0.18896204233169556
train_iter_loss: 0.2660001516342163
train_iter_loss: 0.15048611164093018
train_iter_loss: 0.1869334876537323
train_iter_loss: 0.1378602236509323
train_iter_loss: 0.2430306226015091
train loss :0.1774
---------------------
Validation seg loss: 0.220869871805299 at epoch 200
epoch =    201/  1000, exp = train
train_iter_loss: 0.1102059856057167
train_iter_loss: 0.1433144509792328
train_iter_loss: 0.16015994548797607
train_iter_loss: 0.05899866670370102
train_iter_loss: 0.13205745816230774
train_iter_loss: 0.1457163244485855
train_iter_loss: 0.1562930792570114
train_iter_loss: 0.22670190036296844
train_iter_loss: 0.23040147125720978
train_iter_loss: 0.16939151287078857
train_iter_loss: 0.14005884528160095
train_iter_loss: 0.30369141697883606
train_iter_loss: 0.2148386389017105
train_iter_loss: 0.13171370327472687
train_iter_loss: 0.18937769532203674
train_iter_loss: 0.19970086216926575
train_iter_loss: 0.06948237121105194
train_iter_loss: 0.1665329784154892
train_iter_loss: 0.11033405363559723
train_iter_loss: 0.09228525310754776
train_iter_loss: 0.133622407913208
train_iter_loss: 0.19563661515712738
train_iter_loss: 0.08593659847974777
train_iter_loss: 0.257747083902359
train_iter_loss: 0.23107457160949707
train_iter_loss: 0.2339850515127182
train_iter_loss: 0.1524089127779007
train_iter_loss: 0.06517916917800903
train_iter_loss: 0.12038655579090118
train_iter_loss: 0.07054925709962845
train_iter_loss: 0.0584699809551239
train_iter_loss: 0.13383989036083221
train_iter_loss: 0.18039366602897644
train_iter_loss: 0.15579095482826233
train_iter_loss: 0.13284359872341156
train_iter_loss: 0.12089613080024719
train_iter_loss: 0.23496241867542267
train_iter_loss: 0.12507912516593933
train_iter_loss: 0.21628117561340332
train_iter_loss: 0.144235298037529
train_iter_loss: 0.14994744956493378
train_iter_loss: 0.1854025423526764
train_iter_loss: 0.15211476385593414
train_iter_loss: 0.26632192730903625
train_iter_loss: 0.13812431693077087
train_iter_loss: 0.16843843460083008
train_iter_loss: 0.29093000292778015
train_iter_loss: 0.12360075861215591
train_iter_loss: 0.240758016705513
train_iter_loss: 0.21190005540847778
train_iter_loss: 0.1299021989107132
train_iter_loss: 0.2909056544303894
train_iter_loss: 0.14194318652153015
train_iter_loss: 0.16232134401798248
train_iter_loss: 0.1485646367073059
train_iter_loss: 0.20840758085250854
train_iter_loss: 0.1451219916343689
train_iter_loss: 0.17664173245429993
train_iter_loss: 0.10712149739265442
train_iter_loss: 0.1303245574235916
train_iter_loss: 0.2998397946357727
train_iter_loss: 0.06356524676084518
train_iter_loss: 0.1469242423772812
train_iter_loss: 0.18504387140274048
train_iter_loss: 0.16656464338302612
train_iter_loss: 0.24606376886367798
train_iter_loss: 0.3188132047653198
train_iter_loss: 0.283347487449646
train_iter_loss: 0.18106840550899506
train_iter_loss: 0.14888353645801544
train_iter_loss: 0.2891369163990021
train_iter_loss: 0.1347772628068924
train_iter_loss: 0.046723753213882446
train_iter_loss: 0.2178916335105896
train_iter_loss: 0.18333378434181213
train_iter_loss: 0.11636140197515488
train_iter_loss: 0.19229565560817719
train_iter_loss: 0.2898358404636383
train_iter_loss: 0.14399464428424835
train_iter_loss: 0.19547612965106964
train_iter_loss: 0.10193148255348206
train_iter_loss: 0.10283168405294418
train_iter_loss: 0.12881295382976532
train_iter_loss: 0.20214441418647766
train_iter_loss: 0.15830282866954803
train_iter_loss: 0.13594824075698853
train_iter_loss: 0.08635405451059341
train_iter_loss: 0.11393782496452332
train_iter_loss: 0.1979106217622757
train_iter_loss: 0.21015679836273193
train_iter_loss: 0.17663536965847015
train_iter_loss: 0.18778342008590698
train_iter_loss: 0.1691320836544037
train_iter_loss: 0.18162131309509277
train_iter_loss: 0.12790139019489288
train_iter_loss: 0.2084236890077591
train_iter_loss: 0.21898552775382996
train_iter_loss: 0.1718612015247345
train_iter_loss: 0.1352231353521347
train_iter_loss: 0.14444667100906372
train loss :0.1696
---------------------
Validation seg loss: 0.2224769945836292 at epoch 201
epoch =    202/  1000, exp = train
train_iter_loss: 0.17380866408348083
train_iter_loss: 0.0902647078037262
train_iter_loss: 0.17324815690517426
train_iter_loss: 0.32372722029685974
train_iter_loss: 0.0902278795838356
train_iter_loss: 0.09857185930013657
train_iter_loss: 0.13008327782154083
train_iter_loss: 0.15507818758487701
train_iter_loss: 0.09804847836494446
train_iter_loss: 0.23737302422523499
train_iter_loss: 0.16309641301631927
train_iter_loss: 0.205995574593544
train_iter_loss: 0.3021370768547058
train_iter_loss: 0.24109631776809692
train_iter_loss: 0.1014295443892479
train_iter_loss: 0.14090558886528015
train_iter_loss: 0.12226378917694092
train_iter_loss: 0.10785793513059616
train_iter_loss: 0.40464434027671814
train_iter_loss: 0.12087272107601166
train_iter_loss: 0.10747586190700531
train_iter_loss: 0.10178640484809875
train_iter_loss: 0.09615396708250046
train_iter_loss: 0.28074726462364197
train_iter_loss: 0.18774378299713135
train_iter_loss: 0.18398340046405792
train_iter_loss: 0.23237277567386627
train_iter_loss: 0.18921513855457306
train_iter_loss: 0.11490816622972488
train_iter_loss: 0.15139354765415192
train_iter_loss: 0.20527635514736176
train_iter_loss: 0.20676273107528687
train_iter_loss: 0.12687775492668152
train_iter_loss: 0.1828126162290573
train_iter_loss: 0.3250938951969147
train_iter_loss: 0.11526545882225037
train_iter_loss: 0.03362390026450157
train_iter_loss: 0.2310221940279007
train_iter_loss: 0.14633125066757202
train_iter_loss: 0.277540922164917
train_iter_loss: 0.16416648030281067
train_iter_loss: 0.17185281217098236
train_iter_loss: 0.21016116440296173
train_iter_loss: 0.08326040208339691
train_iter_loss: 0.20266129076480865
train_iter_loss: 0.13879388570785522
train_iter_loss: 0.24565808475017548
train_iter_loss: 0.1416528820991516
train_iter_loss: 0.15118694305419922
train_iter_loss: 0.16195856034755707
train_iter_loss: 0.331114262342453
train_iter_loss: 0.12831878662109375
train_iter_loss: 0.24139295518398285
train_iter_loss: 0.15221725404262543
train_iter_loss: 0.21893687546253204
train_iter_loss: 0.22499586641788483
train_iter_loss: 0.12000370025634766
train_iter_loss: 0.1506330668926239
train_iter_loss: 0.11046690493822098
train_iter_loss: 0.23197977244853973
train_iter_loss: 0.110463947057724
train_iter_loss: 0.20192061364650726
train_iter_loss: 0.22007694840431213
train_iter_loss: 0.1767607033252716
train_iter_loss: 0.13973388075828552
train_iter_loss: 0.36533430218696594
train_iter_loss: 0.17594817280769348
train_iter_loss: 0.09258455783128738
train_iter_loss: 0.20000430941581726
train_iter_loss: 0.1071997657418251
train_iter_loss: 0.16847896575927734
train_iter_loss: 0.1541997492313385
train_iter_loss: 0.12730196118354797
train_iter_loss: 0.2194359451532364
train_iter_loss: 0.18783268332481384
train_iter_loss: 0.22267861664295197
train_iter_loss: 0.1651776283979416
train_iter_loss: 0.18178695440292358
train_iter_loss: 0.13746610283851624
train_iter_loss: 0.1358220875263214
train_iter_loss: 0.16494303941726685
train_iter_loss: 0.4416618049144745
train_iter_loss: 0.2682926654815674
train_iter_loss: 0.08085759729146957
train_iter_loss: 0.08270564675331116
train_iter_loss: 0.18381312489509583
train_iter_loss: 0.2394755631685257
train_iter_loss: 0.25412076711654663
train_iter_loss: 0.14320889115333557
train_iter_loss: 0.08922825753688812
train_iter_loss: 0.14227081835269928
train_iter_loss: 0.10659545660018921
train_iter_loss: 0.2095051109790802
train_iter_loss: 0.09460815042257309
train_iter_loss: 0.23844309151172638
train_iter_loss: 0.1302257627248764
train_iter_loss: 0.11193107813596725
train_iter_loss: 0.07364337891340256
train_iter_loss: 0.0999675989151001
train_iter_loss: 0.14121419191360474
train loss :0.1752
---------------------
Validation seg loss: 0.21944491943028174 at epoch 202
********************
best_val_epoch_loss:  0.21944491943028174
MODEL UPDATED
epoch =    203/  1000, exp = train
train_iter_loss: 0.26908329129219055
train_iter_loss: 0.18819086253643036
train_iter_loss: 0.1476566344499588
train_iter_loss: 0.13643990457057953
train_iter_loss: 0.13962431252002716
train_iter_loss: 0.19135993719100952
train_iter_loss: 0.15876303613185883
train_iter_loss: 0.11911791563034058
train_iter_loss: 0.25350475311279297
train_iter_loss: 0.12185124307870865
train_iter_loss: 0.19301287829875946
train_iter_loss: 0.21259433031082153
train_iter_loss: 0.16732747852802277
train_iter_loss: 0.18492703139781952
train_iter_loss: 0.22832965850830078
train_iter_loss: 0.1079336628317833
train_iter_loss: 0.0751621425151825
train_iter_loss: 0.12061387300491333
train_iter_loss: 0.1765168309211731
train_iter_loss: 0.1340959072113037
train_iter_loss: 0.24513140320777893
train_iter_loss: 0.15735700726509094
train_iter_loss: 0.33963534235954285
train_iter_loss: 0.09500430524349213
train_iter_loss: 0.142894446849823
train_iter_loss: 0.03834671154618263
train_iter_loss: 0.12998689711093903
train_iter_loss: 0.1517876535654068
train_iter_loss: 0.2554967403411865
train_iter_loss: 0.11211744695901871
train_iter_loss: 0.1431860476732254
train_iter_loss: 0.08157119154930115
train_iter_loss: 0.17023086547851562
train_iter_loss: 0.1628669947385788
train_iter_loss: 0.12032502144575119
train_iter_loss: 0.12800706923007965
train_iter_loss: 0.12651629745960236
train_iter_loss: 0.1340596228837967
train_iter_loss: 0.12390308082103729
train_iter_loss: 0.3293110430240631
train_iter_loss: 0.2182989865541458
train_iter_loss: 0.11058740317821503
train_iter_loss: 0.14368513226509094
train_iter_loss: 0.3351668417453766
train_iter_loss: 0.1648823618888855
train_iter_loss: 0.11209172755479813
train_iter_loss: 0.2386835664510727
train_iter_loss: 0.14207254350185394
train_iter_loss: 0.16983440518379211
train_iter_loss: 0.10720917582511902
train_iter_loss: 0.10255098342895508
train_iter_loss: 0.17243535816669464
train_iter_loss: 0.16017091274261475
train_iter_loss: 0.38276323676109314
train_iter_loss: 0.13260646164417267
train_iter_loss: 0.12866270542144775
train_iter_loss: 0.17392724752426147
train_iter_loss: 0.21124878525733948
train_iter_loss: 0.15912596881389618
train_iter_loss: 0.15635058283805847
train_iter_loss: 0.22572939097881317
train_iter_loss: 0.20473262667655945
train_iter_loss: 0.17933765053749084
train_iter_loss: 0.15114884078502655
train_iter_loss: 0.08910404145717621
train_iter_loss: 0.10763359069824219
train_iter_loss: 0.2026890218257904
train_iter_loss: 0.2659352719783783
train_iter_loss: 0.19342269003391266
train_iter_loss: 0.0688459649682045
train_iter_loss: 0.1546776443719864
train_iter_loss: 0.262035608291626
train_iter_loss: 0.3734891414642334
train_iter_loss: 0.1507183313369751
train_iter_loss: 0.19158798456192017
train_iter_loss: 0.2067342847585678
train_iter_loss: 0.14486755430698395
train_iter_loss: 0.15219180285930634
train_iter_loss: 0.21003498136997223
train_iter_loss: 0.19181440770626068
train_iter_loss: 0.13252893090248108
train_iter_loss: 0.17156949639320374
train_iter_loss: 0.08589450269937515
train_iter_loss: 0.41673311591148376
train_iter_loss: 0.1310521960258484
train_iter_loss: 0.11224886775016785
train_iter_loss: 0.1974920779466629
train_iter_loss: 0.14614643156528473
train_iter_loss: 0.1334085911512375
train_iter_loss: 0.15877990424633026
train_iter_loss: 0.09765166789293289
train_iter_loss: 0.08160746842622757
train_iter_loss: 0.08407332748174667
train_iter_loss: 0.12112458795309067
train_iter_loss: 0.22501017153263092
train_iter_loss: 0.13593026995658875
train_iter_loss: 0.22605904936790466
train_iter_loss: 0.1517232209444046
train_iter_loss: 0.1461430937051773
train_iter_loss: 0.13397864997386932
train loss :0.1703
---------------------
Validation seg loss: 0.22362322076487373 at epoch 203
epoch =    204/  1000, exp = train
train_iter_loss: 0.13048996031284332
train_iter_loss: 0.16211843490600586
train_iter_loss: 0.09845166653394699
train_iter_loss: 0.3015175759792328
train_iter_loss: 0.21952423453330994
train_iter_loss: 0.15630412101745605
train_iter_loss: 0.19766323268413544
train_iter_loss: 0.13918715715408325
train_iter_loss: 0.2946409285068512
train_iter_loss: 0.23872403800487518
train_iter_loss: 0.17708681523799896
train_iter_loss: 0.089168019592762
train_iter_loss: 0.2078191190958023
train_iter_loss: 0.11712111532688141
train_iter_loss: 0.11832457035779953
train_iter_loss: 0.11828994005918503
train_iter_loss: 0.2423536330461502
train_iter_loss: 0.36468154191970825
train_iter_loss: 0.17719563841819763
train_iter_loss: 0.12135472148656845
train_iter_loss: 0.1955299973487854
train_iter_loss: 0.19510920345783234
train_iter_loss: 0.16983984410762787
train_iter_loss: 0.2662819027900696
train_iter_loss: 0.35114455223083496
train_iter_loss: 0.09892509132623672
train_iter_loss: 0.11046572029590607
train_iter_loss: 0.16071486473083496
train_iter_loss: 0.25629499554634094
train_iter_loss: 0.17578479647636414
train_iter_loss: 0.1382395625114441
train_iter_loss: 0.21111704409122467
train_iter_loss: 0.21816962957382202
train_iter_loss: 0.142942875623703
train_iter_loss: 0.1521768718957901
train_iter_loss: 0.20633870363235474
train_iter_loss: 0.13038161396980286
train_iter_loss: 0.1979793906211853
train_iter_loss: 0.1364181786775589
train_iter_loss: 0.28980252146720886
train_iter_loss: 0.09888376295566559
train_iter_loss: 0.11852909624576569
train_iter_loss: 0.25066500902175903
train_iter_loss: 0.09297401458024979
train_iter_loss: 0.19878827035427094
train_iter_loss: 0.1207725778222084
train_iter_loss: 0.18392214179039001
train_iter_loss: 0.12504449486732483
train_iter_loss: 0.2598922848701477
train_iter_loss: 0.2650893032550812
train_iter_loss: 0.0949331596493721
train_iter_loss: 0.1503869891166687
train_iter_loss: 0.19174647331237793
train_iter_loss: 0.2020779252052307
train_iter_loss: 0.26115837693214417
train_iter_loss: 0.055261533707380295
train_iter_loss: 0.14537537097930908
train_iter_loss: 0.2350969761610031
train_iter_loss: 0.17166990041732788
train_iter_loss: 0.09297242760658264
train_iter_loss: 0.11786926537752151
train_iter_loss: 0.18801233172416687
train_iter_loss: 0.11666198819875717
train_iter_loss: 0.19567307829856873
train_iter_loss: 0.14463555812835693
train_iter_loss: 0.11251896619796753
train_iter_loss: 0.07556506246328354
train_iter_loss: 0.20524127781391144
train_iter_loss: 0.20636345446109772
train_iter_loss: 0.18800948560237885
train_iter_loss: 0.20491792261600494
train_iter_loss: 0.13197733461856842
train_iter_loss: 0.156194269657135
train_iter_loss: 0.12773340940475464
train_iter_loss: 0.06875047087669373
train_iter_loss: 0.11921218782663345
train_iter_loss: 0.1323811411857605
train_iter_loss: 0.1752772331237793
train_iter_loss: 0.23232312500476837
train_iter_loss: 0.11537636071443558
train_iter_loss: 0.1880122870206833
train_iter_loss: 0.1681850403547287
train_iter_loss: 0.11327395588159561
train_iter_loss: 0.15255241096019745
train_iter_loss: 0.11791063100099564
train_iter_loss: 0.283756822347641
train_iter_loss: 0.15873503684997559
train_iter_loss: 0.21178153157234192
train_iter_loss: 0.114667147397995
train_iter_loss: 0.09342119097709656
train_iter_loss: 0.12688572704792023
train_iter_loss: 0.18668264150619507
train_iter_loss: 0.17026151716709137
train_iter_loss: 0.10794198513031006
train_iter_loss: 0.12431837618350983
train_iter_loss: 0.25324365496635437
train_iter_loss: 0.12996213138103485
train_iter_loss: 0.16292493045330048
train_iter_loss: 0.1252782940864563
train_iter_loss: 0.11886879056692123
train loss :0.1708
---------------------
Validation seg loss: 0.22270240256879129 at epoch 204
epoch =    205/  1000, exp = train
train_iter_loss: 0.14417628943920135
train_iter_loss: 0.1366548091173172
train_iter_loss: 0.12051095813512802
train_iter_loss: 0.14376091957092285
train_iter_loss: 0.13573351502418518
train_iter_loss: 0.20252585411071777
train_iter_loss: 0.34195563197135925
train_iter_loss: 0.09157204627990723
train_iter_loss: 0.27598753571510315
train_iter_loss: 0.13485413789749146
train_iter_loss: 0.12768813967704773
train_iter_loss: 0.12424936145544052
train_iter_loss: 0.2014426738023758
train_iter_loss: 0.13829316198825836
train_iter_loss: 0.07475284487009048
train_iter_loss: 0.1737842708826065
train_iter_loss: 0.3557824194431305
train_iter_loss: 0.22287701070308685
train_iter_loss: 0.37398555874824524
train_iter_loss: 0.345392107963562
train_iter_loss: 0.12869013845920563
train_iter_loss: 0.3017590343952179
train_iter_loss: 0.2536146938800812
train_iter_loss: 0.20100277662277222
train_iter_loss: 0.13054528832435608
train_iter_loss: 0.19188961386680603
train_iter_loss: 0.13986334204673767
train_iter_loss: 0.07016748934984207
train_iter_loss: 0.19824959337711334
train_iter_loss: 0.05705928057432175
train_iter_loss: 0.15237610042095184
train_iter_loss: 0.08926812559366226
train_iter_loss: 0.24525418877601624
train_iter_loss: 0.1814853847026825
train_iter_loss: 0.09456251561641693
train_iter_loss: 0.17789915204048157
train_iter_loss: 0.1006646379828453
train_iter_loss: 0.17343851923942566
train_iter_loss: 0.14505401253700256
train_iter_loss: 0.13837184011936188
train_iter_loss: 0.20777066051959991
train_iter_loss: 0.14456631243228912
train_iter_loss: 0.14387723803520203
train_iter_loss: 0.12634402513504028
train_iter_loss: 0.1457623541355133
train_iter_loss: 0.08953569829463959
train_iter_loss: 0.21644943952560425
train_iter_loss: 0.11428133398294449
train_iter_loss: 0.16769486665725708
train_iter_loss: 0.14492495357990265
train_iter_loss: 0.18161936104297638
train_iter_loss: 0.17485308647155762
train_iter_loss: 0.1114528626203537
train_iter_loss: 0.22840572893619537
train_iter_loss: 0.26200830936431885
train_iter_loss: 0.15638576447963715
train_iter_loss: 0.043494705110788345
train_iter_loss: 0.12932564318180084
train_iter_loss: 0.1158035546541214
train_iter_loss: 0.2031346559524536
train_iter_loss: 0.12730921804904938
train_iter_loss: 0.14580568671226501
train_iter_loss: 0.14621998369693756
train_iter_loss: 0.2058068960905075
train_iter_loss: 0.10989770293235779
train_iter_loss: 0.1592761129140854
train_iter_loss: 0.16511765122413635
train_iter_loss: 0.1274518221616745
train_iter_loss: 0.12779416143894196
train_iter_loss: 0.07043149322271347
train_iter_loss: 0.22283215820789337
train_iter_loss: 0.09698065370321274
train_iter_loss: 0.1333732306957245
train_iter_loss: 0.1901131570339203
train_iter_loss: 0.17117814719676971
train_iter_loss: 0.16336868703365326
train_iter_loss: 0.1595909595489502
train_iter_loss: 0.14611995220184326
train_iter_loss: 0.16263268887996674
train_iter_loss: 0.17964601516723633
train_iter_loss: 0.1572137176990509
train_iter_loss: 0.166652113199234
train_iter_loss: 0.18053117394447327
train_iter_loss: 0.1816834956407547
train_iter_loss: 0.19638431072235107
train_iter_loss: 0.131602942943573
train_iter_loss: 0.30319440364837646
train_iter_loss: 0.11221515387296677
train_iter_loss: 0.1357591450214386
train_iter_loss: 0.21092329919338226
train_iter_loss: 0.21095389127731323
train_iter_loss: 0.16674764454364777
train_iter_loss: 0.1875295490026474
train_iter_loss: 0.10435835272073746
train_iter_loss: 0.11008798331022263
train_iter_loss: 0.24203120172023773
train_iter_loss: 0.07406493276357651
train_iter_loss: 0.14763188362121582
train_iter_loss: 0.19560615718364716
train_iter_loss: 0.28404778242111206
train loss :0.1683
---------------------
Validation seg loss: 0.22867443879082236 at epoch 205
epoch =    206/  1000, exp = train
train_iter_loss: 0.16251523792743683
train_iter_loss: 0.10143733769655228
train_iter_loss: 0.15761618316173553
train_iter_loss: 0.12009624391794205
train_iter_loss: 0.21886104345321655
train_iter_loss: 0.19637073576450348
train_iter_loss: 0.0813698098063469
train_iter_loss: 0.27925044298171997
train_iter_loss: 0.16368331015110016
train_iter_loss: 0.0992516353726387
train_iter_loss: 0.13580723106861115
train_iter_loss: 0.10215508192777634
train_iter_loss: 0.28127801418304443
train_iter_loss: 0.21406209468841553
train_iter_loss: 0.08319951593875885
train_iter_loss: 0.13668912649154663
train_iter_loss: 0.07685840874910355
train_iter_loss: 0.4214593470096588
train_iter_loss: 0.11392588913440704
train_iter_loss: 0.15923839807510376
train_iter_loss: 0.13266679644584656
train_iter_loss: 0.2622675597667694
train_iter_loss: 0.3016032576560974
train_iter_loss: 0.047205857932567596
train_iter_loss: 0.12726996839046478
train_iter_loss: 0.14414934813976288
train_iter_loss: 0.24867621064186096
train_iter_loss: 0.13967691361904144
train_iter_loss: 0.15820756554603577
train_iter_loss: 0.09910770505666733
train_iter_loss: 0.04393812268972397
train_iter_loss: 0.10327457636594772
train_iter_loss: 0.23929928243160248
train_iter_loss: 0.25930485129356384
train_iter_loss: 0.256611168384552
train_iter_loss: 0.1288510262966156
train_iter_loss: 0.18761831521987915
train_iter_loss: 0.19237761199474335
train_iter_loss: 0.29145121574401855
train_iter_loss: 0.22881656885147095
train_iter_loss: 0.1926211565732956
train_iter_loss: 0.16966679692268372
train_iter_loss: 0.1715574562549591
train_iter_loss: 0.12810175120830536
train_iter_loss: 0.11570483446121216
train_iter_loss: 0.08732950687408447
train_iter_loss: 0.17399579286575317
train_iter_loss: 0.11542609333992004
train_iter_loss: 0.1262296438217163
train_iter_loss: 0.31308895349502563
train_iter_loss: 0.17457322776317596
train_iter_loss: 0.2670643925666809
train_iter_loss: 0.48681819438934326
train_iter_loss: 0.10181298106908798
train_iter_loss: 0.24772948026657104
train_iter_loss: 0.30225247144699097
train_iter_loss: 0.1915421336889267
train_iter_loss: 0.2018035352230072
train_iter_loss: 0.20398466289043427
train_iter_loss: 0.1663571000099182
train_iter_loss: 0.3483653962612152
train_iter_loss: 0.13852758705615997
train_iter_loss: 0.22433578968048096
train_iter_loss: 0.1881823092699051
train_iter_loss: 0.13691888749599457
train_iter_loss: 0.2260717898607254
train_iter_loss: 0.12475664913654327
train_iter_loss: 0.14097844064235687
train_iter_loss: 0.16379739344120026
train_iter_loss: 0.2163780778646469
train_iter_loss: 0.1604943871498108
train_iter_loss: 0.132725790143013
train_iter_loss: 0.08878058195114136
train_iter_loss: 0.14477962255477905
train_iter_loss: 0.06887064129114151
train_iter_loss: 0.20387804508209229
train_iter_loss: 0.18330538272857666
train_iter_loss: 0.1481175273656845
train_iter_loss: 0.11610478162765503
train_iter_loss: 0.13688746094703674
train_iter_loss: 0.12391376495361328
train_iter_loss: 0.17199598252773285
train_iter_loss: 0.12681932747364044
train_iter_loss: 0.1409725695848465
train_iter_loss: 0.12198387831449509
train_iter_loss: 0.15467916429042816
train_iter_loss: 0.37176036834716797
train_iter_loss: 0.09975026547908783
train_iter_loss: 0.14337725937366486
train_iter_loss: 0.26507768034935
train_iter_loss: 0.17941470444202423
train_iter_loss: 0.1518971025943756
train_iter_loss: 0.19025617837905884
train_iter_loss: 0.20611397922039032
train_iter_loss: 0.2681547999382019
train_iter_loss: 0.1633279025554657
train_iter_loss: 0.1281753033399582
train_iter_loss: 0.24678359925746918
train_iter_loss: 0.14417068660259247
train_iter_loss: 0.1505572646856308
train loss :0.1784
---------------------
Validation seg loss: 0.22052759418861484 at epoch 206
epoch =    207/  1000, exp = train
train_iter_loss: 0.25494053959846497
train_iter_loss: 0.08853437751531601
train_iter_loss: 0.11449966579675674
train_iter_loss: 0.28589022159576416
train_iter_loss: 0.21887823939323425
train_iter_loss: 0.14110222458839417
train_iter_loss: 0.25498467683792114
train_iter_loss: 0.17014802992343903
train_iter_loss: 0.10453557968139648
train_iter_loss: 0.24232330918312073
train_iter_loss: 0.15527032315731049
train_iter_loss: 0.28611600399017334
train_iter_loss: 0.2226482480764389
train_iter_loss: 0.14964790642261505
train_iter_loss: 0.24689948558807373
train_iter_loss: 0.14612799882888794
train_iter_loss: 0.11476083844900131
train_iter_loss: 0.1336265504360199
train_iter_loss: 0.1531454175710678
train_iter_loss: 0.30208975076675415
train_iter_loss: 0.1223050132393837
train_iter_loss: 0.22961640357971191
train_iter_loss: 0.35972216725349426
train_iter_loss: 0.14172358810901642
train_iter_loss: 0.16884391009807587
train_iter_loss: 0.16108734905719757
train_iter_loss: 0.12846453487873077
train_iter_loss: 0.17897337675094604
train_iter_loss: 0.26100677251815796
train_iter_loss: 0.21037991344928741
train_iter_loss: 0.17473798990249634
train_iter_loss: 0.12558767199516296
train_iter_loss: 0.10093042999505997
train_iter_loss: 0.22371713817119598
train_iter_loss: 0.27612391114234924
train_iter_loss: 0.16709628701210022
train_iter_loss: 0.15831054747104645
train_iter_loss: 0.17932164669036865
train_iter_loss: 0.18001314997673035
train_iter_loss: 0.14747965335845947
train_iter_loss: 0.19721947610378265
train_iter_loss: 0.20280808210372925
train_iter_loss: 0.10259666293859482
train_iter_loss: 0.11477439850568771
train_iter_loss: 0.25953391194343567
train_iter_loss: 0.18662874400615692
train_iter_loss: 0.12715394794940948
train_iter_loss: 0.18731848895549774
train_iter_loss: 0.3137853443622589
train_iter_loss: 0.21743282675743103
train_iter_loss: 0.13092251121997833
train_iter_loss: 0.22934012115001678
train_iter_loss: 0.2201874703168869
train_iter_loss: 0.1841592788696289
train_iter_loss: 0.12881775200366974
train_iter_loss: 0.27992865443229675
train_iter_loss: 0.21955475211143494
train_iter_loss: 0.19229434430599213
train_iter_loss: 0.15706440806388855
train_iter_loss: 0.08053016662597656
train_iter_loss: 0.34886327385902405
train_iter_loss: 0.14975745975971222
train_iter_loss: 0.2540874481201172
train_iter_loss: 0.17234554886817932
train_iter_loss: 0.1881941705942154
train_iter_loss: 0.15197989344596863
train_iter_loss: 0.1269027590751648
train_iter_loss: 0.10836989432573318
train_iter_loss: 0.09369932860136032
train_iter_loss: 0.13987137377262115
train_iter_loss: 0.23110756278038025
train_iter_loss: 0.2970726191997528
train_iter_loss: 0.17269007861614227
train_iter_loss: 0.1696006804704666
train_iter_loss: 0.15387633442878723
train_iter_loss: 0.15849336981773376
train_iter_loss: 0.18621261417865753
train_iter_loss: 0.17400817573070526
train_iter_loss: 0.04626181721687317
train_iter_loss: 0.16258783638477325
train_iter_loss: 0.1276402324438095
train_iter_loss: 0.15214456617832184
train_iter_loss: 0.16045360267162323
train_iter_loss: 0.1560133695602417
train_iter_loss: 0.05412251874804497
train_iter_loss: 0.24735099077224731
train_iter_loss: 0.06839591264724731
train_iter_loss: 0.10320710390806198
train_iter_loss: 0.1188989132642746
train_iter_loss: 0.16273237764835358
train_iter_loss: 0.1684710681438446
train_iter_loss: 0.18292157351970673
train_iter_loss: 0.16837720572948456
train_iter_loss: 0.2384272813796997
train_iter_loss: 0.11640284955501556
train_iter_loss: 0.09147392958402634
train_iter_loss: 0.13702309131622314
train_iter_loss: 0.08406160771846771
train_iter_loss: 0.15735754370689392
train_iter_loss: 0.1268247663974762
train loss :0.1770
---------------------
Validation seg loss: 0.22381003040222908 at epoch 207
epoch =    208/  1000, exp = train
train_iter_loss: 0.22171303629875183
train_iter_loss: 0.39337262511253357
train_iter_loss: 0.2713407576084137
train_iter_loss: 0.24510978162288666
train_iter_loss: 0.24664896726608276
train_iter_loss: 0.14980539679527283
train_iter_loss: 0.17548809945583344
train_iter_loss: 0.08705447614192963
train_iter_loss: 0.15389536321163177
train_iter_loss: 0.16528989374637604
train_iter_loss: 0.16398128867149353
train_iter_loss: 0.13266892731189728
train_iter_loss: 0.16304565966129303
train_iter_loss: 0.23545752465724945
train_iter_loss: 0.1864747554063797
train_iter_loss: 0.08043467253446579
train_iter_loss: 0.111468605697155
train_iter_loss: 0.09996303915977478
train_iter_loss: 0.24802809953689575
train_iter_loss: 0.07669457793235779
train_iter_loss: 0.21427138149738312
train_iter_loss: 0.04158981144428253
train_iter_loss: 0.15110865235328674
train_iter_loss: 0.1415691077709198
train_iter_loss: 0.17534984648227692
train_iter_loss: 0.17841635644435883
train_iter_loss: 0.12634581327438354
train_iter_loss: 0.22444596886634827
train_iter_loss: 0.24148087203502655
train_iter_loss: 0.19849559664726257
train_iter_loss: 0.11301490664482117
train_iter_loss: 0.14659743010997772
train_iter_loss: 0.3055361807346344
train_iter_loss: 0.06675981730222702
train_iter_loss: 0.171730637550354
train_iter_loss: 0.17310909926891327
train_iter_loss: 0.14912307262420654
train_iter_loss: 0.2142638862133026
train_iter_loss: 0.22642061114311218
train_iter_loss: 0.06906560063362122
train_iter_loss: 0.21102696657180786
train_iter_loss: 0.08036208152770996
train_iter_loss: 0.213272824883461
train_iter_loss: 0.17778916656970978
train_iter_loss: 0.05702543258666992
train_iter_loss: 0.17136944830417633
train_iter_loss: 0.140579953789711
train_iter_loss: 0.15157735347747803
train_iter_loss: 0.17581459879875183
train_iter_loss: 0.20581820607185364
train_iter_loss: 0.15695565938949585
train_iter_loss: 0.26058876514434814
train_iter_loss: 0.13970765471458435
train_iter_loss: 0.14333829283714294
train_iter_loss: 0.15772414207458496
train_iter_loss: 0.1524810791015625
train_iter_loss: 0.3623548150062561
train_iter_loss: 0.23903262615203857
train_iter_loss: 0.23995894193649292
train_iter_loss: 0.13670769333839417
train_iter_loss: 0.2023129016160965
train_iter_loss: 0.13376113772392273
train_iter_loss: 0.18751204013824463
train_iter_loss: 0.11055859923362732
train_iter_loss: 0.12129830569028854
train_iter_loss: 0.24638281762599945
train_iter_loss: 0.17865441739559174
train_iter_loss: 0.08819673210382462
train_iter_loss: 0.11734863370656967
train_iter_loss: 0.22211553156375885
train_iter_loss: 0.10549845546483994
train_iter_loss: 0.1961289495229721
train_iter_loss: 0.19269788265228271
train_iter_loss: 0.11512625962495804
train_iter_loss: 0.1310577094554901
train_iter_loss: 0.13901425898075104
train_iter_loss: 0.20928430557250977
train_iter_loss: 0.3241412937641144
train_iter_loss: 0.2171592265367508
train_iter_loss: 0.19687120616436005
train_iter_loss: 0.18275775015354156
train_iter_loss: 0.13520623743534088
train_iter_loss: 0.16832531988620758
train_iter_loss: 0.20744025707244873
train_iter_loss: 0.16939105093479156
train_iter_loss: 0.22426216304302216
train_iter_loss: 0.11457620561122894
train_iter_loss: 0.08554437756538391
train_iter_loss: 0.16256363689899445
train_iter_loss: 0.21883288025856018
train_iter_loss: 0.1625288873910904
train_iter_loss: 0.22687113285064697
train_iter_loss: 0.12452895939350128
train_iter_loss: 0.18554259836673737
train_iter_loss: 0.0961221307516098
train_iter_loss: 0.1989140510559082
train_iter_loss: 0.3829615116119385
train_iter_loss: 0.12120595574378967
train_iter_loss: 0.2509254217147827
train_iter_loss: 0.15214641392230988
train loss :0.1769
---------------------
Validation seg loss: 0.22233254342990103 at epoch 208
epoch =    209/  1000, exp = train
train_iter_loss: 0.13276374340057373
train_iter_loss: 0.25854718685150146
train_iter_loss: 0.16726215183734894
train_iter_loss: 0.1089143231511116
train_iter_loss: 0.06652697920799255
train_iter_loss: 0.22010457515716553
train_iter_loss: 0.05662216618657112
train_iter_loss: 0.15579505264759064
train_iter_loss: 0.24737171828746796
train_iter_loss: 0.1415889859199524
train_iter_loss: 0.10356422513723373
train_iter_loss: 0.12749281525611877
train_iter_loss: 0.17356954514980316
train_iter_loss: 0.05621405318379402
train_iter_loss: 0.34179818630218506
train_iter_loss: 0.15800444781780243
train_iter_loss: 0.15541377663612366
train_iter_loss: 0.2733374238014221
train_iter_loss: 0.1879340410232544
train_iter_loss: 0.31830140948295593
train_iter_loss: 0.30567535758018494
train_iter_loss: 0.1778520792722702
train_iter_loss: 0.12139692902565002
train_iter_loss: 0.07864389568567276
train_iter_loss: 0.14723840355873108
train_iter_loss: 0.08299481868743896
train_iter_loss: 0.24490809440612793
train_iter_loss: 0.09711974114179611
train_iter_loss: 0.22765591740608215
train_iter_loss: 0.3417038023471832
train_iter_loss: 0.31612923741340637
train_iter_loss: 0.1037946343421936
train_iter_loss: 0.10071577876806259
train_iter_loss: 0.1326535940170288
train_iter_loss: 0.21131455898284912
train_iter_loss: 0.14164163172245026
train_iter_loss: 0.18950317800045013
train_iter_loss: 0.10223205387592316
train_iter_loss: 0.21929514408111572
train_iter_loss: 0.10547424107789993
train_iter_loss: 0.14979928731918335
train_iter_loss: 0.17034564912319183
train_iter_loss: 0.22609853744506836
train_iter_loss: 0.19019968807697296
train_iter_loss: 0.1525011658668518
train_iter_loss: 0.18721787631511688
train_iter_loss: 0.1878906935453415
train_iter_loss: 0.15288160741329193
train_iter_loss: 0.1051352471113205
train_iter_loss: 0.24343526363372803
train_iter_loss: 0.16883999109268188
train_iter_loss: 0.18537451326847076
train_iter_loss: 0.16170525550842285
train_iter_loss: 0.19050189852714539
train_iter_loss: 0.13595832884311676
train_iter_loss: 0.1580737680196762
train_iter_loss: 0.06835465878248215
train_iter_loss: 0.12937182188034058
train_iter_loss: 0.18010477721691132
train_iter_loss: 0.24651911854743958
train_iter_loss: 0.2670830488204956
train_iter_loss: 0.18323390185832977
train_iter_loss: 0.11023668944835663
train_iter_loss: 0.3254444897174835
train_iter_loss: 0.21183878183364868
train_iter_loss: 0.18442882597446442
train_iter_loss: 0.09183994680643082
train_iter_loss: 0.19765529036521912
train_iter_loss: 0.17271779477596283
train_iter_loss: 0.14757609367370605
train_iter_loss: 0.21277984976768494
train_iter_loss: 0.19361978769302368
train_iter_loss: 0.08837632089853287
train_iter_loss: 0.2246980369091034
train_iter_loss: 0.15378700196743011
train_iter_loss: 0.22993946075439453
train_iter_loss: 0.16427724063396454
train_iter_loss: 0.17818869650363922
train_iter_loss: 0.12370738387107849
train_iter_loss: 0.07286939769983292
train_iter_loss: 0.22878208756446838
train_iter_loss: 0.10386288911104202
train_iter_loss: 0.07247120141983032
train_iter_loss: 0.12030211836099625
train_iter_loss: 0.27057555317878723
train_iter_loss: 0.15050794184207916
train_iter_loss: 0.15334662795066833
train_iter_loss: 0.21554811298847198
train_iter_loss: 0.1905343234539032
train_iter_loss: 0.24211306869983673
train_iter_loss: 0.19849549233913422
train_iter_loss: 0.14626367390155792
train_iter_loss: 0.18466417491436005
train_iter_loss: 0.14819902181625366
train_iter_loss: 0.19902893900871277
train_iter_loss: 0.20389389991760254
train_iter_loss: 0.1226862445473671
train_iter_loss: 0.11706830561161041
train_iter_loss: 0.13249561190605164
train_iter_loss: 0.2166500985622406
train loss :0.1742
---------------------
Validation seg loss: 0.22191375917491485 at epoch 209
epoch =    210/  1000, exp = train
train_iter_loss: 0.145664781332016
train_iter_loss: 0.12050651013851166
train_iter_loss: 0.12806068360805511
train_iter_loss: 0.2759127914905548
train_iter_loss: 0.17353834211826324
train_iter_loss: 0.09969688206911087
train_iter_loss: 0.13972431421279907
train_iter_loss: 0.17998600006103516
train_iter_loss: 0.06641770899295807
train_iter_loss: 0.2093849927186966
train_iter_loss: 0.1722945123910904
train_iter_loss: 0.09995796531438828
train_iter_loss: 0.2526065707206726
train_iter_loss: 0.19755017757415771
train_iter_loss: 0.12957888841629028
train_iter_loss: 0.21518458425998688
train_iter_loss: 0.10715829581022263
train_iter_loss: 0.3424224257469177
train_iter_loss: 0.18972496688365936
train_iter_loss: 0.225852832198143
train_iter_loss: 0.18332497775554657
train_iter_loss: 0.04250655695796013
train_iter_loss: 0.19216589629650116
train_iter_loss: 0.1102176159620285
train_iter_loss: 0.20144958794116974
train_iter_loss: 0.09323021024465561
train_iter_loss: 0.12337115406990051
train_iter_loss: 0.17784522473812103
train_iter_loss: 0.12588781118392944
train_iter_loss: 0.23368076980113983
train_iter_loss: 0.13844826817512512
train_iter_loss: 0.09515125304460526
train_iter_loss: 0.19868837296962738
train_iter_loss: 0.17918086051940918
train_iter_loss: 0.17116622626781464
train_iter_loss: 0.1442468911409378
train_iter_loss: 0.13276346027851105
train_iter_loss: 0.09603960812091827
train_iter_loss: 0.17616678774356842
train_iter_loss: 0.15740785002708435
train_iter_loss: 0.25327110290527344
train_iter_loss: 0.19412358105182648
train_iter_loss: 0.27829957008361816
train_iter_loss: 0.27864158153533936
train_iter_loss: 0.14755551517009735
train_iter_loss: 0.12269815802574158
train_iter_loss: 0.2959088385105133
train_iter_loss: 0.1619013547897339
train_iter_loss: 0.14204230904579163
train_iter_loss: 0.22978368401527405
train_iter_loss: 0.15990819036960602
train_iter_loss: 0.18721772730350494
train_iter_loss: 0.1336461454629898
train_iter_loss: 0.18540075421333313
train_iter_loss: 0.33245089650154114
train_iter_loss: 0.12612202763557434
train_iter_loss: 0.14929574728012085
train_iter_loss: 0.08825657516717911
train_iter_loss: 0.1314927637577057
train_iter_loss: 0.07232300937175751
train_iter_loss: 0.1674058735370636
train_iter_loss: 0.05671163275837898
train_iter_loss: 0.09953470528125763
train_iter_loss: 0.3756171762943268
train_iter_loss: 0.18929140269756317
train_iter_loss: 0.1952683925628662
train_iter_loss: 0.11126107722520828
train_iter_loss: 0.18807269632816315
train_iter_loss: 0.1831507682800293
train_iter_loss: 0.15326224267482758
train_iter_loss: 0.10782880336046219
train_iter_loss: 0.10118143260478973
train_iter_loss: 0.16735686361789703
train_iter_loss: 0.14834247529506683
train_iter_loss: 0.31425929069519043
train_iter_loss: 0.2537245452404022
train_iter_loss: 0.15538005530834198
train_iter_loss: 0.23429900407791138
train_iter_loss: 0.1325908750295639
train_iter_loss: 0.17861372232437134
train_iter_loss: 0.330843061208725
train_iter_loss: 0.16668067872524261
train_iter_loss: 0.14884024858474731
train_iter_loss: 0.22715908288955688
train_iter_loss: 0.055059079080820084
train_iter_loss: 0.10840614885091782
train_iter_loss: 0.16329868137836456
train_iter_loss: 0.15294286608695984
train_iter_loss: 0.15379521250724792
train_iter_loss: 0.10886453092098236
train_iter_loss: 0.10717446357011795
train_iter_loss: 0.14775073528289795
train_iter_loss: 0.124483622610569
train_iter_loss: 0.08397407084703445
train_iter_loss: 0.24659982323646545
train_iter_loss: 0.25220003724098206
train_iter_loss: 0.2312210649251938
train_iter_loss: 0.29318928718566895
train_iter_loss: 0.11809445172548294
train_iter_loss: 0.13200639188289642
train loss :0.1715
---------------------
Validation seg loss: 0.2228613627529791 at epoch 210
epoch =    211/  1000, exp = train
train_iter_loss: 0.24820926785469055
train_iter_loss: 0.16741923987865448
train_iter_loss: 0.12301629781723022
train_iter_loss: 0.15379439294338226
train_iter_loss: 0.25558438897132874
train_iter_loss: 0.2704116404056549
train_iter_loss: 0.11733771860599518
train_iter_loss: 0.2965414524078369
train_iter_loss: 0.1523646116256714
train_iter_loss: 0.13301928341388702
train_iter_loss: 0.19765326380729675
train_iter_loss: 0.19991731643676758
train_iter_loss: 0.14550843834877014
train_iter_loss: 0.20435182750225067
train_iter_loss: 0.11091925203800201
train_iter_loss: 0.11570475995540619
train_iter_loss: 0.22445955872535706
train_iter_loss: 0.28314000368118286
train_iter_loss: 0.15408889949321747
train_iter_loss: 0.08599581569433212
train_iter_loss: 0.17419376969337463
train_iter_loss: 0.09624292701482773
train_iter_loss: 0.13660873472690582
train_iter_loss: 0.16903212666511536
train_iter_loss: 0.1422094851732254
train_iter_loss: 0.14711490273475647
train_iter_loss: 0.18505455553531647
train_iter_loss: 0.26431071758270264
train_iter_loss: 0.19777317345142365
train_iter_loss: 0.20860345661640167
train_iter_loss: 0.08933904767036438
train_iter_loss: 0.09782448410987854
train_iter_loss: 0.46725746989250183
train_iter_loss: 0.23635615408420563
train_iter_loss: 0.08020439743995667
train_iter_loss: 0.26833638548851013
train_iter_loss: 0.07832781225442886
train_iter_loss: 0.048686083406209946
train_iter_loss: 0.14593225717544556
train_iter_loss: 0.03876028582453728
train_iter_loss: 0.2624047100543976
train_iter_loss: 0.15776820480823517
train_iter_loss: 0.15631692111492157
train_iter_loss: 0.20515590906143188
train_iter_loss: 0.22276785969734192
train_iter_loss: 0.190877303481102
train_iter_loss: 0.11867377161979675
train_iter_loss: 0.07883167266845703
train_iter_loss: 0.285316526889801
train_iter_loss: 0.1553691029548645
train_iter_loss: 0.12537145614624023
train_iter_loss: 0.0778765082359314
train_iter_loss: 0.10240387171506882
train_iter_loss: 0.16836994886398315
train_iter_loss: 0.11103427410125732
train_iter_loss: 0.12308694422245026
train_iter_loss: 0.19120152294635773
train_iter_loss: 0.1943330317735672
train_iter_loss: 0.3025709092617035
train_iter_loss: 0.13146553933620453
train_iter_loss: 0.22185973823070526
train_iter_loss: 0.1460413932800293
train_iter_loss: 0.1438346654176712
train_iter_loss: 0.15421366691589355
train_iter_loss: 0.18318793177604675
train_iter_loss: 0.12183883786201477
train_iter_loss: 0.18276569247245789
train_iter_loss: 0.17658796906471252
train_iter_loss: 0.15642811357975006
train_iter_loss: 0.21378540992736816
train_iter_loss: 0.2517086863517761
train_iter_loss: 0.11803433299064636
train_iter_loss: 0.1449938267469406
train_iter_loss: 0.10744243115186691
train_iter_loss: 0.21498790383338928
train_iter_loss: 0.11334500461816788
train_iter_loss: 0.2813575267791748
train_iter_loss: 0.11377565562725067
train_iter_loss: 0.3011876344680786
train_iter_loss: 0.2057902216911316
train_iter_loss: 0.1108335554599762
train_iter_loss: 0.19267617166042328
train_iter_loss: 0.20897158980369568
train_iter_loss: 0.06302725523710251
train_iter_loss: 0.13947096467018127
train_iter_loss: 0.28811556100845337
train_iter_loss: 0.14741858839988708
train_iter_loss: 0.19465339183807373
train_iter_loss: 0.11106881499290466
train_iter_loss: 0.22023816406726837
train_iter_loss: 0.21457424759864807
train_iter_loss: 0.3595115542411804
train_iter_loss: 0.2721644639968872
train_iter_loss: 0.19552206993103027
train_iter_loss: 0.35938286781311035
train_iter_loss: 0.18028445541858673
train_iter_loss: 0.2135787457227707
train_iter_loss: 0.17006832361221313
train_iter_loss: 0.21674370765686035
train_iter_loss: 0.1589149683713913
train loss :0.1801
---------------------
Validation seg loss: 0.22839195996452616 at epoch 211
epoch =    212/  1000, exp = train
train_iter_loss: 0.22028325498104095
train_iter_loss: 0.14500980079174042
train_iter_loss: 0.16454923152923584
train_iter_loss: 0.2832341194152832
train_iter_loss: 0.07142096012830734
train_iter_loss: 0.13560767471790314
train_iter_loss: 0.06519459187984467
train_iter_loss: 0.2343922108411789
train_iter_loss: 0.14950527250766754
train_iter_loss: 0.21983042359352112
train_iter_loss: 0.17894181609153748
train_iter_loss: 0.26348140835762024
train_iter_loss: 0.16469012200832367
train_iter_loss: 0.1476590633392334
train_iter_loss: 0.12695816159248352
train_iter_loss: 0.11866619437932968
train_iter_loss: 0.22953423857688904
train_iter_loss: 0.12473327666521072
train_iter_loss: 0.14539672434329987
train_iter_loss: 0.09254676848649979
train_iter_loss: 0.1615608036518097
train_iter_loss: 0.2672343850135803
train_iter_loss: 0.14492645859718323
train_iter_loss: 0.29228177666664124
train_iter_loss: 0.28409522771835327
train_iter_loss: 0.30978715419769287
train_iter_loss: 0.16306011378765106
train_iter_loss: 0.14830422401428223
train_iter_loss: 0.2567271292209625
train_iter_loss: 0.19885414838790894
train_iter_loss: 0.12552016973495483
train_iter_loss: 0.13156692683696747
train_iter_loss: 0.10544461756944656
train_iter_loss: 0.14922747015953064
train_iter_loss: 0.33968740701675415
train_iter_loss: 0.032317619770765305
train_iter_loss: 0.10440634936094284
train_iter_loss: 0.10537223517894745
train_iter_loss: 0.08236558735370636
train_iter_loss: 0.1801384836435318
train_iter_loss: 0.19620217382907867
train_iter_loss: 0.10149505734443665
train_iter_loss: 0.17200715839862823
train_iter_loss: 0.23266920447349548
train_iter_loss: 0.11311686038970947
train_iter_loss: 0.11464107036590576
train_iter_loss: 0.1670333743095398
train_iter_loss: 0.13992221653461456
train_iter_loss: 0.07565385103225708
train_iter_loss: 0.13087090849876404
train_iter_loss: 0.18973691761493683
train_iter_loss: 0.1228926032781601
train_iter_loss: 0.14450645446777344
train_iter_loss: 0.23806872963905334
train_iter_loss: 0.09920171648263931
train_iter_loss: 0.10429242253303528
train_iter_loss: 0.08371169865131378
train_iter_loss: 0.2877446711063385
train_iter_loss: 0.2269795536994934
train_iter_loss: 0.1460939198732376
train_iter_loss: 0.16438284516334534
train_iter_loss: 0.26977303624153137
train_iter_loss: 0.10996892303228378
train_iter_loss: 0.23471076786518097
train_iter_loss: 0.20353259146213531
train_iter_loss: 0.14112168550491333
train_iter_loss: 0.07585342228412628
train_iter_loss: 0.21013477444648743
train_iter_loss: 0.06718737632036209
train_iter_loss: 0.1306878924369812
train_iter_loss: 0.2202635556459427
train_iter_loss: 0.17798808217048645
train_iter_loss: 0.21798641979694366
train_iter_loss: 0.21060211956501007
train_iter_loss: 0.28486037254333496
train_iter_loss: 0.16758236289024353
train_iter_loss: 0.16108635067939758
train_iter_loss: 0.15009918808937073
train_iter_loss: 0.2878268361091614
train_iter_loss: 0.17615413665771484
train_iter_loss: 0.07173079252243042
train_iter_loss: 0.19027794897556305
train_iter_loss: 0.12567253410816193
train_iter_loss: 0.16652120649814606
train_iter_loss: 0.15768224000930786
train_iter_loss: 0.07973010092973709
train_iter_loss: 0.1462884396314621
train_iter_loss: 0.2520048916339874
train_iter_loss: 0.13322561979293823
train_iter_loss: 0.3143570125102997
train_iter_loss: 0.120687335729599
train_iter_loss: 0.10607898980379105
train_iter_loss: 0.1342896670103073
train_iter_loss: 0.11458300054073334
train_iter_loss: 0.1695902794599533
train_iter_loss: 0.30012229084968567
train_iter_loss: 0.05889247730374336
train_iter_loss: 0.17876848578453064
train_iter_loss: 0.2668631374835968
train_iter_loss: 0.11225488036870956
train loss :0.1699
---------------------
Validation seg loss: 0.22198010285226805 at epoch 212
epoch =    213/  1000, exp = train
train_iter_loss: 0.22600731253623962
train_iter_loss: 0.11215129494667053
train_iter_loss: 0.18510010838508606
train_iter_loss: 0.1295248121023178
train_iter_loss: 0.1275787502527237
train_iter_loss: 0.13481134176254272
train_iter_loss: 0.24386708438396454
train_iter_loss: 0.09337878972291946
train_iter_loss: 0.2654235064983368
train_iter_loss: 0.18353171646595
train_iter_loss: 0.05536026507616043
train_iter_loss: 0.21607401967048645
train_iter_loss: 0.2760346531867981
train_iter_loss: 0.24551044404506683
train_iter_loss: 0.1767231971025467
train_iter_loss: 0.23480479419231415
train_iter_loss: 0.373250275850296
train_iter_loss: 0.18718519806861877
train_iter_loss: 0.10696364939212799
train_iter_loss: 0.2416888028383255
train_iter_loss: 0.04878832399845123
train_iter_loss: 0.1319873183965683
train_iter_loss: 0.22924098372459412
train_iter_loss: 0.06771683692932129
train_iter_loss: 0.13844114542007446
train_iter_loss: 0.11689620465040207
train_iter_loss: 0.20951785147190094
train_iter_loss: 0.11604481190443039
train_iter_loss: 0.11631736904382706
train_iter_loss: 0.08416922390460968
train_iter_loss: 0.11237633228302002
train_iter_loss: 0.10574904084205627
train_iter_loss: 0.3206002712249756
train_iter_loss: 0.2768314480781555
train_iter_loss: 0.1252637654542923
train_iter_loss: 0.09842518717050552
train_iter_loss: 0.16280938684940338
train_iter_loss: 0.06905075162649155
train_iter_loss: 0.12499643117189407
train_iter_loss: 0.2060157209634781
train_iter_loss: 0.15005557239055634
train_iter_loss: 0.18148024380207062
train_iter_loss: 0.21743415296077728
train_iter_loss: 0.09235329926013947
train_iter_loss: 0.0665699765086174
train_iter_loss: 0.15124846994876862
train_iter_loss: 0.30480536818504333
train_iter_loss: 0.12283606082201004
train_iter_loss: 0.4141300916671753
train_iter_loss: 0.16701477766036987
train_iter_loss: 0.21739867329597473
train_iter_loss: 0.15172339975833893
train_iter_loss: 0.22167260944843292
train_iter_loss: 0.10589329153299332
train_iter_loss: 0.14216108620166779
train_iter_loss: 0.2661297917366028
train_iter_loss: 0.25830185413360596
train_iter_loss: 0.1802160143852234
train_iter_loss: 0.1269984394311905
train_iter_loss: 0.17266400158405304
train_iter_loss: 0.14045649766921997
train_iter_loss: 0.18168793618679047
train_iter_loss: 0.18900997936725616
train_iter_loss: 0.16316236555576324
train_iter_loss: 0.18521159887313843
train_iter_loss: 0.29022926092147827
train_iter_loss: 0.12945586442947388
train_iter_loss: 0.12860672175884247
train_iter_loss: 0.0922650545835495
train_iter_loss: 0.22981290519237518
train_iter_loss: 0.16414369642734528
train_iter_loss: 0.15803034603595734
train_iter_loss: 0.130198135972023
train_iter_loss: 0.1404137909412384
train_iter_loss: 0.2618849575519562
train_iter_loss: 0.1628153920173645
train_iter_loss: 0.22027449309825897
train_iter_loss: 0.10963280498981476
train_iter_loss: 0.12061454355716705
train_iter_loss: 0.19688411056995392
train_iter_loss: 0.1913544237613678
train_iter_loss: 0.22808027267456055
train_iter_loss: 0.11344866454601288
train_iter_loss: 0.1137685626745224
train_iter_loss: 0.1758740246295929
train_iter_loss: 0.15670020878314972
train_iter_loss: 0.17443642020225525
train_iter_loss: 0.20084553956985474
train_iter_loss: 0.11103671044111252
train_iter_loss: 0.14550574123859406
train_iter_loss: 0.09386125206947327
train_iter_loss: 0.12493434548377991
train_iter_loss: 0.23384568095207214
train_iter_loss: 0.19951683282852173
train_iter_loss: 0.27838122844696045
train_iter_loss: 0.1922955960035324
train_iter_loss: 0.09238319098949432
train_iter_loss: 0.14319589734077454
train_iter_loss: 0.15894731879234314
train_iter_loss: 0.20963414013385773
train loss :0.1729
---------------------
Validation seg loss: 0.22279868014859702 at epoch 213
epoch =    214/  1000, exp = train
train_iter_loss: 0.17001311480998993
train_iter_loss: 0.1869632452726364
train_iter_loss: 0.11094020307064056
train_iter_loss: 0.09420614689588547
train_iter_loss: 0.14955110847949982
train_iter_loss: 0.17087994515895844
train_iter_loss: 0.21449705958366394
train_iter_loss: 0.09700582921504974
train_iter_loss: 0.09762901067733765
train_iter_loss: 0.1227637231349945
train_iter_loss: 0.25509488582611084
train_iter_loss: 0.058943115174770355
train_iter_loss: 0.23492461442947388
train_iter_loss: 0.1681303232908249
train_iter_loss: 0.1365288645029068
train_iter_loss: 0.15144920349121094
train_iter_loss: 0.08531005680561066
train_iter_loss: 0.12781095504760742
train_iter_loss: 0.09952466934919357
train_iter_loss: 0.18064992129802704
train_iter_loss: 0.15641076862812042
train_iter_loss: 0.3212208151817322
train_iter_loss: 0.1295449584722519
train_iter_loss: 0.15072201192378998
train_iter_loss: 0.10183479636907578
train_iter_loss: 0.08266517519950867
train_iter_loss: 0.10386912524700165
train_iter_loss: 0.2327466607093811
train_iter_loss: 0.21246042847633362
train_iter_loss: 0.25984522700309753
train_iter_loss: 0.12359417974948883
train_iter_loss: 0.16733933985233307
train_iter_loss: 0.13304118812084198
train_iter_loss: 0.16904021799564362
train_iter_loss: 0.14256545901298523
train_iter_loss: 0.10928383469581604
train_iter_loss: 0.20195917785167694
train_iter_loss: 0.13331927359104156
train_iter_loss: 0.16884185373783112
train_iter_loss: 0.31761622428894043
train_iter_loss: 0.23186534643173218
train_iter_loss: 0.11821357905864716
train_iter_loss: 0.18733808398246765
train_iter_loss: 0.18210554122924805
train_iter_loss: 0.15630657970905304
train_iter_loss: 0.12151151150465012
train_iter_loss: 0.14121229946613312
train_iter_loss: 0.16255095601081848
train_iter_loss: 0.15178576111793518
train_iter_loss: 0.1833232194185257
train_iter_loss: 0.07352650165557861
train_iter_loss: 0.16366998851299286
train_iter_loss: 0.15485872328281403
train_iter_loss: 0.2514287531375885
train_iter_loss: 0.26829013228416443
train_iter_loss: 0.21675525605678558
train_iter_loss: 0.21370984613895416
train_iter_loss: 0.18888702988624573
train_iter_loss: 0.22249241173267365
train_iter_loss: 0.18241849541664124
train_iter_loss: 0.1770612746477127
train_iter_loss: 0.3002427816390991
train_iter_loss: 0.19252614676952362
train_iter_loss: 0.08340439945459366
train_iter_loss: 0.19786310195922852
train_iter_loss: 0.22141683101654053
train_iter_loss: 0.14633797109127045
train_iter_loss: 0.180982768535614
train_iter_loss: 0.06523798406124115
train_iter_loss: 0.0997304692864418
train_iter_loss: 0.2626296281814575
train_iter_loss: 0.17938190698623657
train_iter_loss: 0.17352983355522156
train_iter_loss: 0.14928856492042542
train_iter_loss: 0.08074541389942169
train_iter_loss: 0.16719292104244232
train_iter_loss: 0.13408100605010986
train_iter_loss: 0.11430037021636963
train_iter_loss: 0.07906106859445572
train_iter_loss: 0.0833071768283844
train_iter_loss: 0.1902703046798706
train_iter_loss: 0.1356237828731537
train_iter_loss: 0.15278375148773193
train_iter_loss: 0.19189739227294922
train_iter_loss: 0.1736094206571579
train_iter_loss: 0.22438374161720276
train_iter_loss: 0.16679328680038452
train_iter_loss: 0.10813268274068832
train_iter_loss: 0.12830229103565216
train_iter_loss: 0.11698411405086517
train_iter_loss: 0.2168797105550766
train_iter_loss: 0.19809307157993317
train_iter_loss: 0.23576906323432922
train_iter_loss: 0.233645960688591
train_iter_loss: 0.14325924217700958
train_iter_loss: 0.1656591296195984
train_iter_loss: 0.28205791115760803
train_iter_loss: 0.21483024954795837
train_iter_loss: 0.25078123807907104
train_iter_loss: 0.17754343152046204
train loss :0.1687
---------------------
Validation seg loss: 0.22142379272307428 at epoch 214
epoch =    215/  1000, exp = train
train_iter_loss: 0.18789680302143097
train_iter_loss: 0.10533367097377777
train_iter_loss: 0.1586294025182724
train_iter_loss: 0.09816990792751312
train_iter_loss: 0.0998350977897644
train_iter_loss: 0.08825481683015823
train_iter_loss: 0.11542277783155441
train_iter_loss: 0.1341552883386612
train_iter_loss: 0.2715340554714203
train_iter_loss: 0.1992293894290924
train_iter_loss: 0.13753129541873932
train_iter_loss: 0.22375454008579254
train_iter_loss: 0.09462997317314148
train_iter_loss: 0.33891209959983826
train_iter_loss: 0.18696622550487518
train_iter_loss: 0.11226959526538849
train_iter_loss: 0.13555891811847687
train_iter_loss: 0.1820005476474762
train_iter_loss: 0.14298854768276215
train_iter_loss: 0.10896831750869751
train_iter_loss: 0.07767924666404724
train_iter_loss: 0.3192879557609558
train_iter_loss: 0.13046573102474213
train_iter_loss: 0.1652894914150238
train_iter_loss: 0.11819243431091309
train_iter_loss: 0.17273589968681335
train_iter_loss: 0.0896877571940422
train_iter_loss: 0.11234154552221298
train_iter_loss: 0.29604119062423706
train_iter_loss: 0.16375821828842163
train_iter_loss: 0.10359018296003342
train_iter_loss: 0.1322362869977951
train_iter_loss: 0.14182555675506592
train_iter_loss: 0.15144947171211243
train_iter_loss: 0.13724833726882935
train_iter_loss: 0.13061405718326569
train_iter_loss: 0.10574516654014587
train_iter_loss: 0.20564496517181396
train_iter_loss: 0.21986572444438934
train_iter_loss: 0.19872747361660004
train_iter_loss: 0.1385820209980011
train_iter_loss: 0.11841462552547455
train_iter_loss: 0.14255400002002716
train_iter_loss: 0.1539860963821411
train_iter_loss: 0.15938037633895874
train_iter_loss: 0.124186672270298
train_iter_loss: 0.2320825755596161
train_iter_loss: 0.17736752331256866
train_iter_loss: 0.1863170564174652
train_iter_loss: 0.11669286340475082
train_iter_loss: 0.20287665724754333
train_iter_loss: 0.18183211982250214
train_iter_loss: 0.19930855929851532
train_iter_loss: 0.16701246798038483
train_iter_loss: 0.15502196550369263
train_iter_loss: 0.17091868817806244
train_iter_loss: 0.17924809455871582
train_iter_loss: 0.17403897643089294
train_iter_loss: 0.20878572762012482
train_iter_loss: 0.2002580463886261
train_iter_loss: 0.1550084799528122
train_iter_loss: 0.11984489858150482
train_iter_loss: 0.19987928867340088
train_iter_loss: 0.15119487047195435
train_iter_loss: 0.11400361359119415
train_iter_loss: 0.1621808111667633
train_iter_loss: 0.14937670528888702
train_iter_loss: 0.17278791964054108
train_iter_loss: 0.2025775909423828
train_iter_loss: 0.2746316194534302
train_iter_loss: 0.26110532879829407
train_iter_loss: 0.23519010841846466
train_iter_loss: 0.13452541828155518
train_iter_loss: 0.2577130198478699
train_iter_loss: 0.13834206759929657
train_iter_loss: 0.21505990624427795
train_iter_loss: 0.2850859761238098
train_iter_loss: 0.2684420049190521
train_iter_loss: 0.3441210985183716
train_iter_loss: 0.12368185818195343
train_iter_loss: 0.16786199808120728
train_iter_loss: 0.2164759784936905
train_iter_loss: 0.1307957023382187
train_iter_loss: 0.14185217022895813
train_iter_loss: 0.1876051127910614
train_iter_loss: 0.33058831095695496
train_iter_loss: 0.1250050663948059
train_iter_loss: 0.16649077832698822
train_iter_loss: 0.14500612020492554
train_iter_loss: 0.10715629160404205
train_iter_loss: 0.11679168045520782
train_iter_loss: 0.21224530041217804
train_iter_loss: 0.21730771660804749
train_iter_loss: 0.17555175721645355
train_iter_loss: 0.1954532414674759
train_iter_loss: 0.10074138641357422
train_iter_loss: 0.2533630132675171
train_iter_loss: 0.08821236342191696
train_iter_loss: 0.14390632510185242
train_iter_loss: 0.10277549177408218
train loss :0.1714
---------------------
Validation seg loss: 0.22209302764737382 at epoch 215
epoch =    216/  1000, exp = train
train_iter_loss: 0.20793938636779785
train_iter_loss: 0.15295857191085815
train_iter_loss: 0.09187166392803192
train_iter_loss: 0.12723524868488312
train_iter_loss: 0.05501161888241768
train_iter_loss: 0.24010908603668213
train_iter_loss: 0.0886441245675087
train_iter_loss: 0.14274273812770844
train_iter_loss: 0.19224011898040771
train_iter_loss: 0.24965134263038635
train_iter_loss: 0.15971480309963226
train_iter_loss: 0.13248984515666962
train_iter_loss: 0.11251268535852432
train_iter_loss: 0.27830302715301514
train_iter_loss: 0.24489475786685944
train_iter_loss: 0.3236672282218933
train_iter_loss: 0.1562582105398178
train_iter_loss: 0.12482466548681259
train_iter_loss: 0.23248638212680817
train_iter_loss: 0.14693331718444824
train_iter_loss: 0.21019308269023895
train_iter_loss: 0.1295188069343567
train_iter_loss: 0.19853636622428894
train_iter_loss: 0.16867420077323914
train_iter_loss: 0.13220086693763733
train_iter_loss: 0.15144073963165283
train_iter_loss: 0.11919475346803665
train_iter_loss: 0.13006433844566345
train_iter_loss: 0.16622135043144226
train_iter_loss: 0.22778798639774323
train_iter_loss: 0.12634219229221344
train_iter_loss: 0.17155325412750244
train_iter_loss: 0.146611750125885
train_iter_loss: 0.10748235881328583
train_iter_loss: 0.1690325140953064
train_iter_loss: 0.15735287964344025
train_iter_loss: 0.16818822920322418
train_iter_loss: 0.12388405948877335
train_iter_loss: 0.13881713151931763
train_iter_loss: 0.1464299112558365
train_iter_loss: 0.10648991912603378
train_iter_loss: 0.073331318795681
train_iter_loss: 0.18764810264110565
train_iter_loss: 0.1365639716386795
train_iter_loss: 0.1842411905527115
train_iter_loss: 0.19449251890182495
train_iter_loss: 0.22968856990337372
train_iter_loss: 0.13631512224674225
train_iter_loss: 0.186720073223114
train_iter_loss: 0.18849118053913116
train_iter_loss: 0.11923074722290039
train_iter_loss: 0.14051717519760132
train_iter_loss: 0.31291672587394714
train_iter_loss: 0.2214154303073883
train_iter_loss: 0.11507503688335419
train_iter_loss: 0.18191353976726532
train_iter_loss: 0.14626459777355194
train_iter_loss: 0.15729375183582306
train_iter_loss: 0.20953047275543213
train_iter_loss: 0.12956532835960388
train_iter_loss: 0.13439100980758667
train_iter_loss: 0.1062069833278656
train_iter_loss: 0.1642891764640808
train_iter_loss: 0.13981418311595917
train_iter_loss: 0.23778022825717926
train_iter_loss: 0.21696680784225464
train_iter_loss: 0.20343877375125885
train_iter_loss: 0.1457320749759674
train_iter_loss: 0.18328242003917694
train_iter_loss: 0.2472192496061325
train_iter_loss: 0.18282577395439148
train_iter_loss: 0.22758778929710388
train_iter_loss: 0.2207053005695343
train_iter_loss: 0.16025283932685852
train_iter_loss: 0.11436858028173447
train_iter_loss: 0.22881506383419037
train_iter_loss: 0.2058294713497162
train_iter_loss: 0.15375091135501862
train_iter_loss: 0.13914984464645386
train_iter_loss: 0.13654467463493347
train_iter_loss: 0.20971691608428955
train_iter_loss: 0.15001575648784637
train_iter_loss: 0.1667429506778717
train_iter_loss: 0.14931577444076538
train_iter_loss: 0.09299584478139877
train_iter_loss: 0.05045035853981972
train_iter_loss: 0.16584232449531555
train_iter_loss: 0.10017585754394531
train_iter_loss: 0.1209641695022583
train_iter_loss: 0.18998563289642334
train_iter_loss: 0.15023209154605865
train_iter_loss: 0.10565271228551865
train_iter_loss: 0.14678603410720825
train_iter_loss: 0.185921311378479
train_iter_loss: 0.19236379861831665
train_iter_loss: 0.14762867987155914
train_iter_loss: 0.11469806730747223
train_iter_loss: 0.16801239550113678
train_iter_loss: 0.13575908541679382
train_iter_loss: 0.2636515498161316
train loss :0.1664
---------------------
Validation seg loss: 0.22230596267530378 at epoch 216
epoch =    217/  1000, exp = train
train_iter_loss: 0.09853622317314148
train_iter_loss: 0.07583004236221313
train_iter_loss: 0.1540093570947647
train_iter_loss: 0.12621600925922394
train_iter_loss: 0.09813006967306137
train_iter_loss: 0.09845340251922607
train_iter_loss: 0.33610406517982483
train_iter_loss: 0.1836068034172058
train_iter_loss: 0.07968159019947052
train_iter_loss: 0.050576094537973404
train_iter_loss: 0.2228202223777771
train_iter_loss: 0.11612287908792496
train_iter_loss: 0.1956666111946106
train_iter_loss: 0.1353011578321457
train_iter_loss: 0.15046991407871246
train_iter_loss: 0.18170718848705292
train_iter_loss: 0.1332787275314331
train_iter_loss: 0.10388752818107605
train_iter_loss: 0.1421150267124176
train_iter_loss: 0.17450954020023346
train_iter_loss: 0.18077465891838074
train_iter_loss: 0.27020058035850525
train_iter_loss: 0.19313807785511017
train_iter_loss: 0.20544148981571198
train_iter_loss: 0.10197680443525314
train_iter_loss: 0.24281272292137146
train_iter_loss: 0.24959956109523773
train_iter_loss: 0.2359967827796936
train_iter_loss: 0.18927809596061707
train_iter_loss: 0.11471656709909439
train_iter_loss: 0.10707578808069229
train_iter_loss: 0.1098257452249527
train_iter_loss: 0.08034462481737137
train_iter_loss: 0.27776414155960083
train_iter_loss: 0.23519417643547058
train_iter_loss: 0.15739703178405762
train_iter_loss: 0.1802992969751358
train_iter_loss: 0.08209440112113953
train_iter_loss: 0.1342867910861969
train_iter_loss: 0.14351196587085724
train_iter_loss: 0.12690547108650208
train_iter_loss: 0.11290260404348373
train_iter_loss: 0.11959467083215714
train_iter_loss: 0.199764221906662
train_iter_loss: 0.14344479143619537
train_iter_loss: 0.1949549913406372
train_iter_loss: 0.1250305026769638
train_iter_loss: 0.21065546572208405
train_iter_loss: 0.19385254383087158
train_iter_loss: 0.21585743129253387
train_iter_loss: 0.19988590478897095
train_iter_loss: 0.09257547557353973
train_iter_loss: 0.20286425948143005
train_iter_loss: 0.17281071841716766
train_iter_loss: 0.2490394413471222
train_iter_loss: 0.14545851945877075
train_iter_loss: 0.16362838447093964
train_iter_loss: 0.16170910000801086
train_iter_loss: 0.13355828821659088
train_iter_loss: 0.20585423707962036
train_iter_loss: 0.10530418902635574
train_iter_loss: 0.12260610610246658
train_iter_loss: 0.2435835599899292
train_iter_loss: 0.26084285974502563
train_iter_loss: 0.1704222559928894
train_iter_loss: 0.20248956978321075
train_iter_loss: 0.06756535917520523
train_iter_loss: 0.16842538118362427
train_iter_loss: 0.1623305082321167
train_iter_loss: 0.23529207706451416
train_iter_loss: 0.17598220705986023
train_iter_loss: 0.2902017831802368
train_iter_loss: 0.12745700776576996
train_iter_loss: 0.1752021759748459
train_iter_loss: 0.17462211847305298
train_iter_loss: 0.20848050713539124
train_iter_loss: 0.13812419772148132
train_iter_loss: 0.1739749163389206
train_iter_loss: 0.2702784240245819
train_iter_loss: 0.242620050907135
train_iter_loss: 0.1044672280550003
train_iter_loss: 0.22702915966510773
train_iter_loss: 0.1817677915096283
train_iter_loss: 0.24060013890266418
train_iter_loss: 0.1335391253232956
train_iter_loss: 0.13391470909118652
train_iter_loss: 0.23422731459140778
train_iter_loss: 0.16966916620731354
train_iter_loss: 0.12633062899112701
train_iter_loss: 0.07869642972946167
train_iter_loss: 0.2015959471464157
train_iter_loss: 0.21211250126361847
train_iter_loss: 0.2048371285200119
train_iter_loss: 0.18162895739078522
train_iter_loss: 0.3364541828632355
train_iter_loss: 0.18165111541748047
train_iter_loss: 0.08831460028886795
train_iter_loss: 0.10725893825292587
train_iter_loss: 0.26119863986968994
train_iter_loss: 0.1276136189699173
train loss :0.1713
---------------------
Validation seg loss: 0.22198648307964486 at epoch 217
epoch =    218/  1000, exp = train
train_iter_loss: 0.10588366538286209
train_iter_loss: 0.2731158137321472
train_iter_loss: 0.23316490650177002
train_iter_loss: 0.054967861622571945
train_iter_loss: 0.26770588755607605
train_iter_loss: 0.10990993678569794
train_iter_loss: 0.17473486065864563
train_iter_loss: 0.04229036718606949
train_iter_loss: 0.17597003281116486
train_iter_loss: 0.15571385622024536
train_iter_loss: 0.11052589118480682
train_iter_loss: 0.20476968586444855
train_iter_loss: 0.24968713521957397
train_iter_loss: 0.318289190530777
train_iter_loss: 0.07261967658996582
train_iter_loss: 0.14317193627357483
train_iter_loss: 0.12388874590396881
train_iter_loss: 0.1010894626379013
train_iter_loss: 0.17567166686058044
train_iter_loss: 0.13916750252246857
train_iter_loss: 0.1419985592365265
train_iter_loss: 0.16331851482391357
train_iter_loss: 0.2536561191082001
train_iter_loss: 0.12439893186092377
train_iter_loss: 0.14297550916671753
train_iter_loss: 0.19720222055912018
train_iter_loss: 0.2524734437465668
train_iter_loss: 0.08068543672561646
train_iter_loss: 0.13956008851528168
train_iter_loss: 0.0742822140455246
train_iter_loss: 0.13948413729667664
train_iter_loss: 0.09110236912965775
train_iter_loss: 0.21008837223052979
train_iter_loss: 0.21830156445503235
train_iter_loss: 0.19723504781723022
train_iter_loss: 0.3069906532764435
train_iter_loss: 0.19761064648628235
train_iter_loss: 0.23936152458190918
train_iter_loss: 0.16641363501548767
train_iter_loss: 0.24960608780384064
train_iter_loss: 0.1496656984090805
train_iter_loss: 0.18819725513458252
train_iter_loss: 0.11741594970226288
train_iter_loss: 0.21821065247058868
train_iter_loss: 0.23131872713565826
train_iter_loss: 0.17325180768966675
train_iter_loss: 0.35053735971450806
train_iter_loss: 0.312484472990036
train_iter_loss: 0.08756466209888458
train_iter_loss: 0.2096838802099228
train_iter_loss: 0.162255197763443
train_iter_loss: 0.1057070642709732
train_iter_loss: 0.12892214953899384
train_iter_loss: 0.18946966528892517
train_iter_loss: 0.15956343710422516
train_iter_loss: 0.18739201128482819
train_iter_loss: 0.18182368576526642
train_iter_loss: 0.1540423035621643
train_iter_loss: 0.0781455859541893
train_iter_loss: 0.1632419377565384
train_iter_loss: 0.24642324447631836
train_iter_loss: 0.15691785514354706
train_iter_loss: 0.15771280229091644
train_iter_loss: 0.24443082511425018
train_iter_loss: 0.1593344658613205
train_iter_loss: 0.17544375360012054
train_iter_loss: 0.13173194229602814
train_iter_loss: 0.1984063982963562
train_iter_loss: 0.173293337225914
train_iter_loss: 0.214396134018898
train_iter_loss: 0.08141930401325226
train_iter_loss: 0.1549535095691681
train_iter_loss: 0.1280144602060318
train_iter_loss: 0.20512020587921143
train_iter_loss: 0.1797279566526413
train_iter_loss: 0.09752575308084488
train_iter_loss: 0.12506909668445587
train_iter_loss: 0.14827270805835724
train_iter_loss: 0.11016122251749039
train_iter_loss: 0.09013666957616806
train_iter_loss: 0.1788838505744934
train_iter_loss: 0.14145472645759583
train_iter_loss: 0.17778626084327698
train_iter_loss: 0.2587912976741791
train_iter_loss: 0.14068953692913055
train_iter_loss: 0.2331106811761856
train_iter_loss: 0.2653729021549225
train_iter_loss: 0.08949413895606995
train_iter_loss: 0.20129109919071198
train_iter_loss: 0.16396258771419525
train_iter_loss: 0.1620766818523407
train_iter_loss: 0.08891048282384872
train_iter_loss: 0.09827795624732971
train_iter_loss: 0.13912132382392883
train_iter_loss: 0.14031670987606049
train_iter_loss: 0.13076376914978027
train_iter_loss: 0.2625429332256317
train_iter_loss: 0.24416761100292206
train_iter_loss: 0.1963728666305542
train_iter_loss: 0.17496688663959503
train loss :0.1721
---------------------
Validation seg loss: 0.22138669786377335 at epoch 218
epoch =    219/  1000, exp = train
train_iter_loss: 0.09876402467489243
train_iter_loss: 0.13038288056850433
train_iter_loss: 0.11886802315711975
train_iter_loss: 0.10660603642463684
train_iter_loss: 0.23423878848552704
train_iter_loss: 0.21020440757274628
train_iter_loss: 0.19338878989219666
train_iter_loss: 0.1695891171693802
train_iter_loss: 0.12462339550256729
train_iter_loss: 0.12884344160556793
train_iter_loss: 0.12135235220193863
train_iter_loss: 0.153518944978714
train_iter_loss: 0.12799645960330963
train_iter_loss: 0.08162891119718552
train_iter_loss: 0.15287452936172485
train_iter_loss: 0.215268075466156
train_iter_loss: 0.19583819806575775
train_iter_loss: 0.17650052905082703
train_iter_loss: 0.12269143015146255
train_iter_loss: 0.2684260904788971
train_iter_loss: 0.2107883095741272
train_iter_loss: 0.22865284979343414
train_iter_loss: 0.3317561149597168
train_iter_loss: 0.1562967151403427
train_iter_loss: 0.10936124622821808
train_iter_loss: 0.11213579773902893
train_iter_loss: 0.19051161408424377
train_iter_loss: 0.298927366733551
train_iter_loss: 0.15265192091464996
train_iter_loss: 0.147003173828125
train_iter_loss: 0.11376360803842545
train_iter_loss: 0.1537899225950241
train_iter_loss: 0.24010275304317474
train_iter_loss: 0.14835499227046967
train_iter_loss: 0.20380613207817078
train_iter_loss: 0.236282080411911
train_iter_loss: 0.13207250833511353
train_iter_loss: 0.10130032896995544
train_iter_loss: 0.14183416962623596
train_iter_loss: 0.13173843920230865
train_iter_loss: 0.21574607491493225
train_iter_loss: 0.08534346520900726
train_iter_loss: 0.08052581548690796
train_iter_loss: 0.23601511120796204
train_iter_loss: 0.1573207825422287
train_iter_loss: 0.12487475574016571
train_iter_loss: 0.23330457508563995
train_iter_loss: 0.2959456443786621
train_iter_loss: 0.2245388627052307
train_iter_loss: 0.1923404186964035
train_iter_loss: 0.12458038330078125
train_iter_loss: 0.15980198979377747
train_iter_loss: 0.08987222611904144
train_iter_loss: 0.20173148810863495
train_iter_loss: 0.09563365578651428
train_iter_loss: 0.10982447862625122
train_iter_loss: 0.07814539968967438
train_iter_loss: 0.22839291393756866
train_iter_loss: 0.1856144517660141
train_iter_loss: 0.1232028529047966
train_iter_loss: 0.10410304367542267
train_iter_loss: 0.12403623759746552
train_iter_loss: 0.17838621139526367
train_iter_loss: 0.06806102395057678
train_iter_loss: 0.08188452571630478
train_iter_loss: 0.22605103254318237
train_iter_loss: 0.15232013165950775
train_iter_loss: 0.1157587543129921
train_iter_loss: 0.27989140152931213
train_iter_loss: 0.17609284818172455
train_iter_loss: 0.3045223653316498
train_iter_loss: 0.1604115217924118
train_iter_loss: 0.1519373059272766
train_iter_loss: 0.0834355503320694
train_iter_loss: 0.15281176567077637
train_iter_loss: 0.1543760746717453
train_iter_loss: 0.18713553249835968
train_iter_loss: 0.16847659647464752
train_iter_loss: 0.19150854647159576
train_iter_loss: 0.2684011459350586
train_iter_loss: 0.20474718511104584
train_iter_loss: 0.23248191177845
train_iter_loss: 0.2307099848985672
train_iter_loss: 0.11915413290262222
train_iter_loss: 0.1333058625459671
train_iter_loss: 0.1137564405798912
train_iter_loss: 0.11761169880628586
train_iter_loss: 0.13987800478935242
train_iter_loss: 0.12540124356746674
train_iter_loss: 0.14924053847789764
train_iter_loss: 0.18011747300624847
train_iter_loss: 0.2152593433856964
train_iter_loss: 0.12289444357156754
train_iter_loss: 0.2966611087322235
train_iter_loss: 0.1641673445701599
train_iter_loss: 0.10277289897203445
train_iter_loss: 0.16749387979507446
train_iter_loss: 0.14426395297050476
train_iter_loss: 0.09335954487323761
train_iter_loss: 0.2751275897026062
train loss :0.1675
---------------------
Validation seg loss: 0.22192405720399516 at epoch 219
epoch =    220/  1000, exp = train
train_iter_loss: 0.3060609698295593
train_iter_loss: 0.12509213387966156
train_iter_loss: 0.11078295856714249
train_iter_loss: 0.29626548290252686
train_iter_loss: 0.23035311698913574
train_iter_loss: 0.14347867667675018
train_iter_loss: 0.2004472315311432
train_iter_loss: 0.13974115252494812
train_iter_loss: 0.2857932448387146
train_iter_loss: 0.13011430203914642
train_iter_loss: 0.06237369030714035
train_iter_loss: 0.18142001330852509
train_iter_loss: 0.18624398112297058
train_iter_loss: 0.23564985394477844
train_iter_loss: 0.16551341116428375
train_iter_loss: 0.18000063300132751
train_iter_loss: 0.29250872135162354
train_iter_loss: 0.13171514868736267
train_iter_loss: 0.14322561025619507
train_iter_loss: 0.14089663326740265
train_iter_loss: 0.1644284576177597
train_iter_loss: 0.17077060043811798
train_iter_loss: 0.13175448775291443
train_iter_loss: 0.08174735307693481
train_iter_loss: 0.15803034603595734
train_iter_loss: 0.14546847343444824
train_iter_loss: 0.08937059342861176
train_iter_loss: 0.21915286779403687
train_iter_loss: 0.2238239198923111
train_iter_loss: 0.11539635807275772
train_iter_loss: 0.18047212064266205
train_iter_loss: 0.1957654058933258
train_iter_loss: 0.12259090691804886
train_iter_loss: 0.14796525239944458
train_iter_loss: 0.13230939209461212
train_iter_loss: 0.13087014853954315
train_iter_loss: 0.0942784771323204
train_iter_loss: 0.15492789447307587
train_iter_loss: 0.2972384989261627
train_iter_loss: 0.2234560251235962
train_iter_loss: 0.17813096940517426
train_iter_loss: 0.19874437153339386
train_iter_loss: 0.16109153628349304
train_iter_loss: 0.12345048785209656
train_iter_loss: 0.35527947545051575
train_iter_loss: 0.1338815838098526
train_iter_loss: 0.2186732143163681
train_iter_loss: 0.089023657143116
train_iter_loss: 0.13224877417087555
train_iter_loss: 0.12492648512125015
train_iter_loss: 0.19563797116279602
train_iter_loss: 0.13223956525325775
train_iter_loss: 0.10059501230716705
train_iter_loss: 0.229413241147995
train_iter_loss: 0.1510835438966751
train_iter_loss: 0.25778380036354065
train_iter_loss: 0.1889651119709015
train_iter_loss: 0.11555102467536926
train_iter_loss: 0.1341085284948349
train_iter_loss: 0.11648884415626526
train_iter_loss: 0.13411353528499603
train_iter_loss: 0.1298350691795349
train_iter_loss: 0.23231010138988495
train_iter_loss: 0.13259176909923553
train_iter_loss: 0.21449460089206696
train_iter_loss: 0.06052885949611664
train_iter_loss: 0.1945667713880539
train_iter_loss: 0.1757538765668869
train_iter_loss: 0.14802922308444977
train_iter_loss: 0.17546625435352325
train_iter_loss: 0.12283925712108612
train_iter_loss: 0.14982746541500092
train_iter_loss: 0.18009614944458008
train_iter_loss: 0.2929491698741913
train_iter_loss: 0.21767233312129974
train_iter_loss: 0.24280403554439545
train_iter_loss: 0.0873062014579773
train_iter_loss: 0.2844502925872803
train_iter_loss: 0.16081970930099487
train_iter_loss: 0.25931212306022644
train_iter_loss: 0.15610909461975098
train_iter_loss: 0.17164352536201477
train_iter_loss: 0.20451268553733826
train_iter_loss: 0.1376902312040329
train_iter_loss: 0.1621483564376831
train_iter_loss: 0.1675816923379898
train_iter_loss: 0.15707986056804657
train_iter_loss: 0.11383046954870224
train_iter_loss: 0.19183433055877686
train_iter_loss: 0.1350819319486618
train_iter_loss: 0.10394353419542313
train_iter_loss: 0.17036986351013184
train_iter_loss: 0.2676715850830078
train_iter_loss: 0.1588042974472046
train_iter_loss: 0.11536365747451782
train_iter_loss: 0.16033804416656494
train_iter_loss: 0.08782903105020523
train_iter_loss: 0.2143840789794922
train_iter_loss: 0.1251828819513321
train_iter_loss: 0.17442047595977783
train loss :0.1714
---------------------
Validation seg loss: 0.21925262149142208 at epoch 220
********************
best_val_epoch_loss:  0.21925262149142208
MODEL UPDATED
epoch =    221/  1000, exp = train
train_iter_loss: 0.1332528442144394
train_iter_loss: 0.11990852653980255
train_iter_loss: 0.19395236670970917
train_iter_loss: 0.18971438705921173
train_iter_loss: 0.25492429733276367
train_iter_loss: 0.30302751064300537
train_iter_loss: 0.10078998655080795
train_iter_loss: 0.21084867417812347
train_iter_loss: 0.16199608147144318
train_iter_loss: 0.10403238981962204
train_iter_loss: 0.1589995175600052
train_iter_loss: 0.2180015742778778
train_iter_loss: 0.17071452736854553
train_iter_loss: 0.17603400349617004
train_iter_loss: 0.18723197281360626
train_iter_loss: 0.19524510204792023
train_iter_loss: 0.1273956000804901
train_iter_loss: 0.2908514738082886
train_iter_loss: 0.21316468715667725
train_iter_loss: 0.17585498094558716
train_iter_loss: 0.17623059451580048
train_iter_loss: 0.19130511581897736
train_iter_loss: 0.12725849449634552
train_iter_loss: 0.11433213204145432
train_iter_loss: 0.23536545038223267
train_iter_loss: 0.2643519937992096
train_iter_loss: 0.21029794216156006
train_iter_loss: 0.1534741222858429
train_iter_loss: 0.08210665732622147
train_iter_loss: 0.1531493067741394
train_iter_loss: 0.16129180788993835
train_iter_loss: 0.16026242077350616
train_iter_loss: 0.20715659856796265
train_iter_loss: 0.22826530039310455
train_iter_loss: 0.20414237678050995
train_iter_loss: 0.24642769992351532
train_iter_loss: 0.12611940503120422
train_iter_loss: 0.13418763875961304
train_iter_loss: 0.19191569089889526
train_iter_loss: 0.2393600046634674
train_iter_loss: 0.12448310106992722
train_iter_loss: 0.08088802546262741
train_iter_loss: 0.1632593870162964
train_iter_loss: 0.19096454977989197
train_iter_loss: 0.1934128850698471
train_iter_loss: 0.10707389563322067
train_iter_loss: 0.22452621161937714
train_iter_loss: 0.3361031711101532
train_iter_loss: 0.1875590831041336
train_iter_loss: 0.09943525493144989
train_iter_loss: 0.1334015280008316
train_iter_loss: 0.4194839894771576
train_iter_loss: 0.3015856444835663
train_iter_loss: 0.09993899613618851
train_iter_loss: 0.22480955719947815
train_iter_loss: 0.34761327505111694
train_iter_loss: 0.16330468654632568
train_iter_loss: 0.2417348474264145
train_iter_loss: 0.13799376785755157
train_iter_loss: 0.13019154965877533
train_iter_loss: 0.24495691061019897
train_iter_loss: 0.09222089499235153
train_iter_loss: 0.2917160093784332
train_iter_loss: 0.21631905436515808
train_iter_loss: 0.11508123576641083
train_iter_loss: 0.20791827142238617
train_iter_loss: 0.14718304574489594
train_iter_loss: 0.079258032143116
train_iter_loss: 0.21961632370948792
train_iter_loss: 0.07244259119033813
train_iter_loss: 0.06476261466741562
train_iter_loss: 0.15829497575759888
train_iter_loss: 0.15003137290477753
train_iter_loss: 0.11287259310483932
train_iter_loss: 0.1248248890042305
train_iter_loss: 0.13787156343460083
train_iter_loss: 0.18707488477230072
train_iter_loss: 0.20017236471176147
train_iter_loss: 0.07059842348098755
train_iter_loss: 0.20031733810901642
train_iter_loss: 0.1262374371290207
train_iter_loss: 0.12837298214435577
train_iter_loss: 0.2274605929851532
train_iter_loss: 0.04288038611412048
train_iter_loss: 0.20033226907253265
train_iter_loss: 0.1384575068950653
train_iter_loss: 0.16390076279640198
train_iter_loss: 0.10791371762752533
train_iter_loss: 0.14824353158473969
train_iter_loss: 0.11451204866170883
train_iter_loss: 0.23748424649238586
train_iter_loss: 0.10846268385648727
train_iter_loss: 0.175912544131279
train_iter_loss: 0.10890551656484604
train_iter_loss: 0.22592751681804657
train_iter_loss: 0.16427890956401825
train_iter_loss: 0.1733425259590149
train_iter_loss: 0.033900536596775055
train_iter_loss: 0.13970570266246796
train_iter_loss: 0.11074507981538773
train loss :0.1734
---------------------
Validation seg loss: 0.2233919722186226 at epoch 221
epoch =    222/  1000, exp = train
train_iter_loss: 0.1411796659231186
train_iter_loss: 0.15434630215168
train_iter_loss: 0.06295221298933029
train_iter_loss: 0.07887107878923416
train_iter_loss: 0.11135995388031006
train_iter_loss: 0.18546362221240997
train_iter_loss: 0.12532803416252136
train_iter_loss: 0.18073660135269165
train_iter_loss: 0.18183289468288422
train_iter_loss: 0.23325583338737488
train_iter_loss: 0.13196586072444916
train_iter_loss: 0.28386759757995605
train_iter_loss: 0.11541631817817688
train_iter_loss: 0.09788202494382858
train_iter_loss: 0.2037588357925415
train_iter_loss: 0.16657429933547974
train_iter_loss: 0.18748503923416138
train_iter_loss: 0.2856670618057251
train_iter_loss: 0.21644152700901031
train_iter_loss: 0.13399188220500946
train_iter_loss: 0.23232172429561615
train_iter_loss: 0.13866765797138214
train_iter_loss: 0.16661494970321655
train_iter_loss: 0.1015850380063057
train_iter_loss: 0.18471591174602509
train_iter_loss: 0.17446111142635345
train_iter_loss: 0.19011008739471436
train_iter_loss: 0.12851685285568237
train_iter_loss: 0.18398024141788483
train_iter_loss: 0.1565205305814743
train_iter_loss: 0.14979569613933563
train_iter_loss: 0.22273652255535126
train_iter_loss: 0.13256396353244781
train_iter_loss: 0.10222617536783218
train_iter_loss: 0.14311273396015167
train_iter_loss: 0.12092076241970062
train_iter_loss: 0.18540875613689423
train_iter_loss: 0.22522005438804626
train_iter_loss: 0.1815406084060669
train_iter_loss: 0.1519617736339569
train_iter_loss: 0.13589142262935638
train_iter_loss: 0.19023102521896362
train_iter_loss: 0.173291876912117
train_iter_loss: 0.16832780838012695
train_iter_loss: 0.06830033659934998
train_iter_loss: 0.20191152393817902
train_iter_loss: 0.25959309935569763
train_iter_loss: 0.10165112465620041
train_iter_loss: 0.15862925350666046
train_iter_loss: 0.20705582201480865
train_iter_loss: 0.1823401004076004
train_iter_loss: 0.07486831396818161
train_iter_loss: 0.23503044247627258
train_iter_loss: 0.16063208878040314
train_iter_loss: 0.2584056556224823
train_iter_loss: 0.1591627150774002
train_iter_loss: 0.09866074472665787
train_iter_loss: 0.11607430130243301
train_iter_loss: 0.1577417105436325
train_iter_loss: 0.14929284155368805
train_iter_loss: 0.13439667224884033
train_iter_loss: 0.13647723197937012
train_iter_loss: 0.18163016438484192
train_iter_loss: 0.202583447098732
train_iter_loss: 0.22650514543056488
train_iter_loss: 0.11995052546262741
train_iter_loss: 0.08279372751712799
train_iter_loss: 0.18901190161705017
train_iter_loss: 0.1345934271812439
train_iter_loss: 0.14668084681034088
train_iter_loss: 0.12827424705028534
train_iter_loss: 0.17350314557552338
train_iter_loss: 0.1780003309249878
train_iter_loss: 0.0914151594042778
train_iter_loss: 0.24852615594863892
train_iter_loss: 0.12976714968681335
train_iter_loss: 0.1123833954334259
train_iter_loss: 0.1546328365802765
train_iter_loss: 0.15052317082881927
train_iter_loss: 0.07142246514558792
train_iter_loss: 0.20437999069690704
train_iter_loss: 0.20340971648693085
train_iter_loss: 0.11655791848897934
train_iter_loss: 0.21650511026382446
train_iter_loss: 0.15216964483261108
train_iter_loss: 0.1865333765745163
train_iter_loss: 0.10527852922677994
train_iter_loss: 0.1670946329832077
train_iter_loss: 0.12556767463684082
train_iter_loss: 0.16893672943115234
train_iter_loss: 0.17926958203315735
train_iter_loss: 0.21044060587882996
train_iter_loss: 0.1948266625404358
train_iter_loss: 0.24279774725437164
train_iter_loss: 0.15334981679916382
train_iter_loss: 0.14359699189662933
train_iter_loss: 0.371626615524292
train_iter_loss: 0.0891999751329422
train_iter_loss: 0.10716375708580017
train_iter_loss: 0.11041474342346191
train loss :0.1642
---------------------
Validation seg loss: 0.22576954735021265 at epoch 222
epoch =    223/  1000, exp = train
train_iter_loss: 0.1108197495341301
train_iter_loss: 0.29379770159721375
train_iter_loss: 0.17085000872612
train_iter_loss: 0.13457733392715454
train_iter_loss: 0.2979322075843811
train_iter_loss: 0.16582435369491577
train_iter_loss: 0.09534186124801636
train_iter_loss: 0.038548316806554794
train_iter_loss: 0.06833706796169281
train_iter_loss: 0.15061384439468384
train_iter_loss: 0.1337728202342987
train_iter_loss: 0.14940787851810455
train_iter_loss: 0.21738013625144958
train_iter_loss: 0.1434512585401535
train_iter_loss: 0.13832460343837738
train_iter_loss: 0.18901477754116058
train_iter_loss: 0.19777065515518188
train_iter_loss: 0.10861311107873917
train_iter_loss: 0.122108593583107
train_iter_loss: 0.08439692854881287
train_iter_loss: 0.1785123497247696
train_iter_loss: 0.1089610904455185
train_iter_loss: 0.1823955774307251
train_iter_loss: 0.1009979173541069
train_iter_loss: 0.17362430691719055
train_iter_loss: 0.1260366439819336
train_iter_loss: 0.11433851718902588
train_iter_loss: 0.19432081282138824
train_iter_loss: 0.06608240306377411
train_iter_loss: 0.1344226747751236
train_iter_loss: 0.25108057260513306
train_iter_loss: 0.18470244109630585
train_iter_loss: 0.09961830079555511
train_iter_loss: 0.38906335830688477
train_iter_loss: 0.20924144983291626
train_iter_loss: 0.12584887444972992
train_iter_loss: 0.19067999720573425
train_iter_loss: 0.15070506930351257
train_iter_loss: 0.152340829372406
train_iter_loss: 0.174668550491333
train_iter_loss: 0.16890332102775574
train_iter_loss: 0.1862645000219345
train_iter_loss: 0.14137883484363556
train_iter_loss: 0.049403589218854904
train_iter_loss: 0.2294832020998001
train_iter_loss: 0.2333703637123108
train_iter_loss: 0.1457323431968689
train_iter_loss: 0.18699391186237335
train_iter_loss: 0.11785776168107986
train_iter_loss: 0.17174847424030304
train_iter_loss: 0.18565356731414795
train_iter_loss: 0.28193652629852295
train_iter_loss: 0.17212969064712524
train_iter_loss: 0.14077210426330566
train_iter_loss: 0.1391279399394989
train_iter_loss: 0.2169884294271469
train_iter_loss: 0.1398683339357376
train_iter_loss: 0.21873678267002106
train_iter_loss: 0.19317053258419037
train_iter_loss: 0.3477048873901367
train_iter_loss: 0.05775643140077591
train_iter_loss: 0.2731849253177643
train_iter_loss: 0.21411140263080597
train_iter_loss: 0.24556539952754974
train_iter_loss: 0.08546479791402817
train_iter_loss: 0.2240128368139267
train_iter_loss: 0.249468594789505
train_iter_loss: 0.21686485409736633
train_iter_loss: 0.21082624793052673
train_iter_loss: 0.15704931318759918
train_iter_loss: 0.1970299631357193
train_iter_loss: 0.10741393268108368
train_iter_loss: 0.30269891023635864
train_iter_loss: 0.1409122347831726
train_iter_loss: 0.14959841966629028
train_iter_loss: 0.14194731414318085
train_iter_loss: 0.12396766245365143
train_iter_loss: 0.16400404274463654
train_iter_loss: 0.07715602219104767
train_iter_loss: 0.08480492979288101
train_iter_loss: 0.201504185795784
train_iter_loss: 0.17832113802433014
train_iter_loss: 0.16366982460021973
train_iter_loss: 0.24789486825466156
train_iter_loss: 0.11376921832561493
train_iter_loss: 0.17141155898571014
train_iter_loss: 0.13973437249660492
train_iter_loss: 0.14355608820915222
train_iter_loss: 0.15634937584400177
train_iter_loss: 0.20009228587150574
train_iter_loss: 0.17131415009498596
train_iter_loss: 0.09669169783592224
train_iter_loss: 0.17230342328548431
train_iter_loss: 0.1184573695063591
train_iter_loss: 0.14160755276679993
train_iter_loss: 0.21252405643463135
train_iter_loss: 0.18253611028194427
train_iter_loss: 0.1928560584783554
train_iter_loss: 0.16848917305469513
train_iter_loss: 0.307803213596344
train loss :0.1705
---------------------
Validation seg loss: 0.22013570771570196 at epoch 223
epoch =    224/  1000, exp = train
train_iter_loss: 0.18999819457530975
train_iter_loss: 0.19175590574741364
train_iter_loss: 0.2390156090259552
train_iter_loss: 0.14607422053813934
train_iter_loss: 0.14887019991874695
train_iter_loss: 0.14795774221420288
train_iter_loss: 0.2968393564224243
train_iter_loss: 0.20972400903701782
train_iter_loss: 0.09371351450681686
train_iter_loss: 0.09122022241353989
train_iter_loss: 0.1619013249874115
train_iter_loss: 0.10811874270439148
train_iter_loss: 0.21306940913200378
train_iter_loss: 0.04936985671520233
train_iter_loss: 0.15931659936904907
train_iter_loss: 0.16808156669139862
train_iter_loss: 0.11750634759664536
train_iter_loss: 0.28798675537109375
train_iter_loss: 0.13809068500995636
train_iter_loss: 0.1760818362236023
train_iter_loss: 0.13269729912281036
train_iter_loss: 0.35922905802726746
train_iter_loss: 0.13949593901634216
train_iter_loss: 0.12222185730934143
train_iter_loss: 0.08585719764232635
train_iter_loss: 0.15120691061019897
train_iter_loss: 0.2142292708158493
train_iter_loss: 0.2456221729516983
train_iter_loss: 0.34986934065818787
train_iter_loss: 0.20655640959739685
train_iter_loss: 0.15162569284439087
train_iter_loss: 0.16200010478496552
train_iter_loss: 0.10478222370147705
train_iter_loss: 0.06581881642341614
train_iter_loss: 0.24972352385520935
train_iter_loss: 0.230349600315094
train_iter_loss: 0.17871958017349243
train_iter_loss: 0.0958709865808487
train_iter_loss: 0.16218192875385284
train_iter_loss: 0.2911592423915863
train_iter_loss: 0.22005780041217804
train_iter_loss: 0.05743814632296562
train_iter_loss: 0.13738009333610535
train_iter_loss: 0.19635950028896332
train_iter_loss: 0.31883159279823303
train_iter_loss: 0.3677239418029785
train_iter_loss: 0.16872501373291016
train_iter_loss: 0.29444020986557007
train_iter_loss: 0.10204657167196274
train_iter_loss: 0.18931299448013306
train_iter_loss: 0.2136741429567337
train_iter_loss: 0.2019742727279663
train_iter_loss: 0.1330207735300064
train_iter_loss: 0.1714034229516983
train_iter_loss: 0.1810540407896042
train_iter_loss: 0.07483910769224167
train_iter_loss: 0.06212759390473366
train_iter_loss: 0.2034139335155487
train_iter_loss: 0.15211334824562073
train_iter_loss: 0.19897422194480896
train_iter_loss: 0.16130921244621277
train_iter_loss: 0.0963289812207222
train_iter_loss: 0.11180958151817322
train_iter_loss: 0.10422944277524948
train_iter_loss: 0.08216413110494614
train_iter_loss: 0.09994654357433319
train_iter_loss: 0.14172372221946716
train_iter_loss: 0.1021081954240799
train_iter_loss: 0.12118329107761383
train_iter_loss: 0.2288297861814499
train_iter_loss: 0.15495973825454712
train_iter_loss: 0.046929825097322464
train_iter_loss: 0.12169624865055084
train_iter_loss: 0.3304123878479004
train_iter_loss: 0.1634729653596878
train_iter_loss: 0.18148626387119293
train_iter_loss: 0.2499821037054062
train_iter_loss: 0.20466259121894836
train_iter_loss: 0.0788864940404892
train_iter_loss: 0.09589995443820953
train_iter_loss: 0.06492207199335098
train_iter_loss: 0.1616014987230301
train_iter_loss: 0.11983520537614822
train_iter_loss: 0.142573282122612
train_iter_loss: 0.22701112926006317
train_iter_loss: 0.22735297679901123
train_iter_loss: 0.22915039956569672
train_iter_loss: 0.13949957489967346
train_iter_loss: 0.14393718540668488
train_iter_loss: 0.24537228047847748
train_iter_loss: 0.18506065011024475
train_iter_loss: 0.12819162011146545
train_iter_loss: 0.19069838523864746
train_iter_loss: 0.2043054848909378
train_iter_loss: 0.09767243266105652
train_iter_loss: 0.11394622921943665
train_iter_loss: 0.17347198724746704
train_iter_loss: 0.20386743545532227
train_iter_loss: 0.22965282201766968
train_iter_loss: 0.20942503213882446
train loss :0.1716
---------------------
Validation seg loss: 0.22491920955549433 at epoch 224
epoch =    225/  1000, exp = train
train_iter_loss: 0.12481015920639038
train_iter_loss: 0.08732163161039352
train_iter_loss: 0.17944085597991943
train_iter_loss: 0.06741614639759064
train_iter_loss: 0.13505540788173676
train_iter_loss: 0.19969548285007477
train_iter_loss: 0.15550780296325684
train_iter_loss: 0.11391598731279373
train_iter_loss: 0.10763763636350632
train_iter_loss: 0.17412205040454865
train_iter_loss: 0.10015659779310226
train_iter_loss: 0.16105704009532928
train_iter_loss: 0.16851544380187988
train_iter_loss: 0.25832125544548035
train_iter_loss: 0.24406324326992035
train_iter_loss: 0.27801191806793213
train_iter_loss: 0.129725843667984
train_iter_loss: 0.11083818972110748
train_iter_loss: 0.07628302276134491
train_iter_loss: 0.2000310868024826
train_iter_loss: 0.2100091427564621
train_iter_loss: 0.18770940601825714
train_iter_loss: 0.2114514857530594
train_iter_loss: 0.20725111663341522
train_iter_loss: 0.2134343683719635
train_iter_loss: 0.18455937504768372
train_iter_loss: 0.1567682921886444
train_iter_loss: 0.21403121948242188
train_iter_loss: 0.2785358726978302
train_iter_loss: 0.2624928951263428
train_iter_loss: 0.1044984981417656
train_iter_loss: 0.2579461336135864
train_iter_loss: 0.24223245680332184
train_iter_loss: 0.18180713057518005
train_iter_loss: 0.1580517441034317
train_iter_loss: 0.11277461796998978
train_iter_loss: 0.14244593679904938
train_iter_loss: 0.13277433812618256
train_iter_loss: 0.12980177998542786
train_iter_loss: 0.0711611658334732
train_iter_loss: 0.2607625424861908
train_iter_loss: 0.18106718361377716
train_iter_loss: 0.23052865266799927
train_iter_loss: 0.17298370599746704
train_iter_loss: 0.23395511507987976
train_iter_loss: 0.1736578494310379
train_iter_loss: 0.15916965901851654
train_iter_loss: 0.28484031558036804
train_iter_loss: 0.1619003266096115
train_iter_loss: 0.07923384010791779
train_iter_loss: 0.16817036271095276
train_iter_loss: 0.12713143229484558
train_iter_loss: 0.19091954827308655
train_iter_loss: 0.24340620636940002
train_iter_loss: 0.25832536816596985
train_iter_loss: 0.2680259943008423
train_iter_loss: 0.19907881319522858
train_iter_loss: 0.16948850452899933
train_iter_loss: 0.14596283435821533
train_iter_loss: 0.10162711888551712
train_iter_loss: 0.2013058364391327
train_iter_loss: 0.13651832938194275
train_iter_loss: 0.12507633864879608
train_iter_loss: 0.15362435579299927
train_iter_loss: 0.15139362215995789
train_iter_loss: 0.0992111936211586
train_iter_loss: 0.15852876007556915
train_iter_loss: 0.25902676582336426
train_iter_loss: 0.09224230796098709
train_iter_loss: 0.20478127896785736
train_iter_loss: 0.1686767041683197
train_iter_loss: 0.1525411605834961
train_iter_loss: 0.3626205325126648
train_iter_loss: 0.18556125462055206
train_iter_loss: 0.14187896251678467
train_iter_loss: 0.10267423838376999
train_iter_loss: 0.04331699386239052
train_iter_loss: 0.0892276018857956
train_iter_loss: 0.06142958253622055
train_iter_loss: 0.1505560427904129
train_iter_loss: 0.11340653896331787
train_iter_loss: 0.3785656690597534
train_iter_loss: 0.15327247977256775
train_iter_loss: 0.1702837198972702
train_iter_loss: 0.14391851425170898
train_iter_loss: 0.135409414768219
train_iter_loss: 0.3930402100086212
train_iter_loss: 0.22787271440029144
train_iter_loss: 0.16720809042453766
train_iter_loss: 0.09840898960828781
train_iter_loss: 0.19357283413410187
train_iter_loss: 0.1859322339296341
train_iter_loss: 0.14669187366962433
train_iter_loss: 0.14552076160907745
train_iter_loss: 0.18763142824172974
train_iter_loss: 0.06270959228277206
train_iter_loss: 0.16917923092842102
train_iter_loss: 0.14205703139305115
train_iter_loss: 0.1928762048482895
train_iter_loss: 0.18001149594783783
train loss :0.1734
---------------------
Validation seg loss: 0.22646591489045126 at epoch 225
epoch =    226/  1000, exp = train
train_iter_loss: 0.2511008381843567
train_iter_loss: 0.2253270298242569
train_iter_loss: 0.18082770705223083
train_iter_loss: 0.18183721601963043
train_iter_loss: 0.13727544248104095
train_iter_loss: 0.14467234909534454
train_iter_loss: 0.23481696844100952
train_iter_loss: 0.1887543499469757
train_iter_loss: 0.20842298865318298
train_iter_loss: 0.20694765448570251
train_iter_loss: 0.1627625972032547
train_iter_loss: 0.15516109764575958
train_iter_loss: 0.2030920535326004
train_iter_loss: 0.1949765682220459
train_iter_loss: 0.24929897487163544
train_iter_loss: 0.15563373267650604
train_iter_loss: 0.0897136777639389
train_iter_loss: 0.1411762237548828
train_iter_loss: 0.10456070303916931
train_iter_loss: 0.07220471650362015
train_iter_loss: 0.2638770043849945
train_iter_loss: 0.1566089391708374
train_iter_loss: 0.21631094813346863
train_iter_loss: 0.1682756394147873
train_iter_loss: 0.05949214845895767
train_iter_loss: 0.10849403589963913
train_iter_loss: 0.0771028995513916
train_iter_loss: 0.1795031577348709
train_iter_loss: 0.1684066206216812
train_iter_loss: 0.14046311378479004
train_iter_loss: 0.17799128592014313
train_iter_loss: 0.1684424728155136
train_iter_loss: 0.19563987851142883
train_iter_loss: 0.09422270208597183
train_iter_loss: 0.11439826339483261
train_iter_loss: 0.18719260394573212
train_iter_loss: 0.08669694513082504
train_iter_loss: 0.201117143034935
train_iter_loss: 0.24579699337482452
train_iter_loss: 0.2634163498878479
train_iter_loss: 0.17354631423950195
train_iter_loss: 0.17781981825828552
train_iter_loss: 0.13209523260593414
train_iter_loss: 0.1987227350473404
train_iter_loss: 0.1611611396074295
train_iter_loss: 0.14768458902835846
train_iter_loss: 0.11134389787912369
train_iter_loss: 0.14593853056430817
train_iter_loss: 0.24238477647304535
train_iter_loss: 0.20988218486309052
train_iter_loss: 0.31023770570755005
train_iter_loss: 0.23748250305652618
train_iter_loss: 0.1266624927520752
train_iter_loss: 0.1449504941701889
train_iter_loss: 0.14050020277500153
train_iter_loss: 0.20350392162799835
train_iter_loss: 0.085276298224926
train_iter_loss: 0.17891918122768402
train_iter_loss: 0.12502466142177582
train_iter_loss: 0.1412363350391388
train_iter_loss: 0.13706131279468536
train_iter_loss: 0.11496751755475998
train_iter_loss: 0.09403429925441742
train_iter_loss: 0.05170382559299469
train_iter_loss: 0.16386647522449493
train_iter_loss: 0.12990902364253998
train_iter_loss: 0.1415081024169922
train_iter_loss: 0.14774242043495178
train_iter_loss: 0.1421387493610382
train_iter_loss: 0.31707853078842163
train_iter_loss: 0.20158199965953827
train_iter_loss: 0.19948215782642365
train_iter_loss: 0.16095855832099915
train_iter_loss: 0.22575227916240692
train_iter_loss: 0.17224152386188507
train_iter_loss: 0.15140359103679657
train_iter_loss: 0.180757537484169
train_iter_loss: 0.15430694818496704
train_iter_loss: 0.19750761985778809
train_iter_loss: 0.1743273138999939
train_iter_loss: 0.09384163469076157
train_iter_loss: 0.2407822161912918
train_iter_loss: 0.1583937555551529
train_iter_loss: 0.16538076102733612
train_iter_loss: 0.17381764948368073
train_iter_loss: 0.3177315890789032
train_iter_loss: 0.18065489828586578
train_iter_loss: 0.2622678577899933
train_iter_loss: 0.15480737388134003
train_iter_loss: 0.1597091555595398
train_iter_loss: 0.09078418463468552
train_iter_loss: 0.17889675498008728
train_iter_loss: 0.18430206179618835
train_iter_loss: 0.24999120831489563
train_iter_loss: 0.22277605533599854
train_iter_loss: 0.10751580446958542
train_iter_loss: 0.20594026148319244
train_iter_loss: 0.14178554713726044
train_iter_loss: 0.2117481529712677
train_iter_loss: 0.12554244697093964
train loss :0.1721
---------------------
Validation seg loss: 0.2227609946726347 at epoch 226
epoch =    227/  1000, exp = train
train_iter_loss: 0.16733810305595398
train_iter_loss: 0.20449034869670868
train_iter_loss: 0.1609429270029068
train_iter_loss: 0.210384339094162
train_iter_loss: 0.16972169280052185
train_iter_loss: 0.14037881791591644
train_iter_loss: 0.13006484508514404
train_iter_loss: 0.08155090361833572
train_iter_loss: 0.23391340672969818
train_iter_loss: 0.2768176794052124
train_iter_loss: 0.14625480771064758
train_iter_loss: 0.16810663044452667
train_iter_loss: 0.23858365416526794
train_iter_loss: 0.06867621839046478
train_iter_loss: 0.16106514632701874
train_iter_loss: 0.1855040341615677
train_iter_loss: 0.17873696982860565
train_iter_loss: 0.12500599026679993
train_iter_loss: 0.20610590279102325
train_iter_loss: 0.12868641316890717
train_iter_loss: 0.2267839014530182
train_iter_loss: 0.20213435590267181
train_iter_loss: 0.19987808167934418
train_iter_loss: 0.21716168522834778
train_iter_loss: 0.19137312471866608
train_iter_loss: 0.15712347626686096
train_iter_loss: 0.1879035234451294
train_iter_loss: 0.21274298429489136
train_iter_loss: 0.1501270830631256
train_iter_loss: 0.2007855623960495
train_iter_loss: 0.23227405548095703
train_iter_loss: 0.22263804078102112
train_iter_loss: 0.07307422161102295
train_iter_loss: 0.10909949988126755
train_iter_loss: 0.1246364638209343
train_iter_loss: 0.06427840143442154
train_iter_loss: 0.2374274879693985
train_iter_loss: 0.10675439983606339
train_iter_loss: 0.1522826850414276
train_iter_loss: 0.07932517677545547
train_iter_loss: 0.15339122712612152
train_iter_loss: 0.10763834416866302
train_iter_loss: 0.10445350408554077
train_iter_loss: 0.15253004431724548
train_iter_loss: 0.22102662920951843
train_iter_loss: 0.15340211987495422
train_iter_loss: 0.20284481346607208
train_iter_loss: 0.14343786239624023
train_iter_loss: 0.10598311573266983
train_iter_loss: 0.234734445810318
train_iter_loss: 0.12305518984794617
train_iter_loss: 0.11351395398378372
train_iter_loss: 0.2054636925458908
train_iter_loss: 0.19962969422340393
train_iter_loss: 0.2166680097579956
train_iter_loss: 0.30338579416275024
train_iter_loss: 0.18729422986507416
train_iter_loss: 0.12515288591384888
train_iter_loss: 0.11293388158082962
train_iter_loss: 0.17118018865585327
train_iter_loss: 0.16559229791164398
train_iter_loss: 0.2241230010986328
train_iter_loss: 0.17509043216705322
train_iter_loss: 0.24414893984794617
train_iter_loss: 0.18826335668563843
train_iter_loss: 0.19508281350135803
train_iter_loss: 0.1712246686220169
train_iter_loss: 0.16208131611347198
train_iter_loss: 0.12087233364582062
train_iter_loss: 0.08333500474691391
train_iter_loss: 0.16273799538612366
train_iter_loss: 0.1412382423877716
train_iter_loss: 0.07161618024110794
train_iter_loss: 0.11807072162628174
train_iter_loss: 0.26741868257522583
train_iter_loss: 0.12075472623109818
train_iter_loss: 0.1059354841709137
train_iter_loss: 0.433476984500885
train_iter_loss: 0.1295437216758728
train_iter_loss: 0.21742042899131775
train_iter_loss: 0.1415351778268814
train_iter_loss: 0.17125150561332703
train_iter_loss: 0.10445158928632736
train_iter_loss: 0.3085443377494812
train_iter_loss: 0.1332414150238037
train_iter_loss: 0.21743468940258026
train_iter_loss: 0.11752209812402725
train_iter_loss: 0.16327232122421265
train_iter_loss: 0.17272989451885223
train_iter_loss: 0.13208146393299103
train_iter_loss: 0.08561039716005325
train_iter_loss: 0.17818349599838257
train_iter_loss: 0.14033572375774384
train_iter_loss: 0.1108425036072731
train_iter_loss: 0.2076990157365799
train_iter_loss: 0.1652979999780655
train_iter_loss: 0.10965942591428757
train_iter_loss: 0.10265113413333893
train_iter_loss: 0.33673760294914246
train_iter_loss: 0.17159897089004517
train loss :0.1701
---------------------
Validation seg loss: 0.21969087614308833 at epoch 227
epoch =    228/  1000, exp = train
train_iter_loss: 0.16694526374340057
train_iter_loss: 0.0872531607747078
train_iter_loss: 0.17289836704730988
train_iter_loss: 0.2707521319389343
train_iter_loss: 0.13058067858219147
train_iter_loss: 0.2869282364845276
train_iter_loss: 0.3047774136066437
train_iter_loss: 0.13518975675106049
train_iter_loss: 0.09314624965190887
train_iter_loss: 0.1419771909713745
train_iter_loss: 0.19206000864505768
train_iter_loss: 0.1263040006160736
train_iter_loss: 0.16197946667671204
train_iter_loss: 0.14548823237419128
train_iter_loss: 0.1110566258430481
train_iter_loss: 0.20561698079109192
train_iter_loss: 0.14613012969493866
train_iter_loss: 0.20743471384048462
train_iter_loss: 0.24621936678886414
train_iter_loss: 0.1476260870695114
train_iter_loss: 0.14648641645908356
train_iter_loss: 0.09432652592658997
train_iter_loss: 0.20345668494701385
train_iter_loss: 0.3270922005176544
train_iter_loss: 0.1526283174753189
train_iter_loss: 0.21269308030605316
train_iter_loss: 0.23146912455558777
train_iter_loss: 0.2443562150001526
train_iter_loss: 0.18066763877868652
train_iter_loss: 0.16346438229084015
train_iter_loss: 0.19491451978683472
train_iter_loss: 0.16320303082466125
train_iter_loss: 0.22574876248836517
train_iter_loss: 0.13610827922821045
train_iter_loss: 0.11640866100788116
train_iter_loss: 0.2251192331314087
train_iter_loss: 0.09674116224050522
train_iter_loss: 0.11830101162195206
train_iter_loss: 0.1598772406578064
train_iter_loss: 0.15642699599266052
train_iter_loss: 0.08831752836704254
train_iter_loss: 0.1350252628326416
train_iter_loss: 0.1299397200345993
train_iter_loss: 0.3115089535713196
train_iter_loss: 0.1721220314502716
train_iter_loss: 0.1726740449666977
train_iter_loss: 0.14554724097251892
train_iter_loss: 0.16008639335632324
train_iter_loss: 0.2009773999452591
train_iter_loss: 0.09059912711381912
train_iter_loss: 0.12074977159500122
train_iter_loss: 0.34976422786712646
train_iter_loss: 0.1416875123977661
train_iter_loss: 0.15416568517684937
train_iter_loss: 0.12817643582820892
train_iter_loss: 0.10903030633926392
train_iter_loss: 0.07329411804676056
train_iter_loss: 0.06741204112768173
train_iter_loss: 0.08097673207521439
train_iter_loss: 0.37660929560661316
train_iter_loss: 0.28405842185020447
train_iter_loss: 0.26743167638778687
train_iter_loss: 0.08462037891149521
train_iter_loss: 0.10909420251846313
train_iter_loss: 0.1900070309638977
train_iter_loss: 0.2385154813528061
train_iter_loss: 0.15028832852840424
train_iter_loss: 0.21345797181129456
train_iter_loss: 0.20649045705795288
train_iter_loss: 0.219105064868927
train_iter_loss: 0.14128658175468445
train_iter_loss: 0.20458073914051056
train_iter_loss: 0.15200576186180115
train_iter_loss: 0.12768885493278503
train_iter_loss: 0.15238775312900543
train_iter_loss: 0.15504130721092224
train_iter_loss: 0.0841001346707344
train_iter_loss: 0.1598731279373169
train_iter_loss: 0.1967012882232666
train_iter_loss: 0.24916382133960724
train_iter_loss: 0.14014391601085663
train_iter_loss: 0.13440053164958954
train_iter_loss: 0.1917589008808136
train_iter_loss: 0.2357432246208191
train_iter_loss: 0.2767988443374634
train_iter_loss: 0.12699535489082336
train_iter_loss: 0.09034066647291183
train_iter_loss: 0.1760941445827484
train_iter_loss: 0.094350665807724
train_iter_loss: 0.1761716902256012
train_iter_loss: 0.09695781767368317
train_iter_loss: 0.22153019905090332
train_iter_loss: 0.1682204306125641
train_iter_loss: 0.17640982568264008
train_iter_loss: 0.1420595347881317
train_iter_loss: 0.12423581629991531
train_iter_loss: 0.21576324105262756
train_iter_loss: 0.17502820491790771
train_iter_loss: 0.12379442900419235
train_iter_loss: 0.17541398108005524
train loss :0.1726
---------------------
Validation seg loss: 0.21884030746823213 at epoch 228
********************
best_val_epoch_loss:  0.21884030746823213
MODEL UPDATED
epoch =    229/  1000, exp = train
train_iter_loss: 0.2005649358034134
train_iter_loss: 0.2697144150733948
train_iter_loss: 0.17570142447948456
train_iter_loss: 0.30675598978996277
train_iter_loss: 0.0661691278219223
train_iter_loss: 0.14989960193634033
train_iter_loss: 0.13466107845306396
train_iter_loss: 0.103907011449337
train_iter_loss: 0.0938694030046463
train_iter_loss: 0.14250721037387848
train_iter_loss: 0.1363169252872467
train_iter_loss: 0.31289032101631165
train_iter_loss: 0.27023258805274963
train_iter_loss: 0.1916101723909378
train_iter_loss: 0.3780060112476349
train_iter_loss: 0.1827521026134491
train_iter_loss: 0.14480704069137573
train_iter_loss: 0.06366758048534393
train_iter_loss: 0.12162309885025024
train_iter_loss: 0.1046137884259224
train_iter_loss: 0.18179677426815033
train_iter_loss: 0.12978532910346985
train_iter_loss: 0.19980573654174805
train_iter_loss: 0.17735350131988525
train_iter_loss: 0.20324882864952087
train_iter_loss: 0.18644019961357117
train_iter_loss: 0.16553470492362976
train_iter_loss: 0.1471533477306366
train_iter_loss: 0.21257339417934418
train_iter_loss: 0.18269264698028564
train_iter_loss: 0.23531748354434967
train_iter_loss: 0.406008243560791
train_iter_loss: 0.13209109008312225
train_iter_loss: 0.17798703908920288
train_iter_loss: 0.14249007403850555
train_iter_loss: 0.28906869888305664
train_iter_loss: 0.2935207784175873
train_iter_loss: 0.11717318743467331
train_iter_loss: 0.059268079698085785
train_iter_loss: 0.15636727213859558
train_iter_loss: 0.07411342859268188
train_iter_loss: 0.19153286516666412
train_iter_loss: 0.3128415644168854
train_iter_loss: 0.12663520872592926
train_iter_loss: 0.14064304530620575
train_iter_loss: 0.09967876225709915
train_iter_loss: 0.203374445438385
train_iter_loss: 0.1418575495481491
train_iter_loss: 0.08240342140197754
train_iter_loss: 0.17128393054008484
train_iter_loss: 0.24199293553829193
train_iter_loss: 0.10263659805059433
train_iter_loss: 0.09296640753746033
train_iter_loss: 0.13224413990974426
train_iter_loss: 0.056168459355831146
train_iter_loss: 0.20032238960266113
train_iter_loss: 0.22008390724658966
train_iter_loss: 0.08023563772439957
train_iter_loss: 0.14000721275806427
train_iter_loss: 0.16334308683872223
train_iter_loss: 0.22093285620212555
train_iter_loss: 0.058089643716812134
train_iter_loss: 0.1184578612446785
train_iter_loss: 0.15309742093086243
train_iter_loss: 0.16469360888004303
train_iter_loss: 0.0992184430360794
train_iter_loss: 0.26516789197921753
train_iter_loss: 0.19459553062915802
train_iter_loss: 0.25434616208076477
train_iter_loss: 0.16480569541454315
train_iter_loss: 0.08068905770778656
train_iter_loss: 0.058357179164886475
train_iter_loss: 0.13408921658992767
train_iter_loss: 0.2726146876811981
train_iter_loss: 0.12182792276144028
train_iter_loss: 0.26669037342071533
train_iter_loss: 0.07520348578691483
train_iter_loss: 0.3681330382823944
train_iter_loss: 0.28189167380332947
train_iter_loss: 0.16586583852767944
train_iter_loss: 0.18707498908042908
train_iter_loss: 0.1429167538881302
train_iter_loss: 0.14903125166893005
train_iter_loss: 0.20995911955833435
train_iter_loss: 0.1689566820859909
train_iter_loss: 0.21191895008087158
train_iter_loss: 0.14691993594169617
train_iter_loss: 0.16554784774780273
train_iter_loss: 0.14416632056236267
train_iter_loss: 0.17664735019207
train_iter_loss: 0.22773054242134094
train_iter_loss: 0.19597749412059784
train_iter_loss: 0.2613624930381775
train_iter_loss: 0.14076556265354156
train_iter_loss: 0.2762123644351959
train_iter_loss: 0.1369134783744812
train_iter_loss: 0.18928787112236023
train_iter_loss: 0.17056579887866974
train_iter_loss: 0.22473369538784027
train_iter_loss: 0.16765910387039185
train loss :0.1771
---------------------
Validation seg loss: 0.22385723434634647 at epoch 229
epoch =    230/  1000, exp = train
train_iter_loss: 0.24526174366474152
train_iter_loss: 0.08427323400974274
train_iter_loss: 0.13394896686077118
train_iter_loss: 0.15878279507160187
train_iter_loss: 0.11496802419424057
train_iter_loss: 0.1635393649339676
train_iter_loss: 0.21666903793811798
train_iter_loss: 0.028945207595825195
train_iter_loss: 0.09160532057285309
train_iter_loss: 0.19477848708629608
train_iter_loss: 0.26562246680259705
train_iter_loss: 0.08497969061136246
train_iter_loss: 0.13663610816001892
train_iter_loss: 0.2985413372516632
train_iter_loss: 0.2250356674194336
train_iter_loss: 0.09248189628124237
train_iter_loss: 0.06413215398788452
train_iter_loss: 0.11124846339225769
train_iter_loss: 0.23212459683418274
train_iter_loss: 0.24189357459545135
train_iter_loss: 0.12905848026275635
train_iter_loss: 0.17604543268680573
train_iter_loss: 0.14484156668186188
train_iter_loss: 0.12534907460212708
train_iter_loss: 0.1447763293981552
train_iter_loss: 0.20232853293418884
train_iter_loss: 0.1583060920238495
train_iter_loss: 0.16375070810317993
train_iter_loss: 0.12352186441421509
train_iter_loss: 0.19710637629032135
train_iter_loss: 0.28239500522613525
train_iter_loss: 0.24698114395141602
train_iter_loss: 0.09316069632768631
train_iter_loss: 0.06297584623098373
train_iter_loss: 0.14634108543395996
train_iter_loss: 0.18173354864120483
train_iter_loss: 0.11957988142967224
train_iter_loss: 0.0897713154554367
train_iter_loss: 0.08619725704193115
train_iter_loss: 0.17117594182491302
train_iter_loss: 0.13746081292629242
train_iter_loss: 0.09499198198318481
train_iter_loss: 0.1182628720998764
train_iter_loss: 0.18455544114112854
train_iter_loss: 0.15408405661582947
train_iter_loss: 0.10099956393241882
train_iter_loss: 0.19988387823104858
train_iter_loss: 0.2309832125902176
train_iter_loss: 0.14826375246047974
train_iter_loss: 0.226628839969635
train_iter_loss: 0.10797905176877975
train_iter_loss: 0.15835292637348175
train_iter_loss: 0.12012823671102524
train_iter_loss: 0.13087455928325653
train_iter_loss: 0.3937978446483612
train_iter_loss: 0.1363421082496643
train_iter_loss: 0.12501536309719086
train_iter_loss: 0.1307675987482071
train_iter_loss: 0.35896381735801697
train_iter_loss: 0.10512539744377136
train_iter_loss: 0.15182550251483917
train_iter_loss: 0.1004663035273552
train_iter_loss: 0.1345854103565216
train_iter_loss: 0.15498685836791992
train_iter_loss: 0.16760368645191193
train_iter_loss: 0.23769141733646393
train_iter_loss: 0.15559551119804382
train_iter_loss: 0.22082939743995667
train_iter_loss: 0.19518662989139557
train_iter_loss: 0.23150822520256042
train_iter_loss: 0.24617908895015717
train_iter_loss: 0.08185480535030365
train_iter_loss: 0.174268901348114
train_iter_loss: 0.1782515048980713
train_iter_loss: 0.15177373588085175
train_iter_loss: 0.12598499655723572
train_iter_loss: 0.10427794605493546
train_iter_loss: 0.2811642587184906
train_iter_loss: 0.08181241154670715
train_iter_loss: 0.13897176086902618
train_iter_loss: 0.1627306491136551
train_iter_loss: 0.21270644664764404
train_iter_loss: 0.0746164545416832
train_iter_loss: 0.18748964369297028
train_iter_loss: 0.13824734091758728
train_iter_loss: 0.46991389989852905
train_iter_loss: 0.13141803443431854
train_iter_loss: 0.1516701728105545
train_iter_loss: 0.12157587707042694
train_iter_loss: 0.085359126329422
train_iter_loss: 0.058531954884529114
train_iter_loss: 0.24682831764221191
train_iter_loss: 0.3584504723548889
train_iter_loss: 0.24035534262657166
train_iter_loss: 0.14975322782993317
train_iter_loss: 0.22402945160865784
train_iter_loss: 0.0834038034081459
train_iter_loss: 0.14732372760772705
train_iter_loss: 0.08860915154218674
train_iter_loss: 0.34005284309387207
train loss :0.1676
---------------------
Validation seg loss: 0.2214168578843182 at epoch 230
epoch =    231/  1000, exp = train
train_iter_loss: 0.1716049462556839
train_iter_loss: 0.24491725862026215
train_iter_loss: 0.22328485548496246
train_iter_loss: 0.15547357499599457
train_iter_loss: 0.1252819448709488
train_iter_loss: 0.1812017410993576
train_iter_loss: 0.2385644167661667
train_iter_loss: 0.12204938381910324
train_iter_loss: 0.02339773438870907
train_iter_loss: 0.06813859939575195
train_iter_loss: 0.18142759799957275
train_iter_loss: 0.3936375379562378
train_iter_loss: 0.2078401893377304
train_iter_loss: 0.2653563618659973
train_iter_loss: 0.16213339567184448
train_iter_loss: 0.14840829372406006
train_iter_loss: 0.22029833495616913
train_iter_loss: 0.12922699749469757
train_iter_loss: 0.25057098269462585
train_iter_loss: 0.13136067986488342
train_iter_loss: 0.15090206265449524
train_iter_loss: 0.19079487025737762
train_iter_loss: 0.16894525289535522
train_iter_loss: 0.0755084678530693
train_iter_loss: 0.17670181393623352
train_iter_loss: 0.251239150762558
train_iter_loss: 0.19522340595722198
train_iter_loss: 0.14772018790245056
train_iter_loss: 0.10192088037729263
train_iter_loss: 0.2909758687019348
train_iter_loss: 0.1580025851726532
train_iter_loss: 0.1667962670326233
train_iter_loss: 0.1117452010512352
train_iter_loss: 0.1511230319738388
train_iter_loss: 0.15240320563316345
train_iter_loss: 0.2651163935661316
train_iter_loss: 0.1748548299074173
train_iter_loss: 0.3714125156402588
train_iter_loss: 0.18839456140995026
train_iter_loss: 0.20767319202423096
train_iter_loss: 0.26581937074661255
train_iter_loss: 0.2415626049041748
train_iter_loss: 0.18856245279312134
train_iter_loss: 0.1839156299829483
train_iter_loss: 0.14406442642211914
train_iter_loss: 0.1515619158744812
train_iter_loss: 0.17126351594924927
train_iter_loss: 0.13659189641475677
train_iter_loss: 0.20247477293014526
train_iter_loss: 0.08635791391134262
train_iter_loss: 0.1345939338207245
train_iter_loss: 0.15245094895362854
train_iter_loss: 0.1699146032333374
train_iter_loss: 0.14367416501045227
train_iter_loss: 0.14255553483963013
train_iter_loss: 0.14859826862812042
train_iter_loss: 0.11146240681409836
train_iter_loss: 0.10056466609239578
train_iter_loss: 0.29121842980384827
train_iter_loss: 0.1871168166399002
train_iter_loss: 0.25382161140441895
train_iter_loss: 0.19182896614074707
train_iter_loss: 0.11211632937192917
train_iter_loss: 0.0774507001042366
train_iter_loss: 0.08678863942623138
train_iter_loss: 0.41722315549850464
train_iter_loss: 0.12117138504981995
train_iter_loss: 0.17437085509300232
train_iter_loss: 0.14853590726852417
train_iter_loss: 0.27572697401046753
train_iter_loss: 0.0885256677865982
train_iter_loss: 0.20044560730457306
train_iter_loss: 0.1538202464580536
train_iter_loss: 0.15068945288658142
train_iter_loss: 0.08460567891597748
train_iter_loss: 0.17541854083538055
train_iter_loss: 0.17310842871665955
train_iter_loss: 0.1620311141014099
train_iter_loss: 0.14413779973983765
train_iter_loss: 0.15537627041339874
train_iter_loss: 0.2506418824195862
train_iter_loss: 0.11777074635028839
train_iter_loss: 0.15306583046913147
train_iter_loss: 0.17126691341400146
train_iter_loss: 0.21817250549793243
train_iter_loss: 0.09894785284996033
train_iter_loss: 0.30131790041923523
train_iter_loss: 0.13505133986473083
train_iter_loss: 0.10950184613466263
train_iter_loss: 0.06656894832849503
train_iter_loss: 0.1080801784992218
train_iter_loss: 0.07192935049533844
train_iter_loss: 0.1855296492576599
train_iter_loss: 0.22518125176429749
train_iter_loss: 0.10290149599313736
train_iter_loss: 0.16019941866397858
train_iter_loss: 0.24696840345859528
train_iter_loss: 0.13472653925418854
train_iter_loss: 0.08503168076276779
train_iter_loss: 0.08205115050077438
train loss :0.1724
---------------------
Validation seg loss: 0.22202058890307286 at epoch 231
epoch =    232/  1000, exp = train
train_iter_loss: 0.09096428751945496
train_iter_loss: 0.08491183817386627
train_iter_loss: 0.2390923947095871
train_iter_loss: 0.13799551129341125
train_iter_loss: 0.1864164173603058
train_iter_loss: 0.09216666966676712
train_iter_loss: 0.09229841828346252
train_iter_loss: 0.17676673829555511
train_iter_loss: 0.44701120257377625
train_iter_loss: 0.17775030434131622
train_iter_loss: 0.1761522889137268
train_iter_loss: 0.2006932497024536
train_iter_loss: 0.1085592657327652
train_iter_loss: 0.11811328679323196
train_iter_loss: 0.158411905169487
train_iter_loss: 0.2305704951286316
train_iter_loss: 0.18555068969726562
train_iter_loss: 0.11694584786891937
train_iter_loss: 0.17786864936351776
train_iter_loss: 0.10269982367753983
train_iter_loss: 0.0945219025015831
train_iter_loss: 0.18757310509681702
train_iter_loss: 0.081584632396698
train_iter_loss: 0.1647825390100479
train_iter_loss: 0.11275729537010193
train_iter_loss: 0.08942486345767975
train_iter_loss: 0.17828553915023804
train_iter_loss: 0.20451024174690247
train_iter_loss: 0.04870030656456947
train_iter_loss: 0.20043276250362396
train_iter_loss: 0.08940162509679794
train_iter_loss: 0.19007588922977448
train_iter_loss: 0.11084989458322525
train_iter_loss: 0.13814851641654968
train_iter_loss: 0.12245510518550873
train_iter_loss: 0.15508593618869781
train_iter_loss: 0.2243773639202118
train_iter_loss: 0.1899837702512741
train_iter_loss: 0.22443526983261108
train_iter_loss: 0.0848112553358078
train_iter_loss: 0.19453459978103638
train_iter_loss: 0.20558910071849823
train_iter_loss: 0.12533815205097198
train_iter_loss: 0.20407941937446594
train_iter_loss: 0.20576435327529907
train_iter_loss: 0.1570097655057907
train_iter_loss: 0.09082102030515671
train_iter_loss: 0.14904949069023132
train_iter_loss: 0.18153181672096252
train_iter_loss: 0.2032812386751175
train_iter_loss: 0.14737173914909363
train_iter_loss: 0.20229317247867584
train_iter_loss: 0.21446405351161957
train_iter_loss: 0.15105822682380676
train_iter_loss: 0.18321894109249115
train_iter_loss: 0.21534423530101776
train_iter_loss: 0.2812761962413788
train_iter_loss: 0.17854930460453033
train_iter_loss: 0.19644929468631744
train_iter_loss: 0.15504558384418488
train_iter_loss: 0.1991868019104004
train_iter_loss: 0.2441505491733551
train_iter_loss: 0.22483451664447784
train_iter_loss: 0.18628264963626862
train_iter_loss: 0.08156704157590866
train_iter_loss: 0.11659687012434006
train_iter_loss: 0.06075342372059822
train_iter_loss: 0.10534828156232834
train_iter_loss: 0.2541518211364746
train_iter_loss: 0.20347090065479279
train_iter_loss: 0.19193491339683533
train_iter_loss: 0.13253015279769897
train_iter_loss: 0.1427876204252243
train_iter_loss: 0.06726959347724915
train_iter_loss: 0.14100220799446106
train_iter_loss: 0.1424129754304886
train_iter_loss: 0.1731935441493988
train_iter_loss: 0.25638440251350403
train_iter_loss: 0.12448058277368546
train_iter_loss: 0.1159190759062767
train_iter_loss: 0.18482927978038788
train_iter_loss: 0.18802718818187714
train_iter_loss: 0.27746573090553284
train_iter_loss: 0.16034696996212006
train_iter_loss: 0.17236100137233734
train_iter_loss: 0.20915314555168152
train_iter_loss: 0.07918783277273178
train_iter_loss: 0.09121465682983398
train_iter_loss: 0.15019141137599945
train_iter_loss: 0.1185312420129776
train_iter_loss: 0.11088994145393372
train_iter_loss: 0.2645377516746521
train_iter_loss: 0.15867099165916443
train_iter_loss: 0.13093335926532745
train_iter_loss: 0.3009572923183441
train_iter_loss: 0.16215300559997559
train_iter_loss: 0.2960553467273712
train_iter_loss: 0.09926474094390869
train_iter_loss: 0.21192316710948944
train_iter_loss: 0.19069045782089233
train loss :0.1673
---------------------
Validation seg loss: 0.22320461918090312 at epoch 232
epoch =    233/  1000, exp = train
train_iter_loss: 0.18925923109054565
train_iter_loss: 0.20009763538837433
train_iter_loss: 0.22823183238506317
train_iter_loss: 0.11207820475101471
train_iter_loss: 0.22715143859386444
train_iter_loss: 0.08663973957300186
train_iter_loss: 0.13639682531356812
train_iter_loss: 0.13452886044979095
train_iter_loss: 0.35598865151405334
train_iter_loss: 0.10154346376657486
train_iter_loss: 0.13031955063343048
train_iter_loss: 0.11104398220777512
train_iter_loss: 0.13918569684028625
train_iter_loss: 0.11641936004161835
train_iter_loss: 0.19107304513454437
train_iter_loss: 0.15510229766368866
train_iter_loss: 0.24023611843585968
train_iter_loss: 0.18170049786567688
train_iter_loss: 0.2712188959121704
train_iter_loss: 0.13806095719337463
train_iter_loss: 0.10445316135883331
train_iter_loss: 0.23079173266887665
train_iter_loss: 0.1524733453989029
train_iter_loss: 0.18637414276599884
train_iter_loss: 0.06696166843175888
train_iter_loss: 0.14368900656700134
train_iter_loss: 0.19297069311141968
train_iter_loss: 0.1600317806005478
train_iter_loss: 0.11655871570110321
train_iter_loss: 0.10893350094556808
train_iter_loss: 0.18021482229232788
train_iter_loss: 0.11879079788923264
train_iter_loss: 0.07634104043245316
train_iter_loss: 0.2020607739686966
train_iter_loss: 0.19265620410442352
train_iter_loss: 0.21912364661693573
train_iter_loss: 0.25074175000190735
train_iter_loss: 0.11683090031147003
train_iter_loss: 0.1261816918849945
train_iter_loss: 0.10215557366609573
train_iter_loss: 0.1519293338060379
train_iter_loss: 0.03904328867793083
train_iter_loss: 0.12487009912729263
train_iter_loss: 0.3025061786174774
train_iter_loss: 0.17084340751171112
train_iter_loss: 0.12087080627679825
train_iter_loss: 0.10691864043474197
train_iter_loss: 0.13999882340431213
train_iter_loss: 0.09662672132253647
train_iter_loss: 0.3387943506240845
train_iter_loss: 0.1529431790113449
train_iter_loss: 0.22349555790424347
train_iter_loss: 0.17550496757030487
train_iter_loss: 0.23528559505939484
train_iter_loss: 0.17117929458618164
train_iter_loss: 0.204278826713562
train_iter_loss: 0.16900478303432465
train_iter_loss: 0.35624027252197266
train_iter_loss: 0.18294276297092438
train_iter_loss: 0.1301925778388977
train_iter_loss: 0.3493711054325104
train_iter_loss: 0.10692819207906723
train_iter_loss: 0.22810214757919312
train_iter_loss: 0.16092978417873383
train_iter_loss: 0.1683574616909027
train_iter_loss: 0.10752073675394058
train_iter_loss: 0.23834779858589172
train_iter_loss: 0.27912092208862305
train_iter_loss: 0.13508710265159607
train_iter_loss: 0.16846761107444763
train_iter_loss: 0.1416659951210022
train_iter_loss: 0.14154790341854095
train_iter_loss: 0.10954993218183517
train_iter_loss: 0.34705087542533875
train_iter_loss: 0.1774020791053772
train_iter_loss: 0.22625629603862762
train_iter_loss: 0.04826551675796509
train_iter_loss: 0.09272245317697525
train_iter_loss: 0.14866934716701508
train_iter_loss: 0.23372195661067963
train_iter_loss: 0.2190868854522705
train_iter_loss: 0.14854978024959564
train_iter_loss: 0.32815611362457275
train_iter_loss: 0.37178730964660645
train_iter_loss: 0.19608376920223236
train_iter_loss: 0.11797557026147842
train_iter_loss: 0.12960781157016754
train_iter_loss: 0.13872914016246796
train_iter_loss: 0.2203371673822403
train_iter_loss: 0.10143738240003586
train_iter_loss: 0.15929605066776276
train_iter_loss: 0.12236663699150085
train_iter_loss: 0.22270289063453674
train_iter_loss: 0.18408118188381195
train_iter_loss: 0.1162400171160698
train_iter_loss: 0.13788913190364838
train_iter_loss: 0.0703890398144722
train_iter_loss: 0.10650742799043655
train_iter_loss: 0.06782830506563187
train_iter_loss: 0.17915059626102448
train loss :0.1720
---------------------
Validation seg loss: 0.2232390195307023 at epoch 233
epoch =    234/  1000, exp = train
train_iter_loss: 0.20103605091571808
train_iter_loss: 0.11797133088111877
train_iter_loss: 0.1760023534297943
train_iter_loss: 0.15543006360530853
train_iter_loss: 0.23376357555389404
train_iter_loss: 0.09763924032449722
train_iter_loss: 0.18176613748073578
train_iter_loss: 0.2022295743227005
train_iter_loss: 0.11881574243307114
train_iter_loss: 0.18229685723781586
train_iter_loss: 0.26715874671936035
train_iter_loss: 0.20014165341854095
train_iter_loss: 0.07455068081617355
train_iter_loss: 0.15178392827510834
train_iter_loss: 0.0683266893029213
train_iter_loss: 0.10847698152065277
train_iter_loss: 0.21145014464855194
train_iter_loss: 0.2434930056333542
train_iter_loss: 0.31308287382125854
train_iter_loss: 0.14195789396762848
train_iter_loss: 0.2939474880695343
train_iter_loss: 0.10245143622159958
train_iter_loss: 0.21025238931179047
train_iter_loss: 0.12854771316051483
train_iter_loss: 0.2507789731025696
train_iter_loss: 0.09827268123626709
train_iter_loss: 0.140360027551651
train_iter_loss: 0.25252360105514526
train_iter_loss: 0.12128289043903351
train_iter_loss: 0.16879792511463165
train_iter_loss: 0.04196817800402641
train_iter_loss: 0.15184850990772247
train_iter_loss: 0.11253587156534195
train_iter_loss: 0.06398089230060577
train_iter_loss: 0.13027581572532654
train_iter_loss: 0.20312294363975525
train_iter_loss: 0.08670622110366821
train_iter_loss: 0.2065069079399109
train_iter_loss: 0.2800748944282532
train_iter_loss: 0.2972794771194458
train_iter_loss: 0.5762608051300049
train_iter_loss: 0.12883824110031128
train_iter_loss: 0.13208326697349548
train_iter_loss: 0.1659594178199768
train_iter_loss: 0.14236889779567719
train_iter_loss: 0.17772488296031952
train_iter_loss: 0.157382071018219
train_iter_loss: 0.17515631020069122
train_iter_loss: 0.1752738356590271
train_iter_loss: 0.17665058374404907
train_iter_loss: 0.1456821709871292
train_iter_loss: 0.12291394174098969
train_iter_loss: 0.1656394898891449
train_iter_loss: 0.2254216969013214
train_iter_loss: 0.238999605178833
train_iter_loss: 0.17955194413661957
train_iter_loss: 0.22661201655864716
train_iter_loss: 0.18007904291152954
train_iter_loss: 0.18554982542991638
train_iter_loss: 0.21728457510471344
train_iter_loss: 0.15308085083961487
train_iter_loss: 0.11554476618766785
train_iter_loss: 0.11612732708454132
train_iter_loss: 0.25515317916870117
train_iter_loss: 0.09010221809148788
train_iter_loss: 0.15964747965335846
train_iter_loss: 0.10314948111772537
train_iter_loss: 0.14891009032726288
train_iter_loss: 0.13014927506446838
train_iter_loss: 0.13229282200336456
train_iter_loss: 0.21041782200336456
train_iter_loss: 0.31817924976348877
train_iter_loss: 0.18196667730808258
train_iter_loss: 0.15519560873508453
train_iter_loss: 0.17363841831684113
train_iter_loss: 0.22870630025863647
train_iter_loss: 0.16964422166347504
train_iter_loss: 0.18976695835590363
train_iter_loss: 0.24645163118839264
train_iter_loss: 0.21743105351924896
train_iter_loss: 0.21735060214996338
train_iter_loss: 0.09316519647836685
train_iter_loss: 0.12343572825193405
train_iter_loss: 0.15359656512737274
train_iter_loss: 0.0867772102355957
train_iter_loss: 0.10859610885381699
train_iter_loss: 0.11885039508342743
train_iter_loss: 0.12384123355150223
train_iter_loss: 0.14374037086963654
train_iter_loss: 0.10618942975997925
train_iter_loss: 0.15637324750423431
train_iter_loss: 0.28639012575149536
train_iter_loss: 0.09351831674575806
train_iter_loss: 0.21203279495239258
train_iter_loss: 0.13984733819961548
train_iter_loss: 0.16011907160282135
train_iter_loss: 0.1444811224937439
train_iter_loss: 0.19037574529647827
train_iter_loss: 0.12654893100261688
train_iter_loss: 0.20735390484333038
train loss :0.1734
---------------------
Validation seg loss: 0.22031397662424254 at epoch 234
epoch =    235/  1000, exp = train
train_iter_loss: 0.2302085906267166
train_iter_loss: 0.3501986861228943
train_iter_loss: 0.15816013514995575
train_iter_loss: 0.1453484296798706
train_iter_loss: 0.09160073101520538
train_iter_loss: 0.10713480412960052
train_iter_loss: 0.35290655493736267
train_iter_loss: 0.25018808245658875
train_iter_loss: 0.10791003704071045
train_iter_loss: 0.23980769515037537
train_iter_loss: 0.21064531803131104
train_iter_loss: 0.2039383202791214
train_iter_loss: 0.17239874601364136
train_iter_loss: 0.13965275883674622
train_iter_loss: 0.18606901168823242
train_iter_loss: 0.054944396018981934
train_iter_loss: 0.0735405832529068
train_iter_loss: 0.1170174777507782
train_iter_loss: 0.1788972169160843
train_iter_loss: 0.222053661942482
train_iter_loss: 0.2256605476140976
train_iter_loss: 0.13391029834747314
train_iter_loss: 0.13026893138885498
train_iter_loss: 0.1205766499042511
train_iter_loss: 0.21043498814105988
train_iter_loss: 0.22805330157279968
train_iter_loss: 0.17851370573043823
train_iter_loss: 0.20037005841732025
train_iter_loss: 0.41701582074165344
train_iter_loss: 0.13386951386928558
train_iter_loss: 0.3323245942592621
train_iter_loss: 0.2614712417125702
train_iter_loss: 0.18229395151138306
train_iter_loss: 0.19372019171714783
train_iter_loss: 0.30640700459480286
train_iter_loss: 0.5351656079292297
train_iter_loss: 0.20834288001060486
train_iter_loss: 0.17208294570446014
train_iter_loss: 0.2317916750907898
train_iter_loss: 0.17423997819423676
train_iter_loss: 0.1822105497121811
train_iter_loss: 0.2196640968322754
train_iter_loss: 0.07887006551027298
train_iter_loss: 0.26318359375
train_iter_loss: 0.13368894159793854
train_iter_loss: 0.19874922931194305
train_iter_loss: 0.1997423619031906
train_iter_loss: 0.16838142275810242
train_iter_loss: 0.10591845959424973
train_iter_loss: 0.09651646018028259
train_iter_loss: 0.2469877302646637
train_iter_loss: 0.13141298294067383
train_iter_loss: 0.17436422407627106
train_iter_loss: 0.23360884189605713
train_iter_loss: 0.10466578602790833
train_iter_loss: 0.14540429413318634
train_iter_loss: 0.10300417244434357
train_iter_loss: 0.17659863829612732
train_iter_loss: 0.12410470843315125
train_iter_loss: 0.1275077760219574
train_iter_loss: 0.07180638611316681
train_iter_loss: 0.07513627409934998
train_iter_loss: 0.17200419306755066
train_iter_loss: 0.22397366166114807
train_iter_loss: 0.1116146370768547
train_iter_loss: 0.22689279913902283
train_iter_loss: 0.15974995493888855
train_iter_loss: 0.14614802598953247
train_iter_loss: 0.06554489582777023
train_iter_loss: 0.1171019896864891
train_iter_loss: 0.16588880121707916
train_iter_loss: 0.13652168214321136
train_iter_loss: 0.09351390600204468
train_iter_loss: 0.12588970363140106
train_iter_loss: 0.1445586383342743
train_iter_loss: 0.22325752675533295
train_iter_loss: 0.14052392542362213
train_iter_loss: 0.09098998457193375
train_iter_loss: 0.12702767550945282
train_iter_loss: 0.20569473505020142
train_iter_loss: 0.1684180498123169
train_iter_loss: 0.2652583718299866
train_iter_loss: 0.2162628173828125
train_iter_loss: 0.15599095821380615
train_iter_loss: 0.1742091029882431
train_iter_loss: 0.11352143436670303
train_iter_loss: 0.14335989952087402
train_iter_loss: 0.17244985699653625
train_iter_loss: 0.21933163702487946
train_iter_loss: 0.10947870463132858
train_iter_loss: 0.2421046495437622
train_iter_loss: 0.19343331456184387
train_iter_loss: 0.16686128079891205
train_iter_loss: 0.16685666143894196
train_iter_loss: 0.20335650444030762
train_iter_loss: 0.13268552720546722
train_iter_loss: 0.09911154955625534
train_iter_loss: 0.06611867994070053
train_iter_loss: 0.14564459025859833
train_iter_loss: 0.12783387303352356
train loss :0.1766
---------------------
Validation seg loss: 0.22313792085415632 at epoch 235
epoch =    236/  1000, exp = train
train_iter_loss: 0.15749859809875488
train_iter_loss: 0.16820788383483887
train_iter_loss: 0.19480137526988983
train_iter_loss: 0.19669947028160095
train_iter_loss: 0.1572994738817215
train_iter_loss: 0.480333536863327
train_iter_loss: 0.22116616368293762
train_iter_loss: 0.11701297760009766
train_iter_loss: 0.34150683879852295
train_iter_loss: 0.14026498794555664
train_iter_loss: 0.15202972292900085
train_iter_loss: 0.32961738109588623
train_iter_loss: 0.2573731243610382
train_iter_loss: 0.15315143764019012
train_iter_loss: 0.1220925971865654
train_iter_loss: 0.09136123210191727
train_iter_loss: 0.1810102015733719
train_iter_loss: 0.06699086725711823
train_iter_loss: 0.07458685338497162
train_iter_loss: 0.07966256886720657
train_iter_loss: 0.14906707406044006
train_iter_loss: 0.23284433782100677
train_iter_loss: 0.1608879417181015
train_iter_loss: 0.2246495932340622
train_iter_loss: 0.17507648468017578
train_iter_loss: 0.14012962579727173
train_iter_loss: 0.08711248636245728
train_iter_loss: 0.11891847103834152
train_iter_loss: 0.1286567598581314
train_iter_loss: 0.1720104217529297
train_iter_loss: 0.05224938318133354
train_iter_loss: 0.24197299778461456
train_iter_loss: 0.06358157098293304
train_iter_loss: 0.10887470841407776
train_iter_loss: 0.1732407808303833
train_iter_loss: 0.22952379286289215
train_iter_loss: 0.2724020779132843
train_iter_loss: 0.1873726099729538
train_iter_loss: 0.21212100982666016
train_iter_loss: 0.2577570080757141
train_iter_loss: 0.14595939218997955
train_iter_loss: 0.2633213400840759
train_iter_loss: 0.18734662234783173
train_iter_loss: 0.2766580581665039
train_iter_loss: 0.17605257034301758
train_iter_loss: 0.11567956209182739
train_iter_loss: 0.2294447422027588
train_iter_loss: 0.10920161753892899
train_iter_loss: 0.16810354590415955
train_iter_loss: 0.09421353042125702
train_iter_loss: 0.43196234107017517
train_iter_loss: 0.14825846254825592
train_iter_loss: 0.1058991551399231
train_iter_loss: 0.1626506745815277
train_iter_loss: 0.23150330781936646
train_iter_loss: 0.17764835059642792
train_iter_loss: 0.06936657428741455
train_iter_loss: 0.1816985011100769
train_iter_loss: 0.16152693331241608
train_iter_loss: 0.41893109679222107
train_iter_loss: 0.15454204380512238
train_iter_loss: 0.1378350853919983
train_iter_loss: 0.07913773506879807
train_iter_loss: 0.21686773002147675
train_iter_loss: 0.21801182627677917
train_iter_loss: 0.2407926768064499
train_iter_loss: 0.25617095828056335
train_iter_loss: 0.10209479928016663
train_iter_loss: 0.19743849337100983
train_iter_loss: 0.12855996191501617
train_iter_loss: 0.24872073531150818
train_iter_loss: 0.15298134088516235
train_iter_loss: 0.12040040642023087
train_iter_loss: 0.19334517419338226
train_iter_loss: 0.22820688784122467
train_iter_loss: 0.08119705319404602
train_iter_loss: 0.11443950980901718
train_iter_loss: 0.1631070375442505
train_iter_loss: 0.1587836742401123
train_iter_loss: 0.156168594956398
train_iter_loss: 0.16993913054466248
train_iter_loss: 0.2971748411655426
train_iter_loss: 0.10263484716415405
train_iter_loss: 0.11143182218074799
train_iter_loss: 0.14322224259376526
train_iter_loss: 0.0919780507683754
train_iter_loss: 0.15230944752693176
train_iter_loss: 0.08657341450452805
train_iter_loss: 0.10538168996572495
train_iter_loss: 0.16409265995025635
train_iter_loss: 0.15420477092266083
train_iter_loss: 0.17170754075050354
train_iter_loss: 0.2634991705417633
train_iter_loss: 0.05605796352028847
train_iter_loss: 0.10488814860582352
train_iter_loss: 0.09588897228240967
train_iter_loss: 0.13679960370063782
train_iter_loss: 0.1296507567167282
train_iter_loss: 0.28813332319259644
train_iter_loss: 0.3132456839084625
train loss :0.1759
---------------------
Validation seg loss: 0.21999005785317355 at epoch 236
epoch =    237/  1000, exp = train
train_iter_loss: 0.08228747546672821
train_iter_loss: 0.13985377550125122
train_iter_loss: 0.12213334441184998
train_iter_loss: 0.17043748497962952
train_iter_loss: 0.09045573323965073
train_iter_loss: 0.22337308526039124
train_iter_loss: 0.14719805121421814
train_iter_loss: 0.11568572372198105
train_iter_loss: 0.16905201971530914
train_iter_loss: 0.22363264858722687
train_iter_loss: 0.09882517904043198
train_iter_loss: 0.13752318918704987
train_iter_loss: 0.2317415326833725
train_iter_loss: 0.2757059633731842
train_iter_loss: 0.16970378160476685
train_iter_loss: 0.22023731470108032
train_iter_loss: 0.21617190539836884
train_iter_loss: 0.12125125527381897
train_iter_loss: 0.10607805848121643
train_iter_loss: 0.40590226650238037
train_iter_loss: 0.09360222518444061
train_iter_loss: 0.17925862967967987
train_iter_loss: 0.18603543937206268
train_iter_loss: 0.17966927587985992
train_iter_loss: 0.1708388328552246
train_iter_loss: 0.17182976007461548
train_iter_loss: 0.13176165521144867
train_iter_loss: 0.15779075026512146
train_iter_loss: 0.20162808895111084
train_iter_loss: 0.19143320620059967
train_iter_loss: 0.3014748990535736
train_iter_loss: 0.1538403481245041
train_iter_loss: 0.22511635720729828
train_iter_loss: 0.13808715343475342
train_iter_loss: 0.14339737594127655
train_iter_loss: 0.19528591632843018
train_iter_loss: 0.2129838466644287
train_iter_loss: 0.1680099219083786
train_iter_loss: 0.11288571357727051
train_iter_loss: 0.1057886928319931
train_iter_loss: 0.13371072709560394
train_iter_loss: 0.156217560172081
train_iter_loss: 0.11963103711605072
train_iter_loss: 0.1570129692554474
train_iter_loss: 0.21355874836444855
train_iter_loss: 0.2382204234600067
train_iter_loss: 0.35764992237091064
train_iter_loss: 0.04645970091223717
train_iter_loss: 0.20752346515655518
train_iter_loss: 0.21099479496479034
train_iter_loss: 0.1333695501089096
train_iter_loss: 0.12081191688776016
train_iter_loss: 0.156139075756073
train_iter_loss: 0.1540062576532364
train_iter_loss: 0.18629008531570435
train_iter_loss: 0.14352551102638245
train_iter_loss: 0.11237737536430359
train_iter_loss: 0.2668512761592865
train_iter_loss: 0.22547145187854767
train_iter_loss: 0.1336710900068283
train_iter_loss: 0.12825557589530945
train_iter_loss: 0.15958549082279205
train_iter_loss: 0.16434518992900848
train_iter_loss: 0.13157105445861816
train_iter_loss: 0.13817548751831055
train_iter_loss: 0.1657525599002838
train_iter_loss: 0.16075369715690613
train_iter_loss: 0.07639613002538681
train_iter_loss: 0.1982073336839676
train_iter_loss: 0.2119530737400055
train_iter_loss: 0.08070544898509979
train_iter_loss: 0.036803875118494034
train_iter_loss: 0.06703455746173859
train_iter_loss: 0.10297705978155136
train_iter_loss: 0.22264495491981506
train_iter_loss: 0.06434525549411774
train_iter_loss: 0.15592406690120697
train_iter_loss: 0.15044386684894562
train_iter_loss: 0.1313561499118805
train_iter_loss: 0.1448058784008026
train_iter_loss: 0.13107162714004517
train_iter_loss: 0.1334286779165268
train_iter_loss: 0.0962209403514862
train_iter_loss: 0.28775346279144287
train_iter_loss: 0.17949002981185913
train_iter_loss: 0.05642241984605789
train_iter_loss: 0.13100606203079224
train_iter_loss: 0.17430660128593445
train_iter_loss: 0.13414092361927032
train_iter_loss: 0.25612905621528625
train_iter_loss: 0.16917681694030762
train_iter_loss: 0.12201985716819763
train_iter_loss: 0.1896737664937973
train_iter_loss: 0.06902392208576202
train_iter_loss: 0.19643454253673553
train_iter_loss: 0.18666686117649078
train_iter_loss: 0.11266304552555084
train_iter_loss: 0.19851917028427124
train_iter_loss: 0.1580965667963028
train_iter_loss: 0.22974249720573425
train loss :0.1644
---------------------
Validation seg loss: 0.22192844389147073 at epoch 237
epoch =    238/  1000, exp = train
train_iter_loss: 0.20235294103622437
train_iter_loss: 0.17172135412693024
train_iter_loss: 0.1415427327156067
train_iter_loss: 0.10541142523288727
train_iter_loss: 0.12026894092559814
train_iter_loss: 0.1536756008863449
train_iter_loss: 0.09754007309675217
train_iter_loss: 0.1412113755941391
train_iter_loss: 0.1784694939851761
train_iter_loss: 0.09250620007514954
train_iter_loss: 0.1527175009250641
train_iter_loss: 0.061854202300310135
train_iter_loss: 0.13018301129341125
train_iter_loss: 0.154419407248497
train_iter_loss: 0.47167378664016724
train_iter_loss: 0.3166639506816864
train_iter_loss: 0.12618295848369598
train_iter_loss: 0.2390318661928177
train_iter_loss: 0.1755605936050415
train_iter_loss: 0.17206373810768127
train_iter_loss: 0.09941618144512177
train_iter_loss: 0.12432456016540527
train_iter_loss: 0.174433171749115
train_iter_loss: 0.14257022738456726
train_iter_loss: 0.20859269797801971
train_iter_loss: 0.17418435215950012
train_iter_loss: 0.10302533954381943
train_iter_loss: 0.14880983531475067
train_iter_loss: 0.1528807133436203
train_iter_loss: 0.35597628355026245
train_iter_loss: 0.22900378704071045
train_iter_loss: 0.06252755969762802
train_iter_loss: 0.1961597353219986
train_iter_loss: 0.15321627259254456
train_iter_loss: 0.2080506533384323
train_iter_loss: 0.15909013152122498
train_iter_loss: 0.1119190976023674
train_iter_loss: 0.06212586537003517
train_iter_loss: 0.12478944659233093
train_iter_loss: 0.3538537323474884
train_iter_loss: 0.0958489328622818
train_iter_loss: 0.12781311571598053
train_iter_loss: 0.20670340955257416
train_iter_loss: 0.2547788619995117
train_iter_loss: 0.1215926930308342
train_iter_loss: 0.21243418753147125
train_iter_loss: 0.11495517939329147
train_iter_loss: 0.057346537709236145
train_iter_loss: 0.15349741280078888
train_iter_loss: 0.1635698676109314
train_iter_loss: 0.1173599436879158
train_iter_loss: 0.32463639974594116
train_iter_loss: 0.09212170541286469
train_iter_loss: 0.1277129054069519
train_iter_loss: 0.14716140925884247
train_iter_loss: 0.14884482324123383
train_iter_loss: 0.19131708145141602
train_iter_loss: 0.1778930127620697
train_iter_loss: 0.10039511322975159
train_iter_loss: 0.267609566450119
train_iter_loss: 0.23392267525196075
train_iter_loss: 0.129551500082016
train_iter_loss: 0.18794389069080353
train_iter_loss: 0.14981387555599213
train_iter_loss: 0.0977536290884018
train_iter_loss: 0.17796531319618225
train_iter_loss: 0.11868517100811005
train_iter_loss: 0.1048320084810257
train_iter_loss: 0.1487710028886795
train_iter_loss: 0.2081756293773651
train_iter_loss: 0.1535329669713974
train_iter_loss: 0.15592654049396515
train_iter_loss: 0.16671638190746307
train_iter_loss: 0.21036827564239502
train_iter_loss: 0.15721876919269562
train_iter_loss: 0.06855861097574234
train_iter_loss: 0.19534903764724731
train_iter_loss: 0.12657712399959564
train_iter_loss: 0.22833077609539032
train_iter_loss: 0.2892167866230011
train_iter_loss: 0.1284707486629486
train_iter_loss: 0.17509818077087402
train_iter_loss: 0.287677526473999
train_iter_loss: 0.11022438108921051
train_iter_loss: 0.15072037279605865
train_iter_loss: 0.13795489072799683
train_iter_loss: 0.20605704188346863
train_iter_loss: 0.20653130114078522
train_iter_loss: 0.11943820863962173
train_iter_loss: 0.21204477548599243
train_iter_loss: 0.1789088249206543
train_iter_loss: 0.1463826447725296
train_iter_loss: 0.1658822000026703
train_iter_loss: 0.24019835889339447
train_iter_loss: 0.18587422370910645
train_iter_loss: 0.16643400490283966
train_iter_loss: 0.1312481164932251
train_iter_loss: 0.20504041016101837
train_iter_loss: 0.3619348406791687
train_iter_loss: 0.16900630295276642
train loss :0.1711
---------------------
Validation seg loss: 0.222106643908499 at epoch 238
epoch =    239/  1000, exp = train
train_iter_loss: 0.14703230559825897
train_iter_loss: 0.07652726769447327
train_iter_loss: 0.08728288859128952
train_iter_loss: 0.1305389702320099
train_iter_loss: 0.20128357410430908
train_iter_loss: 0.18916438519954681
train_iter_loss: 0.1221596971154213
train_iter_loss: 0.16292427480220795
train_iter_loss: 0.1432926058769226
train_iter_loss: 0.18416351079940796
train_iter_loss: 0.34025394916534424
train_iter_loss: 0.1840953379869461
train_iter_loss: 0.15322916209697723
train_iter_loss: 0.22407101094722748
train_iter_loss: 0.11127765476703644
train_iter_loss: 0.16369101405143738
train_iter_loss: 0.21821583807468414
train_iter_loss: 0.22588779032230377
train_iter_loss: 0.26264193654060364
train_iter_loss: 0.13646841049194336
train_iter_loss: 0.11398930102586746
train_iter_loss: 0.15124857425689697
train_iter_loss: 0.19074563682079315
train_iter_loss: 0.34038200974464417
train_iter_loss: 0.23673272132873535
train_iter_loss: 0.12204843759536743
train_iter_loss: 0.20004741847515106
train_iter_loss: 0.12943892180919647
train_iter_loss: 0.17367902398109436
train_iter_loss: 0.10796286910772324
train_iter_loss: 0.20037755370140076
train_iter_loss: 0.1084730252623558
train_iter_loss: 0.19066275656223297
train_iter_loss: 0.18037253618240356
train_iter_loss: 0.11264537274837494
train_iter_loss: 0.17888511717319489
train_iter_loss: 0.15195129811763763
train_iter_loss: 0.1088535338640213
train_iter_loss: 0.14962798357009888
train_iter_loss: 0.16837435960769653
train_iter_loss: 0.13355602324008942
train_iter_loss: 0.17944176495075226
train_iter_loss: 0.13790875673294067
train_iter_loss: 0.09629738330841064
train_iter_loss: 0.11152440309524536
train_iter_loss: 0.103156179189682
train_iter_loss: 0.12852580845355988
train_iter_loss: 0.15521949529647827
train_iter_loss: 0.1544034630060196
train_iter_loss: 0.13036935031414032
train_iter_loss: 0.4248080253601074
train_iter_loss: 0.11291921138763428
train_iter_loss: 0.23235267400741577
train_iter_loss: 0.1682172566652298
train_iter_loss: 0.20003890991210938
train_iter_loss: 0.12047732621431351
train_iter_loss: 0.10615455359220505
train_iter_loss: 0.13441093266010284
train_iter_loss: 0.20951852202415466
train_iter_loss: 0.14338627457618713
train_iter_loss: 0.25999554991722107
train_iter_loss: 0.12565301358699799
train_iter_loss: 0.05087404325604439
train_iter_loss: 0.21216076612472534
train_iter_loss: 0.20867303013801575
train_iter_loss: 0.16810235381126404
train_iter_loss: 0.11941748112440109
train_iter_loss: 0.16034558415412903
train_iter_loss: 0.13556624948978424
train_iter_loss: 0.1273096799850464
train_iter_loss: 0.1690186709165573
train_iter_loss: 0.216018408536911
train_iter_loss: 0.12316921353340149
train_iter_loss: 0.08889076113700867
train_iter_loss: 0.1977415531873703
train_iter_loss: 0.19357505440711975
train_iter_loss: 0.14005343616008759
train_iter_loss: 0.13974937796592712
train_iter_loss: 0.19380204379558563
train_iter_loss: 0.21779361367225647
train_iter_loss: 0.08449738472700119
train_iter_loss: 0.2007865607738495
train_iter_loss: 0.11766550689935684
train_iter_loss: 0.17951326072216034
train_iter_loss: 0.17905081808567047
train_iter_loss: 0.15794885158538818
train_iter_loss: 0.20456086099147797
train_iter_loss: 0.14998391270637512
train_iter_loss: 0.2049446403980255
train_iter_loss: 0.1067068874835968
train_iter_loss: 0.16115455329418182
train_iter_loss: 0.2368583083152771
train_iter_loss: 0.22332704067230225
train_iter_loss: 0.21464161574840546
train_iter_loss: 0.17535895109176636
train_iter_loss: 0.1343579888343811
train_iter_loss: 0.1940182000398636
train_iter_loss: 0.19695088267326355
train_iter_loss: 0.13797883689403534
train_iter_loss: 0.20491153001785278
train loss :0.1685
---------------------
Validation seg loss: 0.22383587629059856 at epoch 239
epoch =    240/  1000, exp = train
train_iter_loss: 0.15614023804664612
train_iter_loss: 0.12154226750135422
train_iter_loss: 0.18586021661758423
train_iter_loss: 0.18749591708183289
train_iter_loss: 0.1730431616306305
train_iter_loss: 0.23017777502536774
train_iter_loss: 0.16510702669620514
train_iter_loss: 0.16236348450183868
train_iter_loss: 0.19646424055099487
train_iter_loss: 0.1897267997264862
train_iter_loss: 0.22945496439933777
train_iter_loss: 0.14377117156982422
train_iter_loss: 0.18011802434921265
train_iter_loss: 0.11492318660020828
train_iter_loss: 0.19679899513721466
train_iter_loss: 0.19699615240097046
train_iter_loss: 0.0875762477517128
train_iter_loss: 0.09913191944360733
train_iter_loss: 0.17243385314941406
train_iter_loss: 0.1717229038476944
train_iter_loss: 0.12448157370090485
train_iter_loss: 0.2277451753616333
train_iter_loss: 0.12355577200651169
train_iter_loss: 0.07773027569055557
train_iter_loss: 0.182636559009552
train_iter_loss: 0.10932411253452301
train_iter_loss: 0.1199936792254448
train_iter_loss: 0.17115518450737
train_iter_loss: 0.209807887673378
train_iter_loss: 0.08362288028001785
train_iter_loss: 0.16954369843006134
train_iter_loss: 0.2626427114009857
train_iter_loss: 0.15629228949546814
train_iter_loss: 0.2022417187690735
train_iter_loss: 0.2033265233039856
train_iter_loss: 0.16420532763004303
train_iter_loss: 0.16759392619132996
train_iter_loss: 0.18425215780735016
train_iter_loss: 0.12849828600883484
train_iter_loss: 0.07695328444242477
train_iter_loss: 0.1986207515001297
train_iter_loss: 0.11013752222061157
train_iter_loss: 0.1302311271429062
train_iter_loss: 0.18616646528244019
train_iter_loss: 0.3415483832359314
train_iter_loss: 0.14654985070228577
train_iter_loss: 0.2075088620185852
train_iter_loss: 0.1939058005809784
train_iter_loss: 0.17130419611930847
train_iter_loss: 0.09150473028421402
train_iter_loss: 0.12416587769985199
train_iter_loss: 0.14003096520900726
train_iter_loss: 0.16878348588943481
train_iter_loss: 0.11292875558137894
train_iter_loss: 0.07791192829608917
train_iter_loss: 0.19500787556171417
train_iter_loss: 0.07631732523441315
train_iter_loss: 0.16765154898166656
train_iter_loss: 0.09788596630096436
train_iter_loss: 0.20256821811199188
train_iter_loss: 0.05690053105354309
train_iter_loss: 0.29340091347694397
train_iter_loss: 0.25715893507003784
train_iter_loss: 0.13827526569366455
train_iter_loss: 0.12561412155628204
train_iter_loss: 0.10774669796228409
train_iter_loss: 0.07539667934179306
train_iter_loss: 0.13077481091022491
train_iter_loss: 0.21621564030647278
train_iter_loss: 0.19070999324321747
train_iter_loss: 0.12494812160730362
train_iter_loss: 0.16258619725704193
train_iter_loss: 0.17394231259822845
train_iter_loss: 0.33673104643821716
train_iter_loss: 0.10288212448358536
train_iter_loss: 0.11048317700624466
train_iter_loss: 0.283462792634964
train_iter_loss: 0.15470798313617706
train_iter_loss: 0.26157984137535095
train_iter_loss: 0.16215647757053375
train_iter_loss: 0.17503654956817627
train_iter_loss: 0.13674871623516083
train_iter_loss: 0.1214321106672287
train_iter_loss: 0.20807240903377533
train_iter_loss: 0.0784263014793396
train_iter_loss: 0.09685344994068146
train_iter_loss: 0.22871264815330505
train_iter_loss: 0.23340964317321777
train_iter_loss: 0.140481099486351
train_iter_loss: 0.20036174356937408
train_iter_loss: 0.02609795704483986
train_iter_loss: 0.15383458137512207
train_iter_loss: 0.14172804355621338
train_iter_loss: 0.3116835355758667
train_iter_loss: 0.1628728061914444
train_iter_loss: 0.1753263622522354
train_iter_loss: 0.2074885368347168
train_iter_loss: 0.19028936326503754
train_iter_loss: 0.20730766654014587
train_iter_loss: 0.20217226445674896
train loss :0.1668
---------------------
Validation seg loss: 0.22010390742523772 at epoch 240
epoch =    241/  1000, exp = train
train_iter_loss: 0.2078937590122223
train_iter_loss: 0.14183419942855835
train_iter_loss: 0.2888219356536865
train_iter_loss: 0.28375181555747986
train_iter_loss: 0.08777135610580444
train_iter_loss: 0.1389167606830597
train_iter_loss: 0.2211569845676422
train_iter_loss: 0.2907913327217102
train_iter_loss: 0.20771369338035583
train_iter_loss: 0.21374298632144928
train_iter_loss: 0.2120702564716339
train_iter_loss: 0.08344005793333054
train_iter_loss: 0.14184921979904175
train_iter_loss: 0.30434566736221313
train_iter_loss: 0.2028045505285263
train_iter_loss: 0.17352712154388428
train_iter_loss: 0.2020849585533142
train_iter_loss: 0.16697850823402405
train_iter_loss: 0.11595518887042999
train_iter_loss: 0.06890330463647842
train_iter_loss: 0.14489814639091492
train_iter_loss: 0.14214268326759338
train_iter_loss: 0.24512635171413422
train_iter_loss: 0.2595164179801941
train_iter_loss: 0.12525595724582672
train_iter_loss: 0.11087343841791153
train_iter_loss: 0.22419913113117218
train_iter_loss: 0.09075222164392471
train_iter_loss: 0.17984873056411743
train_iter_loss: 0.11873415857553482
train_iter_loss: 0.09719983488321304
train_iter_loss: 0.0907977893948555
train_iter_loss: 0.13494184613227844
train_iter_loss: 0.16449356079101562
train_iter_loss: 0.11871356517076492
train_iter_loss: 0.13173320889472961
train_iter_loss: 0.13565625250339508
train_iter_loss: 0.29763907194137573
train_iter_loss: 0.16102971136569977
train_iter_loss: 0.24985961616039276
train_iter_loss: 0.26917192339897156
train_iter_loss: 0.21378745138645172
train_iter_loss: 0.19081047177314758
train_iter_loss: 0.13886763155460358
train_iter_loss: 0.13805612921714783
train_iter_loss: 0.23623839020729065
train_iter_loss: 0.2173822522163391
train_iter_loss: 0.36730703711509705
train_iter_loss: 0.19560712575912476
train_iter_loss: 0.20174670219421387
train_iter_loss: 0.2646145522594452
train_iter_loss: 0.13826656341552734
train_iter_loss: 0.15838585793972015
train_iter_loss: 0.17606882750988007
train_iter_loss: 0.2727372646331787
train_iter_loss: 0.17801563441753387
train_iter_loss: 0.15805044770240784
train_iter_loss: 0.13037604093551636
train_iter_loss: 0.14541426301002502
train_iter_loss: 0.20651789009571075
train_iter_loss: 0.24165506660938263
train_iter_loss: 0.31383073329925537
train_iter_loss: 0.1485176831483841
train_iter_loss: 0.05399351939558983
train_iter_loss: 0.1358495056629181
train_iter_loss: 0.11620640009641647
train_iter_loss: 0.14791333675384521
train_iter_loss: 0.16786356270313263
train_iter_loss: 0.10589250177145004
train_iter_loss: 0.11047832667827606
train_iter_loss: 0.1418594866991043
train_iter_loss: 0.2865421175956726
train_iter_loss: 0.21944373846054077
train_iter_loss: 0.18192116916179657
train_iter_loss: 0.21911346912384033
train_iter_loss: 0.09441816061735153
train_iter_loss: 0.20265339314937592
train_iter_loss: 0.10967893898487091
train_iter_loss: 0.1118660569190979
train_iter_loss: 0.19569605588912964
train_iter_loss: 0.1244235560297966
train_iter_loss: 0.183502659201622
train_iter_loss: 0.1408202350139618
train_iter_loss: 0.137167826294899
train_iter_loss: 0.11043310165405273
train_iter_loss: 0.11758265644311905
train_iter_loss: 0.08062783628702164
train_iter_loss: 0.09327481687068939
train_iter_loss: 0.18504156172275543
train_iter_loss: 0.2095806747674942
train_iter_loss: 0.14810052514076233
train_iter_loss: 0.13608375191688538
train_iter_loss: 0.2518804371356964
train_iter_loss: 0.13691334426403046
train_iter_loss: 0.23279528319835663
train_iter_loss: 0.2945805788040161
train_iter_loss: 0.127682626247406
train_iter_loss: 0.1672104299068451
train_iter_loss: 0.10162392258644104
train_iter_loss: 0.2174985259771347
train loss :0.1764
---------------------
Validation seg loss: 0.22439056648960654 at epoch 241
epoch =    242/  1000, exp = train
train_iter_loss: 0.0892525464296341
train_iter_loss: 0.2124454528093338
train_iter_loss: 0.2035808116197586
train_iter_loss: 0.12873268127441406
train_iter_loss: 0.10859393328428268
train_iter_loss: 0.11106489598751068
train_iter_loss: 0.09775073826313019
train_iter_loss: 0.307724267244339
train_iter_loss: 0.13384883105754852
train_iter_loss: 0.14236518740653992
train_iter_loss: 0.09938857704401016
train_iter_loss: 0.13429085910320282
train_iter_loss: 0.36177870631217957
train_iter_loss: 0.11255725473165512
train_iter_loss: 0.11241348087787628
train_iter_loss: 0.20385123789310455
train_iter_loss: 0.21305373311042786
train_iter_loss: 0.1515672653913498
train_iter_loss: 0.2372402548789978
train_iter_loss: 0.27465641498565674
train_iter_loss: 0.053048573434352875
train_iter_loss: 0.13956193625926971
train_iter_loss: 0.26615944504737854
train_iter_loss: 0.17941352725028992
train_iter_loss: 0.11331391334533691
train_iter_loss: 0.19531750679016113
train_iter_loss: 0.18852056562900543
train_iter_loss: 0.13281354308128357
train_iter_loss: 0.22225143015384674
train_iter_loss: 0.1638386845588684
train_iter_loss: 0.3027949929237366
train_iter_loss: 0.15274648368358612
train_iter_loss: 0.2763446867465973
train_iter_loss: 0.09455519169569016
train_iter_loss: 0.1867040991783142
train_iter_loss: 0.21693018078804016
train_iter_loss: 0.09357473999261856
train_iter_loss: 0.3279360830783844
train_iter_loss: 0.10724044591188431
train_iter_loss: 0.07806438952684402
train_iter_loss: 0.11127202957868576
train_iter_loss: 0.14316056668758392
train_iter_loss: 0.1584371030330658
train_iter_loss: 0.144403874874115
train_iter_loss: 0.13427340984344482
train_iter_loss: 0.24687477946281433
train_iter_loss: 0.1476152092218399
train_iter_loss: 0.16074366867542267
train_iter_loss: 0.18359436094760895
train_iter_loss: 0.28264036774635315
train_iter_loss: 0.12704552710056305
train_iter_loss: 0.0953066423535347
train_iter_loss: 0.207395538687706
train_iter_loss: 0.258392870426178
train_iter_loss: 0.11036733537912369
train_iter_loss: 0.2111378163099289
train_iter_loss: 0.1592593789100647
train_iter_loss: 0.07026074081659317
train_iter_loss: 0.05315982550382614
train_iter_loss: 0.08044059574604034
train_iter_loss: 0.036465685814619064
train_iter_loss: 0.2857648730278015
train_iter_loss: 0.3055257201194763
train_iter_loss: 0.12025682628154755
train_iter_loss: 0.20324207842350006
train_iter_loss: 0.10993190109729767
train_iter_loss: 0.1566389501094818
train_iter_loss: 0.22695086896419525
train_iter_loss: 0.18019919097423553
train_iter_loss: 0.24379943311214447
train_iter_loss: 0.2704198658466339
train_iter_loss: 0.2149500846862793
train_iter_loss: 0.08166252076625824
train_iter_loss: 0.21247534453868866
train_iter_loss: 0.07296020537614822
train_iter_loss: 0.15884023904800415
train_iter_loss: 0.12610742449760437
train_iter_loss: 0.20450885593891144
train_iter_loss: 0.14515572786331177
train_iter_loss: 0.27444007992744446
train_iter_loss: 0.17309464514255524
train_iter_loss: 0.1731603890657425
train_iter_loss: 0.22846348583698273
train_iter_loss: 0.15553461015224457
train_iter_loss: 0.17442302405834198
train_iter_loss: 0.342593789100647
train_iter_loss: 0.16307903826236725
train_iter_loss: 0.248929962515831
train_iter_loss: 0.11582615971565247
train_iter_loss: 0.10620424896478653
train_iter_loss: 0.12962545454502106
train_iter_loss: 0.24225114285945892
train_iter_loss: 0.12841348350048065
train_iter_loss: 0.24592088162899017
train_iter_loss: 0.13950690627098083
train_iter_loss: 0.2001470923423767
train_iter_loss: 0.11300968378782272
train_iter_loss: 0.14233338832855225
train_iter_loss: 0.1145208328962326
train_iter_loss: 0.13556456565856934
train loss :0.1729
---------------------
Validation seg loss: 0.22284460208326015 at epoch 242
epoch =    243/  1000, exp = train
train_iter_loss: 0.13303294777870178
train_iter_loss: 0.18031620979309082
train_iter_loss: 0.2800382375717163
train_iter_loss: 0.145045205950737
train_iter_loss: 0.11091730743646622
train_iter_loss: 0.28714537620544434
train_iter_loss: 0.09881798177957535
train_iter_loss: 0.11882366240024567
train_iter_loss: 0.12357979267835617
train_iter_loss: 0.11732255667448044
train_iter_loss: 0.22106090188026428
train_iter_loss: 0.07500696182250977
train_iter_loss: 0.1703505963087082
train_iter_loss: 0.2651541829109192
train_iter_loss: 0.20620796084403992
train_iter_loss: 0.09908324480056763
train_iter_loss: 0.1746782809495926
train_iter_loss: 0.07247874140739441
train_iter_loss: 0.06270819902420044
train_iter_loss: 0.10881287604570389
train_iter_loss: 0.22188645601272583
train_iter_loss: 0.18398456275463104
train_iter_loss: 0.14591573178768158
train_iter_loss: 0.12180802971124649
train_iter_loss: 0.13613201677799225
train_iter_loss: 0.2065303474664688
train_iter_loss: 0.14009806513786316
train_iter_loss: 0.13881812989711761
train_iter_loss: 0.15211577713489532
train_iter_loss: 0.13505230844020844
train_iter_loss: 0.16347000002861023
train_iter_loss: 0.09358438849449158
train_iter_loss: 0.20719636976718903
train_iter_loss: 0.08225354552268982
train_iter_loss: 0.20218528807163239
train_iter_loss: 0.07128340750932693
train_iter_loss: 0.2561934292316437
train_iter_loss: 0.20820428431034088
train_iter_loss: 0.22249983251094818
train_iter_loss: 0.11454321444034576
train_iter_loss: 0.14923283457756042
train_iter_loss: 0.15377116203308105
train_iter_loss: 0.14788766205310822
train_iter_loss: 0.24845074117183685
train_iter_loss: 0.21979829668998718
train_iter_loss: 0.1372068226337433
train_iter_loss: 0.18267303705215454
train_iter_loss: 0.1691320538520813
train_iter_loss: 0.20684146881103516
train_iter_loss: 0.13130629062652588
train_iter_loss: 0.2205268293619156
train_iter_loss: 0.1883491426706314
train_iter_loss: 0.16381612420082092
train_iter_loss: 0.15625813603401184
train_iter_loss: 0.10770288854837418
train_iter_loss: 0.12624061107635498
train_iter_loss: 0.17591997981071472
train_iter_loss: 0.1432710736989975
train_iter_loss: 0.30886080861091614
train_iter_loss: 0.2445409744977951
train_iter_loss: 0.24022720754146576
train_iter_loss: 0.18102987110614777
train_iter_loss: 0.16459202766418457
train_iter_loss: 0.17838945984840393
train_iter_loss: 0.16495345532894135
train_iter_loss: 0.0955309346318245
train_iter_loss: 0.12095452100038528
train_iter_loss: 0.11718564480543137
train_iter_loss: 0.159662663936615
train_iter_loss: 0.34058547019958496
train_iter_loss: 0.17888258397579193
train_iter_loss: 0.10482104122638702
train_iter_loss: 0.14976005256175995
train_iter_loss: 0.19943034648895264
train_iter_loss: 0.10819156467914581
train_iter_loss: 0.09712110459804535
train_iter_loss: 0.14102703332901
train_iter_loss: 0.14505115151405334
train_iter_loss: 0.1082843691110611
train_iter_loss: 0.14368705451488495
train_iter_loss: 0.12905122339725494
train_iter_loss: 0.2204805463552475
train_iter_loss: 0.11288340389728546
train_iter_loss: 0.08950165659189224
train_iter_loss: 0.22688917815685272
train_iter_loss: 0.264713853597641
train_iter_loss: 0.14969459176063538
train_iter_loss: 0.1534455567598343
train_iter_loss: 0.12561355531215668
train_iter_loss: 0.21660545468330383
train_iter_loss: 0.11530312150716782
train_iter_loss: 0.24705907702445984
train_iter_loss: 0.10515951365232468
train_iter_loss: 0.17350877821445465
train_iter_loss: 0.23848877847194672
train_iter_loss: 0.13582657277584076
train_iter_loss: 0.24862070381641388
train_iter_loss: 0.22327710688114166
train_iter_loss: 0.24319259822368622
train_iter_loss: 0.3072708249092102
train loss :0.1688
---------------------
Validation seg loss: 0.22162010257114778 at epoch 243
epoch =    244/  1000, exp = train
train_iter_loss: 0.05496219918131828
train_iter_loss: 0.10013606399297714
train_iter_loss: 0.3127802312374115
train_iter_loss: 0.14133818447589874
train_iter_loss: 0.16635794937610626
train_iter_loss: 0.0984698235988617
train_iter_loss: 0.15193620324134827
train_iter_loss: 0.2927592694759369
train_iter_loss: 0.11966726928949356
train_iter_loss: 0.13864733278751373
train_iter_loss: 0.1899002492427826
train_iter_loss: 0.19582223892211914
train_iter_loss: 0.16684147715568542
train_iter_loss: 0.16201242804527283
train_iter_loss: 0.21004948019981384
train_iter_loss: 0.10801347345113754
train_iter_loss: 0.16562257707118988
train_iter_loss: 0.13939954340457916
train_iter_loss: 0.11197879165410995
train_iter_loss: 0.17446529865264893
train_iter_loss: 0.278941810131073
train_iter_loss: 0.1605442762374878
train_iter_loss: 0.23873008787631989
train_iter_loss: 0.04676412418484688
train_iter_loss: 0.22866787016391754
train_iter_loss: 0.17059719562530518
train_iter_loss: 0.16791172325611115
train_iter_loss: 0.12495293468236923
train_iter_loss: 0.2050165832042694
train_iter_loss: 0.12416600435972214
train_iter_loss: 0.07977066189050674
train_iter_loss: 0.16656020283699036
train_iter_loss: 0.18364204466342926
train_iter_loss: 0.1503898799419403
train_iter_loss: 0.18689404428005219
train_iter_loss: 0.09920770674943924
train_iter_loss: 0.12510599195957184
train_iter_loss: 0.11813675612211227
train_iter_loss: 0.10079421103000641
train_iter_loss: 0.14414405822753906
train_iter_loss: 0.1323411911725998
train_iter_loss: 0.1038222461938858
train_iter_loss: 0.09978824108839035
train_iter_loss: 0.07251793146133423
train_iter_loss: 0.1750207096338272
train_iter_loss: 0.12247619777917862
train_iter_loss: 0.15970703959465027
train_iter_loss: 0.21640852093696594
train_iter_loss: 0.1737045794725418
train_iter_loss: 0.19433771073818207
train_iter_loss: 0.08494706451892853
train_iter_loss: 0.20338913798332214
train_iter_loss: 0.11291022598743439
train_iter_loss: 0.08165636658668518
train_iter_loss: 0.12962983548641205
train_iter_loss: 0.15355734527111053
train_iter_loss: 0.1361280381679535
train_iter_loss: 0.15135663747787476
train_iter_loss: 0.14851348102092743
train_iter_loss: 0.24979831278324127
train_iter_loss: 0.25469061732292175
train_iter_loss: 0.09147682785987854
train_iter_loss: 0.1910066455602646
train_iter_loss: 0.3078504502773285
train_iter_loss: 0.15087977051734924
train_iter_loss: 0.17188231647014618
train_iter_loss: 0.38009145855903625
train_iter_loss: 0.2565690279006958
train_iter_loss: 0.2641921937465668
train_iter_loss: 0.14837613701820374
train_iter_loss: 0.10983488708734512
train_iter_loss: 0.21322622895240784
train_iter_loss: 0.1885354369878769
train_iter_loss: 0.17521332204341888
train_iter_loss: 0.1945316642522812
train_iter_loss: 0.3467143476009369
train_iter_loss: 0.237829327583313
train_iter_loss: 0.1651611626148224
train_iter_loss: 0.16882945597171783
train_iter_loss: 0.1495671570301056
train_iter_loss: 0.20302987098693848
train_iter_loss: 0.1572759449481964
train_iter_loss: 0.15711599588394165
train_iter_loss: 0.09222909808158875
train_iter_loss: 0.08844926953315735
train_iter_loss: 0.18693305552005768
train_iter_loss: 0.22527627646923065
train_iter_loss: 0.3737715780735016
train_iter_loss: 0.3951494097709656
train_iter_loss: 0.17736147344112396
train_iter_loss: 0.16234977543354034
train_iter_loss: 0.12897548079490662
train_iter_loss: 0.1083107590675354
train_iter_loss: 0.15385930240154266
train_iter_loss: 0.19532321393489838
train_iter_loss: 0.10440460592508316
train_iter_loss: 0.19159188866615295
train_iter_loss: 0.10943635553121567
train_iter_loss: 0.13178642094135284
train_iter_loss: 0.19990453124046326
train loss :0.1708
---------------------
Validation seg loss: 0.2198222075009121 at epoch 244
epoch =    245/  1000, exp = train
train_iter_loss: 0.186733216047287
train_iter_loss: 0.07738373428583145
train_iter_loss: 0.15894198417663574
train_iter_loss: 0.17887333035469055
train_iter_loss: 0.16613413393497467
train_iter_loss: 0.07458563148975372
train_iter_loss: 0.0397493876516819
train_iter_loss: 0.1120716854929924
train_iter_loss: 0.1166396290063858
train_iter_loss: 0.29744282364845276
train_iter_loss: 0.12419908493757248
train_iter_loss: 0.23918533325195312
train_iter_loss: 0.13703952729701996
train_iter_loss: 0.09682086110115051
train_iter_loss: 0.10246213525533676
train_iter_loss: 0.08003640919923782
train_iter_loss: 0.1076132133603096
train_iter_loss: 0.1369035691022873
train_iter_loss: 0.10435795783996582
train_iter_loss: 0.2019374519586563
train_iter_loss: 0.21799051761627197
train_iter_loss: 0.1279335618019104
train_iter_loss: 0.14002293348312378
train_iter_loss: 0.13028591871261597
train_iter_loss: 0.1833685040473938
train_iter_loss: 0.18248993158340454
train_iter_loss: 0.09217888116836548
train_iter_loss: 0.09151729196310043
train_iter_loss: 0.2123950868844986
train_iter_loss: 0.2235174924135208
train_iter_loss: 0.15157294273376465
train_iter_loss: 0.12158651649951935
train_iter_loss: 0.08414135128259659
train_iter_loss: 0.11476004868745804
train_iter_loss: 0.2241779863834381
train_iter_loss: 0.32298895716667175
train_iter_loss: 0.1270713359117508
train_iter_loss: 0.22123390436172485
train_iter_loss: 0.23315882682800293
train_iter_loss: 0.15132004022598267
train_iter_loss: 0.4250316619873047
train_iter_loss: 0.1558087319135666
train_iter_loss: 0.1632227599620819
train_iter_loss: 0.1576954871416092
train_iter_loss: 0.2329203188419342
train_iter_loss: 0.19623756408691406
train_iter_loss: 0.1431904435157776
train_iter_loss: 0.14427222311496735
train_iter_loss: 0.17664849758148193
train_iter_loss: 0.11780683696269989
train_iter_loss: 0.19045881927013397
train_iter_loss: 0.18224304914474487
train_iter_loss: 0.23353900015354156
train_iter_loss: 0.1847427934408188
train_iter_loss: 0.12043013423681259
train_iter_loss: 0.12224291265010834
train_iter_loss: 0.15717953443527222
train_iter_loss: 0.07481430470943451
train_iter_loss: 0.18037641048431396
train_iter_loss: 0.3135915696620941
train_iter_loss: 0.15454451739788055
train_iter_loss: 0.1579657942056656
train_iter_loss: 0.13790850341320038
train_iter_loss: 0.15673808753490448
train_iter_loss: 0.14374633133411407
train_iter_loss: 0.10426431894302368
train_iter_loss: 0.0797954648733139
train_iter_loss: 0.2087857872247696
train_iter_loss: 0.1398763358592987
train_iter_loss: 0.19266313314437866
train_iter_loss: 0.15724065899848938
train_iter_loss: 0.20411111414432526
train_iter_loss: 0.06368622183799744
train_iter_loss: 0.21628974378108978
train_iter_loss: 0.06617072224617004
train_iter_loss: 0.25386908650398254
train_iter_loss: 0.3437681496143341
train_iter_loss: 0.19958582520484924
train_iter_loss: 0.11439898610115051
train_iter_loss: 0.2112712413072586
train_iter_loss: 0.16939720511436462
train_iter_loss: 0.21248191595077515
train_iter_loss: 0.14516955614089966
train_iter_loss: 0.11649297922849655
train_iter_loss: 0.21559955179691315
train_iter_loss: 0.437219500541687
train_iter_loss: 0.1669544279575348
train_iter_loss: 0.17747342586517334
train_iter_loss: 0.12731023132801056
train_iter_loss: 0.2731485068798065
train_iter_loss: 0.09289833903312683
train_iter_loss: 0.08898177742958069
train_iter_loss: 0.2126835286617279
train_iter_loss: 0.14177419245243073
train_iter_loss: 0.1610897034406662
train_iter_loss: 0.14838220179080963
train_iter_loss: 0.12117549031972885
train_iter_loss: 0.18260514736175537
train_iter_loss: 0.1006774827837944
train_iter_loss: 0.08798061311244965
train loss :0.1662
---------------------
Validation seg loss: 0.22385565376014643 at epoch 245
epoch =    246/  1000, exp = train
train_iter_loss: 0.0920475646853447
train_iter_loss: 0.18234753608703613
train_iter_loss: 0.20886211097240448
train_iter_loss: 0.23300205171108246
train_iter_loss: 0.11960286647081375
train_iter_loss: 0.06607884913682938
train_iter_loss: 0.21492142975330353
train_iter_loss: 0.11642412096261978
train_iter_loss: 0.34122565388679504
train_iter_loss: 0.1332470178604126
train_iter_loss: 0.15675972402095795
train_iter_loss: 0.27155476808547974
train_iter_loss: 0.12493165582418442
train_iter_loss: 0.17999295890331268
train_iter_loss: 0.12352719157934189
train_iter_loss: 0.21157819032669067
train_iter_loss: 0.21244186162948608
train_iter_loss: 0.10946129262447357
train_iter_loss: 0.16607794165611267
train_iter_loss: 0.31857094168663025
train_iter_loss: 0.16695290803909302
train_iter_loss: 0.24358990788459778
train_iter_loss: 0.12805722653865814
train_iter_loss: 0.12310751527547836
train_iter_loss: 0.16929666697978973
train_iter_loss: 0.0544542632997036
train_iter_loss: 0.21905794739723206
train_iter_loss: 0.1745542287826538
train_iter_loss: 0.1314580887556076
train_iter_loss: 0.11774087697267532
train_iter_loss: 0.1451861709356308
train_iter_loss: 0.25985804200172424
train_iter_loss: 0.1392274796962738
train_iter_loss: 0.36773252487182617
train_iter_loss: 0.17449118196964264
train_iter_loss: 0.23452740907669067
train_iter_loss: 0.3200821578502655
train_iter_loss: 0.17483001947402954
train_iter_loss: 0.1464938372373581
train_iter_loss: 0.1899508833885193
train_iter_loss: 0.20686031877994537
train_iter_loss: 0.12115533649921417
train_iter_loss: 0.22258655726909637
train_iter_loss: 0.16905918717384338
train_iter_loss: 0.2265520840883255
train_iter_loss: 0.2017836719751358
train_iter_loss: 0.19072812795639038
train_iter_loss: 0.14254048466682434
train_iter_loss: 0.11676541715860367
train_iter_loss: 0.18647804856300354
train_iter_loss: 0.20908257365226746
train_iter_loss: 0.17875906825065613
train_iter_loss: 0.1400734931230545
train_iter_loss: 0.16143305599689484
train_iter_loss: 0.15547733008861542
train_iter_loss: 0.23451222479343414
train_iter_loss: 0.21665656566619873
train_iter_loss: 0.23683644831180573
train_iter_loss: 0.09785441309213638
train_iter_loss: 0.22594444453716278
train_iter_loss: 0.1647392064332962
train_iter_loss: 0.15192732214927673
train_iter_loss: 0.25859883427619934
train_iter_loss: 0.10514891147613525
train_iter_loss: 0.1085541695356369
train_iter_loss: 0.08203931152820587
train_iter_loss: 0.21169230341911316
train_iter_loss: 0.17064028978347778
train_iter_loss: 0.07339752465486526
train_iter_loss: 0.2220781296491623
train_iter_loss: 0.12334587424993515
train_iter_loss: 0.1942106932401657
train_iter_loss: 0.1928190290927887
train_iter_loss: 0.10958780348300934
train_iter_loss: 0.17620274424552917
train_iter_loss: 0.10808707773685455
train_iter_loss: 0.10688688606023788
train_iter_loss: 0.19846533238887787
train_iter_loss: 0.16404041647911072
train_iter_loss: 0.08322405815124512
train_iter_loss: 0.1552053689956665
train_iter_loss: 0.1667294204235077
train_iter_loss: 0.18668906390666962
train_iter_loss: 0.2718435823917389
train_iter_loss: 0.11426439136266708
train_iter_loss: 0.11032982170581818
train_iter_loss: 0.14970114827156067
train_iter_loss: 0.08743854612112045
train_iter_loss: 0.12117690593004227
train_iter_loss: 0.10582233220338821
train_iter_loss: 0.30557921528816223
train_iter_loss: 0.13101425766944885
train_iter_loss: 0.11638835072517395
train_iter_loss: 0.10247945785522461
train_iter_loss: 0.21431976556777954
train_iter_loss: 0.15159647166728973
train_iter_loss: 0.19885942339897156
train_iter_loss: 0.2557864785194397
train_iter_loss: 0.053259652107954025
train_iter_loss: 0.16172392666339874
train loss :0.1724
---------------------
Validation seg loss: 0.22128974260621756 at epoch 246
epoch =    247/  1000, exp = train
train_iter_loss: 0.22089853882789612
train_iter_loss: 0.2566209137439728
train_iter_loss: 0.14946527779102325
train_iter_loss: 0.1701022982597351
train_iter_loss: 0.13298837840557098
train_iter_loss: 0.14365606009960175
train_iter_loss: 0.1730499118566513
train_iter_loss: 0.17625926434993744
train_iter_loss: 0.20107822120189667
train_iter_loss: 0.21601970493793488
train_iter_loss: 0.1525341123342514
train_iter_loss: 0.2705443799495697
train_iter_loss: 0.2554433047771454
train_iter_loss: 0.2957547605037689
train_iter_loss: 0.09597624093294144
train_iter_loss: 0.07592421770095825
train_iter_loss: 0.175153911113739
train_iter_loss: 0.1125379428267479
train_iter_loss: 0.1428879201412201
train_iter_loss: 0.13400506973266602
train_iter_loss: 0.3514188230037689
train_iter_loss: 0.11594633013010025
train_iter_loss: 0.07902795821428299
train_iter_loss: 0.07963859289884567
train_iter_loss: 0.23311376571655273
train_iter_loss: 0.1836191713809967
train_iter_loss: 0.2231069803237915
train_iter_loss: 0.09728427231311798
train_iter_loss: 0.19826875627040863
train_iter_loss: 0.08114592730998993
train_iter_loss: 0.10168808698654175
train_iter_loss: 0.2387278825044632
train_iter_loss: 0.20023295283317566
train_iter_loss: 0.2229565531015396
train_iter_loss: 0.18689638376235962
train_iter_loss: 0.2998487651348114
train_iter_loss: 0.1514587551355362
train_iter_loss: 0.2182295173406601
train_iter_loss: 0.2474546581506729
train_iter_loss: 0.21128875017166138
train_iter_loss: 0.09211473166942596
train_iter_loss: 0.1912924349308014
train_iter_loss: 0.1800263524055481
train_iter_loss: 0.10808955132961273
train_iter_loss: 0.25179344415664673
train_iter_loss: 0.21404080092906952
train_iter_loss: 0.1510973870754242
train_iter_loss: 0.08336207270622253
train_iter_loss: 0.20138117671012878
train_iter_loss: 0.16512823104858398
train_iter_loss: 0.13738201558589935
train_iter_loss: 0.14578187465667725
train_iter_loss: 0.12991993129253387
train_iter_loss: 0.13276003301143646
train_iter_loss: 0.09805816411972046
train_iter_loss: 0.13490380346775055
train_iter_loss: 0.1892920583486557
train_iter_loss: 0.17212869226932526
train_iter_loss: 0.08684512227773666
train_iter_loss: 0.12115790694952011
train_iter_loss: 0.07250349223613739
train_iter_loss: 0.06190688908100128
train_iter_loss: 0.1944654881954193
train_iter_loss: 0.1841885894536972
train_iter_loss: 0.2105911523103714
train_iter_loss: 0.1281345933675766
train_iter_loss: 0.18429076671600342
train_iter_loss: 0.08246922492980957
train_iter_loss: 0.06274870783090591
train_iter_loss: 0.10251490771770477
train_iter_loss: 0.1801948994398117
train_iter_loss: 0.23597079515457153
train_iter_loss: 0.08395855128765106
train_iter_loss: 0.22184795141220093
train_iter_loss: 0.1307573765516281
train_iter_loss: 0.1302669495344162
train_iter_loss: 0.18430620431900024
train_iter_loss: 0.11001155525445938
train_iter_loss: 0.2313174307346344
train_iter_loss: 0.18271973729133606
train_iter_loss: 0.2568349540233612
train_iter_loss: 0.191641166806221
train_iter_loss: 0.2215082049369812
train_iter_loss: 0.1900976300239563
train_iter_loss: 0.13785947859287262
train_iter_loss: 0.24363574385643005
train_iter_loss: 0.14672327041625977
train_iter_loss: 0.1398472934961319
train_iter_loss: 0.05807102471590042
train_iter_loss: 0.18929794430732727
train_iter_loss: 0.1864752620458603
train_iter_loss: 0.24168036878108978
train_iter_loss: 0.1377083957195282
train_iter_loss: 0.19468215107917786
train_iter_loss: 0.23848789930343628
train_iter_loss: 0.14270740747451782
train_iter_loss: 0.13588663935661316
train_iter_loss: 0.16099371016025543
train_iter_loss: 0.2507888972759247
train_iter_loss: 0.11630368232727051
train loss :0.1699
---------------------
Validation seg loss: 0.2261982848867774 at epoch 247
epoch =    248/  1000, exp = train
train_iter_loss: 0.1772875189781189
train_iter_loss: 0.17732103168964386
train_iter_loss: 0.1249261125922203
train_iter_loss: 0.07331174612045288
train_iter_loss: 0.06812314689159393
train_iter_loss: 0.17811286449432373
train_iter_loss: 0.13625763356685638
train_iter_loss: 0.1937326341867447
train_iter_loss: 0.07893749326467514
train_iter_loss: 0.22344684600830078
train_iter_loss: 0.176513209939003
train_iter_loss: 0.13017147779464722
train_iter_loss: 0.16665323078632355
train_iter_loss: 0.13492439687252045
train_iter_loss: 0.1260496973991394
train_iter_loss: 0.11632471531629562
train_iter_loss: 0.07567469030618668
train_iter_loss: 0.07956628501415253
train_iter_loss: 0.07694314420223236
train_iter_loss: 0.1455470323562622
train_iter_loss: 0.1311342716217041
train_iter_loss: 0.12462744861841202
train_iter_loss: 0.18007487058639526
train_iter_loss: 0.2587625980377197
train_iter_loss: 0.11335283517837524
train_iter_loss: 0.20071181654930115
train_iter_loss: 0.18814502656459808
train_iter_loss: 0.11534971743822098
train_iter_loss: 0.24652054905891418
train_iter_loss: 0.20039959251880646
train_iter_loss: 0.15334440767765045
train_iter_loss: 0.19069573283195496
train_iter_loss: 0.15952783823013306
train_iter_loss: 0.11756609380245209
train_iter_loss: 0.24560546875
train_iter_loss: 0.13600146770477295
train_iter_loss: 0.10515707731246948
train_iter_loss: 0.1397232562303543
train_iter_loss: 0.2908310890197754
train_iter_loss: 0.13921336829662323
train_iter_loss: 0.14123846590518951
train_iter_loss: 0.11862430721521378
train_iter_loss: 0.19857099652290344
train_iter_loss: 0.18787024915218353
train_iter_loss: 0.3700363337993622
train_iter_loss: 0.14499658346176147
train_iter_loss: 0.1313934177160263
train_iter_loss: 0.1528029441833496
train_iter_loss: 0.1777835637331009
train_iter_loss: 0.17804944515228271
train_iter_loss: 0.1496734619140625
train_iter_loss: 0.17109599709510803
train_iter_loss: 0.23589517176151276
train_iter_loss: 0.21578259766101837
train_iter_loss: 0.06378152966499329
train_iter_loss: 0.20607906579971313
train_iter_loss: 0.16209259629249573
train_iter_loss: 0.11189063638448715
train_iter_loss: 0.15706807374954224
train_iter_loss: 0.1688288003206253
train_iter_loss: 0.059203799813985825
train_iter_loss: 0.13701596856117249
train_iter_loss: 0.29247426986694336
train_iter_loss: 0.18175861239433289
train_iter_loss: 0.21727672219276428
train_iter_loss: 0.09614942222833633
train_iter_loss: 0.1464749425649643
train_iter_loss: 0.15393346548080444
train_iter_loss: 0.18384172022342682
train_iter_loss: 0.24760723114013672
train_iter_loss: 0.07367853075265884
train_iter_loss: 0.16235406696796417
train_iter_loss: 0.24302488565444946
train_iter_loss: 0.25998854637145996
train_iter_loss: 0.11885008215904236
train_iter_loss: 0.06438471376895905
train_iter_loss: 0.21356453001499176
train_iter_loss: 0.06571304053068161
train_iter_loss: 0.213217630982399
train_iter_loss: 0.274717777967453
train_iter_loss: 0.36533597111701965
train_iter_loss: 0.1621820628643036
train_iter_loss: 0.1088193729519844
train_iter_loss: 0.15120814740657806
train_iter_loss: 0.12791705131530762
train_iter_loss: 0.1779622733592987
train_iter_loss: 0.10394366830587387
train_iter_loss: 0.31180906295776367
train_iter_loss: 0.35688385367393494
train_iter_loss: 0.3547312617301941
train_iter_loss: 0.2006773203611374
train_iter_loss: 0.10052493214607239
train_iter_loss: 0.21236319839954376
train_iter_loss: 0.2628454864025116
train_iter_loss: 0.1517757624387741
train_iter_loss: 0.28019437193870544
train_iter_loss: 0.11379418522119522
train_iter_loss: 0.13911789655685425
train_iter_loss: 0.09955427795648575
train_iter_loss: 0.23660622537136078
train loss :0.1713
---------------------
Validation seg loss: 0.22530199633911252 at epoch 248
epoch =    249/  1000, exp = train
train_iter_loss: 0.11419133096933365
train_iter_loss: 0.23477831482887268
train_iter_loss: 0.22695966064929962
train_iter_loss: 0.07338899374008179
train_iter_loss: 0.2357674241065979
train_iter_loss: 0.2016703337430954
train_iter_loss: 0.21743211150169373
train_iter_loss: 0.08408582955598831
train_iter_loss: 0.1889386922121048
train_iter_loss: 0.12856073677539825
train_iter_loss: 0.1569357067346573
train_iter_loss: 0.11070747673511505
train_iter_loss: 0.13015779852867126
train_iter_loss: 0.21986180543899536
train_iter_loss: 0.15303555130958557
train_iter_loss: 0.1739528775215149
train_iter_loss: 0.22550462186336517
train_iter_loss: 0.08526352792978287
train_iter_loss: 0.0462520457804203
train_iter_loss: 0.13029852509498596
train_iter_loss: 0.308231920003891
train_iter_loss: 0.13508331775665283
train_iter_loss: 0.13828571140766144
train_iter_loss: 0.17797665297985077
train_iter_loss: 0.23103918135166168
train_iter_loss: 0.1838608980178833
train_iter_loss: 0.14659211039543152
train_iter_loss: 0.1367645412683487
train_iter_loss: 0.20637147128582
train_iter_loss: 0.20362861454486847
train_iter_loss: 0.1218918040394783
train_iter_loss: 0.11211822181940079
train_iter_loss: 0.1403724104166031
train_iter_loss: 0.18689033389091492
train_iter_loss: 0.11324231326580048
train_iter_loss: 0.20487721264362335
train_iter_loss: 0.40973037481307983
train_iter_loss: 0.19873781502246857
train_iter_loss: 0.23983915150165558
train_iter_loss: 0.2206575721502304
train_iter_loss: 0.21206647157669067
train_iter_loss: 0.20771631598472595
train_iter_loss: 0.20567013323307037
train_iter_loss: 0.13579565286636353
train_iter_loss: 0.13655255734920502
train_iter_loss: 0.1696995347738266
train_iter_loss: 0.1154673621058464
train_iter_loss: 0.2312246561050415
train_iter_loss: 0.1282426416873932
train_iter_loss: 0.12955361604690552
train_iter_loss: 0.15631556510925293
train_iter_loss: 0.13012324273586273
train_iter_loss: 0.14943775534629822
train_iter_loss: 0.19066649675369263
train_iter_loss: 0.24888336658477783
train_iter_loss: 0.17563758790493011
train_iter_loss: 0.33863958716392517
train_iter_loss: 0.2747335135936737
train_iter_loss: 0.18605493009090424
train_iter_loss: 0.11906301975250244
train_iter_loss: 0.12730631232261658
train_iter_loss: 0.1957562118768692
train_iter_loss: 0.11131993681192398
train_iter_loss: 0.12810462713241577
train_iter_loss: 0.1989685446023941
train_iter_loss: 0.24782893061637878
train_iter_loss: 0.14326733350753784
train_iter_loss: 0.2535863220691681
train_iter_loss: 0.183586984872818
train_iter_loss: 0.1376921534538269
train_iter_loss: 0.11262089014053345
train_iter_loss: 0.0892363041639328
train_iter_loss: 0.3412129580974579
train_iter_loss: 0.24342720210552216
train_iter_loss: 0.18442736566066742
train_iter_loss: 0.15426380932331085
train_iter_loss: 0.1166679859161377
train_iter_loss: 0.1792721301317215
train_iter_loss: 0.1727948635816574
train_iter_loss: 0.11478827893733978
train_iter_loss: 0.12527777254581451
train_iter_loss: 0.1324874311685562
train_iter_loss: 0.106151282787323
train_iter_loss: 0.167854443192482
train_iter_loss: 0.18896672129631042
train_iter_loss: 0.1089676097035408
train_iter_loss: 0.14801332354545593
train_iter_loss: 0.28984972834587097
train_iter_loss: 0.1222439557313919
train_iter_loss: 0.24363109469413757
train_iter_loss: 0.17319706082344055
train_iter_loss: 0.20868420600891113
train_iter_loss: 0.07484236359596252
train_iter_loss: 0.1123233214020729
train_iter_loss: 0.2244555801153183
train_iter_loss: 0.15749478340148926
train_iter_loss: 0.23217631876468658
train_iter_loss: 0.15777122974395752
train_iter_loss: 0.15335243940353394
train_iter_loss: 0.14215894043445587
train loss :0.1747
---------------------
Validation seg loss: 0.2220548438084013 at epoch 249
epoch =    250/  1000, exp = train
train_iter_loss: 0.16351276636123657
train_iter_loss: 0.2346181571483612
train_iter_loss: 0.11542581766843796
train_iter_loss: 0.12001381069421768
train_iter_loss: 0.09069157391786575
train_iter_loss: 0.17533788084983826
train_iter_loss: 0.14214928448200226
train_iter_loss: 0.19005626440048218
train_iter_loss: 0.09964459389448166
train_iter_loss: 0.1715525984764099
train_iter_loss: 0.12917444109916687
train_iter_loss: 0.1655624508857727
train_iter_loss: 0.14560645818710327
train_iter_loss: 0.13644160330295563
train_iter_loss: 0.17138898372650146
train_iter_loss: 0.08798486739397049
train_iter_loss: 0.21305696666240692
train_iter_loss: 0.12073598057031631
train_iter_loss: 0.21741366386413574
train_iter_loss: 0.2550794780254364
train_iter_loss: 0.22007393836975098
train_iter_loss: 0.1363508403301239
train_iter_loss: 0.14267811179161072
train_iter_loss: 0.11776152998209
train_iter_loss: 0.19595016539096832
train_iter_loss: 0.059894733130931854
train_iter_loss: 0.16249579191207886
train_iter_loss: 0.18802084028720856
train_iter_loss: 0.09929511696100235
train_iter_loss: 0.2567422091960907
train_iter_loss: 0.46170809864997864
train_iter_loss: 0.20485056936740875
train_iter_loss: 0.18503978848457336
train_iter_loss: 0.0843745544552803
train_iter_loss: 0.13379855453968048
train_iter_loss: 0.13328973948955536
train_iter_loss: 0.20684942603111267
train_iter_loss: 0.0934441089630127
train_iter_loss: 0.1372787356376648
train_iter_loss: 0.13563832640647888
train_iter_loss: 0.07710271328687668
train_iter_loss: 0.14716775715351105
train_iter_loss: 0.3052240312099457
train_iter_loss: 0.17729468643665314
train_iter_loss: 0.12371259182691574
train_iter_loss: 0.12789630889892578
train_iter_loss: 0.19632762670516968
train_iter_loss: 0.3485974669456482
train_iter_loss: 0.2891753911972046
train_iter_loss: 0.13661101460456848
train_iter_loss: 0.17738203704357147
train_iter_loss: 0.2827710807323456
train_iter_loss: 0.08040633052587509
train_iter_loss: 0.09439346194267273
train_iter_loss: 0.19623762369155884
train_iter_loss: 0.1925794631242752
train_iter_loss: 0.12233249098062515
train_iter_loss: 0.13061285018920898
train_iter_loss: 0.17688623070716858
train_iter_loss: 0.11728837341070175
train_iter_loss: 0.2712562084197998
train_iter_loss: 0.1311974972486496
train_iter_loss: 0.11083485931158066
train_iter_loss: 0.2874283492565155
train_iter_loss: 0.23291626572608948
train_iter_loss: 0.18875941634178162
train_iter_loss: 0.1429913491010666
train_iter_loss: 0.14236488938331604
train_iter_loss: 0.18320050835609436
train_iter_loss: 0.11215478181838989
train_iter_loss: 0.09280101209878922
train_iter_loss: 0.18671317398548126
train_iter_loss: 0.23321551084518433
train_iter_loss: 0.1821255087852478
train_iter_loss: 0.3135785460472107
train_iter_loss: 0.17444957792758942
train_iter_loss: 0.31447237730026245
train_iter_loss: 0.078399658203125
train_iter_loss: 0.13838225603103638
train_iter_loss: 0.1962747573852539
train_iter_loss: 0.09523186087608337
train_iter_loss: 0.12428965419530869
train_iter_loss: 0.17416225373744965
train_iter_loss: 0.16064991056919098
train_iter_loss: 0.043836332857608795
train_iter_loss: 0.19774426519870758
train_iter_loss: 0.07061402499675751
train_iter_loss: 0.3302195072174072
train_iter_loss: 0.18015307188034058
train_iter_loss: 0.4541058838367462
train_iter_loss: 0.1315426379442215
train_iter_loss: 0.18606455624103546
train_iter_loss: 0.19392375648021698
train_iter_loss: 0.15223678946495056
train_iter_loss: 0.23265881836414337
train_iter_loss: 0.11762148886919022
train_iter_loss: 0.2840563952922821
train_iter_loss: 0.20355293154716492
train_iter_loss: 0.11501497030258179
train_iter_loss: 0.20230598747730255
train loss :0.1754
---------------------
Validation seg loss: 0.21850547749760016 at epoch 250
********************
best_val_epoch_loss:  0.21850547749760016
MODEL UPDATED
epoch =    251/  1000, exp = train
train_iter_loss: 0.2534931004047394
train_iter_loss: 0.12539061903953552
train_iter_loss: 0.16383205354213715
train_iter_loss: 0.15132524073123932
train_iter_loss: 0.21645963191986084
train_iter_loss: 0.17223505675792694
train_iter_loss: 0.13277645409107208
train_iter_loss: 0.3520911633968353
train_iter_loss: 0.16514764726161957
train_iter_loss: 0.13579720258712769
train_iter_loss: 0.04961508885025978
train_iter_loss: 0.13489960134029388
train_iter_loss: 0.1768253743648529
train_iter_loss: 0.05107796564698219
train_iter_loss: 0.08438663929700851
train_iter_loss: 0.2221914380788803
train_iter_loss: 0.16231438517570496
train_iter_loss: 0.19823893904685974
train_iter_loss: 0.1942889392375946
train_iter_loss: 0.14283598959445953
train_iter_loss: 0.20895616710186005
train_iter_loss: 0.13243523240089417
train_iter_loss: 0.19971305131912231
train_iter_loss: 0.1700601577758789
train_iter_loss: 0.1768779754638672
train_iter_loss: 0.0923999771475792
train_iter_loss: 0.16384710371494293
train_iter_loss: 0.1077519953250885
train_iter_loss: 0.2967981696128845
train_iter_loss: 0.1580679565668106
train_iter_loss: 0.17079667747020721
train_iter_loss: 0.10263925045728683
train_iter_loss: 0.1714734584093094
train_iter_loss: 0.12928135693073273
train_iter_loss: 0.26731589436531067
train_iter_loss: 0.09480385482311249
train_iter_loss: 0.1723126322031021
train_iter_loss: 0.13314300775527954
train_iter_loss: 0.23461556434631348
train_iter_loss: 0.15842702984809875
train_iter_loss: 0.1589057892560959
train_iter_loss: 0.23266594111919403
train_iter_loss: 0.26672792434692383
train_iter_loss: 0.2253691405057907
train_iter_loss: 0.21961787343025208
train_iter_loss: 0.05712919682264328
train_iter_loss: 0.21205805242061615
train_iter_loss: 0.17406456172466278
train_iter_loss: 0.13621902465820312
train_iter_loss: 0.09576302766799927
train_iter_loss: 0.1799503117799759
train_iter_loss: 0.1155007854104042
train_iter_loss: 0.06806940585374832
train_iter_loss: 0.30426591634750366
train_iter_loss: 0.09676124155521393
train_iter_loss: 0.20310857892036438
train_iter_loss: 0.09624394029378891
train_iter_loss: 0.23732435703277588
train_iter_loss: 0.2327505648136139
train_iter_loss: 0.07421655207872391
train_iter_loss: 0.20860309898853302
train_iter_loss: 0.16110767424106598
train_iter_loss: 0.07378052175045013
train_iter_loss: 0.19409377872943878
train_iter_loss: 0.10793529450893402
train_iter_loss: 0.25987645983695984
train_iter_loss: 0.14653268456459045
train_iter_loss: 0.20318058133125305
train_iter_loss: 0.17713642120361328
train_iter_loss: 0.2318069189786911
train_iter_loss: 0.18268738687038422
train_iter_loss: 0.0647624060511589
train_iter_loss: 0.20667849481105804
train_iter_loss: 0.10387053340673447
train_iter_loss: 0.2697313129901886
train_iter_loss: 0.16834822297096252
train_iter_loss: 0.180727019906044
train_iter_loss: 0.220815509557724
train_iter_loss: 0.11128910630941391
train_iter_loss: 0.15097151696681976
train_iter_loss: 0.19736889004707336
train_iter_loss: 0.14848093688488007
train_iter_loss: 0.07285135984420776
train_iter_loss: 0.1830950230360031
train_iter_loss: 0.16429509222507477
train_iter_loss: 0.17955166101455688
train_iter_loss: 0.22886529564857483
train_iter_loss: 0.08268007636070251
train_iter_loss: 0.21019905805587769
train_iter_loss: 0.2168554812669754
train_iter_loss: 0.10472717136144638
train_iter_loss: 0.18289445340633392
train_iter_loss: 0.13822685182094574
train_iter_loss: 0.0740852802991867
train_iter_loss: 0.4556141793727875
train_iter_loss: 0.0931510478258133
train_iter_loss: 0.3615072965621948
train_iter_loss: 0.10384488850831985
train_iter_loss: 0.1123942881822586
train_iter_loss: 0.10719412565231323
train loss :0.1703
---------------------
Validation seg loss: 0.2244461334029318 at epoch 251
epoch =    252/  1000, exp = train
train_iter_loss: 0.11126680672168732
train_iter_loss: 0.1902780830860138
train_iter_loss: 0.2624988853931427
train_iter_loss: 0.2260582000017166
train_iter_loss: 0.21739041805267334
train_iter_loss: 0.2097579687833786
train_iter_loss: 0.1387971043586731
train_iter_loss: 0.15970008075237274
train_iter_loss: 0.09022875130176544
train_iter_loss: 0.16876070201396942
train_iter_loss: 0.1956949383020401
train_iter_loss: 0.17113645374774933
train_iter_loss: 0.12852859497070312
train_iter_loss: 0.1761859953403473
train_iter_loss: 0.18388578295707703
train_iter_loss: 0.20117999613285065
train_iter_loss: 0.2721478044986725
train_iter_loss: 0.0898979976773262
train_iter_loss: 0.12023080885410309
train_iter_loss: 0.1851508915424347
train_iter_loss: 0.18270355463027954
train_iter_loss: 0.16340230405330658
train_iter_loss: 0.04717326909303665
train_iter_loss: 0.2881350815296173
train_iter_loss: 0.1114584431052208
train_iter_loss: 0.17949189245700836
train_iter_loss: 0.1690738946199417
train_iter_loss: 0.11009381711483002
train_iter_loss: 0.2642415165901184
train_iter_loss: 0.1723344624042511
train_iter_loss: 0.1607923060655594
train_iter_loss: 0.22761639952659607
train_iter_loss: 0.08940032869577408
train_iter_loss: 0.20358195900917053
train_iter_loss: 0.1482182741165161
train_iter_loss: 0.13601931929588318
train_iter_loss: 0.12116511166095734
train_iter_loss: 0.12432550638914108
train_iter_loss: 0.1983104646205902
train_iter_loss: 0.18359874188899994
train_iter_loss: 0.22228474915027618
train_iter_loss: 0.17482422292232513
train_iter_loss: 0.23061607778072357
train_iter_loss: 0.22883549332618713
train_iter_loss: 0.10964188724756241
train_iter_loss: 0.1039767786860466
train_iter_loss: 0.20932039618492126
train_iter_loss: 0.08006133139133453
train_iter_loss: 0.33365488052368164
train_iter_loss: 0.17537732422351837
train_iter_loss: 0.15178461372852325
train_iter_loss: 0.18828526139259338
train_iter_loss: 0.16090212762355804
train_iter_loss: 0.16635878384113312
train_iter_loss: 0.2895022928714752
train_iter_loss: 0.17219144105911255
train_iter_loss: 0.1992432177066803
train_iter_loss: 0.09805738180875778
train_iter_loss: 0.21621866524219513
train_iter_loss: 0.07302214205265045
train_iter_loss: 0.21310585737228394
train_iter_loss: 0.15367232263088226
train_iter_loss: 0.15386994183063507
train_iter_loss: 0.19807429611682892
train_iter_loss: 0.18859203159809113
train_iter_loss: 0.3533523678779602
train_iter_loss: 0.13311035931110382
train_iter_loss: 0.11955541372299194
train_iter_loss: 0.1435820311307907
train_iter_loss: 0.11027530580759048
train_iter_loss: 0.03801925107836723
train_iter_loss: 0.15537333488464355
train_iter_loss: 0.14433538913726807
train_iter_loss: 0.23604950308799744
train_iter_loss: 0.16505691409111023
train_iter_loss: 0.12411800026893616
train_iter_loss: 0.20586088299751282
train_iter_loss: 0.20141370594501495
train_iter_loss: 0.12227567285299301
train_iter_loss: 0.10620447993278503
train_iter_loss: 0.22820287942886353
train_iter_loss: 0.1567676067352295
train_iter_loss: 0.19655022025108337
train_iter_loss: 0.24335125088691711
train_iter_loss: 0.11542370915412903
train_iter_loss: 0.11130816489458084
train_iter_loss: 0.21877169609069824
train_iter_loss: 0.31545794010162354
train_iter_loss: 0.12413632869720459
train_iter_loss: 0.15230637788772583
train_iter_loss: 0.11539889872074127
train_iter_loss: 0.1603376865386963
train_iter_loss: 0.21703371405601501
train_iter_loss: 0.17538128793239594
train_iter_loss: 0.1921215057373047
train_iter_loss: 0.10949742794036865
train_iter_loss: 0.14667955040931702
train_iter_loss: 0.07275239378213882
train_iter_loss: 0.08960356563329697
train_iter_loss: 0.08833111822605133
train loss :0.1703
---------------------
Validation seg loss: 0.22200938859813898 at epoch 252
epoch =    253/  1000, exp = train
train_iter_loss: 0.08049709349870682
train_iter_loss: 0.060741741210222244
train_iter_loss: 0.11376111954450607
train_iter_loss: 0.09684199094772339
train_iter_loss: 0.12574629485607147
train_iter_loss: 0.36198702454566956
train_iter_loss: 0.29306256771087646
train_iter_loss: 0.20918959379196167
train_iter_loss: 0.08680453151464462
train_iter_loss: 0.11419842392206192
train_iter_loss: 0.11831529438495636
train_iter_loss: 0.15903419256210327
train_iter_loss: 0.12735652923583984
train_iter_loss: 0.3136794865131378
train_iter_loss: 0.1613088846206665
train_iter_loss: 0.13882693648338318
train_iter_loss: 0.16815857589244843
train_iter_loss: 0.1076594665646553
train_iter_loss: 0.1914452165365219
train_iter_loss: 0.2066916525363922
train_iter_loss: 0.11208198964595795
train_iter_loss: 0.09037119895219803
train_iter_loss: 0.22877736389636993
train_iter_loss: 0.1473248153924942
train_iter_loss: 0.21143165230751038
train_iter_loss: 0.21931946277618408
train_iter_loss: 0.17386017739772797
train_iter_loss: 0.1451086848974228
train_iter_loss: 0.2622767984867096
train_iter_loss: 0.25973233580589294
train_iter_loss: 0.1151614859700203
train_iter_loss: 0.14120961725711823
train_iter_loss: 0.2034902274608612
train_iter_loss: 0.09170307219028473
train_iter_loss: 0.07830440998077393
train_iter_loss: 0.12032417207956314
train_iter_loss: 0.0882323682308197
train_iter_loss: 0.39197900891304016
train_iter_loss: 0.1816987544298172
train_iter_loss: 0.1473197638988495
train_iter_loss: 0.20276124775409698
train_iter_loss: 0.12659546732902527
train_iter_loss: 0.11718583852052689
train_iter_loss: 0.17171382904052734
train_iter_loss: 0.1553483009338379
train_iter_loss: 0.29972749948501587
train_iter_loss: 0.2508516013622284
train_iter_loss: 0.38749071955680847
train_iter_loss: 0.2033374160528183
train_iter_loss: 0.14699772000312805
train_iter_loss: 0.14844103157520294
train_iter_loss: 0.16955845057964325
train_iter_loss: 0.17865286767482758
train_iter_loss: 0.19392818212509155
train_iter_loss: 0.16764888167381287
train_iter_loss: 0.18549023568630219
train_iter_loss: 0.11160384118556976
train_iter_loss: 0.10445831716060638
train_iter_loss: 0.09428040683269501
train_iter_loss: 0.2030714452266693
train_iter_loss: 0.1344456821680069
train_iter_loss: 0.2487567812204361
train_iter_loss: 0.14643660187721252
train_iter_loss: 0.16431035101413727
train_iter_loss: 0.176602303981781
train_iter_loss: 0.21981777250766754
train_iter_loss: 0.13872531056404114
train_iter_loss: 0.2144002765417099
train_iter_loss: 0.08914162218570709
train_iter_loss: 0.21040791273117065
train_iter_loss: 0.2547657787799835
train_iter_loss: 0.14267876744270325
train_iter_loss: 0.1675097495317459
train_iter_loss: 0.18466827273368835
train_iter_loss: 0.09633026272058487
train_iter_loss: 0.18129214644432068
train_iter_loss: 0.13095293939113617
train_iter_loss: 0.15462766587734222
train_iter_loss: 0.21488557755947113
train_iter_loss: 0.1484961211681366
train_iter_loss: 0.1514001190662384
train_iter_loss: 0.1797277331352234
train_iter_loss: 0.0932822972536087
train_iter_loss: 0.14393918216228485
train_iter_loss: 0.16968633234500885
train_iter_loss: 0.19059722125530243
train_iter_loss: 0.16509786248207092
train_iter_loss: 0.12955854833126068
train_iter_loss: 0.24159522354602814
train_iter_loss: 0.09054207056760788
train_iter_loss: 0.22469459474086761
train_iter_loss: 0.12645232677459717
train_iter_loss: 0.1396770477294922
train_iter_loss: 0.4126105308532715
train_iter_loss: 0.10034309327602386
train_iter_loss: 0.11298622190952301
train_iter_loss: 0.20576152205467224
train_iter_loss: 0.0741966962814331
train_iter_loss: 0.09917926788330078
train_iter_loss: 0.16102367639541626
train loss :0.1707
---------------------
Validation seg loss: 0.21995358841212573 at epoch 253
epoch =    254/  1000, exp = train
train_iter_loss: 0.28218507766723633
train_iter_loss: 0.20411387085914612
train_iter_loss: 0.1254449039697647
train_iter_loss: 0.10221505910158157
train_iter_loss: 0.25113731622695923
train_iter_loss: 0.17028287053108215
train_iter_loss: 0.177018940448761
train_iter_loss: 0.13366946578025818
train_iter_loss: 0.1479826420545578
train_iter_loss: 0.14861592650413513
train_iter_loss: 0.20674775540828705
train_iter_loss: 0.21185676753520966
train_iter_loss: 0.22043360769748688
train_iter_loss: 0.26481249928474426
train_iter_loss: 0.12164773046970367
train_iter_loss: 0.17229941487312317
train_iter_loss: 0.2521442174911499
train_iter_loss: 0.1750410497188568
train_iter_loss: 0.21507249772548676
train_iter_loss: 0.18899913132190704
train_iter_loss: 0.13496133685112
train_iter_loss: 0.2692553699016571
train_iter_loss: 0.06307520717382431
train_iter_loss: 0.17554672062397003
train_iter_loss: 0.07168664783239365
train_iter_loss: 0.1709611415863037
train_iter_loss: 0.18095088005065918
train_iter_loss: 0.1405285894870758
train_iter_loss: 0.23616021871566772
train_iter_loss: 0.1311805248260498
train_iter_loss: 0.10500706732273102
train_iter_loss: 0.11778594553470612
train_iter_loss: 0.18816901743412018
train_iter_loss: 0.19963501393795013
train_iter_loss: 0.12310617417097092
train_iter_loss: 0.40492865443229675
train_iter_loss: 0.10148092359304428
train_iter_loss: 0.18501810729503632
train_iter_loss: 0.198399156332016
train_iter_loss: 0.17017410695552826
train_iter_loss: 0.05406631901860237
train_iter_loss: 0.16516166925430298
train_iter_loss: 0.1520114541053772
train_iter_loss: 0.13503536581993103
train_iter_loss: 0.1676512211561203
train_iter_loss: 0.19753950834274292
train_iter_loss: 0.06270759552717209
train_iter_loss: 0.1491599678993225
train_iter_loss: 0.15254279971122742
train_iter_loss: 0.11295831948518753
train_iter_loss: 0.164340540766716
train_iter_loss: 0.16274380683898926
train_iter_loss: 0.21597841382026672
train_iter_loss: 0.13993380963802338
train_iter_loss: 0.21718525886535645
train_iter_loss: 0.11599742621183395
train_iter_loss: 0.12231019139289856
train_iter_loss: 0.1987280696630478
train_iter_loss: 0.303282231092453
train_iter_loss: 0.1890447586774826
train_iter_loss: 0.07783977687358856
train_iter_loss: 0.12622082233428955
train_iter_loss: 0.1656249761581421
train_iter_loss: 0.17929919064044952
train_iter_loss: 0.120172418653965
train_iter_loss: 0.13644415140151978
train_iter_loss: 0.12098964303731918
train_iter_loss: 0.2082827091217041
train_iter_loss: 0.15099473297595978
train_iter_loss: 0.18084324896335602
train_iter_loss: 0.08899962157011032
train_iter_loss: 0.13943883776664734
train_iter_loss: 0.22914205491542816
train_iter_loss: 0.21597138047218323
train_iter_loss: 0.13732466101646423
train_iter_loss: 0.20837195217609406
train_iter_loss: 0.12387192994356155
train_iter_loss: 0.20778919756412506
train_iter_loss: 0.11636199057102203
train_iter_loss: 0.23955078423023224
train_iter_loss: 0.15413418412208557
train_iter_loss: 0.12537485361099243
train_iter_loss: 0.13078157603740692
train_iter_loss: 0.061508238315582275
train_iter_loss: 0.18154248595237732
train_iter_loss: 0.20810729265213013
train_iter_loss: 0.1660163551568985
train_iter_loss: 0.19894959032535553
train_iter_loss: 0.1898040771484375
train_iter_loss: 0.08962633460760117
train_iter_loss: 0.2979300916194916
train_iter_loss: 0.29798075556755066
train_iter_loss: 0.14235082268714905
train_iter_loss: 0.17353269457817078
train_iter_loss: 0.1044628918170929
train_iter_loss: 0.10825441032648087
train_iter_loss: 0.06018636003136635
train_iter_loss: 0.0711117684841156
train_iter_loss: 0.23094111680984497
train_iter_loss: 0.10435323417186737
train loss :0.1676
---------------------
Validation seg loss: 0.22085990234858022 at epoch 254
epoch =    255/  1000, exp = train
train_iter_loss: 0.09584964066743851
train_iter_loss: 0.1695179045200348
train_iter_loss: 0.26531803607940674
train_iter_loss: 0.10516192764043808
train_iter_loss: 0.09736465662717819
train_iter_loss: 0.22346824407577515
train_iter_loss: 0.13764779269695282
train_iter_loss: 0.20794996619224548
train_iter_loss: 0.19141311943531036
train_iter_loss: 0.1821829080581665
train_iter_loss: 0.13300903141498566
train_iter_loss: 0.199013814330101
train_iter_loss: 0.10672836005687714
train_iter_loss: 0.10019823908805847
train_iter_loss: 0.09146266430616379
train_iter_loss: 0.1788012534379959
train_iter_loss: 0.10080796480178833
train_iter_loss: 0.12389785051345825
train_iter_loss: 0.16432785987854004
train_iter_loss: 0.18377915024757385
train_iter_loss: 0.1171666756272316
train_iter_loss: 0.1262279599905014
train_iter_loss: 0.184805765748024
train_iter_loss: 0.17935314774513245
train_iter_loss: 0.10575356334447861
train_iter_loss: 0.24761180579662323
train_iter_loss: 0.2926851212978363
train_iter_loss: 0.14487195014953613
train_iter_loss: 0.10125793516635895
train_iter_loss: 0.22245533764362335
train_iter_loss: 0.23045246303081512
train_iter_loss: 0.2976316511631012
train_iter_loss: 0.2674224376678467
train_iter_loss: 0.13155117630958557
train_iter_loss: 0.23738078773021698
train_iter_loss: 0.18762175738811493
train_iter_loss: 0.06227470934391022
train_iter_loss: 0.1733190268278122
train_iter_loss: 0.057579156011343
train_iter_loss: 0.12742194533348083
train_iter_loss: 0.13038469851016998
train_iter_loss: 0.14334283769130707
train_iter_loss: 0.1264919638633728
train_iter_loss: 0.20935888588428497
train_iter_loss: 0.26055261492729187
train_iter_loss: 0.12608110904693604
train_iter_loss: 0.141947403550148
train_iter_loss: 0.11880943179130554
train_iter_loss: 0.2897522449493408
train_iter_loss: 0.07929514348506927
train_iter_loss: 0.16571281850337982
train_iter_loss: 0.09084662795066833
train_iter_loss: 0.164401575922966
train_iter_loss: 0.3054969906806946
train_iter_loss: 0.18223007023334503
train_iter_loss: 0.19957837462425232
train_iter_loss: 0.16186495125293732
train_iter_loss: 0.13373517990112305
train_iter_loss: 0.06058141216635704
train_iter_loss: 0.1271217167377472
train_iter_loss: 0.18778403103351593
train_iter_loss: 0.1277492791414261
train_iter_loss: 0.26997581124305725
train_iter_loss: 0.08714225888252258
train_iter_loss: 0.2035055160522461
train_iter_loss: 0.2100585550069809
train_iter_loss: 0.28123578429222107
train_iter_loss: 0.12288923561573029
train_iter_loss: 0.2309577912092209
train_iter_loss: 0.10651321709156036
train_iter_loss: 0.20173723995685577
train_iter_loss: 0.26771584153175354
train_iter_loss: 0.0739908441901207
train_iter_loss: 0.09422342479228973
train_iter_loss: 0.2416602373123169
train_iter_loss: 0.18716786801815033
train_iter_loss: 0.15537606179714203
train_iter_loss: 0.1664639413356781
train_iter_loss: 0.18576422333717346
train_iter_loss: 0.103557288646698
train_iter_loss: 0.09232132136821747
train_iter_loss: 0.059470612555742264
train_iter_loss: 0.1458246111869812
train_iter_loss: 0.22313104569911957
train_iter_loss: 0.07170767337083817
train_iter_loss: 0.2807547152042389
train_iter_loss: 0.13283604383468628
train_iter_loss: 0.24041132628917694
train_iter_loss: 0.1252029538154602
train_iter_loss: 0.23365236818790436
train_iter_loss: 0.16639144718647003
train_iter_loss: 0.22523729503154755
train_iter_loss: 0.05142025277018547
train_iter_loss: 0.19168975949287415
train_iter_loss: 0.12429212033748627
train_iter_loss: 0.30209454894065857
train_iter_loss: 0.36271435022354126
train_iter_loss: 0.09291528910398483
train_iter_loss: 0.11893431842327118
train_iter_loss: 0.2623234987258911
train loss :0.1695
---------------------
Validation seg loss: 0.2216791021725479 at epoch 255
epoch =    256/  1000, exp = train
train_iter_loss: 0.2965993285179138
train_iter_loss: 0.11734379827976227
train_iter_loss: 0.15881314873695374
train_iter_loss: 0.06604472547769547
train_iter_loss: 0.15626464784145355
train_iter_loss: 0.2524678409099579
train_iter_loss: 0.2180674523115158
train_iter_loss: 0.2180566042661667
train_iter_loss: 0.20697219669818878
train_iter_loss: 0.11289852112531662
train_iter_loss: 0.13241207599639893
train_iter_loss: 0.10749875009059906
train_iter_loss: 0.10063332319259644
train_iter_loss: 0.1354842633008957
train_iter_loss: 0.15393759310245514
train_iter_loss: 0.1483178287744522
train_iter_loss: 0.18470492959022522
train_iter_loss: 0.25733575224876404
train_iter_loss: 0.11397257447242737
train_iter_loss: 0.2013162076473236
train_iter_loss: 0.14576564729213715
train_iter_loss: 0.21577884256839752
train_iter_loss: 0.23794037103652954
train_iter_loss: 0.16985762119293213
train_iter_loss: 0.15998531877994537
train_iter_loss: 0.12390969693660736
train_iter_loss: 0.12716679275035858
train_iter_loss: 0.13803435862064362
train_iter_loss: 0.19999070465564728
train_iter_loss: 0.16648836433887482
train_iter_loss: 0.14442625641822815
train_iter_loss: 0.16564063727855682
train_iter_loss: 0.13979068398475647
train_iter_loss: 0.17561209201812744
train_iter_loss: 0.13972480595111847
train_iter_loss: 0.04492713138461113
train_iter_loss: 0.15535113215446472
train_iter_loss: 0.09891775250434875
train_iter_loss: 0.12783083319664001
train_iter_loss: 0.19050157070159912
train_iter_loss: 0.12638622522354126
train_iter_loss: 0.15741467475891113
train_iter_loss: 0.15194247663021088
train_iter_loss: 0.17606352269649506
train_iter_loss: 0.21911804378032684
train_iter_loss: 0.18757151067256927
train_iter_loss: 0.30163902044296265
train_iter_loss: 0.3619633913040161
train_iter_loss: 0.11677393317222595
train_iter_loss: 0.06201089546084404
train_iter_loss: 0.18875646591186523
train_iter_loss: 0.20828162133693695
train_iter_loss: 0.17308151721954346
train_iter_loss: 0.2320394217967987
train_iter_loss: 0.20024773478507996
train_iter_loss: 0.09810752421617508
train_iter_loss: 0.21452529728412628
train_iter_loss: 0.21125496923923492
train_iter_loss: 0.22326457500457764
train_iter_loss: 0.2060389369726181
train_iter_loss: 0.29888680577278137
train_iter_loss: 0.1576499491930008
train_iter_loss: 0.1863102912902832
train_iter_loss: 0.15722548961639404
train_iter_loss: 0.1904544085264206
train_iter_loss: 0.18596456944942474
train_iter_loss: 0.13673405349254608
train_iter_loss: 0.1759260594844818
train_iter_loss: 0.13029734790325165
train_iter_loss: 0.19759558141231537
train_iter_loss: 0.2632599472999573
train_iter_loss: 0.2146354615688324
train_iter_loss: 0.18135955929756165
train_iter_loss: 0.14391572773456573
train_iter_loss: 0.20548313856124878
train_iter_loss: 0.14133091270923615
train_iter_loss: 0.16842614114284515
train_iter_loss: 0.10534105449914932
train_iter_loss: 0.10868635028600693
train_iter_loss: 0.23556667566299438
train_iter_loss: 0.10871462523937225
train_iter_loss: 0.26943129301071167
train_iter_loss: 0.11736330389976501
train_iter_loss: 0.140085369348526
train_iter_loss: 0.14376890659332275
train_iter_loss: 0.16752927005290985
train_iter_loss: 0.09294116497039795
train_iter_loss: 0.29328301548957825
train_iter_loss: 0.09168094396591187
train_iter_loss: 0.196438729763031
train_iter_loss: 0.2526453733444214
train_iter_loss: 0.11465682834386826
train_iter_loss: 0.15839555859565735
train_iter_loss: 0.14586369693279266
train_iter_loss: 0.18500369787216187
train_iter_loss: 0.1579189896583557
train_iter_loss: 0.036102309823036194
train_iter_loss: 0.2584362030029297
train_iter_loss: 0.17401885986328125
train_iter_loss: 0.09028929471969604
train loss :0.1717
---------------------
Validation seg loss: 0.2205199257136797 at epoch 256
epoch =    257/  1000, exp = train
train_iter_loss: 0.09832276403903961
train_iter_loss: 0.026529395952820778
train_iter_loss: 0.11300195008516312
train_iter_loss: 0.11766764521598816
train_iter_loss: 0.2131749391555786
train_iter_loss: 0.09361591190099716
train_iter_loss: 0.06085698679089546
train_iter_loss: 0.16621904075145721
train_iter_loss: 0.24488748610019684
train_iter_loss: 0.08462824672460556
train_iter_loss: 0.17174600064754486
train_iter_loss: 0.14201673865318298
train_iter_loss: 0.2850668132305145
train_iter_loss: 0.3094206750392914
train_iter_loss: 0.12908095121383667
train_iter_loss: 0.09484612941741943
train_iter_loss: 0.4850238561630249
train_iter_loss: 0.2688630521297455
train_iter_loss: 0.08300578594207764
train_iter_loss: 0.08788645267486572
train_iter_loss: 0.15421441197395325
train_iter_loss: 0.11382553726434708
train_iter_loss: 0.23613840341567993
train_iter_loss: 0.10142698884010315
train_iter_loss: 0.15458017587661743
train_iter_loss: 0.11585505306720734
train_iter_loss: 0.13462519645690918
train_iter_loss: 0.07431096583604813
train_iter_loss: 0.15324661135673523
train_iter_loss: 0.10748868435621262
train_iter_loss: 0.393419086933136
train_iter_loss: 0.13838806748390198
train_iter_loss: 0.1908886432647705
train_iter_loss: 0.2961583733558655
train_iter_loss: 0.16978535056114197
train_iter_loss: 0.1845514327287674
train_iter_loss: 0.15445683896541595
train_iter_loss: 0.11068312078714371
train_iter_loss: 0.15941406786441803
train_iter_loss: 0.10169445723295212
train_iter_loss: 0.4359489679336548
train_iter_loss: 0.1289166808128357
train_iter_loss: 0.21489721536636353
train_iter_loss: 0.20226630568504333
train_iter_loss: 0.16375960409641266
train_iter_loss: 0.171597421169281
train_iter_loss: 0.06842228770256042
train_iter_loss: 0.1709463745355606
train_iter_loss: 0.2021101862192154
train_iter_loss: 0.1550065279006958
train_iter_loss: 0.12558485567569733
train_iter_loss: 0.15976661443710327
train_iter_loss: 0.13312475383281708
train_iter_loss: 0.14478319883346558
train_iter_loss: 0.21647244691848755
train_iter_loss: 0.1237407997250557
train_iter_loss: 0.1975022554397583
train_iter_loss: 0.2331741750240326
train_iter_loss: 0.09274651110172272
train_iter_loss: 0.15136945247650146
train_iter_loss: 0.06556607782840729
train_iter_loss: 0.15425650775432587
train_iter_loss: 0.17122961580753326
train_iter_loss: 0.21123166382312775
train_iter_loss: 0.13629066944122314
train_iter_loss: 0.1420215517282486
train_iter_loss: 0.2215488851070404
train_iter_loss: 0.144236221909523
train_iter_loss: 0.19545742869377136
train_iter_loss: 0.16479399800300598
train_iter_loss: 0.18528474867343903
train_iter_loss: 0.18476669490337372
train_iter_loss: 0.17755454778671265
train_iter_loss: 0.17310626804828644
train_iter_loss: 0.17569230496883392
train_iter_loss: 0.14274835586547852
train_iter_loss: 0.17540007829666138
train_iter_loss: 0.2121693342924118
train_iter_loss: 0.18101558089256287
train_iter_loss: 0.19327475130558014
train_iter_loss: 0.16713719069957733
train_iter_loss: 0.14127740263938904
train_iter_loss: 0.0991239994764328
train_iter_loss: 0.21008354425430298
train_iter_loss: 0.2097100466489792
train_iter_loss: 0.10100934654474258
train_iter_loss: 0.18283818662166595
train_iter_loss: 0.07050899416208267
train_iter_loss: 0.16038638353347778
train_iter_loss: 0.1587936133146286
train_iter_loss: 0.11452800035476685
train_iter_loss: 0.1570242941379547
train_iter_loss: 0.2743179202079773
train_iter_loss: 0.1605326533317566
train_iter_loss: 0.19490689039230347
train_iter_loss: 0.19109494984149933
train_iter_loss: 0.08145230263471603
train_iter_loss: 0.1332685649394989
train_iter_loss: 0.09853457659482956
train_iter_loss: 0.21753636002540588
train loss :0.1672
---------------------
Validation seg loss: 0.2215897145083631 at epoch 257
epoch =    258/  1000, exp = train
train_iter_loss: 0.16163060069084167
train_iter_loss: 0.2570148706436157
train_iter_loss: 0.15291263163089752
train_iter_loss: 0.22348858416080475
train_iter_loss: 0.17797556519508362
train_iter_loss: 0.06622521579265594
train_iter_loss: 0.16330310702323914
train_iter_loss: 0.16038207709789276
train_iter_loss: 0.17739038169384003
train_iter_loss: 0.15097354352474213
train_iter_loss: 0.15462683141231537
train_iter_loss: 0.1606157422065735
train_iter_loss: 0.13429972529411316
train_iter_loss: 0.1269322633743286
train_iter_loss: 0.09995442628860474
train_iter_loss: 0.3607885241508484
train_iter_loss: 0.28268203139305115
train_iter_loss: 0.1599392145872116
train_iter_loss: 0.1460179090499878
train_iter_loss: 0.15954317152500153
train_iter_loss: 0.2063591331243515
train_iter_loss: 0.1476978212594986
train_iter_loss: 0.3371604382991791
train_iter_loss: 0.20756737887859344
train_iter_loss: 0.20739558339118958
train_iter_loss: 0.2050478756427765
train_iter_loss: 0.12463798373937607
train_iter_loss: 0.13867813348770142
train_iter_loss: 0.12505000829696655
train_iter_loss: 0.19368331134319305
train_iter_loss: 0.0960431769490242
train_iter_loss: 0.14538072049617767
train_iter_loss: 0.12854927778244019
train_iter_loss: 0.05535260587930679
train_iter_loss: 0.10526334494352341
train_iter_loss: 0.09986762702465057
train_iter_loss: 0.1847074329853058
train_iter_loss: 0.1397349238395691
train_iter_loss: 0.07184307277202606
train_iter_loss: 0.16775073111057281
train_iter_loss: 0.2754456400871277
train_iter_loss: 0.1151433140039444
train_iter_loss: 0.11917153000831604
train_iter_loss: 0.27262040972709656
train_iter_loss: 0.18159843981266022
train_iter_loss: 0.14735804498195648
train_iter_loss: 0.0676499754190445
train_iter_loss: 0.1120779886841774
train_iter_loss: 0.1544388234615326
train_iter_loss: 0.13225533068180084
train_iter_loss: 0.11342516541481018
train_iter_loss: 0.1274343729019165
train_iter_loss: 0.15542954206466675
train_iter_loss: 0.17274785041809082
train_iter_loss: 0.2369939088821411
train_iter_loss: 0.19703808426856995
train_iter_loss: 0.212729811668396
train_iter_loss: 0.2521115839481354
train_iter_loss: 0.29064562916755676
train_iter_loss: 0.22755029797554016
train_iter_loss: 0.1838681995868683
train_iter_loss: 0.23788879811763763
train_iter_loss: 0.1223958358168602
train_iter_loss: 0.1895344853401184
train_iter_loss: 0.12578272819519043
train_iter_loss: 0.12997789680957794
train_iter_loss: 0.17715106904506683
train_iter_loss: 0.09109534323215485
train_iter_loss: 0.12083341181278229
train_iter_loss: 0.1465418040752411
train_iter_loss: 0.15456943213939667
train_iter_loss: 0.15689608454704285
train_iter_loss: 0.11704414337873459
train_iter_loss: 0.1588210016489029
train_iter_loss: 0.14118346571922302
train_iter_loss: 0.07675272226333618
train_iter_loss: 0.09193297475576401
train_iter_loss: 0.2616339921951294
train_iter_loss: 0.09603198617696762
train_iter_loss: 0.17655444145202637
train_iter_loss: 0.15219710767269135
train_iter_loss: 0.11972168833017349
train_iter_loss: 0.1528472751379013
train_iter_loss: 0.2584143877029419
train_iter_loss: 0.189936101436615
train_iter_loss: 0.05805489793419838
train_iter_loss: 0.24724146723747253
train_iter_loss: 0.14258860051631927
train_iter_loss: 0.22520194947719574
train_iter_loss: 0.2642575800418854
train_iter_loss: 0.0718076080083847
train_iter_loss: 0.1503182053565979
train_iter_loss: 0.16480720043182373
train_iter_loss: 0.1539459526538849
train_iter_loss: 0.12006610631942749
train_iter_loss: 0.3128162622451782
train_iter_loss: 0.25826603174209595
train_iter_loss: 0.10397080332040787
train_iter_loss: 0.20051653683185577
train_iter_loss: 0.08229148387908936
train loss :0.1668
---------------------
Validation seg loss: 0.22425778449144004 at epoch 258
epoch =    259/  1000, exp = train
train_iter_loss: 0.2364930659532547
train_iter_loss: 0.1519073247909546
train_iter_loss: 0.13583199679851532
train_iter_loss: 0.215094193816185
train_iter_loss: 0.17829594016075134
train_iter_loss: 0.1617731750011444
train_iter_loss: 0.14449535310268402
train_iter_loss: 0.1748359054327011
train_iter_loss: 0.19814938306808472
train_iter_loss: 0.10109734535217285
train_iter_loss: 0.18806473910808563
train_iter_loss: 0.062451738864183426
train_iter_loss: 0.10863669216632843
train_iter_loss: 0.1850135773420334
train_iter_loss: 0.2658066153526306
train_iter_loss: 0.15665142238140106
train_iter_loss: 0.3327769637107849
train_iter_loss: 0.1458095759153366
train_iter_loss: 0.06978346407413483
train_iter_loss: 0.08351151645183563
train_iter_loss: 0.16311441361904144
train_iter_loss: 0.11353668570518494
train_iter_loss: 0.17501918971538544
train_iter_loss: 0.1660369634628296
train_iter_loss: 0.17150498926639557
train_iter_loss: 0.15034134685993195
train_iter_loss: 0.12883055210113525
train_iter_loss: 0.47163140773773193
train_iter_loss: 0.19404655694961548
train_iter_loss: 0.17583438754081726
train_iter_loss: 0.12370184808969498
train_iter_loss: 0.1235581785440445
train_iter_loss: 0.2402869611978531
train_iter_loss: 0.14802347123622894
train_iter_loss: 0.10256882756948471
train_iter_loss: 0.10831707715988159
train_iter_loss: 0.1779090315103531
train_iter_loss: 0.12513193488121033
train_iter_loss: 0.13937295973300934
train_iter_loss: 0.17159536480903625
train_iter_loss: 0.13986410200595856
train_iter_loss: 0.2720291316509247
train_iter_loss: 0.18679942190647125
train_iter_loss: 0.16134892404079437
train_iter_loss: 0.0925569161772728
train_iter_loss: 0.09781362116336823
train_iter_loss: 0.20527899265289307
train_iter_loss: 0.06746764481067657
train_iter_loss: 0.17789465188980103
train_iter_loss: 0.11571143567562103
train_iter_loss: 0.4001240134239197
train_iter_loss: 0.2561355531215668
train_iter_loss: 0.13307207822799683
train_iter_loss: 0.17152072489261627
train_iter_loss: 0.18523192405700684
train_iter_loss: 0.11956744641065598
train_iter_loss: 0.12632934749126434
train_iter_loss: 0.12630298733711243
train_iter_loss: 0.18584157526493073
train_iter_loss: 0.08006105571985245
train_iter_loss: 0.1349114626646042
train_iter_loss: 0.22173793613910675
train_iter_loss: 0.21789704263210297
train_iter_loss: 0.12706762552261353
train_iter_loss: 0.11931943148374557
train_iter_loss: 0.22901134192943573
train_iter_loss: 0.10037235170602798
train_iter_loss: 0.16505616903305054
train_iter_loss: 0.1417684257030487
train_iter_loss: 0.17990224063396454
train_iter_loss: 0.13066242635250092
train_iter_loss: 0.08658125251531601
train_iter_loss: 0.206773579120636
train_iter_loss: 0.23835714161396027
train_iter_loss: 0.027421509847044945
train_iter_loss: 0.16603675484657288
train_iter_loss: 0.10034232586622238
train_iter_loss: 0.19031430780887604
train_iter_loss: 0.16804741322994232
train_iter_loss: 0.265758216381073
train_iter_loss: 0.283133864402771
train_iter_loss: 0.17180201411247253
train_iter_loss: 0.09767250716686249
train_iter_loss: 0.11770904064178467
train_iter_loss: 0.03792940825223923
train_iter_loss: 0.19142067432403564
train_iter_loss: 0.17694391310214996
train_iter_loss: 0.1794443130493164
train_iter_loss: 0.13887235522270203
train_iter_loss: 0.11822646856307983
train_iter_loss: 0.19366337358951569
train_iter_loss: 0.15837928652763367
train_iter_loss: 0.12443872541189194
train_iter_loss: 0.24356846511363983
train_iter_loss: 0.024801481515169144
train_iter_loss: 0.1630311757326126
train_iter_loss: 0.09927898645401001
train_iter_loss: 0.14938262104988098
train_iter_loss: 0.23456235229969025
train_iter_loss: 0.3074339032173157
train loss :0.1656
---------------------
Validation seg loss: 0.22308877174619515 at epoch 259
epoch =    260/  1000, exp = train
train_iter_loss: 0.1513180136680603
train_iter_loss: 0.1420157104730606
train_iter_loss: 0.19081398844718933
train_iter_loss: 0.13356442749500275
train_iter_loss: 0.1466609537601471
train_iter_loss: 0.1581123173236847
train_iter_loss: 0.21744830906391144
train_iter_loss: 0.1527174562215805
train_iter_loss: 0.07652724534273148
train_iter_loss: 0.11243129521608353
train_iter_loss: 0.13542680442333221
train_iter_loss: 0.04757331311702728
train_iter_loss: 0.13665685057640076
train_iter_loss: 0.18757475912570953
train_iter_loss: 0.25119906663894653
train_iter_loss: 0.26502785086631775
train_iter_loss: 0.29273614287376404
train_iter_loss: 0.15650857985019684
train_iter_loss: 0.15533357858657837
train_iter_loss: 0.20645812153816223
train_iter_loss: 0.17722944915294647
train_iter_loss: 0.2817765474319458
train_iter_loss: 0.1681412160396576
train_iter_loss: 0.12475136667490005
train_iter_loss: 0.11720869690179825
train_iter_loss: 0.18628449738025665
train_iter_loss: 0.2547171413898468
train_iter_loss: 0.16814658045768738
train_iter_loss: 0.19796261191368103
train_iter_loss: 0.1794201284646988
train_iter_loss: 0.10911031812429428
train_iter_loss: 0.1180264949798584
train_iter_loss: 0.1485457569360733
train_iter_loss: 0.19552728533744812
train_iter_loss: 0.23124413192272186
train_iter_loss: 0.2932453155517578
train_iter_loss: 0.2039213329553604
train_iter_loss: 0.18294952809810638
train_iter_loss: 0.1543693095445633
train_iter_loss: 0.20660392940044403
train_iter_loss: 0.040059637278318405
train_iter_loss: 0.07010458409786224
train_iter_loss: 0.24129736423492432
train_iter_loss: 0.1485671103000641
train_iter_loss: 0.13158659636974335
train_iter_loss: 0.18112300336360931
train_iter_loss: 0.15677450597286224
train_iter_loss: 0.17533782124519348
train_iter_loss: 0.16279131174087524
train_iter_loss: 0.13651442527770996
train_iter_loss: 0.06233815476298332
train_iter_loss: 0.08828890323638916
train_iter_loss: 0.19553142786026
train_iter_loss: 0.2277165800333023
train_iter_loss: 0.13879792392253876
train_iter_loss: 0.12066545337438583
train_iter_loss: 0.10967860370874405
train_iter_loss: 0.19098730385303497
train_iter_loss: 0.14773142337799072
train_iter_loss: 0.1735110878944397
train_iter_loss: 0.3047865629196167
train_iter_loss: 0.29570966958999634
train_iter_loss: 0.1640906035900116
train_iter_loss: 0.047755833715200424
train_iter_loss: 0.18203632533550262
train_iter_loss: 0.11563095450401306
train_iter_loss: 0.1982123702764511
train_iter_loss: 0.21083086729049683
train_iter_loss: 0.16413716971874237
train_iter_loss: 0.2038714587688446
train_iter_loss: 0.22811730206012726
train_iter_loss: 0.11100682616233826
train_iter_loss: 0.08761801570653915
train_iter_loss: 0.10695118457078934
train_iter_loss: 0.13838642835617065
train_iter_loss: 0.15996119379997253
train_iter_loss: 0.13878333568572998
train_iter_loss: 0.13542428612709045
train_iter_loss: 0.16430127620697021
train_iter_loss: 0.08193693310022354
train_iter_loss: 0.13477882742881775
train_iter_loss: 0.1975134015083313
train_iter_loss: 0.16827347874641418
train_iter_loss: 0.09849103540182114
train_iter_loss: 0.14979711174964905
train_iter_loss: 0.19395771622657776
train_iter_loss: 0.15646620094776154
train_iter_loss: 0.18280188739299774
train_iter_loss: 0.13045184314250946
train_iter_loss: 0.15657514333724976
train_iter_loss: 0.12477187067270279
train_iter_loss: 0.24519987404346466
train_iter_loss: 0.2420286238193512
train_iter_loss: 0.2990519404411316
train_iter_loss: 0.20064377784729004
train_iter_loss: 0.1394929587841034
train_iter_loss: 0.20528315007686615
train_iter_loss: 0.1441124677658081
train_iter_loss: 0.12612755596637726
train_iter_loss: 0.17710334062576294
train loss :0.1679
---------------------
Validation seg loss: 0.2270287111278553 at epoch 260
epoch =    261/  1000, exp = train
train_iter_loss: 0.1630590260028839
train_iter_loss: 0.18082906305789948
train_iter_loss: 0.2102532684803009
train_iter_loss: 0.07646370679140091
train_iter_loss: 0.12264952063560486
train_iter_loss: 0.15809613466262817
train_iter_loss: 0.1303088366985321
train_iter_loss: 0.20902474224567413
train_iter_loss: 0.1302565485239029
train_iter_loss: 0.12339042872190475
train_iter_loss: 0.08829665929079056
train_iter_loss: 0.22518286108970642
train_iter_loss: 0.1776275783777237
train_iter_loss: 0.07788236439228058
train_iter_loss: 0.2626784145832062
train_iter_loss: 0.2571278214454651
train_iter_loss: 0.17412540316581726
train_iter_loss: 0.11886458843946457
train_iter_loss: 0.13713200390338898
train_iter_loss: 0.09509354829788208
train_iter_loss: 0.23419027030467987
train_iter_loss: 0.13614124059677124
train_iter_loss: 0.1631963849067688
train_iter_loss: 0.17254520952701569
train_iter_loss: 0.15180760622024536
train_iter_loss: 0.2555292546749115
train_iter_loss: 0.2546450197696686
train_iter_loss: 0.11838949471712112
train_iter_loss: 0.2444494515657425
train_iter_loss: 0.07001249492168427
train_iter_loss: 0.2023303508758545
train_iter_loss: 0.20088335871696472
train_iter_loss: 0.2004011571407318
train_iter_loss: 0.10092334449291229
train_iter_loss: 0.23427040874958038
train_iter_loss: 0.21844717860221863
train_iter_loss: 0.1379089206457138
train_iter_loss: 0.1187225952744484
train_iter_loss: 0.2607174217700958
train_iter_loss: 0.1124468445777893
train_iter_loss: 0.20584705471992493
train_iter_loss: 0.15107162296772003
train_iter_loss: 0.08936280757188797
train_iter_loss: 0.10900786519050598
train_iter_loss: 0.16876034438610077
train_iter_loss: 0.22006891667842865
train_iter_loss: 0.2374117374420166
train_iter_loss: 0.15047261118888855
train_iter_loss: 0.06315166503190994
train_iter_loss: 0.14527639746665955
train_iter_loss: 0.10947077721357346
train_iter_loss: 0.1431363821029663
train_iter_loss: 0.10282651335000992
train_iter_loss: 0.29123228788375854
train_iter_loss: 0.17296543717384338
train_iter_loss: 0.15278318524360657
train_iter_loss: 0.1025233194231987
train_iter_loss: 0.20700161159038544
train_iter_loss: 0.15580590069293976
train_iter_loss: 0.26052239537239075
train_iter_loss: 0.08464086800813675
train_iter_loss: 0.19771268963813782
train_iter_loss: 0.13508768379688263
train_iter_loss: 0.08217326551675797
train_iter_loss: 0.22307533025741577
train_iter_loss: 0.24479065835475922
train_iter_loss: 0.15328457951545715
train_iter_loss: 0.06937752664089203
train_iter_loss: 0.11280686408281326
train_iter_loss: 0.17628613114356995
train_iter_loss: 0.2422620952129364
train_iter_loss: 0.09427213668823242
train_iter_loss: 0.16652369499206543
train_iter_loss: 0.12673695385456085
train_iter_loss: 0.15337832272052765
train_iter_loss: 0.30780231952667236
train_iter_loss: 0.10004887729883194
train_iter_loss: 0.16881585121154785
train_iter_loss: 0.32040852308273315
train_iter_loss: 0.07699878513813019
train_iter_loss: 0.17740459740161896
train_iter_loss: 0.11012043058872223
train_iter_loss: 0.21903827786445618
train_iter_loss: 0.11396978795528412
train_iter_loss: 0.272615522146225
train_iter_loss: 0.19154317677021027
train_iter_loss: 0.16701802611351013
train_iter_loss: 0.17530778050422668
train_iter_loss: 0.15778093039989471
train_iter_loss: 0.1471295803785324
train_iter_loss: 0.19651691615581512
train_iter_loss: 0.21366260945796967
train_iter_loss: 0.21115487813949585
train_iter_loss: 0.1497412472963333
train_iter_loss: 0.15261343121528625
train_iter_loss: 0.24207548797130585
train_iter_loss: 0.12832430005073547
train_iter_loss: 0.16522134840488434
train_iter_loss: 0.1378536969423294
train_iter_loss: 0.20967678725719452
train loss :0.1689
---------------------
Validation seg loss: 0.22286571802149685 at epoch 261
epoch =    262/  1000, exp = train
train_iter_loss: 0.08951956033706665
train_iter_loss: 0.15580888092517853
train_iter_loss: 0.2396642118692398
train_iter_loss: 0.20186910033226013
train_iter_loss: 0.18308565020561218
train_iter_loss: 0.08614418655633926
train_iter_loss: 0.16791686415672302
train_iter_loss: 0.13064688444137573
train_iter_loss: 0.17682096362113953
train_iter_loss: 0.15132835507392883
train_iter_loss: 0.16989640891551971
train_iter_loss: 0.09420523792505264
train_iter_loss: 0.12370127439498901
train_iter_loss: 0.1759784072637558
train_iter_loss: 0.23052801191806793
train_iter_loss: 0.10837322473526001
train_iter_loss: 0.14542673528194427
train_iter_loss: 0.1364772468805313
train_iter_loss: 0.1957635134458542
train_iter_loss: 0.14092212915420532
train_iter_loss: 0.19585064053535461
train_iter_loss: 0.19102269411087036
train_iter_loss: 0.2438686490058899
train_iter_loss: 0.29154741764068604
train_iter_loss: 0.16096463799476624
train_iter_loss: 0.13145411014556885
train_iter_loss: 0.09531482309103012
train_iter_loss: 0.26204541325569153
train_iter_loss: 0.13558977842330933
train_iter_loss: 0.09871544688940048
train_iter_loss: 0.07202882319688797
train_iter_loss: 0.13707543909549713
train_iter_loss: 0.0786946639418602
train_iter_loss: 0.12476368248462677
train_iter_loss: 0.2024618536233902
train_iter_loss: 0.18787260353565216
train_iter_loss: 0.12367764860391617
train_iter_loss: 0.1038961261510849
train_iter_loss: 0.24483750760555267
train_iter_loss: 0.1701018363237381
train_iter_loss: 0.154231995344162
train_iter_loss: 0.10503781586885452
train_iter_loss: 0.2734508514404297
train_iter_loss: 0.1453525424003601
train_iter_loss: 0.10335154831409454
train_iter_loss: 0.10645312815904617
train_iter_loss: 0.10699358582496643
train_iter_loss: 0.08491578698158264
train_iter_loss: 0.17721439898014069
train_iter_loss: 0.17911376059055328
train_iter_loss: 0.32863566279411316
train_iter_loss: 0.10470430552959442
train_iter_loss: 0.252862811088562
train_iter_loss: 0.1372961401939392
train_iter_loss: 0.1631793975830078
train_iter_loss: 0.3047666549682617
train_iter_loss: 0.16925834119319916
train_iter_loss: 0.23105326294898987
train_iter_loss: 0.24589036405086517
train_iter_loss: 0.11369674652814865
train_iter_loss: 0.24064259231090546
train_iter_loss: 0.17917749285697937
train_iter_loss: 0.2130044847726822
train_iter_loss: 0.20121966302394867
train_iter_loss: 0.07021759450435638
train_iter_loss: 0.16917970776557922
train_iter_loss: 0.2475127875804901
train_iter_loss: 0.19144922494888306
train_iter_loss: 0.11562451720237732
train_iter_loss: 0.22000837326049805
train_iter_loss: 0.144413024187088
train_iter_loss: 0.1916174739599228
train_iter_loss: 0.14011409878730774
train_iter_loss: 0.17748643457889557
train_iter_loss: 0.07864746451377869
train_iter_loss: 0.15702757239341736
train_iter_loss: 0.18759393692016602
train_iter_loss: 0.31617408990859985
train_iter_loss: 0.09442069381475449
train_iter_loss: 0.2603433132171631
train_iter_loss: 0.2286883294582367
train_iter_loss: 0.2145618349313736
train_iter_loss: 0.1531328707933426
train_iter_loss: 0.26500123739242554
train_iter_loss: 0.20780280232429504
train_iter_loss: 0.0842011570930481
train_iter_loss: 0.1901623159646988
train_iter_loss: 0.16187098622322083
train_iter_loss: 0.052197545766830444
train_iter_loss: 0.08492130041122437
train_iter_loss: 0.22750622034072876
train_iter_loss: 0.17889967560768127
train_iter_loss: 0.10866305232048035
train_iter_loss: 0.14699645340442657
train_iter_loss: 0.19540677964687347
train_iter_loss: 0.22637690603733063
train_iter_loss: 0.22700288891792297
train_iter_loss: 0.06594908982515335
train_iter_loss: 0.18435725569725037
train_iter_loss: 0.19293685257434845
train loss :0.1700
---------------------
Validation seg loss: 0.2191183197593211 at epoch 262
epoch =    263/  1000, exp = train
train_iter_loss: 0.23590458929538727
train_iter_loss: 0.1463269591331482
train_iter_loss: 0.2281382828950882
train_iter_loss: 0.21060480177402496
train_iter_loss: 0.11857429146766663
train_iter_loss: 0.1863592565059662
train_iter_loss: 0.1208886206150055
train_iter_loss: 0.1264605075120926
train_iter_loss: 0.2655078172683716
train_iter_loss: 0.1848594695329666
train_iter_loss: 0.07920022308826447
train_iter_loss: 0.10560303926467896
train_iter_loss: 0.5813552737236023
train_iter_loss: 0.19376032054424286
train_iter_loss: 0.10749920457601547
train_iter_loss: 0.15845277905464172
train_iter_loss: 0.04916318506002426
train_iter_loss: 0.24519450962543488
train_iter_loss: 0.26297232508659363
train_iter_loss: 0.28407731652259827
train_iter_loss: 0.33696556091308594
train_iter_loss: 0.2124374806880951
train_iter_loss: 0.1458864063024521
train_iter_loss: 0.2090698778629303
train_iter_loss: 0.16114918887615204
train_iter_loss: 0.17628946900367737
train_iter_loss: 0.183144211769104
train_iter_loss: 0.15438641607761383
train_iter_loss: 0.15832944214344025
train_iter_loss: 0.21847189962863922
train_iter_loss: 0.1416628211736679
train_iter_loss: 0.14065808057785034
train_iter_loss: 0.14003866910934448
train_iter_loss: 0.15697234869003296
train_iter_loss: 0.20247763395309448
train_iter_loss: 0.12476789206266403
train_iter_loss: 0.25934338569641113
train_iter_loss: 0.16718879342079163
train_iter_loss: 0.24552719295024872
train_iter_loss: 0.13546864688396454
train_iter_loss: 0.15596440434455872
train_iter_loss: 0.17770875990390778
train_iter_loss: 0.11814858019351959
train_iter_loss: 0.20795726776123047
train_iter_loss: 0.18865740299224854
train_iter_loss: 0.20413322746753693
train_iter_loss: 0.05839075148105621
train_iter_loss: 0.13356903195381165
train_iter_loss: 0.10723157227039337
train_iter_loss: 0.10085795819759369
train_iter_loss: 0.18048332631587982
train_iter_loss: 0.08581289649009705
train_iter_loss: 0.13495184481143951
train_iter_loss: 0.222189262509346
train_iter_loss: 0.1156458631157875
train_iter_loss: 0.18290534615516663
train_iter_loss: 0.15022292733192444
train_iter_loss: 0.08593108505010605
train_iter_loss: 0.24534855782985687
train_iter_loss: 0.16714419424533844
train_iter_loss: 0.12471035867929459
train_iter_loss: 0.1077687218785286
train_iter_loss: 0.1258261650800705
train_iter_loss: 0.29766976833343506
train_iter_loss: 0.17144446074962616
train_iter_loss: 0.2779075801372528
train_iter_loss: 0.1021374762058258
train_iter_loss: 0.11385500431060791
train_iter_loss: 0.16663701832294464
train_iter_loss: 0.23167800903320312
train_iter_loss: 0.09693639725446701
train_iter_loss: 0.1369558870792389
train_iter_loss: 0.1912512183189392
train_iter_loss: 0.1649891436100006
train_iter_loss: 0.043364018201828
train_iter_loss: 0.2153647392988205
train_iter_loss: 0.1267019808292389
train_iter_loss: 0.06803525984287262
train_iter_loss: 0.19932058453559875
train_iter_loss: 0.1793251633644104
train_iter_loss: 0.18153108656406403
train_iter_loss: 0.2023022323846817
train_iter_loss: 0.16848455369472504
train_iter_loss: 0.12060381472110748
train_iter_loss: 0.14305083453655243
train_iter_loss: 0.13911394774913788
train_iter_loss: 0.20410630106925964
train_iter_loss: 0.30335885286331177
train_iter_loss: 0.09881666302680969
train_iter_loss: 0.2704460620880127
train_iter_loss: 0.13387031853199005
train_iter_loss: 0.10445675253868103
train_iter_loss: 0.12440908700227737
train_iter_loss: 0.165400892496109
train_iter_loss: 0.18451440334320068
train_iter_loss: 0.18909528851509094
train_iter_loss: 0.24087771773338318
train_iter_loss: 0.11432941257953644
train_iter_loss: 0.11640284955501556
train_iter_loss: 0.15581265091896057
train loss :0.1726
---------------------
Validation seg loss: 0.22160260062137582 at epoch 263
epoch =    264/  1000, exp = train
train_iter_loss: 0.046741414815187454
train_iter_loss: 0.2514059245586395
train_iter_loss: 0.21061789989471436
train_iter_loss: 0.2746831774711609
train_iter_loss: 0.1465800702571869
train_iter_loss: 0.10794760286808014
train_iter_loss: 0.09833753108978271
train_iter_loss: 0.09905030578374863
train_iter_loss: 0.09784264862537384
train_iter_loss: 0.11559845507144928
train_iter_loss: 0.20511452853679657
train_iter_loss: 0.24143032729625702
train_iter_loss: 0.06023483723402023
train_iter_loss: 0.2538399398326874
train_iter_loss: 0.08589036017656326
train_iter_loss: 0.16647247970104218
train_iter_loss: 0.1507253348827362
train_iter_loss: 0.10266631096601486
train_iter_loss: 0.11692755669355392
train_iter_loss: 0.2008923888206482
train_iter_loss: 0.1734427809715271
train_iter_loss: 0.1742512285709381
train_iter_loss: 0.22925126552581787
train_iter_loss: 0.15159520506858826
train_iter_loss: 0.12723897397518158
train_iter_loss: 0.17314140498638153
train_iter_loss: 0.1650359332561493
train_iter_loss: 0.10319718718528748
train_iter_loss: 0.20619182288646698
train_iter_loss: 0.20699018239974976
train_iter_loss: 0.1246156170964241
train_iter_loss: 0.268640398979187
train_iter_loss: 0.12378375232219696
train_iter_loss: 0.05472090467810631
train_iter_loss: 0.15772606432437897
train_iter_loss: 0.12233977764844894
train_iter_loss: 0.2103888839483261
train_iter_loss: 0.09403317421674728
train_iter_loss: 0.15253953635692596
train_iter_loss: 0.17160332202911377
train_iter_loss: 0.2560655176639557
train_iter_loss: 0.15151819586753845
train_iter_loss: 0.1717241108417511
train_iter_loss: 0.17406430840492249
train_iter_loss: 0.26147696375846863
train_iter_loss: 0.26603779196739197
train_iter_loss: 0.09682425111532211
train_iter_loss: 0.1462467610836029
train_iter_loss: 0.10798835754394531
train_iter_loss: 0.2502318322658539
train_iter_loss: 0.11969345808029175
train_iter_loss: 0.22668714821338654
train_iter_loss: 0.06861060857772827
train_iter_loss: 0.15473854541778564
train_iter_loss: 0.1428270787000656
train_iter_loss: 0.12150021642446518
train_iter_loss: 0.16925132274627686
train_iter_loss: 0.13613073527812958
train_iter_loss: 0.2601662874221802
train_iter_loss: 0.12267798185348511
train_iter_loss: 0.1448504477739334
train_iter_loss: 0.2085346132516861
train_iter_loss: 0.19083639979362488
train_iter_loss: 0.14759103953838348
train_iter_loss: 0.1838332712650299
train_iter_loss: 0.15391457080841064
train_iter_loss: 0.09284567832946777
train_iter_loss: 0.11771898716688156
train_iter_loss: 0.2531130313873291
train_iter_loss: 0.17534953355789185
train_iter_loss: 0.11122288554906845
train_iter_loss: 0.11529860645532608
train_iter_loss: 0.15753430128097534
train_iter_loss: 0.1279003918170929
train_iter_loss: 0.175956130027771
train_iter_loss: 0.20650643110275269
train_iter_loss: 0.1574423611164093
train_iter_loss: 0.28064072132110596
train_iter_loss: 0.19932569563388824
train_iter_loss: 0.10324457287788391
train_iter_loss: 0.14358828961849213
train_iter_loss: 0.1778697520494461
train_iter_loss: 0.12018108367919922
train_iter_loss: 0.20696958899497986
train_iter_loss: 0.18056681752204895
train_iter_loss: 0.5494982004165649
train_iter_loss: 0.11508163809776306
train_iter_loss: 0.17687028646469116
train_iter_loss: 0.2094496339559555
train_iter_loss: 0.12728489935398102
train_iter_loss: 0.10518296808004379
train_iter_loss: 0.15081687271595
train_iter_loss: 0.1479281187057495
train_iter_loss: 0.2940211594104767
train_iter_loss: 0.118253193795681
train_iter_loss: 0.14835576713085175
train_iter_loss: 0.166825532913208
train_iter_loss: 0.18902873992919922
train_iter_loss: 0.05716770887374878
train_iter_loss: 0.20700077712535858
train loss :0.1670
---------------------
Validation seg loss: 0.22307062972301864 at epoch 264
epoch =    265/  1000, exp = train
train_iter_loss: 0.15741217136383057
train_iter_loss: 0.20606380701065063
train_iter_loss: 0.15733999013900757
train_iter_loss: 0.13945463299751282
train_iter_loss: 0.15616044402122498
train_iter_loss: 0.13193249702453613
train_iter_loss: 0.12866929173469543
train_iter_loss: 0.2939487397670746
train_iter_loss: 0.3447200357913971
train_iter_loss: 0.1319132149219513
train_iter_loss: 0.20156431198120117
train_iter_loss: 0.24068273603916168
train_iter_loss: 0.11907655745744705
train_iter_loss: 0.1769019067287445
train_iter_loss: 0.1924050748348236
train_iter_loss: 0.1594371199607849
train_iter_loss: 0.15479278564453125
train_iter_loss: 0.10606253147125244
train_iter_loss: 0.2661644518375397
train_iter_loss: 0.12858076393604279
train_iter_loss: 0.2548392117023468
train_iter_loss: 0.2830843925476074
train_iter_loss: 0.1259320080280304
train_iter_loss: 0.1883343607187271
train_iter_loss: 0.25066763162612915
train_iter_loss: 0.15295584499835968
train_iter_loss: 0.26128050684928894
train_iter_loss: 0.22706466913223267
train_iter_loss: 0.13332614302635193
train_iter_loss: 0.10587895661592484
train_iter_loss: 0.14172010123729706
train_iter_loss: 0.17505519092082977
train_iter_loss: 0.2911870777606964
train_iter_loss: 0.1436876505613327
train_iter_loss: 0.22240832448005676
train_iter_loss: 0.20619294047355652
train_iter_loss: 0.16614893078804016
train_iter_loss: 0.13344529271125793
train_iter_loss: 0.28175947070121765
train_iter_loss: 0.15959115326404572
train_iter_loss: 0.15841063857078552
train_iter_loss: 0.09632883220911026
train_iter_loss: 0.1891666054725647
train_iter_loss: 0.10374362021684647
train_iter_loss: 0.18413466215133667
train_iter_loss: 0.28235799074172974
train_iter_loss: 0.19217677414417267
train_iter_loss: 0.133897066116333
train_iter_loss: 0.17205515503883362
train_iter_loss: 0.33871421217918396
train_iter_loss: 0.11200431734323502
train_iter_loss: 0.13607844710350037
train_iter_loss: 0.08275455981492996
train_iter_loss: 0.19002534449100494
train_iter_loss: 0.10944540798664093
train_iter_loss: 0.26275235414505005
train_iter_loss: 0.21015037596225739
train_iter_loss: 0.13780540227890015
train_iter_loss: 0.17046982049942017
train_iter_loss: 0.20651589334011078
train_iter_loss: 0.08990621566772461
train_iter_loss: 0.19202739000320435
train_iter_loss: 0.147886723279953
train_iter_loss: 0.16440433263778687
train_iter_loss: 0.16576915979385376
train_iter_loss: 0.0753154531121254
train_iter_loss: 0.1434851437807083
train_iter_loss: 0.1874030977487564
train_iter_loss: 0.1354949027299881
train_iter_loss: 0.10230346024036407
train_iter_loss: 0.14589159190654755
train_iter_loss: 0.08147719502449036
train_iter_loss: 0.1602793186903
train_iter_loss: 0.08844107389450073
train_iter_loss: 0.08666171878576279
train_iter_loss: 0.25903016328811646
train_iter_loss: 0.07331926375627518
train_iter_loss: 0.16059400141239166
train_iter_loss: 0.06431452184915543
train_iter_loss: 0.12512972950935364
train_iter_loss: 0.14507822692394257
train_iter_loss: 0.11626055091619492
train_iter_loss: 0.18725477159023285
train_iter_loss: 0.14106714725494385
train_iter_loss: 0.2749750316143036
train_iter_loss: 0.3071746528148651
train_iter_loss: 0.23597535490989685
train_iter_loss: 0.19162309169769287
train_iter_loss: 0.2877756953239441
train_iter_loss: 0.15183071792125702
train_iter_loss: 0.16154026985168457
train_iter_loss: 0.2218456119298935
train_iter_loss: 0.146274596452713
train_iter_loss: 0.14937829971313477
train_iter_loss: 0.18535448610782623
train_iter_loss: 0.07845748215913773
train_iter_loss: 0.09679287672042847
train_iter_loss: 0.14448288083076477
train_iter_loss: 0.20094425976276398
train_iter_loss: 0.10231627523899078
train loss :0.1731
---------------------
Validation seg loss: 0.22382010578371161 at epoch 265
epoch =    266/  1000, exp = train
train_iter_loss: 0.2027030885219574
train_iter_loss: 0.10206219553947449
train_iter_loss: 0.1982293426990509
train_iter_loss: 0.20257435739040375
train_iter_loss: 0.12341926246881485
train_iter_loss: 0.11010943353176117
train_iter_loss: 0.291729211807251
train_iter_loss: 0.18852628767490387
train_iter_loss: 0.15147224068641663
train_iter_loss: 0.11108025163412094
train_iter_loss: 0.17290765047073364
train_iter_loss: 0.15834428369998932
train_iter_loss: 0.18864688277244568
train_iter_loss: 0.08694000542163849
train_iter_loss: 0.19315968453884125
train_iter_loss: 0.16127365827560425
train_iter_loss: 0.15107059478759766
train_iter_loss: 0.23658141493797302
train_iter_loss: 0.13064491748809814
train_iter_loss: 0.11954645812511444
train_iter_loss: 0.10883516818284988
train_iter_loss: 0.13418561220169067
train_iter_loss: 0.07818085700273514
train_iter_loss: 0.13782517611980438
train_iter_loss: 0.0710575133562088
train_iter_loss: 0.19180545210838318
train_iter_loss: 0.1429959088563919
train_iter_loss: 0.13547244668006897
train_iter_loss: 0.3050670623779297
train_iter_loss: 0.24044109880924225
train_iter_loss: 0.16805122792720795
train_iter_loss: 0.19732359051704407
train_iter_loss: 0.20414112508296967
train_iter_loss: 0.17369487881660461
train_iter_loss: 0.1422300487756729
train_iter_loss: 0.08718256652355194
train_iter_loss: 0.11205445975065231
train_iter_loss: 0.16085407137870789
train_iter_loss: 0.057357870042324066
train_iter_loss: 0.2029411345720291
train_iter_loss: 0.41068407893180847
train_iter_loss: 0.07236164808273315
train_iter_loss: 0.11377160996198654
train_iter_loss: 0.2711449861526489
train_iter_loss: 0.1312299519777298
train_iter_loss: 0.20794795453548431
train_iter_loss: 0.1597122997045517
train_iter_loss: 0.13343071937561035
train_iter_loss: 0.20814211666584015
train_iter_loss: 0.11733286827802658
train_iter_loss: 0.17542052268981934
train_iter_loss: 0.26921364665031433
train_iter_loss: 0.2397100031375885
train_iter_loss: 0.13954775035381317
train_iter_loss: 0.17341768741607666
train_iter_loss: 0.10564921796321869
train_iter_loss: 0.1880103051662445
train_iter_loss: 0.11045484989881516
train_iter_loss: 0.11565446108579636
train_iter_loss: 0.11078807711601257
train_iter_loss: 0.08275353908538818
train_iter_loss: 0.19407126307487488
train_iter_loss: 0.11220601201057434
train_iter_loss: 0.18235374987125397
train_iter_loss: 0.14394471049308777
train_iter_loss: 0.03518066555261612
train_iter_loss: 0.1893051713705063
train_iter_loss: 0.12646128237247467
train_iter_loss: 0.23664742708206177
train_iter_loss: 0.0807032361626625
train_iter_loss: 0.184002086520195
train_iter_loss: 0.16893726587295532
train_iter_loss: 0.1993066668510437
train_iter_loss: 0.09400554746389389
train_iter_loss: 0.2512749433517456
train_iter_loss: 0.15750692784786224
train_iter_loss: 0.17734676599502563
train_iter_loss: 0.0600367933511734
train_iter_loss: 0.12207607179880142
train_iter_loss: 0.1586003601551056
train_iter_loss: 0.07570213824510574
train_iter_loss: 0.10292868316173553
train_iter_loss: 0.17015957832336426
train_iter_loss: 0.1648470163345337
train_iter_loss: 0.2047903835773468
train_iter_loss: 0.1945050060749054
train_iter_loss: 0.2072339802980423
train_iter_loss: 0.13641716539859772
train_iter_loss: 0.20595484972000122
train_iter_loss: 0.11494611203670502
train_iter_loss: 0.13778939843177795
train_iter_loss: 0.18618929386138916
train_iter_loss: 0.1422417312860489
train_iter_loss: 0.24772679805755615
train_iter_loss: 0.16554130613803864
train_iter_loss: 0.16035564243793488
train_iter_loss: 0.29337453842163086
train_iter_loss: 0.11936012655496597
train_iter_loss: 0.37985342741012573
train_iter_loss: 0.17898645997047424
train loss :0.1651
---------------------
Validation seg loss: 0.22326377876770664 at epoch 266
epoch =    267/  1000, exp = train
train_iter_loss: 0.14509300887584686
train_iter_loss: 0.14100439846515656
train_iter_loss: 0.24593660235404968
train_iter_loss: 0.1355273425579071
train_iter_loss: 0.14844191074371338
train_iter_loss: 0.12260044366121292
train_iter_loss: 0.25475844740867615
train_iter_loss: 0.15280050039291382
train_iter_loss: 0.11928779631853104
train_iter_loss: 0.1802452653646469
train_iter_loss: 0.14359408617019653
train_iter_loss: 0.10871240496635437
train_iter_loss: 0.24128727614879608
train_iter_loss: 0.1221390813589096
train_iter_loss: 0.1215936467051506
train_iter_loss: 0.3494172692298889
train_iter_loss: 0.16192889213562012
train_iter_loss: 0.1998569667339325
train_iter_loss: 0.12964269518852234
train_iter_loss: 0.27344295382499695
train_iter_loss: 0.08014322072267532
train_iter_loss: 0.19392068684101105
train_iter_loss: 0.23176687955856323
train_iter_loss: 0.1344909965991974
train_iter_loss: 0.17197716236114502
train_iter_loss: 0.10042980313301086
train_iter_loss: 0.09437376260757446
train_iter_loss: 0.2433864325284958
train_iter_loss: 0.07332209497690201
train_iter_loss: 0.15793466567993164
train_iter_loss: 0.16469252109527588
train_iter_loss: 0.1066218838095665
train_iter_loss: 0.15014855563640594
train_iter_loss: 0.1915641725063324
train_iter_loss: 0.03761198744177818
train_iter_loss: 0.11522828787565231
train_iter_loss: 0.20867156982421875
train_iter_loss: 0.16958889365196228
train_iter_loss: 0.2436496615409851
train_iter_loss: 0.1442272812128067
train_iter_loss: 0.059440210461616516
train_iter_loss: 0.25449830293655396
train_iter_loss: 0.222276508808136
train_iter_loss: 0.09590985625982285
train_iter_loss: 0.2141353189945221
train_iter_loss: 0.14473018050193787
train_iter_loss: 0.17952975630760193
train_iter_loss: 0.19383057951927185
train_iter_loss: 0.08487483859062195
train_iter_loss: 0.20401032269001007
train_iter_loss: 0.19799864292144775
train_iter_loss: 0.22031313180923462
train_iter_loss: 0.2519512176513672
train_iter_loss: 0.20043344795703888
train_iter_loss: 0.12367832660675049
train_iter_loss: 0.2391805648803711
train_iter_loss: 0.25409695506095886
train_iter_loss: 0.059332169592380524
train_iter_loss: 0.2520923912525177
train_iter_loss: 0.13693876564502716
train_iter_loss: 0.16925844550132751
train_iter_loss: 0.11128360033035278
train_iter_loss: 0.21913208067417145
train_iter_loss: 0.15179741382598877
train_iter_loss: 0.13016460835933685
train_iter_loss: 0.08982577174901962
train_iter_loss: 0.19217190146446228
train_iter_loss: 0.16656659543514252
train_iter_loss: 0.11532839387655258
train_iter_loss: 0.17501504719257355
train_iter_loss: 0.10697932541370392
train_iter_loss: 0.1377563327550888
train_iter_loss: 0.16013161838054657
train_iter_loss: 0.18697243928909302
train_iter_loss: 0.09960287064313889
train_iter_loss: 0.24523219466209412
train_iter_loss: 0.05879631265997887
train_iter_loss: 0.13256703317165375
train_iter_loss: 0.2081233710050583
train_iter_loss: 0.16344361007213593
train_iter_loss: 0.1910349428653717
train_iter_loss: 0.17952808737754822
train_iter_loss: 0.21172520518302917
train_iter_loss: 0.1502748429775238
train_iter_loss: 0.19526369869709015
train_iter_loss: 0.1571204513311386
train_iter_loss: 0.2092561423778534
train_iter_loss: 0.15926098823547363
train_iter_loss: 0.19927851855754852
train_iter_loss: 0.2012663334608078
train_iter_loss: 0.14566941559314728
train_iter_loss: 0.1580006182193756
train_iter_loss: 0.12442049384117126
train_iter_loss: 0.17479471862316132
train_iter_loss: 0.12719564139842987
train_iter_loss: 0.24303840100765228
train_iter_loss: 0.1639050543308258
train_iter_loss: 0.2360059767961502
train_iter_loss: 0.12603043019771576
train_iter_loss: 0.08464279770851135
train loss :0.1672
---------------------
Validation seg loss: 0.22458359223948615 at epoch 267
epoch =    268/  1000, exp = train
train_iter_loss: 0.12629523873329163
train_iter_loss: 0.1843002438545227
train_iter_loss: 0.20083913207054138
train_iter_loss: 0.1489579826593399
train_iter_loss: 0.12829485535621643
train_iter_loss: 0.13500528037548065
train_iter_loss: 0.09310309588909149
train_iter_loss: 0.16716518998146057
train_iter_loss: 0.10557297617197037
train_iter_loss: 0.2345586121082306
train_iter_loss: 0.10937053710222244
train_iter_loss: 0.1611717939376831
train_iter_loss: 0.26429301500320435
train_iter_loss: 0.1546821892261505
train_iter_loss: 0.2071913480758667
train_iter_loss: 0.1388968676328659
train_iter_loss: 0.20388731360435486
train_iter_loss: 0.09227271378040314
train_iter_loss: 0.09746149182319641
train_iter_loss: 0.14136357605457306
train_iter_loss: 0.06065145507454872
train_iter_loss: 0.06052087992429733
train_iter_loss: 0.2832180857658386
train_iter_loss: 0.16845820844173431
train_iter_loss: 0.09032808989286423
train_iter_loss: 0.18162013590335846
train_iter_loss: 0.17795604467391968
train_iter_loss: 0.1250387579202652
train_iter_loss: 0.0994696244597435
train_iter_loss: 0.26432228088378906
train_iter_loss: 0.15211664140224457
train_iter_loss: 0.273111492395401
train_iter_loss: 0.15419626235961914
train_iter_loss: 0.13578225672245026
train_iter_loss: 0.1345038264989853
train_iter_loss: 0.19343353807926178
train_iter_loss: 0.19224362075328827
train_iter_loss: 0.2465893179178238
train_iter_loss: 0.16350960731506348
train_iter_loss: 0.2951788306236267
train_iter_loss: 0.209877148270607
train_iter_loss: 0.17766155302524567
train_iter_loss: 0.25946366786956787
train_iter_loss: 0.3659632205963135
train_iter_loss: 0.18407639861106873
train_iter_loss: 0.2470094859600067
train_iter_loss: 0.13858453929424286
train_iter_loss: 0.15004189312458038
train_iter_loss: 0.17338548600673676
train_iter_loss: 0.21642611920833588
train_iter_loss: 0.27670902013778687
train_iter_loss: 0.0868697464466095
train_iter_loss: 0.2928599715232849
train_iter_loss: 0.09402970969676971
train_iter_loss: 0.1580914407968521
train_iter_loss: 0.298257440328598
train_iter_loss: 0.13034188747406006
train_iter_loss: 0.12334009259939194
train_iter_loss: 0.17086875438690186
train_iter_loss: 0.18143552541732788
train_iter_loss: 0.19508449733257294
train_iter_loss: 0.2912619411945343
train_iter_loss: 0.14097541570663452
train_iter_loss: 0.1482473760843277
train_iter_loss: 0.19261860847473145
train_iter_loss: 0.2155459225177765
train_iter_loss: 0.22252441942691803
train_iter_loss: 0.08126050233840942
train_iter_loss: 0.09351794421672821
train_iter_loss: 0.24573487043380737
train_iter_loss: 0.21030010282993317
train_iter_loss: 0.17471055686473846
train_iter_loss: 0.11016245186328888
train_iter_loss: 0.14455446600914001
train_iter_loss: 0.20594587922096252
train_iter_loss: 0.18975162506103516
train_iter_loss: 0.15240436792373657
train_iter_loss: 0.1754905879497528
train_iter_loss: 0.13534483313560486
train_iter_loss: 0.32532721757888794
train_iter_loss: 0.118290014564991
train_iter_loss: 0.132489413022995
train_iter_loss: 0.1747732013463974
train_iter_loss: 0.16971169412136078
train_iter_loss: 0.17579790949821472
train_iter_loss: 0.13532833755016327
train_iter_loss: 0.08387931436300278
train_iter_loss: 0.1813925951719284
train_iter_loss: 0.12760107219219208
train_iter_loss: 0.07677678763866425
train_iter_loss: 0.07976094633340836
train_iter_loss: 0.18445797264575958
train_iter_loss: 0.14865565299987793
train_iter_loss: 0.06825123727321625
train_iter_loss: 0.19111809134483337
train_iter_loss: 0.08008750528097153
train_iter_loss: 0.15866993367671967
train_iter_loss: 0.21711497008800507
train_iter_loss: 0.15680129826068878
train_iter_loss: 0.11025474220514297
train loss :0.1707
---------------------
Validation seg loss: 0.22007164326584283 at epoch 268
epoch =    269/  1000, exp = train
train_iter_loss: 0.12800106406211853
train_iter_loss: 0.14082907140254974
train_iter_loss: 0.18213418126106262
train_iter_loss: 0.17577426135540009
train_iter_loss: 0.29041263461112976
train_iter_loss: 0.11084093153476715
train_iter_loss: 0.16499032080173492
train_iter_loss: 0.19141586124897003
train_iter_loss: 0.15196451544761658
train_iter_loss: 0.11385072767734528
train_iter_loss: 0.1869761347770691
train_iter_loss: 0.1774669587612152
train_iter_loss: 0.10633198916912079
train_iter_loss: 0.2591947615146637
train_iter_loss: 0.17736081779003143
train_iter_loss: 0.15403665602207184
train_iter_loss: 0.12179815769195557
train_iter_loss: 0.1562003344297409
train_iter_loss: 0.1324683129787445
train_iter_loss: 0.20724749565124512
train_iter_loss: 0.07561857998371124
train_iter_loss: 0.2281360626220703
train_iter_loss: 0.22493323683738708
train_iter_loss: 0.1093546524643898
train_iter_loss: 0.1338380128145218
train_iter_loss: 0.10520842671394348
train_iter_loss: 0.10269912332296371
train_iter_loss: 0.13681147992610931
train_iter_loss: 0.09052359312772751
train_iter_loss: 0.11450755596160889
train_iter_loss: 0.2812652587890625
train_iter_loss: 0.18807187676429749
train_iter_loss: 0.1455712914466858
train_iter_loss: 0.16565580666065216
train_iter_loss: 0.13403302431106567
train_iter_loss: 0.11200300604104996
train_iter_loss: 0.10573217272758484
train_iter_loss: 0.15442988276481628
train_iter_loss: 0.28412196040153503
train_iter_loss: 0.19346459209918976
train_iter_loss: 0.2228882759809494
train_iter_loss: 0.3032882809638977
train_iter_loss: 0.1946636438369751
train_iter_loss: 0.17048178613185883
train_iter_loss: 0.16532497107982635
train_iter_loss: 0.2106606811285019
train_iter_loss: 0.18316207826137543
train_iter_loss: 0.19611015915870667
train_iter_loss: 0.31149041652679443
train_iter_loss: 0.204203799366951
train_iter_loss: 0.2046433687210083
train_iter_loss: 0.1794106364250183
train_iter_loss: 0.12979090213775635
train_iter_loss: 0.14516420662403107
train_iter_loss: 0.09007513523101807
train_iter_loss: 0.11376940459012985
train_iter_loss: 0.23244674503803253
train_iter_loss: 0.12356492131948471
train_iter_loss: 0.22465276718139648
train_iter_loss: 0.19240596890449524
train_iter_loss: 0.09882869571447372
train_iter_loss: 0.09786637127399445
train_iter_loss: 0.12235423177480698
train_iter_loss: 0.15452367067337036
train_iter_loss: 0.13671870529651642
train_iter_loss: 0.16291913390159607
train_iter_loss: 0.13682609796524048
train_iter_loss: 0.13544389605522156
train_iter_loss: 0.1345045417547226
train_iter_loss: 0.18650656938552856
train_iter_loss: 0.1724986732006073
train_iter_loss: 0.27325868606567383
train_iter_loss: 0.15702076256275177
train_iter_loss: 0.1358790397644043
train_iter_loss: 0.04356876388192177
train_iter_loss: 0.10263839364051819
train_iter_loss: 0.03905309736728668
train_iter_loss: 0.1997411698102951
train_iter_loss: 0.22957976162433624
train_iter_loss: 0.07558312267065048
train_iter_loss: 0.15826210379600525
train_iter_loss: 0.27875468134880066
train_iter_loss: 0.34484395384788513
train_iter_loss: 0.2107013612985611
train_iter_loss: 0.19456538558006287
train_iter_loss: 0.17194519937038422
train_iter_loss: 0.2115597128868103
train_iter_loss: 0.17410100996494293
train_iter_loss: 0.28106728196144104
train_iter_loss: 0.1390148103237152
train_iter_loss: 0.14917157590389252
train_iter_loss: 0.12106465548276901
train_iter_loss: 0.11777126044034958
train_iter_loss: 0.06639168411493301
train_iter_loss: 0.15381264686584473
train_iter_loss: 0.1881006509065628
train_iter_loss: 0.16175340116024017
train_iter_loss: 0.3019506633281708
train_iter_loss: 0.13738971948623657
train_iter_loss: 0.11814692616462708
train loss :0.1686
---------------------
Validation seg loss: 0.22221427101571606 at epoch 269
epoch =    270/  1000, exp = train
train_iter_loss: 0.16092464327812195
train_iter_loss: 0.23084138333797455
train_iter_loss: 0.1434767097234726
train_iter_loss: 0.13748463988304138
train_iter_loss: 0.19436289370059967
train_iter_loss: 0.19178412854671478
train_iter_loss: 0.14242210984230042
train_iter_loss: 0.22749967873096466
train_iter_loss: 0.16600151360034943
train_iter_loss: 0.04382792487740517
train_iter_loss: 0.14392219483852386
train_iter_loss: 0.1933152824640274
train_iter_loss: 0.16319125890731812
train_iter_loss: 0.09618066251277924
train_iter_loss: 0.18351289629936218
train_iter_loss: 0.1893782615661621
train_iter_loss: 0.1940644383430481
train_iter_loss: 0.18194469809532166
train_iter_loss: 0.16801060736179352
train_iter_loss: 0.12299393117427826
train_iter_loss: 0.09585458785295486
train_iter_loss: 0.1845710426568985
train_iter_loss: 0.10370037704706192
train_iter_loss: 0.10527968406677246
train_iter_loss: 0.12904046475887299
train_iter_loss: 0.1188877671957016
train_iter_loss: 0.21528002619743347
train_iter_loss: 0.1780356615781784
train_iter_loss: 0.19097019731998444
train_iter_loss: 0.17308923602104187
train_iter_loss: 0.09807740896940231
train_iter_loss: 0.3027823567390442
train_iter_loss: 0.08980739116668701
train_iter_loss: 0.1786498725414276
train_iter_loss: 0.20559971034526825
train_iter_loss: 0.1641826182603836
train_iter_loss: 0.19868604838848114
train_iter_loss: 0.08173725754022598
train_iter_loss: 0.22086037695407867
train_iter_loss: 0.1578284502029419
train_iter_loss: 0.10868488997220993
train_iter_loss: 0.11110244691371918
train_iter_loss: 0.16914768517017365
train_iter_loss: 0.20327265560626984
train_iter_loss: 0.11862129718065262
train_iter_loss: 0.11140000820159912
train_iter_loss: 0.1825094074010849
train_iter_loss: 0.127970352768898
train_iter_loss: 0.11448390781879425
train_iter_loss: 0.1378546953201294
train_iter_loss: 0.053776878863573074
train_iter_loss: 0.11248799413442612
train_iter_loss: 0.19239889085292816
train_iter_loss: 0.09454623609781265
train_iter_loss: 0.11792155355215073
train_iter_loss: 0.14551909267902374
train_iter_loss: 0.1021755188703537
train_iter_loss: 0.2520501911640167
train_iter_loss: 0.39587125182151794
train_iter_loss: 0.15253306925296783
train_iter_loss: 0.30580568313598633
train_iter_loss: 0.19226127862930298
train_iter_loss: 0.1552438735961914
train_iter_loss: 0.1168951466679573
train_iter_loss: 0.08492434024810791
train_iter_loss: 0.22698582708835602
train_iter_loss: 0.13447576761245728
train_iter_loss: 0.15934385359287262
train_iter_loss: 0.17037859559059143
train_iter_loss: 0.26043766736984253
train_iter_loss: 0.1360073685646057
train_iter_loss: 0.24280717968940735
train_iter_loss: 0.42553672194480896
train_iter_loss: 0.20123213529586792
train_iter_loss: 0.1302698850631714
train_iter_loss: 0.16384072601795197
train_iter_loss: 0.20545551180839539
train_iter_loss: 0.46659043431282043
train_iter_loss: 0.1853983998298645
train_iter_loss: 0.12237828224897385
train_iter_loss: 0.2699263393878937
train_iter_loss: 0.1616039127111435
train_iter_loss: 0.05842091515660286
train_iter_loss: 0.10442882776260376
train_iter_loss: 0.2390127182006836
train_iter_loss: 0.07255957275629044
train_iter_loss: 0.16749492287635803
train_iter_loss: 0.16500690579414368
train_iter_loss: 0.258569598197937
train_iter_loss: 0.13568763434886932
train_iter_loss: 0.17942485213279724
train_iter_loss: 0.19815076887607574
train_iter_loss: 0.15089885890483856
train_iter_loss: 0.16955217719078064
train_iter_loss: 0.085036501288414
train_iter_loss: 0.12210676819086075
train_iter_loss: 0.09509751945734024
train_iter_loss: 0.2826770842075348
train_iter_loss: 0.16072219610214233
train_iter_loss: 0.21662896871566772
train loss :0.1702
---------------------
Validation seg loss: 0.22096514234424763 at epoch 270
epoch =    271/  1000, exp = train
train_iter_loss: 0.12853632867336273
train_iter_loss: 0.11333464086055756
train_iter_loss: 0.20006011426448822
train_iter_loss: 0.1441659927368164
train_iter_loss: 0.165053591132164
train_iter_loss: 0.15314553678035736
train_iter_loss: 0.3489673435688019
train_iter_loss: 0.13398319482803345
train_iter_loss: 0.1366754025220871
train_iter_loss: 0.13484038412570953
train_iter_loss: 0.07442101091146469
train_iter_loss: 0.10037188231945038
train_iter_loss: 0.15755632519721985
train_iter_loss: 0.16911666095256805
train_iter_loss: 0.1156313568353653
train_iter_loss: 0.3218925893306732
train_iter_loss: 0.2736324965953827
train_iter_loss: 0.1436738669872284
train_iter_loss: 0.1909758597612381
train_iter_loss: 0.11942998319864273
train_iter_loss: 0.2554812431335449
train_iter_loss: 0.103152796626091
train_iter_loss: 0.23530110716819763
train_iter_loss: 0.2729375660419464
train_iter_loss: 0.1026732474565506
train_iter_loss: 0.2042248547077179
train_iter_loss: 0.1817721724510193
train_iter_loss: 0.1697903871536255
train_iter_loss: 0.15438196063041687
train_iter_loss: 0.11318409442901611
train_iter_loss: 0.13926145434379578
train_iter_loss: 0.19484743475914001
train_iter_loss: 0.1747250109910965
train_iter_loss: 0.15560297667980194
train_iter_loss: 0.19099360704421997
train_iter_loss: 0.08828233927488327
train_iter_loss: 0.1343715637922287
train_iter_loss: 0.15818320214748383
train_iter_loss: 0.12913937866687775
train_iter_loss: 0.17313943803310394
train_iter_loss: 0.14328505098819733
train_iter_loss: 0.18105341494083405
train_iter_loss: 0.17135819792747498
train_iter_loss: 0.15952840447425842
train_iter_loss: 0.28021690249443054
train_iter_loss: 0.17189525067806244
train_iter_loss: 0.23469485342502594
train_iter_loss: 0.08645973354578018
train_iter_loss: 0.1587984263896942
train_iter_loss: 0.2692435085773468
train_iter_loss: 0.2628641724586487
train_iter_loss: 0.30003005266189575
train_iter_loss: 0.15750069916248322
train_iter_loss: 0.22412988543510437
train_iter_loss: 0.1842491775751114
train_iter_loss: 0.16748018562793732
train_iter_loss: 0.31635674834251404
train_iter_loss: 0.1713334619998932
train_iter_loss: 0.14363379776477814
train_iter_loss: 0.10752891004085541
train_iter_loss: 0.11418838798999786
train_iter_loss: 0.12086757272481918
train_iter_loss: 0.1392567753791809
train_iter_loss: 0.11113371700048447
train_iter_loss: 0.15445058047771454
train_iter_loss: 0.15156970918178558
train_iter_loss: 0.13106177747249603
train_iter_loss: 0.2218521237373352
train_iter_loss: 0.16491441428661346
train_iter_loss: 0.12834933400154114
train_iter_loss: 0.19009800255298615
train_iter_loss: 0.13736207783222198
train_iter_loss: 0.14494845271110535
train_iter_loss: 0.21566392481327057
train_iter_loss: 0.12191957980394363
train_iter_loss: 0.09885136783123016
train_iter_loss: 0.13901732861995697
train_iter_loss: 0.11877366900444031
train_iter_loss: 0.09546536952257156
train_iter_loss: 0.27137690782546997
train_iter_loss: 0.15393154323101044
train_iter_loss: 0.1952967643737793
train_iter_loss: 0.15018221735954285
train_iter_loss: 0.11518760770559311
train_iter_loss: 0.17391033470630646
train_iter_loss: 0.12764325737953186
train_iter_loss: 0.14293506741523743
train_iter_loss: 0.2732796370983124
train_iter_loss: 0.19711241126060486
train_iter_loss: 0.1140810176730156
train_iter_loss: 0.17280642688274384
train_iter_loss: 0.30546700954437256
train_iter_loss: 0.1595378816127777
train_iter_loss: 0.07775776833295822
train_iter_loss: 0.22416743636131287
train_iter_loss: 0.12011231482028961
train_iter_loss: 0.24985696375370026
train_iter_loss: 0.07289671152830124
train_iter_loss: 0.16115660965442657
train_iter_loss: 0.2231866866350174
train loss :0.1710
---------------------
Validation seg loss: 0.21750206759480653 at epoch 271
********************
best_val_epoch_loss:  0.21750206759480653
MODEL UPDATED
epoch =    272/  1000, exp = train
train_iter_loss: 0.186921626329422
train_iter_loss: 0.15537677705287933
train_iter_loss: 0.044858869165182114
train_iter_loss: 0.18400557339191437
train_iter_loss: 0.10551752150058746
train_iter_loss: 0.11753480136394501
train_iter_loss: 0.34564876556396484
train_iter_loss: 0.15128487348556519
train_iter_loss: 0.2822269797325134
train_iter_loss: 0.12596040964126587
train_iter_loss: 0.1389959752559662
train_iter_loss: 0.17424650490283966
train_iter_loss: 0.2655961513519287
train_iter_loss: 0.12765178084373474
train_iter_loss: 0.15256929397583008
train_iter_loss: 0.1400233507156372
train_iter_loss: 0.11192432045936584
train_iter_loss: 0.14000681042671204
train_iter_loss: 0.2125372290611267
train_iter_loss: 0.09446368366479874
train_iter_loss: 0.21407067775726318
train_iter_loss: 0.2242906093597412
train_iter_loss: 0.22448022663593292
train_iter_loss: 0.172605961561203
train_iter_loss: 0.06920929253101349
train_iter_loss: 0.14593586325645447
train_iter_loss: 0.13059182465076447
train_iter_loss: 0.3136644959449768
train_iter_loss: 0.10478644073009491
train_iter_loss: 0.18668442964553833
train_iter_loss: 0.11750610917806625
train_iter_loss: 0.18835046887397766
train_iter_loss: 0.12618516385555267
train_iter_loss: 0.16215144097805023
train_iter_loss: 0.20646831393241882
train_iter_loss: 0.19316890835762024
train_iter_loss: 0.12660939991474152
train_iter_loss: 0.07218509167432785
train_iter_loss: 0.07300952821969986
train_iter_loss: 0.14692986011505127
train_iter_loss: 0.19073602557182312
train_iter_loss: 0.0583338662981987
train_iter_loss: 0.13951514661312103
train_iter_loss: 0.16125932335853577
train_iter_loss: 0.0837772935628891
train_iter_loss: 0.1906234323978424
train_iter_loss: 0.20832176506519318
train_iter_loss: 0.17909608781337738
train_iter_loss: 0.1839786320924759
train_iter_loss: 0.19311091303825378
train_iter_loss: 0.20151127874851227
train_iter_loss: 0.11023443192243576
train_iter_loss: 0.12199511379003525
train_iter_loss: 0.2650412321090698
train_iter_loss: 0.08050352334976196
train_iter_loss: 0.18880125880241394
train_iter_loss: 0.13981418311595917
train_iter_loss: 0.13168278336524963
train_iter_loss: 0.16167418658733368
train_iter_loss: 0.10301417857408524
train_iter_loss: 0.15618515014648438
train_iter_loss: 0.2597822844982147
train_iter_loss: 0.15755018591880798
train_iter_loss: 0.10803648829460144
train_iter_loss: 0.2116839438676834
train_iter_loss: 0.148439422249794
train_iter_loss: 0.17499953508377075
train_iter_loss: 0.15928992629051208
train_iter_loss: 0.16074304282665253
train_iter_loss: 0.22917860746383667
train_iter_loss: 0.15944719314575195
train_iter_loss: 0.10418552905321121
train_iter_loss: 0.15742690861225128
train_iter_loss: 0.08724655210971832
train_iter_loss: 0.1478535532951355
train_iter_loss: 0.17157979309558868
train_iter_loss: 0.2153283953666687
train_iter_loss: 0.18687570095062256
train_iter_loss: 0.07660214602947235
train_iter_loss: 0.17361381649971008
train_iter_loss: 0.32291120290756226
train_iter_loss: 0.14970941841602325
train_iter_loss: 0.19088587164878845
train_iter_loss: 0.12776432931423187
train_iter_loss: 0.16798950731754303
train_iter_loss: 0.13968832790851593
train_iter_loss: 0.16431359946727753
train_iter_loss: 0.10568589717149734
train_iter_loss: 0.13735105097293854
train_iter_loss: 0.07868681848049164
train_iter_loss: 0.15025846660137177
train_iter_loss: 0.21863658726215363
train_iter_loss: 0.24022561311721802
train_iter_loss: 0.1399105340242386
train_iter_loss: 0.1397596150636673
train_iter_loss: 0.12073662877082825
train_iter_loss: 0.25861692428588867
train_iter_loss: 0.14297574758529663
train_iter_loss: 0.3453359305858612
train_iter_loss: 0.16365478932857513
train loss :0.1647
---------------------
Validation seg loss: 0.22040678942048886 at epoch 272
epoch =    273/  1000, exp = train
train_iter_loss: 0.08571817725896835
train_iter_loss: 0.22201725840568542
train_iter_loss: 0.08622858673334122
train_iter_loss: 0.2767173647880554
train_iter_loss: 0.2079935073852539
train_iter_loss: 0.19918471574783325
train_iter_loss: 0.16632965207099915
train_iter_loss: 0.21257251501083374
train_iter_loss: 0.17334558069705963
train_iter_loss: 0.10065093636512756
train_iter_loss: 0.2745824456214905
train_iter_loss: 0.1390577256679535
train_iter_loss: 0.16337254643440247
train_iter_loss: 0.14031824469566345
train_iter_loss: 0.32960045337677
train_iter_loss: 0.10154455900192261
train_iter_loss: 0.13145139813423157
train_iter_loss: 0.27325543761253357
train_iter_loss: 0.1009935736656189
train_iter_loss: 0.1699315309524536
train_iter_loss: 0.07522217929363251
train_iter_loss: 0.19227509200572968
train_iter_loss: 0.09203067421913147
train_iter_loss: 0.1879824548959732
train_iter_loss: 0.1579872965812683
train_iter_loss: 0.16144321858882904
train_iter_loss: 0.17964111268520355
train_iter_loss: 0.11304566264152527
train_iter_loss: 0.07136817276477814
train_iter_loss: 0.18259155750274658
train_iter_loss: 0.212076798081398
train_iter_loss: 0.3849070370197296
train_iter_loss: 0.14344313740730286
train_iter_loss: 0.16011670231819153
train_iter_loss: 0.1513747125864029
train_iter_loss: 0.16137577593326569
train_iter_loss: 0.178004190325737
train_iter_loss: 0.0726550742983818
train_iter_loss: 0.11966263502836227
train_iter_loss: 0.15177759528160095
train_iter_loss: 0.11335843056440353
train_iter_loss: 0.36280062794685364
train_iter_loss: 0.1104559674859047
train_iter_loss: 0.10685808211565018
train_iter_loss: 0.18805928528308868
train_iter_loss: 0.20439943671226501
train_iter_loss: 0.13658177852630615
train_iter_loss: 0.12766094505786896
train_iter_loss: 0.13545340299606323
train_iter_loss: 0.10779173672199249
train_iter_loss: 0.20001736283302307
train_iter_loss: 0.17547976970672607
train_iter_loss: 0.14409182965755463
train_iter_loss: 0.15220488607883453
train_iter_loss: 0.16039176285266876
train_iter_loss: 0.0920981839299202
train_iter_loss: 0.10757903754711151
train_iter_loss: 0.31166666746139526
train_iter_loss: 0.18883080780506134
train_iter_loss: 0.1702703833580017
train_iter_loss: 0.061878692358732224
train_iter_loss: 0.2247304767370224
train_iter_loss: 0.17187586426734924
train_iter_loss: 0.12970790266990662
train_iter_loss: 0.24875666201114655
train_iter_loss: 0.16443520784378052
train_iter_loss: 0.09084794670343399
train_iter_loss: 0.16905322670936584
train_iter_loss: 0.16712526977062225
train_iter_loss: 0.19873900711536407
train_iter_loss: 0.2008039355278015
train_iter_loss: 0.11727332323789597
train_iter_loss: 0.1798131912946701
train_iter_loss: 0.09223544597625732
train_iter_loss: 0.39851272106170654
train_iter_loss: 0.17055459320545197
train_iter_loss: 0.1772524118423462
train_iter_loss: 0.2565397322177887
train_iter_loss: 0.16105996072292328
train_iter_loss: 0.07214517891407013
train_iter_loss: 0.3045560419559479
train_iter_loss: 0.12615565955638885
train_iter_loss: 0.12513995170593262
train_iter_loss: 0.1171039417386055
train_iter_loss: 0.155914306640625
train_iter_loss: 0.18439601361751556
train_iter_loss: 0.21163995563983917
train_iter_loss: 0.19302429258823395
train_iter_loss: 0.21995875239372253
train_iter_loss: 0.19283942878246307
train_iter_loss: 0.14546571671962738
train_iter_loss: 0.11938439309597015
train_iter_loss: 0.09276964515447617
train_iter_loss: 0.2802720367908478
train_iter_loss: 0.11767223477363586
train_iter_loss: 0.2062244564294815
train_iter_loss: 0.20097653567790985
train_iter_loss: 0.3552762567996979
train_iter_loss: 0.5082840323448181
train_iter_loss: 0.17527322471141815
train loss :0.1765
---------------------
Validation seg loss: 0.22096199851553394 at epoch 273
epoch =    274/  1000, exp = train
train_iter_loss: 0.1633869856595993
train_iter_loss: 0.28527379035949707
train_iter_loss: 0.16643841564655304
train_iter_loss: 0.18193742632865906
train_iter_loss: 0.14821863174438477
train_iter_loss: 0.07980980724096298
train_iter_loss: 0.19002395868301392
train_iter_loss: 0.12047434598207474
train_iter_loss: 0.13481099903583527
train_iter_loss: 0.12216632068157196
train_iter_loss: 0.10450349748134613
train_iter_loss: 0.30873358249664307
train_iter_loss: 0.3932373821735382
train_iter_loss: 0.13790792226791382
train_iter_loss: 0.09266382455825806
train_iter_loss: 0.15927641093730927
train_iter_loss: 0.12441606819629669
train_iter_loss: 0.15156346559524536
train_iter_loss: 0.23092596232891083
train_iter_loss: 0.19231420755386353
train_iter_loss: 0.22486059367656708
train_iter_loss: 0.17304390668869019
train_iter_loss: 0.18667294085025787
train_iter_loss: 0.06575311720371246
train_iter_loss: 0.22748368978500366
train_iter_loss: 0.12921197712421417
train_iter_loss: 0.12594833970069885
train_iter_loss: 0.3377169668674469
train_iter_loss: 0.2753404974937439
train_iter_loss: 0.1333504468202591
train_iter_loss: 0.16225944459438324
train_iter_loss: 0.14659924805164337
train_iter_loss: 0.1943102777004242
train_iter_loss: 0.30265313386917114
train_iter_loss: 0.19863303005695343
train_iter_loss: 0.15246272087097168
train_iter_loss: 0.1261473000049591
train_iter_loss: 0.19826902449131012
train_iter_loss: 0.11239491403102875
train_iter_loss: 0.08572767674922943
train_iter_loss: 0.19521935284137726
train_iter_loss: 0.17719466984272003
train_iter_loss: 0.25807684659957886
train_iter_loss: 0.11007462441921234
train_iter_loss: 0.1875878870487213
train_iter_loss: 0.14288754761219025
train_iter_loss: 0.19523349404335022
train_iter_loss: 0.21568170189857483
train_iter_loss: 0.1411314308643341
train_iter_loss: 0.12990550696849823
train_iter_loss: 0.1959884911775589
train_iter_loss: 0.1753002256155014
train_iter_loss: 0.14035771787166595
train_iter_loss: 0.19847895205020905
train_iter_loss: 0.12886680662631989
train_iter_loss: 0.1248222067952156
train_iter_loss: 0.09196408838033676
train_iter_loss: 0.11475104838609695
train_iter_loss: 0.08571013063192368
train_iter_loss: 0.12013363093137741
train_iter_loss: 0.12990513443946838
train_iter_loss: 0.14134100079536438
train_iter_loss: 0.1946527361869812
train_iter_loss: 0.20028838515281677
train_iter_loss: 0.14802180230617523
train_iter_loss: 0.15885978937149048
train_iter_loss: 0.249261736869812
train_iter_loss: 0.12054930627346039
train_iter_loss: 0.09825883060693741
train_iter_loss: 0.24533924460411072
train_iter_loss: 0.20466898381710052
train_iter_loss: 0.15373092889785767
train_iter_loss: 0.20904627442359924
train_iter_loss: 0.3199540376663208
train_iter_loss: 0.1984352171421051
train_iter_loss: 0.16280007362365723
train_iter_loss: 0.12719672918319702
train_iter_loss: 0.1988697648048401
train_iter_loss: 0.10590393841266632
train_iter_loss: 0.229772686958313
train_iter_loss: 0.10496173799037933
train_iter_loss: 0.10783032327890396
train_iter_loss: 0.06178704649209976
train_iter_loss: 0.08136457204818726
train_iter_loss: 0.1590399444103241
train_iter_loss: 0.14092138409614563
train_iter_loss: 0.2259742021560669
train_iter_loss: 0.08389396965503693
train_iter_loss: 0.1394721418619156
train_iter_loss: 0.19185099005699158
train_iter_loss: 0.14378978312015533
train_iter_loss: 0.12957863509655
train_iter_loss: 0.1533917933702469
train_iter_loss: 0.17274440824985504
train_iter_loss: 0.197927787899971
train_iter_loss: 0.12962891161441803
train_iter_loss: 0.11676830798387527
train_iter_loss: 0.090875044465065
train_iter_loss: 0.1991683691740036
train_iter_loss: 0.11452792584896088
train loss :0.1670
---------------------
Validation seg loss: 0.21913874715144904 at epoch 274
epoch =    275/  1000, exp = train
train_iter_loss: 0.04406200349330902
train_iter_loss: 0.07055108994245529
train_iter_loss: 0.1424567699432373
train_iter_loss: 0.2360529601573944
train_iter_loss: 0.07534322887659073
train_iter_loss: 0.1689707636833191
train_iter_loss: 0.12091075628995895
train_iter_loss: 0.19358953833580017
train_iter_loss: 0.10748355090618134
train_iter_loss: 0.23514269292354584
train_iter_loss: 0.2745566666126251
train_iter_loss: 0.17769673466682434
train_iter_loss: 0.16176030039787292
train_iter_loss: 0.22785627841949463
train_iter_loss: 0.1268167942762375
train_iter_loss: 0.21959829330444336
train_iter_loss: 0.12875930964946747
train_iter_loss: 0.14299353957176208
train_iter_loss: 0.18593552708625793
train_iter_loss: 0.22919975221157074
train_iter_loss: 0.11557456105947495
train_iter_loss: 0.1365923136472702
train_iter_loss: 0.15158002078533173
train_iter_loss: 0.12486065179109573
train_iter_loss: 0.16011445224285126
train_iter_loss: 0.12981534004211426
train_iter_loss: 0.21259278059005737
train_iter_loss: 0.14551669359207153
train_iter_loss: 0.2610324025154114
train_iter_loss: 0.08812680095434189
train_iter_loss: 0.13263796269893646
train_iter_loss: 0.11878379434347153
train_iter_loss: 0.05861673131585121
train_iter_loss: 0.19359290599822998
train_iter_loss: 0.12377498298883438
train_iter_loss: 0.18632854521274567
train_iter_loss: 0.11758963763713837
train_iter_loss: 0.1791476607322693
train_iter_loss: 0.1443406641483307
train_iter_loss: 0.13683776557445526
train_iter_loss: 0.1008477509021759
train_iter_loss: 0.2318885177373886
train_iter_loss: 0.11013983190059662
train_iter_loss: 0.34623977541923523
train_iter_loss: 0.16886955499649048
train_iter_loss: 0.18504105508327484
train_iter_loss: 0.06289856880903244
train_iter_loss: 0.14943738281726837
train_iter_loss: 0.10256489366292953
train_iter_loss: 0.4245704412460327
train_iter_loss: 0.1818300187587738
train_iter_loss: 0.1469642072916031
train_iter_loss: 0.2648125886917114
train_iter_loss: 0.14571815729141235
train_iter_loss: 0.12993068993091583
train_iter_loss: 0.15394598245620728
train_iter_loss: 0.27634716033935547
train_iter_loss: 0.18406575918197632
train_iter_loss: 0.13624626398086548
train_iter_loss: 0.1819143295288086
train_iter_loss: 0.20276854932308197
train_iter_loss: 0.20610849559307098
train_iter_loss: 0.11283774673938751
train_iter_loss: 0.06654219329357147
train_iter_loss: 0.08627696335315704
train_iter_loss: 0.11428193747997284
train_iter_loss: 0.25864526629447937
train_iter_loss: 0.14021433889865875
train_iter_loss: 0.21557177603244781
train_iter_loss: 0.09828716516494751
train_iter_loss: 0.18271005153656006
train_iter_loss: 0.09222649037837982
train_iter_loss: 0.19988705217838287
train_iter_loss: 0.17744362354278564
train_iter_loss: 0.17877595126628876
train_iter_loss: 0.15942136943340302
train_iter_loss: 0.19721373915672302
train_iter_loss: 0.13292185962200165
train_iter_loss: 0.2344963699579239
train_iter_loss: 0.10338333994150162
train_iter_loss: 0.1293313354253769
train_iter_loss: 0.1868736445903778
train_iter_loss: 0.1742202192544937
train_iter_loss: 0.12183492630720139
train_iter_loss: 0.2959488034248352
train_iter_loss: 0.10683861374855042
train_iter_loss: 0.151679128408432
train_iter_loss: 0.32077786326408386
train_iter_loss: 0.1703019142150879
train_iter_loss: 0.23417185246944427
train_iter_loss: 0.10138770937919617
train_iter_loss: 0.1469275802373886
train_iter_loss: 0.13190676271915436
train_iter_loss: 0.10128491371870041
train_iter_loss: 0.11901043355464935
train_iter_loss: 0.12168147414922714
train_iter_loss: 0.21607959270477295
train_iter_loss: 0.18530526757240295
train_iter_loss: 0.14072196185588837
train_iter_loss: 0.13991227746009827
train loss :0.1649
---------------------
Validation seg loss: 0.22244444589640172 at epoch 275
epoch =    276/  1000, exp = train
train_iter_loss: 0.19966545701026917
train_iter_loss: 0.2890850007534027
train_iter_loss: 0.08344925194978714
train_iter_loss: 0.12366173416376114
train_iter_loss: 0.2632972002029419
train_iter_loss: 0.1574377715587616
train_iter_loss: 0.10680603981018066
train_iter_loss: 0.18517857789993286
train_iter_loss: 0.3623524010181427
train_iter_loss: 0.35269731283187866
train_iter_loss: 0.14785915613174438
train_iter_loss: 0.1784999519586563
train_iter_loss: 0.1962982416152954
train_iter_loss: 0.3781915605068207
train_iter_loss: 0.12747201323509216
train_iter_loss: 0.1588883101940155
train_iter_loss: 0.14369261264801025
train_iter_loss: 0.15695008635520935
train_iter_loss: 0.24041984975337982
train_iter_loss: 0.20042355358600616
train_iter_loss: 0.07867719978094101
train_iter_loss: 0.2844812572002411
train_iter_loss: 0.16078785061836243
train_iter_loss: 0.1747632771730423
train_iter_loss: 0.1192973181605339
train_iter_loss: 0.16896399855613708
train_iter_loss: 0.1161099448800087
train_iter_loss: 0.2548011541366577
train_iter_loss: 0.12731856107711792
train_iter_loss: 0.2141171246767044
train_iter_loss: 0.16478219628334045
train_iter_loss: 0.163136288523674
train_iter_loss: 0.08045154809951782
train_iter_loss: 0.14588987827301025
train_iter_loss: 0.14734995365142822
train_iter_loss: 0.24341782927513123
train_iter_loss: 0.14846153557300568
train_iter_loss: 0.13164210319519043
train_iter_loss: 0.1656561940908432
train_iter_loss: 0.1627938449382782
train_iter_loss: 0.05387591943144798
train_iter_loss: 0.27079135179519653
train_iter_loss: 0.11089163273572922
train_iter_loss: 0.2941451072692871
train_iter_loss: 0.22009971737861633
train_iter_loss: 0.1455342024564743
train_iter_loss: 0.15647505223751068
train_iter_loss: 0.17968256771564484
train_iter_loss: 0.1234622374176979
train_iter_loss: 0.24940292537212372
train_iter_loss: 0.24338878691196442
train_iter_loss: 0.20004267990589142
train_iter_loss: 0.22234046459197998
train_iter_loss: 0.144466832280159
train_iter_loss: 0.24211385846138
train_iter_loss: 0.11115661263465881
train_iter_loss: 0.12744778394699097
train_iter_loss: 0.1337507665157318
train_iter_loss: 0.09700054675340652
train_iter_loss: 0.15327107906341553
train_iter_loss: 0.23039643466472626
train_iter_loss: 0.14678505063056946
train_iter_loss: 0.1691061407327652
train_iter_loss: 0.10360479354858398
train_iter_loss: 0.1488042026758194
train_iter_loss: 0.21262313425540924
train_iter_loss: 0.2047044038772583
train_iter_loss: 0.14834186434745789
train_iter_loss: 0.06780485063791275
train_iter_loss: 0.12944509088993073
train_iter_loss: 0.06784704327583313
train_iter_loss: 0.15255428850650787
train_iter_loss: 0.17684508860111237
train_iter_loss: 0.1677607148885727
train_iter_loss: 0.13318565487861633
train_iter_loss: 0.17541709542274475
train_iter_loss: 0.24097053706645966
train_iter_loss: 0.13581369817256927
train_iter_loss: 0.20664183795452118
train_iter_loss: 0.15472163259983063
train_iter_loss: 0.12093762308359146
train_iter_loss: 0.17636647820472717
train_iter_loss: 0.1469441056251526
train_iter_loss: 0.13522125780582428
train_iter_loss: 0.1498492956161499
train_iter_loss: 0.11937060952186584
train_iter_loss: 0.09220805019140244
train_iter_loss: 0.27492013573646545
train_iter_loss: 0.15903763473033905
train_iter_loss: 0.1429917812347412
train_iter_loss: 0.14274334907531738
train_iter_loss: 0.0675218477845192
train_iter_loss: 0.13013269007205963
train_iter_loss: 0.14063747227191925
train_iter_loss: 0.18943041563034058
train_iter_loss: 0.15694120526313782
train_iter_loss: 0.18036243319511414
train_iter_loss: 0.11492134630680084
train_iter_loss: 0.20205901563167572
train_iter_loss: 0.18837009370326996
train loss :0.1715
---------------------
Validation seg loss: 0.22322225789451655 at epoch 276
epoch =    277/  1000, exp = train
train_iter_loss: 0.10146129131317139
train_iter_loss: 0.20267914235591888
train_iter_loss: 0.12809480726718903
train_iter_loss: 0.15500272810459137
train_iter_loss: 0.226153165102005
train_iter_loss: 0.10260823369026184
train_iter_loss: 0.1734824925661087
train_iter_loss: 0.1794612854719162
train_iter_loss: 0.19511397182941437
train_iter_loss: 0.2225446254014969
train_iter_loss: 0.21451975405216217
train_iter_loss: 0.30904656648635864
train_iter_loss: 0.10563401132822037
train_iter_loss: 0.06371518969535828
train_iter_loss: 0.10493402183055878
train_iter_loss: 0.1957881599664688
train_iter_loss: 0.12285380810499191
train_iter_loss: 0.13583455979824066
train_iter_loss: 0.19766142964363098
train_iter_loss: 0.20426388084888458
train_iter_loss: 0.21142281591892242
train_iter_loss: 0.18600572645664215
train_iter_loss: 0.28455623984336853
train_iter_loss: 0.21074210107326508
train_iter_loss: 0.11315647512674332
train_iter_loss: 0.2145783007144928
train_iter_loss: 0.190264493227005
train_iter_loss: 0.12138451635837555
train_iter_loss: 0.18735112249851227
train_iter_loss: 0.0976594015955925
train_iter_loss: 0.11457835137844086
train_iter_loss: 0.2728775441646576
train_iter_loss: 0.14572443068027496
train_iter_loss: 0.13950370252132416
train_iter_loss: 0.15271863341331482
train_iter_loss: 0.15458516776561737
train_iter_loss: 0.23637276887893677
train_iter_loss: 0.07151956856250763
train_iter_loss: 0.13009224832057953
train_iter_loss: 0.05354749411344528
train_iter_loss: 0.14703388512134552
train_iter_loss: 0.18261444568634033
train_iter_loss: 0.17931212484836578
train_iter_loss: 0.16064053773880005
train_iter_loss: 0.1276145875453949
train_iter_loss: 0.1333460956811905
train_iter_loss: 0.19961945712566376
train_iter_loss: 0.20238308608531952
train_iter_loss: 0.22966556251049042
train_iter_loss: 0.1726706176996231
train_iter_loss: 0.14080451428890228
train_iter_loss: 0.16527345776557922
train_iter_loss: 0.22525633871555328
train_iter_loss: 0.16379939019680023
train_iter_loss: 0.12362764775753021
train_iter_loss: 0.22856558859348297
train_iter_loss: 0.24070140719413757
train_iter_loss: 0.222696915268898
train_iter_loss: 0.14455953240394592
train_iter_loss: 0.21448062360286713
train_iter_loss: 0.18228250741958618
train_iter_loss: 0.20923957228660583
train_iter_loss: 0.21665778756141663
train_iter_loss: 0.16057513654232025
train_iter_loss: 0.08595601469278336
train_iter_loss: 0.16383245587348938
train_iter_loss: 0.11290242522954941
train_iter_loss: 0.13075757026672363
train_iter_loss: 0.16163349151611328
train_iter_loss: 0.1717621386051178
train_iter_loss: 0.13086219131946564
train_iter_loss: 0.16758134961128235
train_iter_loss: 0.08933865278959274
train_iter_loss: 0.16750437021255493
train_iter_loss: 0.4119540750980377
train_iter_loss: 0.13010652363300323
train_iter_loss: 0.12749622762203217
train_iter_loss: 0.09494847059249878
train_iter_loss: 0.2311667501926422
train_iter_loss: 0.27311772108078003
train_iter_loss: 0.17205657064914703
train_iter_loss: 0.11343463510274887
train_iter_loss: 0.17213137447834015
train_iter_loss: 0.13018614053726196
train_iter_loss: 0.12958607077598572
train_iter_loss: 0.11708322912454605
train_iter_loss: 0.16288630664348602
train_iter_loss: 0.11861342936754227
train_iter_loss: 0.13439439237117767
train_iter_loss: 0.16626965999603271
train_iter_loss: 0.1295444816350937
train_iter_loss: 0.14572355151176453
train_iter_loss: 0.3255850374698639
train_iter_loss: 0.19043004512786865
train_iter_loss: 0.1867699921131134
train_iter_loss: 0.22604605555534363
train_iter_loss: 0.18661870062351227
train_iter_loss: 0.1664460152387619
train_iter_loss: 0.11663219332695007
train_iter_loss: 0.1826741248369217
train loss :0.1710
---------------------
Validation seg loss: 0.21842536765133153 at epoch 277
epoch =    278/  1000, exp = train
train_iter_loss: 0.22508631646633148
train_iter_loss: 0.1520071029663086
train_iter_loss: 0.20182351768016815
train_iter_loss: 0.1685778647661209
train_iter_loss: 0.29507267475128174
train_iter_loss: 0.1845548301935196
train_iter_loss: 0.04626814275979996
train_iter_loss: 0.12251973152160645
train_iter_loss: 0.1561698466539383
train_iter_loss: 0.22667966783046722
train_iter_loss: 0.1799604892730713
train_iter_loss: 0.251028835773468
train_iter_loss: 0.21476265788078308
train_iter_loss: 0.10811587423086166
train_iter_loss: 0.1864015758037567
train_iter_loss: 0.07777023315429688
train_iter_loss: 0.11958495527505875
train_iter_loss: 0.2959037125110626
train_iter_loss: 0.1506892293691635
train_iter_loss: 0.16549620032310486
train_iter_loss: 0.10326001793146133
train_iter_loss: 0.10621988028287888
train_iter_loss: 0.09472550451755524
train_iter_loss: 0.23588603734970093
train_iter_loss: 0.19699110090732574
train_iter_loss: 0.21234524250030518
train_iter_loss: 0.2050587385892868
train_iter_loss: 0.12491213530302048
train_iter_loss: 0.1307840347290039
train_iter_loss: 0.19163183867931366
train_iter_loss: 0.22480370104312897
train_iter_loss: 0.21876609325408936
train_iter_loss: 0.2183762639760971
train_iter_loss: 0.08583097159862518
train_iter_loss: 0.163641557097435
train_iter_loss: 0.09229343384504318
train_iter_loss: 0.18399867415428162
train_iter_loss: 0.10547443479299545
train_iter_loss: 0.15210163593292236
train_iter_loss: 0.2920415699481964
train_iter_loss: 0.12593966722488403
train_iter_loss: 0.09919970482587814
train_iter_loss: 0.19768331944942474
train_iter_loss: 0.3697049617767334
train_iter_loss: 0.11635413765907288
train_iter_loss: 0.0858289897441864
train_iter_loss: 0.1325676143169403
train_iter_loss: 0.16986094415187836
train_iter_loss: 0.11143063753843307
train_iter_loss: 0.15047988295555115
train_iter_loss: 0.2316012978553772
train_iter_loss: 0.17587929964065552
train_iter_loss: 0.2584346532821655
train_iter_loss: 0.20637470483779907
train_iter_loss: 0.11208412051200867
train_iter_loss: 0.1516847014427185
train_iter_loss: 0.2560555636882782
train_iter_loss: 0.10461007058620453
train_iter_loss: 0.08711394667625427
train_iter_loss: 0.16469955444335938
train_iter_loss: 0.20816737413406372
train_iter_loss: 0.13512609899044037
train_iter_loss: 0.1084272712469101
train_iter_loss: 0.1702919751405716
train_iter_loss: 0.07008885592222214
train_iter_loss: 0.09195146709680557
train_iter_loss: 0.1172853410243988
train_iter_loss: 0.27128326892852783
train_iter_loss: 0.19426889717578888
train_iter_loss: 0.09012119472026825
train_iter_loss: 0.11823093891143799
train_iter_loss: 0.12058264762163162
train_iter_loss: 0.1687399297952652
train_iter_loss: 0.17197155952453613
train_iter_loss: 0.1183556318283081
train_iter_loss: 0.15988723933696747
train_iter_loss: 0.1777184009552002
train_iter_loss: 0.12976625561714172
train_iter_loss: 0.14751701056957245
train_iter_loss: 0.10794979333877563
train_iter_loss: 0.11466550081968307
train_iter_loss: 0.172491192817688
train_iter_loss: 0.14458411931991577
train_iter_loss: 0.13458384573459625
train_iter_loss: 0.2513759732246399
train_iter_loss: 0.07186400890350342
train_iter_loss: 0.16501109302043915
train_iter_loss: 0.1791778802871704
train_iter_loss: 0.2067740112543106
train_iter_loss: 0.10811606049537659
train_iter_loss: 0.182375967502594
train_iter_loss: 0.18278168141841888
train_iter_loss: 0.12735335528850555
train_iter_loss: 0.17144767940044403
train_iter_loss: 0.22172051668167114
train_iter_loss: 0.306056946516037
train_iter_loss: 0.215704083442688
train_iter_loss: 0.2179846167564392
train_iter_loss: 0.1861112117767334
train_iter_loss: 0.16335904598236084
train loss :0.1675
---------------------
Validation seg loss: 0.2230398702646061 at epoch 278
epoch =    279/  1000, exp = train
train_iter_loss: 0.10505754500627518
train_iter_loss: 0.2618356943130493
train_iter_loss: 0.2072267085313797
train_iter_loss: 0.19421866536140442
train_iter_loss: 0.1749342530965805
train_iter_loss: 0.08603180944919586
train_iter_loss: 0.1475445032119751
train_iter_loss: 0.18542499840259552
train_iter_loss: 0.15902681648731232
train_iter_loss: 0.1868908405303955
train_iter_loss: 0.17778541147708893
train_iter_loss: 0.2014341503381729
train_iter_loss: 0.10734515637159348
train_iter_loss: 0.17540252208709717
train_iter_loss: 0.15067316591739655
train_iter_loss: 0.08698216080665588
train_iter_loss: 0.09040326625108719
train_iter_loss: 0.1255757212638855
train_iter_loss: 0.10756508260965347
train_iter_loss: 0.1832498162984848
train_iter_loss: 0.12718798220157623
train_iter_loss: 0.15852725505828857
train_iter_loss: 0.19333656132221222
train_iter_loss: 0.11839859932661057
train_iter_loss: 0.1835881620645523
train_iter_loss: 0.08837082237005234
train_iter_loss: 0.21309883892536163
train_iter_loss: 0.16630351543426514
train_iter_loss: 0.1879715472459793
train_iter_loss: 0.024615231901407242
train_iter_loss: 0.2812727987766266
train_iter_loss: 0.07763613760471344
train_iter_loss: 0.08870509266853333
train_iter_loss: 0.09478819370269775
train_iter_loss: 0.15098567306995392
train_iter_loss: 0.06882430613040924
train_iter_loss: 0.19698432087898254
train_iter_loss: 0.19716954231262207
train_iter_loss: 0.1120312362909317
train_iter_loss: 0.18054214119911194
train_iter_loss: 0.28212323784828186
train_iter_loss: 0.2261410802602768
train_iter_loss: 0.13727378845214844
train_iter_loss: 0.2155909240245819
train_iter_loss: 0.10713428258895874
train_iter_loss: 0.11851396411657333
train_iter_loss: 0.21919816732406616
train_iter_loss: 0.19860418140888214
train_iter_loss: 0.11961793899536133
train_iter_loss: 0.21793119609355927
train_iter_loss: 0.16767004132270813
train_iter_loss: 0.09240133315324783
train_iter_loss: 0.29610610008239746
train_iter_loss: 0.11107994616031647
train_iter_loss: 0.16161933541297913
train_iter_loss: 0.21199558675289154
train_iter_loss: 0.12741440534591675
train_iter_loss: 0.17404764890670776
train_iter_loss: 0.08953741937875748
train_iter_loss: 0.15017180144786835
train_iter_loss: 0.23703783750534058
train_iter_loss: 0.2259265035390854
train_iter_loss: 0.15170568227767944
train_iter_loss: 0.14137472212314606
train_iter_loss: 0.1841629445552826
train_iter_loss: 0.12290450185537338
train_iter_loss: 0.24471309781074524
train_iter_loss: 0.16033221781253815
train_iter_loss: 0.1497175544500351
train_iter_loss: 0.1740902066230774
train_iter_loss: 0.16188274323940277
train_iter_loss: 0.3141196668148041
train_iter_loss: 0.20457002520561218
train_iter_loss: 0.2404249757528305
train_iter_loss: 0.13435815274715424
train_iter_loss: 0.12653915584087372
train_iter_loss: 0.10547202825546265
train_iter_loss: 0.10770662128925323
train_iter_loss: 0.2276819944381714
train_iter_loss: 0.07265395671129227
train_iter_loss: 0.11976940929889679
train_iter_loss: 0.15307439863681793
train_iter_loss: 0.2024659663438797
train_iter_loss: 0.18358869850635529
train_iter_loss: 0.1509062796831131
train_iter_loss: 0.2179286628961563
train_iter_loss: 0.15275396406650543
train_iter_loss: 0.1273062378168106
train_iter_loss: 0.3259155750274658
train_iter_loss: 0.16784226894378662
train_iter_loss: 0.2869465947151184
train_iter_loss: 0.2997985780239105
train_iter_loss: 0.13473699986934662
train_iter_loss: 0.09179158508777618
train_iter_loss: 0.16353128850460052
train_iter_loss: 0.21905316412448883
train_iter_loss: 0.27197742462158203
train_iter_loss: 0.18513916432857513
train_iter_loss: 0.10394823551177979
train_iter_loss: 0.1061444878578186
train loss :0.1677
---------------------
Validation seg loss: 0.21940195440086271 at epoch 279
epoch =    280/  1000, exp = train
train_iter_loss: 0.12459950894117355
train_iter_loss: 0.15255366265773773
train_iter_loss: 0.07445426285266876
train_iter_loss: 0.10723331570625305
train_iter_loss: 0.07642944157123566
train_iter_loss: 0.18704596161842346
train_iter_loss: 0.29687052965164185
train_iter_loss: 0.1880405992269516
train_iter_loss: 0.26412826776504517
train_iter_loss: 0.1762649416923523
train_iter_loss: 0.09669703245162964
train_iter_loss: 0.15444332361221313
train_iter_loss: 0.1824982762336731
train_iter_loss: 0.10394597053527832
train_iter_loss: 0.33829227089881897
train_iter_loss: 0.12266041338443756
train_iter_loss: 0.212919220328331
train_iter_loss: 0.11913426220417023
train_iter_loss: 0.1422850340604782
train_iter_loss: 0.18852679431438446
train_iter_loss: 0.09661729633808136
train_iter_loss: 0.10820193588733673
train_iter_loss: 0.13375450670719147
train_iter_loss: 0.22571831941604614
train_iter_loss: 0.15716952085494995
train_iter_loss: 0.09473475813865662
train_iter_loss: 0.126989483833313
train_iter_loss: 0.11506732553243637
train_iter_loss: 0.23016692698001862
train_iter_loss: 0.21663084626197815
train_iter_loss: 0.10636141896247864
train_iter_loss: 0.1268390417098999
train_iter_loss: 0.1863894760608673
train_iter_loss: 0.10598976910114288
train_iter_loss: 0.20321184396743774
train_iter_loss: 0.20175763964653015
train_iter_loss: 0.13005714118480682
train_iter_loss: 0.23170475661754608
train_iter_loss: 0.1591118574142456
train_iter_loss: 0.2412586361169815
train_iter_loss: 0.3064294755458832
train_iter_loss: 0.2832033634185791
train_iter_loss: 0.16597367823123932
train_iter_loss: 0.19367846846580505
train_iter_loss: 0.15131646394729614
train_iter_loss: 0.18587537109851837
train_iter_loss: 0.15664198994636536
train_iter_loss: 0.1263427734375
train_iter_loss: 0.19172737002372742
train_iter_loss: 0.0917007178068161
train_iter_loss: 0.16505704820156097
train_iter_loss: 0.13064584136009216
train_iter_loss: 0.1648038923740387
train_iter_loss: 0.11575577408075333
train_iter_loss: 0.11099343001842499
train_iter_loss: 0.11922349780797958
train_iter_loss: 0.19456273317337036
train_iter_loss: 0.14286860823631287
train_iter_loss: 0.18345221877098083
train_iter_loss: 0.18823523819446564
train_iter_loss: 0.2274528443813324
train_iter_loss: 0.173147052526474
train_iter_loss: 0.19361409544944763
train_iter_loss: 0.16701850295066833
train_iter_loss: 0.09742467850446701
train_iter_loss: 0.3411964178085327
train_iter_loss: 0.13679389655590057
train_iter_loss: 0.14150196313858032
train_iter_loss: 0.08194993436336517
train_iter_loss: 0.12271150201559067
train_iter_loss: 0.16933004558086395
train_iter_loss: 0.15867316722869873
train_iter_loss: 0.18613910675048828
train_iter_loss: 0.13144107162952423
train_iter_loss: 0.0982750728726387
train_iter_loss: 0.12115395069122314
train_iter_loss: 0.15877540409564972
train_iter_loss: 0.19557300209999084
train_iter_loss: 0.1194821298122406
train_iter_loss: 0.17765186727046967
train_iter_loss: 0.08662082254886627
train_iter_loss: 0.07711035013198853
train_iter_loss: 0.23891803622245789
train_iter_loss: 0.09423094242811203
train_iter_loss: 0.15763135254383087
train_iter_loss: 0.2737627625465393
train_iter_loss: 0.3334154188632965
train_iter_loss: 0.3592930734157562
train_iter_loss: 0.13916704058647156
train_iter_loss: 0.185064896941185
train_iter_loss: 0.10426020622253418
train_iter_loss: 0.09509452432394028
train_iter_loss: 0.2703332304954529
train_iter_loss: 0.12376780807971954
train_iter_loss: 0.23105847835540771
train_iter_loss: 0.27175968885421753
train_iter_loss: 0.13845017552375793
train_iter_loss: 0.11113423854112625
train_iter_loss: 0.14564238488674164
train_iter_loss: 0.2215786725282669
train loss :0.1690
---------------------
Validation seg loss: 0.2215106666334114 at epoch 280
epoch =    281/  1000, exp = train
train_iter_loss: 0.18879586458206177
train_iter_loss: 0.12127643823623657
train_iter_loss: 0.13708040118217468
train_iter_loss: 0.17568856477737427
train_iter_loss: 0.08166090399026871
train_iter_loss: 0.1474900245666504
train_iter_loss: 0.14854168891906738
train_iter_loss: 0.10410735756158829
train_iter_loss: 0.1274310052394867
train_iter_loss: 0.23640906810760498
train_iter_loss: 0.1382959932088852
train_iter_loss: 0.2218131422996521
train_iter_loss: 0.06966032832860947
train_iter_loss: 0.09966747462749481
train_iter_loss: 0.14547395706176758
train_iter_loss: 0.14328402280807495
train_iter_loss: 0.1444961577653885
train_iter_loss: 0.05378826707601547
train_iter_loss: 0.09092004597187042
train_iter_loss: 0.07709329575300217
train_iter_loss: 0.1466306746006012
train_iter_loss: 0.08562453091144562
train_iter_loss: 0.09953955560922623
train_iter_loss: 0.24231873452663422
train_iter_loss: 0.3101588189601898
train_iter_loss: 0.17067068815231323
train_iter_loss: 0.18722864985466003
train_iter_loss: 0.13321034610271454
train_iter_loss: 0.111906997859478
train_iter_loss: 0.1856449842453003
train_iter_loss: 0.13492093980312347
train_iter_loss: 0.19855771958827972
train_iter_loss: 0.2093101292848587
train_iter_loss: 0.05425744876265526
train_iter_loss: 0.2758108377456665
train_iter_loss: 0.15003378689289093
train_iter_loss: 0.03199591860175133
train_iter_loss: 0.15892654657363892
train_iter_loss: 0.13136643171310425
train_iter_loss: 0.1907714307308197
train_iter_loss: 0.16403765976428986
train_iter_loss: 0.05440681427717209
train_iter_loss: 0.11779677867889404
train_iter_loss: 0.18821074068546295
train_iter_loss: 0.10953399538993835
train_iter_loss: 0.10724276304244995
train_iter_loss: 0.11751583963632584
train_iter_loss: 0.0994713082909584
train_iter_loss: 0.16913621127605438
train_iter_loss: 0.14646156132221222
train_iter_loss: 0.13572387397289276
train_iter_loss: 0.11693006008863449
train_iter_loss: 0.14732642471790314
train_iter_loss: 0.13197028636932373
train_iter_loss: 0.15022139251232147
train_iter_loss: 0.15312647819519043
train_iter_loss: 0.15574513375759125
train_iter_loss: 0.2509353756904602
train_iter_loss: 0.27001720666885376
train_iter_loss: 0.2998011112213135
train_iter_loss: 0.06447285413742065
train_iter_loss: 0.1654428243637085
train_iter_loss: 0.1854865550994873
train_iter_loss: 0.24563826620578766
train_iter_loss: 0.11602356284856796
train_iter_loss: 0.351217657327652
train_iter_loss: 0.21984602510929108
train_iter_loss: 0.15187299251556396
train_iter_loss: 0.08324830234050751
train_iter_loss: 0.14132243394851685
train_iter_loss: 0.26529595255851746
train_iter_loss: 0.1658899337053299
train_iter_loss: 0.13915513455867767
train_iter_loss: 0.11719625443220139
train_iter_loss: 0.23823073506355286
train_iter_loss: 0.32137301564216614
train_iter_loss: 0.06610193103551865
train_iter_loss: 0.2096043974161148
train_iter_loss: 0.21407218277454376
train_iter_loss: 0.14228658378124237
train_iter_loss: 0.20435896515846252
train_iter_loss: 0.15907174348831177
train_iter_loss: 0.18320393562316895
train_iter_loss: 0.10987697541713715
train_iter_loss: 0.15374524891376495
train_iter_loss: 0.16001273691654205
train_iter_loss: 0.17961622774600983
train_iter_loss: 0.1817866563796997
train_iter_loss: 0.1332346349954605
train_iter_loss: 0.1504102349281311
train_iter_loss: 0.07784584909677505
train_iter_loss: 0.07653602957725525
train_iter_loss: 0.21835894882678986
train_iter_loss: 0.1739019900560379
train_iter_loss: 0.23156718909740448
train_iter_loss: 0.3176140785217285
train_iter_loss: 0.17958983778953552
train_iter_loss: 0.10410358011722565
train_iter_loss: 0.11650509387254715
train_iter_loss: 0.2934184968471527
train loss :0.1613
---------------------
Validation seg loss: 0.21944294027316402 at epoch 281
epoch =    282/  1000, exp = train
train_iter_loss: 0.23399564623832703
train_iter_loss: 0.161058247089386
train_iter_loss: 0.38820162415504456
train_iter_loss: 0.1998450756072998
train_iter_loss: 0.21770285069942474
train_iter_loss: 0.15803804993629456
train_iter_loss: 0.2183966040611267
train_iter_loss: 0.20163078606128693
train_iter_loss: 0.23728284239768982
train_iter_loss: 0.2106359302997589
train_iter_loss: 0.2568114101886749
train_iter_loss: 0.08975819498300552
train_iter_loss: 0.06915798783302307
train_iter_loss: 0.18735572695732117
train_iter_loss: 0.211472749710083
train_iter_loss: 0.1040010154247284
train_iter_loss: 0.24178814888000488
train_iter_loss: 0.16295365989208221
train_iter_loss: 0.202803373336792
train_iter_loss: 0.12954829633235931
train_iter_loss: 0.11503133177757263
train_iter_loss: 0.12680986523628235
train_iter_loss: 0.1388324350118637
train_iter_loss: 0.15654100477695465
train_iter_loss: 0.18127009272575378
train_iter_loss: 0.12743805348873138
train_iter_loss: 0.13011376559734344
train_iter_loss: 0.13301192224025726
train_iter_loss: 0.1340782344341278
train_iter_loss: 0.17265698313713074
train_iter_loss: 0.14457795023918152
train_iter_loss: 0.11392035335302353
train_iter_loss: 0.1462811678647995
train_iter_loss: 0.15819014608860016
train_iter_loss: 0.12119945138692856
train_iter_loss: 0.26098212599754333
train_iter_loss: 0.2176906168460846
train_iter_loss: 0.19851234555244446
train_iter_loss: 0.23895993828773499
train_iter_loss: 0.22596152126789093
train_iter_loss: 0.08505848050117493
train_iter_loss: 0.0827273577451706
train_iter_loss: 0.09650275856256485
train_iter_loss: 0.28864726424217224
train_iter_loss: 0.19008025527000427
train_iter_loss: 0.17246581614017487
train_iter_loss: 0.18697881698608398
train_iter_loss: 0.16403788328170776
train_iter_loss: 0.1598573476076126
train_iter_loss: 0.14333246648311615
train_iter_loss: 0.18195030093193054
train_iter_loss: 0.07720234990119934
train_iter_loss: 0.181289404630661
train_iter_loss: 0.12887319922447205
train_iter_loss: 0.22376909852027893
train_iter_loss: 0.16142494976520538
train_iter_loss: 0.1480964571237564
train_iter_loss: 0.16080687940120697
train_iter_loss: 0.16559983789920807
train_iter_loss: 0.1954892873764038
train_iter_loss: 0.1944039762020111
train_iter_loss: 0.19755636155605316
train_iter_loss: 0.18745727837085724
train_iter_loss: 0.16415958106517792
train_iter_loss: 0.09234361350536346
train_iter_loss: 0.13376760482788086
train_iter_loss: 0.1096733808517456
train_iter_loss: 0.20767159759998322
train_iter_loss: 0.24892064929008484
train_iter_loss: 0.0891919657588005
train_iter_loss: 0.15779948234558105
train_iter_loss: 0.0957259014248848
train_iter_loss: 0.1460268348455429
train_iter_loss: 0.15234807133674622
train_iter_loss: 0.29898521304130554
train_iter_loss: 0.06365261226892471
train_iter_loss: 0.1574767529964447
train_iter_loss: 0.10321734100580215
train_iter_loss: 0.1644885540008545
train_iter_loss: 0.11325511336326599
train_iter_loss: 0.21852657198905945
train_iter_loss: 0.11677173525094986
train_iter_loss: 0.109136663377285
train_iter_loss: 0.109311044216156
train_iter_loss: 0.10982456058263779
train_iter_loss: 0.13005183637142181
train_iter_loss: 0.14674486219882965
train_iter_loss: 0.18212521076202393
train_iter_loss: 0.22590261697769165
train_iter_loss: 0.12265487015247345
train_iter_loss: 0.275531142950058
train_iter_loss: 0.12083342671394348
train_iter_loss: 0.5104185342788696
train_iter_loss: 0.16507770121097565
train_iter_loss: 0.20143894851207733
train_iter_loss: 0.12816743552684784
train_iter_loss: 0.11449385434389114
train_iter_loss: 0.28333351016044617
train_iter_loss: 0.19559037685394287
train_iter_loss: 0.15502402186393738
train loss :0.1718
---------------------
Validation seg loss: 0.22080550442080735 at epoch 282
epoch =    283/  1000, exp = train
train_iter_loss: 0.18052984774112701
train_iter_loss: 0.3243158161640167
train_iter_loss: 0.10390350967645645
train_iter_loss: 0.16732296347618103
train_iter_loss: 0.22027160227298737
train_iter_loss: 0.18190601468086243
train_iter_loss: 0.10799790173768997
train_iter_loss: 0.16611990332603455
train_iter_loss: 0.1772775799036026
train_iter_loss: 0.17354069650173187
train_iter_loss: 0.12651020288467407
train_iter_loss: 0.21205464005470276
train_iter_loss: 0.28617432713508606
train_iter_loss: 0.11364287883043289
train_iter_loss: 0.1504715234041214
train_iter_loss: 0.15123949944972992
train_iter_loss: 0.16698431968688965
train_iter_loss: 0.06564760208129883
train_iter_loss: 0.2689458429813385
train_iter_loss: 0.20754028856754303
train_iter_loss: 0.22353139519691467
train_iter_loss: 0.10044165700674057
train_iter_loss: 0.264209121465683
train_iter_loss: 0.17539453506469727
train_iter_loss: 0.05394566431641579
train_iter_loss: 0.17524611949920654
train_iter_loss: 0.12114232778549194
train_iter_loss: 0.1467934399843216
train_iter_loss: 0.17768004536628723
train_iter_loss: 0.14662081003189087
train_iter_loss: 0.1754646748304367
train_iter_loss: 0.13747191429138184
train_iter_loss: 0.29681387543678284
train_iter_loss: 0.11171238869428635
train_iter_loss: 0.23412546515464783
train_iter_loss: 0.10240865498781204
train_iter_loss: 0.1827009916305542
train_iter_loss: 0.09586474299430847
train_iter_loss: 0.1393183469772339
train_iter_loss: 0.09546571969985962
train_iter_loss: 0.2372560352087021
train_iter_loss: 0.09260893613100052
train_iter_loss: 0.31342941522598267
train_iter_loss: 0.15172415971755981
train_iter_loss: 0.19726677238941193
train_iter_loss: 0.12996095418930054
train_iter_loss: 0.12880326807498932
train_iter_loss: 0.24564149975776672
train_iter_loss: 0.19659052789211273
train_iter_loss: 0.1388157457113266
train_iter_loss: 0.25086450576782227
train_iter_loss: 0.17968401312828064
train_iter_loss: 0.07264937460422516
train_iter_loss: 0.22556690871715546
train_iter_loss: 0.2973097264766693
train_iter_loss: 0.1571923941373825
train_iter_loss: 0.15636572241783142
train_iter_loss: 0.10302796959877014
train_iter_loss: 0.1438901126384735
train_iter_loss: 0.16037233173847198
train_iter_loss: 0.0875287801027298
train_iter_loss: 0.1249835193157196
train_iter_loss: 0.20631644129753113
train_iter_loss: 0.17707039415836334
train_iter_loss: 0.20757044851779938
train_iter_loss: 0.1977149099111557
train_iter_loss: 0.2417284995317459
train_iter_loss: 0.15244628489017487
train_iter_loss: 0.15475410223007202
train_iter_loss: 0.1001911461353302
train_iter_loss: 0.151404470205307
train_iter_loss: 0.13292695581912994
train_iter_loss: 0.1865522712469101
train_iter_loss: 0.13044919073581696
train_iter_loss: 0.12656556069850922
train_iter_loss: 0.3143329322338104
train_iter_loss: 0.09528712928295135
train_iter_loss: 0.16548213362693787
train_iter_loss: 0.10417287051677704
train_iter_loss: 0.18330535292625427
train_iter_loss: 0.1135324090719223
train_iter_loss: 0.15511742234230042
train_iter_loss: 0.3057326078414917
train_iter_loss: 0.18196901679039001
train_iter_loss: 0.18192178010940552
train_iter_loss: 0.10167981684207916
train_iter_loss: 0.2170584797859192
train_iter_loss: 0.20133858919143677
train_iter_loss: 0.24977168440818787
train_iter_loss: 0.12476704269647598
train_iter_loss: 0.14664290845394135
train_iter_loss: 0.057621028274297714
train_iter_loss: 0.19546498358249664
train_iter_loss: 0.18768079578876495
train_iter_loss: 0.22788165509700775
train_iter_loss: 0.1187191978096962
train_iter_loss: 0.12114466726779938
train_iter_loss: 0.32558420300483704
train_iter_loss: 0.1751016229391098
train_iter_loss: 0.1721259206533432
train loss :0.1726
---------------------
Validation seg loss: 0.22005520819879645 at epoch 283
epoch =    284/  1000, exp = train
train_iter_loss: 0.08054476231336594
train_iter_loss: 0.08323357254266739
train_iter_loss: 0.03307186812162399
train_iter_loss: 0.309939980506897
train_iter_loss: 0.1320849061012268
train_iter_loss: 0.1743614375591278
train_iter_loss: 0.1451009064912796
train_iter_loss: 0.1602490246295929
train_iter_loss: 0.24099548161029816
train_iter_loss: 0.13192854821681976
train_iter_loss: 0.21224716305732727
train_iter_loss: 0.09917833656072617
train_iter_loss: 0.11671643704175949
train_iter_loss: 0.08866042643785477
train_iter_loss: 0.17336377501487732
train_iter_loss: 0.1776198297739029
train_iter_loss: 0.11552508920431137
train_iter_loss: 0.08855459094047546
train_iter_loss: 0.13148513436317444
train_iter_loss: 0.16941148042678833
train_iter_loss: 0.2487931102514267
train_iter_loss: 0.2051958590745926
train_iter_loss: 0.11457881331443787
train_iter_loss: 0.15113677084445953
train_iter_loss: 0.2006264179944992
train_iter_loss: 0.2547493875026703
train_iter_loss: 0.11537080258131027
train_iter_loss: 0.1476144790649414
train_iter_loss: 0.1203131377696991
train_iter_loss: 0.08554103970527649
train_iter_loss: 0.18254268169403076
train_iter_loss: 0.11436334252357483
train_iter_loss: 0.2104371041059494
train_iter_loss: 0.1792251467704773
train_iter_loss: 0.20752272009849548
train_iter_loss: 0.1842191219329834
train_iter_loss: 0.21464376151561737
train_iter_loss: 0.18733727931976318
train_iter_loss: 0.10466516762971878
train_iter_loss: 0.1465819627046585
train_iter_loss: 0.2009248584508896
train_iter_loss: 0.1422998160123825
train_iter_loss: 0.10303381830453873
train_iter_loss: 0.23715873062610626
train_iter_loss: 0.1686740666627884
train_iter_loss: 0.16199016571044922
train_iter_loss: 0.10750866681337357
train_iter_loss: 0.0982036218047142
train_iter_loss: 0.2371796816587448
train_iter_loss: 0.25577273964881897
train_iter_loss: 0.09644235670566559
train_iter_loss: 0.11431451141834259
train_iter_loss: 0.08563976734876633
train_iter_loss: 0.10941006243228912
train_iter_loss: 0.11864829063415527
train_iter_loss: 0.10682619363069534
train_iter_loss: 0.1213744506239891
train_iter_loss: 0.23824134469032288
train_iter_loss: 0.0941518247127533
train_iter_loss: 0.267514169216156
train_iter_loss: 0.22175277769565582
train_iter_loss: 0.15614140033721924
train_iter_loss: 0.2579768896102905
train_iter_loss: 0.17968089878559113
train_iter_loss: 0.423521488904953
train_iter_loss: 0.16444461047649384
train_iter_loss: 0.19856342673301697
train_iter_loss: 0.15069812536239624
train_iter_loss: 0.12631729245185852
train_iter_loss: 0.21513882279396057
train_iter_loss: 0.15144097805023193
train_iter_loss: 0.14764633774757385
train_iter_loss: 0.15524113178253174
train_iter_loss: 0.17910106480121613
train_iter_loss: 0.1642124056816101
train_iter_loss: 0.18984930217266083
train_iter_loss: 0.178145170211792
train_iter_loss: 0.04019850492477417
train_iter_loss: 0.18205516040325165
train_iter_loss: 0.1349755972623825
train_iter_loss: 0.11527632921934128
train_iter_loss: 0.20646998286247253
train_iter_loss: 0.24664029479026794
train_iter_loss: 0.10495716333389282
train_iter_loss: 0.1576627492904663
train_iter_loss: 0.15306508541107178
train_iter_loss: 0.055117569863796234
train_iter_loss: 0.1098468229174614
train_iter_loss: 0.10196438431739807
train_iter_loss: 0.14866094291210175
train_iter_loss: 0.17147651314735413
train_iter_loss: 0.15155750513076782
train_iter_loss: 0.32675573229789734
train_iter_loss: 0.17000284790992737
train_iter_loss: 0.07245893031358719
train_iter_loss: 0.198939710855484
train_iter_loss: 0.14703617990016937
train_iter_loss: 0.16396740078926086
train_iter_loss: 0.07747598737478256
train_iter_loss: 0.18376165628433228
train loss :0.1616
---------------------
Validation seg loss: 0.21792676971944155 at epoch 284
epoch =    285/  1000, exp = train
train_iter_loss: 0.09613794833421707
train_iter_loss: 0.09969493746757507
train_iter_loss: 0.15405872464179993
train_iter_loss: 0.1808745265007019
train_iter_loss: 0.22675395011901855
train_iter_loss: 0.12591372430324554
train_iter_loss: 0.15008121728897095
train_iter_loss: 0.14226305484771729
train_iter_loss: 0.09000734984874725
train_iter_loss: 0.17226076126098633
train_iter_loss: 0.15933920443058014
train_iter_loss: 0.17148305475711823
train_iter_loss: 0.15191903710365295
train_iter_loss: 0.11381281912326813
train_iter_loss: 0.1270885169506073
train_iter_loss: 0.17054633796215057
train_iter_loss: 0.1447739601135254
train_iter_loss: 0.240703284740448
train_iter_loss: 0.19969689846038818
train_iter_loss: 0.14309626817703247
train_iter_loss: 0.17626136541366577
train_iter_loss: 0.22057893872261047
train_iter_loss: 0.2032577246427536
train_iter_loss: 0.366169810295105
train_iter_loss: 0.1291392296552658
train_iter_loss: 0.240591362118721
train_iter_loss: 0.1268915832042694
train_iter_loss: 0.07020042091608047
train_iter_loss: 0.23473650217056274
train_iter_loss: 0.09986454993486404
train_iter_loss: 0.1867699772119522
train_iter_loss: 0.11539802700281143
train_iter_loss: 0.10399944335222244
train_iter_loss: 0.28700879216194153
train_iter_loss: 0.15021277964115143
train_iter_loss: 0.18750767409801483
train_iter_loss: 0.16213080286979675
train_iter_loss: 0.13606677949428558
train_iter_loss: 0.15823133289813995
train_iter_loss: 0.070522740483284
train_iter_loss: 0.32250845432281494
train_iter_loss: 0.14155158400535583
train_iter_loss: 0.21902674436569214
train_iter_loss: 0.07271014899015427
train_iter_loss: 0.16020607948303223
train_iter_loss: 0.1941472291946411
train_iter_loss: 0.19310125708580017
train_iter_loss: 0.09190990030765533
train_iter_loss: 0.1628047376871109
train_iter_loss: 0.19411052763462067
train_iter_loss: 0.29623615741729736
train_iter_loss: 0.1456119269132614
train_iter_loss: 0.1793525218963623
train_iter_loss: 0.11656486243009567
train_iter_loss: 0.08161856979131699
train_iter_loss: 0.170980304479599
train_iter_loss: 0.2040310651063919
train_iter_loss: 0.07795114815235138
train_iter_loss: 0.09457938373088837
train_iter_loss: 0.189378023147583
train_iter_loss: 0.20555123686790466
train_iter_loss: 0.08385524153709412
train_iter_loss: 0.23536796867847443
train_iter_loss: 0.13573722541332245
train_iter_loss: 0.14909344911575317
train_iter_loss: 0.09666317701339722
train_iter_loss: 0.18157832324504852
train_iter_loss: 0.11243433505296707
train_iter_loss: 0.15008535981178284
train_iter_loss: 0.09222280979156494
train_iter_loss: 0.15432125329971313
train_iter_loss: 0.15814316272735596
train_iter_loss: 0.20606069266796112
train_iter_loss: 0.16061516106128693
train_iter_loss: 0.3053234815597534
train_iter_loss: 0.12356875091791153
train_iter_loss: 0.13934534788131714
train_iter_loss: 0.1322241723537445
train_iter_loss: 0.1328451931476593
train_iter_loss: 0.22337551414966583
train_iter_loss: 0.1451594978570938
train_iter_loss: 0.07277283072471619
train_iter_loss: 0.20799745619297028
train_iter_loss: 0.15005822479724884
train_iter_loss: 0.22981879115104675
train_iter_loss: 0.23504653573036194
train_iter_loss: 0.20389513671398163
train_iter_loss: 0.18426139652729034
train_iter_loss: 0.35572272539138794
train_iter_loss: 0.10872246325016022
train_iter_loss: 0.11668846011161804
train_iter_loss: 0.25739455223083496
train_iter_loss: 0.25028079748153687
train_iter_loss: 0.21199177205562592
train_iter_loss: 0.16638918220996857
train_iter_loss: 0.23740701377391815
train_iter_loss: 0.09313894063234329
train_iter_loss: 0.105869360268116
train_iter_loss: 0.11418747901916504
train_iter_loss: 0.20196354389190674
train loss :0.1678
---------------------
Validation seg loss: 0.22289329403484204 at epoch 285
epoch =    286/  1000, exp = train
train_iter_loss: 0.166630357503891
train_iter_loss: 0.14007464051246643
train_iter_loss: 0.1702294647693634
train_iter_loss: 0.11340219527482986
train_iter_loss: 0.30451154708862305
train_iter_loss: 0.2098245769739151
train_iter_loss: 0.17648500204086304
train_iter_loss: 0.12433230131864548
train_iter_loss: 0.10264350473880768
train_iter_loss: 0.14691810309886932
train_iter_loss: 0.2788335084915161
train_iter_loss: 0.15697714686393738
train_iter_loss: 0.18644699454307556
train_iter_loss: 0.15874475240707397
train_iter_loss: 0.19947358965873718
train_iter_loss: 0.23228143155574799
train_iter_loss: 0.12588702142238617
train_iter_loss: 0.1880476027727127
train_iter_loss: 0.1675678938627243
train_iter_loss: 0.07865744084119797
train_iter_loss: 0.1812816560268402
train_iter_loss: 0.06341027468442917
train_iter_loss: 0.09404531121253967
train_iter_loss: 0.05942432954907417
train_iter_loss: 0.13430045545101166
train_iter_loss: 0.22490814328193665
train_iter_loss: 0.15751737356185913
train_iter_loss: 0.17042791843414307
train_iter_loss: 0.10570777207612991
train_iter_loss: 0.11826100945472717
train_iter_loss: 0.1975717395544052
train_iter_loss: 0.38223975896835327
train_iter_loss: 0.1839011162519455
train_iter_loss: 0.138161301612854
train_iter_loss: 0.15316177904605865
train_iter_loss: 0.1802019476890564
train_iter_loss: 0.22545145452022552
train_iter_loss: 0.12396572530269623
train_iter_loss: 0.18402640521526337
train_iter_loss: 0.15230906009674072
train_iter_loss: 0.3004946708679199
train_iter_loss: 0.18179951608181
train_iter_loss: 0.16910581290721893
train_iter_loss: 0.08305732905864716
train_iter_loss: 0.2496449053287506
train_iter_loss: 0.1756276786327362
train_iter_loss: 0.0948944240808487
train_iter_loss: 0.09597066789865494
train_iter_loss: 0.14804379642009735
train_iter_loss: 0.2012178897857666
train_iter_loss: 0.1987576186656952
train_iter_loss: 0.13042478263378143
train_iter_loss: 0.2002098262310028
train_iter_loss: 0.10244538635015488
train_iter_loss: 0.2636202275753021
train_iter_loss: 0.14017169177532196
train_iter_loss: 0.1263590008020401
train_iter_loss: 0.12877832353115082
train_iter_loss: 0.16573035717010498
train_iter_loss: 0.25994375348091125
train_iter_loss: 0.05774863064289093
train_iter_loss: 0.22664211690425873
train_iter_loss: 0.12253767251968384
train_iter_loss: 0.24037712812423706
train_iter_loss: 0.15206745266914368
train_iter_loss: 0.17902816832065582
train_iter_loss: 0.11413539946079254
train_iter_loss: 0.14926902949810028
train_iter_loss: 0.2593482732772827
train_iter_loss: 0.2962719798088074
train_iter_loss: 0.055557120591402054
train_iter_loss: 0.25019019842147827
train_iter_loss: 0.15235210955142975
train_iter_loss: 0.15318487584590912
train_iter_loss: 0.13045598566532135
train_iter_loss: 0.15337353944778442
train_iter_loss: 0.1653010994195938
train_iter_loss: 0.1325020045042038
train_iter_loss: 0.1442963033914566
train_iter_loss: 0.055514004081487656
train_iter_loss: 0.23847322165966034
train_iter_loss: 0.15200403332710266
train_iter_loss: 0.13938689231872559
train_iter_loss: 0.11225748062133789
train_iter_loss: 0.20930665731430054
train_iter_loss: 0.3326473832130432
train_iter_loss: 0.2553451657295227
train_iter_loss: 0.15349340438842773
train_iter_loss: 0.12945792078971863
train_iter_loss: 0.22121936082839966
train_iter_loss: 0.09308837354183197
train_iter_loss: 0.15809448063373566
train_iter_loss: 0.15106067061424255
train_iter_loss: 0.2990979254245758
train_iter_loss: 0.29736271500587463
train_iter_loss: 0.13649281859397888
train_iter_loss: 0.1257893145084381
train_iter_loss: 0.1303798258304596
train_iter_loss: 0.20142856240272522
train_iter_loss: 0.08189032971858978
train loss :0.1705
---------------------
Validation seg loss: 0.21929464268691415 at epoch 286
epoch =    287/  1000, exp = train
train_iter_loss: 0.1684560775756836
train_iter_loss: 0.17651911079883575
train_iter_loss: 0.21677078306674957
train_iter_loss: 0.21411366760730743
train_iter_loss: 0.13447360694408417
train_iter_loss: 0.12887868285179138
train_iter_loss: 0.16075648367404938
train_iter_loss: 0.19428622722625732
train_iter_loss: 0.056695494800806046
train_iter_loss: 0.09299542754888535
train_iter_loss: 0.08746544271707535
train_iter_loss: 0.13243451714515686
train_iter_loss: 0.17397719621658325
train_iter_loss: 0.14081703126430511
train_iter_loss: 0.25546008348464966
train_iter_loss: 0.09934023767709732
train_iter_loss: 0.21910449862480164
train_iter_loss: 0.12027784436941147
train_iter_loss: 0.14400984346866608
train_iter_loss: 0.2071303129196167
train_iter_loss: 0.10611993074417114
train_iter_loss: 0.22162465751171112
train_iter_loss: 0.2366001456975937
train_iter_loss: 0.09487473964691162
train_iter_loss: 0.10272728651762009
train_iter_loss: 0.19855086505413055
train_iter_loss: 0.3264191150665283
train_iter_loss: 0.04162783548235893
train_iter_loss: 0.08180361241102219
train_iter_loss: 0.14146307110786438
train_iter_loss: 0.18543946743011475
train_iter_loss: 0.14662125706672668
train_iter_loss: 0.15401101112365723
train_iter_loss: 0.16636690497398376
train_iter_loss: 0.11970516294240952
train_iter_loss: 0.30990922451019287
train_iter_loss: 0.11822453886270523
train_iter_loss: 0.18278904259204865
train_iter_loss: 0.14093855023384094
train_iter_loss: 0.11122908443212509
train_iter_loss: 0.17993927001953125
train_iter_loss: 0.18227612972259521
train_iter_loss: 0.06763067841529846
train_iter_loss: 0.1470125913619995
train_iter_loss: 0.10775456577539444
train_iter_loss: 0.16627949476242065
train_iter_loss: 0.3159264028072357
train_iter_loss: 0.1401028335094452
train_iter_loss: 0.15664732456207275
train_iter_loss: 0.24203620851039886
train_iter_loss: 0.18124380707740784
train_iter_loss: 0.18942326307296753
train_iter_loss: 0.22953948378562927
train_iter_loss: 0.33669644594192505
train_iter_loss: 0.22813817858695984
train_iter_loss: 0.08212938159704208
train_iter_loss: 0.21405772864818573
train_iter_loss: 0.14740967750549316
train_iter_loss: 0.1980312466621399
train_iter_loss: 0.19495634734630585
train_iter_loss: 0.09099920839071274
train_iter_loss: 0.1306905746459961
train_iter_loss: 0.0786636471748352
train_iter_loss: 0.1157493144273758
train_iter_loss: 0.15905645489692688
train_iter_loss: 0.12488440424203873
train_iter_loss: 0.19928179681301117
train_iter_loss: 0.1368931531906128
train_iter_loss: 0.19687959551811218
train_iter_loss: 0.1669381707906723
train_iter_loss: 0.0831901952624321
train_iter_loss: 0.08293095976114273
train_iter_loss: 0.17653867602348328
train_iter_loss: 0.23689334094524384
train_iter_loss: 0.08951132744550705
train_iter_loss: 0.12270893156528473
train_iter_loss: 0.3267521858215332
train_iter_loss: 0.11715047061443329
train_iter_loss: 0.09984590113162994
train_iter_loss: 0.1777881383895874
train_iter_loss: 0.2551928162574768
train_iter_loss: 0.15204614400863647
train_iter_loss: 0.16725556552410126
train_iter_loss: 0.16174796223640442
train_iter_loss: 0.12412979453802109
train_iter_loss: 0.1305539906024933
train_iter_loss: 0.2097063809633255
train_iter_loss: 0.3525077700614929
train_iter_loss: 0.1536337286233902
train_iter_loss: 0.13188999891281128
train_iter_loss: 0.28263863921165466
train_iter_loss: 0.046989262104034424
train_iter_loss: 0.1077917069196701
train_iter_loss: 0.16847415268421173
train_iter_loss: 0.18512776494026184
train_iter_loss: 0.1724724918603897
train_iter_loss: 0.2323167324066162
train_iter_loss: 0.19696810841560364
train_iter_loss: 0.1611008644104004
train_iter_loss: 0.10419672727584839
train loss :0.1660
---------------------
Validation seg loss: 0.22051767445222106 at epoch 287
epoch =    288/  1000, exp = train
train_iter_loss: 0.09524035453796387
train_iter_loss: 0.202119380235672
train_iter_loss: 0.08855746686458588
train_iter_loss: 0.1805972456932068
train_iter_loss: 0.30561158061027527
train_iter_loss: 0.20234039425849915
train_iter_loss: 0.13833381235599518
train_iter_loss: 0.12753230333328247
train_iter_loss: 0.23370158672332764
train_iter_loss: 0.19712218642234802
train_iter_loss: 0.04500683769583702
train_iter_loss: 0.1208108514547348
train_iter_loss: 0.1106325313448906
train_iter_loss: 0.16130997240543365
train_iter_loss: 0.13444086909294128
train_iter_loss: 0.03111509419977665
train_iter_loss: 0.1919483095407486
train_iter_loss: 0.10305345803499222
train_iter_loss: 0.15227720141410828
train_iter_loss: 0.20325228571891785
train_iter_loss: 0.20147250592708588
train_iter_loss: 0.08127004653215408
train_iter_loss: 0.15264810621738434
train_iter_loss: 0.2154662162065506
train_iter_loss: 0.1268063634634018
train_iter_loss: 0.14376848936080933
train_iter_loss: 0.21728220582008362
train_iter_loss: 0.2220841497182846
train_iter_loss: 0.11754267662763596
train_iter_loss: 0.11868399381637573
train_iter_loss: 0.32955101132392883
train_iter_loss: 0.17657488584518433
train_iter_loss: 0.24171623587608337
train_iter_loss: 0.17565089464187622
train_iter_loss: 0.13865408301353455
train_iter_loss: 0.1316228061914444
train_iter_loss: 0.18516989052295685
train_iter_loss: 0.07926966995000839
train_iter_loss: 0.14523302018642426
train_iter_loss: 0.19055651128292084
train_iter_loss: 0.12148279696702957
train_iter_loss: 0.04214863479137421
train_iter_loss: 0.15948620438575745
train_iter_loss: 0.21069248020648956
train_iter_loss: 0.17111580073833466
train_iter_loss: 0.14860451221466064
train_iter_loss: 0.23148280382156372
train_iter_loss: 0.22896574437618256
train_iter_loss: 0.20372462272644043
train_iter_loss: 0.17878732085227966
train_iter_loss: 0.18615195155143738
train_iter_loss: 0.20798437297344208
train_iter_loss: 0.17834243178367615
train_iter_loss: 0.126369908452034
train_iter_loss: 0.1349167376756668
train_iter_loss: 0.17028573155403137
train_iter_loss: 0.3747249245643616
train_iter_loss: 0.08111158013343811
train_iter_loss: 0.2012237012386322
train_iter_loss: 0.1696557104587555
train_iter_loss: 0.1991175264120102
train_iter_loss: 0.048811204731464386
train_iter_loss: 0.2753526270389557
train_iter_loss: 0.1349993348121643
train_iter_loss: 0.1553863137960434
train_iter_loss: 0.17121797800064087
train_iter_loss: 0.14371410012245178
train_iter_loss: 0.12442518770694733
train_iter_loss: 0.14998933672904968
train_iter_loss: 0.1298830658197403
train_iter_loss: 0.06504411995410919
train_iter_loss: 0.1493016630411148
train_iter_loss: 0.11487281322479248
train_iter_loss: 0.22745440900325775
train_iter_loss: 0.15281865000724792
train_iter_loss: 0.1083616316318512
train_iter_loss: 0.10507755726575851
train_iter_loss: 0.12287972122430801
train_iter_loss: 0.15548765659332275
train_iter_loss: 0.16359201073646545
train_iter_loss: 0.17487207055091858
train_iter_loss: 0.104130819439888
train_iter_loss: 0.2511102855205536
train_iter_loss: 0.26391106843948364
train_iter_loss: 0.1870940625667572
train_iter_loss: 0.12014035135507584
train_iter_loss: 0.18280641734600067
train_iter_loss: 0.0963096097111702
train_iter_loss: 0.17684869468212128
train_iter_loss: 0.2370748072862625
train_iter_loss: 0.2681456506252289
train_iter_loss: 0.21778149902820587
train_iter_loss: 0.2326388657093048
train_iter_loss: 0.16318480670452118
train_iter_loss: 0.0993373766541481
train_iter_loss: 0.25660762190818787
train_iter_loss: 0.0958053469657898
train_iter_loss: 0.10393939167261124
train_iter_loss: 0.11833293735980988
train_iter_loss: 0.0964726060628891
train loss :0.1645
---------------------
Validation seg loss: 0.22243649131214283 at epoch 288
epoch =    289/  1000, exp = train
train_iter_loss: 0.1063988134264946
train_iter_loss: 0.12299191951751709
train_iter_loss: 0.10068100690841675
train_iter_loss: 0.14477591216564178
train_iter_loss: 0.17328490316867828
train_iter_loss: 0.2000129073858261
train_iter_loss: 0.09843575954437256
train_iter_loss: 0.1668216437101364
train_iter_loss: 0.22327329218387604
train_iter_loss: 0.19936427474021912
train_iter_loss: 0.21212413907051086
train_iter_loss: 0.23020166158676147
train_iter_loss: 0.07001089304685593
train_iter_loss: 0.1955970972776413
train_iter_loss: 0.18894334137439728
train_iter_loss: 0.36153048276901245
train_iter_loss: 0.12200742214918137
train_iter_loss: 0.16025829315185547
train_iter_loss: 0.18983708322048187
train_iter_loss: 0.09372209757566452
train_iter_loss: 0.09089391678571701
train_iter_loss: 0.16768322885036469
train_iter_loss: 0.14694951474666595
train_iter_loss: 0.11956185847520828
train_iter_loss: 0.08306104689836502
train_iter_loss: 0.1705407202243805
train_iter_loss: 0.2215423732995987
train_iter_loss: 0.0738474652171135
train_iter_loss: 0.1650272011756897
train_iter_loss: 0.09416186809539795
train_iter_loss: 0.2227063775062561
train_iter_loss: 0.11016422510147095
train_iter_loss: 0.07165201008319855
train_iter_loss: 0.1567945033311844
train_iter_loss: 0.1747828722000122
train_iter_loss: 0.17869408428668976
train_iter_loss: 0.19204850494861603
train_iter_loss: 0.10537387430667877
train_iter_loss: 0.2543661296367645
train_iter_loss: 0.11152581870555878
train_iter_loss: 0.1387181282043457
train_iter_loss: 0.09615612775087357
train_iter_loss: 0.21494634449481964
train_iter_loss: 0.0898127406835556
train_iter_loss: 0.2024686187505722
train_iter_loss: 0.18800410628318787
train_iter_loss: 0.27637454867362976
train_iter_loss: 0.07351448386907578
train_iter_loss: 0.1759403496980667
train_iter_loss: 0.1697668880224228
train_iter_loss: 0.17622309923171997
train_iter_loss: 0.09556902945041656
train_iter_loss: 0.09123902022838593
train_iter_loss: 0.18520395457744598
train_iter_loss: 0.1244565024971962
train_iter_loss: 0.11247868090867996
train_iter_loss: 0.17805510759353638
train_iter_loss: 0.23781314492225647
train_iter_loss: 0.4222744405269623
train_iter_loss: 0.162001833319664
train_iter_loss: 0.1711455136537552
train_iter_loss: 0.11130385845899582
train_iter_loss: 0.1447707861661911
train_iter_loss: 0.1918942779302597
train_iter_loss: 0.2228250950574875
train_iter_loss: 0.23690035939216614
train_iter_loss: 0.2689869701862335
train_iter_loss: 0.17215430736541748
train_iter_loss: 0.11942317336797714
train_iter_loss: 0.08448202162981033
train_iter_loss: 0.24621787667274475
train_iter_loss: 0.19535881280899048
train_iter_loss: 0.10872527211904526
train_iter_loss: 0.07687534391880035
train_iter_loss: 0.17835387587547302
train_iter_loss: 0.2390771359205246
train_iter_loss: 0.1058160737156868
train_iter_loss: 0.06034660339355469
train_iter_loss: 0.279318243265152
train_iter_loss: 0.1398586630821228
train_iter_loss: 0.093620665371418
train_iter_loss: 0.1282232105731964
train_iter_loss: 0.1454920619726181
train_iter_loss: 0.1353808045387268
train_iter_loss: 0.14857997000217438
train_iter_loss: 0.13077683746814728
train_iter_loss: 0.051293544471263885
train_iter_loss: 0.38588011264801025
train_iter_loss: 0.14504992961883545
train_iter_loss: 0.15369877219200134
train_iter_loss: 0.10561825335025787
train_iter_loss: 0.23068273067474365
train_iter_loss: 0.12734690308570862
train_iter_loss: 0.31820055842399597
train_iter_loss: 0.12105268985033035
train_iter_loss: 0.14403599500656128
train_iter_loss: 0.23372413218021393
train_iter_loss: 0.14199994504451752
train_iter_loss: 0.13484065234661102
train_iter_loss: 0.22745053470134735
train loss :0.1650
---------------------
Validation seg loss: 0.2207883147364658 at epoch 289
epoch =    290/  1000, exp = train
train_iter_loss: 0.15652801096439362
train_iter_loss: 0.10389912128448486
train_iter_loss: 0.12631972134113312
train_iter_loss: 0.2471056580543518
train_iter_loss: 0.15066607296466827
train_iter_loss: 0.23081225156784058
train_iter_loss: 0.1510341912508011
train_iter_loss: 0.21834027767181396
train_iter_loss: 0.14015136659145355
train_iter_loss: 0.12732109427452087
train_iter_loss: 0.13594400882720947
train_iter_loss: 0.12319347262382507
train_iter_loss: 0.125642791390419
train_iter_loss: 0.20258542895317078
train_iter_loss: 0.09705806523561478
train_iter_loss: 0.2578910291194916
train_iter_loss: 0.12336807698011398
train_iter_loss: 0.266159325838089
train_iter_loss: 0.1978672444820404
train_iter_loss: 0.10500765591859818
train_iter_loss: 0.23931202292442322
train_iter_loss: 0.23826751112937927
train_iter_loss: 0.15593349933624268
train_iter_loss: 0.28244081139564514
train_iter_loss: 0.24128438532352448
train_iter_loss: 0.14768819510936737
train_iter_loss: 0.2546251118183136
train_iter_loss: 0.2788577079772949
train_iter_loss: 0.11865263432264328
train_iter_loss: 0.09838312119245529
train_iter_loss: 0.11182445287704468
train_iter_loss: 0.1096939668059349
train_iter_loss: 0.2823845446109772
train_iter_loss: 0.14637491106987
train_iter_loss: 0.14278414845466614
train_iter_loss: 0.13002309203147888
train_iter_loss: 0.04963159188628197
train_iter_loss: 0.10817453265190125
train_iter_loss: 0.1391076147556305
train_iter_loss: 0.1428837925195694
train_iter_loss: 0.22368356585502625
train_iter_loss: 0.14764255285263062
train_iter_loss: 0.11089842766523361
train_iter_loss: 0.22534407675266266
train_iter_loss: 0.08147819340229034
train_iter_loss: 0.08649936318397522
train_iter_loss: 0.10838014632463455
train_iter_loss: 0.11333784461021423
train_iter_loss: 0.10153146833181381
train_iter_loss: 0.07876081764698029
train_iter_loss: 0.13078998029232025
train_iter_loss: 0.22172942757606506
train_iter_loss: 0.1437268704175949
train_iter_loss: 0.210871621966362
train_iter_loss: 0.16594266891479492
train_iter_loss: 0.16261664032936096
train_iter_loss: 0.4940425753593445
train_iter_loss: 0.14006724953651428
train_iter_loss: 0.2128349095582962
train_iter_loss: 0.14556895196437836
train_iter_loss: 0.12969279289245605
train_iter_loss: 0.2738654315471649
train_iter_loss: 0.18004921078681946
train_iter_loss: 0.1695990413427353
train_iter_loss: 0.24845921993255615
train_iter_loss: 0.2831118404865265
train_iter_loss: 0.192666158080101
train_iter_loss: 0.23413187265396118
train_iter_loss: 0.2700141966342926
train_iter_loss: 0.10849551111459732
train_iter_loss: 0.0638323649764061
train_iter_loss: 0.18518809974193573
train_iter_loss: 0.17527270317077637
train_iter_loss: 0.0982080027461052
train_iter_loss: 0.2405434399843216
train_iter_loss: 0.3544946610927582
train_iter_loss: 0.12186393141746521
train_iter_loss: 0.2032119184732437
train_iter_loss: 0.1778590977191925
train_iter_loss: 0.1501893848180771
train_iter_loss: 0.21093860268592834
train_iter_loss: 0.07908274978399277
train_iter_loss: 0.17052564024925232
train_iter_loss: 0.19894945621490479
train_iter_loss: 0.12358725070953369
train_iter_loss: 0.11875202506780624
train_iter_loss: 0.2130734771490097
train_iter_loss: 0.20728585124015808
train_iter_loss: 0.14273540675640106
train_iter_loss: 0.03677193075418472
train_iter_loss: 0.17726682126522064
train_iter_loss: 0.15590636432170868
train_iter_loss: 0.2488286793231964
train_iter_loss: 0.242782860994339
train_iter_loss: 0.2741660475730896
train_iter_loss: 0.12478388100862503
train_iter_loss: 0.14637745916843414
train_iter_loss: 0.1250680834054947
train_iter_loss: 0.14335446059703827
train_iter_loss: 0.08047773689031601
train loss :0.1726
---------------------
Validation seg loss: 0.21851723975906112 at epoch 290
epoch =    291/  1000, exp = train
train_iter_loss: 0.22501206398010254
train_iter_loss: 0.1969676911830902
train_iter_loss: 0.16838988661766052
train_iter_loss: 0.19914667308330536
train_iter_loss: 0.18522727489471436
train_iter_loss: 0.14141467213630676
train_iter_loss: 0.19125911593437195
train_iter_loss: 0.1275990754365921
train_iter_loss: 0.13655968010425568
train_iter_loss: 0.27046674489974976
train_iter_loss: 0.20903216302394867
train_iter_loss: 0.1738244742155075
train_iter_loss: 0.06580356508493423
train_iter_loss: 0.12086672335863113
train_iter_loss: 0.07963423430919647
train_iter_loss: 0.16053543984889984
train_iter_loss: 0.13808676600456238
train_iter_loss: 0.15019015967845917
train_iter_loss: 0.09268546849489212
train_iter_loss: 0.05875376984477043
train_iter_loss: 0.1673041582107544
train_iter_loss: 0.1175599917769432
train_iter_loss: 0.09102702885866165
train_iter_loss: 0.2560974657535553
train_iter_loss: 0.19934304058551788
train_iter_loss: 0.2744218707084656
train_iter_loss: 0.21460287272930145
train_iter_loss: 0.14834174513816833
train_iter_loss: 0.12712669372558594
train_iter_loss: 0.1442001610994339
train_iter_loss: 0.14520825445652008
train_iter_loss: 0.2279960960149765
train_iter_loss: 0.14764073491096497
train_iter_loss: 0.14821569621562958
train_iter_loss: 0.11981208622455597
train_iter_loss: 0.08371070772409439
train_iter_loss: 0.05776212736964226
train_iter_loss: 0.16263850033283234
train_iter_loss: 0.233407124876976
train_iter_loss: 0.08565890789031982
train_iter_loss: 0.09571895003318787
train_iter_loss: 0.15102314949035645
train_iter_loss: 0.04585854709148407
train_iter_loss: 0.07729334384202957
train_iter_loss: 0.08061046898365021
train_iter_loss: 0.26803910732269287
train_iter_loss: 0.2904398441314697
train_iter_loss: 0.212917760014534
train_iter_loss: 0.11090464890003204
train_iter_loss: 0.267180472612381
train_iter_loss: 0.16611717641353607
train_iter_loss: 0.13192807137966156
train_iter_loss: 0.23403166234493256
train_iter_loss: 0.12198414653539658
train_iter_loss: 0.24092800915241241
train_iter_loss: 0.17757031321525574
train_iter_loss: 0.09092547744512558
train_iter_loss: 0.18136219680309296
train_iter_loss: 0.1477140486240387
train_iter_loss: 0.23741257190704346
train_iter_loss: 0.15986014902591705
train_iter_loss: 0.10356833785772324
train_iter_loss: 0.19799618422985077
train_iter_loss: 0.07749487459659576
train_iter_loss: 0.09588928520679474
train_iter_loss: 0.217623770236969
train_iter_loss: 0.1697198748588562
train_iter_loss: 0.18621324002742767
train_iter_loss: 0.15560570359230042
train_iter_loss: 0.13871580362319946
train_iter_loss: 0.21719709038734436
train_iter_loss: 0.07056315243244171
train_iter_loss: 0.168958380818367
train_iter_loss: 0.18159730732440948
train_iter_loss: 0.3560791015625
train_iter_loss: 0.1671832948923111
train_iter_loss: 0.2355499565601349
train_iter_loss: 0.13065659999847412
train_iter_loss: 0.1225358098745346
train_iter_loss: 0.20912910997867584
train_iter_loss: 0.2259325385093689
train_iter_loss: 0.1828571856021881
train_iter_loss: 0.16156703233718872
train_iter_loss: 0.1413680613040924
train_iter_loss: 0.2189854085445404
train_iter_loss: 0.22923222184181213
train_iter_loss: 0.1459789127111435
train_iter_loss: 0.16332945227622986
train_iter_loss: 0.18550731241703033
train_iter_loss: 0.3227958679199219
train_iter_loss: 0.16878347098827362
train_iter_loss: 0.15572835505008698
train_iter_loss: 0.11216183751821518
train_iter_loss: 0.24045130610466003
train_iter_loss: 0.1921168714761734
train_iter_loss: 0.14558696746826172
train_iter_loss: 0.12012381851673126
train_iter_loss: 0.1322113424539566
train_iter_loss: 0.1192573830485344
train_iter_loss: 0.1411331444978714
train loss :0.1661
---------------------
Validation seg loss: 0.2247431614559214 at epoch 291
epoch =    292/  1000, exp = train
train_iter_loss: 0.1664196252822876
train_iter_loss: 0.16143058240413666
train_iter_loss: 0.23672625422477722
train_iter_loss: 0.15891757607460022
train_iter_loss: 0.1923847198486328
train_iter_loss: 0.16039352118968964
train_iter_loss: 0.17572607100009918
train_iter_loss: 0.16893628239631653
train_iter_loss: 0.11596668511629105
train_iter_loss: 0.12538643181324005
train_iter_loss: 0.17476820945739746
train_iter_loss: 0.11707127839326859
train_iter_loss: 0.26297658681869507
train_iter_loss: 0.17410029470920563
train_iter_loss: 0.19024789333343506
train_iter_loss: 0.16244147717952728
train_iter_loss: 0.15320336818695068
train_iter_loss: 0.15722474455833435
train_iter_loss: 0.1300191432237625
train_iter_loss: 0.17514987289905548
train_iter_loss: 0.1982075721025467
train_iter_loss: 0.06911449879407883
train_iter_loss: 0.08097994327545166
train_iter_loss: 0.14195765554904938
train_iter_loss: 0.16388650238513947
train_iter_loss: 0.20870469510555267
train_iter_loss: 0.0738862082362175
train_iter_loss: 0.2166001796722412
train_iter_loss: 0.16353462636470795
train_iter_loss: 0.18365173041820526
train_iter_loss: 0.14732109010219574
train_iter_loss: 0.21171317994594574
train_iter_loss: 0.06914053857326508
train_iter_loss: 0.13218027353286743
train_iter_loss: 0.19984260201454163
train_iter_loss: 0.20736292004585266
train_iter_loss: 0.1476503312587738
train_iter_loss: 0.3313957750797272
train_iter_loss: 0.0990196019411087
train_iter_loss: 0.112773098051548
train_iter_loss: 0.16141441464424133
train_iter_loss: 0.12707172334194183
train_iter_loss: 0.344077467918396
train_iter_loss: 0.1240479052066803
train_iter_loss: 0.1895376741886139
train_iter_loss: 0.12914952635765076
train_iter_loss: 0.20832356810569763
train_iter_loss: 0.15610483288764954
train_iter_loss: 0.2055186927318573
train_iter_loss: 0.09679382294416428
train_iter_loss: 0.2865245044231415
train_iter_loss: 0.3167833089828491
train_iter_loss: 0.17440944910049438
train_iter_loss: 0.12832647562026978
train_iter_loss: 0.1586141586303711
train_iter_loss: 0.10758306086063385
train_iter_loss: 0.1087379902601242
train_iter_loss: 0.19369736313819885
train_iter_loss: 0.17271831631660461
train_iter_loss: 0.1720157116651535
train_iter_loss: 0.10809000581502914
train_iter_loss: 0.10952024161815643
train_iter_loss: 0.2645091414451599
train_iter_loss: 0.16314558684825897
train_iter_loss: 0.15828484296798706
train_iter_loss: 0.14083980023860931
train_iter_loss: 0.2307196408510208
train_iter_loss: 0.16877496242523193
train_iter_loss: 0.18544615805149078
train_iter_loss: 0.2798026204109192
train_iter_loss: 0.18303681910037994
train_iter_loss: 0.20887310802936554
train_iter_loss: 0.18494179844856262
train_iter_loss: 0.11554604768753052
train_iter_loss: 0.1914108693599701
train_iter_loss: 0.09693140536546707
train_iter_loss: 0.2768145203590393
train_iter_loss: 0.2075502872467041
train_iter_loss: 0.13694405555725098
train_iter_loss: 0.13785989582538605
train_iter_loss: 0.16664069890975952
train_iter_loss: 0.24778898060321808
train_iter_loss: 0.2219369113445282
train_iter_loss: 0.17480354011058807
train_iter_loss: 0.2741764783859253
train_iter_loss: 0.12656080722808838
train_iter_loss: 0.14226217567920685
train_iter_loss: 0.17960898578166962
train_iter_loss: 0.10365752875804901
train_iter_loss: 0.19593653082847595
train_iter_loss: 0.17236708104610443
train_iter_loss: 0.09794566035270691
train_iter_loss: 0.1985464245080948
train_iter_loss: 0.10548970848321915
train_iter_loss: 0.10044016689062119
train_iter_loss: 0.05793716385960579
train_iter_loss: 0.19846515357494354
train_iter_loss: 0.12494135648012161
train_iter_loss: 0.09433390200138092
train_iter_loss: 0.1363898068666458
train loss :0.1692
---------------------
Validation seg loss: 0.22030221141066472 at epoch 292
epoch =    293/  1000, exp = train
train_iter_loss: 0.24093836545944214
train_iter_loss: 0.14468808472156525
train_iter_loss: 0.11104926466941833
train_iter_loss: 0.036670904606580734
train_iter_loss: 0.17621174454689026
train_iter_loss: 0.2409861832857132
train_iter_loss: 0.21485793590545654
train_iter_loss: 0.16478613018989563
train_iter_loss: 0.1471472531557083
train_iter_loss: 0.14192990958690643
train_iter_loss: 0.2993084490299225
train_iter_loss: 0.2486923485994339
train_iter_loss: 0.13972324132919312
train_iter_loss: 0.0741400271654129
train_iter_loss: 0.3123449981212616
train_iter_loss: 0.1507870852947235
train_iter_loss: 0.0823691338300705
train_iter_loss: 0.17082124948501587
train_iter_loss: 0.04876604303717613
train_iter_loss: 0.2715991735458374
train_iter_loss: 0.1514919251203537
train_iter_loss: 0.19029682874679565
train_iter_loss: 0.24416625499725342
train_iter_loss: 0.1401204913854599
train_iter_loss: 0.12004297226667404
train_iter_loss: 0.2038339227437973
train_iter_loss: 0.14861340820789337
train_iter_loss: 0.05753215029835701
train_iter_loss: 0.15180940926074982
train_iter_loss: 0.17818793654441833
train_iter_loss: 0.0843399241566658
train_iter_loss: 0.1226843073964119
train_iter_loss: 0.2035517692565918
train_iter_loss: 0.16130217909812927
train_iter_loss: 0.18328531086444855
train_iter_loss: 0.1811392605304718
train_iter_loss: 0.1658327430486679
train_iter_loss: 0.21172656118869781
train_iter_loss: 0.2019762098789215
train_iter_loss: 0.16929812729358673
train_iter_loss: 0.09777668118476868
train_iter_loss: 0.10594453662633896
train_iter_loss: 0.16058221459388733
train_iter_loss: 0.14034882187843323
train_iter_loss: 0.3016262650489807
train_iter_loss: 0.09733108431100845
train_iter_loss: 0.0912022516131401
train_iter_loss: 0.09621085971593857
train_iter_loss: 0.18625204265117645
train_iter_loss: 0.16294045746326447
train_iter_loss: 0.11247675865888596
train_iter_loss: 0.1826394945383072
train_iter_loss: 0.2253721058368683
train_iter_loss: 0.14967887103557587
train_iter_loss: 0.18354113399982452
train_iter_loss: 0.15816807746887207
train_iter_loss: 0.24033217132091522
train_iter_loss: 0.11225088685750961
train_iter_loss: 0.17257092893123627
train_iter_loss: 0.13872386515140533
train_iter_loss: 0.12337213009595871
train_iter_loss: 0.15619079768657684
train_iter_loss: 0.1727105975151062
train_iter_loss: 0.10389687865972519
train_iter_loss: 0.16085976362228394
train_iter_loss: 0.2322060912847519
train_iter_loss: 0.3475185036659241
train_iter_loss: 0.14145752787590027
train_iter_loss: 0.13647149503231049
train_iter_loss: 0.16011552512645721
train_iter_loss: 0.1726439744234085
train_iter_loss: 0.3023022413253784
train_iter_loss: 0.16021791100502014
train_iter_loss: 0.11613570898771286
train_iter_loss: 0.20335963368415833
train_iter_loss: 0.1366911083459854
train_iter_loss: 0.10599106550216675
train_iter_loss: 0.1421872079372406
train_iter_loss: 0.12514442205429077
train_iter_loss: 0.13154911994934082
train_iter_loss: 0.14616341888904572
train_iter_loss: 0.17268624901771545
train_iter_loss: 0.28491339087486267
train_iter_loss: 0.1761675775051117
train_iter_loss: 0.1401027888059616
train_iter_loss: 0.1821819543838501
train_iter_loss: 0.1852327138185501
train_iter_loss: 0.10640407353639603
train_iter_loss: 0.2514856457710266
train_iter_loss: 0.1708262413740158
train_iter_loss: 0.15576089918613434
train_iter_loss: 0.15030737221240997
train_iter_loss: 0.11541897058486938
train_iter_loss: 0.09379463642835617
train_iter_loss: 0.23766571283340454
train_iter_loss: 0.14468999207019806
train_iter_loss: 0.1886134147644043
train_iter_loss: 0.3577272295951843
train_iter_loss: 0.12298732995986938
train_iter_loss: 0.18316800892353058
train loss :0.1687
---------------------
Validation seg loss: 0.2191677252094279 at epoch 293
epoch =    294/  1000, exp = train
train_iter_loss: 0.11895386874675751
train_iter_loss: 0.188027024269104
train_iter_loss: 0.22205528616905212
train_iter_loss: 0.20692433416843414
train_iter_loss: 0.23030976951122284
train_iter_loss: 0.2052493840456009
train_iter_loss: 0.135559543967247
train_iter_loss: 0.28042516112327576
train_iter_loss: 0.09985457360744476
train_iter_loss: 0.1851695328950882
train_iter_loss: 0.2423185408115387
train_iter_loss: 0.146545872092247
train_iter_loss: 0.1715102344751358
train_iter_loss: 0.17207635939121246
train_iter_loss: 0.2065410017967224
train_iter_loss: 0.15173271298408508
train_iter_loss: 0.12014327943325043
train_iter_loss: 0.1432802379131317
train_iter_loss: 0.1654650866985321
train_iter_loss: 0.11275488883256912
train_iter_loss: 0.14586809277534485
train_iter_loss: 0.15811403095722198
train_iter_loss: 0.17154408991336823
train_iter_loss: 0.18347394466400146
train_iter_loss: 0.11007556319236755
train_iter_loss: 0.13520483672618866
train_iter_loss: 0.21267299354076385
train_iter_loss: 0.1686171293258667
train_iter_loss: 0.05892710015177727
train_iter_loss: 0.16526338458061218
train_iter_loss: 0.14734742045402527
train_iter_loss: 0.20941823720932007
train_iter_loss: 0.22242872416973114
train_iter_loss: 0.21169865131378174
train_iter_loss: 0.4012216031551361
train_iter_loss: 0.1891084611415863
train_iter_loss: 0.1965964138507843
train_iter_loss: 0.1961280107498169
train_iter_loss: 0.1507250815629959
train_iter_loss: 0.21186833083629608
train_iter_loss: 0.12738020718097687
train_iter_loss: 0.37064412236213684
train_iter_loss: 0.16029605269432068
train_iter_loss: 0.17723006010055542
train_iter_loss: 0.08562222123146057
train_iter_loss: 0.20224228501319885
train_iter_loss: 0.0887780413031578
train_iter_loss: 0.18425852060317993
train_iter_loss: 0.25203871726989746
train_iter_loss: 0.24758736789226532
train_iter_loss: 0.09453059732913971
train_iter_loss: 0.2576353847980499
train_iter_loss: 0.08126667141914368
train_iter_loss: 0.11527145653963089
train_iter_loss: 0.09424272179603577
train_iter_loss: 0.11309417337179184
train_iter_loss: 0.07185159623622894
train_iter_loss: 0.1361488550901413
train_iter_loss: 0.24256423115730286
train_iter_loss: 0.1330798864364624
train_iter_loss: 0.10417421907186508
train_iter_loss: 0.15463873744010925
train_iter_loss: 0.07660440355539322
train_iter_loss: 0.13791149854660034
train_iter_loss: 0.151670441031456
train_iter_loss: 0.09635744243860245
train_iter_loss: 0.14316867291927338
train_iter_loss: 0.12055578082799911
train_iter_loss: 0.23477962613105774
train_iter_loss: 0.19690904021263123
train_iter_loss: 0.23876459896564484
train_iter_loss: 0.08841587603092194
train_iter_loss: 0.13251462578773499
train_iter_loss: 0.22655099630355835
train_iter_loss: 0.2061631828546524
train_iter_loss: 0.2334468960762024
train_iter_loss: 0.16805817186832428
train_iter_loss: 0.025817563757300377
train_iter_loss: 0.08858465403318405
train_iter_loss: 0.14502647519111633
train_iter_loss: 0.14170116186141968
train_iter_loss: 0.28995952010154724
train_iter_loss: 0.14554119110107422
train_iter_loss: 0.14817970991134644
train_iter_loss: 0.11384818702936172
train_iter_loss: 0.16728626191616058
train_iter_loss: 0.31012162566185
train_iter_loss: 0.09199942648410797
train_iter_loss: 0.11983644962310791
train_iter_loss: 0.10747874528169632
train_iter_loss: 0.17619791626930237
train_iter_loss: 0.13875994086265564
train_iter_loss: 0.24564304947853088
train_iter_loss: 0.14094868302345276
train_iter_loss: 0.10661498457193375
train_iter_loss: 0.22523659467697144
train_iter_loss: 0.14402814209461212
train_iter_loss: 0.21897312998771667
train_iter_loss: 0.12828423082828522
train_iter_loss: 0.09225676208734512
train loss :0.1678
---------------------
Validation seg loss: 0.22018485674458854 at epoch 294
epoch =    295/  1000, exp = train
train_iter_loss: 0.16572102904319763
train_iter_loss: 0.17204436659812927
train_iter_loss: 0.0976017415523529
train_iter_loss: 0.14093485474586487
train_iter_loss: 0.16480711102485657
train_iter_loss: 0.17839616537094116
train_iter_loss: 0.15737468004226685
train_iter_loss: 0.22147105634212494
train_iter_loss: 0.1368749439716339
train_iter_loss: 0.134029358625412
train_iter_loss: 0.19653604924678802
train_iter_loss: 0.05800406262278557
train_iter_loss: 0.1427362859249115
train_iter_loss: 0.29106661677360535
train_iter_loss: 0.43884122371673584
train_iter_loss: 0.23795369267463684
train_iter_loss: 0.23212376236915588
train_iter_loss: 0.07851335406303406
train_iter_loss: 0.061119191348552704
train_iter_loss: 0.13727572560310364
train_iter_loss: 0.0990559458732605
train_iter_loss: 0.12369465827941895
train_iter_loss: 0.1189124584197998
train_iter_loss: 0.09445659071207047
train_iter_loss: 0.1948302537202835
train_iter_loss: 0.24406041204929352
train_iter_loss: 0.14691920578479767
train_iter_loss: 0.1793348640203476
train_iter_loss: 0.19206178188323975
train_iter_loss: 0.26505008339881897
train_iter_loss: 0.20805716514587402
train_iter_loss: 0.14472520351409912
train_iter_loss: 0.1032639890909195
train_iter_loss: 0.15709654986858368
train_iter_loss: 0.12778633832931519
train_iter_loss: 0.08044818788766861
train_iter_loss: 0.14799225330352783
train_iter_loss: 0.14843203127384186
train_iter_loss: 0.23316200077533722
train_iter_loss: 0.12992256879806519
train_iter_loss: 0.24381205439567566
train_iter_loss: 0.2108299881219864
train_iter_loss: 0.21316741406917572
train_iter_loss: 0.18423090875148773
train_iter_loss: 0.16799777746200562
train_iter_loss: 0.22463831305503845
train_iter_loss: 0.09734498709440231
train_iter_loss: 0.12712180614471436
train_iter_loss: 0.2592785358428955
train_iter_loss: 0.13645868003368378
train_iter_loss: 0.13636808097362518
train_iter_loss: 0.19283750653266907
train_iter_loss: 0.12799420952796936
train_iter_loss: 0.35672175884246826
train_iter_loss: 0.3057553172111511
train_iter_loss: 0.18362154066562653
train_iter_loss: 0.17287057638168335
train_iter_loss: 0.17385025322437286
train_iter_loss: 0.09470725059509277
train_iter_loss: 0.1309916377067566
train_iter_loss: 0.16667324304580688
train_iter_loss: 0.24760156869888306
train_iter_loss: 0.10043301433324814
train_iter_loss: 0.060478582978248596
train_iter_loss: 0.17958815395832062
train_iter_loss: 0.15410956740379333
train_iter_loss: 0.16005688905715942
train_iter_loss: 0.08056743443012238
train_iter_loss: 0.20487989485263824
train_iter_loss: 0.1881905049085617
train_iter_loss: 0.23515819013118744
train_iter_loss: 0.12088126689195633
train_iter_loss: 0.17488019168376923
train_iter_loss: 0.2815471291542053
train_iter_loss: 0.20951785147190094
train_iter_loss: 0.09602265059947968
train_iter_loss: 0.25351178646087646
train_iter_loss: 0.0674595907330513
train_iter_loss: 0.2284885048866272
train_iter_loss: 0.22730877995491028
train_iter_loss: 0.1371326893568039
train_iter_loss: 0.2610785663127899
train_iter_loss: 0.07811829447746277
train_iter_loss: 0.08347497135400772
train_iter_loss: 0.2283797413110733
train_iter_loss: 0.21082523465156555
train_iter_loss: 0.17623333632946014
train_iter_loss: 0.23105882108211517
train_iter_loss: 0.07254725694656372
train_iter_loss: 0.12127679586410522
train_iter_loss: 0.155901238322258
train_iter_loss: 0.13754354417324066
train_iter_loss: 0.12979961931705475
train_iter_loss: 0.09026843309402466
train_iter_loss: 0.13035385310649872
train_iter_loss: 0.1639755666255951
train_iter_loss: 0.23556163907051086
train_iter_loss: 0.18796825408935547
train_iter_loss: 0.12608757615089417
train_iter_loss: 0.17251744866371155
train loss :0.1706
---------------------
Validation seg loss: 0.21977151068419498 at epoch 295
epoch =    296/  1000, exp = train
train_iter_loss: 0.1882910430431366
train_iter_loss: 0.13699284195899963
train_iter_loss: 0.08302070945501328
train_iter_loss: 0.20255398750305176
train_iter_loss: 0.07659980654716492
train_iter_loss: 0.064140684902668
train_iter_loss: 0.2141677439212799
train_iter_loss: 0.17106732726097107
train_iter_loss: 0.21982881426811218
train_iter_loss: 0.1881602257490158
train_iter_loss: 0.11238376796245575
train_iter_loss: 0.16051161289215088
train_iter_loss: 0.08672849088907242
train_iter_loss: 0.14080169796943665
train_iter_loss: 0.23632575571537018
train_iter_loss: 0.10888849943876266
train_iter_loss: 0.15378311276435852
train_iter_loss: 0.2031954824924469
train_iter_loss: 0.18120165169239044
train_iter_loss: 0.23675082623958588
train_iter_loss: 0.16311299800872803
train_iter_loss: 0.1254747211933136
train_iter_loss: 0.2811773717403412
train_iter_loss: 0.1449703872203827
train_iter_loss: 0.1733546257019043
train_iter_loss: 0.14911676943302155
train_iter_loss: 0.22906777262687683
train_iter_loss: 0.1331159621477127
train_iter_loss: 0.1341680884361267
train_iter_loss: 0.0995250940322876
train_iter_loss: 0.24061129987239838
train_iter_loss: 0.16953858733177185
train_iter_loss: 0.20779988169670105
train_iter_loss: 0.12662023305892944
train_iter_loss: 0.16370168328285217
train_iter_loss: 0.17456108331680298
train_iter_loss: 0.16224345564842224
train_iter_loss: 0.30767297744750977
train_iter_loss: 0.11564471572637558
train_iter_loss: 0.16041304171085358
train_iter_loss: 0.051298703998327255
train_iter_loss: 0.10940609127283096
train_iter_loss: 0.18414095044136047
train_iter_loss: 0.1878664493560791
train_iter_loss: 0.22784224152565002
train_iter_loss: 0.1614997237920761
train_iter_loss: 0.12284522503614426
train_iter_loss: 0.11067526787519455
train_iter_loss: 0.08608077466487885
train_iter_loss: 0.09534924477338791
train_iter_loss: 0.16035591065883636
train_iter_loss: 0.22814294695854187
train_iter_loss: 0.11508826166391373
train_iter_loss: 0.11631354689598083
train_iter_loss: 0.11680922657251358
train_iter_loss: 0.4565931260585785
train_iter_loss: 0.11640441417694092
train_iter_loss: 0.25099703669548035
train_iter_loss: 0.18650516867637634
train_iter_loss: 0.16582944989204407
train_iter_loss: 0.07169929146766663
train_iter_loss: 0.09509894996881485
train_iter_loss: 0.13762815296649933
train_iter_loss: 0.10164821147918701
train_iter_loss: 0.2859313189983368
train_iter_loss: 0.1620621234178543
train_iter_loss: 0.1827227771282196
train_iter_loss: 0.1970551759004593
train_iter_loss: 0.12054402381181717
train_iter_loss: 0.20541183650493622
train_iter_loss: 0.2597060203552246
train_iter_loss: 0.11067593842744827
train_iter_loss: 0.12062046676874161
train_iter_loss: 0.35960474610328674
train_iter_loss: 0.142830029129982
train_iter_loss: 0.2462073117494583
train_iter_loss: 0.2576371133327484
train_iter_loss: 0.24313858151435852
train_iter_loss: 0.14132840931415558
train_iter_loss: 0.23843003809452057
train_iter_loss: 0.14733284711837769
train_iter_loss: 0.14025309681892395
train_iter_loss: 0.12189685553312302
train_iter_loss: 0.18902207911014557
train_iter_loss: 0.18079520761966705
train_iter_loss: 0.14767621457576752
train_iter_loss: 0.17274519801139832
train_iter_loss: 0.1463543027639389
train_iter_loss: 0.20141196250915527
train_iter_loss: 0.1487613022327423
train_iter_loss: 0.19627854228019714
train_iter_loss: 0.17427030205726624
train_iter_loss: 0.1745772659778595
train_iter_loss: 0.24586668610572815
train_iter_loss: 0.14932183921337128
train_iter_loss: 0.13121742010116577
train_iter_loss: 0.2534129023551941
train_iter_loss: 0.18486247956752777
train_iter_loss: 0.24512147903442383
train_iter_loss: 0.0796360895037651
train loss :0.1716
---------------------
Validation seg loss: 0.2188502238044199 at epoch 296
epoch =    297/  1000, exp = train
train_iter_loss: 0.17971359193325043
train_iter_loss: 0.16761398315429688
train_iter_loss: 0.2580011188983917
train_iter_loss: 0.1986006647348404
train_iter_loss: 0.14185427129268646
train_iter_loss: 0.176932692527771
train_iter_loss: 0.2874521017074585
train_iter_loss: 0.19270065426826477
train_iter_loss: 0.169968381524086
train_iter_loss: 0.08966661989688873
train_iter_loss: 0.24337588250637054
train_iter_loss: 0.19155412912368774
train_iter_loss: 0.19788454473018646
train_iter_loss: 0.06158304959535599
train_iter_loss: 0.18109913170337677
train_iter_loss: 0.07355501502752304
train_iter_loss: 0.2256922572851181
train_iter_loss: 0.13096264004707336
train_iter_loss: 0.09158942848443985
train_iter_loss: 0.13601085543632507
train_iter_loss: 0.19814863801002502
train_iter_loss: 0.16410008072853088
train_iter_loss: 0.07982798665761948
train_iter_loss: 0.22021949291229248
train_iter_loss: 0.20903906226158142
train_iter_loss: 0.0978347510099411
train_iter_loss: 0.2297777682542801
train_iter_loss: 0.07623714208602905
train_iter_loss: 0.2154487520456314
train_iter_loss: 0.1190098375082016
train_iter_loss: 0.13591168820858002
train_iter_loss: 0.15445323288440704
train_iter_loss: 0.11089613288640976
train_iter_loss: 0.06225315481424332
train_iter_loss: 0.08785742521286011
train_iter_loss: 0.04311863332986832
train_iter_loss: 0.1904299110174179
train_iter_loss: 0.09019585698843002
train_iter_loss: 0.11237364262342453
train_iter_loss: 0.20441903173923492
train_iter_loss: 0.13680504262447357
train_iter_loss: 0.11768016964197159
train_iter_loss: 0.225677028298378
train_iter_loss: 0.15503239631652832
train_iter_loss: 0.13361242413520813
train_iter_loss: 0.1895887404680252
train_iter_loss: 0.2389640063047409
train_iter_loss: 0.1196913868188858
train_iter_loss: 0.24444672465324402
train_iter_loss: 0.3373951315879822
train_iter_loss: 0.07552114129066467
train_iter_loss: 0.19719690084457397
train_iter_loss: 0.11306285113096237
train_iter_loss: 0.1740349978208542
train_iter_loss: 0.133620485663414
train_iter_loss: 0.17163749039173126
train_iter_loss: 0.13010652363300323
train_iter_loss: 0.11178191751241684
train_iter_loss: 0.07192756235599518
train_iter_loss: 0.14220015704631805
train_iter_loss: 0.1202717274427414
train_iter_loss: 0.20956610143184662
train_iter_loss: 0.2532183527946472
train_iter_loss: 0.18378683924674988
train_iter_loss: 0.12791693210601807
train_iter_loss: 0.14460617303848267
train_iter_loss: 0.12551678717136383
train_iter_loss: 0.1314418762922287
train_iter_loss: 0.15474383533000946
train_iter_loss: 0.21740120649337769
train_iter_loss: 0.2542957067489624
train_iter_loss: 0.23241838812828064
train_iter_loss: 0.09339872002601624
train_iter_loss: 0.205362468957901
train_iter_loss: 0.24103234708309174
train_iter_loss: 0.1917506754398346
train_iter_loss: 0.2899053990840912
train_iter_loss: 0.2845109701156616
train_iter_loss: 0.2406875491142273
train_iter_loss: 0.3738718330860138
train_iter_loss: 0.18080127239227295
train_iter_loss: 0.1974240392446518
train_iter_loss: 0.13806843757629395
train_iter_loss: 0.09016790241003036
train_iter_loss: 0.17834998667240143
train_iter_loss: 0.09386143088340759
train_iter_loss: 0.179818257689476
train_iter_loss: 0.27889588475227356
train_iter_loss: 0.19013097882270813
train_iter_loss: 0.1489260345697403
train_iter_loss: 0.10589155554771423
train_iter_loss: 0.1835128366947174
train_iter_loss: 0.1848679929971695
train_iter_loss: 0.25076594948768616
train_iter_loss: 0.1314966082572937
train_iter_loss: 0.16948476433753967
train_iter_loss: 0.14319366216659546
train_iter_loss: 0.24815617501735687
train_iter_loss: 0.11952733993530273
train_iter_loss: 0.17074978351593018
train loss :0.1705
---------------------
Validation seg loss: 0.21875765059529892 at epoch 297
epoch =    298/  1000, exp = train
train_iter_loss: 0.20098136365413666
train_iter_loss: 0.2327185720205307
train_iter_loss: 0.16466429829597473
train_iter_loss: 0.1692969650030136
train_iter_loss: 0.20176783204078674
train_iter_loss: 0.18156978487968445
train_iter_loss: 0.1680966019630432
train_iter_loss: 0.12676934897899628
train_iter_loss: 0.09624027460813522
train_iter_loss: 0.1577887237071991
train_iter_loss: 0.17525841295719147
train_iter_loss: 0.08656219393014908
train_iter_loss: 0.15644842386245728
train_iter_loss: 0.11546540260314941
train_iter_loss: 0.21347272396087646
train_iter_loss: 0.24091488122940063
train_iter_loss: 0.043341439217329025
train_iter_loss: 0.11484145373106003
train_iter_loss: 0.1314488649368286
train_iter_loss: 0.10190340876579285
train_iter_loss: 0.10347000509500504
train_iter_loss: 0.15542112290859222
train_iter_loss: 0.17602668702602386
train_iter_loss: 0.44109073281288147
train_iter_loss: 0.08445827662944794
train_iter_loss: 0.1541026085615158
train_iter_loss: 0.12272355705499649
train_iter_loss: 0.05398416519165039
train_iter_loss: 0.16089972853660583
train_iter_loss: 0.14833925664424896
train_iter_loss: 0.1534578502178192
train_iter_loss: 0.25944167375564575
train_iter_loss: 0.2526456415653229
train_iter_loss: 0.0772414430975914
train_iter_loss: 0.23065929114818573
train_iter_loss: 0.09940676391124725
train_iter_loss: 0.13413317501544952
train_iter_loss: 0.25882187485694885
train_iter_loss: 0.09078814834356308
train_iter_loss: 0.12341923266649246
train_iter_loss: 0.0708291158080101
train_iter_loss: 0.1560649871826172
train_iter_loss: 0.047750700265169144
train_iter_loss: 0.22672908008098602
train_iter_loss: 0.1070462092757225
train_iter_loss: 0.06601516902446747
train_iter_loss: 0.11897801607847214
train_iter_loss: 0.2061491459608078
train_iter_loss: 0.15679338574409485
train_iter_loss: 0.18541890382766724
train_iter_loss: 0.20658977329730988
train_iter_loss: 0.19444328546524048
train_iter_loss: 0.23272505402565002
train_iter_loss: 0.12036547809839249
train_iter_loss: 0.19299301505088806
train_iter_loss: 0.24138525128364563
train_iter_loss: 0.14552722871303558
train_iter_loss: 0.17604555189609528
train_iter_loss: 0.11049201339483261
train_iter_loss: 0.11656676977872849
train_iter_loss: 0.1266913115978241
train_iter_loss: 0.1664734035730362
train_iter_loss: 0.39193105697631836
train_iter_loss: 0.17361444234848022
train_iter_loss: 0.12460868805646896
train_iter_loss: 0.2251870483160019
train_iter_loss: 0.10355289280414581
train_iter_loss: 0.14403988420963287
train_iter_loss: 0.2352433204650879
train_iter_loss: 0.13417136669158936
train_iter_loss: 0.20215409994125366
train_iter_loss: 0.1275501549243927
train_iter_loss: 0.22146150469779968
train_iter_loss: 0.21028533577919006
train_iter_loss: 0.13835088908672333
train_iter_loss: 0.0681704431772232
train_iter_loss: 0.11577115952968597
train_iter_loss: 0.15069597959518433
train_iter_loss: 0.12436220049858093
train_iter_loss: 0.14240209758281708
train_iter_loss: 0.28464484214782715
train_iter_loss: 0.21308013796806335
train_iter_loss: 0.2337104082107544
train_iter_loss: 0.11747570335865021
train_iter_loss: 0.18126487731933594
train_iter_loss: 0.1527327299118042
train_iter_loss: 0.15494103729724884
train_iter_loss: 0.345641165971756
train_iter_loss: 0.08791618049144745
train_iter_loss: 0.25183454155921936
train_iter_loss: 0.2607690095901489
train_iter_loss: 0.4187672734260559
train_iter_loss: 0.12232419848442078
train_iter_loss: 0.12594738602638245
train_iter_loss: 0.2268950343132019
train_iter_loss: 0.13036812841892242
train_iter_loss: 0.1427767276763916
train_iter_loss: 0.06794384866952896
train_iter_loss: 0.10312478244304657
train_iter_loss: 0.1338021457195282
train loss :0.1669
---------------------
Validation seg loss: 0.22010075543427243 at epoch 298
epoch =    299/  1000, exp = train
train_iter_loss: 0.11461476981639862
train_iter_loss: 0.05655562877655029
train_iter_loss: 0.17313317954540253
train_iter_loss: 0.26568323373794556
train_iter_loss: 0.24658134579658508
train_iter_loss: 0.14919926226139069
train_iter_loss: 0.18006613850593567
train_iter_loss: 0.0883759930729866
train_iter_loss: 0.18232186138629913
train_iter_loss: 0.13277575373649597
train_iter_loss: 0.19900937378406525
train_iter_loss: 0.24259653687477112
train_iter_loss: 0.12049812078475952
train_iter_loss: 0.1271691918373108
train_iter_loss: 0.12492448836565018
train_iter_loss: 0.07559593766927719
train_iter_loss: 0.1652129590511322
train_iter_loss: 0.10198470205068588
train_iter_loss: 0.21675291657447815
train_iter_loss: 0.2512945532798767
train_iter_loss: 0.18783877789974213
train_iter_loss: 0.061861902475357056
train_iter_loss: 0.2110678255558014
train_iter_loss: 0.21727265417575836
train_iter_loss: 0.3515167236328125
train_iter_loss: 0.19675011932849884
train_iter_loss: 0.12297943979501724
train_iter_loss: 0.1482282429933548
train_iter_loss: 0.09133017063140869
train_iter_loss: 0.19146189093589783
train_iter_loss: 0.19807219505310059
train_iter_loss: 0.10236421972513199
train_iter_loss: 0.14324678480625153
train_iter_loss: 0.1905062347650528
train_iter_loss: 0.1561134159564972
train_iter_loss: 0.24455256760120392
train_iter_loss: 0.25528886914253235
train_iter_loss: 0.19873656332492828
train_iter_loss: 0.09841011464595795
train_iter_loss: 0.15353181958198547
train_iter_loss: 0.19530661404132843
train_iter_loss: 0.265062153339386
train_iter_loss: 0.19686846435070038
train_iter_loss: 0.06996311992406845
train_iter_loss: 0.21865734457969666
train_iter_loss: 0.14938883483409882
train_iter_loss: 0.11577758938074112
train_iter_loss: 0.1173209547996521
train_iter_loss: 0.22340771555900574
train_iter_loss: 0.43478497862815857
train_iter_loss: 0.21531672775745392
train_iter_loss: 0.10657023638486862
train_iter_loss: 0.18747736513614655
train_iter_loss: 0.2090146541595459
train_iter_loss: 0.10375536978244781
train_iter_loss: 0.26194480061531067
train_iter_loss: 0.11499045044183731
train_iter_loss: 0.21281060576438904
train_iter_loss: 0.12750378251075745
train_iter_loss: 0.16218236088752747
train_iter_loss: 0.10188670456409454
train_iter_loss: 0.2863309383392334
train_iter_loss: 0.12435007095336914
train_iter_loss: 0.12044071406126022
train_iter_loss: 0.16266901791095734
train_iter_loss: 0.08335340768098831
train_iter_loss: 0.12925848364830017
train_iter_loss: 0.1740373820066452
train_iter_loss: 0.16547802090644836
train_iter_loss: 0.13077056407928467
train_iter_loss: 0.12163311243057251
train_iter_loss: 0.15902675688266754
train_iter_loss: 0.12321589142084122
train_iter_loss: 0.15143658220767975
train_iter_loss: 0.1278303861618042
train_iter_loss: 0.15366080403327942
train_iter_loss: 0.12080118060112
train_iter_loss: 0.10154447704553604
train_iter_loss: 0.15954828262329102
train_iter_loss: 0.1424148827791214
train_iter_loss: 0.07350113987922668
train_iter_loss: 0.11721401661634445
train_iter_loss: 0.31247052550315857
train_iter_loss: 0.17745810747146606
train_iter_loss: 0.1244049146771431
train_iter_loss: 0.10753929615020752
train_iter_loss: 0.14064297080039978
train_iter_loss: 0.1640089899301529
train_iter_loss: 0.18039283156394958
train_iter_loss: 0.161973774433136
train_iter_loss: 0.10453213006258011
train_iter_loss: 0.11198657751083374
train_iter_loss: 0.3271382451057434
train_iter_loss: 0.08960122615098953
train_iter_loss: 0.11632352322340012
train_iter_loss: 0.09619570523500443
train_iter_loss: 0.12588776648044586
train_iter_loss: 0.09052639454603195
train_iter_loss: 0.20204517245292664
train_iter_loss: 0.2316903918981552
train loss :0.1649
---------------------
Validation seg loss: 0.21983957361816517 at epoch 299
epoch =    300/  1000, exp = train
train_iter_loss: 0.04853213578462601
train_iter_loss: 0.14050613343715668
train_iter_loss: 0.21500632166862488
train_iter_loss: 0.10544435679912567
train_iter_loss: 0.14190338551998138
train_iter_loss: 0.12945573031902313
train_iter_loss: 0.13780225813388824
train_iter_loss: 0.11644423007965088
train_iter_loss: 0.16627700626850128
train_iter_loss: 0.12329433858394623
train_iter_loss: 0.1664297878742218
train_iter_loss: 0.10838056355714798
train_iter_loss: 0.12078986316919327
train_iter_loss: 0.10066010802984238
train_iter_loss: 0.15896879136562347
train_iter_loss: 0.1878141164779663
train_iter_loss: 0.16473238170146942
train_iter_loss: 0.1933538019657135
train_iter_loss: 0.32548612356185913
train_iter_loss: 0.2070065140724182
train_iter_loss: 0.19593371450901031
train_iter_loss: 0.1935296207666397
train_iter_loss: 0.20950041711330414
train_iter_loss: 0.26524680852890015
train_iter_loss: 0.09867831319570541
train_iter_loss: 0.27651160955429077
train_iter_loss: 0.16344429552555084
train_iter_loss: 0.19993196427822113
train_iter_loss: 0.2017006129026413
train_iter_loss: 0.24004238843917847
train_iter_loss: 0.151656836271286
train_iter_loss: 0.18242508172988892
train_iter_loss: 0.2086208313703537
train_iter_loss: 0.13348841667175293
train_iter_loss: 0.14829397201538086
train_iter_loss: 0.1266922950744629
train_iter_loss: 0.13864797353744507
train_iter_loss: 0.19016920030117035
train_iter_loss: 0.13228222727775574
train_iter_loss: 0.08744250982999802
train_iter_loss: 0.1603047251701355
train_iter_loss: 0.15401138365268707
train_iter_loss: 0.176091268658638
train_iter_loss: 0.13628265261650085
train_iter_loss: 0.22246648371219635
train_iter_loss: 0.21083608269691467
train_iter_loss: 0.20751044154167175
train_iter_loss: 0.14834558963775635
train_iter_loss: 0.25161513686180115
train_iter_loss: 0.12541168928146362
train_iter_loss: 0.21236257255077362
train_iter_loss: 0.13102734088897705
train_iter_loss: 0.22637440264225006
train_iter_loss: 0.14295031130313873
train_iter_loss: 0.13438710570335388
train_iter_loss: 0.1641520857810974
train_iter_loss: 0.24616535007953644
train_iter_loss: 0.16794496774673462
train_iter_loss: 0.17206455767154694
train_iter_loss: 0.22124949097633362
train_iter_loss: 0.19248159229755402
train_iter_loss: 0.07039529085159302
train_iter_loss: 0.20826593041419983
train_iter_loss: 0.2588809132575989
train_iter_loss: 0.11925575137138367
train_iter_loss: 0.1026914119720459
train_iter_loss: 0.13165494799613953
train_iter_loss: 0.1423092782497406
train_iter_loss: 0.06946931779384613
train_iter_loss: 0.15265077352523804
train_iter_loss: 0.0958656296133995
train_iter_loss: 0.10120397806167603
train_iter_loss: 0.12137935310602188
train_iter_loss: 0.13542357087135315
train_iter_loss: 0.15649914741516113
train_iter_loss: 0.15819071233272552
train_iter_loss: 0.05416548624634743
train_iter_loss: 0.16193115711212158
train_iter_loss: 0.19379910826683044
train_iter_loss: 0.217056006193161
train_iter_loss: 0.11782380938529968
train_iter_loss: 0.1394827514886856
train_iter_loss: 0.10698356479406357
train_iter_loss: 0.1349148452281952
train_iter_loss: 0.2151222974061966
train_iter_loss: 0.12206510454416275
train_iter_loss: 0.13034972548484802
train_iter_loss: 0.05395534262061119
train_iter_loss: 0.1756761372089386
train_iter_loss: 0.1369093507528305
train_iter_loss: 0.16783879697322845
train_iter_loss: 0.16000092029571533
train_iter_loss: 0.12481679767370224
train_iter_loss: 0.12481743842363358
train_iter_loss: 0.14696736633777618
train_iter_loss: 0.1449793428182602
train_iter_loss: 0.19117502868175507
train_iter_loss: 0.18959078192710876
train_iter_loss: 0.21219514310359955
train_iter_loss: 0.20530281960964203
train loss :0.1622
---------------------
Validation seg loss: 0.22059094127408177 at epoch 300
epoch =    301/  1000, exp = train
train_iter_loss: 0.28642380237579346
train_iter_loss: 0.2488812357187271
train_iter_loss: 0.11393726617097855
train_iter_loss: 0.23725292086601257
train_iter_loss: 0.21402382850646973
train_iter_loss: 0.16553910076618195
train_iter_loss: 0.2078809142112732
train_iter_loss: 0.14527931809425354
train_iter_loss: 0.21183976531028748
train_iter_loss: 0.08266665041446686
train_iter_loss: 0.1384046971797943
train_iter_loss: 0.14926619827747345
train_iter_loss: 0.0981719046831131
train_iter_loss: 0.1667298972606659
train_iter_loss: 0.2671053111553192
train_iter_loss: 0.10559426993131638
train_iter_loss: 0.19440840184688568
train_iter_loss: 0.1310466080904007
train_iter_loss: 0.25219154357910156
train_iter_loss: 0.18578945100307465
train_iter_loss: 0.08600081503391266
train_iter_loss: 0.15813149511814117
train_iter_loss: 0.13235418498516083
train_iter_loss: 0.10356085002422333
train_iter_loss: 0.2892399728298187
train_iter_loss: 0.13966158032417297
train_iter_loss: 0.13526831567287445
train_iter_loss: 0.2270234078168869
train_iter_loss: 0.24643614888191223
train_iter_loss: 0.1869213581085205
train_iter_loss: 0.03687191382050514
train_iter_loss: 0.1632062792778015
train_iter_loss: 0.1091664656996727
train_iter_loss: 0.139456108212471
train_iter_loss: 0.2006203681230545
train_iter_loss: 0.11363467574119568
train_iter_loss: 0.24728523194789886
train_iter_loss: 0.23435549437999725
train_iter_loss: 0.29781684279441833
train_iter_loss: 0.22830982506275177
train_iter_loss: 0.18169522285461426
train_iter_loss: 0.2224055975675583
train_iter_loss: 0.057069290429353714
train_iter_loss: 0.13056352734565735
train_iter_loss: 0.11806916445493698
train_iter_loss: 0.12457222491502762
train_iter_loss: 0.35750535130500793
train_iter_loss: 0.08796828240156174
train_iter_loss: 0.3643304109573364
train_iter_loss: 0.1879316121339798
train_iter_loss: 0.07280755043029785
train_iter_loss: 0.22111068665981293
train_iter_loss: 0.15332479774951935
train_iter_loss: 0.2571299374103546
train_iter_loss: 0.253686785697937
train_iter_loss: 0.10278243571519852
train_iter_loss: 0.15511879324913025
train_iter_loss: 0.13545721769332886
train_iter_loss: 0.10892682522535324
train_iter_loss: 0.13292548060417175
train_iter_loss: 0.15022195875644684
train_iter_loss: 0.1316208392381668
train_iter_loss: 0.164096400141716
train_iter_loss: 0.19075347483158112
train_iter_loss: 0.13528776168823242
train_iter_loss: 0.15353798866271973
train_iter_loss: 0.07785060256719589
train_iter_loss: 0.19587630033493042
train_iter_loss: 0.14801472425460815
train_iter_loss: 0.161887988448143
train_iter_loss: 0.07802432030439377
train_iter_loss: 0.07256174087524414
train_iter_loss: 0.16558553278446198
train_iter_loss: 0.14682812988758087
train_iter_loss: 0.2216048389673233
train_iter_loss: 0.14009520411491394
train_iter_loss: 0.09564673155546188
train_iter_loss: 0.5902382135391235
train_iter_loss: 0.1714051365852356
train_iter_loss: 0.13713008165359497
train_iter_loss: 0.3121306896209717
train_iter_loss: 0.13535821437835693
train_iter_loss: 0.15086150169372559
train_iter_loss: 0.13002385199069977
train_iter_loss: 0.12387070059776306
train_iter_loss: 0.3369765281677246
train_iter_loss: 0.1840607076883316
train_iter_loss: 0.17717333137989044
train_iter_loss: 0.14427755773067474
train_iter_loss: 0.10004299879074097
train_iter_loss: 0.13385652005672455
train_iter_loss: 0.07708483189344406
train_iter_loss: 0.08041485399007797
train_iter_loss: 0.10631205886602402
train_iter_loss: 0.09845247864723206
train_iter_loss: 0.25923094153404236
train_iter_loss: 0.09294608980417252
train_iter_loss: 0.11559312045574188
train_iter_loss: 0.18414951860904694
train_iter_loss: 0.1585601419210434
train loss :0.1710
---------------------
Validation seg loss: 0.2200975032470558 at epoch 301
epoch =    302/  1000, exp = train
train_iter_loss: 0.1679282933473587
train_iter_loss: 0.16276338696479797
train_iter_loss: 0.1370137333869934
train_iter_loss: 0.15717847645282745
train_iter_loss: 0.2712154984474182
train_iter_loss: 0.19962769746780396
train_iter_loss: 0.14779768884181976
train_iter_loss: 0.15022119879722595
train_iter_loss: 0.1483127772808075
train_iter_loss: 0.24484087526798248
train_iter_loss: 0.17003417015075684
train_iter_loss: 0.06506528705358505
train_iter_loss: 0.2076658308506012
train_iter_loss: 0.20821550488471985
train_iter_loss: 0.1894211769104004
train_iter_loss: 0.18199194967746735
train_iter_loss: 0.22343303263187408
train_iter_loss: 0.0985809788107872
train_iter_loss: 0.1499868780374527
train_iter_loss: 0.1280360072851181
train_iter_loss: 0.1819814294576645
train_iter_loss: 0.11090124398469925
train_iter_loss: 0.15869899094104767
train_iter_loss: 0.10264788568019867
train_iter_loss: 0.12829045951366425
train_iter_loss: 0.12762703001499176
train_iter_loss: 0.09145835041999817
train_iter_loss: 0.2509641945362091
train_iter_loss: 0.10237101465463638
train_iter_loss: 0.18143251538276672
train_iter_loss: 0.17536860704421997
train_iter_loss: 0.15141762793064117
train_iter_loss: 0.2341625690460205
train_iter_loss: 0.15517225861549377
train_iter_loss: 0.27993619441986084
train_iter_loss: 0.2310899794101715
train_iter_loss: 0.2136559635400772
train_iter_loss: 0.18976755440235138
train_iter_loss: 0.2019207924604416
train_iter_loss: 0.061577506363391876
train_iter_loss: 0.17336080968379974
train_iter_loss: 0.16381484270095825
train_iter_loss: 0.09258881956338882
train_iter_loss: 0.0955209881067276
train_iter_loss: 0.11131976544857025
train_iter_loss: 0.137584388256073
train_iter_loss: 0.20994509756565094
train_iter_loss: 0.24623432755470276
train_iter_loss: 0.09241234511137009
train_iter_loss: 0.184866264462471
train_iter_loss: 0.22053778171539307
train_iter_loss: 0.10653620958328247
train_iter_loss: 0.2345878630876541
train_iter_loss: 0.2036832869052887
train_iter_loss: 0.0745827928185463
train_iter_loss: 0.1523178666830063
train_iter_loss: 0.20192021131515503
train_iter_loss: 0.09208080172538757
train_iter_loss: 0.2246149480342865
train_iter_loss: 0.20455896854400635
train_iter_loss: 0.12695379555225372
train_iter_loss: 0.2326921671628952
train_iter_loss: 0.13987118005752563
train_iter_loss: 0.09479361027479172
train_iter_loss: 0.19262978434562683
train_iter_loss: 0.20403695106506348
train_iter_loss: 0.1355917751789093
train_iter_loss: 0.1359403282403946
train_iter_loss: 0.19591762125492096
train_iter_loss: 0.14911814033985138
train_iter_loss: 0.10999076813459396
train_iter_loss: 0.17221082746982574
train_iter_loss: 0.057489655911922455
train_iter_loss: 0.21797169744968414
train_iter_loss: 0.13195829093456268
train_iter_loss: 0.07639466971158981
train_iter_loss: 0.2776538133621216
train_iter_loss: 0.15439312160015106
train_iter_loss: 0.3731030523777008
train_iter_loss: 0.17129002511501312
train_iter_loss: 0.11862753331661224
train_iter_loss: 0.16159886121749878
train_iter_loss: 0.1311706006526947
train_iter_loss: 0.17017720639705658
train_iter_loss: 0.09093986451625824
train_iter_loss: 0.18281163275241852
train_iter_loss: 0.06964436173439026
train_iter_loss: 0.1681114137172699
train_iter_loss: 0.11588877439498901
train_iter_loss: 0.1683826893568039
train_iter_loss: 0.07897460460662842
train_iter_loss: 0.1318458467721939
train_iter_loss: 0.10974856466054916
train_iter_loss: 0.1207764595746994
train_iter_loss: 0.12088511884212494
train_iter_loss: 0.24486084282398224
train_iter_loss: 0.14136523008346558
train_iter_loss: 0.1207984983921051
train_iter_loss: 0.09437624365091324
train_iter_loss: 0.17010128498077393
train loss :0.1619
---------------------
Validation seg loss: 0.2244167250914956 at epoch 302
epoch =    303/  1000, exp = train
train_iter_loss: 0.1694401055574417
train_iter_loss: 0.16964875161647797
train_iter_loss: 0.18377099931240082
train_iter_loss: 0.24734194576740265
train_iter_loss: 0.09197395294904709
train_iter_loss: 0.1906258910894394
train_iter_loss: 0.4035714864730835
train_iter_loss: 0.2346087247133255
train_iter_loss: 0.15387031435966492
train_iter_loss: 0.13176202774047852
train_iter_loss: 0.14242538809776306
train_iter_loss: 0.22312159836292267
train_iter_loss: 0.08541297912597656
train_iter_loss: 0.07994719594717026
train_iter_loss: 0.13432054221630096
train_iter_loss: 0.11405602097511292
train_iter_loss: 0.1305694729089737
train_iter_loss: 0.12091085314750671
train_iter_loss: 0.24648047983646393
train_iter_loss: 0.15921291708946228
train_iter_loss: 0.19347867369651794
train_iter_loss: 0.12496461719274521
train_iter_loss: 0.1671539843082428
train_iter_loss: 0.21226024627685547
train_iter_loss: 0.17037683725357056
train_iter_loss: 0.10618080943822861
train_iter_loss: 0.20285359025001526
train_iter_loss: 0.1721913069486618
train_iter_loss: 0.2140720933675766
train_iter_loss: 0.15061704814434052
train_iter_loss: 0.1383569836616516
train_iter_loss: 0.31400564312934875
train_iter_loss: 0.1270264834165573
train_iter_loss: 0.19971036911010742
train_iter_loss: 0.11807972192764282
train_iter_loss: 0.22097362577915192
train_iter_loss: 0.09713602811098099
train_iter_loss: 0.18777787685394287
train_iter_loss: 0.1248316839337349
train_iter_loss: 0.15191297233104706
train_iter_loss: 0.14493489265441895
train_iter_loss: 0.11690735071897507
train_iter_loss: 0.1376553475856781
train_iter_loss: 0.06200006976723671
train_iter_loss: 0.175524041056633
train_iter_loss: 0.12062154710292816
train_iter_loss: 0.12631160020828247
train_iter_loss: 0.13015085458755493
train_iter_loss: 0.1554543823003769
train_iter_loss: 0.28062793612480164
train_iter_loss: 0.08629615604877472
train_iter_loss: 0.2074580043554306
train_iter_loss: 0.1274428814649582
train_iter_loss: 0.20366498827934265
train_iter_loss: 0.0913955569267273
train_iter_loss: 0.04481684789061546
train_iter_loss: 0.09910883009433746
train_iter_loss: 0.08563069254159927
train_iter_loss: 0.2037869244813919
train_iter_loss: 0.16878628730773926
train_iter_loss: 0.11141063272953033
train_iter_loss: 0.42551082372665405
train_iter_loss: 0.15088675916194916
train_iter_loss: 0.14990589022636414
train_iter_loss: 0.12508200109004974
train_iter_loss: 0.2453421652317047
train_iter_loss: 0.16696415841579437
train_iter_loss: 0.15678025782108307
train_iter_loss: 0.1792929470539093
train_iter_loss: 0.12712790071964264
train_iter_loss: 0.20363517105579376
train_iter_loss: 0.07459371536970139
train_iter_loss: 0.1566486656665802
train_iter_loss: 0.16274867951869965
train_iter_loss: 0.1295570731163025
train_iter_loss: 0.15764565765857697
train_iter_loss: 0.15358766913414001
train_iter_loss: 0.1813182383775711
train_iter_loss: 0.13688823580741882
train_iter_loss: 0.21970203518867493
train_iter_loss: 0.1610253006219864
train_iter_loss: 0.267037570476532
train_iter_loss: 0.10250318795442581
train_iter_loss: 0.132890984416008
train_iter_loss: 0.23790813982486725
train_iter_loss: 0.26850658655166626
train_iter_loss: 0.2045757919549942
train_iter_loss: 0.1822492629289627
train_iter_loss: 0.10590629279613495
train_iter_loss: 0.13343049585819244
train_iter_loss: 0.18945904076099396
train_iter_loss: 0.1345684975385666
train_iter_loss: 0.19564402103424072
train_iter_loss: 0.1025414764881134
train_iter_loss: 0.1229487955570221
train_iter_loss: 0.18271803855895996
train_iter_loss: 0.2814911901950836
train_iter_loss: 0.13861380517482758
train_iter_loss: 0.17691858112812042
train_iter_loss: 0.1727546602487564
train loss :0.1665
---------------------
Validation seg loss: 0.22085007094725403 at epoch 303
epoch =    304/  1000, exp = train
train_iter_loss: 0.2173895388841629
train_iter_loss: 0.23183093965053558
train_iter_loss: 0.05838261544704437
train_iter_loss: 0.2481497824192047
train_iter_loss: 0.17634691298007965
train_iter_loss: 0.20289365947246552
train_iter_loss: 0.17405174672603607
train_iter_loss: 0.13081850111484528
train_iter_loss: 0.26004236936569214
train_iter_loss: 0.20803922414779663
train_iter_loss: 0.11283613741397858
train_iter_loss: 0.40279507637023926
train_iter_loss: 0.12374512851238251
train_iter_loss: 0.174788698554039
train_iter_loss: 0.12695427238941193
train_iter_loss: 0.15684863924980164
train_iter_loss: 0.0937119573354721
train_iter_loss: 0.11995504796504974
train_iter_loss: 0.1532183140516281
train_iter_loss: 0.1976972371339798
train_iter_loss: 0.31209835410118103
train_iter_loss: 0.17322829365730286
train_iter_loss: 0.13025395572185516
train_iter_loss: 0.2470642328262329
train_iter_loss: 0.1603514701128006
train_iter_loss: 0.1277739405632019
train_iter_loss: 0.2195044457912445
train_iter_loss: 0.14136908948421478
train_iter_loss: 0.152340367436409
train_iter_loss: 0.13637377321720123
train_iter_loss: 0.07601036876440048
train_iter_loss: 0.1407184898853302
train_iter_loss: 0.25602662563323975
train_iter_loss: 0.14525273442268372
train_iter_loss: 0.10707350820302963
train_iter_loss: 0.033633094280958176
train_iter_loss: 0.07679589092731476
train_iter_loss: 0.15899242460727692
train_iter_loss: 0.27857616543769836
train_iter_loss: 0.22361275553703308
train_iter_loss: 0.5926584005355835
train_iter_loss: 0.13023367524147034
train_iter_loss: 0.11325528472661972
train_iter_loss: 0.12524569034576416
train_iter_loss: 0.17234165966510773
train_iter_loss: 0.15270325541496277
train_iter_loss: 0.23467867076396942
train_iter_loss: 0.20769277215003967
train_iter_loss: 0.13389112055301666
train_iter_loss: 0.19697284698486328
train_iter_loss: 0.11291784048080444
train_iter_loss: 0.16830971837043762
train_iter_loss: 0.15230514109134674
train_iter_loss: 0.19332045316696167
train_iter_loss: 0.16391392052173615
train_iter_loss: 0.10250537097454071
train_iter_loss: 0.1969829797744751
train_iter_loss: 0.14956316351890564
train_iter_loss: 0.10870615392923355
train_iter_loss: 0.11019442230463028
train_iter_loss: 0.07692809402942657
train_iter_loss: 0.22446872293949127
train_iter_loss: 0.19657540321350098
train_iter_loss: 0.12423120439052582
train_iter_loss: 0.11367850750684738
train_iter_loss: 0.19456414878368378
train_iter_loss: 0.12549571692943573
train_iter_loss: 0.09849260002374649
train_iter_loss: 0.16213053464889526
train_iter_loss: 0.13101652264595032
train_iter_loss: 0.05977644771337509
train_iter_loss: 0.17427372932434082
train_iter_loss: 0.13105522096157074
train_iter_loss: 0.11930910497903824
train_iter_loss: 0.08256321400403976
train_iter_loss: 0.14667145907878876
train_iter_loss: 0.21685750782489777
train_iter_loss: 0.1129019558429718
train_iter_loss: 0.14008721709251404
train_iter_loss: 0.19677431881427765
train_iter_loss: 0.1622866690158844
train_iter_loss: 0.4385876953601837
train_iter_loss: 0.08565036952495575
train_iter_loss: 0.17509807646274567
train_iter_loss: 0.08363934606313705
train_iter_loss: 0.16284999251365662
train_iter_loss: 0.11854782700538635
train_iter_loss: 0.1440044641494751
train_iter_loss: 0.1787150800228119
train_iter_loss: 0.1482648402452469
train_iter_loss: 0.1929439902305603
train_iter_loss: 0.18029874563217163
train_iter_loss: 0.15876859426498413
train_iter_loss: 0.1947304904460907
train_iter_loss: 0.06944698095321655
train_iter_loss: 0.2044583112001419
train_iter_loss: 0.10521840304136276
train_iter_loss: 0.14998552203178406
train_iter_loss: 0.08909127116203308
train_iter_loss: 0.1382141411304474
train loss :0.1656
---------------------
Validation seg loss: 0.2181791403278146 at epoch 304
epoch =    305/  1000, exp = train
train_iter_loss: 0.18776308000087738
train_iter_loss: 0.10294418781995773
train_iter_loss: 0.15004825592041016
train_iter_loss: 0.15797173976898193
train_iter_loss: 0.09569097310304642
train_iter_loss: 0.13299475610256195
train_iter_loss: 0.09835125505924225
train_iter_loss: 0.17996905744075775
train_iter_loss: 0.15181918442249298
train_iter_loss: 0.13209186494350433
train_iter_loss: 0.1327194720506668
train_iter_loss: 0.09056482464075089
train_iter_loss: 0.22169038653373718
train_iter_loss: 0.10851426422595978
train_iter_loss: 0.15436121821403503
train_iter_loss: 0.1683870553970337
train_iter_loss: 0.29456865787506104
train_iter_loss: 0.12808509171009064
train_iter_loss: 0.23490898311138153
train_iter_loss: 0.18176697194576263
train_iter_loss: 0.07507868111133575
train_iter_loss: 0.3397350609302521
train_iter_loss: 0.16602903604507446
train_iter_loss: 0.12434134632349014
train_iter_loss: 0.23943133652210236
train_iter_loss: 0.20915918052196503
train_iter_loss: 0.11959047615528107
train_iter_loss: 0.5146364569664001
train_iter_loss: 0.19117361307144165
train_iter_loss: 0.22046445310115814
train_iter_loss: 0.1420711725950241
train_iter_loss: 0.24716688692569733
train_iter_loss: 0.23018021881580353
train_iter_loss: 0.1321290284395218
train_iter_loss: 0.1275186985731125
train_iter_loss: 0.15200889110565186
train_iter_loss: 0.22017121315002441
train_iter_loss: 0.15242011845111847
train_iter_loss: 0.07186604291200638
train_iter_loss: 0.21963748335838318
train_iter_loss: 0.10381761938333511
train_iter_loss: 0.05283527821302414
train_iter_loss: 0.14782129228115082
train_iter_loss: 0.11925177276134491
train_iter_loss: 0.16015344858169556
train_iter_loss: 0.1438339203596115
train_iter_loss: 0.20720328390598297
train_iter_loss: 0.06797908991575241
train_iter_loss: 0.1945767104625702
train_iter_loss: 0.11334505677223206
train_iter_loss: 0.17502665519714355
train_iter_loss: 0.15591764450073242
train_iter_loss: 0.11276518553495407
train_iter_loss: 0.0769268274307251
train_iter_loss: 0.21995726227760315
train_iter_loss: 0.37290388345718384
train_iter_loss: 0.09265317022800446
train_iter_loss: 0.12722837924957275
train_iter_loss: 0.19040609896183014
train_iter_loss: 0.11366810649633408
train_iter_loss: 0.19133174419403076
train_iter_loss: 0.11255666613578796
train_iter_loss: 0.15842153131961823
train_iter_loss: 0.38291847705841064
train_iter_loss: 0.18010775744915009
train_iter_loss: 0.10168720036745071
train_iter_loss: 0.19320861995220184
train_iter_loss: 0.1344345062971115
train_iter_loss: 0.18840250372886658
train_iter_loss: 0.216670423746109
train_iter_loss: 0.09445370733737946
train_iter_loss: 0.06594809889793396
train_iter_loss: 0.1662312150001526
train_iter_loss: 0.19577282667160034
train_iter_loss: 0.17059385776519775
train_iter_loss: 0.16043323278427124
train_iter_loss: 0.16766664385795593
train_iter_loss: 0.12607191503047943
train_iter_loss: 0.14759859442710876
train_iter_loss: 0.10401812195777893
train_iter_loss: 0.1869625449180603
train_iter_loss: 0.25151756405830383
train_iter_loss: 0.13565920293331146
train_iter_loss: 0.24966172873973846
train_iter_loss: 0.10691606253385544
train_iter_loss: 0.17020031809806824
train_iter_loss: 0.12825365364551544
train_iter_loss: 0.16693803668022156
train_iter_loss: 0.14974966645240784
train_iter_loss: 0.07650178670883179
train_iter_loss: 0.09745240211486816
train_iter_loss: 0.07319676876068115
train_iter_loss: 0.25252974033355713
train_iter_loss: 0.3510419726371765
train_iter_loss: 0.23540379106998444
train_iter_loss: 0.2209024429321289
train_iter_loss: 0.14818333089351654
train_iter_loss: 0.09124141186475754
train_iter_loss: 0.24245481193065643
train_iter_loss: 0.19341804087162018
train loss :0.1690
---------------------
Validation seg loss: 0.22100753761511646 at epoch 305
epoch =    306/  1000, exp = train
train_iter_loss: 0.13329008221626282
train_iter_loss: 0.2804769277572632
train_iter_loss: 0.08496652543544769
train_iter_loss: 0.17509418725967407
train_iter_loss: 0.3381255865097046
train_iter_loss: 0.1785043179988861
train_iter_loss: 0.20534929633140564
train_iter_loss: 0.11378705501556396
train_iter_loss: 0.135276660323143
train_iter_loss: 0.16188809275627136
train_iter_loss: 0.1346355527639389
train_iter_loss: 0.2322789877653122
train_iter_loss: 0.15000927448272705
train_iter_loss: 0.0697219967842102
train_iter_loss: 0.18476317822933197
train_iter_loss: 0.11484046280384064
train_iter_loss: 0.21993763744831085
train_iter_loss: 0.11086282879114151
train_iter_loss: 0.05421093478798866
train_iter_loss: 0.12143804877996445
train_iter_loss: 0.06439510732889175
train_iter_loss: 0.20953400433063507
train_iter_loss: 0.14101669192314148
train_iter_loss: 0.18775434792041779
train_iter_loss: 0.10538221150636673
train_iter_loss: 0.17244671285152435
train_iter_loss: 0.16460295021533966
train_iter_loss: 0.2345401495695114
train_iter_loss: 0.12195311486721039
train_iter_loss: 0.07777868211269379
train_iter_loss: 0.1298113614320755
train_iter_loss: 0.20307257771492004
train_iter_loss: 0.14403443038463593
train_iter_loss: 0.19930922985076904
train_iter_loss: 0.28621241450309753
train_iter_loss: 0.1205827072262764
train_iter_loss: 0.21507316827774048
train_iter_loss: 0.182186558842659
train_iter_loss: 0.22331881523132324
train_iter_loss: 0.14072291553020477
train_iter_loss: 0.2641264796257019
train_iter_loss: 0.1743461936712265
train_iter_loss: 0.195606991648674
train_iter_loss: 0.0971374362707138
train_iter_loss: 0.239548459649086
train_iter_loss: 0.10674173384904861
train_iter_loss: 0.3228283226490021
train_iter_loss: 0.22061742842197418
train_iter_loss: 0.12841202318668365
train_iter_loss: 0.10869865119457245
train_iter_loss: 0.17205190658569336
train_iter_loss: 0.20219889283180237
train_iter_loss: 0.14356788992881775
train_iter_loss: 0.13804084062576294
train_iter_loss: 0.16860289871692657
train_iter_loss: 0.2566363215446472
train_iter_loss: 0.10328813642263412
train_iter_loss: 0.28413787484169006
train_iter_loss: 0.24930937588214874
train_iter_loss: 0.10354249179363251
train_iter_loss: 0.13376423716545105
train_iter_loss: 0.1499669998884201
train_iter_loss: 0.19985143840312958
train_iter_loss: 0.24034330248832703
train_iter_loss: 0.17926497757434845
train_iter_loss: 0.18887244164943695
train_iter_loss: 0.15646420419216156
train_iter_loss: 0.1552579700946808
train_iter_loss: 0.17595089972019196
train_iter_loss: 0.1307007074356079
train_iter_loss: 0.25587284564971924
train_iter_loss: 0.17073769867420197
train_iter_loss: 0.10618380457162857
train_iter_loss: 0.1807841807603836
train_iter_loss: 0.18660491704940796
train_iter_loss: 0.14589285850524902
train_iter_loss: 0.1968008130788803
train_iter_loss: 0.15735548734664917
train_iter_loss: 0.3932708501815796
train_iter_loss: 0.20630639791488647
train_iter_loss: 0.10553944110870361
train_iter_loss: 0.15205667912960052
train_iter_loss: 0.10162142664194107
train_iter_loss: 0.19186186790466309
train_iter_loss: 0.15075820684432983
train_iter_loss: 0.15255609154701233
train_iter_loss: 0.05542433634400368
train_iter_loss: 0.26198941469192505
train_iter_loss: 0.13912039995193481
train_iter_loss: 0.1831989884376526
train_iter_loss: 0.15436284244060516
train_iter_loss: 0.10238713026046753
train_iter_loss: 0.13723023235797882
train_iter_loss: 0.2536742091178894
train_iter_loss: 0.13230465352535248
train_iter_loss: 0.10408588498830795
train_iter_loss: 0.1019156202673912
train_iter_loss: 0.11158373206853867
train_iter_loss: 0.09223036468029022
train_iter_loss: 0.07241488993167877
train loss :0.1684
---------------------
Validation seg loss: 0.22033709564404386 at epoch 306
epoch =    307/  1000, exp = train
train_iter_loss: 0.24156107008457184
train_iter_loss: 0.1658494472503662
train_iter_loss: 0.19418062269687653
train_iter_loss: 0.0639573261141777
train_iter_loss: 0.14382384717464447
train_iter_loss: 0.1890411674976349
train_iter_loss: 0.1575721651315689
train_iter_loss: 0.1511925458908081
train_iter_loss: 0.21894574165344238
train_iter_loss: 0.10297644883394241
train_iter_loss: 0.3366279602050781
train_iter_loss: 0.17528437077999115
train_iter_loss: 0.08838903903961182
train_iter_loss: 0.27783286571502686
train_iter_loss: 0.12968315184116364
train_iter_loss: 0.22040621936321259
train_iter_loss: 0.1371392458677292
train_iter_loss: 0.32043084502220154
train_iter_loss: 0.14589369297027588
train_iter_loss: 0.17518611252307892
train_iter_loss: 0.15187031030654907
train_iter_loss: 0.23497194051742554
train_iter_loss: 0.11489444971084595
train_iter_loss: 0.20559902489185333
train_iter_loss: 0.23452404141426086
train_iter_loss: 0.27595001459121704
train_iter_loss: 0.2061413675546646
train_iter_loss: 0.15777717530727386
train_iter_loss: 0.14150400459766388
train_iter_loss: 0.14570066332817078
train_iter_loss: 0.13142357766628265
train_iter_loss: 0.221338152885437
train_iter_loss: 0.18688711524009705
train_iter_loss: 0.19456607103347778
train_iter_loss: 0.1794024258852005
train_iter_loss: 0.24258896708488464
train_iter_loss: 0.06890816986560822
train_iter_loss: 0.07673106342554092
train_iter_loss: 0.10411828756332397
train_iter_loss: 0.12341642379760742
train_iter_loss: 0.08489533513784409
train_iter_loss: 0.2716354429721832
train_iter_loss: 0.15267638862133026
train_iter_loss: 0.20761661231517792
train_iter_loss: 0.12275797128677368
train_iter_loss: 0.14895285665988922
train_iter_loss: 0.1425430327653885
train_iter_loss: 0.11009400337934494
train_iter_loss: 0.13264170289039612
train_iter_loss: 0.10293766856193542
train_iter_loss: 0.09034447371959686
train_iter_loss: 0.17835792899131775
train_iter_loss: 0.11411403864622116
train_iter_loss: 0.21529044210910797
train_iter_loss: 0.1159655749797821
train_iter_loss: 0.14407993853092194
train_iter_loss: 0.24873922765254974
train_iter_loss: 0.15134625136852264
train_iter_loss: 0.11246023327112198
train_iter_loss: 0.09485428780317307
train_iter_loss: 0.21763445436954498
train_iter_loss: 0.14703229069709778
train_iter_loss: 0.18000636994838715
train_iter_loss: 0.18388497829437256
train_iter_loss: 0.1567603349685669
train_iter_loss: 0.13324758410453796
train_iter_loss: 0.21760903298854828
train_iter_loss: 0.09853654354810715
train_iter_loss: 0.06797914206981659
train_iter_loss: 0.13500328361988068
train_iter_loss: 0.15198060870170593
train_iter_loss: 0.13786433637142181
train_iter_loss: 0.09027837216854095
train_iter_loss: 0.13105399906635284
train_iter_loss: 0.32422298192977905
train_iter_loss: 0.17027826607227325
train_iter_loss: 0.1439807265996933
train_iter_loss: 0.18874572217464447
train_iter_loss: 0.15515513718128204
train_iter_loss: 0.13897261023521423
train_iter_loss: 0.21166647970676422
train_iter_loss: 0.12151619046926498
train_iter_loss: 0.15349328517913818
train_iter_loss: 0.06056007370352745
train_iter_loss: 0.19223149120807648
train_iter_loss: 0.2600407898426056
train_iter_loss: 0.2899746000766754
train_iter_loss: 0.128249391913414
train_iter_loss: 0.24308255314826965
train_iter_loss: 0.17524681985378265
train_iter_loss: 0.12135451287031174
train_iter_loss: 0.2080359160900116
train_iter_loss: 0.26908987760543823
train_iter_loss: 0.13470524549484253
train_iter_loss: 0.21166382730007172
train_iter_loss: 0.07265464961528778
train_iter_loss: 0.23758983612060547
train_iter_loss: 0.16440428793430328
train_iter_loss: 0.16534318029880524
train_iter_loss: 0.14312289655208588
train loss :0.1688
---------------------
Validation seg loss: 0.22449290574932435 at epoch 307
epoch =    308/  1000, exp = train
train_iter_loss: 0.2701685130596161
train_iter_loss: 0.20936056971549988
train_iter_loss: 0.24501729011535645
train_iter_loss: 0.14739306271076202
train_iter_loss: 0.16816926002502441
train_iter_loss: 0.12091030180454254
train_iter_loss: 0.1218707337975502
train_iter_loss: 0.24028228223323822
train_iter_loss: 0.13555724918842316
train_iter_loss: 0.2743856608867645
train_iter_loss: 0.22027528285980225
train_iter_loss: 0.17470337450504303
train_iter_loss: 0.2918996512889862
train_iter_loss: 0.2525768578052521
train_iter_loss: 0.06985844671726227
train_iter_loss: 0.08477684110403061
train_iter_loss: 0.1715463399887085
train_iter_loss: 0.2749682366847992
train_iter_loss: 0.08791404962539673
train_iter_loss: 0.19804413616657257
train_iter_loss: 0.11996947973966599
train_iter_loss: 0.19111008942127228
train_iter_loss: 0.29181334376335144
train_iter_loss: 0.23907364904880524
train_iter_loss: 0.16746218502521515
train_iter_loss: 0.17312024533748627
train_iter_loss: 0.1912478655576706
train_iter_loss: 0.3009575307369232
train_iter_loss: 0.16396558284759521
train_iter_loss: 0.189157173037529
train_iter_loss: 0.10849415510892868
train_iter_loss: 0.1647520661354065
train_iter_loss: 0.12931717932224274
train_iter_loss: 0.28387251496315
train_iter_loss: 0.09741053730249405
train_iter_loss: 0.1595430076122284
train_iter_loss: 0.2667942941188812
train_iter_loss: 0.08183175325393677
train_iter_loss: 0.19351163506507874
train_iter_loss: 0.12098367512226105
train_iter_loss: 0.2043294906616211
train_iter_loss: 0.1291361004114151
train_iter_loss: 0.10175292938947678
train_iter_loss: 0.15219104290008545
train_iter_loss: 0.16730737686157227
train_iter_loss: 0.10096775740385056
train_iter_loss: 0.16123433411121368
train_iter_loss: 0.1641097366809845
train_iter_loss: 0.10522021353244781
train_iter_loss: 0.1817907989025116
train_iter_loss: 0.16850797832012177
train_iter_loss: 0.18485219776630402
train_iter_loss: 0.08619220554828644
train_iter_loss: 0.13424672186374664
train_iter_loss: 0.11274141073226929
train_iter_loss: 0.17314413189888
train_iter_loss: 0.21689088642597198
train_iter_loss: 0.16988012194633484
train_iter_loss: 0.11854813992977142
train_iter_loss: 0.0915936827659607
train_iter_loss: 0.10576340556144714
train_iter_loss: 0.153298020362854
train_iter_loss: 0.22522518038749695
train_iter_loss: 0.22905096411705017
train_iter_loss: 0.21969540417194366
train_iter_loss: 0.1534072607755661
train_iter_loss: 0.08886077255010605
train_iter_loss: 0.16942469775676727
train_iter_loss: 0.27439576387405396
train_iter_loss: 0.13231514394283295
train_iter_loss: 0.0697677880525589
train_iter_loss: 0.07969313114881516
train_iter_loss: 0.12078743427991867
train_iter_loss: 0.12617209553718567
train_iter_loss: 0.1741185039281845
train_iter_loss: 0.15773510932922363
train_iter_loss: 0.11077407747507095
train_iter_loss: 0.13386325538158417
train_iter_loss: 0.136724591255188
train_iter_loss: 0.1452985256910324
train_iter_loss: 0.13204146921634674
train_iter_loss: 0.17881262302398682
train_iter_loss: 0.13809643685817719
train_iter_loss: 0.0994454026222229
train_iter_loss: 0.109871044754982
train_iter_loss: 0.05492974445223808
train_iter_loss: 0.1724620908498764
train_iter_loss: 0.09671176970005035
train_iter_loss: 0.10421664267778397
train_iter_loss: 0.05945935100317001
train_iter_loss: 0.2194863110780716
train_iter_loss: 0.21377664804458618
train_iter_loss: 0.02887353114783764
train_iter_loss: 0.22811952233314514
train_iter_loss: 0.2024819254875183
train_iter_loss: 0.16345204412937164
train_iter_loss: 0.2611738443374634
train_iter_loss: 0.29879826307296753
train_iter_loss: 0.23116222023963928
train_iter_loss: 0.06286461651325226
train loss :0.1655
---------------------
Validation seg loss: 0.22072291174762934 at epoch 308
epoch =    309/  1000, exp = train
train_iter_loss: 0.23909594118595123
train_iter_loss: 0.11841287463903427
train_iter_loss: 0.18719278275966644
train_iter_loss: 0.13821886479854584
train_iter_loss: 0.040089644491672516
train_iter_loss: 0.15190713107585907
train_iter_loss: 0.2833026051521301
train_iter_loss: 0.1630479246377945
train_iter_loss: 0.22297543287277222
train_iter_loss: 0.14634336531162262
train_iter_loss: 0.1019749566912651
train_iter_loss: 0.18701481819152832
train_iter_loss: 0.13744927942752838
train_iter_loss: 0.12217099964618683
train_iter_loss: 0.15503409504890442
train_iter_loss: 0.1654142439365387
train_iter_loss: 0.16357873380184174
train_iter_loss: 0.21339327096939087
train_iter_loss: 0.08658092468976974
train_iter_loss: 0.17202723026275635
train_iter_loss: 0.10069088637828827
train_iter_loss: 0.12819845974445343
train_iter_loss: 0.18317413330078125
train_iter_loss: 0.19160351157188416
train_iter_loss: 0.1065814197063446
train_iter_loss: 0.30488672852516174
train_iter_loss: 0.08461587876081467
train_iter_loss: 0.24810242652893066
train_iter_loss: 0.13189616799354553
train_iter_loss: 0.13606718182563782
train_iter_loss: 0.18985265493392944
train_iter_loss: 0.1975572556257248
train_iter_loss: 0.21014031767845154
train_iter_loss: 0.08709052205085754
train_iter_loss: 0.1410340666770935
train_iter_loss: 0.5078164935112
train_iter_loss: 0.20795342326164246
train_iter_loss: 0.05754513666033745
train_iter_loss: 0.08765242993831635
train_iter_loss: 0.24449214339256287
train_iter_loss: 0.14334769546985626
train_iter_loss: 0.15236146748065948
train_iter_loss: 0.21113593876361847
train_iter_loss: 0.21045027673244476
train_iter_loss: 0.15950514376163483
train_iter_loss: 0.14688216149806976
train_iter_loss: 0.24130026996135712
train_iter_loss: 0.16443748772144318
train_iter_loss: 0.1975320279598236
train_iter_loss: 0.10656549781560898
train_iter_loss: 0.19125114381313324
train_iter_loss: 0.15842582285404205
train_iter_loss: 0.2378421425819397
train_iter_loss: 0.20819877088069916
train_iter_loss: 0.0713895708322525
train_iter_loss: 0.12118062376976013
train_iter_loss: 0.09751810878515244
train_iter_loss: 0.15023623406887054
train_iter_loss: 0.22678647935390472
train_iter_loss: 0.2490076720714569
train_iter_loss: 0.17162802815437317
train_iter_loss: 0.08206908404827118
train_iter_loss: 0.27967938780784607
train_iter_loss: 0.13878896832466125
train_iter_loss: 0.1869400590658188
train_iter_loss: 0.18019770085811615
train_iter_loss: 0.17017635703086853
train_iter_loss: 0.09121240675449371
train_iter_loss: 0.07405851036310196
train_iter_loss: 0.08668820559978485
train_iter_loss: 0.19850003719329834
train_iter_loss: 0.16282495856285095
train_iter_loss: 0.12121430784463882
train_iter_loss: 0.07331930845975876
train_iter_loss: 0.22386570274829865
train_iter_loss: 0.140460804104805
train_iter_loss: 0.19226416945457458
train_iter_loss: 0.13729357719421387
train_iter_loss: 0.18719756603240967
train_iter_loss: 0.08628856390714645
train_iter_loss: 0.1974402666091919
train_iter_loss: 0.26201769709587097
train_iter_loss: 0.26589235663414
train_iter_loss: 0.1914970874786377
train_iter_loss: 0.12325157225131989
train_iter_loss: 0.1392344981431961
train_iter_loss: 0.2889977693557739
train_iter_loss: 0.18028803169727325
train_iter_loss: 0.11344888061285019
train_iter_loss: 0.3363167643547058
train_iter_loss: 0.1707388013601303
train_iter_loss: 0.1913536638021469
train_iter_loss: 0.075650155544281
train_iter_loss: 0.1165236383676529
train_iter_loss: 0.19654349982738495
train_iter_loss: 0.24218900501728058
train_iter_loss: 0.18524305522441864
train_iter_loss: 0.19739803671836853
train_iter_loss: 0.1293175369501114
train_iter_loss: 0.09420288354158401
train loss :0.1704
---------------------
Validation seg loss: 0.22357152311904532 at epoch 309
epoch =    310/  1000, exp = train
train_iter_loss: 0.20738087594509125
train_iter_loss: 0.2060072124004364
train_iter_loss: 0.20673057436943054
train_iter_loss: 0.23715680837631226
train_iter_loss: 0.14440952241420746
train_iter_loss: 0.14858976006507874
train_iter_loss: 0.20784012973308563
train_iter_loss: 0.09817798435688019
train_iter_loss: 0.21907992660999298
train_iter_loss: 0.12219641357660294
train_iter_loss: 0.4540514647960663
train_iter_loss: 0.14934585988521576
train_iter_loss: 0.09843618422746658
train_iter_loss: 0.13333596289157867
train_iter_loss: 0.10540241748094559
train_iter_loss: 0.15690137445926666
train_iter_loss: 0.19842205941677094
train_iter_loss: 0.0939934253692627
train_iter_loss: 0.1058349758386612
train_iter_loss: 0.15348240733146667
train_iter_loss: 0.2815369665622711
train_iter_loss: 0.3969384431838989
train_iter_loss: 0.24825409054756165
train_iter_loss: 0.1775837540626526
train_iter_loss: 0.05286707356572151
train_iter_loss: 0.13089381158351898
train_iter_loss: 0.22989794611930847
train_iter_loss: 0.2818712592124939
train_iter_loss: 0.16240046918392181
train_iter_loss: 0.15671005845069885
train_iter_loss: 0.19731129705905914
train_iter_loss: 0.12655527889728546
train_iter_loss: 0.12428455054759979
train_iter_loss: 0.1752202957868576
train_iter_loss: 0.27496933937072754
train_iter_loss: 0.13235217332839966
train_iter_loss: 0.07742947340011597
train_iter_loss: 0.39970824122428894
train_iter_loss: 0.049807898700237274
train_iter_loss: 0.37181347608566284
train_iter_loss: 0.1873929351568222
train_iter_loss: 0.19242607057094574
train_iter_loss: 0.20587794482707977
train_iter_loss: 0.09804265946149826
train_iter_loss: 0.12987452745437622
train_iter_loss: 0.16144241392612457
train_iter_loss: 0.17314815521240234
train_iter_loss: 0.08954080194234848
train_iter_loss: 0.08748777955770493
train_iter_loss: 0.28664636611938477
train_iter_loss: 0.11781793087720871
train_iter_loss: 0.16708463430404663
train_iter_loss: 0.12440168112516403
train_iter_loss: 0.18346528708934784
train_iter_loss: 0.15297257900238037
train_iter_loss: 0.0992853194475174
train_iter_loss: 0.1388070285320282
train_iter_loss: 0.16277891397476196
train_iter_loss: 0.19837123155593872
train_iter_loss: 0.16775281727313995
train_iter_loss: 0.19132965803146362
train_iter_loss: 0.10485637933015823
train_iter_loss: 0.3318064510822296
train_iter_loss: 0.23556071519851685
train_iter_loss: 0.20255780220031738
train_iter_loss: 0.07708651572465897
train_iter_loss: 0.20316767692565918
train_iter_loss: 0.17298421263694763
train_iter_loss: 0.1586994230747223
train_iter_loss: 0.19256240129470825
train_iter_loss: 0.07398643344640732
train_iter_loss: 0.20193441212177277
train_iter_loss: 0.2272251397371292
train_iter_loss: 0.14134463667869568
train_iter_loss: 0.3855440318584442
train_iter_loss: 0.1230735331773758
train_iter_loss: 0.07183697819709778
train_iter_loss: 0.0760287344455719
train_iter_loss: 0.10024110972881317
train_iter_loss: 0.2249402552843094
train_iter_loss: 0.1208949163556099
train_iter_loss: 0.11540079116821289
train_iter_loss: 0.13181565701961517
train_iter_loss: 0.08172915875911713
train_iter_loss: 0.13762837648391724
train_iter_loss: 0.08716946095228195
train_iter_loss: 0.1715414673089981
train_iter_loss: 0.1865837424993515
train_iter_loss: 0.19211114943027496
train_iter_loss: 0.14895664155483246
train_iter_loss: 0.1867731511592865
train_iter_loss: 0.11622605472803116
train_iter_loss: 0.14262156188488007
train_iter_loss: 0.10028842091560364
train_iter_loss: 0.16552585363388062
train_iter_loss: 0.12622110545635223
train_iter_loss: 0.11727625876665115
train_iter_loss: 0.20039160549640656
train_iter_loss: 0.07413671910762787
train_iter_loss: 0.0894499346613884
train loss :0.1698
---------------------
Validation seg loss: 0.2211876681056928 at epoch 310
epoch =    311/  1000, exp = train
train_iter_loss: 0.1636599749326706
train_iter_loss: 0.08878078311681747
train_iter_loss: 0.14270298182964325
train_iter_loss: 0.17308388650417328
train_iter_loss: 0.09709896147251129
train_iter_loss: 0.1786459982395172
train_iter_loss: 0.19119098782539368
train_iter_loss: 0.17312303185462952
train_iter_loss: 0.20221048593521118
train_iter_loss: 0.30414271354675293
train_iter_loss: 0.08916161209344864
train_iter_loss: 0.052526623010635376
train_iter_loss: 0.31370145082473755
train_iter_loss: 0.21804110705852509
train_iter_loss: 0.1377880722284317
train_iter_loss: 0.1697617620229721
train_iter_loss: 0.12149914354085922
train_iter_loss: 0.10547161847352982
train_iter_loss: 0.19607356190681458
train_iter_loss: 0.22852173447608948
train_iter_loss: 0.13829779624938965
train_iter_loss: 0.12361002713441849
train_iter_loss: 0.24871328473091125
train_iter_loss: 0.13728216290473938
train_iter_loss: 0.1198681965470314
train_iter_loss: 0.2035578340291977
train_iter_loss: 0.22091171145439148
train_iter_loss: 0.14638416469097137
train_iter_loss: 0.09852106869220734
train_iter_loss: 0.1790800243616104
train_iter_loss: 0.0646517351269722
train_iter_loss: 0.1709171086549759
train_iter_loss: 0.1712481826543808
train_iter_loss: 0.37473416328430176
train_iter_loss: 0.36704346537590027
train_iter_loss: 0.31465789675712585
train_iter_loss: 0.14851529896259308
train_iter_loss: 0.11739757657051086
train_iter_loss: 0.14508992433547974
train_iter_loss: 0.2150450199842453
train_iter_loss: 0.1995389759540558
train_iter_loss: 0.03353561460971832
train_iter_loss: 0.1901533305644989
train_iter_loss: 0.15238893032073975
train_iter_loss: 0.19624392688274384
train_iter_loss: 0.15398922562599182
train_iter_loss: 0.19889414310455322
train_iter_loss: 0.08345005661249161
train_iter_loss: 0.1368149071931839
train_iter_loss: 0.19698785245418549
train_iter_loss: 0.15660063922405243
train_iter_loss: 0.11185596883296967
train_iter_loss: 0.09218768775463104
train_iter_loss: 0.24813231825828552
train_iter_loss: 0.042519599199295044
train_iter_loss: 0.1754971742630005
train_iter_loss: 0.14936193823814392
train_iter_loss: 0.15236002206802368
train_iter_loss: 0.2238180786371231
train_iter_loss: 0.2405320256948471
train_iter_loss: 0.13794712722301483
train_iter_loss: 0.16247734427452087
train_iter_loss: 0.10444290935993195
train_iter_loss: 0.17059440910816193
train_iter_loss: 0.2270667850971222
train_iter_loss: 0.15836186707019806
train_iter_loss: 0.23047716915607452
train_iter_loss: 0.10124892741441727
train_iter_loss: 0.1995006799697876
train_iter_loss: 0.20157405734062195
train_iter_loss: 0.07825762033462524
train_iter_loss: 0.18297363817691803
train_iter_loss: 0.2132410854101181
train_iter_loss: 0.10054847598075867
train_iter_loss: 0.269813597202301
train_iter_loss: 0.1419404298067093
train_iter_loss: 0.23985819518566132
train_iter_loss: 0.1883058249950409
train_iter_loss: 0.29539525508880615
train_iter_loss: 0.1708734929561615
train_iter_loss: 0.17741695046424866
train_iter_loss: 0.2092251181602478
train_iter_loss: 0.12968333065509796
train_iter_loss: 0.11704528331756592
train_iter_loss: 0.1822616159915924
train_iter_loss: 0.15932738780975342
train_iter_loss: 0.20121078193187714
train_iter_loss: 0.14251895248889923
train_iter_loss: 0.098997101187706
train_iter_loss: 0.09113861620426178
train_iter_loss: 0.16432997584342957
train_iter_loss: 0.14026890695095062
train_iter_loss: 0.1747177243232727
train_iter_loss: 0.1836487203836441
train_iter_loss: 0.16050104796886444
train_iter_loss: 0.02135438099503517
train_iter_loss: 0.2302962839603424
train_iter_loss: 0.11005639284849167
train_iter_loss: 0.18402604758739471
train_iter_loss: 0.07168637961149216
train loss :0.1688
---------------------
Validation seg loss: 0.21756079707751577 at epoch 311
epoch =    312/  1000, exp = train
train_iter_loss: 0.22898247838020325
train_iter_loss: 0.06184015050530434
train_iter_loss: 0.2003658562898636
train_iter_loss: 0.09576315432786942
train_iter_loss: 0.21678860485553741
train_iter_loss: 0.1008094996213913
train_iter_loss: 0.1937176138162613
train_iter_loss: 0.1686595231294632
train_iter_loss: 0.1651267409324646
train_iter_loss: 0.21908478438854218
train_iter_loss: 0.14153268933296204
train_iter_loss: 0.17872551083564758
train_iter_loss: 0.1719031035900116
train_iter_loss: 0.23749414086341858
train_iter_loss: 0.1355157345533371
train_iter_loss: 0.21336455643177032
train_iter_loss: 0.254029244184494
train_iter_loss: 0.2444458305835724
train_iter_loss: 0.14765509963035583
train_iter_loss: 0.1723315566778183
train_iter_loss: 0.22348789870738983
train_iter_loss: 0.13032770156860352
train_iter_loss: 0.13243988156318665
train_iter_loss: 0.046401575207710266
train_iter_loss: 0.2569088935852051
train_iter_loss: 0.24479742348194122
train_iter_loss: 0.18139982223510742
train_iter_loss: 0.09866191446781158
train_iter_loss: 0.11476922035217285
train_iter_loss: 0.23055674135684967
train_iter_loss: 0.07848189771175385
train_iter_loss: 0.15426069498062134
train_iter_loss: 0.1368575543165207
train_iter_loss: 0.16351208090782166
train_iter_loss: 0.05555073171854019
train_iter_loss: 0.10333751887083054
train_iter_loss: 0.16482575237751007
train_iter_loss: 0.12185506522655487
train_iter_loss: 0.1504828780889511
train_iter_loss: 0.08987914025783539
train_iter_loss: 0.15680086612701416
train_iter_loss: 0.2278086245059967
train_iter_loss: 0.13888344168663025
train_iter_loss: 0.10358670353889465
train_iter_loss: 0.10728195309638977
train_iter_loss: 0.16044437885284424
train_iter_loss: 0.17643454670906067
train_iter_loss: 0.20541398227214813
train_iter_loss: 0.2447180300951004
train_iter_loss: 0.1222296953201294
train_iter_loss: 0.15779642760753632
train_iter_loss: 0.08208440989255905
train_iter_loss: 0.17185817658901215
train_iter_loss: 0.1348327249288559
train_iter_loss: 0.1275293231010437
train_iter_loss: 0.22796446084976196
train_iter_loss: 0.3021479845046997
train_iter_loss: 0.1682247519493103
train_iter_loss: 0.15463769435882568
train_iter_loss: 0.1916954666376114
train_iter_loss: 0.20420849323272705
train_iter_loss: 0.09019935131072998
train_iter_loss: 0.17649726569652557
train_iter_loss: 0.1703377515077591
train_iter_loss: 0.17160236835479736
train_iter_loss: 0.2188582420349121
train_iter_loss: 0.147964745759964
train_iter_loss: 0.13406969606876373
train_iter_loss: 0.2194862961769104
train_iter_loss: 0.15613071620464325
train_iter_loss: 0.11469364166259766
train_iter_loss: 0.13421320915222168
train_iter_loss: 0.14132925868034363
train_iter_loss: 0.2301255762577057
train_iter_loss: 0.11334303766489029
train_iter_loss: 0.12680189311504364
train_iter_loss: 0.3054092824459076
train_iter_loss: 0.13961030542850494
train_iter_loss: 0.09253084659576416
train_iter_loss: 0.24488261342048645
train_iter_loss: 0.12454544752836227
train_iter_loss: 0.1456824541091919
train_iter_loss: 0.22712533175945282
train_iter_loss: 0.2412303388118744
train_iter_loss: 0.12449124455451965
train_iter_loss: 0.12145776301622391
train_iter_loss: 0.16938026249408722
train_iter_loss: 0.09405484795570374
train_iter_loss: 0.14414723217487335
train_iter_loss: 0.1599414050579071
train_iter_loss: 0.07772314548492432
train_iter_loss: 0.1605190932750702
train_iter_loss: 0.18201975524425507
train_iter_loss: 0.06596153229475021
train_iter_loss: 0.18343591690063477
train_iter_loss: 0.2076544314622879
train_iter_loss: 0.1823122650384903
train_iter_loss: 0.08204134553670883
train_iter_loss: 0.2695011794567108
train_iter_loss: 0.17336805164813995
train loss :0.1643
---------------------
Validation seg loss: 0.22004412095290873 at epoch 312
epoch =    313/  1000, exp = train
train_iter_loss: 0.13750098645687103
train_iter_loss: 0.11655668169260025
train_iter_loss: 0.22656191885471344
train_iter_loss: 0.11917457729578018
train_iter_loss: 0.08869396895170212
train_iter_loss: 0.1179850623011589
train_iter_loss: 0.12709301710128784
train_iter_loss: 0.26623472571372986
train_iter_loss: 0.10510041564702988
train_iter_loss: 0.2975950837135315
train_iter_loss: 0.09652276337146759
train_iter_loss: 0.15304534137248993
train_iter_loss: 0.21211475133895874
train_iter_loss: 0.16096945106983185
train_iter_loss: 0.14823263883590698
train_iter_loss: 0.19809094071388245
train_iter_loss: 0.13088461756706238
train_iter_loss: 0.2445136159658432
train_iter_loss: 0.17316748201847076
train_iter_loss: 0.16358399391174316
train_iter_loss: 0.1473953127861023
train_iter_loss: 0.13503848016262054
train_iter_loss: 0.09712629020214081
train_iter_loss: 0.0927160233259201
train_iter_loss: 0.20018956065177917
train_iter_loss: 0.20588067173957825
train_iter_loss: 0.13939133286476135
train_iter_loss: 0.1895969957113266
train_iter_loss: 0.059022318571805954
train_iter_loss: 0.2400745451450348
train_iter_loss: 0.10881447046995163
train_iter_loss: 0.11384453624486923
train_iter_loss: 0.06558601558208466
train_iter_loss: 0.2004440873861313
train_iter_loss: 0.04843446984887123
train_iter_loss: 0.21315205097198486
train_iter_loss: 0.19405896961688995
train_iter_loss: 0.3173801004886627
train_iter_loss: 0.16765232384204865
train_iter_loss: 0.17819605767726898
train_iter_loss: 0.1813415139913559
train_iter_loss: 0.15844354033470154
train_iter_loss: 0.1642201542854309
train_iter_loss: 0.17706730961799622
train_iter_loss: 0.12167061120271683
train_iter_loss: 0.07740511745214462
train_iter_loss: 0.1436476856470108
train_iter_loss: 0.2051295042037964
train_iter_loss: 0.1254037320613861
train_iter_loss: 0.1788085550069809
train_iter_loss: 0.15522997081279755
train_iter_loss: 0.17283986508846283
train_iter_loss: 0.177472323179245
train_iter_loss: 0.09901534020900726
train_iter_loss: 0.1909184604883194
train_iter_loss: 0.21433326601982117
train_iter_loss: 0.11533991247415543
train_iter_loss: 0.12128324806690216
train_iter_loss: 0.23613424599170685
train_iter_loss: 0.10124445706605911
train_iter_loss: 0.2054208517074585
train_iter_loss: 0.17618536949157715
train_iter_loss: 0.1555943340063095
train_iter_loss: 0.1993868499994278
train_iter_loss: 0.19994157552719116
train_iter_loss: 0.227894589304924
train_iter_loss: 0.08105102181434631
train_iter_loss: 0.07191193103790283
train_iter_loss: 0.08577802777290344
train_iter_loss: 0.20596562325954437
train_iter_loss: 0.21611186861991882
train_iter_loss: 0.18170404434204102
train_iter_loss: 0.18199412524700165
train_iter_loss: 0.16524946689605713
train_iter_loss: 0.19566239416599274
train_iter_loss: 0.19096103310585022
train_iter_loss: 0.15629097819328308
train_iter_loss: 0.14601927995681763
train_iter_loss: 0.1683701127767563
train_iter_loss: 0.11341928690671921
train_iter_loss: 0.16566215455532074
train_iter_loss: 0.17947296798229218
train_iter_loss: 0.37071189284324646
train_iter_loss: 0.09587036073207855
train_iter_loss: 0.04859727621078491
train_iter_loss: 0.19285401701927185
train_iter_loss: 0.12635181844234467
train_iter_loss: 0.10056490451097488
train_iter_loss: 0.16260415315628052
train_iter_loss: 0.15792524814605713
train_iter_loss: 0.15960778295993805
train_iter_loss: 0.11471705138683319
train_iter_loss: 0.2102293223142624
train_iter_loss: 0.15427766740322113
train_iter_loss: 0.12110146135091782
train_iter_loss: 0.1215573325753212
train_iter_loss: 0.16926905512809753
train_iter_loss: 0.19466817378997803
train_iter_loss: 0.10515597462654114
train_iter_loss: 0.15480774641036987
train loss :0.1611
---------------------
Validation seg loss: 0.22021401596238027 at epoch 313
epoch =    314/  1000, exp = train
train_iter_loss: 0.06396853923797607
train_iter_loss: 0.2989816665649414
train_iter_loss: 0.07326287031173706
train_iter_loss: 0.2797803282737732
train_iter_loss: 0.17070862650871277
train_iter_loss: 0.09421247988939285
train_iter_loss: 0.12462235242128372
train_iter_loss: 0.09681355953216553
train_iter_loss: 0.13461706042289734
train_iter_loss: 0.22386039793491364
train_iter_loss: 0.1265556663274765
train_iter_loss: 0.186443492770195
train_iter_loss: 0.11026428639888763
train_iter_loss: 0.09482189267873764
train_iter_loss: 0.06325402855873108
train_iter_loss: 0.19148683547973633
train_iter_loss: 0.12538057565689087
train_iter_loss: 0.19728466868400574
train_iter_loss: 0.26104536652565
train_iter_loss: 0.12853801250457764
train_iter_loss: 0.09842896461486816
train_iter_loss: 0.09952080249786377
train_iter_loss: 0.23982831835746765
train_iter_loss: 0.2479780614376068
train_iter_loss: 0.1542590707540512
train_iter_loss: 0.1651671975851059
train_iter_loss: 0.10882411897182465
train_iter_loss: 0.1791955530643463
train_iter_loss: 0.18679490685462952
train_iter_loss: 0.39631733298301697
train_iter_loss: 0.17913185060024261
train_iter_loss: 0.2381601780653
train_iter_loss: 0.20820142328739166
train_iter_loss: 0.075536347925663
train_iter_loss: 0.1342015117406845
train_iter_loss: 0.2345910221338272
train_iter_loss: 0.15207280218601227
train_iter_loss: 0.1969626247882843
train_iter_loss: 0.2248162031173706
train_iter_loss: 0.05275976285338402
train_iter_loss: 0.1829376518726349
train_iter_loss: 0.13143093883991241
train_iter_loss: 0.14082129299640656
train_iter_loss: 0.13858632743358612
train_iter_loss: 0.12312362343072891
train_iter_loss: 0.10338102281093597
train_iter_loss: 0.15836654603481293
train_iter_loss: 0.10760662704706192
train_iter_loss: 0.1819731593132019
train_iter_loss: 0.10105663537979126
train_iter_loss: 0.14170174300670624
train_iter_loss: 0.14549291133880615
train_iter_loss: 0.13824574649333954
train_iter_loss: 0.14945855736732483
train_iter_loss: 0.16367167234420776
train_iter_loss: 0.08920171111822128
train_iter_loss: 0.24850134551525116
train_iter_loss: 0.1753637045621872
train_iter_loss: 0.17325818538665771
train_iter_loss: 0.18773123621940613
train_iter_loss: 0.14357614517211914
train_iter_loss: 0.263257771730423
train_iter_loss: 0.10805555433034897
train_iter_loss: 0.1636512577533722
train_iter_loss: 0.16128164529800415
train_iter_loss: 0.22400107979774475
train_iter_loss: 0.14530086517333984
train_iter_loss: 0.1653074324131012
train_iter_loss: 0.11029203981161118
train_iter_loss: 0.13202667236328125
train_iter_loss: 0.07673447579145432
train_iter_loss: 0.08320054411888123
train_iter_loss: 0.19680440425872803
train_iter_loss: 0.14692319929599762
train_iter_loss: 0.20097149908542633
train_iter_loss: 0.06311680376529694
train_iter_loss: 0.1664273589849472
train_iter_loss: 0.24025431275367737
train_iter_loss: 0.12103980779647827
train_iter_loss: 0.10920331627130508
train_iter_loss: 0.10547094792127609
train_iter_loss: 0.24065503478050232
train_iter_loss: 0.15792332589626312
train_iter_loss: 0.19392885267734528
train_iter_loss: 0.3415254056453705
train_iter_loss: 0.08295224606990814
train_iter_loss: 0.1074928417801857
train_iter_loss: 0.0760536789894104
train_iter_loss: 0.16710804402828217
train_iter_loss: 0.2983969449996948
train_iter_loss: 0.11772358417510986
train_iter_loss: 0.18336635828018188
train_iter_loss: 0.22789417207241058
train_iter_loss: 0.06940986961126328
train_iter_loss: 0.15585657954216003
train_iter_loss: 0.1431065797805786
train_iter_loss: 0.2406291514635086
train_iter_loss: 0.12658387422561646
train_iter_loss: 0.09841486811637878
train_iter_loss: 0.32037702202796936
train loss :0.1624
---------------------
Validation seg loss: 0.22160546037034606 at epoch 314
epoch =    315/  1000, exp = train
train_iter_loss: 0.14921675622463226
train_iter_loss: 0.205079585313797
train_iter_loss: 0.2075694352388382
train_iter_loss: 0.12368544936180115
train_iter_loss: 0.09847807139158249
train_iter_loss: 0.09954576939344406
train_iter_loss: 0.20031139254570007
train_iter_loss: 0.2551192045211792
train_iter_loss: 0.168844535946846
train_iter_loss: 0.1882166862487793
train_iter_loss: 0.26431235671043396
train_iter_loss: 0.3418557941913605
train_iter_loss: 0.21285982429981232
train_iter_loss: 0.21807362139225006
train_iter_loss: 0.09860936552286148
train_iter_loss: 0.15521198511123657
train_iter_loss: 0.24721692502498627
train_iter_loss: 0.13095997273921967
train_iter_loss: 0.12681658565998077
train_iter_loss: 0.20431628823280334
train_iter_loss: 0.08596104383468628
train_iter_loss: 0.15102440118789673
train_iter_loss: 0.1792323887348175
train_iter_loss: 0.16924606263637543
train_iter_loss: 0.16812308132648468
train_iter_loss: 0.13095547258853912
train_iter_loss: 0.1540834605693817
train_iter_loss: 0.1545737385749817
train_iter_loss: 0.1112486869096756
train_iter_loss: 0.3736514151096344
train_iter_loss: 0.20221339166164398
train_iter_loss: 0.07928997278213501
train_iter_loss: 0.09548725932836533
train_iter_loss: 0.18471243977546692
train_iter_loss: 0.10917098075151443
train_iter_loss: 0.15291012823581696
train_iter_loss: 0.10372092574834824
train_iter_loss: 0.21120359003543854
train_iter_loss: 0.14731162786483765
train_iter_loss: 0.1796017438173294
train_iter_loss: 0.14329734444618225
train_iter_loss: 0.26698604226112366
train_iter_loss: 0.14276182651519775
train_iter_loss: 0.23226884007453918
train_iter_loss: 0.0937645360827446
train_iter_loss: 0.07077083736658096
train_iter_loss: 0.1779593676328659
train_iter_loss: 0.1350049078464508
train_iter_loss: 0.060822002589702606
train_iter_loss: 0.12445927411317825
train_iter_loss: 0.1807434856891632
train_iter_loss: 0.2836311459541321
train_iter_loss: 0.14819905161857605
train_iter_loss: 0.12292955070734024
train_iter_loss: 0.3128354549407959
train_iter_loss: 0.1595989316701889
train_iter_loss: 0.15128937363624573
train_iter_loss: 0.23989759385585785
train_iter_loss: 0.13094551861286163
train_iter_loss: 0.21102246642112732
train_iter_loss: 0.10298359394073486
train_iter_loss: 0.18453598022460938
train_iter_loss: 0.0815642774105072
train_iter_loss: 0.08358252048492432
train_iter_loss: 0.1180444210767746
train_iter_loss: 0.1940675526857376
train_iter_loss: 0.15139292180538177
train_iter_loss: 0.1250831037759781
train_iter_loss: 0.15940023958683014
train_iter_loss: 0.1884676218032837
train_iter_loss: 0.1989261955022812
train_iter_loss: 0.18763791024684906
train_iter_loss: 0.07605499029159546
train_iter_loss: 0.1319398134946823
train_iter_loss: 0.20006531476974487
train_iter_loss: 0.14309455454349518
train_iter_loss: 0.1337815672159195
train_iter_loss: 0.21213047206401825
train_iter_loss: 0.1845148354768753
train_iter_loss: 0.10085674375295639
train_iter_loss: 0.19929739832878113
train_iter_loss: 0.26204371452331543
train_iter_loss: 0.12328853458166122
train_iter_loss: 0.14312702417373657
train_iter_loss: 0.10459984093904495
train_iter_loss: 0.15766368806362152
train_iter_loss: 0.13194945454597473
train_iter_loss: 0.17084971070289612
train_iter_loss: 0.17366071045398712
train_iter_loss: 0.09931380301713943
train_iter_loss: 0.14981548488140106
train_iter_loss: 0.09858901053667068
train_iter_loss: 0.17101262509822845
train_iter_loss: 0.16214920580387115
train_iter_loss: 0.10576410591602325
train_iter_loss: 0.08700140565633774
train_iter_loss: 0.23971737921237946
train_iter_loss: 0.19407795369625092
train_iter_loss: 0.33372488617897034
train_iter_loss: 0.07303347438573837
train loss :0.1653
---------------------
Validation seg loss: 0.22171130161380992 at epoch 315
epoch =    316/  1000, exp = train
train_iter_loss: 0.16571533679962158
train_iter_loss: 0.22175295650959015
train_iter_loss: 0.18821023404598236
train_iter_loss: 0.26417306065559387
train_iter_loss: 0.38746461272239685
train_iter_loss: 0.06615086644887924
train_iter_loss: 0.1334202140569687
train_iter_loss: 0.15269362926483154
train_iter_loss: 0.2052740901708603
train_iter_loss: 0.10953199863433838
train_iter_loss: 0.1561029851436615
train_iter_loss: 0.12460638582706451
train_iter_loss: 0.10353928059339523
train_iter_loss: 0.1524425745010376
train_iter_loss: 0.2052745372056961
train_iter_loss: 0.1361149698495865
train_iter_loss: 0.12370578199625015
train_iter_loss: 0.1703130453824997
train_iter_loss: 0.1429162323474884
train_iter_loss: 0.13996635377407074
train_iter_loss: 0.12570425868034363
train_iter_loss: 0.4310985803604126
train_iter_loss: 0.1963862031698227
train_iter_loss: 0.05444660410284996
train_iter_loss: 0.2990873157978058
train_iter_loss: 0.311891108751297
train_iter_loss: 0.318118155002594
train_iter_loss: 0.14038701355457306
train_iter_loss: 0.1490003913640976
train_iter_loss: 0.1211206316947937
train_iter_loss: 0.14008250832557678
train_iter_loss: 0.15006080269813538
train_iter_loss: 0.14512038230895996
train_iter_loss: 0.16016657650470734
train_iter_loss: 0.0649501383304596
train_iter_loss: 0.19366958737373352
train_iter_loss: 0.13740740716457367
train_iter_loss: 0.1425071358680725
train_iter_loss: 0.14050331711769104
train_iter_loss: 0.11432568728923798
train_iter_loss: 0.04802345484495163
train_iter_loss: 0.3225174844264984
train_iter_loss: 0.19808237254619598
train_iter_loss: 0.14324069023132324
train_iter_loss: 0.159171462059021
train_iter_loss: 0.16535857319831848
train_iter_loss: 0.28999313712120056
train_iter_loss: 0.10712939500808716
train_iter_loss: 0.1512165367603302
train_iter_loss: 0.12895634770393372
train_iter_loss: 0.11169257014989853
train_iter_loss: 0.12675943970680237
train_iter_loss: 0.09052696079015732
train_iter_loss: 0.11097198724746704
train_iter_loss: 0.1787392795085907
train_iter_loss: 0.20560966432094574
train_iter_loss: 0.09540224075317383
train_iter_loss: 0.20126034319400787
train_iter_loss: 0.19067980349063873
train_iter_loss: 0.20363286137580872
train_iter_loss: 0.14464208483695984
train_iter_loss: 0.1742428094148636
train_iter_loss: 0.06700586527585983
train_iter_loss: 0.17066729068756104
train_iter_loss: 0.25321558117866516
train_iter_loss: 0.13359665870666504
train_iter_loss: 0.187940314412117
train_iter_loss: 0.18270352482795715
train_iter_loss: 0.23422688245773315
train_iter_loss: 0.3688953220844269
train_iter_loss: 0.11531563103199005
train_iter_loss: 0.1439610719680786
train_iter_loss: 0.11445718258619308
train_iter_loss: 0.2387930303812027
train_iter_loss: 0.20600256323814392
train_iter_loss: 0.23356732726097107
train_iter_loss: 0.08726222068071365
train_iter_loss: 0.2714149057865143
train_iter_loss: 0.11128709465265274
train_iter_loss: 0.15801888704299927
train_iter_loss: 0.1383877992630005
train_iter_loss: 0.10447470098733902
train_iter_loss: 0.13719609379768372
train_iter_loss: 0.37442949414253235
train_iter_loss: 0.15561607480049133
train_iter_loss: 0.20186251401901245
train_iter_loss: 0.1113828495144844
train_iter_loss: 0.1851615011692047
train_iter_loss: 0.10176796466112137
train_iter_loss: 0.09455839544534683
train_iter_loss: 0.1757628321647644
train_iter_loss: 0.2612011432647705
train_iter_loss: 0.1399734616279602
train_iter_loss: 0.09344381839036942
train_iter_loss: 0.10598578304052353
train_iter_loss: 0.07649640738964081
train_iter_loss: 0.22434799373149872
train_iter_loss: 0.14786002039909363
train_iter_loss: 0.2069482058286667
train_iter_loss: 0.2035675197839737
train loss :0.1712
---------------------
Validation seg loss: 0.21736953112314333 at epoch 316
********************
best_val_epoch_loss:  0.21736953112314333
MODEL UPDATED
epoch =    317/  1000, exp = train
train_iter_loss: 0.1353362798690796
train_iter_loss: 0.1399305760860443
train_iter_loss: 0.12730158865451813
train_iter_loss: 0.09772635251283646
train_iter_loss: 0.15862776339054108
train_iter_loss: 0.2329939603805542
train_iter_loss: 0.16178038716316223
train_iter_loss: 0.09878938645124435
train_iter_loss: 0.13063794374465942
train_iter_loss: 0.1855303794145584
train_iter_loss: 0.16780313849449158
train_iter_loss: 0.24685318768024445
train_iter_loss: 0.17619246244430542
train_iter_loss: 0.14331986010074615
train_iter_loss: 0.10975814610719681
train_iter_loss: 0.18956513702869415
train_iter_loss: 0.13268911838531494
train_iter_loss: 0.11058108508586884
train_iter_loss: 0.18728382885456085
train_iter_loss: 0.11857961863279343
train_iter_loss: 0.18269994854927063
train_iter_loss: 0.2627132833003998
train_iter_loss: 0.15912283957004547
train_iter_loss: 0.20308761298656464
train_iter_loss: 0.05209256708621979
train_iter_loss: 0.08366689085960388
train_iter_loss: 0.3571598529815674
train_iter_loss: 0.09968960285186768
train_iter_loss: 0.20537516474723816
train_iter_loss: 0.21963685750961304
train_iter_loss: 0.19240616261959076
train_iter_loss: 0.25399187207221985
train_iter_loss: 0.11546063423156738
train_iter_loss: 0.19773460924625397
train_iter_loss: 0.2733500599861145
train_iter_loss: 0.1303960382938385
train_iter_loss: 0.23208269476890564
train_iter_loss: 0.16625745594501495
train_iter_loss: 0.18251004815101624
train_iter_loss: 0.1309630423784256
train_iter_loss: 0.05975877493619919
train_iter_loss: 0.22384460270404816
train_iter_loss: 0.2641456425189972
train_iter_loss: 0.19505757093429565
train_iter_loss: 0.1853100061416626
train_iter_loss: 0.1183842271566391
train_iter_loss: 0.13415153324604034
train_iter_loss: 0.18957898020744324
train_iter_loss: 0.1956438571214676
train_iter_loss: 0.24473269283771515
train_iter_loss: 0.15671533346176147
train_iter_loss: 0.16727739572525024
train_iter_loss: 0.23412244021892548
train_iter_loss: 0.1212308332324028
train_iter_loss: 0.1616673469543457
train_iter_loss: 0.1765989512205124
train_iter_loss: 0.1360442191362381
train_iter_loss: 0.10925695300102234
train_iter_loss: 0.18469400703907013
train_iter_loss: 0.22122329473495483
train_iter_loss: 0.3246064782142639
train_iter_loss: 0.2251635044813156
train_iter_loss: 0.05059593543410301
train_iter_loss: 0.1341945230960846
train_iter_loss: 0.31510457396507263
train_iter_loss: 0.1501191258430481
train_iter_loss: 0.24154236912727356
train_iter_loss: 0.29252785444259644
train_iter_loss: 0.21231815218925476
train_iter_loss: 0.14664795994758606
train_iter_loss: 0.17737385630607605
train_iter_loss: 0.10972969233989716
train_iter_loss: 0.1108798012137413
train_iter_loss: 0.36020782589912415
train_iter_loss: 0.1012716069817543
train_iter_loss: 0.20040108263492584
train_iter_loss: 0.22022351622581482
train_iter_loss: 0.03180418536067009
train_iter_loss: 0.14675721526145935
train_iter_loss: 0.13782921433448792
train_iter_loss: 0.0435652881860733
train_iter_loss: 0.2166329175233841
train_iter_loss: 0.18007954955101013
train_iter_loss: 0.1407809555530548
train_iter_loss: 0.26270490884780884
train_iter_loss: 0.1353304535150528
train_iter_loss: 0.057104066014289856
train_iter_loss: 0.18732434511184692
train_iter_loss: 0.1676841527223587
train_iter_loss: 0.20826534926891327
train_iter_loss: 0.17218267917633057
train_iter_loss: 0.1905708611011505
train_iter_loss: 0.1041657030582428
train_iter_loss: 0.12819820642471313
train_iter_loss: 0.23503942787647247
train_iter_loss: 0.1846490204334259
train_iter_loss: 0.09922415018081665
train_iter_loss: 0.11179937422275543
train_iter_loss: 0.19689682126045227
train_iter_loss: 0.11454231292009354
train loss :0.1723
---------------------
Validation seg loss: 0.223380177290583 at epoch 317
epoch =    318/  1000, exp = train
train_iter_loss: 0.2785709798336029
train_iter_loss: 0.13180270791053772
train_iter_loss: 0.12473499774932861
train_iter_loss: 0.2907419502735138
train_iter_loss: 0.1408921629190445
train_iter_loss: 0.2658887505531311
train_iter_loss: 0.2455841451883316
train_iter_loss: 0.1979074478149414
train_iter_loss: 0.17829446494579315
train_iter_loss: 0.12807460129261017
train_iter_loss: 0.09656972438097
train_iter_loss: 0.10798255354166031
train_iter_loss: 0.17706060409545898
train_iter_loss: 0.17416350543498993
train_iter_loss: 0.23129835724830627
train_iter_loss: 0.10243114829063416
train_iter_loss: 0.09895861893892288
train_iter_loss: 0.1408030092716217
train_iter_loss: 0.11209771037101746
train_iter_loss: 0.43565797805786133
train_iter_loss: 0.13836000859737396
train_iter_loss: 0.19878070056438446
train_iter_loss: 0.2975328266620636
train_iter_loss: 0.14728786051273346
train_iter_loss: 0.19362123310565948
train_iter_loss: 0.20446504652500153
train_iter_loss: 0.12456753849983215
train_iter_loss: 0.14282169938087463
train_iter_loss: 0.1489851325750351
train_iter_loss: 0.14775846898555756
train_iter_loss: 0.1419801563024521
train_iter_loss: 0.06967705488204956
train_iter_loss: 0.13782759010791779
train_iter_loss: 0.1645354926586151
train_iter_loss: 0.148590087890625
train_iter_loss: 0.17368073761463165
train_iter_loss: 0.06784967333078384
train_iter_loss: 0.07794025540351868
train_iter_loss: 0.08410289883613586
train_iter_loss: 0.13455632328987122
train_iter_loss: 0.08173681050539017
train_iter_loss: 0.23139959573745728
train_iter_loss: 0.16223372519016266
train_iter_loss: 0.19436337053775787
train_iter_loss: 0.07510532438755035
train_iter_loss: 0.14534306526184082
train_iter_loss: 0.13894757628440857
train_iter_loss: 0.1874903440475464
train_iter_loss: 0.1450422704219818
train_iter_loss: 0.1528751701116562
train_iter_loss: 0.18883411586284637
train_iter_loss: 0.13951990008354187
train_iter_loss: 0.12730355560779572
train_iter_loss: 0.31809625029563904
train_iter_loss: 0.13332852721214294
train_iter_loss: 0.20111921429634094
train_iter_loss: 0.15731248259544373
train_iter_loss: 0.05640439689159393
train_iter_loss: 0.11931712925434113
train_iter_loss: 0.21631142497062683
train_iter_loss: 0.17742404341697693
train_iter_loss: 0.05043961852788925
train_iter_loss: 0.1338917464017868
train_iter_loss: 0.1771862953901291
train_iter_loss: 0.2059650719165802
train_iter_loss: 0.14403726160526276
train_iter_loss: 0.2797826826572418
train_iter_loss: 0.25689229369163513
train_iter_loss: 0.14411835372447968
train_iter_loss: 0.1605844795703888
train_iter_loss: 0.244382843375206
train_iter_loss: 0.08350060135126114
train_iter_loss: 0.13744814693927765
train_iter_loss: 0.244007408618927
train_iter_loss: 0.18130409717559814
train_iter_loss: 0.20521578192710876
train_iter_loss: 0.19174931943416595
train_iter_loss: 0.2213486135005951
train_iter_loss: 0.10030870139598846
train_iter_loss: 0.17211201786994934
train_iter_loss: 0.11417780816555023
train_iter_loss: 0.09852397441864014
train_iter_loss: 0.2207440435886383
train_iter_loss: 0.13237521052360535
train_iter_loss: 0.12421897053718567
train_iter_loss: 0.10896451771259308
train_iter_loss: 0.1209285706281662
train_iter_loss: 0.2874135375022888
train_iter_loss: 0.2154790163040161
train_iter_loss: 0.26662102341651917
train_iter_loss: 0.19281885027885437
train_iter_loss: 0.17871014773845673
train_iter_loss: 0.1570795476436615
train_iter_loss: 0.10507933795452118
train_iter_loss: 0.15013384819030762
train_iter_loss: 0.1278626173734665
train_iter_loss: 0.18935313820838928
train_iter_loss: 0.15060582756996155
train_iter_loss: 0.14036694169044495
train_iter_loss: 0.14214499294757843
train loss :0.1668
---------------------
Validation seg loss: 0.21945692762240487 at epoch 318
epoch =    319/  1000, exp = train
train_iter_loss: 0.17018863558769226
train_iter_loss: 0.1141078919172287
train_iter_loss: 0.19343143701553345
train_iter_loss: 0.12925361096858978
train_iter_loss: 0.11347167938947678
train_iter_loss: 0.13079409301280975
train_iter_loss: 0.13626034557819366
train_iter_loss: 0.223816379904747
train_iter_loss: 0.0640571117401123
train_iter_loss: 0.22859899699687958
train_iter_loss: 0.12304603308439255
train_iter_loss: 0.15301620960235596
train_iter_loss: 0.1021750420331955
train_iter_loss: 0.16856993734836578
train_iter_loss: 0.1859140843153
train_iter_loss: 0.15266622602939606
train_iter_loss: 0.29941892623901367
train_iter_loss: 0.1634681224822998
train_iter_loss: 0.16971728205680847
train_iter_loss: 0.08003123849630356
train_iter_loss: 0.08593837171792984
train_iter_loss: 0.18326906859874725
train_iter_loss: 0.14564479887485504
train_iter_loss: 0.23386648297309875
train_iter_loss: 0.12433139979839325
train_iter_loss: 0.3814130127429962
train_iter_loss: 0.17805108428001404
train_iter_loss: 0.1564560979604721
train_iter_loss: 0.21519619226455688
train_iter_loss: 0.10697007179260254
train_iter_loss: 0.18238303065299988
train_iter_loss: 0.16104398667812347
train_iter_loss: 0.045526523143053055
train_iter_loss: 0.18476493656635284
train_iter_loss: 0.14292217791080475
train_iter_loss: 0.14939828217029572
train_iter_loss: 0.25870609283447266
train_iter_loss: 0.12568168342113495
train_iter_loss: 0.16594313085079193
train_iter_loss: 0.18335263431072235
train_iter_loss: 0.19665886461734772
train_iter_loss: 0.10595391690731049
train_iter_loss: 0.10151582211256027
train_iter_loss: 0.19506320357322693
train_iter_loss: 0.22263988852500916
train_iter_loss: 0.190945565700531
train_iter_loss: 0.1324191689491272
train_iter_loss: 0.12274520099163055
train_iter_loss: 0.0892389714717865
train_iter_loss: 0.2609832286834717
train_iter_loss: 0.1313071846961975
train_iter_loss: 0.17478755116462708
train_iter_loss: 0.2203034609556198
train_iter_loss: 0.16806495189666748
train_iter_loss: 0.19879083335399628
train_iter_loss: 0.190609872341156
train_iter_loss: 0.097676582634449
train_iter_loss: 0.13501574099063873
train_iter_loss: 0.09018412232398987
train_iter_loss: 0.04607735201716423
train_iter_loss: 0.3512125015258789
train_iter_loss: 0.2653025686740875
train_iter_loss: 0.25006747245788574
train_iter_loss: 0.1468646377325058
train_iter_loss: 0.24605827033519745
train_iter_loss: 0.13735318183898926
train_iter_loss: 0.13361571729183197
train_iter_loss: 0.18019697070121765
train_iter_loss: 0.29742443561553955
train_iter_loss: 0.2034715712070465
train_iter_loss: 0.32318535447120667
train_iter_loss: 0.10244008153676987
train_iter_loss: 0.19297149777412415
train_iter_loss: 0.09568293392658234
train_iter_loss: 0.0868564248085022
train_iter_loss: 0.1271074265241623
train_iter_loss: 0.1400037705898285
train_iter_loss: 0.2519768476486206
train_iter_loss: 0.10316278785467148
train_iter_loss: 0.2710418999195099
train_iter_loss: 0.1090582013130188
train_iter_loss: 0.19257748126983643
train_iter_loss: 0.10239305347204208
train_iter_loss: 0.11420790106058121
train_iter_loss: 0.18654455244541168
train_iter_loss: 0.1695963591337204
train_iter_loss: 0.11348473280668259
train_iter_loss: 0.1442389041185379
train_iter_loss: 0.19864743947982788
train_iter_loss: 0.19837087392807007
train_iter_loss: 0.13632091879844666
train_iter_loss: 0.1927645057439804
train_iter_loss: 0.256024569272995
train_iter_loss: 0.10096219927072525
train_iter_loss: 0.22235585749149323
train_iter_loss: 0.28770557045936584
train_iter_loss: 0.12383981049060822
train_iter_loss: 0.2746196389198303
train_iter_loss: 0.22333447635173798
train_iter_loss: 0.05059100687503815
train loss :0.1705
---------------------
Validation seg loss: 0.2210580094295712 at epoch 319
epoch =    320/  1000, exp = train
train_iter_loss: 0.14461374282836914
train_iter_loss: 0.10868654400110245
train_iter_loss: 0.3155204951763153
train_iter_loss: 0.20812056958675385
train_iter_loss: 0.05422091484069824
train_iter_loss: 0.17716482281684875
train_iter_loss: 0.18037037551403046
train_iter_loss: 0.04575327783823013
train_iter_loss: 0.15002068877220154
train_iter_loss: 0.2846068739891052
train_iter_loss: 0.13013215363025665
train_iter_loss: 0.16780789196491241
train_iter_loss: 0.09848320484161377
train_iter_loss: 0.1746205985546112
train_iter_loss: 0.11328806728124619
train_iter_loss: 0.215179443359375
train_iter_loss: 0.2643159329891205
train_iter_loss: 0.1713193953037262
train_iter_loss: 0.1399400532245636
train_iter_loss: 0.12249065190553665
train_iter_loss: 0.08129455149173737
train_iter_loss: 0.24211445450782776
train_iter_loss: 0.21890926361083984
train_iter_loss: 0.15056289732456207
train_iter_loss: 0.18419960141181946
train_iter_loss: 0.17097480595111847
train_iter_loss: 0.1968710571527481
train_iter_loss: 0.13956165313720703
train_iter_loss: 0.1234080120921135
train_iter_loss: 0.06628023833036423
train_iter_loss: 0.15702366828918457
train_iter_loss: 0.2308225929737091
train_iter_loss: 0.19909356534481049
train_iter_loss: 0.13186877965927124
train_iter_loss: 0.12303546816110611
train_iter_loss: 0.1749185025691986
train_iter_loss: 0.12922678887844086
train_iter_loss: 0.14612062275409698
train_iter_loss: 0.10525781661272049
train_iter_loss: 0.13615334033966064
train_iter_loss: 0.15870611369609833
train_iter_loss: 0.259829580783844
train_iter_loss: 0.21115469932556152
train_iter_loss: 0.08598145097494125
train_iter_loss: 0.15554296970367432
train_iter_loss: 0.047958966344594955
train_iter_loss: 0.23035003244876862
train_iter_loss: 0.08963895589113235
train_iter_loss: 0.10287202149629593
train_iter_loss: 0.2787231504917145
train_iter_loss: 0.12805511057376862
train_iter_loss: 0.22007884085178375
train_iter_loss: 0.18284067511558533
train_iter_loss: 0.12569935619831085
train_iter_loss: 0.21248756349086761
train_iter_loss: 0.09540783613920212
train_iter_loss: 0.16866286098957062
train_iter_loss: 0.20076754689216614
train_iter_loss: 0.27769556641578674
train_iter_loss: 0.1792251318693161
train_iter_loss: 0.29675644636154175
train_iter_loss: 0.13926713168621063
train_iter_loss: 0.15048204362392426
train_iter_loss: 0.20284605026245117
train_iter_loss: 0.1986989676952362
train_iter_loss: 0.4393082857131958
train_iter_loss: 0.15905171632766724
train_iter_loss: 0.3084026873111725
train_iter_loss: 0.12304020673036575
train_iter_loss: 0.15990740060806274
train_iter_loss: 0.1344112604856491
train_iter_loss: 0.21001079678535461
train_iter_loss: 0.13902795314788818
train_iter_loss: 0.18825072050094604
train_iter_loss: 0.09360922127962112
train_iter_loss: 0.12159372121095657
train_iter_loss: 0.1317797154188156
train_iter_loss: 0.2073100358247757
train_iter_loss: 0.15591762959957123
train_iter_loss: 0.21364974975585938
train_iter_loss: 0.11472836881875992
train_iter_loss: 0.2436874359846115
train_iter_loss: 0.17948654294013977
train_iter_loss: 0.128005251288414
train_iter_loss: 0.1758263260126114
train_iter_loss: 0.08582630753517151
train_iter_loss: 0.16906791925430298
train_iter_loss: 0.12243465334177017
train_iter_loss: 0.11724962294101715
train_iter_loss: 0.18912452459335327
train_iter_loss: 0.05355673283338547
train_iter_loss: 0.05433181673288345
train_iter_loss: 0.28865256905555725
train_iter_loss: 0.1464356631040573
train_iter_loss: 0.2298772633075714
train_iter_loss: 0.07146629691123962
train_iter_loss: 0.1962364763021469
train_iter_loss: 0.19546616077423096
train_iter_loss: 0.08734709024429321
train_iter_loss: 0.08357945829629898
train loss :0.1666
---------------------
Validation seg loss: 0.2208958907966625 at epoch 320
epoch =    321/  1000, exp = train
train_iter_loss: 0.13390053808689117
train_iter_loss: 0.11895356327295303
train_iter_loss: 0.1512937694787979
train_iter_loss: 0.2319822609424591
train_iter_loss: 0.17951372265815735
train_iter_loss: 0.12221099436283112
train_iter_loss: 0.14165906608104706
train_iter_loss: 0.12302611768245697
train_iter_loss: 0.09719647467136383
train_iter_loss: 0.1438533216714859
train_iter_loss: 0.08505081385374069
train_iter_loss: 0.1650455892086029
train_iter_loss: 0.22041544318199158
train_iter_loss: 0.10381565243005753
train_iter_loss: 0.14745061099529266
train_iter_loss: 0.2253490388393402
train_iter_loss: 0.07777412235736847
train_iter_loss: 0.2883703112602234
train_iter_loss: 0.17713811993598938
train_iter_loss: 0.1551801860332489
train_iter_loss: 0.10342247039079666
train_iter_loss: 0.14129993319511414
train_iter_loss: 0.0642152652144432
train_iter_loss: 0.2492266595363617
train_iter_loss: 0.08534901589155197
train_iter_loss: 0.13276314735412598
train_iter_loss: 0.2854861319065094
train_iter_loss: 0.138197660446167
train_iter_loss: 0.17299555242061615
train_iter_loss: 0.1432906538248062
train_iter_loss: 0.1436186581850052
train_iter_loss: 0.2723114490509033
train_iter_loss: 0.1897047609090805
train_iter_loss: 0.22182902693748474
train_iter_loss: 0.2022138237953186
train_iter_loss: 0.15029022097587585
train_iter_loss: 0.08771829307079315
train_iter_loss: 0.34921994805336
train_iter_loss: 0.18560999631881714
train_iter_loss: 0.14509914815425873
train_iter_loss: 0.10376179963350296
train_iter_loss: 0.1939685195684433
train_iter_loss: 0.2169070690870285
train_iter_loss: 0.09071682393550873
train_iter_loss: 0.09682545810937881
train_iter_loss: 0.20617540180683136
train_iter_loss: 0.17622019350528717
train_iter_loss: 0.06188305467367172
train_iter_loss: 0.16632603108882904
train_iter_loss: 0.2540605664253235
train_iter_loss: 0.08403973281383514
train_iter_loss: 0.13777798414230347
train_iter_loss: 0.17431245744228363
train_iter_loss: 0.1790766716003418
train_iter_loss: 0.16852623224258423
train_iter_loss: 0.2026882916688919
train_iter_loss: 0.14623205363750458
train_iter_loss: 0.1782585233449936
train_iter_loss: 0.13520829379558563
train_iter_loss: 0.18244323134422302
train_iter_loss: 0.28291648626327515
train_iter_loss: 0.08936558663845062
train_iter_loss: 0.16878552734851837
train_iter_loss: 0.1537669450044632
train_iter_loss: 0.17585410177707672
train_iter_loss: 0.1497216373682022
train_iter_loss: 0.07132300734519958
train_iter_loss: 0.13539209961891174
train_iter_loss: 0.15336789190769196
train_iter_loss: 0.2355482578277588
train_iter_loss: 0.12527400255203247
train_iter_loss: 0.26691552996635437
train_iter_loss: 0.11849680542945862
train_iter_loss: 0.1049351617693901
train_iter_loss: 0.1611586958169937
train_iter_loss: 0.102794349193573
train_iter_loss: 0.16453278064727783
train_iter_loss: 0.23866339027881622
train_iter_loss: 0.21641825139522552
train_iter_loss: 0.24558940529823303
train_iter_loss: 0.10479895770549774
train_iter_loss: 0.13298730552196503
train_iter_loss: 0.14336928725242615
train_iter_loss: 0.4021167457103729
train_iter_loss: 0.20916640758514404
train_iter_loss: 0.130653977394104
train_iter_loss: 0.19104617834091187
train_iter_loss: 0.1280190497636795
train_iter_loss: 0.13500075042247772
train_iter_loss: 0.0974610447883606
train_iter_loss: 0.1916070282459259
train_iter_loss: 0.18686391413211823
train_iter_loss: 0.21032780408859253
train_iter_loss: 0.10639503598213196
train_iter_loss: 0.08404283225536346
train_iter_loss: 0.1189054474234581
train_iter_loss: 0.2066299319267273
train_iter_loss: 0.14903035759925842
train_iter_loss: 0.10086095333099365
train_iter_loss: 0.19235476851463318
train loss :0.1646
---------------------
Validation seg loss: 0.22193242893290688 at epoch 321
epoch =    322/  1000, exp = train
train_iter_loss: 0.07039414346218109
train_iter_loss: 0.1582147777080536
train_iter_loss: 0.2133600115776062
train_iter_loss: 0.14092406630516052
train_iter_loss: 0.16001515090465546
train_iter_loss: 0.1576252579689026
train_iter_loss: 0.19210530817508698
train_iter_loss: 0.22430096566677094
train_iter_loss: 0.216350719332695
train_iter_loss: 0.08996035158634186
train_iter_loss: 0.08812765032052994
train_iter_loss: 0.14578785002231598
train_iter_loss: 0.12931448221206665
train_iter_loss: 0.1677565723657608
train_iter_loss: 0.25828373432159424
train_iter_loss: 0.2231927365064621
train_iter_loss: 0.17670483887195587
train_iter_loss: 0.1878441870212555
train_iter_loss: 0.4420694410800934
train_iter_loss: 0.17888982594013214
train_iter_loss: 0.15168914198875427
train_iter_loss: 0.12664106488227844
train_iter_loss: 0.10511637479066849
train_iter_loss: 0.05191535875201225
train_iter_loss: 0.19507688283920288
train_iter_loss: 0.3107314109802246
train_iter_loss: 0.14504452049732208
train_iter_loss: 0.15250493586063385
train_iter_loss: 0.18167398869991302
train_iter_loss: 0.18377232551574707
train_iter_loss: 0.17053666710853577
train_iter_loss: 0.18066182732582092
train_iter_loss: 0.18124791979789734
train_iter_loss: 0.1214698925614357
train_iter_loss: 0.17386670410633087
train_iter_loss: 0.1603422313928604
train_iter_loss: 0.19492653012275696
train_iter_loss: 0.05799531936645508
train_iter_loss: 0.22391146421432495
train_iter_loss: 0.23376214504241943
train_iter_loss: 0.11156107485294342
train_iter_loss: 0.2765786647796631
train_iter_loss: 0.1270856112241745
train_iter_loss: 0.20324212312698364
train_iter_loss: 0.06845664978027344
train_iter_loss: 0.17106294631958008
train_iter_loss: 0.1478865146636963
train_iter_loss: 0.2077004313468933
train_iter_loss: 0.06192746385931969
train_iter_loss: 0.13076871633529663
train_iter_loss: 0.11938422173261642
train_iter_loss: 0.14776740968227386
train_iter_loss: 0.12739987671375275
train_iter_loss: 0.23757359385490417
train_iter_loss: 0.17313693463802338
train_iter_loss: 0.11191704869270325
train_iter_loss: 0.026474980637431145
train_iter_loss: 0.2063475251197815
train_iter_loss: 0.13929331302642822
train_iter_loss: 0.21163228154182434
train_iter_loss: 0.14358747005462646
train_iter_loss: 0.21583494544029236
train_iter_loss: 0.09675180912017822
train_iter_loss: 0.10129454731941223
train_iter_loss: 0.16372160613536835
train_iter_loss: 0.28393056988716125
train_iter_loss: 0.18048487603664398
train_iter_loss: 0.13463711738586426
train_iter_loss: 0.2269798368215561
train_iter_loss: 0.4977628290653229
train_iter_loss: 0.14582067728042603
train_iter_loss: 0.24618515372276306
train_iter_loss: 0.10623746365308762
train_iter_loss: 0.10091876238584518
train_iter_loss: 0.17953263223171234
train_iter_loss: 0.3160879909992218
train_iter_loss: 0.14467687904834747
train_iter_loss: 0.18006998300552368
train_iter_loss: 0.16425973176956177
train_iter_loss: 0.22894519567489624
train_iter_loss: 0.19617125391960144
train_iter_loss: 0.14063578844070435
train_iter_loss: 0.13147509098052979
train_iter_loss: 0.24364900588989258
train_iter_loss: 0.1088467389345169
train_iter_loss: 0.14109238982200623
train_iter_loss: 0.06243382394313812
train_iter_loss: 0.08867616951465607
train_iter_loss: 0.1508059948682785
train_iter_loss: 0.1493358612060547
train_iter_loss: 0.1463540643453598
train_iter_loss: 0.26168835163116455
train_iter_loss: 0.06389126181602478
train_iter_loss: 0.14795464277267456
train_iter_loss: 0.23858842253684998
train_iter_loss: 0.06854792684316635
train_iter_loss: 0.1731465458869934
train_iter_loss: 0.11628711968660355
train_iter_loss: 0.08906841278076172
train_iter_loss: 0.07550377398729324
train loss :0.1675
---------------------
Validation seg loss: 0.2185058047959827 at epoch 322
epoch =    323/  1000, exp = train
train_iter_loss: 0.24658116698265076
train_iter_loss: 0.1724802404642105
train_iter_loss: 0.09919702261686325
train_iter_loss: 0.16907243430614471
train_iter_loss: 0.16157639026641846
train_iter_loss: 0.23912209272384644
train_iter_loss: 0.20077531039714813
train_iter_loss: 0.15645302832126617
train_iter_loss: 0.19020037353038788
train_iter_loss: 0.22568923234939575
train_iter_loss: 0.06891170144081116
train_iter_loss: 0.19135519862174988
train_iter_loss: 0.15698036551475525
train_iter_loss: 0.15937551856040955
train_iter_loss: 0.21978861093521118
train_iter_loss: 0.16922684013843536
train_iter_loss: 0.11869217455387115
train_iter_loss: 0.18550123274326324
train_iter_loss: 0.10632824152708054
train_iter_loss: 0.2536807954311371
train_iter_loss: 0.14648883044719696
train_iter_loss: 0.12482454627752304
train_iter_loss: 0.20270904898643494
train_iter_loss: 0.18632163107395172
train_iter_loss: 0.0848737508058548
train_iter_loss: 0.1072409600019455
train_iter_loss: 0.21130366623401642
train_iter_loss: 0.3144926130771637
train_iter_loss: 0.1892721951007843
train_iter_loss: 0.1387655884027481
train_iter_loss: 0.09558515250682831
train_iter_loss: 0.10582324117422104
train_iter_loss: 0.09463537484407425
train_iter_loss: 0.3138122856616974
train_iter_loss: 0.1336786448955536
train_iter_loss: 0.1535833477973938
train_iter_loss: 0.3109680414199829
train_iter_loss: 0.14044485986232758
train_iter_loss: 0.2619453966617584
train_iter_loss: 0.1477540135383606
train_iter_loss: 0.1524006724357605
train_iter_loss: 0.06811410188674927
train_iter_loss: 0.10222207009792328
train_iter_loss: 0.20295728743076324
train_iter_loss: 0.34949401021003723
train_iter_loss: 0.09426843374967575
train_iter_loss: 0.10804690420627594
train_iter_loss: 0.22557026147842407
train_iter_loss: 0.1896810084581375
train_iter_loss: 0.07444804161787033
train_iter_loss: 0.07737289369106293
train_iter_loss: 0.14866545796394348
train_iter_loss: 0.2873327136039734
train_iter_loss: 0.18679282069206238
train_iter_loss: 0.19847169518470764
train_iter_loss: 0.12863916158676147
train_iter_loss: 0.08363017439842224
train_iter_loss: 0.2678360342979431
train_iter_loss: 0.2022913694381714
train_iter_loss: 0.08584494888782501
train_iter_loss: 0.13015511631965637
train_iter_loss: 0.3216981589794159
train_iter_loss: 0.3119032680988312
train_iter_loss: 0.10467596352100372
train_iter_loss: 0.033587366342544556
train_iter_loss: 0.11401931941509247
train_iter_loss: 0.11783000826835632
train_iter_loss: 0.12723535299301147
train_iter_loss: 0.09691986441612244
train_iter_loss: 0.08599020540714264
train_iter_loss: 0.2859336733818054
train_iter_loss: 0.09448760002851486
train_iter_loss: 0.1232840046286583
train_iter_loss: 0.14677774906158447
train_iter_loss: 0.10661011189222336
train_iter_loss: 0.19559445977210999
train_iter_loss: 0.2691227197647095
train_iter_loss: 0.10339249670505524
train_iter_loss: 0.2961384356021881
train_iter_loss: 0.12750078737735748
train_iter_loss: 0.11804009974002838
train_iter_loss: 0.21819816529750824
train_iter_loss: 0.10310668498277664
train_iter_loss: 0.23560413718223572
train_iter_loss: 0.13057833909988403
train_iter_loss: 0.1947803497314453
train_iter_loss: 0.131211519241333
train_iter_loss: 0.08241622149944305
train_iter_loss: 0.16897700726985931
train_iter_loss: 0.1455046385526657
train_iter_loss: 0.10581616312265396
train_iter_loss: 0.17398172616958618
train_iter_loss: 0.18085193634033203
train_iter_loss: 0.14722634851932526
train_iter_loss: 0.13561777770519257
train_iter_loss: 0.13115251064300537
train_iter_loss: 0.09834159910678864
train_iter_loss: 0.11595295369625092
train_iter_loss: 0.0534522719681263
train_iter_loss: 0.3189023733139038
train loss :0.1654
---------------------
Validation seg loss: 0.21836524049066147 at epoch 323
epoch =    324/  1000, exp = train
train_iter_loss: 0.1781163215637207
train_iter_loss: 0.2052721530199051
train_iter_loss: 0.12220524251461029
train_iter_loss: 0.14556416869163513
train_iter_loss: 0.3814631402492523
train_iter_loss: 0.15970751643180847
train_iter_loss: 0.2455703169107437
train_iter_loss: 0.2805400490760803
train_iter_loss: 0.1478581577539444
train_iter_loss: 0.09448382258415222
train_iter_loss: 0.189840629696846
train_iter_loss: 0.09274707734584808
train_iter_loss: 0.04318356141448021
train_iter_loss: 0.16578836739063263
train_iter_loss: 0.027573183178901672
train_iter_loss: 0.3378114402294159
train_iter_loss: 0.22787396609783173
train_iter_loss: 0.1979464739561081
train_iter_loss: 0.1999358832836151
train_iter_loss: 0.08582885563373566
train_iter_loss: 0.2081228643655777
train_iter_loss: 0.14436285197734833
train_iter_loss: 0.1308058500289917
train_iter_loss: 0.195677250623703
train_iter_loss: 0.08897945284843445
train_iter_loss: 0.10667679458856583
train_iter_loss: 0.09801375865936279
train_iter_loss: 0.10358130186796188
train_iter_loss: 0.12904104590415955
train_iter_loss: 0.3588414192199707
train_iter_loss: 0.12301948666572571
train_iter_loss: 0.34758415818214417
train_iter_loss: 0.24855169653892517
train_iter_loss: 0.13464763760566711
train_iter_loss: 0.1487063765525818
train_iter_loss: 0.15984657406806946
train_iter_loss: 0.2425743192434311
train_iter_loss: 0.293632447719574
train_iter_loss: 0.1541159301996231
train_iter_loss: 0.15001989901065826
train_iter_loss: 0.2780258357524872
train_iter_loss: 0.15967293083667755
train_iter_loss: 0.1957022249698639
train_iter_loss: 0.23350650072097778
train_iter_loss: 0.2016933560371399
train_iter_loss: 0.17804323136806488
train_iter_loss: 0.19882485270500183
train_iter_loss: 0.21884754300117493
train_iter_loss: 0.19476816058158875
train_iter_loss: 0.027155842632055283
train_iter_loss: 0.12883704900741577
train_iter_loss: 0.3020930886268616
train_iter_loss: 0.18724703788757324
train_iter_loss: 0.167490154504776
train_iter_loss: 0.17693893611431122
train_iter_loss: 0.1921880841255188
train_iter_loss: 0.1508600264787674
train_iter_loss: 0.1608295738697052
train_iter_loss: 0.12837839126586914
train_iter_loss: 0.1385747790336609
train_iter_loss: 0.20601403713226318
train_iter_loss: 0.1365555226802826
train_iter_loss: 0.15582780539989471
train_iter_loss: 0.1531149297952652
train_iter_loss: 0.15276603400707245
train_iter_loss: 0.16377313435077667
train_iter_loss: 0.12411493808031082
train_iter_loss: 0.19824562966823578
train_iter_loss: 0.14290912449359894
train_iter_loss: 0.26096484065055847
train_iter_loss: 0.10710826516151428
train_iter_loss: 0.3129083216190338
train_iter_loss: 0.10452244430780411
train_iter_loss: 0.17292635142803192
train_iter_loss: 0.06989003717899323
train_iter_loss: 0.12661941349506378
train_iter_loss: 0.17615295946598053
train_iter_loss: 0.0773228257894516
train_iter_loss: 0.11464782059192657
train_iter_loss: 0.06256834417581558
train_iter_loss: 0.23067320883274078
train_iter_loss: 0.3508753478527069
train_iter_loss: 0.16475453972816467
train_iter_loss: 0.17276684939861298
train_iter_loss: 0.09810356050729752
train_iter_loss: 0.24645106494426727
train_iter_loss: 0.2422342598438263
train_iter_loss: 0.18034352362155914
train_iter_loss: 0.14011190831661224
train_iter_loss: 0.08675627410411835
train_iter_loss: 0.17689833045005798
train_iter_loss: 0.11961758881807327
train_iter_loss: 0.18837204575538635
train_iter_loss: 0.11007474362850189
train_iter_loss: 0.13475166261196136
train_iter_loss: 0.08696165680885315
train_iter_loss: 0.16657014191150665
train_iter_loss: 0.19094614684581757
train_iter_loss: 0.13737912476062775
train_iter_loss: 0.14049582183361053
train loss :0.1727
---------------------
Validation seg loss: 0.21807712053809808 at epoch 324
epoch =    325/  1000, exp = train
train_iter_loss: 0.1510072499513626
train_iter_loss: 0.23601046204566956
train_iter_loss: 0.10739845037460327
train_iter_loss: 0.23328913748264313
train_iter_loss: 0.15409278869628906
train_iter_loss: 0.12541647255420685
train_iter_loss: 0.09077699482440948
train_iter_loss: 0.11241967976093292
train_iter_loss: 0.17171569168567657
train_iter_loss: 0.1186746209859848
train_iter_loss: 0.1472446322441101
train_iter_loss: 0.11957923322916031
train_iter_loss: 0.1463390439748764
train_iter_loss: 0.20191830396652222
train_iter_loss: 0.1959601491689682
train_iter_loss: 0.17883257567882538
train_iter_loss: 0.20450332760810852
train_iter_loss: 0.2916163206100464
train_iter_loss: 0.08580154180526733
train_iter_loss: 0.19339384138584137
train_iter_loss: 0.19534897804260254
train_iter_loss: 0.05584130063652992
train_iter_loss: 0.12126880884170532
train_iter_loss: 0.11917513608932495
train_iter_loss: 0.10536222159862518
train_iter_loss: 0.3595779836177826
train_iter_loss: 0.12150327116250992
train_iter_loss: 0.14792965352535248
train_iter_loss: 0.12835343182086945
train_iter_loss: 0.13313020765781403
train_iter_loss: 0.17317770421504974
train_iter_loss: 0.29665276408195496
train_iter_loss: 0.20727457106113434
train_iter_loss: 0.14220385253429413
train_iter_loss: 0.049916595220565796
train_iter_loss: 0.1323346346616745
train_iter_loss: 0.03247014433145523
train_iter_loss: 0.11206085234880447
train_iter_loss: 0.2635272741317749
train_iter_loss: 0.14832735061645508
train_iter_loss: 0.2554030418395996
train_iter_loss: 0.21009661257266998
train_iter_loss: 0.2023092359304428
train_iter_loss: 0.2060564011335373
train_iter_loss: 0.12982939183712006
train_iter_loss: 0.11464980244636536
train_iter_loss: 0.0859118103981018
train_iter_loss: 0.17732438445091248
train_iter_loss: 0.08858423680067062
train_iter_loss: 0.14808741211891174
train_iter_loss: 0.22367234528064728
train_iter_loss: 0.1829383820295334
train_iter_loss: 0.1980123519897461
train_iter_loss: 0.179796501994133
train_iter_loss: 0.2380276471376419
train_iter_loss: 0.27070561051368713
train_iter_loss: 0.13329225778579712
train_iter_loss: 0.1657576560974121
train_iter_loss: 0.10322259366512299
train_iter_loss: 0.21984925866127014
train_iter_loss: 0.06316900253295898
train_iter_loss: 0.18245741724967957
train_iter_loss: 0.16534140706062317
train_iter_loss: 0.21838724613189697
train_iter_loss: 0.2520096004009247
train_iter_loss: 0.13444051146507263
train_iter_loss: 0.1438869684934616
train_iter_loss: 0.21062809228897095
train_iter_loss: 0.1689171940088272
train_iter_loss: 0.31930291652679443
train_iter_loss: 0.09205441921949387
train_iter_loss: 0.11045268177986145
train_iter_loss: 0.12183865904808044
train_iter_loss: 0.21040883660316467
train_iter_loss: 0.18155288696289062
train_iter_loss: 0.33929604291915894
train_iter_loss: 0.20024968683719635
train_iter_loss: 0.0984436422586441
train_iter_loss: 0.12195839732885361
train_iter_loss: 0.1862768977880478
train_iter_loss: 0.15915881097316742
train_iter_loss: 0.2080233246088028
train_iter_loss: 0.12655878067016602
train_iter_loss: 0.07277960330247879
train_iter_loss: 0.1558881402015686
train_iter_loss: 0.2311861515045166
train_iter_loss: 0.14411666989326477
train_iter_loss: 0.1559159904718399
train_iter_loss: 0.041807617992162704
train_iter_loss: 0.1806974560022354
train_iter_loss: 0.16448494791984558
train_iter_loss: 0.19156816601753235
train_iter_loss: 0.06005053594708443
train_iter_loss: 0.24475279450416565
train_iter_loss: 0.20987170934677124
train_iter_loss: 0.2247498780488968
train_iter_loss: 0.05988419055938721
train_iter_loss: 0.13240677118301392
train_iter_loss: 0.14220452308654785
train_iter_loss: 0.24307169020175934
train loss :0.1669
---------------------
Validation seg loss: 0.21891378133364445 at epoch 325
epoch =    326/  1000, exp = train
train_iter_loss: 0.15454880893230438
train_iter_loss: 0.15932798385620117
train_iter_loss: 0.1118280291557312
train_iter_loss: 0.21380606293678284
train_iter_loss: 0.22611913084983826
train_iter_loss: 0.0922846645116806
train_iter_loss: 0.1059250608086586
train_iter_loss: 0.30133819580078125
train_iter_loss: 0.2381950467824936
train_iter_loss: 0.3020619750022888
train_iter_loss: 0.1854952871799469
train_iter_loss: 0.15657711029052734
train_iter_loss: 0.08143279701471329
train_iter_loss: 0.2877459228038788
train_iter_loss: 0.13873007893562317
train_iter_loss: 0.12060766667127609
train_iter_loss: 0.11206156760454178
train_iter_loss: 0.1650710105895996
train_iter_loss: 0.20454412698745728
train_iter_loss: 0.09885500371456146
train_iter_loss: 0.1203109472990036
train_iter_loss: 0.12413778901100159
train_iter_loss: 0.17892295122146606
train_iter_loss: 0.13632160425186157
train_iter_loss: 0.14212952554225922
train_iter_loss: 0.12445998936891556
train_iter_loss: 0.1737598478794098
train_iter_loss: 0.14345842599868774
train_iter_loss: 0.3237546384334564
train_iter_loss: 0.10808960348367691
train_iter_loss: 0.2833433449268341
train_iter_loss: 0.16784979403018951
train_iter_loss: 0.10913360863924026
train_iter_loss: 0.15917596220970154
train_iter_loss: 0.19484126567840576
train_iter_loss: 0.2325555682182312
train_iter_loss: 0.11265601962804794
train_iter_loss: 0.17079980671405792
train_iter_loss: 0.1142512634396553
train_iter_loss: 0.08734260499477386
train_iter_loss: 0.07252413034439087
train_iter_loss: 0.12841327488422394
train_iter_loss: 0.1512276828289032
train_iter_loss: 0.05557774007320404
train_iter_loss: 0.1771879494190216
train_iter_loss: 0.3042662739753723
train_iter_loss: 0.16440697014331818
train_iter_loss: 0.49682822823524475
train_iter_loss: 0.10809239000082016
train_iter_loss: 0.18487751483917236
train_iter_loss: 0.0757354274392128
train_iter_loss: 0.029262349009513855
train_iter_loss: 0.15155565738677979
train_iter_loss: 0.025660093873739243
train_iter_loss: 0.16344767808914185
train_iter_loss: 0.24791915714740753
train_iter_loss: 0.1284121572971344
train_iter_loss: 0.16284045577049255
train_iter_loss: 0.20070025324821472
train_iter_loss: 0.2689361572265625
train_iter_loss: 0.1792048215866089
train_iter_loss: 0.02770078368484974
train_iter_loss: 0.1584842950105667
train_iter_loss: 0.10689474642276764
train_iter_loss: 0.19201557338237762
train_iter_loss: 0.14063863456249237
train_iter_loss: 0.15567469596862793
train_iter_loss: 0.0917264074087143
train_iter_loss: 0.20880883932113647
train_iter_loss: 0.18944184482097626
train_iter_loss: 0.1563611626625061
train_iter_loss: 0.22124947607517242
train_iter_loss: 0.08986544609069824
train_iter_loss: 0.10918424278497696
train_iter_loss: 0.12276250869035721
train_iter_loss: 0.0980154499411583
train_iter_loss: 0.1295621246099472
train_iter_loss: 0.13180026412010193
train_iter_loss: 0.12494761496782303
train_iter_loss: 0.2504532039165497
train_iter_loss: 0.2296067327260971
train_iter_loss: 0.15509635210037231
train_iter_loss: 0.15862956643104553
train_iter_loss: 0.18121464550495148
train_iter_loss: 0.14638324081897736
train_iter_loss: 0.15144696831703186
train_iter_loss: 0.09119068831205368
train_iter_loss: 0.09026801586151123
train_iter_loss: 0.14606249332427979
train_iter_loss: 0.27120399475097656
train_iter_loss: 0.11495134979486465
train_iter_loss: 0.24198296666145325
train_iter_loss: 0.22881993651390076
train_iter_loss: 0.1517292559146881
train_iter_loss: 0.3121224641799927
train_iter_loss: 0.12530146539211273
train_iter_loss: 0.14563189446926117
train_iter_loss: 0.12174836546182632
train_iter_loss: 0.09175905585289001
train_iter_loss: 0.19321677088737488
train loss :0.1636
---------------------
Validation seg loss: 0.2213960880905671 at epoch 326
epoch =    327/  1000, exp = train
train_iter_loss: 0.1306619942188263
train_iter_loss: 0.09037284553050995
train_iter_loss: 0.18104656040668488
train_iter_loss: 0.19681435823440552
train_iter_loss: 0.1990141123533249
train_iter_loss: 0.36726003885269165
train_iter_loss: 0.08712669461965561
train_iter_loss: 0.07865729182958603
train_iter_loss: 0.19901619851589203
train_iter_loss: 0.09591975063085556
train_iter_loss: 0.2058805376291275
train_iter_loss: 0.21829773485660553
train_iter_loss: 0.31762972474098206
train_iter_loss: 0.09234068542718887
train_iter_loss: 0.16783490777015686
train_iter_loss: 0.21931037306785583
train_iter_loss: 0.19477218389511108
train_iter_loss: 0.1302463412284851
train_iter_loss: 0.21606774628162384
train_iter_loss: 0.2576749920845032
train_iter_loss: 0.1264977902173996
train_iter_loss: 0.11605279892683029
train_iter_loss: 0.18543490767478943
train_iter_loss: 0.2070682942867279
train_iter_loss: 0.09520021826028824
train_iter_loss: 0.1858026683330536
train_iter_loss: 0.188004732131958
train_iter_loss: 0.1736602932214737
train_iter_loss: 0.17860282957553864
train_iter_loss: 0.03927471116185188
train_iter_loss: 0.08042534440755844
train_iter_loss: 0.15616488456726074
train_iter_loss: 0.1697697639465332
train_iter_loss: 0.1883726567029953
train_iter_loss: 0.0813838541507721
train_iter_loss: 0.17010918259620667
train_iter_loss: 0.21287627518177032
train_iter_loss: 0.1664336770772934
train_iter_loss: 0.047660864889621735
train_iter_loss: 0.24244078993797302
train_iter_loss: 0.11427976936101913
train_iter_loss: 0.0812678262591362
train_iter_loss: 0.2039353996515274
train_iter_loss: 0.10697855055332184
train_iter_loss: 0.08630659431219101
train_iter_loss: 0.14520418643951416
train_iter_loss: 0.23589850962162018
train_iter_loss: 0.08816167712211609
train_iter_loss: 0.09729717671871185
train_iter_loss: 0.10468947142362595
train_iter_loss: 0.19604326784610748
train_iter_loss: 0.18581867218017578
train_iter_loss: 0.16542646288871765
train_iter_loss: 0.1851721704006195
train_iter_loss: 0.17148706316947937
train_iter_loss: 0.20116403698921204
train_iter_loss: 0.31454840302467346
train_iter_loss: 0.23432932794094086
train_iter_loss: 0.20453207194805145
train_iter_loss: 0.21421584486961365
train_iter_loss: 0.1957710087299347
train_iter_loss: 0.27893495559692383
train_iter_loss: 0.15352484583854675
train_iter_loss: 0.20370669662952423
train_iter_loss: 0.180277019739151
train_iter_loss: 0.06291547417640686
train_iter_loss: 0.16930465400218964
train_iter_loss: 0.10096508264541626
train_iter_loss: 0.0976325049996376
train_iter_loss: 0.10624264180660248
train_iter_loss: 0.13085483014583588
train_iter_loss: 0.24299663305282593
train_iter_loss: 0.21165570616722107
train_iter_loss: 0.1349799931049347
train_iter_loss: 0.24174439907073975
train_iter_loss: 0.35034650564193726
train_iter_loss: 0.14705052971839905
train_iter_loss: 0.17676521837711334
train_iter_loss: 0.2552606463432312
train_iter_loss: 0.14349643886089325
train_iter_loss: 0.23321078717708588
train_iter_loss: 0.08037075400352478
train_iter_loss: 0.18664728105068207
train_iter_loss: 0.18318849802017212
train_iter_loss: 0.1965705007314682
train_iter_loss: 0.2679683566093445
train_iter_loss: 0.1330588459968567
train_iter_loss: 0.11100312322378159
train_iter_loss: 0.23027664422988892
train_iter_loss: 0.13552263379096985
train_iter_loss: 0.10840369015932083
train_iter_loss: 0.08617248386144638
train_iter_loss: 0.17970138788223267
train_iter_loss: 0.08583477884531021
train_iter_loss: 0.10320873558521271
train_iter_loss: 0.07391193509101868
train_iter_loss: 0.15875525772571564
train_iter_loss: 0.04298248514533043
train_iter_loss: 0.18790680170059204
train_iter_loss: 0.1120009496808052
train loss :0.1664
---------------------
Validation seg loss: 0.2163882831041262 at epoch 327
********************
best_val_epoch_loss:  0.2163882831041262
MODEL UPDATED
epoch =    328/  1000, exp = train
train_iter_loss: 0.15366938710212708
train_iter_loss: 0.04570319503545761
train_iter_loss: 0.11300497502088547
train_iter_loss: 0.12489411979913712
train_iter_loss: 0.2087291181087494
train_iter_loss: 0.3096238970756531
train_iter_loss: 0.11568121612071991
train_iter_loss: 0.035834550857543945
train_iter_loss: 0.08086744695901871
train_iter_loss: 0.12241272628307343
train_iter_loss: 0.1075119748711586
train_iter_loss: 0.0721515417098999
train_iter_loss: 0.14164134860038757
train_iter_loss: 0.07202701270580292
train_iter_loss: 0.14792770147323608
train_iter_loss: 0.16839075088500977
train_iter_loss: 0.2854558229446411
train_iter_loss: 0.12574175000190735
train_iter_loss: 0.1372334361076355
train_iter_loss: 0.09197894483804703
train_iter_loss: 0.17403139173984528
train_iter_loss: 0.15351596474647522
train_iter_loss: 0.20941828191280365
train_iter_loss: 0.12525703012943268
train_iter_loss: 0.3630249798297882
train_iter_loss: 0.16312184929847717
train_iter_loss: 0.15816473960876465
train_iter_loss: 0.19480136036872864
train_iter_loss: 0.21702070534229279
train_iter_loss: 0.2142471820116043
train_iter_loss: 0.28765517473220825
train_iter_loss: 0.11037746071815491
train_iter_loss: 0.1686428338289261
train_iter_loss: 0.09542927145957947
train_iter_loss: 0.08055361360311508
train_iter_loss: 0.12161627411842346
train_iter_loss: 0.1851011961698532
train_iter_loss: 0.227990984916687
train_iter_loss: 0.2952888607978821
train_iter_loss: 0.15404899418354034
train_iter_loss: 0.11266859620809555
train_iter_loss: 0.21594741940498352
train_iter_loss: 0.18890319764614105
train_iter_loss: 0.10653536766767502
train_iter_loss: 0.142244353890419
train_iter_loss: 0.23178772628307343
train_iter_loss: 0.17151567339897156
train_iter_loss: 0.11613934487104416
train_iter_loss: 0.10422679781913757
train_iter_loss: 0.14643728733062744
train_iter_loss: 0.18334418535232544
train_iter_loss: 0.09599662572145462
train_iter_loss: 0.16566742956638336
train_iter_loss: 0.15005965530872345
train_iter_loss: 0.15705208480358124
train_iter_loss: 0.18853138387203217
train_iter_loss: 0.21903572976589203
train_iter_loss: 0.12581868469715118
train_iter_loss: 0.13359466195106506
train_iter_loss: 0.13004235923290253
train_iter_loss: 0.2539491653442383
train_iter_loss: 0.2557205855846405
train_iter_loss: 0.13072757422924042
train_iter_loss: 0.16566631197929382
train_iter_loss: 0.06646881252527237
train_iter_loss: 0.08827167004346848
train_iter_loss: 0.30472609400749207
train_iter_loss: 0.17243465781211853
train_iter_loss: 0.13719618320465088
train_iter_loss: 0.12063079327344894
train_iter_loss: 0.25794050097465515
train_iter_loss: 0.15958595275878906
train_iter_loss: 0.17816054821014404
train_iter_loss: 0.22931461036205292
train_iter_loss: 0.11066149920225143
train_iter_loss: 0.1557801514863968
train_iter_loss: 0.18153274059295654
train_iter_loss: 0.12353622913360596
train_iter_loss: 0.4074130654335022
train_iter_loss: 0.25334325432777405
train_iter_loss: 0.2764965891838074
train_iter_loss: 0.31123054027557373
train_iter_loss: 0.12829671800136566
train_iter_loss: 0.07030846178531647
train_iter_loss: 0.07560558617115021
train_iter_loss: 0.21000000834465027
train_iter_loss: 0.14544431865215302
train_iter_loss: 0.3011661469936371
train_iter_loss: 0.18219266831874847
train_iter_loss: 0.21023701131343842
train_iter_loss: 0.1483912616968155
train_iter_loss: 0.19305536150932312
train_iter_loss: 0.054698728024959564
train_iter_loss: 0.187453493475914
train_iter_loss: 0.10009701550006866
train_iter_loss: 0.2501767873764038
train_iter_loss: 0.11971201002597809
train_iter_loss: 0.11062371730804443
train_iter_loss: 0.2693527936935425
train_iter_loss: 0.16545142233371735
train loss :0.1687
---------------------
Validation seg loss: 0.2181270734740878 at epoch 328
epoch =    329/  1000, exp = train
train_iter_loss: 0.2728295624256134
train_iter_loss: 0.07963261008262634
train_iter_loss: 0.095985047519207
train_iter_loss: 0.11853660643100739
train_iter_loss: 0.18177159130573273
train_iter_loss: 0.23064211010932922
train_iter_loss: 0.10083804279565811
train_iter_loss: 0.06917231529951096
train_iter_loss: 0.18100996315479279
train_iter_loss: 0.253800630569458
train_iter_loss: 0.1535995453596115
train_iter_loss: 0.21394062042236328
train_iter_loss: 0.2092350274324417
train_iter_loss: 0.20621247589588165
train_iter_loss: 0.14556828141212463
train_iter_loss: 0.09925027191638947
train_iter_loss: 0.10697437077760696
train_iter_loss: 0.13179557025432587
train_iter_loss: 0.13845857977867126
train_iter_loss: 0.1922910362482071
train_iter_loss: 0.22163476049900055
train_iter_loss: 0.07728724926710129
train_iter_loss: 0.4079855680465698
train_iter_loss: 0.16817134618759155
train_iter_loss: 0.0920574963092804
train_iter_loss: 0.049351245164871216
train_iter_loss: 0.12404051423072815
train_iter_loss: 0.17256022989749908
train_iter_loss: 0.12348100543022156
train_iter_loss: 0.08574970811605453
train_iter_loss: 0.1392168253660202
train_iter_loss: 0.08820793777704239
train_iter_loss: 0.12679888308048248
train_iter_loss: 0.2410440295934677
train_iter_loss: 0.11933645606040955
train_iter_loss: 0.19853749871253967
train_iter_loss: 0.10061677545309067
train_iter_loss: 0.10950317233800888
train_iter_loss: 0.13256533443927765
train_iter_loss: 0.21433264017105103
train_iter_loss: 0.08950695395469666
train_iter_loss: 0.11817438900470734
train_iter_loss: 0.21864351630210876
train_iter_loss: 0.10501976311206818
train_iter_loss: 0.11702502518892288
train_iter_loss: 0.13119778037071228
train_iter_loss: 0.11813076585531235
train_iter_loss: 0.2402256429195404
train_iter_loss: 0.20221959054470062
train_iter_loss: 0.15623348951339722
train_iter_loss: 0.09280546009540558
train_iter_loss: 0.17029552161693573
train_iter_loss: 0.11830310523509979
train_iter_loss: 0.08209497481584549
train_iter_loss: 0.18788085877895355
train_iter_loss: 0.23933373391628265
train_iter_loss: 0.15290102362632751
train_iter_loss: 0.14475907385349274
train_iter_loss: 0.29223695397377014
train_iter_loss: 0.12026304751634598
train_iter_loss: 0.17103947699069977
train_iter_loss: 0.2518467307090759
train_iter_loss: 0.1418001353740692
train_iter_loss: 0.07423213124275208
train_iter_loss: 0.22462444007396698
train_iter_loss: 0.15581145882606506
train_iter_loss: 0.18229860067367554
train_iter_loss: 0.11896543949842453
train_iter_loss: 0.20967209339141846
train_iter_loss: 0.188493549823761
train_iter_loss: 0.1295051872730255
train_iter_loss: 0.19878610968589783
train_iter_loss: 0.09542183578014374
train_iter_loss: 0.14642414450645447
train_iter_loss: 0.09784422069787979
train_iter_loss: 0.08210613578557968
train_iter_loss: 0.10716734826564789
train_iter_loss: 0.20414727926254272
train_iter_loss: 0.0936364009976387
train_iter_loss: 0.22104719281196594
train_iter_loss: 0.21381299197673798
train_iter_loss: 0.08539678901433945
train_iter_loss: 0.15231479704380035
train_iter_loss: 0.10362788289785385
train_iter_loss: 0.08801031857728958
train_iter_loss: 0.14153924584388733
train_iter_loss: 0.22576066851615906
train_iter_loss: 0.09989363700151443
train_iter_loss: 0.2173648327589035
train_iter_loss: 0.07475349307060242
train_iter_loss: 0.3290208578109741
train_iter_loss: 0.15166817605495453
train_iter_loss: 0.2349620908498764
train_iter_loss: 0.10304377973079681
train_iter_loss: 0.07952360808849335
train_iter_loss: 0.1453692764043808
train_iter_loss: 0.27447912096977234
train_iter_loss: 0.1883605271577835
train_iter_loss: 0.129084512591362
train_iter_loss: 0.19207508862018585
train loss :0.1576
---------------------
Validation seg loss: 0.22111878576600608 at epoch 329
epoch =    330/  1000, exp = train
train_iter_loss: 0.17909105122089386
train_iter_loss: 0.05772469565272331
train_iter_loss: 0.16420158743858337
train_iter_loss: 0.23761847615242004
train_iter_loss: 0.203349769115448
train_iter_loss: 0.20699003338813782
train_iter_loss: 0.23262879252433777
train_iter_loss: 0.2979765832424164
train_iter_loss: 0.14010080695152283
train_iter_loss: 0.11879661679267883
train_iter_loss: 0.11467202007770538
train_iter_loss: 0.2052905559539795
train_iter_loss: 0.10938555002212524
train_iter_loss: 0.23806969821453094
train_iter_loss: 0.11281518638134003
train_iter_loss: 0.3503260910511017
train_iter_loss: 0.11870475858449936
train_iter_loss: 0.22301143407821655
train_iter_loss: 0.30457785725593567
train_iter_loss: 0.10299035906791687
train_iter_loss: 0.20617817342281342
train_iter_loss: 0.05446580797433853
train_iter_loss: 0.29091259837150574
train_iter_loss: 0.32332420349121094
train_iter_loss: 0.3250468969345093
train_iter_loss: 0.2005457878112793
train_iter_loss: 0.3268783986568451
train_iter_loss: 0.23037119209766388
train_iter_loss: 0.21411177515983582
train_iter_loss: 0.13940048217773438
train_iter_loss: 0.2342785745859146
train_iter_loss: 0.2827857434749603
train_iter_loss: 0.11493826657533646
train_iter_loss: 0.13607901334762573
train_iter_loss: 0.09721765667200089
train_iter_loss: 0.09981194138526917
train_iter_loss: 0.35567426681518555
train_iter_loss: 0.03925299644470215
train_iter_loss: 0.1562916785478592
train_iter_loss: 0.12607866525650024
train_iter_loss: 0.11194412410259247
train_iter_loss: 0.07461075484752655
train_iter_loss: 0.17116650938987732
train_iter_loss: 0.1371995359659195
train_iter_loss: 0.19600695371627808
train_iter_loss: 0.11014120280742645
train_iter_loss: 0.22208450734615326
train_iter_loss: 0.39705872535705566
train_iter_loss: 0.16301962733268738
train_iter_loss: 0.15874265134334564
train_iter_loss: 0.09577590972185135
train_iter_loss: 0.12586508691310883
train_iter_loss: 0.1973261833190918
train_iter_loss: 0.13752152025699615
train_iter_loss: 0.13456939160823822
train_iter_loss: 0.1260441094636917
train_iter_loss: 0.2849981188774109
train_iter_loss: 0.22153915464878082
train_iter_loss: 0.17884080111980438
train_iter_loss: 0.0753265991806984
train_iter_loss: 0.15201227366924286
train_iter_loss: 0.1310698240995407
train_iter_loss: 0.2258184850215912
train_iter_loss: 0.14560545980930328
train_iter_loss: 0.23872177302837372
train_iter_loss: 0.04822495952248573
train_iter_loss: 0.14725317060947418
train_iter_loss: 0.22130490839481354
train_iter_loss: 0.1957857459783554
train_iter_loss: 0.11357751488685608
train_iter_loss: 0.2232017070055008
train_iter_loss: 0.19391748309135437
train_iter_loss: 0.20952817797660828
train_iter_loss: 0.15620775520801544
train_iter_loss: 0.24627500772476196
train_iter_loss: 0.12554068863391876
train_iter_loss: 0.21032044291496277
train_iter_loss: 0.15067578852176666
train_iter_loss: 0.2153918743133545
train_iter_loss: 0.15418937802314758
train_iter_loss: 0.08492039889097214
train_iter_loss: 0.1041237935423851
train_iter_loss: 0.1951286494731903
train_iter_loss: 0.07994949817657471
train_iter_loss: 0.09654505550861359
train_iter_loss: 0.151661679148674
train_iter_loss: 0.26875248551368713
train_iter_loss: 0.18112827837467194
train_iter_loss: 0.1293325275182724
train_iter_loss: 0.1874675303697586
train_iter_loss: 0.1777661144733429
train_iter_loss: 0.11110855638980865
train_iter_loss: 0.13470806181430817
train_iter_loss: 0.1650141179561615
train_iter_loss: 0.12263714522123337
train_iter_loss: 0.1345715969800949
train_iter_loss: 0.13378344476222992
train_iter_loss: 0.15145866572856903
train_iter_loss: 0.035114288330078125
train_iter_loss: 0.28709739446640015
train loss :0.1759
---------------------
Validation seg loss: 0.220819935404678 at epoch 330
epoch =    331/  1000, exp = train
train_iter_loss: 0.10903503000736237
train_iter_loss: 0.13102349638938904
train_iter_loss: 0.16474874317646027
train_iter_loss: 0.18685917556285858
train_iter_loss: 0.1656353920698166
train_iter_loss: 0.12392549216747284
train_iter_loss: 0.12437072396278381
train_iter_loss: 0.18397696316242218
train_iter_loss: 0.14785152673721313
train_iter_loss: 0.02588776685297489
train_iter_loss: 0.30747848749160767
train_iter_loss: 0.11006903648376465
train_iter_loss: 0.18244819343090057
train_iter_loss: 0.1914152055978775
train_iter_loss: 0.1371036022901535
train_iter_loss: 0.20547142624855042
train_iter_loss: 0.2587504982948303
train_iter_loss: 0.15311726927757263
train_iter_loss: 0.24138143658638
train_iter_loss: 0.13791817426681519
train_iter_loss: 0.08321607112884521
train_iter_loss: 0.17120245099067688
train_iter_loss: 0.12662871181964874
train_iter_loss: 0.12891770899295807
train_iter_loss: 0.15091285109519958
train_iter_loss: 0.1802661418914795
train_iter_loss: 0.028072291985154152
train_iter_loss: 0.18806137144565582
train_iter_loss: 0.15008890628814697
train_iter_loss: 0.16614294052124023
train_iter_loss: 0.1315971314907074
train_iter_loss: 0.17669695615768433
train_iter_loss: 0.10724841058254242
train_iter_loss: 0.2884792387485504
train_iter_loss: 0.12007975578308105
train_iter_loss: 0.20057882368564606
train_iter_loss: 0.1303826868534088
train_iter_loss: 0.17870227992534637
train_iter_loss: 0.3615102171897888
train_iter_loss: 0.18983137607574463
train_iter_loss: 0.16241081058979034
train_iter_loss: 0.1915850043296814
train_iter_loss: 0.1831969916820526
train_iter_loss: 0.15432016551494598
train_iter_loss: 0.1530187875032425
train_iter_loss: 0.16446532309055328
train_iter_loss: 0.17839112877845764
train_iter_loss: 0.21979258954524994
train_iter_loss: 0.21946807205677032
train_iter_loss: 0.158968985080719
train_iter_loss: 0.14006949961185455
train_iter_loss: 0.1561170518398285
train_iter_loss: 0.1346871703863144
train_iter_loss: 0.12733770906925201
train_iter_loss: 0.13245193660259247
train_iter_loss: 0.1561136692762375
train_iter_loss: 0.07478055357933044
train_iter_loss: 0.18212705850601196
train_iter_loss: 0.22157995402812958
train_iter_loss: 0.26198670268058777
train_iter_loss: 0.17525556683540344
train_iter_loss: 0.1932850033044815
train_iter_loss: 0.04999922588467598
train_iter_loss: 0.162235826253891
train_iter_loss: 0.20790071785449982
train_iter_loss: 0.18114636838436127
train_iter_loss: 0.1393875926733017
train_iter_loss: 0.17835399508476257
train_iter_loss: 0.2455282360315323
train_iter_loss: 0.3365851044654846
train_iter_loss: 0.06355460733175278
train_iter_loss: 0.15845489501953125
train_iter_loss: 0.16158658266067505
train_iter_loss: 0.36496788263320923
train_iter_loss: 0.21968068182468414
train_iter_loss: 0.16394846141338348
train_iter_loss: 0.300507515668869
train_iter_loss: 0.2532065510749817
train_iter_loss: 0.1830834448337555
train_iter_loss: 0.05557484179735184
train_iter_loss: 0.13471028208732605
train_iter_loss: 0.3131299316883087
train_iter_loss: 0.29671579599380493
train_iter_loss: 0.16990172863006592
train_iter_loss: 0.12937834858894348
train_iter_loss: 0.10512278974056244
train_iter_loss: 0.10196562856435776
train_iter_loss: 0.19007742404937744
train_iter_loss: 0.3038114905357361
train_iter_loss: 0.1200433149933815
train_iter_loss: 0.17478790879249573
train_iter_loss: 0.15401478111743927
train_iter_loss: 0.20315390825271606
train_iter_loss: 0.10490118712186813
train_iter_loss: 0.10772506147623062
train_iter_loss: 0.09799815714359283
train_iter_loss: 0.15734222531318665
train_iter_loss: 0.2109198272228241
train_iter_loss: 0.1279183328151703
train_iter_loss: 0.15043605864048004
train loss :0.1724
---------------------
Validation seg loss: 0.21666096584906555 at epoch 331
epoch =    332/  1000, exp = train
train_iter_loss: 0.17199401557445526
train_iter_loss: 0.21988044679164886
train_iter_loss: 0.15652833878993988
train_iter_loss: 0.3098217248916626
train_iter_loss: 0.17703533172607422
train_iter_loss: 0.13406714797019958
train_iter_loss: 0.1626056730747223
train_iter_loss: 0.274187296628952
train_iter_loss: 0.15739858150482178
train_iter_loss: 0.1552002876996994
train_iter_loss: 0.12553414702415466
train_iter_loss: 0.09844576567411423
train_iter_loss: 0.15888838469982147
train_iter_loss: 0.13539955019950867
train_iter_loss: 0.24644796550273895
train_iter_loss: 0.20297054946422577
train_iter_loss: 0.16666650772094727
train_iter_loss: 0.393344521522522
train_iter_loss: 0.22203895449638367
train_iter_loss: 0.16285671293735504
train_iter_loss: 0.2586415410041809
train_iter_loss: 0.15913638472557068
train_iter_loss: 0.1056736633181572
train_iter_loss: 0.11902904510498047
train_iter_loss: 0.1750922054052353
train_iter_loss: 0.19387982785701752
train_iter_loss: 0.14561647176742554
train_iter_loss: 0.14135712385177612
train_iter_loss: 0.08866953104734421
train_iter_loss: 0.0474301278591156
train_iter_loss: 0.2700164020061493
train_iter_loss: 0.09134052693843842
train_iter_loss: 0.19723159074783325
train_iter_loss: 0.11738785356283188
train_iter_loss: 0.10297752916812897
train_iter_loss: 0.11428824067115784
train_iter_loss: 0.1096716895699501
train_iter_loss: 0.09685064852237701
train_iter_loss: 0.10001412779092789
train_iter_loss: 0.18470528721809387
train_iter_loss: 0.14131517708301544
train_iter_loss: 0.1985296905040741
train_iter_loss: 0.27651602029800415
train_iter_loss: 0.13437998294830322
train_iter_loss: 0.16070783138275146
train_iter_loss: 0.17164888978004456
train_iter_loss: 0.17244280874729156
train_iter_loss: 0.3807435929775238
train_iter_loss: 0.19824720919132233
train_iter_loss: 0.15092767775058746
train_iter_loss: 0.18573395907878876
train_iter_loss: 0.10366031527519226
train_iter_loss: 0.15173818171024323
train_iter_loss: 0.15759184956550598
train_iter_loss: 0.12711909413337708
train_iter_loss: 0.13521507382392883
train_iter_loss: 0.4349261522293091
train_iter_loss: 0.07401139289140701
train_iter_loss: 0.06985515356063843
train_iter_loss: 0.07159481197595596
train_iter_loss: 0.2302704155445099
train_iter_loss: 0.45998045802116394
train_iter_loss: 0.09245751053094864
train_iter_loss: 0.22992311418056488
train_iter_loss: 0.27658501267433167
train_iter_loss: 0.1613428145647049
train_iter_loss: 0.10642353445291519
train_iter_loss: 0.20738352835178375
train_iter_loss: 0.14751338958740234
train_iter_loss: 0.10575864464044571
train_iter_loss: 0.2679932117462158
train_iter_loss: 0.10640320926904678
train_iter_loss: 0.061682406812906265
train_iter_loss: 0.08617620170116425
train_iter_loss: 0.03923821821808815
train_iter_loss: 0.18764592707157135
train_iter_loss: 0.23060466349124908
train_iter_loss: 0.2626924216747284
train_iter_loss: 0.16307763755321503
train_iter_loss: 0.07107215374708176
train_iter_loss: 0.4003065228462219
train_iter_loss: 0.14073356986045837
train_iter_loss: 0.20558927953243256
train_iter_loss: 0.1502530723810196
train_iter_loss: 0.1407158076763153
train_iter_loss: 0.17162804305553436
train_iter_loss: 0.12973210215568542
train_iter_loss: 0.2253437042236328
train_iter_loss: 0.19572274386882782
train_iter_loss: 0.04075019434094429
train_iter_loss: 0.11358945816755295
train_iter_loss: 0.18818983435630798
train_iter_loss: 0.11208929866552353
train_iter_loss: 0.15260010957717896
train_iter_loss: 0.18637032806873322
train_iter_loss: 0.16032502055168152
train_iter_loss: 0.08429253846406937
train_iter_loss: 0.17181357741355896
train_iter_loss: 0.12028306722640991
train_iter_loss: 0.3602783679962158
train loss :0.1726
---------------------
Validation seg loss: 0.22063399231026196 at epoch 332
epoch =    333/  1000, exp = train
train_iter_loss: 0.08911547809839249
train_iter_loss: 0.09182622283697128
train_iter_loss: 0.0833999514579773
train_iter_loss: 0.12841354310512543
train_iter_loss: 0.21562917530536652
train_iter_loss: 0.0715794637799263
train_iter_loss: 0.18869158625602722
train_iter_loss: 0.08427147567272186
train_iter_loss: 0.1522568166255951
train_iter_loss: 0.11827893555164337
train_iter_loss: 0.15993930399417877
train_iter_loss: 0.18073472380638123
train_iter_loss: 0.23564766347408295
train_iter_loss: 0.10178907215595245
train_iter_loss: 0.2640315890312195
train_iter_loss: 0.1322544366121292
train_iter_loss: 0.10403705388307571
train_iter_loss: 0.22008302807807922
train_iter_loss: 0.06352490931749344
train_iter_loss: 0.18845942616462708
train_iter_loss: 0.15068377554416656
train_iter_loss: 0.22044560313224792
train_iter_loss: 0.2923870086669922
train_iter_loss: 0.17689211666584015
train_iter_loss: 0.16031749546527863
train_iter_loss: 0.1300256848335266
train_iter_loss: 0.19710060954093933
train_iter_loss: 0.27362337708473206
train_iter_loss: 0.2432539165019989
train_iter_loss: 0.10176217555999756
train_iter_loss: 0.20438751578330994
train_iter_loss: 0.20918528735637665
train_iter_loss: 0.14455845952033997
train_iter_loss: 0.08845676481723785
train_iter_loss: 0.04565408080816269
train_iter_loss: 0.09294546395540237
train_iter_loss: 0.08383107930421829
train_iter_loss: 0.1304405927658081
train_iter_loss: 0.17824240028858185
train_iter_loss: 0.1669941395521164
train_iter_loss: 0.3315964639186859
train_iter_loss: 0.11765875667333603
train_iter_loss: 0.19758352637290955
train_iter_loss: 0.2134697288274765
train_iter_loss: 0.15077152848243713
train_iter_loss: 0.12834475934505463
train_iter_loss: 0.14526955783367157
train_iter_loss: 0.3400169610977173
train_iter_loss: 0.17976942658424377
train_iter_loss: 0.18245619535446167
train_iter_loss: 0.12638486921787262
train_iter_loss: 0.16029003262519836
train_iter_loss: 0.26647529006004333
train_iter_loss: 0.17304225265979767
train_iter_loss: 0.13568466901779175
train_iter_loss: 0.17471130192279816
train_iter_loss: 0.15508265793323517
train_iter_loss: 0.12642507255077362
train_iter_loss: 0.0967114046216011
train_iter_loss: 0.09830058366060257
train_iter_loss: 0.12102071940898895
train_iter_loss: 0.13326433300971985
train_iter_loss: 0.17262259125709534
train_iter_loss: 0.14083944261074066
train_iter_loss: 0.15140827000141144
train_iter_loss: 0.14744015038013458
train_iter_loss: 0.15800052881240845
train_iter_loss: 0.13923045992851257
train_iter_loss: 0.18762224912643433
train_iter_loss: 0.20507274568080902
train_iter_loss: 0.1695442944765091
train_iter_loss: 0.20745623111724854
train_iter_loss: 0.23968574404716492
train_iter_loss: 0.12334805727005005
train_iter_loss: 0.10494200140237808
train_iter_loss: 0.12208368629217148
train_iter_loss: 0.2627788484096527
train_iter_loss: 0.14143651723861694
train_iter_loss: 0.2545680105686188
train_iter_loss: 0.19424428045749664
train_iter_loss: 0.10877290368080139
train_iter_loss: 0.0996556356549263
train_iter_loss: 0.21784555912017822
train_iter_loss: 0.06400395929813385
train_iter_loss: 0.09024568647146225
train_iter_loss: 0.20226332545280457
train_iter_loss: 0.13165637850761414
train_iter_loss: 0.11372479051351547
train_iter_loss: 0.15987418591976166
train_iter_loss: 0.07905277609825134
train_iter_loss: 0.1573324203491211
train_iter_loss: 0.09527700394392014
train_iter_loss: 0.16934575140476227
train_iter_loss: 0.17658939957618713
train_iter_loss: 0.1580933928489685
train_iter_loss: 0.23696094751358032
train_iter_loss: 0.26843416690826416
train_iter_loss: 0.20132885873317719
train_iter_loss: 0.10731102526187897
train_iter_loss: 0.30835556983947754
train loss :0.1636
---------------------
Validation seg loss: 0.22233987415982587 at epoch 333
epoch =    334/  1000, exp = train
train_iter_loss: 0.12002427875995636
train_iter_loss: 0.21344606578350067
train_iter_loss: 0.09222713857889175
train_iter_loss: 0.2120264768600464
train_iter_loss: 0.21280813217163086
train_iter_loss: 0.31935039162635803
train_iter_loss: 0.17369404435157776
train_iter_loss: 0.23191316425800323
train_iter_loss: 0.15728026628494263
train_iter_loss: 0.13336291909217834
train_iter_loss: 0.14742618799209595
train_iter_loss: 0.25022634863853455
train_iter_loss: 0.1478380560874939
train_iter_loss: 0.2642260491847992
train_iter_loss: 0.13863150775432587
train_iter_loss: 0.07881567627191544
train_iter_loss: 0.2243126779794693
train_iter_loss: 0.17395490407943726
train_iter_loss: 0.23008766770362854
train_iter_loss: 0.12444961071014404
train_iter_loss: 0.15106837451457977
train_iter_loss: 0.17387755215168
train_iter_loss: 0.09604978561401367
train_iter_loss: 0.14407584071159363
train_iter_loss: 0.11050624400377274
train_iter_loss: 0.1762956827878952
train_iter_loss: 0.1035892441868782
train_iter_loss: 0.1433103084564209
train_iter_loss: 0.17523279786109924
train_iter_loss: 0.2094612717628479
train_iter_loss: 0.09109137952327728
train_iter_loss: 0.09888561069965363
train_iter_loss: 0.2982427179813385
train_iter_loss: 0.12218887358903885
train_iter_loss: 0.21684050559997559
train_iter_loss: 0.18235479295253754
train_iter_loss: 0.09297017008066177
train_iter_loss: 0.12823393940925598
train_iter_loss: 0.2063731551170349
train_iter_loss: 0.21807673573493958
train_iter_loss: 0.12897323071956635
train_iter_loss: 0.1489030122756958
train_iter_loss: 0.19528910517692566
train_iter_loss: 0.1529010534286499
train_iter_loss: 0.10801148414611816
train_iter_loss: 0.13741986453533173
train_iter_loss: 0.21087037026882172
train_iter_loss: 0.2065805345773697
train_iter_loss: 0.12459404021501541
train_iter_loss: 0.09935279935598373
train_iter_loss: 0.13823603093624115
train_iter_loss: 0.1074564978480339
train_iter_loss: 0.15178950130939484
train_iter_loss: 0.12874609231948853
train_iter_loss: 0.057417482137680054
train_iter_loss: 0.16415278613567352
train_iter_loss: 0.11223962157964706
train_iter_loss: 0.14684009552001953
train_iter_loss: 0.13955999910831451
train_iter_loss: 0.10584660619497299
train_iter_loss: 0.22803539037704468
train_iter_loss: 0.3139740824699402
train_iter_loss: 0.08835700154304504
train_iter_loss: 0.1458253115415573
train_iter_loss: 0.2271442413330078
train_iter_loss: 0.12945078313350677
train_iter_loss: 0.16130155324935913
train_iter_loss: 0.3000412583351135
train_iter_loss: 0.21481935679912567
train_iter_loss: 0.08299918472766876
train_iter_loss: 0.13455361127853394
train_iter_loss: 0.21182332932949066
train_iter_loss: 0.1354924589395523
train_iter_loss: 0.11917958408594131
train_iter_loss: 0.1493540108203888
train_iter_loss: 0.10597523301839828
train_iter_loss: 0.10326716303825378
train_iter_loss: 0.14768852293491364
train_iter_loss: 0.0484735481441021
train_iter_loss: 0.21187132596969604
train_iter_loss: 0.27816241979599
train_iter_loss: 0.20077556371688843
train_iter_loss: 0.22230739891529083
train_iter_loss: 0.19722895324230194
train_iter_loss: 0.19372375309467316
train_iter_loss: 0.09055863320827484
train_iter_loss: 0.1507205069065094
train_iter_loss: 0.1698940247297287
train_iter_loss: 0.21990077197551727
train_iter_loss: 0.165207639336586
train_iter_loss: 0.11830677092075348
train_iter_loss: 0.09449522197246552
train_iter_loss: 0.18318715691566467
train_iter_loss: 0.12406697869300842
train_iter_loss: 0.11389632523059845
train_iter_loss: 0.1301463544368744
train_iter_loss: 0.14898963272571564
train_iter_loss: 0.23105093836784363
train_iter_loss: 0.22197870910167694
train_iter_loss: 0.23290061950683594
train loss :0.1646
---------------------
Validation seg loss: 0.22008287357517853 at epoch 334
epoch =    335/  1000, exp = train
train_iter_loss: 0.12475606799125671
train_iter_loss: 0.24938833713531494
train_iter_loss: 0.10626142472028732
train_iter_loss: 0.3080097436904907
train_iter_loss: 0.2012186348438263
train_iter_loss: 0.11864912509918213
train_iter_loss: 0.13348329067230225
train_iter_loss: 0.13565689325332642
train_iter_loss: 0.10053170472383499
train_iter_loss: 0.20972399413585663
train_iter_loss: 0.14758658409118652
train_iter_loss: 0.1744796484708786
train_iter_loss: 0.21063487231731415
train_iter_loss: 0.11204898357391357
train_iter_loss: 0.12744241952896118
train_iter_loss: 0.10349886864423752
train_iter_loss: 0.09923146665096283
train_iter_loss: 0.1366875171661377
train_iter_loss: 0.090058833360672
train_iter_loss: 0.16922973096370697
train_iter_loss: 0.08676756918430328
train_iter_loss: 0.10011356323957443
train_iter_loss: 0.1786758154630661
train_iter_loss: 0.15499290823936462
train_iter_loss: 0.16460904479026794
train_iter_loss: 0.08246641606092453
train_iter_loss: 0.10393013805150986
train_iter_loss: 0.121904157102108
train_iter_loss: 0.18501850962638855
train_iter_loss: 0.15986326336860657
train_iter_loss: 0.20881982147693634
train_iter_loss: 0.16771608591079712
train_iter_loss: 0.2755051255226135
train_iter_loss: 0.16461753845214844
train_iter_loss: 0.27223828434944153
train_iter_loss: 0.16564682126045227
train_iter_loss: 0.118123859167099
train_iter_loss: 0.14111052453517914
train_iter_loss: 0.12159201502799988
train_iter_loss: 0.09452551603317261
train_iter_loss: 0.15146611630916595
train_iter_loss: 0.18396376073360443
train_iter_loss: 0.15370617806911469
train_iter_loss: 0.12695488333702087
train_iter_loss: 0.07268394529819489
train_iter_loss: 0.06887278705835342
train_iter_loss: 0.17401355504989624
train_iter_loss: 0.16684769093990326
train_iter_loss: 0.25598326325416565
train_iter_loss: 0.17577609419822693
train_iter_loss: 0.30980220437049866
train_iter_loss: 0.0869823470711708
train_iter_loss: 0.11466334760189056
train_iter_loss: 0.1810431331396103
train_iter_loss: 0.14660155773162842
train_iter_loss: 0.1465308666229248
train_iter_loss: 0.11387300491333008
train_iter_loss: 0.23387686908245087
train_iter_loss: 0.11982086300849915
train_iter_loss: 0.15660175681114197
train_iter_loss: 0.1619882732629776
train_iter_loss: 0.13109338283538818
train_iter_loss: 0.1814170926809311
train_iter_loss: 0.25021639466285706
train_iter_loss: 0.24262703955173492
train_iter_loss: 0.09829290211200714
train_iter_loss: 0.16378121078014374
train_iter_loss: 0.18581674993038177
train_iter_loss: 0.17576998472213745
train_iter_loss: 0.11558249592781067
train_iter_loss: 0.07507459074258804
train_iter_loss: 0.14156793057918549
train_iter_loss: 0.12047278881072998
train_iter_loss: 0.09835934638977051
train_iter_loss: 0.15285545587539673
train_iter_loss: 0.14426890015602112
train_iter_loss: 0.16653209924697876
train_iter_loss: 0.16853024065494537
train_iter_loss: 0.23182456195354462
train_iter_loss: 0.1377224326133728
train_iter_loss: 0.18658439815044403
train_iter_loss: 0.2027471661567688
train_iter_loss: 0.1336928755044937
train_iter_loss: 0.106021448969841
train_iter_loss: 0.1471807360649109
train_iter_loss: 0.19295157492160797
train_iter_loss: 0.26923272013664246
train_iter_loss: 0.18675830960273743
train_iter_loss: 0.11183127015829086
train_iter_loss: 0.12124672532081604
train_iter_loss: 0.24892714619636536
train_iter_loss: 0.19669784605503082
train_iter_loss: 0.16289713978767395
train_iter_loss: 0.13950522243976593
train_iter_loss: 0.13746316730976105
train_iter_loss: 0.20283134281635284
train_iter_loss: 0.20696179568767548
train_iter_loss: 0.06672108918428421
train_iter_loss: 0.14365841448307037
train_iter_loss: 0.08176233619451523
train loss :0.1582
---------------------
Validation seg loss: 0.2199104749861191 at epoch 335
epoch =    336/  1000, exp = train
train_iter_loss: 0.1439407467842102
train_iter_loss: 0.180350661277771
train_iter_loss: 0.20951974391937256
train_iter_loss: 0.13364970684051514
train_iter_loss: 0.11407994478940964
train_iter_loss: 0.09991911798715591
train_iter_loss: 0.2664926052093506
train_iter_loss: 0.15323378145694733
train_iter_loss: 0.1953929215669632
train_iter_loss: 0.13209912180900574
train_iter_loss: 0.1615103930234909
train_iter_loss: 0.1695910543203354
train_iter_loss: 0.10675451904535294
train_iter_loss: 0.04683639109134674
train_iter_loss: 0.10195256024599075
train_iter_loss: 0.12945859134197235
train_iter_loss: 0.16010957956314087
train_iter_loss: 0.22321805357933044
train_iter_loss: 0.15550099313259125
train_iter_loss: 0.07457691431045532
train_iter_loss: 0.09405075013637543
train_iter_loss: 0.15288086235523224
train_iter_loss: 0.18077526986598969
train_iter_loss: 0.2670859396457672
train_iter_loss: 0.19005565345287323
train_iter_loss: 0.12693242728710175
train_iter_loss: 0.17612046003341675
train_iter_loss: 0.3096178472042084
train_iter_loss: 0.0960264652967453
train_iter_loss: 0.15047813951969147
train_iter_loss: 0.21864484250545502
train_iter_loss: 0.20668262243270874
train_iter_loss: 0.13062340021133423
train_iter_loss: 0.30383527278900146
train_iter_loss: 0.1705770045518875
train_iter_loss: 0.08020336925983429
train_iter_loss: 0.14528189599514008
train_iter_loss: 0.16104921698570251
train_iter_loss: 0.16311058402061462
train_iter_loss: 0.1735391616821289
train_iter_loss: 0.17859655618667603
train_iter_loss: 0.12012221664190292
train_iter_loss: 0.17717920243740082
train_iter_loss: 0.1461586356163025
train_iter_loss: 0.12985393404960632
train_iter_loss: 0.060699980705976486
train_iter_loss: 0.15433369576931
train_iter_loss: 0.14060239493846893
train_iter_loss: 0.12163612991571426
train_iter_loss: 0.2157210409641266
train_iter_loss: 0.20774954557418823
train_iter_loss: 0.1438734531402588
train_iter_loss: 0.096407949924469
train_iter_loss: 0.13323569297790527
train_iter_loss: 0.2329881191253662
train_iter_loss: 0.06514860689640045
train_iter_loss: 0.15359969437122345
train_iter_loss: 0.1109110563993454
train_iter_loss: 0.14538685977458954
train_iter_loss: 0.23341041803359985
train_iter_loss: 0.10810422152280807
train_iter_loss: 0.2077452540397644
train_iter_loss: 0.09046851843595505
train_iter_loss: 0.1770578920841217
train_iter_loss: 0.42311665415763855
train_iter_loss: 0.23229730129241943
train_iter_loss: 0.19043825566768646
train_iter_loss: 0.1706053912639618
train_iter_loss: 0.06181991100311279
train_iter_loss: 0.13653172552585602
train_iter_loss: 0.14478546380996704
train_iter_loss: 0.24272523820400238
train_iter_loss: 0.17941240966320038
train_iter_loss: 0.24753381311893463
train_iter_loss: 0.16979826986789703
train_iter_loss: 0.23146116733551025
train_iter_loss: 0.08958105742931366
train_iter_loss: 0.13137488067150116
train_iter_loss: 0.09964954107999802
train_iter_loss: 0.18398478627204895
train_iter_loss: 0.18075567483901978
train_iter_loss: 0.14697983860969543
train_iter_loss: 0.24198974668979645
train_iter_loss: 0.13117481768131256
train_iter_loss: 0.1023842915892601
train_iter_loss: 0.1872360110282898
train_iter_loss: 0.2714398503303528
train_iter_loss: 0.2871477007865906
train_iter_loss: 0.1975059062242508
train_iter_loss: 0.09978118538856506
train_iter_loss: 0.3153989613056183
train_iter_loss: 0.20120151340961456
train_iter_loss: 0.1202540323138237
train_iter_loss: 0.12040696293115616
train_iter_loss: 0.12758256494998932
train_iter_loss: 0.24141249060630798
train_iter_loss: 0.23859798908233643
train_iter_loss: 0.3845449984073639
train_iter_loss: 0.023338008671998978
train_iter_loss: 0.15714673697948456
train loss :0.1688
---------------------
Validation seg loss: 0.22512959072120348 at epoch 336
epoch =    337/  1000, exp = train
train_iter_loss: 0.057249486446380615
train_iter_loss: 0.14406584203243256
train_iter_loss: 0.1648685187101364
train_iter_loss: 0.1911139190196991
train_iter_loss: 0.13773448765277863
train_iter_loss: 0.21343937516212463
train_iter_loss: 0.223630890250206
train_iter_loss: 0.08103279024362564
train_iter_loss: 0.11263403296470642
train_iter_loss: 0.1426827758550644
train_iter_loss: 0.07984726876020432
train_iter_loss: 0.10277049243450165
train_iter_loss: 0.3173028528690338
train_iter_loss: 0.23927590250968933
train_iter_loss: 0.14521214365959167
train_iter_loss: 0.10529591143131256
train_iter_loss: 0.11951029300689697
train_iter_loss: 0.09682928025722504
train_iter_loss: 0.0727192834019661
train_iter_loss: 0.37803876399993896
train_iter_loss: 0.11382757872343063
train_iter_loss: 0.1206197515130043
train_iter_loss: 0.15415696799755096
train_iter_loss: 0.10523626208305359
train_iter_loss: 0.16221939027309418
train_iter_loss: 0.16360841691493988
train_iter_loss: 0.09633339196443558
train_iter_loss: 0.13347218930721283
train_iter_loss: 0.0964166671037674
train_iter_loss: 0.2485072761774063
train_iter_loss: 0.09934774041175842
train_iter_loss: 0.22250239551067352
train_iter_loss: 0.14483480155467987
train_iter_loss: 0.23127783834934235
train_iter_loss: 0.14231757819652557
train_iter_loss: 0.1846667230129242
train_iter_loss: 0.2115485817193985
train_iter_loss: 0.18139220774173737
train_iter_loss: 0.08725208789110184
train_iter_loss: 0.19329200685024261
train_iter_loss: 0.1678863912820816
train_iter_loss: 0.08894474804401398
train_iter_loss: 0.22192469239234924
train_iter_loss: 0.22517335414886475
train_iter_loss: 0.1983008086681366
train_iter_loss: 0.1477217972278595
train_iter_loss: 0.1840617060661316
train_iter_loss: 0.15173515677452087
train_iter_loss: 0.15912754833698273
train_iter_loss: 0.19663818180561066
train_iter_loss: 0.15539224445819855
train_iter_loss: 0.157582089304924
train_iter_loss: 0.09679479897022247
train_iter_loss: 0.14985434710979462
train_iter_loss: 0.156915083527565
train_iter_loss: 0.26153022050857544
train_iter_loss: 0.18850955367088318
train_iter_loss: 0.12457332760095596
train_iter_loss: 0.21533724665641785
train_iter_loss: 0.12405624985694885
train_iter_loss: 0.13395175337791443
train_iter_loss: 0.11314716190099716
train_iter_loss: 0.20236968994140625
train_iter_loss: 0.14571434259414673
train_iter_loss: 0.25252804160118103
train_iter_loss: 0.1968177855014801
train_iter_loss: 0.10313622653484344
train_iter_loss: 0.15641382336616516
train_iter_loss: 0.25462040305137634
train_iter_loss: 0.22270067036151886
train_iter_loss: 0.11953675001859665
train_iter_loss: 0.19791468977928162
train_iter_loss: 0.2317940592765808
train_iter_loss: 0.17268772423267365
train_iter_loss: 0.13332237303256989
train_iter_loss: 0.1987784057855606
train_iter_loss: 0.11892320960760117
train_iter_loss: 0.11507251113653183
train_iter_loss: 0.1712983399629593
train_iter_loss: 0.20173513889312744
train_iter_loss: 0.10419036448001862
train_iter_loss: 0.12233269959688187
train_iter_loss: 0.13675624132156372
train_iter_loss: 0.16512790322303772
train_iter_loss: 0.11335626244544983
train_iter_loss: 0.19550080597400665
train_iter_loss: 0.13147729635238647
train_iter_loss: 0.27395719289779663
train_iter_loss: 0.32370713353157043
train_iter_loss: 0.13520847260951996
train_iter_loss: 0.20884668827056885
train_iter_loss: 0.07543472945690155
train_iter_loss: 0.1981745958328247
train_iter_loss: 0.09898054599761963
train_iter_loss: 0.19170238077640533
train_iter_loss: 0.10338644683361053
train_iter_loss: 0.17882537841796875
train_iter_loss: 0.0905652567744255
train_iter_loss: 0.16351747512817383
train_iter_loss: 0.11338800191879272
train loss :0.1633
---------------------
Validation seg loss: 0.2214368800105492 at epoch 337
epoch =    338/  1000, exp = train
train_iter_loss: 0.16536615788936615
train_iter_loss: 0.10577177256345749
train_iter_loss: 0.13973242044448853
train_iter_loss: 0.1215939149260521
train_iter_loss: 0.1593155711889267
train_iter_loss: 0.11981561779975891
train_iter_loss: 0.12095166742801666
train_iter_loss: 0.13580971956253052
train_iter_loss: 0.10492338985204697
train_iter_loss: 0.06795796006917953
train_iter_loss: 0.39089423418045044
train_iter_loss: 0.15015830099582672
train_iter_loss: 0.14691324532032013
train_iter_loss: 0.08578760176897049
train_iter_loss: 0.10576143115758896
train_iter_loss: 0.2478746622800827
train_iter_loss: 0.13429807126522064
train_iter_loss: 0.25958406925201416
train_iter_loss: 0.17271356284618378
train_iter_loss: 0.2216344028711319
train_iter_loss: 0.16397301852703094
train_iter_loss: 0.17885221540927887
train_iter_loss: 0.14205051958560944
train_iter_loss: 0.20443950593471527
train_iter_loss: 0.1469966471195221
train_iter_loss: 0.24486702680587769
train_iter_loss: 0.18018291890621185
train_iter_loss: 0.17091982066631317
train_iter_loss: 0.1061459556221962
train_iter_loss: 0.12928970158100128
train_iter_loss: 0.18017208576202393
train_iter_loss: 0.09725445508956909
train_iter_loss: 0.25068411231040955
train_iter_loss: 0.20254462957382202
train_iter_loss: 0.13794772326946259
train_iter_loss: 0.08788204938173294
train_iter_loss: 0.17753922939300537
train_iter_loss: 0.257409006357193
train_iter_loss: 0.14974993467330933
train_iter_loss: 0.14456622302532196
train_iter_loss: 0.10027517378330231
train_iter_loss: 0.09444818645715714
train_iter_loss: 0.15325014293193817
train_iter_loss: 0.23164620995521545
train_iter_loss: 0.09786958247423172
train_iter_loss: 0.19901718199253082
train_iter_loss: 0.1444956511259079
train_iter_loss: 0.365025132894516
train_iter_loss: 0.14712150394916534
train_iter_loss: 0.1516859382390976
train_iter_loss: 0.13138987123966217
train_iter_loss: 0.20330408215522766
train_iter_loss: 0.11950439214706421
train_iter_loss: 0.19534383714199066
train_iter_loss: 0.12831604480743408
train_iter_loss: 0.12435857206583023
train_iter_loss: 0.4512922465801239
train_iter_loss: 0.2053849697113037
train_iter_loss: 0.17212636768817902
train_iter_loss: 0.2827489674091339
train_iter_loss: 0.2998085916042328
train_iter_loss: 0.18021127581596375
train_iter_loss: 0.17810861766338348
train_iter_loss: 0.06125506013631821
train_iter_loss: 0.11755118519067764
train_iter_loss: 0.13443899154663086
train_iter_loss: 0.23066355288028717
train_iter_loss: 0.18477964401245117
train_iter_loss: 0.1819721758365631
train_iter_loss: 0.2455190271139145
train_iter_loss: 0.06796706467866898
train_iter_loss: 0.10962630063295364
train_iter_loss: 0.09550197422504425
train_iter_loss: 0.17384597659111023
train_iter_loss: 0.14430761337280273
train_iter_loss: 0.15555617213249207
train_iter_loss: 0.1919415444135666
train_iter_loss: 0.1837078332901001
train_iter_loss: 0.24577997624874115
train_iter_loss: 0.18439948558807373
train_iter_loss: 0.09650260955095291
train_iter_loss: 0.10145168751478195
train_iter_loss: 0.2026052325963974
train_iter_loss: 0.13871482014656067
train_iter_loss: 0.14582562446594238
train_iter_loss: 0.12842288613319397
train_iter_loss: 0.10401712357997894
train_iter_loss: 0.0982164815068245
train_iter_loss: 0.12725776433944702
train_iter_loss: 0.19532765448093414
train_iter_loss: 0.08649531751871109
train_iter_loss: 0.11148733645677567
train_iter_loss: 0.12119822949171066
train_iter_loss: 0.08453022688627243
train_iter_loss: 0.1808636635541916
train_iter_loss: 0.16458982229232788
train_iter_loss: 0.16105319559574127
train_iter_loss: 0.2418334037065506
train_iter_loss: 0.16370812058448792
train_iter_loss: 0.10571451485157013
train loss :0.1650
---------------------
Validation seg loss: 0.22108772486659153 at epoch 338
epoch =    339/  1000, exp = train
train_iter_loss: 0.2245892882347107
train_iter_loss: 0.15481415390968323
train_iter_loss: 0.18514426052570343
train_iter_loss: 0.1581869274377823
train_iter_loss: 0.19953493773937225
train_iter_loss: 0.18477073311805725
train_iter_loss: 0.23964369297027588
train_iter_loss: 0.1846083402633667
train_iter_loss: 0.08458257466554642
train_iter_loss: 0.12266908586025238
train_iter_loss: 0.08705127239227295
train_iter_loss: 0.17053845524787903
train_iter_loss: 0.12729626893997192
train_iter_loss: 0.219394713640213
train_iter_loss: 0.14048734307289124
train_iter_loss: 0.185151144862175
train_iter_loss: 0.2769659459590912
train_iter_loss: 0.13946537673473358
train_iter_loss: 0.1216798648238182
train_iter_loss: 0.23383986949920654
train_iter_loss: 0.18828076124191284
train_iter_loss: 0.13666094839572906
train_iter_loss: 0.1353415846824646
train_iter_loss: 0.1632474958896637
train_iter_loss: 0.11871352046728134
train_iter_loss: 0.14992043375968933
train_iter_loss: 0.15365424752235413
train_iter_loss: 0.10172954201698303
train_iter_loss: 0.28723326325416565
train_iter_loss: 0.21515242755413055
train_iter_loss: 0.10470987111330032
train_iter_loss: 0.29283425211906433
train_iter_loss: 0.12871745228767395
train_iter_loss: 0.20489950478076935
train_iter_loss: 0.12354355305433273
train_iter_loss: 0.09897226095199585
train_iter_loss: 0.31437134742736816
train_iter_loss: 0.20323972404003143
train_iter_loss: 0.17702439427375793
train_iter_loss: 0.20745638012886047
train_iter_loss: 0.07769718021154404
train_iter_loss: 0.13644616305828094
train_iter_loss: 0.10443244129419327
train_iter_loss: 0.07664749026298523
train_iter_loss: 0.1469329446554184
train_iter_loss: 0.19304950535297394
train_iter_loss: 0.32852044701576233
train_iter_loss: 0.19270600378513336
train_iter_loss: 0.1548299491405487
train_iter_loss: 0.17769315838813782
train_iter_loss: 0.1584886908531189
train_iter_loss: 0.1387174427509308
train_iter_loss: 0.0996067225933075
train_iter_loss: 0.18416950106620789
train_iter_loss: 0.16986824572086334
train_iter_loss: 0.15741056203842163
train_iter_loss: 0.139299213886261
train_iter_loss: 0.11305675655603409
train_iter_loss: 0.0884600281715393
train_iter_loss: 0.05669732391834259
train_iter_loss: 0.14704576134681702
train_iter_loss: 0.137578085064888
train_iter_loss: 0.15149956941604614
train_iter_loss: 0.10089019685983658
train_iter_loss: 0.12071549147367477
train_iter_loss: 0.19318801164627075
train_iter_loss: 0.12448287010192871
train_iter_loss: 0.21484942734241486
train_iter_loss: 0.13656137883663177
train_iter_loss: 0.08521858602762222
train_iter_loss: 0.08419208228588104
train_iter_loss: 0.1731581687927246
train_iter_loss: 0.15542271733283997
train_iter_loss: 0.29596689343452454
train_iter_loss: 0.14906638860702515
train_iter_loss: 0.1705547422170639
train_iter_loss: 0.15996944904327393
train_iter_loss: 0.13608655333518982
train_iter_loss: 0.0999756008386612
train_iter_loss: 0.09524524956941605
train_iter_loss: 0.16001415252685547
train_iter_loss: 0.16069146990776062
train_iter_loss: 0.10972145944833755
train_iter_loss: 0.17164680361747742
train_iter_loss: 0.31173238158226013
train_iter_loss: 0.23074428737163544
train_iter_loss: 0.2013600915670395
train_iter_loss: 0.16286613047122955
train_iter_loss: 0.22565123438835144
train_iter_loss: 0.18652936816215515
train_iter_loss: 0.1320091187953949
train_iter_loss: 0.25541865825653076
train_iter_loss: 0.1715826839208603
train_iter_loss: 0.14718849956989288
train_iter_loss: 0.14851409196853638
train_iter_loss: 0.1931857019662857
train_iter_loss: 0.19513368606567383
train_iter_loss: 0.10799635946750641
train_iter_loss: 0.139690101146698
train_iter_loss: 0.13229013979434967
train loss :0.1648
---------------------
Validation seg loss: 0.21896579291425505 at epoch 339
epoch =    340/  1000, exp = train
train_iter_loss: 0.26328060030937195
train_iter_loss: 0.11040070652961731
train_iter_loss: 0.18780435621738434
train_iter_loss: 0.13233575224876404
train_iter_loss: 0.21416813135147095
train_iter_loss: 0.15847694873809814
train_iter_loss: 0.1265638917684555
train_iter_loss: 0.2416103482246399
train_iter_loss: 0.08174663782119751
train_iter_loss: 0.15970753133296967
train_iter_loss: 0.13957086205482483
train_iter_loss: 0.2638804316520691
train_iter_loss: 0.1608363687992096
train_iter_loss: 0.14260953664779663
train_iter_loss: 0.13905368745326996
train_iter_loss: 0.08160752803087234
train_iter_loss: 0.1434861421585083
train_iter_loss: 0.09403807669878006
train_iter_loss: 0.11968537420034409
train_iter_loss: 0.06221278756856918
train_iter_loss: 0.18079859018325806
train_iter_loss: 0.08304297924041748
train_iter_loss: 0.11712243407964706
train_iter_loss: 0.06601601839065552
train_iter_loss: 0.15099813044071198
train_iter_loss: 0.11566103249788284
train_iter_loss: 0.14926092326641083
train_iter_loss: 0.1379804164171219
train_iter_loss: 0.16274821758270264
train_iter_loss: 0.17051847279071808
train_iter_loss: 0.09157222509384155
train_iter_loss: 0.21385681629180908
train_iter_loss: 0.20324616134166718
train_iter_loss: 0.17434677481651306
train_iter_loss: 0.16251440346240997
train_iter_loss: 0.29662400484085083
train_iter_loss: 0.04416879639029503
train_iter_loss: 0.20162640511989594
train_iter_loss: 0.11486659198999405
train_iter_loss: 0.16953584551811218
train_iter_loss: 0.1304176151752472
train_iter_loss: 0.26199012994766235
train_iter_loss: 0.13132144510746002
train_iter_loss: 0.30780932307243347
train_iter_loss: 0.26589372754096985
train_iter_loss: 0.09235306829214096
train_iter_loss: 0.1782708615064621
train_iter_loss: 0.2660253942012787
train_iter_loss: 0.21792945265769958
train_iter_loss: 0.19126984477043152
train_iter_loss: 0.18984022736549377
train_iter_loss: 0.0713765099644661
train_iter_loss: 0.13913699984550476
train_iter_loss: 0.10721178352832794
train_iter_loss: 0.2250264286994934
train_iter_loss: 0.1742493063211441
train_iter_loss: 0.1382429152727127
train_iter_loss: 0.1936304122209549
train_iter_loss: 0.21597470343112946
train_iter_loss: 0.2588728368282318
train_iter_loss: 0.18776389956474304
train_iter_loss: 0.3464921712875366
train_iter_loss: 0.10144266486167908
train_iter_loss: 0.15377521514892578
train_iter_loss: 0.0752311572432518
train_iter_loss: 0.1309187114238739
train_iter_loss: 0.1845303475856781
train_iter_loss: 0.10874220728874207
train_iter_loss: 0.13165995478630066
train_iter_loss: 0.1510523110628128
train_iter_loss: 0.15495464205741882
train_iter_loss: 0.21110187470912933
train_iter_loss: 0.22820447385311127
train_iter_loss: 0.20670221745967865
train_iter_loss: 0.21108336746692657
train_iter_loss: 0.09430307149887085
train_iter_loss: 0.1875978261232376
train_iter_loss: 0.23804771900177002
train_iter_loss: 0.18098706007003784
train_iter_loss: 0.14357173442840576
train_iter_loss: 0.19061075150966644
train_iter_loss: 0.2521056830883026
train_iter_loss: 0.16764993965625763
train_iter_loss: 0.1517983227968216
train_iter_loss: 0.1660701036453247
train_iter_loss: 0.16078989207744598
train_iter_loss: 0.06316301226615906
train_iter_loss: 0.15611818432807922
train_iter_loss: 0.16186699271202087
train_iter_loss: 0.09178458899259567
train_iter_loss: 0.15143996477127075
train_iter_loss: 0.2463361918926239
train_iter_loss: 0.19310982525348663
train_iter_loss: 0.19466254115104675
train_iter_loss: 0.20508050918579102
train_iter_loss: 0.2927272915840149
train_iter_loss: 0.1425957977771759
train_iter_loss: 0.15057653188705444
train_iter_loss: 0.1394231617450714
train_iter_loss: 0.08205718547105789
train loss :0.1674
---------------------
Validation seg loss: 0.21888255306853438 at epoch 340
epoch =    341/  1000, exp = train
train_iter_loss: 0.11592192947864532
train_iter_loss: 0.38017332553863525
train_iter_loss: 0.1582769900560379
train_iter_loss: 0.07104825228452682
train_iter_loss: 0.20702116191387177
train_iter_loss: 0.1721792072057724
train_iter_loss: 0.06494244188070297
train_iter_loss: 0.1380593478679657
train_iter_loss: 0.13254305720329285
train_iter_loss: 0.2517438530921936
train_iter_loss: 0.020874440670013428
train_iter_loss: 0.2009408175945282
train_iter_loss: 0.07787319272756577
train_iter_loss: 0.1510791927576065
train_iter_loss: 0.11901148408651352
train_iter_loss: 0.12940311431884766
train_iter_loss: 0.20748670399188995
train_iter_loss: 0.16550999879837036
train_iter_loss: 0.16526728868484497
train_iter_loss: 0.09366177022457123
train_iter_loss: 0.23959651589393616
train_iter_loss: 0.2380794882774353
train_iter_loss: 0.13989441096782684
train_iter_loss: 0.20531557500362396
train_iter_loss: 0.15519185364246368
train_iter_loss: 0.16253268718719482
train_iter_loss: 0.2643018066883087
train_iter_loss: 0.1150018498301506
train_iter_loss: 0.15963228046894073
train_iter_loss: 0.2506430447101593
train_iter_loss: 0.1813386082649231
train_iter_loss: 0.09828811138868332
train_iter_loss: 0.1645897477865219
train_iter_loss: 0.17689557373523712
train_iter_loss: 0.20905180275440216
train_iter_loss: 0.18077658116817474
train_iter_loss: 0.24057936668395996
train_iter_loss: 0.1546861082315445
train_iter_loss: 0.2018309235572815
train_iter_loss: 0.21301297843456268
train_iter_loss: 0.12998662889003754
train_iter_loss: 0.09858917444944382
train_iter_loss: 0.08772490918636322
train_iter_loss: 0.2647588849067688
train_iter_loss: 0.1253444254398346
train_iter_loss: 0.13484565913677216
train_iter_loss: 0.09159909188747406
train_iter_loss: 0.07487638294696808
train_iter_loss: 0.12358774989843369
train_iter_loss: 0.26854097843170166
train_iter_loss: 0.18270234763622284
train_iter_loss: 0.27664923667907715
train_iter_loss: 0.1692432314157486
train_iter_loss: 0.15916162729263306
train_iter_loss: 0.09209118038415909
train_iter_loss: 0.11890147626399994
train_iter_loss: 0.1374790370464325
train_iter_loss: 0.1539134532213211
train_iter_loss: 0.1443604975938797
train_iter_loss: 0.10362458974123001
train_iter_loss: 0.1386091113090515
train_iter_loss: 0.12097252160310745
train_iter_loss: 0.09157410264015198
train_iter_loss: 0.21660833060741425
train_iter_loss: 0.17098918557167053
train_iter_loss: 0.14565277099609375
train_iter_loss: 0.24116362631320953
train_iter_loss: 0.12087924778461456
train_iter_loss: 0.19137078523635864
train_iter_loss: 0.06918618083000183
train_iter_loss: 0.16541463136672974
train_iter_loss: 0.14626310765743256
train_iter_loss: 0.1501716524362564
train_iter_loss: 0.36560776829719543
train_iter_loss: 0.11633647233247757
train_iter_loss: 0.14473025500774384
train_iter_loss: 0.18464259803295135
train_iter_loss: 0.13183064758777618
train_iter_loss: 0.1620631217956543
train_iter_loss: 0.1273106038570404
train_iter_loss: 0.10461319983005524
train_iter_loss: 0.27891451120376587
train_iter_loss: 0.3272712528705597
train_iter_loss: 0.042253222316503525
train_iter_loss: 0.11983908712863922
train_iter_loss: 0.17563144862651825
train_iter_loss: 0.11812079697847366
train_iter_loss: 0.1496877670288086
train_iter_loss: 0.2439577579498291
train_iter_loss: 0.1939726620912552
train_iter_loss: 0.1994439661502838
train_iter_loss: 0.13145166635513306
train_iter_loss: 0.15088926255702972
train_iter_loss: 0.24240535497665405
train_iter_loss: 0.2831350564956665
train_iter_loss: 0.2379315197467804
train_iter_loss: 0.20898935198783875
train_iter_loss: 0.20659247040748596
train_iter_loss: 0.11537192016839981
train_iter_loss: 0.18729110062122345
train loss :0.1680
---------------------
Validation seg loss: 0.2228908217775653 at epoch 341
epoch =    342/  1000, exp = train
train_iter_loss: 0.10891196131706238
train_iter_loss: 0.3152152895927429
train_iter_loss: 0.08636989444494247
train_iter_loss: 0.15116003155708313
train_iter_loss: 0.17691558599472046
train_iter_loss: 0.0833858773112297
train_iter_loss: 0.15337827801704407
train_iter_loss: 0.1157611757516861
train_iter_loss: 0.10423533618450165
train_iter_loss: 0.2048182636499405
train_iter_loss: 0.16785888373851776
train_iter_loss: 0.11216522753238678
train_iter_loss: 0.2618519365787506
train_iter_loss: 0.17197617888450623
train_iter_loss: 0.14077381789684296
train_iter_loss: 0.20102272927761078
train_iter_loss: 0.12690921127796173
train_iter_loss: 0.12091018259525299
train_iter_loss: 0.22208064794540405
train_iter_loss: 0.16074256598949432
train_iter_loss: 0.2240644246339798
train_iter_loss: 0.08749182522296906
train_iter_loss: 0.10909850150346756
train_iter_loss: 0.1710108518600464
train_iter_loss: 0.12848973274230957
train_iter_loss: 0.25159376859664917
train_iter_loss: 0.1866617053747177
train_iter_loss: 0.24521255493164062
train_iter_loss: 0.1811354160308838
train_iter_loss: 0.0822126567363739
train_iter_loss: 0.1985381841659546
train_iter_loss: 0.10944068431854248
train_iter_loss: 0.10326697677373886
train_iter_loss: 0.16600602865219116
train_iter_loss: 0.11325723677873611
train_iter_loss: 0.04489089921116829
train_iter_loss: 0.18901172280311584
train_iter_loss: 0.16861116886138916
train_iter_loss: 0.11254270374774933
train_iter_loss: 0.14374825358390808
train_iter_loss: 0.14486080408096313
train_iter_loss: 0.057739268988370895
train_iter_loss: 0.20187494158744812
train_iter_loss: 0.20010776817798615
train_iter_loss: 0.23404940962791443
train_iter_loss: 0.2215414196252823
train_iter_loss: 0.1344880759716034
train_iter_loss: 0.10195427387952805
train_iter_loss: 0.16020932793617249
train_iter_loss: 0.09027963876724243
train_iter_loss: 0.12490493059158325
train_iter_loss: 0.14207753539085388
train_iter_loss: 0.051709599792957306
train_iter_loss: 0.19698341190814972
train_iter_loss: 0.15457966923713684
train_iter_loss: 0.17345117032527924
train_iter_loss: 0.11149179935455322
train_iter_loss: 0.19438232481479645
train_iter_loss: 0.04214528948068619
train_iter_loss: 0.11653543263673782
train_iter_loss: 0.3041665852069855
train_iter_loss: 0.11740799248218536
train_iter_loss: 0.15723633766174316
train_iter_loss: 0.10118178278207779
train_iter_loss: 0.21214553713798523
train_iter_loss: 0.17057615518569946
train_iter_loss: 0.17786158621311188
train_iter_loss: 0.1746387481689453
train_iter_loss: 0.10744363814592361
train_iter_loss: 0.09930819272994995
train_iter_loss: 0.08863457292318344
train_iter_loss: 0.17979688942432404
train_iter_loss: 0.15905022621154785
train_iter_loss: 0.14299045503139496
train_iter_loss: 0.0948689877986908
train_iter_loss: 0.20429615676403046
train_iter_loss: 0.07695291936397552
train_iter_loss: 0.15565861761569977
train_iter_loss: 0.09330379962921143
train_iter_loss: 0.18738022446632385
train_iter_loss: 0.3735457956790924
train_iter_loss: 0.06765808910131454
train_iter_loss: 0.14168183505535126
train_iter_loss: 0.14842118322849274
train_iter_loss: 0.23358091711997986
train_iter_loss: 0.20595701038837433
train_iter_loss: 0.19504468142986298
train_iter_loss: 0.22543904185295105
train_iter_loss: 0.10510250926017761
train_iter_loss: 0.2337692677974701
train_iter_loss: 0.1273852437734604
train_iter_loss: 0.15469177067279816
train_iter_loss: 0.15764576196670532
train_iter_loss: 0.15464042127132416
train_iter_loss: 0.31098130345344543
train_iter_loss: 0.07445066422224045
train_iter_loss: 0.17618432641029358
train_iter_loss: 0.19032351672649384
train_iter_loss: 0.1789250671863556
train_iter_loss: 0.18760241568088531
train loss :0.1587
---------------------
Validation seg loss: 0.21836386444398537 at epoch 342
epoch =    343/  1000, exp = train
train_iter_loss: 0.16934950649738312
train_iter_loss: 0.19904053211212158
train_iter_loss: 0.25603288412094116
train_iter_loss: 0.14385375380516052
train_iter_loss: 0.26000165939331055
train_iter_loss: 0.29600778222084045
train_iter_loss: 0.13894782960414886
train_iter_loss: 0.17700862884521484
train_iter_loss: 0.17786458134651184
train_iter_loss: 0.1257837563753128
train_iter_loss: 0.26349207758903503
train_iter_loss: 0.19013266265392303
train_iter_loss: 0.12135381996631622
train_iter_loss: 0.14924460649490356
train_iter_loss: 0.1721624881029129
train_iter_loss: 0.23827287554740906
train_iter_loss: 0.17616480588912964
train_iter_loss: 0.10316929966211319
train_iter_loss: 0.11116757243871689
train_iter_loss: 0.35440340638160706
train_iter_loss: 0.12282798439264297
train_iter_loss: 0.10932230204343796
train_iter_loss: 0.09419666975736618
train_iter_loss: 0.15647229552268982
train_iter_loss: 0.08663149923086166
train_iter_loss: 0.11489718407392502
train_iter_loss: 0.16782549023628235
train_iter_loss: 0.16828598082065582
train_iter_loss: 0.14020930230617523
train_iter_loss: 0.14792272448539734
train_iter_loss: 0.2645531892776489
train_iter_loss: 0.23703573644161224
train_iter_loss: 0.14100700616836548
train_iter_loss: 0.19733765721321106
train_iter_loss: 0.14503341913223267
train_iter_loss: 0.12128512561321259
train_iter_loss: 0.1766662895679474
train_iter_loss: 0.13587425649166107
train_iter_loss: 0.27503833174705505
train_iter_loss: 0.06141132116317749
train_iter_loss: 0.13378417491912842
train_iter_loss: 0.07055605947971344
train_iter_loss: 0.18236488103866577
train_iter_loss: 0.18833757936954498
train_iter_loss: 0.3454321622848511
train_iter_loss: 0.11196263134479523
train_iter_loss: 0.16586878895759583
train_iter_loss: 0.29632988572120667
train_iter_loss: 0.1720750629901886
train_iter_loss: 0.16287286579608917
train_iter_loss: 0.07627762109041214
train_iter_loss: 0.1737029254436493
train_iter_loss: 0.05703530088067055
train_iter_loss: 0.25758054852485657
train_iter_loss: 0.07100441306829453
train_iter_loss: 0.14045202732086182
train_iter_loss: 0.1860317587852478
train_iter_loss: 0.332670122385025
train_iter_loss: 0.1604333221912384
train_iter_loss: 0.2238006889820099
train_iter_loss: 0.2095634788274765
train_iter_loss: 0.07177497446537018
train_iter_loss: 0.19006194174289703
train_iter_loss: 0.18327946960926056
train_iter_loss: 0.06691356748342514
train_iter_loss: 0.10135994851589203
train_iter_loss: 0.2301301807165146
train_iter_loss: 0.16815024614334106
train_iter_loss: 0.06093750521540642
train_iter_loss: 0.17776815593242645
train_iter_loss: 0.1874540150165558
train_iter_loss: 0.12480021268129349
train_iter_loss: 0.15559367835521698
train_iter_loss: 0.2696148157119751
train_iter_loss: 0.10689122974872589
train_iter_loss: 0.1930629163980484
train_iter_loss: 0.18706941604614258
train_iter_loss: 0.18100427091121674
train_iter_loss: 0.11965984851121902
train_iter_loss: 0.10245377570390701
train_iter_loss: 0.11566958576440811
train_iter_loss: 0.13225491344928741
train_iter_loss: 0.19479802250862122
train_iter_loss: 0.15478511154651642
train_iter_loss: 0.3152218759059906
train_iter_loss: 0.1272856891155243
train_iter_loss: 0.2692411243915558
train_iter_loss: 0.16162028908729553
train_iter_loss: 0.14588505029678345
train_iter_loss: 0.12218142300844193
train_iter_loss: 0.07753957062959671
train_iter_loss: 0.2079685628414154
train_iter_loss: 0.16761325299739838
train_iter_loss: 0.11814063042402267
train_iter_loss: 0.12589964270591736
train_iter_loss: 0.33023974299430847
train_iter_loss: 0.17137540876865387
train_iter_loss: 0.12279714643955231
train_iter_loss: 0.10840373486280441
train_iter_loss: 0.18471522629261017
train loss :0.1700
---------------------
Validation seg loss: 0.2213250134702561 at epoch 343
epoch =    344/  1000, exp = train
train_iter_loss: 0.14311662316322327
train_iter_loss: 0.09983842074871063
train_iter_loss: 0.3195834159851074
train_iter_loss: 0.14620956778526306
train_iter_loss: 0.11199671775102615
train_iter_loss: 0.16370169818401337
train_iter_loss: 0.116569384932518
train_iter_loss: 0.16335178911685944
train_iter_loss: 0.48822951316833496
train_iter_loss: 0.24217833578586578
train_iter_loss: 0.1160263642668724
train_iter_loss: 0.1609630435705185
train_iter_loss: 0.12899243831634521
train_iter_loss: 0.13968326151371002
train_iter_loss: 0.26058682799339294
train_iter_loss: 0.11412060260772705
train_iter_loss: 0.11048562824726105
train_iter_loss: 0.1762688010931015
train_iter_loss: 0.14504307508468628
train_iter_loss: 0.27300891280174255
train_iter_loss: 0.1133161187171936
train_iter_loss: 0.2965180277824402
train_iter_loss: 0.23499739170074463
train_iter_loss: 0.14759759604930878
train_iter_loss: 0.13242647051811218
train_iter_loss: 0.15619346499443054
train_iter_loss: 0.10666180402040482
train_iter_loss: 0.18419986963272095
train_iter_loss: 0.2243957370519638
train_iter_loss: 0.2028687298297882
train_iter_loss: 0.17995689809322357
train_iter_loss: 0.1202322319149971
train_iter_loss: 0.10615913569927216
train_iter_loss: 0.1880275458097458
train_iter_loss: 0.17405670881271362
train_iter_loss: 0.08522295206785202
train_iter_loss: 0.1626061052083969
train_iter_loss: 0.24512708187103271
train_iter_loss: 0.15424731373786926
train_iter_loss: 0.20368699729442596
train_iter_loss: 0.11446616798639297
train_iter_loss: 0.09882836788892746
train_iter_loss: 0.2542588710784912
train_iter_loss: 0.20300190150737762
train_iter_loss: 0.1479281336069107
train_iter_loss: 0.2269541174173355
train_iter_loss: 0.11828608065843582
train_iter_loss: 0.056279148906469345
train_iter_loss: 0.15949194133281708
train_iter_loss: 0.14995700120925903
train_iter_loss: 0.12081290781497955
train_iter_loss: 0.14228810369968414
train_iter_loss: 0.15208134055137634
train_iter_loss: 0.2385229915380478
train_iter_loss: 0.2746543884277344
train_iter_loss: 0.16410233080387115
train_iter_loss: 0.15054844319820404
train_iter_loss: 0.13921727240085602
train_iter_loss: 0.10760865360498428
train_iter_loss: 0.15416428446769714
train_iter_loss: 0.07870683073997498
train_iter_loss: 0.14225202798843384
train_iter_loss: 0.12842975556850433
train_iter_loss: 0.2041967213153839
train_iter_loss: 0.22560730576515198
train_iter_loss: 0.14541621506214142
train_iter_loss: 0.2120649665594101
train_iter_loss: 0.12615570425987244
train_iter_loss: 0.07726945728063583
train_iter_loss: 0.1975049376487732
train_iter_loss: 0.06470038741827011
train_iter_loss: 0.14216142892837524
train_iter_loss: 0.21902015805244446
train_iter_loss: 0.1198575496673584
train_iter_loss: 0.08555512130260468
train_iter_loss: 0.13738112151622772
train_iter_loss: 0.34416744112968445
train_iter_loss: 0.3409785330295563
train_iter_loss: 0.11479505896568298
train_iter_loss: 0.11598041653633118
train_iter_loss: 0.18981975317001343
train_iter_loss: 0.13402877748012543
train_iter_loss: 0.07576525956392288
train_iter_loss: 0.19547535479068756
train_iter_loss: 0.11644309759140015
train_iter_loss: 0.16407686471939087
train_iter_loss: 0.09271006286144257
train_iter_loss: 0.1481212079524994
train_iter_loss: 0.19373278319835663
train_iter_loss: 0.27494531869888306
train_iter_loss: 0.14459632337093353
train_iter_loss: 0.0532868355512619
train_iter_loss: 0.24811527132987976
train_iter_loss: 0.12584015727043152
train_iter_loss: 0.2598409056663513
train_iter_loss: 0.19810208678245544
train_iter_loss: 0.16366097331047058
train_iter_loss: 0.22731152176856995
train_iter_loss: 0.1532505452632904
train_iter_loss: 0.12384029477834702
train loss :0.1689
---------------------
Validation seg loss: 0.219591515375969 at epoch 344
epoch =    345/  1000, exp = train
train_iter_loss: 0.11130277067422867
train_iter_loss: 0.13214141130447388
train_iter_loss: 0.1712365597486496
train_iter_loss: 0.32601678371429443
train_iter_loss: 0.19781477749347687
train_iter_loss: 0.09710220992565155
train_iter_loss: 0.1718125343322754
train_iter_loss: 0.15688340365886688
train_iter_loss: 0.06897170096635818
train_iter_loss: 0.10739128291606903
train_iter_loss: 0.07849906384944916
train_iter_loss: 0.10676132887601852
train_iter_loss: 0.1480053812265396
train_iter_loss: 0.12610754370689392
train_iter_loss: 0.23575803637504578
train_iter_loss: 0.1336873173713684
train_iter_loss: 0.11410535126924515
train_iter_loss: 0.21534863114356995
train_iter_loss: 0.22623154520988464
train_iter_loss: 0.16902883350849152
train_iter_loss: 0.0811547189950943
train_iter_loss: 0.1172761619091034
train_iter_loss: 0.10746601969003677
train_iter_loss: 0.028614869341254234
train_iter_loss: 0.14444349706172943
train_iter_loss: 0.17828476428985596
train_iter_loss: 0.12502184510231018
train_iter_loss: 0.2363375723361969
train_iter_loss: 0.3179568648338318
train_iter_loss: 0.14320990443229675
train_iter_loss: 0.22024139761924744
train_iter_loss: 0.12935392558574677
train_iter_loss: 0.26647889614105225
train_iter_loss: 0.14277075231075287
train_iter_loss: 0.14271371066570282
train_iter_loss: 0.18083344399929047
train_iter_loss: 0.18128712475299835
train_iter_loss: 0.1256949007511139
train_iter_loss: 0.17588412761688232
train_iter_loss: 0.18052086234092712
train_iter_loss: 0.1384849101305008
train_iter_loss: 0.10367840528488159
train_iter_loss: 0.07184888422489166
train_iter_loss: 0.24583713710308075
train_iter_loss: 0.1185169517993927
train_iter_loss: 0.13832902908325195
train_iter_loss: 0.15021131932735443
train_iter_loss: 0.3142644464969635
train_iter_loss: 0.17632687091827393
train_iter_loss: 0.04983732849359512
train_iter_loss: 0.2025991529226303
train_iter_loss: 0.17389267683029175
train_iter_loss: 0.12371309101581573
train_iter_loss: 0.2526567876338959
train_iter_loss: 0.10648014396429062
train_iter_loss: 0.17776864767074585
train_iter_loss: 0.21403975784778595
train_iter_loss: 0.1989719122648239
train_iter_loss: 0.18342797458171844
train_iter_loss: 0.1470283716917038
train_iter_loss: 0.11078683286905289
train_iter_loss: 0.19949522614479065
train_iter_loss: 0.1467619687318802
train_iter_loss: 0.27952462434768677
train_iter_loss: 0.22612152993679047
train_iter_loss: 0.275475412607193
train_iter_loss: 0.10392722487449646
train_iter_loss: 0.11498923599720001
train_iter_loss: 0.10316480696201324
train_iter_loss: 0.18853120505809784
train_iter_loss: 0.17175979912281036
train_iter_loss: 0.24367018043994904
train_iter_loss: 0.08038749545812607
train_iter_loss: 0.20659732818603516
train_iter_loss: 0.11287643760442734
train_iter_loss: 0.1531423181295395
train_iter_loss: 0.11628290265798569
train_iter_loss: 0.23413261771202087
train_iter_loss: 0.1263623684644699
train_iter_loss: 0.15340499579906464
train_iter_loss: 0.10739830881357193
train_iter_loss: 0.16294151544570923
train_iter_loss: 0.15214581787586212
train_iter_loss: 0.08743450045585632
train_iter_loss: 0.22972820699214935
train_iter_loss: 0.1480254828929901
train_iter_loss: 0.19719810783863068
train_iter_loss: 0.17936745285987854
train_iter_loss: 0.10838572680950165
train_iter_loss: 0.16083572804927826
train_iter_loss: 0.16444027423858643
train_iter_loss: 0.1821664571762085
train_iter_loss: 0.33066877722740173
train_iter_loss: 0.19933147728443146
train_iter_loss: 0.2165318876504898
train_iter_loss: 0.11407484859228134
train_iter_loss: 0.18334828317165375
train_iter_loss: 0.16172434389591217
train_iter_loss: 0.10039174556732178
train_iter_loss: 0.11797644942998886
train loss :0.1643
---------------------
Validation seg loss: 0.2196850898710765 at epoch 345
epoch =    346/  1000, exp = train
train_iter_loss: 0.14064553380012512
train_iter_loss: 0.16988231241703033
train_iter_loss: 0.16453905403614044
train_iter_loss: 0.12549538910388947
train_iter_loss: 0.13631398975849152
train_iter_loss: 0.1935155689716339
train_iter_loss: 0.15054087340831757
train_iter_loss: 0.10876746475696564
train_iter_loss: 0.16360308229923248
train_iter_loss: 0.18226449191570282
train_iter_loss: 0.11092629283666611
train_iter_loss: 0.07555491477251053
train_iter_loss: 0.18623371422290802
train_iter_loss: 0.097768135368824
train_iter_loss: 0.16161254048347473
train_iter_loss: 0.334952175617218
train_iter_loss: 0.1313447803258896
train_iter_loss: 0.3024563193321228
train_iter_loss: 0.08561182022094727
train_iter_loss: 0.2227310985326767
train_iter_loss: 0.11707956343889236
train_iter_loss: 0.0611521415412426
train_iter_loss: 0.20761746168136597
train_iter_loss: 0.11587624251842499
train_iter_loss: 0.13926376402378082
train_iter_loss: 0.13859708607196808
train_iter_loss: 0.13170456886291504
train_iter_loss: 0.11802207678556442
train_iter_loss: 0.13347963988780975
train_iter_loss: 0.17688794434070587
train_iter_loss: 0.09830586612224579
train_iter_loss: 0.19639122486114502
train_iter_loss: 0.22477205097675323
train_iter_loss: 0.18718884885311127
train_iter_loss: 0.170726016163826
train_iter_loss: 0.15106505155563354
train_iter_loss: 0.10215425491333008
train_iter_loss: 0.2679867446422577
train_iter_loss: 0.1726927012205124
train_iter_loss: 0.1607813686132431
train_iter_loss: 0.19816672801971436
train_iter_loss: 0.2561231553554535
train_iter_loss: 0.22383959591388702
train_iter_loss: 0.11192800104618073
train_iter_loss: 0.148001566529274
train_iter_loss: 0.09934547543525696
train_iter_loss: 0.07914022356271744
train_iter_loss: 0.11509586870670319
train_iter_loss: 0.19630233943462372
train_iter_loss: 0.15692052245140076
train_iter_loss: 0.3131231963634491
train_iter_loss: 0.12783515453338623
train_iter_loss: 0.31571564078330994
train_iter_loss: 0.2210043966770172
train_iter_loss: 0.22267436981201172
train_iter_loss: 0.12289003282785416
train_iter_loss: 0.172150120139122
train_iter_loss: 0.1538814902305603
train_iter_loss: 0.12438027560710907
train_iter_loss: 0.051786087453365326
train_iter_loss: 0.19875618815422058
train_iter_loss: 0.08873868733644485
train_iter_loss: 0.20798523724079132
train_iter_loss: 0.19241498410701752
train_iter_loss: 0.11527541279792786
train_iter_loss: 0.07103167474269867
train_iter_loss: 0.21387842297554016
train_iter_loss: 0.22642765939235687
train_iter_loss: 0.19360889494419098
train_iter_loss: 0.2122708261013031
train_iter_loss: 0.09802842885255814
train_iter_loss: 0.1961911916732788
train_iter_loss: 0.15903782844543457
train_iter_loss: 0.12013474106788635
train_iter_loss: 0.18473504483699799
train_iter_loss: 0.19168271124362946
train_iter_loss: 0.12731344997882843
train_iter_loss: 0.27326884865760803
train_iter_loss: 0.17991510033607483
train_iter_loss: 0.1360228806734085
train_iter_loss: 0.10458582639694214
train_iter_loss: 0.07698279619216919
train_iter_loss: 0.14890985190868378
train_iter_loss: 0.1506386250257492
train_iter_loss: 0.08661842346191406
train_iter_loss: 0.03803516924381256
train_iter_loss: 0.07041282951831818
train_iter_loss: 0.12010138481855392
train_iter_loss: 0.15865947306156158
train_iter_loss: 0.2560892701148987
train_iter_loss: 0.20065520703792572
train_iter_loss: 0.294195294380188
train_iter_loss: 0.37725406885147095
train_iter_loss: 0.2219649851322174
train_iter_loss: 0.14370805025100708
train_iter_loss: 0.15551936626434326
train_iter_loss: 0.12801741063594818
train_iter_loss: 0.07788393646478653
train_iter_loss: 0.10226777195930481
train_iter_loss: 0.18060676753520966
train loss :0.1637
---------------------
Validation seg loss: 0.22388857941336507 at epoch 346
epoch =    347/  1000, exp = train
train_iter_loss: 0.07238007336854935
train_iter_loss: 0.1094045341014862
train_iter_loss: 0.1555882841348648
train_iter_loss: 0.20166824758052826
train_iter_loss: 0.3544023036956787
train_iter_loss: 0.19259852170944214
train_iter_loss: 0.2272108942270279
train_iter_loss: 0.16161037981510162
train_iter_loss: 0.10656765848398209
train_iter_loss: 0.3381231427192688
train_iter_loss: 0.07976963371038437
train_iter_loss: 0.2261030673980713
train_iter_loss: 0.1527802050113678
train_iter_loss: 0.1365467756986618
train_iter_loss: 0.1721213459968567
train_iter_loss: 0.22244097292423248
train_iter_loss: 0.09729210287332535
train_iter_loss: 0.3235280215740204
train_iter_loss: 0.10378901660442352
train_iter_loss: 0.08184107393026352
train_iter_loss: 0.38170528411865234
train_iter_loss: 0.10728219896554947
train_iter_loss: 0.23263908922672272
train_iter_loss: 0.198152557015419
train_iter_loss: 0.2801966965198517
train_iter_loss: 0.17101961374282837
train_iter_loss: 0.08876672387123108
train_iter_loss: 0.3095569610595703
train_iter_loss: 0.08208112418651581
train_iter_loss: 0.11194466799497604
train_iter_loss: 0.1425803303718567
train_iter_loss: 0.10131452232599258
train_iter_loss: 0.1705227941274643
train_iter_loss: 0.10923413187265396
train_iter_loss: 0.21588017046451569
train_iter_loss: 0.0846172496676445
train_iter_loss: 0.1717160940170288
train_iter_loss: 0.09434593468904495
train_iter_loss: 0.09613708406686783
train_iter_loss: 0.10708625614643097
train_iter_loss: 0.25727275013923645
train_iter_loss: 0.1219862550497055
train_iter_loss: 0.20665548741817474
train_iter_loss: 0.25765329599380493
train_iter_loss: 0.32840925455093384
train_iter_loss: 0.17834670841693878
train_iter_loss: 0.1835315078496933
train_iter_loss: 0.1965370774269104
train_iter_loss: 0.06443989276885986
train_iter_loss: 0.09591832011938095
train_iter_loss: 0.167929008603096
train_iter_loss: 0.08988502621650696
train_iter_loss: 0.1490696519613266
train_iter_loss: 0.10460180789232254
train_iter_loss: 0.16354039311408997
train_iter_loss: 0.09771141409873962
train_iter_loss: 0.18766960501670837
train_iter_loss: 0.08756819367408752
train_iter_loss: 0.15244518220424652
train_iter_loss: 0.21215087175369263
train_iter_loss: 0.2929317057132721
train_iter_loss: 0.12125229090452194
train_iter_loss: 0.06759194284677505
train_iter_loss: 0.16579197347164154
train_iter_loss: 0.11240750551223755
train_iter_loss: 0.1076025515794754
train_iter_loss: 0.10753435641527176
train_iter_loss: 0.1851925104856491
train_iter_loss: 0.20670823752880096
train_iter_loss: 0.11582938581705093
train_iter_loss: 0.12462949752807617
train_iter_loss: 0.22421805560588837
train_iter_loss: 0.1098204255104065
train_iter_loss: 0.1586197465658188
train_iter_loss: 0.11562174558639526
train_iter_loss: 0.36743777990341187
train_iter_loss: 0.14422620832920074
train_iter_loss: 0.1328306794166565
train_iter_loss: 0.06948070228099823
train_iter_loss: 0.06580950319766998
train_iter_loss: 0.13674774765968323
train_iter_loss: 0.12204276025295258
train_iter_loss: 0.21940001845359802
train_iter_loss: 0.16504594683647156
train_iter_loss: 0.25336551666259766
train_iter_loss: 0.18262843787670135
train_iter_loss: 0.11259280890226364
train_iter_loss: 0.1828504204750061
train_iter_loss: 0.28413325548171997
train_iter_loss: 0.10639672726392746
train_iter_loss: 0.1337660849094391
train_iter_loss: 0.17128868401050568
train_iter_loss: 0.276628315448761
train_iter_loss: 0.23688744008541107
train_iter_loss: 0.05965207517147064
train_iter_loss: 0.08137152343988419
train_iter_loss: 0.17547012865543365
train_iter_loss: 0.17942595481872559
train_iter_loss: 0.2669447064399719
train_iter_loss: 0.1796988546848297
train loss :0.1676
---------------------
Validation seg loss: 0.22243885462225046 at epoch 347
epoch =    348/  1000, exp = train
train_iter_loss: 0.12535296380519867
train_iter_loss: 0.2245030403137207
train_iter_loss: 0.30385246872901917
train_iter_loss: 0.12789539992809296
train_iter_loss: 0.12733379006385803
train_iter_loss: 0.1427687406539917
train_iter_loss: 0.15356861054897308
train_iter_loss: 0.14486244320869446
train_iter_loss: 0.15766358375549316
train_iter_loss: 0.1915307193994522
train_iter_loss: 0.11917255073785782
train_iter_loss: 0.12409760802984238
train_iter_loss: 0.13029193878173828
train_iter_loss: 0.1502637416124344
train_iter_loss: 0.20128896832466125
train_iter_loss: 0.14087112247943878
train_iter_loss: 0.2820199131965637
train_iter_loss: 0.17933151125907898
train_iter_loss: 0.081903375685215
train_iter_loss: 0.5465523600578308
train_iter_loss: 0.13955096900463104
train_iter_loss: 0.12840169668197632
train_iter_loss: 0.295193076133728
train_iter_loss: 0.059168923646211624
train_iter_loss: 0.18406333029270172
train_iter_loss: 0.14394602179527283
train_iter_loss: 0.08235538005828857
train_iter_loss: 0.1743011474609375
train_iter_loss: 0.09795168042182922
train_iter_loss: 0.2520223557949066
train_iter_loss: 0.23268243670463562
train_iter_loss: 0.15103724598884583
train_iter_loss: 0.099051333963871
train_iter_loss: 0.31572291254997253
train_iter_loss: 0.14711418747901917
train_iter_loss: 0.24429450929164886
train_iter_loss: 0.316129207611084
train_iter_loss: 0.16672645509243011
train_iter_loss: 0.18762153387069702
train_iter_loss: 0.1526774913072586
train_iter_loss: 0.13865748047828674
train_iter_loss: 0.150483176112175
train_iter_loss: 0.18630175292491913
train_iter_loss: 0.09412994980812073
train_iter_loss: 0.18436002731323242
train_iter_loss: 0.2168157994747162
train_iter_loss: 0.1681983917951584
train_iter_loss: 0.0880732536315918
train_iter_loss: 0.24573738873004913
train_iter_loss: 0.139593705534935
train_iter_loss: 0.13524582982063293
train_iter_loss: 0.1772335171699524
train_iter_loss: 0.17358000576496124
train_iter_loss: 0.1614982932806015
train_iter_loss: 0.16173389554023743
train_iter_loss: 0.12636546790599823
train_iter_loss: 0.11676042526960373
train_iter_loss: 0.1835232526063919
train_iter_loss: 0.12205184251070023
train_iter_loss: 0.22276756167411804
train_iter_loss: 0.12919239699840546
train_iter_loss: 0.07561291754245758
train_iter_loss: 0.11968715488910675
train_iter_loss: 0.05276241898536682
train_iter_loss: 0.18017734587192535
train_iter_loss: 0.15643459558486938
train_iter_loss: 0.16811273992061615
train_iter_loss: 0.11740302294492722
train_iter_loss: 0.1415187120437622
train_iter_loss: 0.1601952463388443
train_iter_loss: 0.1334208995103836
train_iter_loss: 0.23610083758831024
train_iter_loss: 0.1065826565027237
train_iter_loss: 0.2264322191476822
train_iter_loss: 0.06489888578653336
train_iter_loss: 0.18329006433486938
train_iter_loss: 0.17102529108524323
train_iter_loss: 0.07149844616651535
train_iter_loss: 0.18952739238739014
train_iter_loss: 0.21840757131576538
train_iter_loss: 0.11501205712556839
train_iter_loss: 0.2871386706829071
train_iter_loss: 0.2512204051017761
train_iter_loss: 0.1498139500617981
train_iter_loss: 0.3152259886264801
train_iter_loss: 0.11914358288049698
train_iter_loss: 0.13553935289382935
train_iter_loss: 0.10568419843912125
train_iter_loss: 0.18737401068210602
train_iter_loss: 0.1496935337781906
train_iter_loss: 0.14198055863380432
train_iter_loss: 0.2565962076187134
train_iter_loss: 0.07764416933059692
train_iter_loss: 0.14288875460624695
train_iter_loss: 0.14173132181167603
train_iter_loss: 0.074614018201828
train_iter_loss: 0.1276373267173767
train_iter_loss: 0.09957902133464813
train_iter_loss: 0.1519143134355545
train_iter_loss: 0.15615826845169067
train loss :0.1667
---------------------
Validation seg loss: 0.21748589284120584 at epoch 348
epoch =    349/  1000, exp = train
train_iter_loss: 0.1271408349275589
train_iter_loss: 0.09089048951864243
train_iter_loss: 0.06343062967061996
train_iter_loss: 0.11599432677030563
train_iter_loss: 0.1334189623594284
train_iter_loss: 0.2079078108072281
train_iter_loss: 0.3517666161060333
train_iter_loss: 0.13706840574741364
train_iter_loss: 0.1421389877796173
train_iter_loss: 0.12794804573059082
train_iter_loss: 0.1103631779551506
train_iter_loss: 0.22248604893684387
train_iter_loss: 0.08709821850061417
train_iter_loss: 0.12368076294660568
train_iter_loss: 0.16496993601322174
train_iter_loss: 0.03361906111240387
train_iter_loss: 0.10849426686763763
train_iter_loss: 0.21508057415485382
train_iter_loss: 0.19594547152519226
train_iter_loss: 0.2139589637517929
train_iter_loss: 0.17798465490341187
train_iter_loss: 0.13559691607952118
train_iter_loss: 0.1587201952934265
train_iter_loss: 0.08859039843082428
train_iter_loss: 0.14896763861179352
train_iter_loss: 0.13434568047523499
train_iter_loss: 0.13248683512210846
train_iter_loss: 0.07258128374814987
train_iter_loss: 0.21478790044784546
train_iter_loss: 0.32126694917678833
train_iter_loss: 0.20355181396007538
train_iter_loss: 0.148519366979599
train_iter_loss: 0.18500365316867828
train_iter_loss: 0.09466569125652313
train_iter_loss: 0.05206025019288063
train_iter_loss: 0.14841066300868988
train_iter_loss: 0.25082170963287354
train_iter_loss: 0.1947421431541443
train_iter_loss: 0.19281111657619476
train_iter_loss: 0.14624905586242676
train_iter_loss: 0.18356356024742126
train_iter_loss: 0.1400212049484253
train_iter_loss: 0.26681971549987793
train_iter_loss: 0.26398977637290955
train_iter_loss: 0.17062819004058838
train_iter_loss: 0.10135253518819809
train_iter_loss: 0.3076035976409912
train_iter_loss: 0.17963658273220062
train_iter_loss: 0.180613175034523
train_iter_loss: 0.21992909908294678
train_iter_loss: 0.12592928111553192
train_iter_loss: 0.15340332686901093
train_iter_loss: 0.15177594125270844
train_iter_loss: 0.1659364253282547
train_iter_loss: 0.4480651915073395
train_iter_loss: 0.11765599995851517
train_iter_loss: 0.09639593213796616
train_iter_loss: 0.16926157474517822
train_iter_loss: 0.25382494926452637
train_iter_loss: 0.15779158473014832
train_iter_loss: 0.15171416103839874
train_iter_loss: 0.24546736478805542
train_iter_loss: 0.13047613203525543
train_iter_loss: 0.1905204951763153
train_iter_loss: 0.12636002898216248
train_iter_loss: 0.09228246659040451
train_iter_loss: 0.1289658397436142
train_iter_loss: 0.12931863963603973
train_iter_loss: 0.17055413126945496
train_iter_loss: 0.15161211788654327
train_iter_loss: 0.12260685116052628
train_iter_loss: 0.16962139308452606
train_iter_loss: 0.13654598593711853
train_iter_loss: 0.08373638242483139
train_iter_loss: 0.23768988251686096
train_iter_loss: 0.22233600914478302
train_iter_loss: 0.20853587985038757
train_iter_loss: 0.17300131916999817
train_iter_loss: 0.1488327831029892
train_iter_loss: 0.18732425570487976
train_iter_loss: 0.2925463318824768
train_iter_loss: 0.2863272428512573
train_iter_loss: 0.20089684426784515
train_iter_loss: 0.19110693037509918
train_iter_loss: 0.1957041621208191
train_iter_loss: 0.0978688970208168
train_iter_loss: 0.08663273602724075
train_iter_loss: 0.1935252845287323
train_iter_loss: 0.1291484832763672
train_iter_loss: 0.10278850793838501
train_iter_loss: 0.16934573650360107
train_iter_loss: 0.16993074119091034
train_iter_loss: 0.1625775843858719
train_iter_loss: 0.10258709639310837
train_iter_loss: 0.17070461809635162
train_iter_loss: 0.12815247476100922
train_iter_loss: 0.058383334428071976
train_iter_loss: 0.12793657183647156
train_iter_loss: 0.07074723392724991
train_iter_loss: 0.1527005285024643
train loss :0.1649
---------------------
Validation seg loss: 0.21797836939189233 at epoch 349
epoch =    350/  1000, exp = train
train_iter_loss: 0.11576508730649948
train_iter_loss: 0.11869277060031891
train_iter_loss: 0.2585490047931671
train_iter_loss: 0.17770811915397644
train_iter_loss: 0.11090204119682312
train_iter_loss: 0.1425747126340866
train_iter_loss: 0.14501234889030457
train_iter_loss: 0.15960167348384857
train_iter_loss: 0.16814835369586945
train_iter_loss: 0.11266425997018814
train_iter_loss: 0.10287206619977951
train_iter_loss: 0.09334319084882736
train_iter_loss: 0.13463273644447327
train_iter_loss: 0.11682993918657303
train_iter_loss: 0.2585190236568451
train_iter_loss: 0.15877673029899597
train_iter_loss: 0.07605619728565216
train_iter_loss: 0.1798272579908371
train_iter_loss: 0.11187384277582169
train_iter_loss: 0.12054823338985443
train_iter_loss: 0.25563642382621765
train_iter_loss: 0.10543881356716156
train_iter_loss: 0.1982024908065796
train_iter_loss: 0.12237508594989777
train_iter_loss: 0.13976289331912994
train_iter_loss: 0.18358319997787476
train_iter_loss: 0.1619633138179779
train_iter_loss: 0.1546459048986435
train_iter_loss: 0.11759497225284576
train_iter_loss: 0.21280935406684875
train_iter_loss: 0.10148961842060089
train_iter_loss: 0.17580607533454895
train_iter_loss: 0.18911375105381012
train_iter_loss: 0.07811471074819565
train_iter_loss: 0.20783939957618713
train_iter_loss: 0.12479139864444733
train_iter_loss: 0.10421346127986908
train_iter_loss: 0.12597092986106873
train_iter_loss: 0.246881902217865
train_iter_loss: 0.1120610460639
train_iter_loss: 0.11519818753004074
train_iter_loss: 0.3316221535205841
train_iter_loss: 0.15341997146606445
train_iter_loss: 0.068418949842453
train_iter_loss: 0.14348644018173218
train_iter_loss: 0.1881798505783081
train_iter_loss: 0.2554706335067749
train_iter_loss: 0.06882783025503159
train_iter_loss: 0.1368853598833084
train_iter_loss: 0.30353403091430664
train_iter_loss: 0.08223578333854675
train_iter_loss: 0.16054320335388184
train_iter_loss: 0.21199387311935425
train_iter_loss: 0.24061505496501923
train_iter_loss: 0.13528043031692505
train_iter_loss: 0.17223933339118958
train_iter_loss: 0.13247834146022797
train_iter_loss: 0.1518630087375641
train_iter_loss: 0.18505218625068665
train_iter_loss: 0.2328609675168991
train_iter_loss: 0.2794497609138489
train_iter_loss: 0.19292961061000824
train_iter_loss: 0.11779063940048218
train_iter_loss: 0.2061302214860916
train_iter_loss: 0.14015932381153107
train_iter_loss: 0.19857822358608246
train_iter_loss: 0.06871268898248672
train_iter_loss: 0.157958522439003
train_iter_loss: 0.3464805483818054
train_iter_loss: 0.1854846477508545
train_iter_loss: 0.07191869616508484
train_iter_loss: 0.04390842840075493
train_iter_loss: 0.18092219531536102
train_iter_loss: 0.11380896717309952
train_iter_loss: 0.16209299862384796
train_iter_loss: 0.3179786503314972
train_iter_loss: 0.23673605918884277
train_iter_loss: 0.1383010298013687
train_iter_loss: 0.1525506228208542
train_iter_loss: 0.08676406741142273
train_iter_loss: 0.19811870157718658
train_iter_loss: 0.21999675035476685
train_iter_loss: 0.21010155975818634
train_iter_loss: 0.3013176918029785
train_iter_loss: 0.06355731189250946
train_iter_loss: 0.114909827709198
train_iter_loss: 0.16733066737651825
train_iter_loss: 0.16239702701568604
train_iter_loss: 0.0998663678765297
train_iter_loss: 0.059532348066568375
train_iter_loss: 0.15216240286827087
train_iter_loss: 0.1749456226825714
train_iter_loss: 0.09564657509326935
train_iter_loss: 0.13008815050125122
train_iter_loss: 0.19860820472240448
train_iter_loss: 0.14132633805274963
train_iter_loss: 0.19661465287208557
train_iter_loss: 0.2144778072834015
train_iter_loss: 0.16467197239398956
train_iter_loss: 0.1608952432870865
train loss :0.1624
---------------------
Validation seg loss: 0.22109483960875362 at epoch 350
epoch =    351/  1000, exp = train
train_iter_loss: 0.24265722930431366
train_iter_loss: 0.13426223397254944
train_iter_loss: 0.19092969596385956
train_iter_loss: 0.18783977627754211
train_iter_loss: 0.14357522130012512
train_iter_loss: 0.10081910341978073
train_iter_loss: 0.18082977831363678
train_iter_loss: 0.13875016570091248
train_iter_loss: 0.1381186842918396
train_iter_loss: 0.05441290885210037
train_iter_loss: 0.1994728446006775
train_iter_loss: 0.18977119028568268
train_iter_loss: 0.32754045724868774
train_iter_loss: 0.1602019965648651
train_iter_loss: 0.2161017656326294
train_iter_loss: 0.14992213249206543
train_iter_loss: 0.46003276109695435
train_iter_loss: 0.2046421468257904
train_iter_loss: 0.2970055937767029
train_iter_loss: 0.10863346606492996
train_iter_loss: 0.10398276895284653
train_iter_loss: 0.1580600142478943
train_iter_loss: 0.1540532410144806
train_iter_loss: 0.09079622477293015
train_iter_loss: 0.13869430124759674
train_iter_loss: 0.12868516147136688
train_iter_loss: 0.2525027394294739
train_iter_loss: 0.15414880216121674
train_iter_loss: 0.1739903688430786
train_iter_loss: 0.09234067052602768
train_iter_loss: 0.1251971274614334
train_iter_loss: 0.07696670293807983
train_iter_loss: 0.3019106686115265
train_iter_loss: 0.08358278125524521
train_iter_loss: 0.16327863931655884
train_iter_loss: 0.10025140643119812
train_iter_loss: 0.15729109942913055
train_iter_loss: 0.26497122645378113
train_iter_loss: 0.16665326058864594
train_iter_loss: 0.13461634516716003
train_iter_loss: 0.10801471024751663
train_iter_loss: 0.294206827878952
train_iter_loss: 0.09582462906837463
train_iter_loss: 0.125833660364151
train_iter_loss: 0.1227208748459816
train_iter_loss: 0.14025509357452393
train_iter_loss: 0.15810401737689972
train_iter_loss: 0.11376133561134338
train_iter_loss: 0.10231022536754608
train_iter_loss: 0.28344252705574036
train_iter_loss: 0.11375346779823303
train_iter_loss: 0.21780119836330414
train_iter_loss: 0.12789799273014069
train_iter_loss: 0.14812356233596802
train_iter_loss: 0.16797122359275818
train_iter_loss: 0.21429912745952606
train_iter_loss: 0.16864119470119476
train_iter_loss: 0.12144993990659714
train_iter_loss: 0.2736632823944092
train_iter_loss: 0.18696068227291107
train_iter_loss: 0.14385829865932465
train_iter_loss: 0.06525731086730957
train_iter_loss: 0.22266517579555511
train_iter_loss: 0.2223026156425476
train_iter_loss: 0.1941666156053543
train_iter_loss: 0.16933941841125488
train_iter_loss: 0.1788586974143982
train_iter_loss: 0.23359715938568115
train_iter_loss: 0.18006819486618042
train_iter_loss: 0.3473864793777466
train_iter_loss: 0.20728518068790436
train_iter_loss: 0.10212238132953644
train_iter_loss: 0.24870365858078003
train_iter_loss: 0.1231735497713089
train_iter_loss: 0.08596282452344894
train_iter_loss: 0.18106894195079803
train_iter_loss: 0.03952159732580185
train_iter_loss: 0.15494190156459808
train_iter_loss: 0.22766898572444916
train_iter_loss: 0.17682622373104095
train_iter_loss: 0.1130661740899086
train_iter_loss: 0.1922016739845276
train_iter_loss: 0.12578433752059937
train_iter_loss: 0.22021450102329254
train_iter_loss: 0.20775271952152252
train_iter_loss: 0.19225594401359558
train_iter_loss: 0.11512605845928192
train_iter_loss: 0.12932340800762177
train_iter_loss: 0.2866719663143158
train_iter_loss: 0.16316883265972137
train_iter_loss: 0.2460959106683731
train_iter_loss: 0.14839200675487518
train_iter_loss: 0.17484620213508606
train_iter_loss: 0.14041095972061157
train_iter_loss: 0.1447552740573883
train_iter_loss: 0.11497808992862701
train_iter_loss: 0.1365710347890854
train_iter_loss: 0.11578052490949631
train_iter_loss: 0.08924701809883118
train_iter_loss: 0.18606629967689514
train loss :0.1704
---------------------
Validation seg loss: 0.22079838909876515 at epoch 351
epoch =    352/  1000, exp = train
train_iter_loss: 0.13412097096443176
train_iter_loss: 0.25353726744651794
train_iter_loss: 0.15634483098983765
train_iter_loss: 0.18860052525997162
train_iter_loss: 0.18013529479503632
train_iter_loss: 0.14595340192317963
train_iter_loss: 0.09002051502466202
train_iter_loss: 0.1372850388288498
train_iter_loss: 0.18776877224445343
train_iter_loss: 0.20998834073543549
train_iter_loss: 0.08475390821695328
train_iter_loss: 0.2008041888475418
train_iter_loss: 0.1942949891090393
train_iter_loss: 0.12936072051525116
train_iter_loss: 0.5738285779953003
train_iter_loss: 0.07252956926822662
train_iter_loss: 0.13670529425144196
train_iter_loss: 0.20199619233608246
train_iter_loss: 0.1696603000164032
train_iter_loss: 0.2374265193939209
train_iter_loss: 0.22706882655620575
train_iter_loss: 0.2110178917646408
train_iter_loss: 0.2727576792240143
train_iter_loss: 0.08101910352706909
train_iter_loss: 0.1327868402004242
train_iter_loss: 0.11564681679010391
train_iter_loss: 0.0500553734600544
train_iter_loss: 0.20358891785144806
train_iter_loss: 0.12110498547554016
train_iter_loss: 0.10578072816133499
train_iter_loss: 0.09603520482778549
train_iter_loss: 0.12241315841674805
train_iter_loss: 0.19732530415058136
train_iter_loss: 0.08796887844800949
train_iter_loss: 0.21113651990890503
train_iter_loss: 0.1294328272342682
train_iter_loss: 0.12170349806547165
train_iter_loss: 0.2234525978565216
train_iter_loss: 0.12578584253787994
train_iter_loss: 0.17358343303203583
train_iter_loss: 0.22585749626159668
train_iter_loss: 0.2536095380783081
train_iter_loss: 0.16917531192302704
train_iter_loss: 0.10131572932004929
train_iter_loss: 0.13431382179260254
train_iter_loss: 0.2919054627418518
train_iter_loss: 0.2678808867931366
train_iter_loss: 0.09338978677988052
train_iter_loss: 0.07683068513870239
train_iter_loss: 0.058678172528743744
train_iter_loss: 0.09448280185461044
train_iter_loss: 0.10484239459037781
train_iter_loss: 0.15267133712768555
train_iter_loss: 0.2286948412656784
train_iter_loss: 0.0603652186691761
train_iter_loss: 0.159091055393219
train_iter_loss: 0.11793973296880722
train_iter_loss: 0.12468957155942917
train_iter_loss: 0.18261343240737915
train_iter_loss: 0.18690356612205505
train_iter_loss: 0.1358867883682251
train_iter_loss: 0.08100137859582901
train_iter_loss: 0.2437969297170639
train_iter_loss: 0.13758692145347595
train_iter_loss: 0.19125257432460785
train_iter_loss: 0.11945140361785889
train_iter_loss: 0.07948034256696701
train_iter_loss: 0.22309131920337677
train_iter_loss: 0.16494247317314148
train_iter_loss: 0.041226133704185486
train_iter_loss: 0.16817887127399445
train_iter_loss: 0.11922823637723923
train_iter_loss: 0.24798502027988434
train_iter_loss: 0.18916504085063934
train_iter_loss: 0.237924262881279
train_iter_loss: 0.26236727833747864
train_iter_loss: 0.13397790491580963
train_iter_loss: 0.08323676884174347
train_iter_loss: 0.17962118983268738
train_iter_loss: 0.24448710680007935
train_iter_loss: 0.11296150833368301
train_iter_loss: 0.15354248881340027
train_iter_loss: 0.169065922498703
train_iter_loss: 0.11756645888090134
train_iter_loss: 0.08694837987422943
train_iter_loss: 0.20501761138439178
train_iter_loss: 0.1811240017414093
train_iter_loss: 0.23615191876888275
train_iter_loss: 0.28734344244003296
train_iter_loss: 0.2786291241645813
train_iter_loss: 0.1055949330329895
train_iter_loss: 0.22147496044635773
train_iter_loss: 0.0804704949259758
train_iter_loss: 0.16611425578594208
train_iter_loss: 0.07938036322593689
train_iter_loss: 0.1332852840423584
train_iter_loss: 0.2322125881910324
train_iter_loss: 0.22054703533649445
train_iter_loss: 0.15373073518276215
train_iter_loss: 0.17319335043430328
train loss :0.1663
---------------------
Validation seg loss: 0.22123425790407467 at epoch 352
epoch =    353/  1000, exp = train
train_iter_loss: 0.21584029495716095
train_iter_loss: 0.1923937052488327
train_iter_loss: 0.13858558237552643
train_iter_loss: 0.10093902051448822
train_iter_loss: 0.08282561600208282
train_iter_loss: 0.12687496840953827
train_iter_loss: 0.13415555655956268
train_iter_loss: 0.1645289808511734
train_iter_loss: 0.21605515480041504
train_iter_loss: 0.15297938883304596
train_iter_loss: 0.1543569415807724
train_iter_loss: 0.13763204216957092
train_iter_loss: 0.08012598007917404
train_iter_loss: 0.1290368288755417
train_iter_loss: 0.15712299942970276
train_iter_loss: 0.1009395569562912
train_iter_loss: 0.15065941214561462
train_iter_loss: 0.12654155492782593
train_iter_loss: 0.09632983058691025
train_iter_loss: 0.14067864418029785
train_iter_loss: 0.16796576976776123
train_iter_loss: 0.19146563112735748
train_iter_loss: 0.17512282729148865
train_iter_loss: 0.1347726583480835
train_iter_loss: 0.08248452842235565
train_iter_loss: 0.1089320108294487
train_iter_loss: 0.343147873878479
train_iter_loss: 0.1028100848197937
train_iter_loss: 0.22154588997364044
train_iter_loss: 0.1750669926404953
train_iter_loss: 0.24093951284885406
train_iter_loss: 0.2894599437713623
train_iter_loss: 0.18104006350040436
train_iter_loss: 0.11355340480804443
train_iter_loss: 0.13410663604736328
train_iter_loss: 0.14306193590164185
train_iter_loss: 0.17389951646327972
train_iter_loss: 0.15767955780029297
train_iter_loss: 0.14134074747562408
train_iter_loss: 0.12627685070037842
train_iter_loss: 0.14418520033359528
train_iter_loss: 0.25928881764411926
train_iter_loss: 0.10128302872180939
train_iter_loss: 0.10082723945379257
train_iter_loss: 0.10580305010080338
train_iter_loss: 0.18472498655319214
train_iter_loss: 0.07554953545331955
train_iter_loss: 0.14073528349399567
train_iter_loss: 0.2520603537559509
train_iter_loss: 0.15358175337314606
train_iter_loss: 0.2700810730457306
train_iter_loss: 0.18616443872451782
train_iter_loss: 0.06808481365442276
train_iter_loss: 0.15642833709716797
train_iter_loss: 0.13858067989349365
train_iter_loss: 0.19412580132484436
train_iter_loss: 0.14643636345863342
train_iter_loss: 0.16731181740760803
train_iter_loss: 0.24403560161590576
train_iter_loss: 0.18781548738479614
train_iter_loss: 0.11665243655443192
train_iter_loss: 0.17221283912658691
train_iter_loss: 0.13383722305297852
train_iter_loss: 0.29398807883262634
train_iter_loss: 0.2492828071117401
train_iter_loss: 0.24515889585018158
train_iter_loss: 0.20490260422229767
train_iter_loss: 0.2142978012561798
train_iter_loss: 0.22433537244796753
train_iter_loss: 0.3472389578819275
train_iter_loss: 0.20147308707237244
train_iter_loss: 0.10920796543359756
train_iter_loss: 0.09834876656532288
train_iter_loss: 0.11488582193851471
train_iter_loss: 0.16767290234565735
train_iter_loss: 0.12862235307693481
train_iter_loss: 0.21252739429473877
train_iter_loss: 0.24208086729049683
train_iter_loss: 0.32484766840934753
train_iter_loss: 0.1416931450366974
train_iter_loss: 0.08628609031438828
train_iter_loss: 0.18895323574543
train_iter_loss: 0.15888868272304535
train_iter_loss: 0.1410207897424698
train_iter_loss: 0.2025504857301712
train_iter_loss: 0.1919749230146408
train_iter_loss: 0.08428335189819336
train_iter_loss: 0.2504459619522095
train_iter_loss: 0.11442428827285767
train_iter_loss: 0.0346187949180603
train_iter_loss: 0.17704589664936066
train_iter_loss: 0.1852397918701172
train_iter_loss: 0.0876694768667221
train_iter_loss: 0.07660001516342163
train_iter_loss: 0.16228385269641876
train_iter_loss: 0.09753768891096115
train_iter_loss: 0.17082373797893524
train_iter_loss: 0.18172292411327362
train_iter_loss: 0.2347697764635086
train_iter_loss: 0.10720959305763245
train loss :0.1652
---------------------
Validation seg loss: 0.21944284034049455 at epoch 353
epoch =    354/  1000, exp = train
train_iter_loss: 0.16251491010189056
train_iter_loss: 0.11426104605197906
train_iter_loss: 0.07400099188089371
train_iter_loss: 0.18596991896629333
train_iter_loss: 0.1401534527540207
train_iter_loss: 0.25908610224723816
train_iter_loss: 0.1050570011138916
train_iter_loss: 0.08231841772794724
train_iter_loss: 0.15269002318382263
train_iter_loss: 0.19478698074817657
train_iter_loss: 0.0842919647693634
train_iter_loss: 0.08934236317873001
train_iter_loss: 0.20745104551315308
train_iter_loss: 0.15777619183063507
train_iter_loss: 0.14228087663650513
train_iter_loss: 0.08275598287582397
train_iter_loss: 0.12659645080566406
train_iter_loss: 0.09380648285150528
train_iter_loss: 0.09733911603689194
train_iter_loss: 0.1912362426519394
train_iter_loss: 0.13924837112426758
train_iter_loss: 0.1591850370168686
train_iter_loss: 0.16302835941314697
train_iter_loss: 0.16488958895206451
train_iter_loss: 0.14989681541919708
train_iter_loss: 0.12812423706054688
train_iter_loss: 0.22057729959487915
train_iter_loss: 0.1518225520849228
train_iter_loss: 0.06518945097923279
train_iter_loss: 0.15041214227676392
train_iter_loss: 0.1657213270664215
train_iter_loss: 0.17147229611873627
train_iter_loss: 0.07801036536693573
train_iter_loss: 0.08597534894943237
train_iter_loss: 0.17983713746070862
train_iter_loss: 0.24272455275058746
train_iter_loss: 0.21622493863105774
train_iter_loss: 0.16818775236606598
train_iter_loss: 0.1881433129310608
train_iter_loss: 0.16649042069911957
train_iter_loss: 0.17219266295433044
train_iter_loss: 0.09996134042739868
train_iter_loss: 0.05510154366493225
train_iter_loss: 0.1747990846633911
train_iter_loss: 0.1848946362733841
train_iter_loss: 0.15528883039951324
train_iter_loss: 0.1577463001012802
train_iter_loss: 0.14470823109149933
train_iter_loss: 0.17866772413253784
train_iter_loss: 0.059636440128088
train_iter_loss: 0.2387951761484146
train_iter_loss: 0.133714497089386
train_iter_loss: 0.1424916386604309
train_iter_loss: 0.2861246168613434
train_iter_loss: 0.4396255910396576
train_iter_loss: 0.11398080736398697
train_iter_loss: 0.08916887640953064
train_iter_loss: 0.1333758383989334
train_iter_loss: 0.1073441430926323
train_iter_loss: 0.20404621958732605
train_iter_loss: 0.20052172243595123
train_iter_loss: 0.16980186104774475
train_iter_loss: 0.09595688432455063
train_iter_loss: 0.07058028876781464
train_iter_loss: 0.09459986537694931
train_iter_loss: 0.11226111650466919
train_iter_loss: 0.281673789024353
train_iter_loss: 0.20156532526016235
train_iter_loss: 0.16044311225414276
train_iter_loss: 0.19672399759292603
train_iter_loss: 0.14094318449497223
train_iter_loss: 0.20012807846069336
train_iter_loss: 0.18532663583755493
train_iter_loss: 0.10748269408941269
train_iter_loss: 0.1919315755367279
train_iter_loss: 0.1956280618906021
train_iter_loss: 0.2843914031982422
train_iter_loss: 0.25339335203170776
train_iter_loss: 0.22741934657096863
train_iter_loss: 0.1516260951757431
train_iter_loss: 0.10947618633508682
train_iter_loss: 0.30891624093055725
train_iter_loss: 0.15749162435531616
train_iter_loss: 0.16700567305088043
train_iter_loss: 0.2247646152973175
train_iter_loss: 0.13722676038742065
train_iter_loss: 0.1177864819765091
train_iter_loss: 0.2127789556980133
train_iter_loss: 0.13226673007011414
train_iter_loss: 0.15619753301143646
train_iter_loss: 0.22618748247623444
train_iter_loss: 0.10354491323232651
train_iter_loss: 0.22065068781375885
train_iter_loss: 0.39170628786087036
train_iter_loss: 0.28914034366607666
train_iter_loss: 0.06370396912097931
train_iter_loss: 0.0693398043513298
train_iter_loss: 0.23789335787296295
train_iter_loss: 0.11790811270475388
train_iter_loss: 0.19105252623558044
train loss :0.1649
---------------------
Validation seg loss: 0.21746808844882082 at epoch 354
epoch =    355/  1000, exp = train
train_iter_loss: 0.11108869314193726
train_iter_loss: 0.2013302743434906
train_iter_loss: 0.20198696851730347
train_iter_loss: 0.26793816685676575
train_iter_loss: 0.21230851113796234
train_iter_loss: 0.21083347499370575
train_iter_loss: 0.29306140542030334
train_iter_loss: 0.27582523226737976
train_iter_loss: 0.1679016649723053
train_iter_loss: 0.10571526736021042
train_iter_loss: 0.09367195516824722
train_iter_loss: 0.17618785798549652
train_iter_loss: 0.07812099158763885
train_iter_loss: 0.10991445928812027
train_iter_loss: 0.060425616800785065
train_iter_loss: 0.1373118907213211
train_iter_loss: 0.10391955077648163
train_iter_loss: 0.19886891543865204
train_iter_loss: 0.11566702276468277
train_iter_loss: 0.17718087136745453
train_iter_loss: 0.11082828044891357
train_iter_loss: 0.16131585836410522
train_iter_loss: 0.15694576501846313
train_iter_loss: 0.26702430844306946
train_iter_loss: 0.20462855696678162
train_iter_loss: 0.07702456414699554
train_iter_loss: 0.1860944777727127
train_iter_loss: 0.10615206509828568
train_iter_loss: 0.11892025917768478
train_iter_loss: 0.15755975246429443
train_iter_loss: 0.1634378433227539
train_iter_loss: 0.13569730520248413
train_iter_loss: 0.2789185345172882
train_iter_loss: 0.17994309961795807
train_iter_loss: 0.1445922553539276
train_iter_loss: 0.17201246321201324
train_iter_loss: 0.1034916341304779
train_iter_loss: 0.20467516779899597
train_iter_loss: 0.14920932054519653
train_iter_loss: 0.17324313521385193
train_iter_loss: 0.11060880869626999
train_iter_loss: 0.20104943215847015
train_iter_loss: 0.0822659283876419
train_iter_loss: 0.10819708555936813
train_iter_loss: 0.1948499232530594
train_iter_loss: 0.1874723732471466
train_iter_loss: 0.15602780878543854
train_iter_loss: 0.16108503937721252
train_iter_loss: 0.10656645894050598
train_iter_loss: 0.2592368423938751
train_iter_loss: 0.27540746331214905
train_iter_loss: 0.18320804834365845
train_iter_loss: 0.19306334853172302
train_iter_loss: 0.15044870972633362
train_iter_loss: 0.0646093487739563
train_iter_loss: 0.16480839252471924
train_iter_loss: 0.5097361207008362
train_iter_loss: 0.1759699434041977
train_iter_loss: 0.06184258684515953
train_iter_loss: 0.23705939948558807
train_iter_loss: 0.1957116574048996
train_iter_loss: 0.12382497638463974
train_iter_loss: 0.1952817142009735
train_iter_loss: 0.3352327346801758
train_iter_loss: 0.1992960423231125
train_iter_loss: 0.2236974537372589
train_iter_loss: 0.19698716700077057
train_iter_loss: 0.10778842121362686
train_iter_loss: 0.14262937009334564
train_iter_loss: 0.0905841663479805
train_iter_loss: 0.21296857297420502
train_iter_loss: 0.2710792124271393
train_iter_loss: 0.12139592319726944
train_iter_loss: 0.17771509289741516
train_iter_loss: 0.21147042512893677
train_iter_loss: 0.17043939232826233
train_iter_loss: 0.17027448117733002
train_iter_loss: 0.19505539536476135
train_iter_loss: 0.11827491223812103
train_iter_loss: 0.09170356392860413
train_iter_loss: 0.15472300350666046
train_iter_loss: 0.1631818562746048
train_iter_loss: 0.12666532397270203
train_iter_loss: 0.0812121257185936
train_iter_loss: 0.12198604643344879
train_iter_loss: 0.1728164106607437
train_iter_loss: 0.09876686334609985
train_iter_loss: 0.15947872400283813
train_iter_loss: 0.10718617588281631
train_iter_loss: 0.08241239190101624
train_iter_loss: 0.11959762871265411
train_iter_loss: 0.11800049990415573
train_iter_loss: 0.19088023900985718
train_iter_loss: 0.14170046150684357
train_iter_loss: 0.04895495995879173
train_iter_loss: 0.14863689243793488
train_iter_loss: 0.27659323811531067
train_iter_loss: 0.13968448340892792
train_iter_loss: 0.16655580699443817
train_iter_loss: 0.0958789810538292
train loss :0.1657
---------------------
Validation seg loss: 0.21961417523616888 at epoch 355
epoch =    356/  1000, exp = train
train_iter_loss: 0.08571004867553711
train_iter_loss: 0.17777253687381744
train_iter_loss: 0.11411206424236298
train_iter_loss: 0.14568331837654114
train_iter_loss: 0.07294382154941559
train_iter_loss: 0.20449426770210266
train_iter_loss: 0.10995228588581085
train_iter_loss: 0.14418229460716248
train_iter_loss: 0.1008729487657547
train_iter_loss: 0.10828565806150436
train_iter_loss: 0.2016228288412094
train_iter_loss: 0.09944432228803635
train_iter_loss: 0.18870286643505096
train_iter_loss: 0.3223775029182434
train_iter_loss: 0.13217110931873322
train_iter_loss: 0.21846333146095276
train_iter_loss: 0.15595686435699463
train_iter_loss: 0.14752458035945892
train_iter_loss: 0.15248556435108185
train_iter_loss: 0.13315734267234802
train_iter_loss: 0.15540583431720734
train_iter_loss: 0.1965092122554779
train_iter_loss: 0.22955924272537231
train_iter_loss: 0.180073082447052
train_iter_loss: 0.11016031354665756
train_iter_loss: 0.11536093056201935
train_iter_loss: 0.09383486956357956
train_iter_loss: 0.17692895233631134
train_iter_loss: 0.25453394651412964
train_iter_loss: 0.080118328332901
train_iter_loss: 0.13532273471355438
train_iter_loss: 0.14353786408901215
train_iter_loss: 0.26351723074913025
train_iter_loss: 0.2913700044155121
train_iter_loss: 0.16991646587848663
train_iter_loss: 0.0682840645313263
train_iter_loss: 0.06833217293024063
train_iter_loss: 0.13048958778381348
train_iter_loss: 0.20761051774024963
train_iter_loss: 0.15227657556533813
train_iter_loss: 0.30294886231422424
train_iter_loss: 0.20572109520435333
train_iter_loss: 0.08612015843391418
train_iter_loss: 0.28622353076934814
train_iter_loss: 0.13937728106975555
train_iter_loss: 0.16042208671569824
train_iter_loss: 0.0945749580860138
train_iter_loss: 0.15272954106330872
train_iter_loss: 0.22485458850860596
train_iter_loss: 0.1172252893447876
train_iter_loss: 0.14080092310905457
train_iter_loss: 0.1926114559173584
train_iter_loss: 0.10477547347545624
train_iter_loss: 0.16010060906410217
train_iter_loss: 0.3407120108604431
train_iter_loss: 0.27713462710380554
train_iter_loss: 0.2754945456981659
train_iter_loss: 0.13639846444129944
train_iter_loss: 0.28692391514778137
train_iter_loss: 0.2062746286392212
train_iter_loss: 0.08703038841485977
train_iter_loss: 0.3433564305305481
train_iter_loss: 0.1272195279598236
train_iter_loss: 0.28314492106437683
train_iter_loss: 0.21052107214927673
train_iter_loss: 0.13733109831809998
train_iter_loss: 0.08732900768518448
train_iter_loss: 0.21784643828868866
train_iter_loss: 0.13973300158977509
train_iter_loss: 0.11913719773292542
train_iter_loss: 0.3598185181617737
train_iter_loss: 0.16107045114040375
train_iter_loss: 0.27822145819664
train_iter_loss: 0.13604769110679626
train_iter_loss: 0.1337059736251831
train_iter_loss: 0.14119617640972137
train_iter_loss: 0.18200553953647614
train_iter_loss: 0.13285546004772186
train_iter_loss: 0.18924379348754883
train_iter_loss: 0.09664656221866608
train_iter_loss: 0.11077745258808136
train_iter_loss: 0.17538295686244965
train_iter_loss: 0.26767316460609436
train_iter_loss: 0.15886828303337097
train_iter_loss: 0.09901230037212372
train_iter_loss: 0.2082538902759552
train_iter_loss: 0.16841092705726624
train_iter_loss: 0.1311940848827362
train_iter_loss: 0.13498158752918243
train_iter_loss: 0.13809935748577118
train_iter_loss: 0.07490331679582596
train_iter_loss: 0.21712052822113037
train_iter_loss: 0.09688126295804977
train_iter_loss: 0.2258630245923996
train_iter_loss: 0.17810699343681335
train_iter_loss: 0.16508223116397858
train_iter_loss: 0.18492016196250916
train_iter_loss: 0.10163483768701553
train_iter_loss: 0.15866905450820923
train_iter_loss: 0.13959331810474396
train loss :0.1700
---------------------
Validation seg loss: 0.21947402289172388 at epoch 356
epoch =    357/  1000, exp = train
train_iter_loss: 0.10970496386289597
train_iter_loss: 0.09566479921340942
train_iter_loss: 0.2592625916004181
train_iter_loss: 0.332811176776886
train_iter_loss: 0.1493479162454605
train_iter_loss: 0.2015453577041626
train_iter_loss: 0.06844809651374817
train_iter_loss: 0.06286252290010452
train_iter_loss: 0.18045581877231598
train_iter_loss: 0.16404901444911957
train_iter_loss: 0.10238068550825119
train_iter_loss: 0.1986592561006546
train_iter_loss: 0.20254012942314148
train_iter_loss: 0.1862230896949768
train_iter_loss: 0.1233544647693634
train_iter_loss: 0.2542492747306824
train_iter_loss: 0.17066892981529236
train_iter_loss: 0.1232447549700737
train_iter_loss: 0.1162845715880394
train_iter_loss: 0.35262712836265564
train_iter_loss: 0.11359835416078568
train_iter_loss: 0.14407603442668915
train_iter_loss: 0.188815638422966
train_iter_loss: 0.1856875717639923
train_iter_loss: 0.20389030873775482
train_iter_loss: 0.07406345754861832
train_iter_loss: 0.15044283866882324
train_iter_loss: 0.23572030663490295
train_iter_loss: 0.13098081946372986
train_iter_loss: 0.21931666135787964
train_iter_loss: 0.1283886581659317
train_iter_loss: 0.06531455367803574
train_iter_loss: 0.20142598450183868
train_iter_loss: 0.18921901285648346
train_iter_loss: 0.06944864243268967
train_iter_loss: 0.1768866330385208
train_iter_loss: 0.12775583565235138
train_iter_loss: 0.10640910267829895
train_iter_loss: 0.22122161090373993
train_iter_loss: 0.12719160318374634
train_iter_loss: 0.23481422662734985
train_iter_loss: 0.19286641478538513
train_iter_loss: 0.1433996856212616
train_iter_loss: 0.05258992686867714
train_iter_loss: 0.1087842583656311
train_iter_loss: 0.10629041492938995
train_iter_loss: 0.174197718501091
train_iter_loss: 0.25375089049339294
train_iter_loss: 0.16335076093673706
train_iter_loss: 0.11369039118289948
train_iter_loss: 0.2858014404773712
train_iter_loss: 0.12775880098342896
train_iter_loss: 0.12147252261638641
train_iter_loss: 0.20370155572891235
train_iter_loss: 0.16683483123779297
train_iter_loss: 0.19972510635852814
train_iter_loss: 0.09621268510818481
train_iter_loss: 0.2979729175567627
train_iter_loss: 0.22978882491588593
train_iter_loss: 0.16515693068504333
train_iter_loss: 0.11438004672527313
train_iter_loss: 0.18079856038093567
train_iter_loss: 0.2022317796945572
train_iter_loss: 0.1134599819779396
train_iter_loss: 0.13410115242004395
train_iter_loss: 0.1536829173564911
train_iter_loss: 0.1295083910226822
train_iter_loss: 0.21596236526966095
train_iter_loss: 0.14974212646484375
train_iter_loss: 0.11305369436740875
train_iter_loss: 0.09268984198570251
train_iter_loss: 0.04936053603887558
train_iter_loss: 0.11967294663190842
train_iter_loss: 0.2548760771751404
train_iter_loss: 0.13021281361579895
train_iter_loss: 0.2015203833580017
train_iter_loss: 0.11147180944681168
train_iter_loss: 0.19477003812789917
train_iter_loss: 0.2509487271308899
train_iter_loss: 0.15478777885437012
train_iter_loss: 0.08741381019353867
train_iter_loss: 0.1492929458618164
train_iter_loss: 0.149983748793602
train_iter_loss: 0.0829504057765007
train_iter_loss: 0.09973505139350891
train_iter_loss: 0.1221269741654396
train_iter_loss: 0.22787001729011536
train_iter_loss: 0.16568639874458313
train_iter_loss: 0.12100762873888016
train_iter_loss: 0.09433959424495697
train_iter_loss: 0.30243849754333496
train_iter_loss: 0.24373291432857513
train_iter_loss: 0.14451470971107483
train_iter_loss: 0.185659259557724
train_iter_loss: 0.23260678350925446
train_iter_loss: 0.27113407850265503
train_iter_loss: 0.08232492953538895
train_iter_loss: 0.0917230024933815
train_iter_loss: 0.1746160387992859
train_iter_loss: 0.16452237963676453
train loss :0.1638
---------------------
Validation seg loss: 0.22163571089611583 at epoch 357
epoch =    358/  1000, exp = train
train_iter_loss: 0.15349812805652618
train_iter_loss: 0.2273057997226715
train_iter_loss: 0.2211461067199707
train_iter_loss: 0.24127095937728882
train_iter_loss: 0.11118960380554199
train_iter_loss: 0.09008266776800156
train_iter_loss: 0.1500803679227829
train_iter_loss: 0.19496624171733856
train_iter_loss: 0.15190790593624115
train_iter_loss: 0.208338662981987
train_iter_loss: 0.10003471374511719
train_iter_loss: 0.15595577657222748
train_iter_loss: 0.14372801780700684
train_iter_loss: 0.2381913810968399
train_iter_loss: 0.2164037525653839
train_iter_loss: 0.13508643209934235
train_iter_loss: 0.23754844069480896
train_iter_loss: 0.14085990190505981
train_iter_loss: 0.1326814740896225
train_iter_loss: 0.17907772958278656
train_iter_loss: 0.14363880455493927
train_iter_loss: 0.23436850309371948
train_iter_loss: 0.1307702213525772
train_iter_loss: 0.12474449723958969
train_iter_loss: 0.17916077375411987
train_iter_loss: 0.0910000130534172
train_iter_loss: 0.1207881048321724
train_iter_loss: 0.17287881672382355
train_iter_loss: 0.13407866656780243
train_iter_loss: 0.16437801718711853
train_iter_loss: 0.06919028609991074
train_iter_loss: 0.10324957966804504
train_iter_loss: 0.12172755599021912
train_iter_loss: 0.1678323745727539
train_iter_loss: 0.21675792336463928
train_iter_loss: 0.11855079233646393
train_iter_loss: 0.1611068993806839
train_iter_loss: 0.17583072185516357
train_iter_loss: 0.19942735135555267
train_iter_loss: 0.1065976545214653
train_iter_loss: 0.12815254926681519
train_iter_loss: 0.15638689696788788
train_iter_loss: 0.15974779427051544
train_iter_loss: 0.06926137208938599
train_iter_loss: 0.20966003835201263
train_iter_loss: 0.1073571965098381
train_iter_loss: 0.06851740926504135
train_iter_loss: 0.22355496883392334
train_iter_loss: 0.16918696463108063
train_iter_loss: 0.2726723849773407
train_iter_loss: 0.18784300982952118
train_iter_loss: 0.15730012953281403
train_iter_loss: 0.1268933117389679
train_iter_loss: 0.19964143633842468
train_iter_loss: 0.15846674144268036
train_iter_loss: 0.15901854634284973
train_iter_loss: 0.19354116916656494
train_iter_loss: 0.11348157376050949
train_iter_loss: 0.19960950314998627
train_iter_loss: 0.12276175618171692
train_iter_loss: 0.1248452216386795
train_iter_loss: 0.08905496448278427
train_iter_loss: 0.12977208197116852
train_iter_loss: 0.1570781022310257
train_iter_loss: 0.12231340259313583
train_iter_loss: 0.19497065246105194
train_iter_loss: 0.17098906636238098
train_iter_loss: 0.1793466955423355
train_iter_loss: 0.4113461375236511
train_iter_loss: 0.26263147592544556
train_iter_loss: 0.12939690053462982
train_iter_loss: 0.1461717039346695
train_iter_loss: 0.25108012557029724
train_iter_loss: 0.12334615737199783
train_iter_loss: 0.08441773802042007
train_iter_loss: 0.17061595618724823
train_iter_loss: 0.17013859748840332
train_iter_loss: 0.265134334564209
train_iter_loss: 0.20311930775642395
train_iter_loss: 0.16328641772270203
train_iter_loss: 0.182550847530365
train_iter_loss: 0.136220321059227
train_iter_loss: 0.18684358894824982
train_iter_loss: 0.1467360109090805
train_iter_loss: 0.24627800285816193
train_iter_loss: 0.2167438119649887
train_iter_loss: 0.15805789828300476
train_iter_loss: 0.13626594841480255
train_iter_loss: 0.14054004848003387
train_iter_loss: 0.19468358159065247
train_iter_loss: 0.20700101554393768
train_iter_loss: 0.11865822225809097
train_iter_loss: 0.16197322309017181
train_iter_loss: 0.10929711163043976
train_iter_loss: 0.11864832788705826
train_iter_loss: 0.14456836879253387
train_iter_loss: 0.09257348626852036
train_iter_loss: 0.21148681640625
train_iter_loss: 0.11698589473962784
train_iter_loss: 0.13968104124069214
train loss :0.1643
---------------------
Validation seg loss: 0.21977628593526358 at epoch 358
epoch =    359/  1000, exp = train
train_iter_loss: 0.09246712923049927
train_iter_loss: 0.210073322057724
train_iter_loss: 0.1323598325252533
train_iter_loss: 0.0741237998008728
train_iter_loss: 0.11753888428211212
train_iter_loss: 0.15606090426445007
train_iter_loss: 0.13562701642513275
train_iter_loss: 0.15486761927604675
train_iter_loss: 0.12224549055099487
train_iter_loss: 0.11220446974039078
train_iter_loss: 0.2683334946632385
train_iter_loss: 0.2367977499961853
train_iter_loss: 0.13776718080043793
train_iter_loss: 0.14611682295799255
train_iter_loss: 0.08515439182519913
train_iter_loss: 0.15281324088573456
train_iter_loss: 0.08766603469848633
train_iter_loss: 0.2088218331336975
train_iter_loss: 0.11527086794376373
train_iter_loss: 0.19918806850910187
train_iter_loss: 0.15171951055526733
train_iter_loss: 0.1994701623916626
train_iter_loss: 0.15085072815418243
train_iter_loss: 0.394029438495636
train_iter_loss: 0.19293037056922913
train_iter_loss: 0.09056214988231659
train_iter_loss: 0.13982151448726654
train_iter_loss: 0.15934596955776215
train_iter_loss: 0.15558169782161713
train_iter_loss: 0.10272727161645889
train_iter_loss: 0.09185949712991714
train_iter_loss: 0.131071999669075
train_iter_loss: 0.20458857715129852
train_iter_loss: 0.34247612953186035
train_iter_loss: 0.09495329111814499
train_iter_loss: 0.23151546716690063
train_iter_loss: 0.17237266898155212
train_iter_loss: 0.17634694278240204
train_iter_loss: 0.09822343289852142
train_iter_loss: 0.11772817373275757
train_iter_loss: 0.14819936454296112
train_iter_loss: 0.08367329835891724
train_iter_loss: 0.14782792329788208
train_iter_loss: 0.19137081503868103
train_iter_loss: 0.0691772997379303
train_iter_loss: 0.16912853717803955
train_iter_loss: 0.1159205213189125
train_iter_loss: 0.14357779920101166
train_iter_loss: 0.09202034026384354
train_iter_loss: 0.13929738104343414
train_iter_loss: 0.22697746753692627
train_iter_loss: 0.18102866411209106
train_iter_loss: 0.32449546456336975
train_iter_loss: 0.1565520316362381
train_iter_loss: 0.1293010264635086
train_iter_loss: 0.1451372653245926
train_iter_loss: 0.062141064554452896
train_iter_loss: 0.07996024936437607
train_iter_loss: 0.20107820630073547
train_iter_loss: 0.33846035599708557
train_iter_loss: 0.18454308807849884
train_iter_loss: 0.16570396721363068
train_iter_loss: 0.1649898737668991
train_iter_loss: 0.21844781935214996
train_iter_loss: 0.11381757259368896
train_iter_loss: 0.10054231435060501
train_iter_loss: 0.1743858903646469
train_iter_loss: 0.08717629313468933
train_iter_loss: 0.15877383947372437
train_iter_loss: 0.2197609841823578
train_iter_loss: 0.2292822152376175
train_iter_loss: 0.12906883656978607
train_iter_loss: 0.13207505643367767
train_iter_loss: 0.13042160868644714
train_iter_loss: 0.1631496101617813
train_iter_loss: 0.06843169778585434
train_iter_loss: 0.2508486211299896
train_iter_loss: 0.3171096742153168
train_iter_loss: 0.17664681375026703
train_iter_loss: 0.14370952546596527
train_iter_loss: 0.15240909159183502
train_iter_loss: 0.13558733463287354
train_iter_loss: 0.24110963940620422
train_iter_loss: 0.10467660427093506
train_iter_loss: 0.0993974506855011
train_iter_loss: 0.16751161217689514
train_iter_loss: 0.2222510129213333
train_iter_loss: 0.1773233264684677
train_iter_loss: 0.13480429351329803
train_iter_loss: 0.18767587840557098
train_iter_loss: 0.10772084444761276
train_iter_loss: 0.10139308124780655
train_iter_loss: 0.20455357432365417
train_iter_loss: 0.2338417023420334
train_iter_loss: 0.33043742179870605
train_iter_loss: 0.25935959815979004
train_iter_loss: 0.3102729022502899
train_iter_loss: 0.1759011149406433
train_iter_loss: 0.18535153567790985
train_iter_loss: 0.09336750954389572
train loss :0.1661
---------------------
Validation seg loss: 0.2163726737951192 at epoch 359
********************
best_val_epoch_loss:  0.2163726737951192
MODEL UPDATED
epoch =    360/  1000, exp = train
train_iter_loss: 0.13784825801849365
train_iter_loss: 0.20931579172611237
train_iter_loss: 0.19326776266098022
train_iter_loss: 0.13621786236763
train_iter_loss: 0.06437720358371735
train_iter_loss: 0.13199742138385773
train_iter_loss: 0.07859569042921066
train_iter_loss: 0.11876441538333893
train_iter_loss: 0.16580215096473694
train_iter_loss: 0.15945932269096375
train_iter_loss: 0.2770325541496277
train_iter_loss: 0.12299127131700516
train_iter_loss: 0.12155114859342575
train_iter_loss: 0.11150360852479935
train_iter_loss: 0.20912720263004303
train_iter_loss: 0.17683982849121094
train_iter_loss: 0.21912115812301636
train_iter_loss: 0.16687500476837158
train_iter_loss: 0.09602373838424683
train_iter_loss: 0.17080283164978027
train_iter_loss: 0.2067994922399521
train_iter_loss: 0.10374969244003296
train_iter_loss: 0.18511319160461426
train_iter_loss: 0.11379314959049225
train_iter_loss: 0.2173590511083603
train_iter_loss: 0.16820508241653442
train_iter_loss: 0.251930832862854
train_iter_loss: 0.21439972519874573
train_iter_loss: 0.20345188677310944
train_iter_loss: 0.20853370428085327
train_iter_loss: 0.32589486241340637
train_iter_loss: 0.2206624448299408
train_iter_loss: 0.16888932883739471
train_iter_loss: 0.11758658289909363
train_iter_loss: 0.11841645836830139
train_iter_loss: 0.09713238477706909
train_iter_loss: 0.0768686905503273
train_iter_loss: 0.09331697970628738
train_iter_loss: 0.14609907567501068
train_iter_loss: 0.06015680357813835
train_iter_loss: 0.15647494792938232
train_iter_loss: 0.1413964033126831
train_iter_loss: 0.1326070874929428
train_iter_loss: 0.1176287904381752
train_iter_loss: 0.12458696216344833
train_iter_loss: 0.17339830100536346
train_iter_loss: 0.34157803654670715
train_iter_loss: 0.18607459962368011
train_iter_loss: 0.1095234751701355
train_iter_loss: 0.09496091306209564
train_iter_loss: 0.20030160248279572
train_iter_loss: 0.21739603579044342
train_iter_loss: 0.18851050734519958
train_iter_loss: 0.19421443343162537
train_iter_loss: 0.1155753955245018
train_iter_loss: 0.2012893706560135
train_iter_loss: 0.22040727734565735
train_iter_loss: 0.1774507761001587
train_iter_loss: 0.11708926409482956
train_iter_loss: 0.14529040455818176
train_iter_loss: 0.18954510986804962
train_iter_loss: 0.14001864194869995
train_iter_loss: 0.17081128060817719
train_iter_loss: 0.11952719837427139
train_iter_loss: 0.16403600573539734
train_iter_loss: 0.06489156931638718
train_iter_loss: 0.15105928480625153
train_iter_loss: 0.07469969242811203
train_iter_loss: 0.17188742756843567
train_iter_loss: 0.23598960041999817
train_iter_loss: 0.14242413640022278
train_iter_loss: 0.1981932669878006
train_iter_loss: 0.10123994946479797
train_iter_loss: 0.1851942241191864
train_iter_loss: 0.16536156833171844
train_iter_loss: 0.11276718229055405
train_iter_loss: 0.20168477296829224
train_iter_loss: 0.21690969169139862
train_iter_loss: 0.1591840535402298
train_iter_loss: 0.14753825962543488
train_iter_loss: 0.2041226178407669
train_iter_loss: 0.13372886180877686
train_iter_loss: 0.1810728758573532
train_iter_loss: 0.24757125973701477
train_iter_loss: 0.09039187431335449
train_iter_loss: 0.13451972603797913
train_iter_loss: 0.16319996118545532
train_iter_loss: 0.10483241081237793
train_iter_loss: 0.15867505967617035
train_iter_loss: 0.22105856239795685
train_iter_loss: 0.2997225821018219
train_iter_loss: 0.1972583383321762
train_iter_loss: 0.14145123958587646
train_iter_loss: 0.22143051028251648
train_iter_loss: 0.2095744013786316
train_iter_loss: 0.18888770043849945
train_iter_loss: 0.11816569417715073
train_iter_loss: 0.1408323049545288
train_iter_loss: 0.12691260874271393
train_iter_loss: 0.18123222887516022
train loss :0.1647
---------------------
Validation seg loss: 0.21961483597439133 at epoch 360
epoch =    361/  1000, exp = train
train_iter_loss: 0.17843833565711975
train_iter_loss: 0.14446362853050232
train_iter_loss: 0.15110057592391968
train_iter_loss: 0.12190897017717361
train_iter_loss: 0.2220257818698883
train_iter_loss: 0.1502470076084137
train_iter_loss: 0.12769034504890442
train_iter_loss: 0.28277042508125305
train_iter_loss: 0.09686309844255447
train_iter_loss: 0.18296635150909424
train_iter_loss: 0.2328946441411972
train_iter_loss: 0.19599738717079163
train_iter_loss: 0.18985766172409058
train_iter_loss: 0.17627571523189545
train_iter_loss: 0.2515207529067993
train_iter_loss: 0.4482545256614685
train_iter_loss: 0.14863578975200653
train_iter_loss: 0.15803037583827972
train_iter_loss: 0.09254787862300873
train_iter_loss: 0.10301726311445236
train_iter_loss: 0.10129734128713608
train_iter_loss: 0.2428438812494278
train_iter_loss: 0.05019218102097511
train_iter_loss: 0.117564857006073
train_iter_loss: 0.251390665769577
train_iter_loss: 0.05425497516989708
train_iter_loss: 0.0830429345369339
train_iter_loss: 0.2219577133655548
train_iter_loss: 0.1876339167356491
train_iter_loss: 0.16675430536270142
train_iter_loss: 0.18830862641334534
train_iter_loss: 0.21186448633670807
train_iter_loss: 0.2570543885231018
train_iter_loss: 0.12715329229831696
train_iter_loss: 0.1347806453704834
train_iter_loss: 0.17329354584217072
train_iter_loss: 0.17370590567588806
train_iter_loss: 0.2568467855453491
train_iter_loss: 0.2590753734111786
train_iter_loss: 0.10759055614471436
train_iter_loss: 0.16327965259552002
train_iter_loss: 0.16956789791584015
train_iter_loss: 0.13696527481079102
train_iter_loss: 0.10294511169195175
train_iter_loss: 0.10168404877185822
train_iter_loss: 0.2041783183813095
train_iter_loss: 0.11841098964214325
train_iter_loss: 0.08129768818616867
train_iter_loss: 0.2284637987613678
train_iter_loss: 0.26235267519950867
train_iter_loss: 0.1343591809272766
train_iter_loss: 0.1656966656446457
train_iter_loss: 0.09171324223279953
train_iter_loss: 0.189601868391037
train_iter_loss: 0.1271122545003891
train_iter_loss: 0.12990877032279968
train_iter_loss: 0.10053463280200958
train_iter_loss: 0.2983497381210327
train_iter_loss: 0.2565363645553589
train_iter_loss: 0.22000157833099365
train_iter_loss: 0.18022814393043518
train_iter_loss: 0.13144125044345856
train_iter_loss: 0.22834351658821106
train_iter_loss: 0.10742446780204773
train_iter_loss: 0.147968590259552
train_iter_loss: 0.11714954674243927
train_iter_loss: 0.2720794677734375
train_iter_loss: 0.1703135371208191
train_iter_loss: 0.1590120792388916
train_iter_loss: 0.19396236538887024
train_iter_loss: 0.0717136412858963
train_iter_loss: 0.0606708899140358
train_iter_loss: 0.10615590959787369
train_iter_loss: 0.06347160786390305
train_iter_loss: 0.13400596380233765
train_iter_loss: 0.1489681899547577
train_iter_loss: 0.24973872303962708
train_iter_loss: 0.13194122910499573
train_iter_loss: 0.10767129063606262
train_iter_loss: 0.2902465760707855
train_iter_loss: 0.13923729956150055
train_iter_loss: 0.12855251133441925
train_iter_loss: 0.1990540623664856
train_iter_loss: 0.1980564296245575
train_iter_loss: 0.1936068832874298
train_iter_loss: 0.15057238936424255
train_iter_loss: 0.12398956716060638
train_iter_loss: 0.11432607471942902
train_iter_loss: 0.07969310879707336
train_iter_loss: 0.11094126105308533
train_iter_loss: 0.22562934458255768
train_iter_loss: 0.16305668652057648
train_iter_loss: 0.19373787939548492
train_iter_loss: 0.13736490905284882
train_iter_loss: 0.17537741363048553
train_iter_loss: 0.1360914558172226
train_iter_loss: 0.18984732031822205
train_iter_loss: 0.03845709562301636
train_iter_loss: 0.10100401192903519
train_iter_loss: 0.2027665227651596
train loss :0.1654
---------------------
Validation seg loss: 0.2197228129689564 at epoch 361
epoch =    362/  1000, exp = train
train_iter_loss: 0.08964269608259201
train_iter_loss: 0.23118647933006287
train_iter_loss: 0.09053487330675125
train_iter_loss: 0.12011200189590454
train_iter_loss: 0.18501989543437958
train_iter_loss: 0.0986647978425026
train_iter_loss: 0.20151077210903168
train_iter_loss: 0.09703271090984344
train_iter_loss: 0.11274800449609756
train_iter_loss: 0.24519455432891846
train_iter_loss: 0.20449092984199524
train_iter_loss: 0.15623565018177032
train_iter_loss: 0.11401251703500748
train_iter_loss: 0.11962507665157318
train_iter_loss: 0.11081626266241074
train_iter_loss: 0.15075935423374176
train_iter_loss: 0.2384607344865799
train_iter_loss: 0.14790990948677063
train_iter_loss: 0.15557245910167694
train_iter_loss: 0.2739946246147156
train_iter_loss: 0.15256036818027496
train_iter_loss: 0.24277205765247345
train_iter_loss: 0.15451723337173462
train_iter_loss: 0.14627116918563843
train_iter_loss: 0.13425610959529877
train_iter_loss: 0.19491516053676605
train_iter_loss: 0.1499108225107193
train_iter_loss: 0.14658820629119873
train_iter_loss: 0.16957899928092957
train_iter_loss: 0.189146488904953
train_iter_loss: 0.28321102261543274
train_iter_loss: 0.07165747135877609
train_iter_loss: 0.33906665444374084
train_iter_loss: 0.1589319109916687
train_iter_loss: 0.0747397318482399
train_iter_loss: 0.22907166182994843
train_iter_loss: 0.16720789670944214
train_iter_loss: 0.14499172568321228
train_iter_loss: 0.10386186838150024
train_iter_loss: 0.10914493352174759
train_iter_loss: 0.10818060487508774
train_iter_loss: 0.0822344645857811
train_iter_loss: 0.1558075249195099
train_iter_loss: 0.13541018962860107
train_iter_loss: 0.17332929372787476
train_iter_loss: 0.161076158285141
train_iter_loss: 0.1740388572216034
train_iter_loss: 0.11208149045705795
train_iter_loss: 0.04910916090011597
train_iter_loss: 0.1835545152425766
train_iter_loss: 0.19507019221782684
train_iter_loss: 0.1451234072446823
train_iter_loss: 0.22390183806419373
train_iter_loss: 0.22694291174411774
train_iter_loss: 0.18695303797721863
train_iter_loss: 0.23851433396339417
train_iter_loss: 0.14872795343399048
train_iter_loss: 0.24009233713150024
train_iter_loss: 0.17261186242103577
train_iter_loss: 0.19152593612670898
train_iter_loss: 0.09013227373361588
train_iter_loss: 0.09590838104486465
train_iter_loss: 0.34143030643463135
train_iter_loss: 0.13579188287258148
train_iter_loss: 0.08962851017713547
train_iter_loss: 0.14097343385219574
train_iter_loss: 0.10120406001806259
train_iter_loss: 0.17418257892131805
train_iter_loss: 0.09918847680091858
train_iter_loss: 0.15120093524456024
train_iter_loss: 0.08429433405399323
train_iter_loss: 0.1261729896068573
train_iter_loss: 0.14060160517692566
train_iter_loss: 0.19577549397945404
train_iter_loss: 0.08773089945316315
train_iter_loss: 0.2026512324810028
train_iter_loss: 0.2718663811683655
train_iter_loss: 0.1665264517068863
train_iter_loss: 0.31327739357948303
train_iter_loss: 0.13497033715248108
train_iter_loss: 0.10249397158622742
train_iter_loss: 0.07801907509565353
train_iter_loss: 0.05750720947980881
train_iter_loss: 0.19050681591033936
train_iter_loss: 0.18555472791194916
train_iter_loss: 0.13407260179519653
train_iter_loss: 0.2521686851978302
train_iter_loss: 0.1269560009241104
train_iter_loss: 0.10733241587877274
train_iter_loss: 0.17072553932666779
train_iter_loss: 0.22571200132369995
train_iter_loss: 0.15542569756507874
train_iter_loss: 0.08800577372312546
train_iter_loss: 0.16358239948749542
train_iter_loss: 0.14453408122062683
train_iter_loss: 0.1728513538837433
train_iter_loss: 0.11537636071443558
train_iter_loss: 0.1459786295890808
train_iter_loss: 0.19652974605560303
train_iter_loss: 0.18147510290145874
train loss :0.1611
---------------------
Validation seg loss: 0.22071168902066518 at epoch 362
epoch =    363/  1000, exp = train
train_iter_loss: 0.13490916788578033
train_iter_loss: 0.31092381477355957
train_iter_loss: 0.13030783832073212
train_iter_loss: 0.20194318890571594
train_iter_loss: 0.10543937981128693
train_iter_loss: 0.21128548681735992
train_iter_loss: 0.20363228023052216
train_iter_loss: 0.20594912767410278
train_iter_loss: 0.24599288403987885
train_iter_loss: 0.19601036608219147
train_iter_loss: 0.13957317173480988
train_iter_loss: 0.1540423333644867
train_iter_loss: 0.1603461652994156
train_iter_loss: 0.11457362771034241
train_iter_loss: 0.19036352634429932
train_iter_loss: 0.23092502355575562
train_iter_loss: 0.0913008451461792
train_iter_loss: 0.16602081060409546
train_iter_loss: 0.18664371967315674
train_iter_loss: 0.11617625504732132
train_iter_loss: 0.040759216994047165
train_iter_loss: 0.18881885707378387
train_iter_loss: 0.17114073038101196
train_iter_loss: 0.1633913666009903
train_iter_loss: 0.10516972839832306
train_iter_loss: 0.2693960666656494
train_iter_loss: 0.14395223557949066
train_iter_loss: 0.1495404690504074
train_iter_loss: 0.15311667323112488
train_iter_loss: 0.05154342204332352
train_iter_loss: 0.1217660903930664
train_iter_loss: 0.18139499425888062
train_iter_loss: 0.2016480714082718
train_iter_loss: 0.0881296768784523
train_iter_loss: 0.17582596838474274
train_iter_loss: 0.16766224801540375
train_iter_loss: 0.22402843832969666
train_iter_loss: 0.15202216804027557
train_iter_loss: 0.1936105191707611
train_iter_loss: 0.17890432476997375
train_iter_loss: 0.2219237983226776
train_iter_loss: 0.17292249202728271
train_iter_loss: 0.13505899906158447
train_iter_loss: 0.14591889083385468
train_iter_loss: 0.11846927553415298
train_iter_loss: 0.19620998203754425
train_iter_loss: 0.15536876022815704
train_iter_loss: 0.1605505347251892
train_iter_loss: 0.13164137303829193
train_iter_loss: 0.10746041685342789
train_iter_loss: 0.2976457476615906
train_iter_loss: 0.2815016806125641
train_iter_loss: 0.27637726068496704
train_iter_loss: 0.1398306041955948
train_iter_loss: 0.17106497287750244
train_iter_loss: 0.19409234821796417
train_iter_loss: 0.3898365795612335
train_iter_loss: 0.15704329311847687
train_iter_loss: 0.23649650812149048
train_iter_loss: 0.20645906031131744
train_iter_loss: 0.10348818451166153
train_iter_loss: 0.22662563621997833
train_iter_loss: 0.15073516964912415
train_iter_loss: 0.1856899857521057
train_iter_loss: 0.1322055608034134
train_iter_loss: 0.10856961458921432
train_iter_loss: 0.15069355070590973
train_iter_loss: 0.216636061668396
train_iter_loss: 0.12056002020835876
train_iter_loss: 0.25974240899086
train_iter_loss: 0.10012152045965195
train_iter_loss: 0.21334075927734375
train_iter_loss: 0.15344975888729095
train_iter_loss: 0.10902108252048492
train_iter_loss: 0.15262103080749512
train_iter_loss: 0.14672234654426575
train_iter_loss: 0.18549780547618866
train_iter_loss: 0.10637447237968445
train_iter_loss: 0.06752993911504745
train_iter_loss: 0.1348118633031845
train_iter_loss: 0.11080565303564072
train_iter_loss: 0.25559401512145996
train_iter_loss: 0.0819065049290657
train_iter_loss: 0.15834391117095947
train_iter_loss: 0.26014140248298645
train_iter_loss: 0.13692575693130493
train_iter_loss: 0.11405905336141586
train_iter_loss: 0.11774871498346329
train_iter_loss: 0.10643111169338226
train_iter_loss: 0.13310691714286804
train_iter_loss: 0.19904042780399323
train_iter_loss: 0.09681256860494614
train_iter_loss: 0.09238862246274948
train_iter_loss: 0.10638375580310822
train_iter_loss: 0.21861504018306732
train_iter_loss: 0.14957210421562195
train_iter_loss: 0.1974724978208542
train_iter_loss: 0.1625717282295227
train_iter_loss: 0.20329508185386658
train_iter_loss: 0.10882294178009033
train loss :0.1671
---------------------
Validation seg loss: 0.22057214060376557 at epoch 363
epoch =    364/  1000, exp = train
train_iter_loss: 0.15674768388271332
train_iter_loss: 0.19283825159072876
train_iter_loss: 0.10945970565080643
train_iter_loss: 0.0963948592543602
train_iter_loss: 0.1795581877231598
train_iter_loss: 0.22030870616436005
train_iter_loss: 0.11968974769115448
train_iter_loss: 0.22785396873950958
train_iter_loss: 0.0961567685008049
train_iter_loss: 0.15267132222652435
train_iter_loss: 0.09972906112670898
train_iter_loss: 0.1885165274143219
train_iter_loss: 0.15631072223186493
train_iter_loss: 0.20529615879058838
train_iter_loss: 0.1156875491142273
train_iter_loss: 0.22552981972694397
train_iter_loss: 0.19396860897541046
train_iter_loss: 0.22455914318561554
train_iter_loss: 0.5840649604797363
train_iter_loss: 0.12767964601516724
train_iter_loss: 0.16022303700447083
train_iter_loss: 0.10398340970277786
train_iter_loss: 0.2358497977256775
train_iter_loss: 0.10805582255125046
train_iter_loss: 0.151152566075325
train_iter_loss: 0.2634885013103485
train_iter_loss: 0.1897197961807251
train_iter_loss: 0.12292981147766113
train_iter_loss: 0.08015917241573334
train_iter_loss: 0.26679548621177673
train_iter_loss: 0.25065383315086365
train_iter_loss: 0.295891135931015
train_iter_loss: 0.13112778961658478
train_iter_loss: 0.095136359333992
train_iter_loss: 0.11370496451854706
train_iter_loss: 0.15558277070522308
train_iter_loss: 0.10419297963380814
train_iter_loss: 0.12694087624549866
train_iter_loss: 0.1652059406042099
train_iter_loss: 0.18779559433460236
train_iter_loss: 0.21026241779327393
train_iter_loss: 0.18890409171581268
train_iter_loss: 0.26012879610061646
train_iter_loss: 0.20148132741451263
train_iter_loss: 0.21375250816345215
train_iter_loss: 0.0774984359741211
train_iter_loss: 0.19392597675323486
train_iter_loss: 0.15032045543193817
train_iter_loss: 0.07390644401311874
train_iter_loss: 0.10687029361724854
train_iter_loss: 0.16694535315036774
train_iter_loss: 0.15297214686870575
train_iter_loss: 0.1264912486076355
train_iter_loss: 0.06987858563661575
train_iter_loss: 0.1314389556646347
train_iter_loss: 0.1275998055934906
train_iter_loss: 0.09037704765796661
train_iter_loss: 0.17656832933425903
train_iter_loss: 0.27324962615966797
train_iter_loss: 0.20620541274547577
train_iter_loss: 0.20245936512947083
train_iter_loss: 0.167557030916214
train_iter_loss: 0.2144833654165268
train_iter_loss: 0.12123650312423706
train_iter_loss: 0.10423842072486877
train_iter_loss: 0.05747103691101074
train_iter_loss: 0.12777531147003174
train_iter_loss: 0.18209318816661835
train_iter_loss: 0.06517905741930008
train_iter_loss: 0.13235680758953094
train_iter_loss: 0.07155752182006836
train_iter_loss: 0.13823442161083221
train_iter_loss: 0.08764190971851349
train_iter_loss: 0.1340385526418686
train_iter_loss: 0.052786801010370255
train_iter_loss: 0.2172962874174118
train_iter_loss: 0.11645857989788055
train_iter_loss: 0.22022376954555511
train_iter_loss: 0.271476686000824
train_iter_loss: 0.10559079051017761
train_iter_loss: 0.22598430514335632
train_iter_loss: 0.21805857121944427
train_iter_loss: 0.1874917596578598
train_iter_loss: 0.17913910746574402
train_iter_loss: 0.1769605427980423
train_iter_loss: 0.20131494104862213
train_iter_loss: 0.16656097769737244
train_iter_loss: 0.19386371970176697
train_iter_loss: 0.1968226283788681
train_iter_loss: 0.1090860366821289
train_iter_loss: 0.07404864579439163
train_iter_loss: 0.11182747781276703
train_iter_loss: 0.16205139458179474
train_iter_loss: 0.11514415591955185
train_iter_loss: 0.14325428009033203
train_iter_loss: 0.19026651978492737
train_iter_loss: 0.14278258383274078
train_iter_loss: 0.1403968632221222
train_iter_loss: 0.16867369413375854
train_iter_loss: 0.05674368143081665
train loss :0.1630
---------------------
Validation seg loss: 0.2243584291306588 at epoch 364
epoch =    365/  1000, exp = train
train_iter_loss: 0.1529826819896698
train_iter_loss: 0.1317657232284546
train_iter_loss: 0.11231279373168945
train_iter_loss: 0.07812445610761642
train_iter_loss: 0.3719821274280548
train_iter_loss: 0.26254144310951233
train_iter_loss: 0.15931615233421326
train_iter_loss: 0.2223396599292755
train_iter_loss: 0.1714434176683426
train_iter_loss: 0.07631948590278625
train_iter_loss: 0.10276985913515091
train_iter_loss: 0.18065738677978516
train_iter_loss: 0.21276333928108215
train_iter_loss: 0.2283928394317627
train_iter_loss: 0.33437591791152954
train_iter_loss: 0.17488999664783478
train_iter_loss: 0.14038048684597015
train_iter_loss: 0.08781491219997406
train_iter_loss: 0.20600354671478271
train_iter_loss: 0.20769059658050537
train_iter_loss: 0.15255846083164215
train_iter_loss: 0.19476711750030518
train_iter_loss: 0.06811413913965225
train_iter_loss: 0.1313067525625229
train_iter_loss: 0.10149803012609482
train_iter_loss: 0.15093201398849487
train_iter_loss: 0.0976252406835556
train_iter_loss: 0.1086384505033493
train_iter_loss: 0.11165115982294083
train_iter_loss: 0.24294926226139069
train_iter_loss: 0.19338417053222656
train_iter_loss: 0.25362321734428406
train_iter_loss: 0.21216322481632233
train_iter_loss: 0.18344345688819885
train_iter_loss: 0.045800648629665375
train_iter_loss: 0.15119002759456635
train_iter_loss: 0.12407500296831131
train_iter_loss: 0.12205298244953156
train_iter_loss: 0.2437625378370285
train_iter_loss: 0.20254644751548767
train_iter_loss: 0.13635973632335663
train_iter_loss: 0.2067735493183136
train_iter_loss: 0.05346624180674553
train_iter_loss: 0.12586438655853271
train_iter_loss: 0.11127877980470657
train_iter_loss: 0.2407463937997818
train_iter_loss: 0.13562080264091492
train_iter_loss: 0.11866240203380585
train_iter_loss: 0.0858302041888237
train_iter_loss: 0.20462666451931
train_iter_loss: 0.17006359994411469
train_iter_loss: 0.09422264993190765
train_iter_loss: 0.1227208748459816
train_iter_loss: 0.14333204925060272
train_iter_loss: 0.10598050057888031
train_iter_loss: 0.18564371764659882
train_iter_loss: 0.07059407979249954
train_iter_loss: 0.2500092685222626
train_iter_loss: 0.14519156515598297
train_iter_loss: 0.185060515999794
train_iter_loss: 0.18253333866596222
train_iter_loss: 0.15556108951568604
train_iter_loss: 0.16774620115756989
train_iter_loss: 0.08868957310914993
train_iter_loss: 0.1280238926410675
train_iter_loss: 0.2772004008293152
train_iter_loss: 0.13953614234924316
train_iter_loss: 0.12412623316049576
train_iter_loss: 0.30500462651252747
train_iter_loss: 0.12423761188983917
train_iter_loss: 0.27689504623413086
train_iter_loss: 0.19820719957351685
train_iter_loss: 0.1544952392578125
train_iter_loss: 0.1508786678314209
train_iter_loss: 0.17319205403327942
train_iter_loss: 0.1401316225528717
train_iter_loss: 0.16636162996292114
train_iter_loss: 0.09140931069850922
train_iter_loss: 0.11690229177474976
train_iter_loss: 0.2052026391029358
train_iter_loss: 0.18358711898326874
train_iter_loss: 0.10632112622261047
train_iter_loss: 0.14898227155208588
train_iter_loss: 0.23355211317539215
train_iter_loss: 0.07883141189813614
train_iter_loss: 0.2232748568058014
train_iter_loss: 0.17112696170806885
train_iter_loss: 0.16973382234573364
train_iter_loss: 0.07170134037733078
train_iter_loss: 0.16223689913749695
train_iter_loss: 0.1748935878276825
train_iter_loss: 0.15362296998500824
train_iter_loss: 0.32159623503685
train_iter_loss: 0.16411922872066498
train_iter_loss: 0.1525065004825592
train_iter_loss: 0.21780836582183838
train_iter_loss: 0.07085741311311722
train_iter_loss: 0.08278379589319229
train_iter_loss: 0.15006792545318604
train_iter_loss: 0.09620501846075058
train loss :0.1626
---------------------
Validation seg loss: 0.21680244327224088 at epoch 365
epoch =    366/  1000, exp = train
train_iter_loss: 0.0907311737537384
train_iter_loss: 0.23205234110355377
train_iter_loss: 0.17967061698436737
train_iter_loss: 0.15511256456375122
train_iter_loss: 0.21272385120391846
train_iter_loss: 0.1584712266921997
train_iter_loss: 0.2935933470726013
train_iter_loss: 0.11038712412118912
train_iter_loss: 0.0959797352552414
train_iter_loss: 0.27786487340927124
train_iter_loss: 0.12540669739246368
train_iter_loss: 0.06980328261852264
train_iter_loss: 0.23365026712417603
train_iter_loss: 0.21023643016815186
train_iter_loss: 0.16100269556045532
train_iter_loss: 0.25471991300582886
train_iter_loss: 0.21545392274856567
train_iter_loss: 0.1486622840166092
train_iter_loss: 0.052611496299505234
train_iter_loss: 0.2106439769268036
train_iter_loss: 0.2137153148651123
train_iter_loss: 0.1590171605348587
train_iter_loss: 0.13599811494350433
train_iter_loss: 0.32567036151885986
train_iter_loss: 0.18716946244239807
train_iter_loss: 0.31200870871543884
train_iter_loss: 0.09818333387374878
train_iter_loss: 0.15458765625953674
train_iter_loss: 0.1556379497051239
train_iter_loss: 0.1774115264415741
train_iter_loss: 0.24347223341464996
train_iter_loss: 0.13551700115203857
train_iter_loss: 0.15267157554626465
train_iter_loss: 0.20425446331501007
train_iter_loss: 0.09875208884477615
train_iter_loss: 0.1280958354473114
train_iter_loss: 0.21177861094474792
train_iter_loss: 0.1704220473766327
train_iter_loss: 0.21541427075862885
train_iter_loss: 0.04921770840883255
train_iter_loss: 0.17342950403690338
train_iter_loss: 0.17465144395828247
train_iter_loss: 0.17248249053955078
train_iter_loss: 0.2031780183315277
train_iter_loss: 0.10803616791963577
train_iter_loss: 0.22443091869354248
train_iter_loss: 0.11253202706575394
train_iter_loss: 0.17388345301151276
train_iter_loss: 0.18700706958770752
train_iter_loss: 0.111337810754776
train_iter_loss: 0.11284290999174118
train_iter_loss: 0.06996039301156998
train_iter_loss: 0.1508118361234665
train_iter_loss: 0.11590991169214249
train_iter_loss: 0.1560305953025818
train_iter_loss: 0.17741218209266663
train_iter_loss: 0.1013561338186264
train_iter_loss: 0.06839660555124283
train_iter_loss: 0.13008156418800354
train_iter_loss: 0.11646465212106705
train_iter_loss: 0.07964120805263519
train_iter_loss: 0.28403544425964355
train_iter_loss: 0.12419810891151428
train_iter_loss: 0.1721426099538803
train_iter_loss: 0.14480555057525635
train_iter_loss: 0.21652349829673767
train_iter_loss: 0.19581802189350128
train_iter_loss: 0.17543399333953857
train_iter_loss: 0.10614906996488571
train_iter_loss: 0.13174104690551758
train_iter_loss: 0.25527212023735046
train_iter_loss: 0.19429431855678558
train_iter_loss: 0.09096533805131912
train_iter_loss: 0.14989127218723297
train_iter_loss: 0.17544396221637726
train_iter_loss: 0.1372622847557068
train_iter_loss: 0.1497495323419571
train_iter_loss: 0.16492821276187897
train_iter_loss: 0.18825678527355194
train_iter_loss: 0.15244369208812714
train_iter_loss: 0.12825042009353638
train_iter_loss: 0.0600152388215065
train_iter_loss: 0.2713647186756134
train_iter_loss: 0.08608412742614746
train_iter_loss: 0.19354723393917084
train_iter_loss: 0.05158681422472
train_iter_loss: 0.1150667741894722
train_iter_loss: 0.12921202182769775
train_iter_loss: 0.0789923369884491
train_iter_loss: 0.07780395448207855
train_iter_loss: 0.18669414520263672
train_iter_loss: 0.170517697930336
train_iter_loss: 0.19910642504692078
train_iter_loss: 0.28453847765922546
train_iter_loss: 0.08143620938062668
train_iter_loss: 0.09794993698596954
train_iter_loss: 0.17919187247753143
train_iter_loss: 0.13236352801322937
train_iter_loss: 0.07246200740337372
train_iter_loss: 0.22801952064037323
train loss :0.1613
---------------------
Validation seg loss: 0.2219618532964784 at epoch 366
epoch =    367/  1000, exp = train
train_iter_loss: 0.13566653430461884
train_iter_loss: 0.1523614227771759
train_iter_loss: 0.15259195864200592
train_iter_loss: 0.12218598276376724
train_iter_loss: 0.21237348020076752
train_iter_loss: 0.12619586288928986
train_iter_loss: 0.13310235738754272
train_iter_loss: 0.16833871603012085
train_iter_loss: 0.12340003997087479
train_iter_loss: 0.15827041864395142
train_iter_loss: 0.11373303830623627
train_iter_loss: 0.19428430497646332
train_iter_loss: 0.16560228168964386
train_iter_loss: 0.334401935338974
train_iter_loss: 0.2960490584373474
train_iter_loss: 0.15349362790584564
train_iter_loss: 0.16506408154964447
train_iter_loss: 0.10375308245420456
train_iter_loss: 0.16558168828487396
train_iter_loss: 0.08641517907381058
train_iter_loss: 0.3587484657764435
train_iter_loss: 0.1317688226699829
train_iter_loss: 0.08733930438756943
train_iter_loss: 0.06898194551467896
train_iter_loss: 0.14680582284927368
train_iter_loss: 0.17959176003932953
train_iter_loss: 0.07021662592887878
train_iter_loss: 0.1624082624912262
train_iter_loss: 0.15190307796001434
train_iter_loss: 0.16172650456428528
train_iter_loss: 0.44091975688934326
train_iter_loss: 0.11311259865760803
train_iter_loss: 0.1627972424030304
train_iter_loss: 0.17356222867965698
train_iter_loss: 0.12327831238508224
train_iter_loss: 0.14540374279022217
train_iter_loss: 0.13419824838638306
train_iter_loss: 0.07809977978467941
train_iter_loss: 0.12271695584058762
train_iter_loss: 0.07056456804275513
train_iter_loss: 0.11740675568580627
train_iter_loss: 0.20729368925094604
train_iter_loss: 0.19615517556667328
train_iter_loss: 0.1191638931632042
train_iter_loss: 0.23014143109321594
train_iter_loss: 0.25891807675361633
train_iter_loss: 0.14333069324493408
train_iter_loss: 0.13389021158218384
train_iter_loss: 0.13024157285690308
train_iter_loss: 0.10851258039474487
train_iter_loss: 0.16256816685199738
train_iter_loss: 0.15713633596897125
train_iter_loss: 0.16100309789180756
train_iter_loss: 0.16002848744392395
train_iter_loss: 0.19764859974384308
train_iter_loss: 0.2041076272726059
train_iter_loss: 0.1412457823753357
train_iter_loss: 0.04885819926857948
train_iter_loss: 0.18008749186992645
train_iter_loss: 0.15111370384693146
train_iter_loss: 0.13033340871334076
train_iter_loss: 0.22940567135810852
train_iter_loss: 0.21224625408649445
train_iter_loss: 0.18405742943286896
train_iter_loss: 0.11634580045938492
train_iter_loss: 0.2025577574968338
train_iter_loss: 0.13588373363018036
train_iter_loss: 0.15532584488391876
train_iter_loss: 0.24941489100456238
train_iter_loss: 0.22109223902225494
train_iter_loss: 0.12627403438091278
train_iter_loss: 0.24302634596824646
train_iter_loss: 0.16559363901615143
train_iter_loss: 0.25526079535484314
train_iter_loss: 0.16573093831539154
train_iter_loss: 0.198551207780838
train_iter_loss: 0.1263784021139145
train_iter_loss: 0.199014812707901
train_iter_loss: 0.1305433213710785
train_iter_loss: 0.1493089348077774
train_iter_loss: 0.09072821587324142
train_iter_loss: 0.12739720940589905
train_iter_loss: 0.16212736070156097
train_iter_loss: 0.1614728420972824
train_iter_loss: 0.048898544162511826
train_iter_loss: 0.08263209462165833
train_iter_loss: 0.0883140042424202
train_iter_loss: 0.15281705558300018
train_iter_loss: 0.12784157693386078
train_iter_loss: 0.4258231520652771
train_iter_loss: 0.18438024818897247
train_iter_loss: 0.2637404203414917
train_iter_loss: 0.15863341093063354
train_iter_loss: 0.11816515028476715
train_iter_loss: 0.1833779364824295
train_iter_loss: 0.12276536971330643
train_iter_loss: 0.1157500371336937
train_iter_loss: 0.16075399518013
train_iter_loss: 0.21486617624759674
train_iter_loss: 0.06439671665430069
train loss :0.1637
---------------------
Validation seg loss: 0.2217150572335945 at epoch 367
epoch =    368/  1000, exp = train
train_iter_loss: 0.1806645542383194
train_iter_loss: 0.12225548177957535
train_iter_loss: 0.23303408920764923
train_iter_loss: 0.10562977194786072
train_iter_loss: 0.14033502340316772
train_iter_loss: 0.09081535041332245
train_iter_loss: 0.10312816500663757
train_iter_loss: 0.208245187997818
train_iter_loss: 0.09825774282217026
train_iter_loss: 0.12862680852413177
train_iter_loss: 0.05879422277212143
train_iter_loss: 0.1420513391494751
train_iter_loss: 0.20532681047916412
train_iter_loss: 0.20645423233509064
train_iter_loss: 0.13497735559940338
train_iter_loss: 0.22143515944480896
train_iter_loss: 0.2141626924276352
train_iter_loss: 0.18088948726654053
train_iter_loss: 0.1170545145869255
train_iter_loss: 0.11138946563005447
train_iter_loss: 0.12100730836391449
train_iter_loss: 0.32643938064575195
train_iter_loss: 0.20989996194839478
train_iter_loss: 0.27533578872680664
train_iter_loss: 0.0969860851764679
train_iter_loss: 0.08974874764680862
train_iter_loss: 0.1775369793176651
train_iter_loss: 0.12661638855934143
train_iter_loss: 0.06443855166435242
train_iter_loss: 0.1399029940366745
train_iter_loss: 0.10627453029155731
train_iter_loss: 0.29693108797073364
train_iter_loss: 0.060391973704099655
train_iter_loss: 0.14805929362773895
train_iter_loss: 0.15089687705039978
train_iter_loss: 0.11333136260509491
train_iter_loss: 0.24693606793880463
train_iter_loss: 0.16575095057487488
train_iter_loss: 0.09398321807384491
train_iter_loss: 0.171711727976799
train_iter_loss: 0.13326863944530487
train_iter_loss: 0.0678812712430954
train_iter_loss: 0.11490140855312347
train_iter_loss: 0.18559177219867706
train_iter_loss: 0.21225544810295105
train_iter_loss: 0.1472632735967636
train_iter_loss: 0.28782135248184204
train_iter_loss: 0.08021973073482513
train_iter_loss: 0.13647201657295227
train_iter_loss: 0.1486506164073944
train_iter_loss: 0.18958333134651184
train_iter_loss: 0.16779646277427673
train_iter_loss: 0.10643812268972397
train_iter_loss: 0.16803547739982605
train_iter_loss: 0.07047705352306366
train_iter_loss: 0.11882086843252182
train_iter_loss: 0.1629919856786728
train_iter_loss: 0.04250089079141617
train_iter_loss: 0.16866563260555267
train_iter_loss: 0.43778571486473083
train_iter_loss: 0.30031976103782654
train_iter_loss: 0.14922639727592468
train_iter_loss: 0.1005319282412529
train_iter_loss: 0.2578275799751282
train_iter_loss: 0.3135449290275574
train_iter_loss: 0.19197620451450348
train_iter_loss: 0.19769613444805145
train_iter_loss: 0.11302726715803146
train_iter_loss: 0.24019943177700043
train_iter_loss: 0.18926924467086792
train_iter_loss: 0.20447105169296265
train_iter_loss: 0.16382423043251038
train_iter_loss: 0.350144624710083
train_iter_loss: 0.1582934856414795
train_iter_loss: 0.19684834778308868
train_iter_loss: 0.09341980516910553
train_iter_loss: 0.09485788643360138
train_iter_loss: 0.16325315833091736
train_iter_loss: 0.12330064922571182
train_iter_loss: 0.2696487605571747
train_iter_loss: 0.1685885339975357
train_iter_loss: 0.07852347940206528
train_iter_loss: 0.1731325089931488
train_iter_loss: 0.04330438748002052
train_iter_loss: 0.0777057483792305
train_iter_loss: 0.09522956609725952
train_iter_loss: 0.12858453392982483
train_iter_loss: 0.09535429626703262
train_iter_loss: 0.24375200271606445
train_iter_loss: 0.14647595584392548
train_iter_loss: 0.15091946721076965
train_iter_loss: 0.23558995127677917
train_iter_loss: 0.19432896375656128
train_iter_loss: 0.19412055611610413
train_iter_loss: 0.21570193767547607
train_iter_loss: 0.1630491316318512
train_iter_loss: 0.16827620565891266
train_iter_loss: 0.06854299455881119
train_iter_loss: 0.1884823441505432
train_iter_loss: 0.16932412981987
train loss :0.1637
---------------------
Validation seg loss: 0.22282811709380937 at epoch 368
epoch =    369/  1000, exp = train
train_iter_loss: 0.1585739254951477
train_iter_loss: 0.1269230991601944
train_iter_loss: 0.19024518132209778
train_iter_loss: 0.39305374026298523
train_iter_loss: 0.12976762652397156
train_iter_loss: 0.12456753849983215
train_iter_loss: 0.10403274744749069
train_iter_loss: 0.08828967809677124
train_iter_loss: 0.2142869234085083
train_iter_loss: 0.14437471330165863
train_iter_loss: 0.17489929497241974
train_iter_loss: 0.20950496196746826
train_iter_loss: 0.23108570277690887
train_iter_loss: 0.0796048492193222
train_iter_loss: 0.09810072928667068
train_iter_loss: 0.1520320326089859
train_iter_loss: 0.24779696762561798
train_iter_loss: 0.07381705194711685
train_iter_loss: 0.06698879599571228
train_iter_loss: 0.2371230125427246
train_iter_loss: 0.1514907032251358
train_iter_loss: 0.13558098673820496
train_iter_loss: 0.05689777061343193
train_iter_loss: 0.397821843624115
train_iter_loss: 0.3266017735004425
train_iter_loss: 0.18938109278678894
train_iter_loss: 0.14345373213291168
train_iter_loss: 0.12723979353904724
train_iter_loss: 0.17735520005226135
train_iter_loss: 0.14974084496498108
train_iter_loss: 0.29642701148986816
train_iter_loss: 0.12318267673254013
train_iter_loss: 0.13182254135608673
train_iter_loss: 0.29049378633499146
train_iter_loss: 0.1908874660730362
train_iter_loss: 0.1994563192129135
train_iter_loss: 0.12906287610530853
train_iter_loss: 0.11250150203704834
train_iter_loss: 0.1468351036310196
train_iter_loss: 0.299235075712204
train_iter_loss: 0.19562523066997528
train_iter_loss: 0.18627285957336426
train_iter_loss: 0.21995778381824493
train_iter_loss: 0.11664558202028275
train_iter_loss: 0.12396807968616486
train_iter_loss: 0.14217336475849152
train_iter_loss: 0.11229278892278671
train_iter_loss: 0.038542281836271286
train_iter_loss: 0.1712203472852707
train_iter_loss: 0.1797499656677246
train_iter_loss: 0.1611076146364212
train_iter_loss: 0.11189450323581696
train_iter_loss: 0.13069595396518707
train_iter_loss: 0.10978168994188309
train_iter_loss: 0.04751836508512497
train_iter_loss: 0.23099194467067719
train_iter_loss: 0.12517580389976501
train_iter_loss: 0.07162132114171982
train_iter_loss: 0.09022629261016846
train_iter_loss: 0.1200365275144577
train_iter_loss: 0.060106806457042694
train_iter_loss: 0.21140295267105103
train_iter_loss: 0.07669861614704132
train_iter_loss: 0.2363595962524414
train_iter_loss: 0.09497679769992828
train_iter_loss: 0.21421211957931519
train_iter_loss: 0.17109966278076172
train_iter_loss: 0.1533137857913971
train_iter_loss: 0.13239966332912445
train_iter_loss: 0.16635702550411224
train_iter_loss: 0.20048682391643524
train_iter_loss: 0.18267205357551575
train_iter_loss: 0.14136329293251038
train_iter_loss: 0.1976742297410965
train_iter_loss: 0.10711653530597687
train_iter_loss: 0.1639024168252945
train_iter_loss: 0.14443151652812958
train_iter_loss: 0.17257601022720337
train_iter_loss: 0.11743836849927902
train_iter_loss: 0.12360218167304993
train_iter_loss: 0.1621997058391571
train_iter_loss: 0.12946230173110962
train_iter_loss: 0.2036878615617752
train_iter_loss: 0.16823221743106842
train_iter_loss: 0.16053490340709686
train_iter_loss: 0.10427562147378922
train_iter_loss: 0.12476040422916412
train_iter_loss: 0.1757729947566986
train_iter_loss: 0.16721153259277344
train_iter_loss: 0.09733006358146667
train_iter_loss: 0.16730868816375732
train_iter_loss: 0.23872241377830505
train_iter_loss: 0.13385576009750366
train_iter_loss: 0.16406652331352234
train_iter_loss: 0.140527606010437
train_iter_loss: 0.2774743139743805
train_iter_loss: 0.13373994827270508
train_iter_loss: 0.14314775168895721
train_iter_loss: 0.2087852656841278
train_iter_loss: 0.2289082258939743
train loss :0.1627
---------------------
Validation seg loss: 0.22139863902702928 at epoch 369
epoch =    370/  1000, exp = train
train_iter_loss: 0.21745418012142181
train_iter_loss: 0.1256973296403885
train_iter_loss: 0.1824553906917572
train_iter_loss: 0.1464221477508545
train_iter_loss: 0.1971082091331482
train_iter_loss: 0.2217842936515808
train_iter_loss: 0.08593162149190903
train_iter_loss: 0.05386542156338692
train_iter_loss: 0.22565382719039917
train_iter_loss: 0.06465823948383331
train_iter_loss: 0.28219783306121826
train_iter_loss: 0.13339661061763763
train_iter_loss: 0.07779928296804428
train_iter_loss: 0.14907900989055634
train_iter_loss: 0.28038573265075684
train_iter_loss: 0.20215676724910736
train_iter_loss: 0.19845855236053467
train_iter_loss: 0.18918868899345398
train_iter_loss: 0.18106041848659515
train_iter_loss: 0.143995463848114
train_iter_loss: 0.10585711151361465
train_iter_loss: 0.14827267825603485
train_iter_loss: 0.2058127522468567
train_iter_loss: 0.2544063627719879
train_iter_loss: 0.20195534825325012
train_iter_loss: 0.21779413521289825
train_iter_loss: 0.12839514017105103
train_iter_loss: 0.11377223581075668
train_iter_loss: 0.1558142900466919
train_iter_loss: 0.07568659633398056
train_iter_loss: 0.2874584197998047
train_iter_loss: 0.1300705522298813
train_iter_loss: 0.21788254380226135
train_iter_loss: 0.2140544354915619
train_iter_loss: 0.11243068426847458
train_iter_loss: 0.08732668310403824
train_iter_loss: 0.13298282027244568
train_iter_loss: 0.17928344011306763
train_iter_loss: 0.16346320509910583
train_iter_loss: 0.11324721574783325
train_iter_loss: 0.05636593699455261
train_iter_loss: 0.17014923691749573
train_iter_loss: 0.1265459507703781
train_iter_loss: 0.12301069498062134
train_iter_loss: 0.30773621797561646
train_iter_loss: 0.13973329961299896
train_iter_loss: 0.28984567523002625
train_iter_loss: 0.11377139389514923
train_iter_loss: 0.07258671522140503
train_iter_loss: 0.1892092525959015
train_iter_loss: 0.17999082803726196
train_iter_loss: 0.19106987118721008
train_iter_loss: 0.14266245067119598
train_iter_loss: 0.1573236584663391
train_iter_loss: 0.16441065073013306
train_iter_loss: 0.2390683889389038
train_iter_loss: 0.17460714280605316
train_iter_loss: 0.09370969235897064
train_iter_loss: 0.1644057184457779
train_iter_loss: 0.37575384974479675
train_iter_loss: 0.251985639333725
train_iter_loss: 0.1324719339609146
train_iter_loss: 0.13602212071418762
train_iter_loss: 0.18768653273582458
train_iter_loss: 0.20675763487815857
train_iter_loss: 0.2598990797996521
train_iter_loss: 0.17285892367362976
train_iter_loss: 0.07342702150344849
train_iter_loss: 0.14580892026424408
train_iter_loss: 0.17431145906448364
train_iter_loss: 0.09177803248167038
train_iter_loss: 0.12340069562196732
train_iter_loss: 0.12624964118003845
train_iter_loss: 0.1589420586824417
train_iter_loss: 0.12843649089336395
train_iter_loss: 0.2504759132862091
train_iter_loss: 0.16760069131851196
train_iter_loss: 0.1262597292661667
train_iter_loss: 0.12757274508476257
train_iter_loss: 0.18342943489551544
train_iter_loss: 0.10476800054311752
train_iter_loss: 0.21293827891349792
train_iter_loss: 0.21879172325134277
train_iter_loss: 0.14633682370185852
train_iter_loss: 0.11409923434257507
train_iter_loss: 0.14163272082805634
train_iter_loss: 0.06684546917676926
train_iter_loss: 0.1745007038116455
train_iter_loss: 0.1131841316819191
train_iter_loss: 0.13352105021476746
train_iter_loss: 0.17954619228839874
train_iter_loss: 0.08609779179096222
train_iter_loss: 0.14167945086956024
train_iter_loss: 0.08802220225334167
train_iter_loss: 0.12346457690000534
train_iter_loss: 0.19266965985298157
train_iter_loss: 0.32778963446617126
train_iter_loss: 0.13484390079975128
train_iter_loss: 0.16598647832870483
train_iter_loss: 0.14054042100906372
train loss :0.1647
---------------------
Validation seg loss: 0.2174985908534167 at epoch 370
epoch =    371/  1000, exp = train
train_iter_loss: 0.16966891288757324
train_iter_loss: 0.1402694284915924
train_iter_loss: 0.14067275822162628
train_iter_loss: 0.11648779362440109
train_iter_loss: 0.3187892436981201
train_iter_loss: 0.1774360090494156
train_iter_loss: 0.0696568414568901
train_iter_loss: 0.14810094237327576
train_iter_loss: 0.16049820184707642
train_iter_loss: 0.22169595956802368
train_iter_loss: 0.15501432120800018
train_iter_loss: 0.10018692165613174
train_iter_loss: 0.14662092924118042
train_iter_loss: 0.11633340269327164
train_iter_loss: 0.09046228229999542
train_iter_loss: 0.08292803913354874
train_iter_loss: 0.12141568213701248
train_iter_loss: 0.12149769812822342
train_iter_loss: 0.09944949299097061
train_iter_loss: 0.21463999152183533
train_iter_loss: 0.18962867558002472
train_iter_loss: 0.21333563327789307
train_iter_loss: 0.11012698709964752
train_iter_loss: 0.08215812593698502
train_iter_loss: 0.27250057458877563
train_iter_loss: 0.17071685194969177
train_iter_loss: 0.1796300709247589
train_iter_loss: 0.1693856120109558
train_iter_loss: 0.13094791769981384
train_iter_loss: 0.23528069257736206
train_iter_loss: 0.22079907357692719
train_iter_loss: 0.1459151804447174
train_iter_loss: 0.18517591059207916
train_iter_loss: 0.1276831030845642
train_iter_loss: 0.12480191886425018
train_iter_loss: 0.17880181968212128
train_iter_loss: 0.16644608974456787
train_iter_loss: 0.08361165225505829
train_iter_loss: 0.0862203910946846
train_iter_loss: 0.10756198316812515
train_iter_loss: 0.31771013140678406
train_iter_loss: 0.11286633461713791
train_iter_loss: 0.1330176293849945
train_iter_loss: 0.19861222803592682
train_iter_loss: 0.09271664917469025
train_iter_loss: 0.17873552441596985
train_iter_loss: 0.2000458687543869
train_iter_loss: 0.21260157227516174
train_iter_loss: 0.16570907831192017
train_iter_loss: 0.14072875678539276
train_iter_loss: 0.09923458844423294
train_iter_loss: 0.1247391551733017
train_iter_loss: 0.1246148943901062
train_iter_loss: 0.3127140998840332
train_iter_loss: 0.3228244483470917
train_iter_loss: 0.2331148236989975
train_iter_loss: 0.15175066888332367
train_iter_loss: 0.16874530911445618
train_iter_loss: 0.24995514750480652
train_iter_loss: 0.16004624962806702
train_iter_loss: 0.1076168641448021
train_iter_loss: 0.1812974363565445
train_iter_loss: 0.16335922479629517
train_iter_loss: 0.1368461549282074
train_iter_loss: 0.16501569747924805
train_iter_loss: 0.3562678396701813
train_iter_loss: 0.1856001913547516
train_iter_loss: 0.1530660092830658
train_iter_loss: 0.16823507845401764
train_iter_loss: 0.15269765257835388
train_iter_loss: 0.12565931677818298
train_iter_loss: 0.13467095792293549
train_iter_loss: 0.13099516928195953
train_iter_loss: 0.07562737911939621
train_iter_loss: 0.18008831143379211
train_iter_loss: 0.16412028670310974
train_iter_loss: 0.09881545603275299
train_iter_loss: 0.15260078012943268
train_iter_loss: 0.1667201668024063
train_iter_loss: 0.1374717354774475
train_iter_loss: 0.1486847847700119
train_iter_loss: 0.31875520944595337
train_iter_loss: 0.18009011447429657
train_iter_loss: 0.09535093605518341
train_iter_loss: 0.07475546002388
train_iter_loss: 0.13764497637748718
train_iter_loss: 0.18769751489162445
train_iter_loss: 0.1624523252248764
train_iter_loss: 0.40576261281967163
train_iter_loss: 0.21049953997135162
train_iter_loss: 0.22130729258060455
train_iter_loss: 0.18014048039913177
train_iter_loss: 0.12481257319450378
train_iter_loss: 0.10888432711362839
train_iter_loss: 0.2034827172756195
train_iter_loss: 0.17322157323360443
train_iter_loss: 0.19834081828594208
train_iter_loss: 0.0668359324336052
train_iter_loss: 0.13651537895202637
train_iter_loss: 0.13198800384998322
train loss :0.1656
---------------------
Validation seg loss: 0.2176818900899786 at epoch 371
epoch =    372/  1000, exp = train
train_iter_loss: 0.08951107412576675
train_iter_loss: 0.19164766371250153
train_iter_loss: 0.13895703852176666
train_iter_loss: 0.168692484498024
train_iter_loss: 0.09664338082075119
train_iter_loss: 0.08520404994487762
train_iter_loss: 0.0534493587911129
train_iter_loss: 0.08994138240814209
train_iter_loss: 0.09885907918214798
train_iter_loss: 0.14866700768470764
train_iter_loss: 0.13176539540290833
train_iter_loss: 0.18170759081840515
train_iter_loss: 0.11935450881719589
train_iter_loss: 0.17583277821540833
train_iter_loss: 0.11874909698963165
train_iter_loss: 0.16379337012767792
train_iter_loss: 0.25265392661094666
train_iter_loss: 0.30893954634666443
train_iter_loss: 0.19052058458328247
train_iter_loss: 0.17784811556339264
train_iter_loss: 0.0874585509300232
train_iter_loss: 0.08687906712293625
train_iter_loss: 0.3709905445575714
train_iter_loss: 0.12368974834680557
train_iter_loss: 0.13728009164333344
train_iter_loss: 0.19032752513885498
train_iter_loss: 0.09290722757577896
train_iter_loss: 0.11960690468549728
train_iter_loss: 0.15824130177497864
train_iter_loss: 0.10686487704515457
train_iter_loss: 0.1643376499414444
train_iter_loss: 0.09836685657501221
train_iter_loss: 0.15761196613311768
train_iter_loss: 0.13209852576255798
train_iter_loss: 0.1577904373407364
train_iter_loss: 0.20268550515174866
train_iter_loss: 0.0929730162024498
train_iter_loss: 0.21802382171154022
train_iter_loss: 0.10031293332576752
train_iter_loss: 0.18214072287082672
train_iter_loss: 0.20495949685573578
train_iter_loss: 0.05036124959588051
train_iter_loss: 0.2753167152404785
train_iter_loss: 0.19114074110984802
train_iter_loss: 0.17144598066806793
train_iter_loss: 0.2474367469549179
train_iter_loss: 0.17345395684242249
train_iter_loss: 0.34936636686325073
train_iter_loss: 0.27653688192367554
train_iter_loss: 0.14919736981391907
train_iter_loss: 0.3137691617012024
train_iter_loss: 0.07096601277589798
train_iter_loss: 0.22852547466754913
train_iter_loss: 0.11165362596511841
train_iter_loss: 0.21191993355751038
train_iter_loss: 0.2361965924501419
train_iter_loss: 0.21622461080551147
train_iter_loss: 0.20558954775333405
train_iter_loss: 0.12728162109851837
train_iter_loss: 0.17556646466255188
train_iter_loss: 0.12130730599164963
train_iter_loss: 0.37729015946388245
train_iter_loss: 0.07962331920862198
train_iter_loss: 0.1292962282896042
train_iter_loss: 0.09987012296915054
train_iter_loss: 0.21124790608882904
train_iter_loss: 0.15954050421714783
train_iter_loss: 0.13670699298381805
train_iter_loss: 0.23839549720287323
train_iter_loss: 0.14802148938179016
train_iter_loss: 0.11007127910852432
train_iter_loss: 0.21186687052249908
train_iter_loss: 0.09610936790704727
train_iter_loss: 0.22690071165561676
train_iter_loss: 0.12860387563705444
train_iter_loss: 0.13948191702365875
train_iter_loss: 0.14941631257534027
train_iter_loss: 0.11722926050424576
train_iter_loss: 0.07492685317993164
train_iter_loss: 0.1841372847557068
train_iter_loss: 0.17647460103034973
train_iter_loss: 0.1644470989704132
train_iter_loss: 0.24390004575252533
train_iter_loss: 0.10163746029138565
train_iter_loss: 0.1896589696407318
train_iter_loss: 0.18325352668762207
train_iter_loss: 0.13773618638515472
train_iter_loss: 0.21503183245658875
train_iter_loss: 0.23427964746952057
train_iter_loss: 0.10917084664106369
train_iter_loss: 0.30282852053642273
train_iter_loss: 0.18609125912189484
train_iter_loss: 0.11674853414297104
train_iter_loss: 0.16654033958911896
train_iter_loss: 0.10855549573898315
train_iter_loss: 0.22839292883872986
train_iter_loss: 0.17144477367401123
train_iter_loss: 0.1155233159661293
train_iter_loss: 0.055279798805713654
train_iter_loss: 0.1223226934671402
train loss :0.1659
---------------------
Validation seg loss: 0.224927021884623 at epoch 372
epoch =    373/  1000, exp = train
train_iter_loss: 0.175939679145813
train_iter_loss: 0.12974083423614502
train_iter_loss: 0.07431818544864655
train_iter_loss: 0.06268871575593948
train_iter_loss: 0.1653052419424057
train_iter_loss: 0.1870882660150528
train_iter_loss: 0.12241978943347931
train_iter_loss: 0.13678070902824402
train_iter_loss: 0.13833564519882202
train_iter_loss: 0.11513534933328629
train_iter_loss: 0.20569822192192078
train_iter_loss: 0.10742902010679245
train_iter_loss: 0.1783052235841751
train_iter_loss: 0.27169397473335266
train_iter_loss: 0.10603660345077515
train_iter_loss: 0.12187997996807098
train_iter_loss: 0.16954487562179565
train_iter_loss: 0.2538582682609558
train_iter_loss: 0.09924490749835968
train_iter_loss: 0.14013461768627167
train_iter_loss: 0.12591449916362762
train_iter_loss: 0.25208067893981934
train_iter_loss: 0.12014846503734589
train_iter_loss: 0.15896359086036682
train_iter_loss: 0.17782138288021088
train_iter_loss: 0.19511030614376068
train_iter_loss: 0.10187491029500961
train_iter_loss: 0.15663984417915344
train_iter_loss: 0.18291816115379333
train_iter_loss: 0.15712858736515045
train_iter_loss: 0.09829407185316086
train_iter_loss: 0.11378892511129379
train_iter_loss: 0.059080783277750015
train_iter_loss: 0.28091251850128174
train_iter_loss: 0.18703456223011017
train_iter_loss: 0.12871067225933075
train_iter_loss: 0.1531052589416504
train_iter_loss: 0.1581503301858902
train_iter_loss: 0.1390722543001175
train_iter_loss: 0.07280926406383514
train_iter_loss: 0.15517883002758026
train_iter_loss: 0.10919494926929474
train_iter_loss: 0.23731808364391327
train_iter_loss: 0.19772927463054657
train_iter_loss: 0.11190232634544373
train_iter_loss: 0.29145514965057373
train_iter_loss: 0.23484684526920319
train_iter_loss: 0.11523529887199402
train_iter_loss: 0.23852281272411346
train_iter_loss: 0.1527833193540573
train_iter_loss: 0.3088832497596741
train_iter_loss: 0.15565824508666992
train_iter_loss: 0.26688501238822937
train_iter_loss: 0.25364190340042114
train_iter_loss: 0.17211675643920898
train_iter_loss: 0.18079997599124908
train_iter_loss: 0.28247103095054626
train_iter_loss: 0.21123386919498444
train_iter_loss: 0.16874471306800842
train_iter_loss: 0.111720509827137
train_iter_loss: 0.11778834462165833
train_iter_loss: 0.07253286242485046
train_iter_loss: 0.09305669367313385
train_iter_loss: 0.1548539400100708
train_iter_loss: 0.21423819661140442
train_iter_loss: 0.044549744576215744
train_iter_loss: 0.15319706499576569
train_iter_loss: 0.16670270264148712
train_iter_loss: 0.06873150914907455
train_iter_loss: 0.08613232523202896
train_iter_loss: 0.15985508263111115
train_iter_loss: 0.144579216837883
train_iter_loss: 0.19147053360939026
train_iter_loss: 0.07214733213186264
train_iter_loss: 0.1583964228630066
train_iter_loss: 0.20368309319019318
train_iter_loss: 0.1787121444940567
train_iter_loss: 0.16207081079483032
train_iter_loss: 0.10158900171518326
train_iter_loss: 0.2128564417362213
train_iter_loss: 0.0950780063867569
train_iter_loss: 0.2022825926542282
train_iter_loss: 0.09687548130750656
train_iter_loss: 0.14361616969108582
train_iter_loss: 0.1577824056148529
train_iter_loss: 0.10745539516210556
train_iter_loss: 0.22981685400009155
train_iter_loss: 0.11554572731256485
train_iter_loss: 0.3858339488506317
train_iter_loss: 0.11130019277334213
train_iter_loss: 0.1256735920906067
train_iter_loss: 0.11941934376955032
train_iter_loss: 0.12412522733211517
train_iter_loss: 0.12989765405654907
train_iter_loss: 0.18722787499427795
train_iter_loss: 0.2650960087776184
train_iter_loss: 0.06947176903486252
train_iter_loss: 0.25979742407798767
train_iter_loss: 0.14106585085391998
train_iter_loss: 0.19325818121433258
train loss :0.1612
---------------------
Validation seg loss: 0.22442706843908383 at epoch 373
epoch =    374/  1000, exp = train
train_iter_loss: 0.14881016314029694
train_iter_loss: 0.15876537561416626
train_iter_loss: 0.15296441316604614
train_iter_loss: 0.20218349993228912
train_iter_loss: 0.13719205558300018
train_iter_loss: 0.30062034726142883
train_iter_loss: 0.20704849064350128
train_iter_loss: 0.3221109211444855
train_iter_loss: 0.11819202452898026
train_iter_loss: 0.15587805211544037
train_iter_loss: 0.12572216987609863
train_iter_loss: 0.22834309935569763
train_iter_loss: 0.20797359943389893
train_iter_loss: 0.19664479792118073
train_iter_loss: 0.13732269406318665
train_iter_loss: 0.08635634183883667
train_iter_loss: 0.11834000051021576
train_iter_loss: 0.0575413778424263
train_iter_loss: 0.2163640260696411
train_iter_loss: 0.13522522151470184
train_iter_loss: 0.14369283616542816
train_iter_loss: 0.12877073884010315
train_iter_loss: 0.19175457954406738
train_iter_loss: 0.07340236008167267
train_iter_loss: 0.09042026102542877
train_iter_loss: 0.26902323961257935
train_iter_loss: 0.14555400609970093
train_iter_loss: 0.13731959462165833
train_iter_loss: 0.1637028157711029
train_iter_loss: 0.10527023673057556
train_iter_loss: 0.2122080773115158
train_iter_loss: 0.11703455448150635
train_iter_loss: 0.10785819590091705
train_iter_loss: 0.09039399027824402
train_iter_loss: 0.12043717503547668
train_iter_loss: 0.11910267919301987
train_iter_loss: 0.1310088187456131
train_iter_loss: 0.09183292835950851
train_iter_loss: 0.09759517014026642
train_iter_loss: 0.10745827108621597
train_iter_loss: 0.1622384935617447
train_iter_loss: 0.17376695573329926
train_iter_loss: 0.09859073162078857
train_iter_loss: 0.4717312753200531
train_iter_loss: 0.14744728803634644
train_iter_loss: 0.13712309300899506
train_iter_loss: 0.13404197990894318
train_iter_loss: 0.23264889419078827
train_iter_loss: 0.09983741492033005
train_iter_loss: 0.17674170434474945
train_iter_loss: 0.15358661115169525
train_iter_loss: 0.10828331112861633
train_iter_loss: 0.11322023719549179
train_iter_loss: 0.18465514481067657
train_iter_loss: 0.034904662519693375
train_iter_loss: 0.16428886353969574
train_iter_loss: 0.12970899045467377
train_iter_loss: 0.15743155777454376
train_iter_loss: 0.20180006325244904
train_iter_loss: 0.1560974419116974
train_iter_loss: 0.07175150513648987
train_iter_loss: 0.18039213120937347
train_iter_loss: 0.24492357671260834
train_iter_loss: 0.17173130810260773
train_iter_loss: 0.13339214026927948
train_iter_loss: 0.2908127009868622
train_iter_loss: 0.21046872437000275
train_iter_loss: 0.15493933856487274
train_iter_loss: 0.2651718556880951
train_iter_loss: 0.16513213515281677
train_iter_loss: 0.32723841071128845
train_iter_loss: 0.09565529972314835
train_iter_loss: 0.2604556977748871
train_iter_loss: 0.11579472571611404
train_iter_loss: 0.06246408075094223
train_iter_loss: 0.11529656499624252
train_iter_loss: 0.27204304933547974
train_iter_loss: 0.19605255126953125
train_iter_loss: 0.1437423974275589
train_iter_loss: 0.3439773917198181
train_iter_loss: 0.403018981218338
train_iter_loss: 0.17401641607284546
train_iter_loss: 0.32609546184539795
train_iter_loss: 0.22240795195102692
train_iter_loss: 0.1257093995809555
train_iter_loss: 0.08661569654941559
train_iter_loss: 0.09195715934038162
train_iter_loss: 0.12679898738861084
train_iter_loss: 0.3075299859046936
train_iter_loss: 0.18785935640335083
train_iter_loss: 0.13160023093223572
train_iter_loss: 0.18481215834617615
train_iter_loss: 0.11246184259653091
train_iter_loss: 0.11014747619628906
train_iter_loss: 0.1686663180589676
train_iter_loss: 0.2417965680360794
train_iter_loss: 0.23793010413646698
train_iter_loss: 0.13830748200416565
train_iter_loss: 0.08199989050626755
train_iter_loss: 0.15624913573265076
train loss :0.1689
---------------------
Validation seg loss: 0.22067658488093964 at epoch 374
epoch =    375/  1000, exp = train
train_iter_loss: 0.11596573889255524
train_iter_loss: 0.20744477212429047
train_iter_loss: 0.18513767421245575
train_iter_loss: 0.1693873554468155
train_iter_loss: 0.07149683684110641
train_iter_loss: 0.12566684186458588
train_iter_loss: 0.11471368372440338
train_iter_loss: 0.10769576579332352
train_iter_loss: 0.11575626581907272
train_iter_loss: 0.10060842335224152
train_iter_loss: 0.23358964920043945
train_iter_loss: 0.2556960880756378
train_iter_loss: 0.2473931461572647
train_iter_loss: 0.12042319774627686
train_iter_loss: 0.05288402736186981
train_iter_loss: 0.1258249133825302
train_iter_loss: 0.23157788813114166
train_iter_loss: 0.061423514038324356
train_iter_loss: 0.16877621412277222
train_iter_loss: 0.16967278718948364
train_iter_loss: 0.11285077780485153
train_iter_loss: 0.16505293548107147
train_iter_loss: 0.1404009610414505
train_iter_loss: 0.07432820647954941
train_iter_loss: 0.08531069755554199
train_iter_loss: 0.1669933944940567
train_iter_loss: 0.17594397068023682
train_iter_loss: 0.17404325306415558
train_iter_loss: 0.08888110518455505
train_iter_loss: 0.20762670040130615
train_iter_loss: 0.23984922468662262
train_iter_loss: 0.1659672111272812
train_iter_loss: 0.2248559445142746
train_iter_loss: 0.08724112808704376
train_iter_loss: 0.11683984845876694
train_iter_loss: 0.1292887032032013
train_iter_loss: 0.13314424455165863
train_iter_loss: 0.15032659471035004
train_iter_loss: 0.14453041553497314
train_iter_loss: 0.17316143214702606
train_iter_loss: 0.13443049788475037
train_iter_loss: 0.2297607809305191
train_iter_loss: 0.11655288189649582
train_iter_loss: 0.13270913064479828
train_iter_loss: 0.23486970365047455
train_iter_loss: 0.14018280804157257
train_iter_loss: 0.23261480033397675
train_iter_loss: 0.10954444855451584
train_iter_loss: 0.1541372388601303
train_iter_loss: 0.22241730988025665
train_iter_loss: 0.11903555691242218
train_iter_loss: 0.12420495599508286
train_iter_loss: 0.15281713008880615
train_iter_loss: 0.2627180516719818
train_iter_loss: 0.1455250233411789
train_iter_loss: 0.1328638345003128
train_iter_loss: 0.08028722554445267
train_iter_loss: 0.13575026392936707
train_iter_loss: 0.13575808703899384
train_iter_loss: 0.13425026834011078
train_iter_loss: 0.1709406077861786
train_iter_loss: 0.2881260812282562
train_iter_loss: 0.2268606424331665
train_iter_loss: 0.14878910779953003
train_iter_loss: 0.19332653284072876
train_iter_loss: 0.14642003178596497
train_iter_loss: 0.13139930367469788
train_iter_loss: 0.25553271174430847
train_iter_loss: 0.1711903065443039
train_iter_loss: 0.11912374198436737
train_iter_loss: 0.2537446916103363
train_iter_loss: 0.29929253458976746
train_iter_loss: 0.14052791893482208
train_iter_loss: 0.15348756313323975
train_iter_loss: 0.09208281338214874
train_iter_loss: 0.25502312183380127
train_iter_loss: 0.1637512892484665
train_iter_loss: 0.31240585446357727
train_iter_loss: 0.23920167982578278
train_iter_loss: 0.12743701040744781
train_iter_loss: 0.15876725316047668
train_iter_loss: 0.18770653009414673
train_iter_loss: 0.11404208093881607
train_iter_loss: 0.16820697486400604
train_iter_loss: 0.12219002842903137
train_iter_loss: 0.10536558926105499
train_iter_loss: 0.17012695968151093
train_iter_loss: 0.04580492898821831
train_iter_loss: 0.20055021345615387
train_iter_loss: 0.13062387704849243
train_iter_loss: 0.18308362364768982
train_iter_loss: 0.1456885188817978
train_iter_loss: 0.11439954489469528
train_iter_loss: 0.14402350783348083
train_iter_loss: 0.16383640468120575
train_iter_loss: 0.21200841665267944
train_iter_loss: 0.09826898574829102
train_iter_loss: 0.1614043116569519
train_iter_loss: 0.11268828064203262
train_iter_loss: 0.2628151476383209
train loss :0.1613
---------------------
Validation seg loss: 0.2183738647309958 at epoch 375
epoch =    376/  1000, exp = train
train_iter_loss: 0.27921062707901
train_iter_loss: 0.09351669996976852
train_iter_loss: 0.09354107826948166
train_iter_loss: 0.026918182149529457
train_iter_loss: 0.27866342663764954
train_iter_loss: 0.3508332073688507
train_iter_loss: 0.11207541078329086
train_iter_loss: 0.06678358465433121
train_iter_loss: 0.27440351247787476
train_iter_loss: 0.18612921237945557
train_iter_loss: 0.10868227481842041
train_iter_loss: 0.12759295105934143
train_iter_loss: 0.06625504791736603
train_iter_loss: 0.1610022485256195
train_iter_loss: 0.08971170336008072
train_iter_loss: 0.14845909178256989
train_iter_loss: 0.14060735702514648
train_iter_loss: 0.25279197096824646
train_iter_loss: 0.13269340991973877
train_iter_loss: 0.2383621633052826
train_iter_loss: 0.12419899553060532
train_iter_loss: 0.1566138118505478
train_iter_loss: 0.20679308474063873
train_iter_loss: 0.10425642877817154
train_iter_loss: 0.21525420248508453
train_iter_loss: 0.21803227066993713
train_iter_loss: 0.16945359110832214
train_iter_loss: 0.28715407848358154
train_iter_loss: 0.3251390755176544
train_iter_loss: 0.1147521361708641
train_iter_loss: 0.13025526702404022
train_iter_loss: 0.09959354251623154
train_iter_loss: 0.2622073292732239
train_iter_loss: 0.1360381841659546
train_iter_loss: 0.1988646686077118
train_iter_loss: 0.31166112422943115
train_iter_loss: 0.1588057279586792
train_iter_loss: 0.10844361782073975
train_iter_loss: 0.1575431376695633
train_iter_loss: 0.12565232813358307
train_iter_loss: 0.09366034716367722
train_iter_loss: 0.24201005697250366
train_iter_loss: 0.13335610926151276
train_iter_loss: 0.1507468819618225
train_iter_loss: 0.18433058261871338
train_iter_loss: 0.15029750764369965
train_iter_loss: 0.11558446288108826
train_iter_loss: 0.1988796442747116
train_iter_loss: 0.08237521350383759
train_iter_loss: 0.05529263988137245
train_iter_loss: 0.09741313755512238
train_iter_loss: 0.09395112842321396
train_iter_loss: 0.12582209706306458
train_iter_loss: 0.17591659724712372
train_iter_loss: 0.180266335606575
train_iter_loss: 0.2002982795238495
train_iter_loss: 0.21735280752182007
train_iter_loss: 0.27044039964675903
train_iter_loss: 0.1679689735174179
train_iter_loss: 0.15449953079223633
train_iter_loss: 0.3149228096008301
train_iter_loss: 0.17526093125343323
train_iter_loss: 0.11702176928520203
train_iter_loss: 0.1097099557518959
train_iter_loss: 0.14444267749786377
train_iter_loss: 0.21044789254665375
train_iter_loss: 0.16338267922401428
train_iter_loss: 0.07797610014677048
train_iter_loss: 0.18963195383548737
train_iter_loss: 0.13875743746757507
train_iter_loss: 0.19564375281333923
train_iter_loss: 0.16237324476242065
train_iter_loss: 0.11655082553625107
train_iter_loss: 0.10211888700723648
train_iter_loss: 0.11186051368713379
train_iter_loss: 0.19180023670196533
train_iter_loss: 0.11798492819070816
train_iter_loss: 0.267621785402298
train_iter_loss: 0.175442174077034
train_iter_loss: 0.15554435551166534
train_iter_loss: 0.15640459954738617
train_iter_loss: 0.1512187421321869
train_iter_loss: 0.07042908668518066
train_iter_loss: 0.13479548692703247
train_iter_loss: 0.10545512288808823
train_iter_loss: 0.15596206486225128
train_iter_loss: 0.20524446666240692
train_iter_loss: 0.11939015239477158
train_iter_loss: 0.09809871762990952
train_iter_loss: 0.2236328423023224
train_iter_loss: 0.07089979946613312
train_iter_loss: 0.2664107382297516
train_iter_loss: 0.16187040507793427
train_iter_loss: 0.2035992443561554
train_iter_loss: 0.0980619266629219
train_iter_loss: 0.18525999784469604
train_iter_loss: 0.11252431571483612
train_iter_loss: 0.18006379902362823
train_iter_loss: 0.19217553734779358
train_iter_loss: 0.17229484021663666
train loss :0.1639
---------------------
Validation seg loss: 0.2185062928793003 at epoch 376
epoch =    377/  1000, exp = train
train_iter_loss: 0.24140022695064545
train_iter_loss: 0.07446122169494629
train_iter_loss: 0.10708291083574295
train_iter_loss: 0.26621297001838684
train_iter_loss: 0.14228329062461853
train_iter_loss: 0.12846563756465912
train_iter_loss: 0.12234069406986237
train_iter_loss: 0.1604883372783661
train_iter_loss: 0.16184359788894653
train_iter_loss: 0.06468377262353897
train_iter_loss: 0.13801109790802002
train_iter_loss: 0.10940814763307571
train_iter_loss: 0.17003749310970306
train_iter_loss: 0.15995101630687714
train_iter_loss: 0.3614305257797241
train_iter_loss: 0.31288987398147583
train_iter_loss: 0.12064167857170105
train_iter_loss: 0.1837535798549652
train_iter_loss: 0.16496971249580383
train_iter_loss: 0.1593957394361496
train_iter_loss: 0.13311876356601715
train_iter_loss: 0.1339765340089798
train_iter_loss: 0.22150827944278717
train_iter_loss: 0.13506105542182922
train_iter_loss: 0.14474445581436157
train_iter_loss: 0.3233701288700104
train_iter_loss: 0.1483672708272934
train_iter_loss: 0.16385908424854279
train_iter_loss: 0.15486454963684082
train_iter_loss: 0.12011712789535522
train_iter_loss: 0.2254863828420639
train_iter_loss: 0.1287253201007843
train_iter_loss: 0.1495734453201294
train_iter_loss: 0.16248802840709686
train_iter_loss: 0.174325630068779
train_iter_loss: 0.08270173519849777
train_iter_loss: 0.2617846131324768
train_iter_loss: 0.14856715500354767
train_iter_loss: 0.15363772213459015
train_iter_loss: 0.0909549817442894
train_iter_loss: 0.20166614651679993
train_iter_loss: 0.20420193672180176
train_iter_loss: 0.0785762295126915
train_iter_loss: 0.19510725140571594
train_iter_loss: 0.15387514233589172
train_iter_loss: 0.13667123019695282
train_iter_loss: 0.1782563477754593
train_iter_loss: 0.08718015998601913
train_iter_loss: 0.2002405822277069
train_iter_loss: 0.14999136328697205
train_iter_loss: 0.16516123712062836
train_iter_loss: 0.16521401703357697
train_iter_loss: 0.21665313839912415
train_iter_loss: 0.34986329078674316
train_iter_loss: 0.1695682406425476
train_iter_loss: 0.14795628190040588
train_iter_loss: 0.26966437697410583
train_iter_loss: 0.11871001124382019
train_iter_loss: 0.157165065407753
train_iter_loss: 0.1699504554271698
train_iter_loss: 0.11667963862419128
train_iter_loss: 0.19296510517597198
train_iter_loss: 0.11964718252420425
train_iter_loss: 0.2290198653936386
train_iter_loss: 0.15705978870391846
train_iter_loss: 0.17464721202850342
train_iter_loss: 0.17690235376358032
train_iter_loss: 0.07549046725034714
train_iter_loss: 0.24675928056240082
train_iter_loss: 0.1521173119544983
train_iter_loss: 0.1602686196565628
train_iter_loss: 0.06330855935811996
train_iter_loss: 0.2800886631011963
train_iter_loss: 0.14727464318275452
train_iter_loss: 0.14869807660579681
train_iter_loss: 0.2785096764564514
train_iter_loss: 0.17369353771209717
train_iter_loss: 0.15731462836265564
train_iter_loss: 0.1837460994720459
train_iter_loss: 0.06636382639408112
train_iter_loss: 0.30313369631767273
train_iter_loss: 0.18382064998149872
train_iter_loss: 0.1437516063451767
train_iter_loss: 0.15583814680576324
train_iter_loss: 0.2033868432044983
train_iter_loss: 0.177162304520607
train_iter_loss: 0.14283332228660583
train_iter_loss: 0.13789735734462738
train_iter_loss: 0.19981734454631805
train_iter_loss: 0.1705179363489151
train_iter_loss: 0.16882812976837158
train_iter_loss: 0.140730619430542
train_iter_loss: 0.17248398065567017
train_iter_loss: 0.0950692817568779
train_iter_loss: 0.11437986046075821
train_iter_loss: 0.24300462007522583
train_iter_loss: 0.1589004397392273
train_iter_loss: 0.10697992146015167
train_iter_loss: 0.17121264338493347
train_iter_loss: 0.09494892507791519
train loss :0.1687
---------------------
Validation seg loss: 0.2201719994850035 at epoch 377
epoch =    378/  1000, exp = train
train_iter_loss: 0.10620398074388504
train_iter_loss: 0.19604487717151642
train_iter_loss: 0.1911371499300003
train_iter_loss: 0.15013012290000916
train_iter_loss: 0.2353963404893875
train_iter_loss: 0.14746461808681488
train_iter_loss: 0.16539834439754486
train_iter_loss: 0.1275349259376526
train_iter_loss: 0.14185714721679688
train_iter_loss: 0.19188302755355835
train_iter_loss: 0.09394297003746033
train_iter_loss: 0.16946490108966827
train_iter_loss: 0.21620213985443115
train_iter_loss: 0.10016115009784698
train_iter_loss: 0.27481746673583984
train_iter_loss: 0.17110756039619446
train_iter_loss: 0.09908987581729889
train_iter_loss: 0.1346987932920456
train_iter_loss: 0.1989092230796814
train_iter_loss: 0.1663730889558792
train_iter_loss: 0.26228266954421997
train_iter_loss: 0.09213638305664062
train_iter_loss: 0.23252424597740173
train_iter_loss: 0.33310064673423767
train_iter_loss: 0.1134951189160347
train_iter_loss: 0.13835182785987854
train_iter_loss: 0.1552801877260208
train_iter_loss: 0.1587906926870346
train_iter_loss: 0.2978203594684601
train_iter_loss: 0.02880716882646084
train_iter_loss: 0.08170807361602783
train_iter_loss: 0.17870941758155823
train_iter_loss: 0.20482350885868073
train_iter_loss: 0.12168044596910477
train_iter_loss: 0.2641463875770569
train_iter_loss: 0.06264613568782806
train_iter_loss: 0.2587531805038452
train_iter_loss: 0.25602757930755615
train_iter_loss: 0.14968658983707428
train_iter_loss: 0.19931958615779877
train_iter_loss: 0.2585526406764984
train_iter_loss: 0.13651354610919952
train_iter_loss: 0.19783762097358704
train_iter_loss: 0.13745802640914917
train_iter_loss: 0.2488710880279541
train_iter_loss: 0.10834517329931259
train_iter_loss: 0.09562862664461136
train_iter_loss: 0.08721666783094406
train_iter_loss: 0.19290828704833984
train_iter_loss: 0.1716526746749878
train_iter_loss: 0.24512217938899994
train_iter_loss: 0.13382576406002045
train_iter_loss: 0.14395609498023987
train_iter_loss: 0.12447793781757355
train_iter_loss: 0.05474737286567688
train_iter_loss: 0.22728058695793152
train_iter_loss: 0.16777293384075165
train_iter_loss: 0.10426821559667587
train_iter_loss: 0.2599906027317047
train_iter_loss: 0.16144616901874542
train_iter_loss: 0.13614995777606964
train_iter_loss: 0.1915474683046341
train_iter_loss: 0.14845451712608337
train_iter_loss: 0.10395538061857224
train_iter_loss: 0.17274191975593567
train_iter_loss: 0.3216724097728729
train_iter_loss: 0.13786548376083374
train_iter_loss: 0.08039098232984543
train_iter_loss: 0.2693534791469574
train_iter_loss: 0.23031236231327057
train_iter_loss: 0.12327857315540314
train_iter_loss: 0.117258720099926
train_iter_loss: 0.16237808763980865
train_iter_loss: 0.1638983190059662
train_iter_loss: 0.18085379898548126
train_iter_loss: 0.15380612015724182
train_iter_loss: 0.37209057807922363
train_iter_loss: 0.12469007074832916
train_iter_loss: 0.12745632231235504
train_iter_loss: 0.10478229820728302
train_iter_loss: 0.14605024456977844
train_iter_loss: 0.29957470297813416
train_iter_loss: 0.18513508141040802
train_iter_loss: 0.11717405915260315
train_iter_loss: 0.15332382917404175
train_iter_loss: 0.11965444684028625
train_iter_loss: 0.1831766813993454
train_iter_loss: 0.10819890350103378
train_iter_loss: 0.21289795637130737
train_iter_loss: 0.09611323475837708
train_iter_loss: 0.1203397884964943
train_iter_loss: 0.262520968914032
train_iter_loss: 0.23951444029808044
train_iter_loss: 0.09544750303030014
train_iter_loss: 0.11289077997207642
train_iter_loss: 0.1629701852798462
train_iter_loss: 0.0952005460858345
train_iter_loss: 0.059314530342817307
train_iter_loss: 0.08221365511417389
train_iter_loss: 0.10519180446863174
train loss :0.1667
---------------------
Validation seg loss: 0.22108257329970035 at epoch 378
epoch =    379/  1000, exp = train
train_iter_loss: 0.15315578877925873
train_iter_loss: 0.04895215108990669
train_iter_loss: 0.096990205347538
train_iter_loss: 0.17809715867042542
train_iter_loss: 0.12189707159996033
train_iter_loss: 0.11733776330947876
train_iter_loss: 0.202454075217247
train_iter_loss: 0.09341523051261902
train_iter_loss: 0.027233906090259552
train_iter_loss: 0.0892244353890419
train_iter_loss: 0.15272273123264313
train_iter_loss: 0.1771955043077469
train_iter_loss: 0.18742701411247253
train_iter_loss: 0.1372137814760208
train_iter_loss: 0.12334297597408295
train_iter_loss: 0.21617354452610016
train_iter_loss: 0.13391925394535065
train_iter_loss: 0.2158629447221756
train_iter_loss: 0.14581699669361115
train_iter_loss: 0.2573513984680176
train_iter_loss: 0.15605078637599945
train_iter_loss: 0.28559091687202454
train_iter_loss: 0.1181851252913475
train_iter_loss: 0.2719208896160126
train_iter_loss: 0.21554405987262726
train_iter_loss: 0.10311048477888107
train_iter_loss: 0.2272433340549469
train_iter_loss: 0.22551511228084564
train_iter_loss: 0.12666535377502441
train_iter_loss: 0.1550837904214859
train_iter_loss: 0.23204320669174194
train_iter_loss: 0.1259821504354477
train_iter_loss: 0.07620644569396973
train_iter_loss: 0.16787588596343994
train_iter_loss: 0.17074276506900787
train_iter_loss: 0.0611257366836071
train_iter_loss: 0.13553716242313385
train_iter_loss: 0.09238798916339874
train_iter_loss: 0.16319340467453003
train_iter_loss: 0.1423039585351944
train_iter_loss: 0.06596004962921143
train_iter_loss: 0.21971014142036438
train_iter_loss: 0.19226820766925812
train_iter_loss: 0.1570485532283783
train_iter_loss: 0.22409839928150177
train_iter_loss: 0.14016547799110413
train_iter_loss: 0.3750394880771637
train_iter_loss: 0.15366096794605255
train_iter_loss: 0.2274952232837677
train_iter_loss: 0.19132432341575623
train_iter_loss: 0.1471378356218338
train_iter_loss: 0.08882058411836624
train_iter_loss: 0.239394873380661
train_iter_loss: 0.28245478868484497
train_iter_loss: 0.09222768247127533
train_iter_loss: 0.14980635046958923
train_iter_loss: 0.23376834392547607
train_iter_loss: 0.09972906857728958
train_iter_loss: 0.14997217059135437
train_iter_loss: 0.1378992795944214
train_iter_loss: 0.20639605820178986
train_iter_loss: 0.09166032820940018
train_iter_loss: 0.18004019558429718
train_iter_loss: 0.16938365995883942
train_iter_loss: 0.16122396290302277
train_iter_loss: 0.12886914610862732
train_iter_loss: 0.14954112470149994
train_iter_loss: 0.1060670018196106
train_iter_loss: 0.15512040257453918
train_iter_loss: 0.14870966970920563
train_iter_loss: 0.23574812710285187
train_iter_loss: 0.2096652239561081
train_iter_loss: 0.12662872672080994
train_iter_loss: 0.05755931884050369
train_iter_loss: 0.14836278557777405
train_iter_loss: 0.1321791112422943
train_iter_loss: 0.24046801030635834
train_iter_loss: 0.13721369206905365
train_iter_loss: 0.10143367201089859
train_iter_loss: 0.1652214229106903
train_iter_loss: 0.1047607958316803
train_iter_loss: 0.1362040489912033
train_iter_loss: 0.16926588118076324
train_iter_loss: 0.21744482219219208
train_iter_loss: 0.1539101004600525
train_iter_loss: 0.20353636145591736
train_iter_loss: 0.15883836150169373
train_iter_loss: 0.20101116597652435
train_iter_loss: 0.2448919713497162
train_iter_loss: 0.09911488741636276
train_iter_loss: 0.13764619827270508
train_iter_loss: 0.11492253094911575
train_iter_loss: 0.1967959702014923
train_iter_loss: 0.22112183272838593
train_iter_loss: 0.23505444824695587
train_iter_loss: 0.0884406715631485
train_iter_loss: 0.12363262474536896
train_iter_loss: 0.1824467033147812
train_iter_loss: 0.12547585368156433
train_iter_loss: 0.14905866980552673
train loss :0.1618
---------------------
Validation seg loss: 0.22165503307192955 at epoch 379
epoch =    380/  1000, exp = train
train_iter_loss: 0.15400239825248718
train_iter_loss: 0.1466531604528427
train_iter_loss: 0.09606839716434479
train_iter_loss: 0.1147550418972969
train_iter_loss: 0.12215201556682587
train_iter_loss: 0.083077073097229
train_iter_loss: 0.18123416602611542
train_iter_loss: 0.2769260108470917
train_iter_loss: 0.16105420887470245
train_iter_loss: 0.17328323423862457
train_iter_loss: 0.10494387149810791
train_iter_loss: 0.14631390571594238
train_iter_loss: 0.10397738963365555
train_iter_loss: 0.11016673594713211
train_iter_loss: 0.16273143887519836
train_iter_loss: 0.23612064123153687
train_iter_loss: 0.1445615291595459
train_iter_loss: 0.2624071538448334
train_iter_loss: 0.11673083156347275
train_iter_loss: 0.13201315701007843
train_iter_loss: 0.1444522887468338
train_iter_loss: 0.22415603697299957
train_iter_loss: 0.11359938979148865
train_iter_loss: 0.239526629447937
train_iter_loss: 0.1957990825176239
train_iter_loss: 0.2938138544559479
train_iter_loss: 0.15775395929813385
train_iter_loss: 0.1325238198041916
train_iter_loss: 0.13009369373321533
train_iter_loss: 0.06277383863925934
train_iter_loss: 0.2979942560195923
train_iter_loss: 0.16986119747161865
train_iter_loss: 0.1430906504392624
train_iter_loss: 0.15950538218021393
train_iter_loss: 0.1893046498298645
train_iter_loss: 0.21024680137634277
train_iter_loss: 0.16927418112754822
train_iter_loss: 0.1161063089966774
train_iter_loss: 0.17069777846336365
train_iter_loss: 0.21364504098892212
train_iter_loss: 0.23705053329467773
train_iter_loss: 0.12141502648591995
train_iter_loss: 0.21368630230426788
train_iter_loss: 0.21334469318389893
train_iter_loss: 0.144890695810318
train_iter_loss: 0.17301863431930542
train_iter_loss: 0.2808239459991455
train_iter_loss: 0.15805509686470032
train_iter_loss: 0.20968104898929596
train_iter_loss: 0.08269703388214111
train_iter_loss: 0.15704506635665894
train_iter_loss: 0.23657916486263275
train_iter_loss: 0.09887632727622986
train_iter_loss: 0.12964625656604767
train_iter_loss: 0.21674638986587524
train_iter_loss: 0.10549870133399963
train_iter_loss: 0.18540768325328827
train_iter_loss: 0.21172507107257843
train_iter_loss: 0.11471831798553467
train_iter_loss: 0.11387916654348373
train_iter_loss: 0.13285452127456665
train_iter_loss: 0.15535449981689453
train_iter_loss: 0.14160118997097015
train_iter_loss: 0.11526542901992798
train_iter_loss: 0.195838063955307
train_iter_loss: 0.12947499752044678
train_iter_loss: 0.1203528568148613
train_iter_loss: 0.17938527464866638
train_iter_loss: 0.16919396817684174
train_iter_loss: 0.19025827944278717
train_iter_loss: 0.038494598120450974
train_iter_loss: 0.21034908294677734
train_iter_loss: 0.23594282567501068
train_iter_loss: 0.157345250248909
train_iter_loss: 0.13433736562728882
train_iter_loss: 0.14943721890449524
train_iter_loss: 0.13132137060165405
train_iter_loss: 0.09495143592357635
train_iter_loss: 0.17107278108596802
train_iter_loss: 0.18606826663017273
train_iter_loss: 0.23456089198589325
train_iter_loss: 0.0207216739654541
train_iter_loss: 0.11067608743906021
train_iter_loss: 0.17200087010860443
train_iter_loss: 0.09386434406042099
train_iter_loss: 0.11618182808160782
train_iter_loss: 0.19709327816963196
train_iter_loss: 0.21876570582389832
train_iter_loss: 0.27627652883529663
train_iter_loss: 0.13183367252349854
train_iter_loss: 0.18777240812778473
train_iter_loss: 0.06156929209828377
train_iter_loss: 0.1659892499446869
train_iter_loss: 0.0860949158668518
train_iter_loss: 0.21517595648765564
train_iter_loss: 0.15991424024105072
train_iter_loss: 0.11189812421798706
train_iter_loss: 0.08297629654407501
train_iter_loss: 0.16428685188293457
train_iter_loss: 0.1995944380760193
train loss :0.1618
---------------------
Validation seg loss: 0.2205647039842212 at epoch 380
epoch =    381/  1000, exp = train
train_iter_loss: 0.12488137930631638
train_iter_loss: 0.1614803969860077
train_iter_loss: 0.15277528762817383
train_iter_loss: 0.22062525153160095
train_iter_loss: 0.10287158936262131
train_iter_loss: 0.21002431213855743
train_iter_loss: 0.10807941108942032
train_iter_loss: 0.16251394152641296
train_iter_loss: 0.225385382771492
train_iter_loss: 0.11207050085067749
train_iter_loss: 0.22266291081905365
train_iter_loss: 0.15078282356262207
train_iter_loss: 0.2529381215572357
train_iter_loss: 0.15872307121753693
train_iter_loss: 0.12158223986625671
train_iter_loss: 0.035121671855449677
train_iter_loss: 0.2187580168247223
train_iter_loss: 0.2574840784072876
train_iter_loss: 0.15942545235157013
train_iter_loss: 0.15561144053936005
train_iter_loss: 0.22624197602272034
train_iter_loss: 0.21102893352508545
train_iter_loss: 0.11626355350017548
train_iter_loss: 0.11638089269399643
train_iter_loss: 0.20535655319690704
train_iter_loss: 0.17468056082725525
train_iter_loss: 0.1770707666873932
train_iter_loss: 0.12982429563999176
train_iter_loss: 0.12444384396076202
train_iter_loss: 0.07614900916814804
train_iter_loss: 0.11119145900011063
train_iter_loss: 0.20708730816841125
train_iter_loss: 0.07155769318342209
train_iter_loss: 0.11597856879234314
train_iter_loss: 0.16503767669200897
train_iter_loss: 0.19315464794635773
train_iter_loss: 0.1952802836894989
train_iter_loss: 0.18501529097557068
train_iter_loss: 0.16759096086025238
train_iter_loss: 0.18511679768562317
train_iter_loss: 0.18504612147808075
train_iter_loss: 0.13202716410160065
train_iter_loss: 0.12610770761966705
train_iter_loss: 0.1750997006893158
train_iter_loss: 0.2720501720905304
train_iter_loss: 0.265165776014328
train_iter_loss: 0.1438605934381485
train_iter_loss: 0.11038625240325928
train_iter_loss: 0.14889614284038544
train_iter_loss: 0.15998335182666779
train_iter_loss: 0.10660260170698166
train_iter_loss: 0.09188678115606308
train_iter_loss: 0.09910101443529129
train_iter_loss: 0.18178415298461914
train_iter_loss: 0.10465633869171143
train_iter_loss: 0.10614943504333496
train_iter_loss: 0.22141017019748688
train_iter_loss: 0.13211478292942047
train_iter_loss: 0.12124352902173996
train_iter_loss: 0.19076083600521088
train_iter_loss: 0.34519919753074646
train_iter_loss: 0.23722928762435913
train_iter_loss: 0.1190667375922203
train_iter_loss: 0.15945672988891602
train_iter_loss: 0.15942522883415222
train_iter_loss: 0.08057884126901627
train_iter_loss: 0.3633684813976288
train_iter_loss: 0.14853809773921967
train_iter_loss: 0.20350119471549988
train_iter_loss: 0.18526248633861542
train_iter_loss: 0.16668330132961273
train_iter_loss: 0.18400418758392334
train_iter_loss: 0.19816893339157104
train_iter_loss: 0.11411914974451065
train_iter_loss: 0.18540266156196594
train_iter_loss: 0.1345142275094986
train_iter_loss: 0.13280229270458221
train_iter_loss: 0.14793281257152557
train_iter_loss: 0.16009443998336792
train_iter_loss: 0.21247678995132446
train_iter_loss: 0.08904018253087997
train_iter_loss: 0.10528846830129623
train_iter_loss: 0.17307627201080322
train_iter_loss: 0.1911783367395401
train_iter_loss: 0.12806853652000427
train_iter_loss: 0.12883076071739197
train_iter_loss: 0.11225280165672302
train_iter_loss: 0.07236699759960175
train_iter_loss: 0.10345156490802765
train_iter_loss: 0.17308367788791656
train_iter_loss: 0.18744733929634094
train_iter_loss: 0.1627001166343689
train_iter_loss: 0.16256393492221832
train_iter_loss: 0.16301384568214417
train_iter_loss: 0.24975286424160004
train_iter_loss: 0.13399812579154968
train_iter_loss: 0.29412171244621277
train_iter_loss: 0.08264908194541931
train_iter_loss: 0.11683572828769684
train_iter_loss: 0.14054185152053833
train loss :0.1631
---------------------
Validation seg loss: 0.21720923160923258 at epoch 381
epoch =    382/  1000, exp = train
train_iter_loss: 0.15963944792747498
train_iter_loss: 0.23989751935005188
train_iter_loss: 0.1201833039522171
train_iter_loss: 0.06877493113279343
train_iter_loss: 0.2926191985607147
train_iter_loss: 0.06637106835842133
train_iter_loss: 0.10340089350938797
train_iter_loss: 0.09911566227674484
train_iter_loss: 0.12851503491401672
train_iter_loss: 0.0911412388086319
train_iter_loss: 0.13779455423355103
train_iter_loss: 0.06733148545026779
train_iter_loss: 0.0971921905875206
train_iter_loss: 0.1736023873090744
train_iter_loss: 0.17310406267642975
train_iter_loss: 0.13468004763126373
train_iter_loss: 0.2113778442144394
train_iter_loss: 0.2675105035305023
train_iter_loss: 0.2136269211769104
train_iter_loss: 0.13485345244407654
train_iter_loss: 0.1867460310459137
train_iter_loss: 0.030371831730008125
train_iter_loss: 0.10805495083332062
train_iter_loss: 0.1832096129655838
train_iter_loss: 0.2827775776386261
train_iter_loss: 0.14190125465393066
train_iter_loss: 0.20951469242572784
train_iter_loss: 0.1766975373029709
train_iter_loss: 0.21974797546863556
train_iter_loss: 0.20074640214443207
train_iter_loss: 0.12294119596481323
train_iter_loss: 0.13860158622264862
train_iter_loss: 0.12405416369438171
train_iter_loss: 0.1322358101606369
train_iter_loss: 0.35979974269866943
train_iter_loss: 0.1635984182357788
train_iter_loss: 0.16502894461154938
train_iter_loss: 0.11788696050643921
train_iter_loss: 0.14118969440460205
train_iter_loss: 0.12825633585453033
train_iter_loss: 0.1251576691865921
train_iter_loss: 0.17192064225673676
train_iter_loss: 0.27302849292755127
train_iter_loss: 0.14732840657234192
train_iter_loss: 0.21428990364074707
train_iter_loss: 0.0720350444316864
train_iter_loss: 0.09420571476221085
train_iter_loss: 0.2642386257648468
train_iter_loss: 0.20369887351989746
train_iter_loss: 0.0932052955031395
train_iter_loss: 0.11555439233779907
train_iter_loss: 0.16585496068000793
train_iter_loss: 0.16552530229091644
train_iter_loss: 0.2960556447505951
train_iter_loss: 0.06297063827514648
train_iter_loss: 0.1814962774515152
train_iter_loss: 0.10687936842441559
train_iter_loss: 0.07030507922172546
train_iter_loss: 0.16866999864578247
train_iter_loss: 0.21500568091869354
train_iter_loss: 0.14409074187278748
train_iter_loss: 0.10206373780965805
train_iter_loss: 0.09386952221393585
train_iter_loss: 0.18281827867031097
train_iter_loss: 0.14489638805389404
train_iter_loss: 0.1439390480518341
train_iter_loss: 0.17016863822937012
train_iter_loss: 0.20328961312770844
train_iter_loss: 0.1412903368473053
train_iter_loss: 0.16264590620994568
train_iter_loss: 0.2585068941116333
train_iter_loss: 0.19880621135234833
train_iter_loss: 0.12971077859401703
train_iter_loss: 0.1339237242937088
train_iter_loss: 0.20507551729679108
train_iter_loss: 0.09758000820875168
train_iter_loss: 0.06572706252336502
train_iter_loss: 0.15990041196346283
train_iter_loss: 0.1193903237581253
train_iter_loss: 0.08612366020679474
train_iter_loss: 0.26364457607269287
train_iter_loss: 0.11338531225919724
train_iter_loss: 0.0856480598449707
train_iter_loss: 0.21676059067249298
train_iter_loss: 0.1127120852470398
train_iter_loss: 0.12324906140565872
train_iter_loss: 0.1145622506737709
train_iter_loss: 0.08795539289712906
train_iter_loss: 0.11765945702791214
train_iter_loss: 0.1875288486480713
train_iter_loss: 0.1018369197845459
train_iter_loss: 0.23612233996391296
train_iter_loss: 0.2614050805568695
train_iter_loss: 0.19893072545528412
train_iter_loss: 0.222474604845047
train_iter_loss: 0.288856565952301
train_iter_loss: 0.1782350242137909
train_iter_loss: 0.1959051638841629
train_iter_loss: 0.09122373908758163
train_iter_loss: 0.08292453736066818
train loss :0.1591
---------------------
Validation seg loss: 0.2215929773882172 at epoch 382
epoch =    383/  1000, exp = train
train_iter_loss: 0.13387076556682587
train_iter_loss: 0.15844844281673431
train_iter_loss: 0.22400449216365814
train_iter_loss: 0.1181671991944313
train_iter_loss: 0.17498140037059784
train_iter_loss: 0.15433597564697266
train_iter_loss: 0.13123349845409393
train_iter_loss: 0.147531658411026
train_iter_loss: 0.18896399438381195
train_iter_loss: 0.12265424430370331
train_iter_loss: 0.12072872370481491
train_iter_loss: 0.24448786675930023
train_iter_loss: 0.26358580589294434
train_iter_loss: 0.23845824599266052
train_iter_loss: 0.20492564141750336
train_iter_loss: 0.22900132834911346
train_iter_loss: 0.10825178772211075
train_iter_loss: 0.2679644525051117
train_iter_loss: 0.08974877744913101
train_iter_loss: 0.29961565136909485
train_iter_loss: 0.06774910539388657
train_iter_loss: 0.1294841319322586
train_iter_loss: 0.22978663444519043
train_iter_loss: 0.16083088517189026
train_iter_loss: 0.07861925661563873
train_iter_loss: 0.14149630069732666
train_iter_loss: 0.13458573818206787
train_iter_loss: 0.12037619948387146
train_iter_loss: 0.19940675795078278
train_iter_loss: 0.036902252584695816
train_iter_loss: 0.08727963268756866
train_iter_loss: 0.1240789145231247
train_iter_loss: 0.15030185878276825
train_iter_loss: 0.1891053318977356
train_iter_loss: 0.16161029040813446
train_iter_loss: 0.13045768439769745
train_iter_loss: 0.17480462789535522
train_iter_loss: 0.220539852976799
train_iter_loss: 0.16722436249256134
train_iter_loss: 0.1309390813112259
train_iter_loss: 0.1956418752670288
train_iter_loss: 0.1585899293422699
train_iter_loss: 0.15925762057304382
train_iter_loss: 0.2086055874824524
train_iter_loss: 0.1007552221417427
train_iter_loss: 0.2460266649723053
train_iter_loss: 0.15344677865505219
train_iter_loss: 0.11010678857564926
train_iter_loss: 0.10190166532993317
train_iter_loss: 0.17751604318618774
train_iter_loss: 0.23289854824543
train_iter_loss: 0.03635646030306816
train_iter_loss: 0.16961458325386047
train_iter_loss: 0.19655509293079376
train_iter_loss: 0.12193769216537476
train_iter_loss: 0.14125852286815643
train_iter_loss: 0.17430414259433746
train_iter_loss: 0.19284118711948395
train_iter_loss: 0.11834061145782471
train_iter_loss: 0.33020633459091187
train_iter_loss: 0.37523558735847473
train_iter_loss: 0.19706806540489197
train_iter_loss: 0.17411604523658752
train_iter_loss: 0.20654436945915222
train_iter_loss: 0.1538013517856598
train_iter_loss: 0.1287270486354828
train_iter_loss: 0.1993391364812851
train_iter_loss: 0.14024479687213898
train_iter_loss: 0.1623826026916504
train_iter_loss: 0.1469094157218933
train_iter_loss: 0.16350191831588745
train_iter_loss: 0.167948916554451
train_iter_loss: 0.12191548943519592
train_iter_loss: 0.20855358242988586
train_iter_loss: 0.3211697041988373
train_iter_loss: 0.10476583242416382
train_iter_loss: 0.1314476579427719
train_iter_loss: 0.24709831178188324
train_iter_loss: 0.10207103192806244
train_iter_loss: 0.08859742432832718
train_iter_loss: 0.24031351506710052
train_iter_loss: 0.18358168005943298
train_iter_loss: 0.09312927722930908
train_iter_loss: 0.197551891207695
train_iter_loss: 0.1552545577287674
train_iter_loss: 0.1366576850414276
train_iter_loss: 0.11479230970144272
train_iter_loss: 0.1339445263147354
train_iter_loss: 0.23299698531627655
train_iter_loss: 0.20092108845710754
train_iter_loss: 0.13591821491718292
train_iter_loss: 0.08838241547346115
train_iter_loss: 0.15393802523612976
train_iter_loss: 0.160347118973732
train_iter_loss: 0.22102755308151245
train_iter_loss: 0.17332160472869873
train_iter_loss: 0.11730021238327026
train_iter_loss: 0.17713530361652374
train_iter_loss: 0.08790963143110275
train_iter_loss: 0.12146282941102982
train loss :0.1661
---------------------
Validation seg loss: 0.22113843434402403 at epoch 383
epoch =    384/  1000, exp = train
train_iter_loss: 0.20424959063529968
train_iter_loss: 0.16768081486225128
train_iter_loss: 0.1775616556406021
train_iter_loss: 0.2347523719072342
train_iter_loss: 0.2551863491535187
train_iter_loss: 0.19303478300571442
train_iter_loss: 0.21897968649864197
train_iter_loss: 0.18086335062980652
train_iter_loss: 0.3128531277179718
train_iter_loss: 0.1851763278245926
train_iter_loss: 0.268769234418869
train_iter_loss: 0.17796319723129272
train_iter_loss: 0.17591699957847595
train_iter_loss: 0.20267605781555176
train_iter_loss: 0.2198389768600464
train_iter_loss: 0.1500708907842636
train_iter_loss: 0.0813356265425682
train_iter_loss: 0.09270210564136505
train_iter_loss: 0.1406830996274948
train_iter_loss: 0.11046857386827469
train_iter_loss: 0.058771610260009766
train_iter_loss: 0.1377079039812088
train_iter_loss: 0.18580873310565948
train_iter_loss: 0.10240879654884338
train_iter_loss: 0.13671444356441498
train_iter_loss: 0.19215889275074005
train_iter_loss: 0.19513735175132751
train_iter_loss: 0.2851369082927704
train_iter_loss: 0.09862623363733292
train_iter_loss: 0.0791589543223381
train_iter_loss: 0.12483853846788406
train_iter_loss: 0.20955896377563477
train_iter_loss: 0.13949520885944366
train_iter_loss: 0.12498338520526886
train_iter_loss: 0.14749160408973694
train_iter_loss: 0.19455482065677643
train_iter_loss: 0.10491940379142761
train_iter_loss: 0.25445520877838135
train_iter_loss: 0.1167275682091713
train_iter_loss: 0.14269007742404938
train_iter_loss: 0.21538490056991577
train_iter_loss: 0.1572052240371704
train_iter_loss: 0.19194257259368896
train_iter_loss: 0.19134721159934998
train_iter_loss: 0.07533958554267883
train_iter_loss: 0.1302243173122406
train_iter_loss: 0.15899443626403809
train_iter_loss: 0.24287033081054688
train_iter_loss: 0.097060427069664
train_iter_loss: 0.06567714363336563
train_iter_loss: 0.22467844188213348
train_iter_loss: 0.10314113646745682
train_iter_loss: 0.1596575677394867
train_iter_loss: 0.11332622915506363
train_iter_loss: 0.054312076419591904
train_iter_loss: 0.2407175451517105
train_iter_loss: 0.20403902232646942
train_iter_loss: 0.13059893250465393
train_iter_loss: 0.17455561459064484
train_iter_loss: 0.06833842396736145
train_iter_loss: 0.19320833683013916
train_iter_loss: 0.04743034392595291
train_iter_loss: 0.1864023208618164
train_iter_loss: 0.17429496347904205
train_iter_loss: 0.13072249293327332
train_iter_loss: 0.1322164535522461
train_iter_loss: 0.1445726752281189
train_iter_loss: 0.2247091680765152
train_iter_loss: 0.19996808469295502
train_iter_loss: 0.24720820784568787
train_iter_loss: 0.15752243995666504
train_iter_loss: 0.08861574530601501
train_iter_loss: 0.1991458684206009
train_iter_loss: 0.29085487127304077
train_iter_loss: 0.07835088670253754
train_iter_loss: 0.13424381613731384
train_iter_loss: 0.1433323621749878
train_iter_loss: 0.18996402621269226
train_iter_loss: 0.10253708064556122
train_iter_loss: 0.16596345603466034
train_iter_loss: 0.10231653600931168
train_iter_loss: 0.1599932163953781
train_iter_loss: 0.16551312804222107
train_iter_loss: 0.16338467597961426
train_iter_loss: 0.24901804327964783
train_iter_loss: 0.12645617127418518
train_iter_loss: 0.06934790313243866
train_iter_loss: 0.08969800919294357
train_iter_loss: 0.23148579895496368
train_iter_loss: 0.14258410036563873
train_iter_loss: 0.1609678864479065
train_iter_loss: 0.14468447864055634
train_iter_loss: 0.18097490072250366
train_iter_loss: 0.21999475359916687
train_iter_loss: 0.09928728640079498
train_iter_loss: 0.17072464525699615
train_iter_loss: 0.2501828372478485
train_iter_loss: 0.1270565241575241
train_iter_loss: 0.21732379496097565
train_iter_loss: 0.2006947547197342
train loss :0.1645
---------------------
Validation seg loss: 0.21733023435369414 at epoch 384
epoch =    385/  1000, exp = train
train_iter_loss: 0.15268436074256897
train_iter_loss: 0.2210390567779541
train_iter_loss: 0.3198707401752472
train_iter_loss: 0.22897572815418243
train_iter_loss: 0.1492031216621399
train_iter_loss: 0.1340898722410202
train_iter_loss: 0.1845816671848297
train_iter_loss: 0.24722471833229065
train_iter_loss: 0.4001864492893219
train_iter_loss: 0.11644630879163742
train_iter_loss: 0.1413937211036682
train_iter_loss: 0.1037524864077568
train_iter_loss: 0.1455947607755661
train_iter_loss: 0.1339721977710724
train_iter_loss: 0.12924693524837494
train_iter_loss: 0.14978207647800446
train_iter_loss: 0.20666693150997162
train_iter_loss: 0.161661759018898
train_iter_loss: 0.09651610255241394
train_iter_loss: 0.21333545446395874
train_iter_loss: 0.22515112161636353
train_iter_loss: 0.15448567271232605
train_iter_loss: 0.18713407218456268
train_iter_loss: 0.28705811500549316
train_iter_loss: 0.17455926537513733
train_iter_loss: 0.15325596928596497
train_iter_loss: 0.13935203850269318
train_iter_loss: 0.08965393155813217
train_iter_loss: 0.23582115769386292
train_iter_loss: 0.09071328490972519
train_iter_loss: 0.12069620192050934
train_iter_loss: 0.11946584284305573
train_iter_loss: 0.331733763217926
train_iter_loss: 0.16465841233730316
train_iter_loss: 0.1565256267786026
train_iter_loss: 0.1113843023777008
train_iter_loss: 0.133280411362648
train_iter_loss: 0.17144452035427094
train_iter_loss: 0.15203814208507538
train_iter_loss: 0.13410823047161102
train_iter_loss: 0.1699199229478836
train_iter_loss: 0.12129960209131241
train_iter_loss: 0.09703927487134933
train_iter_loss: 0.13616301119327545
train_iter_loss: 0.0697549432516098
train_iter_loss: 0.19597944617271423
train_iter_loss: 0.10250794142484665
train_iter_loss: 0.11548328399658203
train_iter_loss: 0.2015843391418457
train_iter_loss: 0.1452777087688446
train_iter_loss: 0.09145135432481766
train_iter_loss: 0.09802050143480301
train_iter_loss: 0.12332053482532501
train_iter_loss: 0.16698725521564484
train_iter_loss: 0.09881608933210373
train_iter_loss: 0.09857147932052612
train_iter_loss: 0.16216523945331573
train_iter_loss: 0.08919795602560043
train_iter_loss: 0.06963136047124863
train_iter_loss: 0.11965545266866684
train_iter_loss: 0.08112810552120209
train_iter_loss: 0.17907093465328217
train_iter_loss: 0.1756519377231598
train_iter_loss: 0.2938728928565979
train_iter_loss: 0.21711541712284088
train_iter_loss: 0.09692118316888809
train_iter_loss: 0.22588807344436646
train_iter_loss: 0.2895158529281616
train_iter_loss: 0.09186781197786331
train_iter_loss: 0.23462803661823273
train_iter_loss: 0.2371894270181656
train_iter_loss: 0.14640066027641296
train_iter_loss: 0.24829384684562683
train_iter_loss: 0.23115749657154083
train_iter_loss: 0.10465537011623383
train_iter_loss: 0.1139104962348938
train_iter_loss: 0.15079878270626068
train_iter_loss: 0.12300819903612137
train_iter_loss: 0.12742650508880615
train_iter_loss: 0.09459009021520615
train_iter_loss: 0.16340090334415436
train_iter_loss: 0.08732502162456512
train_iter_loss: 0.22705033421516418
train_iter_loss: 0.1287967711687088
train_iter_loss: 0.22033478319644928
train_iter_loss: 0.48034170269966125
train_iter_loss: 0.17500972747802734
train_iter_loss: 0.13704460859298706
train_iter_loss: 0.08366711437702179
train_iter_loss: 0.12077855318784714
train_iter_loss: 0.1329721063375473
train_iter_loss: 0.20386391878128052
train_iter_loss: 0.12639153003692627
train_iter_loss: 0.2888530194759369
train_iter_loss: 0.2663649320602417
train_iter_loss: 0.1809222251176834
train_iter_loss: 0.1774728000164032
train_iter_loss: 0.17528793215751648
train_iter_loss: 0.17140206694602966
train_iter_loss: 0.1432744562625885
train loss :0.1676
---------------------
Validation seg loss: 0.22414891919964608 at epoch 385
epoch =    386/  1000, exp = train
train_iter_loss: 0.13940764963626862
train_iter_loss: 0.09305921196937561
train_iter_loss: 0.21893365681171417
train_iter_loss: 0.13621507585048676
train_iter_loss: 0.1841682493686676
train_iter_loss: 0.21132776141166687
train_iter_loss: 0.11824560910463333
train_iter_loss: 0.12019560486078262
train_iter_loss: 0.07553761452436447
train_iter_loss: 0.28675198554992676
train_iter_loss: 0.1214832067489624
train_iter_loss: 0.16125553846359253
train_iter_loss: 0.13243193924427032
train_iter_loss: 0.17013387382030487
train_iter_loss: 0.252353698015213
train_iter_loss: 0.3063115179538727
train_iter_loss: 0.18156564235687256
train_iter_loss: 0.2259451299905777
train_iter_loss: 0.25135767459869385
train_iter_loss: 0.2291107326745987
train_iter_loss: 0.23582205176353455
train_iter_loss: 0.17332327365875244
train_iter_loss: 0.09823715686798096
train_iter_loss: 0.22239276766777039
train_iter_loss: 0.20972967147827148
train_iter_loss: 0.1534358710050583
train_iter_loss: 0.1567169725894928
train_iter_loss: 0.13436757028102875
train_iter_loss: 0.07788063585758209
train_iter_loss: 0.09425056725740433
train_iter_loss: 0.07501458376646042
train_iter_loss: 0.2101370096206665
train_iter_loss: 0.14417652785778046
train_iter_loss: 0.1280776411294937
train_iter_loss: 0.11995144933462143
train_iter_loss: 0.1051785871386528
train_iter_loss: 0.12137899547815323
train_iter_loss: 0.25449874997138977
train_iter_loss: 0.1914527863264084
train_iter_loss: 0.09142554551362991
train_iter_loss: 0.12052388489246368
train_iter_loss: 0.09933846443891525
train_iter_loss: 0.1697557419538498
train_iter_loss: 0.12219250202178955
train_iter_loss: 0.15413063764572144
train_iter_loss: 0.08246953040361404
train_iter_loss: 0.2924647331237793
train_iter_loss: 0.1969936341047287
train_iter_loss: 0.1489235758781433
train_iter_loss: 0.11296875029802322
train_iter_loss: 0.10332568734884262
train_iter_loss: 0.14858384430408478
train_iter_loss: 0.11539271473884583
train_iter_loss: 0.16286495327949524
train_iter_loss: 0.0694039836525917
train_iter_loss: 0.1347745656967163
train_iter_loss: 0.11942458897829056
train_iter_loss: 0.032221198081970215
train_iter_loss: 0.1534620076417923
train_iter_loss: 0.18050625920295715
train_iter_loss: 0.15162792801856995
train_iter_loss: 0.2442432940006256
train_iter_loss: 0.1553400754928589
train_iter_loss: 0.22876693308353424
train_iter_loss: 0.10071034729480743
train_iter_loss: 0.0764961689710617
train_iter_loss: 0.1401188224554062
train_iter_loss: 0.26985496282577515
train_iter_loss: 0.1925063580274582
train_iter_loss: 0.22140184044837952
train_iter_loss: 0.22709400951862335
train_iter_loss: 0.12406512349843979
train_iter_loss: 0.15748433768749237
train_iter_loss: 0.15159626305103302
train_iter_loss: 0.21142716705799103
train_iter_loss: 0.09340512007474899
train_iter_loss: 0.14460599422454834
train_iter_loss: 0.2768968939781189
train_iter_loss: 0.20139212906360626
train_iter_loss: 0.13376401364803314
train_iter_loss: 0.1633998155593872
train_iter_loss: 0.14121823012828827
train_iter_loss: 0.24837827682495117
train_iter_loss: 0.14562749862670898
train_iter_loss: 0.198689803481102
train_iter_loss: 0.22917808592319489
train_iter_loss: 0.17587953805923462
train_iter_loss: 0.20448847115039825
train_iter_loss: 0.3440515398979187
train_iter_loss: 0.11003348976373672
train_iter_loss: 0.1238611564040184
train_iter_loss: 0.13818298280239105
train_iter_loss: 0.14320474863052368
train_iter_loss: 0.26214298605918884
train_iter_loss: 0.2137945592403412
train_iter_loss: 0.2519233226776123
train_iter_loss: 0.11039406061172485
train_iter_loss: 0.1850169152021408
train_iter_loss: 0.14680702984333038
train_iter_loss: 0.15396128594875336
train loss :0.1669
---------------------
Validation seg loss: 0.21676686696075606 at epoch 386
epoch =    387/  1000, exp = train
train_iter_loss: 0.1666841059923172
train_iter_loss: 0.0769178718328476
train_iter_loss: 0.1278204768896103
train_iter_loss: 0.2619655728340149
train_iter_loss: 0.20457009971141815
train_iter_loss: 0.2954326570034027
train_iter_loss: 0.2714671492576599
train_iter_loss: 0.13300767540931702
train_iter_loss: 0.24836516380310059
train_iter_loss: 0.1677412986755371
train_iter_loss: 0.42228466272354126
train_iter_loss: 0.09261526167392731
train_iter_loss: 0.1754457652568817
train_iter_loss: 0.10796020925045013
train_iter_loss: 0.1533764898777008
train_iter_loss: 0.08271512389183044
train_iter_loss: 0.1227002963423729
train_iter_loss: 0.11195497214794159
train_iter_loss: 0.20013509690761566
train_iter_loss: 0.2514626979827881
train_iter_loss: 0.12293168902397156
train_iter_loss: 0.17555400729179382
train_iter_loss: 0.11395964026451111
train_iter_loss: 0.14885687828063965
train_iter_loss: 0.29275867342948914
train_iter_loss: 0.14869198203086853
train_iter_loss: 0.1824265420436859
train_iter_loss: 0.07126564532518387
train_iter_loss: 0.09480465948581696
train_iter_loss: 0.296335905790329
train_iter_loss: 0.1656685471534729
train_iter_loss: 0.15549421310424805
train_iter_loss: 0.09042776376008987
train_iter_loss: 0.1409274935722351
train_iter_loss: 0.13761889934539795
train_iter_loss: 0.20073433220386505
train_iter_loss: 0.20082701742649078
train_iter_loss: 0.18916669487953186
train_iter_loss: 0.12365196645259857
train_iter_loss: 0.27763620018959045
train_iter_loss: 0.15232305228710175
train_iter_loss: 0.20150819420814514
train_iter_loss: 0.18287329375743866
train_iter_loss: 0.1726488322019577
train_iter_loss: 0.21776503324508667
train_iter_loss: 0.17357276380062103
train_iter_loss: 0.09814518690109253
train_iter_loss: 0.1659800112247467
train_iter_loss: 0.1888580620288849
train_iter_loss: 0.10610835999250412
train_iter_loss: 0.039005741477012634
train_iter_loss: 0.16329710185527802
train_iter_loss: 0.14917679131031036
train_iter_loss: 0.028299015015363693
train_iter_loss: 0.163103386759758
train_iter_loss: 0.0835748091340065
train_iter_loss: 0.12231409549713135
train_iter_loss: 0.26546135544776917
train_iter_loss: 0.1321244239807129
train_iter_loss: 0.1730509102344513
train_iter_loss: 0.15539459884166718
train_iter_loss: 0.10632388293743134
train_iter_loss: 0.12411058694124222
train_iter_loss: 0.25601160526275635
train_iter_loss: 0.24628177285194397
train_iter_loss: 0.13978834450244904
train_iter_loss: 0.19493445754051208
train_iter_loss: 0.17744247615337372
train_iter_loss: 0.051851313561201096
train_iter_loss: 0.26637890934944153
train_iter_loss: 0.16535037755966187
train_iter_loss: 0.15392595529556274
train_iter_loss: 0.13457311689853668
train_iter_loss: 0.06598145514726639
train_iter_loss: 0.1476672887802124
train_iter_loss: 0.23688718676567078
train_iter_loss: 0.26666033267974854
train_iter_loss: 0.11225718259811401
train_iter_loss: 0.2306239902973175
train_iter_loss: 0.1884552538394928
train_iter_loss: 0.1382148712873459
train_iter_loss: 0.07824589312076569
train_iter_loss: 0.14280205965042114
train_iter_loss: 0.06360294669866562
train_iter_loss: 0.23898811638355255
train_iter_loss: 0.09283176809549332
train_iter_loss: 0.14540007710456848
train_iter_loss: 0.23270075023174286
train_iter_loss: 0.21368376910686493
train_iter_loss: 0.13023220002651215
train_iter_loss: 0.0730946734547615
train_iter_loss: 0.2792793810367584
train_iter_loss: 0.11883875727653503
train_iter_loss: 0.11840261518955231
train_iter_loss: 0.10647650808095932
train_iter_loss: 0.22588033974170685
train_iter_loss: 0.157637357711792
train_iter_loss: 0.1235506609082222
train_iter_loss: 0.15956471860408783
train_iter_loss: 0.13787148892879486
train loss :0.1655
---------------------
Validation seg loss: 0.22326266410637577 at epoch 387
epoch =    388/  1000, exp = train
train_iter_loss: 0.22782309353351593
train_iter_loss: 0.30864134430885315
train_iter_loss: 0.14449943602085114
train_iter_loss: 0.1493598222732544
train_iter_loss: 0.2219485193490982
train_iter_loss: 0.16331608593463898
train_iter_loss: 0.10221469402313232
train_iter_loss: 0.14937791228294373
train_iter_loss: 0.2557867169380188
train_iter_loss: 0.19304850697517395
train_iter_loss: 0.21756897866725922
train_iter_loss: 0.17603415250778198
train_iter_loss: 0.1964951455593109
train_iter_loss: 0.15997223556041718
train_iter_loss: 0.20610199868679047
train_iter_loss: 0.08458919078111649
train_iter_loss: 0.07122696191072464
train_iter_loss: 0.13532720506191254
train_iter_loss: 0.2616972327232361
train_iter_loss: 0.10791239142417908
train_iter_loss: 0.1374679058790207
train_iter_loss: 0.16906988620758057
train_iter_loss: 0.050639983266592026
train_iter_loss: 0.13947561383247375
train_iter_loss: 0.09674468636512756
train_iter_loss: 0.1889565885066986
train_iter_loss: 0.13713961839675903
train_iter_loss: 0.21564534306526184
train_iter_loss: 0.1790868490934372
train_iter_loss: 0.17498421669006348
train_iter_loss: 0.08700572699308395
train_iter_loss: 0.20307646691799164
train_iter_loss: 0.16145263612270355
train_iter_loss: 0.09077082574367523
train_iter_loss: 0.13130594789981842
train_iter_loss: 0.07064924389123917
train_iter_loss: 0.12385547906160355
train_iter_loss: 0.18259933590888977
train_iter_loss: 0.15830948948860168
train_iter_loss: 0.20593897998332977
train_iter_loss: 0.24839997291564941
train_iter_loss: 0.11181796342134476
train_iter_loss: 0.2178526371717453
train_iter_loss: 0.0793912336230278
train_iter_loss: 0.16496114432811737
train_iter_loss: 0.23470695316791534
train_iter_loss: 0.13245947659015656
train_iter_loss: 0.13317689299583435
train_iter_loss: 0.171077698469162
train_iter_loss: 0.1087673008441925
train_iter_loss: 0.18008017539978027
train_iter_loss: 0.17235848307609558
train_iter_loss: 0.19513185322284698
train_iter_loss: 0.24972866475582123
train_iter_loss: 0.11375463753938675
train_iter_loss: 0.1520640105009079
train_iter_loss: 0.14491006731987
train_iter_loss: 0.2568495273590088
train_iter_loss: 0.211432546377182
train_iter_loss: 0.171540766954422
train_iter_loss: 0.09218181669712067
train_iter_loss: 0.17124588787555695
train_iter_loss: 0.21099020540714264
train_iter_loss: 0.15848106145858765
train_iter_loss: 0.10715582966804504
train_iter_loss: 0.09797448664903641
train_iter_loss: 0.10774048417806625
train_iter_loss: 0.069301076233387
train_iter_loss: 0.18939955532550812
train_iter_loss: 0.2053128182888031
train_iter_loss: 0.08321832865476608
train_iter_loss: 0.16978372633457184
train_iter_loss: 0.20522934198379517
train_iter_loss: 0.21614280343055725
train_iter_loss: 0.16056770086288452
train_iter_loss: 0.12689264118671417
train_iter_loss: 0.050938066095113754
train_iter_loss: 0.03687734156847
train_iter_loss: 0.16438372433185577
train_iter_loss: 0.0985727459192276
train_iter_loss: 0.23802927136421204
train_iter_loss: 0.1054455116391182
train_iter_loss: 0.26831328868865967
train_iter_loss: 0.18762953579425812
train_iter_loss: 0.15732668340206146
train_iter_loss: 0.0767972469329834
train_iter_loss: 0.23763465881347656
train_iter_loss: 0.1411491334438324
train_iter_loss: 0.15429933369159698
train_iter_loss: 0.12829652428627014
train_iter_loss: 0.1286541223526001
train_iter_loss: 0.13021065294742584
train_iter_loss: 0.29652100801467896
train_iter_loss: 0.1479608565568924
train_iter_loss: 0.14918434619903564
train_iter_loss: 0.2462761551141739
train_iter_loss: 0.22257904708385468
train_iter_loss: 0.18448977172374725
train_iter_loss: 0.1355017125606537
train_iter_loss: 0.07947174459695816
train loss :0.1619
---------------------
Validation seg loss: 0.2195231236763439 at epoch 388
epoch =    389/  1000, exp = train
train_iter_loss: 0.2520592212677002
train_iter_loss: 0.1990119367837906
train_iter_loss: 0.18805795907974243
train_iter_loss: 0.20346413552761078
train_iter_loss: 0.10827016830444336
train_iter_loss: 0.02558082714676857
train_iter_loss: 0.15797485411167145
train_iter_loss: 0.14062368869781494
train_iter_loss: 0.28304290771484375
train_iter_loss: 0.06572209298610687
train_iter_loss: 0.10616934299468994
train_iter_loss: 0.20062029361724854
train_iter_loss: 0.28308963775634766
train_iter_loss: 0.1370631605386734
train_iter_loss: 0.11542763561010361
train_iter_loss: 0.1758722960948944
train_iter_loss: 0.09046439081430435
train_iter_loss: 0.06196139007806778
train_iter_loss: 0.15497972071170807
train_iter_loss: 0.18590019643306732
train_iter_loss: 0.13632503151893616
train_iter_loss: 0.21456843614578247
train_iter_loss: 0.1821509748697281
train_iter_loss: 0.25707700848579407
train_iter_loss: 0.18999840319156647
train_iter_loss: 0.18604618310928345
train_iter_loss: 0.09652458131313324
train_iter_loss: 0.19750984013080597
train_iter_loss: 0.10941197723150253
train_iter_loss: 0.2507103681564331
train_iter_loss: 0.2518206536769867
train_iter_loss: 0.12936392426490784
train_iter_loss: 0.11556742340326309
train_iter_loss: 0.12169209867715836
train_iter_loss: 0.15945564210414886
train_iter_loss: 0.2211615890264511
train_iter_loss: 0.2621610164642334
train_iter_loss: 0.22219853103160858
train_iter_loss: 0.1589183211326599
train_iter_loss: 0.06734475493431091
train_iter_loss: 0.19974155724048615
train_iter_loss: 0.07869154214859009
train_iter_loss: 0.11431857198476791
train_iter_loss: 0.1617589294910431
train_iter_loss: 0.16362425684928894
train_iter_loss: 0.1626458764076233
train_iter_loss: 0.18075786530971527
train_iter_loss: 0.07427015900611877
train_iter_loss: 0.25872603058815
train_iter_loss: 0.3802482485771179
train_iter_loss: 0.19650676846504211
train_iter_loss: 0.12294446676969528
train_iter_loss: 0.10823426395654678
train_iter_loss: 0.23641318082809448
train_iter_loss: 0.1615303009748459
train_iter_loss: 0.15119107067584991
train_iter_loss: 0.11489932984113693
train_iter_loss: 0.16372279822826385
train_iter_loss: 0.1503133326768875
train_iter_loss: 0.12861034274101257
train_iter_loss: 0.16798366606235504
train_iter_loss: 0.12930840253829956
train_iter_loss: 0.1516072303056717
train_iter_loss: 0.16577260196208954
train_iter_loss: 0.15298044681549072
train_iter_loss: 0.21764995157718658
train_iter_loss: 0.07155317068099976
train_iter_loss: 0.12089914828538895
train_iter_loss: 0.07931787520647049
train_iter_loss: 0.28443002700805664
train_iter_loss: 0.1469237208366394
train_iter_loss: 0.2074187695980072
train_iter_loss: 0.1066359207034111
train_iter_loss: 0.18617013096809387
train_iter_loss: 0.2455805242061615
train_iter_loss: 0.2162293642759323
train_iter_loss: 0.24592746794223785
train_iter_loss: 0.18525296449661255
train_iter_loss: 0.12292608618736267
train_iter_loss: 0.18228933215141296
train_iter_loss: 0.07901983708143234
train_iter_loss: 0.19408294558525085
train_iter_loss: 0.0759977251291275
train_iter_loss: 0.19777224957942963
train_iter_loss: 0.14243994653224945
train_iter_loss: 0.18552523851394653
train_iter_loss: 0.20211350917816162
train_iter_loss: 0.17914649844169617
train_iter_loss: 0.24024544656276703
train_iter_loss: 0.12194215506315231
train_iter_loss: 0.23540295660495758
train_iter_loss: 0.24817852675914764
train_iter_loss: 0.1568484753370285
train_iter_loss: 0.1237158551812172
train_iter_loss: 0.12977899610996246
train_iter_loss: 0.14595556259155273
train_iter_loss: 0.13239628076553345
train_iter_loss: 0.10858403891324997
train_iter_loss: 0.14117799699306488
train_iter_loss: 0.1835847944021225
train loss :0.1675
---------------------
Validation seg loss: 0.2184516575257733 at epoch 389
epoch =    390/  1000, exp = train
train_iter_loss: 0.16630113124847412
train_iter_loss: 0.1692328304052353
train_iter_loss: 0.028111565858125687
train_iter_loss: 0.2980796694755554
train_iter_loss: 0.14492079615592957
train_iter_loss: 0.07869289815425873
train_iter_loss: 0.2614476978778839
train_iter_loss: 0.17747896909713745
train_iter_loss: 0.31025251746177673
train_iter_loss: 0.20256805419921875
train_iter_loss: 0.17760594189167023
train_iter_loss: 0.1534663587808609
train_iter_loss: 0.169589102268219
train_iter_loss: 0.2171890288591385
train_iter_loss: 0.13694672286510468
train_iter_loss: 0.18400098383426666
train_iter_loss: 0.15629728138446808
train_iter_loss: 0.11266074329614639
train_iter_loss: 0.1948958784341812
train_iter_loss: 0.16018860042095184
train_iter_loss: 0.14485250413417816
train_iter_loss: 0.2152377814054489
train_iter_loss: 0.1318124681711197
train_iter_loss: 0.14312922954559326
train_iter_loss: 0.204492449760437
train_iter_loss: 0.14879977703094482
train_iter_loss: 0.07847867906093597
train_iter_loss: 0.1419207900762558
train_iter_loss: 0.1579848825931549
train_iter_loss: 0.12507393956184387
train_iter_loss: 0.1606619656085968
train_iter_loss: 0.19800728559494019
train_iter_loss: 0.1116364449262619
train_iter_loss: 0.1396566778421402
train_iter_loss: 0.08517022430896759
train_iter_loss: 0.1915234923362732
train_iter_loss: 0.09889689087867737
train_iter_loss: 0.16569532454013824
train_iter_loss: 0.0996565893292427
train_iter_loss: 0.17878389358520508
train_iter_loss: 0.24381579458713531
train_iter_loss: 0.0823485404253006
train_iter_loss: 0.08493166416883469
train_iter_loss: 0.13294818997383118
train_iter_loss: 0.18276460468769073
train_iter_loss: 0.11125712841749191
train_iter_loss: 0.22368073463439941
train_iter_loss: 0.09707466512918472
train_iter_loss: 0.09843936562538147
train_iter_loss: 0.1724199503660202
train_iter_loss: 0.14598071575164795
train_iter_loss: 0.05944972112774849
train_iter_loss: 0.26048538088798523
train_iter_loss: 0.15648292005062103
train_iter_loss: 0.2039494514465332
train_iter_loss: 0.10558965057134628
train_iter_loss: 0.23259463906288147
train_iter_loss: 0.10764311999082565
train_iter_loss: 0.24749194085597992
train_iter_loss: 0.1654871553182602
train_iter_loss: 0.12509116530418396
train_iter_loss: 0.2326427698135376
train_iter_loss: 0.06046150624752045
train_iter_loss: 0.19402828812599182
train_iter_loss: 0.25617218017578125
train_iter_loss: 0.19979560375213623
train_iter_loss: 0.16577863693237305
train_iter_loss: 0.18485331535339355
train_iter_loss: 0.12439578026533127
train_iter_loss: 0.3064374327659607
train_iter_loss: 0.1987130045890808
train_iter_loss: 0.1940523236989975
train_iter_loss: 0.10279010236263275
train_iter_loss: 0.23952428996562958
train_iter_loss: 0.09872474521398544
train_iter_loss: 0.15484900772571564
train_iter_loss: 0.13494552671909332
train_iter_loss: 0.11563269048929214
train_iter_loss: 0.13009808957576752
train_iter_loss: 0.17669498920440674
train_iter_loss: 0.15877114236354828
train_iter_loss: 0.07974714785814285
train_iter_loss: 0.08626370131969452
train_iter_loss: 0.06030084192752838
train_iter_loss: 0.18374210596084595
train_iter_loss: 0.3819231390953064
train_iter_loss: 0.12984496355056763
train_iter_loss: 0.12964947521686554
train_iter_loss: 0.23303431272506714
train_iter_loss: 0.16330212354660034
train_iter_loss: 0.4500126540660858
train_iter_loss: 0.21956174075603485
train_iter_loss: 0.14192746579647064
train_iter_loss: 0.21541649103164673
train_iter_loss: 0.043715186417102814
train_iter_loss: 0.1382531225681305
train_iter_loss: 0.20771825313568115
train_iter_loss: 0.1732543408870697
train_iter_loss: 0.17260970175266266
train_iter_loss: 0.1678939163684845
train loss :0.1664
---------------------
Validation seg loss: 0.22129226372278524 at epoch 390
epoch =    391/  1000, exp = train
train_iter_loss: 0.1806807965040207
train_iter_loss: 0.15623115003108978
train_iter_loss: 0.3047347664833069
train_iter_loss: 0.16857413947582245
train_iter_loss: 0.25064411759376526
train_iter_loss: 0.14956167340278625
train_iter_loss: 0.23398171365261078
train_iter_loss: 0.18575778603553772
train_iter_loss: 0.08313869684934616
train_iter_loss: 0.1331423670053482
train_iter_loss: 0.18151451647281647
train_iter_loss: 0.18423742055892944
train_iter_loss: 0.10875596851110458
train_iter_loss: 0.24133872985839844
train_iter_loss: 0.16506583988666534
train_iter_loss: 0.1854780912399292
train_iter_loss: 0.12518420815467834
train_iter_loss: 0.08995170891284943
train_iter_loss: 0.26231974363327026
train_iter_loss: 0.2413354516029358
train_iter_loss: 0.14331817626953125
train_iter_loss: 0.1590563952922821
train_iter_loss: 0.15444988012313843
train_iter_loss: 0.2752789258956909
train_iter_loss: 0.20017093420028687
train_iter_loss: 0.18353578448295593
train_iter_loss: 0.21412354707717896
train_iter_loss: 0.2811243236064911
train_iter_loss: 0.1251138299703598
train_iter_loss: 0.23125022649765015
train_iter_loss: 0.25515443086624146
train_iter_loss: 0.17349885404109955
train_iter_loss: 0.20492275059223175
train_iter_loss: 0.110537089407444
train_iter_loss: 0.3015454411506653
train_iter_loss: 0.15731534361839294
train_iter_loss: 0.17398011684417725
train_iter_loss: 0.13316641747951508
train_iter_loss: 0.14903025329113007
train_iter_loss: 0.19715122878551483
train_iter_loss: 0.11415757238864899
train_iter_loss: 0.11831264942884445
train_iter_loss: 0.1738058179616928
train_iter_loss: 0.21326564252376556
train_iter_loss: 0.23828309774398804
train_iter_loss: 0.10765548795461655
train_iter_loss: 0.21773386001586914
train_iter_loss: 0.14881768822669983
train_iter_loss: 0.08913551270961761
train_iter_loss: 0.23733291029930115
train_iter_loss: 0.17984884977340698
train_iter_loss: 0.15149225294589996
train_iter_loss: 0.06457027047872543
train_iter_loss: 0.12407699227333069
train_iter_loss: 0.2549576163291931
train_iter_loss: 0.16102652251720428
train_iter_loss: 0.1810176819562912
train_iter_loss: 0.16167062520980835
train_iter_loss: 0.2866407632827759
train_iter_loss: 0.12066207081079483
train_iter_loss: 0.10443301498889923
train_iter_loss: 0.14669597148895264
train_iter_loss: 0.12342742830514908
train_iter_loss: 0.14951910078525543
train_iter_loss: 0.05755893886089325
train_iter_loss: 0.15387842059135437
train_iter_loss: 0.14830167591571808
train_iter_loss: 0.2547279894351959
train_iter_loss: 0.12098562717437744
train_iter_loss: 0.19616997241973877
train_iter_loss: 0.13545352220535278
train_iter_loss: 0.12131825089454651
train_iter_loss: 0.13999317586421967
train_iter_loss: 0.09801395237445831
train_iter_loss: 0.17414511740207672
train_iter_loss: 0.2061871886253357
train_iter_loss: 0.07220766693353653
train_iter_loss: 0.2379467636346817
train_iter_loss: 0.5705873966217041
train_iter_loss: 0.14525772631168365
train_iter_loss: 0.12643353641033173
train_iter_loss: 0.13142317533493042
train_iter_loss: 0.14093704521656036
train_iter_loss: 0.10540442168712616
train_iter_loss: 0.23799683153629303
train_iter_loss: 0.08918556571006775
train_iter_loss: 0.10875942558050156
train_iter_loss: 0.16181959211826324
train_iter_loss: 0.1844102144241333
train_iter_loss: 0.06975405663251877
train_iter_loss: 0.13564828038215637
train_iter_loss: 0.2605683207511902
train_iter_loss: 0.06443807482719421
train_iter_loss: 0.11064133793115616
train_iter_loss: 0.23060515522956848
train_iter_loss: 0.122463159263134
train_iter_loss: 0.3307824730873108
train_iter_loss: 0.20380793511867523
train_iter_loss: 0.14414282143115997
train_iter_loss: 0.19657070934772491
train loss :0.1748
---------------------
Validation seg loss: 0.21377243424923914 at epoch 391
********************
best_val_epoch_loss:  0.21377243424923914
MODEL UPDATED
epoch =    392/  1000, exp = train
train_iter_loss: 0.14568014442920685
train_iter_loss: 0.1573898047208786
train_iter_loss: 0.17294180393218994
train_iter_loss: 0.11978473514318466
train_iter_loss: 0.15552708506584167
train_iter_loss: 0.17682720720767975
train_iter_loss: 0.20779165625572205
train_iter_loss: 0.18339012563228607
train_iter_loss: 0.33053186535835266
train_iter_loss: 0.20764906704425812
train_iter_loss: 0.18168140947818756
train_iter_loss: 0.19845469295978546
train_iter_loss: 0.19735771417617798
train_iter_loss: 0.14559325575828552
train_iter_loss: 0.1261022984981537
train_iter_loss: 0.16482146084308624
train_iter_loss: 0.12254101037979126
train_iter_loss: 0.12655332684516907
train_iter_loss: 0.1545315533876419
train_iter_loss: 0.1720171719789505
train_iter_loss: 0.25594383478164673
train_iter_loss: 0.2155226618051529
train_iter_loss: 0.18547846376895905
train_iter_loss: 0.16692574322223663
train_iter_loss: 0.10865405946969986
train_iter_loss: 0.13308574259281158
train_iter_loss: 0.11326691508293152
train_iter_loss: 0.10415831208229065
train_iter_loss: 0.1717112809419632
train_iter_loss: 0.19669538736343384
train_iter_loss: 0.0730600655078888
train_iter_loss: 0.1563638299703598
train_iter_loss: 0.11986253410577774
train_iter_loss: 0.13523530960083008
train_iter_loss: 0.2715924382209778
train_iter_loss: 0.22076283395290375
train_iter_loss: 0.21779702603816986
train_iter_loss: 0.09663163125514984
train_iter_loss: 0.22951066493988037
train_iter_loss: 0.26617366075515747
train_iter_loss: 0.2520139217376709
train_iter_loss: 0.1736985296010971
train_iter_loss: 0.11025670170783997
train_iter_loss: 0.09887492656707764
train_iter_loss: 0.13139304518699646
train_iter_loss: 0.3407885432243347
train_iter_loss: 0.1207595244050026
train_iter_loss: 0.10394728928804398
train_iter_loss: 0.10422590374946594
train_iter_loss: 0.15625803172588348
train_iter_loss: 0.21649226546287537
train_iter_loss: 0.12619377672672272
train_iter_loss: 0.17001992464065552
train_iter_loss: 0.1625952571630478
train_iter_loss: 0.13503463566303253
train_iter_loss: 0.19361481070518494
train_iter_loss: 0.21856863796710968
train_iter_loss: 0.11563900113105774
train_iter_loss: 0.2512543499469757
train_iter_loss: 0.13436654210090637
train_iter_loss: 0.11839564889669418
train_iter_loss: 0.138591930270195
train_iter_loss: 0.11213633418083191
train_iter_loss: 0.15547850728034973
train_iter_loss: 0.16588379442691803
train_iter_loss: 0.11916781961917877
train_iter_loss: 0.0974002555012703
train_iter_loss: 0.15229648351669312
train_iter_loss: 0.14557047188282013
train_iter_loss: 0.1552324742078781
train_iter_loss: 0.18522557616233826
train_iter_loss: 0.25391772389411926
train_iter_loss: 0.13044515252113342
train_iter_loss: 0.2317432165145874
train_iter_loss: 0.15669280290603638
train_iter_loss: 0.11420624703168869
train_iter_loss: 0.2528444528579712
train_iter_loss: 0.15578728914260864
train_iter_loss: 0.1553286761045456
train_iter_loss: 0.2033809870481491
train_iter_loss: 0.14984066784381866
train_iter_loss: 0.08560419827699661
train_iter_loss: 0.12753744423389435
train_iter_loss: 0.11071062833070755
train_iter_loss: 0.18303033709526062
train_iter_loss: 0.09765717387199402
train_iter_loss: 0.19728778302669525
train_iter_loss: 0.05396226793527603
train_iter_loss: 0.1634119302034378
train_iter_loss: 0.1923021525144577
train_iter_loss: 0.10184412449598312
train_iter_loss: 0.1290900856256485
train_iter_loss: 0.27616551518440247
train_iter_loss: 0.18659155070781708
train_iter_loss: 0.3154899477958679
train_iter_loss: 0.16536849737167358
train_iter_loss: 0.0589003786444664
train_iter_loss: 0.12366963922977448
train_iter_loss: 0.1707576960325241
train_iter_loss: 0.16249501705169678
train loss :0.1663
---------------------
Validation seg loss: 0.21856792814115872 at epoch 392
epoch =    393/  1000, exp = train
train_iter_loss: 0.33634984493255615
train_iter_loss: 0.2093179076910019
train_iter_loss: 0.11355983465909958
train_iter_loss: 0.1387590765953064
train_iter_loss: 0.19220909476280212
train_iter_loss: 0.1263478398323059
train_iter_loss: 0.048071883618831635
train_iter_loss: 0.21034875512123108
train_iter_loss: 0.1319068968296051
train_iter_loss: 0.15831996500492096
train_iter_loss: 0.11070951819419861
train_iter_loss: 0.21209657192230225
train_iter_loss: 0.11185859143733978
train_iter_loss: 0.09904348105192184
train_iter_loss: 0.13822320103645325
train_iter_loss: 0.10585282742977142
train_iter_loss: 0.09288389980792999
train_iter_loss: 0.23802220821380615
train_iter_loss: 0.09513425827026367
train_iter_loss: 0.27594801783561707
train_iter_loss: 0.06258045136928558
train_iter_loss: 0.14622938632965088
train_iter_loss: 0.12134259939193726
train_iter_loss: 0.12421424686908722
train_iter_loss: 0.07485982030630112
train_iter_loss: 0.12890391051769257
train_iter_loss: 0.134343221783638
train_iter_loss: 0.14999446272850037
train_iter_loss: 0.06465180218219757
train_iter_loss: 0.22850273549556732
train_iter_loss: 0.1658903956413269
train_iter_loss: 0.08206523209810257
train_iter_loss: 0.13811878859996796
train_iter_loss: 0.10544268786907196
train_iter_loss: 0.25773540139198303
train_iter_loss: 0.30172497034072876
train_iter_loss: 0.24890056252479553
train_iter_loss: 0.17387013137340546
train_iter_loss: 0.27348217368125916
train_iter_loss: 0.26927828788757324
train_iter_loss: 0.15221258997917175
train_iter_loss: 0.4137405455112457
train_iter_loss: 0.058955978602170944
train_iter_loss: 0.23873406648635864
train_iter_loss: 0.24940115213394165
train_iter_loss: 0.07514715939760208
train_iter_loss: 0.21435512602329254
train_iter_loss: 0.2419036477804184
train_iter_loss: 0.14521373808383942
train_iter_loss: 0.1718561351299286
train_iter_loss: 0.11969172209501266
train_iter_loss: 0.10396913439035416
train_iter_loss: 0.1430709809064865
train_iter_loss: 0.2671239376068115
train_iter_loss: 0.10421524196863174
train_iter_loss: 0.13904669880867004
train_iter_loss: 0.172282874584198
train_iter_loss: 0.13399015367031097
train_iter_loss: 0.12032493948936462
train_iter_loss: 0.14982692897319794
train_iter_loss: 0.3104504346847534
train_iter_loss: 0.1966225951910019
train_iter_loss: 0.17851078510284424
train_iter_loss: 0.2311176210641861
train_iter_loss: 0.2623099982738495
train_iter_loss: 0.16561485826969147
train_iter_loss: 0.188683420419693
train_iter_loss: 0.10326245427131653
train_iter_loss: 0.08156768232584
train_iter_loss: 0.17583715915679932
train_iter_loss: 0.0939907431602478
train_iter_loss: 0.1767791509628296
train_iter_loss: 0.15942002832889557
train_iter_loss: 0.3263168931007385
train_iter_loss: 0.11551336199045181
train_iter_loss: 0.1782868206501007
train_iter_loss: 0.248321533203125
train_iter_loss: 0.05883315950632095
train_iter_loss: 0.10145044326782227
train_iter_loss: 0.24336837232112885
train_iter_loss: 0.06936801224946976
train_iter_loss: 0.1204930990934372
train_iter_loss: 0.14163950085639954
train_iter_loss: 0.11577777564525604
train_iter_loss: 0.08581576496362686
train_iter_loss: 0.0967116504907608
train_iter_loss: 0.23003141582012177
train_iter_loss: 0.2552073001861572
train_iter_loss: 0.19987843930721283
train_iter_loss: 0.12435685098171234
train_iter_loss: 0.13272467255592346
train_iter_loss: 0.1470288634300232
train_iter_loss: 0.19041691720485687
train_iter_loss: 0.1134980320930481
train_iter_loss: 0.16007624566555023
train_iter_loss: 0.1366506665945053
train_iter_loss: 0.06930915266275406
train_iter_loss: 0.1533336043357849
train_iter_loss: 0.24372288584709167
train_iter_loss: 0.18333527445793152
train loss :0.1654
---------------------
Validation seg loss: 0.2194304928921585 at epoch 393
epoch =    394/  1000, exp = train
train_iter_loss: 0.09113238751888275
train_iter_loss: 0.2604410648345947
train_iter_loss: 0.1699526309967041
train_iter_loss: 0.16537322103977203
train_iter_loss: 0.14754608273506165
train_iter_loss: 0.46863701939582825
train_iter_loss: 0.19122453033924103
train_iter_loss: 0.13839091360569
train_iter_loss: 0.12019885331392288
train_iter_loss: 0.11198706924915314
train_iter_loss: 0.09601490944623947
train_iter_loss: 0.12541823089122772
train_iter_loss: 0.1511307656764984
train_iter_loss: 0.19542695581912994
train_iter_loss: 0.09282353520393372
train_iter_loss: 0.13590574264526367
train_iter_loss: 0.11266164481639862
train_iter_loss: 0.14420829713344574
train_iter_loss: 0.18236292898654938
train_iter_loss: 0.2300521731376648
train_iter_loss: 0.049665503203868866
train_iter_loss: 0.3456432819366455
train_iter_loss: 0.1256645768880844
train_iter_loss: 0.18820397555828094
train_iter_loss: 0.24250414967536926
train_iter_loss: 0.1557832956314087
train_iter_loss: 0.11614415049552917
train_iter_loss: 0.1392507702112198
train_iter_loss: 0.18443401157855988
train_iter_loss: 0.0997501015663147
train_iter_loss: 0.11677026003599167
train_iter_loss: 0.211787611246109
train_iter_loss: 0.1176883727312088
train_iter_loss: 0.20667122304439545
train_iter_loss: 0.176755890250206
train_iter_loss: 0.12118884921073914
train_iter_loss: 0.10817088931798935
train_iter_loss: 0.16835865378379822
train_iter_loss: 0.10843170434236526
train_iter_loss: 0.25380098819732666
train_iter_loss: 0.11230072379112244
train_iter_loss: 0.09836479276418686
train_iter_loss: 0.112711600959301
train_iter_loss: 0.16178753972053528
train_iter_loss: 0.2074640691280365
train_iter_loss: 0.06284277886152267
train_iter_loss: 0.1683928221464157
train_iter_loss: 0.1311512589454651
train_iter_loss: 0.17069268226623535
train_iter_loss: 0.11046981066465378
train_iter_loss: 0.13430817425251007
train_iter_loss: 0.06886846572160721
train_iter_loss: 0.12218718230724335
train_iter_loss: 0.1064440980553627
train_iter_loss: 0.0656476616859436
train_iter_loss: 0.3067531883716583
train_iter_loss: 0.18673181533813477
train_iter_loss: 0.10005773603916168
train_iter_loss: 0.21512186527252197
train_iter_loss: 0.0872272178530693
train_iter_loss: 0.18514585494995117
train_iter_loss: 0.18030185997486115
train_iter_loss: 0.045807305723428726
train_iter_loss: 0.23448659479618073
train_iter_loss: 0.20613907277584076
train_iter_loss: 0.1333717256784439
train_iter_loss: 0.22500112652778625
train_iter_loss: 0.13442856073379517
train_iter_loss: 0.28439050912857056
train_iter_loss: 0.19444063305854797
train_iter_loss: 0.11380157619714737
train_iter_loss: 0.14443576335906982
train_iter_loss: 0.19359056651592255
train_iter_loss: 0.13970978558063507
train_iter_loss: 0.12155979126691818
train_iter_loss: 0.12483744323253632
train_iter_loss: 0.21713627874851227
train_iter_loss: 0.13907548785209656
train_iter_loss: 0.22027687728405
train_iter_loss: 0.2223902940750122
train_iter_loss: 0.21414457261562347
train_iter_loss: 0.14319856464862823
train_iter_loss: 0.11403435468673706
train_iter_loss: 0.1599382609128952
train_iter_loss: 0.1988094300031662
train_iter_loss: 0.28188902139663696
train_iter_loss: 0.1695096492767334
train_iter_loss: 0.1494160145521164
train_iter_loss: 0.16544729471206665
train_iter_loss: 0.1471317857503891
train_iter_loss: 0.3246883749961853
train_iter_loss: 0.31315651535987854
train_iter_loss: 0.4027464985847473
train_iter_loss: 0.09854269027709961
train_iter_loss: 0.2250664383172989
train_iter_loss: 0.13607488572597504
train_iter_loss: 0.058278925716876984
train_iter_loss: 0.17188262939453125
train_iter_loss: 0.2299375981092453
train_iter_loss: 0.08958061784505844
train loss :0.1672
---------------------
Validation seg loss: 0.22222543656298854 at epoch 394
epoch =    395/  1000, exp = train
train_iter_loss: 0.18071001768112183
train_iter_loss: 0.05753953382372856
train_iter_loss: 0.16796724498271942
train_iter_loss: 0.21219582855701447
train_iter_loss: 0.1309078335762024
train_iter_loss: 0.14851579070091248
train_iter_loss: 0.1496271789073944
train_iter_loss: 0.17387591302394867
train_iter_loss: 0.10295691341161728
train_iter_loss: 0.20434655249118805
train_iter_loss: 0.12882038950920105
train_iter_loss: 0.2067434936761856
train_iter_loss: 0.16257905960083008
train_iter_loss: 0.10409984737634659
train_iter_loss: 0.15279783308506012
train_iter_loss: 0.23284389078617096
train_iter_loss: 0.15078581869602203
train_iter_loss: 0.09870626032352448
train_iter_loss: 0.14652149379253387
train_iter_loss: 0.11252070963382721
train_iter_loss: 0.10035005956888199
train_iter_loss: 0.0905633345246315
train_iter_loss: 0.15046295523643494
train_iter_loss: 0.36072155833244324
train_iter_loss: 0.25409069657325745
train_iter_loss: 0.13256770372390747
train_iter_loss: 0.1849999725818634
train_iter_loss: 0.32110682129859924
train_iter_loss: 0.17902632057666779
train_iter_loss: 0.17685839533805847
train_iter_loss: 0.19867411255836487
train_iter_loss: 0.07887864857912064
train_iter_loss: 0.09868969768285751
train_iter_loss: 0.13001984357833862
train_iter_loss: 0.21139436960220337
train_iter_loss: 0.1940896064043045
train_iter_loss: 0.17139995098114014
train_iter_loss: 0.17171616852283478
train_iter_loss: 0.13217031955718994
train_iter_loss: 0.12622979283332825
train_iter_loss: 0.13293501734733582
train_iter_loss: 0.16985563933849335
train_iter_loss: 0.10045617073774338
train_iter_loss: 0.3163639307022095
train_iter_loss: 0.38530251383781433
train_iter_loss: 0.10405202209949493
train_iter_loss: 0.2287711203098297
train_iter_loss: 0.1726001352071762
train_iter_loss: 0.1614692211151123
train_iter_loss: 0.16189658641815186
train_iter_loss: 0.08479858934879303
train_iter_loss: 0.21106095612049103
train_iter_loss: 0.18962053954601288
train_iter_loss: 0.09427977353334427
train_iter_loss: 0.13238130509853363
train_iter_loss: 0.15924881398677826
train_iter_loss: 0.35965240001678467
train_iter_loss: 0.13844306766986847
train_iter_loss: 0.1171480044722557
train_iter_loss: 0.14123737812042236
train_iter_loss: 0.19493308663368225
train_iter_loss: 0.12663817405700684
train_iter_loss: 0.37433990836143494
train_iter_loss: 0.2475731521844864
train_iter_loss: 0.20382215082645416
train_iter_loss: 0.13073524832725525
train_iter_loss: 0.1428397297859192
train_iter_loss: 0.29620078206062317
train_iter_loss: 0.11153914034366608
train_iter_loss: 0.15757238864898682
train_iter_loss: 0.1201755553483963
train_iter_loss: 0.14282865822315216
train_iter_loss: 0.237521231174469
train_iter_loss: 0.1461821049451828
train_iter_loss: 0.14056207239627838
train_iter_loss: 0.1107855886220932
train_iter_loss: 0.14299844205379486
train_iter_loss: 0.1492321640253067
train_iter_loss: 0.20805975794792175
train_iter_loss: 0.13368608057498932
train_iter_loss: 0.17743892967700958
train_iter_loss: 0.12719151377677917
train_iter_loss: 0.1113470047712326
train_iter_loss: 0.0732959434390068
train_iter_loss: 0.10224834084510803
train_iter_loss: 0.2471630871295929
train_iter_loss: 0.1486920565366745
train_iter_loss: 0.1607944369316101
train_iter_loss: 0.1469794064760208
train_iter_loss: 0.23738622665405273
train_iter_loss: 0.0724358782172203
train_iter_loss: 0.16753941774368286
train_iter_loss: 0.1030842587351799
train_iter_loss: 0.0854402557015419
train_iter_loss: 0.2926892936229706
train_iter_loss: 0.17282281816005707
train_iter_loss: 0.07105324417352676
train_iter_loss: 0.15934325754642487
train_iter_loss: 0.18322354555130005
train_iter_loss: 0.17790338397026062
train loss :0.1675
---------------------
Validation seg loss: 0.22076896982232355 at epoch 395
epoch =    396/  1000, exp = train
train_iter_loss: 0.23920492827892303
train_iter_loss: 0.2132984846830368
train_iter_loss: 0.1594318449497223
train_iter_loss: 0.14904063940048218
train_iter_loss: 0.12346171587705612
train_iter_loss: 0.0951286256313324
train_iter_loss: 0.12268123775720596
train_iter_loss: 0.21608518064022064
train_iter_loss: 0.24801501631736755
train_iter_loss: 0.20022815465927124
train_iter_loss: 0.1464565098285675
train_iter_loss: 0.2312479466199875
train_iter_loss: 0.1427464336156845
train_iter_loss: 0.07239221781492233
train_iter_loss: 0.2727266550064087
train_iter_loss: 0.17118051648139954
train_iter_loss: 0.1517004668712616
train_iter_loss: 0.23951217532157898
train_iter_loss: 0.06781940162181854
train_iter_loss: 0.15726308524608612
train_iter_loss: 0.17958956956863403
train_iter_loss: 0.15164229273796082
train_iter_loss: 0.08610112220048904
train_iter_loss: 0.10082931071519852
train_iter_loss: 0.16012734174728394
train_iter_loss: 0.21773433685302734
train_iter_loss: 0.21222670376300812
train_iter_loss: 0.1313546895980835
train_iter_loss: 0.15087558329105377
train_iter_loss: 0.20504975318908691
train_iter_loss: 0.113982655107975
train_iter_loss: 0.22779150307178497
train_iter_loss: 0.03589828684926033
train_iter_loss: 0.2578964829444885
train_iter_loss: 0.07457448542118073
train_iter_loss: 0.16056811809539795
train_iter_loss: 0.11014533042907715
train_iter_loss: 0.10423282533884048
train_iter_loss: 0.14316260814666748
train_iter_loss: 0.12949621677398682
train_iter_loss: 0.2464739978313446
train_iter_loss: 0.20337696373462677
train_iter_loss: 0.28891754150390625
train_iter_loss: 0.15444831550121307
train_iter_loss: 0.10198590159416199
train_iter_loss: 0.19123592972755432
train_iter_loss: 0.16174408793449402
train_iter_loss: 0.05914147198200226
train_iter_loss: 0.15330809354782104
train_iter_loss: 0.23761425912380219
train_iter_loss: 0.19537805020809174
train_iter_loss: 0.179612398147583
train_iter_loss: 0.25390350818634033
train_iter_loss: 0.14363183081150055
train_iter_loss: 0.11751203238964081
train_iter_loss: 0.12105056643486023
train_iter_loss: 0.12366724014282227
train_iter_loss: 0.20595461130142212
train_iter_loss: 0.21623794734477997
train_iter_loss: 0.18628069758415222
train_iter_loss: 0.17879727482795715
train_iter_loss: 0.1428850293159485
train_iter_loss: 0.04785265401005745
train_iter_loss: 0.1981504112482071
train_iter_loss: 0.13911710679531097
train_iter_loss: 0.13251742720603943
train_iter_loss: 0.14990471303462982
train_iter_loss: 0.14082202315330505
train_iter_loss: 0.16466639935970306
train_iter_loss: 0.13514558970928192
train_iter_loss: 0.179954394698143
train_iter_loss: 0.298374742269516
train_iter_loss: 0.14534495770931244
train_iter_loss: 0.15428592264652252
train_iter_loss: 0.19510622322559357
train_iter_loss: 0.21537116169929504
train_iter_loss: 0.10012031346559525
train_iter_loss: 0.19932755827903748
train_iter_loss: 0.11466091126203537
train_iter_loss: 0.134460911154747
train_iter_loss: 0.129497691988945
train_iter_loss: 0.13699185848236084
train_iter_loss: 0.23442643880844116
train_iter_loss: 0.20652586221694946
train_iter_loss: 0.19924336671829224
train_iter_loss: 0.08716309815645218
train_iter_loss: 0.13359159231185913
train_iter_loss: 0.12003005295991898
train_iter_loss: 0.04357471317052841
train_iter_loss: 0.16036243736743927
train_iter_loss: 0.1487426608800888
train_iter_loss: 0.12466267496347427
train_iter_loss: 0.16433049738407135
train_iter_loss: 0.14818555116653442
train_iter_loss: 0.09638535231351852
train_iter_loss: 0.1625620722770691
train_iter_loss: 0.17843887209892273
train_iter_loss: 0.10035772621631622
train_iter_loss: 0.10752634704113007
train_iter_loss: 0.1566930115222931
train loss :0.1609
---------------------
Validation seg loss: 0.21671889023855329 at epoch 396
epoch =    397/  1000, exp = train
train_iter_loss: 0.19351550936698914
train_iter_loss: 0.10877719521522522
train_iter_loss: 0.22092749178409576
train_iter_loss: 0.08615853637456894
train_iter_loss: 0.06458884477615356
train_iter_loss: 0.1933455914258957
train_iter_loss: 0.18038906157016754
train_iter_loss: 0.23172099888324738
train_iter_loss: 0.12746986746788025
train_iter_loss: 0.2034260481595993
train_iter_loss: 0.16296489536762238
train_iter_loss: 0.17615464329719543
train_iter_loss: 0.12769098579883575
train_iter_loss: 0.10738354921340942
train_iter_loss: 0.23700495064258575
train_iter_loss: 0.18005028367042542
train_iter_loss: 0.18491679430007935
train_iter_loss: 0.17469090223312378
train_iter_loss: 0.21531851589679718
train_iter_loss: 0.3017783761024475
train_iter_loss: 0.14545845985412598
train_iter_loss: 0.2203797549009323
train_iter_loss: 0.1895170509815216
train_iter_loss: 0.17885196208953857
train_iter_loss: 0.25147920846939087
train_iter_loss: 0.07346207648515701
train_iter_loss: 0.1320771872997284
train_iter_loss: 0.16475431621074677
train_iter_loss: 0.13366396725177765
train_iter_loss: 0.39720138907432556
train_iter_loss: 0.17368832230567932
train_iter_loss: 0.09953708946704865
train_iter_loss: 0.1643766164779663
train_iter_loss: 0.09462463110685349
train_iter_loss: 0.0901799276471138
train_iter_loss: 0.100519560277462
train_iter_loss: 0.14654244482517242
train_iter_loss: 0.17157185077667236
train_iter_loss: 0.17497947812080383
train_iter_loss: 0.12143952399492264
train_iter_loss: 0.1292201280593872
train_iter_loss: 0.17193089425563812
train_iter_loss: 0.07882445305585861
train_iter_loss: 0.13576988875865936
train_iter_loss: 0.11970949918031693
train_iter_loss: 0.25576502084732056
train_iter_loss: 0.24290573596954346
train_iter_loss: 0.15233591198921204
train_iter_loss: 0.20528161525726318
train_iter_loss: 0.1300886869430542
train_iter_loss: 0.08623339980840683
train_iter_loss: 0.102731853723526
train_iter_loss: 0.08309093117713928
train_iter_loss: 0.13714462518692017
train_iter_loss: 0.11495751887559891
train_iter_loss: 0.20103059709072113
train_iter_loss: 0.1509091854095459
train_iter_loss: 0.153420552611351
train_iter_loss: 0.09055150300264359
train_iter_loss: 0.13132824003696442
train_iter_loss: 0.22160989046096802
train_iter_loss: 0.18493404984474182
train_iter_loss: 0.14595146477222443
train_iter_loss: 0.1258755326271057
train_iter_loss: 0.18242791295051575
train_iter_loss: 0.16636568307876587
train_iter_loss: 0.08521407842636108
train_iter_loss: 0.21693359315395355
train_iter_loss: 0.1321551352739334
train_iter_loss: 0.2201618105173111
train_iter_loss: 0.14441922307014465
train_iter_loss: 0.15366382896900177
train_iter_loss: 0.12735383212566376
train_iter_loss: 0.11611180752515793
train_iter_loss: 0.2550627291202545
train_iter_loss: 0.29649361968040466
train_iter_loss: 0.26588401198387146
train_iter_loss: 0.12265137583017349
train_iter_loss: 0.22783133387565613
train_iter_loss: 0.1307356357574463
train_iter_loss: 0.23617994785308838
train_iter_loss: 0.10356777906417847
train_iter_loss: 0.18472635746002197
train_iter_loss: 0.17508837580680847
train_iter_loss: 0.255094051361084
train_iter_loss: 0.21438567340373993
train_iter_loss: 0.10994282364845276
train_iter_loss: 0.15385952591896057
train_iter_loss: 0.09250897169113159
train_iter_loss: 0.1788146197795868
train_iter_loss: 0.2030225545167923
train_iter_loss: 0.22718283534049988
train_iter_loss: 0.3534223735332489
train_iter_loss: 0.15045613050460815
train_iter_loss: 0.1910327970981598
train_iter_loss: 0.14344079792499542
train_iter_loss: 0.30828365683555603
train_iter_loss: 0.11797196418046951
train_iter_loss: 0.21942101418972015
train_iter_loss: 0.11701710522174835
train loss :0.1699
---------------------
Validation seg loss: 0.22092719606282013 at epoch 397
epoch =    398/  1000, exp = train
train_iter_loss: 0.12077402323484421
train_iter_loss: 0.14623568952083588
train_iter_loss: 0.12107017636299133
train_iter_loss: 0.13317053020000458
train_iter_loss: 0.31371235847473145
train_iter_loss: 0.1855076104402542
train_iter_loss: 0.2108740359544754
train_iter_loss: 0.13301147520542145
train_iter_loss: 0.16615989804267883
train_iter_loss: 0.1539566069841385
train_iter_loss: 0.2548927962779999
train_iter_loss: 0.10031967610120773
train_iter_loss: 0.09683474153280258
train_iter_loss: 0.1796574890613556
train_iter_loss: 0.19998429715633392
train_iter_loss: 0.20953480899333954
train_iter_loss: 0.1272622048854828
train_iter_loss: 0.13089805841445923
train_iter_loss: 0.11357884109020233
train_iter_loss: 0.08483181148767471
train_iter_loss: 0.12139398604631424
train_iter_loss: 0.08792215585708618
train_iter_loss: 0.1274343878030777
train_iter_loss: 0.32004794478416443
train_iter_loss: 0.10224909335374832
train_iter_loss: 0.3110352158546448
train_iter_loss: 0.07101735472679138
train_iter_loss: 0.0673321858048439
train_iter_loss: 0.13118135929107666
train_iter_loss: 0.1393527090549469
train_iter_loss: 0.17092886567115784
train_iter_loss: 0.11573760956525803
train_iter_loss: 0.153262197971344
train_iter_loss: 0.06297522038221359
train_iter_loss: 0.08499424159526825
train_iter_loss: 0.1516287624835968
train_iter_loss: 0.13238118588924408
train_iter_loss: 0.21361719071865082
train_iter_loss: 0.17767328023910522
train_iter_loss: 0.11916887015104294
train_iter_loss: 0.37432989478111267
train_iter_loss: 0.21667487919330597
train_iter_loss: 0.21861842274665833
train_iter_loss: 0.05824802815914154
train_iter_loss: 0.12689051032066345
train_iter_loss: 0.13605979084968567
train_iter_loss: 0.0703970268368721
train_iter_loss: 0.12785223126411438
train_iter_loss: 0.1397857964038849
train_iter_loss: 0.18968242406845093
train_iter_loss: 0.13527077436447144
train_iter_loss: 0.04076597839593887
train_iter_loss: 0.14853376150131226
train_iter_loss: 0.16234077513217926
train_iter_loss: 0.19967755675315857
train_iter_loss: 0.17817406356334686
train_iter_loss: 0.09643995016813278
train_iter_loss: 0.11938200891017914
train_iter_loss: 0.1921178251504898
train_iter_loss: 0.11620110273361206
train_iter_loss: 0.12751038372516632
train_iter_loss: 0.0824754387140274
train_iter_loss: 0.1680610626935959
train_iter_loss: 0.09499102085828781
train_iter_loss: 0.10287675261497498
train_iter_loss: 0.16626204550266266
train_iter_loss: 0.0961378812789917
train_iter_loss: 0.15894785523414612
train_iter_loss: 0.2059028595685959
train_iter_loss: 0.16357581317424774
train_iter_loss: 0.194996178150177
train_iter_loss: 0.19488179683685303
train_iter_loss: 0.2599564492702484
train_iter_loss: 0.20913203060626984
train_iter_loss: 0.12836602330207825
train_iter_loss: 0.10640780627727509
train_iter_loss: 0.20052988827228546
train_iter_loss: 0.25608029961586
train_iter_loss: 0.1477057784795761
train_iter_loss: 0.18264563381671906
train_iter_loss: 0.206437885761261
train_iter_loss: 0.16285844147205353
train_iter_loss: 0.12272047251462936
train_iter_loss: 0.09284558147192001
train_iter_loss: 0.2404138296842575
train_iter_loss: 0.08972658962011337
train_iter_loss: 0.10219117999076843
train_iter_loss: 0.09564288705587387
train_iter_loss: 0.2650536596775055
train_iter_loss: 0.2794429659843445
train_iter_loss: 0.13188014924526215
train_iter_loss: 0.08375823497772217
train_iter_loss: 0.19365741312503815
train_iter_loss: 0.20099800825119019
train_iter_loss: 0.12785980105400085
train_iter_loss: 0.09077748656272888
train_iter_loss: 0.23312218487262726
train_iter_loss: 0.22729317843914032
train_iter_loss: 0.16985902190208435
train_iter_loss: 0.22343669831752777
train loss :0.1585
---------------------
Validation seg loss: 0.21832736548176915 at epoch 398
epoch =    399/  1000, exp = train
train_iter_loss: 0.13399526476860046
train_iter_loss: 0.30235186219215393
train_iter_loss: 0.0884828045964241
train_iter_loss: 0.1508694589138031
train_iter_loss: 0.12111524492502213
train_iter_loss: 0.2441527098417282
train_iter_loss: 0.06944157928228378
train_iter_loss: 0.14778169989585876
train_iter_loss: 0.16376043856143951
train_iter_loss: 0.14537706971168518
train_iter_loss: 0.15441158413887024
train_iter_loss: 0.3449363112449646
train_iter_loss: 0.19221003353595734
train_iter_loss: 0.11169540882110596
train_iter_loss: 0.20453917980194092
train_iter_loss: 0.20749877393245697
train_iter_loss: 0.1669161021709442
train_iter_loss: 0.09048806130886078
train_iter_loss: 0.08274199068546295
train_iter_loss: 0.3053951859474182
train_iter_loss: 0.12628096342086792
train_iter_loss: 0.08926665782928467
train_iter_loss: 0.1518591046333313
train_iter_loss: 0.10225104540586472
train_iter_loss: 0.21689850091934204
train_iter_loss: 0.02447512187063694
train_iter_loss: 0.1763310432434082
train_iter_loss: 0.284431129693985
train_iter_loss: 0.09317181259393692
train_iter_loss: 0.15612466633319855
train_iter_loss: 0.12324141710996628
train_iter_loss: 0.1971549540758133
train_iter_loss: 0.12372617423534393
train_iter_loss: 0.21535435318946838
train_iter_loss: 0.09830540418624878
train_iter_loss: 0.10376831889152527
train_iter_loss: 0.188445582985878
train_iter_loss: 0.10857836902141571
train_iter_loss: 0.28158849477767944
train_iter_loss: 0.14709483087062836
train_iter_loss: 0.264448344707489
train_iter_loss: 0.14659199118614197
train_iter_loss: 0.16383390128612518
train_iter_loss: 0.15924327075481415
train_iter_loss: 0.15610308945178986
train_iter_loss: 0.16546817123889923
train_iter_loss: 0.219810351729393
train_iter_loss: 0.18312808871269226
train_iter_loss: 0.13204586505889893
train_iter_loss: 0.16537544131278992
train_iter_loss: 0.08280541002750397
train_iter_loss: 0.24336305260658264
train_iter_loss: 0.09432078152894974
train_iter_loss: 0.2699420154094696
train_iter_loss: 0.11941404640674591
train_iter_loss: 0.18935713171958923
train_iter_loss: 0.24325567483901978
train_iter_loss: 0.07719595730304718
train_iter_loss: 0.11200977116823196
train_iter_loss: 0.1438957154750824
train_iter_loss: 0.1965702474117279
train_iter_loss: 0.08708100765943527
train_iter_loss: 0.13606417179107666
train_iter_loss: 0.15694868564605713
train_iter_loss: 0.20578965544700623
train_iter_loss: 0.1159725934267044
train_iter_loss: 0.23411867022514343
train_iter_loss: 0.11556816101074219
train_iter_loss: 0.1396419107913971
train_iter_loss: 0.31658217310905457
train_iter_loss: 0.26194754242897034
train_iter_loss: 0.1708916276693344
train_iter_loss: 0.1598406583070755
train_iter_loss: 0.19320687651634216
train_iter_loss: 0.13422229886054993
train_iter_loss: 0.1466834992170334
train_iter_loss: 0.23305287957191467
train_iter_loss: 0.14522768557071686
train_iter_loss: 0.13626685738563538
train_iter_loss: 0.2094666063785553
train_iter_loss: 0.14817018806934357
train_iter_loss: 0.11731605231761932
train_iter_loss: 0.09140709787607193
train_iter_loss: 0.21302199363708496
train_iter_loss: 0.11519089341163635
train_iter_loss: 0.07533229887485504
train_iter_loss: 0.22792351245880127
train_iter_loss: 0.1678410917520523
train_iter_loss: 0.07191265374422073
train_iter_loss: 0.09471280127763748
train_iter_loss: 0.1614115983247757
train_iter_loss: 0.10408475250005722
train_iter_loss: 0.13229326903820038
train_iter_loss: 0.22094273567199707
train_iter_loss: 0.16146662831306458
train_iter_loss: 0.15232503414154053
train_iter_loss: 0.2628869414329529
train_iter_loss: 0.27563226222991943
train_iter_loss: 0.22590748965740204
train_iter_loss: 0.22338007390499115
train loss :0.1666
---------------------
Validation seg loss: 0.2183066201747729 at epoch 399
epoch =    400/  1000, exp = train
train_iter_loss: 0.2556028366088867
train_iter_loss: 0.16041845083236694
train_iter_loss: 0.16878311336040497
train_iter_loss: 0.1656675934791565
train_iter_loss: 0.12759408354759216
train_iter_loss: 0.16382695734500885
train_iter_loss: 0.14314162731170654
train_iter_loss: 0.15668034553527832
train_iter_loss: 0.19442592561244965
train_iter_loss: 0.15279418230056763
train_iter_loss: 0.13064680993556976
train_iter_loss: 0.2285062074661255
train_iter_loss: 0.16545133292675018
train_iter_loss: 0.23232664167881012
train_iter_loss: 0.27575382590293884
train_iter_loss: 0.14646375179290771
train_iter_loss: 0.12464268505573273
train_iter_loss: 0.1501510888338089
train_iter_loss: 0.18240107595920563
train_iter_loss: 0.16186043620109558
train_iter_loss: 0.1797512173652649
train_iter_loss: 0.21180661022663116
train_iter_loss: 0.21579766273498535
train_iter_loss: 0.11963725090026855
train_iter_loss: 0.06653887033462524
train_iter_loss: 0.08920344710350037
train_iter_loss: 0.0915275439620018
train_iter_loss: 0.13742472231388092
train_iter_loss: 0.09486381709575653
train_iter_loss: 0.08439941704273224
train_iter_loss: 0.09289143979549408
train_iter_loss: 0.28291910886764526
train_iter_loss: 0.11595170199871063
train_iter_loss: 0.127071350812912
train_iter_loss: 0.3548596501350403
train_iter_loss: 0.1186428889632225
train_iter_loss: 0.19505175948143005
train_iter_loss: 0.263226717710495
train_iter_loss: 0.15533515810966492
train_iter_loss: 0.1306808739900589
train_iter_loss: 0.14410772919654846
train_iter_loss: 0.15653012692928314
train_iter_loss: 0.19114573299884796
train_iter_loss: 0.1372421532869339
train_iter_loss: 0.15747159719467163
train_iter_loss: 0.19896063208580017
train_iter_loss: 0.13811257481575012
train_iter_loss: 0.0796833485364914
train_iter_loss: 0.1502164900302887
train_iter_loss: 0.15003621578216553
train_iter_loss: 0.2649027109146118
train_iter_loss: 0.1601567566394806
train_iter_loss: 0.16721265017986298
train_iter_loss: 0.17009614408016205
train_iter_loss: 0.10001497715711594
train_iter_loss: 0.1548456847667694
train_iter_loss: 0.15903766453266144
train_iter_loss: 0.06735362112522125
train_iter_loss: 0.04072057083249092
train_iter_loss: 0.07846695929765701
train_iter_loss: 0.14628177881240845
train_iter_loss: 0.05327077955007553
train_iter_loss: 0.1120414063334465
train_iter_loss: 0.1933765858411789
train_iter_loss: 0.19225679337978363
train_iter_loss: 0.1590644270181656
train_iter_loss: 0.1376139521598816
train_iter_loss: 0.16997027397155762
train_iter_loss: 0.3020530045032501
train_iter_loss: 0.16480115056037903
train_iter_loss: 0.18846513330936432
train_iter_loss: 0.1301501989364624
train_iter_loss: 0.18255747854709625
train_iter_loss: 0.28101691603660583
train_iter_loss: 0.12086250633001328
train_iter_loss: 0.21683594584465027
train_iter_loss: 0.16751323640346527
train_iter_loss: 0.17981620132923126
train_iter_loss: 0.176600843667984
train_iter_loss: 0.14371830224990845
train_iter_loss: 0.17815259099006653
train_iter_loss: 0.2016880363225937
train_iter_loss: 0.07299546152353287
train_iter_loss: 0.2832368016242981
train_iter_loss: 0.08522389829158783
train_iter_loss: 0.17606721818447113
train_iter_loss: 0.09568972140550613
train_iter_loss: 0.19392704963684082
train_iter_loss: 0.10358032584190369
train_iter_loss: 0.15718798339366913
train_iter_loss: 0.16536466777324677
train_iter_loss: 0.2178792953491211
train_iter_loss: 0.19612233340740204
train_iter_loss: 0.103044293820858
train_iter_loss: 0.15073949098587036
train_iter_loss: 0.10732335597276688
train_iter_loss: 0.11244308203458786
train_iter_loss: 0.14043119549751282
train_iter_loss: 0.11013983935117722
train_iter_loss: 0.182723268866539
train loss :0.1610
---------------------
Validation seg loss: 0.21481899590284195 at epoch 400
epoch =    401/  1000, exp = train
train_iter_loss: 0.05566474050283432
train_iter_loss: 0.16238988935947418
train_iter_loss: 0.18068982660770416
train_iter_loss: 0.19700686633586884
train_iter_loss: 0.26390230655670166
train_iter_loss: 0.1351184844970703
train_iter_loss: 0.12106932699680328
train_iter_loss: 0.11405674368143082
train_iter_loss: 0.08949381113052368
train_iter_loss: 0.08408863097429276
train_iter_loss: 0.17480584979057312
train_iter_loss: 0.1515081375837326
train_iter_loss: 0.14996233582496643
train_iter_loss: 0.21155056357383728
train_iter_loss: 0.24763700366020203
train_iter_loss: 0.21100139617919922
train_iter_loss: 0.094554103910923
train_iter_loss: 0.3085266649723053
train_iter_loss: 0.11493457108736038
train_iter_loss: 0.1266258955001831
train_iter_loss: 0.18167220056056976
train_iter_loss: 0.21778427064418793
train_iter_loss: 0.13783258199691772
train_iter_loss: 0.09651773422956467
train_iter_loss: 0.19063962996006012
train_iter_loss: 0.18331654369831085
train_iter_loss: 0.08620579540729523
train_iter_loss: 0.14644308388233185
train_iter_loss: 0.088126040995121
train_iter_loss: 0.07884049415588379
train_iter_loss: 0.2555358409881592
train_iter_loss: 0.09163501113653183
train_iter_loss: 0.1753220558166504
train_iter_loss: 0.08130432665348053
train_iter_loss: 0.2654801905155182
train_iter_loss: 0.13023404777050018
train_iter_loss: 0.14279837906360626
train_iter_loss: 0.17772150039672852
train_iter_loss: 0.08339689671993256
train_iter_loss: 0.3438784182071686
train_iter_loss: 0.05708947777748108
train_iter_loss: 0.2914297878742218
train_iter_loss: 0.16324584186077118
train_iter_loss: 0.1163121834397316
train_iter_loss: 0.16102921962738037
train_iter_loss: 0.19029156863689423
train_iter_loss: 0.1558913290500641
train_iter_loss: 0.42287522554397583
train_iter_loss: 0.1307404488325119
train_iter_loss: 0.1504850685596466
train_iter_loss: 0.1443934142589569
train_iter_loss: 0.1298694610595703
train_iter_loss: 0.07509683817625046
train_iter_loss: 0.22096173465251923
train_iter_loss: 0.15149936079978943
train_iter_loss: 0.17092783749103546
train_iter_loss: 0.10833663493394852
train_iter_loss: 0.10637182742357254
train_iter_loss: 0.1364961564540863
train_iter_loss: 0.2658501863479614
train_iter_loss: 0.087825708091259
train_iter_loss: 0.13915890455245972
train_iter_loss: 0.22943952679634094
train_iter_loss: 0.2426031529903412
train_iter_loss: 0.13648205995559692
train_iter_loss: 0.14596696197986603
train_iter_loss: 0.13784486055374146
train_iter_loss: 0.16692185401916504
train_iter_loss: 0.1011800467967987
train_iter_loss: 0.17009881138801575
train_iter_loss: 0.20879365503787994
train_iter_loss: 0.09751102328300476
train_iter_loss: 0.1767338514328003
train_iter_loss: 0.21323880553245544
train_iter_loss: 0.06944477558135986
train_iter_loss: 0.1238679438829422
train_iter_loss: 0.16659076511859894
train_iter_loss: 0.2962847948074341
train_iter_loss: 0.14476455748081207
train_iter_loss: 0.04681698605418205
train_iter_loss: 0.13011938333511353
train_iter_loss: 0.14598122239112854
train_iter_loss: 0.1568237543106079
train_iter_loss: 0.23470793664455414
train_iter_loss: 0.14268441498279572
train_iter_loss: 0.31373676657676697
train_iter_loss: 0.13836614787578583
train_iter_loss: 0.14623229205608368
train_iter_loss: 0.1521821916103363
train_iter_loss: 0.27642500400543213
train_iter_loss: 0.0797133669257164
train_iter_loss: 0.1519322395324707
train_iter_loss: 0.14325551688671112
train_iter_loss: 0.220414936542511
train_iter_loss: 0.299709290266037
train_iter_loss: 0.28841254115104675
train_iter_loss: 0.10611595213413239
train_iter_loss: 0.07553417235612869
train_iter_loss: 0.1626109629869461
train_iter_loss: 0.29128244519233704
train loss :0.1662
---------------------
Validation seg loss: 0.2258060158862961 at epoch 401
epoch =    402/  1000, exp = train
train_iter_loss: 0.25428080558776855
train_iter_loss: 0.13376834988594055
train_iter_loss: 0.14199621975421906
train_iter_loss: 0.13207246363162994
train_iter_loss: 0.15008091926574707
train_iter_loss: 0.19565372169017792
train_iter_loss: 0.1727754771709442
train_iter_loss: 0.09338399022817612
train_iter_loss: 0.12731409072875977
train_iter_loss: 0.09588795155286789
train_iter_loss: 0.17108561098575592
train_iter_loss: 0.1543169468641281
train_iter_loss: 0.19844506680965424
train_iter_loss: 0.10849080234766006
train_iter_loss: 0.148213729262352
train_iter_loss: 0.1422501653432846
train_iter_loss: 0.2478483021259308
train_iter_loss: 0.13471665978431702
train_iter_loss: 0.13395032286643982
train_iter_loss: 0.20088763535022736
train_iter_loss: 0.08223479986190796
train_iter_loss: 0.0735933929681778
train_iter_loss: 0.0832718163728714
train_iter_loss: 0.20031386613845825
train_iter_loss: 0.1849685162305832
train_iter_loss: 0.34090089797973633
train_iter_loss: 0.19891095161437988
train_iter_loss: 0.2067255973815918
train_iter_loss: 0.12051767855882645
train_iter_loss: 0.2262842208147049
train_iter_loss: 0.054274823516607285
train_iter_loss: 0.21226581931114197
train_iter_loss: 0.19643348455429077
train_iter_loss: 0.09280204772949219
train_iter_loss: 0.14744523167610168
train_iter_loss: 0.1457408219575882
train_iter_loss: 0.16203664243221283
train_iter_loss: 0.1562615931034088
train_iter_loss: 0.24754682183265686
train_iter_loss: 0.22923775017261505
train_iter_loss: 0.09826178103685379
train_iter_loss: 0.11063797771930695
train_iter_loss: 0.1746864765882492
train_iter_loss: 0.12319034337997437
train_iter_loss: 0.09713076800107956
train_iter_loss: 0.18258537352085114
train_iter_loss: 0.15877623856067657
train_iter_loss: 0.12149853259325027
train_iter_loss: 0.1505429744720459
train_iter_loss: 0.15925240516662598
train_iter_loss: 0.1794712245464325
train_iter_loss: 0.14981290698051453
train_iter_loss: 0.1994626224040985
train_iter_loss: 0.22330787777900696
train_iter_loss: 0.12406596541404724
train_iter_loss: 0.15534040331840515
train_iter_loss: 0.11373751610517502
train_iter_loss: 0.107061006128788
train_iter_loss: 0.1808897852897644
train_iter_loss: 0.17283806204795837
train_iter_loss: 0.07749345898628235
train_iter_loss: 0.20327463746070862
train_iter_loss: 0.14421582221984863
train_iter_loss: 0.09785989671945572
train_iter_loss: 0.155197411775589
train_iter_loss: 0.14540830254554749
train_iter_loss: 0.07196342200040817
train_iter_loss: 0.16827505826950073
train_iter_loss: 0.119980089366436
train_iter_loss: 0.15853114426136017
train_iter_loss: 0.10641897469758987
train_iter_loss: 0.3095456063747406
train_iter_loss: 0.14238391816616058
train_iter_loss: 0.10563664883375168
train_iter_loss: 0.14527633786201477
train_iter_loss: 0.06710167974233627
train_iter_loss: 0.2546744644641876
train_iter_loss: 0.22369740903377533
train_iter_loss: 0.16386009752750397
train_iter_loss: 0.37054288387298584
train_iter_loss: 0.2160704880952835
train_iter_loss: 0.15915308892726898
train_iter_loss: 0.07386074960231781
train_iter_loss: 0.10953295975923538
train_iter_loss: 0.22652597725391388
train_iter_loss: 0.13385477662086487
train_iter_loss: 0.1380368322134018
train_iter_loss: 0.11558641493320465
train_iter_loss: 0.2159961462020874
train_iter_loss: 0.13907639682292938
train_iter_loss: 0.1371479034423828
train_iter_loss: 0.2230394333600998
train_iter_loss: 0.3554382920265198
train_iter_loss: 0.2488294243812561
train_iter_loss: 0.17367719113826752
train_iter_loss: 0.039274897426366806
train_iter_loss: 0.1319124549627304
train_iter_loss: 0.08034295588731766
train_iter_loss: 0.12236567586660385
train_iter_loss: 0.1149909645318985
train loss :0.1603
---------------------
Validation seg loss: 0.22562853152037793 at epoch 402
epoch =    403/  1000, exp = train
train_iter_loss: 0.13122448325157166
train_iter_loss: 0.17860911786556244
train_iter_loss: 0.1837640106678009
train_iter_loss: 0.14126703143119812
train_iter_loss: 0.2577308416366577
train_iter_loss: 0.1386091709136963
train_iter_loss: 0.21500080823898315
train_iter_loss: 0.31753501296043396
train_iter_loss: 0.2435704469680786
train_iter_loss: 0.12314104288816452
train_iter_loss: 0.20050832629203796
train_iter_loss: 0.13040079176425934
train_iter_loss: 0.10600249469280243
train_iter_loss: 0.1421215683221817
train_iter_loss: 0.20951306819915771
train_iter_loss: 0.3106565773487091
train_iter_loss: 0.18030422925949097
train_iter_loss: 0.1071576327085495
train_iter_loss: 0.1419140100479126
train_iter_loss: 0.13173040747642517
train_iter_loss: 0.10869411379098892
train_iter_loss: 0.0760861411690712
train_iter_loss: 0.17971713840961456
train_iter_loss: 0.07545330375432968
train_iter_loss: 0.13933877646923065
train_iter_loss: 0.159529447555542
train_iter_loss: 0.1208920106291771
train_iter_loss: 0.1779254823923111
train_iter_loss: 0.2012215107679367
train_iter_loss: 0.09437934309244156
train_iter_loss: 0.15157584846019745
train_iter_loss: 0.17120173573493958
train_iter_loss: 0.13067671656608582
train_iter_loss: 0.05270039290189743
train_iter_loss: 0.11642339825630188
train_iter_loss: 0.09026454389095306
train_iter_loss: 0.13332775235176086
train_iter_loss: 0.19969335198402405
train_iter_loss: 0.26961275935173035
train_iter_loss: 0.07932554185390472
train_iter_loss: 0.10689482092857361
train_iter_loss: 0.12221107631921768
train_iter_loss: 0.2510366141796112
train_iter_loss: 0.15646782517433167
train_iter_loss: 0.21717588603496552
train_iter_loss: 0.20351092517375946
train_iter_loss: 0.16175752878189087
train_iter_loss: 0.18407215178012848
train_iter_loss: 0.25370922684669495
train_iter_loss: 0.1897154301404953
train_iter_loss: 0.298570454120636
train_iter_loss: 0.1249578446149826
train_iter_loss: 0.19668005406856537
train_iter_loss: 0.13088949024677277
train_iter_loss: 0.15738511085510254
train_iter_loss: 0.10483025014400482
train_iter_loss: 0.1418207883834839
train_iter_loss: 0.05495482683181763
train_iter_loss: 0.1096387654542923
train_iter_loss: 0.1115177571773529
train_iter_loss: 0.2404285967350006
train_iter_loss: 0.3160056471824646
train_iter_loss: 0.18791700899600983
train_iter_loss: 0.2136273980140686
train_iter_loss: 0.13854853808879852
train_iter_loss: 0.15441156923770905
train_iter_loss: 0.11813380569219589
train_iter_loss: 0.16435697674751282
train_iter_loss: 0.1279725432395935
train_iter_loss: 0.19134435057640076
train_iter_loss: 0.1528099775314331
train_iter_loss: 0.18973256647586823
train_iter_loss: 0.1614055037498474
train_iter_loss: 0.12503932416439056
train_iter_loss: 0.1184229701757431
train_iter_loss: 0.14733831584453583
train_iter_loss: 0.19849833846092224
train_iter_loss: 0.07126262038946152
train_iter_loss: 0.2304311841726303
train_iter_loss: 0.11752258241176605
train_iter_loss: 0.14621460437774658
train_iter_loss: 0.09433282911777496
train_iter_loss: 0.08647524565458298
train_iter_loss: 0.18688689172267914
train_iter_loss: 0.11277090013027191
train_iter_loss: 0.23201243579387665
train_iter_loss: 0.09874922037124634
train_iter_loss: 0.10860414057970047
train_iter_loss: 0.11868732422590256
train_iter_loss: 0.2364165037870407
train_iter_loss: 0.22337031364440918
train_iter_loss: 0.08753051608800888
train_iter_loss: 0.2738780081272125
train_iter_loss: 0.15579380095005035
train_iter_loss: 0.2641740143299103
train_iter_loss: 0.17658592760562897
train_iter_loss: 0.19862061738967896
train_iter_loss: 0.051085907965898514
train_iter_loss: 0.5374923348426819
train_iter_loss: 0.07644512504339218
train loss :0.1659
---------------------
Validation seg loss: 0.21985496701848395 at epoch 403
epoch =    404/  1000, exp = train
train_iter_loss: 0.07285911589860916
train_iter_loss: 0.17448759078979492
train_iter_loss: 0.11271612346172333
train_iter_loss: 0.27265238761901855
train_iter_loss: 0.06840094178915024
train_iter_loss: 0.18190251290798187
train_iter_loss: 0.15755228698253632
train_iter_loss: 0.19834861159324646
train_iter_loss: 0.13867586851119995
train_iter_loss: 0.1303994059562683
train_iter_loss: 0.17670229077339172
train_iter_loss: 0.23049521446228027
train_iter_loss: 0.207906574010849
train_iter_loss: 0.12435995042324066
train_iter_loss: 0.1522950530052185
train_iter_loss: 0.17927439510822296
train_iter_loss: 0.14866195619106293
train_iter_loss: 0.15317782759666443
train_iter_loss: 0.1912209838628769
train_iter_loss: 0.18264569342136383
train_iter_loss: 0.0957731157541275
train_iter_loss: 0.28430792689323425
train_iter_loss: 0.23113782703876495
train_iter_loss: 0.27977684140205383
train_iter_loss: 0.3092958331108093
train_iter_loss: 0.1511715203523636
train_iter_loss: 0.049637019634246826
train_iter_loss: 0.22679509222507477
train_iter_loss: 0.2802784740924835
train_iter_loss: 0.12438417226076126
train_iter_loss: 0.07426038384437561
train_iter_loss: 0.3835674524307251
train_iter_loss: 0.16822777688503265
train_iter_loss: 0.15937720239162445
train_iter_loss: 0.10786625742912292
train_iter_loss: 0.14331214129924774
train_iter_loss: 0.101543128490448
train_iter_loss: 0.1388106644153595
train_iter_loss: 0.1206270083785057
train_iter_loss: 0.1826128214597702
train_iter_loss: 0.09624700993299484
train_iter_loss: 0.15930026769638062
train_iter_loss: 0.1805141121149063
train_iter_loss: 0.05465955659747124
train_iter_loss: 0.16586199402809143
train_iter_loss: 0.10452794283628464
train_iter_loss: 0.1264786273241043
train_iter_loss: 0.17787635326385498
train_iter_loss: 0.1043921634554863
train_iter_loss: 0.14775389432907104
train_iter_loss: 0.20248232781887054
train_iter_loss: 0.12962087988853455
train_iter_loss: 0.11707114428281784
train_iter_loss: 0.2333245575428009
train_iter_loss: 0.12149902433156967
train_iter_loss: 0.17960551381111145
train_iter_loss: 0.11635839939117432
train_iter_loss: 0.13468430936336517
train_iter_loss: 0.1630982905626297
train_iter_loss: 0.3125515580177307
train_iter_loss: 0.1848631203174591
train_iter_loss: 0.18831579387187958
train_iter_loss: 0.05594560131430626
train_iter_loss: 0.1456829160451889
train_iter_loss: 0.16357238590717316
train_iter_loss: 0.31063759326934814
train_iter_loss: 0.13715717196464539
train_iter_loss: 0.17254073917865753
train_iter_loss: 0.15886808931827545
train_iter_loss: 0.08770804852247238
train_iter_loss: 0.1955288052558899
train_iter_loss: 0.055795516818761826
train_iter_loss: 0.21421396732330322
train_iter_loss: 0.12896744906902313
train_iter_loss: 0.41251417994499207
train_iter_loss: 0.15141111612319946
train_iter_loss: 0.14681948721408844
train_iter_loss: 0.08510778099298477
train_iter_loss: 0.17736127972602844
train_iter_loss: 0.2169094830751419
train_iter_loss: 0.12349358201026917
train_iter_loss: 0.08527124673128128
train_iter_loss: 0.17632539570331573
train_iter_loss: 0.14252802729606628
train_iter_loss: 0.16635631024837494
train_iter_loss: 0.2507825195789337
train_iter_loss: 0.09533614665269852
train_iter_loss: 0.19520382583141327
train_iter_loss: 0.12578865885734558
train_iter_loss: 0.2412310093641281
train_iter_loss: 0.17325450479984283
train_iter_loss: 0.13844843208789825
train_iter_loss: 0.1603335291147232
train_iter_loss: 0.2505135238170624
train_iter_loss: 0.0740518718957901
train_iter_loss: 0.11375338584184647
train_iter_loss: 0.15455512702465057
train_iter_loss: 0.15647006034851074
train_iter_loss: 0.14224465191364288
train_iter_loss: 0.12497022747993469
train loss :0.1654
---------------------
Validation seg loss: 0.21974316263079363 at epoch 404
epoch =    405/  1000, exp = train
train_iter_loss: 0.22586680948734283
train_iter_loss: 0.21089191734790802
train_iter_loss: 0.09512356668710709
train_iter_loss: 0.22008897364139557
train_iter_loss: 0.33892425894737244
train_iter_loss: 0.19495633244514465
train_iter_loss: 0.13439857959747314
train_iter_loss: 0.06193956360220909
train_iter_loss: 0.17918629944324493
train_iter_loss: 0.2753813862800598
train_iter_loss: 0.14588426053524017
train_iter_loss: 0.3217574656009674
train_iter_loss: 0.1425294131040573
train_iter_loss: 0.09670089930295944
train_iter_loss: 0.1656865030527115
train_iter_loss: 0.06382979452610016
train_iter_loss: 0.13116399943828583
train_iter_loss: 0.14127583801746368
train_iter_loss: 0.10340059548616409
train_iter_loss: 0.13493654131889343
train_iter_loss: 0.1751284897327423
train_iter_loss: 0.09930416196584702
train_iter_loss: 0.2407388538122177
train_iter_loss: 0.10024629533290863
train_iter_loss: 0.1995842456817627
train_iter_loss: 0.07796550542116165
train_iter_loss: 0.14586597681045532
train_iter_loss: 0.21253395080566406
train_iter_loss: 0.21357326209545135
train_iter_loss: 0.1978914588689804
train_iter_loss: 0.3482922315597534
train_iter_loss: 0.08442428708076477
train_iter_loss: 0.10268545150756836
train_iter_loss: 0.2198513150215149
train_iter_loss: 0.17223592102527618
train_iter_loss: 0.09427466243505478
train_iter_loss: 0.1659243255853653
train_iter_loss: 0.14781616628170013
train_iter_loss: 0.2119741588830948
train_iter_loss: 0.15037663280963898
train_iter_loss: 0.09597074240446091
train_iter_loss: 0.14871662855148315
train_iter_loss: 0.12673352658748627
train_iter_loss: 0.25874340534210205
train_iter_loss: 0.14662130177021027
train_iter_loss: 0.056175265461206436
train_iter_loss: 0.18176937103271484
train_iter_loss: 0.09784423559904099
train_iter_loss: 0.08648595958948135
train_iter_loss: 0.18341685831546783
train_iter_loss: 0.1020171120762825
train_iter_loss: 0.0665268823504448
train_iter_loss: 0.21197813749313354
train_iter_loss: 0.12190728634595871
train_iter_loss: 0.22408095002174377
train_iter_loss: 0.1248449981212616
train_iter_loss: 0.08113516122102737
train_iter_loss: 0.25236964225769043
train_iter_loss: 0.15323606133460999
train_iter_loss: 0.13883063197135925
train_iter_loss: 0.13715341687202454
train_iter_loss: 0.172695592045784
train_iter_loss: 0.15350328385829926
train_iter_loss: 0.16927240788936615
train_iter_loss: 0.1636970043182373
train_iter_loss: 0.12751685082912445
train_iter_loss: 0.19699694216251373
train_iter_loss: 0.10241620242595673
train_iter_loss: 0.35261645913124084
train_iter_loss: 0.24294033646583557
train_iter_loss: 0.05805545672774315
train_iter_loss: 0.0859939232468605
train_iter_loss: 0.07497300952672958
train_iter_loss: 0.18566259741783142
train_iter_loss: 0.1557374894618988
train_iter_loss: 0.18977631628513336
train_iter_loss: 0.15667489171028137
train_iter_loss: 0.21904243528842926
train_iter_loss: 0.1988449990749359
train_iter_loss: 0.2798033654689789
train_iter_loss: 0.05077650398015976
train_iter_loss: 0.18951760232448578
train_iter_loss: 0.1718141883611679
train_iter_loss: 0.13657408952713013
train_iter_loss: 0.1797369420528412
train_iter_loss: 0.1993882656097412
train_iter_loss: 0.18940608203411102
train_iter_loss: 0.1500285416841507
train_iter_loss: 0.1771610975265503
train_iter_loss: 0.3098498582839966
train_iter_loss: 0.19469153881072998
train_iter_loss: 0.19913919270038605
train_iter_loss: 0.232914000749588
train_iter_loss: 0.17345812916755676
train_iter_loss: 0.16648535430431366
train_iter_loss: 0.12207841128110886
train_iter_loss: 0.0871940478682518
train_iter_loss: 0.15728642046451569
train_iter_loss: 0.1868383139371872
train_iter_loss: 0.0790545865893364
train loss :0.1655
---------------------
Validation seg loss: 0.2230772370525267 at epoch 405
epoch =    406/  1000, exp = train
train_iter_loss: 0.14093638956546783
train_iter_loss: 0.10563252866268158
train_iter_loss: 0.09152594953775406
train_iter_loss: 0.1305224448442459
train_iter_loss: 0.20406833291053772
train_iter_loss: 0.08843326568603516
train_iter_loss: 0.21709522604942322
train_iter_loss: 0.16048289835453033
train_iter_loss: 0.22436533868312836
train_iter_loss: 0.22613109648227692
train_iter_loss: 0.19350969791412354
train_iter_loss: 0.07527881860733032
train_iter_loss: 0.13250577449798584
train_iter_loss: 0.16505883634090424
train_iter_loss: 0.16565923392772675
train_iter_loss: 0.18898633122444153
train_iter_loss: 0.06807732582092285
train_iter_loss: 0.20671308040618896
train_iter_loss: 0.18014055490493774
train_iter_loss: 0.07108713686466217
train_iter_loss: 0.23689229786396027
train_iter_loss: 0.12530352175235748
train_iter_loss: 0.2133500874042511
train_iter_loss: 0.25181975960731506
train_iter_loss: 0.224537655711174
train_iter_loss: 0.055435337126255035
train_iter_loss: 0.2546748220920563
train_iter_loss: 0.24220509827136993
train_iter_loss: 0.25921547412872314
train_iter_loss: 0.11464457958936691
train_iter_loss: 0.06470584869384766
train_iter_loss: 0.24290058016777039
train_iter_loss: 0.3160970211029053
train_iter_loss: 0.2869403064250946
train_iter_loss: 0.094830721616745
train_iter_loss: 0.1283390372991562
train_iter_loss: 0.17552123963832855
train_iter_loss: 0.1163397952914238
train_iter_loss: 0.23938266932964325
train_iter_loss: 0.10453836619853973
train_iter_loss: 0.24777114391326904
train_iter_loss: 0.24622632563114166
train_iter_loss: 0.23613838851451874
train_iter_loss: 0.19361408054828644
train_iter_loss: 0.1008782833814621
train_iter_loss: 0.1866929829120636
train_iter_loss: 0.13285692036151886
train_iter_loss: 0.16032980382442474
train_iter_loss: 0.1396680474281311
train_iter_loss: 0.11412511020898819
train_iter_loss: 0.1602255403995514
train_iter_loss: 0.14693260192871094
train_iter_loss: 0.2685048580169678
train_iter_loss: 0.11525311321020126
train_iter_loss: 0.11747763305902481
train_iter_loss: 0.22177183628082275
train_iter_loss: 0.17561891674995422
train_iter_loss: 0.18724295496940613
train_iter_loss: 0.11951786279678345
train_iter_loss: 0.16539062559604645
train_iter_loss: 0.06821564584970474
train_iter_loss: 0.15383495390415192
train_iter_loss: 0.31272611021995544
train_iter_loss: 0.1592262089252472
train_iter_loss: 0.07620657980442047
train_iter_loss: 0.21493150293827057
train_iter_loss: 0.1240999698638916
train_iter_loss: 0.19174332916736603
train_iter_loss: 0.11640329658985138
train_iter_loss: 0.12898953258991241
train_iter_loss: 0.0732351765036583
train_iter_loss: 0.12906679511070251
train_iter_loss: 0.05648357793688774
train_iter_loss: 0.06409715116024017
train_iter_loss: 0.17302942276000977
train_iter_loss: 0.14984732866287231
train_iter_loss: 0.18991105258464813
train_iter_loss: 0.13269537687301636
train_iter_loss: 0.14503908157348633
train_iter_loss: 0.13995887339115143
train_iter_loss: 0.24630367755889893
train_iter_loss: 0.08183063566684723
train_iter_loss: 0.1813811957836151
train_iter_loss: 0.14027468860149384
train_iter_loss: 0.13195005059242249
train_iter_loss: 0.16577458381652832
train_iter_loss: 0.1422136127948761
train_iter_loss: 0.11115998029708862
train_iter_loss: 0.24086599051952362
train_iter_loss: 0.11319318413734436
train_iter_loss: 0.08614373207092285
train_iter_loss: 0.25324714183807373
train_iter_loss: 0.10459867119789124
train_iter_loss: 0.08994502574205399
train_iter_loss: 0.16895245015621185
train_iter_loss: 0.1582479327917099
train_iter_loss: 0.420681357383728
train_iter_loss: 0.13815626502037048
train_iter_loss: 0.19244073331356049
train_iter_loss: 0.12723389267921448
train loss :0.1648
---------------------
Validation seg loss: 0.2193295571092024 at epoch 406
epoch =    407/  1000, exp = train
train_iter_loss: 0.16287703812122345
train_iter_loss: 0.1284554898738861
train_iter_loss: 0.20655037462711334
train_iter_loss: 0.16475164890289307
train_iter_loss: 0.20908820629119873
train_iter_loss: 0.13152283430099487
train_iter_loss: 0.12437165528535843
train_iter_loss: 0.14854665100574493
train_iter_loss: 0.11863099783658981
train_iter_loss: 0.1633790135383606
train_iter_loss: 0.16689178347587585
train_iter_loss: 0.12332018464803696
train_iter_loss: 0.12359923124313354
train_iter_loss: 0.09837023168802261
train_iter_loss: 0.23580755293369293
train_iter_loss: 0.07179899513721466
train_iter_loss: 0.20346443355083466
train_iter_loss: 0.17177730798721313
train_iter_loss: 0.46477070450782776
train_iter_loss: 0.1799597442150116
train_iter_loss: 0.1605089157819748
train_iter_loss: 0.19520363211631775
train_iter_loss: 0.34062135219573975
train_iter_loss: 0.16078783571720123
train_iter_loss: 0.10958804935216904
train_iter_loss: 0.05818581208586693
train_iter_loss: 0.15920120477676392
train_iter_loss: 0.20298928022384644
train_iter_loss: 0.0561508908867836
train_iter_loss: 0.18221066892147064
train_iter_loss: 0.10216227173805237
train_iter_loss: 0.11491551995277405
train_iter_loss: 0.2158065140247345
train_iter_loss: 0.11865906417369843
train_iter_loss: 0.19455227255821228
train_iter_loss: 0.1071498692035675
train_iter_loss: 0.2043619453907013
train_iter_loss: 0.20561964809894562
train_iter_loss: 0.1587940901517868
train_iter_loss: 0.11426065862178802
train_iter_loss: 0.12827308475971222
train_iter_loss: 0.15746740996837616
train_iter_loss: 0.10758904367685318
train_iter_loss: 0.14969435334205627
train_iter_loss: 0.19787830114364624
train_iter_loss: 0.1686965525150299
train_iter_loss: 0.14057347178459167
train_iter_loss: 0.07774805277585983
train_iter_loss: 0.2390238493680954
train_iter_loss: 0.2950458824634552
train_iter_loss: 0.1977708786725998
train_iter_loss: 0.17585991322994232
train_iter_loss: 0.16934771835803986
train_iter_loss: 0.21131247282028198
train_iter_loss: 0.0933445543050766
train_iter_loss: 0.17618723213672638
train_iter_loss: 0.1858290433883667
train_iter_loss: 0.0576631985604763
train_iter_loss: 0.22850263118743896
train_iter_loss: 0.1465466469526291
train_iter_loss: 0.1386934518814087
train_iter_loss: 0.27569225430488586
train_iter_loss: 0.09924715012311935
train_iter_loss: 0.1033293604850769
train_iter_loss: 0.15817053616046906
train_iter_loss: 0.11496758460998535
train_iter_loss: 0.15609821677207947
train_iter_loss: 0.1695009022951126
train_iter_loss: 0.10184583812952042
train_iter_loss: 0.2742241621017456
train_iter_loss: 0.1630290448665619
train_iter_loss: 0.0968949943780899
train_iter_loss: 0.11048803478479385
train_iter_loss: 0.12827982008457184
train_iter_loss: 0.1530112326145172
train_iter_loss: 0.15094996988773346
train_iter_loss: 0.15784040093421936
train_iter_loss: 0.127914696931839
train_iter_loss: 0.060270756483078
train_iter_loss: 0.1467418521642685
train_iter_loss: 0.1196121945977211
train_iter_loss: 0.33427682518959045
train_iter_loss: 0.07514683157205582
train_iter_loss: 0.20674432814121246
train_iter_loss: 0.10975194722414017
train_iter_loss: 0.15133273601531982
train_iter_loss: 0.09225630015134811
train_iter_loss: 0.23143017292022705
train_iter_loss: 0.17766566574573517
train_iter_loss: 0.14224353432655334
train_iter_loss: 0.23940414190292358
train_iter_loss: 0.07635033130645752
train_iter_loss: 0.13688872754573822
train_iter_loss: 0.2010207176208496
train_iter_loss: 0.1657644510269165
train_iter_loss: 0.1097363755106926
train_iter_loss: 0.16723085939884186
train_iter_loss: 0.2657541334629059
train_iter_loss: 0.13585348427295685
train_iter_loss: 0.09162861853837967
train loss :0.1615
---------------------
Validation seg loss: 0.22241325693134711 at epoch 407
epoch =    408/  1000, exp = train
train_iter_loss: 0.1016995832324028
train_iter_loss: 0.13833346962928772
train_iter_loss: 0.19461528956890106
train_iter_loss: 0.10409568250179291
train_iter_loss: 0.10502421110868454
train_iter_loss: 0.14476874470710754
train_iter_loss: 0.2298946976661682
train_iter_loss: 0.2798522412776947
train_iter_loss: 0.09391190111637115
train_iter_loss: 0.1384921371936798
train_iter_loss: 0.0935014858841896
train_iter_loss: 0.1016235202550888
train_iter_loss: 0.15113547444343567
train_iter_loss: 0.22142024338245392
train_iter_loss: 0.13844239711761475
train_iter_loss: 0.10537252575159073
train_iter_loss: 0.12824241816997528
train_iter_loss: 0.1368434578180313
train_iter_loss: 0.20672471821308136
train_iter_loss: 0.096954345703125
train_iter_loss: 0.09968893975019455
train_iter_loss: 0.27131885290145874
train_iter_loss: 0.16738738119602203
train_iter_loss: 0.24549756944179535
train_iter_loss: 0.25822681188583374
train_iter_loss: 0.12326154857873917
train_iter_loss: 0.18544377386569977
train_iter_loss: 0.11676597595214844
train_iter_loss: 0.43748021125793457
train_iter_loss: 0.18725918233394623
train_iter_loss: 0.17612779140472412
train_iter_loss: 0.13538989424705505
train_iter_loss: 0.1780671328306198
train_iter_loss: 0.12504269182682037
train_iter_loss: 0.21668735146522522
train_iter_loss: 0.2007535845041275
train_iter_loss: 0.16257961094379425
train_iter_loss: 0.22264350950717926
train_iter_loss: 0.12656360864639282
train_iter_loss: 0.2810048460960388
train_iter_loss: 0.1892154961824417
train_iter_loss: 0.1947239637374878
train_iter_loss: 0.10954878479242325
train_iter_loss: 0.12122206389904022
train_iter_loss: 0.28088560700416565
train_iter_loss: 0.17107725143432617
train_iter_loss: 0.08532539755105972
train_iter_loss: 0.42917370796203613
train_iter_loss: 0.08371436595916748
train_iter_loss: 0.17797477543354034
train_iter_loss: 0.17525847256183624
train_iter_loss: 0.15323245525360107
train_iter_loss: 0.1578735113143921
train_iter_loss: 0.20361392199993134
train_iter_loss: 0.11762858927249908
train_iter_loss: 0.15372109413146973
train_iter_loss: 0.12974926829338074
train_iter_loss: 0.2160738706588745
train_iter_loss: 0.2636578381061554
train_iter_loss: 0.13421301543712616
train_iter_loss: 0.12910273671150208
train_iter_loss: 0.12734545767307281
train_iter_loss: 0.04069632291793823
train_iter_loss: 0.17000699043273926
train_iter_loss: 0.11972077190876007
train_iter_loss: 0.21802568435668945
train_iter_loss: 0.10791284590959549
train_iter_loss: 0.10348601639270782
train_iter_loss: 0.11838056892156601
train_iter_loss: 0.12755027413368225
train_iter_loss: 0.10251080244779587
train_iter_loss: 0.12324443459510803
train_iter_loss: 0.18693217635154724
train_iter_loss: 0.16884617507457733
train_iter_loss: 0.2402118295431137
train_iter_loss: 0.2415655553340912
train_iter_loss: 0.1724945455789566
train_iter_loss: 0.1118345856666565
train_iter_loss: 0.09554218500852585
train_iter_loss: 0.09922920167446136
train_iter_loss: 0.08385737240314484
train_iter_loss: 0.149541437625885
train_iter_loss: 0.0978846326470375
train_iter_loss: 0.145394429564476
train_iter_loss: 0.1503870189189911
train_iter_loss: 0.1314878910779953
train_iter_loss: 0.12503580749034882
train_iter_loss: 0.19497902691364288
train_iter_loss: 0.09300121665000916
train_iter_loss: 0.10727179050445557
train_iter_loss: 0.16902367770671844
train_iter_loss: 0.10111740976572037
train_iter_loss: 0.08188442885875702
train_iter_loss: 0.19186271727085114
train_iter_loss: 0.20606815814971924
train_iter_loss: 0.10207977890968323
train_iter_loss: 0.18871508538722992
train_iter_loss: 0.14401304721832275
train_iter_loss: 0.12577396631240845
train_iter_loss: 0.19917508959770203
train loss :0.1613
---------------------
Validation seg loss: 0.22014627489700633 at epoch 408
epoch =    409/  1000, exp = train
train_iter_loss: 0.058255404233932495
train_iter_loss: 0.2369765341281891
train_iter_loss: 0.1823204904794693
train_iter_loss: 0.22828508913516998
train_iter_loss: 0.1404544562101364
train_iter_loss: 0.08642595261335373
train_iter_loss: 0.19933633506298065
train_iter_loss: 0.24085715413093567
train_iter_loss: 0.2346382737159729
train_iter_loss: 0.05815180763602257
train_iter_loss: 0.15976949036121368
train_iter_loss: 0.39118972420692444
train_iter_loss: 0.2621050477027893
train_iter_loss: 0.24181479215621948
train_iter_loss: 0.21637243032455444
train_iter_loss: 0.14910180866718292
train_iter_loss: 0.09857023507356644
train_iter_loss: 0.1040368601679802
train_iter_loss: 0.2130269557237625
train_iter_loss: 0.2866430878639221
train_iter_loss: 0.16422751545906067
train_iter_loss: 0.16709579527378082
train_iter_loss: 0.16851983964443207
train_iter_loss: 0.1685706526041031
train_iter_loss: 0.204043447971344
train_iter_loss: 0.13535888493061066
train_iter_loss: 0.16475339233875275
train_iter_loss: 0.1367151290178299
train_iter_loss: 0.14242571592330933
train_iter_loss: 0.1213604062795639
train_iter_loss: 0.16043934226036072
train_iter_loss: 0.1596963107585907
train_iter_loss: 0.10350596159696579
train_iter_loss: 0.2084963172674179
train_iter_loss: 0.2845863103866577
train_iter_loss: 0.148737370967865
train_iter_loss: 0.18707875907421112
train_iter_loss: 0.10976402461528778
train_iter_loss: 0.19398508965969086
train_iter_loss: 0.13792069256305695
train_iter_loss: 0.18171754479408264
train_iter_loss: 0.24002933502197266
train_iter_loss: 0.2825591266155243
train_iter_loss: 0.12161695957183838
train_iter_loss: 0.33709728717803955
train_iter_loss: 0.07751021534204483
train_iter_loss: 0.11401038616895676
train_iter_loss: 0.06448901444673538
train_iter_loss: 0.15353593230247498
train_iter_loss: 0.082953080534935
train_iter_loss: 0.20616255700588226
train_iter_loss: 0.21193234622478485
train_iter_loss: 0.2279745638370514
train_iter_loss: 0.14068305492401123
train_iter_loss: 0.17949365079402924
train_iter_loss: 0.1036888062953949
train_iter_loss: 0.11749031394720078
train_iter_loss: 0.2735893428325653
train_iter_loss: 0.08455392718315125
train_iter_loss: 0.21477600932121277
train_iter_loss: 0.21030668914318085
train_iter_loss: 0.09702029824256897
train_iter_loss: 0.17094163596630096
train_iter_loss: 0.13389858603477478
train_iter_loss: 0.36946287751197815
train_iter_loss: 0.12476427108049393
train_iter_loss: 0.11809524893760681
train_iter_loss: 0.18006299436092377
train_iter_loss: 0.09592577069997787
train_iter_loss: 0.07391473650932312
train_iter_loss: 0.18630088865756989
train_iter_loss: 0.13493020832538605
train_iter_loss: 0.16942071914672852
train_iter_loss: 0.10835137963294983
train_iter_loss: 0.24062621593475342
train_iter_loss: 0.1543729156255722
train_iter_loss: 0.10408514738082886
train_iter_loss: 0.16425326466560364
train_iter_loss: 0.1554948091506958
train_iter_loss: 0.16594845056533813
train_iter_loss: 0.371801495552063
train_iter_loss: 0.0897645652294159
train_iter_loss: 0.2549983263015747
train_iter_loss: 0.17063705623149872
train_iter_loss: 0.0786958560347557
train_iter_loss: 0.0851239562034607
train_iter_loss: 0.26497775316238403
train_iter_loss: 0.1806783825159073
train_iter_loss: 0.12199405580759048
train_iter_loss: 0.22311772406101227
train_iter_loss: 0.2647740840911865
train_iter_loss: 0.07788822054862976
train_iter_loss: 0.15985198318958282
train_iter_loss: 0.12286395579576492
train_iter_loss: 0.1448564976453781
train_iter_loss: 0.18608541786670685
train_iter_loss: 0.22737157344818115
train_iter_loss: 0.16600294411182404
train_iter_loss: 0.3152261972427368
train_iter_loss: 0.1438634693622589
train loss :0.1744
---------------------
Validation seg loss: 0.22373435144611406 at epoch 409
epoch =    410/  1000, exp = train
train_iter_loss: 0.18408776819705963
train_iter_loss: 0.08105699717998505
train_iter_loss: 0.13774427771568298
train_iter_loss: 0.21767990291118622
train_iter_loss: 0.16556158661842346
train_iter_loss: 0.13738521933555603
train_iter_loss: 0.1338788866996765
train_iter_loss: 0.17832764983177185
train_iter_loss: 0.0747801661491394
train_iter_loss: 0.2876201272010803
train_iter_loss: 0.11610673367977142
train_iter_loss: 0.25495976209640503
train_iter_loss: 0.11100877076387405
train_iter_loss: 0.11401309072971344
train_iter_loss: 0.09021062403917313
train_iter_loss: 0.2741864025592804
train_iter_loss: 0.11176062375307083
train_iter_loss: 0.0723612904548645
train_iter_loss: 0.17872929573059082
train_iter_loss: 0.15613427758216858
train_iter_loss: 0.13401007652282715
train_iter_loss: 0.2109445333480835
train_iter_loss: 0.13799940049648285
train_iter_loss: 0.13571977615356445
train_iter_loss: 0.1816982477903366
train_iter_loss: 0.2188728004693985
train_iter_loss: 0.12946799397468567
train_iter_loss: 0.25241363048553467
train_iter_loss: 0.026930686086416245
train_iter_loss: 0.07831314206123352
train_iter_loss: 0.07055608183145523
train_iter_loss: 0.24827134609222412
train_iter_loss: 0.14471296966075897
train_iter_loss: 0.15858758985996246
train_iter_loss: 0.2129841446876526
train_iter_loss: 0.1670723557472229
train_iter_loss: 0.12920768558979034
train_iter_loss: 0.24028898775577545
train_iter_loss: 0.18569257855415344
train_iter_loss: 0.14302025735378265
train_iter_loss: 0.11438189446926117
train_iter_loss: 0.07870373129844666
train_iter_loss: 0.15266995131969452
train_iter_loss: 0.26021942496299744
train_iter_loss: 0.23550501465797424
train_iter_loss: 0.1536528468132019
train_iter_loss: 0.1270860880613327
train_iter_loss: 0.1989825814962387
train_iter_loss: 0.14556080102920532
train_iter_loss: 0.22598589956760406
train_iter_loss: 0.11936767399311066
train_iter_loss: 0.17383770644664764
train_iter_loss: 0.1905035823583603
train_iter_loss: 0.07821805775165558
train_iter_loss: 0.1092144101858139
train_iter_loss: 0.12362565100193024
train_iter_loss: 0.18231147527694702
train_iter_loss: 0.10071376711130142
train_iter_loss: 0.2434069812297821
train_iter_loss: 0.2294810265302658
train_iter_loss: 0.07737989723682404
train_iter_loss: 0.16077478229999542
train_iter_loss: 0.13576540350914001
train_iter_loss: 0.10721199959516525
train_iter_loss: 0.09545291215181351
train_iter_loss: 0.2672702372074127
train_iter_loss: 0.12479180842638016
train_iter_loss: 0.16556470096111298
train_iter_loss: 0.1645953506231308
train_iter_loss: 0.13433165848255157
train_iter_loss: 0.2433532327413559
train_iter_loss: 0.20844201743602753
train_iter_loss: 0.12248560041189194
train_iter_loss: 0.22631415724754333
train_iter_loss: 0.16128963232040405
train_iter_loss: 0.19389967620372772
train_iter_loss: 0.20263879001140594
train_iter_loss: 0.16227613389492035
train_iter_loss: 0.14236848056316376
train_iter_loss: 0.11116579920053482
train_iter_loss: 0.11139032244682312
train_iter_loss: 0.19589722156524658
train_iter_loss: 0.15954706072807312
train_iter_loss: 0.22610726952552795
train_iter_loss: 0.16404812037944794
train_iter_loss: 0.13910943269729614
train_iter_loss: 0.14864321053028107
train_iter_loss: 0.2576316297054291
train_iter_loss: 0.13049980998039246
train_iter_loss: 0.2396501749753952
train_iter_loss: 0.15836191177368164
train_iter_loss: 0.1200280487537384
train_iter_loss: 0.21694476902484894
train_iter_loss: 0.17750094830989838
train_iter_loss: 0.11572291702032089
train_iter_loss: 0.20299486815929413
train_iter_loss: 0.12875188887119293
train_iter_loss: 0.21490946412086487
train_iter_loss: 0.22974573075771332
train_iter_loss: 0.11034184694290161
train loss :0.1636
---------------------
Validation seg loss: 0.21751823594336803 at epoch 410
epoch =    411/  1000, exp = train
train_iter_loss: 0.16761337220668793
train_iter_loss: 0.14451029896736145
train_iter_loss: 0.15683941543102264
train_iter_loss: 0.05516337975859642
train_iter_loss: 0.1496855765581131
train_iter_loss: 0.09211797267198563
train_iter_loss: 0.09984968602657318
train_iter_loss: 0.1358647346496582
train_iter_loss: 0.12906426191329956
train_iter_loss: 0.1563262939453125
train_iter_loss: 0.16897840797901154
train_iter_loss: 0.0961284264922142
train_iter_loss: 0.20318304002285004
train_iter_loss: 0.2717348635196686
train_iter_loss: 0.15611480176448822
train_iter_loss: 0.1846393197774887
train_iter_loss: 0.10689052194356918
train_iter_loss: 0.17341439425945282
train_iter_loss: 0.09908486902713776
train_iter_loss: 0.13012564182281494
train_iter_loss: 0.14957064390182495
train_iter_loss: 0.1930522620677948
train_iter_loss: 0.09561031311750412
train_iter_loss: 0.16886916756629944
train_iter_loss: 0.08431588858366013
train_iter_loss: 0.09132276475429535
train_iter_loss: 0.1085115596652031
train_iter_loss: 0.08338519185781479
train_iter_loss: 0.11088541150093079
train_iter_loss: 0.18935608863830566
train_iter_loss: 0.20155063271522522
train_iter_loss: 0.1261005848646164
train_iter_loss: 0.12081393599510193
train_iter_loss: 0.3275703191757202
train_iter_loss: 0.14261773228645325
train_iter_loss: 0.1639147698879242
train_iter_loss: 0.09965330362319946
train_iter_loss: 0.1186666414141655
train_iter_loss: 0.15960626304149628
train_iter_loss: 0.2847120761871338
train_iter_loss: 0.24468101561069489
train_iter_loss: 0.16682474315166473
train_iter_loss: 0.08669155836105347
train_iter_loss: 0.2469945102930069
train_iter_loss: 0.0904453918337822
train_iter_loss: 0.1490679383277893
train_iter_loss: 0.12815862894058228
train_iter_loss: 0.1772753745317459
train_iter_loss: 0.13635709881782532
train_iter_loss: 0.11013229936361313
train_iter_loss: 0.1314190775156021
train_iter_loss: 0.22211188077926636
train_iter_loss: 0.12623290717601776
train_iter_loss: 0.15347278118133545
train_iter_loss: 0.10054115951061249
train_iter_loss: 0.269185870885849
train_iter_loss: 0.09542284905910492
train_iter_loss: 0.14683470129966736
train_iter_loss: 0.2370399832725525
train_iter_loss: 0.13411341607570648
train_iter_loss: 0.16938316822052002
train_iter_loss: 0.2421429455280304
train_iter_loss: 0.16523860394954681
train_iter_loss: 0.1547268182039261
train_iter_loss: 0.15688207745552063
train_iter_loss: 0.14921076595783234
train_iter_loss: 0.5162623524665833
train_iter_loss: 0.14237503707408905
train_iter_loss: 0.1672767847776413
train_iter_loss: 0.1832735389471054
train_iter_loss: 0.17347535490989685
train_iter_loss: 0.15363052487373352
train_iter_loss: 0.16121791303157806
train_iter_loss: 0.15061460435390472
train_iter_loss: 0.13014864921569824
train_iter_loss: 0.061059288680553436
train_iter_loss: 0.1560588926076889
train_iter_loss: 0.14075227081775665
train_iter_loss: 0.19469228386878967
train_iter_loss: 0.23171044886112213
train_iter_loss: 0.0919322744011879
train_iter_loss: 0.2579839825630188
train_iter_loss: 0.2346169650554657
train_iter_loss: 0.05638013035058975
train_iter_loss: 0.1406707912683487
train_iter_loss: 0.15642818808555603
train_iter_loss: 0.1766812652349472
train_iter_loss: 0.13245758414268494
train_iter_loss: 0.1745401918888092
train_iter_loss: 0.1392916738986969
train_iter_loss: 0.2123112827539444
train_iter_loss: 0.31776702404022217
train_iter_loss: 0.08184295147657394
train_iter_loss: 0.1473369151353836
train_iter_loss: 0.12475940585136414
train_iter_loss: 0.08970927447080612
train_iter_loss: 0.24155162274837494
train_iter_loss: 0.38925424218177795
train_iter_loss: 0.12610672414302826
train_iter_loss: 0.16833554208278656
train loss :0.1627
---------------------
Validation seg loss: 0.21924163374768676 at epoch 411
epoch =    412/  1000, exp = train
train_iter_loss: 0.13114887475967407
train_iter_loss: 0.17328105866909027
train_iter_loss: 0.16956058144569397
train_iter_loss: 0.28441330790519714
train_iter_loss: 0.15626630187034607
train_iter_loss: 0.12267476320266724
train_iter_loss: 0.20317500829696655
train_iter_loss: 0.12977825105190277
train_iter_loss: 0.14563389122486115
train_iter_loss: 0.16476787626743317
train_iter_loss: 0.18759359419345856
train_iter_loss: 0.15216857194900513
train_iter_loss: 0.11815781891345978
train_iter_loss: 0.11832252889871597
train_iter_loss: 0.09711284935474396
train_iter_loss: 0.10753733664751053
train_iter_loss: 0.24079932272434235
train_iter_loss: 0.08927641063928604
train_iter_loss: 0.06015896797180176
train_iter_loss: 0.16470009088516235
train_iter_loss: 0.159753680229187
train_iter_loss: 0.0773269385099411
train_iter_loss: 0.17218470573425293
train_iter_loss: 0.2750745117664337
train_iter_loss: 0.12395630031824112
train_iter_loss: 0.12529446184635162
train_iter_loss: 0.11470110714435577
train_iter_loss: 0.12121953815221786
train_iter_loss: 0.09057021141052246
train_iter_loss: 0.28727975487709045
train_iter_loss: 0.09829119592905045
train_iter_loss: 0.0911114513874054
train_iter_loss: 0.14253823459148407
train_iter_loss: 0.1609196662902832
train_iter_loss: 0.13559970259666443
train_iter_loss: 0.1763765662908554
train_iter_loss: 0.19109229743480682
train_iter_loss: 0.1410016268491745
train_iter_loss: 0.13587242364883423
train_iter_loss: 0.27931055426597595
train_iter_loss: 0.2675386369228363
train_iter_loss: 0.17079700529575348
train_iter_loss: 0.1238415539264679
train_iter_loss: 0.2638787031173706
train_iter_loss: 0.13687282800674438
train_iter_loss: 0.12035506218671799
train_iter_loss: 0.10775138437747955
train_iter_loss: 0.09672243148088455
train_iter_loss: 0.15760470926761627
train_iter_loss: 0.15279562771320343
train_iter_loss: 0.10668891668319702
train_iter_loss: 0.18296268582344055
train_iter_loss: 0.14635416865348816
train_iter_loss: 0.13883554935455322
train_iter_loss: 0.1255066692829132
train_iter_loss: 0.21420200169086456
train_iter_loss: 0.16969045996665955
train_iter_loss: 0.17972299456596375
train_iter_loss: 0.3172204792499542
train_iter_loss: 0.15525582432746887
train_iter_loss: 0.3044080138206482
train_iter_loss: 0.15761828422546387
train_iter_loss: 0.15874333679676056
train_iter_loss: 0.17337310314178467
train_iter_loss: 0.2630794644355774
train_iter_loss: 0.08217007666826248
train_iter_loss: 0.10349228978157043
train_iter_loss: 0.13008207082748413
train_iter_loss: 0.08702010661363602
train_iter_loss: 0.17553143203258514
train_iter_loss: 0.11107467114925385
train_iter_loss: 0.20747560262680054
train_iter_loss: 0.2049652636051178
train_iter_loss: 0.12052151560783386
train_iter_loss: 0.2590789794921875
train_iter_loss: 0.22139210999011993
train_iter_loss: 0.11127041280269623
train_iter_loss: 0.1813259869813919
train_iter_loss: 0.0527152493596077
train_iter_loss: 0.14280082285404205
train_iter_loss: 0.1925516128540039
train_iter_loss: 0.18030451238155365
train_iter_loss: 0.14085368812084198
train_iter_loss: 0.10577700287103653
train_iter_loss: 0.11456780135631561
train_iter_loss: 0.12509283423423767
train_iter_loss: 0.17432567477226257
train_iter_loss: 0.23110635578632355
train_iter_loss: 0.15757469832897186
train_iter_loss: 0.256717711687088
train_iter_loss: 0.2995465397834778
train_iter_loss: 0.18287146091461182
train_iter_loss: 0.11681876331567764
train_iter_loss: 0.15317048132419586
train_iter_loss: 0.15548886358737946
train_iter_loss: 0.28702178597450256
train_iter_loss: 0.19276946783065796
train_iter_loss: 0.16602155566215515
train_iter_loss: 0.27036112546920776
train_iter_loss: 0.07985448092222214
train loss :0.1644
---------------------
Validation seg loss: 0.2181246917132499 at epoch 412
epoch =    413/  1000, exp = train
train_iter_loss: 0.1670268177986145
train_iter_loss: 0.08183419704437256
train_iter_loss: 0.10704879462718964
train_iter_loss: 0.3028799295425415
train_iter_loss: 0.05754132196307182
train_iter_loss: 0.10661114752292633
train_iter_loss: 0.11773326247930527
train_iter_loss: 0.2867356240749359
train_iter_loss: 0.13012778759002686
train_iter_loss: 0.11659544706344604
train_iter_loss: 0.22601695358753204
train_iter_loss: 0.06502316147089005
train_iter_loss: 0.1570035219192505
train_iter_loss: 0.2019045054912567
train_iter_loss: 0.2757521867752075
train_iter_loss: 0.2673734724521637
train_iter_loss: 0.151151642203331
train_iter_loss: 0.18350721895694733
train_iter_loss: 0.13537174463272095
train_iter_loss: 0.17636092007160187
train_iter_loss: 0.20494776964187622
train_iter_loss: 0.0956185832619667
train_iter_loss: 0.18245671689510345
train_iter_loss: 0.1676240861415863
train_iter_loss: 0.09190040081739426
train_iter_loss: 0.1046116054058075
train_iter_loss: 0.17354559898376465
train_iter_loss: 0.17368419468402863
train_iter_loss: 0.1300186961889267
train_iter_loss: 0.26993924379348755
train_iter_loss: 0.11839378625154495
train_iter_loss: 0.25422996282577515
train_iter_loss: 0.09135086834430695
train_iter_loss: 0.22227084636688232
train_iter_loss: 0.14912830293178558
train_iter_loss: 0.09765872359275818
train_iter_loss: 0.20025214552879333
train_iter_loss: 0.13725368678569794
train_iter_loss: 0.19893307983875275
train_iter_loss: 0.17470748722553253
train_iter_loss: 0.25936537981033325
train_iter_loss: 0.07965631783008575
train_iter_loss: 0.151243194937706
train_iter_loss: 0.09847835451364517
train_iter_loss: 0.1105557531118393
train_iter_loss: 0.08764703571796417
train_iter_loss: 0.1537688672542572
train_iter_loss: 0.10484305769205093
train_iter_loss: 0.09685441106557846
train_iter_loss: 0.12215383350849152
train_iter_loss: 0.15811650454998016
train_iter_loss: 0.16625036299228668
train_iter_loss: 0.16810545325279236
train_iter_loss: 0.1862756460905075
train_iter_loss: 0.10708919912576675
train_iter_loss: 0.09674915671348572
train_iter_loss: 0.2452283799648285
train_iter_loss: 0.13286356627941132
train_iter_loss: 0.08740856498479843
train_iter_loss: 0.20535919070243835
train_iter_loss: 0.15284426510334015
train_iter_loss: 0.17364804446697235
train_iter_loss: 0.19687029719352722
train_iter_loss: 0.14324867725372314
train_iter_loss: 0.12085390836000443
train_iter_loss: 0.0953146442770958
train_iter_loss: 0.12444133311510086
train_iter_loss: 0.26095640659332275
train_iter_loss: 0.1779937595129013
train_iter_loss: 0.15991000831127167
train_iter_loss: 0.19111791253089905
train_iter_loss: 0.13290876150131226
train_iter_loss: 0.21933329105377197
train_iter_loss: 0.1976541429758072
train_iter_loss: 0.13552254438400269
train_iter_loss: 0.10495651513338089
train_iter_loss: 0.3794018030166626
train_iter_loss: 0.16046978533267975
train_iter_loss: 0.08187131583690643
train_iter_loss: 0.2524452805519104
train_iter_loss: 0.17036236822605133
train_iter_loss: 0.10959934443235397
train_iter_loss: 0.16349154710769653
train_iter_loss: 0.15795926749706268
train_iter_loss: 0.1948612481355667
train_iter_loss: 0.09319312125444412
train_iter_loss: 0.23916402459144592
train_iter_loss: 0.14234624803066254
train_iter_loss: 0.0946125015616417
train_iter_loss: 0.07591020315885544
train_iter_loss: 0.13119927048683167
train_iter_loss: 0.09683987498283386
train_iter_loss: 0.12428665906190872
train_iter_loss: 0.3204127550125122
train_iter_loss: 0.10916828364133835
train_iter_loss: 0.16794490814208984
train_iter_loss: 0.12240418791770935
train_iter_loss: 0.17750447988510132
train_iter_loss: 0.1564602255821228
train_iter_loss: 0.2128845900297165
train loss :0.1606
---------------------
Validation seg loss: 0.2160953010854153 at epoch 413
epoch =    414/  1000, exp = train
train_iter_loss: 0.1588137447834015
train_iter_loss: 0.10303512215614319
train_iter_loss: 0.23405559360980988
train_iter_loss: 0.30945220589637756
train_iter_loss: 0.2289336919784546
train_iter_loss: 0.23267292976379395
train_iter_loss: 0.1199965551495552
train_iter_loss: 0.17680324614048004
train_iter_loss: 0.2963915467262268
train_iter_loss: 0.12739332020282745
train_iter_loss: 0.21473199129104614
train_iter_loss: 0.13557998836040497
train_iter_loss: 0.15766744315624237
train_iter_loss: 0.0591462180018425
train_iter_loss: 0.19010238349437714
train_iter_loss: 0.15064920485019684
train_iter_loss: 0.11552328616380692
train_iter_loss: 0.16558724641799927
train_iter_loss: 0.1350405067205429
train_iter_loss: 0.1976972073316574
train_iter_loss: 0.1337471902370453
train_iter_loss: 0.13767820596694946
train_iter_loss: 0.12796321511268616
train_iter_loss: 0.0897836908698082
train_iter_loss: 0.08695239573717117
train_iter_loss: 0.17208141088485718
train_iter_loss: 0.2153894007205963
train_iter_loss: 0.08132725954055786
train_iter_loss: 0.3267252445220947
train_iter_loss: 0.20003847777843475
train_iter_loss: 0.09837950021028519
train_iter_loss: 0.19603677093982697
train_iter_loss: 0.13166017830371857
train_iter_loss: 0.08716336637735367
train_iter_loss: 0.1522299349308014
train_iter_loss: 0.16661542654037476
train_iter_loss: 0.26453155279159546
train_iter_loss: 0.1675487756729126
train_iter_loss: 0.04725911095738411
train_iter_loss: 0.1645279824733734
train_iter_loss: 0.08932843804359436
train_iter_loss: 0.09708429127931595
train_iter_loss: 0.08983681350946426
train_iter_loss: 0.1848745048046112
train_iter_loss: 0.15777291357517242
train_iter_loss: 0.0776463970541954
train_iter_loss: 0.16162285208702087
train_iter_loss: 0.06988388299942017
train_iter_loss: 0.18875838816165924
train_iter_loss: 0.199502632021904
train_iter_loss: 0.10757420212030411
train_iter_loss: 0.12092342227697372
train_iter_loss: 0.2831127643585205
train_iter_loss: 0.2014666050672531
train_iter_loss: 0.1718875765800476
train_iter_loss: 0.16338369250297546
train_iter_loss: 0.24737350642681122
train_iter_loss: 0.19968962669372559
train_iter_loss: 0.22050626575946808
train_iter_loss: 0.15728963911533356
train_iter_loss: 0.1663086712360382
train_iter_loss: 0.1662813276052475
train_iter_loss: 0.11676307022571564
train_iter_loss: 0.15421748161315918
train_iter_loss: 0.07994666695594788
train_iter_loss: 0.1707272231578827
train_iter_loss: 0.08992684632539749
train_iter_loss: 0.1958146095275879
train_iter_loss: 0.20915648341178894
train_iter_loss: 0.11313078552484512
train_iter_loss: 0.10960302501916885
train_iter_loss: 0.1794687956571579
train_iter_loss: 0.12419737130403519
train_iter_loss: 0.3371959626674652
train_iter_loss: 0.1389569342136383
train_iter_loss: 0.1230769008398056
train_iter_loss: 0.07937108725309372
train_iter_loss: 0.1406448930501938
train_iter_loss: 0.1918017715215683
train_iter_loss: 0.22606635093688965
train_iter_loss: 0.30968600511550903
train_iter_loss: 0.1285048872232437
train_iter_loss: 0.2626894414424896
train_iter_loss: 0.1901424676179886
train_iter_loss: 0.16547076404094696
train_iter_loss: 0.06932463496923447
train_iter_loss: 0.20004861056804657
train_iter_loss: 0.1476534903049469
train_iter_loss: 0.1808343082666397
train_iter_loss: 0.25156277418136597
train_iter_loss: 0.17836959660053253
train_iter_loss: 0.084018275141716
train_iter_loss: 0.48955005407333374
train_iter_loss: 0.17024749517440796
train_iter_loss: 0.19413021206855774
train_iter_loss: 0.10830061882734299
train_iter_loss: 0.12180302292108536
train_iter_loss: 0.03640336915850639
train_iter_loss: 0.19367855787277222
train_iter_loss: 0.24259722232818604
train loss :0.1674
---------------------
Validation seg loss: 0.21617955922693857 at epoch 414
epoch =    415/  1000, exp = train
train_iter_loss: 0.13724926114082336
train_iter_loss: 0.10323413461446762
train_iter_loss: 0.2091415673494339
train_iter_loss: 0.06817658245563507
train_iter_loss: 0.1337609440088272
train_iter_loss: 0.11762456595897675
train_iter_loss: 0.11670728027820587
train_iter_loss: 0.2674562633037567
train_iter_loss: 0.13982896506786346
train_iter_loss: 0.15781348943710327
train_iter_loss: 0.07118184119462967
train_iter_loss: 0.10150710493326187
train_iter_loss: 0.14199505746364594
train_iter_loss: 0.18439875543117523
train_iter_loss: 0.14806032180786133
train_iter_loss: 0.30344003438949585
train_iter_loss: 0.031075196340680122
train_iter_loss: 0.2597203254699707
train_iter_loss: 0.16793116927146912
train_iter_loss: 0.15371692180633545
train_iter_loss: 0.1827254444360733
train_iter_loss: 0.1353113353252411
train_iter_loss: 0.11842219531536102
train_iter_loss: 0.27444565296173096
train_iter_loss: 0.16201938688755035
train_iter_loss: 0.12331248819828033
train_iter_loss: 0.27816230058670044
train_iter_loss: 0.16380514204502106
train_iter_loss: 0.0940183475613594
train_iter_loss: 0.11174274981021881
train_iter_loss: 0.10336944460868835
train_iter_loss: 0.17956052720546722
train_iter_loss: 0.1710248738527298
train_iter_loss: 0.12529446184635162
train_iter_loss: 0.06642761081457138
train_iter_loss: 0.14182205498218536
train_iter_loss: 0.17602428793907166
train_iter_loss: 0.1206493154168129
train_iter_loss: 0.1771770715713501
train_iter_loss: 0.08348129689693451
train_iter_loss: 0.20521633327007294
train_iter_loss: 0.07141593098640442
train_iter_loss: 0.15903320908546448
train_iter_loss: 0.08956433832645416
train_iter_loss: 0.3073870539665222
train_iter_loss: 0.1789732277393341
train_iter_loss: 0.31926965713500977
train_iter_loss: 0.23615096509456635
train_iter_loss: 0.0951278880238533
train_iter_loss: 0.16430263221263885
train_iter_loss: 0.24328739941120148
train_iter_loss: 0.3916967511177063
train_iter_loss: 0.21402354538440704
train_iter_loss: 0.15318641066551208
train_iter_loss: 0.18178799748420715
train_iter_loss: 0.11744726449251175
train_iter_loss: 0.13342152535915375
train_iter_loss: 0.1874878853559494
train_iter_loss: 0.1723550409078598
train_iter_loss: 0.11015915125608444
train_iter_loss: 0.07302066683769226
train_iter_loss: 0.10972831398248672
train_iter_loss: 0.11662542819976807
train_iter_loss: 0.1491287499666214
train_iter_loss: 0.17910778522491455
train_iter_loss: 0.1698850393295288
train_iter_loss: 0.19962090253829956
train_iter_loss: 0.10156349092721939
train_iter_loss: 0.195007786154747
train_iter_loss: 0.12132428586483002
train_iter_loss: 0.13343767821788788
train_iter_loss: 0.14610449969768524
train_iter_loss: 0.17970675230026245
train_iter_loss: 0.13419152796268463
train_iter_loss: 0.19682107865810394
train_iter_loss: 0.19274407625198364
train_iter_loss: 0.12364187091588974
train_iter_loss: 0.1693699210882187
train_iter_loss: 0.1195036917924881
train_iter_loss: 0.11622437834739685
train_iter_loss: 0.15765857696533203
train_iter_loss: 0.18804022669792175
train_iter_loss: 0.1254325658082962
train_iter_loss: 0.2227417379617691
train_iter_loss: 0.17417527735233307
train_iter_loss: 0.15763166546821594
train_iter_loss: 0.1947721689939499
train_iter_loss: 0.12491496652364731
train_iter_loss: 0.22686287760734558
train_iter_loss: 0.12050943076610565
train_iter_loss: 0.08267316967248917
train_iter_loss: 0.11701169610023499
train_iter_loss: 0.10649465024471283
train_iter_loss: 0.17688804864883423
train_iter_loss: 0.32547956705093384
train_iter_loss: 0.19686518609523773
train_iter_loss: 0.17191986739635468
train_iter_loss: 0.11934666335582733
train_iter_loss: 0.25063902139663696
train_iter_loss: 0.06837113946676254
train loss :0.1614
---------------------
Validation seg loss: 0.21917899388749645 at epoch 415
epoch =    416/  1000, exp = train
train_iter_loss: 0.18123777210712433
train_iter_loss: 0.18638984858989716
train_iter_loss: 0.09236550331115723
train_iter_loss: 0.09351540356874466
train_iter_loss: 0.07974545657634735
train_iter_loss: 0.22615797817707062
train_iter_loss: 0.23072633147239685
train_iter_loss: 0.16137930750846863
train_iter_loss: 0.2183483988046646
train_iter_loss: 0.10569671541452408
train_iter_loss: 0.17448240518569946
train_iter_loss: 0.17128705978393555
train_iter_loss: 0.1535845845937729
train_iter_loss: 0.30118194222450256
train_iter_loss: 0.14448514580726624
train_iter_loss: 0.12086577713489532
train_iter_loss: 0.11031696200370789
train_iter_loss: 0.12865965068340302
train_iter_loss: 0.11888353526592255
train_iter_loss: 0.10971372574567795
train_iter_loss: 0.1570889949798584
train_iter_loss: 0.2196337729692459
train_iter_loss: 0.039120230823755264
train_iter_loss: 0.09934882819652557
train_iter_loss: 0.19471436738967896
train_iter_loss: 0.3969540596008301
train_iter_loss: 0.10860705375671387
train_iter_loss: 0.21885627508163452
train_iter_loss: 0.2516174912452698
train_iter_loss: 0.25359854102134705
train_iter_loss: 0.11175644397735596
train_iter_loss: 0.09512149542570114
train_iter_loss: 0.29921412467956543
train_iter_loss: 0.27907073497772217
train_iter_loss: 0.17289231717586517
train_iter_loss: 0.1292153000831604
train_iter_loss: 0.1153230369091034
train_iter_loss: 0.164754718542099
train_iter_loss: 0.15691065788269043
train_iter_loss: 0.14339280128479004
train_iter_loss: 0.17051327228546143
train_iter_loss: 0.1404138058423996
train_iter_loss: 0.13968560099601746
train_iter_loss: 0.17221197485923767
train_iter_loss: 0.1137717142701149
train_iter_loss: 0.18316854536533356
train_iter_loss: 0.14675988256931305
train_iter_loss: 0.14452983438968658
train_iter_loss: 0.25073903799057007
train_iter_loss: 0.21008437871932983
train_iter_loss: 0.20544429123401642
train_iter_loss: 0.20987746119499207
train_iter_loss: 0.1264854222536087
train_iter_loss: 0.13596075773239136
train_iter_loss: 0.21741947531700134
train_iter_loss: 0.09647077322006226
train_iter_loss: 0.11803426593542099
train_iter_loss: 0.08588669449090958
train_iter_loss: 0.1680271178483963
train_iter_loss: 0.15865087509155273
train_iter_loss: 0.1478833109140396
train_iter_loss: 0.32087090611457825
train_iter_loss: 0.3267478942871094
train_iter_loss: 0.1940375715494156
train_iter_loss: 0.18929971754550934
train_iter_loss: 0.11477652192115784
train_iter_loss: 0.160336434841156
train_iter_loss: 0.16088096797466278
train_iter_loss: 0.151753768324852
train_iter_loss: 0.08864076435565948
train_iter_loss: 0.17555689811706543
train_iter_loss: 0.04126182943582535
train_iter_loss: 0.16732938587665558
train_iter_loss: 0.17875294387340546
train_iter_loss: 0.1707877814769745
train_iter_loss: 0.16195909678936005
train_iter_loss: 0.13000032305717468
train_iter_loss: 0.17044100165367126
train_iter_loss: 0.2140152007341385
train_iter_loss: 0.11019706726074219
train_iter_loss: 0.14443963766098022
train_iter_loss: 0.2735989987850189
train_iter_loss: 0.11770527809858322
train_iter_loss: 0.18458586931228638
train_iter_loss: 0.3521864116191864
train_iter_loss: 0.10009098052978516
train_iter_loss: 0.12681682407855988
train_iter_loss: 0.14509637653827667
train_iter_loss: 0.12357529252767563
train_iter_loss: 0.1386939138174057
train_iter_loss: 0.07604900747537613
train_iter_loss: 0.21919381618499756
train_iter_loss: 0.08871946483850479
train_iter_loss: 0.15373285114765167
train_iter_loss: 0.09371478110551834
train_iter_loss: 0.18044736981391907
train_iter_loss: 0.3109719753265381
train_iter_loss: 0.10962936282157898
train_iter_loss: 0.3347702920436859
train_iter_loss: 0.19192039966583252
train loss :0.1691
---------------------
Validation seg loss: 0.22317556743422207 at epoch 416
epoch =    417/  1000, exp = train
train_iter_loss: 0.23664911091327667
train_iter_loss: 0.07366759330034256
train_iter_loss: 0.11854639649391174
train_iter_loss: 0.09792152792215347
train_iter_loss: 0.18434292078018188
train_iter_loss: 0.09198111295700073
train_iter_loss: 0.10765212029218674
train_iter_loss: 0.16141414642333984
train_iter_loss: 0.04831218719482422
train_iter_loss: 0.157633975148201
train_iter_loss: 0.07731176912784576
train_iter_loss: 0.18132148683071136
train_iter_loss: 0.15276522934436798
train_iter_loss: 0.20504991710186005
train_iter_loss: 0.08829176425933838
train_iter_loss: 0.11300017684698105
train_iter_loss: 0.19448697566986084
train_iter_loss: 0.17565450072288513
train_iter_loss: 0.0996110811829567
train_iter_loss: 0.07465860992670059
train_iter_loss: 0.13940393924713135
train_iter_loss: 0.3753153383731842
train_iter_loss: 0.235092893242836
train_iter_loss: 0.12218664586544037
train_iter_loss: 0.1512298732995987
train_iter_loss: 0.12047047913074493
train_iter_loss: 0.2495400458574295
train_iter_loss: 0.11526872962713242
train_iter_loss: 0.213624507188797
train_iter_loss: 0.6117173433303833
train_iter_loss: 0.15401197969913483
train_iter_loss: 0.15709401667118073
train_iter_loss: 0.27509966492652893
train_iter_loss: 0.2119280993938446
train_iter_loss: 0.14130307734012604
train_iter_loss: 0.27700236439704895
train_iter_loss: 0.1723077893257141
train_iter_loss: 0.1647859364748001
train_iter_loss: 0.0940697193145752
train_iter_loss: 0.15476080775260925
train_iter_loss: 0.1514514982700348
train_iter_loss: 0.14380799233913422
train_iter_loss: 0.1573086827993393
train_iter_loss: 0.1771140694618225
train_iter_loss: 0.22806020081043243
train_iter_loss: 0.1685144156217575
train_iter_loss: 0.12968440353870392
train_iter_loss: 0.24586716294288635
train_iter_loss: 0.13053198158740997
train_iter_loss: 0.14935889840126038
train_iter_loss: 0.19254665076732635
train_iter_loss: 0.1249421238899231
train_iter_loss: 0.1408199518918991
train_iter_loss: 0.13798946142196655
train_iter_loss: 0.18666969239711761
train_iter_loss: 0.14194457232952118
train_iter_loss: 0.0989861860871315
train_iter_loss: 0.16415919363498688
train_iter_loss: 0.10555689036846161
train_iter_loss: 0.1513780653476715
train_iter_loss: 0.3378010094165802
train_iter_loss: 0.2214929312467575
train_iter_loss: 0.1388116031885147
train_iter_loss: 0.20757776498794556
train_iter_loss: 0.13718751072883606
train_iter_loss: 0.06430896371603012
train_iter_loss: 0.1473916918039322
train_iter_loss: 0.11155437678098679
train_iter_loss: 0.2424909919500351
train_iter_loss: 0.1406739205121994
train_iter_loss: 0.18282021582126617
train_iter_loss: 0.1439211666584015
train_iter_loss: 0.16263777017593384
train_iter_loss: 0.13426467776298523
train_iter_loss: 0.15294194221496582
train_iter_loss: 0.18737147748470306
train_iter_loss: 0.1646907478570938
train_iter_loss: 0.19979824125766754
train_iter_loss: 0.2005661427974701
train_iter_loss: 0.09623826295137405
train_iter_loss: 0.21345944702625275
train_iter_loss: 0.1859021931886673
train_iter_loss: 0.10008800029754639
train_iter_loss: 0.09636162966489792
train_iter_loss: 0.11687209457159042
train_iter_loss: 0.26064372062683105
train_iter_loss: 0.08942890912294388
train_iter_loss: 0.1239563599228859
train_iter_loss: 0.17601124942302704
train_iter_loss: 0.20626060664653778
train_iter_loss: 0.11261778324842453
train_iter_loss: 0.11451492458581924
train_iter_loss: 0.15048477053642273
train_iter_loss: 0.12996816635131836
train_iter_loss: 0.19274933636188507
train_iter_loss: 0.3397330045700073
train_iter_loss: 0.13174697756767273
train_iter_loss: 0.21757423877716064
train_iter_loss: 0.13250240683555603
train_iter_loss: 0.2738324701786041
train loss :0.1681
---------------------
Validation seg loss: 0.21660242631223123 at epoch 417
epoch =    418/  1000, exp = train
train_iter_loss: 0.11851285398006439
train_iter_loss: 0.0924287661910057
train_iter_loss: 0.22277817130088806
train_iter_loss: 0.21532079577445984
train_iter_loss: 0.1857433021068573
train_iter_loss: 0.16760040819644928
train_iter_loss: 0.16299687325954437
train_iter_loss: 0.123224638402462
train_iter_loss: 0.13996052742004395
train_iter_loss: 0.1779775768518448
train_iter_loss: 0.1088482066988945
train_iter_loss: 0.17058487236499786
train_iter_loss: 0.19320979714393616
train_iter_loss: 0.3396190106868744
train_iter_loss: 0.22388385236263275
train_iter_loss: 0.1366536021232605
train_iter_loss: 0.22838443517684937
train_iter_loss: 0.09159398078918457
train_iter_loss: 0.15039408206939697
train_iter_loss: 0.11611099541187286
train_iter_loss: 0.07127798348665237
train_iter_loss: 0.11731410771608353
train_iter_loss: 0.27742141485214233
train_iter_loss: 0.15142972767353058
train_iter_loss: 0.1848587691783905
train_iter_loss: 0.18634219467639923
train_iter_loss: 0.14221073687076569
train_iter_loss: 0.14729976654052734
train_iter_loss: 0.1029607430100441
train_iter_loss: 0.12306089699268341
train_iter_loss: 0.11106821894645691
train_iter_loss: 0.17208415269851685
train_iter_loss: 0.09548565000295639
train_iter_loss: 0.2250642329454422
train_iter_loss: 0.11230688542127609
train_iter_loss: 0.1714676171541214
train_iter_loss: 0.1923271268606186
train_iter_loss: 0.1190609335899353
train_iter_loss: 0.09223177284002304
train_iter_loss: 0.24903592467308044
train_iter_loss: 0.09034021198749542
train_iter_loss: 0.1918107122182846
train_iter_loss: 0.20303691923618317
train_iter_loss: 0.2281905710697174
train_iter_loss: 0.05368741974234581
train_iter_loss: 0.14983990788459778
train_iter_loss: 0.11031416803598404
train_iter_loss: 0.09370513260364532
train_iter_loss: 0.174258291721344
train_iter_loss: 0.18080028891563416
train_iter_loss: 0.17647795379161835
train_iter_loss: 0.11504004150629044
train_iter_loss: 0.15494588017463684
train_iter_loss: 0.1581527441740036
train_iter_loss: 0.0649678036570549
train_iter_loss: 0.11140187829732895
train_iter_loss: 0.1162916049361229
train_iter_loss: 0.22731062769889832
train_iter_loss: 0.1879965215921402
train_iter_loss: 0.11254595220088959
train_iter_loss: 0.23059767484664917
train_iter_loss: 0.09875395148992538
train_iter_loss: 0.10124564170837402
train_iter_loss: 0.09059048444032669
train_iter_loss: 0.1648712158203125
train_iter_loss: 0.251193106174469
train_iter_loss: 0.16296713054180145
train_iter_loss: 0.20894820988178253
train_iter_loss: 0.1425279974937439
train_iter_loss: 0.15526072680950165
train_iter_loss: 0.13903380930423737
train_iter_loss: 0.09237544238567352
train_iter_loss: 0.07326830178499222
train_iter_loss: 0.22194908559322357
train_iter_loss: 0.1763496994972229
train_iter_loss: 0.22389863431453705
train_iter_loss: 0.177734375
train_iter_loss: 0.23138265311717987
train_iter_loss: 0.1641753613948822
train_iter_loss: 0.10388672351837158
train_iter_loss: 0.22961066663265228
train_iter_loss: 0.14953340590000153
train_iter_loss: 0.17782606184482574
train_iter_loss: 0.0998496487736702
train_iter_loss: 0.16807007789611816
train_iter_loss: 0.12816761434078217
train_iter_loss: 0.11859975755214691
train_iter_loss: 0.16753531992435455
train_iter_loss: 0.1481168270111084
train_iter_loss: 0.18347977101802826
train_iter_loss: 0.09123753011226654
train_iter_loss: 0.1134888082742691
train_iter_loss: 0.13697968423366547
train_iter_loss: 0.20274139940738678
train_iter_loss: 0.13656866550445557
train_iter_loss: 0.1717374622821808
train_iter_loss: 0.10164185613393784
train_iter_loss: 0.16329573094844818
train_iter_loss: 0.1677907109260559
train_iter_loss: 0.257670521736145
train loss :0.1580
---------------------
Validation seg loss: 0.2181526330949844 at epoch 418
epoch =    419/  1000, exp = train
train_iter_loss: 0.13815687596797943
train_iter_loss: 0.09993041306734085
train_iter_loss: 0.19546927511692047
train_iter_loss: 0.189544677734375
train_iter_loss: 0.23938848078250885
train_iter_loss: 0.1499047428369522
train_iter_loss: 0.20926423370838165
train_iter_loss: 0.08845718950033188
train_iter_loss: 0.04961227998137474
train_iter_loss: 0.1465211659669876
train_iter_loss: 0.10295718908309937
train_iter_loss: 0.14668376743793488
train_iter_loss: 0.16062958538532257
train_iter_loss: 0.184853196144104
train_iter_loss: 0.23200106620788574
train_iter_loss: 0.06404785811901093
train_iter_loss: 0.20702871680259705
train_iter_loss: 0.10845886915922165
train_iter_loss: 0.13824231922626495
train_iter_loss: 0.10649557411670685
train_iter_loss: 0.12060753256082535
train_iter_loss: 0.06377720087766647
train_iter_loss: 0.0528123639523983
train_iter_loss: 0.07706718891859055
train_iter_loss: 0.31828024983406067
train_iter_loss: 0.13376474380493164
train_iter_loss: 0.23344722390174866
train_iter_loss: 0.09603364020586014
train_iter_loss: 0.17308162152767181
train_iter_loss: 0.1058359295129776
train_iter_loss: 0.14041317999362946
train_iter_loss: 0.12276890128850937
train_iter_loss: 0.29379430413246155
train_iter_loss: 0.1141558587551117
train_iter_loss: 0.249099001288414
train_iter_loss: 0.0836370587348938
train_iter_loss: 0.20564989745616913
train_iter_loss: 0.1535170078277588
train_iter_loss: 0.11017460376024246
train_iter_loss: 0.20705857872962952
train_iter_loss: 0.0829116627573967
train_iter_loss: 0.1774723380804062
train_iter_loss: 0.17314785718917847
train_iter_loss: 0.12533070147037506
train_iter_loss: 0.14443732798099518
train_iter_loss: 0.09752639383077621
train_iter_loss: 0.18292227387428284
train_iter_loss: 0.18375438451766968
train_iter_loss: 0.10221521556377411
train_iter_loss: 0.195030078291893
train_iter_loss: 0.24585573375225067
train_iter_loss: 0.19170527160167694
train_iter_loss: 0.0963687151670456
train_iter_loss: 0.19594775140285492
train_iter_loss: 0.21922798454761505
train_iter_loss: 0.18757463991641998
train_iter_loss: 0.20607778429985046
train_iter_loss: 0.12864689528942108
train_iter_loss: 0.20419654250144958
train_iter_loss: 0.1750687211751938
train_iter_loss: 0.2546514868736267
train_iter_loss: 0.24096941947937012
train_iter_loss: 0.1278388351202011
train_iter_loss: 0.08709398657083511
train_iter_loss: 0.1698741763830185
train_iter_loss: 0.3110346496105194
train_iter_loss: 0.15382038056850433
train_iter_loss: 0.1646622270345688
train_iter_loss: 0.11492792516946793
train_iter_loss: 0.23754790425300598
train_iter_loss: 0.15955404937267303
train_iter_loss: 0.10571665316820145
train_iter_loss: 0.08538347482681274
train_iter_loss: 0.030410785228013992
train_iter_loss: 0.20694969594478607
train_iter_loss: 0.14415670931339264
train_iter_loss: 0.1408364623785019
train_iter_loss: 0.1998928040266037
train_iter_loss: 0.24974456429481506
train_iter_loss: 0.12074308842420578
train_iter_loss: 0.18882831931114197
train_iter_loss: 0.10998523980379105
train_iter_loss: 0.12744463980197906
train_iter_loss: 0.15864233672618866
train_iter_loss: 0.09146909415721893
train_iter_loss: 0.10644648969173431
train_iter_loss: 0.16702793538570404
train_iter_loss: 0.21566902101039886
train_iter_loss: 0.17509068548679352
train_iter_loss: 0.0976778194308281
train_iter_loss: 0.152458056807518
train_iter_loss: 0.18941980600357056
train_iter_loss: 0.10518812388181686
train_iter_loss: 0.1890254020690918
train_iter_loss: 0.18884694576263428
train_iter_loss: 0.23120005428791046
train_iter_loss: 0.132065087556839
train_iter_loss: 0.13945715129375458
train_iter_loss: 0.16297684609889984
train_iter_loss: 0.1524232029914856
train loss :0.1587
---------------------
Validation seg loss: 0.22032416249045506 at epoch 419
epoch =    420/  1000, exp = train
train_iter_loss: 0.14489899575710297
train_iter_loss: 0.16094405949115753
train_iter_loss: 0.1714274138212204
train_iter_loss: 0.1023784875869751
train_iter_loss: 0.18876579403877258
train_iter_loss: 0.14711584150791168
train_iter_loss: 0.08165758848190308
train_iter_loss: 0.18893688917160034
train_iter_loss: 0.12019171565771103
train_iter_loss: 0.25371167063713074
train_iter_loss: 0.1748320758342743
train_iter_loss: 0.14836831390857697
train_iter_loss: 0.17434802651405334
train_iter_loss: 0.13136808574199677
train_iter_loss: 0.15837362408638
train_iter_loss: 0.13488025963306427
train_iter_loss: 0.14875537157058716
train_iter_loss: 0.2543964684009552
train_iter_loss: 0.040760669857263565
train_iter_loss: 0.1885090172290802
train_iter_loss: 0.20874854922294617
train_iter_loss: 0.10535149276256561
train_iter_loss: 0.12687215209007263
train_iter_loss: 0.1585974246263504
train_iter_loss: 0.09449830651283264
train_iter_loss: 0.22613288462162018
train_iter_loss: 0.19027914106845856
train_iter_loss: 0.248831108212471
train_iter_loss: 0.16356591880321503
train_iter_loss: 0.1185639426112175
train_iter_loss: 0.16829454898834229
train_iter_loss: 0.18238931894302368
train_iter_loss: 0.16659530997276306
train_iter_loss: 0.075792096555233
train_iter_loss: 0.16268962621688843
train_iter_loss: 0.098537877202034
train_iter_loss: 0.17702274024486542
train_iter_loss: 0.1642858237028122
train_iter_loss: 0.11093108355998993
train_iter_loss: 0.18382102251052856
train_iter_loss: 0.35567232966423035
train_iter_loss: 0.16204248368740082
train_iter_loss: 0.24313034117221832
train_iter_loss: 0.15432995557785034
train_iter_loss: 0.09417030215263367
train_iter_loss: 0.16306115686893463
train_iter_loss: 0.09210941940546036
train_iter_loss: 0.2799017131328583
train_iter_loss: 0.15986953675746918
train_iter_loss: 0.12808363139629364
train_iter_loss: 0.18907344341278076
train_iter_loss: 0.1990409791469574
train_iter_loss: 0.20365296304225922
train_iter_loss: 0.09009677171707153
train_iter_loss: 0.16079352796077728
train_iter_loss: 0.11910369247198105
train_iter_loss: 0.10317858308553696
train_iter_loss: 0.09904778748750687
train_iter_loss: 0.2934435307979584
train_iter_loss: 0.16742351651191711
train_iter_loss: 0.24663375318050385
train_iter_loss: 0.1701057255268097
train_iter_loss: 0.18120937049388885
train_iter_loss: 0.19441482424736023
train_iter_loss: 0.13869790732860565
train_iter_loss: 0.2212214469909668
train_iter_loss: 0.15807484090328217
train_iter_loss: 0.3154100477695465
train_iter_loss: 0.1890253722667694
train_iter_loss: 0.08691330254077911
train_iter_loss: 0.2389688938856125
train_iter_loss: 0.13504824042320251
train_iter_loss: 0.032029688358306885
train_iter_loss: 0.22372613847255707
train_iter_loss: 0.13109418749809265
train_iter_loss: 0.09616726636886597
train_iter_loss: 0.18114879727363586
train_iter_loss: 0.13250793516635895
train_iter_loss: 0.20473583042621613
train_iter_loss: 0.22048278152942657
train_iter_loss: 0.1879035234451294
train_iter_loss: 0.16580715775489807
train_iter_loss: 0.08579888194799423
train_iter_loss: 0.23308537900447845
train_iter_loss: 0.1115938127040863
train_iter_loss: 0.07083366066217422
train_iter_loss: 0.20949871838092804
train_iter_loss: 0.23731684684753418
train_iter_loss: 0.12590515613555908
train_iter_loss: 0.1977454423904419
train_iter_loss: 0.2212800681591034
train_iter_loss: 0.17341400682926178
train_iter_loss: 0.19991371035575867
train_iter_loss: 0.036211296916007996
train_iter_loss: 0.13153257966041565
train_iter_loss: 0.09011799842119217
train_iter_loss: 0.28819724917411804
train_iter_loss: 0.09606033563613892
train_iter_loss: 0.1777775138616562
train_iter_loss: 0.15902121365070343
train loss :0.1656
---------------------
Validation seg loss: 0.21908394149768184 at epoch 420
epoch =    421/  1000, exp = train
train_iter_loss: 0.09121126681566238
train_iter_loss: 0.27964770793914795
train_iter_loss: 0.1381908357143402
train_iter_loss: 0.10321085900068283
train_iter_loss: 0.25996777415275574
train_iter_loss: 0.2169313281774521
train_iter_loss: 0.13455048203468323
train_iter_loss: 0.18295355141162872
train_iter_loss: 0.11521685868501663
train_iter_loss: 0.3207610845565796
train_iter_loss: 0.08406537026166916
train_iter_loss: 0.24185946583747864
train_iter_loss: 0.1559196412563324
train_iter_loss: 0.11972544342279434
train_iter_loss: 0.14131735265254974
train_iter_loss: 0.1493423879146576
train_iter_loss: 0.15669287741184235
train_iter_loss: 0.15616898238658905
train_iter_loss: 0.11341674625873566
train_iter_loss: 0.1744147539138794
train_iter_loss: 0.18368488550186157
train_iter_loss: 0.02477634698152542
train_iter_loss: 0.11041148751974106
train_iter_loss: 0.11587680876255035
train_iter_loss: 0.1953102946281433
train_iter_loss: 0.18038833141326904
train_iter_loss: 0.1840025782585144
train_iter_loss: 0.20737196505069733
train_iter_loss: 0.08214502781629562
train_iter_loss: 0.13869716227054596
train_iter_loss: 0.24121960997581482
train_iter_loss: 0.17928069829940796
train_iter_loss: 0.09875841438770294
train_iter_loss: 0.08266737312078476
train_iter_loss: 0.09633927047252655
train_iter_loss: 0.23114551603794098
train_iter_loss: 0.21352985501289368
train_iter_loss: 0.13802433013916016
train_iter_loss: 0.06350360065698624
train_iter_loss: 0.206075057387352
train_iter_loss: 0.08428885787725449
train_iter_loss: 0.19112549722194672
train_iter_loss: 0.08203182369470596
train_iter_loss: 0.22471100091934204
train_iter_loss: 0.22431986033916473
train_iter_loss: 0.11243801563978195
train_iter_loss: 0.25386637449264526
train_iter_loss: 0.14122873544692993
train_iter_loss: 0.03671155497431755
train_iter_loss: 0.3332240879535675
train_iter_loss: 0.14151956140995026
train_iter_loss: 0.1245017796754837
train_iter_loss: 0.13525478541851044
train_iter_loss: 0.1375565081834793
train_iter_loss: 0.16573333740234375
train_iter_loss: 0.18640172481536865
train_iter_loss: 0.17946772277355194
train_iter_loss: 0.15758267045021057
train_iter_loss: 0.19303208589553833
train_iter_loss: 0.30138343572616577
train_iter_loss: 0.08335357159376144
train_iter_loss: 0.18992692232131958
train_iter_loss: 0.10428845882415771
train_iter_loss: 0.27844923734664917
train_iter_loss: 0.13836561143398285
train_iter_loss: 0.16368646919727325
train_iter_loss: 0.1020599752664566
train_iter_loss: 0.14473338425159454
train_iter_loss: 0.37465110421180725
train_iter_loss: 0.18312758207321167
train_iter_loss: 0.3736664652824402
train_iter_loss: 0.07696906477212906
train_iter_loss: 0.0918954610824585
train_iter_loss: 0.11661043763160706
train_iter_loss: 0.1123339831829071
train_iter_loss: 0.3095671236515045
train_iter_loss: 0.08179587870836258
train_iter_loss: 0.12877455353736877
train_iter_loss: 0.12963521480560303
train_iter_loss: 0.05787285789847374
train_iter_loss: 0.12115468084812164
train_iter_loss: 0.15620651841163635
train_iter_loss: 0.1432560235261917
train_iter_loss: 0.14384645223617554
train_iter_loss: 0.1333381086587906
train_iter_loss: 0.20802448689937592
train_iter_loss: 0.14253729581832886
train_iter_loss: 0.16573616862297058
train_iter_loss: 0.23393337428569794
train_iter_loss: 0.14190217852592468
train_iter_loss: 0.15662021934986115
train_iter_loss: 0.22289419174194336
train_iter_loss: 0.22577406466007233
train_iter_loss: 0.1657264232635498
train_iter_loss: 0.19857023656368256
train_iter_loss: 0.1690235286951065
train_iter_loss: 0.2498941868543625
train_iter_loss: 0.14364725351333618
train_iter_loss: 0.3134880065917969
train_iter_loss: 0.16474266350269318
train loss :0.1672
---------------------
Validation seg loss: 0.2239789785202241 at epoch 421
epoch =    422/  1000, exp = train
train_iter_loss: 0.06178629770874977
train_iter_loss: 0.15388329327106476
train_iter_loss: 0.25582799315452576
train_iter_loss: 0.08223370462656021
train_iter_loss: 0.17475369572639465
train_iter_loss: 0.14622676372528076
train_iter_loss: 0.20258307456970215
train_iter_loss: 0.11892055720090866
train_iter_loss: 0.14944307506084442
train_iter_loss: 0.22902028262615204
train_iter_loss: 0.22898784279823303
train_iter_loss: 0.1520860195159912
train_iter_loss: 0.1258648931980133
train_iter_loss: 0.2293982058763504
train_iter_loss: 0.2392219752073288
train_iter_loss: 0.20374302566051483
train_iter_loss: 0.11112935841083527
train_iter_loss: 0.16141746938228607
train_iter_loss: 0.22450438141822815
train_iter_loss: 0.14794579148292542
train_iter_loss: 0.144034281373024
train_iter_loss: 0.17197975516319275
train_iter_loss: 0.14706173539161682
train_iter_loss: 0.23067207634449005
train_iter_loss: 0.19431591033935547
train_iter_loss: 0.16846111416816711
train_iter_loss: 0.17457978427410126
train_iter_loss: 0.1445634365081787
train_iter_loss: 0.16415023803710938
train_iter_loss: 0.2925983667373657
train_iter_loss: 0.1906624287366867
train_iter_loss: 0.05968497321009636
train_iter_loss: 0.1370105743408203
train_iter_loss: 0.21787428855895996
train_iter_loss: 0.19375990331172943
train_iter_loss: 0.1079045832157135
train_iter_loss: 0.23278076946735382
train_iter_loss: 0.17626194655895233
train_iter_loss: 0.13974840939044952
train_iter_loss: 0.0666160136461258
train_iter_loss: 0.16002458333969116
train_iter_loss: 0.141090527176857
train_iter_loss: 0.18086135387420654
train_iter_loss: 0.08120059221982956
train_iter_loss: 0.0906083881855011
train_iter_loss: 0.17929765582084656
train_iter_loss: 0.08617230504751205
train_iter_loss: 0.1290227174758911
train_iter_loss: 0.1120164766907692
train_iter_loss: 0.24331121146678925
train_iter_loss: 0.14159096777439117
train_iter_loss: 0.21271619200706482
train_iter_loss: 0.07289943844079971
train_iter_loss: 0.1282338947057724
train_iter_loss: 0.11635866016149521
train_iter_loss: 0.27039140462875366
train_iter_loss: 0.21331441402435303
train_iter_loss: 0.15224574506282806
train_iter_loss: 0.2101210057735443
train_iter_loss: 0.15048126876354218
train_iter_loss: 0.2601315677165985
train_iter_loss: 0.15332002937793732
train_iter_loss: 0.15196309983730316
train_iter_loss: 0.12755101919174194
train_iter_loss: 0.19401150941848755
train_iter_loss: 0.32248353958129883
train_iter_loss: 0.12944179773330688
train_iter_loss: 0.1506361961364746
train_iter_loss: 0.21004663407802582
train_iter_loss: 0.1369124948978424
train_iter_loss: 0.13196881115436554
train_iter_loss: 0.4782872200012207
train_iter_loss: 0.19067049026489258
train_iter_loss: 0.13923247158527374
train_iter_loss: 0.14007054269313812
train_iter_loss: 0.22935432195663452
train_iter_loss: 0.1827777773141861
train_iter_loss: 0.09987753629684448
train_iter_loss: 0.10809338837862015
train_iter_loss: 0.13038070499897003
train_iter_loss: 0.16866514086723328
train_iter_loss: 0.08410338312387466
train_iter_loss: 0.2360474318265915
train_iter_loss: 0.20211496949195862
train_iter_loss: 0.16356261074543
train_iter_loss: 0.1432337611913681
train_iter_loss: 0.14860288798809052
train_iter_loss: 0.1214112788438797
train_iter_loss: 0.10922030359506607
train_iter_loss: 0.20083965361118317
train_iter_loss: 0.1525810807943344
train_iter_loss: 0.09180083870887756
train_iter_loss: 0.11783331632614136
train_iter_loss: 0.14812448620796204
train_iter_loss: 0.0647585317492485
train_iter_loss: 0.22314471006393433
train_iter_loss: 0.1479419618844986
train_iter_loss: 0.18561366200447083
train_iter_loss: 0.228990375995636
train_iter_loss: 0.2014329880475998
train loss :0.1680
---------------------
Validation seg loss: 0.2180303075210244 at epoch 422
epoch =    423/  1000, exp = train
train_iter_loss: 0.12102571129798889
train_iter_loss: 0.09920863807201385
train_iter_loss: 0.18818844854831696
train_iter_loss: 0.13890299201011658
train_iter_loss: 0.10866958647966385
train_iter_loss: 0.14868995547294617
train_iter_loss: 0.40641504526138306
train_iter_loss: 0.08602005988359451
train_iter_loss: 0.046772826462984085
train_iter_loss: 0.09318488836288452
train_iter_loss: 0.15842536091804504
train_iter_loss: 0.1114017590880394
train_iter_loss: 0.2387474626302719
train_iter_loss: 0.11439435184001923
train_iter_loss: 0.20353330671787262
train_iter_loss: 0.2138974517583847
train_iter_loss: 0.3202962577342987
train_iter_loss: 0.20885826647281647
train_iter_loss: 0.08286207169294357
train_iter_loss: 0.1710149049758911
train_iter_loss: 0.15118293464183807
train_iter_loss: 0.1597282588481903
train_iter_loss: 0.08629663288593292
train_iter_loss: 0.08973602205514908
train_iter_loss: 0.2012016624212265
train_iter_loss: 0.15078367292881012
train_iter_loss: 0.13944701850414276
train_iter_loss: 0.15591877698898315
train_iter_loss: 0.11660641431808472
train_iter_loss: 0.10284639894962311
train_iter_loss: 0.17191900312900543
train_iter_loss: 0.2052060216665268
train_iter_loss: 0.05505634471774101
train_iter_loss: 0.32428786158561707
train_iter_loss: 0.08843883126974106
train_iter_loss: 0.36857908964157104
train_iter_loss: 0.25478577613830566
train_iter_loss: 0.18011474609375
train_iter_loss: 0.16633950173854828
train_iter_loss: 0.1681717485189438
train_iter_loss: 0.20352408289909363
train_iter_loss: 0.2396167665719986
train_iter_loss: 0.16452744603157043
train_iter_loss: 0.1568298041820526
train_iter_loss: 0.3979720175266266
train_iter_loss: 0.12175241857767105
train_iter_loss: 0.12780322134494781
train_iter_loss: 0.10992936789989471
train_iter_loss: 0.3271535336971283
train_iter_loss: 0.08586202561855316
train_iter_loss: 0.20575538277626038
train_iter_loss: 0.43647584319114685
train_iter_loss: 0.17652758955955505
train_iter_loss: 0.20911337435245514
train_iter_loss: 0.20578008890151978
train_iter_loss: 0.12671437859535217
train_iter_loss: 0.1250184029340744
train_iter_loss: 0.11765969544649124
train_iter_loss: 0.12302635610103607
train_iter_loss: 0.09599827975034714
train_iter_loss: 0.19579565525054932
train_iter_loss: 0.34623289108276367
train_iter_loss: 0.1896059811115265
train_iter_loss: 0.6009880900382996
train_iter_loss: 0.08003229647874832
train_iter_loss: 0.08094437420368195
train_iter_loss: 0.20042704045772552
train_iter_loss: 0.09113159030675888
train_iter_loss: 0.21763582527637482
train_iter_loss: 0.11293400824069977
train_iter_loss: 0.13219326734542847
train_iter_loss: 0.1922966092824936
train_iter_loss: 0.16423314809799194
train_iter_loss: 0.12156244367361069
train_iter_loss: 0.2772952914237976
train_iter_loss: 0.12381341308355331
train_iter_loss: 0.21558471024036407
train_iter_loss: 0.17960895597934723
train_iter_loss: 0.17790253460407257
train_iter_loss: 0.1606440544128418
train_iter_loss: 0.13579517602920532
train_iter_loss: 0.12515877187252045
train_iter_loss: 0.10614989697933197
train_iter_loss: 0.12395183742046356
train_iter_loss: 0.2421291172504425
train_iter_loss: 0.231662318110466
train_iter_loss: 0.21057870984077454
train_iter_loss: 0.1983318030834198
train_iter_loss: 0.13062836229801178
train_iter_loss: 0.1659378707408905
train_iter_loss: 0.1703753024339676
train_iter_loss: 0.11326689273118973
train_iter_loss: 0.06338779628276825
train_iter_loss: 0.12123450636863708
train_iter_loss: 0.13018225133419037
train_iter_loss: 0.15778137743473053
train_iter_loss: 0.1567658632993698
train_iter_loss: 0.20156608521938324
train_iter_loss: 0.13455237448215485
train_iter_loss: 0.16725307703018188
train loss :0.1747
---------------------
Validation seg loss: 0.22152817485643164 at epoch 423
epoch =    424/  1000, exp = train
train_iter_loss: 0.20834064483642578
train_iter_loss: 0.08145483583211899
train_iter_loss: 0.1731272041797638
train_iter_loss: 0.140315979719162
train_iter_loss: 0.09499220550060272
train_iter_loss: 0.11465731263160706
train_iter_loss: 0.1453281044960022
train_iter_loss: 0.059700317680835724
train_iter_loss: 0.1538134664297104
train_iter_loss: 0.15158867835998535
train_iter_loss: 0.060853537172079086
train_iter_loss: 0.1297813206911087
train_iter_loss: 0.1971805840730667
train_iter_loss: 0.1289500892162323
train_iter_loss: 0.17993620038032532
train_iter_loss: 0.2009241133928299
train_iter_loss: 0.17939448356628418
train_iter_loss: 0.0644962340593338
train_iter_loss: 0.14585980772972107
train_iter_loss: 0.22153626382350922
train_iter_loss: 0.05508464574813843
train_iter_loss: 0.21882128715515137
train_iter_loss: 0.35194942355155945
train_iter_loss: 0.08835773169994354
train_iter_loss: 0.1538994163274765
train_iter_loss: 0.2375558316707611
train_iter_loss: 0.12089215964078903
train_iter_loss: 0.12036861479282379
train_iter_loss: 0.07490807771682739
train_iter_loss: 0.06899934262037277
train_iter_loss: 0.19689638912677765
train_iter_loss: 0.22739529609680176
train_iter_loss: 0.1356835812330246
train_iter_loss: 0.1604483723640442
train_iter_loss: 0.1668550968170166
train_iter_loss: 0.12930341064929962
train_iter_loss: 0.204392671585083
train_iter_loss: 0.20381629467010498
train_iter_loss: 0.1231779158115387
train_iter_loss: 0.046015843749046326
train_iter_loss: 0.10554815828800201
train_iter_loss: 0.19897282123565674
train_iter_loss: 0.08610320091247559
train_iter_loss: 0.22727593779563904
train_iter_loss: 0.19519762694835663
train_iter_loss: 0.3197544515132904
train_iter_loss: 0.11015091836452484
train_iter_loss: 0.3014090657234192
train_iter_loss: 0.10248897969722748
train_iter_loss: 0.21393567323684692
train_iter_loss: 0.28224071860313416
train_iter_loss: 0.136175736784935
train_iter_loss: 0.12566785514354706
train_iter_loss: 0.2552930414676666
train_iter_loss: 0.20500881969928741
train_iter_loss: 0.10015017539262772
train_iter_loss: 0.2202232927083969
train_iter_loss: 0.22346284985542297
train_iter_loss: 0.22799767553806305
train_iter_loss: 0.2523791491985321
train_iter_loss: 0.19152510166168213
train_iter_loss: 0.19227728247642517
train_iter_loss: 0.1431906819343567
train_iter_loss: 0.19124053418636322
train_iter_loss: 0.2120155394077301
train_iter_loss: 0.17152339220046997
train_iter_loss: 0.0441252775490284
train_iter_loss: 0.21134215593338013
train_iter_loss: 0.15735696256160736
train_iter_loss: 0.18608322739601135
train_iter_loss: 0.17574542760849
train_iter_loss: 0.15437214076519012
train_iter_loss: 0.23781520128250122
train_iter_loss: 0.19050335884094238
train_iter_loss: 0.14151863753795624
train_iter_loss: 0.11738714575767517
train_iter_loss: 0.14083369076251984
train_iter_loss: 0.12706933915615082
train_iter_loss: 0.22176408767700195
train_iter_loss: 0.17236654460430145
train_iter_loss: 0.07986149191856384
train_iter_loss: 0.06572281569242477
train_iter_loss: 0.13380838930606842
train_iter_loss: 0.16150382161140442
train_iter_loss: 0.14846932888031006
train_iter_loss: 0.1374608874320984
train_iter_loss: 0.18689002096652985
train_iter_loss: 0.22516430914402008
train_iter_loss: 0.10765120387077332
train_iter_loss: 0.07536382228136063
train_iter_loss: 0.08310147374868393
train_iter_loss: 0.10503776371479034
train_iter_loss: 0.2245911955833435
train_iter_loss: 0.11161968111991882
train_iter_loss: 0.13486027717590332
train_iter_loss: 0.26138415932655334
train_iter_loss: 0.1351298838853836
train_iter_loss: 0.16228285431861877
train_iter_loss: 0.2312425971031189
train_iter_loss: 0.22572007775306702
train loss :0.1636
---------------------
Validation seg loss: 0.21997086615916692 at epoch 424
epoch =    425/  1000, exp = train
train_iter_loss: 0.16393762826919556
train_iter_loss: 0.08111293613910675
train_iter_loss: 0.14979226887226105
train_iter_loss: 0.13598518073558807
train_iter_loss: 0.09599050134420395
train_iter_loss: 0.07780247926712036
train_iter_loss: 0.19859834015369415
train_iter_loss: 0.22801703214645386
train_iter_loss: 0.15009351074695587
train_iter_loss: 0.12734363973140717
train_iter_loss: 0.18437603116035461
train_iter_loss: 0.16499808430671692
train_iter_loss: 0.16206499934196472
train_iter_loss: 0.1419178992509842
train_iter_loss: 0.036022841930389404
train_iter_loss: 0.11171477288007736
train_iter_loss: 0.15863734483718872
train_iter_loss: 0.17699630558490753
train_iter_loss: 0.12285086512565613
train_iter_loss: 0.1666717529296875
train_iter_loss: 0.22435767948627472
train_iter_loss: 0.18664899468421936
train_iter_loss: 0.07536371797323227
train_iter_loss: 0.12243020534515381
train_iter_loss: 0.1282324194908142
train_iter_loss: 0.2107793241739273
train_iter_loss: 0.182501420378685
train_iter_loss: 0.2139800488948822
train_iter_loss: 0.11188811808824539
train_iter_loss: 0.05799413472414017
train_iter_loss: 0.1665150225162506
train_iter_loss: 0.13402503728866577
train_iter_loss: 0.10892293602228165
train_iter_loss: 0.10853798687458038
train_iter_loss: 0.19301657378673553
train_iter_loss: 0.036130569875240326
train_iter_loss: 0.14116638898849487
train_iter_loss: 0.21350112557411194
train_iter_loss: 0.17574261128902435
train_iter_loss: 0.14407910406589508
train_iter_loss: 0.1618293672800064
train_iter_loss: 0.12394142895936966
train_iter_loss: 0.13320355117321014
train_iter_loss: 0.217637300491333
train_iter_loss: 0.18730224668979645
train_iter_loss: 0.13984958827495575
train_iter_loss: 0.1692575067281723
train_iter_loss: 0.11581681668758392
train_iter_loss: 0.14599578082561493
train_iter_loss: 0.17158235609531403
train_iter_loss: 0.2962886691093445
train_iter_loss: 0.07684468477964401
train_iter_loss: 0.31137895584106445
train_iter_loss: 0.17096492648124695
train_iter_loss: 0.1389894187450409
train_iter_loss: 0.14719393849372864
train_iter_loss: 0.09493046253919601
train_iter_loss: 0.1408471018075943
train_iter_loss: 0.17680485546588898
train_iter_loss: 0.05934698134660721
train_iter_loss: 0.16541734337806702
train_iter_loss: 0.2071717381477356
train_iter_loss: 0.1134541779756546
train_iter_loss: 0.21697507798671722
train_iter_loss: 0.15531684458255768
train_iter_loss: 0.030392082408070564
train_iter_loss: 0.12007713317871094
train_iter_loss: 0.16661331057548523
train_iter_loss: 0.23832854628562927
train_iter_loss: 0.15087465941905975
train_iter_loss: 0.24268615245819092
train_iter_loss: 0.28056302666664124
train_iter_loss: 0.393135666847229
train_iter_loss: 0.12513533234596252
train_iter_loss: 0.2840120792388916
train_iter_loss: 0.10415690392255783
train_iter_loss: 0.13845041394233704
train_iter_loss: 0.07944412529468536
train_iter_loss: 0.11643478274345398
train_iter_loss: 0.20633038878440857
train_iter_loss: 0.19253183901309967
train_iter_loss: 0.2059062272310257
train_iter_loss: 0.2896527945995331
train_iter_loss: 0.2605256736278534
train_iter_loss: 0.15246380865573883
train_iter_loss: 0.1650945544242859
train_iter_loss: 0.08446003496646881
train_iter_loss: 0.20225153863430023
train_iter_loss: 0.18082831799983978
train_iter_loss: 0.10865506529808044
train_iter_loss: 0.11932957172393799
train_iter_loss: 0.2638334035873413
train_iter_loss: 0.20335653424263
train_iter_loss: 0.26762527227401733
train_iter_loss: 0.20905542373657227
train_iter_loss: 0.17469005286693573
train_iter_loss: 0.24259132146835327
train_iter_loss: 0.25534188747406006
train_iter_loss: 0.14235636591911316
train_iter_loss: 0.1886644810438156
train loss :0.1656
---------------------
Validation seg loss: 0.220763365333935 at epoch 425
epoch =    426/  1000, exp = train
train_iter_loss: 0.08748428523540497
train_iter_loss: 0.17881543934345245
train_iter_loss: 0.1352294683456421
train_iter_loss: 0.14572834968566895
train_iter_loss: 0.20958629250526428
train_iter_loss: 0.17545980215072632
train_iter_loss: 0.1174893006682396
train_iter_loss: 0.22963841259479523
train_iter_loss: 0.15771380066871643
train_iter_loss: 0.10310923308134079
train_iter_loss: 0.12930123507976532
train_iter_loss: 0.10395962744951248
train_iter_loss: 0.11522838473320007
train_iter_loss: 0.10725583881139755
train_iter_loss: 0.19865299761295319
train_iter_loss: 0.12677361071109772
train_iter_loss: 0.19255685806274414
train_iter_loss: 0.22843480110168457
train_iter_loss: 0.15426747500896454
train_iter_loss: 0.24857261776924133
train_iter_loss: 0.177829772233963
train_iter_loss: 0.2805081903934479
train_iter_loss: 0.2659297287464142
train_iter_loss: 0.22134652733802795
train_iter_loss: 0.1703159511089325
train_iter_loss: 0.31497839093208313
train_iter_loss: 0.13669773936271667
train_iter_loss: 0.09561377018690109
train_iter_loss: 0.1686834841966629
train_iter_loss: 0.17078512907028198
train_iter_loss: 0.10294339060783386
train_iter_loss: 0.15933972597122192
train_iter_loss: 0.09177917242050171
train_iter_loss: 0.20046912133693695
train_iter_loss: 0.09960650652647018
train_iter_loss: 0.20268262922763824
train_iter_loss: 0.31501564383506775
train_iter_loss: 0.10721179097890854
train_iter_loss: 0.10562380403280258
train_iter_loss: 0.3084552586078644
train_iter_loss: 0.1426752656698227
train_iter_loss: 0.2026480734348297
train_iter_loss: 0.08584089577198029
train_iter_loss: 0.12690506875514984
train_iter_loss: 0.13007353246212006
train_iter_loss: 0.16468894481658936
train_iter_loss: 0.1389564722776413
train_iter_loss: 0.15218566358089447
train_iter_loss: 0.11278659105300903
train_iter_loss: 0.19218623638153076
train_iter_loss: 0.12858575582504272
train_iter_loss: 0.11157256364822388
train_iter_loss: 0.11143942177295685
train_iter_loss: 0.3040662109851837
train_iter_loss: 0.17737455666065216
train_iter_loss: 0.21294522285461426
train_iter_loss: 0.15290406346321106
train_iter_loss: 0.22130867838859558
train_iter_loss: 0.09677068144083023
train_iter_loss: 0.1816796064376831
train_iter_loss: 0.2525182366371155
train_iter_loss: 0.11693958193063736
train_iter_loss: 0.061287760734558105
train_iter_loss: 0.1660715788602829
train_iter_loss: 0.17234891653060913
train_iter_loss: 0.08472464233636856
train_iter_loss: 0.10430152714252472
train_iter_loss: 0.21204355359077454
train_iter_loss: 0.1191435158252716
train_iter_loss: 0.10739907622337341
train_iter_loss: 0.1384419947862625
train_iter_loss: 0.30887433886528015
train_iter_loss: 0.2525673508644104
train_iter_loss: 0.10884461551904678
train_iter_loss: 0.10097450762987137
train_iter_loss: 0.22275686264038086
train_iter_loss: 0.1899973750114441
train_iter_loss: 0.16627256572246552
train_iter_loss: 0.098212331533432
train_iter_loss: 0.14500108361244202
train_iter_loss: 0.14856857061386108
train_iter_loss: 0.19904398918151855
train_iter_loss: 0.16361601650714874
train_iter_loss: 0.2468656748533249
train_iter_loss: 0.27086108922958374
train_iter_loss: 0.14219841361045837
train_iter_loss: 0.16682547330856323
train_iter_loss: 0.09981490671634674
train_iter_loss: 0.13536721467971802
train_iter_loss: 0.05904732644557953
train_iter_loss: 0.07806229591369629
train_iter_loss: 0.2680699825286865
train_iter_loss: 0.23047249019145966
train_iter_loss: 0.30313387513160706
train_iter_loss: 0.07677026838064194
train_iter_loss: 0.15222114324569702
train_iter_loss: 0.09478845447301865
train_iter_loss: 0.18193474411964417
train_iter_loss: 0.1011727973818779
train_iter_loss: 0.35322609543800354
train loss :0.1676
---------------------
Validation seg loss: 0.2240954959800221 at epoch 426
epoch =    427/  1000, exp = train
train_iter_loss: 0.17797018587589264
train_iter_loss: 0.20484043657779694
train_iter_loss: 0.19966857135295868
train_iter_loss: 0.17611542344093323
train_iter_loss: 0.1131502240896225
train_iter_loss: 0.1844491809606552
train_iter_loss: 0.09163139015436172
train_iter_loss: 0.17816537618637085
train_iter_loss: 0.13957011699676514
train_iter_loss: 0.10813253372907639
train_iter_loss: 0.30020955204963684
train_iter_loss: 0.1644933670759201
train_iter_loss: 0.26755133271217346
train_iter_loss: 0.1284702569246292
train_iter_loss: 0.09260712563991547
train_iter_loss: 0.23233042657375336
train_iter_loss: 0.1667301505804062
train_iter_loss: 0.09640435129404068
train_iter_loss: 0.21074546873569489
train_iter_loss: 0.21570420265197754
train_iter_loss: 0.25800853967666626
train_iter_loss: 0.19626934826374054
train_iter_loss: 0.17330466210842133
train_iter_loss: 0.1358920782804489
train_iter_loss: 0.10528893023729324
train_iter_loss: 0.18847352266311646
train_iter_loss: 0.13669636845588684
train_iter_loss: 0.20060312747955322
train_iter_loss: 0.16998739540576935
train_iter_loss: 0.12242041528224945
train_iter_loss: 0.19527097046375275
train_iter_loss: 0.1945524364709854
train_iter_loss: 0.10583019256591797
train_iter_loss: 0.2011406421661377
train_iter_loss: 0.09947257488965988
train_iter_loss: 0.07060346752405167
train_iter_loss: 0.13754647970199585
train_iter_loss: 0.08933967351913452
train_iter_loss: 0.12620225548744202
train_iter_loss: 0.1922619491815567
train_iter_loss: 0.1576099544763565
train_iter_loss: 0.15312567353248596
train_iter_loss: 0.1451859027147293
train_iter_loss: 0.11486830562353134
train_iter_loss: 0.16212256252765656
train_iter_loss: 0.18344208598136902
train_iter_loss: 0.1864810734987259
train_iter_loss: 0.1313341110944748
train_iter_loss: 0.24614520370960236
train_iter_loss: 0.12975668907165527
train_iter_loss: 0.20077835023403168
train_iter_loss: 0.09958860278129578
train_iter_loss: 0.12717953324317932
train_iter_loss: 0.06042037531733513
train_iter_loss: 0.17309698462486267
train_iter_loss: 0.2554505467414856
train_iter_loss: 0.11567381024360657
train_iter_loss: 0.2623835504055023
train_iter_loss: 0.1503583788871765
train_iter_loss: 0.14794647693634033
train_iter_loss: 0.14831051230430603
train_iter_loss: 0.1984204202890396
train_iter_loss: 0.26398971676826477
train_iter_loss: 0.0804763063788414
train_iter_loss: 0.09251426160335541
train_iter_loss: 0.16402730345726013
train_iter_loss: 0.12419439107179642
train_iter_loss: 0.2038896381855011
train_iter_loss: 0.1258842945098877
train_iter_loss: 0.19549670815467834
train_iter_loss: 0.5666647553443909
train_iter_loss: 0.09536144137382507
train_iter_loss: 0.06712550669908524
train_iter_loss: 0.19664672017097473
train_iter_loss: 0.10341954976320267
train_iter_loss: 0.14030177891254425
train_iter_loss: 0.2163180708885193
train_iter_loss: 0.0994800478219986
train_iter_loss: 0.09455645084381104
train_iter_loss: 0.21703527867794037
train_iter_loss: 0.12057679146528244
train_iter_loss: 0.11062438786029816
train_iter_loss: 0.3097333610057831
train_iter_loss: 0.11108872294425964
train_iter_loss: 0.13101564347743988
train_iter_loss: 0.33548447489738464
train_iter_loss: 0.13399147987365723
train_iter_loss: 0.0871451124548912
train_iter_loss: 0.11474491655826569
train_iter_loss: 0.3056327700614929
train_iter_loss: 0.11844391375780106
train_iter_loss: 0.11182153224945068
train_iter_loss: 0.10630269348621368
train_iter_loss: 0.20899833738803864
train_iter_loss: 0.14511308073997498
train_iter_loss: 0.17831100523471832
train_iter_loss: 0.19426700472831726
train_iter_loss: 0.105325847864151
train_iter_loss: 0.19695085287094116
train_iter_loss: 0.12931816279888153
train loss :0.1657
---------------------
Validation seg loss: 0.21848068881850197 at epoch 427
epoch =    428/  1000, exp = train
train_iter_loss: 0.219631627202034
train_iter_loss: 0.2008957862854004
train_iter_loss: 0.24142883718013763
train_iter_loss: 0.07344918698072433
train_iter_loss: 0.12530717253684998
train_iter_loss: 0.12154290080070496
train_iter_loss: 0.12185844033956528
train_iter_loss: 0.14068907499313354
train_iter_loss: 0.13738828897476196
train_iter_loss: 0.10168733447790146
train_iter_loss: 0.12191739678382874
train_iter_loss: 0.10570424795150757
train_iter_loss: 0.10846555978059769
train_iter_loss: 0.2778112292289734
train_iter_loss: 0.20340892672538757
train_iter_loss: 0.14613501727581024
train_iter_loss: 0.1675610989332199
train_iter_loss: 0.091646209359169
train_iter_loss: 0.3641025722026825
train_iter_loss: 0.13707588613033295
train_iter_loss: 0.18218953907489777
train_iter_loss: 0.1310410052537918
train_iter_loss: 0.15058797597885132
train_iter_loss: 0.12148992717266083
train_iter_loss: 0.12592065334320068
train_iter_loss: 0.2377888709306717
train_iter_loss: 0.31787770986557007
train_iter_loss: 0.16619184613227844
train_iter_loss: 0.08644130825996399
train_iter_loss: 0.30046573281288147
train_iter_loss: 0.29977279901504517
train_iter_loss: 0.19359397888183594
train_iter_loss: 0.20264044404029846
train_iter_loss: 0.13254234194755554
train_iter_loss: 0.1386920064687729
train_iter_loss: 0.10182491689920425
train_iter_loss: 0.11247757077217102
train_iter_loss: 0.10815112292766571
train_iter_loss: 0.12618038058280945
train_iter_loss: 0.16225899755954742
train_iter_loss: 0.2408214956521988
train_iter_loss: 0.22765521705150604
train_iter_loss: 0.22633513808250427
train_iter_loss: 0.14541922509670258
train_iter_loss: 0.08919135481119156
train_iter_loss: 0.15754711627960205
train_iter_loss: 0.05429549887776375
train_iter_loss: 0.18636693060398102
train_iter_loss: 0.13547497987747192
train_iter_loss: 0.07705077528953552
train_iter_loss: 0.1719934493303299
train_iter_loss: 0.13819025456905365
train_iter_loss: 0.06413472443819046
train_iter_loss: 0.13562707602977753
train_iter_loss: 0.1689186990261078
train_iter_loss: 0.18694117665290833
train_iter_loss: 0.19202792644500732
train_iter_loss: 0.12141606211662292
train_iter_loss: 0.21410612761974335
train_iter_loss: 0.12667495012283325
train_iter_loss: 0.23323945701122284
train_iter_loss: 0.1195698082447052
train_iter_loss: 0.17509034276008606
train_iter_loss: 0.27379119396209717
train_iter_loss: 0.10690808296203613
train_iter_loss: 0.160072922706604
train_iter_loss: 0.1798485815525055
train_iter_loss: 0.1379697024822235
train_iter_loss: 0.14165449142456055
train_iter_loss: 0.16813869774341583
train_iter_loss: 0.10829188674688339
train_iter_loss: 0.15066125988960266
train_iter_loss: 0.1247163638472557
train_iter_loss: 0.18882125616073608
train_iter_loss: 0.22747741639614105
train_iter_loss: 0.18721707165241241
train_iter_loss: 0.1297827959060669
train_iter_loss: 0.25450968742370605
train_iter_loss: 0.1918066442012787
train_iter_loss: 0.12274651229381561
train_iter_loss: 0.31644126772880554
train_iter_loss: 0.12381365895271301
train_iter_loss: 0.0987580269575119
train_iter_loss: 0.10563483834266663
train_iter_loss: 0.18889686465263367
train_iter_loss: 0.1424340307712555
train_iter_loss: 0.19068758189678192
train_iter_loss: 0.1791163980960846
train_iter_loss: 0.1563316434621811
train_iter_loss: 0.1393483579158783
train_iter_loss: 0.13744233548641205
train_iter_loss: 0.10612336546182632
train_iter_loss: 0.0934169739484787
train_iter_loss: 0.20730169117450714
train_iter_loss: 0.131748765707016
train_iter_loss: 0.09213492274284363
train_iter_loss: 0.1473737508058548
train_iter_loss: 0.16333812475204468
train_iter_loss: 0.15848980844020844
train_iter_loss: 0.20579591393470764
train loss :0.1630
---------------------
Validation seg loss: 0.21690823696553707 at epoch 428
epoch =    429/  1000, exp = train
train_iter_loss: 0.2534932494163513
train_iter_loss: 0.20150133967399597
train_iter_loss: 0.23961302638053894
train_iter_loss: 0.2082638293504715
train_iter_loss: 0.2142985612154007
train_iter_loss: 0.2060106247663498
train_iter_loss: 0.14276756346225739
train_iter_loss: 0.20144006609916687
train_iter_loss: 0.07746655493974686
train_iter_loss: 0.36661720275878906
train_iter_loss: 0.10505016893148422
train_iter_loss: 0.22521619498729706
train_iter_loss: 0.18844464421272278
train_iter_loss: 0.26137277483940125
train_iter_loss: 0.12637531757354736
train_iter_loss: 0.14655762910842896
train_iter_loss: 0.1541481465101242
train_iter_loss: 0.09683074802160263
train_iter_loss: 0.20524951815605164
train_iter_loss: 0.1730012744665146
train_iter_loss: 0.19892697036266327
train_iter_loss: 0.10584387928247452
train_iter_loss: 0.15047210454940796
train_iter_loss: 0.13495996594429016
train_iter_loss: 0.18229947984218597
train_iter_loss: 0.1304929405450821
train_iter_loss: 0.2087288796901703
train_iter_loss: 0.17761224508285522
train_iter_loss: 0.13858474791049957
train_iter_loss: 0.16833779215812683
train_iter_loss: 0.14369657635688782
train_iter_loss: 0.11424577981233597
train_iter_loss: 0.09780395030975342
train_iter_loss: 0.0925377756357193
train_iter_loss: 0.11533556133508682
train_iter_loss: 0.28906288743019104
train_iter_loss: 0.047286853194236755
train_iter_loss: 0.11408626288175583
train_iter_loss: 0.14681841433048248
train_iter_loss: 0.13598860800266266
train_iter_loss: 0.11788880079984665
train_iter_loss: 0.12483999878168106
train_iter_loss: 0.12420803308486938
train_iter_loss: 0.08787838369607925
train_iter_loss: 0.211313858628273
train_iter_loss: 0.2478266805410385
train_iter_loss: 0.17523665726184845
train_iter_loss: 0.1567220389842987
train_iter_loss: 0.14941821992397308
train_iter_loss: 0.1707417070865631
train_iter_loss: 0.18303532898426056
train_iter_loss: 0.14451929926872253
train_iter_loss: 0.07909829914569855
train_iter_loss: 0.1320151686668396
train_iter_loss: 0.216077521443367
train_iter_loss: 0.17318949103355408
train_iter_loss: 0.202720507979393
train_iter_loss: 0.11033827811479568
train_iter_loss: 0.24054576456546783
train_iter_loss: 0.13571131229400635
train_iter_loss: 0.23261304199695587
train_iter_loss: 0.13983994722366333
train_iter_loss: 0.09652907401323318
train_iter_loss: 0.10499833524227142
train_iter_loss: 0.1601540893316269
train_iter_loss: 0.27632617950439453
train_iter_loss: 0.11085840314626694
train_iter_loss: 0.19553013145923615
train_iter_loss: 0.13957206904888153
train_iter_loss: 0.13356831669807434
train_iter_loss: 0.1737600415945053
train_iter_loss: 0.1043022945523262
train_iter_loss: 0.13804182410240173
train_iter_loss: 0.09241490811109543
train_iter_loss: 0.1665956825017929
train_iter_loss: 0.1411135494709015
train_iter_loss: 0.06607887893915176
train_iter_loss: 0.19415025413036346
train_iter_loss: 0.1819881945848465
train_iter_loss: 0.1563892364501953
train_iter_loss: 0.10204873979091644
train_iter_loss: 0.13751952350139618
train_iter_loss: 0.1050185039639473
train_iter_loss: 0.16403529047966003
train_iter_loss: 0.1258651316165924
train_iter_loss: 0.249502494931221
train_iter_loss: 0.10570332407951355
train_iter_loss: 0.14550064504146576
train_iter_loss: 0.12591411173343658
train_iter_loss: 0.13784551620483398
train_iter_loss: 0.17138154804706573
train_iter_loss: 0.11699674278497696
train_iter_loss: 0.09258929640054703
train_iter_loss: 0.18198588490486145
train_iter_loss: 0.11261201649904251
train_iter_loss: 0.09765292704105377
train_iter_loss: 0.1152496263384819
train_iter_loss: 0.20016062259674072
train_iter_loss: 0.19953611493110657
train_iter_loss: 0.12196088582277298
train loss :0.1585
---------------------
Validation seg loss: 0.22139637894436437 at epoch 429
epoch =    430/  1000, exp = train
train_iter_loss: 0.12495575845241547
train_iter_loss: 0.19250325858592987
train_iter_loss: 0.12554918229579926
train_iter_loss: 0.1560981720685959
train_iter_loss: 0.12952429056167603
train_iter_loss: 0.12573207914829254
train_iter_loss: 0.39323660731315613
train_iter_loss: 0.07142652571201324
train_iter_loss: 0.17328932881355286
train_iter_loss: 0.16132250428199768
train_iter_loss: 0.136811763048172
train_iter_loss: 0.07586408406496048
train_iter_loss: 0.12457501888275146
train_iter_loss: 0.158236563205719
train_iter_loss: 0.12716740369796753
train_iter_loss: 0.2418372482061386
train_iter_loss: 0.2063424289226532
train_iter_loss: 0.06314108520746231
train_iter_loss: 0.11145767569541931
train_iter_loss: 0.15624013543128967
train_iter_loss: 0.13356420397758484
train_iter_loss: 0.2457856982946396
train_iter_loss: 0.07944847643375397
train_iter_loss: 0.06399957090616226
train_iter_loss: 0.21051400899887085
train_iter_loss: 0.22724442183971405
train_iter_loss: 0.1694861352443695
train_iter_loss: 0.24811410903930664
train_iter_loss: 0.23425889015197754
train_iter_loss: 0.17956744134426117
train_iter_loss: 0.1588243544101715
train_iter_loss: 0.1307242214679718
train_iter_loss: 0.24978554248809814
train_iter_loss: 0.29650115966796875
train_iter_loss: 0.0791182816028595
train_iter_loss: 0.28883329033851624
train_iter_loss: 0.11250690370798111
train_iter_loss: 0.1117151752114296
train_iter_loss: 0.22330960631370544
train_iter_loss: 0.1610259860754013
train_iter_loss: 0.11030645668506622
train_iter_loss: 0.1271989345550537
train_iter_loss: 0.09773334115743637
train_iter_loss: 0.20878618955612183
train_iter_loss: 0.3204801380634308
train_iter_loss: 0.24926890432834625
train_iter_loss: 0.10100801289081573
train_iter_loss: 0.1441756933927536
train_iter_loss: 0.12084555625915527
train_iter_loss: 0.05404729023575783
train_iter_loss: 0.08816340565681458
train_iter_loss: 0.24241942167282104
train_iter_loss: 0.14777496457099915
train_iter_loss: 0.0560932382941246
train_iter_loss: 0.16999216377735138
train_iter_loss: 0.1608697474002838
train_iter_loss: 0.11350405216217041
train_iter_loss: 0.24256178736686707
train_iter_loss: 0.14397168159484863
train_iter_loss: 0.1659037172794342
train_iter_loss: 0.12156389653682709
train_iter_loss: 0.14617854356765747
train_iter_loss: 0.15564613044261932
train_iter_loss: 0.17451541125774384
train_iter_loss: 0.10392792522907257
train_iter_loss: 0.335409939289093
train_iter_loss: 0.18281850218772888
train_iter_loss: 0.16175761818885803
train_iter_loss: 0.14234305918216705
train_iter_loss: 0.21479949355125427
train_iter_loss: 0.1111128181219101
train_iter_loss: 0.10875043272972107
train_iter_loss: 0.07316425442695618
train_iter_loss: 0.12737669050693512
train_iter_loss: 0.1963733285665512
train_iter_loss: 0.13813523948192596
train_iter_loss: 0.2515206038951874
train_iter_loss: 0.15710222721099854
train_iter_loss: 0.17496386170387268
train_iter_loss: 0.04735425114631653
train_iter_loss: 0.11244507133960724
train_iter_loss: 0.1840033233165741
train_iter_loss: 0.14708276093006134
train_iter_loss: 0.13700741529464722
train_iter_loss: 0.1492493897676468
train_iter_loss: 0.19909989833831787
train_iter_loss: 0.1272062063217163
train_iter_loss: 0.12578745186328888
train_iter_loss: 0.10827161371707916
train_iter_loss: 0.21175643801689148
train_iter_loss: 0.11446303874254227
train_iter_loss: 0.2178884595632553
train_iter_loss: 0.24869486689567566
train_iter_loss: 0.31194576621055603
train_iter_loss: 0.09825047105550766
train_iter_loss: 0.11866682022809982
train_iter_loss: 0.052898991852998734
train_iter_loss: 0.1563480794429779
train_iter_loss: 0.08966291695833206
train_iter_loss: 0.23909763991832733
train loss :0.1616
---------------------
Validation seg loss: 0.2247136489417896 at epoch 430
epoch =    431/  1000, exp = train
train_iter_loss: 0.187313050031662
train_iter_loss: 0.09315916150808334
train_iter_loss: 0.16990160942077637
train_iter_loss: 0.16774654388427734
train_iter_loss: 0.1207616925239563
train_iter_loss: 0.13120748102664948
train_iter_loss: 0.2154679149389267
train_iter_loss: 0.22505521774291992
train_iter_loss: 0.21379519999027252
train_iter_loss: 0.1617199182510376
train_iter_loss: 0.1550557017326355
train_iter_loss: 0.14362822473049164
train_iter_loss: 0.17478425800800323
train_iter_loss: 0.14340679347515106
train_iter_loss: 0.06687553226947784
train_iter_loss: 0.24701449275016785
train_iter_loss: 0.16519984602928162
train_iter_loss: 0.10985712707042694
train_iter_loss: 0.16560335457324982
train_iter_loss: 0.15372397005558014
train_iter_loss: 0.1525663435459137
train_iter_loss: 0.08618061989545822
train_iter_loss: 0.252006471157074
train_iter_loss: 0.08945707231760025
train_iter_loss: 0.16703395545482635
train_iter_loss: 0.1743072122335434
train_iter_loss: 0.21311433613300323
train_iter_loss: 0.12175262719392776
train_iter_loss: 0.17257332801818848
train_iter_loss: 0.1455763876438141
train_iter_loss: 0.28739625215530396
train_iter_loss: 0.15555088222026825
train_iter_loss: 0.1864076852798462
train_iter_loss: 0.14549259841442108
train_iter_loss: 0.15954706072807312
train_iter_loss: 0.13136717677116394
train_iter_loss: 0.24915443360805511
train_iter_loss: 0.08468551188707352
train_iter_loss: 0.08854818344116211
train_iter_loss: 0.17761468887329102
train_iter_loss: 0.16684041917324066
train_iter_loss: 0.11130894720554352
train_iter_loss: 0.10736110806465149
train_iter_loss: 0.23795881867408752
train_iter_loss: 0.16871130466461182
train_iter_loss: 0.17188702523708344
train_iter_loss: 0.12388733774423599
train_iter_loss: 0.11995154619216919
train_iter_loss: 0.07558497786521912
train_iter_loss: 0.10119283199310303
train_iter_loss: 0.15561726689338684
train_iter_loss: 0.13613446056842804
train_iter_loss: 0.139833465218544
train_iter_loss: 0.19428420066833496
train_iter_loss: 0.15522301197052002
train_iter_loss: 0.18180802464485168
train_iter_loss: 0.22090373933315277
train_iter_loss: 0.2320859432220459
train_iter_loss: 0.17319762706756592
train_iter_loss: 0.13691458106040955
train_iter_loss: 0.16933438181877136
train_iter_loss: 0.20941036939620972
train_iter_loss: 0.12004278600215912
train_iter_loss: 0.1424073874950409
train_iter_loss: 0.21380990743637085
train_iter_loss: 0.1764877587556839
train_iter_loss: 0.09658657014369965
train_iter_loss: 0.19776149094104767
train_iter_loss: 0.11146287620067596
train_iter_loss: 0.0860205739736557
train_iter_loss: 0.09297563135623932
train_iter_loss: 0.30356091260910034
train_iter_loss: 0.1887463480234146
train_iter_loss: 0.23737774789333344
train_iter_loss: 0.14870239794254303
train_iter_loss: 0.1346883922815323
train_iter_loss: 0.18242402374744415
train_iter_loss: 0.2701808214187622
train_iter_loss: 0.20819929242134094
train_iter_loss: 0.07363490760326385
train_iter_loss: 0.13754186034202576
train_iter_loss: 0.2657454013824463
train_iter_loss: 0.27516409754753113
train_iter_loss: 0.23433123528957367
train_iter_loss: 0.1260949671268463
train_iter_loss: 0.2186678797006607
train_iter_loss: 0.2465618997812271
train_iter_loss: 0.28341570496559143
train_iter_loss: 0.11287874728441238
train_iter_loss: 0.11053736507892609
train_iter_loss: 0.1694343090057373
train_iter_loss: 0.10110228508710861
train_iter_loss: 0.09239618480205536
train_iter_loss: 0.22033077478408813
train_iter_loss: 0.24537484347820282
train_iter_loss: 0.15236258506774902
train_iter_loss: 0.09096822887659073
train_iter_loss: 0.2489660382270813
train_iter_loss: 0.3325245976448059
train_iter_loss: 0.21767744421958923
train loss :0.1697
---------------------
Validation seg loss: 0.2258294564759675 at epoch 431
epoch =    432/  1000, exp = train
train_iter_loss: 0.08194314688444138
train_iter_loss: 0.07521218061447144
train_iter_loss: 0.13736535608768463
train_iter_loss: 0.20680144429206848
train_iter_loss: 0.19069364666938782
train_iter_loss: 0.18223096430301666
train_iter_loss: 0.2897604703903198
train_iter_loss: 0.1446179747581482
train_iter_loss: 0.10553910583257675
train_iter_loss: 0.17762799561023712
train_iter_loss: 0.15999247133731842
train_iter_loss: 0.13209731876850128
train_iter_loss: 0.15106789767742157
train_iter_loss: 0.1827632486820221
train_iter_loss: 0.15973860025405884
train_iter_loss: 0.07739125192165375
train_iter_loss: 0.16221177577972412
train_iter_loss: 0.21636873483657837
train_iter_loss: 0.08894041925668716
train_iter_loss: 0.048948876559734344
train_iter_loss: 0.12763045728206635
train_iter_loss: 0.06445270776748657
train_iter_loss: 0.17992429435253143
train_iter_loss: 0.21792660653591156
train_iter_loss: 0.31092971563339233
train_iter_loss: 0.19798362255096436
train_iter_loss: 0.15668216347694397
train_iter_loss: 0.16656054556369781
train_iter_loss: 0.14932194352149963
train_iter_loss: 0.14377543330192566
train_iter_loss: 0.13112512230873108
train_iter_loss: 0.2370733767747879
train_iter_loss: 0.14316979050636292
train_iter_loss: 0.13730889558792114
train_iter_loss: 0.209507554769516
train_iter_loss: 0.15882383286952972
train_iter_loss: 0.10257037729024887
train_iter_loss: 0.14335080981254578
train_iter_loss: 0.21275319159030914
train_iter_loss: 0.2561749815940857
train_iter_loss: 0.15047501027584076
train_iter_loss: 0.144249826669693
train_iter_loss: 0.14201435446739197
train_iter_loss: 0.13407211005687714
train_iter_loss: 0.14407047629356384
train_iter_loss: 0.14370672404766083
train_iter_loss: 0.10964196175336838
train_iter_loss: 0.297637015581131
train_iter_loss: 0.1175154373049736
train_iter_loss: 0.19610680639743805
train_iter_loss: 0.24273812770843506
train_iter_loss: 0.17001526057720184
train_iter_loss: 0.1797471046447754
train_iter_loss: 0.14936847984790802
train_iter_loss: 0.144750714302063
train_iter_loss: 0.21867430210113525
train_iter_loss: 0.19588682055473328
train_iter_loss: 0.13753412663936615
train_iter_loss: 0.1579209268093109
train_iter_loss: 0.10682457685470581
train_iter_loss: 0.10708975046873093
train_iter_loss: 0.17336899042129517
train_iter_loss: 0.2511965334415436
train_iter_loss: 0.16381622850894928
train_iter_loss: 0.07546216249465942
train_iter_loss: 0.21498262882232666
train_iter_loss: 0.16142626106739044
train_iter_loss: 0.20576056838035583
train_iter_loss: 0.1534266173839569
train_iter_loss: 0.1584511399269104
train_iter_loss: 0.15237215161323547
train_iter_loss: 0.21328061819076538
train_iter_loss: 0.16031239926815033
train_iter_loss: 0.05701335147023201
train_iter_loss: 0.10110408812761307
train_iter_loss: 0.24872633814811707
train_iter_loss: 0.06322507560253143
train_iter_loss: 0.23560616374015808
train_iter_loss: 0.10725810378789902
train_iter_loss: 0.17186029255390167
train_iter_loss: 0.21148112416267395
train_iter_loss: 0.2013239711523056
train_iter_loss: 0.1518857777118683
train_iter_loss: 0.07567894458770752
train_iter_loss: 0.1018279641866684
train_iter_loss: 0.18906734883785248
train_iter_loss: 0.206128790974617
train_iter_loss: 0.2845344543457031
train_iter_loss: 0.13878409564495087
train_iter_loss: 0.19497287273406982
train_iter_loss: 0.10163605958223343
train_iter_loss: 0.1790764480829239
train_iter_loss: 0.12078312784433365
train_iter_loss: 0.13802717626094818
train_iter_loss: 0.279134064912796
train_iter_loss: 0.11996151506900787
train_iter_loss: 0.2054959237575531
train_iter_loss: 0.13142243027687073
train_iter_loss: 0.14335118234157562
train_iter_loss: 0.1458965241909027
train loss :0.1637
---------------------
Validation seg loss: 0.2202401771473716 at epoch 432
epoch =    433/  1000, exp = train
train_iter_loss: 0.2354106307029724
train_iter_loss: 0.15271338820457458
train_iter_loss: 0.21368469297885895
train_iter_loss: 0.11552195996046066
train_iter_loss: 0.1298108994960785
train_iter_loss: 0.1125769093632698
train_iter_loss: 0.10228385776281357
train_iter_loss: 0.1514778882265091
train_iter_loss: 0.09720947593450546
train_iter_loss: 0.08684668689966202
train_iter_loss: 0.19092442095279694
train_iter_loss: 0.24040326476097107
train_iter_loss: 0.09019705653190613
train_iter_loss: 0.11891679465770721
train_iter_loss: 0.23599773645401
train_iter_loss: 0.1407790631055832
train_iter_loss: 0.2297331988811493
train_iter_loss: 0.16652148962020874
train_iter_loss: 0.10109914839267731
train_iter_loss: 0.13691598176956177
train_iter_loss: 0.1719805896282196
train_iter_loss: 0.23615474998950958
train_iter_loss: 0.24411766231060028
train_iter_loss: 0.2427676022052765
train_iter_loss: 0.17837564647197723
train_iter_loss: 0.1991821676492691
train_iter_loss: 0.14416705071926117
train_iter_loss: 0.17092540860176086
train_iter_loss: 0.19765706360340118
train_iter_loss: 0.19451764225959778
train_iter_loss: 0.0768011286854744
train_iter_loss: 0.26911672949790955
train_iter_loss: 0.18823684751987457
train_iter_loss: 0.2547900974750519
train_iter_loss: 0.12812857329845428
train_iter_loss: 0.142215758562088
train_iter_loss: 0.08022774010896683
train_iter_loss: 0.1416732221841812
train_iter_loss: 0.16392149031162262
train_iter_loss: 0.11987567692995071
train_iter_loss: 0.07993435114622116
train_iter_loss: 0.12032105773687363
train_iter_loss: 0.28584426641464233
train_iter_loss: 0.06835412979125977
train_iter_loss: 0.18029941618442535
train_iter_loss: 0.08121190965175629
train_iter_loss: 0.22077524662017822
train_iter_loss: 0.09969127923250198
train_iter_loss: 0.18876464664936066
train_iter_loss: 0.18160180747509003
train_iter_loss: 0.08903133124113083
train_iter_loss: 0.2200603187084198
train_iter_loss: 0.24499927461147308
train_iter_loss: 0.1349102109670639
train_iter_loss: 0.15438497066497803
train_iter_loss: 0.25556740164756775
train_iter_loss: 0.09373386204242706
train_iter_loss: 0.0757519081234932
train_iter_loss: 0.17059434950351715
train_iter_loss: 0.1656949371099472
train_iter_loss: 0.1674666553735733
train_iter_loss: 0.41097351908683777
train_iter_loss: 0.23952071368694305
train_iter_loss: 0.09193962812423706
train_iter_loss: 0.07349853962659836
train_iter_loss: 0.18674863874912262
train_iter_loss: 0.10377497225999832
train_iter_loss: 0.12356020510196686
train_iter_loss: 0.11608239263296127
train_iter_loss: 0.10063386708498001
train_iter_loss: 0.1965564340353012
train_iter_loss: 0.09241216629743576
train_iter_loss: 0.18861311674118042
train_iter_loss: 0.14600782096385956
train_iter_loss: 0.10531206429004669
train_iter_loss: 0.13030138611793518
train_iter_loss: 0.1741405874490738
train_iter_loss: 0.16432520747184753
train_iter_loss: 0.145309180021286
train_iter_loss: 0.1601727157831192
train_iter_loss: 0.28216588497161865
train_iter_loss: 0.11648210138082504
train_iter_loss: 0.2455424666404724
train_iter_loss: 0.20226258039474487
train_iter_loss: 0.18701520562171936
train_iter_loss: 0.18607789278030396
train_iter_loss: 0.17806211113929749
train_iter_loss: 0.21196900308132172
train_iter_loss: 0.15697826445102692
train_iter_loss: 0.12203184515237808
train_iter_loss: 0.4454324543476105
train_iter_loss: 0.10018973797559738
train_iter_loss: 0.12676921486854553
train_iter_loss: 0.19338861107826233
train_iter_loss: 0.10794104635715485
train_iter_loss: 0.10790209472179413
train_iter_loss: 0.2062777876853943
train_iter_loss: 0.11115407943725586
train_iter_loss: 0.21507839858531952
train_iter_loss: 0.20449666678905487
train loss :0.1669
---------------------
Validation seg loss: 0.22095375233944856 at epoch 433
epoch =    434/  1000, exp = train
train_iter_loss: 0.12997660040855408
train_iter_loss: 0.06691642850637436
train_iter_loss: 0.1569192260503769
train_iter_loss: 0.11327257007360458
train_iter_loss: 0.11894403398036957
train_iter_loss: 0.04682892933487892
train_iter_loss: 0.13871437311172485
train_iter_loss: 0.1475803554058075
train_iter_loss: 0.13138045370578766
train_iter_loss: 0.22006040811538696
train_iter_loss: 0.14058977365493774
train_iter_loss: 0.17331808805465698
train_iter_loss: 0.20419403910636902
train_iter_loss: 0.163651242852211
train_iter_loss: 0.14732736349105835
train_iter_loss: 0.18326562643051147
train_iter_loss: 0.12032320350408554
train_iter_loss: 0.19808797538280487
train_iter_loss: 0.1732562929391861
train_iter_loss: 0.14174829423427582
train_iter_loss: 0.12435740977525711
train_iter_loss: 0.14987191557884216
train_iter_loss: 0.13880299031734467
train_iter_loss: 0.20329023897647858
train_iter_loss: 0.2317061424255371
train_iter_loss: 0.16591167449951172
train_iter_loss: 0.15460489690303802
train_iter_loss: 0.30896833539009094
train_iter_loss: 0.14875082671642303
train_iter_loss: 0.09887337684631348
train_iter_loss: 0.2177564650774002
train_iter_loss: 0.11469673365354538
train_iter_loss: 0.12525935471057892
train_iter_loss: 0.09311904013156891
train_iter_loss: 0.0607120580971241
train_iter_loss: 0.22865663468837738
train_iter_loss: 0.1273626983165741
train_iter_loss: 0.13093435764312744
train_iter_loss: 0.16495846211910248
train_iter_loss: 0.10774526745080948
train_iter_loss: 0.1310717761516571
train_iter_loss: 0.1041937917470932
train_iter_loss: 0.2093728631734848
train_iter_loss: 0.201592355966568
train_iter_loss: 0.18320997059345245
train_iter_loss: 0.15188191831111908
train_iter_loss: 0.1635172963142395
train_iter_loss: 0.36991873383522034
train_iter_loss: 0.08849331736564636
train_iter_loss: 0.34024402499198914
train_iter_loss: 0.3273313641548157
train_iter_loss: 0.14858698844909668
train_iter_loss: 0.13472869992256165
train_iter_loss: 0.18521077930927277
train_iter_loss: 0.18005932867527008
train_iter_loss: 0.19491468369960785
train_iter_loss: 0.18366903066635132
train_iter_loss: 0.24395707249641418
train_iter_loss: 0.3173997700214386
train_iter_loss: 0.1326511651277542
train_iter_loss: 0.23407737910747528
train_iter_loss: 0.1749117523431778
train_iter_loss: 0.25608178973197937
train_iter_loss: 0.08542415499687195
train_iter_loss: 0.20382165908813477
train_iter_loss: 0.1570713222026825
train_iter_loss: 0.11229339987039566
train_iter_loss: 0.10746461153030396
train_iter_loss: 0.1575208157300949
train_iter_loss: 0.20708148181438446
train_iter_loss: 0.19307184219360352
train_iter_loss: 0.12490034103393555
train_iter_loss: 0.10207291692495346
train_iter_loss: 0.0828511193394661
train_iter_loss: 0.20618052780628204
train_iter_loss: 0.09615480154752731
train_iter_loss: 0.061285607516765594
train_iter_loss: 0.27658379077911377
train_iter_loss: 0.12067580968141556
train_iter_loss: 0.238554909825325
train_iter_loss: 0.12899303436279297
train_iter_loss: 0.10168541967868805
train_iter_loss: 0.21926787495613098
train_iter_loss: 0.06044720858335495
train_iter_loss: 0.11425600945949554
train_iter_loss: 0.1453522890806198
train_iter_loss: 0.1966579407453537
train_iter_loss: 0.07158973067998886
train_iter_loss: 0.1927085518836975
train_iter_loss: 0.16891279816627502
train_iter_loss: 0.20696675777435303
train_iter_loss: 0.15618090331554413
train_iter_loss: 0.14172016084194183
train_iter_loss: 0.2398342788219452
train_iter_loss: 0.17439702153205872
train_iter_loss: 0.13804315030574799
train_iter_loss: 0.04770341515541077
train_iter_loss: 0.3914715349674225
train_iter_loss: 0.19874456524848938
train_iter_loss: 0.12869493663311005
train loss :0.1659
---------------------
Validation seg loss: 0.22327567216114336 at epoch 434
epoch =    435/  1000, exp = train
train_iter_loss: 0.11727782338857651
train_iter_loss: 0.13655325770378113
train_iter_loss: 0.23942378163337708
train_iter_loss: 0.1735139638185501
train_iter_loss: 0.04938506707549095
train_iter_loss: 0.15450572967529297
train_iter_loss: 0.09700984507799149
train_iter_loss: 0.22437074780464172
train_iter_loss: 0.08589214086532593
train_iter_loss: 0.0890081599354744
train_iter_loss: 0.11402247846126556
train_iter_loss: 0.19405099749565125
train_iter_loss: 0.10444919019937515
train_iter_loss: 0.17162661254405975
train_iter_loss: 0.1071525514125824
train_iter_loss: 0.2066727727651596
train_iter_loss: 0.13150838017463684
train_iter_loss: 0.13473618030548096
train_iter_loss: 0.1312687247991562
train_iter_loss: 0.09255833923816681
train_iter_loss: 0.1336037963628769
train_iter_loss: 0.14857704937458038
train_iter_loss: 0.09451402723789215
train_iter_loss: 0.10258655995130539
train_iter_loss: 0.21362997591495514
train_iter_loss: 0.13054022192955017
train_iter_loss: 0.1876557469367981
train_iter_loss: 0.2405608743429184
train_iter_loss: 0.17239609360694885
train_iter_loss: 0.11507966369390488
train_iter_loss: 0.2024919092655182
train_iter_loss: 0.1324964016675949
train_iter_loss: 0.14662761986255646
train_iter_loss: 0.05878707021474838
train_iter_loss: 0.17128591239452362
train_iter_loss: 0.3175951838493347
train_iter_loss: 0.13569481670856476
train_iter_loss: 0.13954490423202515
train_iter_loss: 0.2701859176158905
train_iter_loss: 0.1996254026889801
train_iter_loss: 0.07658065110445023
train_iter_loss: 0.25094306468963623
train_iter_loss: 0.13729332387447357
train_iter_loss: 0.21735407412052155
train_iter_loss: 0.2050710916519165
train_iter_loss: 0.06366679817438126
train_iter_loss: 0.1333036869764328
train_iter_loss: 0.1990184187889099
train_iter_loss: 0.19553902745246887
train_iter_loss: 0.09843584895133972
train_iter_loss: 0.1324080228805542
train_iter_loss: 0.15998511016368866
train_iter_loss: 0.17851407825946808
train_iter_loss: 0.23676568269729614
train_iter_loss: 0.17242933809757233
train_iter_loss: 0.14718589186668396
train_iter_loss: 0.12424372136592865
train_iter_loss: 0.23272337019443512
train_iter_loss: 0.1977928727865219
train_iter_loss: 0.0997711718082428
train_iter_loss: 0.13942618668079376
train_iter_loss: 0.16656044125556946
train_iter_loss: 0.13147425651550293
train_iter_loss: 0.21779987215995789
train_iter_loss: 0.20010429620742798
train_iter_loss: 0.07855425029993057
train_iter_loss: 0.10451892018318176
train_iter_loss: 0.12184972316026688
train_iter_loss: 0.07865165174007416
train_iter_loss: 0.0847204402089119
train_iter_loss: 0.20091447234153748
train_iter_loss: 0.18787138164043427
train_iter_loss: 0.12342559546232224
train_iter_loss: 0.16894914209842682
train_iter_loss: 0.12618409097194672
train_iter_loss: 0.1497090607881546
train_iter_loss: 0.25136643648147583
train_iter_loss: 0.060799989849328995
train_iter_loss: 0.2112346887588501
train_iter_loss: 0.2585562765598297
train_iter_loss: 0.1472015082836151
train_iter_loss: 0.3910275101661682
train_iter_loss: 0.11363041400909424
train_iter_loss: 0.16850003600120544
train_iter_loss: 0.18391044437885284
train_iter_loss: 0.14495457708835602
train_iter_loss: 0.2155900001525879
train_iter_loss: 0.14373114705085754
train_iter_loss: 0.2036544382572174
train_iter_loss: 0.08205188810825348
train_iter_loss: 0.10429682582616806
train_iter_loss: 0.2358999103307724
train_iter_loss: 0.1879534125328064
train_iter_loss: 0.11820672452449799
train_iter_loss: 0.15790347754955292
train_iter_loss: 0.12411308288574219
train_iter_loss: 0.12008030712604523
train_iter_loss: 0.11916983872652054
train_iter_loss: 0.07187055051326752
train_iter_loss: 0.13720117509365082
train loss :0.1573
---------------------
Validation seg loss: 0.21995310999347634 at epoch 435
epoch =    436/  1000, exp = train
train_iter_loss: 0.16316001117229462
train_iter_loss: 0.15993966162204742
train_iter_loss: 0.33069247007369995
train_iter_loss: 0.18724144995212555
train_iter_loss: 0.13692058622837067
train_iter_loss: 0.22073043882846832
train_iter_loss: 0.1738836169242859
train_iter_loss: 0.1865759789943695
train_iter_loss: 0.11787253618240356
train_iter_loss: 0.2428245097398758
train_iter_loss: 0.11084159463644028
train_iter_loss: 0.13495022058486938
train_iter_loss: 0.12052170932292938
train_iter_loss: 0.11645113676786423
train_iter_loss: 0.15207785367965698
train_iter_loss: 0.19855189323425293
train_iter_loss: 0.09309303760528564
train_iter_loss: 0.2598137855529785
train_iter_loss: 0.12768492102622986
train_iter_loss: 0.27155014872550964
train_iter_loss: 0.22247272729873657
train_iter_loss: 0.20589083433151245
train_iter_loss: 0.14826585352420807
train_iter_loss: 0.22676123678684235
train_iter_loss: 0.13422532379627228
train_iter_loss: 0.19357594847679138
train_iter_loss: 0.2318478673696518
train_iter_loss: 0.12820535898208618
train_iter_loss: 0.16593751311302185
train_iter_loss: 0.22660376131534576
train_iter_loss: 0.08884870260953903
train_iter_loss: 0.20772859454154968
train_iter_loss: 0.14532577991485596
train_iter_loss: 0.1772560328245163
train_iter_loss: 0.1211743950843811
train_iter_loss: 0.15234914422035217
train_iter_loss: 0.0590992271900177
train_iter_loss: 0.197769433259964
train_iter_loss: 0.05292273312807083
train_iter_loss: 0.0910482257604599
train_iter_loss: 0.2513853907585144
train_iter_loss: 0.18359501659870148
train_iter_loss: 0.211589515209198
train_iter_loss: 0.1583850383758545
train_iter_loss: 0.1573106348514557
train_iter_loss: 0.13239607214927673
train_iter_loss: 0.22508646547794342
train_iter_loss: 0.10960499942302704
train_iter_loss: 0.16516701877117157
train_iter_loss: 0.17277675867080688
train_iter_loss: 0.2250581979751587
train_iter_loss: 0.14469298720359802
train_iter_loss: 0.22719569504261017
train_iter_loss: 0.18770162761211395
train_iter_loss: 0.19284647703170776
train_iter_loss: 0.12684623897075653
train_iter_loss: 0.11583453416824341
train_iter_loss: 0.2744464576244354
train_iter_loss: 0.2799402177333832
train_iter_loss: 0.16700951755046844
train_iter_loss: 0.1652155965566635
train_iter_loss: 0.13466279208660126
train_iter_loss: 0.07122925668954849
train_iter_loss: 0.053384341299533844
train_iter_loss: 0.1988784521818161
train_iter_loss: 0.05955331772565842
train_iter_loss: 0.14331665635108948
train_iter_loss: 0.2674233019351959
train_iter_loss: 0.15658266842365265
train_iter_loss: 0.12100091576576233
train_iter_loss: 0.09960289299488068
train_iter_loss: 0.16640207171440125
train_iter_loss: 0.13842876255512238
train_iter_loss: 0.1187305599451065
train_iter_loss: 0.22876131534576416
train_iter_loss: 0.17000947892665863
train_iter_loss: 0.1454887092113495
train_iter_loss: 0.19537873566150665
train_iter_loss: 0.09272759407758713
train_iter_loss: 0.09237739443778992
train_iter_loss: 0.16377989947795868
train_iter_loss: 0.12003783136606216
train_iter_loss: 0.13955874741077423
train_iter_loss: 0.13364115357398987
train_iter_loss: 0.06610307842493057
train_iter_loss: 0.0796271488070488
train_iter_loss: 0.16820643842220306
train_iter_loss: 0.2656044065952301
train_iter_loss: 0.17764224112033844
train_iter_loss: 0.26340028643608093
train_iter_loss: 0.0703413113951683
train_iter_loss: 0.19174185395240784
train_iter_loss: 0.19672733545303345
train_iter_loss: 0.18225806951522827
train_iter_loss: 0.22269336879253387
train_iter_loss: 0.19134332239627838
train_iter_loss: 0.09235828369855881
train_iter_loss: 0.262891560792923
train_iter_loss: 0.1745770126581192
train_iter_loss: 0.09970689564943314
train loss :0.1663
---------------------
Validation seg loss: 0.2240926930944453 at epoch 436
epoch =    437/  1000, exp = train
train_iter_loss: 0.3071804642677307
train_iter_loss: 0.16709047555923462
train_iter_loss: 0.16643400490283966
train_iter_loss: 0.274847149848938
train_iter_loss: 0.19322235882282257
train_iter_loss: 0.18478333950042725
train_iter_loss: 0.18864323198795319
train_iter_loss: 0.09442121535539627
train_iter_loss: 0.19587036967277527
train_iter_loss: 0.12134189158678055
train_iter_loss: 0.23138411343097687
train_iter_loss: 0.13439394533634186
train_iter_loss: 0.08921080827713013
train_iter_loss: 0.18479320406913757
train_iter_loss: 0.23409393429756165
train_iter_loss: 0.31052327156066895
train_iter_loss: 0.16687583923339844
train_iter_loss: 0.27096232771873474
train_iter_loss: 0.290218323469162
train_iter_loss: 0.18948623538017273
train_iter_loss: 0.1170516386628151
train_iter_loss: 0.17200540006160736
train_iter_loss: 0.19042031466960907
train_iter_loss: 0.25138333439826965
train_iter_loss: 0.18703287839889526
train_iter_loss: 0.18265433609485626
train_iter_loss: 0.13662613928318024
train_iter_loss: 0.26805242896080017
train_iter_loss: 0.06997989863157272
train_iter_loss: 0.18924736976623535
train_iter_loss: 0.21941158175468445
train_iter_loss: 0.1037508100271225
train_iter_loss: 0.13265827298164368
train_iter_loss: 0.04560016095638275
train_iter_loss: 0.21754378080368042
train_iter_loss: 0.13319994509220123
train_iter_loss: 0.14473487436771393
train_iter_loss: 0.14285708963871002
train_iter_loss: 0.13080456852912903
train_iter_loss: 0.08457722514867783
train_iter_loss: 0.24016305804252625
train_iter_loss: 0.16134072840213776
train_iter_loss: 0.1417013257741928
train_iter_loss: 0.10073722898960114
train_iter_loss: 0.20202375948429108
train_iter_loss: 0.26284733414649963
train_iter_loss: 0.07589086145162582
train_iter_loss: 0.05434658005833626
train_iter_loss: 0.29062697291374207
train_iter_loss: 0.10446514189243317
train_iter_loss: 0.13242876529693604
train_iter_loss: 0.24560773372650146
train_iter_loss: 0.08575979620218277
train_iter_loss: 0.13367968797683716
train_iter_loss: 0.24199745059013367
train_iter_loss: 0.14295414090156555
train_iter_loss: 0.17500127851963043
train_iter_loss: 0.09944130480289459
train_iter_loss: 0.18553265929222107
train_iter_loss: 0.16373665630817413
train_iter_loss: 0.17217718064785004
train_iter_loss: 0.2129559963941574
train_iter_loss: 0.08758632093667984
train_iter_loss: 0.13210342824459076
train_iter_loss: 0.12722088396549225
train_iter_loss: 0.1778186708688736
train_iter_loss: 0.1954003870487213
train_iter_loss: 0.13600778579711914
train_iter_loss: 0.11437776684761047
train_iter_loss: 0.22757525742053986
train_iter_loss: 0.21547560393810272
train_iter_loss: 0.2170432209968567
train_iter_loss: 0.12019553035497665
train_iter_loss: 0.17317183315753937
train_iter_loss: 0.132013201713562
train_iter_loss: 0.20913800597190857
train_iter_loss: 0.23834846913814545
train_iter_loss: 0.19259655475616455
train_iter_loss: 0.15914537012577057
train_iter_loss: 0.09613707661628723
train_iter_loss: 0.0838853195309639
train_iter_loss: 0.1313828080892563
train_iter_loss: 0.15249338746070862
train_iter_loss: 0.21922752261161804
train_iter_loss: 0.1691482663154602
train_iter_loss: 0.1711152195930481
train_iter_loss: 0.21289004385471344
train_iter_loss: 0.08744530379772186
train_iter_loss: 0.15245652198791504
train_iter_loss: 0.08332929015159607
train_iter_loss: 0.0741172805428505
train_iter_loss: 0.20085057616233826
train_iter_loss: 0.0912400409579277
train_iter_loss: 0.2159697264432907
train_iter_loss: 0.05229814350605011
train_iter_loss: 0.0941712036728859
train_iter_loss: 0.2160119116306305
train_iter_loss: 0.18805073201656342
train_iter_loss: 0.3834603726863861
train_iter_loss: 0.13859476149082184
train loss :0.1698
---------------------
Validation seg loss: 0.22005241390598831 at epoch 437
epoch =    438/  1000, exp = train
train_iter_loss: 0.09677533805370331
train_iter_loss: 0.08186418563127518
train_iter_loss: 0.16458559036254883
train_iter_loss: 0.2999168336391449
train_iter_loss: 0.13838377594947815
train_iter_loss: 0.20018033683300018
train_iter_loss: 0.16635213792324066
train_iter_loss: 0.1891123205423355
train_iter_loss: 0.10812275856733322
train_iter_loss: 0.12279698997735977
train_iter_loss: 0.13235323131084442
train_iter_loss: 0.12196890264749527
train_iter_loss: 0.23110361397266388
train_iter_loss: 0.17625156044960022
train_iter_loss: 0.20784445106983185
train_iter_loss: 0.37569135427474976
train_iter_loss: 0.08208868652582169
train_iter_loss: 0.1317746937274933
train_iter_loss: 0.14278703927993774
train_iter_loss: 0.11411792039871216
train_iter_loss: 0.1142779067158699
train_iter_loss: 0.19578103721141815
train_iter_loss: 0.2248307466506958
train_iter_loss: 0.1685560941696167
train_iter_loss: 0.1389247626066208
train_iter_loss: 0.19299572706222534
train_iter_loss: 0.17653726041316986
train_iter_loss: 0.09232773631811142
train_iter_loss: 0.11238793283700943
train_iter_loss: 0.1271819919347763
train_iter_loss: 0.15937618911266327
train_iter_loss: 0.2519889175891876
train_iter_loss: 0.13205862045288086
train_iter_loss: 0.19135180115699768
train_iter_loss: 0.12700295448303223
train_iter_loss: 0.16867771744728088
train_iter_loss: 0.12097328901290894
train_iter_loss: 0.1649426966905594
train_iter_loss: 0.07350265234708786
train_iter_loss: 0.05502160266041756
train_iter_loss: 0.17785611748695374
train_iter_loss: 0.09665881097316742
train_iter_loss: 0.20303408801555634
train_iter_loss: 0.3197227120399475
train_iter_loss: 0.4893898069858551
train_iter_loss: 0.18479058146476746
train_iter_loss: 0.08943896740674973
train_iter_loss: 0.21266770362854004
train_iter_loss: 0.10205409675836563
train_iter_loss: 0.13842982053756714
train_iter_loss: 0.2450878769159317
train_iter_loss: 0.1721765249967575
train_iter_loss: 0.0437425933778286
train_iter_loss: 0.330483078956604
train_iter_loss: 0.04224759712815285
train_iter_loss: 0.09206190705299377
train_iter_loss: 0.08080866187810898
train_iter_loss: 0.15259778499603271
train_iter_loss: 0.11905142664909363
train_iter_loss: 0.20351681113243103
train_iter_loss: 0.1369253396987915
train_iter_loss: 0.14185316860675812
train_iter_loss: 0.09904863685369492
train_iter_loss: 0.23365418612957
train_iter_loss: 0.35335707664489746
train_iter_loss: 0.21285748481750488
train_iter_loss: 0.14579088985919952
train_iter_loss: 0.22818459570407867
train_iter_loss: 0.2288985699415207
train_iter_loss: 0.18416202068328857
train_iter_loss: 0.3098430931568146
train_iter_loss: 0.13287484645843506
train_iter_loss: 0.19962911307811737
train_iter_loss: 0.19165785610675812
train_iter_loss: 0.12732121348381042
train_iter_loss: 0.12069860100746155
train_iter_loss: 0.10845136642456055
train_iter_loss: 0.15186679363250732
train_iter_loss: 0.16738227009773254
train_iter_loss: 0.08835792541503906
train_iter_loss: 0.3109520971775055
train_iter_loss: 0.12034763395786285
train_iter_loss: 0.1610506772994995
train_iter_loss: 0.12702590227127075
train_iter_loss: 0.24994443356990814
train_iter_loss: 0.2011222541332245
train_iter_loss: 0.20987765491008759
train_iter_loss: 0.18010550737380981
train_iter_loss: 0.06398820877075195
train_iter_loss: 0.18363994359970093
train_iter_loss: 0.22770929336547852
train_iter_loss: 0.16569127142429352
train_iter_loss: 0.10871677100658417
train_iter_loss: 0.23658326268196106
train_iter_loss: 0.14332731068134308
train_iter_loss: 0.12598109245300293
train_iter_loss: 0.1450432389974594
train_iter_loss: 0.0819680243730545
train_iter_loss: 0.3082795739173889
train_iter_loss: 0.08489309251308441
train loss :0.1694
---------------------
Validation seg loss: 0.21843147779516173 at epoch 438
epoch =    439/  1000, exp = train
train_iter_loss: 0.11949774622917175
train_iter_loss: 0.3389604091644287
train_iter_loss: 0.05080263689160347
train_iter_loss: 0.12632185220718384
train_iter_loss: 0.1514468640089035
train_iter_loss: 0.14563731849193573
train_iter_loss: 0.22849644720554352
train_iter_loss: 0.13870850205421448
train_iter_loss: 0.17169146239757538
train_iter_loss: 0.13282357156276703
train_iter_loss: 0.1815401017665863
train_iter_loss: 0.17049400508403778
train_iter_loss: 0.0983998030424118
train_iter_loss: 0.1524444967508316
train_iter_loss: 0.1385425329208374
train_iter_loss: 0.06504753977060318
train_iter_loss: 0.05996602028608322
train_iter_loss: 0.15839897096157074
train_iter_loss: 0.1899346113204956
train_iter_loss: 0.1764598786830902
train_iter_loss: 0.18644709885120392
train_iter_loss: 0.11844822764396667
train_iter_loss: 0.1832408756017685
train_iter_loss: 0.09525907039642334
train_iter_loss: 0.14843975007534027
train_iter_loss: 0.3452947735786438
train_iter_loss: 0.12115394324064255
train_iter_loss: 0.1504785716533661
train_iter_loss: 0.2599571943283081
train_iter_loss: 0.13345597684383392
train_iter_loss: 0.19322063028812408
train_iter_loss: 0.13268285989761353
train_iter_loss: 0.21953366696834564
train_iter_loss: 0.0929456427693367
train_iter_loss: 0.17658527195453644
train_iter_loss: 0.1696733981370926
train_iter_loss: 0.2239542454481125
train_iter_loss: 0.11615265160799026
train_iter_loss: 0.09933916479349136
train_iter_loss: 0.13073061406612396
train_iter_loss: 0.10210961103439331
train_iter_loss: 0.12420013546943665
train_iter_loss: 0.13437657058238983
train_iter_loss: 0.16697493195533752
train_iter_loss: 0.16906946897506714
train_iter_loss: 0.23048846423625946
train_iter_loss: 0.043154481798410416
train_iter_loss: 0.20168359577655792
train_iter_loss: 0.15112033486366272
train_iter_loss: 0.3406953811645508
train_iter_loss: 0.1265285462141037
train_iter_loss: 0.24365951120853424
train_iter_loss: 0.2882550060749054
train_iter_loss: 0.2025996446609497
train_iter_loss: 0.04056447744369507
train_iter_loss: 0.0855807363986969
train_iter_loss: 0.14169371128082275
train_iter_loss: 0.179711252450943
train_iter_loss: 0.15663090348243713
train_iter_loss: 0.10518362373113632
train_iter_loss: 0.2502734661102295
train_iter_loss: 0.2114529311656952
train_iter_loss: 0.18096879124641418
train_iter_loss: 0.13295699656009674
train_iter_loss: 0.23648786544799805
train_iter_loss: 0.09113503247499466
train_iter_loss: 0.12534092366695404
train_iter_loss: 0.17324198782444
train_iter_loss: 0.1193947121500969
train_iter_loss: 0.23356685042381287
train_iter_loss: 0.1195366233587265
train_iter_loss: 0.09415034204721451
train_iter_loss: 0.1711672842502594
train_iter_loss: 0.17331570386886597
train_iter_loss: 0.27708837389945984
train_iter_loss: 0.19499844312667847
train_iter_loss: 0.16573132574558258
train_iter_loss: 0.16092954576015472
train_iter_loss: 0.16899316012859344
train_iter_loss: 0.18503700196743011
train_iter_loss: 0.21668437123298645
train_iter_loss: 0.21921829879283905
train_iter_loss: 0.1699988692998886
train_iter_loss: 0.11064503341913223
train_iter_loss: 0.16317914426326752
train_iter_loss: 0.2696392238140106
train_iter_loss: 0.192336305975914
train_iter_loss: 0.17978860437870026
train_iter_loss: 0.17396411299705505
train_iter_loss: 0.332898885011673
train_iter_loss: 0.08425798267126083
train_iter_loss: 0.07336331903934479
train_iter_loss: 0.07932176440954208
train_iter_loss: 0.13038182258605957
train_iter_loss: 0.1280757635831833
train_iter_loss: 0.28172069787979126
train_iter_loss: 0.24500367045402527
train_iter_loss: 0.21225133538246155
train_iter_loss: 0.2385851889848709
train_iter_loss: 0.14080533385276794
train loss :0.1680
---------------------
Validation seg loss: 0.22138945325189885 at epoch 439
epoch =    440/  1000, exp = train
train_iter_loss: 0.21281035244464874
train_iter_loss: 0.12754559516906738
train_iter_loss: 0.18649576604366302
train_iter_loss: 0.26052460074424744
train_iter_loss: 0.17890523374080658
train_iter_loss: 0.1569070965051651
train_iter_loss: 0.17676974833011627
train_iter_loss: 0.1408228725194931
train_iter_loss: 0.17348137497901917
train_iter_loss: 0.07207125425338745
train_iter_loss: 0.12055131793022156
train_iter_loss: 0.1645916998386383
train_iter_loss: 0.16830545663833618
train_iter_loss: 0.1266295313835144
train_iter_loss: 0.24748675525188446
train_iter_loss: 0.17285118997097015
train_iter_loss: 0.18853692710399628
train_iter_loss: 0.16154156625270844
train_iter_loss: 0.25559425354003906
train_iter_loss: 0.17048929631710052
train_iter_loss: 0.289068341255188
train_iter_loss: 0.09476713091135025
train_iter_loss: 0.10380125045776367
train_iter_loss: 0.21471059322357178
train_iter_loss: 0.11640023440122604
train_iter_loss: 0.11746077984571457
train_iter_loss: 0.11956572532653809
train_iter_loss: 0.16555289924144745
train_iter_loss: 0.0789237916469574
train_iter_loss: 0.15169097483158112
train_iter_loss: 0.23313084244728088
train_iter_loss: 0.14871497452259064
train_iter_loss: 0.22465857863426208
train_iter_loss: 0.12349128723144531
train_iter_loss: 0.10602770000696182
train_iter_loss: 0.11281509697437286
train_iter_loss: 0.20870865881443024
train_iter_loss: 0.08058945089578629
train_iter_loss: 0.41536128520965576
train_iter_loss: 0.1339133083820343
train_iter_loss: 0.24528595805168152
train_iter_loss: 0.10923203080892563
train_iter_loss: 0.04879675433039665
train_iter_loss: 0.3212691843509674
train_iter_loss: 0.19180116057395935
train_iter_loss: 0.11030848324298859
train_iter_loss: 0.06771137565374374
train_iter_loss: 0.028102004900574684
train_iter_loss: 0.1443556398153305
train_iter_loss: 0.1553976833820343
train_iter_loss: 0.24846577644348145
train_iter_loss: 0.17154422402381897
train_iter_loss: 0.05354920029640198
train_iter_loss: 0.13493503630161285
train_iter_loss: 0.2127014398574829
train_iter_loss: 0.16721007227897644
train_iter_loss: 0.1442345529794693
train_iter_loss: 0.13820618391036987
train_iter_loss: 0.2788957953453064
train_iter_loss: 0.1113966852426529
train_iter_loss: 0.25589948892593384
train_iter_loss: 0.15122337639331818
train_iter_loss: 0.1584559977054596
train_iter_loss: 0.1278000921010971
train_iter_loss: 0.052286263555288315
train_iter_loss: 0.12730777263641357
train_iter_loss: 0.15561892092227936
train_iter_loss: 0.1803778111934662
train_iter_loss: 0.1294477880001068
train_iter_loss: 0.06821243464946747
train_iter_loss: 0.10786772519350052
train_iter_loss: 0.22506172955036163
train_iter_loss: 0.07496900856494904
train_iter_loss: 0.15381307899951935
train_iter_loss: 0.13431431353092194
train_iter_loss: 0.15433098375797272
train_iter_loss: 0.06075863540172577
train_iter_loss: 0.22432513535022736
train_iter_loss: 0.08535289019346237
train_iter_loss: 0.1406758427619934
train_iter_loss: 0.07381561398506165
train_iter_loss: 0.16369105875492096
train_iter_loss: 0.09981469810009003
train_iter_loss: 0.0908874049782753
train_iter_loss: 0.12166336178779602
train_iter_loss: 0.21776627004146576
train_iter_loss: 0.20841647684574127
train_iter_loss: 0.1459464281797409
train_iter_loss: 0.16661995649337769
train_iter_loss: 0.2662433087825775
train_iter_loss: 0.24067968130111694
train_iter_loss: 0.1382790505886078
train_iter_loss: 0.08336497843265533
train_iter_loss: 0.2223130166530609
train_iter_loss: 0.17861223220825195
train_iter_loss: 0.17210108041763306
train_iter_loss: 0.14043140411376953
train_iter_loss: 0.24269938468933105
train_iter_loss: 0.13326245546340942
train_iter_loss: 0.25931045413017273
train loss :0.1609
---------------------
Validation seg loss: 0.22392756948774717 at epoch 440
epoch =    441/  1000, exp = train
train_iter_loss: 0.15768013894557953
train_iter_loss: 0.25788816809654236
train_iter_loss: 0.1422240436077118
train_iter_loss: 0.16971856355667114
train_iter_loss: 0.16028860211372375
train_iter_loss: 0.21762987971305847
train_iter_loss: 0.18366247415542603
train_iter_loss: 0.15655730664730072
train_iter_loss: 0.10453300178050995
train_iter_loss: 0.09932402521371841
train_iter_loss: 0.0941193625330925
train_iter_loss: 0.05808650702238083
train_iter_loss: 0.15540875494480133
train_iter_loss: 0.3208177089691162
train_iter_loss: 0.1180153489112854
train_iter_loss: 0.07346907258033752
train_iter_loss: 0.15114276111125946
train_iter_loss: 0.1553778499364853
train_iter_loss: 0.1501166671514511
train_iter_loss: 0.05692952498793602
train_iter_loss: 0.2635766565799713
train_iter_loss: 0.1769159734249115
train_iter_loss: 0.1166883185505867
train_iter_loss: 0.20389217138290405
train_iter_loss: 0.0928618311882019
train_iter_loss: 0.1939476728439331
train_iter_loss: 0.19536304473876953
train_iter_loss: 0.1349114328622818
train_iter_loss: 0.13962675631046295
train_iter_loss: 0.17780764400959015
train_iter_loss: 0.1892530769109726
train_iter_loss: 0.10327363759279251
train_iter_loss: 0.16385234892368317
train_iter_loss: 0.1050044596195221
train_iter_loss: 0.15710008144378662
train_iter_loss: 0.13524910807609558
train_iter_loss: 0.18143555521965027
train_iter_loss: 0.1013728454709053
train_iter_loss: 0.13301633298397064
train_iter_loss: 0.2023228257894516
train_iter_loss: 0.4011840224266052
train_iter_loss: 0.15452857315540314
train_iter_loss: 0.21832074224948883
train_iter_loss: 0.11482842266559601
train_iter_loss: 0.23721148073673248
train_iter_loss: 0.20148800313472748
train_iter_loss: 0.34908947348594666
train_iter_loss: 0.10987938195466995
train_iter_loss: 0.21672366559505463
train_iter_loss: 0.20517192780971527
train_iter_loss: 0.18672780692577362
train_iter_loss: 0.15237505733966827
train_iter_loss: 0.08205147087574005
train_iter_loss: 0.21626342833042145
train_iter_loss: 0.18678607046604156
train_iter_loss: 0.10030900686979294
train_iter_loss: 0.11088500171899796
train_iter_loss: 0.236873596906662
train_iter_loss: 0.19714601337909698
train_iter_loss: 0.23727178573608398
train_iter_loss: 0.1781759262084961
train_iter_loss: 0.18093295395374298
train_iter_loss: 0.18495404720306396
train_iter_loss: 0.0776800662279129
train_iter_loss: 0.2111920416355133
train_iter_loss: 0.23487921059131622
train_iter_loss: 0.21649496257305145
train_iter_loss: 0.17953559756278992
train_iter_loss: 0.1381053477525711
train_iter_loss: 0.11056098341941833
train_iter_loss: 0.10051162540912628
train_iter_loss: 0.08392544835805893
train_iter_loss: 0.11018702387809753
train_iter_loss: 0.2338486909866333
train_iter_loss: 0.11291935294866562
train_iter_loss: 0.13618682324886322
train_iter_loss: 0.15703867375850677
train_iter_loss: 0.052963875234127045
train_iter_loss: 0.1619347631931305
train_iter_loss: 0.21885919570922852
train_iter_loss: 0.1700374037027359
train_iter_loss: 0.3078373074531555
train_iter_loss: 0.07429391890764236
train_iter_loss: 0.0480840690433979
train_iter_loss: 0.1880224049091339
train_iter_loss: 0.1215641051530838
train_iter_loss: 0.22338007390499115
train_iter_loss: 0.08841574192047119
train_iter_loss: 0.12825150787830353
train_iter_loss: 0.15204280614852905
train_iter_loss: 0.17634761333465576
train_iter_loss: 0.2514547109603882
train_iter_loss: 0.13680610060691833
train_iter_loss: 0.18726018071174622
train_iter_loss: 0.1521812081336975
train_iter_loss: 0.22859928011894226
train_iter_loss: 0.09941809624433517
train_iter_loss: 0.2192160189151764
train_iter_loss: 0.19902734458446503
train_iter_loss: 0.06845073401927948
train loss :0.1653
---------------------
Validation seg loss: 0.21818599183955845 at epoch 441
epoch =    442/  1000, exp = train
train_iter_loss: 0.05452786013484001
train_iter_loss: 0.14217951893806458
train_iter_loss: 0.10523531585931778
train_iter_loss: 0.21687020361423492
train_iter_loss: 0.14849355816841125
train_iter_loss: 0.18062996864318848
train_iter_loss: 0.08807454258203506
train_iter_loss: 0.15928292274475098
train_iter_loss: 0.12312962859869003
train_iter_loss: 0.1207888275384903
train_iter_loss: 0.12706533074378967
train_iter_loss: 0.16410595178604126
train_iter_loss: 0.17852671444416046
train_iter_loss: 0.19860219955444336
train_iter_loss: 0.23695363104343414
train_iter_loss: 0.16432493925094604
train_iter_loss: 0.1115877777338028
train_iter_loss: 0.16095072031021118
train_iter_loss: 0.08582443743944168
train_iter_loss: 0.05916007235646248
train_iter_loss: 0.19078905880451202
train_iter_loss: 0.23305152356624603
train_iter_loss: 0.216146782040596
train_iter_loss: 0.1402662843465805
train_iter_loss: 0.3250473737716675
train_iter_loss: 0.17510980367660522
train_iter_loss: 0.24912475049495697
train_iter_loss: 0.11866448074579239
train_iter_loss: 0.11109820753335953
train_iter_loss: 0.12399914115667343
train_iter_loss: 0.07321381568908691
train_iter_loss: 0.29694274067878723
train_iter_loss: 0.1771394908428192
train_iter_loss: 0.15175358951091766
train_iter_loss: 0.2035643756389618
train_iter_loss: 0.13489434123039246
train_iter_loss: 0.11483152955770493
train_iter_loss: 0.057912711054086685
train_iter_loss: 0.21270231902599335
train_iter_loss: 0.18187610805034637
train_iter_loss: 0.37241846323013306
train_iter_loss: 0.10928641259670258
train_iter_loss: 0.1422952115535736
train_iter_loss: 0.22020937502384186
train_iter_loss: 0.09756258875131607
train_iter_loss: 0.09742504358291626
train_iter_loss: 0.16159427165985107
train_iter_loss: 0.1174260824918747
train_iter_loss: 0.14281614124774933
train_iter_loss: 0.13196779787540436
train_iter_loss: 0.08740632981061935
train_iter_loss: 0.45420217514038086
train_iter_loss: 0.14011803269386292
train_iter_loss: 0.10969825834035873
train_iter_loss: 0.25498414039611816
train_iter_loss: 0.14276328682899475
train_iter_loss: 0.08595852553844452
train_iter_loss: 0.19125030934810638
train_iter_loss: 0.0751030445098877
train_iter_loss: 0.16475319862365723
train_iter_loss: 0.18241293728351593
train_iter_loss: 0.13859589397907257
train_iter_loss: 0.15804265439510345
train_iter_loss: 0.21994557976722717
train_iter_loss: 0.1264701634645462
train_iter_loss: 0.3228016197681427
train_iter_loss: 0.11458618193864822
train_iter_loss: 0.20421555638313293
train_iter_loss: 0.18294000625610352
train_iter_loss: 0.24211880564689636
train_iter_loss: 0.1862208992242813
train_iter_loss: 0.20375147461891174
train_iter_loss: 0.31250324845314026
train_iter_loss: 0.14003637433052063
train_iter_loss: 0.13411711156368256
train_iter_loss: 0.15410134196281433
train_iter_loss: 0.15264277160167694
train_iter_loss: 0.14257027208805084
train_iter_loss: 0.1610489934682846
train_iter_loss: 0.13801977038383484
train_iter_loss: 0.34091299772262573
train_iter_loss: 0.26443251967430115
train_iter_loss: 0.2147727757692337
train_iter_loss: 0.11631672829389572
train_iter_loss: 0.17131687700748444
train_iter_loss: 0.09461893886327744
train_iter_loss: 0.10884926468133926
train_iter_loss: 0.14381082355976105
train_iter_loss: 0.1475200206041336
train_iter_loss: 0.06370483338832855
train_iter_loss: 0.1670946329832077
train_iter_loss: 0.18068507313728333
train_iter_loss: 0.12522687017917633
train_iter_loss: 0.14557895064353943
train_iter_loss: 0.07212390750646591
train_iter_loss: 0.18987447023391724
train_iter_loss: 0.1856347918510437
train_iter_loss: 0.08318661898374557
train_iter_loss: 0.25385525822639465
train_iter_loss: 0.16626985371112823
train loss :0.1663
---------------------
Validation seg loss: 0.2171961879269835 at epoch 442
epoch =    443/  1000, exp = train
train_iter_loss: 0.20209506154060364
train_iter_loss: 0.10367316007614136
train_iter_loss: 0.18484215438365936
train_iter_loss: 0.06963314861059189
train_iter_loss: 0.15774735808372498
train_iter_loss: 0.11732372641563416
train_iter_loss: 0.08471240103244781
train_iter_loss: 0.17652280628681183
train_iter_loss: 0.12632599472999573
train_iter_loss: 0.19255559146404266
train_iter_loss: 0.11615918576717377
train_iter_loss: 0.1526697725057602
train_iter_loss: 0.2786054313182831
train_iter_loss: 0.09378446638584137
train_iter_loss: 0.12114308029413223
train_iter_loss: 0.11711063235998154
train_iter_loss: 0.2376335710287094
train_iter_loss: 0.10252169519662857
train_iter_loss: 0.12435374408960342
train_iter_loss: 0.16433824598789215
train_iter_loss: 0.16549469530582428
train_iter_loss: 0.09838255494832993
train_iter_loss: 0.16106659173965454
train_iter_loss: 0.17646828293800354
train_iter_loss: 0.21276521682739258
train_iter_loss: 0.1817547231912613
train_iter_loss: 0.16966809332370758
train_iter_loss: 0.20576311647891998
train_iter_loss: 0.2334004044532776
train_iter_loss: 0.14787892997264862
train_iter_loss: 0.22703798115253448
train_iter_loss: 0.13176552951335907
train_iter_loss: 0.08692978322505951
train_iter_loss: 0.3054059147834778
train_iter_loss: 0.08751116693019867
train_iter_loss: 0.18328426778316498
train_iter_loss: 0.029682043939828873
train_iter_loss: 0.14199186861515045
train_iter_loss: 0.15671345591545105
train_iter_loss: 0.08602680265903473
train_iter_loss: 0.048732172697782516
train_iter_loss: 0.1058296486735344
train_iter_loss: 0.14281794428825378
train_iter_loss: 0.20669814944267273
train_iter_loss: 0.13751448690891266
train_iter_loss: 0.17146983742713928
train_iter_loss: 0.29320257902145386
train_iter_loss: 0.12776818871498108
train_iter_loss: 0.18587042391300201
train_iter_loss: 0.11548362672328949
train_iter_loss: 0.13817352056503296
train_iter_loss: 0.1278712898492813
train_iter_loss: 0.3153756260871887
train_iter_loss: 0.07493454962968826
train_iter_loss: 0.1329890638589859
train_iter_loss: 0.11733195185661316
train_iter_loss: 0.1786719560623169
train_iter_loss: 0.1321464478969574
train_iter_loss: 0.2233053594827652
train_iter_loss: 0.16056647896766663
train_iter_loss: 0.1743253618478775
train_iter_loss: 0.19238081574440002
train_iter_loss: 0.2506044805049896
train_iter_loss: 0.11648886650800705
train_iter_loss: 0.17380265891551971
train_iter_loss: 0.2653968036174774
train_iter_loss: 0.18668429553508759
train_iter_loss: 0.12575940787792206
train_iter_loss: 0.13086558878421783
train_iter_loss: 0.11110426485538483
train_iter_loss: 0.24528534710407257
train_iter_loss: 0.17992046475410461
train_iter_loss: 0.20958158373832703
train_iter_loss: 0.12794359028339386
train_iter_loss: 0.34201374650001526
train_iter_loss: 0.2515394687652588
train_iter_loss: 0.09478545188903809
train_iter_loss: 0.23001620173454285
train_iter_loss: 0.15291714668273926
train_iter_loss: 0.11059829592704773
train_iter_loss: 0.12967655062675476
train_iter_loss: 0.12710031867027283
train_iter_loss: 0.17927752435207367
train_iter_loss: 0.1702384054660797
train_iter_loss: 0.17765845358371735
train_iter_loss: 0.1595563441514969
train_iter_loss: 0.22930490970611572
train_iter_loss: 0.12655498087406158
train_iter_loss: 0.19807137548923492
train_iter_loss: 0.09187732636928558
train_iter_loss: 0.13840022683143616
train_iter_loss: 0.1621599793434143
train_iter_loss: 0.18060830235481262
train_iter_loss: 0.13237528502941132
train_iter_loss: 0.2066330760717392
train_iter_loss: 0.15578876435756683
train_iter_loss: 0.0688343271613121
train_iter_loss: 0.13347618281841278
train_iter_loss: 0.24190329015254974
train_iter_loss: 0.12860457599163055
train loss :0.1623
---------------------
Validation seg loss: 0.21788498538739556 at epoch 443
epoch =    444/  1000, exp = train
train_iter_loss: 0.14952395856380463
train_iter_loss: 0.16273830831050873
train_iter_loss: 0.2151188850402832
train_iter_loss: 0.05246315151453018
train_iter_loss: 0.10229896754026413
train_iter_loss: 0.20540115237236023
train_iter_loss: 0.08677324652671814
train_iter_loss: 0.24518734216690063
train_iter_loss: 0.16183719038963318
train_iter_loss: 0.1689472645521164
train_iter_loss: 0.20436152815818787
train_iter_loss: 0.1559845209121704
train_iter_loss: 0.212354376912117
train_iter_loss: 0.179660826921463
train_iter_loss: 0.1732775717973709
train_iter_loss: 0.08050355315208435
train_iter_loss: 0.0841372013092041
train_iter_loss: 0.19722183048725128
train_iter_loss: 0.14279232919216156
train_iter_loss: 0.24402989447116852
train_iter_loss: 0.09127623587846756
train_iter_loss: 0.15510836243629456
train_iter_loss: 0.05911286920309067
train_iter_loss: 0.20014387369155884
train_iter_loss: 0.12008713185787201
train_iter_loss: 0.16827213764190674
train_iter_loss: 0.09953965246677399
train_iter_loss: 0.15841799974441528
train_iter_loss: 0.15938027203083038
train_iter_loss: 0.1794782429933548
train_iter_loss: 0.08718937635421753
train_iter_loss: 0.1761501133441925
train_iter_loss: 0.1337408572435379
train_iter_loss: 0.11700031906366348
train_iter_loss: 0.07658252865076065
train_iter_loss: 0.33790501952171326
train_iter_loss: 0.24358190596103668
train_iter_loss: 0.14042437076568604
train_iter_loss: 0.2022189199924469
train_iter_loss: 0.04193349555134773
train_iter_loss: 0.17937226593494415
train_iter_loss: 0.2028084546327591
train_iter_loss: 0.2048916220664978
train_iter_loss: 0.07279595732688904
train_iter_loss: 0.09125217795372009
train_iter_loss: 0.2806926369667053
train_iter_loss: 0.29159027338027954
train_iter_loss: 0.1575198620557785
train_iter_loss: 0.09998568147420883
train_iter_loss: 0.1638346165418625
train_iter_loss: 0.09321814030408859
train_iter_loss: 0.16839268803596497
train_iter_loss: 0.2253979593515396
train_iter_loss: 0.3728582262992859
train_iter_loss: 0.17330192029476166
train_iter_loss: 0.1590658724308014
train_iter_loss: 0.0726986676454544
train_iter_loss: 0.29420414566993713
train_iter_loss: 0.15912897884845734
train_iter_loss: 0.18474999070167542
train_iter_loss: 0.16563694179058075
train_iter_loss: 0.0909624844789505
train_iter_loss: 0.258754163980484
train_iter_loss: 0.36149072647094727
train_iter_loss: 0.1463620960712433
train_iter_loss: 0.11083260923624039
train_iter_loss: 0.22425618767738342
train_iter_loss: 0.11647092550992966
train_iter_loss: 0.1543521136045456
train_iter_loss: 0.19619321823120117
train_iter_loss: 0.1876092404127121
train_iter_loss: 0.09899488091468811
train_iter_loss: 0.14197640120983124
train_iter_loss: 0.2686382234096527
train_iter_loss: 0.10978539288043976
train_iter_loss: 0.1253361999988556
train_iter_loss: 0.08860546350479126
train_iter_loss: 0.1365458220243454
train_iter_loss: 0.11515651643276215
train_iter_loss: 0.17859788239002228
train_iter_loss: 0.19858579337596893
train_iter_loss: 0.12638728320598602
train_iter_loss: 0.17112213373184204
train_iter_loss: 0.13633623719215393
train_iter_loss: 0.14553622901439667
train_iter_loss: 0.1459270715713501
train_iter_loss: 0.13501112163066864
train_iter_loss: 0.12745317816734314
train_iter_loss: 0.0820193663239479
train_iter_loss: 0.1262637823820114
train_iter_loss: 0.12993057072162628
train_iter_loss: 0.12541498243808746
train_iter_loss: 0.09745858609676361
train_iter_loss: 0.06992893666028976
train_iter_loss: 0.07725237309932709
train_iter_loss: 0.11775049567222595
train_iter_loss: 0.5052679181098938
train_iter_loss: 0.17588017880916595
train_iter_loss: 0.11994361877441406
train_iter_loss: 0.17152230441570282
train loss :0.1624
---------------------
Validation seg loss: 0.22116016938333521 at epoch 444
epoch =    445/  1000, exp = train
train_iter_loss: 0.1607404500246048
train_iter_loss: 0.1547912210226059
train_iter_loss: 0.2153530716896057
train_iter_loss: 0.44926849007606506
train_iter_loss: 0.11794880032539368
train_iter_loss: 0.09884428977966309
train_iter_loss: 0.11398396641016006
train_iter_loss: 0.12905363738536835
train_iter_loss: 0.2127155065536499
train_iter_loss: 0.15485498309135437
train_iter_loss: 0.18628711998462677
train_iter_loss: 0.11902669817209244
train_iter_loss: 0.143927201628685
train_iter_loss: 0.09758095443248749
train_iter_loss: 0.15714150667190552
train_iter_loss: 0.17792503535747528
train_iter_loss: 0.17721639573574066
train_iter_loss: 0.06536455452442169
train_iter_loss: 0.451889306306839
train_iter_loss: 0.20670080184936523
train_iter_loss: 0.1751428097486496
train_iter_loss: 0.10746919363737106
train_iter_loss: 0.2008180469274521
train_iter_loss: 0.5140583515167236
train_iter_loss: 0.18759861588478088
train_iter_loss: 0.09655053913593292
train_iter_loss: 0.1728084832429886
train_iter_loss: 0.14375156164169312
train_iter_loss: 0.16561676561832428
train_iter_loss: 0.194426029920578
train_iter_loss: 0.21614694595336914
train_iter_loss: 0.1677543669939041
train_iter_loss: 0.23278716206550598
train_iter_loss: 0.07957827299833298
train_iter_loss: 0.1788713037967682
train_iter_loss: 0.09137491136789322
train_iter_loss: 0.21271491050720215
train_iter_loss: 0.1873229295015335
train_iter_loss: 0.14746303856372833
train_iter_loss: 0.25898993015289307
train_iter_loss: 0.1987561136484146
train_iter_loss: 0.08420750498771667
train_iter_loss: 0.18185432255268097
train_iter_loss: 0.12550784647464752
train_iter_loss: 0.19460488855838776
train_iter_loss: 0.032467227429151535
train_iter_loss: 0.13206493854522705
train_iter_loss: 0.15500932931900024
train_iter_loss: 0.2256731539964676
train_iter_loss: 0.18256470561027527
train_iter_loss: 0.07593417167663574
train_iter_loss: 0.1665739268064499
train_iter_loss: 0.19011017680168152
train_iter_loss: 0.1954011470079422
train_iter_loss: 0.1640140265226364
train_iter_loss: 0.11547917872667313
train_iter_loss: 0.1260693371295929
train_iter_loss: 0.14392298460006714
train_iter_loss: 0.11607988178730011
train_iter_loss: 0.07099369913339615
train_iter_loss: 0.08955718576908112
train_iter_loss: 0.1328248828649521
train_iter_loss: 0.12153838574886322
train_iter_loss: 0.16227969527244568
train_iter_loss: 0.15592126548290253
train_iter_loss: 0.15055567026138306
train_iter_loss: 0.15859189629554749
train_iter_loss: 0.15749584138393402
train_iter_loss: 0.1851300746202469
train_iter_loss: 0.13789157569408417
train_iter_loss: 0.15445955097675323
train_iter_loss: 0.17297004163265228
train_iter_loss: 0.1726103574037552
train_iter_loss: 0.17985641956329346
train_iter_loss: 0.08672521263360977
train_iter_loss: 0.13763779401779175
train_iter_loss: 0.09954434633255005
train_iter_loss: 0.17800745368003845
train_iter_loss: 0.11171405762434006
train_iter_loss: 0.12811961770057678
train_iter_loss: 0.17310059070587158
train_iter_loss: 0.2234545350074768
train_iter_loss: 0.14908798038959503
train_iter_loss: 0.131162628531456
train_iter_loss: 0.18236924707889557
train_iter_loss: 0.2111770659685135
train_iter_loss: 0.06726215034723282
train_iter_loss: 0.14062032103538513
train_iter_loss: 0.1596226990222931
train_iter_loss: 0.2542424201965332
train_iter_loss: 0.16391205787658691
train_iter_loss: 0.10929693281650543
train_iter_loss: 0.1007552370429039
train_iter_loss: 0.17924603819847107
train_iter_loss: 0.057241421192884445
train_iter_loss: 0.3061494529247284
train_iter_loss: 0.1917068362236023
train_iter_loss: 0.21631388366222382
train_iter_loss: 0.20201021432876587
train_iter_loss: 0.08815667778253555
train loss :0.1654
---------------------
Validation seg loss: 0.22134129330515862 at epoch 445
epoch =    446/  1000, exp = train
train_iter_loss: 0.30667978525161743
train_iter_loss: 0.1402444988489151
train_iter_loss: 0.10757031291723251
train_iter_loss: 0.1704803705215454
train_iter_loss: 0.0755220502614975
train_iter_loss: 0.12864834070205688
train_iter_loss: 0.21140632033348083
train_iter_loss: 0.154751256108284
train_iter_loss: 0.2468261569738388
train_iter_loss: 0.12313536554574966
train_iter_loss: 0.24723400175571442
train_iter_loss: 0.2543107271194458
train_iter_loss: 0.18341605365276337
train_iter_loss: 0.23831556737422943
train_iter_loss: 0.08867818117141724
train_iter_loss: 0.19488069415092468
train_iter_loss: 0.17213456332683563
train_iter_loss: 0.10389210283756256
train_iter_loss: 0.29301366209983826
train_iter_loss: 0.11107412725687027
train_iter_loss: 0.16872629523277283
train_iter_loss: 0.11191239953041077
train_iter_loss: 0.2622373402118683
train_iter_loss: 0.19303388893604279
train_iter_loss: 0.13733971118927002
train_iter_loss: 0.27273237705230713
train_iter_loss: 0.4267975986003876
train_iter_loss: 0.23746491968631744
train_iter_loss: 0.15508536994457245
train_iter_loss: 0.08644368499517441
train_iter_loss: 0.1387445032596588
train_iter_loss: 0.15879227221012115
train_iter_loss: 0.277585506439209
train_iter_loss: 0.08840034902095795
train_iter_loss: 0.09714967012405396
train_iter_loss: 0.15250857174396515
train_iter_loss: 0.1348845660686493
train_iter_loss: 0.13049784302711487
train_iter_loss: 0.14316807687282562
train_iter_loss: 0.21255618333816528
train_iter_loss: 0.1037120670080185
train_iter_loss: 0.11908519268035889
train_iter_loss: 0.19602376222610474
train_iter_loss: 0.1492072194814682
train_iter_loss: 0.06257858872413635
train_iter_loss: 0.1380568891763687
train_iter_loss: 0.299277126789093
train_iter_loss: 0.1860475093126297
train_iter_loss: 0.1406324803829193
train_iter_loss: 0.18606747686862946
train_iter_loss: 0.13403809070587158
train_iter_loss: 0.1361028552055359
train_iter_loss: 0.14344589412212372
train_iter_loss: 0.11080531030893326
train_iter_loss: 0.14425534009933472
train_iter_loss: 0.13260158896446228
train_iter_loss: 0.22767218947410583
train_iter_loss: 0.06125541776418686
train_iter_loss: 0.10653681308031082
train_iter_loss: 0.25511473417282104
train_iter_loss: 0.1136775091290474
train_iter_loss: 0.14167751371860504
train_iter_loss: 0.13286401331424713
train_iter_loss: 0.10775306075811386
train_iter_loss: 0.16429446637630463
train_iter_loss: 0.14333391189575195
train_iter_loss: 0.2333650141954422
train_iter_loss: 0.06570788472890854
train_iter_loss: 0.14168681204319
train_iter_loss: 0.22057029604911804
train_iter_loss: 0.18086636066436768
train_iter_loss: 0.08846672624349594
train_iter_loss: 0.2204936444759369
train_iter_loss: 0.11898273974657059
train_iter_loss: 0.12935134768486023
train_iter_loss: 0.0715525820851326
train_iter_loss: 0.11280763149261475
train_iter_loss: 0.1831091195344925
train_iter_loss: 0.26289698481559753
train_iter_loss: 0.20796401798725128
train_iter_loss: 0.15777361392974854
train_iter_loss: 0.07710559666156769
train_iter_loss: 0.13781704008579254
train_iter_loss: 0.1322842836380005
train_iter_loss: 0.1402454376220703
train_iter_loss: 0.18763725459575653
train_iter_loss: 0.13973888754844666
train_iter_loss: 0.13872431218624115
train_iter_loss: 0.16898030042648315
train_iter_loss: 0.13771313428878784
train_iter_loss: 0.13135211169719696
train_iter_loss: 0.11466056108474731
train_iter_loss: 0.11825518310070038
train_iter_loss: 0.1836872100830078
train_iter_loss: 0.1464923620223999
train_iter_loss: 0.11738400906324387
train_iter_loss: 0.07413311302661896
train_iter_loss: 0.12264761328697205
train_iter_loss: 0.10863027721643448
train_iter_loss: 0.19456332921981812
train loss :0.1611
---------------------
Validation seg loss: 0.21861334543077732 at epoch 446
epoch =    447/  1000, exp = train
train_iter_loss: 0.09193418174982071
train_iter_loss: 0.12405914068222046
train_iter_loss: 0.08928263187408447
train_iter_loss: 0.12863321602344513
train_iter_loss: 0.28567251563072205
train_iter_loss: 0.2261756956577301
train_iter_loss: 0.09850000590085983
train_iter_loss: 0.08575920760631561
train_iter_loss: 0.16335688531398773
train_iter_loss: 0.12401124835014343
train_iter_loss: 0.34166911244392395
train_iter_loss: 0.12465445697307587
train_iter_loss: 0.18129770457744598
train_iter_loss: 0.16746734082698822
train_iter_loss: 0.10004798322916031
train_iter_loss: 0.21347030997276306
train_iter_loss: 0.12693971395492554
train_iter_loss: 0.11962679773569107
train_iter_loss: 0.10120576620101929
train_iter_loss: 0.10602634400129318
train_iter_loss: 0.12848563492298126
train_iter_loss: 0.2756156325340271
train_iter_loss: 0.17134539783000946
train_iter_loss: 0.16420483589172363
train_iter_loss: 0.14227013289928436
train_iter_loss: 0.06542029976844788
train_iter_loss: 0.1777157336473465
train_iter_loss: 0.10221174359321594
train_iter_loss: 0.20600979030132294
train_iter_loss: 0.12533743679523468
train_iter_loss: 0.21304164826869965
train_iter_loss: 0.1401435136795044
train_iter_loss: 0.1730337291955948
train_iter_loss: 0.08426302671432495
train_iter_loss: 0.22578944265842438
train_iter_loss: 0.12099777907133102
train_iter_loss: 0.25631579756736755
train_iter_loss: 0.22310517728328705
train_iter_loss: 0.13998742401599884
train_iter_loss: 0.17106559872627258
train_iter_loss: 0.1580839902162552
train_iter_loss: 0.11562247574329376
train_iter_loss: 0.034819427877664566
train_iter_loss: 0.1375693678855896
train_iter_loss: 0.10611800849437714
train_iter_loss: 0.08917395025491714
train_iter_loss: 0.17818571627140045
train_iter_loss: 0.08025813102722168
train_iter_loss: 0.11463002115488052
train_iter_loss: 0.19734510779380798
train_iter_loss: 0.20051638782024384
train_iter_loss: 0.10927476733922958
train_iter_loss: 0.06092505902051926
train_iter_loss: 0.10417995601892471
train_iter_loss: 0.25863397121429443
train_iter_loss: 0.24843448400497437
train_iter_loss: 0.16656215488910675
train_iter_loss: 0.14144223928451538
train_iter_loss: 0.14638397097587585
train_iter_loss: 0.1112595647573471
train_iter_loss: 0.09552596509456635
train_iter_loss: 0.050026074051856995
train_iter_loss: 0.44512566924095154
train_iter_loss: 0.20961280167102814
train_iter_loss: 0.14525845646858215
train_iter_loss: 0.2249847948551178
train_iter_loss: 0.1763358861207962
train_iter_loss: 0.2508251667022705
train_iter_loss: 0.21532878279685974
train_iter_loss: 0.12775938212871552
train_iter_loss: 0.27988743782043457
train_iter_loss: 0.04467027261853218
train_iter_loss: 0.12839628756046295
train_iter_loss: 0.1714540272951126
train_iter_loss: 0.10940665751695633
train_iter_loss: 0.20315983891487122
train_iter_loss: 0.15721696615219116
train_iter_loss: 0.14612150192260742
train_iter_loss: 0.23135821521282196
train_iter_loss: 0.3020480275154114
train_iter_loss: 0.2106536477804184
train_iter_loss: 0.19169510900974274
train_iter_loss: 0.0926830843091011
train_iter_loss: 0.20407891273498535
train_iter_loss: 0.12491613626480103
train_iter_loss: 0.23704475164413452
train_iter_loss: 0.16226351261138916
train_iter_loss: 0.1750190556049347
train_iter_loss: 0.2049664407968521
train_iter_loss: 0.12046302109956741
train_iter_loss: 0.16522420942783356
train_iter_loss: 0.13708756864070892
train_iter_loss: 0.11748982965946198
train_iter_loss: 0.22589460015296936
train_iter_loss: 0.15333892405033112
train_iter_loss: 0.14656877517700195
train_iter_loss: 0.1307390332221985
train_iter_loss: 0.23573528230190277
train_iter_loss: 0.22593523561954498
train_iter_loss: 0.07309570908546448
train loss :0.1628
---------------------
Validation seg loss: 0.21612532592360983 at epoch 447
epoch =    448/  1000, exp = train
train_iter_loss: 0.15166403353214264
train_iter_loss: 0.24681253731250763
train_iter_loss: 0.09641508013010025
train_iter_loss: 0.10501064360141754
train_iter_loss: 0.11211325228214264
train_iter_loss: 0.14604397118091583
train_iter_loss: 0.20426225662231445
train_iter_loss: 0.23215749859809875
train_iter_loss: 0.13006937503814697
train_iter_loss: 0.1903793215751648
train_iter_loss: 0.33705973625183105
train_iter_loss: 0.08675353229045868
train_iter_loss: 0.18011170625686646
train_iter_loss: 0.107828289270401
train_iter_loss: 0.17570501565933228
train_iter_loss: 0.15508118271827698
train_iter_loss: 0.20916403830051422
train_iter_loss: 0.22522002458572388
train_iter_loss: 0.09654981642961502
train_iter_loss: 0.15836511552333832
train_iter_loss: 0.18955953419208527
train_iter_loss: 0.10072696954011917
train_iter_loss: 0.06729939579963684
train_iter_loss: 0.1509692519903183
train_iter_loss: 0.12497197836637497
train_iter_loss: 0.11071170121431351
train_iter_loss: 0.09197629988193512
train_iter_loss: 0.3655456006526947
train_iter_loss: 0.18120640516281128
train_iter_loss: 0.10746899992227554
train_iter_loss: 0.15477816760540009
train_iter_loss: 0.15411683917045593
train_iter_loss: 0.12025716155767441
train_iter_loss: 0.17932413518428802
train_iter_loss: 0.17044945061206818
train_iter_loss: 0.16990317404270172
train_iter_loss: 0.17733675241470337
train_iter_loss: 0.08674366772174835
train_iter_loss: 0.1719161868095398
train_iter_loss: 0.05586879700422287
train_iter_loss: 0.16171297430992126
train_iter_loss: 0.1015203520655632
train_iter_loss: 0.11074242740869522
train_iter_loss: 0.09457597136497498
train_iter_loss: 0.18157769739627838
train_iter_loss: 0.2257917821407318
train_iter_loss: 0.16032440960407257
train_iter_loss: 0.1009877622127533
train_iter_loss: 0.1504874974489212
train_iter_loss: 0.14779721200466156
train_iter_loss: 0.261074423789978
train_iter_loss: 0.2370186448097229
train_iter_loss: 0.1226349025964737
train_iter_loss: 0.3173506557941437
train_iter_loss: 0.2415768802165985
train_iter_loss: 0.08630768209695816
train_iter_loss: 0.12645745277404785
train_iter_loss: 0.12391426414251328
train_iter_loss: 0.13738413155078888
train_iter_loss: 0.1735524982213974
train_iter_loss: 0.18738605082035065
train_iter_loss: 0.12292203307151794
train_iter_loss: 0.08657731115818024
train_iter_loss: 0.0770389586687088
train_iter_loss: 0.11861763149499893
train_iter_loss: 0.2848893702030182
train_iter_loss: 0.06457417458295822
train_iter_loss: 0.19204117357730865
train_iter_loss: 0.17977380752563477
train_iter_loss: 0.20066556334495544
train_iter_loss: 0.1934443861246109
train_iter_loss: 0.17056863009929657
train_iter_loss: 0.15267808735370636
train_iter_loss: 0.10910167545080185
train_iter_loss: 0.0950435996055603
train_iter_loss: 0.21015258133411407
train_iter_loss: 0.1794859766960144
train_iter_loss: 0.09566974639892578
train_iter_loss: 0.1247212365269661
train_iter_loss: 0.18997782468795776
train_iter_loss: 0.2451237589120865
train_iter_loss: 0.3029201328754425
train_iter_loss: 0.12221965193748474
train_iter_loss: 0.13184776902198792
train_iter_loss: 0.12174402922391891
train_iter_loss: 0.16951484978199005
train_iter_loss: 0.120890311896801
train_iter_loss: 0.3501352071762085
train_iter_loss: 0.20811592042446136
train_iter_loss: 0.1799243688583374
train_iter_loss: 0.10640401393175125
train_iter_loss: 0.2047606110572815
train_iter_loss: 0.1415604203939438
train_iter_loss: 0.10214617848396301
train_iter_loss: 0.159165158867836
train_iter_loss: 0.14325891435146332
train_iter_loss: 0.15149955451488495
train_iter_loss: 0.24174627661705017
train_iter_loss: 0.15045911073684692
train_iter_loss: 0.16484424471855164
train loss :0.1626
---------------------
Validation seg loss: 0.2184237725769152 at epoch 448
epoch =    449/  1000, exp = train
train_iter_loss: 0.10983704775571823
train_iter_loss: 0.23076939582824707
train_iter_loss: 0.0725521370768547
train_iter_loss: 0.1327689290046692
train_iter_loss: 0.19304656982421875
train_iter_loss: 0.18353869020938873
train_iter_loss: 0.054341793060302734
train_iter_loss: 0.05405768007040024
train_iter_loss: 0.1636604219675064
train_iter_loss: 0.11677410453557968
train_iter_loss: 0.20251017808914185
train_iter_loss: 0.11942940205335617
train_iter_loss: 0.17877687513828278
train_iter_loss: 0.14888498187065125
train_iter_loss: 0.09498348087072372
train_iter_loss: 0.07394102960824966
train_iter_loss: 0.2401251494884491
train_iter_loss: 0.12309124320745468
train_iter_loss: 0.09502105414867401
train_iter_loss: 0.22806942462921143
train_iter_loss: 0.08824472874403
train_iter_loss: 0.17207781970500946
train_iter_loss: 0.10233917087316513
train_iter_loss: 0.16969478130340576
train_iter_loss: 0.2774454653263092
train_iter_loss: 0.1546970009803772
train_iter_loss: 0.15638244152069092
train_iter_loss: 0.2077605277299881
train_iter_loss: 0.24188120663166046
train_iter_loss: 0.07432511448860168
train_iter_loss: 0.1475311517715454
train_iter_loss: 0.17319132387638092
train_iter_loss: 0.15413710474967957
train_iter_loss: 0.07809842377901077
train_iter_loss: 0.17641806602478027
train_iter_loss: 0.15302126109600067
train_iter_loss: 0.1838417500257492
train_iter_loss: 0.10870727896690369
train_iter_loss: 0.12167617678642273
train_iter_loss: 0.2714601457118988
train_iter_loss: 0.16918593645095825
train_iter_loss: 0.2147153913974762
train_iter_loss: 0.17859145998954773
train_iter_loss: 0.0527743399143219
train_iter_loss: 0.16011156141757965
train_iter_loss: 0.06155265495181084
train_iter_loss: 0.16258692741394043
train_iter_loss: 0.13739828765392303
train_iter_loss: 0.16442085802555084
train_iter_loss: 0.08955677598714828
train_iter_loss: 0.23375821113586426
train_iter_loss: 0.23728856444358826
train_iter_loss: 0.16556686162948608
train_iter_loss: 0.19175824522972107
train_iter_loss: 0.07136422395706177
train_iter_loss: 0.10324350744485855
train_iter_loss: 0.11459135264158249
train_iter_loss: 0.3637571930885315
train_iter_loss: 0.07652321457862854
train_iter_loss: 0.12531812489032745
train_iter_loss: 0.12824057042598724
train_iter_loss: 0.24585595726966858
train_iter_loss: 0.20063094794750214
train_iter_loss: 0.22457922995090485
train_iter_loss: 0.07913761585950851
train_iter_loss: 0.10547434538602829
train_iter_loss: 0.10494088381528854
train_iter_loss: 0.23368650674819946
train_iter_loss: 0.2599402666091919
train_iter_loss: 0.2584497332572937
train_iter_loss: 0.15348631143569946
train_iter_loss: 0.10851003974676132
train_iter_loss: 0.24372000992298126
train_iter_loss: 0.26281842589378357
train_iter_loss: 0.17150311172008514
train_iter_loss: 0.13457947969436646
train_iter_loss: 0.28463321924209595
train_iter_loss: 0.07570159435272217
train_iter_loss: 0.08645153790712357
train_iter_loss: 0.13906097412109375
train_iter_loss: 0.17548082768917084
train_iter_loss: 0.25333037972450256
train_iter_loss: 0.09789598733186722
train_iter_loss: 0.1351102888584137
train_iter_loss: 0.09892921894788742
train_iter_loss: 0.11154169589281082
train_iter_loss: 0.23718371987342834
train_iter_loss: 0.07982654124498367
train_iter_loss: 0.13706178963184357
train_iter_loss: 0.1684667468070984
train_iter_loss: 0.141856849193573
train_iter_loss: 0.2179185301065445
train_iter_loss: 0.10487110912799835
train_iter_loss: 0.14088325202465057
train_iter_loss: 0.1263379007577896
train_iter_loss: 0.18810266256332397
train_iter_loss: 0.3361622989177704
train_iter_loss: 0.2332243174314499
train_iter_loss: 0.16217230260372162
train_iter_loss: 0.14587774872779846
train loss :0.1606
---------------------
Validation seg loss: 0.21816249897281798 at epoch 449
epoch =    450/  1000, exp = train
train_iter_loss: 0.27812403440475464
train_iter_loss: 0.1336255967617035
train_iter_loss: 0.13177178800106049
train_iter_loss: 0.13092128932476044
train_iter_loss: 0.2600872218608856
train_iter_loss: 0.08519482612609863
train_iter_loss: 0.3016757071018219
train_iter_loss: 0.09588289260864258
train_iter_loss: 0.16755427420139313
train_iter_loss: 0.10648048669099808
train_iter_loss: 0.09983784705400467
train_iter_loss: 0.0975532978773117
train_iter_loss: 0.12623070180416107
train_iter_loss: 0.16853849589824677
train_iter_loss: 0.08106773346662521
train_iter_loss: 0.08565863221883774
train_iter_loss: 0.13845428824424744
train_iter_loss: 0.19759486615657806
train_iter_loss: 0.13954517245292664
train_iter_loss: 0.1850801557302475
train_iter_loss: 0.15642209351062775
train_iter_loss: 0.3096366226673126
train_iter_loss: 0.23991858959197998
train_iter_loss: 0.12143891304731369
train_iter_loss: 0.26016348600387573
train_iter_loss: 0.23532022535800934
train_iter_loss: 0.14628149569034576
train_iter_loss: 0.14149384200572968
train_iter_loss: 0.1342734843492508
train_iter_loss: 0.13929139077663422
train_iter_loss: 0.18276235461235046
train_iter_loss: 0.18632836639881134
train_iter_loss: 0.13711673021316528
train_iter_loss: 0.15033237636089325
train_iter_loss: 0.16696633398532867
train_iter_loss: 0.1518757939338684
train_iter_loss: 0.13298700749874115
train_iter_loss: 0.125582754611969
train_iter_loss: 0.259567528963089
train_iter_loss: 0.11531012505292892
train_iter_loss: 0.15271404385566711
train_iter_loss: 0.084090955555439
train_iter_loss: 0.28247392177581787
train_iter_loss: 0.13929036259651184
train_iter_loss: 0.21464785933494568
train_iter_loss: 0.13362979888916016
train_iter_loss: 0.1668628305196762
train_iter_loss: 0.1110917180776596
train_iter_loss: 0.08168219774961472
train_iter_loss: 0.21749375760555267
train_iter_loss: 0.12827669084072113
train_iter_loss: 0.06471946090459824
train_iter_loss: 0.12089931964874268
train_iter_loss: 0.12510915100574493
train_iter_loss: 0.1987224519252777
train_iter_loss: 0.0909559577703476
train_iter_loss: 0.1563267856836319
train_iter_loss: 0.09755685925483704
train_iter_loss: 0.23374053835868835
train_iter_loss: 0.21535809338092804
train_iter_loss: 0.23314036428928375
train_iter_loss: 0.1604190468788147
train_iter_loss: 0.20328094065189362
train_iter_loss: 0.19936439394950867
train_iter_loss: 0.14829209446907043
train_iter_loss: 0.14762228727340698
train_iter_loss: 0.15731847286224365
train_iter_loss: 0.1872621476650238
train_iter_loss: 0.18465861678123474
train_iter_loss: 0.13902896642684937
train_iter_loss: 0.13192680478096008
train_iter_loss: 0.105284184217453
train_iter_loss: 0.19747699797153473
train_iter_loss: 0.179230198264122
train_iter_loss: 0.13017867505550385
train_iter_loss: 0.30065998435020447
train_iter_loss: 0.13117358088493347
train_iter_loss: 0.25180336833000183
train_iter_loss: 0.16440074145793915
train_iter_loss: 0.15043030679225922
train_iter_loss: 0.16896311938762665
train_iter_loss: 0.13328982889652252
train_iter_loss: 0.10017674416303635
train_iter_loss: 0.14539283514022827
train_iter_loss: 0.14379793405532837
train_iter_loss: 0.25059911608695984
train_iter_loss: 0.2177281677722931
train_iter_loss: 0.3374246060848236
train_iter_loss: 0.08867540955543518
train_iter_loss: 0.17079958319664001
train_iter_loss: 0.2174489051103592
train_iter_loss: 0.050538569688797
train_iter_loss: 0.20258471369743347
train_iter_loss: 0.051334086805582047
train_iter_loss: 0.18132241070270538
train_iter_loss: 0.07635949552059174
train_iter_loss: 0.17726674675941467
train_iter_loss: 0.2584454119205475
train_iter_loss: 0.1867944449186325
train_iter_loss: 0.2374640852212906
train loss :0.1658
---------------------
Validation seg loss: 0.22046382817492452 at epoch 450
epoch =    451/  1000, exp = train
train_iter_loss: 0.11077547818422318
train_iter_loss: 0.13532385230064392
train_iter_loss: 0.2018858939409256
train_iter_loss: 0.14658738672733307
train_iter_loss: 0.22183340787887573
train_iter_loss: 0.20054113864898682
train_iter_loss: 0.18351322412490845
train_iter_loss: 0.1960650235414505
train_iter_loss: 0.14897292852401733
train_iter_loss: 0.14120091497898102
train_iter_loss: 0.10395912826061249
train_iter_loss: 0.1282338798046112
train_iter_loss: 0.23498179018497467
train_iter_loss: 0.18062445521354675
train_iter_loss: 0.16376741230487823
train_iter_loss: 0.255698025226593
train_iter_loss: 0.16061384975910187
train_iter_loss: 0.1566554754972458
train_iter_loss: 0.08719871193170547
train_iter_loss: 0.1779828816652298
train_iter_loss: 0.11818250268697739
train_iter_loss: 0.10125420987606049
train_iter_loss: 0.10746189206838608
train_iter_loss: 0.14498509466648102
train_iter_loss: 0.04852743074297905
train_iter_loss: 0.10519231855869293
train_iter_loss: 0.26641589403152466
train_iter_loss: 0.139860600233078
train_iter_loss: 0.04120728746056557
train_iter_loss: 0.21862433850765228
train_iter_loss: 0.12804612517356873
train_iter_loss: 0.09028246253728867
train_iter_loss: 0.12156406044960022
train_iter_loss: 0.14359214901924133
train_iter_loss: 0.0947582945227623
train_iter_loss: 0.13178680837154388
train_iter_loss: 0.22793982923030853
train_iter_loss: 0.1337999552488327
train_iter_loss: 0.07636240124702454
train_iter_loss: 0.20341408252716064
train_iter_loss: 0.20366930961608887
train_iter_loss: 0.12439826130867004
train_iter_loss: 0.1467266082763672
train_iter_loss: 0.058140192180871964
train_iter_loss: 0.1203569620847702
train_iter_loss: 0.05951446294784546
train_iter_loss: 0.18593788146972656
train_iter_loss: 0.2804126441478729
train_iter_loss: 0.15350155532360077
train_iter_loss: 0.15896910429000854
train_iter_loss: 0.19402192533016205
train_iter_loss: 0.23341552913188934
train_iter_loss: 0.27777647972106934
train_iter_loss: 0.22448140382766724
train_iter_loss: 0.2006755769252777
train_iter_loss: 0.11948549747467041
train_iter_loss: 0.18803170323371887
train_iter_loss: 0.0880672037601471
train_iter_loss: 0.18169091641902924
train_iter_loss: 0.33387666940689087
train_iter_loss: 0.08761398494243622
train_iter_loss: 0.26719826459884644
train_iter_loss: 0.20882490277290344
train_iter_loss: 0.16795401275157928
train_iter_loss: 0.23324579000473022
train_iter_loss: 0.2247312068939209
train_iter_loss: 0.09430789947509766
train_iter_loss: 0.14015139639377594
train_iter_loss: 0.12390919029712677
train_iter_loss: 0.18552526831626892
train_iter_loss: 0.10834454745054245
train_iter_loss: 0.0809372067451477
train_iter_loss: 0.16370892524719238
train_iter_loss: 0.1916538029909134
train_iter_loss: 0.21354436874389648
train_iter_loss: 0.10170452296733856
train_iter_loss: 0.2335454821586609
train_iter_loss: 0.24282406270503998
train_iter_loss: 0.27560827136039734
train_iter_loss: 0.13725292682647705
train_iter_loss: 0.2956179082393646
train_iter_loss: 0.12316147238016129
train_iter_loss: 0.1890614628791809
train_iter_loss: 0.10948941856622696
train_iter_loss: 0.12109397351741791
train_iter_loss: 0.1593572050333023
train_iter_loss: 0.11759354919195175
train_iter_loss: 0.17696943879127502
train_iter_loss: 0.14730042219161987
train_iter_loss: 0.10967221111059189
train_iter_loss: 0.3078800141811371
train_iter_loss: 0.09087340533733368
train_iter_loss: 0.28301802277565
train_iter_loss: 0.26851123571395874
train_iter_loss: 0.20168627798557281
train_iter_loss: 0.09068067371845245
train_iter_loss: 0.037381935864686966
train_iter_loss: 0.18925903737545013
train_iter_loss: 0.20288164913654327
train_iter_loss: 0.1318856179714203
train loss :0.1651
---------------------
Validation seg loss: 0.2204102194288148 at epoch 451
epoch =    452/  1000, exp = train
train_iter_loss: 0.12285443395376205
train_iter_loss: 0.08667544275522232
train_iter_loss: 0.13731645047664642
train_iter_loss: 0.07246983796358109
train_iter_loss: 0.310224711894989
train_iter_loss: 0.16800491511821747
train_iter_loss: 0.24823202192783356
train_iter_loss: 0.12488933652639389
train_iter_loss: 0.15267248451709747
train_iter_loss: 0.2895544767379761
train_iter_loss: 0.23522593080997467
train_iter_loss: 0.16304656863212585
train_iter_loss: 0.12241050601005554
train_iter_loss: 0.10518719255924225
train_iter_loss: 0.1431845724582672
train_iter_loss: 0.09424401074647903
train_iter_loss: 0.16903933882713318
train_iter_loss: 0.11902900040149689
train_iter_loss: 0.3738727271556854
train_iter_loss: 0.08754532039165497
train_iter_loss: 0.12550948560237885
train_iter_loss: 0.3065841495990753
train_iter_loss: 0.2090628743171692
train_iter_loss: 0.20418480038642883
train_iter_loss: 0.1285129189491272
train_iter_loss: 0.19156360626220703
train_iter_loss: 0.11506131291389465
train_iter_loss: 0.1494777947664261
train_iter_loss: 0.1290094554424286
train_iter_loss: 0.1928042769432068
train_iter_loss: 0.10981135070323944
train_iter_loss: 0.11970546096563339
train_iter_loss: 0.18041378259658813
train_iter_loss: 0.10285194963216782
train_iter_loss: 0.11041343212127686
train_iter_loss: 0.40152204036712646
train_iter_loss: 0.08091855049133301
train_iter_loss: 0.23331919312477112
train_iter_loss: 0.20107325911521912
train_iter_loss: 0.16665351390838623
train_iter_loss: 0.1552789956331253
train_iter_loss: 0.12084808945655823
train_iter_loss: 0.1129552498459816
train_iter_loss: 0.13958638906478882
train_iter_loss: 0.13105100393295288
train_iter_loss: 0.152779683470726
train_iter_loss: 0.14689594507217407
train_iter_loss: 0.17251761257648468
train_iter_loss: 0.23226049542427063
train_iter_loss: 0.105260469019413
train_iter_loss: 0.16005824506282806
train_iter_loss: 0.200176402926445
train_iter_loss: 0.22450421750545502
train_iter_loss: 0.11842215061187744
train_iter_loss: 0.1530996412038803
train_iter_loss: 0.1393413096666336
train_iter_loss: 0.11387757211923599
train_iter_loss: 0.13965637981891632
train_iter_loss: 0.15447813272476196
train_iter_loss: 0.11564741283655167
train_iter_loss: 0.13075296580791473
train_iter_loss: 0.16939863562583923
train_iter_loss: 0.20282353460788727
train_iter_loss: 0.11198664456605911
train_iter_loss: 0.08528140187263489
train_iter_loss: 0.326081782579422
train_iter_loss: 0.1086810752749443
train_iter_loss: 0.2455964982509613
train_iter_loss: 0.2648570239543915
train_iter_loss: 0.15679778158664703
train_iter_loss: 0.18902310729026794
train_iter_loss: 0.32446932792663574
train_iter_loss: 0.14683431386947632
train_iter_loss: 0.17805640399456024
train_iter_loss: 0.17491307854652405
train_iter_loss: 0.17594799399375916
train_iter_loss: 0.09345521032810211
train_iter_loss: 0.06252440065145493
train_iter_loss: 0.11121304333209991
train_iter_loss: 0.12323098629713058
train_iter_loss: 0.2753499746322632
train_iter_loss: 0.07105187326669693
train_iter_loss: 0.20847785472869873
train_iter_loss: 0.3067900836467743
train_iter_loss: 0.13013996183872223
train_iter_loss: 0.10993055254220963
train_iter_loss: 0.13947728276252747
train_iter_loss: 0.24229153990745544
train_iter_loss: 0.13756705820560455
train_iter_loss: 0.19239220023155212
train_iter_loss: 0.17361171543598175
train_iter_loss: 0.13722050189971924
train_iter_loss: 0.17790545523166656
train_iter_loss: 0.1435994803905487
train_iter_loss: 0.08426953852176666
train_iter_loss: 0.19609710574150085
train_iter_loss: 0.1835227608680725
train_iter_loss: 0.10058392584323883
train_iter_loss: 0.2279917299747467
train_iter_loss: 0.2617814242839813
train loss :0.1682
---------------------
Validation seg loss: 0.22096283131999508 at epoch 452
epoch =    453/  1000, exp = train
train_iter_loss: 0.13693509995937347
train_iter_loss: 0.13180260360240936
train_iter_loss: 0.11231175810098648
train_iter_loss: 0.18652033805847168
train_iter_loss: 0.1506769061088562
train_iter_loss: 0.40038764476776123
train_iter_loss: 0.16130569577217102
train_iter_loss: 0.1600823700428009
train_iter_loss: 0.2729048430919647
train_iter_loss: 0.10532025247812271
train_iter_loss: 0.2135276049375534
train_iter_loss: 0.1982591450214386
train_iter_loss: 0.12752170860767365
train_iter_loss: 0.12159933149814606
train_iter_loss: 0.19443385303020477
train_iter_loss: 0.11371094733476639
train_iter_loss: 0.08865451067686081
train_iter_loss: 0.16677998006343842
train_iter_loss: 0.12162570655345917
train_iter_loss: 0.14946015179157257
train_iter_loss: 0.08381311595439911
train_iter_loss: 0.17010952532291412
train_iter_loss: 0.2465580850839615
train_iter_loss: 0.08847598731517792
train_iter_loss: 0.03505122289061546
train_iter_loss: 0.060995977371931076
train_iter_loss: 0.21688932180404663
train_iter_loss: 0.27807244658470154
train_iter_loss: 0.23285070061683655
train_iter_loss: 0.15950259566307068
train_iter_loss: 0.18360844254493713
train_iter_loss: 0.15486009418964386
train_iter_loss: 0.15034249424934387
train_iter_loss: 0.16869543492794037
train_iter_loss: 0.14313681423664093
train_iter_loss: 0.14134691655635834
train_iter_loss: 0.14693963527679443
train_iter_loss: 0.1525225043296814
train_iter_loss: 0.15150678157806396
train_iter_loss: 0.12270710617303848
train_iter_loss: 0.20506221055984497
train_iter_loss: 0.23580172657966614
train_iter_loss: 0.1511356085538864
train_iter_loss: 0.13383601605892181
train_iter_loss: 0.15005123615264893
train_iter_loss: 0.15810883045196533
train_iter_loss: 0.22382795810699463
train_iter_loss: 0.14935612678527832
train_iter_loss: 0.15442073345184326
train_iter_loss: 0.19557341933250427
train_iter_loss: 0.11330690234899521
train_iter_loss: 0.15966172516345978
train_iter_loss: 0.1739557534456253
train_iter_loss: 0.10912210494279861
train_iter_loss: 0.20941710472106934
train_iter_loss: 0.1805431991815567
train_iter_loss: 0.14641641080379486
train_iter_loss: 0.1483529508113861
train_iter_loss: 0.12252362817525864
train_iter_loss: 0.13309890031814575
train_iter_loss: 0.10822074860334396
train_iter_loss: 0.11516918987035751
train_iter_loss: 0.29860687255859375
train_iter_loss: 0.1476915329694748
train_iter_loss: 0.1093878224492073
train_iter_loss: 0.12031549215316772
train_iter_loss: 0.181900292634964
train_iter_loss: 0.27271386981010437
train_iter_loss: 0.10902387648820877
train_iter_loss: 0.09510128200054169
train_iter_loss: 0.06630726903676987
train_iter_loss: 0.13212724030017853
train_iter_loss: 0.17720049619674683
train_iter_loss: 0.11065946519374847
train_iter_loss: 0.39416104555130005
train_iter_loss: 0.19650007784366608
train_iter_loss: 0.13401252031326294
train_iter_loss: 0.2130333036184311
train_iter_loss: 0.13836054503917694
train_iter_loss: 0.16659294068813324
train_iter_loss: 0.023773405700922012
train_iter_loss: 0.11103904992341995
train_iter_loss: 0.10993453860282898
train_iter_loss: 0.2644204795360565
train_iter_loss: 0.10381496697664261
train_iter_loss: 0.1362859159708023
train_iter_loss: 0.12763257324695587
train_iter_loss: 0.20998914539813995
train_iter_loss: 0.17787769436836243
train_iter_loss: 0.19535014033317566
train_iter_loss: 0.14627738296985626
train_iter_loss: 0.10214067250490189
train_iter_loss: 0.14834977686405182
train_iter_loss: 0.15911783277988434
train_iter_loss: 0.10872694104909897
train_iter_loss: 0.11856824159622192
train_iter_loss: 0.15095952153205872
train_iter_loss: 0.11565619707107544
train_iter_loss: 0.20494361221790314
train_iter_loss: 0.19630585610866547
train loss :0.1602
---------------------
Validation seg loss: 0.21903611942774281 at epoch 453
epoch =    454/  1000, exp = train
train_iter_loss: 0.2341100126504898
train_iter_loss: 0.10314828157424927
train_iter_loss: 0.13928881287574768
train_iter_loss: 0.27379414439201355
train_iter_loss: 0.08160753548145294
train_iter_loss: 0.1232198104262352
train_iter_loss: 0.1564955711364746
train_iter_loss: 0.10377806425094604
train_iter_loss: 0.06498779356479645
train_iter_loss: 0.15817515552043915
train_iter_loss: 0.2009793370962143
train_iter_loss: 0.11398302018642426
train_iter_loss: 0.1691092997789383
train_iter_loss: 0.16854900121688843
train_iter_loss: 0.2574060559272766
train_iter_loss: 0.3420521020889282
train_iter_loss: 0.18313609063625336
train_iter_loss: 0.23977433145046234
train_iter_loss: 0.1757914423942566
train_iter_loss: 0.04393288865685463
train_iter_loss: 0.2051549106836319
train_iter_loss: 0.10832373052835464
train_iter_loss: 0.1458299309015274
train_iter_loss: 0.12938424944877625
train_iter_loss: 0.19568589329719543
train_iter_loss: 0.27776387333869934
train_iter_loss: 0.35950401425361633
train_iter_loss: 0.13883309066295624
train_iter_loss: 0.13910159468650818
train_iter_loss: 0.2707362473011017
train_iter_loss: 0.17788438498973846
train_iter_loss: 0.13312667608261108
train_iter_loss: 0.37014710903167725
train_iter_loss: 0.14894671738147736
train_iter_loss: 0.2127326875925064
train_iter_loss: 0.22796784341335297
train_iter_loss: 0.1817900389432907
train_iter_loss: 0.10230587422847748
train_iter_loss: 0.1725156158208847
train_iter_loss: 0.04496809467673302
train_iter_loss: 0.08152512460947037
train_iter_loss: 0.13980500400066376
train_iter_loss: 0.06578345596790314
train_iter_loss: 0.07117162644863129
train_iter_loss: 0.17768006026744843
train_iter_loss: 0.23315075039863586
train_iter_loss: 0.1031198725104332
train_iter_loss: 0.18534024059772491
train_iter_loss: 0.17315594851970673
train_iter_loss: 0.1318349689245224
train_iter_loss: 0.1582276076078415
train_iter_loss: 0.2531949579715729
train_iter_loss: 0.10740523785352707
train_iter_loss: 0.14305883646011353
train_iter_loss: 0.2327268272638321
train_iter_loss: 0.23778584599494934
train_iter_loss: 0.078948475420475
train_iter_loss: 0.07120682299137115
train_iter_loss: 0.10560308396816254
train_iter_loss: 0.34303200244903564
train_iter_loss: 0.13154368102550507
train_iter_loss: 0.07668918371200562
train_iter_loss: 0.32411378622055054
train_iter_loss: 0.17125765979290009
train_iter_loss: 0.23831963539123535
train_iter_loss: 0.18396249413490295
train_iter_loss: 0.13292764127254486
train_iter_loss: 0.125925213098526
train_iter_loss: 0.1482461541891098
train_iter_loss: 0.11300286650657654
train_iter_loss: 0.20643551647663116
train_iter_loss: 0.11932413280010223
train_iter_loss: 0.23639817535877228
train_iter_loss: 0.21972037851810455
train_iter_loss: 0.2111138552427292
train_iter_loss: 0.11303786188364029
train_iter_loss: 0.1324189454317093
train_iter_loss: 0.1306961625814438
train_iter_loss: 0.3296723961830139
train_iter_loss: 0.13048994541168213
train_iter_loss: 0.17696470022201538
train_iter_loss: 0.14157623052597046
train_iter_loss: 0.12095821648836136
train_iter_loss: 0.2020871639251709
train_iter_loss: 0.16699902713298798
train_iter_loss: 0.34397026896476746
train_iter_loss: 0.12207581847906113
train_iter_loss: 0.2156437337398529
train_iter_loss: 0.12154696881771088
train_iter_loss: 0.17781151831150055
train_iter_loss: 0.13235053420066833
train_iter_loss: 0.0980643481016159
train_iter_loss: 0.18231362104415894
train_iter_loss: 0.1321628987789154
train_iter_loss: 0.08288493752479553
train_iter_loss: 0.12162670493125916
train_iter_loss: 0.26790863275527954
train_iter_loss: 0.13314761221408844
train_iter_loss: 0.10449966788291931
train_iter_loss: 0.054199717938899994
train loss :0.1690
---------------------
Validation seg loss: 0.22124600433305186 at epoch 454
epoch =    455/  1000, exp = train
train_iter_loss: 0.10135681182146072
train_iter_loss: 0.13367900252342224
train_iter_loss: 0.2243552803993225
train_iter_loss: 0.10806995630264282
train_iter_loss: 0.16306360065937042
train_iter_loss: 0.11565839499235153
train_iter_loss: 0.14459630846977234
train_iter_loss: 0.16880670189857483
train_iter_loss: 0.14404311776161194
train_iter_loss: 0.22884562611579895
train_iter_loss: 0.14551055431365967
train_iter_loss: 0.1389634609222412
train_iter_loss: 0.23402486741542816
train_iter_loss: 0.13893543183803558
train_iter_loss: 0.2431820034980774
train_iter_loss: 0.20159828662872314
train_iter_loss: 0.20979024469852448
train_iter_loss: 0.14661698043346405
train_iter_loss: 0.1412859410047531
train_iter_loss: 0.13035695254802704
train_iter_loss: 0.11883735656738281
train_iter_loss: 0.20599497854709625
train_iter_loss: 0.07836426794528961
train_iter_loss: 0.12263192981481552
train_iter_loss: 0.1370992511510849
train_iter_loss: 0.20935043692588806
train_iter_loss: 0.16221238672733307
train_iter_loss: 0.16347210109233856
train_iter_loss: 0.15822087228298187
train_iter_loss: 0.08098602294921875
train_iter_loss: 0.2705203890800476
train_iter_loss: 0.20080341398715973
train_iter_loss: 0.14173080027103424
train_iter_loss: 0.23451684415340424
train_iter_loss: 0.22440405189990997
train_iter_loss: 0.1577802300453186
train_iter_loss: 0.10235438495874405
train_iter_loss: 0.12471257895231247
train_iter_loss: 0.26493197679519653
train_iter_loss: 0.3549403250217438
train_iter_loss: 0.21299012005329132
train_iter_loss: 0.1403472125530243
train_iter_loss: 0.1527273952960968
train_iter_loss: 0.16071732342243195
train_iter_loss: 0.11353616416454315
train_iter_loss: 0.14706890285015106
train_iter_loss: 0.10101401060819626
train_iter_loss: 0.17086362838745117
train_iter_loss: 0.12173610925674438
train_iter_loss: 0.13259732723236084
train_iter_loss: 0.22094526886940002
train_iter_loss: 0.24264448881149292
train_iter_loss: 0.13086852431297302
train_iter_loss: 0.1269702911376953
train_iter_loss: 0.17763952910900116
train_iter_loss: 0.046079929918050766
train_iter_loss: 0.2033117115497589
train_iter_loss: 0.1638738214969635
train_iter_loss: 0.1390289068222046
train_iter_loss: 0.10623441636562347
train_iter_loss: 0.23379337787628174
train_iter_loss: 0.08349750936031342
train_iter_loss: 0.09971442073583603
train_iter_loss: 0.1406317949295044
train_iter_loss: 0.14340442419052124
train_iter_loss: 0.1492834985256195
train_iter_loss: 0.1559227705001831
train_iter_loss: 0.1485612392425537
train_iter_loss: 0.16788779199123383
train_iter_loss: 0.09537965059280396
train_iter_loss: 0.11873283982276917
train_iter_loss: 0.13226836919784546
train_iter_loss: 0.27539366483688354
train_iter_loss: 0.43944674730300903
train_iter_loss: 0.08205817639827728
train_iter_loss: 0.2910360097885132
train_iter_loss: 0.028584571555256844
train_iter_loss: 0.13529053330421448
train_iter_loss: 0.27612972259521484
train_iter_loss: 0.18674618005752563
train_iter_loss: 0.16374756395816803
train_iter_loss: 0.12156903743743896
train_iter_loss: 0.07375603169202805
train_iter_loss: 0.14237190783023834
train_iter_loss: 0.09664367139339447
train_iter_loss: 0.0851452425122261
train_iter_loss: 0.05594024807214737
train_iter_loss: 0.09500470012426376
train_iter_loss: 0.19110579788684845
train_iter_loss: 0.17320995032787323
train_iter_loss: 0.11151966452598572
train_iter_loss: 0.23095005750656128
train_iter_loss: 0.11326146870851517
train_iter_loss: 0.07937457412481308
train_iter_loss: 0.15324446558952332
train_iter_loss: 0.17855890095233917
train_iter_loss: 0.14048656821250916
train_iter_loss: 0.06686324626207352
train_iter_loss: 0.11226242035627365
train_iter_loss: 0.17896559834480286
train loss :0.1590
---------------------
Validation seg loss: 0.2194395899456346 at epoch 455
epoch =    456/  1000, exp = train
train_iter_loss: 0.28663602471351624
train_iter_loss: 0.18418872356414795
train_iter_loss: 0.08282166719436646
train_iter_loss: 0.17678505182266235
train_iter_loss: 0.1718762218952179
train_iter_loss: 0.10883792489767075
train_iter_loss: 0.11196038126945496
train_iter_loss: 0.12506672739982605
train_iter_loss: 0.16763991117477417
train_iter_loss: 0.055884115397930145
train_iter_loss: 0.14101451635360718
train_iter_loss: 0.1553676277399063
train_iter_loss: 0.15704290568828583
train_iter_loss: 0.23487837612628937
train_iter_loss: 0.11534254997968674
train_iter_loss: 0.15129147469997406
train_iter_loss: 0.397644966840744
train_iter_loss: 0.2039346694946289
train_iter_loss: 0.19347983598709106
train_iter_loss: 0.05899331718683243
train_iter_loss: 0.14300665259361267
train_iter_loss: 0.17761872708797455
train_iter_loss: 0.08044591546058655
train_iter_loss: 0.19578561186790466
train_iter_loss: 0.19572341442108154
train_iter_loss: 0.12770666182041168
train_iter_loss: 0.12053164094686508
train_iter_loss: 0.16615965962409973
train_iter_loss: 0.11239297688007355
train_iter_loss: 0.15398651361465454
train_iter_loss: 0.14566469192504883
train_iter_loss: 0.1060432642698288
train_iter_loss: 0.175021693110466
train_iter_loss: 0.1673373281955719
train_iter_loss: 0.14311599731445312
train_iter_loss: 0.15942266583442688
train_iter_loss: 0.03515378385782242
train_iter_loss: 0.15826447308063507
train_iter_loss: 0.17236116528511047
train_iter_loss: 0.1559368371963501
train_iter_loss: 0.14789770543575287
train_iter_loss: 0.09377116709947586
train_iter_loss: 0.15699584782123566
train_iter_loss: 0.10958853363990784
train_iter_loss: 0.1829163134098053
train_iter_loss: 0.12805670499801636
train_iter_loss: 0.25529009103775024
train_iter_loss: 0.11726632714271545
train_iter_loss: 0.10016191005706787
train_iter_loss: 0.10913869738578796
train_iter_loss: 0.11271596699953079
train_iter_loss: 0.13052226603031158
train_iter_loss: 0.19097542762756348
train_iter_loss: 0.23175835609436035
train_iter_loss: 0.16121526062488556
train_iter_loss: 0.1565355807542801
train_iter_loss: 0.16474288702011108
train_iter_loss: 0.1909310668706894
train_iter_loss: 0.14734362065792084
train_iter_loss: 0.07031435519456863
train_iter_loss: 0.09602268040180206
train_iter_loss: 0.06339610368013382
train_iter_loss: 0.19535072147846222
train_iter_loss: 0.20061281323432922
train_iter_loss: 0.27177533507347107
train_iter_loss: 0.1339382380247116
train_iter_loss: 0.13465256989002228
train_iter_loss: 0.16464020311832428
train_iter_loss: 0.29287779331207275
train_iter_loss: 0.12924465537071228
train_iter_loss: 0.296428382396698
train_iter_loss: 0.18064111471176147
train_iter_loss: 0.05747010558843613
train_iter_loss: 0.1443842649459839
train_iter_loss: 0.08630163967609406
train_iter_loss: 0.3497309982776642
train_iter_loss: 0.17967799305915833
train_iter_loss: 0.2526231110095978
train_iter_loss: 0.14702923595905304
train_iter_loss: 0.1752050668001175
train_iter_loss: 0.15496446192264557
train_iter_loss: 0.1979091316461563
train_iter_loss: 0.14889992773532867
train_iter_loss: 0.1665298193693161
train_iter_loss: 0.1925015151500702
train_iter_loss: 0.170106440782547
train_iter_loss: 0.11017201840877533
train_iter_loss: 0.19936935603618622
train_iter_loss: 0.19322125613689423
train_iter_loss: 0.21461474895477295
train_iter_loss: 0.14816510677337646
train_iter_loss: 0.04342711716890335
train_iter_loss: 0.13774263858795166
train_iter_loss: 0.23414309322834015
train_iter_loss: 0.09610142558813095
train_iter_loss: 0.18307672441005707
train_iter_loss: 0.10193566232919693
train_iter_loss: 0.3226297199726105
train_iter_loss: 0.17223519086837769
train_iter_loss: 0.10834426432847977
train loss :0.1614
---------------------
Validation seg loss: 0.21833568180577373 at epoch 456
epoch =    457/  1000, exp = train
train_iter_loss: 0.09207329899072647
train_iter_loss: 0.1811223030090332
train_iter_loss: 0.1929335594177246
train_iter_loss: 0.15261420607566833
train_iter_loss: 0.1348874866962433
train_iter_loss: 0.13792657852172852
train_iter_loss: 0.06002846360206604
train_iter_loss: 0.1144242063164711
train_iter_loss: 0.12818913161754608
train_iter_loss: 0.1817859411239624
train_iter_loss: 0.15025347471237183
train_iter_loss: 0.18481537699699402
train_iter_loss: 0.1358945518732071
train_iter_loss: 0.21533243358135223
train_iter_loss: 0.21714751422405243
train_iter_loss: 0.13469107449054718
train_iter_loss: 0.18349912762641907
train_iter_loss: 0.08263210952281952
train_iter_loss: 0.20148421823978424
train_iter_loss: 0.16107238829135895
train_iter_loss: 0.1611204892396927
train_iter_loss: 0.2788824439048767
train_iter_loss: 0.1991468220949173
train_iter_loss: 0.10771086812019348
train_iter_loss: 0.10538198798894882
train_iter_loss: 0.1495596319437027
train_iter_loss: 0.19103941321372986
train_iter_loss: 0.11937328428030014
train_iter_loss: 0.12718573212623596
train_iter_loss: 0.21121658384799957
train_iter_loss: 0.1626974493265152
train_iter_loss: 0.17834632098674774
train_iter_loss: 0.07123695313930511
train_iter_loss: 0.14969566464424133
train_iter_loss: 0.13152258098125458
train_iter_loss: 0.12431180477142334
train_iter_loss: 0.1721474528312683
train_iter_loss: 0.10223830491304398
train_iter_loss: 0.11097586154937744
train_iter_loss: 0.11333241313695908
train_iter_loss: 0.2280522733926773
train_iter_loss: 0.17645953595638275
train_iter_loss: 0.12669877707958221
train_iter_loss: 0.14069430530071259
train_iter_loss: 0.13850101828575134
train_iter_loss: 0.18360485136508942
train_iter_loss: 0.14155162870883942
train_iter_loss: 0.22708863019943237
train_iter_loss: 0.36805975437164307
train_iter_loss: 0.28356480598449707
train_iter_loss: 0.21834112703800201
train_iter_loss: 0.1183527261018753
train_iter_loss: 0.07887686043977737
train_iter_loss: 0.2383042424917221
train_iter_loss: 0.18166418373584747
train_iter_loss: 0.14883504807949066
train_iter_loss: 0.1257357895374298
train_iter_loss: 0.11016975343227386
train_iter_loss: 0.17142833769321442
train_iter_loss: 0.1685519963502884
train_iter_loss: 0.16954351961612701
train_iter_loss: 0.09870452433824539
train_iter_loss: 0.16969150304794312
train_iter_loss: 0.18007658421993256
train_iter_loss: 0.14711809158325195
train_iter_loss: 0.17393839359283447
train_iter_loss: 0.09640923887491226
train_iter_loss: 0.15026435256004333
train_iter_loss: 0.16609226167201996
train_iter_loss: 0.20112495124340057
train_iter_loss: 0.10509344190359116
train_iter_loss: 0.18263904750347137
train_iter_loss: 0.16293276846408844
train_iter_loss: 0.2019023448228836
train_iter_loss: 0.1745002716779709
train_iter_loss: 0.23173139989376068
train_iter_loss: 0.19200663268566132
train_iter_loss: 0.11447302252054214
train_iter_loss: 0.1766500025987625
train_iter_loss: 0.22033078968524933
train_iter_loss: 0.1538672000169754
train_iter_loss: 0.14506030082702637
train_iter_loss: 0.27679476141929626
train_iter_loss: 0.35169440507888794
train_iter_loss: 0.1879536658525467
train_iter_loss: 0.1726246178150177
train_iter_loss: 0.23248058557510376
train_iter_loss: 0.18135018646717072
train_iter_loss: 0.17230790853500366
train_iter_loss: 0.16383960843086243
train_iter_loss: 0.13578663766384125
train_iter_loss: 0.15483377873897552
train_iter_loss: 0.11551500856876373
train_iter_loss: 0.12744800746440887
train_iter_loss: 0.1367439478635788
train_iter_loss: 0.11264833062887192
train_iter_loss: 0.06646108627319336
train_iter_loss: 0.15042369067668915
train_iter_loss: 0.18344822525978088
train_iter_loss: 0.0893203392624855
train loss :0.1637
---------------------
Validation seg loss: 0.21925393747657818 at epoch 457
epoch =    458/  1000, exp = train
train_iter_loss: 0.12082913517951965
train_iter_loss: 0.1333894580602646
train_iter_loss: 0.14153538644313812
train_iter_loss: 0.13163387775421143
train_iter_loss: 0.17022447288036346
train_iter_loss: 0.16321811079978943
train_iter_loss: 0.17671169340610504
train_iter_loss: 0.2432381510734558
train_iter_loss: 0.26173678040504456
train_iter_loss: 0.09489548206329346
train_iter_loss: 0.10877226293087006
train_iter_loss: 0.1418001502752304
train_iter_loss: 0.08000696450471878
train_iter_loss: 0.12012150138616562
train_iter_loss: 0.10862657427787781
train_iter_loss: 0.09580400586128235
train_iter_loss: 0.13707852363586426
train_iter_loss: 0.10826234519481659
train_iter_loss: 0.09025963395833969
train_iter_loss: 0.10184360295534134
train_iter_loss: 0.18031539022922516
train_iter_loss: 0.18378396332263947
train_iter_loss: 0.12294754385948181
train_iter_loss: 0.1761944442987442
train_iter_loss: 0.14916065335273743
train_iter_loss: 0.12225224822759628
train_iter_loss: 0.25738319754600525
train_iter_loss: 0.33511170744895935
train_iter_loss: 0.12327071279287338
train_iter_loss: 0.23534727096557617
train_iter_loss: 0.18190200626850128
train_iter_loss: 0.2368095964193344
train_iter_loss: 0.11787506192922592
train_iter_loss: 0.2018890529870987
train_iter_loss: 0.16465994715690613
train_iter_loss: 0.22028686106204987
train_iter_loss: 0.13742293417453766
train_iter_loss: 0.2365420013666153
train_iter_loss: 0.17214538156986237
train_iter_loss: 0.27085983753204346
train_iter_loss: 0.04518111050128937
train_iter_loss: 0.13107575476169586
train_iter_loss: 0.08664919435977936
train_iter_loss: 0.12864013016223907
train_iter_loss: 0.12303899228572845
train_iter_loss: 0.14747841656208038
train_iter_loss: 0.1445421278476715
train_iter_loss: 0.2141997069120407
train_iter_loss: 0.2628399133682251
train_iter_loss: 0.07521959394216537
train_iter_loss: 0.13475367426872253
train_iter_loss: 0.18010392785072327
train_iter_loss: 0.11358533799648285
train_iter_loss: 0.11376003921031952
train_iter_loss: 0.2018720805644989
train_iter_loss: 0.10569647699594498
train_iter_loss: 0.1779364049434662
train_iter_loss: 0.16919369995594025
train_iter_loss: 0.19767236709594727
train_iter_loss: 0.19782204926013947
train_iter_loss: 0.14180590212345123
train_iter_loss: 0.22405172884464264
train_iter_loss: 0.06644249707460403
train_iter_loss: 0.2669543921947479
train_iter_loss: 0.3916374444961548
train_iter_loss: 0.13299821317195892
train_iter_loss: 0.1682640165090561
train_iter_loss: 0.08129995316267014
train_iter_loss: 0.31456977128982544
train_iter_loss: 0.12450222671031952
train_iter_loss: 0.11402596533298492
train_iter_loss: 0.04932698607444763
train_iter_loss: 0.18901769816875458
train_iter_loss: 0.25848808884620667
train_iter_loss: 0.2231706976890564
train_iter_loss: 0.16202664375305176
train_iter_loss: 0.20105130970478058
train_iter_loss: 0.19996297359466553
train_iter_loss: 0.12174268066883087
train_iter_loss: 0.11580080538988113
train_iter_loss: 0.1107444018125534
train_iter_loss: 0.10205497592687607
train_iter_loss: 0.16960373520851135
train_iter_loss: 0.0862894132733345
train_iter_loss: 0.11716413497924805
train_iter_loss: 0.23499123752117157
train_iter_loss: 0.10980024188756943
train_iter_loss: 0.13026069104671478
train_iter_loss: 0.20253288745880127
train_iter_loss: 0.15359072387218475
train_iter_loss: 0.04812014102935791
train_iter_loss: 0.11861797422170639
train_iter_loss: 0.20550547540187836
train_iter_loss: 0.35452184081077576
train_iter_loss: 0.12158314883708954
train_iter_loss: 0.06174643337726593
train_iter_loss: 0.12049827724695206
train_iter_loss: 0.16201676428318024
train_iter_loss: 0.15253379940986633
train_iter_loss: 0.12332462519407272
train loss :0.1610
---------------------
Validation seg loss: 0.21923880634021084 at epoch 458
epoch =    459/  1000, exp = train
train_iter_loss: 0.1395200788974762
train_iter_loss: 0.0774141252040863
train_iter_loss: 0.17990341782569885
train_iter_loss: 0.12709484994411469
train_iter_loss: 0.2738701105117798
train_iter_loss: 0.16821065545082092
train_iter_loss: 0.1920132339000702
train_iter_loss: 0.10020715743303299
train_iter_loss: 0.19985929131507874
train_iter_loss: 0.1447342485189438
train_iter_loss: 0.1314941942691803
train_iter_loss: 0.18744805455207825
train_iter_loss: 0.1235603392124176
train_iter_loss: 0.1018141508102417
train_iter_loss: 0.11780910938978195
train_iter_loss: 0.19192256033420563
train_iter_loss: 0.1938365250825882
train_iter_loss: 0.16536369919776917
train_iter_loss: 0.2970377802848816
train_iter_loss: 0.19075629115104675
train_iter_loss: 0.21071267127990723
train_iter_loss: 0.14946381747722626
train_iter_loss: 0.15368184447288513
train_iter_loss: 0.1856967806816101
train_iter_loss: 0.18809117376804352
train_iter_loss: 0.25499391555786133
train_iter_loss: 0.12627628445625305
train_iter_loss: 0.11560120433568954
train_iter_loss: 0.17980079352855682
train_iter_loss: 0.1319042295217514
train_iter_loss: 0.16886551678180695
train_iter_loss: 0.13971348106861115
train_iter_loss: 0.1569192260503769
train_iter_loss: 0.2707195281982422
train_iter_loss: 0.25808799266815186
train_iter_loss: 0.09829845279455185
train_iter_loss: 0.12281814962625504
train_iter_loss: 0.10831018537282944
train_iter_loss: 0.15218962728977203
train_iter_loss: 0.12356321513652802
train_iter_loss: 0.17324668169021606
train_iter_loss: 0.11150774359703064
train_iter_loss: 0.24317336082458496
train_iter_loss: 0.15857364237308502
train_iter_loss: 0.1595466434955597
train_iter_loss: 0.12146396934986115
train_iter_loss: 0.15821953117847443
train_iter_loss: 0.13400983810424805
train_iter_loss: 0.08408602327108383
train_iter_loss: 0.147866353392601
train_iter_loss: 0.2173086255788803
train_iter_loss: 0.1341223269701004
train_iter_loss: 0.13975372910499573
train_iter_loss: 0.13919290900230408
train_iter_loss: 0.1326180398464203
train_iter_loss: 0.14895091950893402
train_iter_loss: 0.18407206237316132
train_iter_loss: 0.05429569631814957
train_iter_loss: 0.14307180047035217
train_iter_loss: 0.10790900886058807
train_iter_loss: 0.2020498812198639
train_iter_loss: 0.10901781916618347
train_iter_loss: 0.12656985223293304
train_iter_loss: 0.14838989078998566
train_iter_loss: 0.08730363100767136
train_iter_loss: 0.23958492279052734
train_iter_loss: 0.3158304691314697
train_iter_loss: 0.1791943609714508
train_iter_loss: 0.28944599628448486
train_iter_loss: 0.16151557862758636
train_iter_loss: 0.08204154670238495
train_iter_loss: 0.06315626204013824
train_iter_loss: 0.08661337941884995
train_iter_loss: 0.15781782567501068
train_iter_loss: 0.220393106341362
train_iter_loss: 0.12204994261264801
train_iter_loss: 0.12848231196403503
train_iter_loss: 0.2291036993265152
train_iter_loss: 0.09636577218770981
train_iter_loss: 0.14690040051937103
train_iter_loss: 0.17963124811649323
train_iter_loss: 0.07840904593467712
train_iter_loss: 0.22996604442596436
train_iter_loss: 0.10511607676744461
train_iter_loss: 0.13210715353488922
train_iter_loss: 0.14896708726882935
train_iter_loss: 0.25911641120910645
train_iter_loss: 0.12507419288158417
train_iter_loss: 0.17781133949756622
train_iter_loss: 0.21712172031402588
train_iter_loss: 0.22127050161361694
train_iter_loss: 0.0810803472995758
train_iter_loss: 0.08010096102952957
train_iter_loss: 0.24851931631565094
train_iter_loss: 0.1356734186410904
train_iter_loss: 0.37598916888237
train_iter_loss: 0.09327034652233124
train_iter_loss: 0.2003701627254486
train_iter_loss: 0.10705624520778656
train_iter_loss: 0.15712617337703705
train loss :0.1620
---------------------
Validation seg loss: 0.21693300729055168 at epoch 459
epoch =    460/  1000, exp = train
train_iter_loss: 0.10244584828615189
train_iter_loss: 0.2716207802295685
train_iter_loss: 0.11401587724685669
train_iter_loss: 0.15512625873088837
train_iter_loss: 0.207499161362648
train_iter_loss: 0.2466486394405365
train_iter_loss: 0.20484229922294617
train_iter_loss: 0.0608210526406765
train_iter_loss: 0.1054239496588707
train_iter_loss: 0.16289295256137848
train_iter_loss: 0.1307983249425888
train_iter_loss: 0.15630356967449188
train_iter_loss: 0.17715832591056824
train_iter_loss: 0.12980912625789642
train_iter_loss: 0.20066072046756744
train_iter_loss: 0.1794758439064026
train_iter_loss: 0.1028015986084938
train_iter_loss: 0.20983339846134186
train_iter_loss: 0.2215624749660492
train_iter_loss: 0.31560972332954407
train_iter_loss: 0.1562614142894745
train_iter_loss: 0.19085609912872314
train_iter_loss: 0.2054762840270996
train_iter_loss: 0.188320592045784
train_iter_loss: 0.1471177339553833
train_iter_loss: 0.12422356754541397
train_iter_loss: 0.1164499819278717
train_iter_loss: 0.11171041429042816
train_iter_loss: 0.18686805665493011
train_iter_loss: 0.2823975384235382
train_iter_loss: 0.12238579243421555
train_iter_loss: 0.15027372539043427
train_iter_loss: 0.1685355305671692
train_iter_loss: 0.11014356464147568
train_iter_loss: 0.25209853053092957
train_iter_loss: 0.11612983793020248
train_iter_loss: 0.1710900217294693
train_iter_loss: 0.26131361722946167
train_iter_loss: 0.21174052357673645
train_iter_loss: 0.3200133442878723
train_iter_loss: 0.11751272529363632
train_iter_loss: 0.1718713939189911
train_iter_loss: 0.08610206842422485
train_iter_loss: 0.11679982393980026
train_iter_loss: 0.2116231769323349
train_iter_loss: 0.0739578828215599
train_iter_loss: 0.0856621265411377
train_iter_loss: 0.16941848397254944
train_iter_loss: 0.14575597643852234
train_iter_loss: 0.13227516412734985
train_iter_loss: 0.07540008425712585
train_iter_loss: 0.18210728466510773
train_iter_loss: 0.11333222687244415
train_iter_loss: 0.18695847690105438
train_iter_loss: 0.31576165556907654
train_iter_loss: 0.1928420513868332
train_iter_loss: 0.13841670751571655
train_iter_loss: 0.102045439183712
train_iter_loss: 0.1089964509010315
train_iter_loss: 0.04609635844826698
train_iter_loss: 0.10583095997571945
train_iter_loss: 0.11895733326673508
train_iter_loss: 0.2181321531534195
train_iter_loss: 0.10922752320766449
train_iter_loss: 0.1739104986190796
train_iter_loss: 0.31144773960113525
train_iter_loss: 0.04069194942712784
train_iter_loss: 0.20058460533618927
train_iter_loss: 0.20804359018802643
train_iter_loss: 0.09462933987379074
train_iter_loss: 0.20853987336158752
train_iter_loss: 0.17433050274848938
train_iter_loss: 0.11616681516170502
train_iter_loss: 0.25417113304138184
train_iter_loss: 0.18309973180294037
train_iter_loss: 0.16926386952400208
train_iter_loss: 0.2107355147600174
train_iter_loss: 0.17762771248817444
train_iter_loss: 0.10745832324028015
train_iter_loss: 0.09988876432180405
train_iter_loss: 0.18281978368759155
train_iter_loss: 0.1328045278787613
train_iter_loss: 0.16886942088603973
train_iter_loss: 0.175795316696167
train_iter_loss: 0.15360629558563232
train_iter_loss: 0.19042789936065674
train_iter_loss: 0.13867169618606567
train_iter_loss: 0.32806020975112915
train_iter_loss: 0.08437242358922958
train_iter_loss: 0.18012462556362152
train_iter_loss: 0.18896044790744781
train_iter_loss: 0.1089700311422348
train_iter_loss: 0.12131041288375854
train_iter_loss: 0.3345089554786682
train_iter_loss: 0.20884358882904053
train_iter_loss: 0.1486877202987671
train_iter_loss: 0.3443193733692169
train_iter_loss: 0.15217618644237518
train_iter_loss: 0.10020371526479721
train_iter_loss: 0.08541185408830643
train loss :0.1680
---------------------
Validation seg loss: 0.2213997702237289 at epoch 460
epoch =    461/  1000, exp = train
train_iter_loss: 0.28783339262008667
train_iter_loss: 0.1739031821489334
train_iter_loss: 0.2149006724357605
train_iter_loss: 0.1959168016910553
train_iter_loss: 0.25381913781166077
train_iter_loss: 0.24256394803524017
train_iter_loss: 0.1088545098900795
train_iter_loss: 0.1982259452342987
train_iter_loss: 0.11065088212490082
train_iter_loss: 0.14945967495441437
train_iter_loss: 0.09452689439058304
train_iter_loss: 0.08645302802324295
train_iter_loss: 0.1708936244249344
train_iter_loss: 0.24772287905216217
train_iter_loss: 0.1070854589343071
train_iter_loss: 0.052779801189899445
train_iter_loss: 0.14897139370441437
train_iter_loss: 0.22844694554805756
train_iter_loss: 0.2732193171977997
train_iter_loss: 0.1825670748949051
train_iter_loss: 0.08413412421941757
train_iter_loss: 0.2598138749599457
train_iter_loss: 0.20059336721897125
train_iter_loss: 0.19110776484012604
train_iter_loss: 0.07484033703804016
train_iter_loss: 0.1728663295507431
train_iter_loss: 0.119015172123909
train_iter_loss: 0.3543632924556732
train_iter_loss: 0.25981366634368896
train_iter_loss: 0.26878318190574646
train_iter_loss: 0.07108225673437119
train_iter_loss: 0.23959243297576904
train_iter_loss: 0.06694972515106201
train_iter_loss: 0.2349572628736496
train_iter_loss: 0.06802951544523239
train_iter_loss: 0.1355603188276291
train_iter_loss: 0.14463619887828827
train_iter_loss: 0.3857569694519043
train_iter_loss: 0.1744370013475418
train_iter_loss: 0.08337082713842392
train_iter_loss: 0.204245924949646
train_iter_loss: 0.10372191667556763
train_iter_loss: 0.19767320156097412
train_iter_loss: 0.10468798130750656
train_iter_loss: 0.04069582372903824
train_iter_loss: 0.14421997964382172
train_iter_loss: 0.21639622747898102
train_iter_loss: 0.2524827718734741
train_iter_loss: 0.11625803261995316
train_iter_loss: 0.1407236009836197
train_iter_loss: 0.1297370046377182
train_iter_loss: 0.10138352960348129
train_iter_loss: 0.1375342160463333
train_iter_loss: 0.1311097890138626
train_iter_loss: 0.09148484468460083
train_iter_loss: 0.11242495477199554
train_iter_loss: 0.09056844562292099
train_iter_loss: 0.11529859155416489
train_iter_loss: 0.11948000639677048
train_iter_loss: 0.304812490940094
train_iter_loss: 0.2813694477081299
train_iter_loss: 0.1431521624326706
train_iter_loss: 0.1899721622467041
train_iter_loss: 0.19077564775943756
train_iter_loss: 0.08110256493091583
train_iter_loss: 0.09484850615262985
train_iter_loss: 0.09124594926834106
train_iter_loss: 0.044144440442323685
train_iter_loss: 0.15780898928642273
train_iter_loss: 0.13457641005516052
train_iter_loss: 0.15758591890335083
train_iter_loss: 0.10366469621658325
train_iter_loss: 0.11184854060411453
train_iter_loss: 0.07493139803409576
train_iter_loss: 0.11402161419391632
train_iter_loss: 0.12024150043725967
train_iter_loss: 0.12382401525974274
train_iter_loss: 0.18274138867855072
train_iter_loss: 0.14895035326480865
train_iter_loss: 0.18870322406291962
train_iter_loss: 0.17402112483978271
train_iter_loss: 0.1500813364982605
train_iter_loss: 0.113081194460392
train_iter_loss: 0.16730131208896637
train_iter_loss: 0.06772290170192719
train_iter_loss: 0.22983726859092712
train_iter_loss: 0.194901242852211
train_iter_loss: 0.16885314881801605
train_iter_loss: 0.1631830930709839
train_iter_loss: 0.24288547039031982
train_iter_loss: 0.19304564595222473
train_iter_loss: 0.1767365038394928
train_iter_loss: 0.06939896196126938
train_iter_loss: 0.27556559443473816
train_iter_loss: 0.11742942035198212
train_iter_loss: 0.23563848435878754
train_iter_loss: 0.21171769499778748
train_iter_loss: 0.15035554766654968
train_iter_loss: 0.1545754224061966
train_iter_loss: 0.3276805579662323
train loss :0.1646
---------------------
Validation seg loss: 0.21960965382321826 at epoch 461
epoch =    462/  1000, exp = train
train_iter_loss: 0.05036542937159538
train_iter_loss: 0.048233017325401306
train_iter_loss: 0.24935178458690643
train_iter_loss: 0.13446791470050812
train_iter_loss: 0.16830581426620483
train_iter_loss: 0.13399964570999146
train_iter_loss: 0.0828232392668724
train_iter_loss: 0.1868705153465271
train_iter_loss: 0.113157719373703
train_iter_loss: 0.11227768659591675
train_iter_loss: 0.22438587248325348
train_iter_loss: 0.10333096235990524
train_iter_loss: 0.22502648830413818
train_iter_loss: 0.11733359098434448
train_iter_loss: 0.1404731124639511
train_iter_loss: 0.16153328120708466
train_iter_loss: 0.1144837960600853
train_iter_loss: 0.1494223028421402
train_iter_loss: 0.15033037960529327
train_iter_loss: 0.12223654240369797
train_iter_loss: 0.08347424119710922
train_iter_loss: 0.2173551768064499
train_iter_loss: 0.15407107770442963
train_iter_loss: 0.14837859570980072
train_iter_loss: 0.13045717775821686
train_iter_loss: 0.12518349289894104
train_iter_loss: 0.13794708251953125
train_iter_loss: 0.15396729111671448
train_iter_loss: 0.11477869004011154
train_iter_loss: 0.28597381711006165
train_iter_loss: 0.15717092156410217
train_iter_loss: 0.07201360911130905
train_iter_loss: 0.25094884634017944
train_iter_loss: 0.08610112220048904
train_iter_loss: 0.2746582329273224
train_iter_loss: 0.3157956898212433
train_iter_loss: 0.19783122837543488
train_iter_loss: 0.14060144126415253
train_iter_loss: 0.11345674097537994
train_iter_loss: 0.09903575479984283
train_iter_loss: 0.1585574746131897
train_iter_loss: 0.12628157436847687
train_iter_loss: 0.3017948567867279
train_iter_loss: 0.09133661538362503
train_iter_loss: 0.09151141345500946
train_iter_loss: 0.08121304959058762
train_iter_loss: 0.33223748207092285
train_iter_loss: 0.16230060160160065
train_iter_loss: 0.15086892247200012
train_iter_loss: 0.22292454540729523
train_iter_loss: 0.17738550901412964
train_iter_loss: 0.3740558326244354
train_iter_loss: 0.0921347364783287
train_iter_loss: 0.22945959866046906
train_iter_loss: 0.14950695633888245
train_iter_loss: 0.21476112306118011
train_iter_loss: 0.14797572791576385
train_iter_loss: 0.19841434061527252
train_iter_loss: 0.06325556337833405
train_iter_loss: 0.4275974929332733
train_iter_loss: 0.10954271256923676
train_iter_loss: 0.11741284281015396
train_iter_loss: 0.14708925783634186
train_iter_loss: 0.12450535595417023
train_iter_loss: 0.18155613541603088
train_iter_loss: 0.30945342779159546
train_iter_loss: 0.2001182585954666
train_iter_loss: 0.1492626667022705
train_iter_loss: 0.16576261818408966
train_iter_loss: 0.15872296690940857
train_iter_loss: 0.14462268352508545
train_iter_loss: 0.11630778759717941
train_iter_loss: 0.07887163013219833
train_iter_loss: 0.06863489001989365
train_iter_loss: 0.27289655804634094
train_iter_loss: 0.17272180318832397
train_iter_loss: 0.1661534607410431
train_iter_loss: 0.2401028722524643
train_iter_loss: 0.05314088240265846
train_iter_loss: 0.17232534289360046
train_iter_loss: 0.1559598594903946
train_iter_loss: 0.13897550106048584
train_iter_loss: 0.09282546490430832
train_iter_loss: 0.15806758403778076
train_iter_loss: 0.17768990993499756
train_iter_loss: 0.2414306104183197
train_iter_loss: 0.11646228283643723
train_iter_loss: 0.16547396779060364
train_iter_loss: 0.20874778926372528
train_iter_loss: 0.14939874410629272
train_iter_loss: 0.22640188038349152
train_iter_loss: 0.10228273272514343
train_iter_loss: 0.14936953783035278
train_iter_loss: 0.13106878101825714
train_iter_loss: 0.11501723527908325
train_iter_loss: 0.20083017647266388
train_iter_loss: 0.16896770894527435
train_iter_loss: 0.15212570130825043
train_iter_loss: 0.22257888317108154
train_iter_loss: 0.09991563856601715
train loss :0.1633
---------------------
Validation seg loss: 0.2191604130883824 at epoch 462
epoch =    463/  1000, exp = train
train_iter_loss: 0.1945558488368988
train_iter_loss: 0.2136864811182022
train_iter_loss: 0.1565750390291214
train_iter_loss: 0.13346578180789948
train_iter_loss: 0.1582963466644287
train_iter_loss: 0.1458483785390854
train_iter_loss: 0.18997229635715485
train_iter_loss: 0.14848804473876953
train_iter_loss: 0.17277933657169342
train_iter_loss: 0.08891058713197708
train_iter_loss: 0.1432206928730011
train_iter_loss: 0.18711955845355988
train_iter_loss: 0.2373402863740921
train_iter_loss: 0.1601269394159317
train_iter_loss: 0.19352827966213226
train_iter_loss: 0.16545459628105164
train_iter_loss: 0.22807469964027405
train_iter_loss: 0.04338471591472626
train_iter_loss: 0.16464528441429138
train_iter_loss: 0.16532672941684723
train_iter_loss: 0.15785518288612366
train_iter_loss: 0.13250786066055298
train_iter_loss: 0.12247653305530548
train_iter_loss: 0.1703593134880066
train_iter_loss: 0.08399094641208649
train_iter_loss: 0.1172151044011116
train_iter_loss: 0.12339837849140167
train_iter_loss: 0.17504668235778809
train_iter_loss: 0.07618417590856552
train_iter_loss: 0.11163552850484848
train_iter_loss: 0.06561928242444992
train_iter_loss: 0.22123655676841736
train_iter_loss: 0.14577263593673706
train_iter_loss: 0.2652907967567444
train_iter_loss: 0.19268697500228882
train_iter_loss: 0.15301021933555603
train_iter_loss: 0.2865805923938751
train_iter_loss: 0.1897350698709488
train_iter_loss: 0.09104257076978683
train_iter_loss: 0.11314240097999573
train_iter_loss: 0.08293191343545914
train_iter_loss: 0.24653440713882446
train_iter_loss: 0.1897158920764923
train_iter_loss: 0.17543767392635345
train_iter_loss: 0.06935068964958191
train_iter_loss: 0.11538984626531601
train_iter_loss: 0.0948479026556015
train_iter_loss: 0.11480148881673813
train_iter_loss: 0.34795331954956055
train_iter_loss: 0.31022152304649353
train_iter_loss: 0.14169716835021973
train_iter_loss: 0.151221364736557
train_iter_loss: 0.2347876876592636
train_iter_loss: 0.19027888774871826
train_iter_loss: 0.21763433516025543
train_iter_loss: 0.2077886015176773
train_iter_loss: 0.10257910937070847
train_iter_loss: 0.20274122059345245
train_iter_loss: 0.17552907764911652
train_iter_loss: 0.22789950668811798
train_iter_loss: 0.19097086787223816
train_iter_loss: 0.06826527416706085
train_iter_loss: 0.050618160516023636
train_iter_loss: 0.10901150107383728
train_iter_loss: 0.1405147761106491
train_iter_loss: 0.10898802429437637
train_iter_loss: 0.1947159320116043
train_iter_loss: 0.17417210340499878
train_iter_loss: 0.27713513374328613
train_iter_loss: 0.17784252762794495
train_iter_loss: 0.16925716400146484
train_iter_loss: 0.12893053889274597
train_iter_loss: 0.10783802717924118
train_iter_loss: 0.1874786913394928
train_iter_loss: 0.10675495862960815
train_iter_loss: 0.14494307339191437
train_iter_loss: 0.09019188582897186
train_iter_loss: 0.3863816261291504
train_iter_loss: 0.13264906406402588
train_iter_loss: 0.21876104176044464
train_iter_loss: 0.3239150941371918
train_iter_loss: 0.10846249014139175
train_iter_loss: 0.1983659565448761
train_iter_loss: 0.1059027910232544
train_iter_loss: 0.09227257966995239
train_iter_loss: 0.24867863953113556
train_iter_loss: 0.08064452558755875
train_iter_loss: 0.17262957990169525
train_iter_loss: 0.1588272601366043
train_iter_loss: 0.08819331973791122
train_iter_loss: 0.24389392137527466
train_iter_loss: 0.14942173659801483
train_iter_loss: 0.13253740966320038
train_iter_loss: 0.14502786099910736
train_iter_loss: 0.22633051872253418
train_iter_loss: 0.2280648648738861
train_iter_loss: 0.158254474401474
train_iter_loss: 0.1978052705526352
train_iter_loss: 0.21192124485969543
train_iter_loss: 0.10610648989677429
train loss :0.1659
---------------------
Validation seg loss: 0.21941364444089387 at epoch 463
epoch =    464/  1000, exp = train
train_iter_loss: 0.1266385018825531
train_iter_loss: 0.091600202023983
train_iter_loss: 0.14561620354652405
train_iter_loss: 0.12765297293663025
train_iter_loss: 0.09249784052371979
train_iter_loss: 0.18445423245429993
train_iter_loss: 0.19690561294555664
train_iter_loss: 0.1816088855266571
train_iter_loss: 0.2589920461177826
train_iter_loss: 0.20815597474575043
train_iter_loss: 0.06560788303613663
train_iter_loss: 0.1308222860097885
train_iter_loss: 0.23374979197978973
train_iter_loss: 0.1829930543899536
train_iter_loss: 0.19776691496372223
train_iter_loss: 0.2458280622959137
train_iter_loss: 0.217790424823761
train_iter_loss: 0.10667534917593002
train_iter_loss: 0.2100561559200287
train_iter_loss: 0.0919998437166214
train_iter_loss: 0.14935052394866943
train_iter_loss: 0.09950531274080276
train_iter_loss: 0.12574553489685059
train_iter_loss: 0.16134120523929596
train_iter_loss: 0.17115426063537598
train_iter_loss: 0.20236913859844208
train_iter_loss: 0.10663444548845291
train_iter_loss: 0.02579239197075367
train_iter_loss: 0.11720253527164459
train_iter_loss: 0.31894993782043457
train_iter_loss: 0.1973448097705841
train_iter_loss: 0.16405633091926575
train_iter_loss: 0.1393183320760727
train_iter_loss: 0.15251082181930542
train_iter_loss: 0.1049920842051506
train_iter_loss: 0.112775057554245
train_iter_loss: 0.20871321856975555
train_iter_loss: 0.13451969623565674
train_iter_loss: 0.17016448080539703
train_iter_loss: 0.1170402467250824
train_iter_loss: 0.08530456572771072
train_iter_loss: 0.25501877069473267
train_iter_loss: 0.1643492430448532
train_iter_loss: 0.20945841073989868
train_iter_loss: 0.18910321593284607
train_iter_loss: 0.24578680098056793
train_iter_loss: 0.1438681185245514
train_iter_loss: 0.11214042454957962
train_iter_loss: 0.11239247769117355
train_iter_loss: 0.20954529941082
train_iter_loss: 0.11586745083332062
train_iter_loss: 0.24722474813461304
train_iter_loss: 0.1630701720714569
train_iter_loss: 0.17886793613433838
train_iter_loss: 0.20179587602615356
train_iter_loss: 0.19056464731693268
train_iter_loss: 0.29172033071517944
train_iter_loss: 0.1746499091386795
train_iter_loss: 0.07725779712200165
train_iter_loss: 0.07024887204170227
train_iter_loss: 0.17334526777267456
train_iter_loss: 0.14154592156410217
train_iter_loss: 0.22310945391654968
train_iter_loss: 0.19100084900856018
train_iter_loss: 0.0402582623064518
train_iter_loss: 0.1453358680009842
train_iter_loss: 0.13543812930583954
train_iter_loss: 0.16481417417526245
train_iter_loss: 0.08854618668556213
train_iter_loss: 0.1629248559474945
train_iter_loss: 0.16566891968250275
train_iter_loss: 0.20843183994293213
train_iter_loss: 0.09208907932043076
train_iter_loss: 0.0585305392742157
train_iter_loss: 0.15417471528053284
train_iter_loss: 0.23462246358394623
train_iter_loss: 0.14219655096530914
train_iter_loss: 0.13421550393104553
train_iter_loss: 0.18474504351615906
train_iter_loss: 0.18180523812770844
train_iter_loss: 0.13499687612056732
train_iter_loss: 0.3035804331302643
train_iter_loss: 0.11427509784698486
train_iter_loss: 0.1829633265733719
train_iter_loss: 0.06307544559240341
train_iter_loss: 0.24204912781715393
train_iter_loss: 0.14405155181884766
train_iter_loss: 0.12252306938171387
train_iter_loss: 0.30822914838790894
train_iter_loss: 0.13961967825889587
train_iter_loss: 0.16621734201908112
train_iter_loss: 0.15147629380226135
train_iter_loss: 0.12519054114818573
train_iter_loss: 0.1225130558013916
train_iter_loss: 0.13512727618217468
train_iter_loss: 0.20492155849933624
train_iter_loss: 0.1544020175933838
train_iter_loss: 0.09653287380933762
train_iter_loss: 0.17509132623672485
train_iter_loss: 0.20840254426002502
train loss :0.1620
---------------------
Validation seg loss: 0.22131438115787394 at epoch 464
epoch =    465/  1000, exp = train
train_iter_loss: 0.13636919856071472
train_iter_loss: 0.10849470645189285
train_iter_loss: 0.1165841743350029
train_iter_loss: 0.16890430450439453
train_iter_loss: 0.14999541640281677
train_iter_loss: 0.130250945687294
train_iter_loss: 0.10837439447641373
train_iter_loss: 0.11249613761901855
train_iter_loss: 0.11635173112154007
train_iter_loss: 0.2353927046060562
train_iter_loss: 0.11686860769987106
train_iter_loss: 0.09606015682220459
train_iter_loss: 0.15864618122577667
train_iter_loss: 0.033361516892910004
train_iter_loss: 0.15629880130290985
train_iter_loss: 0.1950857788324356
train_iter_loss: 0.31607669591903687
train_iter_loss: 0.1822332739830017
train_iter_loss: 0.22007742524147034
train_iter_loss: 0.20218822360038757
train_iter_loss: 0.09937593340873718
train_iter_loss: 0.14280003309249878
train_iter_loss: 0.07406439632177353
train_iter_loss: 0.16468064486980438
train_iter_loss: 0.16058418154716492
train_iter_loss: 0.18066081404685974
train_iter_loss: 0.17107953131198883
train_iter_loss: 0.4199030101299286
train_iter_loss: 0.17335566878318787
train_iter_loss: 0.13823331892490387
train_iter_loss: 0.167106494307518
train_iter_loss: 0.15796849131584167
train_iter_loss: 0.11454857885837555
train_iter_loss: 0.14381282031536102
train_iter_loss: 0.2626495957374573
train_iter_loss: 0.15473851561546326
train_iter_loss: 0.15641960501670837
train_iter_loss: 0.14459943771362305
train_iter_loss: 0.17112046480178833
train_iter_loss: 0.14090432226657867
train_iter_loss: 0.19774338603019714
train_iter_loss: 0.23900458216667175
train_iter_loss: 0.09520484507083893
train_iter_loss: 0.12587936222553253
train_iter_loss: 0.04563717544078827
train_iter_loss: 0.060812752693891525
train_iter_loss: 0.18660062551498413
train_iter_loss: 0.17222969233989716
train_iter_loss: 0.18262366950511932
train_iter_loss: 0.14622825384140015
train_iter_loss: 0.20837007462978363
train_iter_loss: 0.17953170835971832
train_iter_loss: 0.4640738368034363
train_iter_loss: 0.07148782163858414
train_iter_loss: 0.2099928855895996
train_iter_loss: 0.14598610997200012
train_iter_loss: 0.16602222621440887
train_iter_loss: 0.18478305637836456
train_iter_loss: 0.19690896570682526
train_iter_loss: 0.1615683138370514
train_iter_loss: 0.12285353243350983
train_iter_loss: 0.09349299222230911
train_iter_loss: 0.09356639534235
train_iter_loss: 0.20831561088562012
train_iter_loss: 0.16227515041828156
train_iter_loss: 0.21486780047416687
train_iter_loss: 0.12665897607803345
train_iter_loss: 0.1408548355102539
train_iter_loss: 0.09166687726974487
train_iter_loss: 0.24800392985343933
train_iter_loss: 0.29775533080101013
train_iter_loss: 0.17356280982494354
train_iter_loss: 0.21315214037895203
train_iter_loss: 0.25832706689834595
train_iter_loss: 0.11543304473161697
train_iter_loss: 0.08103255927562714
train_iter_loss: 0.08382640779018402
train_iter_loss: 0.1951642483472824
train_iter_loss: 0.10976462811231613
train_iter_loss: 0.09020033478736877
train_iter_loss: 0.15458282828330994
train_iter_loss: 0.09727784991264343
train_iter_loss: 0.3305666148662567
train_iter_loss: 0.18138118088245392
train_iter_loss: 0.11490371823310852
train_iter_loss: 0.2034250944852829
train_iter_loss: 0.11089225113391876
train_iter_loss: 0.1222257986664772
train_iter_loss: 0.15169979631900787
train_iter_loss: 0.2533123195171356
train_iter_loss: 0.12620621919631958
train_iter_loss: 0.1353171020746231
train_iter_loss: 0.08567918092012405
train_iter_loss: 0.12121879309415817
train_iter_loss: 0.07691682875156403
train_iter_loss: 0.0956982672214508
train_iter_loss: 0.30051174759864807
train_iter_loss: 0.10273715853691101
train_iter_loss: 0.1018073633313179
train_iter_loss: 0.1703615039587021
train loss :0.1616
---------------------
Validation seg loss: 0.21748536855931272 at epoch 465
epoch =    466/  1000, exp = train
train_iter_loss: 0.07563236355781555
train_iter_loss: 0.20608054101467133
train_iter_loss: 0.2877303659915924
train_iter_loss: 0.2154621183872223
train_iter_loss: 0.27503201365470886
train_iter_loss: 0.13123778998851776
train_iter_loss: 0.11504717171192169
train_iter_loss: 0.2160295695066452
train_iter_loss: 0.15854687988758087
train_iter_loss: 0.2449556589126587
train_iter_loss: 0.16504858434200287
train_iter_loss: 0.19972985982894897
train_iter_loss: 0.11008591204881668
train_iter_loss: 0.17244336009025574
train_iter_loss: 0.11906042695045471
train_iter_loss: 0.14850327372550964
train_iter_loss: 0.15586374700069427
train_iter_loss: 0.11319312453269958
train_iter_loss: 0.1302790492773056
train_iter_loss: 0.2159520536661148
train_iter_loss: 0.12752066552639008
train_iter_loss: 0.18267956376075745
train_iter_loss: 0.23308652639389038
train_iter_loss: 0.21741431951522827
train_iter_loss: 0.13911955058574677
train_iter_loss: 0.15399053692817688
train_iter_loss: 0.19793547689914703
train_iter_loss: 0.20338423550128937
train_iter_loss: 0.0695299282670021
train_iter_loss: 0.12127383053302765
train_iter_loss: 0.16021393239498138
train_iter_loss: 0.15218386054039001
train_iter_loss: 0.11314881592988968
train_iter_loss: 0.22449272871017456
train_iter_loss: 0.30094701051712036
train_iter_loss: 0.26040902733802795
train_iter_loss: 0.10958652198314667
train_iter_loss: 0.17706063389778137
train_iter_loss: 0.17989814281463623
train_iter_loss: 0.062400758266448975
train_iter_loss: 0.22506332397460938
train_iter_loss: 0.14101368188858032
train_iter_loss: 0.1286739706993103
train_iter_loss: 0.06115080788731575
train_iter_loss: 0.20006753504276276
train_iter_loss: 0.23773989081382751
train_iter_loss: 0.26920437812805176
train_iter_loss: 0.1307721585035324
train_iter_loss: 0.1964680701494217
train_iter_loss: 0.16538523137569427
train_iter_loss: 0.1428387314081192
train_iter_loss: 0.238088920712471
train_iter_loss: 0.13754428923130035
train_iter_loss: 0.1454053670167923
train_iter_loss: 0.23852211236953735
train_iter_loss: 0.13248974084854126
train_iter_loss: 0.19884835183620453
train_iter_loss: 0.09750678390264511
train_iter_loss: 0.1904018223285675
train_iter_loss: 0.23946236073970795
train_iter_loss: 0.10461094975471497
train_iter_loss: 0.13461391627788544
train_iter_loss: 0.17028431594371796
train_iter_loss: 0.1765439659357071
train_iter_loss: 0.15246304869651794
train_iter_loss: 0.13940471410751343
train_iter_loss: 0.12920808792114258
train_iter_loss: 0.18468356132507324
train_iter_loss: 0.1388651728630066
train_iter_loss: 0.07528439164161682
train_iter_loss: 0.2191852480173111
train_iter_loss: 0.14846298098564148
train_iter_loss: 0.3207191824913025
train_iter_loss: 0.11039525270462036
train_iter_loss: 0.16126888990402222
train_iter_loss: 0.19880755245685577
train_iter_loss: 0.07247402518987656
train_iter_loss: 0.06884262710809708
train_iter_loss: 0.1059424877166748
train_iter_loss: 0.33273428678512573
train_iter_loss: 0.0963900238275528
train_iter_loss: 0.12478433549404144
train_iter_loss: 0.19384242594242096
train_iter_loss: 0.1473269909620285
train_iter_loss: 0.1885395646095276
train_iter_loss: 0.1036408469080925
train_iter_loss: 0.16825909912586212
train_iter_loss: 0.11319807171821594
train_iter_loss: 0.15679992735385895
train_iter_loss: 0.12464538216590881
train_iter_loss: 0.16282527148723602
train_iter_loss: 0.1996299922466278
train_iter_loss: 0.28116995096206665
train_iter_loss: 0.20029446482658386
train_iter_loss: 0.13909313082695007
train_iter_loss: 0.09272431582212448
train_iter_loss: 0.13052214682102203
train_iter_loss: 0.14056000113487244
train_iter_loss: 0.1664372980594635
train_iter_loss: 0.2531181871891022
train loss :0.1685
---------------------
Validation seg loss: 0.2224239284295659 at epoch 466
epoch =    467/  1000, exp = train
train_iter_loss: 0.15284289419651031
train_iter_loss: 0.10495156049728394
train_iter_loss: 0.0712328553199768
train_iter_loss: 0.18788334727287292
train_iter_loss: 0.197453573346138
train_iter_loss: 0.20629285275936127
train_iter_loss: 0.11722816526889801
train_iter_loss: 0.15360552072525024
train_iter_loss: 0.18742044270038605
train_iter_loss: 0.14956602454185486
train_iter_loss: 0.10799957066774368
train_iter_loss: 0.09579305350780487
train_iter_loss: 0.19471685588359833
train_iter_loss: 0.13678571581840515
train_iter_loss: 0.07840001583099365
train_iter_loss: 0.10163215547800064
train_iter_loss: 0.2144821733236313
train_iter_loss: 0.1848231852054596
train_iter_loss: 0.18061399459838867
train_iter_loss: 0.19158950448036194
train_iter_loss: 0.18073812127113342
train_iter_loss: 0.15465565025806427
train_iter_loss: 0.1641448438167572
train_iter_loss: 0.13277454674243927
train_iter_loss: 0.1485522836446762
train_iter_loss: 0.13837389647960663
train_iter_loss: 0.11585759371519089
train_iter_loss: 0.2874771058559418
train_iter_loss: 0.20792049169540405
train_iter_loss: 0.06222487613558769
train_iter_loss: 0.25423163175582886
train_iter_loss: 0.10176645964384079
train_iter_loss: 0.20240353047847748
train_iter_loss: 0.19413869082927704
train_iter_loss: 0.11539952456951141
train_iter_loss: 0.1090049147605896
train_iter_loss: 0.09645744413137436
train_iter_loss: 0.06160543113946915
train_iter_loss: 0.19323591887950897
train_iter_loss: 0.08279085904359818
train_iter_loss: 0.07454945147037506
train_iter_loss: 0.23401936888694763
train_iter_loss: 0.24277062714099884
train_iter_loss: 0.09525476396083832
train_iter_loss: 0.09822924435138702
train_iter_loss: 0.19842436909675598
train_iter_loss: 0.27014732360839844
train_iter_loss: 0.13082557916641235
train_iter_loss: 0.24112921953201294
train_iter_loss: 0.21579842269420624
train_iter_loss: 0.1439087986946106
train_iter_loss: 0.10736245661973953
train_iter_loss: 0.09132333844900131
train_iter_loss: 0.1859426647424698
train_iter_loss: 0.21037672460079193
train_iter_loss: 0.2183534950017929
train_iter_loss: 0.23476047813892365
train_iter_loss: 0.284400999546051
train_iter_loss: 0.11131270974874496
train_iter_loss: 0.08076127618551254
train_iter_loss: 0.16141963005065918
train_iter_loss: 0.1598525196313858
train_iter_loss: 0.26433634757995605
train_iter_loss: 0.1459643393754959
train_iter_loss: 0.3190292418003082
train_iter_loss: 0.2096717804670334
train_iter_loss: 0.1784360259771347
train_iter_loss: 0.17492641508579254
train_iter_loss: 0.1673443764448166
train_iter_loss: 0.26983675360679626
train_iter_loss: 0.14271987974643707
train_iter_loss: 0.15409936010837555
train_iter_loss: 0.3301366865634918
train_iter_loss: 0.11299178004264832
train_iter_loss: 0.1207541674375534
train_iter_loss: 0.15808211266994476
train_iter_loss: 0.16563160717487335
train_iter_loss: 0.12022800743579865
train_iter_loss: 0.19144031405448914
train_iter_loss: 0.1467135101556778
train_iter_loss: 0.16068662703037262
train_iter_loss: 0.17232070863246918
train_iter_loss: 0.20260722935199738
train_iter_loss: 0.07993544638156891
train_iter_loss: 0.20367376506328583
train_iter_loss: 0.1329929232597351
train_iter_loss: 0.08258041739463806
train_iter_loss: 0.2084922343492508
train_iter_loss: 0.1157832071185112
train_iter_loss: 0.14280986785888672
train_iter_loss: 0.10123760998249054
train_iter_loss: 0.12614287436008453
train_iter_loss: 0.08068034052848816
train_iter_loss: 0.1737685352563858
train_iter_loss: 0.2838122248649597
train_iter_loss: 0.21439196169376373
train_iter_loss: 0.11042329668998718
train_iter_loss: 0.11688673496246338
train_iter_loss: 0.12045108526945114
train_iter_loss: 0.12661395967006683
train loss :0.1631
---------------------
Validation seg loss: 0.21859993966612615 at epoch 467
epoch =    468/  1000, exp = train
train_iter_loss: 0.18145570158958435
train_iter_loss: 0.15207098424434662
train_iter_loss: 0.24255366623401642
train_iter_loss: 0.1548498421907425
train_iter_loss: 0.14753252267837524
train_iter_loss: 0.15491969883441925
train_iter_loss: 0.19935375452041626
train_iter_loss: 0.14030469954013824
train_iter_loss: 0.14469033479690552
train_iter_loss: 0.1116558089852333
train_iter_loss: 0.18343856930732727
train_iter_loss: 0.11325657367706299
train_iter_loss: 0.07558200508356094
train_iter_loss: 0.158416286110878
train_iter_loss: 0.2287554293870926
train_iter_loss: 0.11201305687427521
train_iter_loss: 0.10383127629756927
train_iter_loss: 0.15745218098163605
train_iter_loss: 0.1645515114068985
train_iter_loss: 0.2515381872653961
train_iter_loss: 0.06819671392440796
train_iter_loss: 0.12982329726219177
train_iter_loss: 0.08390934765338898
train_iter_loss: 0.11879251897335052
train_iter_loss: 0.13578584790229797
train_iter_loss: 0.05779866501688957
train_iter_loss: 0.2711723744869232
train_iter_loss: 0.1216401681303978
train_iter_loss: 0.10920779407024384
train_iter_loss: 0.08033039420843124
train_iter_loss: 0.12433131784200668
train_iter_loss: 0.10188499838113785
train_iter_loss: 0.12954340875148773
train_iter_loss: 0.176653653383255
train_iter_loss: 0.10702767223119736
train_iter_loss: 0.10787936300039291
train_iter_loss: 0.26958784461021423
train_iter_loss: 0.18821276724338531
train_iter_loss: 0.15894447267055511
train_iter_loss: 0.14078985154628754
train_iter_loss: 0.20522494614124298
train_iter_loss: 0.142857626080513
train_iter_loss: 0.24116568267345428
train_iter_loss: 0.23950141668319702
train_iter_loss: 0.1577926129102707
train_iter_loss: 0.1155286654829979
train_iter_loss: 0.2069185972213745
train_iter_loss: 0.1409909576177597
train_iter_loss: 0.1922617256641388
train_iter_loss: 0.2754865884780884
train_iter_loss: 0.16272474825382233
train_iter_loss: 0.14593017101287842
train_iter_loss: 0.10998459160327911
train_iter_loss: 0.14684534072875977
train_iter_loss: 0.25439783930778503
train_iter_loss: 0.08961997181177139
train_iter_loss: 0.14952890574932098
train_iter_loss: 0.10595445334911346
train_iter_loss: 0.10867434740066528
train_iter_loss: 0.23812292516231537
train_iter_loss: 0.21136634051799774
train_iter_loss: 0.2848553955554962
train_iter_loss: 0.2204170674085617
train_iter_loss: 0.08169297128915787
train_iter_loss: 0.042343977838754654
train_iter_loss: 0.09677936881780624
train_iter_loss: 0.13088008761405945
train_iter_loss: 0.22072502970695496
train_iter_loss: 0.07700061053037643
train_iter_loss: 0.14975646138191223
train_iter_loss: 0.06539809703826904
train_iter_loss: 0.06311018764972687
train_iter_loss: 0.051470428705215454
train_iter_loss: 0.18908768892288208
train_iter_loss: 0.12444518506526947
train_iter_loss: 0.1776553839445114
train_iter_loss: 0.25329867005348206
train_iter_loss: 0.18696270883083344
train_iter_loss: 0.30493125319480896
train_iter_loss: 0.13719280064105988
train_iter_loss: 0.0797782763838768
train_iter_loss: 0.3721976578235626
train_iter_loss: 0.11606743931770325
train_iter_loss: 0.10059984028339386
train_iter_loss: 0.1807563751935959
train_iter_loss: 0.29808977246284485
train_iter_loss: 0.129451185464859
train_iter_loss: 0.16083982586860657
train_iter_loss: 0.20565462112426758
train_iter_loss: 0.08229655772447586
train_iter_loss: 0.09442596882581711
train_iter_loss: 0.3635077178478241
train_iter_loss: 0.3329983353614807
train_iter_loss: 0.14964258670806885
train_iter_loss: 0.11281348764896393
train_iter_loss: 0.09912598133087158
train_iter_loss: 0.12346251308917999
train_iter_loss: 0.20379158854484558
train_iter_loss: 0.10374733060598373
train_iter_loss: 0.06443078070878983
train loss :0.1590
---------------------
Validation seg loss: 0.21965096853146293 at epoch 468
epoch =    469/  1000, exp = train
train_iter_loss: 0.14740289747714996
train_iter_loss: 0.16015949845314026
train_iter_loss: 0.07965558767318726
train_iter_loss: 0.24735091626644135
train_iter_loss: 0.37689125537872314
train_iter_loss: 0.18000482022762299
train_iter_loss: 0.14127270877361298
train_iter_loss: 0.08868038654327393
train_iter_loss: 0.21209010481834412
train_iter_loss: 0.2840690016746521
train_iter_loss: 0.11206530034542084
train_iter_loss: 0.1443195641040802
train_iter_loss: 0.4797097444534302
train_iter_loss: 0.12915679812431335
train_iter_loss: 0.12274190783500671
train_iter_loss: 0.23003098368644714
train_iter_loss: 0.23235568404197693
train_iter_loss: 0.03622613847255707
train_iter_loss: 0.12741295993328094
train_iter_loss: 0.12610605359077454
train_iter_loss: 0.2775702178478241
train_iter_loss: 0.229842871427536
train_iter_loss: 0.1854095458984375
train_iter_loss: 0.11612556874752045
train_iter_loss: 0.14864711463451385
train_iter_loss: 0.08220509439706802
train_iter_loss: 0.15910875797271729
train_iter_loss: 0.15775111317634583
train_iter_loss: 0.26833972334861755
train_iter_loss: 0.1033359169960022
train_iter_loss: 0.14442084729671478
train_iter_loss: 0.19963711500167847
train_iter_loss: 0.1867387443780899
train_iter_loss: 0.18843497335910797
train_iter_loss: 0.1911076456308365
train_iter_loss: 0.11671096831560135
train_iter_loss: 0.18472708761692047
train_iter_loss: 0.1907913088798523
train_iter_loss: 0.20376263558864594
train_iter_loss: 0.11636383831501007
train_iter_loss: 0.13671813905239105
train_iter_loss: 0.11465206742286682
train_iter_loss: 0.20261657238006592
train_iter_loss: 0.30905231833457947
train_iter_loss: 0.14658944308757782
train_iter_loss: 0.2840918004512787
train_iter_loss: 0.09412354975938797
train_iter_loss: 0.26553764939308167
train_iter_loss: 0.2799707055091858
train_iter_loss: 0.23180660605430603
train_iter_loss: 0.10704584419727325
train_iter_loss: 0.12046283483505249
train_iter_loss: 0.1925569623708725
train_iter_loss: 0.17739827930927277
train_iter_loss: 0.22036108374595642
train_iter_loss: 0.1628285050392151
train_iter_loss: 0.14568398892879486
train_iter_loss: 0.16035637259483337
train_iter_loss: 0.16751641035079956
train_iter_loss: 0.13600248098373413
train_iter_loss: 0.12303735315799713
train_iter_loss: 0.1282084435224533
train_iter_loss: 0.19976167380809784
train_iter_loss: 0.24098140001296997
train_iter_loss: 0.09900488704442978
train_iter_loss: 0.143177330493927
train_iter_loss: 0.14742062985897064
train_iter_loss: 0.20782004296779633
train_iter_loss: 0.15770000219345093
train_iter_loss: 0.0893273651599884
train_iter_loss: 0.08565088361501694
train_iter_loss: 0.14143474400043488
train_iter_loss: 0.17201873660087585
train_iter_loss: 0.1447209119796753
train_iter_loss: 0.13582822680473328
train_iter_loss: 0.18049544095993042
train_iter_loss: 0.06932642310857773
train_iter_loss: 0.25753507018089294
train_iter_loss: 0.1851799041032791
train_iter_loss: 0.18636471033096313
train_iter_loss: 0.12230485677719116
train_iter_loss: 0.09580636769533157
train_iter_loss: 0.3169794976711273
train_iter_loss: 0.28860968351364136
train_iter_loss: 0.09108567237854004
train_iter_loss: 0.23225361108779907
train_iter_loss: 0.25312134623527527
train_iter_loss: 0.06142400577664375
train_iter_loss: 0.12421134114265442
train_iter_loss: 0.1645539402961731
train_iter_loss: 0.25158894062042236
train_iter_loss: 0.18871234357357025
train_iter_loss: 0.08747368305921555
train_iter_loss: 0.15350651741027832
train_iter_loss: 0.09253623336553574
train_iter_loss: 0.1835004687309265
train_iter_loss: 0.12333303689956665
train_iter_loss: 0.10929432511329651
train_iter_loss: 0.13934865593910217
train_iter_loss: 0.13941656053066254
train loss :0.1724
---------------------
Validation seg loss: 0.217524866213284 at epoch 469
epoch =    470/  1000, exp = train
train_iter_loss: 0.18371351063251495
train_iter_loss: 0.15821412205696106
train_iter_loss: 0.044878315180540085
train_iter_loss: 0.19615387916564941
train_iter_loss: 0.16641013324260712
train_iter_loss: 0.10779576748609543
train_iter_loss: 0.12701250612735748
train_iter_loss: 0.10694631934165955
train_iter_loss: 0.13291417062282562
train_iter_loss: 0.15565013885498047
train_iter_loss: 0.15497860312461853
train_iter_loss: 0.2036862075328827
train_iter_loss: 0.17843203246593475
train_iter_loss: 0.1635330468416214
train_iter_loss: 0.15720169246196747
train_iter_loss: 0.13271892070770264
train_iter_loss: 0.07587910443544388
train_iter_loss: 0.24299685657024384
train_iter_loss: 0.1608421355485916
train_iter_loss: 0.1356118619441986
train_iter_loss: 0.1469893455505371
train_iter_loss: 0.15426179766654968
train_iter_loss: 0.1594935953617096
train_iter_loss: 0.1409219652414322
train_iter_loss: 0.14265088737010956
train_iter_loss: 0.18782629072666168
train_iter_loss: 0.05616259202361107
train_iter_loss: 0.1420847326517105
train_iter_loss: 0.2078534960746765
train_iter_loss: 0.03606531769037247
train_iter_loss: 0.23604018986225128
train_iter_loss: 0.11988525092601776
train_iter_loss: 0.1261332929134369
train_iter_loss: 0.18872585892677307
train_iter_loss: 0.08170437067747116
train_iter_loss: 0.202694833278656
train_iter_loss: 0.185594379901886
train_iter_loss: 0.0835924744606018
train_iter_loss: 0.1066993996500969
train_iter_loss: 0.139288529753685
train_iter_loss: 0.19899964332580566
train_iter_loss: 0.20563499629497528
train_iter_loss: 0.2232322245836258
train_iter_loss: 0.23135221004486084
train_iter_loss: 0.22697696089744568
train_iter_loss: 0.11925339698791504
train_iter_loss: 0.1801370233297348
train_iter_loss: 0.18817390501499176
train_iter_loss: 0.11117830127477646
train_iter_loss: 0.1505858302116394
train_iter_loss: 0.14212839305400848
train_iter_loss: 0.15439952909946442
train_iter_loss: 0.14801965653896332
train_iter_loss: 0.24640682339668274
train_iter_loss: 0.23936598002910614
train_iter_loss: 0.20000092685222626
train_iter_loss: 0.14813560247421265
train_iter_loss: 0.13440217077732086
train_iter_loss: 0.24739991128444672
train_iter_loss: 0.18243162333965302
train_iter_loss: 0.1585351675748825
train_iter_loss: 0.13243763148784637
train_iter_loss: 0.19675707817077637
train_iter_loss: 0.11980780214071274
train_iter_loss: 0.18248800933361053
train_iter_loss: 0.3624582588672638
train_iter_loss: 0.09520596265792847
train_iter_loss: 0.06372111290693283
train_iter_loss: 0.15742042660713196
train_iter_loss: 0.09424157440662384
train_iter_loss: 0.1953989565372467
train_iter_loss: 0.15257178246974945
train_iter_loss: 0.0878346785902977
train_iter_loss: 0.09988180547952652
train_iter_loss: 0.23919391632080078
train_iter_loss: 0.13340172171592712
train_iter_loss: 0.12612462043762207
train_iter_loss: 0.1411978155374527
train_iter_loss: 0.259724885225296
train_iter_loss: 0.1011015921831131
train_iter_loss: 0.6223806738853455
train_iter_loss: 0.1796385496854782
train_iter_loss: 0.11789501458406448
train_iter_loss: 0.1666494458913803
train_iter_loss: 0.13888055086135864
train_iter_loss: 0.11765892803668976
train_iter_loss: 0.17463400959968567
train_iter_loss: 0.0671689510345459
train_iter_loss: 0.17768023908138275
train_iter_loss: 0.1746804565191269
train_iter_loss: 0.17265991866588593
train_iter_loss: 0.10159412026405334
train_iter_loss: 0.1597399264574051
train_iter_loss: 0.3110632300376892
train_iter_loss: 0.030985543504357338
train_iter_loss: 0.16036641597747803
train_iter_loss: 0.19986592233181
train_iter_loss: 0.12374690175056458
train_iter_loss: 0.07789083570241928
train_iter_loss: 0.12529297173023224
train loss :0.1618
---------------------
Validation seg loss: 0.22208457819695743 at epoch 470
epoch =    471/  1000, exp = train
train_iter_loss: 0.2612621486186981
train_iter_loss: 0.2826700210571289
train_iter_loss: 0.1280975490808487
train_iter_loss: 0.13264445960521698
train_iter_loss: 0.17147214710712433
train_iter_loss: 0.18006815016269684
train_iter_loss: 0.10315147787332535
train_iter_loss: 0.11547701060771942
train_iter_loss: 0.149090975522995
train_iter_loss: 0.18454742431640625
train_iter_loss: 0.19925446808338165
train_iter_loss: 0.25383415818214417
train_iter_loss: 0.09742024540901184
train_iter_loss: 0.13013677299022675
train_iter_loss: 0.13367284834384918
train_iter_loss: 0.10560417920351028
train_iter_loss: 0.24782408773899078
train_iter_loss: 0.33481475710868835
train_iter_loss: 0.18785087764263153
train_iter_loss: 0.11470808833837509
train_iter_loss: 0.14020594954490662
train_iter_loss: 0.21867242455482483
train_iter_loss: 0.13877110183238983
train_iter_loss: 0.21153023838996887
train_iter_loss: 0.11487686634063721
train_iter_loss: 0.18244794011116028
train_iter_loss: 0.2245185524225235
train_iter_loss: 0.1530722826719284
train_iter_loss: 0.11976355314254761
train_iter_loss: 0.21924471855163574
train_iter_loss: 0.12102362513542175
train_iter_loss: 0.10923998057842255
train_iter_loss: 0.23798011243343353
train_iter_loss: 0.1779206097126007
train_iter_loss: 0.14869552850723267
train_iter_loss: 0.22737881541252136
train_iter_loss: 0.15043871104717255
train_iter_loss: 0.1431918740272522
train_iter_loss: 0.17077460885047913
train_iter_loss: 0.1037248820066452
train_iter_loss: 0.281918466091156
train_iter_loss: 0.1825208216905594
train_iter_loss: 0.0849897488951683
train_iter_loss: 0.2719826400279999
train_iter_loss: 0.10093271732330322
train_iter_loss: 0.1792256087064743
train_iter_loss: 0.3275834918022156
train_iter_loss: 0.20767933130264282
train_iter_loss: 0.1668211668729782
train_iter_loss: 0.06856760382652283
train_iter_loss: 0.14215847849845886
train_iter_loss: 0.07116345316171646
train_iter_loss: 0.21482039988040924
train_iter_loss: 0.2504599392414093
train_iter_loss: 0.0495808869600296
train_iter_loss: 0.12506136298179626
train_iter_loss: 0.0998610183596611
train_iter_loss: 0.11885131150484085
train_iter_loss: 0.22604404389858246
train_iter_loss: 0.09315236657857895
train_iter_loss: 0.16641025245189667
train_iter_loss: 0.09747543931007385
train_iter_loss: 0.1490926593542099
train_iter_loss: 0.11070451885461807
train_iter_loss: 0.18236419558525085
train_iter_loss: 0.09373874962329865
train_iter_loss: 0.10859053581953049
train_iter_loss: 0.11567816883325577
train_iter_loss: 0.11909906566143036
train_iter_loss: 0.14901967346668243
train_iter_loss: 0.22337588667869568
train_iter_loss: 0.17386363446712494
train_iter_loss: 0.1312938779592514
train_iter_loss: 0.2914714813232422
train_iter_loss: 0.13556843996047974
train_iter_loss: 0.13243453204631805
train_iter_loss: 0.07765563577413559
train_iter_loss: 0.12592093646526337
train_iter_loss: 0.1693759560585022
train_iter_loss: 0.12093517929315567
train_iter_loss: 0.07168146222829819
train_iter_loss: 0.09563159942626953
train_iter_loss: 0.06906351447105408
train_iter_loss: 0.22637546062469482
train_iter_loss: 0.20965661108493805
train_iter_loss: 0.13490039110183716
train_iter_loss: 0.144292950630188
train_iter_loss: 0.12086871266365051
train_iter_loss: 0.25264519453048706
train_iter_loss: 0.21855002641677856
train_iter_loss: 0.1564207226037979
train_iter_loss: 0.13660208880901337
train_iter_loss: 0.1505764126777649
train_iter_loss: 0.15813174843788147
train_iter_loss: 0.21711724996566772
train_iter_loss: 0.09115057438611984
train_iter_loss: 0.04858950152993202
train_iter_loss: 0.13453570008277893
train_iter_loss: 0.19212332367897034
train_iter_loss: 0.1016421914100647
train loss :0.1606
---------------------
Validation seg loss: 0.21728653336857567 at epoch 471
epoch =    472/  1000, exp = train
train_iter_loss: 0.09375450760126114
train_iter_loss: 0.18116742372512817
train_iter_loss: 0.2830530107021332
train_iter_loss: 0.16537083685398102
train_iter_loss: 0.13435031473636627
train_iter_loss: 0.26427456736564636
train_iter_loss: 0.09869648516178131
train_iter_loss: 0.13595104217529297
train_iter_loss: 0.1667936146259308
train_iter_loss: 0.1195305734872818
train_iter_loss: 0.13760101795196533
train_iter_loss: 0.1374862939119339
train_iter_loss: 0.10597394406795502
train_iter_loss: 0.1535424441099167
train_iter_loss: 0.22776535153388977
train_iter_loss: 0.1510084867477417
train_iter_loss: 0.18422721326351166
train_iter_loss: 0.1605207771062851
train_iter_loss: 0.08562306314706802
train_iter_loss: 0.14253312349319458
train_iter_loss: 0.3501521646976471
train_iter_loss: 0.14019650220870972
train_iter_loss: 0.15038049221038818
train_iter_loss: 0.13026897609233856
train_iter_loss: 0.14404742419719696
train_iter_loss: 0.1475646197795868
train_iter_loss: 0.09782303869724274
train_iter_loss: 0.1470676064491272
train_iter_loss: 0.1483897864818573
train_iter_loss: 0.21852520108222961
train_iter_loss: 0.1920935958623886
train_iter_loss: 0.12485923618078232
train_iter_loss: 0.18796418607234955
train_iter_loss: 0.088843934237957
train_iter_loss: 0.10875111073255539
train_iter_loss: 0.332012414932251
train_iter_loss: 0.12488257139921188
train_iter_loss: 0.10582246631383896
train_iter_loss: 0.20419146120548248
train_iter_loss: 0.19710370898246765
train_iter_loss: 0.11064586788415909
train_iter_loss: 0.24455557763576508
train_iter_loss: 0.19329853355884552
train_iter_loss: 0.1066426932811737
train_iter_loss: 0.14704816043376923
train_iter_loss: 0.16899332404136658
train_iter_loss: 0.16784721612930298
train_iter_loss: 0.06581827998161316
train_iter_loss: 0.06104707345366478
train_iter_loss: 0.0846974328160286
train_iter_loss: 0.22382795810699463
train_iter_loss: 0.11812008172273636
train_iter_loss: 0.1785467565059662
train_iter_loss: 0.10545044392347336
train_iter_loss: 0.2105000913143158
train_iter_loss: 0.2715931832790375
train_iter_loss: 0.20132635533809662
train_iter_loss: 0.2069934606552124
train_iter_loss: 0.3512808382511139
train_iter_loss: 0.09418092668056488
train_iter_loss: 0.12850265204906464
train_iter_loss: 0.28837043046951294
train_iter_loss: 0.25420695543289185
train_iter_loss: 0.07138039171695709
train_iter_loss: 0.09477970004081726
train_iter_loss: 0.11748850345611572
train_iter_loss: 0.1037905216217041
train_iter_loss: 0.15665926039218903
train_iter_loss: 0.07738231867551804
train_iter_loss: 0.2028329223394394
train_iter_loss: 0.13034673035144806
train_iter_loss: 0.19328740239143372
train_iter_loss: 0.1117195412516594
train_iter_loss: 0.20190493762493134
train_iter_loss: 0.15298299491405487
train_iter_loss: 0.09691327065229416
train_iter_loss: 0.11085318773984909
train_iter_loss: 0.2517065107822418
train_iter_loss: 0.15473425388336182
train_iter_loss: 0.1839345544576645
train_iter_loss: 0.02863181382417679
train_iter_loss: 0.1759749948978424
train_iter_loss: 0.2225046008825302
train_iter_loss: 0.1881154477596283
train_iter_loss: 0.2196066826581955
train_iter_loss: 0.12543530762195587
train_iter_loss: 0.12959709763526917
train_iter_loss: 0.17705439031124115
train_iter_loss: 0.17785409092903137
train_iter_loss: 0.18063251674175262
train_iter_loss: 0.10904694348573685
train_iter_loss: 0.19282381236553192
train_iter_loss: 0.18208648264408112
train_iter_loss: 0.029718555510044098
train_iter_loss: 0.13061417639255524
train_iter_loss: 0.10545876622200012
train_iter_loss: 0.2494044005870819
train_iter_loss: 0.11149768531322479
train_iter_loss: 0.1345255821943283
train_iter_loss: 0.1473684012889862
train loss :0.1604
---------------------
Validation seg loss: 0.21448032608044879 at epoch 472
epoch =    473/  1000, exp = train
train_iter_loss: 0.16237764060497284
train_iter_loss: 0.08083081245422363
train_iter_loss: 0.19383487105369568
train_iter_loss: 0.21500422060489655
train_iter_loss: 0.30616024136543274
train_iter_loss: 0.09310168772935867
train_iter_loss: 0.08439822494983673
train_iter_loss: 0.1004054918885231
train_iter_loss: 0.12556487321853638
train_iter_loss: 0.12541963160037994
train_iter_loss: 0.13179008662700653
train_iter_loss: 0.16967737674713135
train_iter_loss: 0.17364159226417542
train_iter_loss: 0.1668347716331482
train_iter_loss: 0.24278759956359863
train_iter_loss: 0.09168026596307755
train_iter_loss: 0.2854986786842346
train_iter_loss: 0.10195928066968918
train_iter_loss: 0.08076972514390945
train_iter_loss: 0.15816730260849
train_iter_loss: 0.21616080403327942
train_iter_loss: 0.18972459435462952
train_iter_loss: 0.13489475846290588
train_iter_loss: 0.24334001541137695
train_iter_loss: 0.19237491488456726
train_iter_loss: 0.1902419924736023
train_iter_loss: 0.13697408139705658
train_iter_loss: 0.13691890239715576
train_iter_loss: 0.0829085260629654
train_iter_loss: 0.22185412049293518
train_iter_loss: 0.16942909359931946
train_iter_loss: 0.4481496810913086
train_iter_loss: 0.06286514550447464
train_iter_loss: 0.0937403067946434
train_iter_loss: 0.09442951530218124
train_iter_loss: 0.10531909763813019
train_iter_loss: 0.10447405278682709
train_iter_loss: 0.21815422177314758
train_iter_loss: 0.12248287349939346
train_iter_loss: 0.18719039857387543
train_iter_loss: 0.22948649525642395
train_iter_loss: 0.08676215261220932
train_iter_loss: 0.13914377987384796
train_iter_loss: 0.24705226719379425
train_iter_loss: 0.16683951020240784
train_iter_loss: 0.23664981126785278
train_iter_loss: 0.13124506175518036
train_iter_loss: 0.12240009754896164
train_iter_loss: 0.14009740948677063
train_iter_loss: 0.20514291524887085
train_iter_loss: 0.09927470237016678
train_iter_loss: 0.13575614988803864
train_iter_loss: 0.14409534633159637
train_iter_loss: 0.1555863320827484
train_iter_loss: 0.21206627786159515
train_iter_loss: 0.10728912055492401
train_iter_loss: 0.10354369133710861
train_iter_loss: 0.08147421479225159
train_iter_loss: 0.14123448729515076
train_iter_loss: 0.16111521422863007
train_iter_loss: 0.18146350979804993
train_iter_loss: 0.15097367763519287
train_iter_loss: 0.04937271773815155
train_iter_loss: 0.20415830612182617
train_iter_loss: 0.13106396794319153
train_iter_loss: 0.26736980676651
train_iter_loss: 0.1081174910068512
train_iter_loss: 0.2234848290681839
train_iter_loss: 0.14889340102672577
train_iter_loss: 0.20409169793128967
train_iter_loss: 0.12047713994979858
train_iter_loss: 0.1160857304930687
train_iter_loss: 0.14716456830501556
train_iter_loss: 0.12425864487886429
train_iter_loss: 0.27877628803253174
train_iter_loss: 0.21393230557441711
train_iter_loss: 0.01578751765191555
train_iter_loss: 0.1767653226852417
train_iter_loss: 0.14195005595684052
train_iter_loss: 0.18676573038101196
train_iter_loss: 0.2079860419034958
train_iter_loss: 0.0577511303126812
train_iter_loss: 0.13113120198249817
train_iter_loss: 0.17631030082702637
train_iter_loss: 0.1390576958656311
train_iter_loss: 0.14612232148647308
train_iter_loss: 0.16144384443759918
train_iter_loss: 0.10815106332302094
train_iter_loss: 0.2734192907810211
train_iter_loss: 0.18574269115924835
train_iter_loss: 0.27478399872779846
train_iter_loss: 0.12780329585075378
train_iter_loss: 0.1375020444393158
train_iter_loss: 0.23984326422214508
train_iter_loss: 0.06955558061599731
train_iter_loss: 0.19505149126052856
train_iter_loss: 0.15775500237941742
train_iter_loss: 0.12429045885801315
train_iter_loss: 0.09229056537151337
train_iter_loss: 0.13857227563858032
train loss :0.1598
---------------------
Validation seg loss: 0.218650151281354 at epoch 473
epoch =    474/  1000, exp = train
train_iter_loss: 0.05150967836380005
train_iter_loss: 0.22038380801677704
train_iter_loss: 0.23477041721343994
train_iter_loss: 0.046882107853889465
train_iter_loss: 0.1304277628660202
train_iter_loss: 0.1529894769191742
train_iter_loss: 0.1381545215845108
train_iter_loss: 0.29882702231407166
train_iter_loss: 0.20480972528457642
train_iter_loss: 0.08456498384475708
train_iter_loss: 0.21348752081394196
train_iter_loss: 0.044401172548532486
train_iter_loss: 0.1343626081943512
train_iter_loss: 0.16258114576339722
train_iter_loss: 0.21372634172439575
train_iter_loss: 0.19346818327903748
train_iter_loss: 0.14763587713241577
train_iter_loss: 0.27939167618751526
train_iter_loss: 0.14472974836826324
train_iter_loss: 0.12208621948957443
train_iter_loss: 0.05494924262166023
train_iter_loss: 0.11015412956476212
train_iter_loss: 0.19781212508678436
train_iter_loss: 0.033955562859773636
train_iter_loss: 0.12757235765457153
train_iter_loss: 0.04897287115454674
train_iter_loss: 0.13236655294895172
train_iter_loss: 0.10141494870185852
train_iter_loss: 0.24442057311534882
train_iter_loss: 0.17682676017284393
train_iter_loss: 0.17765823006629944
train_iter_loss: 0.16368307173252106
train_iter_loss: 0.3114458918571472
train_iter_loss: 0.2290450930595398
train_iter_loss: 0.21481020748615265
train_iter_loss: 0.23081019520759583
train_iter_loss: 0.1749328374862671
train_iter_loss: 0.13481278717517853
train_iter_loss: 0.08524920791387558
train_iter_loss: 0.3437831699848175
train_iter_loss: 0.0668155699968338
train_iter_loss: 0.17300710082054138
train_iter_loss: 0.22747795283794403
train_iter_loss: 0.21229547262191772
train_iter_loss: 0.14474186301231384
train_iter_loss: 0.10772328823804855
train_iter_loss: 0.1141260489821434
train_iter_loss: 0.18804001808166504
train_iter_loss: 0.13742394745349884
train_iter_loss: 0.12877675890922546
train_iter_loss: 0.22752772271633148
train_iter_loss: 0.235073059797287
train_iter_loss: 0.19810669124126434
train_iter_loss: 0.132379949092865
train_iter_loss: 0.10860374569892883
train_iter_loss: 0.07803834974765778
train_iter_loss: 0.13307151198387146
train_iter_loss: 0.1680176705121994
train_iter_loss: 0.2482064813375473
train_iter_loss: 0.18863283097743988
train_iter_loss: 0.10380163788795471
train_iter_loss: 0.14542686939239502
train_iter_loss: 0.23118102550506592
train_iter_loss: 0.16186338663101196
train_iter_loss: 0.1747184693813324
train_iter_loss: 0.12366259843111038
train_iter_loss: 0.14810948073863983
train_iter_loss: 0.08702822029590607
train_iter_loss: 0.16739751398563385
train_iter_loss: 0.14271743595600128
train_iter_loss: 0.054024189710617065
train_iter_loss: 0.2732735872268677
train_iter_loss: 0.1556239128112793
train_iter_loss: 0.13078653812408447
train_iter_loss: 0.1786036491394043
train_iter_loss: 0.1521545648574829
train_iter_loss: 0.13121142983436584
train_iter_loss: 0.1724776327610016
train_iter_loss: 0.09652376174926758
train_iter_loss: 0.11134850978851318
train_iter_loss: 0.10929179191589355
train_iter_loss: 0.21293069422245026
train_iter_loss: 0.19094955921173096
train_iter_loss: 0.13884355127811432
train_iter_loss: 0.293633371591568
train_iter_loss: 0.1570751965045929
train_iter_loss: 0.156347393989563
train_iter_loss: 0.10402488708496094
train_iter_loss: 0.17496232688426971
train_iter_loss: 0.14314711093902588
train_iter_loss: 0.2648463547229767
train_iter_loss: 0.19842801988124847
train_iter_loss: 0.2019442766904831
train_iter_loss: 0.17663878202438354
train_iter_loss: 0.15728192031383514
train_iter_loss: 0.09601717442274094
train_iter_loss: 0.16987209022045135
train_iter_loss: 0.18686650693416595
train_iter_loss: 0.13266785442829132
train_iter_loss: 0.19868068397045135
train loss :0.1630
---------------------
Validation seg loss: 0.2198264307237037 at epoch 474
epoch =    475/  1000, exp = train
train_iter_loss: 0.05599617213010788
train_iter_loss: 0.24066366255283356
train_iter_loss: 0.09005124121904373
train_iter_loss: 0.04007723182439804
train_iter_loss: 0.10710787773132324
train_iter_loss: 0.24924501776695251
train_iter_loss: 0.14777809381484985
train_iter_loss: 0.10211345553398132
train_iter_loss: 0.17189690470695496
train_iter_loss: 0.24445600807666779
train_iter_loss: 0.1559431105852127
train_iter_loss: 0.1563044935464859
train_iter_loss: 0.24945075809955597
train_iter_loss: 0.1777774542570114
train_iter_loss: 0.13402806222438812
train_iter_loss: 0.045028507709503174
train_iter_loss: 0.18535588681697845
train_iter_loss: 0.17989502847194672
train_iter_loss: 0.19359250366687775
train_iter_loss: 0.2013879418373108
train_iter_loss: 0.3755952715873718
train_iter_loss: 0.12228012084960938
train_iter_loss: 0.1733831912279129
train_iter_loss: 0.12074120342731476
train_iter_loss: 0.2491220086812973
train_iter_loss: 0.09555938094854355
train_iter_loss: 0.2602432668209076
train_iter_loss: 0.18907928466796875
train_iter_loss: 0.17825232446193695
train_iter_loss: 0.06810350716114044
train_iter_loss: 0.2236592173576355
train_iter_loss: 0.17351390421390533
train_iter_loss: 0.14736731350421906
train_iter_loss: 0.12040863186120987
train_iter_loss: 0.15496844053268433
train_iter_loss: 0.16218523681163788
train_iter_loss: 0.028383400291204453
train_iter_loss: 0.2718483507633209
train_iter_loss: 0.11296077817678452
train_iter_loss: 0.1524777114391327
train_iter_loss: 0.24309267103672028
train_iter_loss: 0.15222883224487305
train_iter_loss: 0.119322769343853
train_iter_loss: 0.1531388759613037
train_iter_loss: 0.2070281058549881
train_iter_loss: 0.24573974311351776
train_iter_loss: 0.1274576336145401
train_iter_loss: 0.13592591881752014
train_iter_loss: 0.1470034122467041
train_iter_loss: 0.13928565382957458
train_iter_loss: 0.18389955163002014
train_iter_loss: 0.1783592849969864
train_iter_loss: 0.24060659110546112
train_iter_loss: 0.13754108548164368
train_iter_loss: 0.11852245777845383
train_iter_loss: 0.12213343381881714
train_iter_loss: 0.19965597987174988
train_iter_loss: 0.1679515838623047
train_iter_loss: 0.155885711312294
train_iter_loss: 0.14307095110416412
train_iter_loss: 0.1939411163330078
train_iter_loss: 0.17510586977005005
train_iter_loss: 0.11351584643125534
train_iter_loss: 0.1378176212310791
train_iter_loss: 0.18583784997463226
train_iter_loss: 0.09434594213962555
train_iter_loss: 0.3304876983165741
train_iter_loss: 0.0826272964477539
train_iter_loss: 0.2901977002620697
train_iter_loss: 0.05712264031171799
train_iter_loss: 0.0843527764081955
train_iter_loss: 0.09361515194177628
train_iter_loss: 0.18545857071876526
train_iter_loss: 0.13713829219341278
train_iter_loss: 0.2286870777606964
train_iter_loss: 0.0975813940167427
train_iter_loss: 0.3585463762283325
train_iter_loss: 0.2844226360321045
train_iter_loss: 0.1266147792339325
train_iter_loss: 0.08335943520069122
train_iter_loss: 0.11913339793682098
train_iter_loss: 0.19696518778800964
train_iter_loss: 0.20714789628982544
train_iter_loss: 0.21535831689834595
train_iter_loss: 0.12361695617437363
train_iter_loss: 0.18380838632583618
train_iter_loss: 0.31565868854522705
train_iter_loss: 0.18336287140846252
train_iter_loss: 0.27928757667541504
train_iter_loss: 0.1524999588727951
train_iter_loss: 0.2124563455581665
train_iter_loss: 0.20787033438682556
train_iter_loss: 0.062321700155735016
train_iter_loss: 0.07286424934864044
train_iter_loss: 0.12942852079868317
train_iter_loss: 0.12616488337516785
train_iter_loss: 0.09018143266439438
train_iter_loss: 0.19959057867527008
train_iter_loss: 0.08285363018512726
train_iter_loss: 0.14105777442455292
train loss :0.1663
---------------------
Validation seg loss: 0.21827175745845967 at epoch 475
epoch =    476/  1000, exp = train
train_iter_loss: 0.25569167733192444
train_iter_loss: 0.11755075305700302
train_iter_loss: 0.18814338743686676
train_iter_loss: 0.09402100741863251
train_iter_loss: 0.10214731097221375
train_iter_loss: 0.17831780016422272
train_iter_loss: 0.20481914281845093
train_iter_loss: 0.07301197946071625
train_iter_loss: 0.10944276303052902
train_iter_loss: 0.2645111382007599
train_iter_loss: 0.1586858630180359
train_iter_loss: 0.16735723614692688
train_iter_loss: 0.18356066942214966
train_iter_loss: 0.09013199806213379
train_iter_loss: 0.1549740880727768
train_iter_loss: 0.17869742214679718
train_iter_loss: 0.1856537163257599
train_iter_loss: 0.14681711792945862
train_iter_loss: 0.18291492760181427
train_iter_loss: 0.15635515749454498
train_iter_loss: 0.23616145551204681
train_iter_loss: 0.14406056702136993
train_iter_loss: 0.06294907629489899
train_iter_loss: 0.0897439494729042
train_iter_loss: 0.16967539489269257
train_iter_loss: 0.12156601995229721
train_iter_loss: 0.1058424711227417
train_iter_loss: 0.1149393692612648
train_iter_loss: 0.2979832589626312
train_iter_loss: 0.11247515678405762
train_iter_loss: 0.19880682229995728
train_iter_loss: 0.1782083362340927
train_iter_loss: 0.1817331463098526
train_iter_loss: 0.14294418692588806
train_iter_loss: 0.1166529729962349
train_iter_loss: 0.33813202381134033
train_iter_loss: 0.1315706968307495
train_iter_loss: 0.18624228239059448
train_iter_loss: 0.19519343972206116
train_iter_loss: 0.061651747673749924
train_iter_loss: 0.2731848359107971
train_iter_loss: 0.1777355968952179
train_iter_loss: 0.16267029941082
train_iter_loss: 0.10330761969089508
train_iter_loss: 0.20931045711040497
train_iter_loss: 0.12639927864074707
train_iter_loss: 0.1191418394446373
train_iter_loss: 0.0750395730137825
train_iter_loss: 0.1886371523141861
train_iter_loss: 0.2738388776779175
train_iter_loss: 0.2584168016910553
train_iter_loss: 0.11803344637155533
train_iter_loss: 0.10600583255290985
train_iter_loss: 0.24260151386260986
train_iter_loss: 0.10415107756853104
train_iter_loss: 0.14075560867786407
train_iter_loss: 0.07927224040031433
train_iter_loss: 0.11189822852611542
train_iter_loss: 0.15951117873191833
train_iter_loss: 0.15329603850841522
train_iter_loss: 0.14452584087848663
train_iter_loss: 0.15320274233818054
train_iter_loss: 0.220619335770607
train_iter_loss: 0.21023766696453094
train_iter_loss: 0.2048550397157669
train_iter_loss: 0.11557408422231674
train_iter_loss: 0.21569105982780457
train_iter_loss: 0.07987134903669357
train_iter_loss: 0.13674049079418182
train_iter_loss: 0.10861580073833466
train_iter_loss: 0.1178203895688057
train_iter_loss: 0.25936928391456604
train_iter_loss: 0.14407885074615479
train_iter_loss: 0.18043935298919678
train_iter_loss: 0.24701255559921265
train_iter_loss: 0.16815195977687836
train_iter_loss: 0.24180349707603455
train_iter_loss: 0.14477184414863586
train_iter_loss: 0.16871894896030426
train_iter_loss: 0.06252751499414444
train_iter_loss: 0.15418624877929688
train_iter_loss: 0.21909257769584656
train_iter_loss: 0.20301863551139832
train_iter_loss: 0.1530510038137436
train_iter_loss: 0.10635069757699966
train_iter_loss: 0.1549547165632248
train_iter_loss: 0.07086421549320221
train_iter_loss: 0.07506096363067627
train_iter_loss: 0.18221406638622284
train_iter_loss: 0.13258329033851624
train_iter_loss: 0.12010839581489563
train_iter_loss: 0.16805250942707062
train_iter_loss: 0.14833582937717438
train_iter_loss: 0.13934025168418884
train_iter_loss: 0.21761995553970337
train_iter_loss: 0.15459893643856049
train_iter_loss: 0.12809564173221588
train_iter_loss: 0.13153433799743652
train_iter_loss: 0.1322711706161499
train_iter_loss: 0.08650810271501541
train loss :0.1593
---------------------
Validation seg loss: 0.21813330240547657 at epoch 476
epoch =    477/  1000, exp = train
train_iter_loss: 0.21707338094711304
train_iter_loss: 0.11625160276889801
train_iter_loss: 0.15160420536994934
train_iter_loss: 0.14010214805603027
train_iter_loss: 0.17882578074932098
train_iter_loss: 0.19619455933570862
train_iter_loss: 0.29032042622566223
train_iter_loss: 0.23791520297527313
train_iter_loss: 0.21280644834041595
train_iter_loss: 0.3014965057373047
train_iter_loss: 0.17038215696811676
train_iter_loss: 0.1698005497455597
train_iter_loss: 0.26509878039360046
train_iter_loss: 0.12607790529727936
train_iter_loss: 0.10619211196899414
train_iter_loss: 0.19487237930297852
train_iter_loss: 0.1260172724723816
train_iter_loss: 0.32643669843673706
train_iter_loss: 0.24590176343917847
train_iter_loss: 0.17206691205501556
train_iter_loss: 0.0584057942032814
train_iter_loss: 0.20740915834903717
train_iter_loss: 0.1267196387052536
train_iter_loss: 0.16891920566558838
train_iter_loss: 0.13967382907867432
train_iter_loss: 0.238184854388237
train_iter_loss: 0.1870860606431961
train_iter_loss: 0.19694150984287262
train_iter_loss: 0.22180674970149994
train_iter_loss: 0.1045752689242363
train_iter_loss: 0.18518789112567902
train_iter_loss: 0.10945010930299759
train_iter_loss: 0.07467285543680191
train_iter_loss: 0.2326042503118515
train_iter_loss: 0.20677228271961212
train_iter_loss: 0.1790420562028885
train_iter_loss: 0.08984509110450745
train_iter_loss: 0.19015559554100037
train_iter_loss: 0.24169054627418518
train_iter_loss: 0.04823889583349228
train_iter_loss: 0.23068399727344513
train_iter_loss: 0.11781966686248779
train_iter_loss: 0.0924341008067131
train_iter_loss: 0.11442399770021439
train_iter_loss: 0.11366260051727295
train_iter_loss: 0.11120095103979111
train_iter_loss: 0.22621631622314453
train_iter_loss: 0.12023768573999405
train_iter_loss: 0.3073473274707794
train_iter_loss: 0.1357949823141098
train_iter_loss: 0.1257900595664978
train_iter_loss: 0.1372150033712387
train_iter_loss: 0.10499081760644913
train_iter_loss: 0.10224922001361847
train_iter_loss: 0.1299559623003006
train_iter_loss: 0.22736907005310059
train_iter_loss: 0.15950831770896912
train_iter_loss: 0.18836791813373566
train_iter_loss: 0.15899868309497833
train_iter_loss: 0.20088928937911987
train_iter_loss: 0.15100568532943726
train_iter_loss: 0.12674784660339355
train_iter_loss: 0.1561523824930191
train_iter_loss: 0.21334929764270782
train_iter_loss: 0.07298694550991058
train_iter_loss: 0.1041235700249672
train_iter_loss: 0.17482197284698486
train_iter_loss: 0.1947529911994934
train_iter_loss: 0.14236365258693695
train_iter_loss: 0.16039057075977325
train_iter_loss: 0.1196582242846489
train_iter_loss: 0.40902915596961975
train_iter_loss: 0.1578342318534851
train_iter_loss: 0.15313100814819336
train_iter_loss: 0.19392849504947662
train_iter_loss: 0.0990908145904541
train_iter_loss: 0.11769035458564758
train_iter_loss: 0.16635216772556305
train_iter_loss: 0.16615840792655945
train_iter_loss: 0.13876976072788239
train_iter_loss: 0.20003895461559296
train_iter_loss: 0.0823521763086319
train_iter_loss: 0.15973317623138428
train_iter_loss: 0.15158921480178833
train_iter_loss: 0.14202263951301575
train_iter_loss: 0.15974104404449463
train_iter_loss: 0.07459346950054169
train_iter_loss: 0.266269326210022
train_iter_loss: 0.19651612639427185
train_iter_loss: 0.14238791167736053
train_iter_loss: 0.10162059962749481
train_iter_loss: 0.18521159887313843
train_iter_loss: 0.1447877585887909
train_iter_loss: 0.27796557545661926
train_iter_loss: 0.12579846382141113
train_iter_loss: 0.15883202850818634
train_iter_loss: 0.18762564659118652
train_iter_loss: 0.3006080687046051
train_iter_loss: 0.12646496295928955
train_iter_loss: 0.12487595528364182
train loss :0.1694
---------------------
Validation seg loss: 0.2180216271992562 at epoch 477
epoch =    478/  1000, exp = train
train_iter_loss: 0.09337636828422546
train_iter_loss: 0.08548387885093689
train_iter_loss: 0.0712859258055687
train_iter_loss: 0.20623672008514404
train_iter_loss: 0.21658246219158173
train_iter_loss: 0.16696013510227203
train_iter_loss: 0.14918871223926544
train_iter_loss: 0.24911299347877502
train_iter_loss: 0.118429034948349
train_iter_loss: 0.13889920711517334
train_iter_loss: 0.1852678656578064
train_iter_loss: 0.1639232188463211
train_iter_loss: 0.3160858750343323
train_iter_loss: 0.1679137647151947
train_iter_loss: 0.2612823247909546
train_iter_loss: 0.3633454740047455
train_iter_loss: 0.07863006740808487
train_iter_loss: 0.16583386063575745
train_iter_loss: 0.06818731874227524
train_iter_loss: 0.11822749674320221
train_iter_loss: 0.15518096089363098
train_iter_loss: 0.30342811346054077
train_iter_loss: 0.1977192759513855
train_iter_loss: 0.1157354936003685
train_iter_loss: 0.08813045173883438
train_iter_loss: 0.11546505987644196
train_iter_loss: 0.1882927566766739
train_iter_loss: 0.06562686711549759
train_iter_loss: 0.16897189617156982
train_iter_loss: 0.1623317301273346
train_iter_loss: 0.14277565479278564
train_iter_loss: 0.07978953421115875
train_iter_loss: 0.23236620426177979
train_iter_loss: 0.14173008501529694
train_iter_loss: 0.16592174768447876
train_iter_loss: 0.22139611840248108
train_iter_loss: 0.12395311146974564
train_iter_loss: 0.1033172532916069
train_iter_loss: 0.10896779596805573
train_iter_loss: 0.2430536150932312
train_iter_loss: 0.2334926575422287
train_iter_loss: 0.17765161395072937
train_iter_loss: 0.17776571214199066
train_iter_loss: 0.07144719362258911
train_iter_loss: 0.10381149500608444
train_iter_loss: 0.1942264437675476
train_iter_loss: 0.34049752354621887
train_iter_loss: 0.18217827379703522
train_iter_loss: 0.09990184009075165
train_iter_loss: 0.17903588712215424
train_iter_loss: 0.1572580635547638
train_iter_loss: 0.15654335916042328
train_iter_loss: 0.10150013864040375
train_iter_loss: 0.1608479917049408
train_iter_loss: 0.2037544995546341
train_iter_loss: 0.15099428594112396
train_iter_loss: 0.0995037630200386
train_iter_loss: 0.13397732377052307
train_iter_loss: 0.16739213466644287
train_iter_loss: 0.06389065086841583
train_iter_loss: 0.19502423703670502
train_iter_loss: 0.16889792680740356
train_iter_loss: 0.15771888196468353
train_iter_loss: 0.0836261436343193
train_iter_loss: 0.12556259334087372
train_iter_loss: 0.12019035220146179
train_iter_loss: 0.1282825469970703
train_iter_loss: 0.21494407951831818
train_iter_loss: 0.19418731331825256
train_iter_loss: 0.2055586576461792
train_iter_loss: 0.2259758710861206
train_iter_loss: 0.2560086250305176
train_iter_loss: 0.15574900805950165
train_iter_loss: 0.1411469727754593
train_iter_loss: 0.10651787370443344
train_iter_loss: 0.09402871876955032
train_iter_loss: 0.0947347953915596
train_iter_loss: 0.15053294599056244
train_iter_loss: 0.19675126671791077
train_iter_loss: 0.12969240546226501
train_iter_loss: 0.19405683875083923
train_iter_loss: 0.23181098699569702
train_iter_loss: 0.09175246208906174
train_iter_loss: 0.16052153706550598
train_iter_loss: 0.194789320230484
train_iter_loss: 0.14402766525745392
train_iter_loss: 0.10259845107793808
train_iter_loss: 0.15126486122608185
train_iter_loss: 0.13496233522891998
train_iter_loss: 0.18651807308197021
train_iter_loss: 0.09114377200603485
train_iter_loss: 0.13836702704429626
train_iter_loss: 0.15618060529232025
train_iter_loss: 0.09510583430528641
train_iter_loss: 0.15059132874011993
train_iter_loss: 0.15024395287036896
train_iter_loss: 0.14947888255119324
train_iter_loss: 0.19860564172267914
train_iter_loss: 0.05821522697806358
train_iter_loss: 0.12995709478855133
train loss :0.1589
---------------------
Validation seg loss: 0.2178513074891185 at epoch 478
epoch =    479/  1000, exp = train
train_iter_loss: 0.08203125
train_iter_loss: 0.11017079651355743
train_iter_loss: 0.13819248974323273
train_iter_loss: 0.0407278910279274
train_iter_loss: 0.16974882781505585
train_iter_loss: 0.04164716228842735
train_iter_loss: 0.2535352408885956
train_iter_loss: 0.15878595411777496
train_iter_loss: 0.2351757138967514
train_iter_loss: 0.16408346593379974
train_iter_loss: 0.17399142682552338
train_iter_loss: 0.18032225966453552
train_iter_loss: 0.21563111245632172
train_iter_loss: 0.0382605604827404
train_iter_loss: 0.19206203520298004
train_iter_loss: 0.15646673738956451
train_iter_loss: 0.12016937136650085
train_iter_loss: 0.08260371536016464
train_iter_loss: 0.1187419667840004
train_iter_loss: 0.14052040874958038
train_iter_loss: 0.2979969382286072
train_iter_loss: 0.17991939187049866
train_iter_loss: 0.14745140075683594
train_iter_loss: 0.05730968713760376
train_iter_loss: 0.13153420388698578
train_iter_loss: 0.13435401022434235
train_iter_loss: 0.2022349238395691
train_iter_loss: 0.20799575746059418
train_iter_loss: 0.17697441577911377
train_iter_loss: 0.23915109038352966
train_iter_loss: 0.31117331981658936
train_iter_loss: 0.08087725937366486
train_iter_loss: 0.17411686480045319
train_iter_loss: 0.2105255275964737
train_iter_loss: 0.1591958850622177
train_iter_loss: 0.1370512843132019
train_iter_loss: 0.11926565319299698
train_iter_loss: 0.1064266562461853
train_iter_loss: 0.1300903856754303
train_iter_loss: 0.08449328690767288
train_iter_loss: 0.176156684756279
train_iter_loss: 0.14311540126800537
train_iter_loss: 0.1450711339712143
train_iter_loss: 0.14875519275665283
train_iter_loss: 0.10619591176509857
train_iter_loss: 0.1386992484331131
train_iter_loss: 0.11289103329181671
train_iter_loss: 0.13671216368675232
train_iter_loss: 0.3176300823688507
train_iter_loss: 0.1811274290084839
train_iter_loss: 0.1815587729215622
train_iter_loss: 0.20145902037620544
train_iter_loss: 0.2066228687763214
train_iter_loss: 0.20999571681022644
train_iter_loss: 0.10669364035129547
train_iter_loss: 0.19891197979450226
train_iter_loss: 0.21227331459522247
train_iter_loss: 0.12079474329948425
train_iter_loss: 0.19186745584011078
train_iter_loss: 0.15867817401885986
train_iter_loss: 0.1265050768852234
train_iter_loss: 0.1689874529838562
train_iter_loss: 0.11703196167945862
train_iter_loss: 0.1342136412858963
train_iter_loss: 0.11646130681037903
train_iter_loss: 0.15346351265907288
train_iter_loss: 0.22110353410243988
train_iter_loss: 0.21553996205329895
train_iter_loss: 0.19268430769443512
train_iter_loss: 0.06541841477155685
train_iter_loss: 0.23507529497146606
train_iter_loss: 0.09092848747968674
train_iter_loss: 0.17751960456371307
train_iter_loss: 0.18613003194332123
train_iter_loss: 0.10059007257223129
train_iter_loss: 0.15818443894386292
train_iter_loss: 0.09467308223247528
train_iter_loss: 0.21991541981697083
train_iter_loss: 0.11894072592258453
train_iter_loss: 0.1491210013628006
train_iter_loss: 0.17109528183937073
train_iter_loss: 0.06041492894291878
train_iter_loss: 0.1935006082057953
train_iter_loss: 0.26422351598739624
train_iter_loss: 0.335605651140213
train_iter_loss: 0.3497985005378723
train_iter_loss: 0.24357165396213531
train_iter_loss: 0.1140427440404892
train_iter_loss: 0.22184045612812042
train_iter_loss: 0.03191829100251198
train_iter_loss: 0.15168997645378113
train_iter_loss: 0.13802407681941986
train_iter_loss: 0.17166496813297272
train_iter_loss: 0.16270595788955688
train_iter_loss: 0.06064721941947937
train_iter_loss: 0.15982972085475922
train_iter_loss: 0.15858057141304016
train_iter_loss: 0.15597933530807495
train_iter_loss: 0.23444817960262299
train_iter_loss: 0.22223564982414246
train loss :0.1630
---------------------
Validation seg loss: 0.2206028129132289 at epoch 479
epoch =    480/  1000, exp = train
train_iter_loss: 0.16126121580600739
train_iter_loss: 0.11264801025390625
train_iter_loss: 0.1191958487033844
train_iter_loss: 0.09853439778089523
train_iter_loss: 0.03732138127088547
train_iter_loss: 0.09257408231496811
train_iter_loss: 0.10888346284627914
train_iter_loss: 0.05332556739449501
train_iter_loss: 0.158461794257164
train_iter_loss: 0.1770288348197937
train_iter_loss: 0.20968791842460632
train_iter_loss: 0.12463278323411942
train_iter_loss: 0.118010014295578
train_iter_loss: 0.07676703482866287
train_iter_loss: 0.11639168858528137
train_iter_loss: 0.1258656233549118
train_iter_loss: 0.12601380050182343
train_iter_loss: 0.2388877272605896
train_iter_loss: 0.08373504877090454
train_iter_loss: 0.18028946220874786
train_iter_loss: 0.24782636761665344
train_iter_loss: 0.16903716325759888
train_iter_loss: 0.23144671320915222
train_iter_loss: 0.096138134598732
train_iter_loss: 0.1724427044391632
train_iter_loss: 0.13447479903697968
train_iter_loss: 0.13840916752815247
train_iter_loss: 0.13387903571128845
train_iter_loss: 0.10498235374689102
train_iter_loss: 0.35480624437332153
train_iter_loss: 0.1996445655822754
train_iter_loss: 0.2278454601764679
train_iter_loss: 0.12366499751806259
train_iter_loss: 0.17069277167320251
train_iter_loss: 0.08913259953260422
train_iter_loss: 0.24811656773090363
train_iter_loss: 0.16414722800254822
train_iter_loss: 0.2072710543870926
train_iter_loss: 0.1466027796268463
train_iter_loss: 0.25964879989624023
train_iter_loss: 0.09967375546693802
train_iter_loss: 0.20765191316604614
train_iter_loss: 0.17331542074680328
train_iter_loss: 0.07556114345788956
train_iter_loss: 0.1717589795589447
train_iter_loss: 0.22255146503448486
train_iter_loss: 0.11019513756036758
train_iter_loss: 0.15650932490825653
train_iter_loss: 0.18686449527740479
train_iter_loss: 0.16653728485107422
train_iter_loss: 0.23168984055519104
train_iter_loss: 0.08527738600969315
train_iter_loss: 0.19861628115177155
train_iter_loss: 0.2122773826122284
train_iter_loss: 0.15463262796401978
train_iter_loss: 0.1730339080095291
train_iter_loss: 0.1803089827299118
train_iter_loss: 0.15749326348304749
train_iter_loss: 0.07540155947208405
train_iter_loss: 0.12590903043746948
train_iter_loss: 0.10645173490047455
train_iter_loss: 0.11103860288858414
train_iter_loss: 0.2841170132160187
train_iter_loss: 0.18666991591453552
train_iter_loss: 0.24070098996162415
train_iter_loss: 0.16112099587917328
train_iter_loss: 0.11873416602611542
train_iter_loss: 0.21525225043296814
train_iter_loss: 0.16747228801250458
train_iter_loss: 0.20547036826610565
train_iter_loss: 0.24772357940673828
train_iter_loss: 0.13239791989326477
train_iter_loss: 0.15659940242767334
train_iter_loss: 0.14644576609134674
train_iter_loss: 0.09652276337146759
train_iter_loss: 0.19394676387310028
train_iter_loss: 0.17724667489528656
train_iter_loss: 0.19342683255672455
train_iter_loss: 0.12586373090744019
train_iter_loss: 0.19185547530651093
train_iter_loss: 0.12180888652801514
train_iter_loss: 0.1405218094587326
train_iter_loss: 0.13849668204784393
train_iter_loss: 0.088807612657547
train_iter_loss: 0.2496841847896576
train_iter_loss: 0.1307070404291153
train_iter_loss: 0.10430780053138733
train_iter_loss: 0.09310326725244522
train_iter_loss: 0.16351725161075592
train_iter_loss: 0.18568938970565796
train_iter_loss: 0.1184999942779541
train_iter_loss: 0.08851472288370132
train_iter_loss: 0.10458939522504807
train_iter_loss: 0.14511637389659882
train_iter_loss: 0.16161641478538513
train_iter_loss: 0.18673066794872284
train_iter_loss: 0.16442014276981354
train_iter_loss: 0.1390162706375122
train_iter_loss: 0.21289075911045074
train_iter_loss: 0.1599414199590683
train loss :0.1583
---------------------
Validation seg loss: 0.2159487374812224 at epoch 480
epoch =    481/  1000, exp = train
train_iter_loss: 0.10441877692937851
train_iter_loss: 0.26888585090637207
train_iter_loss: 0.06172260269522667
train_iter_loss: 0.1997123807668686
train_iter_loss: 0.13434763252735138
train_iter_loss: 0.07102472335100174
train_iter_loss: 0.11924079805612564
train_iter_loss: 0.2065427154302597
train_iter_loss: 0.04648888483643532
train_iter_loss: 0.3307124376296997
train_iter_loss: 0.2640543580055237
train_iter_loss: 0.21033142507076263
train_iter_loss: 0.12518586218357086
train_iter_loss: 0.11337348073720932
train_iter_loss: 0.21132643520832062
train_iter_loss: 0.10157694667577744
train_iter_loss: 0.16004931926727295
train_iter_loss: 0.1061720997095108
train_iter_loss: 0.06878167390823364
train_iter_loss: 0.15473519265651703
train_iter_loss: 0.2387951910495758
train_iter_loss: 0.10532764345407486
train_iter_loss: 0.08992695063352585
train_iter_loss: 0.08520203083753586
train_iter_loss: 0.13773025572299957
train_iter_loss: 0.4002532660961151
train_iter_loss: 0.07161790877580643
train_iter_loss: 0.1132732480764389
train_iter_loss: 0.06749323755502701
train_iter_loss: 0.37872007489204407
train_iter_loss: 0.13803546130657196
train_iter_loss: 0.1287289410829544
train_iter_loss: 0.12760719656944275
train_iter_loss: 0.23116157948970795
train_iter_loss: 0.1351478397846222
train_iter_loss: 0.10788028687238693
train_iter_loss: 0.07745026051998138
train_iter_loss: 0.4344736337661743
train_iter_loss: 0.2550843060016632
train_iter_loss: 0.3518945574760437
train_iter_loss: 0.1580222100019455
train_iter_loss: 0.14672526717185974
train_iter_loss: 0.16425612568855286
train_iter_loss: 0.27744922041893005
train_iter_loss: 0.1156884953379631
train_iter_loss: 0.1943666785955429
train_iter_loss: 0.18459948897361755
train_iter_loss: 0.18550100922584534
train_iter_loss: 0.1944105625152588
train_iter_loss: 0.09819751232862473
train_iter_loss: 0.08915117383003235
train_iter_loss: 0.3076513409614563
train_iter_loss: 0.14675866067409515
train_iter_loss: 0.15025489032268524
train_iter_loss: 0.19683891534805298
train_iter_loss: 0.27325427532196045
train_iter_loss: 0.09073144942522049
train_iter_loss: 0.16279323399066925
train_iter_loss: 0.3484499752521515
train_iter_loss: 0.12143656611442566
train_iter_loss: 0.16186676919460297
train_iter_loss: 0.21277284622192383
train_iter_loss: 0.17093996703624725
train_iter_loss: 0.19445392489433289
train_iter_loss: 0.08548027276992798
train_iter_loss: 0.13485968112945557
train_iter_loss: 0.10939927399158478
train_iter_loss: 0.10067199915647507
train_iter_loss: 0.209771066904068
train_iter_loss: 0.12786751985549927
train_iter_loss: 0.16468751430511475
train_iter_loss: 0.0838184580206871
train_iter_loss: 0.14012570679187775
train_iter_loss: 0.1458478569984436
train_iter_loss: 0.18669724464416504
train_iter_loss: 0.12796074151992798
train_iter_loss: 0.13413259387016296
train_iter_loss: 0.14255599677562714
train_iter_loss: 0.14932526648044586
train_iter_loss: 0.22201277315616608
train_iter_loss: 0.18193431198596954
train_iter_loss: 0.12164764851331711
train_iter_loss: 0.1662003993988037
train_iter_loss: 0.17888876795768738
train_iter_loss: 0.14604316651821136
train_iter_loss: 0.055431101471185684
train_iter_loss: 0.20424214005470276
train_iter_loss: 0.1488526612520218
train_iter_loss: 0.29544416069984436
train_iter_loss: 0.15640679001808167
train_iter_loss: 0.11396735161542892
train_iter_loss: 0.2530481517314911
train_iter_loss: 0.173851877450943
train_iter_loss: 0.15062585473060608
train_iter_loss: 0.1779797375202179
train_iter_loss: 0.049685683101415634
train_iter_loss: 0.23134179413318634
train_iter_loss: 0.10600192844867706
train_iter_loss: 0.1484755128622055
train_iter_loss: 0.11681051552295685
train loss :0.1668
---------------------
Validation seg loss: 0.2180260865474647 at epoch 481
epoch =    482/  1000, exp = train
train_iter_loss: 0.08735829591751099
train_iter_loss: 0.16107679903507233
train_iter_loss: 0.22481057047843933
train_iter_loss: 0.17546482384204865
train_iter_loss: 0.14315123856067657
train_iter_loss: 0.16876834630966187
train_iter_loss: 0.16091860830783844
train_iter_loss: 0.1036013513803482
train_iter_loss: 0.1881694346666336
train_iter_loss: 0.1879255473613739
train_iter_loss: 0.15045705437660217
train_iter_loss: 0.0411013625562191
train_iter_loss: 0.11551317572593689
train_iter_loss: 0.1801716387271881
train_iter_loss: 0.22377683222293854
train_iter_loss: 0.17329944670200348
train_iter_loss: 0.2508511543273926
train_iter_loss: 0.1654302477836609
train_iter_loss: 0.1756044626235962
train_iter_loss: 0.18030507862567902
train_iter_loss: 0.05392323061823845
train_iter_loss: 0.05664583668112755
train_iter_loss: 0.18837934732437134
train_iter_loss: 0.22570595145225525
train_iter_loss: 0.09850151836872101
train_iter_loss: 0.22731821238994598
train_iter_loss: 0.1836463212966919
train_iter_loss: 0.1272049993276596
train_iter_loss: 0.20301178097724915
train_iter_loss: 0.17166955769062042
train_iter_loss: 0.11809486150741577
train_iter_loss: 0.15091748535633087
train_iter_loss: 0.2249794751405716
train_iter_loss: 0.09822410345077515
train_iter_loss: 0.05968056991696358
train_iter_loss: 0.15047721564769745
train_iter_loss: 0.09941001236438751
train_iter_loss: 0.16227656602859497
train_iter_loss: 0.3150627911090851
train_iter_loss: 0.20094044506549835
train_iter_loss: 0.1544271558523178
train_iter_loss: 0.13306450843811035
train_iter_loss: 0.19114091992378235
train_iter_loss: 0.13704067468643188
train_iter_loss: 0.2380961924791336
train_iter_loss: 0.14219368994235992
train_iter_loss: 0.08325255662202835
train_iter_loss: 0.2654237151145935
train_iter_loss: 0.11573316901922226
train_iter_loss: 0.18078573048114777
train_iter_loss: 0.10667117685079575
train_iter_loss: 0.21633882820606232
train_iter_loss: 0.21709296107292175
train_iter_loss: 0.14375488460063934
train_iter_loss: 0.06484796851873398
train_iter_loss: 0.16039659082889557
train_iter_loss: 0.13429111242294312
train_iter_loss: 0.10765587538480759
train_iter_loss: 0.17460185289382935
train_iter_loss: 0.17000946402549744
train_iter_loss: 0.16584628820419312
train_iter_loss: 0.1720283478498459
train_iter_loss: 0.17693522572517395
train_iter_loss: 0.20108085870742798
train_iter_loss: 0.0991850197315216
train_iter_loss: 0.11627984791994095
train_iter_loss: 0.13217347860336304
train_iter_loss: 0.11724969744682312
train_iter_loss: 0.14901679754257202
train_iter_loss: 0.1803215891122818
train_iter_loss: 0.21734781563282013
train_iter_loss: 0.22242270410060883
train_iter_loss: 0.14644597470760345
train_iter_loss: 0.13438935577869415
train_iter_loss: 0.12040895968675613
train_iter_loss: 0.13765797019004822
train_iter_loss: 0.130092054605484
train_iter_loss: 0.23261673748493195
train_iter_loss: 0.09193829447031021
train_iter_loss: 0.15026623010635376
train_iter_loss: 0.20227496325969696
train_iter_loss: 0.1631588190793991
train_iter_loss: 0.13042804598808289
train_iter_loss: 0.3034942150115967
train_iter_loss: 0.1366371512413025
train_iter_loss: 0.1049734428524971
train_iter_loss: 0.07348371297121048
train_iter_loss: 0.11561733484268188
train_iter_loss: 0.14655320346355438
train_iter_loss: 0.18514835834503174
train_iter_loss: 0.15542088449001312
train_iter_loss: 0.10064397007226944
train_iter_loss: 0.10390431433916092
train_iter_loss: 0.13706298172473907
train_iter_loss: 0.1679304540157318
train_iter_loss: 0.12091013789176941
train_iter_loss: 0.10073293745517731
train_iter_loss: 0.18880797922611237
train_iter_loss: 0.17440250515937805
train_iter_loss: 0.15739372372627258
train loss :0.1574
---------------------
Validation seg loss: 0.2167346508962647 at epoch 482
epoch =    483/  1000, exp = train
train_iter_loss: 0.17369699478149414
train_iter_loss: 0.1313876211643219
train_iter_loss: 0.09804180264472961
train_iter_loss: 0.1661088615655899
train_iter_loss: 0.08789777010679245
train_iter_loss: 0.23048019409179688
train_iter_loss: 0.1685411036014557
train_iter_loss: 0.0696149691939354
train_iter_loss: 0.13509169220924377
train_iter_loss: 0.21752527356147766
train_iter_loss: 0.24240775406360626
train_iter_loss: 0.08664600551128387
train_iter_loss: 0.19237755239009857
train_iter_loss: 0.16874083876609802
train_iter_loss: 0.149330273270607
train_iter_loss: 0.1524457037448883
train_iter_loss: 0.1689004898071289
train_iter_loss: 0.14339272677898407
train_iter_loss: 0.16634228825569153
train_iter_loss: 0.29798901081085205
train_iter_loss: 0.0431474894285202
train_iter_loss: 0.19999487698078156
train_iter_loss: 0.226784810423851
train_iter_loss: 0.11997728049755096
train_iter_loss: 0.16685885190963745
train_iter_loss: 0.07490527629852295
train_iter_loss: 0.22836390137672424
train_iter_loss: 0.1801733672618866
train_iter_loss: 0.1338987946510315
train_iter_loss: 0.08973879367113113
train_iter_loss: 0.2380913496017456
train_iter_loss: 0.10200870037078857
train_iter_loss: 0.12649361789226532
train_iter_loss: 0.08820138871669769
train_iter_loss: 0.1501568704843521
train_iter_loss: 0.10961484163999557
train_iter_loss: 0.155946284532547
train_iter_loss: 0.09887804836034775
train_iter_loss: 0.16219916939735413
train_iter_loss: 0.12171172350645065
train_iter_loss: 0.18240658938884735
train_iter_loss: 0.21033018827438354
train_iter_loss: 0.2745077908039093
train_iter_loss: 0.08567901700735092
train_iter_loss: 0.1513376086950302
train_iter_loss: 0.11570040136575699
train_iter_loss: 0.20889702439308167
train_iter_loss: 0.16481055319309235
train_iter_loss: 0.21648743748664856
train_iter_loss: 0.1244039386510849
train_iter_loss: 0.0932692214846611
train_iter_loss: 0.1039714366197586
train_iter_loss: 0.07296121120452881
train_iter_loss: 0.17417310178279877
train_iter_loss: 0.20940712094306946
train_iter_loss: 0.16154612600803375
train_iter_loss: 0.08936332911252975
train_iter_loss: 0.032911043614149094
train_iter_loss: 0.1541157215833664
train_iter_loss: 0.14007750153541565
train_iter_loss: 0.23532059788703918
train_iter_loss: 0.15840087831020355
train_iter_loss: 0.2250087857246399
train_iter_loss: 0.21589608490467072
train_iter_loss: 0.18847200274467468
train_iter_loss: 0.18127720057964325
train_iter_loss: 0.13152430951595306
train_iter_loss: 0.30940374732017517
train_iter_loss: 0.176466703414917
train_iter_loss: 0.15102697908878326
train_iter_loss: 0.14738847315311432
train_iter_loss: 0.396364688873291
train_iter_loss: 0.1063963919878006
train_iter_loss: 0.15242768824100494
train_iter_loss: 0.20128367841243744
train_iter_loss: 0.124632328748703
train_iter_loss: 0.06500491499900818
train_iter_loss: 0.10518180578947067
train_iter_loss: 0.1422477811574936
train_iter_loss: 0.06209408491849899
train_iter_loss: 0.1556747555732727
train_iter_loss: 0.15999075770378113
train_iter_loss: 0.17390653491020203
train_iter_loss: 0.18724025785923004
train_iter_loss: 0.2897014021873474
train_iter_loss: 0.4045090675354004
train_iter_loss: 0.06516231596469879
train_iter_loss: 0.100336953997612
train_iter_loss: 0.18081076443195343
train_iter_loss: 0.2080865055322647
train_iter_loss: 0.11677569895982742
train_iter_loss: 0.1676226705312729
train_iter_loss: 0.1959650069475174
train_iter_loss: 0.0735105648636818
train_iter_loss: 0.14096614718437195
train_iter_loss: 0.14162962138652802
train_iter_loss: 0.20622003078460693
train_iter_loss: 0.13015593588352203
train_iter_loss: 0.08904600143432617
train_iter_loss: 0.18046258389949799
train loss :0.1603
---------------------
Validation seg loss: 0.21996669302571495 at epoch 483
epoch =    484/  1000, exp = train
train_iter_loss: 0.26305896043777466
train_iter_loss: 0.15208247303962708
train_iter_loss: 0.20315368473529816
train_iter_loss: 0.17633984982967377
train_iter_loss: 0.25534456968307495
train_iter_loss: 0.2091670036315918
train_iter_loss: 0.05157383158802986
train_iter_loss: 0.05214143544435501
train_iter_loss: 0.1721940040588379
train_iter_loss: 0.17635223269462585
train_iter_loss: 0.37582266330718994
train_iter_loss: 0.1377314031124115
train_iter_loss: 0.13940051198005676
train_iter_loss: 0.17414851486682892
train_iter_loss: 0.18869651854038239
train_iter_loss: 0.23377500474452972
train_iter_loss: 0.11634327471256256
train_iter_loss: 0.1552286595106125
train_iter_loss: 0.2004868984222412
train_iter_loss: 0.15360026061534882
train_iter_loss: 0.1226281225681305
train_iter_loss: 0.10148441046476364
train_iter_loss: 0.20419679582118988
train_iter_loss: 0.12488101422786713
train_iter_loss: 0.1806327849626541
train_iter_loss: 0.19098688662052155
train_iter_loss: 0.19037547707557678
train_iter_loss: 0.14080913364887238
train_iter_loss: 0.2104327231645584
train_iter_loss: 0.15949714183807373
train_iter_loss: 0.14612580835819244
train_iter_loss: 0.09959504008293152
train_iter_loss: 0.18353688716888428
train_iter_loss: 0.12254134565591812
train_iter_loss: 0.1909724920988083
train_iter_loss: 0.20939041674137115
train_iter_loss: 0.12511691451072693
train_iter_loss: 0.1229906752705574
train_iter_loss: 0.15589381754398346
train_iter_loss: 0.18131451308727264
train_iter_loss: 0.2161989063024521
train_iter_loss: 0.1404247134923935
train_iter_loss: 0.10463087260723114
train_iter_loss: 0.16585673391819
train_iter_loss: 0.09652344137430191
train_iter_loss: 0.1954335719347
train_iter_loss: 0.17562749981880188
train_iter_loss: 0.19041210412979126
train_iter_loss: 0.14527477324008942
train_iter_loss: 0.08380161225795746
train_iter_loss: 0.1962461620569229
train_iter_loss: 0.23147359490394592
train_iter_loss: 0.13883018493652344
train_iter_loss: 0.09141195565462112
train_iter_loss: 0.2588656544685364
train_iter_loss: 0.19183564186096191
train_iter_loss: 0.259158194065094
train_iter_loss: 0.20224905014038086
train_iter_loss: 0.1336616724729538
train_iter_loss: 0.16559264063835144
train_iter_loss: 0.15199097990989685
train_iter_loss: 0.12173884361982346
train_iter_loss: 0.10899582505226135
train_iter_loss: 0.19717013835906982
train_iter_loss: 0.04790783300995827
train_iter_loss: 0.06069290265440941
train_iter_loss: 0.3252571225166321
train_iter_loss: 0.2423630654811859
train_iter_loss: 0.11321049183607101
train_iter_loss: 0.1297052651643753
train_iter_loss: 0.16495366394519806
train_iter_loss: 0.2011750340461731
train_iter_loss: 0.07831469923257828
train_iter_loss: 0.22765938937664032
train_iter_loss: 0.129159078001976
train_iter_loss: 0.27652493119239807
train_iter_loss: 0.036881040781736374
train_iter_loss: 0.032080378383398056
train_iter_loss: 0.15458989143371582
train_iter_loss: 0.20526404678821564
train_iter_loss: 0.24725772440433502
train_iter_loss: 0.1908542364835739
train_iter_loss: 0.10080016404390335
train_iter_loss: 0.14241105318069458
train_iter_loss: 0.20492975413799286
train_iter_loss: 0.053779251873493195
train_iter_loss: 0.18952879309654236
train_iter_loss: 0.22640225291252136
train_iter_loss: 0.25540247559547424
train_iter_loss: 0.1136736273765564
train_iter_loss: 0.0973500981926918
train_iter_loss: 0.13567598164081573
train_iter_loss: 0.257156640291214
train_iter_loss: 0.3003140985965729
train_iter_loss: 0.08643603324890137
train_iter_loss: 0.11386176198720932
train_iter_loss: 0.21817930042743683
train_iter_loss: 0.14776407182216644
train_iter_loss: 0.1315615475177765
train_iter_loss: 0.23167425394058228
train loss :0.1674
---------------------
Validation seg loss: 0.2231178971640063 at epoch 484
epoch =    485/  1000, exp = train
train_iter_loss: 0.1211131289601326
train_iter_loss: 0.11171823740005493
train_iter_loss: 0.11620831489562988
train_iter_loss: 0.14432165026664734
train_iter_loss: 0.15415798127651215
train_iter_loss: 0.21421316266059875
train_iter_loss: 0.18207091093063354
train_iter_loss: 0.16435585916042328
train_iter_loss: 0.07179597020149231
train_iter_loss: 0.19652687013149261
train_iter_loss: 0.16694088280200958
train_iter_loss: 0.14690379798412323
train_iter_loss: 0.14819581806659698
train_iter_loss: 0.12314975261688232
train_iter_loss: 0.2494036704301834
train_iter_loss: 0.157391756772995
train_iter_loss: 0.20820987224578857
train_iter_loss: 0.0836438536643982
train_iter_loss: 0.1873065084218979
train_iter_loss: 0.08419099450111389
train_iter_loss: 0.1288989633321762
train_iter_loss: 0.08695802837610245
train_iter_loss: 0.13732321560382843
train_iter_loss: 0.128446564078331
train_iter_loss: 0.07757382094860077
train_iter_loss: 0.13633319735527039
train_iter_loss: 0.10723525285720825
train_iter_loss: 0.19359531998634338
train_iter_loss: 0.17948588728904724
train_iter_loss: 0.09166023135185242
train_iter_loss: 0.06721256673336029
train_iter_loss: 0.11550635099411011
train_iter_loss: 0.19613733887672424
train_iter_loss: 0.2231214940547943
train_iter_loss: 0.10522383451461792
train_iter_loss: 0.1836249679327011
train_iter_loss: 0.1833479404449463
train_iter_loss: 0.21589559316635132
train_iter_loss: 0.2795450687408447
train_iter_loss: 0.193078875541687
train_iter_loss: 0.1806211620569229
train_iter_loss: 0.18744708597660065
train_iter_loss: 0.14136512577533722
train_iter_loss: 0.10736972093582153
train_iter_loss: 0.22747789323329926
train_iter_loss: 0.2093590795993805
train_iter_loss: 0.1303626000881195
train_iter_loss: 0.18374662101268768
train_iter_loss: 0.09252933412790298
train_iter_loss: 0.20352454483509064
train_iter_loss: 0.16918419301509857
train_iter_loss: 0.150958850979805
train_iter_loss: 0.21182109415531158
train_iter_loss: 0.1678355485200882
train_iter_loss: 0.2097601294517517
train_iter_loss: 0.18440017104148865
train_iter_loss: 0.1032777950167656
train_iter_loss: 0.11433713883161545
train_iter_loss: 0.06880933791399002
train_iter_loss: 0.07824505120515823
train_iter_loss: 0.2047765552997589
train_iter_loss: 0.2617914080619812
train_iter_loss: 0.06906872242689133
train_iter_loss: 0.16158173978328705
train_iter_loss: 0.21318234503269196
train_iter_loss: 0.15116171538829803
train_iter_loss: 0.1367844045162201
train_iter_loss: 0.11908566951751709
train_iter_loss: 0.20439523458480835
train_iter_loss: 0.18839794397354126
train_iter_loss: 0.20357339084148407
train_iter_loss: 0.1972384750843048
train_iter_loss: 0.24209976196289062
train_iter_loss: 0.08487839251756668
train_iter_loss: 0.10390760749578476
train_iter_loss: 0.17253361642360687
train_iter_loss: 0.09718704223632812
train_iter_loss: 0.10496740788221359
train_iter_loss: 0.11620543897151947
train_iter_loss: 0.16827857494354248
train_iter_loss: 0.22904355823993683
train_iter_loss: 0.1782049834728241
train_iter_loss: 0.18405334651470184
train_iter_loss: 0.17123398184776306
train_iter_loss: 0.08076901733875275
train_iter_loss: 0.22283518314361572
train_iter_loss: 0.07412798702716827
train_iter_loss: 0.19221481680870056
train_iter_loss: 0.10089225322008133
train_iter_loss: 0.1320442408323288
train_iter_loss: 0.19888608157634735
train_iter_loss: 0.129560187458992
train_iter_loss: 0.2633918821811676
train_iter_loss: 0.047745998948812485
train_iter_loss: 0.18248572945594788
train_iter_loss: 0.1591666340827942
train_iter_loss: 0.15268176794052124
train_iter_loss: 0.20277351140975952
train_iter_loss: 0.11104842275381088
train_iter_loss: 0.06068366765975952
train loss :0.1563
---------------------
Validation seg loss: 0.2204326070528829 at epoch 485
epoch =    486/  1000, exp = train
train_iter_loss: 0.2814154028892517
train_iter_loss: 0.3255714178085327
train_iter_loss: 0.2039991170167923
train_iter_loss: 0.15263250470161438
train_iter_loss: 0.05259696766734123
train_iter_loss: 0.21271610260009766
train_iter_loss: 0.2833282947540283
train_iter_loss: 0.13112907111644745
train_iter_loss: 0.1574746072292328
train_iter_loss: 0.1273326277732849
train_iter_loss: 0.15767954289913177
train_iter_loss: 0.08055280894041061
train_iter_loss: 0.1274363100528717
train_iter_loss: 0.17076164484024048
train_iter_loss: 0.1398979127407074
train_iter_loss: 0.1559012532234192
train_iter_loss: 0.16705986857414246
train_iter_loss: 0.17251449823379517
train_iter_loss: 0.12632931768894196
train_iter_loss: 0.26294252276420593
train_iter_loss: 0.19797085225582123
train_iter_loss: 0.20088283717632294
train_iter_loss: 0.0754028931260109
train_iter_loss: 0.21103663742542267
train_iter_loss: 0.20699754357337952
train_iter_loss: 0.26106369495391846
train_iter_loss: 0.08929947018623352
train_iter_loss: 0.1140144020318985
train_iter_loss: 0.1750851720571518
train_iter_loss: 0.11128638684749603
train_iter_loss: 0.1731051504611969
train_iter_loss: 0.11717335879802704
train_iter_loss: 0.18840141594409943
train_iter_loss: 0.1321936994791031
train_iter_loss: 0.1353641301393509
train_iter_loss: 0.1748862862586975
train_iter_loss: 0.11462832987308502
train_iter_loss: 0.11142228543758392
train_iter_loss: 0.1348717212677002
train_iter_loss: 0.1359502375125885
train_iter_loss: 0.14478248357772827
train_iter_loss: 0.19796347618103027
train_iter_loss: 0.1882636696100235
train_iter_loss: 0.04665910452604294
train_iter_loss: 0.16578471660614014
train_iter_loss: 0.08382569998502731
train_iter_loss: 0.18145549297332764
train_iter_loss: 0.31739330291748047
train_iter_loss: 0.13675829768180847
train_iter_loss: 0.1795308142900467
train_iter_loss: 0.13026313483715057
train_iter_loss: 0.14828699827194214
train_iter_loss: 0.152915820479393
train_iter_loss: 0.12983867526054382
train_iter_loss: 0.1569032371044159
train_iter_loss: 0.16365474462509155
train_iter_loss: 0.30184492468833923
train_iter_loss: 0.20184969902038574
train_iter_loss: 0.15120691061019897
train_iter_loss: 0.19387982785701752
train_iter_loss: 0.1332358717918396
train_iter_loss: 0.1295015662908554
train_iter_loss: 0.13823676109313965
train_iter_loss: 0.13818323612213135
train_iter_loss: 0.20275525748729706
train_iter_loss: 0.37265273928642273
train_iter_loss: 0.165755495429039
train_iter_loss: 0.08755357563495636
train_iter_loss: 0.1059669554233551
train_iter_loss: 0.10310342907905579
train_iter_loss: 0.022168057039380074
train_iter_loss: 0.08900298178195953
train_iter_loss: 0.1748020052909851
train_iter_loss: 0.15691104531288147
train_iter_loss: 0.2360392063856125
train_iter_loss: 0.10936825722455978
train_iter_loss: 0.10617634654045105
train_iter_loss: 0.2384955734014511
train_iter_loss: 0.07375001162290573
train_iter_loss: 0.0970425233244896
train_iter_loss: 0.13072244822978973
train_iter_loss: 0.1714567244052887
train_iter_loss: 0.13418333232402802
train_iter_loss: 0.16350263357162476
train_iter_loss: 0.2036563754081726
train_iter_loss: 0.13538622856140137
train_iter_loss: 0.09445970505475998
train_iter_loss: 0.09025462716817856
train_iter_loss: 0.18197685480117798
train_iter_loss: 0.15875662863254547
train_iter_loss: 0.16737155616283417
train_iter_loss: 0.11449628323316574
train_iter_loss: 0.17099466919898987
train_iter_loss: 0.13884831964969635
train_iter_loss: 0.06949397176504135
train_iter_loss: 0.08834104984998703
train_iter_loss: 0.10225420445203781
train_iter_loss: 0.15183335542678833
train_iter_loss: 0.2789379954338074
train_iter_loss: 0.3160424828529358
train loss :0.1602
---------------------
Validation seg loss: 0.22200660224793092 at epoch 486
epoch =    487/  1000, exp = train
train_iter_loss: 0.1069445088505745
train_iter_loss: 0.19180889427661896
train_iter_loss: 0.14065131545066833
train_iter_loss: 0.13718847930431366
train_iter_loss: 0.16393175721168518
train_iter_loss: 0.14752835035324097
train_iter_loss: 0.18910890817642212
train_iter_loss: 0.07911983132362366
train_iter_loss: 0.14223448932170868
train_iter_loss: 0.10921581834554672
train_iter_loss: 0.18332745134830475
train_iter_loss: 0.27056145668029785
train_iter_loss: 0.0951576754450798
train_iter_loss: 0.3173084259033203
train_iter_loss: 0.20663072168827057
train_iter_loss: 0.1307896077632904
train_iter_loss: 0.26762160658836365
train_iter_loss: 0.15891802310943604
train_iter_loss: 0.16297881305217743
train_iter_loss: 0.16337960958480835
train_iter_loss: 0.08978752046823502
train_iter_loss: 0.08339524269104004
train_iter_loss: 0.1442435085773468
train_iter_loss: 0.12847934663295746
train_iter_loss: 0.2937089800834656
train_iter_loss: 0.2225450724363327
train_iter_loss: 0.2254103720188141
train_iter_loss: 0.1894342452287674
train_iter_loss: 0.196266308426857
train_iter_loss: 0.11224787682294846
train_iter_loss: 0.16133540868759155
train_iter_loss: 0.21487687528133392
train_iter_loss: 0.06975521147251129
train_iter_loss: 0.1144004613161087
train_iter_loss: 0.3066456913948059
train_iter_loss: 0.25801679491996765
train_iter_loss: 0.12671217322349548
train_iter_loss: 0.11782782524824142
train_iter_loss: 0.1430603414773941
train_iter_loss: 0.3187927305698395
train_iter_loss: 0.15572971105575562
train_iter_loss: 0.2707875669002533
train_iter_loss: 0.19981275498867035
train_iter_loss: 0.10322438925504684
train_iter_loss: 0.11781220883131027
train_iter_loss: 0.1285153478384018
train_iter_loss: 0.10656123608350754
train_iter_loss: 0.14397436380386353
train_iter_loss: 0.10044410824775696
train_iter_loss: 0.15355223417282104
train_iter_loss: 0.11718426644802094
train_iter_loss: 0.07478398084640503
train_iter_loss: 0.08064733445644379
train_iter_loss: 0.17689718306064606
train_iter_loss: 0.18391387164592743
train_iter_loss: 0.21882738173007965
train_iter_loss: 0.31204909086227417
train_iter_loss: 0.09820862859487534
train_iter_loss: 0.12511028349399567
train_iter_loss: 0.12796519696712494
train_iter_loss: 0.09208394587039948
train_iter_loss: 0.1544870138168335
train_iter_loss: 0.1189870834350586
train_iter_loss: 0.21057569980621338
train_iter_loss: 0.14245572686195374
train_iter_loss: 0.15974143147468567
train_iter_loss: 0.1021185964345932
train_iter_loss: 0.19673863053321838
train_iter_loss: 0.19459159672260284
train_iter_loss: 0.18078972399234772
train_iter_loss: 0.27955883741378784
train_iter_loss: 0.22273850440979004
train_iter_loss: 0.1741398721933365
train_iter_loss: 0.18913264572620392
train_iter_loss: 0.20686595141887665
train_iter_loss: 0.12870870530605316
train_iter_loss: 0.1177346333861351
train_iter_loss: 0.16107332706451416
train_iter_loss: 0.14981594681739807
train_iter_loss: 0.18161597847938538
train_iter_loss: 0.19223350286483765
train_iter_loss: 0.11842145025730133
train_iter_loss: 0.27359044551849365
train_iter_loss: 0.2001357525587082
train_iter_loss: 0.2257537692785263
train_iter_loss: 0.12165385484695435
train_iter_loss: 0.19499656558036804
train_iter_loss: 0.08496282249689102
train_iter_loss: 0.15934696793556213
train_iter_loss: 0.2327105700969696
train_iter_loss: 0.23217663168907166
train_iter_loss: 0.14268867671489716
train_iter_loss: 0.0944378525018692
train_iter_loss: 0.1985304206609726
train_iter_loss: 0.1793348342180252
train_iter_loss: 0.1713036447763443
train_iter_loss: 0.1330469697713852
train_iter_loss: 0.0730670616030693
train_iter_loss: 0.0872451439499855
train_iter_loss: 0.13031871616840363
train loss :0.1665
---------------------
Validation seg loss: 0.21725362287131403 at epoch 487
epoch =    488/  1000, exp = train
train_iter_loss: 0.19953922927379608
train_iter_loss: 0.08717633038759232
train_iter_loss: 0.11569438129663467
train_iter_loss: 0.16736911237239838
train_iter_loss: 0.13715937733650208
train_iter_loss: 0.1441049873828888
train_iter_loss: 0.2114199995994568
train_iter_loss: 0.36626166105270386
train_iter_loss: 0.13776953518390656
train_iter_loss: 0.14454832673072815
train_iter_loss: 0.07931974530220032
train_iter_loss: 0.1455826461315155
train_iter_loss: 0.13096271455287933
train_iter_loss: 0.12342838943004608
train_iter_loss: 0.17190371453762054
train_iter_loss: 0.34220853447914124
train_iter_loss: 0.11770088225603104
train_iter_loss: 0.1820063591003418
train_iter_loss: 0.24747350811958313
train_iter_loss: 0.20084328949451447
train_iter_loss: 0.19830626249313354
train_iter_loss: 0.16954359412193298
train_iter_loss: 0.13223375380039215
train_iter_loss: 0.05864414572715759
train_iter_loss: 0.1772080957889557
train_iter_loss: 0.15921847522258759
train_iter_loss: 0.1472025215625763
train_iter_loss: 0.20285971462726593
train_iter_loss: 0.12173157185316086
train_iter_loss: 0.12781190872192383
train_iter_loss: 0.18223139643669128
train_iter_loss: 0.13288572430610657
train_iter_loss: 0.056809164583683014
train_iter_loss: 0.13343116641044617
train_iter_loss: 0.21043454110622406
train_iter_loss: 0.1867816299200058
train_iter_loss: 0.15846671164035797
train_iter_loss: 0.1735389232635498
train_iter_loss: 0.1548403799533844
train_iter_loss: 0.13605159521102905
train_iter_loss: 0.10572053492069244
train_iter_loss: 0.16899801790714264
train_iter_loss: 0.06011590361595154
train_iter_loss: 0.22174327075481415
train_iter_loss: 0.11843764036893845
train_iter_loss: 0.19226239621639252
train_iter_loss: 0.116428442299366
train_iter_loss: 0.18629515171051025
train_iter_loss: 0.2913486063480377
train_iter_loss: 0.36826562881469727
train_iter_loss: 0.11474645882844925
train_iter_loss: 0.06017717719078064
train_iter_loss: 0.1945093423128128
train_iter_loss: 0.12139169126749039
train_iter_loss: 0.19604982435703278
train_iter_loss: 0.32678622007369995
train_iter_loss: 0.14211499691009521
train_iter_loss: 0.06623021513223648
train_iter_loss: 0.3251277208328247
train_iter_loss: 0.129599928855896
train_iter_loss: 0.23263421654701233
train_iter_loss: 0.09113559126853943
train_iter_loss: 0.17696306109428406
train_iter_loss: 0.1610722541809082
train_iter_loss: 0.17883795499801636
train_iter_loss: 0.18067611753940582
train_iter_loss: 0.11891291290521622
train_iter_loss: 0.13437534868717194
train_iter_loss: 0.15374869108200073
train_iter_loss: 0.12640640139579773
train_iter_loss: 0.1701866090297699
train_iter_loss: 0.1709166020154953
train_iter_loss: 0.14754484593868256
train_iter_loss: 0.1541975736618042
train_iter_loss: 0.18748758733272552
train_iter_loss: 0.22698354721069336
train_iter_loss: 0.15803326666355133
train_iter_loss: 0.1454150229692459
train_iter_loss: 0.14560656249523163
train_iter_loss: 0.21649262309074402
train_iter_loss: 0.11666689068078995
train_iter_loss: 0.13797657191753387
train_iter_loss: 0.08188552409410477
train_iter_loss: 0.1903497278690338
train_iter_loss: 0.1870918869972229
train_iter_loss: 0.13612070679664612
train_iter_loss: 0.13032959401607513
train_iter_loss: 0.19124658405780792
train_iter_loss: 0.1264580637216568
train_iter_loss: 0.10495275259017944
train_iter_loss: 0.09477131813764572
train_iter_loss: 0.20938217639923096
train_iter_loss: 0.21041235327720642
train_iter_loss: 0.32915198802948
train_iter_loss: 0.21016494929790497
train_iter_loss: 0.0999579206109047
train_iter_loss: 0.04707302525639534
train_iter_loss: 0.11534600704908371
train_iter_loss: 0.10974853485822678
train_iter_loss: 0.14215321838855743
train loss :0.1639
---------------------
Validation seg loss: 0.21738527056251494 at epoch 488
epoch =    489/  1000, exp = train
train_iter_loss: 0.10442361980676651
train_iter_loss: 0.02055857889354229
train_iter_loss: 0.2206016182899475
train_iter_loss: 0.2680239975452423
train_iter_loss: 0.08039474487304688
train_iter_loss: 0.10094545781612396
train_iter_loss: 0.1859986037015915
train_iter_loss: 0.2249515950679779
train_iter_loss: 0.08002442121505737
train_iter_loss: 0.23552200198173523
train_iter_loss: 0.15788833796977997
train_iter_loss: 0.3071601092815399
train_iter_loss: 0.16502462327480316
train_iter_loss: 0.183854877948761
train_iter_loss: 0.09205354005098343
train_iter_loss: 0.14755836129188538
train_iter_loss: 0.09927160292863846
train_iter_loss: 0.1932147592306137
train_iter_loss: 0.13085415959358215
train_iter_loss: 0.1985209435224533
train_iter_loss: 0.2486826777458191
train_iter_loss: 0.205967977643013
train_iter_loss: 0.16028757393360138
train_iter_loss: 0.09208802878856659
train_iter_loss: 0.09825778752565384
train_iter_loss: 0.21302169561386108
train_iter_loss: 0.045093875378370285
train_iter_loss: 0.17387211322784424
train_iter_loss: 0.12801437079906464
train_iter_loss: 0.1762700229883194
train_iter_loss: 0.17948490381240845
train_iter_loss: 0.3002832233905792
train_iter_loss: 0.07500795274972916
train_iter_loss: 0.2577398419380188
train_iter_loss: 0.11303583532571793
train_iter_loss: 0.128435418009758
train_iter_loss: 0.0749569684267044
train_iter_loss: 0.031857047230005264
train_iter_loss: 0.12463308125734329
train_iter_loss: 0.10207054018974304
train_iter_loss: 0.1501401960849762
train_iter_loss: 0.08356055617332458
train_iter_loss: 0.14056234061717987
train_iter_loss: 0.10459195077419281
train_iter_loss: 0.26844578981399536
train_iter_loss: 0.18786142766475677
train_iter_loss: 0.13198043406009674
train_iter_loss: 0.19888938963413239
train_iter_loss: 0.3028135299682617
train_iter_loss: 0.10948719084262848
train_iter_loss: 0.1689319908618927
train_iter_loss: 0.18662478029727936
train_iter_loss: 0.0834064856171608
train_iter_loss: 0.20606081187725067
train_iter_loss: 0.0840640738606453
train_iter_loss: 0.23664504289627075
train_iter_loss: 0.1320950984954834
train_iter_loss: 0.09662213176488876
train_iter_loss: 0.19555014371871948
train_iter_loss: 0.11151258647441864
train_iter_loss: 0.16859614849090576
train_iter_loss: 0.20043623447418213
train_iter_loss: 0.10827670246362686
train_iter_loss: 0.16603483259677887
train_iter_loss: 0.3744135797023773
train_iter_loss: 0.22902201116085052
train_iter_loss: 0.17092423141002655
train_iter_loss: 0.41639652848243713
train_iter_loss: 0.14946776628494263
train_iter_loss: 0.19417527318000793
train_iter_loss: 0.21183869242668152
train_iter_loss: 0.1578928828239441
train_iter_loss: 0.1057850793004036
train_iter_loss: 0.2633545398712158
train_iter_loss: 0.09079141169786453
train_iter_loss: 0.15871362388134003
train_iter_loss: 0.17863237857818604
train_iter_loss: 0.08671403676271439
train_iter_loss: 0.09656854718923569
train_iter_loss: 0.20108704268932343
train_iter_loss: 0.2903817594051361
train_iter_loss: 0.11221123486757278
train_iter_loss: 0.2850179076194763
train_iter_loss: 0.3189208507537842
train_iter_loss: 0.08318611979484558
train_iter_loss: 0.11961983889341354
train_iter_loss: 0.14736652374267578
train_iter_loss: 0.1974201798439026
train_iter_loss: 0.07331375777721405
train_iter_loss: 0.1259494125843048
train_iter_loss: 0.19764621555805206
train_iter_loss: 0.1078895851969719
train_iter_loss: 0.363383412361145
train_iter_loss: 0.15718390047550201
train_iter_loss: 0.12297777831554413
train_iter_loss: 0.09110322594642639
train_iter_loss: 0.14018991589546204
train_iter_loss: 0.1202852874994278
train_iter_loss: 0.18359889090061188
train_iter_loss: 0.1829969584941864
train loss :0.1662
---------------------
Validation seg loss: 0.2179269359868793 at epoch 489
epoch =    490/  1000, exp = train
train_iter_loss: 0.26390519738197327
train_iter_loss: 0.2874084413051605
train_iter_loss: 0.09066462516784668
train_iter_loss: 0.10572722554206848
train_iter_loss: 0.10169276595115662
train_iter_loss: 0.3118517994880676
train_iter_loss: 0.14563342928886414
train_iter_loss: 0.20413602888584137
train_iter_loss: 0.13623759150505066
train_iter_loss: 0.1643614023923874
train_iter_loss: 0.1901078075170517
train_iter_loss: 0.12056481093168259
train_iter_loss: 0.0842132568359375
train_iter_loss: 0.2591475248336792
train_iter_loss: 0.11340191960334778
train_iter_loss: 0.18709930777549744
train_iter_loss: 0.1790074110031128
train_iter_loss: 0.23092074692249298
train_iter_loss: 0.2695794701576233
train_iter_loss: 0.20681330561637878
train_iter_loss: 0.20131433010101318
train_iter_loss: 0.1990307718515396
train_iter_loss: 0.09641364961862564
train_iter_loss: 0.24451690912246704
train_iter_loss: 0.16483914852142334
train_iter_loss: 0.09864826500415802
train_iter_loss: 0.10732986778020859
train_iter_loss: 0.09224265813827515
train_iter_loss: 0.10900097340345383
train_iter_loss: 0.1812630444765091
train_iter_loss: 0.15435034036636353
train_iter_loss: 0.14739187061786652
train_iter_loss: 0.11507687717676163
train_iter_loss: 0.09904497116804123
train_iter_loss: 0.15270276367664337
train_iter_loss: 0.3112269639968872
train_iter_loss: 0.1488714963197708
train_iter_loss: 0.16974662244319916
train_iter_loss: 0.14976747334003448
train_iter_loss: 0.179269477725029
train_iter_loss: 0.08160172402858734
train_iter_loss: 0.16029092669487
train_iter_loss: 0.07111796736717224
train_iter_loss: 0.05680384114384651
train_iter_loss: 0.08094871789216995
train_iter_loss: 0.10795395821332932
train_iter_loss: 0.07455186545848846
train_iter_loss: 0.11249122768640518
train_iter_loss: 0.13841262459754944
train_iter_loss: 0.18889552354812622
train_iter_loss: 0.14833401143550873
train_iter_loss: 0.20022298395633698
train_iter_loss: 0.09881292283535004
train_iter_loss: 0.12245552241802216
train_iter_loss: 0.21562154591083527
train_iter_loss: 0.17386746406555176
train_iter_loss: 0.15269339084625244
train_iter_loss: 0.1972457766532898
train_iter_loss: 0.14540299773216248
train_iter_loss: 0.2583826780319214
train_iter_loss: 0.13433803617954254
train_iter_loss: 0.1541161835193634
train_iter_loss: 0.11594849079847336
train_iter_loss: 0.08130741119384766
train_iter_loss: 0.10060691833496094
train_iter_loss: 0.15297554433345795
train_iter_loss: 0.23338855803012848
train_iter_loss: 0.24389149248600006
train_iter_loss: 0.15737685561180115
train_iter_loss: 0.17415037751197815
train_iter_loss: 0.25327226519584656
train_iter_loss: 0.20551905035972595
train_iter_loss: 0.1648894101381302
train_iter_loss: 0.16031372547149658
train_iter_loss: 0.11174062639474869
train_iter_loss: 0.24050088226795197
train_iter_loss: 0.1476971060037613
train_iter_loss: 0.08768828213214874
train_iter_loss: 0.28503623604774475
train_iter_loss: 0.15651383996009827
train_iter_loss: 0.08004392683506012
train_iter_loss: 0.1433422565460205
train_iter_loss: 0.10080771893262863
train_iter_loss: 0.09374586492776871
train_iter_loss: 0.14588052034378052
train_iter_loss: 0.16889899969100952
train_iter_loss: 0.12732864916324615
train_iter_loss: 0.13018737733364105
train_iter_loss: 0.24763944745063782
train_iter_loss: 0.16060538589954376
train_iter_loss: 0.17362911999225616
train_iter_loss: 0.1139642521739006
train_iter_loss: 0.18816043436527252
train_iter_loss: 0.416983425617218
train_iter_loss: 0.22279489040374756
train_iter_loss: 0.11466933786869049
train_iter_loss: 0.24402554333209991
train_iter_loss: 0.12757056951522827
train_iter_loss: 0.09486503899097443
train_iter_loss: 0.1564478576183319
train loss :0.1634
---------------------
Validation seg loss: 0.22219325698701278 at epoch 490
epoch =    491/  1000, exp = train
train_iter_loss: 0.1320580244064331
train_iter_loss: 0.14773042500019073
train_iter_loss: 0.3468751907348633
train_iter_loss: 0.13862940669059753
train_iter_loss: 0.2055031955242157
train_iter_loss: 0.07343361526727676
train_iter_loss: 0.13731347024440765
train_iter_loss: 0.15692216157913208
train_iter_loss: 0.08785569667816162
train_iter_loss: 0.0746241956949234
train_iter_loss: 0.09366566687822342
train_iter_loss: 0.21365033090114594
train_iter_loss: 0.10865200310945511
train_iter_loss: 0.2682238817214966
train_iter_loss: 0.1863749474287033
train_iter_loss: 0.08419574797153473
train_iter_loss: 0.2284616380929947
train_iter_loss: 0.11691112816333771
train_iter_loss: 0.1438101828098297
train_iter_loss: 0.18367411196231842
train_iter_loss: 0.10555185377597809
train_iter_loss: 0.09088793396949768
train_iter_loss: 0.14103366434574127
train_iter_loss: 0.23430190980434418
train_iter_loss: 0.1114753857254982
train_iter_loss: 0.10400454699993134
train_iter_loss: 0.1875072568655014
train_iter_loss: 0.31442755460739136
train_iter_loss: 0.08366158604621887
train_iter_loss: 0.18983817100524902
train_iter_loss: 0.1707192361354828
train_iter_loss: 0.2760845124721527
train_iter_loss: 0.14338044822216034
train_iter_loss: 0.17391464114189148
train_iter_loss: 0.09408855438232422
train_iter_loss: 0.17490296065807343
train_iter_loss: 0.18953844904899597
train_iter_loss: 0.14651434123516083
train_iter_loss: 0.10396955162286758
train_iter_loss: 0.21300919353961945
train_iter_loss: 0.11525904387235641
train_iter_loss: 0.18448708951473236
train_iter_loss: 0.2993699610233307
train_iter_loss: 0.10584590584039688
train_iter_loss: 0.12048448622226715
train_iter_loss: 0.11811724305152893
train_iter_loss: 0.19321030378341675
train_iter_loss: 0.2162933200597763
train_iter_loss: 0.3940797746181488
train_iter_loss: 0.2453005313873291
train_iter_loss: 0.049660373479127884
train_iter_loss: 0.2128523588180542
train_iter_loss: 0.03264583274722099
train_iter_loss: 0.15082857012748718
train_iter_loss: 0.15301081538200378
train_iter_loss: 0.1677284687757492
train_iter_loss: 0.18686330318450928
train_iter_loss: 0.08831668645143509
train_iter_loss: 0.06761322915554047
train_iter_loss: 0.21837110817432404
train_iter_loss: 0.2799387276172638
train_iter_loss: 0.084124356508255
train_iter_loss: 0.30150991678237915
train_iter_loss: 0.12049420177936554
train_iter_loss: 0.10596507042646408
train_iter_loss: 0.12666048109531403
train_iter_loss: 0.11372100561857224
train_iter_loss: 0.13778714835643768
train_iter_loss: 0.13578182458877563
train_iter_loss: 0.23435896635055542
train_iter_loss: 0.1369338035583496
train_iter_loss: 0.22074873745441437
train_iter_loss: 0.14189383387565613
train_iter_loss: 0.10065542906522751
train_iter_loss: 0.1893525868654251
train_iter_loss: 0.413106769323349
train_iter_loss: 0.16185727715492249
train_iter_loss: 0.15203115344047546
train_iter_loss: 0.22755827009677887
train_iter_loss: 0.11610566824674606
train_iter_loss: 0.40059947967529297
train_iter_loss: 0.15401582419872284
train_iter_loss: 0.05497913062572479
train_iter_loss: 0.1891127973794937
train_iter_loss: 0.043218739330768585
train_iter_loss: 0.1180143803358078
train_iter_loss: 0.16911816596984863
train_iter_loss: 0.1351633220911026
train_iter_loss: 0.11783048510551453
train_iter_loss: 0.10399365425109863
train_iter_loss: 0.18496902287006378
train_iter_loss: 0.23663993179798126
train_iter_loss: 0.13385970890522003
train_iter_loss: 0.07853137701749802
train_iter_loss: 0.17984794080257416
train_iter_loss: 0.12228070944547653
train_iter_loss: 0.14684925973415375
train_iter_loss: 0.062244996428489685
train_iter_loss: 0.07860983163118362
train_iter_loss: 0.20964133739471436
train loss :0.1628
---------------------
Validation seg loss: 0.22454473054704238 at epoch 491
epoch =    492/  1000, exp = train
train_iter_loss: 0.1283387541770935
train_iter_loss: 0.27658405900001526
train_iter_loss: 0.164559006690979
train_iter_loss: 0.06300468742847443
train_iter_loss: 0.0815938264131546
train_iter_loss: 0.24713213741779327
train_iter_loss: 0.10145670920610428
train_iter_loss: 0.11874878406524658
train_iter_loss: 0.04765787720680237
train_iter_loss: 0.18482659757137299
train_iter_loss: 0.07598097622394562
train_iter_loss: 0.0682639554142952
train_iter_loss: 0.14875644445419312
train_iter_loss: 0.08902408182621002
train_iter_loss: 0.16694597899913788
train_iter_loss: 0.07952876389026642
train_iter_loss: 0.1646837592124939
train_iter_loss: 0.19134895503520966
train_iter_loss: 0.1942582130432129
train_iter_loss: 0.1317017525434494
train_iter_loss: 0.13621725142002106
train_iter_loss: 0.22580307722091675
train_iter_loss: 0.2791035771369934
train_iter_loss: 0.10928314179182053
train_iter_loss: 0.1403948962688446
train_iter_loss: 0.059609007090330124
train_iter_loss: 0.12600301206111908
train_iter_loss: 0.21955445408821106
train_iter_loss: 0.19619648158550262
train_iter_loss: 0.16609875857830048
train_iter_loss: 0.12286358326673508
train_iter_loss: 0.17829248309135437
train_iter_loss: 0.22581757605075836
train_iter_loss: 0.16448244452476501
train_iter_loss: 0.1648896336555481
train_iter_loss: 0.25310876965522766
train_iter_loss: 0.15020841360092163
train_iter_loss: 0.12613525986671448
train_iter_loss: 0.2179846465587616
train_iter_loss: 0.12310343980789185
train_iter_loss: 0.15594415366649628
train_iter_loss: 0.15419672429561615
train_iter_loss: 0.1685059666633606
train_iter_loss: 0.33400458097457886
train_iter_loss: 0.1577044129371643
train_iter_loss: 0.18844589591026306
train_iter_loss: 0.2236892729997635
train_iter_loss: 0.2171313613653183
train_iter_loss: 0.09591493755578995
train_iter_loss: 0.1197342500090599
train_iter_loss: 0.06680748611688614
train_iter_loss: 0.1823243647813797
train_iter_loss: 0.24781954288482666
train_iter_loss: 0.18554513156414032
train_iter_loss: 0.1353352963924408
train_iter_loss: 0.13628247380256653
train_iter_loss: 0.17130756378173828
train_iter_loss: 0.10109476000070572
train_iter_loss: 0.21370980143547058
train_iter_loss: 0.16003267467021942
train_iter_loss: 0.16228584945201874
train_iter_loss: 0.08377810567617416
train_iter_loss: 0.07233919948339462
train_iter_loss: 0.13343650102615356
train_iter_loss: 0.17668697237968445
train_iter_loss: 0.17180347442626953
train_iter_loss: 0.2266433835029602
train_iter_loss: 0.3015093505382538
train_iter_loss: 0.31736165285110474
train_iter_loss: 0.11418783664703369
train_iter_loss: 0.19934581220149994
train_iter_loss: 0.054409321397542953
train_iter_loss: 0.20590580999851227
train_iter_loss: 0.11698368191719055
train_iter_loss: 0.18986913561820984
train_iter_loss: 0.14844243228435516
train_iter_loss: 0.18350756168365479
train_iter_loss: 0.23720212280750275
train_iter_loss: 0.1984790563583374
train_iter_loss: 0.1352403312921524
train_iter_loss: 0.09700324386358261
train_iter_loss: 0.12815941870212555
train_iter_loss: 0.1620527058839798
train_iter_loss: 0.1691509187221527
train_iter_loss: 0.10281842201948166
train_iter_loss: 0.13786810636520386
train_iter_loss: 0.053082242608070374
train_iter_loss: 0.13919800519943237
train_iter_loss: 0.16371354460716248
train_iter_loss: 0.04523003101348877
train_iter_loss: 0.14585860073566437
train_iter_loss: 0.1691950410604477
train_iter_loss: 0.14485414326190948
train_iter_loss: 0.23156413435935974
train_iter_loss: 0.24967513978481293
train_iter_loss: 0.109397292137146
train_iter_loss: 0.1557914912700653
train_iter_loss: 0.2704140543937683
train_iter_loss: 0.15182797610759735
train_iter_loss: 0.0892123207449913
train loss :0.1603
---------------------
Validation seg loss: 0.22022995198110365 at epoch 492
epoch =    493/  1000, exp = train
train_iter_loss: 0.19418784976005554
train_iter_loss: 0.19658133387565613
train_iter_loss: 0.1020355224609375
train_iter_loss: 0.19070562720298767
train_iter_loss: 0.1381399929523468
train_iter_loss: 0.2586992681026459
train_iter_loss: 0.10203412920236588
train_iter_loss: 0.20811739563941956
train_iter_loss: 0.1274665892124176
train_iter_loss: 0.2043892741203308
train_iter_loss: 0.12372814863920212
train_iter_loss: 0.24688629806041718
train_iter_loss: 0.20065128803253174
train_iter_loss: 0.08217713981866837
train_iter_loss: 0.3032766878604889
train_iter_loss: 0.14479927718639374
train_iter_loss: 0.14457841217517853
train_iter_loss: 0.15216004848480225
train_iter_loss: 0.10228385776281357
train_iter_loss: 0.3063983619213104
train_iter_loss: 0.1183338314294815
train_iter_loss: 0.23742739856243134
train_iter_loss: 0.1929667443037033
train_iter_loss: 0.13415896892547607
train_iter_loss: 0.15102997422218323
train_iter_loss: 0.15075112879276276
train_iter_loss: 0.16893061995506287
train_iter_loss: 0.19160062074661255
train_iter_loss: 0.22783343493938446
train_iter_loss: 0.12383817881345749
train_iter_loss: 0.12332860380411148
train_iter_loss: 0.13633322715759277
train_iter_loss: 0.12734444439411163
train_iter_loss: 0.08039015531539917
train_iter_loss: 0.30659589171409607
train_iter_loss: 0.15765352547168732
train_iter_loss: 0.09711847454309464
train_iter_loss: 0.12895764410495758
train_iter_loss: 0.1481117159128189
train_iter_loss: 0.10001053661108017
train_iter_loss: 0.13263490796089172
train_iter_loss: 0.18193994462490082
train_iter_loss: 0.14977173507213593
train_iter_loss: 0.1402856558561325
train_iter_loss: 0.12303490191698074
train_iter_loss: 0.08152267336845398
train_iter_loss: 0.3305900990962982
train_iter_loss: 0.18032869696617126
train_iter_loss: 0.19856110215187073
train_iter_loss: 0.09500524401664734
train_iter_loss: 0.0519992969930172
train_iter_loss: 0.11852852255105972
train_iter_loss: 0.19936886429786682
train_iter_loss: 0.14616207778453827
train_iter_loss: 0.13375194370746613
train_iter_loss: 0.08055397123098373
train_iter_loss: 0.20135188102722168
train_iter_loss: 0.10132496058940887
train_iter_loss: 0.1690904051065445
train_iter_loss: 0.11925899982452393
train_iter_loss: 0.09959329664707184
train_iter_loss: 0.12564422190189362
train_iter_loss: 0.08778581023216248
train_iter_loss: 0.12854279577732086
train_iter_loss: 0.16147032380104065
train_iter_loss: 0.11380176246166229
train_iter_loss: 0.14356745779514313
train_iter_loss: 0.24845099449157715
train_iter_loss: 0.16915325820446014
train_iter_loss: 0.11493001133203506
train_iter_loss: 0.14808568358421326
train_iter_loss: 0.2955611050128937
train_iter_loss: 0.1529296487569809
train_iter_loss: 0.21945780515670776
train_iter_loss: 0.22046178579330444
train_iter_loss: 0.13461385667324066
train_iter_loss: 0.1554778516292572
train_iter_loss: 0.10396949201822281
train_iter_loss: 0.2419643998146057
train_iter_loss: 0.2003220021724701
train_iter_loss: 0.05925367400050163
train_iter_loss: 0.16709522902965546
train_iter_loss: 0.18547697365283966
train_iter_loss: 0.1553822010755539
train_iter_loss: 0.20719555020332336
train_iter_loss: 0.2622222602367401
train_iter_loss: 0.15854956209659576
train_iter_loss: 0.07307872176170349
train_iter_loss: 0.11830908060073853
train_iter_loss: 0.19103510677814484
train_iter_loss: 0.24315020442008972
train_iter_loss: 0.13511741161346436
train_iter_loss: 0.2593936622142792
train_iter_loss: 0.1312987506389618
train_iter_loss: 0.08921267837285995
train_iter_loss: 0.17245829105377197
train_iter_loss: 0.061994925141334534
train_iter_loss: 0.20068500936031342
train_iter_loss: 0.1306433379650116
train_iter_loss: 0.09813933074474335
train loss :0.1609
---------------------
Validation seg loss: 0.21731696851586677 at epoch 493
epoch =    494/  1000, exp = train
train_iter_loss: 0.1733284443616867
train_iter_loss: 0.10862663388252258
train_iter_loss: 0.23326894640922546
train_iter_loss: 0.23981918394565582
train_iter_loss: 0.12908203899860382
train_iter_loss: 0.05879436805844307
train_iter_loss: 0.06141779199242592
train_iter_loss: 0.09053845703601837
train_iter_loss: 0.14515337347984314
train_iter_loss: 0.27318984270095825
train_iter_loss: 0.17770636081695557
train_iter_loss: 0.21700316667556763
train_iter_loss: 0.11724819242954254
train_iter_loss: 0.13675862550735474
train_iter_loss: 0.14946521818637848
train_iter_loss: 0.14338044822216034
train_iter_loss: 0.19147254526615143
train_iter_loss: 0.12073744088411331
train_iter_loss: 0.3977358937263489
train_iter_loss: 0.14989137649536133
train_iter_loss: 0.12192535400390625
train_iter_loss: 0.16764222085475922
train_iter_loss: 0.12379558384418488
train_iter_loss: 0.2632843255996704
train_iter_loss: 0.18078528344631195
train_iter_loss: 0.08924506604671478
train_iter_loss: 0.056735552847385406
train_iter_loss: 0.05879541486501694
train_iter_loss: 0.12277549505233765
train_iter_loss: 0.08498626202344894
train_iter_loss: 0.19256193935871124
train_iter_loss: 0.11610572040081024
train_iter_loss: 0.3480891287326813
train_iter_loss: 0.12435346096754074
train_iter_loss: 0.17876993119716644
train_iter_loss: 0.06321430206298828
train_iter_loss: 0.14364935457706451
train_iter_loss: 0.16990981996059418
train_iter_loss: 0.16831190884113312
train_iter_loss: 0.12936344742774963
train_iter_loss: 0.18870306015014648
train_iter_loss: 0.12184812128543854
train_iter_loss: 0.12172963470220566
train_iter_loss: 0.07515627890825272
train_iter_loss: 0.21880361437797546
train_iter_loss: 0.1036173403263092
train_iter_loss: 0.11078577488660812
train_iter_loss: 0.138839453458786
train_iter_loss: 0.23159876465797424
train_iter_loss: 0.08062516152858734
train_iter_loss: 0.18409477174282074
train_iter_loss: 0.16049964725971222
train_iter_loss: 0.18817462027072906
train_iter_loss: 0.20676285028457642
train_iter_loss: 0.12695413827896118
train_iter_loss: 0.1516319066286087
train_iter_loss: 0.1266290694475174
train_iter_loss: 0.2504183053970337
train_iter_loss: 0.1960725337266922
train_iter_loss: 0.07699748128652573
train_iter_loss: 0.09632149338722229
train_iter_loss: 0.13923758268356323
train_iter_loss: 0.18859419226646423
train_iter_loss: 0.16433341801166534
train_iter_loss: 0.42534512281417847
train_iter_loss: 0.16005229949951172
train_iter_loss: 0.28280752897262573
train_iter_loss: 0.10029445588588715
train_iter_loss: 0.1579069346189499
train_iter_loss: 0.2196333408355713
train_iter_loss: 0.22885100543498993
train_iter_loss: 0.18730895221233368
train_iter_loss: 0.10798779875040054
train_iter_loss: 0.12342777848243713
train_iter_loss: 0.2380213439464569
train_iter_loss: 0.2693222165107727
train_iter_loss: 0.17515768110752106
train_iter_loss: 0.2490035444498062
train_iter_loss: 0.10236958414316177
train_iter_loss: 0.15504013001918793
train_iter_loss: 0.24589499831199646
train_iter_loss: 0.12973280251026154
train_iter_loss: 0.24150308966636658
train_iter_loss: 0.11818571388721466
train_iter_loss: 0.1588490605354309
train_iter_loss: 0.30277207493782043
train_iter_loss: 0.1321086287498474
train_iter_loss: 0.2534927427768707
train_iter_loss: 0.17626146972179413
train_iter_loss: 0.20850157737731934
train_iter_loss: 0.0967053547501564
train_iter_loss: 0.10617335140705109
train_iter_loss: 0.3106611669063568
train_iter_loss: 0.13558882474899292
train_iter_loss: 0.04331353306770325
train_iter_loss: 0.05293646082282066
train_iter_loss: 0.1096099391579628
train_iter_loss: 0.09575307369232178
train_iter_loss: 0.18270361423492432
train_iter_loss: 0.07403894513845444
train loss :0.1639
---------------------
Validation seg loss: 0.21532369637861848 at epoch 494
epoch =    495/  1000, exp = train
train_iter_loss: 0.11825821548700333
train_iter_loss: 0.07753045111894608
train_iter_loss: 0.2095925658941269
train_iter_loss: 0.15850064158439636
train_iter_loss: 0.20430034399032593
train_iter_loss: 0.26909980177879333
train_iter_loss: 0.21967700123786926
train_iter_loss: 0.08368059247732162
train_iter_loss: 0.12051597237586975
train_iter_loss: 0.13461683690547943
train_iter_loss: 0.10161419957876205
train_iter_loss: 0.19013798236846924
train_iter_loss: 0.1925763487815857
train_iter_loss: 0.1275826245546341
train_iter_loss: 0.15725266933441162
train_iter_loss: 0.13225680589675903
train_iter_loss: 0.17986977100372314
train_iter_loss: 0.1492607444524765
train_iter_loss: 0.07519376277923584
train_iter_loss: 0.28697794675827026
train_iter_loss: 0.06898405402898788
train_iter_loss: 0.2149706333875656
train_iter_loss: 0.10550687462091446
train_iter_loss: 0.16840478777885437
train_iter_loss: 0.11743337661027908
train_iter_loss: 0.21697098016738892
train_iter_loss: 0.18505896627902985
train_iter_loss: 0.1723005175590515
train_iter_loss: 0.1618783175945282
train_iter_loss: 0.12428584694862366
train_iter_loss: 0.07339928299188614
train_iter_loss: 0.2384486347436905
train_iter_loss: 0.12529174983501434
train_iter_loss: 0.23295338451862335
train_iter_loss: 0.17341920733451843
train_iter_loss: 0.20843973755836487
train_iter_loss: 0.36185184121131897
train_iter_loss: 0.13781854510307312
train_iter_loss: 0.16296112537384033
train_iter_loss: 0.1488141119480133
train_iter_loss: 0.1597626954317093
train_iter_loss: 0.25506311655044556
train_iter_loss: 0.1645849496126175
train_iter_loss: 0.09677714109420776
train_iter_loss: 0.18522262573242188
train_iter_loss: 0.2069416642189026
train_iter_loss: 0.14335206151008606
train_iter_loss: 0.13941602408885956
train_iter_loss: 0.13553853332996368
train_iter_loss: 0.22520887851715088
train_iter_loss: 0.09085032343864441
train_iter_loss: 0.15089279413223267
train_iter_loss: 0.26064443588256836
train_iter_loss: 0.04574747756123543
train_iter_loss: 0.3057010769844055
train_iter_loss: 0.18216180801391602
train_iter_loss: 0.23629876971244812
train_iter_loss: 0.10581912100315094
train_iter_loss: 0.18773740530014038
train_iter_loss: 0.1645556390285492
train_iter_loss: 0.18289776146411896
train_iter_loss: 0.11921647936105728
train_iter_loss: 0.1583951711654663
train_iter_loss: 0.09432818740606308
train_iter_loss: 0.0883791595697403
train_iter_loss: 0.16674624383449554
train_iter_loss: 0.10862582176923752
train_iter_loss: 0.19695580005645752
train_iter_loss: 0.17490501701831818
train_iter_loss: 0.13097265362739563
train_iter_loss: 0.12156755477190018
train_iter_loss: 0.18690142035484314
train_iter_loss: 0.12846285104751587
train_iter_loss: 0.19863256812095642
train_iter_loss: 0.33492445945739746
train_iter_loss: 0.09700064361095428
train_iter_loss: 0.0749647468328476
train_iter_loss: 0.13665775954723358
train_iter_loss: 0.25826841592788696
train_iter_loss: 0.2486228495836258
train_iter_loss: 0.20938070118427277
train_iter_loss: 0.064612977206707
train_iter_loss: 0.09624076634645462
train_iter_loss: 0.22006770968437195
train_iter_loss: 0.2294716238975525
train_iter_loss: 0.20503441989421844
train_iter_loss: 0.15423430502414703
train_iter_loss: 0.0819002240896225
train_iter_loss: 0.20852553844451904
train_iter_loss: 0.13990050554275513
train_iter_loss: 0.1312894970178604
train_iter_loss: 0.1461445391178131
train_iter_loss: 0.0798017680644989
train_iter_loss: 0.09487328678369522
train_iter_loss: 0.14745555818080902
train_iter_loss: 0.1595192849636078
train_iter_loss: 0.2293858528137207
train_iter_loss: 0.1639154553413391
train_iter_loss: 0.12184100598096848
train_iter_loss: 0.09447567164897919
train loss :0.1638
---------------------
Validation seg loss: 0.21668871931450548 at epoch 495
epoch =    496/  1000, exp = train
train_iter_loss: 0.09067824482917786
train_iter_loss: 0.1408875584602356
train_iter_loss: 0.15902702510356903
train_iter_loss: 0.080651193857193
train_iter_loss: 0.13296407461166382
train_iter_loss: 0.13890543580055237
train_iter_loss: 0.10832253843545914
train_iter_loss: 0.1300991028547287
train_iter_loss: 0.24819491803646088
train_iter_loss: 0.2091522067785263
train_iter_loss: 0.20444157719612122
train_iter_loss: 0.23301513493061066
train_iter_loss: 0.1418389230966568
train_iter_loss: 0.23511113226413727
train_iter_loss: 0.2420121133327484
train_iter_loss: 0.14208513498306274
train_iter_loss: 0.14351804554462433
train_iter_loss: 0.05339016020298004
train_iter_loss: 0.15274836122989655
train_iter_loss: 0.15764203667640686
train_iter_loss: 0.2857332229614258
train_iter_loss: 0.2908380329608917
train_iter_loss: 0.4140739142894745
train_iter_loss: 0.2198392152786255
train_iter_loss: 0.20170728862285614
train_iter_loss: 0.23277148604393005
train_iter_loss: 0.16278688609600067
train_iter_loss: 0.19983941316604614
train_iter_loss: 0.09125453978776932
train_iter_loss: 0.16676364839076996
train_iter_loss: 0.09081695228815079
train_iter_loss: 0.18648763000965118
train_iter_loss: 0.08267693221569061
train_iter_loss: 0.10332383960485458
train_iter_loss: 0.13451418280601501
train_iter_loss: 0.18806926906108856
train_iter_loss: 0.07322407513856888
train_iter_loss: 0.10635185986757278
train_iter_loss: 0.1874723881483078
train_iter_loss: 0.1906048208475113
train_iter_loss: 0.09471791237592697
train_iter_loss: 0.12087196856737137
train_iter_loss: 0.22840599715709686
train_iter_loss: 0.07688548415899277
train_iter_loss: 0.2168392390012741
train_iter_loss: 0.07596880197525024
train_iter_loss: 0.15691153705120087
train_iter_loss: 0.20295371115207672
train_iter_loss: 0.1678328663110733
train_iter_loss: 0.11455675214529037
train_iter_loss: 0.12046470493078232
train_iter_loss: 0.17399939894676208
train_iter_loss: 0.3427850902080536
train_iter_loss: 0.10082150250673294
train_iter_loss: 0.10824659466743469
train_iter_loss: 0.10671655088663101
train_iter_loss: 0.12273158133029938
train_iter_loss: 0.032484471797943115
train_iter_loss: 0.23306450247764587
train_iter_loss: 0.16473938524723053
train_iter_loss: 0.31806764006614685
train_iter_loss: 0.31390419602394104
train_iter_loss: 0.15765859186649323
train_iter_loss: 0.14444252848625183
train_iter_loss: 0.25517037510871887
train_iter_loss: 0.21200944483280182
train_iter_loss: 0.1658979207277298
train_iter_loss: 0.17799754440784454
train_iter_loss: 0.08931273221969604
train_iter_loss: 0.07777132838964462
train_iter_loss: 0.12886278331279755
train_iter_loss: 0.13395805656909943
train_iter_loss: 0.098933644592762
train_iter_loss: 0.22794008255004883
train_iter_loss: 0.1534670740365982
train_iter_loss: 0.22210752964019775
train_iter_loss: 0.12925991415977478
train_iter_loss: 0.10460863262414932
train_iter_loss: 0.15550440549850464
train_iter_loss: 0.2598097622394562
train_iter_loss: 0.17380894720554352
train_iter_loss: 0.1426113247871399
train_iter_loss: 0.11786394566297531
train_iter_loss: 0.13567981123924255
train_iter_loss: 0.15930546820163727
train_iter_loss: 0.12219240516424179
train_iter_loss: 0.12730489671230316
train_iter_loss: 0.11044131964445114
train_iter_loss: 0.21422354876995087
train_iter_loss: 0.1514524519443512
train_iter_loss: 0.13785208761692047
train_iter_loss: 0.2753826379776001
train_iter_loss: 0.23097404837608337
train_iter_loss: 0.12386481463909149
train_iter_loss: 0.3352236747741699
train_iter_loss: 0.20652364194393158
train_iter_loss: 0.07719246298074722
train_iter_loss: 0.05360313877463341
train_iter_loss: 0.10471023619174957
train_iter_loss: 0.12509973347187042
train loss :0.1653
---------------------
Validation seg loss: 0.21658307849868852 at epoch 496
epoch =    497/  1000, exp = train
train_iter_loss: 0.12907950580120087
train_iter_loss: 0.10341765731573105
train_iter_loss: 0.1655300408601761
train_iter_loss: 0.1516776978969574
train_iter_loss: 0.16208994388580322
train_iter_loss: 0.09241639822721481
train_iter_loss: 0.12725408375263214
train_iter_loss: 0.14391092956066132
train_iter_loss: 0.19799663126468658
train_iter_loss: 0.21297569572925568
train_iter_loss: 0.15680402517318726
train_iter_loss: 0.13024015724658966
train_iter_loss: 0.15232789516448975
train_iter_loss: 0.23028437793254852
train_iter_loss: 0.0885351225733757
train_iter_loss: 0.07930044829845428
train_iter_loss: 0.135661318898201
train_iter_loss: 0.08320830017328262
train_iter_loss: 0.09825681895017624
train_iter_loss: 0.10760235786437988
train_iter_loss: 0.10959264636039734
train_iter_loss: 0.22061367332935333
train_iter_loss: 0.18733982741832733
train_iter_loss: 0.10062820464372635
train_iter_loss: 0.19410225749015808
train_iter_loss: 0.19338679313659668
train_iter_loss: 0.23083126544952393
train_iter_loss: 0.15612459182739258
train_iter_loss: 0.14489488303661346
train_iter_loss: 0.15077680349349976
train_iter_loss: 0.08688577264547348
train_iter_loss: 0.11346124857664108
train_iter_loss: 0.1953171342611313
train_iter_loss: 0.18275943398475647
train_iter_loss: 0.11098530143499374
train_iter_loss: 0.14581984281539917
train_iter_loss: 0.22768041491508484
train_iter_loss: 0.11558214575052261
train_iter_loss: 0.07480605691671371
train_iter_loss: 0.08725154399871826
train_iter_loss: 0.1053999662399292
train_iter_loss: 0.20057892799377441
train_iter_loss: 0.16070111095905304
train_iter_loss: 0.08902796357870102
train_iter_loss: 0.19929607212543488
train_iter_loss: 0.0604701042175293
train_iter_loss: 0.19651290774345398
train_iter_loss: 0.21793751418590546
train_iter_loss: 0.07457771897315979
train_iter_loss: 0.22781579196453094
train_iter_loss: 0.16371268033981323
train_iter_loss: 0.1701776385307312
train_iter_loss: 0.1606670320034027
train_iter_loss: 0.24769285321235657
train_iter_loss: 0.07390890270471573
train_iter_loss: 0.21435315907001495
train_iter_loss: 0.22287100553512573
train_iter_loss: 0.0591663159430027
train_iter_loss: 0.1199110895395279
train_iter_loss: 0.189097598195076
train_iter_loss: 0.08118941634893417
train_iter_loss: 0.15797990560531616
train_iter_loss: 0.15059290826320648
train_iter_loss: 0.1784549504518509
train_iter_loss: 0.1841510385274887
train_iter_loss: 0.06894774734973907
train_iter_loss: 0.2337670922279358
train_iter_loss: 0.16401590406894684
train_iter_loss: 0.1071280762553215
train_iter_loss: 0.15473411977291107
train_iter_loss: 0.18842196464538574
train_iter_loss: 0.0738612562417984
train_iter_loss: 0.12268874049186707
train_iter_loss: 0.21927596628665924
train_iter_loss: 0.12678350508213043
train_iter_loss: 0.2318762093782425
train_iter_loss: 0.19229243695735931
train_iter_loss: 0.185346782207489
train_iter_loss: 0.11942225694656372
train_iter_loss: 0.2243403047323227
train_iter_loss: 0.26374906301498413
train_iter_loss: 0.16483955085277557
train_iter_loss: 0.0923537090420723
train_iter_loss: 0.10380464047193527
train_iter_loss: 0.05244506150484085
train_iter_loss: 0.19434309005737305
train_iter_loss: 0.12318393588066101
train_iter_loss: 0.19374224543571472
train_iter_loss: 0.22161300480365753
train_iter_loss: 0.2590908110141754
train_iter_loss: 0.1727369874715805
train_iter_loss: 0.11308867484331131
train_iter_loss: 0.13283541798591614
train_iter_loss: 0.3533567488193512
train_iter_loss: 0.35915330052375793
train_iter_loss: 0.15999263525009155
train_iter_loss: 0.1690763682126999
train_iter_loss: 0.08871223777532578
train_iter_loss: 0.15153777599334717
train_iter_loss: 0.25511202216148376
train loss :0.1589
---------------------
Validation seg loss: 0.21985471071148255 at epoch 497
epoch =    498/  1000, exp = train
train_iter_loss: 0.12001951038837433
train_iter_loss: 0.05968311429023743
train_iter_loss: 0.18974725902080536
train_iter_loss: 0.21768784523010254
train_iter_loss: 0.24773269891738892
train_iter_loss: 0.1666530966758728
train_iter_loss: 0.27003002166748047
train_iter_loss: 0.3410121500492096
train_iter_loss: 0.15245600044727325
train_iter_loss: 0.1684340238571167
train_iter_loss: 0.14111779630184174
train_iter_loss: 0.1288381963968277
train_iter_loss: 0.228470116853714
train_iter_loss: 0.1532113254070282
train_iter_loss: 0.22225244343280792
train_iter_loss: 0.17784523963928223
train_iter_loss: 0.21616660058498383
train_iter_loss: 0.12536713480949402
train_iter_loss: 0.18590249121189117
train_iter_loss: 0.19405028223991394
train_iter_loss: 0.11760935187339783
train_iter_loss: 0.09526267647743225
train_iter_loss: 0.10319069772958755
train_iter_loss: 0.21630065143108368
train_iter_loss: 0.2003171145915985
train_iter_loss: 0.17365656793117523
train_iter_loss: 0.11397887766361237
train_iter_loss: 0.1681666374206543
train_iter_loss: 0.2562263309955597
train_iter_loss: 0.0875532478094101
train_iter_loss: 0.0954928770661354
train_iter_loss: 0.12746872007846832
train_iter_loss: 0.15960614383220673
train_iter_loss: 0.12493499368429184
train_iter_loss: 0.07968264818191528
train_iter_loss: 0.16004213690757751
train_iter_loss: 0.2220267802476883
train_iter_loss: 0.19593366980552673
train_iter_loss: 0.16548778116703033
train_iter_loss: 0.13995873928070068
train_iter_loss: 0.060063689947128296
train_iter_loss: 0.08484939485788345
train_iter_loss: 0.05719189718365669
train_iter_loss: 0.12300731241703033
train_iter_loss: 0.2153569906949997
train_iter_loss: 0.06603500992059708
train_iter_loss: 0.18473735451698303
train_iter_loss: 0.11430036276578903
train_iter_loss: 0.08024931699037552
train_iter_loss: 0.10242048650979996
train_iter_loss: 0.15998831391334534
train_iter_loss: 0.18429715931415558
train_iter_loss: 0.08921227604150772
train_iter_loss: 0.054426297545433044
train_iter_loss: 0.10794380307197571
train_iter_loss: 0.191205695271492
train_iter_loss: 0.18551361560821533
train_iter_loss: 0.1535329967737198
train_iter_loss: 0.10860063880681992
train_iter_loss: 0.08548317849636078
train_iter_loss: 0.14604659378528595
train_iter_loss: 0.10483904182910919
train_iter_loss: 0.20059412717819214
train_iter_loss: 0.16087739169597626
train_iter_loss: 0.14273791015148163
train_iter_loss: 0.23124827444553375
train_iter_loss: 0.11154714971780777
train_iter_loss: 0.3131392300128937
train_iter_loss: 0.14946432411670685
train_iter_loss: 0.27074313163757324
train_iter_loss: 0.2065879851579666
train_iter_loss: 0.0824640691280365
train_iter_loss: 0.16329120099544525
train_iter_loss: 0.19377827644348145
train_iter_loss: 0.16839514672756195
train_iter_loss: 0.14624932408332825
train_iter_loss: 0.11108845472335815
train_iter_loss: 0.16043703258037567
train_iter_loss: 0.14555451273918152
train_iter_loss: 0.20499657094478607
train_iter_loss: 0.12286187708377838
train_iter_loss: 0.22002866864204407
train_iter_loss: 0.18325375020503998
train_iter_loss: 0.22558419406414032
train_iter_loss: 0.10762540996074677
train_iter_loss: 0.1446722447872162
train_iter_loss: 0.2525627911090851
train_iter_loss: 0.2032446265220642
train_iter_loss: 0.14331990480422974
train_iter_loss: 0.21840789914131165
train_iter_loss: 0.15013529360294342
train_iter_loss: 0.11121229082345963
train_iter_loss: 0.3224777579307556
train_iter_loss: 0.1352253407239914
train_iter_loss: 0.15207666158676147
train_iter_loss: 0.2212461680173874
train_iter_loss: 0.15937021374702454
train_iter_loss: 0.05615060403943062
train_iter_loss: 0.1633683443069458
train_iter_loss: 0.25680646300315857
train loss :0.1624
---------------------
Validation seg loss: 0.21823353609421625 at epoch 498
epoch =    499/  1000, exp = train
train_iter_loss: 0.10035979002714157
train_iter_loss: 0.14987589418888092
train_iter_loss: 0.17064732313156128
train_iter_loss: 0.3020067512989044
train_iter_loss: 0.2587806284427643
train_iter_loss: 0.08822192251682281
train_iter_loss: 0.1981976479291916
train_iter_loss: 0.2059117704629898
train_iter_loss: 0.09084124863147736
train_iter_loss: 0.22871509194374084
train_iter_loss: 0.10241322964429855
train_iter_loss: 0.1054820716381073
train_iter_loss: 0.1714870184659958
train_iter_loss: 0.19300901889801025
train_iter_loss: 0.14538879692554474
train_iter_loss: 0.20574326813220978
train_iter_loss: 0.2332056164741516
train_iter_loss: 0.31532374024391174
train_iter_loss: 0.17789225280284882
train_iter_loss: 0.12725989520549774
train_iter_loss: 0.11651003360748291
train_iter_loss: 0.17346136271953583
train_iter_loss: 0.1591748148202896
train_iter_loss: 0.18308667838573456
train_iter_loss: 0.1475493609905243
train_iter_loss: 0.0767734944820404
train_iter_loss: 0.1260681003332138
train_iter_loss: 0.13608375191688538
train_iter_loss: 0.16084697842597961
train_iter_loss: 0.14673033356666565
train_iter_loss: 0.24368546903133392
train_iter_loss: 0.08456766605377197
train_iter_loss: 0.11162002384662628
train_iter_loss: 0.14927490055561066
train_iter_loss: 0.16889406740665436
train_iter_loss: 0.20242151618003845
train_iter_loss: 0.1998511254787445
train_iter_loss: 0.1720077395439148
train_iter_loss: 0.09814993292093277
train_iter_loss: 0.10116888582706451
train_iter_loss: 0.09604289382696152
train_iter_loss: 0.18231670558452606
train_iter_loss: 0.0896780714392662
train_iter_loss: 0.07639990001916885
train_iter_loss: 0.11901819705963135
train_iter_loss: 0.1705377995967865
train_iter_loss: 0.18746881186962128
train_iter_loss: 0.12287663668394089
train_iter_loss: 0.25582683086395264
train_iter_loss: 0.11995203793048859
train_iter_loss: 0.1155722513794899
train_iter_loss: 0.11051633208990097
train_iter_loss: 0.04894905164837837
train_iter_loss: 0.14958585798740387
train_iter_loss: 0.16964660584926605
train_iter_loss: 0.14688615500926971
train_iter_loss: 0.23946218192577362
train_iter_loss: 0.14697320759296417
train_iter_loss: 0.22783295810222626
train_iter_loss: 0.1992485225200653
train_iter_loss: 0.22230935096740723
train_iter_loss: 0.09593848139047623
train_iter_loss: 0.21814203262329102
train_iter_loss: 0.16212812066078186
train_iter_loss: 0.2547949552536011
train_iter_loss: 0.11239323765039444
train_iter_loss: 0.3453580141067505
train_iter_loss: 0.3547340929508209
train_iter_loss: 0.1301587074995041
train_iter_loss: 0.17674295604228973
train_iter_loss: 0.08386609703302383
train_iter_loss: 0.13255029916763306
train_iter_loss: 0.18550512194633484
train_iter_loss: 0.30718928575515747
train_iter_loss: 0.14210721850395203
train_iter_loss: 0.13458064198493958
train_iter_loss: 0.18812020123004913
train_iter_loss: 0.10560525953769684
train_iter_loss: 0.12198007851839066
train_iter_loss: 0.2183394581079483
train_iter_loss: 0.2270994335412979
train_iter_loss: 0.2104102373123169
train_iter_loss: 0.16764655709266663
train_iter_loss: 0.1148458868265152
train_iter_loss: 0.06610867381095886
train_iter_loss: 0.23763321340084076
train_iter_loss: 0.30417025089263916
train_iter_loss: 0.15977735817432404
train_iter_loss: 0.1880735605955124
train_iter_loss: 0.06135573983192444
train_iter_loss: 0.23182158172130585
train_iter_loss: 0.09897425770759583
train_iter_loss: 0.12119019776582718
train_iter_loss: 0.20322327315807343
train_iter_loss: 0.07267583906650543
train_iter_loss: 0.13386701047420502
train_iter_loss: 0.16910675168037415
train_iter_loss: 0.077456995844841
train_iter_loss: 0.038658879697322845
train_iter_loss: 0.15929505228996277
train loss :0.1640
---------------------
Validation seg loss: 0.21456769477786883 at epoch 499
epoch =    500/  1000, exp = train
train_iter_loss: 0.043583597987890244
train_iter_loss: 0.11155905574560165
train_iter_loss: 0.17048662900924683
train_iter_loss: 0.043100375682115555
train_iter_loss: 0.1867900788784027
train_iter_loss: 0.1968742311000824
train_iter_loss: 0.1398162990808487
train_iter_loss: 0.22025291621685028
train_iter_loss: 0.1250682771205902
train_iter_loss: 0.1212550476193428
train_iter_loss: 0.291301965713501
train_iter_loss: 0.08583544194698334
train_iter_loss: 0.1067032441496849
train_iter_loss: 0.11986196041107178
train_iter_loss: 0.17472480237483978
train_iter_loss: 0.06542911380529404
train_iter_loss: 0.32508954405784607
train_iter_loss: 0.14660073816776276
train_iter_loss: 0.08045870065689087
train_iter_loss: 0.07573524862527847
train_iter_loss: 0.14839008450508118
train_iter_loss: 0.2072143256664276
train_iter_loss: 0.24134212732315063
train_iter_loss: 0.14173689484596252
train_iter_loss: 0.23923838138580322
train_iter_loss: 0.12225276231765747
train_iter_loss: 0.2746642827987671
train_iter_loss: 0.15279452502727509
train_iter_loss: 0.048671819269657135
train_iter_loss: 0.12077223509550095
train_iter_loss: 0.15166595578193665
train_iter_loss: 0.269547700881958
train_iter_loss: 0.15916167199611664
train_iter_loss: 0.10801873356103897
train_iter_loss: 0.17091022431850433
train_iter_loss: 0.12245800346136093
train_iter_loss: 0.08039361983537674
train_iter_loss: 0.14063888788223267
train_iter_loss: 0.19603130221366882
train_iter_loss: 0.1822328120470047
train_iter_loss: 0.25641804933547974
train_iter_loss: 0.09192170202732086
train_iter_loss: 0.21767203509807587
train_iter_loss: 0.17950625717639923
train_iter_loss: 0.21033187210559845
train_iter_loss: 0.12835340201854706
train_iter_loss: 0.2067810595035553
train_iter_loss: 0.17154689133167267
train_iter_loss: 0.11530794948339462
train_iter_loss: 0.22534658014774323
train_iter_loss: 0.15996727347373962
train_iter_loss: 0.2646365761756897
train_iter_loss: 0.12162525206804276
train_iter_loss: 0.1410318911075592
train_iter_loss: 0.12562954425811768
train_iter_loss: 0.15736810863018036
train_iter_loss: 0.23315148055553436
train_iter_loss: 0.15766313672065735
train_iter_loss: 0.25674980878829956
train_iter_loss: 0.19525612890720367
train_iter_loss: 0.12660285830497742
train_iter_loss: 0.09532150626182556
train_iter_loss: 0.12995265424251556
train_iter_loss: 0.19239665567874908
train_iter_loss: 0.12892326712608337
train_iter_loss: 0.23021480441093445
train_iter_loss: 0.1208421066403389
train_iter_loss: 0.12192962318658829
train_iter_loss: 0.1554388403892517
train_iter_loss: 0.21710756421089172
train_iter_loss: 0.10194435715675354
train_iter_loss: 0.12837673723697662
train_iter_loss: 0.3558124005794525
train_iter_loss: 0.0780443474650383
train_iter_loss: 0.0914262905716896
train_iter_loss: 0.18409091234207153
train_iter_loss: 0.14493195712566376
train_iter_loss: 0.17496655881404877
train_iter_loss: 0.11270531266927719
train_iter_loss: 0.22558113932609558
train_iter_loss: 0.10155147314071655
train_iter_loss: 0.15213893353939056
train_iter_loss: 0.06631617993116379
train_iter_loss: 0.23547272384166718
train_iter_loss: 0.04707811400294304
train_iter_loss: 0.11559807509183884
train_iter_loss: 0.09772587567567825
train_iter_loss: 0.18722261488437653
train_iter_loss: 0.11491718888282776
train_iter_loss: 0.1366429626941681
train_iter_loss: 0.30084994435310364
train_iter_loss: 0.12747006118297577
train_iter_loss: 0.08426666259765625
train_iter_loss: 0.18580001592636108
train_iter_loss: 0.12395378947257996
train_iter_loss: 0.16324841976165771
train_iter_loss: 0.22638913989067078
train_iter_loss: 0.22755245864391327
train_iter_loss: 0.3769289255142212
train_iter_loss: 0.24851061403751373
train loss :0.1633
---------------------
Validation seg loss: 0.22009689650797057 at epoch 500
epoch =    501/  1000, exp = train
train_iter_loss: 0.2578926682472229
train_iter_loss: 0.13954836130142212
train_iter_loss: 0.12096193432807922
train_iter_loss: 0.14450523257255554
train_iter_loss: 0.2263151854276657
train_iter_loss: 0.14586883783340454
train_iter_loss: 0.24830123782157898
train_iter_loss: 0.14579002559185028
train_iter_loss: 0.2156856209039688
train_iter_loss: 0.1347656100988388
train_iter_loss: 0.12024390697479248
train_iter_loss: 0.07704583555459976
train_iter_loss: 0.35693418979644775
train_iter_loss: 0.2625048756599426
train_iter_loss: 0.1770610213279724
train_iter_loss: 0.1622503697872162
train_iter_loss: 0.1442718654870987
train_iter_loss: 0.1532786339521408
train_iter_loss: 0.18588194251060486
train_iter_loss: 0.0922674909234047
train_iter_loss: 0.18970926105976105
train_iter_loss: 0.1523607075214386
train_iter_loss: 0.1039033904671669
train_iter_loss: 0.04378698021173477
train_iter_loss: 0.2900233864784241
train_iter_loss: 0.13151484727859497
train_iter_loss: 0.1846926510334015
train_iter_loss: 0.1273631453514099
train_iter_loss: 0.14807343482971191
train_iter_loss: 0.28026875853538513
train_iter_loss: 0.15110909938812256
train_iter_loss: 0.09733347594738007
train_iter_loss: 0.1179167851805687
train_iter_loss: 0.12770459055900574
train_iter_loss: 0.14214108884334564
train_iter_loss: 0.10075537860393524
train_iter_loss: 0.17516343295574188
train_iter_loss: 0.05000065267086029
train_iter_loss: 0.06225329264998436
train_iter_loss: 0.05074188858270645
train_iter_loss: 0.11409354954957962
train_iter_loss: 0.10783079266548157
train_iter_loss: 0.09232085943222046
train_iter_loss: 0.11859024316072464
train_iter_loss: 0.17947247624397278
train_iter_loss: 0.11776775121688843
train_iter_loss: 0.11884531378746033
train_iter_loss: 0.1433275043964386
train_iter_loss: 0.18401122093200684
train_iter_loss: 0.10582447797060013
train_iter_loss: 0.21621227264404297
train_iter_loss: 0.10565685480833054
train_iter_loss: 0.16237390041351318
train_iter_loss: 0.15113960206508636
train_iter_loss: 0.17964056134223938
train_iter_loss: 0.09668691456317902
train_iter_loss: 0.2399781495332718
train_iter_loss: 0.14347487688064575
train_iter_loss: 0.06796235591173172
train_iter_loss: 0.1438608467578888
train_iter_loss: 0.09400542825460434
train_iter_loss: 0.14408287405967712
train_iter_loss: 0.09980463236570358
train_iter_loss: 0.1270044893026352
train_iter_loss: 0.16696147620677948
train_iter_loss: 0.1475837528705597
train_iter_loss: 0.1541036069393158
train_iter_loss: 0.1573607325553894
train_iter_loss: 0.30837878584861755
train_iter_loss: 0.1569938212633133
train_iter_loss: 0.12209147214889526
train_iter_loss: 0.13281527161598206
train_iter_loss: 0.12481909990310669
train_iter_loss: 0.28067898750305176
train_iter_loss: 0.0831582248210907
train_iter_loss: 0.10631628334522247
train_iter_loss: 0.1495371162891388
train_iter_loss: 0.09825023263692856
train_iter_loss: 0.15959426760673523
train_iter_loss: 0.1215783879160881
train_iter_loss: 0.1951313465833664
train_iter_loss: 0.273783802986145
train_iter_loss: 0.16127590835094452
train_iter_loss: 0.1705039143562317
train_iter_loss: 0.14266808331012726
train_iter_loss: 0.2284511774778366
train_iter_loss: 0.1722484827041626
train_iter_loss: 0.09073610603809357
train_iter_loss: 0.14436329901218414
train_iter_loss: 0.15673793852329254
train_iter_loss: 0.37343961000442505
train_iter_loss: 0.242705300450325
train_iter_loss: 0.12763650715351105
train_iter_loss: 0.1278359740972519
train_iter_loss: 0.21859344840049744
train_iter_loss: 0.11182910203933716
train_iter_loss: 0.21048885583877563
train_iter_loss: 0.1515258401632309
train_iter_loss: 0.22734998166561127
train_iter_loss: 0.26767733693122864
train loss :0.1592
---------------------
Validation seg loss: 0.22177880292512336 at epoch 501
epoch =    502/  1000, exp = train
train_iter_loss: 0.1593526005744934
train_iter_loss: 0.08719231933355331
train_iter_loss: 0.14037947356700897
train_iter_loss: 0.11350617557764053
train_iter_loss: 0.07061412930488586
train_iter_loss: 0.13786610960960388
train_iter_loss: 0.15439242124557495
train_iter_loss: 0.16234850883483887
train_iter_loss: 0.2068396508693695
train_iter_loss: 0.14020287990570068
train_iter_loss: 0.17102974653244019
train_iter_loss: 0.0744820088148117
train_iter_loss: 0.24215838313102722
train_iter_loss: 0.2514939606189728
train_iter_loss: 0.06308312714099884
train_iter_loss: 0.17988115549087524
train_iter_loss: 0.14189288020133972
train_iter_loss: 0.09459993243217468
train_iter_loss: 0.21937844157218933
train_iter_loss: 0.1792256385087967
train_iter_loss: 0.1404045671224594
train_iter_loss: 0.1295316219329834
train_iter_loss: 0.1550229787826538
train_iter_loss: 0.15046754479408264
train_iter_loss: 0.2021627426147461
train_iter_loss: 0.1087811291217804
train_iter_loss: 0.22324824333190918
train_iter_loss: 0.13906341791152954
train_iter_loss: 0.13739317655563354
train_iter_loss: 0.12104397267103195
train_iter_loss: 0.14387542009353638
train_iter_loss: 0.2845774292945862
train_iter_loss: 0.3005445599555969
train_iter_loss: 0.21917393803596497
train_iter_loss: 0.2478068619966507
train_iter_loss: 0.13603444397449493
train_iter_loss: 0.1913219392299652
train_iter_loss: 0.2712377607822418
train_iter_loss: 0.05030982196331024
train_iter_loss: 0.11334273964166641
train_iter_loss: 0.14583638310432434
train_iter_loss: 0.14278462529182434
train_iter_loss: 0.1476850062608719
train_iter_loss: 0.13059091567993164
train_iter_loss: 0.09514397382736206
train_iter_loss: 0.17247100174427032
train_iter_loss: 0.2694840729236603
train_iter_loss: 0.1857203096151352
train_iter_loss: 0.2567620277404785
train_iter_loss: 0.12795458734035492
train_iter_loss: 0.1243479922413826
train_iter_loss: 0.1366976797580719
train_iter_loss: 0.062048036605119705
train_iter_loss: 0.26433148980140686
train_iter_loss: 0.09673750400543213
train_iter_loss: 0.11413329839706421
train_iter_loss: 0.08618117868900299
train_iter_loss: 0.12331811338663101
train_iter_loss: 0.06936672329902649
train_iter_loss: 0.12541240453720093
train_iter_loss: 0.15260760486125946
train_iter_loss: 0.20267386734485626
train_iter_loss: 0.09897191822528839
train_iter_loss: 0.48584628105163574
train_iter_loss: 0.14886581897735596
train_iter_loss: 0.18944430351257324
train_iter_loss: 0.09247644990682602
train_iter_loss: 0.11451561748981476
train_iter_loss: 0.12488777190446854
train_iter_loss: 0.16002042591571808
train_iter_loss: 0.30948904156684875
train_iter_loss: 0.1670827716588974
train_iter_loss: 0.11015192419290543
train_iter_loss: 0.13818438351154327
train_iter_loss: 0.059223514050245285
train_iter_loss: 0.241684690117836
train_iter_loss: 0.2413792908191681
train_iter_loss: 0.3101222515106201
train_iter_loss: 0.11976051330566406
train_iter_loss: 0.08022698014974594
train_iter_loss: 0.18615907430648804
train_iter_loss: 0.11777474731206894
train_iter_loss: 0.17928099632263184
train_iter_loss: 0.22866035997867584
train_iter_loss: 0.13931867480278015
train_iter_loss: 0.21599382162094116
train_iter_loss: 0.0871848538517952
train_iter_loss: 0.15557877719402313
train_iter_loss: 0.1759181022644043
train_iter_loss: 0.15779069066047668
train_iter_loss: 0.35338860750198364
train_iter_loss: 0.16857880353927612
train_iter_loss: 0.20459264516830444
train_iter_loss: 0.2602476477622986
train_iter_loss: 0.08144029974937439
train_iter_loss: 0.11586388945579529
train_iter_loss: 0.10761506110429764
train_iter_loss: 0.1674746572971344
train_iter_loss: 0.15552985668182373
train_iter_loss: 0.13672888278961182
train loss :0.1643
---------------------
Validation seg loss: 0.21764576292754906 at epoch 502
epoch =    503/  1000, exp = train
train_iter_loss: 0.18271353840827942
train_iter_loss: 0.16221167147159576
train_iter_loss: 0.22434651851654053
train_iter_loss: 0.14266179502010345
train_iter_loss: 0.07083521038293839
train_iter_loss: 0.15649361908435822
train_iter_loss: 0.08971676230430603
train_iter_loss: 0.08939255028963089
train_iter_loss: 0.09674238413572311
train_iter_loss: 0.1904907375574112
train_iter_loss: 0.1810479313135147
train_iter_loss: 0.15095661580562592
train_iter_loss: 0.15410161018371582
train_iter_loss: 0.31222549080848694
train_iter_loss: 0.09101065993309021
train_iter_loss: 0.19904421269893646
train_iter_loss: 0.21883568167686462
train_iter_loss: 0.06314584612846375
train_iter_loss: 0.1465708166360855
train_iter_loss: 0.07020331919193268
train_iter_loss: 0.11378403007984161
train_iter_loss: 0.12080637365579605
train_iter_loss: 0.2155151218175888
train_iter_loss: 0.2460104376077652
train_iter_loss: 0.18341535329818726
train_iter_loss: 0.10241436213254929
train_iter_loss: 0.15437151491641998
train_iter_loss: 0.10932835936546326
train_iter_loss: 0.13820135593414307
train_iter_loss: 0.1745542287826538
train_iter_loss: 0.14889395236968994
train_iter_loss: 0.2167857438325882
train_iter_loss: 0.11301778256893158
train_iter_loss: 0.2375742644071579
train_iter_loss: 0.17748787999153137
train_iter_loss: 0.1288212090730667
train_iter_loss: 0.1684565544128418
train_iter_loss: 0.21912690997123718
train_iter_loss: 0.12777936458587646
train_iter_loss: 0.06627976894378662
train_iter_loss: 0.1502319723367691
train_iter_loss: 0.12845197319984436
train_iter_loss: 0.23263835906982422
train_iter_loss: 0.142936110496521
train_iter_loss: 0.11227063834667206
train_iter_loss: 0.14451438188552856
train_iter_loss: 0.15141791105270386
train_iter_loss: 0.1839415282011032
train_iter_loss: 0.24223272502422333
train_iter_loss: 0.33021584153175354
train_iter_loss: 0.2393522411584854
train_iter_loss: 0.13170194625854492
train_iter_loss: 0.07562248408794403
train_iter_loss: 0.09849220514297485
train_iter_loss: 0.07770226895809174
train_iter_loss: 0.1234210655093193
train_iter_loss: 0.1733422577381134
train_iter_loss: 0.11400242149829865
train_iter_loss: 0.1815290004014969
train_iter_loss: 0.178529754281044
train_iter_loss: 0.10802073031663895
train_iter_loss: 0.1616532951593399
train_iter_loss: 0.2584449350833893
train_iter_loss: 0.23557011783123016
train_iter_loss: 0.1375362128019333
train_iter_loss: 0.13822780549526215
train_iter_loss: 0.1784438192844391
train_iter_loss: 0.10013926774263382
train_iter_loss: 0.17870180308818817
train_iter_loss: 0.16467006504535675
train_iter_loss: 0.18639899790287018
train_iter_loss: 0.088506318628788
train_iter_loss: 0.1979532241821289
train_iter_loss: 0.22579075396060944
train_iter_loss: 0.07496514171361923
train_iter_loss: 0.11690827459096909
train_iter_loss: 0.2676483988761902
train_iter_loss: 0.07031730562448502
train_iter_loss: 0.1682208925485611
train_iter_loss: 0.07035375386476517
train_iter_loss: 0.08782796561717987
train_iter_loss: 0.07382991164922714
train_iter_loss: 0.13928192853927612
train_iter_loss: 0.061106741428375244
train_iter_loss: 0.23497840762138367
train_iter_loss: 0.13871881365776062
train_iter_loss: 0.11143021285533905
train_iter_loss: 0.16252349317073822
train_iter_loss: 0.1204109638929367
train_iter_loss: 0.23548345267772675
train_iter_loss: 0.152123361825943
train_iter_loss: 0.18181733787059784
train_iter_loss: 0.1270875334739685
train_iter_loss: 0.1945105493068695
train_iter_loss: 0.22302259504795074
train_iter_loss: 0.10210417956113815
train_iter_loss: 0.17907755076885223
train_iter_loss: 0.17374254763126373
train_iter_loss: 0.25286123156547546
train_iter_loss: 0.13737580180168152
train loss :0.1575
---------------------
Validation seg loss: 0.21729039750501233 at epoch 503
epoch =    504/  1000, exp = train
train_iter_loss: 0.1418532282114029
train_iter_loss: 0.05201050639152527
train_iter_loss: 0.13203370571136475
train_iter_loss: 0.1099432185292244
train_iter_loss: 0.2568344473838806
train_iter_loss: 0.25003287196159363
train_iter_loss: 0.2081877589225769
train_iter_loss: 0.10572094470262527
train_iter_loss: 0.09879732877016068
train_iter_loss: 0.1205209270119667
train_iter_loss: 0.17591562867164612
train_iter_loss: 0.2653671205043793
train_iter_loss: 0.07837250083684921
train_iter_loss: 0.08428273350000381
train_iter_loss: 0.19528010487556458
train_iter_loss: 0.13825304806232452
train_iter_loss: 0.10656991600990295
train_iter_loss: 0.18796755373477936
train_iter_loss: 0.08004958927631378
train_iter_loss: 0.16551357507705688
train_iter_loss: 0.15823818743228912
train_iter_loss: 0.08545113354921341
train_iter_loss: 0.15802428126335144
train_iter_loss: 0.2466556429862976
train_iter_loss: 0.2117198407649994
train_iter_loss: 0.18597128987312317
train_iter_loss: 0.15713372826576233
train_iter_loss: 0.2051493525505066
train_iter_loss: 0.07237330079078674
train_iter_loss: 0.1148127093911171
train_iter_loss: 0.10818720608949661
train_iter_loss: 0.3950943946838379
train_iter_loss: 0.08077958971261978
train_iter_loss: 0.2272188663482666
train_iter_loss: 0.19595769047737122
train_iter_loss: 0.14189964532852173
train_iter_loss: 0.18675044178962708
train_iter_loss: 0.14833909273147583
train_iter_loss: 0.11725104600191116
train_iter_loss: 0.08987915515899658
train_iter_loss: 0.23724372684955597
train_iter_loss: 0.3029918968677521
train_iter_loss: 0.12993192672729492
train_iter_loss: 0.14820028841495514
train_iter_loss: 0.10032657533884048
train_iter_loss: 0.18728335201740265
train_iter_loss: 0.10700445622205734
train_iter_loss: 0.13757570087909698
train_iter_loss: 0.15805256366729736
train_iter_loss: 0.11455133557319641
train_iter_loss: 0.026047799736261368
train_iter_loss: 0.13977620005607605
train_iter_loss: 0.14244584739208221
train_iter_loss: 0.13185003399848938
train_iter_loss: 0.13428886234760284
train_iter_loss: 0.14047126471996307
train_iter_loss: 0.27615034580230713
train_iter_loss: 0.1392955183982849
train_iter_loss: 0.1198904737830162
train_iter_loss: 0.0772554874420166
train_iter_loss: 0.17684543132781982
train_iter_loss: 0.1411590278148651
train_iter_loss: 0.2645035684108734
train_iter_loss: 0.1387910693883896
train_iter_loss: 0.09073268622159958
train_iter_loss: 0.19662737846374512
train_iter_loss: 0.21839958429336548
train_iter_loss: 0.1257656365633011
train_iter_loss: 0.13354158401489258
train_iter_loss: 0.1683431714773178
train_iter_loss: 0.3139442503452301
train_iter_loss: 0.22654852271080017
train_iter_loss: 0.22337552905082703
train_iter_loss: 0.12993896007537842
train_iter_loss: 0.07513713836669922
train_iter_loss: 0.16676317155361176
train_iter_loss: 0.12584921717643738
train_iter_loss: 0.21672101318836212
train_iter_loss: 0.14900757372379303
train_iter_loss: 0.22721469402313232
train_iter_loss: 0.11035118997097015
train_iter_loss: 0.23491309583187103
train_iter_loss: 0.07321486622095108
train_iter_loss: 0.1923946738243103
train_iter_loss: 0.2800748646259308
train_iter_loss: 0.19047045707702637
train_iter_loss: 0.10883776098489761
train_iter_loss: 0.13279928267002106
train_iter_loss: 0.2249263972043991
train_iter_loss: 0.18987718224525452
train_iter_loss: 0.33328086137771606
train_iter_loss: 0.10153824090957642
train_iter_loss: 0.19346822798252106
train_iter_loss: 0.17532290518283844
train_iter_loss: 0.18982882797718048
train_iter_loss: 0.18153969943523407
train_iter_loss: 0.2107417732477188
train_iter_loss: 0.22322846949100494
train_iter_loss: 0.1549292951822281
train_iter_loss: 0.23439931869506836
train loss :0.1659
---------------------
Validation seg loss: 0.22467062388599482 at epoch 504
epoch =    505/  1000, exp = train
train_iter_loss: 0.17721712589263916
train_iter_loss: 0.08979590982198715
train_iter_loss: 0.2017144411802292
train_iter_loss: 0.08238964527845383
train_iter_loss: 0.24270495772361755
train_iter_loss: 0.12130960077047348
train_iter_loss: 0.13251708447933197
train_iter_loss: 0.29643887281417847
train_iter_loss: 0.0809008777141571
train_iter_loss: 0.07176940143108368
train_iter_loss: 0.1318141371011734
train_iter_loss: 0.1438053548336029
train_iter_loss: 0.20420482754707336
train_iter_loss: 0.09533578157424927
train_iter_loss: 0.18885748088359833
train_iter_loss: 0.2583412230014801
train_iter_loss: 0.11785320937633514
train_iter_loss: 0.21682709455490112
train_iter_loss: 0.13749368488788605
train_iter_loss: 0.3560989201068878
train_iter_loss: 0.11006568372249603
train_iter_loss: 0.12191008776426315
train_iter_loss: 0.18441416323184967
train_iter_loss: 0.07281043380498886
train_iter_loss: 0.07343258708715439
train_iter_loss: 0.24196673929691315
train_iter_loss: 0.3015674352645874
train_iter_loss: 0.11458542197942734
train_iter_loss: 0.0816892609000206
train_iter_loss: 0.2643984258174896
train_iter_loss: 0.16894349455833435
train_iter_loss: 0.25329723954200745
train_iter_loss: 0.3848380744457245
train_iter_loss: 0.12052233517169952
train_iter_loss: 0.12801826000213623
train_iter_loss: 0.22503525018692017
train_iter_loss: 0.16178256273269653
train_iter_loss: 0.13954773545265198
train_iter_loss: 0.10186152160167694
train_iter_loss: 0.07571365684270859
train_iter_loss: 0.18620435893535614
train_iter_loss: 0.09589345008134842
train_iter_loss: 0.09182088822126389
train_iter_loss: 0.15945038199424744
train_iter_loss: 0.10303013026714325
train_iter_loss: 0.11449185013771057
train_iter_loss: 0.17301514744758606
train_iter_loss: 0.3198583424091339
train_iter_loss: 0.09057317674160004
train_iter_loss: 0.15021657943725586
train_iter_loss: 0.23193180561065674
train_iter_loss: 0.1917009949684143
train_iter_loss: 0.21002282202243805
train_iter_loss: 0.06857533007860184
train_iter_loss: 0.08603885769844055
train_iter_loss: 0.13805222511291504
train_iter_loss: 0.11887063831090927
train_iter_loss: 0.13884547352790833
train_iter_loss: 0.19556765258312225
train_iter_loss: 0.2208680510520935
train_iter_loss: 0.18193858861923218
train_iter_loss: 0.23867231607437134
train_iter_loss: 0.09616776555776596
train_iter_loss: 0.2126636505126953
train_iter_loss: 0.11436197906732559
train_iter_loss: 0.13299748301506042
train_iter_loss: 0.14848355948925018
train_iter_loss: 0.1753930300474167
train_iter_loss: 0.10450223833322525
train_iter_loss: 0.23852339386940002
train_iter_loss: 0.08906158804893494
train_iter_loss: 0.05558910220861435
train_iter_loss: 0.10890010744333267
train_iter_loss: 0.19536173343658447
train_iter_loss: 0.162152960896492
train_iter_loss: 0.21614624559879303
train_iter_loss: 0.0939914882183075
train_iter_loss: 0.11444757133722305
train_iter_loss: 0.14602893590927124
train_iter_loss: 0.2414940595626831
train_iter_loss: 0.21238155663013458
train_iter_loss: 0.19369609653949738
train_iter_loss: 0.19625790417194366
train_iter_loss: 0.19968855381011963
train_iter_loss: 0.10340970754623413
train_iter_loss: 0.23615963757038116
train_iter_loss: 0.17908184230327606
train_iter_loss: 0.150639608502388
train_iter_loss: 0.09460639208555222
train_iter_loss: 0.16481851041316986
train_iter_loss: 0.08630888164043427
train_iter_loss: 0.1366194486618042
train_iter_loss: 0.10839960724115372
train_iter_loss: 0.21461378037929535
train_iter_loss: 0.18483994901180267
train_iter_loss: 0.07591318339109421
train_iter_loss: 0.12889577448368073
train_iter_loss: 0.25496238470077515
train_iter_loss: 0.1252160668373108
train_iter_loss: 0.305930495262146
train loss :0.1634
---------------------
Validation seg loss: 0.221265463966806 at epoch 505
epoch =    506/  1000, exp = train
train_iter_loss: 0.26288074254989624
train_iter_loss: 0.10015041381120682
train_iter_loss: 0.14557570219039917
train_iter_loss: 0.13306796550750732
train_iter_loss: 0.10427793115377426
train_iter_loss: 0.1928539276123047
train_iter_loss: 0.19325219094753265
train_iter_loss: 0.139865905046463
train_iter_loss: 0.0481145940721035
train_iter_loss: 0.16403397917747498
train_iter_loss: 0.07819940149784088
train_iter_loss: 0.2007433921098709
train_iter_loss: 0.23125295341014862
train_iter_loss: 0.09764906018972397
train_iter_loss: 0.12165860086679459
train_iter_loss: 0.1337900012731552
train_iter_loss: 0.1351119428873062
train_iter_loss: 0.2272094041109085
train_iter_loss: 0.1666126400232315
train_iter_loss: 0.1647253930568695
train_iter_loss: 0.21482016146183014
train_iter_loss: 0.13978436589241028
train_iter_loss: 0.14744147658348083
train_iter_loss: 0.21296244859695435
train_iter_loss: 0.1658439040184021
train_iter_loss: 0.10528884828090668
train_iter_loss: 0.16894008219242096
train_iter_loss: 0.17274121940135956
train_iter_loss: 0.10702159255743027
train_iter_loss: 0.13524186611175537
train_iter_loss: 0.1325019896030426
train_iter_loss: 0.14649325609207153
train_iter_loss: 0.20359770953655243
train_iter_loss: 0.19808243215084076
train_iter_loss: 0.10145354270935059
train_iter_loss: 0.21482674777507782
train_iter_loss: 0.06790964305400848
train_iter_loss: 0.26183733344078064
train_iter_loss: 0.07386316359043121
train_iter_loss: 0.03573162853717804
train_iter_loss: 0.19102002680301666
train_iter_loss: 0.23091158270835876
train_iter_loss: 0.18100647628307343
train_iter_loss: 0.1844474971294403
train_iter_loss: 0.16309839487075806
train_iter_loss: 0.22765189409255981
train_iter_loss: 0.14943207800388336
train_iter_loss: 0.18105776607990265
train_iter_loss: 0.06409333646297455
train_iter_loss: 0.09023439139127731
train_iter_loss: 0.11338242143392563
train_iter_loss: 0.13738779723644257
train_iter_loss: 0.26635435223579407
train_iter_loss: 0.11040767282247543
train_iter_loss: 0.16500556468963623
train_iter_loss: 0.16455240547657013
train_iter_loss: 0.19183553755283356
train_iter_loss: 0.08572492748498917
train_iter_loss: 0.07300212234258652
train_iter_loss: 0.19452637434005737
train_iter_loss: 0.29064372181892395
train_iter_loss: 0.1283889263868332
train_iter_loss: 0.19719956815242767
train_iter_loss: 0.15859563648700714
train_iter_loss: 0.1600407063961029
train_iter_loss: 0.13809801638126373
train_iter_loss: 0.20582154393196106
train_iter_loss: 0.2178259938955307
train_iter_loss: 0.21633979678153992
train_iter_loss: 0.23496882617473602
train_iter_loss: 0.1592535823583603
train_iter_loss: 0.0842074379324913
train_iter_loss: 0.14873990416526794
train_iter_loss: 0.1351507157087326
train_iter_loss: 0.26244717836380005
train_iter_loss: 0.1668529510498047
train_iter_loss: 0.18605393171310425
train_iter_loss: 0.20301972329616547
train_iter_loss: 0.23546355962753296
train_iter_loss: 0.17611262202262878
train_iter_loss: 0.07341951131820679
train_iter_loss: 0.13601788878440857
train_iter_loss: 0.17777618765830994
train_iter_loss: 0.16459277272224426
train_iter_loss: 0.06942950189113617
train_iter_loss: 0.14811192452907562
train_iter_loss: 0.14292126893997192
train_iter_loss: 0.11388583481311798
train_iter_loss: 0.12744544446468353
train_iter_loss: 0.21064789593219757
train_iter_loss: 0.10103780031204224
train_iter_loss: 0.20560415089130402
train_iter_loss: 0.1622125208377838
train_iter_loss: 0.09345751255750656
train_iter_loss: 0.2665766775608063
train_iter_loss: 0.08104254305362701
train_iter_loss: 0.14455877244472504
train_iter_loss: 0.08872843533754349
train_iter_loss: 0.1393699049949646
train_iter_loss: 0.17846059799194336
train loss :0.1591
---------------------
Validation seg loss: 0.2163439541791548 at epoch 506
epoch =    507/  1000, exp = train
train_iter_loss: 0.20455031096935272
train_iter_loss: 0.16588044166564941
train_iter_loss: 0.17578338086605072
train_iter_loss: 0.1568688601255417
train_iter_loss: 0.14855517446994781
train_iter_loss: 0.07318244129419327
train_iter_loss: 0.23534680902957916
train_iter_loss: 0.15556445717811584
train_iter_loss: 0.1465890109539032
train_iter_loss: 0.04802042245864868
train_iter_loss: 0.2821721136569977
train_iter_loss: 0.18942613899707794
train_iter_loss: 0.18396568298339844
train_iter_loss: 0.07493744045495987
train_iter_loss: 0.2718232572078705
train_iter_loss: 0.13891056180000305
train_iter_loss: 0.10776201635599136
train_iter_loss: 0.18573245406150818
train_iter_loss: 0.3003523051738739
train_iter_loss: 0.12617136538028717
train_iter_loss: 0.15920311212539673
train_iter_loss: 0.13945595920085907
train_iter_loss: 0.11771254986524582
train_iter_loss: 0.11900555342435837
train_iter_loss: 0.18806232511997223
train_iter_loss: 0.1430663764476776
train_iter_loss: 0.3138338327407837
train_iter_loss: 0.09877379238605499
train_iter_loss: 0.1505659967660904
train_iter_loss: 0.21374556422233582
train_iter_loss: 0.1002730205655098
train_iter_loss: 0.10232580453157425
train_iter_loss: 0.08807835727930069
train_iter_loss: 0.11824097484350204
train_iter_loss: 0.20494134724140167
train_iter_loss: 0.15323828160762787
train_iter_loss: 0.1343829184770584
train_iter_loss: 0.17854830622673035
train_iter_loss: 0.19440166652202606
train_iter_loss: 0.07907524704933167
train_iter_loss: 0.16564157605171204
train_iter_loss: 0.08382704854011536
train_iter_loss: 0.15742409229278564
train_iter_loss: 0.22946633398532867
train_iter_loss: 0.07520309090614319
train_iter_loss: 0.13896000385284424
train_iter_loss: 0.16064196825027466
train_iter_loss: 0.1356634944677353
train_iter_loss: 0.20891743898391724
train_iter_loss: 0.11114205420017242
train_iter_loss: 0.1292806714773178
train_iter_loss: 0.21228472888469696
train_iter_loss: 0.13050217926502228
train_iter_loss: 0.1701982021331787
train_iter_loss: 0.08577703684568405
train_iter_loss: 0.15956471860408783
train_iter_loss: 0.14876317977905273
train_iter_loss: 0.19242939352989197
train_iter_loss: 0.09927336871623993
train_iter_loss: 0.11588014662265778
train_iter_loss: 0.15352816879749298
train_iter_loss: 0.2088942974805832
train_iter_loss: 0.1682872623205185
train_iter_loss: 0.145080104470253
train_iter_loss: 0.2016838937997818
train_iter_loss: 0.10408184677362442
train_iter_loss: 0.20972928404808044
train_iter_loss: 0.1695285588502884
train_iter_loss: 0.08726206421852112
train_iter_loss: 0.2054692953824997
train_iter_loss: 0.18678876757621765
train_iter_loss: 0.15218065679073334
train_iter_loss: 0.05319002643227577
train_iter_loss: 0.07904268801212311
train_iter_loss: 0.12206126004457474
train_iter_loss: 0.16720011830329895
train_iter_loss: 0.3194947838783264
train_iter_loss: 0.19026480615139008
train_iter_loss: 0.10172592103481293
train_iter_loss: 0.23859728872776031
train_iter_loss: 0.11975477635860443
train_iter_loss: 0.14468812942504883
train_iter_loss: 0.16274985671043396
train_iter_loss: 0.1713978797197342
train_iter_loss: 0.20115184783935547
train_iter_loss: 0.1344621181488037
train_iter_loss: 0.1257675141096115
train_iter_loss: 0.10455118864774704
train_iter_loss: 0.11877269297838211
train_iter_loss: 0.12049639225006104
train_iter_loss: 0.3363640606403351
train_iter_loss: 0.17248912155628204
train_iter_loss: 0.16705232858657837
train_iter_loss: 0.30885058641433716
train_iter_loss: 0.18796202540397644
train_iter_loss: 0.0939154177904129
train_iter_loss: 0.09038792550563812
train_iter_loss: 0.07544595003128052
train_iter_loss: 0.09256114810705185
train_iter_loss: 0.1856994330883026
train loss :0.1582
---------------------
Validation seg loss: 0.220249582486192 at epoch 507
epoch =    508/  1000, exp = train
train_iter_loss: 0.08282999694347382
train_iter_loss: 0.12766608595848083
train_iter_loss: 0.19403879344463348
train_iter_loss: 0.15095330774784088
train_iter_loss: 0.09793906658887863
train_iter_loss: 0.0692230835556984
train_iter_loss: 0.2105966955423355
train_iter_loss: 0.2785436809062958
train_iter_loss: 0.1352129876613617
train_iter_loss: 0.24678796529769897
train_iter_loss: 0.13145829737186432
train_iter_loss: 0.11881338804960251
train_iter_loss: 0.19777743518352509
train_iter_loss: 0.09769070148468018
train_iter_loss: 0.09963055700063705
train_iter_loss: 0.15140210092067719
train_iter_loss: 0.17738564312458038
train_iter_loss: 0.13366927206516266
train_iter_loss: 0.046339504420757294
train_iter_loss: 0.04965328425168991
train_iter_loss: 0.13173669576644897
train_iter_loss: 0.24690686166286469
train_iter_loss: 0.14692065119743347
train_iter_loss: 0.2161722034215927
train_iter_loss: 0.16564306616783142
train_iter_loss: 0.1654936820268631
train_iter_loss: 0.18231378495693207
train_iter_loss: 0.22225911915302277
train_iter_loss: 0.08832100033760071
train_iter_loss: 0.18977467715740204
train_iter_loss: 0.0985092893242836
train_iter_loss: 0.2279970347881317
train_iter_loss: 0.1553024798631668
train_iter_loss: 0.18376986682415009
train_iter_loss: 0.15812312066555023
train_iter_loss: 0.22806903719902039
train_iter_loss: 0.21330520510673523
train_iter_loss: 0.07937350869178772
train_iter_loss: 0.25800663232803345
train_iter_loss: 0.16285598278045654
train_iter_loss: 0.20576556026935577
train_iter_loss: 0.18967899680137634
train_iter_loss: 0.19375847280025482
train_iter_loss: 0.34927698969841003
train_iter_loss: 0.213723286986351
train_iter_loss: 0.20138518512248993
train_iter_loss: 0.10794749855995178
train_iter_loss: 0.11556576192378998
train_iter_loss: 0.1863696426153183
train_iter_loss: 0.12322382628917694
train_iter_loss: 0.35503333806991577
train_iter_loss: 0.08788714557886124
train_iter_loss: 0.21148911118507385
train_iter_loss: 0.290667325258255
train_iter_loss: 0.08967778086662292
train_iter_loss: 0.16825154423713684
train_iter_loss: 0.29967668652534485
train_iter_loss: 0.13653579354286194
train_iter_loss: 0.13584889471530914
train_iter_loss: 0.2277192622423172
train_iter_loss: 0.15903161466121674
train_iter_loss: 0.16376172006130219
train_iter_loss: 0.11316905170679092
train_iter_loss: 0.18533234298229218
train_iter_loss: 0.08293867856264114
train_iter_loss: 0.23213955760002136
train_iter_loss: 0.17603559792041779
train_iter_loss: 0.14810895919799805
train_iter_loss: 0.19144901633262634
train_iter_loss: 0.07820219546556473
train_iter_loss: 0.08714684844017029
train_iter_loss: 0.2513650953769684
train_iter_loss: 0.17085693776607513
train_iter_loss: 0.08702818304300308
train_iter_loss: 0.1813955307006836
train_iter_loss: 0.20057477056980133
train_iter_loss: 0.06406994163990021
train_iter_loss: 0.11093306541442871
train_iter_loss: 0.14894410967826843
train_iter_loss: 0.1307256668806076
train_iter_loss: 0.12221866846084595
train_iter_loss: 0.10469959676265717
train_iter_loss: 0.07940720021724701
train_iter_loss: 0.11386184394359589
train_iter_loss: 0.11093565821647644
train_iter_loss: 0.2320794016122818
train_iter_loss: 0.20771725475788116
train_iter_loss: 0.2545342743396759
train_iter_loss: 0.16443876922130585
train_iter_loss: 0.18849307298660278
train_iter_loss: 0.09309223294258118
train_iter_loss: 0.30662697553634644
train_iter_loss: 0.1517947018146515
train_iter_loss: 0.2702491879463196
train_iter_loss: 0.18273808062076569
train_iter_loss: 0.16352573037147522
train_iter_loss: 0.05874846875667572
train_iter_loss: 0.22512422502040863
train_iter_loss: 0.07894264906644821
train_iter_loss: 0.1796926110982895
train loss :0.1662
---------------------
Validation seg loss: 0.21921403413497895 at epoch 508
epoch =    509/  1000, exp = train
train_iter_loss: 0.1893433779478073
train_iter_loss: 0.1131458505988121
train_iter_loss: 0.05101119726896286
train_iter_loss: 0.11090335994958878
train_iter_loss: 0.14562761783599854
train_iter_loss: 0.07253143191337585
train_iter_loss: 0.16764511168003082
train_iter_loss: 0.19642388820648193
train_iter_loss: 0.1438533216714859
train_iter_loss: 0.0748753622174263
train_iter_loss: 0.15600520372390747
train_iter_loss: 0.10286547988653183
train_iter_loss: 0.116050586104393
train_iter_loss: 0.20886947214603424
train_iter_loss: 0.1451241821050644
train_iter_loss: 0.12357518821954727
train_iter_loss: 0.21872089803218842
train_iter_loss: 0.24591700732707977
train_iter_loss: 0.10084199905395508
train_iter_loss: 0.07609225809574127
train_iter_loss: 0.12001663446426392
train_iter_loss: 0.14245961606502533
train_iter_loss: 0.11727011203765869
train_iter_loss: 0.17750117182731628
train_iter_loss: 0.21017307043075562
train_iter_loss: 0.12775695323944092
train_iter_loss: 0.17584946751594543
train_iter_loss: 0.1273113638162613
train_iter_loss: 0.19103041291236877
train_iter_loss: 0.1543605774641037
train_iter_loss: 0.13858647644519806
train_iter_loss: 0.2963460087776184
train_iter_loss: 0.12094533443450928
train_iter_loss: 0.10953010618686676
train_iter_loss: 0.19484968483448029
train_iter_loss: 0.12489365041255951
train_iter_loss: 0.12252973020076752
train_iter_loss: 0.17446069419384003
train_iter_loss: 0.24423694610595703
train_iter_loss: 0.11177179217338562
train_iter_loss: 0.19084025919437408
train_iter_loss: 0.19549241662025452
train_iter_loss: 0.10826142132282257
train_iter_loss: 0.19522812962532043
train_iter_loss: 0.20327337086200714
train_iter_loss: 0.1329057365655899
train_iter_loss: 0.12950700521469116
train_iter_loss: 0.12398809939622879
train_iter_loss: 0.22669729590415955
train_iter_loss: 0.1259862333536148
train_iter_loss: 0.17492486536502838
train_iter_loss: 0.19879165291786194
train_iter_loss: 0.14912499487400055
train_iter_loss: 0.1661134660243988
train_iter_loss: 0.25398969650268555
train_iter_loss: 0.3791838586330414
train_iter_loss: 0.13630250096321106
train_iter_loss: 0.16715508699417114
train_iter_loss: 0.24758166074752808
train_iter_loss: 0.1301339715719223
train_iter_loss: 0.3839742839336395
train_iter_loss: 0.1436503529548645
train_iter_loss: 0.11832179874181747
train_iter_loss: 0.18170008063316345
train_iter_loss: 0.09449189156293869
train_iter_loss: 0.1925908625125885
train_iter_loss: 0.19297188520431519
train_iter_loss: 0.1061321496963501
train_iter_loss: 0.1491023451089859
train_iter_loss: 0.1672673374414444
train_iter_loss: 0.25282955169677734
train_iter_loss: 0.16216430068016052
train_iter_loss: 0.16703441739082336
train_iter_loss: 0.19252091646194458
train_iter_loss: 0.20423610508441925
train_iter_loss: 0.20579451322555542
train_iter_loss: 0.1260460615158081
train_iter_loss: 0.14319509267807007
train_iter_loss: 0.2319745421409607
train_iter_loss: 0.17591875791549683
train_iter_loss: 0.10586525499820709
train_iter_loss: 0.1346229612827301
train_iter_loss: 0.11856305599212646
train_iter_loss: 0.13034558296203613
train_iter_loss: 0.14123962819576263
train_iter_loss: 0.21484173834323883
train_iter_loss: 0.07352496683597565
train_iter_loss: 0.23870231211185455
train_iter_loss: 0.14557810127735138
train_iter_loss: 0.14292673766613007
train_iter_loss: 0.22390331327915192
train_iter_loss: 0.17548660933971405
train_iter_loss: 0.14942364394664764
train_iter_loss: 0.36970624327659607
train_iter_loss: 0.11703833192586899
train_iter_loss: 0.04026095196604729
train_iter_loss: 0.21342428028583527
train_iter_loss: 0.09733693301677704
train_iter_loss: 0.13411210477352142
train_iter_loss: 0.12815481424331665
train loss :0.1643
---------------------
Validation seg loss: 0.2162302249673543 at epoch 509
epoch =    510/  1000, exp = train
train_iter_loss: 0.1808675080537796
train_iter_loss: 0.09167084842920303
train_iter_loss: 0.181470587849617
train_iter_loss: 0.1186537966132164
train_iter_loss: 0.10971272736787796
train_iter_loss: 0.22532039880752563
train_iter_loss: 0.18508167564868927
train_iter_loss: 0.15810172259807587
train_iter_loss: 0.3116695284843445
train_iter_loss: 0.13726367056369781
train_iter_loss: 0.14862754940986633
train_iter_loss: 0.23716199398040771
train_iter_loss: 0.08287563920021057
train_iter_loss: 0.17531195282936096
train_iter_loss: 0.20150509476661682
train_iter_loss: 0.14737921953201294
train_iter_loss: 0.04540573060512543
train_iter_loss: 0.2126404196023941
train_iter_loss: 0.18420858681201935
train_iter_loss: 0.3389631509780884
train_iter_loss: 0.15998899936676025
train_iter_loss: 0.11051591485738754
train_iter_loss: 0.08916682749986649
train_iter_loss: 0.04711843654513359
train_iter_loss: 0.10063152015209198
train_iter_loss: 0.16491957008838654
train_iter_loss: 0.2604334354400635
train_iter_loss: 0.24123097956180573
train_iter_loss: 0.1263013780117035
train_iter_loss: 0.13397376239299774
train_iter_loss: 0.18440623581409454
train_iter_loss: 0.13236741721630096
train_iter_loss: 0.269620805978775
train_iter_loss: 0.12439461052417755
train_iter_loss: 0.22879517078399658
train_iter_loss: 0.31508058309555054
train_iter_loss: 0.1892230361700058
train_iter_loss: 0.1627843677997589
train_iter_loss: 0.14616765081882477
train_iter_loss: 0.13994398713111877
train_iter_loss: 0.13750088214874268
train_iter_loss: 0.2261834740638733
train_iter_loss: 0.14533564448356628
train_iter_loss: 0.15062202513217926
train_iter_loss: 0.12544195353984833
train_iter_loss: 0.26760685443878174
train_iter_loss: 0.04989437758922577
train_iter_loss: 0.0967971608042717
train_iter_loss: 0.30034762620925903
train_iter_loss: 0.16304807364940643
train_iter_loss: 0.15813186764717102
train_iter_loss: 0.15865713357925415
train_iter_loss: 0.0915316566824913
train_iter_loss: 0.1511635184288025
train_iter_loss: 0.1722811609506607
train_iter_loss: 0.07816039025783539
train_iter_loss: 0.1322811245918274
train_iter_loss: 0.11256450414657593
train_iter_loss: 0.06397853791713715
train_iter_loss: 0.13156700134277344
train_iter_loss: 0.023211412131786346
train_iter_loss: 0.2377711534500122
train_iter_loss: 0.147418811917305
train_iter_loss: 0.17085303366184235
train_iter_loss: 0.20073258876800537
train_iter_loss: 0.05982884019613266
train_iter_loss: 0.11527755856513977
train_iter_loss: 0.14295634627342224
train_iter_loss: 0.2691601514816284
train_iter_loss: 0.05983959510922432
train_iter_loss: 0.15312360227108002
train_iter_loss: 0.09636230021715164
train_iter_loss: 0.12507957220077515
train_iter_loss: 0.13573360443115234
train_iter_loss: 0.18272259831428528
train_iter_loss: 0.14454154670238495
train_iter_loss: 0.14711979031562805
train_iter_loss: 0.26740363240242004
train_iter_loss: 0.12716640532016754
train_iter_loss: 0.24934208393096924
train_iter_loss: 0.14721828699111938
train_iter_loss: 0.16451852023601532
train_iter_loss: 0.22020995616912842
train_iter_loss: 0.15528690814971924
train_iter_loss: 0.21133160591125488
train_iter_loss: 0.10133638232946396
train_iter_loss: 0.10207230597734451
train_iter_loss: 0.14444850385189056
train_iter_loss: 0.17364859580993652
train_iter_loss: 0.09816129505634308
train_iter_loss: 0.2922559082508087
train_iter_loss: 0.34580710530281067
train_iter_loss: 0.14098045229911804
train_iter_loss: 0.13947464525699615
train_iter_loss: 0.10781320929527283
train_iter_loss: 0.2197042852640152
train_iter_loss: 0.1316424310207367
train_iter_loss: 0.16640840470790863
train_iter_loss: 0.21617253124713898
train_iter_loss: 0.17594707012176514
train loss :0.1639
---------------------
Validation seg loss: 0.21948746869445973 at epoch 510
epoch =    511/  1000, exp = train
train_iter_loss: 0.08344034105539322
train_iter_loss: 0.16331110894680023
train_iter_loss: 0.17427238821983337
train_iter_loss: 0.3300921618938446
train_iter_loss: 0.08119705319404602
train_iter_loss: 0.12553323805332184
train_iter_loss: 0.05177965760231018
train_iter_loss: 0.2113557904958725
train_iter_loss: 0.1673731952905655
train_iter_loss: 0.28648412227630615
train_iter_loss: 0.14813894033432007
train_iter_loss: 0.1176665797829628
train_iter_loss: 0.17126823961734772
train_iter_loss: 0.1046241819858551
train_iter_loss: 0.22811919450759888
train_iter_loss: 0.08531729131937027
train_iter_loss: 0.11838654428720474
train_iter_loss: 0.12297623604536057
train_iter_loss: 0.1456637680530548
train_iter_loss: 0.1915101408958435
train_iter_loss: 0.1648339480161667
train_iter_loss: 0.17139984667301178
train_iter_loss: 0.14239501953125
train_iter_loss: 0.13038265705108643
train_iter_loss: 0.25804856419563293
train_iter_loss: 0.15350866317749023
train_iter_loss: 0.10413097590208054
train_iter_loss: 0.0991823673248291
train_iter_loss: 0.24786175787448883
train_iter_loss: 0.2072790563106537
train_iter_loss: 0.27497628331184387
train_iter_loss: 0.15990622341632843
train_iter_loss: 0.1660669595003128
train_iter_loss: 0.1377166509628296
train_iter_loss: 0.11492381244897842
train_iter_loss: 0.17853468656539917
train_iter_loss: 0.19159050285816193
train_iter_loss: 0.10913728922605515
train_iter_loss: 0.16742461919784546
train_iter_loss: 0.19625593721866608
train_iter_loss: 0.11675002425909042
train_iter_loss: 0.15509507060050964
train_iter_loss: 0.13553355634212494
train_iter_loss: 0.18697942793369293
train_iter_loss: 0.05829974263906479
train_iter_loss: 0.20190393924713135
train_iter_loss: 0.09946991503238678
train_iter_loss: 0.1938883364200592
train_iter_loss: 0.12130829691886902
train_iter_loss: 0.1636364609003067
train_iter_loss: 0.11625777930021286
train_iter_loss: 0.14832742512226105
train_iter_loss: 0.20969532430171967
train_iter_loss: 0.1989721953868866
train_iter_loss: 0.22167448699474335
train_iter_loss: 0.20047838985919952
train_iter_loss: 0.12137322872877121
train_iter_loss: 0.3446619510650635
train_iter_loss: 0.13907909393310547
train_iter_loss: 0.11945794522762299
train_iter_loss: 0.11433148384094238
train_iter_loss: 0.17530915141105652
train_iter_loss: 0.15994983911514282
train_iter_loss: 0.18602354824543
train_iter_loss: 0.22744552791118622
train_iter_loss: 0.23896174132823944
train_iter_loss: 0.13446715474128723
train_iter_loss: 0.14016804099082947
train_iter_loss: 0.124688059091568
train_iter_loss: 0.24681290984153748
train_iter_loss: 0.07239789515733719
train_iter_loss: 0.11121263355016708
train_iter_loss: 0.151147723197937
train_iter_loss: 0.11800316721200943
train_iter_loss: 0.18140017986297607
train_iter_loss: 0.15927058458328247
train_iter_loss: 0.07600363343954086
train_iter_loss: 0.09891735017299652
train_iter_loss: 0.1766921579837799
train_iter_loss: 0.14162763953208923
train_iter_loss: 0.10350704193115234
train_iter_loss: 0.13869138062000275
train_iter_loss: 0.12458955496549606
train_iter_loss: 0.12330503016710281
train_iter_loss: 0.10896174609661102
train_iter_loss: 0.12309753894805908
train_iter_loss: 0.2078186571598053
train_iter_loss: 0.14127542078495026
train_iter_loss: 0.22952519357204437
train_iter_loss: 0.09946512430906296
train_iter_loss: 0.2435571253299713
train_iter_loss: 0.27506235241889954
train_iter_loss: 0.06785246729850769
train_iter_loss: 0.2013109028339386
train_iter_loss: 0.09414489567279816
train_iter_loss: 0.20472779870033264
train_iter_loss: 0.18446792662143707
train_iter_loss: 0.20304884016513824
train_iter_loss: 0.15203462541103363
train_iter_loss: 0.16287513077259064
train loss :0.1612
---------------------
Validation seg loss: 0.2168779141656211 at epoch 511
epoch =    512/  1000, exp = train
train_iter_loss: 0.11000115424394608
train_iter_loss: 0.032957252115011215
train_iter_loss: 0.2541256248950958
train_iter_loss: 0.20535092055797577
train_iter_loss: 0.16400092840194702
train_iter_loss: 0.24267449975013733
train_iter_loss: 0.08325869590044022
train_iter_loss: 0.1871640682220459
train_iter_loss: 0.11529861390590668
train_iter_loss: 0.12318388372659683
train_iter_loss: 0.1280342936515808
train_iter_loss: 0.18490645289421082
train_iter_loss: 0.2404772937297821
train_iter_loss: 0.19406327605247498
train_iter_loss: 0.13896147906780243
train_iter_loss: 0.3253863751888275
train_iter_loss: 0.12503260374069214
train_iter_loss: 0.1471560150384903
train_iter_loss: 0.11874453723430634
train_iter_loss: 0.09539569914340973
train_iter_loss: 0.07769366353750229
train_iter_loss: 0.13665679097175598
train_iter_loss: 0.15890362858772278
train_iter_loss: 0.245362788438797
train_iter_loss: 0.14600175619125366
train_iter_loss: 0.2267318069934845
train_iter_loss: 0.1612127125263214
train_iter_loss: 0.1680176854133606
train_iter_loss: 0.0889725610613823
train_iter_loss: 0.19467806816101074
train_iter_loss: 0.09403052181005478
train_iter_loss: 0.11507254838943481
train_iter_loss: 0.1510888785123825
train_iter_loss: 0.09940789639949799
train_iter_loss: 0.13136214017868042
train_iter_loss: 0.2184184491634369
train_iter_loss: 0.10039719939231873
train_iter_loss: 0.07444649934768677
train_iter_loss: 0.07735021412372589
train_iter_loss: 0.1445523351430893
train_iter_loss: 0.2648254632949829
train_iter_loss: 0.290531188249588
train_iter_loss: 0.08110253512859344
train_iter_loss: 0.20624932646751404
train_iter_loss: 0.2807506024837494
train_iter_loss: 0.15982265770435333
train_iter_loss: 0.1678333431482315
train_iter_loss: 0.21797646582126617
train_iter_loss: 0.12494802474975586
train_iter_loss: 0.05072713643312454
train_iter_loss: 0.14990399777889252
train_iter_loss: 0.1483793407678604
train_iter_loss: 0.13598453998565674
train_iter_loss: 0.16149146854877472
train_iter_loss: 0.2175760269165039
train_iter_loss: 0.23824819922447205
train_iter_loss: 0.2770406901836395
train_iter_loss: 0.13985386490821838
train_iter_loss: 0.11642421782016754
train_iter_loss: 0.09433447569608688
train_iter_loss: 0.15112553536891937
train_iter_loss: 0.19746190309524536
train_iter_loss: 0.05520080775022507
train_iter_loss: 0.2664009630680084
train_iter_loss: 0.26267388463020325
train_iter_loss: 0.18015208840370178
train_iter_loss: 0.081325002014637
train_iter_loss: 0.1130046397447586
train_iter_loss: 0.09703101962804794
train_iter_loss: 0.1460440754890442
train_iter_loss: 0.18065758049488068
train_iter_loss: 0.20233094692230225
train_iter_loss: 0.09674379229545593
train_iter_loss: 0.1292540580034256
train_iter_loss: 0.15260544419288635
train_iter_loss: 0.11308039724826813
train_iter_loss: 0.21393553912639618
train_iter_loss: 0.35429829359054565
train_iter_loss: 0.09590207785367966
train_iter_loss: 0.11671777814626694
train_iter_loss: 0.12734153866767883
train_iter_loss: 0.16811978816986084
train_iter_loss: 0.1684180051088333
train_iter_loss: 0.11743269115686417
train_iter_loss: 0.19198626279830933
train_iter_loss: 0.20406773686408997
train_iter_loss: 0.1245049238204956
train_iter_loss: 0.04076956957578659
train_iter_loss: 0.06641733646392822
train_iter_loss: 0.15368498861789703
train_iter_loss: 0.20044977962970734
train_iter_loss: 0.15157654881477356
train_iter_loss: 0.3207017481327057
train_iter_loss: 0.16306425631046295
train_iter_loss: 0.1407747119665146
train_iter_loss: 0.15908347070217133
train_iter_loss: 0.2057904750108719
train_iter_loss: 0.12019632011651993
train_iter_loss: 0.14476722478866577
train_iter_loss: 0.12243753671646118
train loss :0.1601
---------------------
Validation seg loss: 0.21973860073166918 at epoch 512
epoch =    513/  1000, exp = train
train_iter_loss: 0.19533564150333405
train_iter_loss: 0.08387589454650879
train_iter_loss: 0.1709725707769394
train_iter_loss: 0.34163984656333923
train_iter_loss: 0.1679595559835434
train_iter_loss: 0.07228843122720718
train_iter_loss: 0.2456338107585907
train_iter_loss: 0.17304912209510803
train_iter_loss: 0.09000811725854874
train_iter_loss: 0.1549176126718521
train_iter_loss: 0.1294594705104828
train_iter_loss: 0.164871484041214
train_iter_loss: 0.08911923319101334
train_iter_loss: 0.1260278970003128
train_iter_loss: 0.11770816147327423
train_iter_loss: 0.18329833447933197
train_iter_loss: 0.15154114365577698
train_iter_loss: 0.1036902517080307
train_iter_loss: 0.15227046608924866
train_iter_loss: 0.14905475080013275
train_iter_loss: 0.11940307915210724
train_iter_loss: 0.1836797297000885
train_iter_loss: 0.18545220792293549
train_iter_loss: 0.21380247175693512
train_iter_loss: 0.16857878863811493
train_iter_loss: 0.20587098598480225
train_iter_loss: 0.10208703577518463
train_iter_loss: 0.1627883017063141
train_iter_loss: 0.1348068118095398
train_iter_loss: 0.11242929100990295
train_iter_loss: 0.16288022696971893
train_iter_loss: 0.19307191669940948
train_iter_loss: 0.10637709498405457
train_iter_loss: 0.13768579065799713
train_iter_loss: 0.11578715592622757
train_iter_loss: 0.12483231723308563
train_iter_loss: 0.1648472249507904
train_iter_loss: 0.15943120419979095
train_iter_loss: 0.12640585005283356
train_iter_loss: 0.2881182134151459
train_iter_loss: 0.19190318882465363
train_iter_loss: 0.025157762691378593
train_iter_loss: 0.14155155420303345
train_iter_loss: 0.15808036923408508
train_iter_loss: 0.08654855191707611
train_iter_loss: 0.24368759989738464
train_iter_loss: 0.15751643478870392
train_iter_loss: 0.12339706718921661
train_iter_loss: 0.10861197113990784
train_iter_loss: 0.16051387786865234
train_iter_loss: 0.1712334007024765
train_iter_loss: 0.15183186531066895
train_iter_loss: 0.3043310344219208
train_iter_loss: 0.06827683001756668
train_iter_loss: 0.15850938856601715
train_iter_loss: 0.08516185730695724
train_iter_loss: 0.12147551029920578
train_iter_loss: 0.1638498157262802
train_iter_loss: 0.23361437022686005
train_iter_loss: 0.04351440817117691
train_iter_loss: 0.1902526617050171
train_iter_loss: 0.06869526207447052
train_iter_loss: 0.1362733095884323
train_iter_loss: 0.1586826741695404
train_iter_loss: 0.19687804579734802
train_iter_loss: 0.17000634968280792
train_iter_loss: 0.10398870706558228
train_iter_loss: 0.16054625809192657
train_iter_loss: 0.12878188490867615
train_iter_loss: 0.1623324304819107
train_iter_loss: 0.13146474957466125
train_iter_loss: 0.1343199461698532
train_iter_loss: 0.1686185598373413
train_iter_loss: 0.11554818600416183
train_iter_loss: 0.2050849199295044
train_iter_loss: 0.15477077662944794
train_iter_loss: 0.1185431033372879
train_iter_loss: 0.21215535700321198
train_iter_loss: 0.16166149079799652
train_iter_loss: 0.16202130913734436
train_iter_loss: 0.1381266862154007
train_iter_loss: 0.10350490361452103
train_iter_loss: 0.2066507786512375
train_iter_loss: 0.18687279522418976
train_iter_loss: 0.09152407944202423
train_iter_loss: 0.13820618391036987
train_iter_loss: 0.10072515159845352
train_iter_loss: 0.08661677688360214
train_iter_loss: 0.17637307941913605
train_iter_loss: 0.3587924540042877
train_iter_loss: 0.20282700657844543
train_iter_loss: 0.1699407994747162
train_iter_loss: 0.30762171745300293
train_iter_loss: 0.15164583921432495
train_iter_loss: 0.125657856464386
train_iter_loss: 0.07645250856876373
train_iter_loss: 0.13180230557918549
train_iter_loss: 0.21921952068805695
train_iter_loss: 0.1260659545660019
train_iter_loss: 0.15140140056610107
train loss :0.1558
---------------------
Validation seg loss: 0.21765981268418846 at epoch 513
epoch =    514/  1000, exp = train
train_iter_loss: 0.11954990029335022
train_iter_loss: 0.13087470829486847
train_iter_loss: 0.11028630286455154
train_iter_loss: 0.053562890738248825
train_iter_loss: 0.20774602890014648
train_iter_loss: 0.15497982501983643
train_iter_loss: 0.101531483232975
train_iter_loss: 0.1665850132703781
train_iter_loss: 0.10395120084285736
train_iter_loss: 0.1674976497888565
train_iter_loss: 0.27599528431892395
train_iter_loss: 0.16665995121002197
train_iter_loss: 0.05955900624394417
train_iter_loss: 0.24312973022460938
train_iter_loss: 0.22138255834579468
train_iter_loss: 0.16806760430335999
train_iter_loss: 0.19008657336235046
train_iter_loss: 0.2250131219625473
train_iter_loss: 0.11149554699659348
train_iter_loss: 0.1864652782678604
train_iter_loss: 0.14014461636543274
train_iter_loss: 0.20508022606372833
train_iter_loss: 0.1925765424966812
train_iter_loss: 0.15509580075740814
train_iter_loss: 0.12509261071681976
train_iter_loss: 0.1292213350534439
train_iter_loss: 0.10339369624853134
train_iter_loss: 0.15514415502548218
train_iter_loss: 0.1810276210308075
train_iter_loss: 0.1971019208431244
train_iter_loss: 0.19133679568767548
train_iter_loss: 0.15734358131885529
train_iter_loss: 0.14891597628593445
train_iter_loss: 0.14936143159866333
train_iter_loss: 0.1647409051656723
train_iter_loss: 0.19059498608112335
train_iter_loss: 0.19336716830730438
train_iter_loss: 0.04682840406894684
train_iter_loss: 0.16114623844623566
train_iter_loss: 0.2926264703273773
train_iter_loss: 0.19535966217517853
train_iter_loss: 0.17878147959709167
train_iter_loss: 0.095221608877182
train_iter_loss: 0.17333970963954926
train_iter_loss: 0.14640121161937714
train_iter_loss: 0.1781916469335556
train_iter_loss: 0.1282016485929489
train_iter_loss: 0.07534264773130417
train_iter_loss: 0.09490367770195007
train_iter_loss: 0.10444244742393494
train_iter_loss: 0.13782937824726105
train_iter_loss: 0.10665668547153473
train_iter_loss: 0.18518346548080444
train_iter_loss: 0.17099103331565857
train_iter_loss: 0.17536571621894836
train_iter_loss: 0.1352306604385376
train_iter_loss: 0.09864187985658646
train_iter_loss: 0.12153280526399612
train_iter_loss: 0.1685524731874466
train_iter_loss: 0.27940046787261963
train_iter_loss: 0.08813729882240295
train_iter_loss: 0.12863607704639435
train_iter_loss: 0.2357776015996933
train_iter_loss: 0.2035411298274994
train_iter_loss: 0.06523817777633667
train_iter_loss: 0.32038918137550354
train_iter_loss: 0.1738475263118744
train_iter_loss: 0.22696053981781006
train_iter_loss: 0.13763469457626343
train_iter_loss: 0.13831859827041626
train_iter_loss: 0.30018705129623413
train_iter_loss: 0.1617335081100464
train_iter_loss: 0.15579847991466522
train_iter_loss: 0.1273806244134903
train_iter_loss: 0.09219042956829071
train_iter_loss: 0.07903432846069336
train_iter_loss: 0.17426016926765442
train_iter_loss: 0.14301185309886932
train_iter_loss: 0.2348269820213318
train_iter_loss: 0.22171716392040253
train_iter_loss: 0.23518651723861694
train_iter_loss: 0.09460090845823288
train_iter_loss: 0.1505313515663147
train_iter_loss: 0.24697190523147583
train_iter_loss: 0.20080184936523438
train_iter_loss: 0.14336371421813965
train_iter_loss: 0.1749795824289322
train_iter_loss: 0.2024775743484497
train_iter_loss: 0.10742265731096268
train_iter_loss: 0.21333646774291992
train_iter_loss: 0.14183063805103302
train_iter_loss: 0.02417491003870964
train_iter_loss: 0.20234213769435883
train_iter_loss: 0.09480755031108856
train_iter_loss: 0.20466527342796326
train_iter_loss: 0.17540553212165833
train_iter_loss: 0.1849314421415329
train_iter_loss: 0.1823767125606537
train_iter_loss: 0.15506190061569214
train_iter_loss: 0.11750199645757675
train loss :0.1621
---------------------
Validation seg loss: 0.2190853080026946 at epoch 514
epoch =    515/  1000, exp = train
train_iter_loss: 0.13429342210292816
train_iter_loss: 0.1257757991552353
train_iter_loss: 0.09923332929611206
train_iter_loss: 0.10504887998104095
train_iter_loss: 0.0895521268248558
train_iter_loss: 0.07277009636163712
train_iter_loss: 0.2284700870513916
train_iter_loss: 0.1299678087234497
train_iter_loss: 0.1239129900932312
train_iter_loss: 0.1222534105181694
train_iter_loss: 0.2134074866771698
train_iter_loss: 0.2594148814678192
train_iter_loss: 0.11241815239191055
train_iter_loss: 0.09400715678930283
train_iter_loss: 0.20661456882953644
train_iter_loss: 0.13838322460651398
train_iter_loss: 0.11693402379751205
train_iter_loss: 0.1199088841676712
train_iter_loss: 0.060791015625
train_iter_loss: 0.1262703686952591
train_iter_loss: 0.17375397682189941
train_iter_loss: 0.2087736427783966
train_iter_loss: 0.13671068847179413
train_iter_loss: 0.11470051109790802
train_iter_loss: 0.08323992788791656
train_iter_loss: 0.17714646458625793
train_iter_loss: 0.1113721951842308
train_iter_loss: 0.1469997763633728
train_iter_loss: 0.07380984723567963
train_iter_loss: 0.1391712725162506
train_iter_loss: 0.17960333824157715
train_iter_loss: 0.1962355226278305
train_iter_loss: 0.218237042427063
train_iter_loss: 0.1370113492012024
train_iter_loss: 0.06980278342962265
train_iter_loss: 0.31617388129234314
train_iter_loss: 0.15116837620735168
train_iter_loss: 0.14654842019081116
train_iter_loss: 0.08802146464586258
train_iter_loss: 0.16893473267555237
train_iter_loss: 0.11881086230278015
train_iter_loss: 0.09105027467012405
train_iter_loss: 0.11479569226503372
train_iter_loss: 0.4385479688644409
train_iter_loss: 0.03672322258353233
train_iter_loss: 0.23292195796966553
train_iter_loss: 0.11983735114336014
train_iter_loss: 0.08984421193599701
train_iter_loss: 0.11930849403142929
train_iter_loss: 0.17397372424602509
train_iter_loss: 0.13995623588562012
train_iter_loss: 0.1600591242313385
train_iter_loss: 0.20138151943683624
train_iter_loss: 0.10001106560230255
train_iter_loss: 0.20893481373786926
train_iter_loss: 0.12309682369232178
train_iter_loss: 0.3146173357963562
train_iter_loss: 0.1864987164735794
train_iter_loss: 0.20008620619773865
train_iter_loss: 0.17215248942375183
train_iter_loss: 0.12779396772384644
train_iter_loss: 0.29015418887138367
train_iter_loss: 0.22003382444381714
train_iter_loss: 0.11147473752498627
train_iter_loss: 0.11167808622121811
train_iter_loss: 0.1481161117553711
train_iter_loss: 0.24741484224796295
train_iter_loss: 0.19722700119018555
train_iter_loss: 0.18529245257377625
train_iter_loss: 0.300982803106308
train_iter_loss: 0.19059748947620392
train_iter_loss: 0.14058278501033783
train_iter_loss: 0.2567523121833801
train_iter_loss: 0.17224068939685822
train_iter_loss: 0.11490598320960999
train_iter_loss: 0.08437885344028473
train_iter_loss: 0.1503300964832306
train_iter_loss: 0.12876157462596893
train_iter_loss: 0.04049978777766228
train_iter_loss: 0.1269581913948059
train_iter_loss: 0.15380002558231354
train_iter_loss: 0.14784541726112366
train_iter_loss: 0.226007878780365
train_iter_loss: 0.1261926293373108
train_iter_loss: 0.1179603785276413
train_iter_loss: 0.15941554307937622
train_iter_loss: 0.1768692284822464
train_iter_loss: 0.1382230967283249
train_iter_loss: 0.06747587770223618
train_iter_loss: 0.23691555857658386
train_iter_loss: 0.12302377074956894
train_iter_loss: 0.13658086955547333
train_iter_loss: 0.22847488522529602
train_iter_loss: 0.1267823576927185
train_iter_loss: 0.35121646523475647
train_iter_loss: 0.10414249449968338
train_iter_loss: 0.0953107625246048
train_iter_loss: 0.14889058470726013
train_iter_loss: 0.08520049601793289
train_iter_loss: 0.09692952036857605
train loss :0.1561
---------------------
Validation seg loss: 0.21932102773197978 at epoch 515
epoch =    516/  1000, exp = train
train_iter_loss: 0.17271895706653595
train_iter_loss: 0.06138639152050018
train_iter_loss: 0.10486128181219101
train_iter_loss: 0.29126468300819397
train_iter_loss: 0.1794881522655487
train_iter_loss: 0.11859501153230667
train_iter_loss: 0.26358649134635925
train_iter_loss: 0.10217888653278351
train_iter_loss: 0.23587635159492493
train_iter_loss: 0.14986376464366913
train_iter_loss: 0.22783441841602325
train_iter_loss: 0.11018797010183334
train_iter_loss: 0.1349845975637436
train_iter_loss: 0.14699101448059082
train_iter_loss: 0.13873198628425598
train_iter_loss: 0.12448359280824661
train_iter_loss: 0.06476560980081558
train_iter_loss: 0.18589985370635986
train_iter_loss: 0.19581228494644165
train_iter_loss: 0.13946105539798737
train_iter_loss: 0.08332765847444534
train_iter_loss: 0.14751671254634857
train_iter_loss: 0.14885510504245758
train_iter_loss: 0.22111181914806366
train_iter_loss: 0.10239945352077484
train_iter_loss: 0.14127995073795319
train_iter_loss: 0.10845553874969482
train_iter_loss: 0.09358356893062592
train_iter_loss: 0.3478740155696869
train_iter_loss: 0.09849222749471664
train_iter_loss: 0.11529839038848877
train_iter_loss: 0.1872347742319107
train_iter_loss: 0.174613356590271
train_iter_loss: 0.10444649308919907
train_iter_loss: 0.13714128732681274
train_iter_loss: 0.12211959064006805
train_iter_loss: 0.061572037637233734
train_iter_loss: 0.21271371841430664
train_iter_loss: 0.15168966352939606
train_iter_loss: 0.21074075996875763
train_iter_loss: 0.1821696013212204
train_iter_loss: 0.16845914721488953
train_iter_loss: 0.08960074931383133
train_iter_loss: 0.15018044412136078
train_iter_loss: 0.06958702206611633
train_iter_loss: 0.13392187654972076
train_iter_loss: 0.13816815614700317
train_iter_loss: 0.14366300404071808
train_iter_loss: 0.11230068653821945
train_iter_loss: 0.12817376852035522
train_iter_loss: 0.2518429756164551
train_iter_loss: 0.19508446753025055
train_iter_loss: 0.10389545559883118
train_iter_loss: 0.19301839172840118
train_iter_loss: 0.13825160264968872
train_iter_loss: 0.1346338987350464
train_iter_loss: 0.22267647087574005
train_iter_loss: 0.10113367438316345
train_iter_loss: 0.05764906108379364
train_iter_loss: 0.1676444113254547
train_iter_loss: 0.1418568640947342
train_iter_loss: 0.2064051777124405
train_iter_loss: 0.24788179993629456
train_iter_loss: 0.06812576204538345
train_iter_loss: 0.2515493929386139
train_iter_loss: 0.10261618345975876
train_iter_loss: 0.13838855922222137
train_iter_loss: 0.10563228279352188
train_iter_loss: 0.09228931367397308
train_iter_loss: 0.1263953000307083
train_iter_loss: 0.21604886651039124
train_iter_loss: 0.17929095029830933
train_iter_loss: 0.11371923983097076
train_iter_loss: 0.11665260791778564
train_iter_loss: 0.253741979598999
train_iter_loss: 0.17300264537334442
train_iter_loss: 0.06117861345410347
train_iter_loss: 0.13169969618320465
train_iter_loss: 0.14944356679916382
train_iter_loss: 0.06504286080598831
train_iter_loss: 0.1595640629529953
train_iter_loss: 0.15763279795646667
train_iter_loss: 0.20961081981658936
train_iter_loss: 0.17362719774246216
train_iter_loss: 0.22906970977783203
train_iter_loss: 0.10050828009843826
train_iter_loss: 0.09914044290781021
train_iter_loss: 0.10490240901708603
train_iter_loss: 0.1967884600162506
train_iter_loss: 0.2310071885585785
train_iter_loss: 0.0765342116355896
train_iter_loss: 0.211038276553154
train_iter_loss: 0.1373278945684433
train_iter_loss: 0.1402541548013687
train_iter_loss: 0.10447660833597183
train_iter_loss: 0.15475866198539734
train_iter_loss: 0.2925705015659332
train_iter_loss: 0.385851114988327
train_iter_loss: 0.18557870388031006
train_iter_loss: 0.1991492211818695
train loss :0.1565
---------------------
Validation seg loss: 0.21836061984792632 at epoch 516
epoch =    517/  1000, exp = train
train_iter_loss: 0.2192293107509613
train_iter_loss: 0.1077166348695755
train_iter_loss: 0.33262723684310913
train_iter_loss: 0.17376703023910522
train_iter_loss: 0.1010071262717247
train_iter_loss: 0.19458287954330444
train_iter_loss: 0.1828976571559906
train_iter_loss: 0.1218344047665596
train_iter_loss: 0.2479994148015976
train_iter_loss: 0.09648628532886505
train_iter_loss: 0.06771774590015411
train_iter_loss: 0.18827298283576965
train_iter_loss: 0.0886196568608284
train_iter_loss: 0.12278743088245392
train_iter_loss: 0.12874546647071838
train_iter_loss: 0.15914040803909302
train_iter_loss: 0.12825429439544678
train_iter_loss: 0.2365700900554657
train_iter_loss: 0.13743390142917633
train_iter_loss: 0.15202076733112335
train_iter_loss: 0.18787215650081635
train_iter_loss: 0.2234518975019455
train_iter_loss: 0.19755074381828308
train_iter_loss: 0.1555400937795639
train_iter_loss: 0.23948568105697632
train_iter_loss: 0.19057533144950867
train_iter_loss: 0.13545159995555878
train_iter_loss: 0.1940220594406128
train_iter_loss: 0.21701247990131378
train_iter_loss: 0.22632098197937012
train_iter_loss: 0.1025625616312027
train_iter_loss: 0.12580832839012146
train_iter_loss: 0.341500461101532
train_iter_loss: 0.14890800416469574
train_iter_loss: 0.10309892147779465
train_iter_loss: 0.09712500870227814
train_iter_loss: 0.1941220760345459
train_iter_loss: 0.04612400755286217
train_iter_loss: 0.1455240398645401
train_iter_loss: 0.2899107336997986
train_iter_loss: 0.2344089299440384
train_iter_loss: 0.1336391568183899
train_iter_loss: 0.15051227807998657
train_iter_loss: 0.07246142625808716
train_iter_loss: 0.15250076353549957
train_iter_loss: 0.08259488642215729
train_iter_loss: 0.19577129185199738
train_iter_loss: 0.08921615034341812
train_iter_loss: 0.17152510583400726
train_iter_loss: 0.1347094178199768
train_iter_loss: 0.1937832236289978
train_iter_loss: 0.1499357521533966
train_iter_loss: 0.1435975730419159
train_iter_loss: 0.19185371696949005
train_iter_loss: 0.10317318141460419
train_iter_loss: 0.15324552357196808
train_iter_loss: 0.037386152893304825
train_iter_loss: 0.04320630431175232
train_iter_loss: 0.21855203807353973
train_iter_loss: 0.10601817816495895
train_iter_loss: 0.14108531177043915
train_iter_loss: 0.25787755846977234
train_iter_loss: 0.16945332288742065
train_iter_loss: 0.3426872491836548
train_iter_loss: 0.16867877542972565
train_iter_loss: 0.16561876237392426
train_iter_loss: 0.13348591327667236
train_iter_loss: 0.20867350697517395
train_iter_loss: 0.17942988872528076
train_iter_loss: 0.0876297801733017
train_iter_loss: 0.13854746520519257
train_iter_loss: 0.11829226464033127
train_iter_loss: 0.06092728674411774
train_iter_loss: 0.22140635550022125
train_iter_loss: 0.17965593934059143
train_iter_loss: 0.14238931238651276
train_iter_loss: 0.19281317293643951
train_iter_loss: 0.12523908913135529
train_iter_loss: 0.10444022715091705
train_iter_loss: 0.12830498814582825
train_iter_loss: 0.28449127078056335
train_iter_loss: 0.14028829336166382
train_iter_loss: 0.21077321469783783
train_iter_loss: 0.2233206331729889
train_iter_loss: 0.10472939908504486
train_iter_loss: 0.13035736978054047
train_iter_loss: 0.2600115239620209
train_iter_loss: 0.1836029589176178
train_iter_loss: 0.1981838345527649
train_iter_loss: 0.2506551146507263
train_iter_loss: 0.14364182949066162
train_iter_loss: 0.17736229300498962
train_iter_loss: 0.0810202807188034
train_iter_loss: 0.19341646134853363
train_iter_loss: 0.13067851960659027
train_iter_loss: 0.03177683800458908
train_iter_loss: 0.0961662158370018
train_iter_loss: 0.12861940264701843
train_iter_loss: 0.07928641140460968
train_iter_loss: 0.17351683974266052
train loss :0.1615
---------------------
Validation seg loss: 0.22141448034957614 at epoch 517
epoch =    518/  1000, exp = train
train_iter_loss: 0.11319625377655029
train_iter_loss: 0.2037632167339325
train_iter_loss: 0.0683954581618309
train_iter_loss: 0.06830339133739471
train_iter_loss: 0.11850467324256897
train_iter_loss: 0.3343416750431061
train_iter_loss: 0.11402834206819534
train_iter_loss: 0.13477440178394318
train_iter_loss: 0.20518764853477478
train_iter_loss: 0.24437043070793152
train_iter_loss: 0.11368492245674133
train_iter_loss: 0.1310763657093048
train_iter_loss: 0.23518449068069458
train_iter_loss: 0.05141330137848854
train_iter_loss: 0.16279636323451996
train_iter_loss: 0.16675305366516113
train_iter_loss: 0.08429621160030365
train_iter_loss: 0.24414658546447754
train_iter_loss: 0.1661861538887024
train_iter_loss: 0.20781217515468597
train_iter_loss: 0.059950847178697586
train_iter_loss: 0.07493589073419571
train_iter_loss: 0.22039033472537994
train_iter_loss: 0.10083001106977463
train_iter_loss: 0.09438783675432205
train_iter_loss: 0.18489672243595123
train_iter_loss: 0.12334046512842178
train_iter_loss: 0.1411450356245041
train_iter_loss: 0.19455476105213165
train_iter_loss: 0.2120036631822586
train_iter_loss: 0.2270372360944748
train_iter_loss: 0.19467732310295105
train_iter_loss: 0.19976677000522614
train_iter_loss: 0.21892379224300385
train_iter_loss: 0.15371821820735931
train_iter_loss: 0.18119725584983826
train_iter_loss: 0.02464333362877369
train_iter_loss: 0.39848628640174866
train_iter_loss: 0.14999103546142578
train_iter_loss: 0.08643388748168945
train_iter_loss: 0.11449350416660309
train_iter_loss: 0.14697428047657013
train_iter_loss: 0.22663246095180511
train_iter_loss: 0.18672166764736176
train_iter_loss: 0.2027256190776825
train_iter_loss: 0.04820367321372032
train_iter_loss: 0.15193749964237213
train_iter_loss: 0.11848916113376617
train_iter_loss: 0.21002763509750366
train_iter_loss: 0.150745689868927
train_iter_loss: 0.09696370363235474
train_iter_loss: 0.20026662945747375
train_iter_loss: 0.12861990928649902
train_iter_loss: 0.22114449739456177
train_iter_loss: 0.10310819745063782
train_iter_loss: 0.2948835790157318
train_iter_loss: 0.18775594234466553
train_iter_loss: 0.12266048043966293
train_iter_loss: 0.1830298900604248
train_iter_loss: 0.19453245401382446
train_iter_loss: 0.11842209845781326
train_iter_loss: 0.09430832415819168
train_iter_loss: 0.10616881400346756
train_iter_loss: 0.2887917459011078
train_iter_loss: 0.17990364134311676
train_iter_loss: 0.11037711054086685
train_iter_loss: 0.18372997641563416
train_iter_loss: 0.1491020917892456
train_iter_loss: 0.13872778415679932
train_iter_loss: 0.10754422098398209
train_iter_loss: 0.1286894977092743
train_iter_loss: 0.11572854220867157
train_iter_loss: 0.16006559133529663
train_iter_loss: 0.18558640778064728
train_iter_loss: 0.08975395560264587
train_iter_loss: 0.12421643733978271
train_iter_loss: 0.1829237937927246
train_iter_loss: 0.24174173176288605
train_iter_loss: 0.1617133468389511
train_iter_loss: 0.20852826535701752
train_iter_loss: 0.13610219955444336
train_iter_loss: 0.1401934176683426
train_iter_loss: 0.16363444924354553
train_iter_loss: 0.057337429374456406
train_iter_loss: 0.22604505717754364
train_iter_loss: 0.17675244808197021
train_iter_loss: 0.07022756338119507
train_iter_loss: 0.09372514486312866
train_iter_loss: 0.09500639885663986
train_iter_loss: 0.14936566352844238
train_iter_loss: 0.18525566160678864
train_iter_loss: 0.05240399017930031
train_iter_loss: 0.14671437442302704
train_iter_loss: 0.25233975052833557
train_iter_loss: 0.1726934313774109
train_iter_loss: 0.13564848899841309
train_iter_loss: 0.1325015276670456
train_iter_loss: 0.10015692561864853
train_iter_loss: 0.4332682490348816
train_iter_loss: 0.11851608753204346
train loss :0.1597
---------------------
Validation seg loss: 0.21996384442625744 at epoch 518
epoch =    519/  1000, exp = train
train_iter_loss: 0.11157698184251785
train_iter_loss: 0.16706529259681702
train_iter_loss: 0.19553330540657043
train_iter_loss: 0.1725720912218094
train_iter_loss: 0.06941214203834534
train_iter_loss: 0.2875092327594757
train_iter_loss: 0.1475936770439148
train_iter_loss: 0.1723719835281372
train_iter_loss: 0.1371506303548813
train_iter_loss: 0.123475082218647
train_iter_loss: 0.28813526034355164
train_iter_loss: 0.1182669997215271
train_iter_loss: 0.12252786755561829
train_iter_loss: 0.16262856125831604
train_iter_loss: 0.15784628689289093
train_iter_loss: 0.0779949352145195
train_iter_loss: 0.05168972164392471
train_iter_loss: 0.07463160157203674
train_iter_loss: 0.1319405734539032
train_iter_loss: 0.22520093619823456
train_iter_loss: 0.14399124681949615
train_iter_loss: 0.08088821917772293
train_iter_loss: 0.20290404558181763
train_iter_loss: 0.18701885640621185
train_iter_loss: 0.305575966835022
train_iter_loss: 0.15965977311134338
train_iter_loss: 0.1345403790473938
train_iter_loss: 0.1310913860797882
train_iter_loss: 0.19891901314258575
train_iter_loss: 0.19177870452404022
train_iter_loss: 0.0901833102107048
train_iter_loss: 0.16983792185783386
train_iter_loss: 0.10702485591173172
train_iter_loss: 0.11880835145711899
train_iter_loss: 0.2345326840877533
train_iter_loss: 0.10419963300228119
train_iter_loss: 0.08908584713935852
train_iter_loss: 0.15966129302978516
train_iter_loss: 0.08388419449329376
train_iter_loss: 0.13807718455791473
train_iter_loss: 0.16060060262680054
train_iter_loss: 0.20867173373699188
train_iter_loss: 0.2078276127576828
train_iter_loss: 0.20431028306484222
train_iter_loss: 0.1543663740158081
train_iter_loss: 0.09202403575181961
train_iter_loss: 0.1520102471113205
train_iter_loss: 0.1460101157426834
train_iter_loss: 0.166046142578125
train_iter_loss: 0.1267426609992981
train_iter_loss: 0.11632447689771652
train_iter_loss: 0.27580177783966064
train_iter_loss: 0.10127559304237366
train_iter_loss: 0.17073123157024384
train_iter_loss: 0.1837153434753418
train_iter_loss: 0.1545761525630951
train_iter_loss: 0.16223126649856567
train_iter_loss: 0.17202258110046387
train_iter_loss: 0.06803340464830399
train_iter_loss: 0.18538813292980194
train_iter_loss: 0.19260144233703613
train_iter_loss: 0.16627077758312225
train_iter_loss: 0.14586669206619263
train_iter_loss: 0.18772153556346893
train_iter_loss: 0.0846727266907692
train_iter_loss: 0.13752077519893646
train_iter_loss: 0.22162142395973206
train_iter_loss: 0.14760254323482513
train_iter_loss: 0.21237508952617645
train_iter_loss: 0.16803927719593048
train_iter_loss: 0.12211325019598007
train_iter_loss: 0.17638832330703735
train_iter_loss: 0.10792160034179688
train_iter_loss: 0.30885180830955505
train_iter_loss: 0.16229230165481567
train_iter_loss: 0.10643362998962402
train_iter_loss: 0.13305294513702393
train_iter_loss: 0.24748899042606354
train_iter_loss: 0.25581789016723633
train_iter_loss: 0.12012045830488205
train_iter_loss: 0.17429083585739136
train_iter_loss: 0.18068642914295197
train_iter_loss: 0.17523755133152008
train_iter_loss: 0.1923493891954422
train_iter_loss: 0.24378570914268494
train_iter_loss: 0.2199595868587494
train_iter_loss: 0.11638952046632767
train_iter_loss: 0.09585507214069366
train_iter_loss: 0.27620044350624084
train_iter_loss: 0.1032116711139679
train_iter_loss: 0.1594950556755066
train_iter_loss: 0.08680848777294159
train_iter_loss: 0.03066219761967659
train_iter_loss: 0.15141034126281738
train_iter_loss: 0.19007137417793274
train_iter_loss: 0.20325005054473877
train_iter_loss: 0.12242498993873596
train_iter_loss: 0.1838165521621704
train_iter_loss: 0.16012892127037048
train_iter_loss: 0.08337776362895966
train loss :0.1595
---------------------
Validation seg loss: 0.21519721419778917 at epoch 519
epoch =    520/  1000, exp = train
train_iter_loss: 0.06648330390453339
train_iter_loss: 0.20104120671749115
train_iter_loss: 0.13375645875930786
train_iter_loss: 0.29804563522338867
train_iter_loss: 0.19520625472068787
train_iter_loss: 0.22145292162895203
train_iter_loss: 0.17671680450439453
train_iter_loss: 0.12739193439483643
train_iter_loss: 0.1088758036494255
train_iter_loss: 0.07491718977689743
train_iter_loss: 0.11980674415826797
train_iter_loss: 0.10437127947807312
train_iter_loss: 0.26278674602508545
train_iter_loss: 0.2741647958755493
train_iter_loss: 0.11323830485343933
train_iter_loss: 0.11185333132743835
train_iter_loss: 0.12833139300346375
train_iter_loss: 0.06406717002391815
train_iter_loss: 0.1750836968421936
train_iter_loss: 0.1029895767569542
train_iter_loss: 0.20093905925750732
train_iter_loss: 0.11282185465097427
train_iter_loss: 0.08732903003692627
train_iter_loss: 0.12987969815731049
train_iter_loss: 0.09616740047931671
train_iter_loss: 0.0837855413556099
train_iter_loss: 0.21242351830005646
train_iter_loss: 0.028812257573008537
train_iter_loss: 0.34268084168434143
train_iter_loss: 0.1009293645620346
train_iter_loss: 0.20049452781677246
train_iter_loss: 0.16562892496585846
train_iter_loss: 0.1239754855632782
train_iter_loss: 0.18314197659492493
train_iter_loss: 0.14030183851718903
train_iter_loss: 0.15080051124095917
train_iter_loss: 0.21317915618419647
train_iter_loss: 0.2024514526128769
train_iter_loss: 0.15871627628803253
train_iter_loss: 0.2604527473449707
train_iter_loss: 0.13226266205310822
train_iter_loss: 0.11981372535228729
train_iter_loss: 0.2009897083044052
train_iter_loss: 0.09554009884595871
train_iter_loss: 0.20094989240169525
train_iter_loss: 0.029118167236447334
train_iter_loss: 0.2256360948085785
train_iter_loss: 0.2856239378452301
train_iter_loss: 0.23634803295135498
train_iter_loss: 0.22143928706645966
train_iter_loss: 0.3261547386646271
train_iter_loss: 0.05600965395569801
train_iter_loss: 0.12586826086044312
train_iter_loss: 0.29320013523101807
train_iter_loss: 0.2141919732093811
train_iter_loss: 0.15540540218353271
train_iter_loss: 0.20442430675029755
train_iter_loss: 0.13051646947860718
train_iter_loss: 0.10382251441478729
train_iter_loss: 0.20052570104599
train_iter_loss: 0.19301553070545197
train_iter_loss: 0.09316878765821457
train_iter_loss: 0.10695909708738327
train_iter_loss: 0.39132970571517944
train_iter_loss: 0.08203873038291931
train_iter_loss: 0.16921685636043549
train_iter_loss: 0.1983458399772644
train_iter_loss: 0.2366209626197815
train_iter_loss: 0.16422629356384277
train_iter_loss: 0.13337676227092743
train_iter_loss: 0.196584090590477
train_iter_loss: 0.20119626820087433
train_iter_loss: 0.27882590889930725
train_iter_loss: 0.23652854561805725
train_iter_loss: 0.1217968538403511
train_iter_loss: 0.17189081013202667
train_iter_loss: 0.14707012474536896
train_iter_loss: 0.23026247322559357
train_iter_loss: 0.14436811208724976
train_iter_loss: 0.32640790939331055
train_iter_loss: 0.1617739200592041
train_iter_loss: 0.16262343525886536
train_iter_loss: 0.16339445114135742
train_iter_loss: 0.11901673674583435
train_iter_loss: 0.19217848777770996
train_iter_loss: 0.1777498722076416
train_iter_loss: 0.11833471059799194
train_iter_loss: 0.13772842288017273
train_iter_loss: 0.1142115592956543
train_iter_loss: 0.18018953502178192
train_iter_loss: 0.19422155618667603
train_iter_loss: 0.11221686750650406
train_iter_loss: 0.09661756455898285
train_iter_loss: 0.09752514958381653
train_iter_loss: 0.09459322690963745
train_iter_loss: 0.14031483232975006
train_iter_loss: 0.11520079523324966
train_iter_loss: 0.17150218784809113
train_iter_loss: 0.1354130655527115
train_iter_loss: 0.11177995800971985
train loss :0.1659
---------------------
Validation seg loss: 0.22041587923424985 at epoch 520
epoch =    521/  1000, exp = train
train_iter_loss: 0.16070862114429474
train_iter_loss: 0.1501280665397644
train_iter_loss: 0.10553939640522003
train_iter_loss: 0.1648721843957901
train_iter_loss: 0.21779194474220276
train_iter_loss: 0.19078627228736877
train_iter_loss: 0.10605357587337494
train_iter_loss: 0.12987786531448364
train_iter_loss: 0.14617598056793213
train_iter_loss: 0.1137513518333435
train_iter_loss: 0.1563229262828827
train_iter_loss: 0.09183117747306824
train_iter_loss: 0.15999577939510345
train_iter_loss: 0.13310115039348602
train_iter_loss: 0.10973895341157913
train_iter_loss: 0.11704794317483902
train_iter_loss: 0.19044561684131622
train_iter_loss: 0.09203995019197464
train_iter_loss: 0.3042909502983093
train_iter_loss: 0.09259974211454391
train_iter_loss: 0.3144713342189789
train_iter_loss: 0.1709955781698227
train_iter_loss: 0.12063862383365631
train_iter_loss: 0.16410814225673676
train_iter_loss: 0.18796072900295258
train_iter_loss: 0.14947888255119324
train_iter_loss: 0.09861549735069275
train_iter_loss: 0.10479623824357986
train_iter_loss: 0.1227286085486412
train_iter_loss: 0.243144690990448
train_iter_loss: 0.11798255890607834
train_iter_loss: 0.14120014011859894
train_iter_loss: 0.1713658720254898
train_iter_loss: 0.2972502112388611
train_iter_loss: 0.1415230631828308
train_iter_loss: 0.0795469805598259
train_iter_loss: 0.05132148787379265
train_iter_loss: 0.2149827778339386
train_iter_loss: 0.2060079723596573
train_iter_loss: 0.21645987033843994
train_iter_loss: 0.059065356850624084
train_iter_loss: 0.14241184294223785
train_iter_loss: 0.12736962735652924
train_iter_loss: 0.12489505857229233
train_iter_loss: 0.14056456089019775
train_iter_loss: 0.1366628110408783
train_iter_loss: 0.158885195851326
train_iter_loss: 0.2987820506095886
train_iter_loss: 0.162998229265213
train_iter_loss: 0.0971059799194336
train_iter_loss: 0.20170560479164124
train_iter_loss: 0.21749301254749298
train_iter_loss: 0.21214734017848969
train_iter_loss: 0.2699173390865326
train_iter_loss: 0.0910918340086937
train_iter_loss: 0.2134917974472046
train_iter_loss: 0.10228553414344788
train_iter_loss: 0.28474217653274536
train_iter_loss: 0.20214974880218506
train_iter_loss: 0.09817405790090561
train_iter_loss: 0.13398100435733795
train_iter_loss: 0.2201106995344162
train_iter_loss: 0.21680745482444763
train_iter_loss: 0.13834626972675323
train_iter_loss: 0.17066751420497894
train_iter_loss: 0.1381676197052002
train_iter_loss: 0.07361871004104614
train_iter_loss: 0.18132421374320984
train_iter_loss: 0.11777856945991516
train_iter_loss: 0.08268395066261292
train_iter_loss: 0.13587673008441925
train_iter_loss: 0.11508245766162872
train_iter_loss: 0.16035841405391693
train_iter_loss: 0.1120428591966629
train_iter_loss: 0.4584139585494995
train_iter_loss: 0.13194586336612701
train_iter_loss: 0.20971719920635223
train_iter_loss: 0.1716189980506897
train_iter_loss: 0.16005101799964905
train_iter_loss: 0.16068847477436066
train_iter_loss: 0.1917015165090561
train_iter_loss: 0.10028159618377686
train_iter_loss: 0.1889263093471527
train_iter_loss: 0.12784719467163086
train_iter_loss: 0.09420958906412125
train_iter_loss: 0.19098131358623505
train_iter_loss: 0.171653151512146
train_iter_loss: 0.10865861922502518
train_iter_loss: 0.1105148047208786
train_iter_loss: 0.20568612217903137
train_iter_loss: 0.27049046754837036
train_iter_loss: 0.2708592414855957
train_iter_loss: 0.01936827413737774
train_iter_loss: 0.13112999498844147
train_iter_loss: 0.08079780638217926
train_iter_loss: 0.22349540889263153
train_iter_loss: 0.14853091537952423
train_iter_loss: 0.1354152411222458
train_iter_loss: 0.25144705176353455
train_iter_loss: 0.1546214073896408
train loss :0.1622
---------------------
Validation seg loss: 0.22206397176245754 at epoch 521
epoch =    522/  1000, exp = train
train_iter_loss: 0.16047948598861694
train_iter_loss: 0.31712162494659424
train_iter_loss: 0.04399218410253525
train_iter_loss: 0.11271403729915619
train_iter_loss: 0.13194507360458374
train_iter_loss: 0.1606779396533966
train_iter_loss: 0.054846908897161484
train_iter_loss: 0.20636485517024994
train_iter_loss: 0.3287579119205475
train_iter_loss: 0.1830543726682663
train_iter_loss: 0.18291549384593964
train_iter_loss: 0.13174106180667877
train_iter_loss: 0.17772457003593445
train_iter_loss: 0.08481873571872711
train_iter_loss: 0.14671187102794647
train_iter_loss: 0.0996839627623558
train_iter_loss: 0.30239927768707275
train_iter_loss: 0.2318175733089447
train_iter_loss: 0.12425082921981812
train_iter_loss: 0.07008213549852371
train_iter_loss: 0.12659908831119537
train_iter_loss: 0.19927978515625
train_iter_loss: 0.07291578501462936
train_iter_loss: 0.1828135997056961
train_iter_loss: 0.18486227095127106
train_iter_loss: 0.2636885344982147
train_iter_loss: 0.1465930938720703
train_iter_loss: 0.2255372554063797
train_iter_loss: 0.1193716824054718
train_iter_loss: 0.12100803107023239
train_iter_loss: 0.17334116995334625
train_iter_loss: 0.23317338526248932
train_iter_loss: 0.11185278743505478
train_iter_loss: 0.13641317188739777
train_iter_loss: 0.23821713030338287
train_iter_loss: 0.2226668894290924
train_iter_loss: 0.17937102913856506
train_iter_loss: 0.12198487669229507
train_iter_loss: 0.23344124853610992
train_iter_loss: 0.15963627398014069
train_iter_loss: 0.13584120571613312
train_iter_loss: 0.12433318048715591
train_iter_loss: 0.10011949390172958
train_iter_loss: 0.22301553189754486
train_iter_loss: 0.09673599153757095
train_iter_loss: 0.1723085641860962
train_iter_loss: 0.2267460823059082
train_iter_loss: 0.03849771246314049
train_iter_loss: 0.19455492496490479
train_iter_loss: 0.18070895969867706
train_iter_loss: 0.23604916036128998
train_iter_loss: 0.16128015518188477
train_iter_loss: 0.12526541948318481
train_iter_loss: 0.21713989973068237
train_iter_loss: 0.13393905758857727
train_iter_loss: 0.21135832369327545
train_iter_loss: 0.1348802149295807
train_iter_loss: 0.1347261369228363
train_iter_loss: 0.18321800231933594
train_iter_loss: 0.20396485924720764
train_iter_loss: 0.1787126362323761
train_iter_loss: 0.15082447230815887
train_iter_loss: 0.15122514963150024
train_iter_loss: 0.08776018023490906
train_iter_loss: 0.19713594019412994
train_iter_loss: 0.17074565589427948
train_iter_loss: 0.152268186211586
train_iter_loss: 0.1446908414363861
train_iter_loss: 0.10441651940345764
train_iter_loss: 0.06622035801410675
train_iter_loss: 0.203338623046875
train_iter_loss: 0.13143104314804077
train_iter_loss: 0.1679101288318634
train_iter_loss: 0.03993496298789978
train_iter_loss: 0.16767755150794983
train_iter_loss: 0.11423873901367188
train_iter_loss: 0.12469106912612915
train_iter_loss: 0.15115654468536377
train_iter_loss: 0.11884989589452744
train_iter_loss: 0.15909068286418915
train_iter_loss: 0.09453900158405304
train_iter_loss: 0.1141318753361702
train_iter_loss: 0.12362135946750641
train_iter_loss: 0.11533554643392563
train_iter_loss: 0.198637917637825
train_iter_loss: 0.15056079626083374
train_iter_loss: 0.3267333209514618
train_iter_loss: 0.1659129112958908
train_iter_loss: 0.1620309054851532
train_iter_loss: 0.12781554460525513
train_iter_loss: 0.11688683927059174
train_iter_loss: 0.05048389360308647
train_iter_loss: 0.14820803701877594
train_iter_loss: 0.08123745769262314
train_iter_loss: 0.16978032886981964
train_iter_loss: 0.13744625449180603
train_iter_loss: 0.16098196804523468
train_iter_loss: 0.14050643146038055
train_iter_loss: 0.1171785220503807
train_iter_loss: 0.21877479553222656
train loss :0.1580
---------------------
Validation seg loss: 0.2196806120984959 at epoch 522
epoch =    523/  1000, exp = train
train_iter_loss: 0.10705818980932236
train_iter_loss: 0.16861610114574432
train_iter_loss: 0.139469176530838
train_iter_loss: 0.20768895745277405
train_iter_loss: 0.18985958397388458
train_iter_loss: 0.1411776840686798
train_iter_loss: 0.10149792581796646
train_iter_loss: 0.39892181754112244
train_iter_loss: 0.10443710535764694
train_iter_loss: 0.3263098895549774
train_iter_loss: 0.18887852132320404
train_iter_loss: 0.20680758357048035
train_iter_loss: 0.12455332279205322
train_iter_loss: 0.262962281703949
train_iter_loss: 0.12045278400182724
train_iter_loss: 0.20277076959609985
train_iter_loss: 0.04831169173121452
train_iter_loss: 0.26160764694213867
train_iter_loss: 0.051329415291547775
train_iter_loss: 0.15056023001670837
train_iter_loss: 0.13223251700401306
train_iter_loss: 0.12007524818181992
train_iter_loss: 0.2089526355266571
train_iter_loss: 0.15414080023765564
train_iter_loss: 0.0731072723865509
train_iter_loss: 0.22379958629608154
train_iter_loss: 0.24012957513332367
train_iter_loss: 0.07163328677415848
train_iter_loss: 0.07282844185829163
train_iter_loss: 0.14302398264408112
train_iter_loss: 0.1248512789607048
train_iter_loss: 0.1287938356399536
train_iter_loss: 0.21949754655361176
train_iter_loss: 0.12461450695991516
train_iter_loss: 0.07821623980998993
train_iter_loss: 0.1732800155878067
train_iter_loss: 0.21119065582752228
train_iter_loss: 0.1587209403514862
train_iter_loss: 0.1809627115726471
train_iter_loss: 0.3001820743083954
train_iter_loss: 0.09511712193489075
train_iter_loss: 0.19430260360240936
train_iter_loss: 0.20642215013504028
train_iter_loss: 0.1461484581232071
train_iter_loss: 0.11337054520845413
train_iter_loss: 0.1977594494819641
train_iter_loss: 0.2564844489097595
train_iter_loss: 0.14863221347332
train_iter_loss: 0.17962020635604858
train_iter_loss: 0.12097512185573578
train_iter_loss: 0.3044668734073639
train_iter_loss: 0.114485964179039
train_iter_loss: 0.12249905616044998
train_iter_loss: 0.10821279883384705
train_iter_loss: 0.2823593020439148
train_iter_loss: 0.12613753974437714
train_iter_loss: 0.13130992650985718
train_iter_loss: 0.0797247439622879
train_iter_loss: 0.24056419730186462
train_iter_loss: 0.08323410153388977
train_iter_loss: 0.19094839692115784
train_iter_loss: 0.06061796471476555
train_iter_loss: 0.09663863480091095
train_iter_loss: 0.1452087163925171
train_iter_loss: 0.1907321959733963
train_iter_loss: 0.09903107583522797
train_iter_loss: 0.09852872043848038
train_iter_loss: 0.24926318228244781
train_iter_loss: 0.11789018660783768
train_iter_loss: 0.17158985137939453
train_iter_loss: 0.11264435946941376
train_iter_loss: 0.156136155128479
train_iter_loss: 0.09060265123844147
train_iter_loss: 0.2383810132741928
train_iter_loss: 0.11477392166852951
train_iter_loss: 0.27640992403030396
train_iter_loss: 0.1445087492465973
train_iter_loss: 0.26912721991539
train_iter_loss: 0.18470732867717743
train_iter_loss: 0.14828172326087952
train_iter_loss: 0.12727653980255127
train_iter_loss: 0.1463978886604309
train_iter_loss: 0.13827918469905853
train_iter_loss: 0.1477866917848587
train_iter_loss: 0.11624155193567276
train_iter_loss: 0.1182696521282196
train_iter_loss: 0.11484310030937195
train_iter_loss: 0.2690168619155884
train_iter_loss: 0.11590753495693207
train_iter_loss: 0.19663450121879578
train_iter_loss: 0.1888107806444168
train_iter_loss: 0.028297144919633865
train_iter_loss: 0.13136181235313416
train_iter_loss: 0.0614180788397789
train_iter_loss: 0.15179654955863953
train_iter_loss: 0.21776004135608673
train_iter_loss: 0.14330700039863586
train_iter_loss: 0.1697995960712433
train_iter_loss: 0.17785301804542542
train_iter_loss: 0.17079900205135345
train loss :0.1614
---------------------
Validation seg loss: 0.21689334618946854 at epoch 523
epoch =    524/  1000, exp = train
train_iter_loss: 0.14254094660282135
train_iter_loss: 0.06298412382602692
train_iter_loss: 0.09897509217262268
train_iter_loss: 0.18468600511550903
train_iter_loss: 0.34419089555740356
train_iter_loss: 0.10752221941947937
train_iter_loss: 0.11764518916606903
train_iter_loss: 0.07746696472167969
train_iter_loss: 0.14129069447517395
train_iter_loss: 0.2036658078432083
train_iter_loss: 0.08742424100637436
train_iter_loss: 0.1540927141904831
train_iter_loss: 0.032859817147254944
train_iter_loss: 0.16201657056808472
train_iter_loss: 0.14294831454753876
train_iter_loss: 0.1405009627342224
train_iter_loss: 0.13765661418437958
train_iter_loss: 0.14703939855098724
train_iter_loss: 0.23300869762897491
train_iter_loss: 0.14813841879367828
train_iter_loss: 0.15062469244003296
train_iter_loss: 0.16632117331027985
train_iter_loss: 0.18398027122020721
train_iter_loss: 0.2844341993331909
train_iter_loss: 0.38790076971054077
train_iter_loss: 0.1449766606092453
train_iter_loss: 0.1196136623620987
train_iter_loss: 0.14243391156196594
train_iter_loss: 0.2236352413892746
train_iter_loss: 0.12146686017513275
train_iter_loss: 0.1341773271560669
train_iter_loss: 0.2311794012784958
train_iter_loss: 0.2445281744003296
train_iter_loss: 0.09640424698591232
train_iter_loss: 0.17952127754688263
train_iter_loss: 0.1053464412689209
train_iter_loss: 0.2086961716413498
train_iter_loss: 0.13711683452129364
train_iter_loss: 0.13471855223178864
train_iter_loss: 0.13351169228553772
train_iter_loss: 0.1602672040462494
train_iter_loss: 0.12692128121852875
train_iter_loss: 0.2169775366783142
train_iter_loss: 0.13067534565925598
train_iter_loss: 0.25786757469177246
train_iter_loss: 0.113758385181427
train_iter_loss: 0.14562712609767914
train_iter_loss: 0.12541060149669647
train_iter_loss: 0.17808088660240173
train_iter_loss: 0.14235147833824158
train_iter_loss: 0.1995154619216919
train_iter_loss: 0.21512821316719055
train_iter_loss: 0.1514219045639038
train_iter_loss: 0.1315067857503891
train_iter_loss: 0.17427939176559448
train_iter_loss: 0.16114623844623566
train_iter_loss: 0.17857514321804047
train_iter_loss: 0.1793297529220581
train_iter_loss: 0.12614570558071136
train_iter_loss: 0.062255099415779114
train_iter_loss: 0.24452126026153564
train_iter_loss: 0.1571664661169052
train_iter_loss: 0.11787953227758408
train_iter_loss: 0.1654541790485382
train_iter_loss: 0.1768801063299179
train_iter_loss: 0.16774742305278778
train_iter_loss: 0.06292629987001419
train_iter_loss: 0.1674913763999939
train_iter_loss: 0.17252589762210846
train_iter_loss: 0.20890337228775024
train_iter_loss: 0.16952893137931824
train_iter_loss: 0.3315560519695282
train_iter_loss: 0.21274161338806152
train_iter_loss: 0.1434919387102127
train_iter_loss: 0.17196913063526154
train_iter_loss: 0.06554360687732697
train_iter_loss: 0.12492644041776657
train_iter_loss: 0.20662719011306763
train_iter_loss: 0.17882294952869415
train_iter_loss: 0.20628629624843597
train_iter_loss: 0.0878041461110115
train_iter_loss: 0.08187469094991684
train_iter_loss: 0.2436700314283371
train_iter_loss: 0.04306085407733917
train_iter_loss: 0.10946634411811829
train_iter_loss: 0.11774858832359314
train_iter_loss: 0.1422657072544098
train_iter_loss: 0.18368840217590332
train_iter_loss: 0.16785667836666107
train_iter_loss: 0.1623450070619583
train_iter_loss: 0.07764657586812973
train_iter_loss: 0.19815951585769653
train_iter_loss: 0.19088080525398254
train_iter_loss: 0.06232612580060959
train_iter_loss: 0.14052315056324005
train_iter_loss: 0.2416197508573532
train_iter_loss: 0.06368263065814972
train_iter_loss: 0.13155588507652283
train_iter_loss: 0.16252240538597107
train_iter_loss: 0.3298300802707672
train loss :0.1613
---------------------
Validation seg loss: 0.22118889785565013 at epoch 524
epoch =    525/  1000, exp = train
train_iter_loss: 0.1995777189731598
train_iter_loss: 0.2719336748123169
train_iter_loss: 0.12917660176753998
train_iter_loss: 0.1258094757795334
train_iter_loss: 0.09277282655239105
train_iter_loss: 0.07670803368091583
train_iter_loss: 0.10862940549850464
train_iter_loss: 0.06409090012311935
train_iter_loss: 0.12775088846683502
train_iter_loss: 0.17713193595409393
train_iter_loss: 0.09558774530887604
train_iter_loss: 0.12074151635169983
train_iter_loss: 0.13164778053760529
train_iter_loss: 0.20391078293323517
train_iter_loss: 0.2402183711528778
train_iter_loss: 0.1615692675113678
train_iter_loss: 0.23252500593662262
train_iter_loss: 0.04637465253472328
train_iter_loss: 0.09956400096416473
train_iter_loss: 0.14818955957889557
train_iter_loss: 0.10844550281763077
train_iter_loss: 0.10808918625116348
train_iter_loss: 0.22513869404792786
train_iter_loss: 0.10297717154026031
train_iter_loss: 0.1087060421705246
train_iter_loss: 0.19923414289951324
train_iter_loss: 0.23170427978038788
train_iter_loss: 0.1464887112379074
train_iter_loss: 0.09318045526742935
train_iter_loss: 0.055192701518535614
train_iter_loss: 0.24433916807174683
train_iter_loss: 0.17838530242443085
train_iter_loss: 0.11816199123859406
train_iter_loss: 0.16134309768676758
train_iter_loss: 0.16086623072624207
train_iter_loss: 0.1387031078338623
train_iter_loss: 0.08909302204847336
train_iter_loss: 0.2290555089712143
train_iter_loss: 0.18529994785785675
train_iter_loss: 0.1121520921587944
train_iter_loss: 0.2055327147245407
train_iter_loss: 0.08740527182817459
train_iter_loss: 0.1615709811449051
train_iter_loss: 0.1293109804391861
train_iter_loss: 0.04426070675253868
train_iter_loss: 0.19888991117477417
train_iter_loss: 0.1245175302028656
train_iter_loss: 0.19329534471035004
train_iter_loss: 0.16498923301696777
train_iter_loss: 0.15359197556972504
train_iter_loss: 0.19966943562030792
train_iter_loss: 0.1522420346736908
train_iter_loss: 0.188657745718956
train_iter_loss: 0.12064522504806519
train_iter_loss: 0.15168283879756927
train_iter_loss: 0.295917272567749
train_iter_loss: 0.0769348219037056
train_iter_loss: 0.07246611267328262
train_iter_loss: 0.18512922525405884
train_iter_loss: 0.11631030589342117
train_iter_loss: 0.16220760345458984
train_iter_loss: 0.08679614961147308
train_iter_loss: 0.05401318147778511
train_iter_loss: 0.11541314423084259
train_iter_loss: 0.236323744058609
train_iter_loss: 0.2085699588060379
train_iter_loss: 0.1582687795162201
train_iter_loss: 0.20541994273662567
train_iter_loss: 0.17189587652683258
train_iter_loss: 0.08793529868125916
train_iter_loss: 0.16880838572978973
train_iter_loss: 0.08630827069282532
train_iter_loss: 0.1539791226387024
train_iter_loss: 0.15248550474643707
train_iter_loss: 0.3215157985687256
train_iter_loss: 0.14793267846107483
train_iter_loss: 0.13795429468154907
train_iter_loss: 0.0916697084903717
train_iter_loss: 0.13067084550857544
train_iter_loss: 0.1402236670255661
train_iter_loss: 0.13839416205883026
train_iter_loss: 0.1977301388978958
train_iter_loss: 0.12977653741836548
train_iter_loss: 0.17119218409061432
train_iter_loss: 0.22917668521404266
train_iter_loss: 0.30658048391342163
train_iter_loss: 0.136438250541687
train_iter_loss: 0.11748078465461731
train_iter_loss: 0.20646238327026367
train_iter_loss: 0.08846858143806458
train_iter_loss: 0.06664854288101196
train_iter_loss: 0.17959077656269073
train_iter_loss: 0.3165072202682495
train_iter_loss: 0.11363771557807922
train_iter_loss: 0.21970868110656738
train_iter_loss: 0.15770189464092255
train_iter_loss: 0.24044103920459747
train_iter_loss: 0.05815636366605759
train_iter_loss: 0.1334792673587799
train_iter_loss: 0.3466714322566986
train loss :0.1561
---------------------
Validation seg loss: 0.21846012015528274 at epoch 525
epoch =    526/  1000, exp = train
train_iter_loss: 0.17578576505184174
train_iter_loss: 0.21674379706382751
train_iter_loss: 0.27896130084991455
train_iter_loss: 0.23643852770328522
train_iter_loss: 0.16770394146442413
train_iter_loss: 0.15588760375976562
train_iter_loss: 0.13199935853481293
train_iter_loss: 0.12469280511140823
train_iter_loss: 0.04541919380426407
train_iter_loss: 0.10933077335357666
train_iter_loss: 0.14216473698616028
train_iter_loss: 0.18283800780773163
train_iter_loss: 0.08967765420675278
train_iter_loss: 0.1554715633392334
train_iter_loss: 0.0840492770075798
train_iter_loss: 0.11576379835605621
train_iter_loss: 0.11330392211675644
train_iter_loss: 0.16778314113616943
train_iter_loss: 0.07438529282808304
train_iter_loss: 0.19285133481025696
train_iter_loss: 0.16616900265216827
train_iter_loss: 0.14756126701831818
train_iter_loss: 0.10211845487356186
train_iter_loss: 0.1503172367811203
train_iter_loss: 0.14193688333034515
train_iter_loss: 0.1566491425037384
train_iter_loss: 0.1288679838180542
train_iter_loss: 0.06988760083913803
train_iter_loss: 0.10623502731323242
train_iter_loss: 0.23301003873348236
train_iter_loss: 0.15949402749538422
train_iter_loss: 0.19515460729599
train_iter_loss: 0.04684028774499893
train_iter_loss: 0.15821115672588348
train_iter_loss: 0.2876192629337311
train_iter_loss: 0.14843477308750153
train_iter_loss: 0.1893225908279419
train_iter_loss: 0.16546347737312317
train_iter_loss: 0.1018538549542427
train_iter_loss: 0.14755620062351227
train_iter_loss: 0.09794297069311142
train_iter_loss: 0.16716843843460083
train_iter_loss: 0.16763046383857727
train_iter_loss: 0.43332481384277344
train_iter_loss: 0.12337729334831238
train_iter_loss: 0.2175009399652481
train_iter_loss: 0.137171670794487
train_iter_loss: 0.2077799141407013
train_iter_loss: 0.24239027500152588
train_iter_loss: 0.17374826967716217
train_iter_loss: 0.13272838294506073
train_iter_loss: 0.18712681531906128
train_iter_loss: 0.10843733698129654
train_iter_loss: 0.1952715814113617
train_iter_loss: 0.2744603455066681
train_iter_loss: 0.3350503742694855
train_iter_loss: 0.12732377648353577
train_iter_loss: 0.1283867508172989
train_iter_loss: 0.20485451817512512
train_iter_loss: 0.17546992003917694
train_iter_loss: 0.13927976787090302
train_iter_loss: 0.1107034757733345
train_iter_loss: 0.1616787165403366
train_iter_loss: 0.09683901071548462
train_iter_loss: 0.07531367987394333
train_iter_loss: 0.3044547140598297
train_iter_loss: 0.25308242440223694
train_iter_loss: 0.06346224993467331
train_iter_loss: 0.23482239246368408
train_iter_loss: 0.2897448241710663
train_iter_loss: 0.08994840830564499
train_iter_loss: 0.12667247653007507
train_iter_loss: 0.20098921656608582
train_iter_loss: 0.21156004071235657
train_iter_loss: 0.2559913396835327
train_iter_loss: 0.2708878815174103
train_iter_loss: 0.12249834835529327
train_iter_loss: 0.12648482620716095
train_iter_loss: 0.1495511531829834
train_iter_loss: 0.06189630180597305
train_iter_loss: 0.13003574311733246
train_iter_loss: 0.15063786506652832
train_iter_loss: 0.06051676347851753
train_iter_loss: 0.19894129037857056
train_iter_loss: 0.06182824820280075
train_iter_loss: 0.1760398894548416
train_iter_loss: 0.12055812776088715
train_iter_loss: 0.2341987043619156
train_iter_loss: 0.14355014264583588
train_iter_loss: 0.10469596087932587
train_iter_loss: 0.1983160376548767
train_iter_loss: 0.17585884034633636
train_iter_loss: 0.12218170613050461
train_iter_loss: 0.12548300623893738
train_iter_loss: 0.21947628259658813
train_iter_loss: 0.16618028283119202
train_iter_loss: 0.13825151324272156
train_iter_loss: 0.14119258522987366
train_iter_loss: 0.15285466611385345
train_iter_loss: 0.11195967346429825
train loss :0.1624
---------------------
Validation seg loss: 0.2131287689619469 at epoch 526
********************
best_val_epoch_loss:  0.2131287689619469
MODEL UPDATED
epoch =    527/  1000, exp = train
train_iter_loss: 0.19561614096164703
train_iter_loss: 0.13241271674633026
train_iter_loss: 0.22836622595787048
train_iter_loss: 0.08263728767633438
train_iter_loss: 0.17320947349071503
train_iter_loss: 0.11378378421068192
train_iter_loss: 0.09239731729030609
train_iter_loss: 0.10080713033676147
train_iter_loss: 0.1212858334183693
train_iter_loss: 0.1727866530418396
train_iter_loss: 0.13142700493335724
train_iter_loss: 0.14650791883468628
train_iter_loss: 0.11458582431077957
train_iter_loss: 0.12188316136598587
train_iter_loss: 0.16423054039478302
train_iter_loss: 0.20476612448692322
train_iter_loss: 0.09622703492641449
train_iter_loss: 0.14362873136997223
train_iter_loss: 0.08041597902774811
train_iter_loss: 0.1757112443447113
train_iter_loss: 0.09766478836536407
train_iter_loss: 0.16689139604568481
train_iter_loss: 0.10877066105604172
train_iter_loss: 0.05573061481118202
train_iter_loss: 0.1592187136411667
train_iter_loss: 0.2146756500005722
train_iter_loss: 0.22810256481170654
train_iter_loss: 0.27343717217445374
train_iter_loss: 0.2732144594192505
train_iter_loss: 0.16609370708465576
train_iter_loss: 0.1069183200597763
train_iter_loss: 0.1725330352783203
train_iter_loss: 0.1074836328625679
train_iter_loss: 0.2899048626422882
train_iter_loss: 0.22165219485759735
train_iter_loss: 0.1654282510280609
train_iter_loss: 0.16418437659740448
train_iter_loss: 0.18715092539787292
train_iter_loss: 0.12393259257078171
train_iter_loss: 0.11865166574716568
train_iter_loss: 0.16584163904190063
train_iter_loss: 0.12116758525371552
train_iter_loss: 0.08430171012878418
train_iter_loss: 0.14985592663288116
train_iter_loss: 0.279947966337204
train_iter_loss: 0.24724115431308746
train_iter_loss: 0.08772744238376617
train_iter_loss: 0.14025244116783142
train_iter_loss: 0.182122141122818
train_iter_loss: 0.07718037068843842
train_iter_loss: 0.24796248972415924
train_iter_loss: 0.1769760251045227
train_iter_loss: 0.13555626571178436
train_iter_loss: 0.18709450960159302
train_iter_loss: 0.16651037335395813
train_iter_loss: 0.22416715323925018
train_iter_loss: 0.18719501793384552
train_iter_loss: 0.20945002138614655
train_iter_loss: 0.11777296662330627
train_iter_loss: 0.1779015064239502
train_iter_loss: 0.07355216890573502
train_iter_loss: 0.12291843444108963
train_iter_loss: 0.15463514626026154
train_iter_loss: 0.07720175385475159
train_iter_loss: 0.13920097053050995
train_iter_loss: 0.20739763975143433
train_iter_loss: 0.1742536723613739
train_iter_loss: 0.20903734862804413
train_iter_loss: 0.15962859988212585
train_iter_loss: 0.09655115008354187
train_iter_loss: 0.20254375040531158
train_iter_loss: 0.1478472352027893
train_iter_loss: 0.23356151580810547
train_iter_loss: 0.23955096304416656
train_iter_loss: 0.08937709033489227
train_iter_loss: 0.2039080411195755
train_iter_loss: 0.2495180070400238
train_iter_loss: 0.12588097155094147
train_iter_loss: 0.17981955409049988
train_iter_loss: 0.2148497849702835
train_iter_loss: 0.22206427156925201
train_iter_loss: 0.29562899470329285
train_iter_loss: 0.17676319181919098
train_iter_loss: 0.10893198847770691
train_iter_loss: 0.15378187596797943
train_iter_loss: 0.2038334608078003
train_iter_loss: 0.18692883849143982
train_iter_loss: 0.09178008884191513
train_iter_loss: 0.22233946621418
train_iter_loss: 0.1409619152545929
train_iter_loss: 0.12948262691497803
train_iter_loss: 0.15199074149131775
train_iter_loss: 0.15058322250843048
train_iter_loss: 0.19098779559135437
train_iter_loss: 0.2244216352701187
train_iter_loss: 0.3138048052787781
train_iter_loss: 0.19933386147022247
train_iter_loss: 0.16406697034835815
train_iter_loss: 0.1595672369003296
train_iter_loss: 0.10993343591690063
train loss :0.1662
---------------------
Validation seg loss: 0.22045303691389426 at epoch 527
epoch =    528/  1000, exp = train
train_iter_loss: 0.11162269115447998
train_iter_loss: 0.1403251737356186
train_iter_loss: 0.18630331754684448
train_iter_loss: 0.23387038707733154
train_iter_loss: 0.13720877468585968
train_iter_loss: 0.1392417848110199
train_iter_loss: 0.11817258596420288
train_iter_loss: 0.199702188372612
train_iter_loss: 0.1348324418067932
train_iter_loss: 0.11528617888689041
train_iter_loss: 0.26261377334594727
train_iter_loss: 0.08978857845067978
train_iter_loss: 0.1967926323413849
train_iter_loss: 0.10752196609973907
train_iter_loss: 0.2377217561006546
train_iter_loss: 0.15908005833625793
train_iter_loss: 0.22583290934562683
train_iter_loss: 0.14392530918121338
train_iter_loss: 0.10626526921987534
train_iter_loss: 0.14257623255252838
train_iter_loss: 0.13268683850765228
train_iter_loss: 0.2867445647716522
train_iter_loss: 0.1073906272649765
train_iter_loss: 0.0464479885995388
train_iter_loss: 0.18983526527881622
train_iter_loss: 0.21916261315345764
train_iter_loss: 0.19204308092594147
train_iter_loss: 0.08797027915716171
train_iter_loss: 0.17312783002853394
train_iter_loss: 0.14309373497962952
train_iter_loss: 0.14373183250427246
train_iter_loss: 0.1447446197271347
train_iter_loss: 0.22442513704299927
train_iter_loss: 0.21086177229881287
train_iter_loss: 0.07587778568267822
train_iter_loss: 0.09522028267383575
train_iter_loss: 0.08205348253250122
train_iter_loss: 0.11612538993358612
train_iter_loss: 0.1860780119895935
train_iter_loss: 0.10087624192237854
train_iter_loss: 0.23752279579639435
train_iter_loss: 0.20040200650691986
train_iter_loss: 0.08452162891626358
train_iter_loss: 0.1133100688457489
train_iter_loss: 0.10676436871290207
train_iter_loss: 0.08451028168201447
train_iter_loss: 0.19740580022335052
train_iter_loss: 0.17127875983715057
train_iter_loss: 0.033662348985672
train_iter_loss: 0.1509483903646469
train_iter_loss: 0.09620156139135361
train_iter_loss: 0.22526751458644867
train_iter_loss: 0.13174477219581604
train_iter_loss: 0.17801423370838165
train_iter_loss: 0.10920677334070206
train_iter_loss: 0.15040403604507446
train_iter_loss: 0.3566671311855316
train_iter_loss: 0.2631574273109436
train_iter_loss: 0.15419191122055054
train_iter_loss: 0.2079361528158188
train_iter_loss: 0.17264394462108612
train_iter_loss: 0.07265803217887878
train_iter_loss: 0.1325186789035797
train_iter_loss: 0.1456875056028366
train_iter_loss: 0.17930467426776886
train_iter_loss: 0.11212293058633804
train_iter_loss: 0.22097940742969513
train_iter_loss: 0.10616673529148102
train_iter_loss: 0.15902908146381378
train_iter_loss: 0.12127029895782471
train_iter_loss: 0.14081253111362457
train_iter_loss: 0.3839534819126129
train_iter_loss: 0.14224493503570557
train_iter_loss: 0.1322125792503357
train_iter_loss: 0.20024338364601135
train_iter_loss: 0.06609033793210983
train_iter_loss: 0.14870956540107727
train_iter_loss: 0.14777930080890656
train_iter_loss: 0.28056272864341736
train_iter_loss: 0.11992976069450378
train_iter_loss: 0.33705613017082214
train_iter_loss: 0.12532980740070343
train_iter_loss: 0.19835294783115387
train_iter_loss: 0.1612709015607834
train_iter_loss: 0.11229562014341354
train_iter_loss: 0.05789855867624283
train_iter_loss: 0.10779949277639389
train_iter_loss: 0.292270302772522
train_iter_loss: 0.229924738407135
train_iter_loss: 0.10042978078126907
train_iter_loss: 0.19457320868968964
train_iter_loss: 0.18505367636680603
train_iter_loss: 0.2879757881164551
train_iter_loss: 0.1310446709394455
train_iter_loss: 0.26372215151786804
train_iter_loss: 0.1479203701019287
train_iter_loss: 0.11141134053468704
train_iter_loss: 0.08145544677972794
train_iter_loss: 0.13921530544757843
train_iter_loss: 0.14462783932685852
train loss :0.1616
---------------------
Validation seg loss: 0.2178426117741696 at epoch 528
epoch =    529/  1000, exp = train
train_iter_loss: 0.0446101650595665
train_iter_loss: 0.11742834001779556
train_iter_loss: 0.10155998170375824
train_iter_loss: 0.05167793855071068
train_iter_loss: 0.14048124849796295
train_iter_loss: 0.16637413203716278
train_iter_loss: 0.13364745676517487
train_iter_loss: 0.22930169105529785
train_iter_loss: 0.11568799614906311
train_iter_loss: 0.0959497019648552
train_iter_loss: 0.11916173994541168
train_iter_loss: 0.11934579163789749
train_iter_loss: 0.2709004878997803
train_iter_loss: 0.17630520462989807
train_iter_loss: 0.14586172997951508
train_iter_loss: 0.1955261528491974
train_iter_loss: 0.1273806244134903
train_iter_loss: 0.16723506152629852
train_iter_loss: 0.137350395321846
train_iter_loss: 0.11023009568452835
train_iter_loss: 0.15223120152950287
train_iter_loss: 0.20893239974975586
train_iter_loss: 0.1432083249092102
train_iter_loss: 0.18125657737255096
train_iter_loss: 0.1284918487071991
train_iter_loss: 0.0545157752931118
train_iter_loss: 0.12130265682935715
train_iter_loss: 0.194520503282547
train_iter_loss: 0.1908816248178482
train_iter_loss: 0.11380138248205185
train_iter_loss: 0.11991816014051437
train_iter_loss: 0.11091332882642746
train_iter_loss: 0.21641993522644043
train_iter_loss: 0.07895099371671677
train_iter_loss: 0.14671190083026886
train_iter_loss: 0.1658054143190384
train_iter_loss: 0.1772422194480896
train_iter_loss: 0.13943704962730408
train_iter_loss: 0.14934474229812622
train_iter_loss: 0.12928757071495056
train_iter_loss: 0.24664825201034546
train_iter_loss: 0.23058703541755676
train_iter_loss: 0.17699778079986572
train_iter_loss: 0.04142642021179199
train_iter_loss: 0.2269943356513977
train_iter_loss: 0.049010682851076126
train_iter_loss: 0.09146641194820404
train_iter_loss: 0.20401689410209656
train_iter_loss: 0.18373407423496246
train_iter_loss: 0.12391874194145203
train_iter_loss: 0.08949296176433563
train_iter_loss: 0.185958132147789
train_iter_loss: 0.2741551995277405
train_iter_loss: 0.11816976964473724
train_iter_loss: 0.19315922260284424
train_iter_loss: 0.14927929639816284
train_iter_loss: 0.25627803802490234
train_iter_loss: 0.13836483657360077
train_iter_loss: 0.10069181770086288
train_iter_loss: 0.18346332013607025
train_iter_loss: 0.16090954840183258
train_iter_loss: 0.2795400321483612
train_iter_loss: 0.10722902417182922
train_iter_loss: 0.17795532941818237
train_iter_loss: 0.18424150347709656
train_iter_loss: 0.1312430500984192
train_iter_loss: 0.16476042568683624
train_iter_loss: 0.14405305683612823
train_iter_loss: 0.26471418142318726
train_iter_loss: 0.1752728819847107
train_iter_loss: 0.1989339143037796
train_iter_loss: 0.15346790850162506
train_iter_loss: 0.2445138841867447
train_iter_loss: 0.1689213067293167
train_iter_loss: 0.05208528786897659
train_iter_loss: 0.21043334901332855
train_iter_loss: 0.11834000796079636
train_iter_loss: 0.265982449054718
train_iter_loss: 0.1177305355668068
train_iter_loss: 0.12780092656612396
train_iter_loss: 0.10626872628927231
train_iter_loss: 0.30729416012763977
train_iter_loss: 0.11175615340471268
train_iter_loss: 0.3577601909637451
train_iter_loss: 0.07873447984457016
train_iter_loss: 0.07896524667739868
train_iter_loss: 0.2235434651374817
train_iter_loss: 0.18725839257240295
train_iter_loss: 0.12745526432991028
train_iter_loss: 0.11943364143371582
train_iter_loss: 0.1077229380607605
train_iter_loss: 0.025334825739264488
train_iter_loss: 0.2096031904220581
train_iter_loss: 0.25115087628364563
train_iter_loss: 0.13340426981449127
train_iter_loss: 0.12119899690151215
train_iter_loss: 0.11376834660768509
train_iter_loss: 0.15261510014533997
train_iter_loss: 0.14915505051612854
train_iter_loss: 0.2145884782075882
train loss :0.1574
---------------------
Validation seg loss: 0.2189240307670157 at epoch 529
epoch =    530/  1000, exp = train
train_iter_loss: 0.18923428654670715
train_iter_loss: 0.4177490174770355
train_iter_loss: 0.11647262424230576
train_iter_loss: 0.36605411767959595
train_iter_loss: 0.05976332351565361
train_iter_loss: 0.15129682421684265
train_iter_loss: 0.1627385914325714
train_iter_loss: 0.304400235414505
train_iter_loss: 0.1404705047607422
train_iter_loss: 0.14118064939975739
train_iter_loss: 0.15377411246299744
train_iter_loss: 0.222099170088768
train_iter_loss: 0.18359467387199402
train_iter_loss: 0.1594434231519699
train_iter_loss: 0.16633006930351257
train_iter_loss: 0.2101830542087555
train_iter_loss: 0.04757353663444519
train_iter_loss: 0.13219177722930908
train_iter_loss: 0.18252068758010864
train_iter_loss: 0.053002212196588516
train_iter_loss: 0.02376108430325985
train_iter_loss: 0.14990116655826569
train_iter_loss: 0.1621963381767273
train_iter_loss: 0.14277221262454987
train_iter_loss: 0.1430124044418335
train_iter_loss: 0.184214785695076
train_iter_loss: 0.14517848193645477
train_iter_loss: 0.09072824567556381
train_iter_loss: 0.12809987366199493
train_iter_loss: 0.22213341295719147
train_iter_loss: 0.15786053240299225
train_iter_loss: 0.11149142682552338
train_iter_loss: 0.20392358303070068
train_iter_loss: 0.09948478639125824
train_iter_loss: 0.1956111490726471
train_iter_loss: 0.17222124338150024
train_iter_loss: 0.15879084169864655
train_iter_loss: 0.10571027547121048
train_iter_loss: 0.21225568652153015
train_iter_loss: 0.14496879279613495
train_iter_loss: 0.12988348305225372
train_iter_loss: 0.09915460646152496
train_iter_loss: 0.1440860480070114
train_iter_loss: 0.25451111793518066
train_iter_loss: 0.128687784075737
train_iter_loss: 0.1663334220647812
train_iter_loss: 0.045245178043842316
train_iter_loss: 0.17913353443145752
train_iter_loss: 0.15658771991729736
train_iter_loss: 0.09917586296796799
train_iter_loss: 0.0878126248717308
train_iter_loss: 0.12227770686149597
train_iter_loss: 0.18774549663066864
train_iter_loss: 0.12787136435508728
train_iter_loss: 0.18425796926021576
train_iter_loss: 0.20111878216266632
train_iter_loss: 0.18564526736736298
train_iter_loss: 0.18100059032440186
train_iter_loss: 0.190597802400589
train_iter_loss: 0.2334170639514923
train_iter_loss: 0.32070276141166687
train_iter_loss: 0.1954532414674759
train_iter_loss: 0.15546882152557373
train_iter_loss: 0.22285857796669006
train_iter_loss: 0.16084988415241241
train_iter_loss: 0.20794323086738586
train_iter_loss: 0.19860386848449707
train_iter_loss: 0.1300152987241745
train_iter_loss: 0.09200011938810349
train_iter_loss: 0.13759630918502808
train_iter_loss: 0.23385190963745117
train_iter_loss: 0.22717571258544922
train_iter_loss: 0.1503269523382187
train_iter_loss: 0.12870068848133087
train_iter_loss: 0.14960193634033203
train_iter_loss: 0.27761712670326233
train_iter_loss: 0.13711319863796234
train_iter_loss: 0.13985757529735565
train_iter_loss: 0.14664584398269653
train_iter_loss: 0.13063973188400269
train_iter_loss: 0.079987071454525
train_iter_loss: 0.17644894123077393
train_iter_loss: 0.09317780286073685
train_iter_loss: 0.21654199063777924
train_iter_loss: 0.16225454211235046
train_iter_loss: 0.1586626023054123
train_iter_loss: 0.2770645320415497
train_iter_loss: 0.06633986532688141
train_iter_loss: 0.24636085331439972
train_iter_loss: 0.09502363204956055
train_iter_loss: 0.17785926163196564
train_iter_loss: 0.17229501903057098
train_iter_loss: 0.14765022695064545
train_iter_loss: 0.09051832556724548
train_iter_loss: 0.2960214614868164
train_iter_loss: 0.14111247658729553
train_iter_loss: 0.16688284277915955
train_iter_loss: 0.13546912372112274
train_iter_loss: 0.1957714557647705
train_iter_loss: 0.12007828801870346
train loss :0.1653
---------------------
Validation seg loss: 0.2196149411347677 at epoch 530
epoch =    531/  1000, exp = train
train_iter_loss: 0.12703368067741394
train_iter_loss: 0.1177777647972107
train_iter_loss: 0.1290932148694992
train_iter_loss: 0.1168394386768341
train_iter_loss: 0.17991241812705994
train_iter_loss: 0.1397787630558014
train_iter_loss: 0.16659490764141083
train_iter_loss: 0.08418746292591095
train_iter_loss: 0.15624693036079407
train_iter_loss: 0.1558721959590912
train_iter_loss: 0.19621369242668152
train_iter_loss: 0.20384526252746582
train_iter_loss: 0.10789268463850021
train_iter_loss: 0.34208253026008606
train_iter_loss: 0.1655881255865097
train_iter_loss: 0.20046135783195496
train_iter_loss: 0.21431836485862732
train_iter_loss: 0.06768829375505447
train_iter_loss: 0.2507792115211487
train_iter_loss: 0.11527151614427567
train_iter_loss: 0.22260472178459167
train_iter_loss: 0.1477361023426056
train_iter_loss: 0.1812046468257904
train_iter_loss: 0.07209258526563644
train_iter_loss: 0.16460688412189484
train_iter_loss: 0.137451633810997
train_iter_loss: 0.15899333357810974
train_iter_loss: 0.20250342786312103
train_iter_loss: 0.13132674992084503
train_iter_loss: 0.10099060833454132
train_iter_loss: 0.16357819736003876
train_iter_loss: 0.05392453446984291
train_iter_loss: 0.12261439859867096
train_iter_loss: 0.11877204477787018
train_iter_loss: 0.15275338292121887
train_iter_loss: 0.12335464358329773
train_iter_loss: 0.26133057475090027
train_iter_loss: 0.19653040170669556
train_iter_loss: 0.1762503832578659
train_iter_loss: 0.2415020912885666
train_iter_loss: 0.1460946649312973
train_iter_loss: 0.13846129179000854
train_iter_loss: 0.11021251231431961
train_iter_loss: 0.28533652424812317
train_iter_loss: 0.17783118784427643
train_iter_loss: 0.1508188545703888
train_iter_loss: 0.2879645824432373
train_iter_loss: 0.10076454281806946
train_iter_loss: 0.20727068185806274
train_iter_loss: 0.24713388085365295
train_iter_loss: 0.11184195429086685
train_iter_loss: 0.15902377665042877
train_iter_loss: 0.1480599343776703
train_iter_loss: 0.1368224024772644
train_iter_loss: 0.14375601708889008
train_iter_loss: 0.07573261111974716
train_iter_loss: 0.06386353820562363
train_iter_loss: 0.16839705407619476
train_iter_loss: 0.1641201227903366
train_iter_loss: 0.11514665186405182
train_iter_loss: 0.153238445520401
train_iter_loss: 0.12519976496696472
train_iter_loss: 0.10743121802806854
train_iter_loss: 0.27870362997055054
train_iter_loss: 0.159900963306427
train_iter_loss: 0.17263948917388916
train_iter_loss: 0.1375817507505417
train_iter_loss: 0.15356841683387756
train_iter_loss: 0.12554149329662323
train_iter_loss: 0.07482723891735077
train_iter_loss: 0.08670595288276672
train_iter_loss: 0.21817074716091156
train_iter_loss: 0.11094462126493454
train_iter_loss: 0.17476226389408112
train_iter_loss: 0.10231476277112961
train_iter_loss: 0.11739663779735565
train_iter_loss: 0.14063149690628052
train_iter_loss: 0.15894317626953125
train_iter_loss: 0.16682997345924377
train_iter_loss: 0.15203596651554108
train_iter_loss: 0.15557913482189178
train_iter_loss: 0.17945867776870728
train_iter_loss: 0.08657338470220566
train_iter_loss: 0.14856883883476257
train_iter_loss: 0.25749334692955017
train_iter_loss: 0.1362931728363037
train_iter_loss: 0.15718352794647217
train_iter_loss: 0.1574520319700241
train_iter_loss: 0.12033451348543167
train_iter_loss: 0.2688596248626709
train_iter_loss: 0.11498745530843735
train_iter_loss: 0.24706710875034332
train_iter_loss: 0.14862054586410522
train_iter_loss: 0.18928338587284088
train_iter_loss: 0.09412996470928192
train_iter_loss: 0.30247023701667786
train_iter_loss: 0.21621081233024597
train_iter_loss: 0.2465447038412094
train_iter_loss: 0.18794524669647217
train_iter_loss: 0.24066577851772308
train loss :0.1624
---------------------
Validation seg loss: 0.22559839576693638 at epoch 531
epoch =    532/  1000, exp = train
train_iter_loss: 0.1837611198425293
train_iter_loss: 0.18272776901721954
train_iter_loss: 0.08164191246032715
train_iter_loss: 0.18071311712265015
train_iter_loss: 0.1158486008644104
train_iter_loss: 0.26598992943763733
train_iter_loss: 0.08315502852201462
train_iter_loss: 0.16036652028560638
train_iter_loss: 0.13578049838542938
train_iter_loss: 0.12818358838558197
train_iter_loss: 0.18459732830524445
train_iter_loss: 0.17951662838459015
train_iter_loss: 0.09116214513778687
train_iter_loss: 0.14462588727474213
train_iter_loss: 0.23663683235645294
train_iter_loss: 0.08946611732244492
train_iter_loss: 0.05396248400211334
train_iter_loss: 0.07180295884609222
train_iter_loss: 0.17286299169063568
train_iter_loss: 0.17362354695796967
train_iter_loss: 0.18256539106369019
train_iter_loss: 0.22389286756515503
train_iter_loss: 0.21795125305652618
train_iter_loss: 0.11987743526697159
train_iter_loss: 0.20581191778182983
train_iter_loss: 0.16235728561878204
train_iter_loss: 0.12231556326150894
train_iter_loss: 0.10070271044969559
train_iter_loss: 0.12243650108575821
train_iter_loss: 0.17533190548419952
train_iter_loss: 0.17923861742019653
train_iter_loss: 0.12600862979888916
train_iter_loss: 0.2656925320625305
train_iter_loss: 0.2268841713666916
train_iter_loss: 0.10831501334905624
train_iter_loss: 0.14194059371948242
train_iter_loss: 0.0790809616446495
train_iter_loss: 0.1785074770450592
train_iter_loss: 0.13875450193881989
train_iter_loss: 0.16850361227989197
train_iter_loss: 0.22777941823005676
train_iter_loss: 0.20415747165679932
train_iter_loss: 0.10280855000019073
train_iter_loss: 0.14054003357887268
train_iter_loss: 0.13204410672187805
train_iter_loss: 0.19011154770851135
train_iter_loss: 0.12900054454803467
train_iter_loss: 0.20041589438915253
train_iter_loss: 0.16278333961963654
train_iter_loss: 0.10614820569753647
train_iter_loss: 0.10300654172897339
train_iter_loss: 0.20902647078037262
train_iter_loss: 0.32191604375839233
train_iter_loss: 0.216701939702034
train_iter_loss: 0.05977403372526169
train_iter_loss: 0.10874427855014801
train_iter_loss: 0.21339231729507446
train_iter_loss: 0.11914778500795364
train_iter_loss: 0.1910407394170761
train_iter_loss: 0.25496524572372437
train_iter_loss: 0.2643532156944275
train_iter_loss: 0.3118254244327545
train_iter_loss: 0.23148825764656067
train_iter_loss: 0.029131174087524414
train_iter_loss: 0.08593634516000748
train_iter_loss: 0.09637750685214996
train_iter_loss: 0.13325877487659454
train_iter_loss: 0.07053296267986298
train_iter_loss: 0.10829830914735794
train_iter_loss: 0.14434486627578735
train_iter_loss: 0.16791245341300964
train_iter_loss: 0.16188868880271912
train_iter_loss: 0.17090854048728943
train_iter_loss: 0.22993238270282745
train_iter_loss: 0.16676749289035797
train_iter_loss: 0.11251971125602722
train_iter_loss: 0.18877039849758148
train_iter_loss: 0.20903362333774567
train_iter_loss: 0.1829914152622223
train_iter_loss: 0.024515878409147263
train_iter_loss: 0.1760188192129135
train_iter_loss: 0.19939912855625153
train_iter_loss: 0.1768302321434021
train_iter_loss: 0.2390526682138443
train_iter_loss: 0.12941263616085052
train_iter_loss: 0.13723856210708618
train_iter_loss: 0.11275485157966614
train_iter_loss: 0.2234428972005844
train_iter_loss: 0.0989350900053978
train_iter_loss: 0.11043765395879745
train_iter_loss: 0.09213294833898544
train_iter_loss: 0.08043269068002701
train_iter_loss: 0.15488556027412415
train_iter_loss: 0.21103337407112122
train_iter_loss: 0.15017841756343842
train_iter_loss: 0.14012397825717926
train_iter_loss: 0.1660616248846054
train_iter_loss: 0.13441655039787292
train_iter_loss: 0.16362003982067108
train_iter_loss: 0.15615513920783997
train loss :0.1586
---------------------
Validation seg loss: 0.2149176041173907 at epoch 532
epoch =    533/  1000, exp = train
train_iter_loss: 0.12636086344718933
train_iter_loss: 0.09502097219228745
train_iter_loss: 0.14438891410827637
train_iter_loss: 0.14261174201965332
train_iter_loss: 0.17371708154678345
train_iter_loss: 0.19459645450115204
train_iter_loss: 0.0767354965209961
train_iter_loss: 0.17530179023742676
train_iter_loss: 0.2464546114206314
train_iter_loss: 0.15806813538074493
train_iter_loss: 0.11059153825044632
train_iter_loss: 0.12752075493335724
train_iter_loss: 0.14786002039909363
train_iter_loss: 0.18748976290225983
train_iter_loss: 0.1877278983592987
train_iter_loss: 0.08568379282951355
train_iter_loss: 0.20168398320674896
train_iter_loss: 0.26885148882865906
train_iter_loss: 0.2388085424900055
train_iter_loss: 0.12463243305683136
train_iter_loss: 0.14889387786388397
train_iter_loss: 0.12463391572237015
train_iter_loss: 0.28769248723983765
train_iter_loss: 0.17583124339580536
train_iter_loss: 0.13786955177783966
train_iter_loss: 0.12409189343452454
train_iter_loss: 0.09830470383167267
train_iter_loss: 0.2622948884963989
train_iter_loss: 0.1721753478050232
train_iter_loss: 0.1381945163011551
train_iter_loss: 0.14825105667114258
train_iter_loss: 0.09935589879751205
train_iter_loss: 0.2641046941280365
train_iter_loss: 0.1261478066444397
train_iter_loss: 0.18541839718818665
train_iter_loss: 0.10819806903600693
train_iter_loss: 0.051762405782938004
train_iter_loss: 0.11422871798276901
train_iter_loss: 0.17616905272006989
train_iter_loss: 0.2256651222705841
train_iter_loss: 0.14204175770282745
train_iter_loss: 0.12777094542980194
train_iter_loss: 0.20984721183776855
train_iter_loss: 0.04760593920946121
train_iter_loss: 0.11208105087280273
train_iter_loss: 0.1808939427137375
train_iter_loss: 0.12162996828556061
train_iter_loss: 0.13311289250850677
train_iter_loss: 0.20988686382770538
train_iter_loss: 0.216853067278862
train_iter_loss: 0.11059549450874329
train_iter_loss: 0.19942133128643036
train_iter_loss: 0.3797299265861511
train_iter_loss: 0.1274935007095337
train_iter_loss: 0.18434371054172516
train_iter_loss: 0.18998710811138153
train_iter_loss: 0.11640353500843048
train_iter_loss: 0.10180884599685669
train_iter_loss: 0.19016048312187195
train_iter_loss: 0.27447131276130676
train_iter_loss: 0.20463865995407104
train_iter_loss: 0.26575735211372375
train_iter_loss: 0.3203677237033844
train_iter_loss: 0.10623373836278915
train_iter_loss: 0.20979571342468262
train_iter_loss: 0.16382941603660583
train_iter_loss: 0.16349637508392334
train_iter_loss: 0.15578068792819977
train_iter_loss: 0.1119772344827652
train_iter_loss: 0.08014041930437088
train_iter_loss: 0.2318088263273239
train_iter_loss: 0.1459713578224182
train_iter_loss: 0.1667952537536621
train_iter_loss: 0.17583847045898438
train_iter_loss: 0.21354977786540985
train_iter_loss: 0.1705894023180008
train_iter_loss: 0.35317108035087585
train_iter_loss: 0.17971615493297577
train_iter_loss: 0.18683800101280212
train_iter_loss: 0.04665783420205116
train_iter_loss: 0.08414371311664581
train_iter_loss: 0.1788906753063202
train_iter_loss: 0.13811896741390228
train_iter_loss: 0.16914600133895874
train_iter_loss: 0.15937931835651398
train_iter_loss: 0.19295062124729156
train_iter_loss: 0.029492095112800598
train_iter_loss: 0.12725086510181427
train_iter_loss: 0.1685783565044403
train_iter_loss: 0.20765383541584015
train_iter_loss: 0.19325363636016846
train_iter_loss: 0.12264465540647507
train_iter_loss: 0.25070008635520935
train_iter_loss: 0.08155455440282822
train_iter_loss: 0.1005205437541008
train_iter_loss: 0.15907517075538635
train_iter_loss: 0.1175331100821495
train_iter_loss: 0.0752219557762146
train_iter_loss: 0.12323540449142456
train_iter_loss: 0.17019832134246826
train loss :0.1642
---------------------
Validation seg loss: 0.2165370644509511 at epoch 533
epoch =    534/  1000, exp = train
train_iter_loss: 0.17321546375751495
train_iter_loss: 0.0942000299692154
train_iter_loss: 0.24850264191627502
train_iter_loss: 0.13770689070224762
train_iter_loss: 0.25305622816085815
train_iter_loss: 0.17733223736286163
train_iter_loss: 0.12498904764652252
train_iter_loss: 0.24735872447490692
train_iter_loss: 0.10991376638412476
train_iter_loss: 0.09011427313089371
train_iter_loss: 0.28925949335098267
train_iter_loss: 0.1720619648694992
train_iter_loss: 0.18087072670459747
train_iter_loss: 0.16602368652820587
train_iter_loss: 0.13649414479732513
train_iter_loss: 0.14084981381893158
train_iter_loss: 0.19037294387817383
train_iter_loss: 0.11350496858358383
train_iter_loss: 0.15327498316764832
train_iter_loss: 0.10978685319423676
train_iter_loss: 0.15445564687252045
train_iter_loss: 0.13187967240810394
train_iter_loss: 0.10987754166126251
train_iter_loss: 0.14702925086021423
train_iter_loss: 0.1652296483516693
train_iter_loss: 0.09961467981338501
train_iter_loss: 0.10932417213916779
train_iter_loss: 0.28993356227874756
train_iter_loss: 0.14354947209358215
train_iter_loss: 0.13351495563983917
train_iter_loss: 0.2427053302526474
train_iter_loss: 0.029285820201039314
train_iter_loss: 0.15179269015789032
train_iter_loss: 0.13429296016693115
train_iter_loss: 0.1359114944934845
train_iter_loss: 0.15718141198158264
train_iter_loss: 0.11214049160480499
train_iter_loss: 0.24696962535381317
train_iter_loss: 0.10689323395490646
train_iter_loss: 0.18990157544612885
train_iter_loss: 0.20686791837215424
train_iter_loss: 0.15649813413619995
train_iter_loss: 0.09072158485651016
train_iter_loss: 0.1858600378036499
train_iter_loss: 0.12543345987796783
train_iter_loss: 0.10556165874004364
train_iter_loss: 0.17736874520778656
train_iter_loss: 0.1841997355222702
train_iter_loss: 0.22061125934123993
train_iter_loss: 0.05969393998384476
train_iter_loss: 0.152670755982399
train_iter_loss: 0.20594719052314758
train_iter_loss: 0.12308407574892044
train_iter_loss: 0.15701474249362946
train_iter_loss: 0.12736782431602478
train_iter_loss: 0.055169813334941864
train_iter_loss: 0.20763981342315674
train_iter_loss: 0.20444242656230927
train_iter_loss: 0.17462697625160217
train_iter_loss: 0.09408106654882431
train_iter_loss: 0.19675444066524506
train_iter_loss: 0.10201769322156906
train_iter_loss: 0.1927889585494995
train_iter_loss: 0.13133424520492554
train_iter_loss: 0.09595001488924026
train_iter_loss: 0.27908793091773987
train_iter_loss: 0.08729632198810577
train_iter_loss: 0.2020357847213745
train_iter_loss: 0.1378505527973175
train_iter_loss: 0.1251264214515686
train_iter_loss: 0.19556018710136414
train_iter_loss: 0.1677349954843521
train_iter_loss: 0.17298397421836853
train_iter_loss: 0.06751175969839096
train_iter_loss: 0.15075555443763733
train_iter_loss: 0.05945969745516777
train_iter_loss: 0.1822127103805542
train_iter_loss: 0.18222598731517792
train_iter_loss: 0.23899926245212555
train_iter_loss: 0.18414761126041412
train_iter_loss: 0.27226829528808594
train_iter_loss: 0.18198171257972717
train_iter_loss: 0.13094405829906464
train_iter_loss: 0.2673162519931793
train_iter_loss: 0.16442370414733887
train_iter_loss: 0.15535755455493927
train_iter_loss: 0.22716805338859558
train_iter_loss: 0.15364661812782288
train_iter_loss: 0.056247927248477936
train_iter_loss: 0.1586989462375641
train_iter_loss: 0.24450457096099854
train_iter_loss: 0.14761169254779816
train_iter_loss: 0.2575971484184265
train_iter_loss: 0.16428181529045105
train_iter_loss: 0.15752288699150085
train_iter_loss: 0.12001441419124603
train_iter_loss: 0.08925968408584595
train_iter_loss: 0.23770755529403687
train_iter_loss: 0.20462538301944733
train_iter_loss: 0.159958153963089
train loss :0.1618
---------------------
Validation seg loss: 0.21556865954595916 at epoch 534
epoch =    535/  1000, exp = train
train_iter_loss: 0.14649128913879395
train_iter_loss: 0.09625433385372162
train_iter_loss: 0.11082784086465836
train_iter_loss: 0.1923413723707199
train_iter_loss: 0.18572068214416504
train_iter_loss: 0.21135954558849335
train_iter_loss: 0.17061500251293182
train_iter_loss: 0.1319642812013626
train_iter_loss: 0.373861700296402
train_iter_loss: 0.16507023572921753
train_iter_loss: 0.17569416761398315
train_iter_loss: 0.09611963480710983
train_iter_loss: 0.24493394792079926
train_iter_loss: 0.1912091225385666
train_iter_loss: 0.11297088861465454
train_iter_loss: 0.20313885807991028
train_iter_loss: 0.21652275323867798
train_iter_loss: 0.14773625135421753
train_iter_loss: 0.1277266889810562
train_iter_loss: 0.22298185527324677
train_iter_loss: 0.09528940171003342
train_iter_loss: 0.2551719546318054
train_iter_loss: 0.1709747016429901
train_iter_loss: 0.131517231464386
train_iter_loss: 0.1552683264017105
train_iter_loss: 0.17346996068954468
train_iter_loss: 0.12216544896364212
train_iter_loss: 0.13189977407455444
train_iter_loss: 0.14874979853630066
train_iter_loss: 0.2469758242368698
train_iter_loss: 0.1485060304403305
train_iter_loss: 0.08912339806556702
train_iter_loss: 0.2572077810764313
train_iter_loss: 0.15655645728111267
train_iter_loss: 0.0760796070098877
train_iter_loss: 0.12049885094165802
train_iter_loss: 0.2513633072376251
train_iter_loss: 0.08898723870515823
train_iter_loss: 0.16392503678798676
train_iter_loss: 0.12430497258901596
train_iter_loss: 0.14905661344528198
train_iter_loss: 0.1853025108575821
train_iter_loss: 0.2657071352005005
train_iter_loss: 0.0744052454829216
train_iter_loss: 0.23386913537979126
train_iter_loss: 0.09022106975317001
train_iter_loss: 0.10210020840167999
train_iter_loss: 0.2248251587152481
train_iter_loss: 0.18009863793849945
train_iter_loss: 0.17630434036254883
train_iter_loss: 0.10943488776683807
train_iter_loss: 0.13906753063201904
train_iter_loss: 0.1283995658159256
train_iter_loss: 0.10406950116157532
train_iter_loss: 0.11639440059661865
train_iter_loss: 0.1988222599029541
train_iter_loss: 0.2625201344490051
train_iter_loss: 0.12623710930347443
train_iter_loss: 0.16321410238742828
train_iter_loss: 0.1536361277103424
train_iter_loss: 0.14313194155693054
train_iter_loss: 0.09737430512905121
train_iter_loss: 0.23023027181625366
train_iter_loss: 0.07145704329013824
train_iter_loss: 0.2128942459821701
train_iter_loss: 0.19737565517425537
train_iter_loss: 0.08140096813440323
train_iter_loss: 0.1319957673549652
train_iter_loss: 0.1698226034641266
train_iter_loss: 0.10071201622486115
train_iter_loss: 0.2546556293964386
train_iter_loss: 0.24988101422786713
train_iter_loss: 0.218399778008461
train_iter_loss: 0.14516980946063995
train_iter_loss: 0.0706121101975441
train_iter_loss: 0.0931067019701004
train_iter_loss: 0.15669283270835876
train_iter_loss: 0.19108714163303375
train_iter_loss: 0.13357673585414886
train_iter_loss: 0.27241262793540955
train_iter_loss: 0.2894080579280853
train_iter_loss: 0.14546741545200348
train_iter_loss: 0.1220526173710823
train_iter_loss: 0.13203252851963043
train_iter_loss: 0.20081470906734467
train_iter_loss: 0.12922804057598114
train_iter_loss: 0.2564584016799927
train_iter_loss: 0.11225629597902298
train_iter_loss: 0.1861882209777832
train_iter_loss: 0.17529645562171936
train_iter_loss: 0.16098053753376007
train_iter_loss: 0.13056902587413788
train_iter_loss: 0.11457089334726334
train_iter_loss: 0.18026162683963776
train_iter_loss: 0.16654422879219055
train_iter_loss: 0.0865117758512497
train_iter_loss: 0.24190753698349
train_iter_loss: 0.107138492166996
train_iter_loss: 0.17773814499378204
train_iter_loss: 0.2083299160003662
train loss :0.1652
---------------------
Validation seg loss: 0.21599540696040076 at epoch 535
epoch =    536/  1000, exp = train
train_iter_loss: 0.18279196321964264
train_iter_loss: 0.1627635359764099
train_iter_loss: 0.132294699549675
train_iter_loss: 0.2476232796907425
train_iter_loss: 0.0973595380783081
train_iter_loss: 0.2329997569322586
train_iter_loss: 0.11109554022550583
train_iter_loss: 0.1676998734474182
train_iter_loss: 0.14261995255947113
train_iter_loss: 0.11130247265100479
train_iter_loss: 0.09806159883737564
train_iter_loss: 0.22479860484600067
train_iter_loss: 0.11694838851690292
train_iter_loss: 0.2263457328081131
train_iter_loss: 0.10198426991701126
train_iter_loss: 0.12517546117305756
train_iter_loss: 0.18745335936546326
train_iter_loss: 0.14950379729270935
train_iter_loss: 0.12180852890014648
train_iter_loss: 0.0682215616106987
train_iter_loss: 0.14788931608200073
train_iter_loss: 0.1454928070306778
train_iter_loss: 0.15426737070083618
train_iter_loss: 0.1614692360162735
train_iter_loss: 0.17271363735198975
train_iter_loss: 0.0901305302977562
train_iter_loss: 0.27425071597099304
train_iter_loss: 0.2905002534389496
train_iter_loss: 0.14099904894828796
train_iter_loss: 0.1730874627828598
train_iter_loss: 0.1500074863433838
train_iter_loss: 0.1065208837389946
train_iter_loss: 0.11990919709205627
train_iter_loss: 0.13087691366672516
train_iter_loss: 0.2555771470069885
train_iter_loss: 0.15677237510681152
train_iter_loss: 0.0976567417383194
train_iter_loss: 0.2282077521085739
train_iter_loss: 0.12426815181970596
train_iter_loss: 0.10657138377428055
train_iter_loss: 0.18286137282848358
train_iter_loss: 0.19875259697437286
train_iter_loss: 0.12504446506500244
train_iter_loss: 0.19481952488422394
train_iter_loss: 0.14902755618095398
train_iter_loss: 0.12916137278079987
train_iter_loss: 0.1635342687368393
train_iter_loss: 0.23265498876571655
train_iter_loss: 0.24014416337013245
train_iter_loss: 0.14376871287822723
train_iter_loss: 0.09886789321899414
train_iter_loss: 0.0571296326816082
train_iter_loss: 0.1993076652288437
train_iter_loss: 0.04576648399233818
train_iter_loss: 0.17306189239025116
train_iter_loss: 0.18802912533283234
train_iter_loss: 0.14998213946819305
train_iter_loss: 0.13878774642944336
train_iter_loss: 0.13205614686012268
train_iter_loss: 0.17605571448802948
train_iter_loss: 0.18782010674476624
train_iter_loss: 0.16698208451271057
train_iter_loss: 0.12716320157051086
train_iter_loss: 0.20522134006023407
train_iter_loss: 0.1543802171945572
train_iter_loss: 0.17604924738407135
train_iter_loss: 0.04083125293254852
train_iter_loss: 0.15901680290699005
train_iter_loss: 0.1857064664363861
train_iter_loss: 0.09393459558486938
train_iter_loss: 0.14019879698753357
train_iter_loss: 0.17764097452163696
train_iter_loss: 0.04614715650677681
train_iter_loss: 0.21134713292121887
train_iter_loss: 0.07995457947254181
train_iter_loss: 0.13301555812358856
train_iter_loss: 0.22442840039730072
train_iter_loss: 0.15794086456298828
train_iter_loss: 0.2975461483001709
train_iter_loss: 0.09977357089519501
train_iter_loss: 0.1744329035282135
train_iter_loss: 0.153458371758461
train_iter_loss: 0.09359169006347656
train_iter_loss: 0.15068283677101135
train_iter_loss: 0.10577736794948578
train_iter_loss: 0.15768654644489288
train_iter_loss: 0.23071862757205963
train_iter_loss: 0.098176971077919
train_iter_loss: 0.1683899313211441
train_iter_loss: 0.3449462056159973
train_iter_loss: 0.2344847023487091
train_iter_loss: 0.2691020667552948
train_iter_loss: 0.25093206763267517
train_iter_loss: 0.1546805202960968
train_iter_loss: 0.09173451364040375
train_iter_loss: 0.044767897576093674
train_iter_loss: 0.1699238121509552
train_iter_loss: 0.21486644446849823
train_iter_loss: 0.12643200159072876
train_iter_loss: 0.1940687745809555
train loss :0.1601
---------------------
Validation seg loss: 0.2189989034965072 at epoch 536
epoch =    537/  1000, exp = train
train_iter_loss: 0.08620834350585938
train_iter_loss: 0.0960172489285469
train_iter_loss: 0.34423860907554626
train_iter_loss: 0.1748928278684616
train_iter_loss: 0.020769763737916946
train_iter_loss: 0.21439918875694275
train_iter_loss: 0.11526312679052353
train_iter_loss: 0.10412485152482986
train_iter_loss: 0.09866593033075333
train_iter_loss: 0.20226018130779266
train_iter_loss: 0.02131563238799572
train_iter_loss: 0.11150182038545609
train_iter_loss: 0.2431052178144455
train_iter_loss: 0.04768664389848709
train_iter_loss: 0.22220438718795776
train_iter_loss: 0.11400610953569412
train_iter_loss: 0.1708144247531891
train_iter_loss: 0.2004506140947342
train_iter_loss: 0.07076089084148407
train_iter_loss: 0.1655600666999817
train_iter_loss: 0.0871291235089302
train_iter_loss: 0.09473388642072678
train_iter_loss: 0.1794612854719162
train_iter_loss: 0.14320935308933258
train_iter_loss: 0.16922324895858765
train_iter_loss: 0.06005232781171799
train_iter_loss: 0.18500080704689026
train_iter_loss: 0.21360057592391968
train_iter_loss: 0.12936921417713165
train_iter_loss: 0.19936400651931763
train_iter_loss: 0.09114581346511841
train_iter_loss: 0.1710483431816101
train_iter_loss: 0.13986042141914368
train_iter_loss: 0.048481859266757965
train_iter_loss: 0.19238094985485077
train_iter_loss: 0.23180189728736877
train_iter_loss: 0.1191377192735672
train_iter_loss: 0.19826528429985046
train_iter_loss: 0.17378489673137665
train_iter_loss: 0.04470917955040932
train_iter_loss: 0.22702568769454956
train_iter_loss: 0.16622266173362732
train_iter_loss: 0.20481209456920624
train_iter_loss: 0.1732746660709381
train_iter_loss: 0.18444299697875977
train_iter_loss: 0.346383273601532
train_iter_loss: 0.12354012578725815
train_iter_loss: 0.15296489000320435
train_iter_loss: 0.21311567723751068
train_iter_loss: 0.2127312868833542
train_iter_loss: 0.1133187860250473
train_iter_loss: 0.22175288200378418
train_iter_loss: 0.17488642036914825
train_iter_loss: 0.13410666584968567
train_iter_loss: 0.15857915580272675
train_iter_loss: 0.10041006654500961
train_iter_loss: 0.17701464891433716
train_iter_loss: 0.11323978751897812
train_iter_loss: 0.11884503066539764
train_iter_loss: 0.1299654096364975
train_iter_loss: 0.27457258105278015
train_iter_loss: 0.14987727999687195
train_iter_loss: 0.1276593953371048
train_iter_loss: 0.20752203464508057
train_iter_loss: 0.09054146707057953
train_iter_loss: 0.20585493743419647
train_iter_loss: 0.12505292892456055
train_iter_loss: 0.20253527164459229
train_iter_loss: 0.2645532786846161
train_iter_loss: 0.0384615920484066
train_iter_loss: 0.12860839068889618
train_iter_loss: 0.33832138776779175
train_iter_loss: 0.21327315270900726
train_iter_loss: 0.2030268907546997
train_iter_loss: 0.1804264336824417
train_iter_loss: 0.1084238588809967
train_iter_loss: 0.10736735910177231
train_iter_loss: 0.15043684840202332
train_iter_loss: 0.12116366624832153
train_iter_loss: 0.1884537637233734
train_iter_loss: 0.04185732826590538
train_iter_loss: 0.19132986664772034
train_iter_loss: 0.09946637600660324
train_iter_loss: 0.23319387435913086
train_iter_loss: 0.1567893773317337
train_iter_loss: 0.1645403951406479
train_iter_loss: 0.16799260675907135
train_iter_loss: 0.09662680327892303
train_iter_loss: 0.11914251744747162
train_iter_loss: 0.11056523025035858
train_iter_loss: 0.2650305926799774
train_iter_loss: 0.06457037478685379
train_iter_loss: 0.16249258816242218
train_iter_loss: 0.1541210114955902
train_iter_loss: 0.12035509943962097
train_iter_loss: 0.15873044729232788
train_iter_loss: 0.1801314502954483
train_iter_loss: 0.14647765457630157
train_iter_loss: 0.12632983922958374
train_iter_loss: 0.17294852435588837
train loss :0.1566
---------------------
Validation seg loss: 0.21603789885159372 at epoch 537
epoch =    538/  1000, exp = train
train_iter_loss: 0.2887013852596283
train_iter_loss: 0.18180131912231445
train_iter_loss: 0.19443415105342865
train_iter_loss: 0.13338540494441986
train_iter_loss: 0.0807134211063385
train_iter_loss: 0.11371053010225296
train_iter_loss: 0.03177441656589508
train_iter_loss: 0.3030652105808258
train_iter_loss: 0.3525242209434509
train_iter_loss: 0.36495086550712585
train_iter_loss: 0.1401628851890564
train_iter_loss: 0.12728574872016907
train_iter_loss: 0.13793854415416718
train_iter_loss: 0.19201084971427917
train_iter_loss: 0.0495259091258049
train_iter_loss: 0.21561159193515778
train_iter_loss: 0.14087511599063873
train_iter_loss: 0.16915328800678253
train_iter_loss: 0.052782945334911346
train_iter_loss: 0.17579346895217896
train_iter_loss: 0.12828055024147034
train_iter_loss: 0.11495494097471237
train_iter_loss: 0.1425931602716446
train_iter_loss: 0.07621041685342789
train_iter_loss: 0.21228614449501038
train_iter_loss: 0.12014319002628326
train_iter_loss: 0.13070861995220184
train_iter_loss: 0.11091244965791702
train_iter_loss: 0.062088824808597565
train_iter_loss: 0.05274934694170952
train_iter_loss: 0.2668316960334778
train_iter_loss: 0.11437679827213287
train_iter_loss: 0.1831393539905548
train_iter_loss: 0.19827838242053986
train_iter_loss: 0.1397249698638916
train_iter_loss: 0.06300714612007141
train_iter_loss: 0.16118738055229187
train_iter_loss: 0.2594658136367798
train_iter_loss: 0.22629466652870178
train_iter_loss: 0.11175113916397095
train_iter_loss: 0.11426032334566116
train_iter_loss: 0.1401239037513733
train_iter_loss: 0.24022720754146576
train_iter_loss: 0.11887259781360626
train_iter_loss: 0.24482212960720062
train_iter_loss: 0.09104186296463013
train_iter_loss: 0.06321574747562408
train_iter_loss: 0.12036862224340439
train_iter_loss: 0.3355933427810669
train_iter_loss: 0.07290459424257278
train_iter_loss: 0.11450022459030151
train_iter_loss: 0.2601659297943115
train_iter_loss: 0.11915061622858047
train_iter_loss: 0.1739966720342636
train_iter_loss: 0.14601966738700867
train_iter_loss: 0.09741853922605515
train_iter_loss: 0.21638397872447968
train_iter_loss: 0.13294237852096558
train_iter_loss: 0.12458905577659607
train_iter_loss: 0.20899397134780884
train_iter_loss: 0.1123860701918602
train_iter_loss: 0.2269720882177353
train_iter_loss: 0.12228190153837204
train_iter_loss: 0.15256181359291077
train_iter_loss: 0.101407989859581
train_iter_loss: 0.1271916776895523
train_iter_loss: 0.21445268392562866
train_iter_loss: 0.18408441543579102
train_iter_loss: 0.19722704589366913
train_iter_loss: 0.10770954936742783
train_iter_loss: 0.25420141220092773
train_iter_loss: 0.16414405405521393
train_iter_loss: 0.2854679226875305
train_iter_loss: 0.07066016644239426
train_iter_loss: 0.17627649009227753
train_iter_loss: 0.07906721532344818
train_iter_loss: 0.2547718584537506
train_iter_loss: 0.16100169718265533
train_iter_loss: 0.08741600811481476
train_iter_loss: 0.18846280872821808
train_iter_loss: 0.10121969878673553
train_iter_loss: 0.1009516641497612
train_iter_loss: 0.16390445828437805
train_iter_loss: 0.13043418526649475
train_iter_loss: 0.13505472242832184
train_iter_loss: 0.10595416277647018
train_iter_loss: 0.1616310179233551
train_iter_loss: 0.1128348559141159
train_iter_loss: 0.14761324226856232
train_iter_loss: 0.22191458940505981
train_iter_loss: 0.12201224267482758
train_iter_loss: 0.21035194396972656
train_iter_loss: 0.21732231974601746
train_iter_loss: 0.11459849029779434
train_iter_loss: 0.12228718400001526
train_iter_loss: 0.16641905903816223
train_iter_loss: 0.1331242471933365
train_iter_loss: 0.2506858706474304
train_iter_loss: 0.07578682899475098
train_iter_loss: 0.21846739947795868
train loss :0.1589
---------------------
Validation seg loss: 0.2185838649594137 at epoch 538
epoch =    539/  1000, exp = train
train_iter_loss: 0.2384553998708725
train_iter_loss: 0.09189622849225998
train_iter_loss: 0.12807884812355042
train_iter_loss: 0.15324030816555023
train_iter_loss: 0.08627993613481522
train_iter_loss: 0.1792806088924408
train_iter_loss: 0.1401742398738861
train_iter_loss: 0.09407776594161987
train_iter_loss: 0.2682873606681824
train_iter_loss: 0.19763541221618652
train_iter_loss: 0.09727024286985397
train_iter_loss: 0.1687079668045044
train_iter_loss: 0.1570710837841034
train_iter_loss: 0.15026287734508514
train_iter_loss: 0.08933521062135696
train_iter_loss: 0.06275705248117447
train_iter_loss: 0.1345924288034439
train_iter_loss: 0.2663347125053406
train_iter_loss: 0.14512188732624054
train_iter_loss: 0.061051636934280396
train_iter_loss: 0.19236236810684204
train_iter_loss: 0.1394885927438736
train_iter_loss: 0.22398267686367035
train_iter_loss: 0.27919724583625793
train_iter_loss: 0.17459850013256073
train_iter_loss: 0.22308917343616486
train_iter_loss: 0.19380943477153778
train_iter_loss: 0.14036229252815247
train_iter_loss: 0.12552988529205322
train_iter_loss: 0.20386184751987457
train_iter_loss: 0.1853138506412506
train_iter_loss: 0.12108451873064041
train_iter_loss: 0.03893399238586426
train_iter_loss: 0.15821589529514313
train_iter_loss: 0.07943735271692276
train_iter_loss: 0.10497486591339111
train_iter_loss: 0.26949959993362427
train_iter_loss: 0.21076928079128265
train_iter_loss: 0.13587085902690887
train_iter_loss: 0.23410099744796753
train_iter_loss: 0.11665139347314835
train_iter_loss: 0.15281376242637634
train_iter_loss: 0.05800511687994003
train_iter_loss: 0.19490765035152435
train_iter_loss: 0.1207083985209465
train_iter_loss: 0.09968170523643494
train_iter_loss: 0.1920945793390274
train_iter_loss: 0.15620057284832
train_iter_loss: 0.11280613392591476
train_iter_loss: 0.16835322976112366
train_iter_loss: 0.15381237864494324
train_iter_loss: 0.226334810256958
train_iter_loss: 0.13401257991790771
train_iter_loss: 0.07629077136516571
train_iter_loss: 0.1458137184381485
train_iter_loss: 0.09867458790540695
train_iter_loss: 0.12419706583023071
train_iter_loss: 0.12071265280246735
train_iter_loss: 0.1612919270992279
train_iter_loss: 0.21714000403881073
train_iter_loss: 0.2108999788761139
train_iter_loss: 0.07475530356168747
train_iter_loss: 0.11471479386091232
train_iter_loss: 0.27721714973449707
train_iter_loss: 0.11825969070196152
train_iter_loss: 0.07820436358451843
train_iter_loss: 0.23572182655334473
train_iter_loss: 0.11052004247903824
train_iter_loss: 0.10749686509370804
train_iter_loss: 0.2311074435710907
train_iter_loss: 0.08324267715215683
train_iter_loss: 0.1077432632446289
train_iter_loss: 0.2006916105747223
train_iter_loss: 0.17870475351810455
train_iter_loss: 0.1690778285264969
train_iter_loss: 0.10698342323303223
train_iter_loss: 0.16877157986164093
train_iter_loss: 0.10174323618412018
train_iter_loss: 0.12795160710811615
train_iter_loss: 0.2674981951713562
train_iter_loss: 0.1526387631893158
train_iter_loss: 0.18874889612197876
train_iter_loss: 0.3942740261554718
train_iter_loss: 0.0614507794380188
train_iter_loss: 0.18330131471157074
train_iter_loss: 0.18205425143241882
train_iter_loss: 0.11776749044656754
train_iter_loss: 0.1980191320180893
train_iter_loss: 0.15179228782653809
train_iter_loss: 0.11518895626068115
train_iter_loss: 0.3967747092247009
train_iter_loss: 0.08993757516145706
train_iter_loss: 0.1672578603029251
train_iter_loss: 0.06194630637764931
train_iter_loss: 0.1073877140879631
train_iter_loss: 0.2764132022857666
train_iter_loss: 0.08877143263816833
train_iter_loss: 0.3885871469974518
train_iter_loss: 0.10832438617944717
train_iter_loss: 0.23242907226085663
train loss :0.1607
---------------------
Validation seg loss: 0.22295754075155788 at epoch 539
epoch =    540/  1000, exp = train
train_iter_loss: 0.09105660766363144
train_iter_loss: 0.17426393926143646
train_iter_loss: 0.16451138257980347
train_iter_loss: 0.1715392917394638
train_iter_loss: 0.18022798001766205
train_iter_loss: 0.28945183753967285
train_iter_loss: 0.1181621104478836
train_iter_loss: 0.11912389099597931
train_iter_loss: 0.16418668627738953
train_iter_loss: 0.09733503311872482
train_iter_loss: 0.20447230339050293
train_iter_loss: 0.3785994350910187
train_iter_loss: 0.23277641832828522
train_iter_loss: 0.1915602684020996
train_iter_loss: 0.17138314247131348
train_iter_loss: 0.44578754901885986
train_iter_loss: 0.04691270366311073
train_iter_loss: 0.21264316141605377
train_iter_loss: 0.3436424732208252
train_iter_loss: 0.1927926391363144
train_iter_loss: 0.39058858156204224
train_iter_loss: 0.17342877388000488
train_iter_loss: 0.2892359793186188
train_iter_loss: 0.2658707797527313
train_iter_loss: 0.38179102540016174
train_iter_loss: 0.13347375392913818
train_iter_loss: 0.1285577416419983
train_iter_loss: 0.1842854917049408
train_iter_loss: 0.06521258503198624
train_iter_loss: 0.06837841123342514
train_iter_loss: 0.21162809431552887
train_iter_loss: 0.3464759290218353
train_iter_loss: 0.17453816533088684
train_iter_loss: 0.1082744300365448
train_iter_loss: 0.17637541890144348
train_iter_loss: 0.08749563992023468
train_iter_loss: 0.05450671911239624
train_iter_loss: 0.2449224442243576
train_iter_loss: 0.19241291284561157
train_iter_loss: 0.13428036868572235
train_iter_loss: 0.259895920753479
train_iter_loss: 0.13176199793815613
train_iter_loss: 0.18861763179302216
train_iter_loss: 0.21573328971862793
train_iter_loss: 0.17584674060344696
train_iter_loss: 0.16007335484027863
train_iter_loss: 0.14655621349811554
train_iter_loss: 0.25010600686073303
train_iter_loss: 0.10174162685871124
train_iter_loss: 0.11184937506914139
train_iter_loss: 0.2179434597492218
train_iter_loss: 0.1465422660112381
train_iter_loss: 0.15933957695960999
train_iter_loss: 0.06608127802610397
train_iter_loss: 0.101552315056324
train_iter_loss: 0.15054522454738617
train_iter_loss: 0.16874045133590698
train_iter_loss: 0.15573686361312866
train_iter_loss: 0.06716222316026688
train_iter_loss: 0.10144297033548355
train_iter_loss: 0.16009992361068726
train_iter_loss: 0.11835845559835434
train_iter_loss: 0.11561572551727295
train_iter_loss: 0.16321100294589996
train_iter_loss: 0.14474733173847198
train_iter_loss: 0.11934980750083923
train_iter_loss: 0.16238448023796082
train_iter_loss: 0.10515107959508896
train_iter_loss: 0.06029360741376877
train_iter_loss: 0.2137354165315628
train_iter_loss: 0.10045179724693298
train_iter_loss: 0.12539450824260712
train_iter_loss: 0.10068915784358978
train_iter_loss: 0.11573702096939087
train_iter_loss: 0.2516845762729645
train_iter_loss: 0.15695640444755554
train_iter_loss: 0.07148939371109009
train_iter_loss: 0.1142284944653511
train_iter_loss: 0.22906434535980225
train_iter_loss: 0.05641557648777962
train_iter_loss: 0.18804119527339935
train_iter_loss: 0.18093129992485046
train_iter_loss: 0.08792439848184586
train_iter_loss: 0.18835000693798065
train_iter_loss: 0.14728087186813354
train_iter_loss: 0.258951336145401
train_iter_loss: 0.07471416145563126
train_iter_loss: 0.3040236830711365
train_iter_loss: 0.16297905147075653
train_iter_loss: 0.11355548352003098
train_iter_loss: 0.15633928775787354
train_iter_loss: 0.12054776400327682
train_iter_loss: 0.11971742659807205
train_iter_loss: 0.17859181761741638
train_iter_loss: 0.19900542497634888
train_iter_loss: 0.13433469831943512
train_iter_loss: 0.02126535028219223
train_iter_loss: 0.13890087604522705
train_iter_loss: 0.24809718132019043
train_iter_loss: 0.15793132781982422
train loss :0.1691
---------------------
Validation seg loss: 0.22057663799562263 at epoch 540
epoch =    541/  1000, exp = train
train_iter_loss: 0.18188463151454926
train_iter_loss: 0.06799442321062088
train_iter_loss: 0.1332317292690277
train_iter_loss: 0.08482539653778076
train_iter_loss: 0.1108112782239914
train_iter_loss: 0.13072387874126434
train_iter_loss: 0.21146631240844727
train_iter_loss: 0.13563726842403412
train_iter_loss: 0.19595912098884583
train_iter_loss: 0.16381694376468658
train_iter_loss: 0.13503828644752502
train_iter_loss: 0.12710976600646973
train_iter_loss: 0.4016224145889282
train_iter_loss: 0.10159890353679657
train_iter_loss: 0.06286277621984482
train_iter_loss: 0.11263991892337799
train_iter_loss: 0.18631276488304138
train_iter_loss: 0.1987837702035904
train_iter_loss: 0.051157429814338684
train_iter_loss: 0.11322598159313202
train_iter_loss: 0.18398109078407288
train_iter_loss: 0.14828403294086456
train_iter_loss: 0.048597730696201324
train_iter_loss: 0.19325228035449982
train_iter_loss: 0.13639158010482788
train_iter_loss: 0.426014244556427
train_iter_loss: 0.12435336410999298
train_iter_loss: 0.1389816254377365
train_iter_loss: 0.2263074517250061
train_iter_loss: 0.11860504746437073
train_iter_loss: 0.172063946723938
train_iter_loss: 0.22669529914855957
train_iter_loss: 0.25007790327072144
train_iter_loss: 0.27494895458221436
train_iter_loss: 0.15654365718364716
train_iter_loss: 0.1351294219493866
train_iter_loss: 0.22192062437534332
train_iter_loss: 0.3211873173713684
train_iter_loss: 0.2466723620891571
train_iter_loss: 0.10856103152036667
train_iter_loss: 0.12122499942779541
train_iter_loss: 0.18033145368099213
train_iter_loss: 0.13568253815174103
train_iter_loss: 0.18768428266048431
train_iter_loss: 0.13575775921344757
train_iter_loss: 0.153257355093956
train_iter_loss: 0.11448051035404205
train_iter_loss: 0.16561418771743774
train_iter_loss: 0.12394391000270844
train_iter_loss: 0.16505303978919983
train_iter_loss: 0.18475383520126343
train_iter_loss: 0.17450033128261566
train_iter_loss: 0.15175515413284302
train_iter_loss: 0.10781550407409668
train_iter_loss: 0.1245969906449318
train_iter_loss: 0.17273342609405518
train_iter_loss: 0.10743247717618942
train_iter_loss: 0.12664631009101868
train_iter_loss: 0.1265960931777954
train_iter_loss: 0.020600412040948868
train_iter_loss: 0.1483418047428131
train_iter_loss: 0.171144500374794
train_iter_loss: 0.2791009247303009
train_iter_loss: 0.13187666237354279
train_iter_loss: 0.107016421854496
train_iter_loss: 0.24224913120269775
train_iter_loss: 0.1946791261434555
train_iter_loss: 0.09008044004440308
train_iter_loss: 0.11820270866155624
train_iter_loss: 0.043070487678050995
train_iter_loss: 0.06779234111309052
train_iter_loss: 0.11229319870471954
train_iter_loss: 0.10284970700740814
train_iter_loss: 0.2607957720756531
train_iter_loss: 0.10398690402507782
train_iter_loss: 0.1538390964269638
train_iter_loss: 0.20805712044239044
train_iter_loss: 0.18174272775650024
train_iter_loss: 0.14931415021419525
train_iter_loss: 0.2036350518465042
train_iter_loss: 0.04560334235429764
train_iter_loss: 0.08013391494750977
train_iter_loss: 0.15682651102542877
train_iter_loss: 0.23026873171329498
train_iter_loss: 0.18539755046367645
train_iter_loss: 0.10787264257669449
train_iter_loss: 0.16457389295101166
train_iter_loss: 0.12753376364707947
train_iter_loss: 0.19902028143405914
train_iter_loss: 0.12321439385414124
train_iter_loss: 0.13394632935523987
train_iter_loss: 0.049865033477544785
train_iter_loss: 0.09293047338724136
train_iter_loss: 0.15208669006824493
train_iter_loss: 0.23550139367580414
train_iter_loss: 0.13954398036003113
train_iter_loss: 0.1469075232744217
train_iter_loss: 0.18216298520565033
train_iter_loss: 0.17544429004192352
train_iter_loss: 0.20821096003055573
train loss :0.1572
---------------------
Validation seg loss: 0.2218418564946162 at epoch 541
epoch =    542/  1000, exp = train
train_iter_loss: 0.22981560230255127
train_iter_loss: 0.15759140253067017
train_iter_loss: 0.19208355247974396
train_iter_loss: 0.13025979697704315
train_iter_loss: 0.1509583592414856
train_iter_loss: 0.16658785939216614
train_iter_loss: 0.12637247145175934
train_iter_loss: 0.1862342804670334
train_iter_loss: 0.17679493129253387
train_iter_loss: 0.12392234802246094
train_iter_loss: 0.14358770847320557
train_iter_loss: 0.2339642345905304
train_iter_loss: 0.07991216331720352
train_iter_loss: 0.07482706755399704
train_iter_loss: 0.1767159253358841
train_iter_loss: 0.2464948296546936
train_iter_loss: 0.15524806082248688
train_iter_loss: 0.11292057484388351
train_iter_loss: 0.12650369107723236
train_iter_loss: 0.10201536864042282
train_iter_loss: 0.16950209438800812
train_iter_loss: 0.13902391493320465
train_iter_loss: 0.14551101624965668
train_iter_loss: 0.26598310470581055
train_iter_loss: 0.048108939081430435
train_iter_loss: 0.15416386723518372
train_iter_loss: 0.13072185218334198
train_iter_loss: 0.2183775156736374
train_iter_loss: 0.2511041462421417
train_iter_loss: 0.2288241982460022
train_iter_loss: 0.13309720158576965
train_iter_loss: 0.17747443914413452
train_iter_loss: 0.18055735528469086
train_iter_loss: 0.1518045961856842
train_iter_loss: 0.18579185009002686
train_iter_loss: 0.12838239967823029
train_iter_loss: 0.22195953130722046
train_iter_loss: 0.2765200138092041
train_iter_loss: 0.09334710985422134
train_iter_loss: 0.11546450853347778
train_iter_loss: 0.13246305286884308
train_iter_loss: 0.14386892318725586
train_iter_loss: 0.14247414469718933
train_iter_loss: 0.15831519663333893
train_iter_loss: 0.20455096662044525
train_iter_loss: 0.10605257004499435
train_iter_loss: 0.25173068046569824
train_iter_loss: 0.17222772538661957
train_iter_loss: 0.09797686338424683
train_iter_loss: 0.10648631304502487
train_iter_loss: 0.07247677445411682
train_iter_loss: 0.15629129111766815
train_iter_loss: 0.2031008005142212
train_iter_loss: 0.1428806334733963
train_iter_loss: 0.0995982214808464
train_iter_loss: 0.17437700927257538
train_iter_loss: 0.17912402749061584
train_iter_loss: 0.27929073572158813
train_iter_loss: 0.1458825021982193
train_iter_loss: 0.191231369972229
train_iter_loss: 0.1052275076508522
train_iter_loss: 0.16290205717086792
train_iter_loss: 0.1995159536600113
train_iter_loss: 0.25807976722717285
train_iter_loss: 0.13602620363235474
train_iter_loss: 0.09378211945295334
train_iter_loss: 0.1613519936800003
train_iter_loss: 0.188194140791893
train_iter_loss: 0.08873587101697922
train_iter_loss: 0.09706223011016846
train_iter_loss: 0.14981958270072937
train_iter_loss: 0.2135501652956009
train_iter_loss: 0.18883495032787323
train_iter_loss: 0.24641147255897522
train_iter_loss: 0.15509754419326782
train_iter_loss: 0.11966780573129654
train_iter_loss: 0.06152823194861412
train_iter_loss: 0.10561920702457428
train_iter_loss: 0.10802505165338516
train_iter_loss: 0.2587795853614807
train_iter_loss: 0.239212304353714
train_iter_loss: 0.054996587336063385
train_iter_loss: 0.14872239530086517
train_iter_loss: 0.13408181071281433
train_iter_loss: 0.1548893004655838
train_iter_loss: 0.08850973844528198
train_iter_loss: 0.06679019331932068
train_iter_loss: 0.14977788925170898
train_iter_loss: 0.21896035969257355
train_iter_loss: 0.12470946460962296
train_iter_loss: 0.22372005879878998
train_iter_loss: 0.1972571760416031
train_iter_loss: 0.1680281162261963
train_iter_loss: 0.20079335570335388
train_iter_loss: 0.2652798593044281
train_iter_loss: 0.10933282971382141
train_iter_loss: 0.2547755539417267
train_iter_loss: 0.1596856266260147
train_iter_loss: 0.15054433047771454
train_iter_loss: 0.11707911640405655
train loss :0.1615
---------------------
Validation seg loss: 0.22224513680305122 at epoch 542
epoch =    543/  1000, exp = train
train_iter_loss: 0.044034045189619064
train_iter_loss: 0.1890794336795807
train_iter_loss: 0.14652279019355774
train_iter_loss: 0.108733169734478
train_iter_loss: 0.1451837122440338
train_iter_loss: 0.13746723532676697
train_iter_loss: 0.17099642753601074
train_iter_loss: 0.23030303418636322
train_iter_loss: 0.058836452662944794
train_iter_loss: 0.2235865592956543
train_iter_loss: 0.08896570652723312
train_iter_loss: 0.19241034984588623
train_iter_loss: 0.07568509131669998
train_iter_loss: 0.1629038155078888
train_iter_loss: 0.14926575124263763
train_iter_loss: 0.1013750284910202
train_iter_loss: 0.14367282390594482
train_iter_loss: 0.16559548676013947
train_iter_loss: 0.13855034112930298
train_iter_loss: 0.16167417168617249
train_iter_loss: 0.35061192512512207
train_iter_loss: 0.11861854791641235
train_iter_loss: 0.1442641168832779
train_iter_loss: 0.14775806665420532
train_iter_loss: 0.2262415587902069
train_iter_loss: 0.06361325085163116
train_iter_loss: 0.3678548038005829
train_iter_loss: 0.13024327158927917
train_iter_loss: 0.15612488985061646
train_iter_loss: 0.1345367431640625
train_iter_loss: 0.0920717716217041
train_iter_loss: 0.16597260534763336
train_iter_loss: 0.11342664062976837
train_iter_loss: 0.14792609214782715
train_iter_loss: 0.28909632563591003
train_iter_loss: 0.1694369912147522
train_iter_loss: 0.2353542447090149
train_iter_loss: 0.13660235702991486
train_iter_loss: 0.27804943919181824
train_iter_loss: 0.17595024406909943
train_iter_loss: 0.14262960851192474
train_iter_loss: 0.08778327703475952
train_iter_loss: 0.13885001838207245
train_iter_loss: 0.20824283361434937
train_iter_loss: 0.16972096264362335
train_iter_loss: 0.226149320602417
train_iter_loss: 0.07724709808826447
train_iter_loss: 0.08891156315803528
train_iter_loss: 0.3288124203681946
train_iter_loss: 0.1337219774723053
train_iter_loss: 0.31203535199165344
train_iter_loss: 0.2818661630153656
train_iter_loss: 0.09270972013473511
train_iter_loss: 0.0992717444896698
train_iter_loss: 0.09006688743829727
train_iter_loss: 0.11162333935499191
train_iter_loss: 0.21195979416370392
train_iter_loss: 0.1285933256149292
train_iter_loss: 0.19485989212989807
train_iter_loss: 0.19951863586902618
train_iter_loss: 0.16242621839046478
train_iter_loss: 0.20860809087753296
train_iter_loss: 0.18522608280181885
train_iter_loss: 0.09147995710372925
train_iter_loss: 0.1130075678229332
train_iter_loss: 0.20083235204219818
train_iter_loss: 0.1132674589753151
train_iter_loss: 0.15650510787963867
train_iter_loss: 0.24486319720745087
train_iter_loss: 0.07683886587619781
train_iter_loss: 0.16765783727169037
train_iter_loss: 0.1611739695072174
train_iter_loss: 0.21758393943309784
train_iter_loss: 0.11345331370830536
train_iter_loss: 0.15107949078083038
train_iter_loss: 0.1488436907529831
train_iter_loss: 0.0486941784620285
train_iter_loss: 0.2737182378768921
train_iter_loss: 0.15469218790531158
train_iter_loss: 0.19949457049369812
train_iter_loss: 0.15327049791812897
train_iter_loss: 0.04879198968410492
train_iter_loss: 0.11180911213159561
train_iter_loss: 0.1799958497285843
train_iter_loss: 0.2307569533586502
train_iter_loss: 0.149993896484375
train_iter_loss: 0.1417488008737564
train_iter_loss: 0.18820777535438538
train_iter_loss: 0.15691664814949036
train_iter_loss: 0.17433732748031616
train_iter_loss: 0.1421995311975479
train_iter_loss: 0.18151667714118958
train_iter_loss: 0.14970538020133972
train_iter_loss: 0.25777795910835266
train_iter_loss: 0.29211297631263733
train_iter_loss: 0.12855206429958344
train_iter_loss: 0.09611650556325912
train_iter_loss: 0.09477102756500244
train_iter_loss: 0.20824922621250153
train_iter_loss: 0.14990702271461487
train loss :0.1639
---------------------
Validation seg loss: 0.21971547251286092 at epoch 543
epoch =    544/  1000, exp = train
train_iter_loss: 0.13770927488803864
train_iter_loss: 0.2406032383441925
train_iter_loss: 0.17008952796459198
train_iter_loss: 0.19020602107048035
train_iter_loss: 0.12964649498462677
train_iter_loss: 0.17291100323200226
train_iter_loss: 0.12870152294635773
train_iter_loss: 0.2688370645046234
train_iter_loss: 0.17851492762565613
train_iter_loss: 0.1739395707845688
train_iter_loss: 0.24882987141609192
train_iter_loss: 0.2508811354637146
train_iter_loss: 0.13411377370357513
train_iter_loss: 0.07242680341005325
train_iter_loss: 0.06964484602212906
train_iter_loss: 0.1989479660987854
train_iter_loss: 0.1757216453552246
train_iter_loss: 0.11727274954319
train_iter_loss: 0.1894349753856659
train_iter_loss: 0.22905123233795166
train_iter_loss: 0.0960751548409462
train_iter_loss: 0.14817170798778534
train_iter_loss: 0.1933358758687973
train_iter_loss: 0.24109528958797455
train_iter_loss: 0.09117770195007324
train_iter_loss: 0.15369226038455963
train_iter_loss: 0.13595019280910492
train_iter_loss: 0.1296769231557846
train_iter_loss: 0.1332981437444687
train_iter_loss: 0.1383230835199356
train_iter_loss: 0.14362168312072754
train_iter_loss: 0.07871910184621811
train_iter_loss: 0.1792418658733368
train_iter_loss: 0.1454092264175415
train_iter_loss: 0.1896398961544037
train_iter_loss: 0.06790069490671158
train_iter_loss: 0.16505743563175201
train_iter_loss: 0.2857777774333954
train_iter_loss: 0.1090904101729393
train_iter_loss: 0.10046856850385666
train_iter_loss: 0.2909436523914337
train_iter_loss: 0.17454613745212555
train_iter_loss: 0.11313466727733612
train_iter_loss: 0.23562197387218475
train_iter_loss: 0.19371186196804047
train_iter_loss: 0.17343352735042572
train_iter_loss: 0.15365271270275116
train_iter_loss: 0.1346338391304016
train_iter_loss: 0.20630234479904175
train_iter_loss: 0.21108192205429077
train_iter_loss: 0.15443658828735352
train_iter_loss: 0.09447163343429565
train_iter_loss: 0.0983918160200119
train_iter_loss: 0.11312094330787659
train_iter_loss: 0.17716330289840698
train_iter_loss: 0.15616311132907867
train_iter_loss: 0.10619989782571793
train_iter_loss: 0.14350846409797668
train_iter_loss: 0.17501963675022125
train_iter_loss: 0.1711052805185318
train_iter_loss: 0.2047402262687683
train_iter_loss: 0.13605448603630066
train_iter_loss: 0.1730237901210785
train_iter_loss: 0.09666357934474945
train_iter_loss: 0.1278379112482071
train_iter_loss: 0.20002341270446777
train_iter_loss: 0.14581555128097534
train_iter_loss: 0.10927412658929825
train_iter_loss: 0.10506027191877365
train_iter_loss: 0.16604307293891907
train_iter_loss: 0.08413159847259521
train_iter_loss: 0.20667555928230286
train_iter_loss: 0.2948271632194519
train_iter_loss: 0.09348136186599731
train_iter_loss: 0.22838746011257172
train_iter_loss: 0.08995744585990906
train_iter_loss: 0.18032869696617126
train_iter_loss: 0.16315053403377533
train_iter_loss: 0.24395108222961426
train_iter_loss: 0.08509058505296707
train_iter_loss: 0.10580635815858841
train_iter_loss: 0.07328478991985321
train_iter_loss: 0.2212039679288864
train_iter_loss: 0.14369933307170868
train_iter_loss: 0.15971612930297852
train_iter_loss: 0.042419902980327606
train_iter_loss: 0.1506800651550293
train_iter_loss: 0.18478652834892273
train_iter_loss: 0.15025167167186737
train_iter_loss: 0.16124321520328522
train_iter_loss: 0.2038877159357071
train_iter_loss: 0.10745412856340408
train_iter_loss: 0.3075014054775238
train_iter_loss: 0.1377093344926834
train_iter_loss: 0.17819131910800934
train_iter_loss: 0.1857766956090927
train_iter_loss: 0.13033220171928406
train_iter_loss: 0.20550712943077087
train_iter_loss: 0.18353167176246643
train_iter_loss: 0.09039657562971115
train loss :0.1610
---------------------
Validation seg loss: 0.21880063099154043 at epoch 544
epoch =    545/  1000, exp = train
train_iter_loss: 0.08000391721725464
train_iter_loss: 0.022337205708026886
train_iter_loss: 0.20871350169181824
train_iter_loss: 0.16066670417785645
train_iter_loss: 0.11919590085744858
train_iter_loss: 0.16036535799503326
train_iter_loss: 0.15665128827095032
train_iter_loss: 0.2474907636642456
train_iter_loss: 0.08825580775737762
train_iter_loss: 0.15417441725730896
train_iter_loss: 0.23595358431339264
train_iter_loss: 0.35742446780204773
train_iter_loss: 0.14490649104118347
train_iter_loss: 0.11833053082227707
train_iter_loss: 0.2730101943016052
train_iter_loss: 0.07746896147727966
train_iter_loss: 0.09773839265108109
train_iter_loss: 0.11679048836231232
train_iter_loss: 0.15513810515403748
train_iter_loss: 0.18824633955955505
train_iter_loss: 0.13550825417041779
train_iter_loss: 0.1960235834121704
train_iter_loss: 0.15362653136253357
train_iter_loss: 0.1187625378370285
train_iter_loss: 0.17860160768032074
train_iter_loss: 0.18193526566028595
train_iter_loss: 0.06749218702316284
train_iter_loss: 0.17879618704319
train_iter_loss: 0.16686327755451202
train_iter_loss: 0.1122889295220375
train_iter_loss: 0.16873879730701447
train_iter_loss: 0.06086881086230278
train_iter_loss: 0.0642360970377922
train_iter_loss: 0.10652154684066772
train_iter_loss: 0.1311904937028885
train_iter_loss: 0.20653942227363586
train_iter_loss: 0.09761370718479156
train_iter_loss: 0.19356536865234375
train_iter_loss: 0.11687271296977997
train_iter_loss: 0.21741262078285217
train_iter_loss: 0.16623695194721222
train_iter_loss: 0.1493929773569107
train_iter_loss: 0.15566889941692352
train_iter_loss: 0.23857252299785614
train_iter_loss: 0.258915513753891
train_iter_loss: 0.16175836324691772
train_iter_loss: 0.15965969860553741
train_iter_loss: 0.22510339319705963
train_iter_loss: 0.12169413268566132
train_iter_loss: 0.16254377365112305
train_iter_loss: 0.1586008220911026
train_iter_loss: 0.12866249680519104
train_iter_loss: 0.1555320769548416
train_iter_loss: 0.24138842523097992
train_iter_loss: 0.15521694719791412
train_iter_loss: 0.1365099400281906
train_iter_loss: 0.16024485230445862
train_iter_loss: 0.1491309404373169
train_iter_loss: 0.1680266112089157
train_iter_loss: 0.11963794380426407
train_iter_loss: 0.33342018723487854
train_iter_loss: 0.13876518607139587
train_iter_loss: 0.2584511339664459
train_iter_loss: 0.0780537873506546
train_iter_loss: 0.18505243957042694
train_iter_loss: 0.17397764325141907
train_iter_loss: 0.07215201109647751
train_iter_loss: 0.17995809018611908
train_iter_loss: 0.17675969004631042
train_iter_loss: 0.09411279857158661
train_iter_loss: 0.1456795632839203
train_iter_loss: 0.10858158767223358
train_iter_loss: 0.11722340434789658
train_iter_loss: 0.15045849978923798
train_iter_loss: 0.11804679036140442
train_iter_loss: 0.09491130709648132
train_iter_loss: 0.13632653653621674
train_iter_loss: 0.16562044620513916
train_iter_loss: 0.16341115534305573
train_iter_loss: 0.1349143087863922
train_iter_loss: 0.1945882886648178
train_iter_loss: 0.2594641149044037
train_iter_loss: 0.18016834557056427
train_iter_loss: 0.1377195119857788
train_iter_loss: 0.10669612884521484
train_iter_loss: 0.05443337932229042
train_iter_loss: 0.44720181822776794
train_iter_loss: 0.18896985054016113
train_iter_loss: 0.07952068746089935
train_iter_loss: 0.24169836938381195
train_iter_loss: 0.19572462141513824
train_iter_loss: 0.18993116915225983
train_iter_loss: 0.21338455379009247
train_iter_loss: 0.1591324657201767
train_iter_loss: 0.19116942584514618
train_iter_loss: 0.15296857059001923
train_iter_loss: 0.1296849101781845
train_iter_loss: 0.21303331851959229
train_iter_loss: 0.19715143740177155
train_iter_loss: 0.09725508838891983
train loss :0.1621
---------------------
Validation seg loss: 0.22082492610755958 at epoch 545
epoch =    546/  1000, exp = train
train_iter_loss: 0.17865997552871704
train_iter_loss: 0.21336522698402405
train_iter_loss: 0.1722816675901413
train_iter_loss: 0.15572434663772583
train_iter_loss: 0.11702115833759308
train_iter_loss: 0.14242997765541077
train_iter_loss: 0.21168573200702667
train_iter_loss: 0.21266084909439087
train_iter_loss: 0.07396142929792404
train_iter_loss: 0.07482747733592987
train_iter_loss: 0.10918175429105759
train_iter_loss: 0.14091113209724426
train_iter_loss: 0.1664716899394989
train_iter_loss: 0.11957748979330063
train_iter_loss: 0.08395537734031677
train_iter_loss: 0.18895377218723297
train_iter_loss: 0.11561819911003113
train_iter_loss: 0.14452826976776123
train_iter_loss: 0.11518898606300354
train_iter_loss: 0.1510085016489029
train_iter_loss: 0.05818328261375427
train_iter_loss: 0.2758398652076721
train_iter_loss: 0.18211272358894348
train_iter_loss: 0.12863826751708984
train_iter_loss: 0.12083195894956589
train_iter_loss: 0.2924186587333679
train_iter_loss: 0.2466825544834137
train_iter_loss: 0.07259443402290344
train_iter_loss: 0.20744994282722473
train_iter_loss: 0.1261063814163208
train_iter_loss: 0.16010966897010803
train_iter_loss: 0.09192631393671036
train_iter_loss: 0.10296548902988434
train_iter_loss: 0.2635239064693451
train_iter_loss: 0.21317721903324127
train_iter_loss: 0.13453388214111328
train_iter_loss: 0.13377152383327484
train_iter_loss: 0.3159995675086975
train_iter_loss: 0.17252874374389648
train_iter_loss: 0.11309925466775894
train_iter_loss: 0.18002094328403473
train_iter_loss: 0.10920774191617966
train_iter_loss: 0.05832807719707489
train_iter_loss: 0.14038854837417603
train_iter_loss: 0.12703830003738403
train_iter_loss: 0.11863487213850021
train_iter_loss: 0.19866061210632324
train_iter_loss: 0.3140338957309723
train_iter_loss: 0.11445552855730057
train_iter_loss: 0.20475544035434723
train_iter_loss: 0.15730664134025574
train_iter_loss: 0.23619712889194489
train_iter_loss: 0.08671001344919205
train_iter_loss: 0.15444980561733246
train_iter_loss: 0.23364180326461792
train_iter_loss: 0.14135733246803284
train_iter_loss: 0.1537788063287735
train_iter_loss: 0.100029855966568
train_iter_loss: 0.21228982508182526
train_iter_loss: 0.10471393913030624
train_iter_loss: 0.1049778088927269
train_iter_loss: 0.14046043157577515
train_iter_loss: 0.11827220022678375
train_iter_loss: 0.2208748757839203
train_iter_loss: 0.20212866365909576
train_iter_loss: 0.11663114279508591
train_iter_loss: 0.15887990593910217
train_iter_loss: 0.09510406106710434
train_iter_loss: 0.2093055695295334
train_iter_loss: 0.2168397754430771
train_iter_loss: 0.08155285567045212
train_iter_loss: 0.17457923293113708
train_iter_loss: 0.1395982950925827
train_iter_loss: 0.22870424389839172
train_iter_loss: 0.2552058696746826
train_iter_loss: 0.1578153669834137
train_iter_loss: 0.055081263184547424
train_iter_loss: 0.22934682667255402
train_iter_loss: 0.08200555294752121
train_iter_loss: 0.08909602463245392
train_iter_loss: 0.12240278720855713
train_iter_loss: 0.1997857242822647
train_iter_loss: 0.20066769421100616
train_iter_loss: 0.22543197870254517
train_iter_loss: 0.126116082072258
train_iter_loss: 0.08848489075899124
train_iter_loss: 0.1490621268749237
train_iter_loss: 0.1246115192770958
train_iter_loss: 0.14765405654907227
train_iter_loss: 0.16415634751319885
train_iter_loss: 0.11403626203536987
train_iter_loss: 0.24446247518062592
train_iter_loss: 0.08636229485273361
train_iter_loss: 0.14001698791980743
train_iter_loss: 0.1524643748998642
train_iter_loss: 0.20249642431735992
train_iter_loss: 0.1673865020275116
train_iter_loss: 0.10346394777297974
train_iter_loss: 0.1223059594631195
train_iter_loss: 0.136491060256958
train loss :0.1570
---------------------
Validation seg loss: 0.21856761609537984 at epoch 546
epoch =    547/  1000, exp = train
train_iter_loss: 0.03549354895949364
train_iter_loss: 0.11743856966495514
train_iter_loss: 0.1970285028219223
train_iter_loss: 0.1678636521100998
train_iter_loss: 0.20614777505397797
train_iter_loss: 0.16121114790439606
train_iter_loss: 0.1062672957777977
train_iter_loss: 0.09641231596469879
train_iter_loss: 0.08817729353904724
train_iter_loss: 0.24238823354244232
train_iter_loss: 0.1229209154844284
train_iter_loss: 0.1427978128194809
train_iter_loss: 0.11371781677007675
train_iter_loss: 0.30414146184921265
train_iter_loss: 0.19273826479911804
train_iter_loss: 0.12714152038097382
train_iter_loss: 0.2545680105686188
train_iter_loss: 0.13693849742412567
train_iter_loss: 0.11268940567970276
train_iter_loss: 0.2671501040458679
train_iter_loss: 0.12895409762859344
train_iter_loss: 0.13131842017173767
train_iter_loss: 0.11254717409610748
train_iter_loss: 0.1436900645494461
train_iter_loss: 0.05315043777227402
train_iter_loss: 0.10072942078113556
train_iter_loss: 0.09343139082193375
train_iter_loss: 0.17060312628746033
train_iter_loss: 0.30631157755851746
train_iter_loss: 0.13399620354175568
train_iter_loss: 0.1519700288772583
train_iter_loss: 0.15088266134262085
train_iter_loss: 0.22714954614639282
train_iter_loss: 0.15986669063568115
train_iter_loss: 0.17744140326976776
train_iter_loss: 0.2284993678331375
train_iter_loss: 0.15933530032634735
train_iter_loss: 0.25888270139694214
train_iter_loss: 0.20240168273448944
train_iter_loss: 0.19006434082984924
train_iter_loss: 0.11898450553417206
train_iter_loss: 0.15303371846675873
train_iter_loss: 0.23783697187900543
train_iter_loss: 0.1947643905878067
train_iter_loss: 0.21403691172599792
train_iter_loss: 0.15129579603672028
train_iter_loss: 0.3396081030368805
train_iter_loss: 0.14868426322937012
train_iter_loss: 0.15374958515167236
train_iter_loss: 0.11792074888944626
train_iter_loss: 0.1443006843328476
train_iter_loss: 0.14658257365226746
train_iter_loss: 0.20812979340553284
train_iter_loss: 0.1834781914949417
train_iter_loss: 0.2600597143173218
train_iter_loss: 0.21576568484306335
train_iter_loss: 0.1655566543340683
train_iter_loss: 0.213611900806427
train_iter_loss: 0.11939609795808792
train_iter_loss: 0.2124631553888321
train_iter_loss: 0.13328444957733154
train_iter_loss: 0.18650193512439728
train_iter_loss: 0.10750502347946167
train_iter_loss: 0.24130098521709442
train_iter_loss: 0.19271701574325562
train_iter_loss: 0.20447976887226105
train_iter_loss: 0.18317075073719025
train_iter_loss: 0.1507972627878189
train_iter_loss: 0.11335129290819168
train_iter_loss: 0.08543898165225983
train_iter_loss: 0.25466451048851013
train_iter_loss: 0.3336236774921417
train_iter_loss: 0.12488864362239838
train_iter_loss: 0.18293097615242004
train_iter_loss: 0.07264066487550735
train_iter_loss: 0.1350501924753189
train_iter_loss: 0.11968900263309479
train_iter_loss: 0.10237564146518707
train_iter_loss: 0.1819191575050354
train_iter_loss: 0.17858944833278656
train_iter_loss: 0.09609603136777878
train_iter_loss: 0.1769331842660904
train_iter_loss: 0.11626464873552322
train_iter_loss: 0.24542544782161713
train_iter_loss: 0.13114044070243835
train_iter_loss: 0.163765549659729
train_iter_loss: 0.18856871128082275
train_iter_loss: 0.17851245403289795
train_iter_loss: 0.13200250267982483
train_iter_loss: 0.09756002575159073
train_iter_loss: 0.06268823891878128
train_iter_loss: 0.09444357454776764
train_iter_loss: 0.0655263215303421
train_iter_loss: 0.15065567195415497
train_iter_loss: 0.0698072761297226
train_iter_loss: 0.12357605248689651
train_iter_loss: 0.09280523657798767
train_iter_loss: 0.2690545916557312
train_iter_loss: 0.03761346638202667
train_iter_loss: 0.13990208506584167
train loss :0.1625
---------------------
Validation seg loss: 0.21807731540416772 at epoch 547
epoch =    548/  1000, exp = train
train_iter_loss: 0.09389164298772812
train_iter_loss: 0.12361446768045425
train_iter_loss: 0.31230488419532776
train_iter_loss: 0.061621759086847305
train_iter_loss: 0.15489493310451508
train_iter_loss: 0.16269463300704956
train_iter_loss: 0.30222055315971375
train_iter_loss: 0.23975427448749542
train_iter_loss: 0.10925529152154922
train_iter_loss: 0.18536728620529175
train_iter_loss: 0.1424301415681839
train_iter_loss: 0.07174253463745117
train_iter_loss: 0.1276785433292389
train_iter_loss: 0.15666885673999786
train_iter_loss: 0.10584384948015213
train_iter_loss: 0.20410732924938202
train_iter_loss: 0.06864135712385178
train_iter_loss: 0.14880138635635376
train_iter_loss: 0.1269931048154831
train_iter_loss: 0.1887378990650177
train_iter_loss: 0.08246677368879318
train_iter_loss: 0.26718273758888245
train_iter_loss: 0.20629553496837616
train_iter_loss: 0.1322026252746582
train_iter_loss: 0.07031486928462982
train_iter_loss: 0.27251991629600525
train_iter_loss: 0.19877225160598755
train_iter_loss: 0.10501520335674286
train_iter_loss: 0.1767454892396927
train_iter_loss: 0.08325211703777313
train_iter_loss: 0.06940768659114838
train_iter_loss: 0.3036278486251831
train_iter_loss: 0.0942726582288742
train_iter_loss: 0.2105317860841751
train_iter_loss: 0.12966273725032806
train_iter_loss: 0.17608511447906494
train_iter_loss: 0.2134093940258026
train_iter_loss: 0.22339098155498505
train_iter_loss: 0.15431132912635803
train_iter_loss: 0.09135201573371887
train_iter_loss: 0.0674053430557251
train_iter_loss: 0.1357409656047821
train_iter_loss: 0.07872798293828964
train_iter_loss: 0.060269661247730255
train_iter_loss: 0.09808829426765442
train_iter_loss: 0.14986218512058258
train_iter_loss: 0.19429509341716766
train_iter_loss: 0.1785946637392044
train_iter_loss: 0.1488536149263382
train_iter_loss: 0.12678015232086182
train_iter_loss: 0.21183177828788757
train_iter_loss: 0.21542075276374817
train_iter_loss: 0.20037227869033813
train_iter_loss: 0.13088631629943848
train_iter_loss: 0.14497189223766327
train_iter_loss: 0.1516014039516449
train_iter_loss: 0.1519283652305603
train_iter_loss: 0.15605975687503815
train_iter_loss: 0.16468051075935364
train_iter_loss: 0.15890301764011383
train_iter_loss: 0.2332354635000229
train_iter_loss: 0.10878325998783112
train_iter_loss: 0.17378927767276764
train_iter_loss: 0.18573187291622162
train_iter_loss: 0.21506786346435547
train_iter_loss: 0.07849175482988358
train_iter_loss: 0.1039365828037262
train_iter_loss: 0.24187953770160675
train_iter_loss: 0.07180841267108917
train_iter_loss: 0.08358394354581833
train_iter_loss: 0.29615673422813416
train_iter_loss: 0.14976172149181366
train_iter_loss: 0.09057891368865967
train_iter_loss: 0.20976798236370087
train_iter_loss: 0.12381670624017715
train_iter_loss: 0.08448188751935959
train_iter_loss: 0.20260633528232574
train_iter_loss: 0.08936400711536407
train_iter_loss: 0.20721325278282166
train_iter_loss: 0.1483476459980011
train_iter_loss: 0.28555458784103394
train_iter_loss: 0.12003926932811737
train_iter_loss: 0.19624385237693787
train_iter_loss: 0.12341992557048798
train_iter_loss: 0.1599319726228714
train_iter_loss: 0.1487991064786911
train_iter_loss: 0.2319549173116684
train_iter_loss: 0.2092835009098053
train_iter_loss: 0.21879899501800537
train_iter_loss: 0.10823339223861694
train_iter_loss: 0.14832329750061035
train_iter_loss: 0.08525703847408295
train_iter_loss: 0.16707009077072144
train_iter_loss: 0.17027728259563446
train_iter_loss: 0.13140586018562317
train_iter_loss: 0.14936624467372894
train_iter_loss: 0.15283119678497314
train_iter_loss: 0.17586980760097504
train_iter_loss: 0.12038937211036682
train_iter_loss: 0.2034948766231537
train loss :0.1584
---------------------
Validation seg loss: 0.21728834979903866 at epoch 548
epoch =    549/  1000, exp = train
train_iter_loss: 0.10290402173995972
train_iter_loss: 0.18341781198978424
train_iter_loss: 0.11192165315151215
train_iter_loss: 0.20755188167095184
train_iter_loss: 0.15928122401237488
train_iter_loss: 0.16472961008548737
train_iter_loss: 0.11043019592761993
train_iter_loss: 0.10953440517187119
train_iter_loss: 0.1069200187921524
train_iter_loss: 0.1738760620355606
train_iter_loss: 0.15822996199131012
train_iter_loss: 0.17734511196613312
train_iter_loss: 0.20528879761695862
train_iter_loss: 0.13286641240119934
train_iter_loss: 0.23210155963897705
train_iter_loss: 0.11551378667354584
train_iter_loss: 0.2942890226840973
train_iter_loss: 0.2795780599117279
train_iter_loss: 0.13428129255771637
train_iter_loss: 0.1811838150024414
train_iter_loss: 0.12404489517211914
train_iter_loss: 0.09511037170886993
train_iter_loss: 0.06646540015935898
train_iter_loss: 0.1523115634918213
train_iter_loss: 0.08764861524105072
train_iter_loss: 0.13581499457359314
train_iter_loss: 0.12032424658536911
train_iter_loss: 0.1905728429555893
train_iter_loss: 0.11430646479129791
train_iter_loss: 0.1188320517539978
train_iter_loss: 0.1473761647939682
train_iter_loss: 0.23494042456150055
train_iter_loss: 0.10743661969900131
train_iter_loss: 0.1063370481133461
train_iter_loss: 0.17904168367385864
train_iter_loss: 0.18083332479000092
train_iter_loss: 0.17818529903888702
train_iter_loss: 0.17462629079818726
train_iter_loss: 0.19791145622730255
train_iter_loss: 0.11918589472770691
train_iter_loss: 0.11811814457178116
train_iter_loss: 0.2927510738372803
train_iter_loss: 0.1725660264492035
train_iter_loss: 0.1674501597881317
train_iter_loss: 0.13570722937583923
train_iter_loss: 0.09859153628349304
train_iter_loss: 0.1712421327829361
train_iter_loss: 0.1909690499305725
train_iter_loss: 0.16604083776474
train_iter_loss: 0.14169467985630035
train_iter_loss: 0.10197991877794266
train_iter_loss: 0.16154339909553528
train_iter_loss: 0.2674611210823059
train_iter_loss: 0.11218921840190887
train_iter_loss: 0.28819429874420166
train_iter_loss: 0.26952457427978516
train_iter_loss: 0.2992694675922394
train_iter_loss: 0.1257064789533615
train_iter_loss: 0.2555806338787079
train_iter_loss: 0.14472660422325134
train_iter_loss: 0.19632288813591003
train_iter_loss: 0.0509178452193737
train_iter_loss: 0.1709665060043335
train_iter_loss: 0.34494492411613464
train_iter_loss: 0.12332173436880112
train_iter_loss: 0.17216457426548004
train_iter_loss: 0.14162997901439667
train_iter_loss: 0.24630916118621826
train_iter_loss: 0.14934737980365753
train_iter_loss: 0.18505212664604187
train_iter_loss: 0.0937512069940567
train_iter_loss: 0.18215060234069824
train_iter_loss: 0.13079746067523956
train_iter_loss: 0.11516140401363373
train_iter_loss: 0.07586564123630524
train_iter_loss: 0.2335302084684372
train_iter_loss: 0.1021348387002945
train_iter_loss: 0.1168990507721901
train_iter_loss: 0.1592521220445633
train_iter_loss: 0.15398167073726654
train_iter_loss: 0.0363028384745121
train_iter_loss: 0.13054518401622772
train_iter_loss: 0.09809200465679169
train_iter_loss: 0.2419339120388031
train_iter_loss: 0.09809800982475281
train_iter_loss: 0.35486188530921936
train_iter_loss: 0.20983663201332092
train_iter_loss: 0.1459638774394989
train_iter_loss: 0.025584863498806953
train_iter_loss: 0.3079819083213806
train_iter_loss: 0.21546979248523712
train_iter_loss: 0.09468120336532593
train_iter_loss: 0.4300864040851593
train_iter_loss: 0.04813201725482941
train_iter_loss: 0.13054901361465454
train_iter_loss: 0.1417074352502823
train_iter_loss: 0.1892739236354828
train_iter_loss: 0.2184540331363678
train_iter_loss: 0.14255675673484802
train_iter_loss: 0.09715934842824936
train loss :0.1651
---------------------
Validation seg loss: 0.21721478333731867 at epoch 549
epoch =    550/  1000, exp = train
train_iter_loss: 0.19575135409832
train_iter_loss: 0.1327485293149948
train_iter_loss: 0.117324098944664
train_iter_loss: 0.13732454180717468
train_iter_loss: 0.2323196977376938
train_iter_loss: 0.09655904769897461
train_iter_loss: 0.22998686134815216
train_iter_loss: 0.1715148687362671
train_iter_loss: 0.1677386611700058
train_iter_loss: 0.10791514068841934
train_iter_loss: 0.15769077837467194
train_iter_loss: 0.16625158488750458
train_iter_loss: 0.10122933238744736
train_iter_loss: 0.2042420506477356
train_iter_loss: 0.13786722719669342
train_iter_loss: 0.1880294680595398
train_iter_loss: 0.09932907670736313
train_iter_loss: 0.21333648264408112
train_iter_loss: 0.17771387100219727
train_iter_loss: 0.16010788083076477
train_iter_loss: 0.17039898037910461
train_iter_loss: 0.09646999835968018
train_iter_loss: 0.15367837250232697
train_iter_loss: 0.17828433215618134
train_iter_loss: 0.15400348603725433
train_iter_loss: 0.2501366436481476
train_iter_loss: 0.18796613812446594
train_iter_loss: 0.22935587167739868
train_iter_loss: 0.12595674395561218
train_iter_loss: 0.15765635669231415
train_iter_loss: 0.14512687921524048
train_iter_loss: 0.21669046580791473
train_iter_loss: 0.13900434970855713
train_iter_loss: 0.16492362320423126
train_iter_loss: 0.1541300117969513
train_iter_loss: 0.22493258118629456
train_iter_loss: 0.06871150434017181
train_iter_loss: 0.14543595910072327
train_iter_loss: 0.16357910633087158
train_iter_loss: 0.1420828104019165
train_iter_loss: 0.20802664756774902
train_iter_loss: 0.1276405304670334
train_iter_loss: 0.18774046003818512
train_iter_loss: 0.16974899172782898
train_iter_loss: 0.07997138798236847
train_iter_loss: 0.1137949749827385
train_iter_loss: 0.1909966617822647
train_iter_loss: 0.21699315309524536
train_iter_loss: 0.06034516915678978
train_iter_loss: 0.14622162282466888
train_iter_loss: 0.1157272681593895
train_iter_loss: 0.13110420107841492
train_iter_loss: 0.09126449376344681
train_iter_loss: 0.15920639038085938
train_iter_loss: 0.1590437889099121
train_iter_loss: 0.21346360445022583
train_iter_loss: 0.22621716558933258
train_iter_loss: 0.19166328012943268
train_iter_loss: 0.12058337777853012
train_iter_loss: 0.08739559352397919
train_iter_loss: 0.5040109753608704
train_iter_loss: 0.14126893877983093
train_iter_loss: 0.09213221818208694
train_iter_loss: 0.16722095012664795
train_iter_loss: 0.08926691114902496
train_iter_loss: 0.17088107764720917
train_iter_loss: 0.10106312483549118
train_iter_loss: 0.07319524884223938
train_iter_loss: 0.13302309811115265
train_iter_loss: 0.1061011478304863
train_iter_loss: 0.16560059785842896
train_iter_loss: 0.09485611319541931
train_iter_loss: 0.18749094009399414
train_iter_loss: 0.21156416833400726
train_iter_loss: 0.197407528758049
train_iter_loss: 0.10475457459688187
train_iter_loss: 0.2146509885787964
train_iter_loss: 0.2544987201690674
train_iter_loss: 0.11029859632253647
train_iter_loss: 0.13936997950077057
train_iter_loss: 0.08867451548576355
train_iter_loss: 0.12128958106040955
train_iter_loss: 0.23411738872528076
train_iter_loss: 0.15047043561935425
train_iter_loss: 0.13603682816028595
train_iter_loss: 0.2261638194322586
train_iter_loss: 0.08844731748104095
train_iter_loss: 0.15694069862365723
train_iter_loss: 0.277569979429245
train_iter_loss: 0.0968693345785141
train_iter_loss: 0.18480458855628967
train_iter_loss: 0.14344225823879242
train_iter_loss: 0.031553469598293304
train_iter_loss: 0.05655241757631302
train_iter_loss: 0.20989850163459778
train_iter_loss: 0.21104706823825836
train_iter_loss: 0.12973107397556305
train_iter_loss: 0.16032376885414124
train_iter_loss: 0.06916522234678268
train_iter_loss: 0.22464266419410706
train loss :0.1588
---------------------
Validation seg loss: 0.22045299524160208 at epoch 550
epoch =    551/  1000, exp = train
train_iter_loss: 0.2054244577884674
train_iter_loss: 0.16540127992630005
train_iter_loss: 0.04586951807141304
train_iter_loss: 0.16522525250911713
train_iter_loss: 0.23434941470623016
train_iter_loss: 0.2957317531108856
train_iter_loss: 0.276201456785202
train_iter_loss: 0.09874580800533295
train_iter_loss: 0.14562587440013885
train_iter_loss: 0.10302114486694336
train_iter_loss: 0.2106511890888214
train_iter_loss: 0.22079291939735413
train_iter_loss: 0.2781464755535126
train_iter_loss: 0.08274908363819122
train_iter_loss: 0.31673765182495117
train_iter_loss: 0.1127871498465538
train_iter_loss: 0.1583685278892517
train_iter_loss: 0.15405632555484772
train_iter_loss: 0.27136364579200745
train_iter_loss: 0.12451328337192535
train_iter_loss: 0.2225053459405899
train_iter_loss: 0.1809409111738205
train_iter_loss: 0.20993323624134064
train_iter_loss: 0.09017566591501236
train_iter_loss: 0.09889885038137436
train_iter_loss: 0.1363372653722763
train_iter_loss: 0.32823821902275085
train_iter_loss: 0.10596737265586853
train_iter_loss: 0.25727158784866333
train_iter_loss: 0.2160121351480484
train_iter_loss: 0.13716264069080353
train_iter_loss: 0.12005776166915894
train_iter_loss: 0.0733710303902626
train_iter_loss: 0.14956335723400116
train_iter_loss: 0.09100665152072906
train_iter_loss: 0.12634801864624023
train_iter_loss: 0.13753458857536316
train_iter_loss: 0.08103252947330475
train_iter_loss: 0.06819725036621094
train_iter_loss: 0.2057914435863495
train_iter_loss: 0.07765382528305054
train_iter_loss: 0.12698635458946228
train_iter_loss: 0.09835579246282578
train_iter_loss: 0.16757629811763763
train_iter_loss: 0.1129043847322464
train_iter_loss: 0.1943097561597824
train_iter_loss: 0.2228439599275589
train_iter_loss: 0.1592995673418045
train_iter_loss: 0.1338740438222885
train_iter_loss: 0.1256571114063263
train_iter_loss: 0.22875669598579407
train_iter_loss: 0.14813046157360077
train_iter_loss: 0.10684507340192795
train_iter_loss: 0.1961144506931305
train_iter_loss: 0.13490037620067596
train_iter_loss: 0.28896957635879517
train_iter_loss: 0.12513820827007294
train_iter_loss: 0.19814462959766388
train_iter_loss: 0.2642820179462433
train_iter_loss: 0.12341243028640747
train_iter_loss: 0.16728423535823822
train_iter_loss: 0.3029738962650299
train_iter_loss: 0.12048449367284775
train_iter_loss: 0.18412666022777557
train_iter_loss: 0.08625602722167969
train_iter_loss: 0.14651982486248016
train_iter_loss: 0.23618316650390625
train_iter_loss: 0.07638222724199295
train_iter_loss: 0.1392340362071991
train_iter_loss: 0.1087275967001915
train_iter_loss: 0.0689181238412857
train_iter_loss: 0.12291824817657471
train_iter_loss: 0.33731386065483093
train_iter_loss: 0.15258485078811646
train_iter_loss: 0.10622938722372055
train_iter_loss: 0.36037397384643555
train_iter_loss: 0.1370372325181961
train_iter_loss: 0.14843279123306274
train_iter_loss: 0.18438684940338135
train_iter_loss: 0.09395473450422287
train_iter_loss: 0.08134879171848297
train_iter_loss: 0.34809410572052
train_iter_loss: 0.1771741509437561
train_iter_loss: 0.12197408825159073
train_iter_loss: 0.11150570958852768
train_iter_loss: 0.14473414421081543
train_iter_loss: 0.11978908628225327
train_iter_loss: 0.20932650566101074
train_iter_loss: 0.0835094079375267
train_iter_loss: 0.1144588366150856
train_iter_loss: 0.1410892754793167
train_iter_loss: 0.18788497149944305
train_iter_loss: 0.2145550698041916
train_iter_loss: 0.07573772966861725
train_iter_loss: 0.1482633501291275
train_iter_loss: 0.15533368289470673
train_iter_loss: 0.17820905148983002
train_iter_loss: 0.26982465386390686
train_iter_loss: 0.21260014176368713
train_iter_loss: 0.15079787373542786
train loss :0.1662
---------------------
Validation seg loss: 0.21733581526907828 at epoch 551
epoch =    552/  1000, exp = train
train_iter_loss: 0.24952253699302673
train_iter_loss: 0.10084319114685059
train_iter_loss: 0.08787280321121216
train_iter_loss: 0.23013465106487274
train_iter_loss: 0.1778610497713089
train_iter_loss: 0.11469198018312454
train_iter_loss: 0.14625564217567444
train_iter_loss: 0.0925002470612526
train_iter_loss: 0.19958873093128204
train_iter_loss: 0.09939538687467575
train_iter_loss: 0.10404440760612488
train_iter_loss: 0.14992763102054596
train_iter_loss: 0.2746396064758301
train_iter_loss: 0.12733854353427887
train_iter_loss: 0.10878224670886993
train_iter_loss: 0.1880696415901184
train_iter_loss: 0.14613673090934753
train_iter_loss: 0.1270337998867035
train_iter_loss: 0.24902638792991638
train_iter_loss: 0.13713721930980682
train_iter_loss: 0.18396995961666107
train_iter_loss: 0.086954265832901
train_iter_loss: 0.13305582106113434
train_iter_loss: 0.142936110496521
train_iter_loss: 0.12728054821491241
train_iter_loss: 0.17820727825164795
train_iter_loss: 0.1842779815196991
train_iter_loss: 0.11697795987129211
train_iter_loss: 0.13207519054412842
train_iter_loss: 0.26458635926246643
train_iter_loss: 0.159679114818573
train_iter_loss: 0.18231229484081268
train_iter_loss: 0.28707990050315857
train_iter_loss: 0.1919270157814026
train_iter_loss: 0.17430151998996735
train_iter_loss: 0.26627227663993835
train_iter_loss: 0.18903790414333344
train_iter_loss: 0.13610348105430603
train_iter_loss: 0.09883008897304535
train_iter_loss: 0.12026433646678925
train_iter_loss: 0.14546988904476166
train_iter_loss: 0.22026026248931885
train_iter_loss: 0.1847284734249115
train_iter_loss: 0.20779730379581451
train_iter_loss: 0.12260138988494873
train_iter_loss: 0.15743178129196167
train_iter_loss: 0.1447710543870926
train_iter_loss: 0.1406468003988266
train_iter_loss: 0.11526879668235779
train_iter_loss: 0.1866738498210907
train_iter_loss: 0.11976387351751328
train_iter_loss: 0.30625373125076294
train_iter_loss: 0.19247782230377197
train_iter_loss: 0.24455681443214417
train_iter_loss: 0.19154639542102814
train_iter_loss: 0.13377538323402405
train_iter_loss: 0.22900710999965668
train_iter_loss: 0.07392766326665878
train_iter_loss: 0.15109893679618835
train_iter_loss: 0.1525527983903885
train_iter_loss: 0.06910911947488785
train_iter_loss: 0.17929111421108246
train_iter_loss: 0.13337162137031555
train_iter_loss: 0.12302885949611664
train_iter_loss: 0.046488456428050995
train_iter_loss: 0.18391694128513336
train_iter_loss: 0.08636477589607239
train_iter_loss: 0.1801188588142395
train_iter_loss: 0.11347075551748276
train_iter_loss: 0.18124181032180786
train_iter_loss: 0.13483695685863495
train_iter_loss: 0.12577465176582336
train_iter_loss: 0.09066374599933624
train_iter_loss: 0.14473851025104523
train_iter_loss: 0.12246573716402054
train_iter_loss: 0.1215253546833992
train_iter_loss: 0.14369283616542816
train_iter_loss: 0.11317453533411026
train_iter_loss: 0.19250687956809998
train_iter_loss: 0.119416743516922
train_iter_loss: 0.1713429093360901
train_iter_loss: 0.128983274102211
train_iter_loss: 0.05236491188406944
train_iter_loss: 0.24751122295856476
train_iter_loss: 0.23497402667999268
train_iter_loss: 0.1654668152332306
train_iter_loss: 0.16828042268753052
train_iter_loss: 0.11990993469953537
train_iter_loss: 0.17316289246082306
train_iter_loss: 0.1536293923854828
train_iter_loss: 0.13476279377937317
train_iter_loss: 0.09715043008327484
train_iter_loss: 0.2221936732530594
train_iter_loss: 0.13474266231060028
train_iter_loss: 0.11105664819478989
train_iter_loss: 0.1609400510787964
train_iter_loss: 0.11019855737686157
train_iter_loss: 0.10084298253059387
train_iter_loss: 0.17853687703609467
train_iter_loss: 0.1560455858707428
train loss :0.1568
---------------------
Validation seg loss: 0.21999397496077813 at epoch 552
epoch =    553/  1000, exp = train
train_iter_loss: 0.1385595202445984
train_iter_loss: 0.19348807632923126
train_iter_loss: 0.218009814620018
train_iter_loss: 0.09298644214868546
train_iter_loss: 0.1976475864648819
train_iter_loss: 0.08968666195869446
train_iter_loss: 0.1318490207195282
train_iter_loss: 0.11868003755807877
train_iter_loss: 0.19345557689666748
train_iter_loss: 0.2101389318704605
train_iter_loss: 0.0892864540219307
train_iter_loss: 0.1383783519268036
train_iter_loss: 0.09589428454637527
train_iter_loss: 0.0934571623802185
train_iter_loss: 0.2661075294017792
train_iter_loss: 0.21525226533412933
train_iter_loss: 0.10414912551641464
train_iter_loss: 0.08103639632463455
train_iter_loss: 0.17937438189983368
train_iter_loss: 0.17800509929656982
train_iter_loss: 0.14328688383102417
train_iter_loss: 0.20420807600021362
train_iter_loss: 0.10825122147798538
train_iter_loss: 0.1674162596464157
train_iter_loss: 0.18792057037353516
train_iter_loss: 0.2692045569419861
train_iter_loss: 0.10476380586624146
train_iter_loss: 0.29467856884002686
train_iter_loss: 0.16084259748458862
train_iter_loss: 0.24581493437290192
train_iter_loss: 0.1436699777841568
train_iter_loss: 0.18742559850215912
train_iter_loss: 0.17614397406578064
train_iter_loss: 0.1424669772386551
train_iter_loss: 0.1869792938232422
train_iter_loss: 0.23971477150917053
train_iter_loss: 0.14381268620491028
train_iter_loss: 0.18249289691448212
train_iter_loss: 0.22273986041545868
train_iter_loss: 0.2304668128490448
train_iter_loss: 0.05386495217680931
train_iter_loss: 0.129654198884964
train_iter_loss: 0.15491518378257751
train_iter_loss: 0.2238054722547531
train_iter_loss: 0.1570277363061905
train_iter_loss: 0.1415116935968399
train_iter_loss: 0.06294157356023788
train_iter_loss: 0.10076437145471573
train_iter_loss: 0.050172168761491776
train_iter_loss: 0.2096695899963379
train_iter_loss: 0.13569635152816772
train_iter_loss: 0.12446677684783936
train_iter_loss: 0.1641034185886383
train_iter_loss: 0.47292789816856384
train_iter_loss: 0.19252127408981323
train_iter_loss: 0.07696086913347244
train_iter_loss: 0.14954258501529694
train_iter_loss: 0.07269766926765442
train_iter_loss: 0.20396475493907928
train_iter_loss: 0.14060284197330475
train_iter_loss: 0.10492584854364395
train_iter_loss: 0.095033660531044
train_iter_loss: 0.11544007807970047
train_iter_loss: 0.1510363221168518
train_iter_loss: 0.1711547076702118
train_iter_loss: 0.10017247498035431
train_iter_loss: 0.11566434800624847
train_iter_loss: 0.2185683250427246
train_iter_loss: 0.19918912649154663
train_iter_loss: 0.2291308492422104
train_iter_loss: 0.1215275451540947
train_iter_loss: 0.15469597280025482
train_iter_loss: 0.0702880471944809
train_iter_loss: 0.13470785319805145
train_iter_loss: 0.18224067986011505
train_iter_loss: 0.15660299360752106
train_iter_loss: 0.13344934582710266
train_iter_loss: 0.3186758756637573
train_iter_loss: 0.09220387041568756
train_iter_loss: 0.1425705850124359
train_iter_loss: 0.17062227427959442
train_iter_loss: 0.220991849899292
train_iter_loss: 0.07765015214681625
train_iter_loss: 0.2688116431236267
train_iter_loss: 0.1593993902206421
train_iter_loss: 0.12717080116271973
train_iter_loss: 0.11810995638370514
train_iter_loss: 0.24612079560756683
train_iter_loss: 0.1434786468744278
train_iter_loss: 0.08962507545948029
train_iter_loss: 0.1628490388393402
train_iter_loss: 0.13839426636695862
train_iter_loss: 0.1285596638917923
train_iter_loss: 0.15113742649555206
train_iter_loss: 0.1350698471069336
train_iter_loss: 0.14912746846675873
train_iter_loss: 0.1229928582906723
train_iter_loss: 0.12276090681552887
train_iter_loss: 0.13593453168869019
train_iter_loss: 0.20295140147209167
train loss :0.1602
---------------------
Validation seg loss: 0.21907946130014816 at epoch 553
epoch =    554/  1000, exp = train
train_iter_loss: 0.16612184047698975
train_iter_loss: 0.11839316040277481
train_iter_loss: 0.12412454932928085
train_iter_loss: 0.2456599324941635
train_iter_loss: 0.2146427184343338
train_iter_loss: 0.14566637575626373
train_iter_loss: 0.11582177132368088
train_iter_loss: 0.10173672437667847
train_iter_loss: 0.19267410039901733
train_iter_loss: 0.11936146765947342
train_iter_loss: 0.07898809760808945
train_iter_loss: 0.27773717045783997
train_iter_loss: 0.14266860485076904
train_iter_loss: 0.24866075813770294
train_iter_loss: 0.14355650544166565
train_iter_loss: 0.030909769237041473
train_iter_loss: 0.15504923462867737
train_iter_loss: 0.14519472420215607
train_iter_loss: 0.08841404318809509
train_iter_loss: 0.11262011528015137
train_iter_loss: 0.1526980996131897
train_iter_loss: 0.19364961981773376
train_iter_loss: 0.08328548818826675
train_iter_loss: 0.1352059543132782
train_iter_loss: 0.1776905506849289
train_iter_loss: 0.14798317849636078
train_iter_loss: 0.2072010189294815
train_iter_loss: 0.2823813259601593
train_iter_loss: 0.11891622841358185
train_iter_loss: 0.05852194130420685
train_iter_loss: 0.0622260719537735
train_iter_loss: 0.20151902735233307
train_iter_loss: 0.0863538533449173
train_iter_loss: 0.2962489426136017
train_iter_loss: 0.17875675857067108
train_iter_loss: 0.21309882402420044
train_iter_loss: 0.10408592969179153
train_iter_loss: 0.2573356628417969
train_iter_loss: 0.11532613635063171
train_iter_loss: 0.14146307110786438
train_iter_loss: 0.05135409161448479
train_iter_loss: 0.17710867524147034
train_iter_loss: 0.20789088308811188
train_iter_loss: 0.1750447005033493
train_iter_loss: 0.15257495641708374
train_iter_loss: 0.1273180991411209
train_iter_loss: 0.17559528350830078
train_iter_loss: 0.07784999907016754
train_iter_loss: 0.1925046294927597
train_iter_loss: 0.20093221962451935
train_iter_loss: 0.14970828592777252
train_iter_loss: 0.09039382636547089
train_iter_loss: 0.07268567383289337
train_iter_loss: 0.13270007073879242
train_iter_loss: 0.3768932521343231
train_iter_loss: 0.2387848198413849
train_iter_loss: 0.26606181263923645
train_iter_loss: 0.09637819230556488
train_iter_loss: 0.1546468883752823
train_iter_loss: 0.17475228011608124
train_iter_loss: 0.1447572112083435
train_iter_loss: 0.18614384531974792
train_iter_loss: 0.14198698103427887
train_iter_loss: 0.20590351521968842
train_iter_loss: 0.14864268898963928
train_iter_loss: 0.0953177958726883
train_iter_loss: 0.23124997317790985
train_iter_loss: 0.15150749683380127
train_iter_loss: 0.32864174246788025
train_iter_loss: 0.13147366046905518
train_iter_loss: 0.1141536757349968
train_iter_loss: 0.14669764041900635
train_iter_loss: 0.08068870007991791
train_iter_loss: 0.11331336945295334
train_iter_loss: 0.09808600693941116
train_iter_loss: 0.08819598704576492
train_iter_loss: 0.0568024143576622
train_iter_loss: 0.09432604163885117
train_iter_loss: 0.2922183871269226
train_iter_loss: 0.04834064841270447
train_iter_loss: 0.123605877161026
train_iter_loss: 0.6123080849647522
train_iter_loss: 0.21623270213603973
train_iter_loss: 0.028816185891628265
train_iter_loss: 0.1086738109588623
train_iter_loss: 0.1666465699672699
train_iter_loss: 0.1782783418893814
train_iter_loss: 0.16585507988929749
train_iter_loss: 0.21692582964897156
train_iter_loss: 0.1403503566980362
train_iter_loss: 0.15730102360248566
train_iter_loss: 0.15287324786186218
train_iter_loss: 0.17154410481452942
train_iter_loss: 0.20964406430721283
train_iter_loss: 0.27874523401260376
train_iter_loss: 0.10678915679454803
train_iter_loss: 0.16505534946918488
train_iter_loss: 0.23248836398124695
train_iter_loss: 0.2055945247411728
train_iter_loss: 0.26248031854629517
train loss :0.1643
---------------------
Validation seg loss: 0.22178492333986005 at epoch 554
epoch =    555/  1000, exp = train
train_iter_loss: 0.10711218416690826
train_iter_loss: 0.17295663058757782
train_iter_loss: 0.1508447676897049
train_iter_loss: 0.1772155910730362
train_iter_loss: 0.23565687239170074
train_iter_loss: 0.1430598944425583
train_iter_loss: 0.136854887008667
train_iter_loss: 0.10046238452196121
train_iter_loss: 0.06366845965385437
train_iter_loss: 0.2594190239906311
train_iter_loss: 0.18882639706134796
train_iter_loss: 0.08787032961845398
train_iter_loss: 0.1951109766960144
train_iter_loss: 0.1455608308315277
train_iter_loss: 0.0990787222981453
train_iter_loss: 0.11887925118207932
train_iter_loss: 0.142937570810318
train_iter_loss: 0.11393903195858002
train_iter_loss: 0.18875989317893982
train_iter_loss: 0.12486713379621506
train_iter_loss: 0.16624565422534943
train_iter_loss: 0.15643133223056793
train_iter_loss: 0.18472516536712646
train_iter_loss: 0.14252670109272003
train_iter_loss: 0.1382715404033661
train_iter_loss: 0.19280286133289337
train_iter_loss: 0.11479317396879196
train_iter_loss: 0.18822897970676422
train_iter_loss: 0.0534069761633873
train_iter_loss: 0.169797882437706
train_iter_loss: 0.20339235663414001
train_iter_loss: 0.060797069221735
train_iter_loss: 0.10184311866760254
train_iter_loss: 0.14585615694522858
train_iter_loss: 0.19061458110809326
train_iter_loss: 0.1904497891664505
train_iter_loss: 0.12571187317371368
train_iter_loss: 0.18943078815937042
train_iter_loss: 0.1408246010541916
train_iter_loss: 0.06849031150341034
train_iter_loss: 0.24097630381584167
train_iter_loss: 0.19029229879379272
train_iter_loss: 0.09994638711214066
train_iter_loss: 0.18653394281864166
train_iter_loss: 0.21718370914459229
train_iter_loss: 0.17149488627910614
train_iter_loss: 0.20175579190254211
train_iter_loss: 0.18871736526489258
train_iter_loss: 0.14036628603935242
train_iter_loss: 0.09569758176803589
train_iter_loss: 0.12244517356157303
train_iter_loss: 0.15609651803970337
train_iter_loss: 0.0667492002248764
train_iter_loss: 0.1113487109541893
train_iter_loss: 0.17584572732448578
train_iter_loss: 0.163070946931839
train_iter_loss: 0.17422254383563995
train_iter_loss: 0.37775716185569763
train_iter_loss: 0.13822272419929504
train_iter_loss: 0.181230366230011
train_iter_loss: 0.2049345076084137
train_iter_loss: 0.3992276191711426
train_iter_loss: 0.15471522510051727
train_iter_loss: 0.2160266786813736
train_iter_loss: 0.18014998733997345
train_iter_loss: 0.18520286679267883
train_iter_loss: 0.3384438753128052
train_iter_loss: 0.15593107044696808
train_iter_loss: 0.14856581389904022
train_iter_loss: 0.1033545583486557
train_iter_loss: 0.1415630280971527
train_iter_loss: 0.10848653316497803
train_iter_loss: 0.20485961437225342
train_iter_loss: 0.13206687569618225
train_iter_loss: 0.0873996838927269
train_iter_loss: 0.21634028851985931
train_iter_loss: 0.09847070276737213
train_iter_loss: 0.18697454035282135
train_iter_loss: 0.14831076562404633
train_iter_loss: 0.1637141853570938
train_iter_loss: 0.11707399040460587
train_iter_loss: 0.057754337787628174
train_iter_loss: 0.06445293873548508
train_iter_loss: 0.08758604526519775
train_iter_loss: 0.17491111159324646
train_iter_loss: 0.11483973264694214
train_iter_loss: 0.12245114892721176
train_iter_loss: 0.2753748595714569
train_iter_loss: 0.17072118818759918
train_iter_loss: 0.18710112571716309
train_iter_loss: 0.1678914576768875
train_iter_loss: 0.1440984010696411
train_iter_loss: 0.2342458963394165
train_iter_loss: 0.1198873370885849
train_iter_loss: 0.1463198959827423
train_iter_loss: 0.14409594237804413
train_iter_loss: 0.12513045966625214
train_iter_loss: 0.07077332586050034
train_iter_loss: 0.1780269593000412
train_iter_loss: 0.1863243132829666
train loss :0.1591
---------------------
Validation seg loss: 0.2157138477712167 at epoch 555
epoch =    556/  1000, exp = train
train_iter_loss: 0.26644688844680786
train_iter_loss: 0.19740885496139526
train_iter_loss: 0.08526211977005005
train_iter_loss: 0.05746657773852348
train_iter_loss: 0.21268750727176666
train_iter_loss: 0.09512697905302048
train_iter_loss: 0.14252224564552307
train_iter_loss: 0.20076897740364075
train_iter_loss: 0.1758296638727188
train_iter_loss: 0.11704538017511368
train_iter_loss: 0.12267643213272095
train_iter_loss: 0.21186277270317078
train_iter_loss: 0.41313230991363525
train_iter_loss: 0.07527017593383789
train_iter_loss: 0.18233655393123627
train_iter_loss: 0.12645114958286285
train_iter_loss: 0.1782834678888321
train_iter_loss: 0.13046814501285553
train_iter_loss: 0.1330423653125763
train_iter_loss: 0.258012592792511
train_iter_loss: 0.12644493579864502
train_iter_loss: 0.09984652698040009
train_iter_loss: 0.06475865095853806
train_iter_loss: 0.07874386757612228
train_iter_loss: 0.12026733160018921
train_iter_loss: 0.21346427500247955
train_iter_loss: 0.36140474677085876
train_iter_loss: 0.12292952835559845
train_iter_loss: 0.12705408036708832
train_iter_loss: 0.2344324141740799
train_iter_loss: 0.0661056637763977
train_iter_loss: 0.11858843266963959
train_iter_loss: 0.10200902074575424
train_iter_loss: 0.07823380082845688
train_iter_loss: 0.06825132668018341
train_iter_loss: 0.22569112479686737
train_iter_loss: 0.12955734133720398
train_iter_loss: 0.1361805498600006
train_iter_loss: 0.19711829721927643
train_iter_loss: 0.14167551696300507
train_iter_loss: 0.11573056131601334
train_iter_loss: 0.3363310694694519
train_iter_loss: 0.17125602066516876
train_iter_loss: 0.2107740193605423
train_iter_loss: 0.19614283740520477
train_iter_loss: 0.1387040764093399
train_iter_loss: 0.12475994229316711
train_iter_loss: 0.1431199461221695
train_iter_loss: 0.12723761796951294
train_iter_loss: 0.15943193435668945
train_iter_loss: 0.1219155341386795
train_iter_loss: 0.11951103061437607
train_iter_loss: 0.21110175549983978
train_iter_loss: 0.22310884296894073
train_iter_loss: 0.11864826828241348
train_iter_loss: 0.24695616960525513
train_iter_loss: 0.1335207223892212
train_iter_loss: 0.14460058510303497
train_iter_loss: 0.17955036461353302
train_iter_loss: 0.1168968677520752
train_iter_loss: 0.33653050661087036
train_iter_loss: 0.0997605174779892
train_iter_loss: 0.16305923461914062
train_iter_loss: 0.20884458720684052
train_iter_loss: 0.3575669229030609
train_iter_loss: 0.05691372603178024
train_iter_loss: 0.2212514728307724
train_iter_loss: 0.11753831058740616
train_iter_loss: 0.2514060139656067
train_iter_loss: 0.1903972625732422
train_iter_loss: 0.16533789038658142
train_iter_loss: 0.1568576544523239
train_iter_loss: 0.1151912584900856
train_iter_loss: 0.16065290570259094
train_iter_loss: 0.2130291908979416
train_iter_loss: 0.14538449048995972
train_iter_loss: 0.19460183382034302
train_iter_loss: 0.10266967117786407
train_iter_loss: 0.1780439019203186
train_iter_loss: 0.2077236920595169
train_iter_loss: 0.11676522344350815
train_iter_loss: 0.05341324955224991
train_iter_loss: 0.13675999641418457
train_iter_loss: 0.13053002953529358
train_iter_loss: 0.21837691962718964
train_iter_loss: 0.10749061405658722
train_iter_loss: 0.12193969637155533
train_iter_loss: 0.1666017770767212
train_iter_loss: 0.21942313015460968
train_iter_loss: 0.18896645307540894
train_iter_loss: 0.09199891984462738
train_iter_loss: 0.1740388572216034
train_iter_loss: 0.0868234783411026
train_iter_loss: 0.13386335968971252
train_iter_loss: 0.09347613155841827
train_iter_loss: 0.14820565283298492
train_iter_loss: 0.10577858239412308
train_iter_loss: 0.1875753104686737
train_iter_loss: 0.13081513345241547
train_iter_loss: 0.1285981982946396
train loss :0.1608
---------------------
Validation seg loss: 0.21610847316717482 at epoch 556
epoch =    557/  1000, exp = train
train_iter_loss: 0.3349199593067169
train_iter_loss: 0.12358497083187103
train_iter_loss: 0.10309591144323349
train_iter_loss: 0.2381550818681717
train_iter_loss: 0.20726226270198822
train_iter_loss: 0.21986982226371765
train_iter_loss: 0.14839738607406616
train_iter_loss: 0.15890571475028992
train_iter_loss: 0.1583189070224762
train_iter_loss: 0.09130273014307022
train_iter_loss: 0.2871062457561493
train_iter_loss: 0.09671637415885925
train_iter_loss: 0.2304980307817459
train_iter_loss: 0.21680505573749542
train_iter_loss: 0.2557416558265686
train_iter_loss: 0.036798518151044846
train_iter_loss: 0.1280115693807602
train_iter_loss: 0.14879059791564941
train_iter_loss: 0.17428728938102722
train_iter_loss: 0.15096956491470337
train_iter_loss: 0.1414942443370819
train_iter_loss: 0.15636922419071198
train_iter_loss: 0.0996420755982399
train_iter_loss: 0.2227078378200531
train_iter_loss: 0.25573790073394775
train_iter_loss: 0.1365058720111847
train_iter_loss: 0.18509282171726227
train_iter_loss: 0.20916661620140076
train_iter_loss: 0.19636866450309753
train_iter_loss: 0.15922683477401733
train_iter_loss: 0.1036641076207161
train_iter_loss: 0.08858782798051834
train_iter_loss: 0.11930247396230698
train_iter_loss: 0.2089369148015976
train_iter_loss: 0.12959997355937958
train_iter_loss: 0.07748330384492874
train_iter_loss: 0.13386598229408264
train_iter_loss: 0.11273892968893051
train_iter_loss: 0.21004411578178406
train_iter_loss: 0.19981931149959564
train_iter_loss: 0.15486732125282288
train_iter_loss: 0.10455210506916046
train_iter_loss: 0.20655806362628937
train_iter_loss: 0.13944171369075775
train_iter_loss: 0.20867179334163666
train_iter_loss: 0.32548919320106506
train_iter_loss: 0.17278161644935608
train_iter_loss: 0.22909219563007355
train_iter_loss: 0.18534713983535767
train_iter_loss: 0.16507145762443542
train_iter_loss: 0.07590065151453018
train_iter_loss: 0.0877547338604927
train_iter_loss: 0.08840934932231903
train_iter_loss: 0.18637605011463165
train_iter_loss: 0.09006430208683014
train_iter_loss: 0.08612007647752762
train_iter_loss: 0.10971663147211075
train_iter_loss: 0.2760807275772095
train_iter_loss: 0.14619286358356476
train_iter_loss: 0.15999318659305573
train_iter_loss: 0.1393221914768219
train_iter_loss: 0.15930390357971191
train_iter_loss: 0.09419726580381393
train_iter_loss: 0.20871803164482117
train_iter_loss: 0.13586321473121643
train_iter_loss: 0.10394854843616486
train_iter_loss: 0.21835115551948547
train_iter_loss: 0.11965155601501465
train_iter_loss: 0.17837442457675934
train_iter_loss: 0.21063430607318878
train_iter_loss: 0.12376902252435684
train_iter_loss: 0.28020837903022766
train_iter_loss: 0.2494603991508484
train_iter_loss: 0.1493247151374817
train_iter_loss: 0.10100995749235153
train_iter_loss: 0.19811956584453583
train_iter_loss: 0.14015407860279083
train_iter_loss: 0.1445866823196411
train_iter_loss: 0.05223200470209122
train_iter_loss: 0.14343760907649994
train_iter_loss: 0.09452451765537262
train_iter_loss: 0.2763003408908844
train_iter_loss: 0.1452561765909195
train_iter_loss: 0.16738376021385193
train_iter_loss: 0.30481764674186707
train_iter_loss: 0.04412513226270676
train_iter_loss: 0.17717139422893524
train_iter_loss: 0.15793490409851074
train_iter_loss: 0.11834891140460968
train_iter_loss: 0.08898547291755676
train_iter_loss: 0.11775198578834534
train_iter_loss: 0.068904347717762
train_iter_loss: 0.1649526059627533
train_iter_loss: 0.18264542520046234
train_iter_loss: 0.21494078636169434
train_iter_loss: 0.11036432534456253
train_iter_loss: 0.10492230951786041
train_iter_loss: 0.16733568906784058
train_iter_loss: 0.3095768690109253
train_iter_loss: 0.1460786610841751
train loss :0.1633
---------------------
Validation seg loss: 0.2203572484365893 at epoch 557
epoch =    558/  1000, exp = train
train_iter_loss: 0.10570667684078217
train_iter_loss: 0.19087088108062744
train_iter_loss: 0.11136363446712494
train_iter_loss: 0.16053326427936554
train_iter_loss: 0.09212508052587509
train_iter_loss: 0.1948504000902176
train_iter_loss: 0.13155269622802734
train_iter_loss: 0.14672307670116425
train_iter_loss: 0.06800200790166855
train_iter_loss: 0.16436907649040222
train_iter_loss: 0.16396057605743408
train_iter_loss: 0.1122094988822937
train_iter_loss: 0.23462370038032532
train_iter_loss: 0.10588932037353516
train_iter_loss: 0.23154708743095398
train_iter_loss: 0.27634572982788086
train_iter_loss: 0.16202141344547272
train_iter_loss: 0.176410973072052
train_iter_loss: 0.16080056130886078
train_iter_loss: 0.16959494352340698
train_iter_loss: 0.11032875627279282
train_iter_loss: 0.3678584396839142
train_iter_loss: 0.1531423181295395
train_iter_loss: 0.13675539195537567
train_iter_loss: 0.200344979763031
train_iter_loss: 0.07713465392589569
train_iter_loss: 0.1309158354997635
train_iter_loss: 0.08840794116258621
train_iter_loss: 0.1438664197921753
train_iter_loss: 0.19653822481632233
train_iter_loss: 0.223199263215065
train_iter_loss: 0.11574989557266235
train_iter_loss: 0.31000402569770813
train_iter_loss: 0.20219042897224426
train_iter_loss: 0.2622023820877075
train_iter_loss: 0.27325087785720825
train_iter_loss: 0.06540136784315109
train_iter_loss: 0.2472783625125885
train_iter_loss: 0.07668109983205795
train_iter_loss: 0.1270056515932083
train_iter_loss: 0.17912890017032623
train_iter_loss: 0.255933940410614
train_iter_loss: 0.1703241765499115
train_iter_loss: 0.07870449125766754
train_iter_loss: 0.1116119921207428
train_iter_loss: 0.09665119647979736
train_iter_loss: 0.30445727705955505
train_iter_loss: 0.13937436044216156
train_iter_loss: 0.14013800024986267
train_iter_loss: 0.21004095673561096
train_iter_loss: 0.16230380535125732
train_iter_loss: 0.13882005214691162
train_iter_loss: 0.22475631535053253
train_iter_loss: 0.15305110812187195
train_iter_loss: 0.14427229762077332
train_iter_loss: 0.26048603653907776
train_iter_loss: 0.06068672612309456
train_iter_loss: 0.17276470363140106
train_iter_loss: 0.2087099403142929
train_iter_loss: 0.21358731389045715
train_iter_loss: 0.11606934666633606
train_iter_loss: 0.060891274362802505
train_iter_loss: 0.2261800467967987
train_iter_loss: 0.15790583193302155
train_iter_loss: 0.27669280767440796
train_iter_loss: 0.13107647001743317
train_iter_loss: 0.23922571539878845
train_iter_loss: 0.08243744820356369
train_iter_loss: 0.17329435050487518
train_iter_loss: 0.14490026235580444
train_iter_loss: 0.27312496304512024
train_iter_loss: 0.07402392476797104
train_iter_loss: 0.12750065326690674
train_iter_loss: 0.23512768745422363
train_iter_loss: 0.08004383742809296
train_iter_loss: 0.14846257865428925
train_iter_loss: 0.06006574630737305
train_iter_loss: 0.22589439153671265
train_iter_loss: 0.2743087112903595
train_iter_loss: 0.21714434027671814
train_iter_loss: 0.15231621265411377
train_iter_loss: 0.19068704545497894
train_iter_loss: 0.17668673396110535
train_iter_loss: 0.1440480500459671
train_iter_loss: 0.11998556554317474
train_iter_loss: 0.1225951686501503
train_iter_loss: 0.23096469044685364
train_iter_loss: 0.1762780100107193
train_iter_loss: 0.4685880243778229
train_iter_loss: 0.19817803800106049
train_iter_loss: 0.2076122760772705
train_iter_loss: 0.14233805239200592
train_iter_loss: 0.18313711881637573
train_iter_loss: 0.24859346449375153
train_iter_loss: 0.11394733190536499
train_iter_loss: 0.13661761581897736
train_iter_loss: 0.09498882293701172
train_iter_loss: 0.076236791908741
train_iter_loss: 0.16808214783668518
train_iter_loss: 0.07556165009737015
train loss :0.1701
---------------------
Validation seg loss: 0.21553556066674162 at epoch 558
epoch =    559/  1000, exp = train
train_iter_loss: 0.17575164139270782
train_iter_loss: 0.11076318472623825
train_iter_loss: 0.056894220411777496
train_iter_loss: 0.15441855788230896
train_iter_loss: 0.29171136021614075
train_iter_loss: 0.2502171993255615
train_iter_loss: 0.16772831976413727
train_iter_loss: 0.130594402551651
train_iter_loss: 0.1298406571149826
train_iter_loss: 0.3373148739337921
train_iter_loss: 0.11758048087358475
train_iter_loss: 0.20507922768592834
train_iter_loss: 0.1683512181043625
train_iter_loss: 0.16850806772708893
train_iter_loss: 0.18231070041656494
train_iter_loss: 0.14233547449111938
train_iter_loss: 0.17562156915664673
train_iter_loss: 0.14673319458961487
train_iter_loss: 0.10617969930171967
train_iter_loss: 0.14047762751579285
train_iter_loss: 0.14624589681625366
train_iter_loss: 0.18077720701694489
train_iter_loss: 0.14691050350666046
train_iter_loss: 0.13435350358486176
train_iter_loss: 0.10071094334125519
train_iter_loss: 0.18307171761989594
train_iter_loss: 0.21531517803668976
train_iter_loss: 0.2897975742816925
train_iter_loss: 0.11988412588834763
train_iter_loss: 0.11872974783182144
train_iter_loss: 0.357218861579895
train_iter_loss: 0.10254183411598206
train_iter_loss: 0.14865589141845703
train_iter_loss: 0.1839594841003418
train_iter_loss: 0.1385735720396042
train_iter_loss: 0.0807914286851883
train_iter_loss: 0.08202565461397171
train_iter_loss: 0.26461467146873474
train_iter_loss: 0.28070807456970215
train_iter_loss: 0.13724017143249512
train_iter_loss: 0.193526491522789
train_iter_loss: 0.06790554523468018
train_iter_loss: 0.04108685255050659
train_iter_loss: 0.07982134819030762
train_iter_loss: 0.13214804232120514
train_iter_loss: 0.14193904399871826
train_iter_loss: 0.09154795855283737
train_iter_loss: 0.07491163164377213
train_iter_loss: 0.13132129609584808
train_iter_loss: 0.08790544420480728
train_iter_loss: 0.22799021005630493
train_iter_loss: 0.12304608523845673
train_iter_loss: 0.11882928758859634
train_iter_loss: 0.10192273557186127
train_iter_loss: 0.15327893197536469
train_iter_loss: 0.07273673266172409
train_iter_loss: 0.30497270822525024
train_iter_loss: 0.13676202297210693
train_iter_loss: 0.12633559107780457
train_iter_loss: 0.08713890612125397
train_iter_loss: 0.21298383176326752
train_iter_loss: 0.09873177856206894
train_iter_loss: 0.12822040915489197
train_iter_loss: 0.13687023520469666
train_iter_loss: 0.06253121048212051
train_iter_loss: 0.2369316667318344
train_iter_loss: 0.3016752600669861
train_iter_loss: 0.1421281397342682
train_iter_loss: 0.1086839959025383
train_iter_loss: 0.039674337953329086
train_iter_loss: 0.06970623135566711
train_iter_loss: 0.09577005356550217
train_iter_loss: 0.19631695747375488
train_iter_loss: 0.14636008441448212
train_iter_loss: 0.09237683564424515
train_iter_loss: 0.1305054873228073
train_iter_loss: 0.15568873286247253
train_iter_loss: 0.2552650272846222
train_iter_loss: 0.22696293890476227
train_iter_loss: 0.08924727886915207
train_iter_loss: 0.11002010852098465
train_iter_loss: 0.14868353307247162
train_iter_loss: 0.17999176681041718
train_iter_loss: 0.27293136715888977
train_iter_loss: 0.2996984124183655
train_iter_loss: 0.16139079630374908
train_iter_loss: 0.12457802891731262
train_iter_loss: 0.20404812693595886
train_iter_loss: 0.13276349008083344
train_iter_loss: 0.16385816037654877
train_iter_loss: 0.10397499799728394
train_iter_loss: 0.05512771010398865
train_iter_loss: 0.1789950728416443
train_iter_loss: 0.1802872121334076
train_iter_loss: 0.1720007210969925
train_iter_loss: 0.15095219016075134
train_iter_loss: 0.09054677188396454
train_iter_loss: 0.26714539527893066
train_iter_loss: 0.4722159206867218
train_iter_loss: 0.09301045536994934
train loss :0.1592
---------------------
Validation seg loss: 0.22134065920627624 at epoch 559
epoch =    560/  1000, exp = train
train_iter_loss: 0.1391313374042511
train_iter_loss: 0.16570639610290527
train_iter_loss: 0.13744398951530457
train_iter_loss: 0.20498304069042206
train_iter_loss: 0.08979427814483643
train_iter_loss: 0.10755787044763565
train_iter_loss: 0.08025886118412018
train_iter_loss: 0.15838541090488434
train_iter_loss: 0.17598822712898254
train_iter_loss: 0.23288844525814056
train_iter_loss: 0.09650541096925735
train_iter_loss: 0.24664565920829773
train_iter_loss: 0.0866699367761612
train_iter_loss: 0.17421691119670868
train_iter_loss: 0.1946033239364624
train_iter_loss: 0.1576293706893921
train_iter_loss: 0.07622065395116806
train_iter_loss: 0.07226856797933578
train_iter_loss: 0.11502460390329361
train_iter_loss: 0.08228420466184616
train_iter_loss: 0.19197404384613037
train_iter_loss: 0.2188090831041336
train_iter_loss: 0.14293067157268524
train_iter_loss: 0.17891669273376465
train_iter_loss: 0.16829022765159607
train_iter_loss: 0.14405836164951324
train_iter_loss: 0.17641957104206085
train_iter_loss: 0.12732446193695068
train_iter_loss: 0.1919924020767212
train_iter_loss: 0.20699092745780945
train_iter_loss: 0.0893242210149765
train_iter_loss: 0.10445506870746613
train_iter_loss: 0.12151604890823364
train_iter_loss: 0.15784724056720734
train_iter_loss: 0.12148788571357727
train_iter_loss: 0.12337084859609604
train_iter_loss: 0.13378454744815826
train_iter_loss: 0.08348708599805832
train_iter_loss: 0.09541972726583481
train_iter_loss: 0.13433150947093964
train_iter_loss: 0.08531101793050766
train_iter_loss: 0.2225368320941925
train_iter_loss: 0.1815977394580841
train_iter_loss: 0.19859519600868225
train_iter_loss: 0.12596969306468964
train_iter_loss: 0.27319422364234924
train_iter_loss: 0.1696631908416748
train_iter_loss: 0.11154600232839584
train_iter_loss: 0.13441750407218933
train_iter_loss: 0.15275773406028748
train_iter_loss: 0.2044302225112915
train_iter_loss: 0.1463565081357956
train_iter_loss: 0.13562160730361938
train_iter_loss: 0.31095075607299805
train_iter_loss: 0.2038082778453827
train_iter_loss: 0.145939439535141
train_iter_loss: 0.10321402549743652
train_iter_loss: 0.1011875569820404
train_iter_loss: 0.13691329956054688
train_iter_loss: 0.15642987191677094
train_iter_loss: 0.1295526772737503
train_iter_loss: 0.04731850326061249
train_iter_loss: 0.10696066170930862
train_iter_loss: 0.2746947705745697
train_iter_loss: 0.11179620027542114
train_iter_loss: 0.05555021017789841
train_iter_loss: 0.23369872570037842
train_iter_loss: 0.14323113858699799
train_iter_loss: 0.22649556398391724
train_iter_loss: 0.17509755492210388
train_iter_loss: 0.1511102020740509
train_iter_loss: 0.293442964553833
train_iter_loss: 0.25299355387687683
train_iter_loss: 0.10167995095252991
train_iter_loss: 0.2356860637664795
train_iter_loss: 0.11660593003034592
train_iter_loss: 0.06696046143770218
train_iter_loss: 0.18455088138580322
train_iter_loss: 0.3381635844707489
train_iter_loss: 0.24664035439491272
train_iter_loss: 0.13765518367290497
train_iter_loss: 0.12685412168502808
train_iter_loss: 0.16259591281414032
train_iter_loss: 0.12968894839286804
train_iter_loss: 0.13070620596408844
train_iter_loss: 0.15344129502773285
train_iter_loss: 0.19397814571857452
train_iter_loss: 0.23399631679058075
train_iter_loss: 0.169528990983963
train_iter_loss: 0.1730235517024994
train_iter_loss: 0.18793033063411713
train_iter_loss: 0.13484348356723785
train_iter_loss: 0.2059668004512787
train_iter_loss: 0.1096080094575882
train_iter_loss: 0.13830874860286713
train_iter_loss: 0.17807503044605255
train_iter_loss: 0.22885823249816895
train_iter_loss: 0.21689234673976898
train_iter_loss: 0.08053483068943024
train_iter_loss: 0.14658582210540771
train loss :0.1590
---------------------
Validation seg loss: 0.21496973372995853 at epoch 560
epoch =    561/  1000, exp = train
train_iter_loss: 0.09378146380186081
train_iter_loss: 0.12041550129652023
train_iter_loss: 0.19817303121089935
train_iter_loss: 0.19333066046237946
train_iter_loss: 0.22273685038089752
train_iter_loss: 0.22186864912509918
train_iter_loss: 0.20474466681480408
train_iter_loss: 0.24980796873569489
train_iter_loss: 0.19700682163238525
train_iter_loss: 0.4053066670894623
train_iter_loss: 0.1551186740398407
train_iter_loss: 0.056629739701747894
train_iter_loss: 0.32674893736839294
train_iter_loss: 0.14122126996517181
train_iter_loss: 0.17446567118167877
train_iter_loss: 0.15495426952838898
train_iter_loss: 0.11694225668907166
train_iter_loss: 0.2752557396888733
train_iter_loss: 0.11945552378892899
train_iter_loss: 0.028079597279429436
train_iter_loss: 0.14933723211288452
train_iter_loss: 0.18959800899028778
train_iter_loss: 0.2433169186115265
train_iter_loss: 0.18139667809009552
train_iter_loss: 0.1724216192960739
train_iter_loss: 0.13441342115402222
train_iter_loss: 0.06358221173286438
train_iter_loss: 0.09176679700613022
train_iter_loss: 0.11078628152608871
train_iter_loss: 0.14038725197315216
train_iter_loss: 0.1520124077796936
train_iter_loss: 0.13613197207450867
train_iter_loss: 0.1804465651512146
train_iter_loss: 0.1225573867559433
train_iter_loss: 0.13890239596366882
train_iter_loss: 0.22406496107578278
train_iter_loss: 0.301052451133728
train_iter_loss: 0.2150791734457016
train_iter_loss: 0.2114155888557434
train_iter_loss: 0.10769268870353699
train_iter_loss: 0.10624038428068161
train_iter_loss: 0.13161435723304749
train_iter_loss: 0.16186441481113434
train_iter_loss: 0.15897822380065918
train_iter_loss: 0.1937526911497116
train_iter_loss: 0.18230849504470825
train_iter_loss: 0.14451713860034943
train_iter_loss: 0.22677455842494965
train_iter_loss: 0.23971456289291382
train_iter_loss: 0.17066776752471924
train_iter_loss: 0.18335331976413727
train_iter_loss: 0.3179379105567932
train_iter_loss: 0.08990269154310226
train_iter_loss: 0.2275840789079666
train_iter_loss: 0.12541605532169342
train_iter_loss: 0.14668156206607819
train_iter_loss: 0.21290618181228638
train_iter_loss: 0.09970605373382568
train_iter_loss: 0.17422199249267578
train_iter_loss: 0.22785358130931854
train_iter_loss: 0.11059635877609253
train_iter_loss: 0.19252565503120422
train_iter_loss: 0.09679799526929855
train_iter_loss: 0.14521557092666626
train_iter_loss: 0.13552071154117584
train_iter_loss: 0.1645778864622116
train_iter_loss: 0.11031191051006317
train_iter_loss: 0.2988397777080536
train_iter_loss: 0.10606246441602707
train_iter_loss: 0.13765840232372284
train_iter_loss: 0.0776647999882698
train_iter_loss: 0.1251334846019745
train_iter_loss: 0.054885197430849075
train_iter_loss: 0.2836207151412964
train_iter_loss: 0.13184259831905365
train_iter_loss: 0.127494677901268
train_iter_loss: 0.18795572221279144
train_iter_loss: 0.1081518605351448
train_iter_loss: 0.10245969146490097
train_iter_loss: 0.20762714743614197
train_iter_loss: 0.04478553310036659
train_iter_loss: 0.32079535722732544
train_iter_loss: 0.34841492772102356
train_iter_loss: 0.13896992802619934
train_iter_loss: 0.18097873032093048
train_iter_loss: 0.2312059998512268
train_iter_loss: 0.12014995515346527
train_iter_loss: 0.20969730615615845
train_iter_loss: 0.11587337404489517
train_iter_loss: 0.16041772067546844
train_iter_loss: 0.10582596808671951
train_iter_loss: 0.18854370713233948
train_iter_loss: 0.09126900881528854
train_iter_loss: 0.1469077467918396
train_iter_loss: 0.20580579340457916
train_iter_loss: 0.12962771952152252
train_iter_loss: 0.19703157246112823
train_iter_loss: 0.2375229448080063
train_iter_loss: 0.19605112075805664
train_iter_loss: 0.1239098533987999
train loss :0.1700
---------------------
Validation seg loss: 0.21701602442717216 at epoch 561
epoch =    562/  1000, exp = train
train_iter_loss: 0.17971991002559662
train_iter_loss: 0.14961054921150208
train_iter_loss: 0.13320297002792358
train_iter_loss: 0.1404874175786972
train_iter_loss: 0.13845573365688324
train_iter_loss: 0.16354331374168396
train_iter_loss: 0.13195888698101044
train_iter_loss: 0.17938654124736786
train_iter_loss: 0.1113533079624176
train_iter_loss: 0.17862339317798615
train_iter_loss: 0.24718599021434784
train_iter_loss: 0.1705801635980606
train_iter_loss: 0.1706167459487915
train_iter_loss: 0.17579464614391327
train_iter_loss: 0.14835001528263092
train_iter_loss: 0.1122068464756012
train_iter_loss: 0.10084119439125061
train_iter_loss: 0.10061818361282349
train_iter_loss: 0.16951173543930054
train_iter_loss: 0.09110437333583832
train_iter_loss: 0.11377577483654022
train_iter_loss: 0.15840694308280945
train_iter_loss: 0.1912674456834793
train_iter_loss: 0.20496734976768494
train_iter_loss: 0.20807301998138428
train_iter_loss: 0.11909723281860352
train_iter_loss: 0.19142289459705353
train_iter_loss: 0.1361653208732605
train_iter_loss: 0.2463315725326538
train_iter_loss: 0.13204117119312286
train_iter_loss: 0.23708917200565338
train_iter_loss: 0.17532193660736084
train_iter_loss: 0.11927692592144012
train_iter_loss: 0.2033596634864807
train_iter_loss: 0.08689968287944794
train_iter_loss: 0.17062880098819733
train_iter_loss: 0.24219800531864166
train_iter_loss: 0.2685435712337494
train_iter_loss: 0.11640971153974533
train_iter_loss: 0.14071151614189148
train_iter_loss: 0.20995661616325378
train_iter_loss: 0.16161194443702698
train_iter_loss: 0.12068824470043182
train_iter_loss: 0.2184213548898697
train_iter_loss: 0.2659520208835602
train_iter_loss: 0.12376297265291214
train_iter_loss: 0.1850820928812027
train_iter_loss: 0.20001301169395447
train_iter_loss: 0.11266697198152542
train_iter_loss: 0.13279809057712555
train_iter_loss: 0.17969250679016113
train_iter_loss: 0.06541237980127335
train_iter_loss: 0.21408462524414062
train_iter_loss: 0.19098035991191864
train_iter_loss: 0.16190487146377563
train_iter_loss: 0.14000758528709412
train_iter_loss: 0.16331210732460022
train_iter_loss: 0.22096368670463562
train_iter_loss: 0.1692020297050476
train_iter_loss: 0.16679619252681732
train_iter_loss: 0.1408826857805252
train_iter_loss: 0.10919797420501709
train_iter_loss: 0.23317623138427734
train_iter_loss: 0.12686274945735931
train_iter_loss: 0.05886456370353699
train_iter_loss: 0.14111517369747162
train_iter_loss: 0.1205262765288353
train_iter_loss: 0.24433854222297668
train_iter_loss: 0.059155698865652084
train_iter_loss: 0.19861643016338348
train_iter_loss: 0.0652279183268547
train_iter_loss: 0.09987715631723404
train_iter_loss: 0.2794125974178314
train_iter_loss: 0.13905051350593567
train_iter_loss: 0.24290630221366882
train_iter_loss: 0.16755279898643494
train_iter_loss: 0.06456489115953445
train_iter_loss: 0.103984534740448
train_iter_loss: 0.2739505469799042
train_iter_loss: 0.04274437949061394
train_iter_loss: 0.1033800020813942
train_iter_loss: 0.13808225095272064
train_iter_loss: 0.20625244081020355
train_iter_loss: 0.19711880385875702
train_iter_loss: 0.15853391587734222
train_iter_loss: 0.15907305479049683
train_iter_loss: 0.07451259344816208
train_iter_loss: 0.12086565792560577
train_iter_loss: 0.14220142364501953
train_iter_loss: 0.1642157882452011
train_iter_loss: 0.18230170011520386
train_iter_loss: 0.20110270380973816
train_iter_loss: 0.05509364604949951
train_iter_loss: 0.15812601149082184
train_iter_loss: 0.16119977831840515
train_iter_loss: 0.10184742510318756
train_iter_loss: 0.10268736630678177
train_iter_loss: 0.10496245324611664
train_iter_loss: 0.1331072598695755
train_iter_loss: 0.09154592454433441
train loss :0.1570
---------------------
Validation seg loss: 0.21492834557023532 at epoch 562
epoch =    563/  1000, exp = train
train_iter_loss: 0.08829120546579361
train_iter_loss: 0.22862499952316284
train_iter_loss: 0.20856104791164398
train_iter_loss: 0.19676454365253448
train_iter_loss: 0.18704263865947723
train_iter_loss: 0.08253085613250732
train_iter_loss: 0.13274553418159485
train_iter_loss: 0.199110746383667
train_iter_loss: 0.14639580249786377
train_iter_loss: 0.1972169131040573
train_iter_loss: 0.17395591735839844
train_iter_loss: 0.26399269700050354
train_iter_loss: 0.13327649235725403
train_iter_loss: 0.1105969250202179
train_iter_loss: 0.08274896442890167
train_iter_loss: 0.09188437461853027
train_iter_loss: 0.12374187260866165
train_iter_loss: 0.2850748300552368
train_iter_loss: 0.12441332638263702
train_iter_loss: 0.04535270109772682
train_iter_loss: 0.21252849698066711
train_iter_loss: 0.12910285592079163
train_iter_loss: 0.27851104736328125
train_iter_loss: 0.13176488876342773
train_iter_loss: 0.18353873491287231
train_iter_loss: 0.1457565873861313
train_iter_loss: 0.13756246864795685
train_iter_loss: 0.11027707904577255
train_iter_loss: 0.269940584897995
train_iter_loss: 0.11309002339839935
train_iter_loss: 0.18875713646411896
train_iter_loss: 0.1422388255596161
train_iter_loss: 0.13079647719860077
train_iter_loss: 0.234956756234169
train_iter_loss: 0.2144758254289627
train_iter_loss: 0.09656868129968643
train_iter_loss: 0.21171364188194275
train_iter_loss: 0.1947271078824997
train_iter_loss: 0.03843255713582039
train_iter_loss: 0.17427349090576172
train_iter_loss: 0.11974797397851944
train_iter_loss: 0.13362137973308563
train_iter_loss: 0.1001986414194107
train_iter_loss: 0.1476546674966812
train_iter_loss: 0.09591785818338394
train_iter_loss: 0.09065317362546921
train_iter_loss: 0.22431570291519165
train_iter_loss: 0.11360491812229156
train_iter_loss: 0.03838883712887764
train_iter_loss: 0.20770791172981262
train_iter_loss: 0.12624937295913696
train_iter_loss: 0.15617690980434418
train_iter_loss: 0.13465800881385803
train_iter_loss: 0.27639344334602356
train_iter_loss: 0.20659345388412476
train_iter_loss: 0.2082824558019638
train_iter_loss: 0.1924334317445755
train_iter_loss: 0.16760018467903137
train_iter_loss: 0.09993070363998413
train_iter_loss: 0.40279704332351685
train_iter_loss: 0.12781937420368195
train_iter_loss: 0.1359538584947586
train_iter_loss: 0.13046424090862274
train_iter_loss: 0.1316867172718048
train_iter_loss: 0.16077378392219543
train_iter_loss: 0.29570063948631287
train_iter_loss: 0.06130613386631012
train_iter_loss: 0.07463707774877548
train_iter_loss: 0.12115998566150665
train_iter_loss: 0.1242811530828476
train_iter_loss: 0.1824774146080017
train_iter_loss: 0.19133271276950836
train_iter_loss: 0.21293361485004425
train_iter_loss: 0.10382787883281708
train_iter_loss: 0.11242564767599106
train_iter_loss: 0.1844491958618164
train_iter_loss: 0.1944160759449005
train_iter_loss: 0.26909852027893066
train_iter_loss: 0.20792588591575623
train_iter_loss: 0.19772067666053772
train_iter_loss: 0.16731317341327667
train_iter_loss: 0.09014929831027985
train_iter_loss: 0.13415609300136566
train_iter_loss: 0.08277349174022675
train_iter_loss: 0.23141314089298248
train_iter_loss: 0.1430996209383011
train_iter_loss: 0.14926226437091827
train_iter_loss: 0.11154839396476746
train_iter_loss: 0.21237631142139435
train_iter_loss: 0.20290136337280273
train_iter_loss: 0.21489053964614868
train_iter_loss: 0.1405717432498932
train_iter_loss: 0.17361485958099365
train_iter_loss: 0.2567462623119354
train_iter_loss: 0.07080493867397308
train_iter_loss: 0.3269811272621155
train_iter_loss: 0.11291453242301941
train_iter_loss: 0.07030151039361954
train_iter_loss: 0.18020179867744446
train_iter_loss: 0.09602227807044983
train loss :0.1623
---------------------
Validation seg loss: 0.21466619468663098 at epoch 563
epoch =    564/  1000, exp = train
train_iter_loss: 0.21517537534236908
train_iter_loss: 0.12234191596508026
train_iter_loss: 0.20045651495456696
train_iter_loss: 0.21736936271190643
train_iter_loss: 0.06581759452819824
train_iter_loss: 0.36163273453712463
train_iter_loss: 0.11170170456171036
train_iter_loss: 0.15270264446735382
train_iter_loss: 0.12487079948186874
train_iter_loss: 0.21550074219703674
train_iter_loss: 0.08529283851385117
train_iter_loss: 0.12883496284484863
train_iter_loss: 0.10325204581022263
train_iter_loss: 0.25962531566619873
train_iter_loss: 0.16613812744617462
train_iter_loss: 0.18323956429958344
train_iter_loss: 0.149290069937706
train_iter_loss: 0.12211181223392487
train_iter_loss: 0.18458209931850433
train_iter_loss: 0.6396602392196655
train_iter_loss: 0.10756292939186096
train_iter_loss: 0.13786937296390533
train_iter_loss: 0.20666129887104034
train_iter_loss: 0.12121506035327911
train_iter_loss: 0.1572105586528778
train_iter_loss: 0.13983793556690216
train_iter_loss: 0.19604723155498505
train_iter_loss: 0.2731649577617645
train_iter_loss: 0.23027248680591583
train_iter_loss: 0.10995830595493317
train_iter_loss: 0.07911092787981033
train_iter_loss: 0.1811826080083847
train_iter_loss: 0.11374007165431976
train_iter_loss: 0.18995711207389832
train_iter_loss: 0.14656029641628265
train_iter_loss: 0.18419858813285828
train_iter_loss: 0.12278427928686142
train_iter_loss: 0.16685594618320465
train_iter_loss: 0.0927230715751648
train_iter_loss: 0.17287126183509827
train_iter_loss: 0.23544517159461975
train_iter_loss: 0.15219563245773315
train_iter_loss: 0.1235649362206459
train_iter_loss: 0.1463354378938675
train_iter_loss: 0.12459398806095123
train_iter_loss: 0.08527782559394836
train_iter_loss: 0.2649964690208435
train_iter_loss: 0.243479385972023
train_iter_loss: 0.11369503289461136
train_iter_loss: 0.14759886264801025
train_iter_loss: 0.17121867835521698
train_iter_loss: 0.23456726968288422
train_iter_loss: 0.23460720479488373
train_iter_loss: 0.1539115458726883
train_iter_loss: 0.09869888424873352
train_iter_loss: 0.14646898210048676
train_iter_loss: 0.0940214991569519
train_iter_loss: 0.1527092605829239
train_iter_loss: 0.09571711719036102
train_iter_loss: 0.09538039565086365
train_iter_loss: 0.14514794945716858
train_iter_loss: 0.2688841223716736
train_iter_loss: 0.13161738216876984
train_iter_loss: 0.14021438360214233
train_iter_loss: 0.09978754073381424
train_iter_loss: 0.13271774351596832
train_iter_loss: 0.21148835122585297
train_iter_loss: 0.22960767149925232
train_iter_loss: 0.06518971174955368
train_iter_loss: 0.07000182569026947
train_iter_loss: 0.16851677000522614
train_iter_loss: 0.13614986836910248
train_iter_loss: 0.20180469751358032
train_iter_loss: 0.15254215896129608
train_iter_loss: 0.235223650932312
train_iter_loss: 0.18352831900119781
train_iter_loss: 0.16054397821426392
train_iter_loss: 0.11287933588027954
train_iter_loss: 0.17923247814178467
train_iter_loss: 0.17587599158287048
train_iter_loss: 0.1945892721414566
train_iter_loss: 0.1981154978275299
train_iter_loss: 0.16133488714694977
train_iter_loss: 0.06304540485143661
train_iter_loss: 0.0788135975599289
train_iter_loss: 0.15629439055919647
train_iter_loss: 0.10766303539276123
train_iter_loss: 0.162848100066185
train_iter_loss: 0.03417486324906349
train_iter_loss: 0.09321628510951996
train_iter_loss: 0.19637291133403778
train_iter_loss: 0.15571507811546326
train_iter_loss: 0.11660654097795486
train_iter_loss: 0.16553913056850433
train_iter_loss: 0.17646843194961548
train_iter_loss: 0.20788636803627014
train_iter_loss: 0.24832458794116974
train_iter_loss: 0.11037318408489227
train_iter_loss: 0.05215564742684364
train_iter_loss: 0.18484267592430115
train loss :0.1628
---------------------
Validation seg loss: 0.21675483686498032 at epoch 564
epoch =    565/  1000, exp = train
train_iter_loss: 0.2880217730998993
train_iter_loss: 0.12437291443347931
train_iter_loss: 0.13094571232795715
train_iter_loss: 0.16460934281349182
train_iter_loss: 0.20788657665252686
train_iter_loss: 0.0724765956401825
train_iter_loss: 0.20163607597351074
train_iter_loss: 0.2199174463748932
train_iter_loss: 0.14455345273017883
train_iter_loss: 0.13081094622612
train_iter_loss: 0.12470806390047073
train_iter_loss: 0.1846729964017868
train_iter_loss: 0.17240536212921143
train_iter_loss: 0.1915382593870163
train_iter_loss: 0.13895590603351593
train_iter_loss: 0.07725408673286438
train_iter_loss: 0.1777561604976654
train_iter_loss: 0.1154162660241127
train_iter_loss: 0.26910659670829773
train_iter_loss: 0.14809472858905792
train_iter_loss: 0.11875371634960175
train_iter_loss: 0.2018136978149414
train_iter_loss: 0.20674090087413788
train_iter_loss: 0.07958604395389557
train_iter_loss: 0.13182716071605682
train_iter_loss: 0.1871214061975479
train_iter_loss: 0.1788032501935959
train_iter_loss: 0.1890389770269394
train_iter_loss: 0.13386841118335724
train_iter_loss: 0.10797236114740372
train_iter_loss: 0.27013033628463745
train_iter_loss: 0.15281416475772858
train_iter_loss: 0.1749245673418045
train_iter_loss: 0.1319638192653656
train_iter_loss: 0.22195056080818176
train_iter_loss: 0.09225016087293625
train_iter_loss: 0.21653887629508972
train_iter_loss: 0.14480073750019073
train_iter_loss: 0.22233356535434723
train_iter_loss: 0.212546706199646
train_iter_loss: 0.07452119886875153
train_iter_loss: 0.09462565183639526
train_iter_loss: 0.05261366069316864
train_iter_loss: 0.11693862825632095
train_iter_loss: 0.096549853682518
train_iter_loss: 0.11727426946163177
train_iter_loss: 0.09217527508735657
train_iter_loss: 0.1672855019569397
train_iter_loss: 0.19224822521209717
train_iter_loss: 0.11259830743074417
train_iter_loss: 0.2662275433540344
train_iter_loss: 0.07235384732484818
train_iter_loss: 0.13441945612430573
train_iter_loss: 0.09198211133480072
train_iter_loss: 0.21430328488349915
train_iter_loss: 0.16554388403892517
train_iter_loss: 0.140426903963089
train_iter_loss: 0.16181078553199768
train_iter_loss: 0.17737936973571777
train_iter_loss: 0.19153285026550293
train_iter_loss: 0.10091102868318558
train_iter_loss: 0.21397291123867035
train_iter_loss: 0.11941757798194885
train_iter_loss: 0.11527156829833984
train_iter_loss: 0.16066062450408936
train_iter_loss: 0.09735599905252457
train_iter_loss: 0.09514772891998291
train_iter_loss: 0.1235179528594017
train_iter_loss: 0.14767813682556152
train_iter_loss: 0.07375308126211166
train_iter_loss: 0.1689012050628662
train_iter_loss: 0.053061287850141525
train_iter_loss: 0.11826835572719574
train_iter_loss: 0.10502729564905167
train_iter_loss: 0.21715258061885834
train_iter_loss: 0.20094765722751617
train_iter_loss: 0.04608571156859398
train_iter_loss: 0.18038052320480347
train_iter_loss: 0.16517192125320435
train_iter_loss: 0.12784817814826965
train_iter_loss: 0.2210165411233902
train_iter_loss: 0.24089999496936798
train_iter_loss: 0.1309853047132492
train_iter_loss: 0.12941664457321167
train_iter_loss: 0.2535277009010315
train_iter_loss: 0.1416722685098648
train_iter_loss: 0.17299708724021912
train_iter_loss: 0.13322633504867554
train_iter_loss: 0.24632364511489868
train_iter_loss: 0.11243212223052979
train_iter_loss: 0.2115362584590912
train_iter_loss: 0.20641067624092102
train_iter_loss: 0.13840937614440918
train_iter_loss: 0.31355318427085876
train_iter_loss: 0.0912986695766449
train_iter_loss: 0.0872776061296463
train_iter_loss: 0.09065500646829605
train_iter_loss: 0.1613721251487732
train_iter_loss: 0.25390365719795227
train_iter_loss: 0.1300783008337021
train loss :0.1565
---------------------
Validation seg loss: 0.21992174376083432 at epoch 565
epoch =    566/  1000, exp = train
train_iter_loss: 0.1858275830745697
train_iter_loss: 0.1737750619649887
train_iter_loss: 0.1438601166009903
train_iter_loss: 0.100450299680233
train_iter_loss: 0.2846114933490753
train_iter_loss: 0.17534787952899933
train_iter_loss: 0.06453543156385422
train_iter_loss: 0.08189656585454941
train_iter_loss: 0.18845929205417633
train_iter_loss: 0.10476917028427124
train_iter_loss: 0.2005014419555664
train_iter_loss: 0.18511264026165009
train_iter_loss: 0.41890814900398254
train_iter_loss: 0.13831272721290588
train_iter_loss: 0.177180677652359
train_iter_loss: 0.09045103192329407
train_iter_loss: 0.16576501727104187
train_iter_loss: 0.22308489680290222
train_iter_loss: 0.2858631908893585
train_iter_loss: 0.16542281210422516
train_iter_loss: 0.08933619409799576
train_iter_loss: 0.13551215827465057
train_iter_loss: 0.19231586158275604
train_iter_loss: 0.19610749185085297
train_iter_loss: 0.07763873785734177
train_iter_loss: 0.12430021911859512
train_iter_loss: 0.08595867455005646
train_iter_loss: 0.14438903331756592
train_iter_loss: 0.23397433757781982
train_iter_loss: 0.17871811985969543
train_iter_loss: 0.20728273689746857
train_iter_loss: 0.08682797104120255
train_iter_loss: 0.13169223070144653
train_iter_loss: 0.1848568171262741
train_iter_loss: 0.08123332262039185
train_iter_loss: 0.1649010181427002
train_iter_loss: 0.12984617054462433
train_iter_loss: 0.10119535773992538
train_iter_loss: 0.09285924583673477
train_iter_loss: 0.09849506616592407
train_iter_loss: 0.16100621223449707
train_iter_loss: 0.14778445661067963
train_iter_loss: 0.5341325402259827
train_iter_loss: 0.1826493889093399
train_iter_loss: 0.17674463987350464
train_iter_loss: 0.15424782037734985
train_iter_loss: 0.25915583968162537
train_iter_loss: 0.1240774393081665
train_iter_loss: 0.1650218814611435
train_iter_loss: 0.19467711448669434
train_iter_loss: 0.14291730523109436
train_iter_loss: 0.12980927526950836
train_iter_loss: 0.1449454128742218
train_iter_loss: 0.26884201169013977
train_iter_loss: 0.13980509340763092
train_iter_loss: 0.1907622367143631
train_iter_loss: 0.1964631974697113
train_iter_loss: 0.0680391862988472
train_iter_loss: 0.15497463941574097
train_iter_loss: 0.18373838067054749
train_iter_loss: 0.17224335670471191
train_iter_loss: 0.0923774242401123
train_iter_loss: 0.14097702503204346
train_iter_loss: 0.12503626942634583
train_iter_loss: 0.1888691782951355
train_iter_loss: 0.1771189421415329
train_iter_loss: 0.18696586787700653
train_iter_loss: 0.2205682247877121
train_iter_loss: 0.08137282729148865
train_iter_loss: 0.18212072551250458
train_iter_loss: 0.17886985838413239
train_iter_loss: 0.062387630343437195
train_iter_loss: 0.21454957127571106
train_iter_loss: 0.21807588636875153
train_iter_loss: 0.19710655510425568
train_iter_loss: 0.08928140997886658
train_iter_loss: 0.19847218692302704
train_iter_loss: 0.2492503672838211
train_iter_loss: 0.18496811389923096
train_iter_loss: 0.17991995811462402
train_iter_loss: 0.2009831666946411
train_iter_loss: 0.13725106418132782
train_iter_loss: 0.1176268681883812
train_iter_loss: 0.24600236117839813
train_iter_loss: 0.2357141524553299
train_iter_loss: 0.1277407854795456
train_iter_loss: 0.2325015664100647
train_iter_loss: 0.16985948383808136
train_iter_loss: 0.2327904999256134
train_iter_loss: 0.14962029457092285
train_iter_loss: 0.1457327902317047
train_iter_loss: 0.10099753737449646
train_iter_loss: 0.10050386935472488
train_iter_loss: 0.10181634873151779
train_iter_loss: 0.05276361480355263
train_iter_loss: 0.2393655925989151
train_iter_loss: 0.13474909961223602
train_iter_loss: 0.1325903683900833
train_iter_loss: 0.17371439933776855
train_iter_loss: 0.1705867499113083
train loss :0.1671
---------------------
Validation seg loss: 0.21225772974661217 at epoch 566
********************
best_val_epoch_loss:  0.21225772974661217
MODEL UPDATED
epoch =    567/  1000, exp = train
train_iter_loss: 0.0510319359600544
train_iter_loss: 0.08846241980791092
train_iter_loss: 0.24565331637859344
train_iter_loss: 0.23461954295635223
train_iter_loss: 0.10494279861450195
train_iter_loss: 0.2866785526275635
train_iter_loss: 0.11400429904460907
train_iter_loss: 0.18976670503616333
train_iter_loss: 0.14754194021224976
train_iter_loss: 0.12482501566410065
train_iter_loss: 0.24198994040489197
train_iter_loss: 0.14532843232154846
train_iter_loss: 0.2601313591003418
train_iter_loss: 0.23360347747802734
train_iter_loss: 0.1286449283361435
train_iter_loss: 0.11837328225374222
train_iter_loss: 0.1247304156422615
train_iter_loss: 0.11521140486001968
train_iter_loss: 0.1910521686077118
train_iter_loss: 0.07586497813463211
train_iter_loss: 0.14184415340423584
train_iter_loss: 0.1444551944732666
train_iter_loss: 0.2513057589530945
train_iter_loss: 0.13098661601543427
train_iter_loss: 0.12641024589538574
train_iter_loss: 0.08517283201217651
train_iter_loss: 0.12645027041435242
train_iter_loss: 0.21292029321193695
train_iter_loss: 0.1224750354886055
train_iter_loss: 0.1368371695280075
train_iter_loss: 0.1599864959716797
train_iter_loss: 0.08662568032741547
train_iter_loss: 0.25080692768096924
train_iter_loss: 0.16355429589748383
train_iter_loss: 0.29652518033981323
train_iter_loss: 0.24065177142620087
train_iter_loss: 0.14885149896144867
train_iter_loss: 0.2302820235490799
train_iter_loss: 0.09395379573106766
train_iter_loss: 0.14510250091552734
train_iter_loss: 0.1649523675441742
train_iter_loss: 0.05971440300345421
train_iter_loss: 0.11745665222406387
train_iter_loss: 0.2838082015514374
train_iter_loss: 0.1654818058013916
train_iter_loss: 0.08673494309186935
train_iter_loss: 0.22201867401599884
train_iter_loss: 0.08593717217445374
train_iter_loss: 0.14815019071102142
train_iter_loss: 0.1388055831193924
train_iter_loss: 0.08421250432729721
train_iter_loss: 0.2240181714296341
train_iter_loss: 0.09819724410772324
train_iter_loss: 0.19705188274383545
train_iter_loss: 0.062471311539411545
train_iter_loss: 0.10680831223726273
train_iter_loss: 0.14276722073554993
train_iter_loss: 0.12356235831975937
train_iter_loss: 0.09753929823637009
train_iter_loss: 0.2872816026210785
train_iter_loss: 0.0865846797823906
train_iter_loss: 0.12365243583917618
train_iter_loss: 0.16972020268440247
train_iter_loss: 0.1610071361064911
train_iter_loss: 0.09825215488672256
train_iter_loss: 0.19881148636341095
train_iter_loss: 0.05234513804316521
train_iter_loss: 0.18226319551467896
train_iter_loss: 0.31489336490631104
train_iter_loss: 0.1712966114282608
train_iter_loss: 0.16595393419265747
train_iter_loss: 0.17948020994663239
train_iter_loss: 0.20181575417518616
train_iter_loss: 0.3398154079914093
train_iter_loss: 0.12199044227600098
train_iter_loss: 0.16470016539096832
train_iter_loss: 0.13272303342819214
train_iter_loss: 0.15327593684196472
train_iter_loss: 0.2742322087287903
train_iter_loss: 0.09138324111700058
train_iter_loss: 0.14560523629188538
train_iter_loss: 0.1926969587802887
train_iter_loss: 0.10833819955587387
train_iter_loss: 0.12987136840820312
train_iter_loss: 0.24217860400676727
train_iter_loss: 0.2055857926607132
train_iter_loss: 0.20139996707439423
train_iter_loss: 0.168109729886055
train_iter_loss: 0.16150540113449097
train_iter_loss: 0.14123967289924622
train_iter_loss: 0.1628972738981247
train_iter_loss: 0.27629491686820984
train_iter_loss: 0.19934159517288208
train_iter_loss: 0.18353335559368134
train_iter_loss: 0.0771973505616188
train_iter_loss: 0.13704079389572144
train_iter_loss: 0.11652825772762299
train_iter_loss: 0.30371931195259094
train_iter_loss: 0.13056068122386932
train_iter_loss: 0.2109481543302536
train loss :0.1646
---------------------
Validation seg loss: 0.21575895748716198 at epoch 567
epoch =    568/  1000, exp = train
train_iter_loss: 0.2816438376903534
train_iter_loss: 0.0870874896645546
train_iter_loss: 0.14567920565605164
train_iter_loss: 0.20680275559425354
train_iter_loss: 0.18440371751785278
train_iter_loss: 0.069919154047966
train_iter_loss: 0.1692284345626831
train_iter_loss: 0.25950533151626587
train_iter_loss: 0.3559800386428833
train_iter_loss: 0.08841176331043243
train_iter_loss: 0.24725785851478577
train_iter_loss: 0.16694368422031403
train_iter_loss: 0.10063409060239792
train_iter_loss: 0.10363872349262238
train_iter_loss: 0.11308256536722183
train_iter_loss: 0.2181866317987442
train_iter_loss: 0.1091163232922554
train_iter_loss: 0.1568060964345932
train_iter_loss: 0.17744340002536774
train_iter_loss: 0.14309614896774292
train_iter_loss: 0.09839307516813278
train_iter_loss: 0.11304165422916412
train_iter_loss: 0.18040606379508972
train_iter_loss: 0.10532031208276749
train_iter_loss: 0.3050830066204071
train_iter_loss: 0.17907270789146423
train_iter_loss: 0.055339593440294266
train_iter_loss: 0.17069293558597565
train_iter_loss: 0.09854698926210403
train_iter_loss: 0.14262665808200836
train_iter_loss: 0.1214613988995552
train_iter_loss: 0.13857197761535645
train_iter_loss: 0.12304841727018356
train_iter_loss: 0.2542891204357147
train_iter_loss: 0.09506311267614365
train_iter_loss: 0.16844618320465088
train_iter_loss: 0.2221820205450058
train_iter_loss: 0.20380492508411407
train_iter_loss: 0.1394496113061905
train_iter_loss: 0.07181009650230408
train_iter_loss: 0.14692728221416473
train_iter_loss: 0.2819267511367798
train_iter_loss: 0.09288892149925232
train_iter_loss: 0.13930892944335938
train_iter_loss: 0.24430592358112335
train_iter_loss: 0.10006874799728394
train_iter_loss: 0.16689707338809967
train_iter_loss: 0.17886541783809662
train_iter_loss: 0.1330696940422058
train_iter_loss: 0.12077762931585312
train_iter_loss: 0.08747792989015579
train_iter_loss: 0.19941112399101257
train_iter_loss: 0.23420050740242004
train_iter_loss: 0.18289141356945038
train_iter_loss: 0.24713273346424103
train_iter_loss: 0.08265015482902527
train_iter_loss: 0.19511806964874268
train_iter_loss: 0.3254951536655426
train_iter_loss: 0.31255918741226196
train_iter_loss: 0.07025627046823502
train_iter_loss: 0.07737886905670166
train_iter_loss: 0.07836302369832993
train_iter_loss: 0.1267823725938797
train_iter_loss: 0.16744406521320343
train_iter_loss: 0.1354600042104721
train_iter_loss: 0.08958500623703003
train_iter_loss: 0.16278912127017975
train_iter_loss: 0.11995572596788406
train_iter_loss: 0.0949050784111023
train_iter_loss: 0.17457878589630127
train_iter_loss: 0.14126496016979218
train_iter_loss: 0.17598514258861542
train_iter_loss: 0.179353266954422
train_iter_loss: 0.12742948532104492
train_iter_loss: 0.242919921875
train_iter_loss: 0.19919376075267792
train_iter_loss: 0.08330187201499939
train_iter_loss: 0.16524633765220642
train_iter_loss: 0.2625410556793213
train_iter_loss: 0.1661887913942337
train_iter_loss: 0.1246214509010315
train_iter_loss: 0.12441787123680115
train_iter_loss: 0.16267463564872742
train_iter_loss: 0.27668341994285583
train_iter_loss: 0.23387910425662994
train_iter_loss: 0.0575975701212883
train_iter_loss: 0.17096462845802307
train_iter_loss: 0.13760249316692352
train_iter_loss: 0.2548258900642395
train_iter_loss: 0.11838341504335403
train_iter_loss: 0.132791206240654
train_iter_loss: 0.11055665463209152
train_iter_loss: 0.10814115405082703
train_iter_loss: 0.14772561192512512
train_iter_loss: 0.22887863218784332
train_iter_loss: 0.2567334771156311
train_iter_loss: 0.1603161245584488
train_iter_loss: 0.12241560220718384
train_iter_loss: 0.17047756910324097
train_iter_loss: 0.07365240156650543
train loss :0.1622
---------------------
Validation seg loss: 0.21401370873780184 at epoch 568
epoch =    569/  1000, exp = train
train_iter_loss: 0.06308165937662125
train_iter_loss: 0.22848032414913177
train_iter_loss: 0.058140624314546585
train_iter_loss: 0.17259187996387482
train_iter_loss: 0.134858176112175
train_iter_loss: 0.10688071697950363
train_iter_loss: 0.19440367817878723
train_iter_loss: 0.19406738877296448
train_iter_loss: 0.2599727511405945
train_iter_loss: 0.33919450640678406
train_iter_loss: 0.13008645176887512
train_iter_loss: 0.12632238864898682
train_iter_loss: 0.10114536434412003
train_iter_loss: 0.20371544361114502
train_iter_loss: 0.14151513576507568
train_iter_loss: 0.31596076488494873
train_iter_loss: 0.175151064991951
train_iter_loss: 0.14415784180164337
train_iter_loss: 0.160324364900589
train_iter_loss: 0.1185782253742218
train_iter_loss: 0.2328518033027649
train_iter_loss: 0.16376915574073792
train_iter_loss: 0.10964254289865494
train_iter_loss: 0.09817003458738327
train_iter_loss: 0.18248595297336578
train_iter_loss: 0.11254862695932388
train_iter_loss: 0.12432437390089035
train_iter_loss: 0.15801559388637543
train_iter_loss: 0.08289650827646255
train_iter_loss: 0.16258636116981506
train_iter_loss: 0.20348216593265533
train_iter_loss: 0.20796136558055878
train_iter_loss: 0.1416320949792862
train_iter_loss: 0.141823872923851
train_iter_loss: 0.10305982828140259
train_iter_loss: 0.09980445355176926
train_iter_loss: 0.26165640354156494
train_iter_loss: 0.10647398978471756
train_iter_loss: 0.05037081614136696
train_iter_loss: 0.20618095993995667
train_iter_loss: 0.18759317696094513
train_iter_loss: 0.1465458869934082
train_iter_loss: 0.11055018752813339
train_iter_loss: 0.2981846034526825
train_iter_loss: 0.24249592423439026
train_iter_loss: 0.11166080087423325
train_iter_loss: 0.23672270774841309
train_iter_loss: 0.12468840926885605
train_iter_loss: 0.1274007260799408
train_iter_loss: 0.23215846717357635
train_iter_loss: 0.19095993041992188
train_iter_loss: 0.16999752819538116
train_iter_loss: 0.21483074128627777
train_iter_loss: 0.12028998136520386
train_iter_loss: 0.07471419125795364
train_iter_loss: 0.4421244263648987
train_iter_loss: 0.1599043607711792
train_iter_loss: 0.10557953268289566
train_iter_loss: 0.18206866085529327
train_iter_loss: 0.24318888783454895
train_iter_loss: 0.15786774456501007
train_iter_loss: 0.1006164625287056
train_iter_loss: 0.19155822694301605
train_iter_loss: 0.16579867899417877
train_iter_loss: 0.20095084607601166
train_iter_loss: 0.27420786023139954
train_iter_loss: 0.18206225335597992
train_iter_loss: 0.10643193870782852
train_iter_loss: 0.15672136843204498
train_iter_loss: 0.1193961575627327
train_iter_loss: 0.2562275230884552
train_iter_loss: 0.2673536241054535
train_iter_loss: 0.1734681874513626
train_iter_loss: 0.1708061844110489
train_iter_loss: 0.25949880480766296
train_iter_loss: 0.09068191796541214
train_iter_loss: 0.15318623185157776
train_iter_loss: 0.1197836622595787
train_iter_loss: 0.22924189269542694
train_iter_loss: 0.08543264865875244
train_iter_loss: 0.17160917818546295
train_iter_loss: 0.1925409734249115
train_iter_loss: 0.08149112015962601
train_iter_loss: 0.09798894077539444
train_iter_loss: 0.2424491047859192
train_iter_loss: 0.07144124060869217
train_iter_loss: 0.13397064805030823
train_iter_loss: 0.14182865619659424
train_iter_loss: 0.3979751765727997
train_iter_loss: 0.3555518686771393
train_iter_loss: 0.17742666602134705
train_iter_loss: 0.19363003969192505
train_iter_loss: 0.07472272217273712
train_iter_loss: 0.1131279245018959
train_iter_loss: 0.15687255561351776
train_iter_loss: 0.15052329003810883
train_iter_loss: 0.13934725522994995
train_iter_loss: 0.13590040802955627
train_iter_loss: 0.10628918558359146
train_iter_loss: 0.09622419625520706
train loss :0.1689
---------------------
Validation seg loss: 0.21295665648221127 at epoch 569
epoch =    570/  1000, exp = train
train_iter_loss: 0.25150611996650696
train_iter_loss: 0.14747817814350128
train_iter_loss: 0.1762949526309967
train_iter_loss: 0.08533594012260437
train_iter_loss: 0.14348015189170837
train_iter_loss: 0.13419818878173828
train_iter_loss: 0.14536143839359283
train_iter_loss: 0.12521614134311676
train_iter_loss: 0.18527992069721222
train_iter_loss: 0.18246740102767944
train_iter_loss: 0.1334371268749237
train_iter_loss: 0.18898072838783264
train_iter_loss: 0.2245246022939682
train_iter_loss: 0.23048634827136993
train_iter_loss: 0.1213173121213913
train_iter_loss: 0.18507397174835205
train_iter_loss: 0.13623172044754028
train_iter_loss: 0.17062406241893768
train_iter_loss: 0.12151721119880676
train_iter_loss: 0.12263444066047668
train_iter_loss: 0.11853030323982239
train_iter_loss: 0.11139172315597534
train_iter_loss: 0.1230883002281189
train_iter_loss: 0.2967536747455597
train_iter_loss: 0.07668417692184448
train_iter_loss: 0.135115846991539
train_iter_loss: 0.12764282524585724
train_iter_loss: 0.1454240381717682
train_iter_loss: 0.1930071860551834
train_iter_loss: 0.08177322894334793
train_iter_loss: 0.1498250812292099
train_iter_loss: 0.12379112094640732
train_iter_loss: 0.16922585666179657
train_iter_loss: 0.12137997150421143
train_iter_loss: 0.13895390927791595
train_iter_loss: 0.19307617843151093
train_iter_loss: 0.08240584284067154
train_iter_loss: 0.19016605615615845
train_iter_loss: 0.14238300919532776
train_iter_loss: 0.13002263009548187
train_iter_loss: 0.0748797133564949
train_iter_loss: 0.13573533296585083
train_iter_loss: 0.18869732320308685
train_iter_loss: 0.15322744846343994
train_iter_loss: 0.3643365502357483
train_iter_loss: 0.19586770236492157
train_iter_loss: 0.1368882954120636
train_iter_loss: 0.06819064915180206
train_iter_loss: 0.19461728632450104
train_iter_loss: 0.17884665727615356
train_iter_loss: 0.23114489018917084
train_iter_loss: 0.13194018602371216
train_iter_loss: 0.08993728458881378
train_iter_loss: 0.1171955019235611
train_iter_loss: 0.19111762940883636
train_iter_loss: 0.11118049174547195
train_iter_loss: 0.20072069764137268
train_iter_loss: 0.21364650130271912
train_iter_loss: 0.18682348728179932
train_iter_loss: 0.19511030614376068
train_iter_loss: 0.16021010279655457
train_iter_loss: 0.1962064802646637
train_iter_loss: 0.07622390240430832
train_iter_loss: 0.2743549346923828
train_iter_loss: 0.13773392140865326
train_iter_loss: 0.0895320400595665
train_iter_loss: 0.17844782769680023
train_iter_loss: 0.18509280681610107
train_iter_loss: 0.16710330545902252
train_iter_loss: 0.2556566894054413
train_iter_loss: 0.12061412632465363
train_iter_loss: 0.2078588604927063
train_iter_loss: 0.1879330724477768
train_iter_loss: 0.07592887431383133
train_iter_loss: 0.15835820138454437
train_iter_loss: 0.15812331438064575
train_iter_loss: 0.09573522955179214
train_iter_loss: 0.18660876154899597
train_iter_loss: 0.09373383969068527
train_iter_loss: 0.11388496309518814
train_iter_loss: 0.21327464282512665
train_iter_loss: 0.19565249979496002
train_iter_loss: 0.055018067359924316
train_iter_loss: 0.28942668437957764
train_iter_loss: 0.14604051411151886
train_iter_loss: 0.06327779591083527
train_iter_loss: 0.10433164983987808
train_iter_loss: 0.31776487827301025
train_iter_loss: 0.16409635543823242
train_iter_loss: 0.17121295630931854
train_iter_loss: 0.19660630822181702
train_iter_loss: 0.09557951986789703
train_iter_loss: 0.1467372626066208
train_iter_loss: 0.1743585765361786
train_iter_loss: 0.15070828795433044
train_iter_loss: 0.1407947540283203
train_iter_loss: 0.16032862663269043
train_iter_loss: 0.17817482352256775
train_iter_loss: 0.2952992022037506
train_iter_loss: 0.05414809659123421
train loss :0.1602
---------------------
Validation seg loss: 0.2173972045212 at epoch 570
epoch =    571/  1000, exp = train
train_iter_loss: 0.1598629504442215
train_iter_loss: 0.15932360291481018
train_iter_loss: 0.10501573234796524
train_iter_loss: 0.09332606941461563
train_iter_loss: 0.1557738482952118
train_iter_loss: 0.1685834676027298
train_iter_loss: 0.292075514793396
train_iter_loss: 0.0784279853105545
train_iter_loss: 0.14470885694026947
train_iter_loss: 0.2832171320915222
train_iter_loss: 0.20796869695186615
train_iter_loss: 0.1198553815484047
train_iter_loss: 0.1927041858434677
train_iter_loss: 0.1784449815750122
train_iter_loss: 0.1945902407169342
train_iter_loss: 0.2354016751050949
train_iter_loss: 0.09373921155929565
train_iter_loss: 0.18420833349227905
train_iter_loss: 0.13257156312465668
train_iter_loss: 0.20097824931144714
train_iter_loss: 0.2392316460609436
train_iter_loss: 0.05936478078365326
train_iter_loss: 0.07009954005479813
train_iter_loss: 0.1593220829963684
train_iter_loss: 0.11000324785709381
train_iter_loss: 0.19261282682418823
train_iter_loss: 0.12617501616477966
train_iter_loss: 0.10893833637237549
train_iter_loss: 0.22706155478954315
train_iter_loss: 0.10649333149194717
train_iter_loss: 0.1383119523525238
train_iter_loss: 0.0791231170296669
train_iter_loss: 0.12110058963298798
train_iter_loss: 0.13843820989131927
train_iter_loss: 0.12469590455293655
train_iter_loss: 0.10964293777942657
train_iter_loss: 0.19241514801979065
train_iter_loss: 0.18351654708385468
train_iter_loss: 0.237357035279274
train_iter_loss: 0.2484358698129654
train_iter_loss: 0.14666461944580078
train_iter_loss: 0.17928819358348846
train_iter_loss: 0.12585698068141937
train_iter_loss: 0.09016962349414825
train_iter_loss: 0.15314239263534546
train_iter_loss: 0.13826215267181396
train_iter_loss: 0.17512397468090057
train_iter_loss: 0.13924424350261688
train_iter_loss: 0.08125532418489456
train_iter_loss: 0.11595989763736725
train_iter_loss: 0.14422820508480072
train_iter_loss: 0.1614384949207306
train_iter_loss: 0.17221160233020782
train_iter_loss: 0.13905830681324005
train_iter_loss: 0.1489381194114685
train_iter_loss: 0.13739174604415894
train_iter_loss: 0.10597576200962067
train_iter_loss: 0.30972471833229065
train_iter_loss: 0.21278923749923706
train_iter_loss: 0.253445029258728
train_iter_loss: 0.1218222826719284
train_iter_loss: 0.07248244434595108
train_iter_loss: 0.08422739058732986
train_iter_loss: 0.1298031210899353
train_iter_loss: 0.16462722420692444
train_iter_loss: 0.12040315568447113
train_iter_loss: 0.4203283488750458
train_iter_loss: 0.10485651344060898
train_iter_loss: 0.13428109884262085
train_iter_loss: 0.13202638924121857
train_iter_loss: 0.1495848149061203
train_iter_loss: 0.31173092126846313
train_iter_loss: 0.158333420753479
train_iter_loss: 0.10355052351951599
train_iter_loss: 0.18159513175487518
train_iter_loss: 0.19628438353538513
train_iter_loss: 0.09348450601100922
train_iter_loss: 0.15291516482830048
train_iter_loss: 0.09479090571403503
train_iter_loss: 0.1714603453874588
train_iter_loss: 0.12428933382034302
train_iter_loss: 0.2503601908683777
train_iter_loss: 0.20354026556015015
train_iter_loss: 0.1368398815393448
train_iter_loss: 0.2924700379371643
train_iter_loss: 0.07627448439598083
train_iter_loss: 0.07759390771389008
train_iter_loss: 0.22899021208286285
train_iter_loss: 0.12151850014925003
train_iter_loss: 0.1738484501838684
train_iter_loss: 0.24764595925807953
train_iter_loss: 0.264263391494751
train_iter_loss: 0.13881105184555054
train_iter_loss: 0.07613470405340195
train_iter_loss: 0.23942677676677704
train_iter_loss: 0.1212923675775528
train_iter_loss: 0.1589147299528122
train_iter_loss: 0.09066879749298096
train_iter_loss: 0.18592309951782227
train_iter_loss: 0.13836686313152313
train loss :0.1609
---------------------
Validation seg loss: 0.21739571543304986 at epoch 571
epoch =    572/  1000, exp = train
train_iter_loss: 0.2556339204311371
train_iter_loss: 0.19719474017620087
train_iter_loss: 0.14929425716400146
train_iter_loss: 0.23732700943946838
train_iter_loss: 0.16374346613883972
train_iter_loss: 0.16777361929416656
train_iter_loss: 0.20474204421043396
train_iter_loss: 0.16932424902915955
train_iter_loss: 0.12467313557863235
train_iter_loss: 0.13584057986736298
train_iter_loss: 0.18624570965766907
train_iter_loss: 0.10160046070814133
train_iter_loss: 0.28603121638298035
train_iter_loss: 0.21445274353027344
train_iter_loss: 0.13849614560604095
train_iter_loss: 0.10247939079999924
train_iter_loss: 0.10012459754943848
train_iter_loss: 0.129269078373909
train_iter_loss: 0.19963830709457397
train_iter_loss: 0.245564267039299
train_iter_loss: 0.18879131972789764
train_iter_loss: 0.04468750208616257
train_iter_loss: 0.1514873057603836
train_iter_loss: 0.14392060041427612
train_iter_loss: 0.07415241003036499
train_iter_loss: 0.08576001971960068
train_iter_loss: 0.16923846304416656
train_iter_loss: 0.12100066989660263
train_iter_loss: 0.10648205876350403
train_iter_loss: 0.2537182867527008
train_iter_loss: 0.11678224802017212
train_iter_loss: 0.12465903908014297
train_iter_loss: 0.22131085395812988
train_iter_loss: 0.1055794432759285
train_iter_loss: 0.15174219012260437
train_iter_loss: 0.10965988039970398
train_iter_loss: 0.22908411920070648
train_iter_loss: 0.12425827980041504
train_iter_loss: 0.07795092463493347
train_iter_loss: 0.18584389984607697
train_iter_loss: 0.16377060115337372
train_iter_loss: 0.1625988632440567
train_iter_loss: 0.2614084482192993
train_iter_loss: 0.09367725998163223
train_iter_loss: 0.2686688005924225
train_iter_loss: 0.08772606402635574
train_iter_loss: 0.14721152186393738
train_iter_loss: 0.130999356508255
train_iter_loss: 0.23001274466514587
train_iter_loss: 0.07316792756319046
train_iter_loss: 0.03175517916679382
train_iter_loss: 0.163066565990448
train_iter_loss: 0.12275347858667374
train_iter_loss: 0.207003653049469
train_iter_loss: 0.20949958264827728
train_iter_loss: 0.17410136759281158
train_iter_loss: 0.21515661478042603
train_iter_loss: 0.08363547921180725
train_iter_loss: 0.08797066658735275
train_iter_loss: 0.06754368543624878
train_iter_loss: 0.14738421142101288
train_iter_loss: 0.11700553447008133
train_iter_loss: 0.1388085037469864
train_iter_loss: 0.18756555020809174
train_iter_loss: 0.21267352998256683
train_iter_loss: 0.10778891295194626
train_iter_loss: 0.14055410027503967
train_iter_loss: 0.2831133306026459
train_iter_loss: 0.19373466074466705
train_iter_loss: 0.19571857154369354
train_iter_loss: 0.18546654284000397
train_iter_loss: 0.06603995710611343
train_iter_loss: 0.23444783687591553
train_iter_loss: 0.15506523847579956
train_iter_loss: 0.11959409713745117
train_iter_loss: 0.09040173888206482
train_iter_loss: 0.13701678812503815
train_iter_loss: 0.20619355142116547
train_iter_loss: 0.13269326090812683
train_iter_loss: 0.14194196462631226
train_iter_loss: 0.16755439341068268
train_iter_loss: 0.12968987226486206
train_iter_loss: 0.04694785922765732
train_iter_loss: 0.138279527425766
train_iter_loss: 0.3242184519767761
train_iter_loss: 0.0822252482175827
train_iter_loss: 0.13651573657989502
train_iter_loss: 0.14171692728996277
train_iter_loss: 0.34534361958503723
train_iter_loss: 0.1364896446466446
train_iter_loss: 0.09600351005792618
train_iter_loss: 0.20692315697669983
train_iter_loss: 0.17472124099731445
train_iter_loss: 0.15279094874858856
train_iter_loss: 0.35708218812942505
train_iter_loss: 0.07542411237955093
train_iter_loss: 0.25058016180992126
train_iter_loss: 0.18110999464988708
train_iter_loss: 0.1423765867948532
train_iter_loss: 0.12881705164909363
train loss :0.1605
---------------------
Validation seg loss: 0.21523020231112275 at epoch 572
epoch =    573/  1000, exp = train
train_iter_loss: 0.18708811700344086
train_iter_loss: 0.23063501715660095
train_iter_loss: 0.24117429554462433
train_iter_loss: 0.10230223834514618
train_iter_loss: 0.1628962606191635
train_iter_loss: 0.12967237830162048
train_iter_loss: 0.20441067218780518
train_iter_loss: 0.09486419707536697
train_iter_loss: 0.1370621919631958
train_iter_loss: 0.35016271471977234
train_iter_loss: 0.2804717421531677
train_iter_loss: 0.1867242306470871
train_iter_loss: 0.21991032361984253
train_iter_loss: 0.13455037772655487
train_iter_loss: 0.16416065394878387
train_iter_loss: 0.1446867138147354
train_iter_loss: 0.3097460865974426
train_iter_loss: 0.22564391791820526
train_iter_loss: 0.10703805834054947
train_iter_loss: 0.24027560651302338
train_iter_loss: 0.21692927181720734
train_iter_loss: 0.11924665421247482
train_iter_loss: 0.24228771030902863
train_iter_loss: 0.25318804383277893
train_iter_loss: 0.13302558660507202
train_iter_loss: 0.11525657773017883
train_iter_loss: 0.09039440006017685
train_iter_loss: 0.1360149383544922
train_iter_loss: 0.10355477780103683
train_iter_loss: 0.13518045842647552
train_iter_loss: 0.061336323618888855
train_iter_loss: 0.12604261934757233
train_iter_loss: 0.14885066449642181
train_iter_loss: 0.1280115842819214
train_iter_loss: 0.28507697582244873
train_iter_loss: 0.1986228972673416
train_iter_loss: 0.24429740011692047
train_iter_loss: 0.2797268033027649
train_iter_loss: 0.14182716608047485
train_iter_loss: 0.2999373972415924
train_iter_loss: 0.15544040501117706
train_iter_loss: 0.08332762122154236
train_iter_loss: 0.1037272959947586
train_iter_loss: 0.28620535135269165
train_iter_loss: 0.19714269042015076
train_iter_loss: 0.09919318556785583
train_iter_loss: 0.022654399275779724
train_iter_loss: 0.31759750843048096
train_iter_loss: 0.14020365476608276
train_iter_loss: 0.10141576081514359
train_iter_loss: 0.18405364453792572
train_iter_loss: 0.12357621639966965
train_iter_loss: 0.10618328303098679
train_iter_loss: 0.16218696534633636
train_iter_loss: 0.22757992148399353
train_iter_loss: 0.2515466511249542
train_iter_loss: 0.18565534055233002
train_iter_loss: 0.10728393495082855
train_iter_loss: 0.13323096930980682
train_iter_loss: 0.2196267545223236
train_iter_loss: 0.10047721862792969
train_iter_loss: 0.1446734070777893
train_iter_loss: 0.13971452414989471
train_iter_loss: 0.058149758726358414
train_iter_loss: 0.1713196337223053
train_iter_loss: 0.2103399932384491
train_iter_loss: 0.1535346508026123
train_iter_loss: 0.22200308740139008
train_iter_loss: 0.17226088047027588
train_iter_loss: 0.24201364815235138
train_iter_loss: 0.18166609108448029
train_iter_loss: 0.13451538980007172
train_iter_loss: 0.10538307577371597
train_iter_loss: 0.12461914122104645
train_iter_loss: 0.07341863214969635
train_iter_loss: 0.18111486732959747
train_iter_loss: 0.15299658477306366
train_iter_loss: 0.14170879125595093
train_iter_loss: 0.10187416523694992
train_iter_loss: 0.1413407325744629
train_iter_loss: 0.18347735702991486
train_iter_loss: 0.18395638465881348
train_iter_loss: 0.1294381320476532
train_iter_loss: 0.12920024991035461
train_iter_loss: 0.08466731756925583
train_iter_loss: 0.23801156878471375
train_iter_loss: 0.13818512856960297
train_iter_loss: 0.05788937583565712
train_iter_loss: 0.11764506995677948
train_iter_loss: 0.17101961374282837
train_iter_loss: 0.17716337740421295
train_iter_loss: 0.10579484701156616
train_iter_loss: 0.1802167296409607
train_iter_loss: 0.21720187366008759
train_iter_loss: 0.12163099646568298
train_iter_loss: 0.23698590695858002
train_iter_loss: 0.1645413190126419
train_iter_loss: 0.20488867163658142
train_iter_loss: 0.05365435406565666
train_iter_loss: 0.06922990828752518
train loss :0.1660
---------------------
Validation seg loss: 0.21491307314520455 at epoch 573
epoch =    574/  1000, exp = train
train_iter_loss: 0.10369863361120224
train_iter_loss: 0.16546159982681274
train_iter_loss: 0.1563708782196045
train_iter_loss: 0.21403035521507263
train_iter_loss: 0.17437388002872467
train_iter_loss: 0.1482134610414505
train_iter_loss: 0.1495869904756546
train_iter_loss: 0.16636909544467926
train_iter_loss: 0.18428237736225128
train_iter_loss: 0.14130601286888123
train_iter_loss: 0.18364885449409485
train_iter_loss: 0.174172505736351
train_iter_loss: 0.12174241989850998
train_iter_loss: 0.11746308207511902
train_iter_loss: 0.1337658315896988
train_iter_loss: 0.17794248461723328
train_iter_loss: 0.07028153538703918
train_iter_loss: 0.18800370395183563
train_iter_loss: 0.1300850659608841
train_iter_loss: 0.13985008001327515
train_iter_loss: 0.17598293721675873
train_iter_loss: 0.13153624534606934
train_iter_loss: 0.1246613934636116
train_iter_loss: 0.21280911564826965
train_iter_loss: 0.05320703610777855
train_iter_loss: 0.13725002110004425
train_iter_loss: 0.14329718053340912
train_iter_loss: 0.17680220305919647
train_iter_loss: 0.16064788401126862
train_iter_loss: 0.15985515713691711
train_iter_loss: 0.18961696326732635
train_iter_loss: 0.1588529646396637
train_iter_loss: 0.22107236087322235
train_iter_loss: 0.2813628613948822
train_iter_loss: 0.19179587066173553
train_iter_loss: 0.09459824115037918
train_iter_loss: 0.18081329762935638
train_iter_loss: 0.17167912423610687
train_iter_loss: 0.0870070531964302
train_iter_loss: 0.23672480881214142
train_iter_loss: 0.1409377008676529
train_iter_loss: 0.08782222121953964
train_iter_loss: 0.14953258633613586
train_iter_loss: 0.21951337158679962
train_iter_loss: 0.146609827876091
train_iter_loss: 0.11247415840625763
train_iter_loss: 0.11833921819925308
train_iter_loss: 0.10776849836111069
train_iter_loss: 0.18909524381160736
train_iter_loss: 0.07472046464681625
train_iter_loss: 0.16766391694545746
train_iter_loss: 0.19058914482593536
train_iter_loss: 0.19774934649467468
train_iter_loss: 0.08860593289136887
train_iter_loss: 0.28213781118392944
train_iter_loss: 0.1696808636188507
train_iter_loss: 0.17215929925441742
train_iter_loss: 0.19334858655929565
train_iter_loss: 0.1686028689146042
train_iter_loss: 0.15765531361103058
train_iter_loss: 0.24882765114307404
train_iter_loss: 0.14261463284492493
train_iter_loss: 0.14513319730758667
train_iter_loss: 0.14195269346237183
train_iter_loss: 0.28413623571395874
train_iter_loss: 0.2772602438926697
train_iter_loss: 0.16200511157512665
train_iter_loss: 0.034467630088329315
train_iter_loss: 0.1758023053407669
train_iter_loss: 0.10474139451980591
train_iter_loss: 0.12234829366207123
train_iter_loss: 0.1520949900150299
train_iter_loss: 0.09080733358860016
train_iter_loss: 0.09884919226169586
train_iter_loss: 0.11858059465885162
train_iter_loss: 0.1384410262107849
train_iter_loss: 0.07266909629106522
train_iter_loss: 0.2058163434267044
train_iter_loss: 0.15196935832500458
train_iter_loss: 0.14431002736091614
train_iter_loss: 0.06465523689985275
train_iter_loss: 0.1297977864742279
train_iter_loss: 0.23930467665195465
train_iter_loss: 0.16236355900764465
train_iter_loss: 0.11048082262277603
train_iter_loss: 0.06397006660699844
train_iter_loss: 0.05370717495679855
train_iter_loss: 0.2061760574579239
train_iter_loss: 0.15549537539482117
train_iter_loss: 0.16079039871692657
train_iter_loss: 0.24194183945655823
train_iter_loss: 0.10627981275320053
train_iter_loss: 0.23569892346858978
train_iter_loss: 0.08149077743291855
train_iter_loss: 0.2422773838043213
train_iter_loss: 0.06561175733804703
train_iter_loss: 0.193820059299469
train_iter_loss: 0.11499597132205963
train_iter_loss: 0.16765719652175903
train_iter_loss: 0.11890023946762085
train loss :0.1556
---------------------
Validation seg loss: 0.2156406480572977 at epoch 574
epoch =    575/  1000, exp = train
train_iter_loss: 0.09680032730102539
train_iter_loss: 0.2044495940208435
train_iter_loss: 0.13108207285404205
train_iter_loss: 0.1258508414030075
train_iter_loss: 0.13184235990047455
train_iter_loss: 0.07255065441131592
train_iter_loss: 0.0831160694360733
train_iter_loss: 0.21173764765262604
train_iter_loss: 0.09933968633413315
train_iter_loss: 0.18963302671909332
train_iter_loss: 0.09505411982536316
train_iter_loss: 0.179658904671669
train_iter_loss: 0.09576816856861115
train_iter_loss: 0.1610233336687088
train_iter_loss: 0.07158385962247849
train_iter_loss: 0.22834528982639313
train_iter_loss: 0.14122126996517181
train_iter_loss: 0.13246995210647583
train_iter_loss: 0.2772215008735657
train_iter_loss: 0.22476476430892944
train_iter_loss: 0.10126728564500809
train_iter_loss: 0.23752865195274353
train_iter_loss: 0.12527406215667725
train_iter_loss: 0.29398828744888306
train_iter_loss: 0.3089153468608856
train_iter_loss: 0.0909341350197792
train_iter_loss: 0.06825362145900726
train_iter_loss: 0.07046428322792053
train_iter_loss: 0.1520249992609024
train_iter_loss: 0.10955537855625153
train_iter_loss: 0.03325406834483147
train_iter_loss: 0.11473499238491058
train_iter_loss: 0.1654966175556183
train_iter_loss: 0.11276878416538239
train_iter_loss: 0.13985584676265717
train_iter_loss: 0.23660612106323242
train_iter_loss: 0.08237003535032272
train_iter_loss: 0.23234619200229645
train_iter_loss: 0.1356237530708313
train_iter_loss: 0.14007583260536194
train_iter_loss: 0.18715201318264008
train_iter_loss: 0.1783035844564438
train_iter_loss: 0.21215373277664185
train_iter_loss: 0.17263124883174896
train_iter_loss: 0.2630898356437683
train_iter_loss: 0.39526399970054626
train_iter_loss: 0.19727091491222382
train_iter_loss: 0.11174385249614716
train_iter_loss: 0.17398281395435333
train_iter_loss: 0.14269855618476868
train_iter_loss: 0.14845982193946838
train_iter_loss: 0.10543928295373917
train_iter_loss: 0.08428782969713211
train_iter_loss: 0.17172934114933014
train_iter_loss: 0.20945990085601807
train_iter_loss: 0.08139345049858093
train_iter_loss: 0.08491295576095581
train_iter_loss: 0.29529446363449097
train_iter_loss: 0.08641660213470459
train_iter_loss: 0.30455079674720764
train_iter_loss: 0.23617510497570038
train_iter_loss: 0.1692800670862198
train_iter_loss: 0.18759167194366455
train_iter_loss: 0.09392046183347702
train_iter_loss: 0.1660725176334381
train_iter_loss: 0.22043366730213165
train_iter_loss: 0.10169445723295212
train_iter_loss: 0.1233171671628952
train_iter_loss: 0.26353663206100464
train_iter_loss: 0.17955820262432098
train_iter_loss: 0.3149448037147522
train_iter_loss: 0.20062710344791412
train_iter_loss: 0.22608712315559387
train_iter_loss: 0.15398213267326355
train_iter_loss: 0.16235023736953735
train_iter_loss: 0.20968738198280334
train_iter_loss: 0.17803922295570374
train_iter_loss: 0.1631167083978653
train_iter_loss: 0.0935685858130455
train_iter_loss: 0.11453178524971008
train_iter_loss: 0.2763079106807709
train_iter_loss: 0.04497933015227318
train_iter_loss: 0.042531173676252365
train_iter_loss: 0.2985262870788574
train_iter_loss: 0.15577922761440277
train_iter_loss: 0.15293988585472107
train_iter_loss: 0.10619223117828369
train_iter_loss: 0.12465795874595642
train_iter_loss: 0.05973249673843384
train_iter_loss: 0.15281258523464203
train_iter_loss: 0.17808987200260162
train_iter_loss: 0.1830752044916153
train_iter_loss: 0.21092531085014343
train_iter_loss: 0.0923873633146286
train_iter_loss: 0.1813821792602539
train_iter_loss: 0.020991643890738487
train_iter_loss: 0.14865700900554657
train_iter_loss: 0.1264602094888687
train_iter_loss: 0.12400917708873749
train_iter_loss: 0.183473601937294
train loss :0.1609
---------------------
Validation seg loss: 0.21659795030564913 at epoch 575
epoch =    576/  1000, exp = train
train_iter_loss: 0.10478110611438751
train_iter_loss: 0.09707870334386826
train_iter_loss: 0.09231239557266235
train_iter_loss: 0.2329653799533844
train_iter_loss: 0.2893854081630707
train_iter_loss: 0.2444857656955719
train_iter_loss: 0.19571879506111145
train_iter_loss: 0.1411963403224945
train_iter_loss: 0.11202440410852432
train_iter_loss: 0.20792914927005768
train_iter_loss: 0.14439617097377777
train_iter_loss: 0.16887405514717102
train_iter_loss: 0.10840233415365219
train_iter_loss: 0.17777368426322937
train_iter_loss: 0.13405992090702057
train_iter_loss: 0.20373928546905518
train_iter_loss: 0.10970453917980194
train_iter_loss: 0.16019898653030396
train_iter_loss: 0.17849647998809814
train_iter_loss: 0.14050574600696564
train_iter_loss: 0.23046007752418518
train_iter_loss: 0.23237039148807526
train_iter_loss: 0.10378073900938034
train_iter_loss: 0.09847692400217056
train_iter_loss: 0.18915602564811707
train_iter_loss: 0.18708285689353943
train_iter_loss: 0.13876082003116608
train_iter_loss: 0.1735379844903946
train_iter_loss: 0.10023394972085953
train_iter_loss: 0.06477153301239014
train_iter_loss: 0.1370495706796646
train_iter_loss: 0.16427049040794373
train_iter_loss: 0.1762896180152893
train_iter_loss: 0.12480738759040833
train_iter_loss: 0.16928260028362274
train_iter_loss: 0.13497188687324524
train_iter_loss: 0.07324676215648651
train_iter_loss: 0.1280687004327774
train_iter_loss: 0.1269640028476715
train_iter_loss: 0.11839380115270615
train_iter_loss: 0.1502627283334732
train_iter_loss: 0.0535864494740963
train_iter_loss: 0.16272006928920746
train_iter_loss: 0.29512494802474976
train_iter_loss: 0.08921848982572556
train_iter_loss: 0.1585913598537445
train_iter_loss: 0.11234036087989807
train_iter_loss: 0.15190522372722626
train_iter_loss: 0.30060118436813354
train_iter_loss: 0.08036522567272186
train_iter_loss: 0.17781421542167664
train_iter_loss: 0.14883938431739807
train_iter_loss: 0.05994163453578949
train_iter_loss: 0.09613068401813507
train_iter_loss: 0.13140293955802917
train_iter_loss: 0.2393849492073059
train_iter_loss: 0.1032787337899208
train_iter_loss: 0.15337271988391876
train_iter_loss: 0.1146918535232544
train_iter_loss: 0.08913426846265793
train_iter_loss: 0.12715591490268707
train_iter_loss: 0.11308054625988007
train_iter_loss: 0.24994111061096191
train_iter_loss: 0.05984019860625267
train_iter_loss: 0.12183167785406113
train_iter_loss: 0.16080822050571442
train_iter_loss: 0.17567528784275055
train_iter_loss: 0.09669912606477737
train_iter_loss: 0.10733576864004135
train_iter_loss: 0.11072230339050293
train_iter_loss: 0.09119630604982376
train_iter_loss: 0.11525370180606842
train_iter_loss: 0.2372366487979889
train_iter_loss: 0.2019972950220108
train_iter_loss: 0.17029720544815063
train_iter_loss: 0.1546482890844345
train_iter_loss: 0.187081977725029
train_iter_loss: 0.13271622359752655
train_iter_loss: 0.12673962116241455
train_iter_loss: 0.2050599604845047
train_iter_loss: 0.1704149693250656
train_iter_loss: 0.24324552714824677
train_iter_loss: 0.21311549842357635
train_iter_loss: 0.20293927192687988
train_iter_loss: 0.1877141296863556
train_iter_loss: 0.20318609476089478
train_iter_loss: 0.180297389626503
train_iter_loss: 0.3174540102481842
train_iter_loss: 0.19185926020145416
train_iter_loss: 0.15809999406337738
train_iter_loss: 0.16582174599170685
train_iter_loss: 0.15147778391838074
train_iter_loss: 0.13542117178440094
train_iter_loss: 0.17049099504947662
train_iter_loss: 0.15260733664035797
train_iter_loss: 0.06718721985816956
train_iter_loss: 0.11097811907529831
train_iter_loss: 0.12044097483158112
train_iter_loss: 0.235380157828331
train_iter_loss: 0.14731082320213318
train loss :0.1562
---------------------
Validation seg loss: 0.21707163281949624 at epoch 576
epoch =    577/  1000, exp = train
train_iter_loss: 0.20443430542945862
train_iter_loss: 0.14180006086826324
train_iter_loss: 0.1276746392250061
train_iter_loss: 0.17601019144058228
train_iter_loss: 0.16548600792884827
train_iter_loss: 0.22219419479370117
train_iter_loss: 0.23645780980587006
train_iter_loss: 0.08958868682384491
train_iter_loss: 0.15974754095077515
train_iter_loss: 0.14607663452625275
train_iter_loss: 0.10116449743509293
train_iter_loss: 0.2722654938697815
train_iter_loss: 0.08594900369644165
train_iter_loss: 0.1757839173078537
train_iter_loss: 0.20193719863891602
train_iter_loss: 0.1459777057170868
train_iter_loss: 0.19299550354480743
train_iter_loss: 0.2034206986427307
train_iter_loss: 0.19316509366035461
train_iter_loss: 0.19613823294639587
train_iter_loss: 0.2515394687652588
train_iter_loss: 0.19533587992191315
train_iter_loss: 0.15010863542556763
train_iter_loss: 0.09018632769584656
train_iter_loss: 0.11312814801931381
train_iter_loss: 0.12036892771720886
train_iter_loss: 0.09998060762882233
train_iter_loss: 0.2538110315799713
train_iter_loss: 0.06550084799528122
train_iter_loss: 0.1747162789106369
train_iter_loss: 0.11488810926675797
train_iter_loss: 0.18715554475784302
train_iter_loss: 0.22126026451587677
train_iter_loss: 0.11776755005121231
train_iter_loss: 0.22274164855480194
train_iter_loss: 0.18603049218654633
train_iter_loss: 0.15442417562007904
train_iter_loss: 0.1268102377653122
train_iter_loss: 0.11951419711112976
train_iter_loss: 0.08277358114719391
train_iter_loss: 0.1549568772315979
train_iter_loss: 0.2303684800863266
train_iter_loss: 0.1830277144908905
train_iter_loss: 0.14529819786548615
train_iter_loss: 0.10438349843025208
train_iter_loss: 0.1437859833240509
train_iter_loss: 0.14596231281757355
train_iter_loss: 0.08993564546108246
train_iter_loss: 0.12551239132881165
train_iter_loss: 0.08562254905700684
train_iter_loss: 0.13833238184452057
train_iter_loss: 0.1710902899503708
train_iter_loss: 0.08705519139766693
train_iter_loss: 0.2891225814819336
train_iter_loss: 0.23425526916980743
train_iter_loss: 0.09698486328125
train_iter_loss: 0.021472200751304626
train_iter_loss: 0.10608112812042236
train_iter_loss: 0.18889227509498596
train_iter_loss: 0.10949940979480743
train_iter_loss: 0.07900571078062057
train_iter_loss: 0.12910225987434387
train_iter_loss: 0.10620728880167007
train_iter_loss: 0.14448182284832
train_iter_loss: 0.07503639161586761
train_iter_loss: 0.15695498883724213
train_iter_loss: 0.10961603373289108
train_iter_loss: 0.11155171692371368
train_iter_loss: 0.06097584217786789
train_iter_loss: 0.1884859949350357
train_iter_loss: 0.26112180948257446
train_iter_loss: 0.29576393961906433
train_iter_loss: 0.2381495237350464
train_iter_loss: 0.13717706501483917
train_iter_loss: 0.169404074549675
train_iter_loss: 0.15271416306495667
train_iter_loss: 0.2907871901988983
train_iter_loss: 0.15390834212303162
train_iter_loss: 0.24297264218330383
train_iter_loss: 0.08745480328798294
train_iter_loss: 0.16257977485656738
train_iter_loss: 0.17604799568653107
train_iter_loss: 0.129551500082016
train_iter_loss: 0.23517514765262604
train_iter_loss: 0.17309053242206573
train_iter_loss: 0.1522185057401657
train_iter_loss: 0.1758459508419037
train_iter_loss: 0.30694231390953064
train_iter_loss: 0.11689192056655884
train_iter_loss: 0.08869816362857819
train_iter_loss: 0.13009780645370483
train_iter_loss: 0.20600391924381256
train_iter_loss: 0.13926196098327637
train_iter_loss: 0.07505221664905548
train_iter_loss: 0.11482954025268555
train_iter_loss: 0.16197091341018677
train_iter_loss: 0.160293310880661
train_iter_loss: 0.10537119954824448
train_iter_loss: 0.18545329570770264
train_iter_loss: 0.1601882129907608
train loss :0.1584
---------------------
Validation seg loss: 0.2119309892262912 at epoch 577
********************
best_val_epoch_loss:  0.2119309892262912
MODEL UPDATED
epoch =    578/  1000, exp = train
train_iter_loss: 0.18572254478931427
train_iter_loss: 0.10124017298221588
train_iter_loss: 0.1708107888698578
train_iter_loss: 0.2683519124984741
train_iter_loss: 0.12754622101783752
train_iter_loss: 0.17211821675300598
train_iter_loss: 0.20608265697956085
train_iter_loss: 0.32379934191703796
train_iter_loss: 0.0936625599861145
train_iter_loss: 0.1335173398256302
train_iter_loss: 0.20974181592464447
train_iter_loss: 0.16484300792217255
train_iter_loss: 0.20153653621673584
train_iter_loss: 0.16661615669727325
train_iter_loss: 0.12869563698768616
train_iter_loss: 0.2093973606824875
train_iter_loss: 0.13411211967468262
train_iter_loss: 0.15982000529766083
train_iter_loss: 0.1674407422542572
train_iter_loss: 0.17982710897922516
train_iter_loss: 0.12118681520223618
train_iter_loss: 0.066583551466465
train_iter_loss: 0.24117769300937653
train_iter_loss: 0.09447628259658813
train_iter_loss: 0.15641939640045166
train_iter_loss: 0.1902356594800949
train_iter_loss: 0.13988421857357025
train_iter_loss: 0.1071956679224968
train_iter_loss: 0.16478747129440308
train_iter_loss: 0.15850412845611572
train_iter_loss: 0.19973911345005035
train_iter_loss: 0.11754370480775833
train_iter_loss: 0.2479349672794342
train_iter_loss: 0.23866720497608185
train_iter_loss: 0.11605311930179596
train_iter_loss: 0.214756041765213
train_iter_loss: 0.132062166929245
train_iter_loss: 0.11335519701242447
train_iter_loss: 0.2132624238729477
train_iter_loss: 0.20128658413887024
train_iter_loss: 0.1081734374165535
train_iter_loss: 0.1659737378358841
train_iter_loss: 0.12703026831150055
train_iter_loss: 0.18837885558605194
train_iter_loss: 0.12257885187864304
train_iter_loss: 0.19066046178340912
train_iter_loss: 0.04484254866838455
train_iter_loss: 0.2638758718967438
train_iter_loss: 0.10494039207696915
train_iter_loss: 0.1396130919456482
train_iter_loss: 0.13536180555820465
train_iter_loss: 0.11391527205705643
train_iter_loss: 0.1121186912059784
train_iter_loss: 0.028876367956399918
train_iter_loss: 0.08722877502441406
train_iter_loss: 0.10200107842683792
train_iter_loss: 0.16446423530578613
train_iter_loss: 0.16701650619506836
train_iter_loss: 0.13831615447998047
train_iter_loss: 0.16130225360393524
train_iter_loss: 0.11908803880214691
train_iter_loss: 0.1724453568458557
train_iter_loss: 0.10126836597919464
train_iter_loss: 0.182427778840065
train_iter_loss: 0.06083636358380318
train_iter_loss: 0.20075871050357819
train_iter_loss: 0.13119396567344666
train_iter_loss: 0.12497945874929428
train_iter_loss: 0.14720113575458527
train_iter_loss: 0.08648086339235306
train_iter_loss: 0.1178860068321228
train_iter_loss: 0.1826004534959793
train_iter_loss: 0.2066204845905304
train_iter_loss: 0.1374036967754364
train_iter_loss: 0.30237051844596863
train_iter_loss: 0.24324116110801697
train_iter_loss: 0.3055565655231476
train_iter_loss: 0.13837240636348724
train_iter_loss: 0.10374008119106293
train_iter_loss: 0.16892923414707184
train_iter_loss: 0.1232052892446518
train_iter_loss: 0.23053885996341705
train_iter_loss: 0.31422045826911926
train_iter_loss: 0.2918611168861389
train_iter_loss: 0.13924260437488556
train_iter_loss: 0.16346076130867004
train_iter_loss: 0.15658876299858093
train_iter_loss: 0.2264784872531891
train_iter_loss: 0.15908288955688477
train_iter_loss: 0.10934269428253174
train_iter_loss: 0.21438705921173096
train_iter_loss: 0.15182240307331085
train_iter_loss: 0.16495275497436523
train_iter_loss: 0.17811451852321625
train_iter_loss: 0.24974720180034637
train_iter_loss: 0.05116875469684601
train_iter_loss: 0.1334535777568817
train_iter_loss: 0.4139688313007355
train_iter_loss: 0.18054722249507904
train_iter_loss: 0.05555481091141701
train loss :0.1651
---------------------
Validation seg loss: 0.21648392839497835 at epoch 578
epoch =    579/  1000, exp = train
train_iter_loss: 0.06977400183677673
train_iter_loss: 0.18831853568553925
train_iter_loss: 0.19739338755607605
train_iter_loss: 0.10126220434904099
train_iter_loss: 0.19246645271778107
train_iter_loss: 0.06705649942159653
train_iter_loss: 0.21316786110401154
train_iter_loss: 0.09047316759824753
train_iter_loss: 0.20876650512218475
train_iter_loss: 0.12044475227594376
train_iter_loss: 0.1169789582490921
train_iter_loss: 0.06873377412557602
train_iter_loss: 0.166790172457695
train_iter_loss: 0.1531325876712799
train_iter_loss: 0.07588006556034088
train_iter_loss: 0.2172417938709259
train_iter_loss: 0.08093275874853134
train_iter_loss: 0.195326566696167
train_iter_loss: 0.12916682660579681
train_iter_loss: 0.15090331435203552
train_iter_loss: 0.15778948366641998
train_iter_loss: 0.0987512469291687
train_iter_loss: 0.13883952796459198
train_iter_loss: 0.0483868308365345
train_iter_loss: 0.22509023547172546
train_iter_loss: 0.17926691472530365
train_iter_loss: 0.14082413911819458
train_iter_loss: 0.19208890199661255
train_iter_loss: 0.2185857594013214
train_iter_loss: 0.13049493730068207
train_iter_loss: 0.1770944744348526
train_iter_loss: 0.12666907906532288
train_iter_loss: 0.1581922024488449
train_iter_loss: 0.18093131482601166
train_iter_loss: 0.12275443971157074
train_iter_loss: 0.07019919902086258
train_iter_loss: 0.13040553033351898
train_iter_loss: 0.24545152485370636
train_iter_loss: 0.1322861760854721
train_iter_loss: 0.20362353324890137
train_iter_loss: 0.06788641959428787
train_iter_loss: 0.1273796409368515
train_iter_loss: 0.15866751968860626
train_iter_loss: 0.1626754105091095
train_iter_loss: 0.13124258816242218
train_iter_loss: 0.323300838470459
train_iter_loss: 0.12634435296058655
train_iter_loss: 0.19118300080299377
train_iter_loss: 0.15165427327156067
train_iter_loss: 0.14943690598011017
train_iter_loss: 0.29704752564430237
train_iter_loss: 0.1977369487285614
train_iter_loss: 0.1231296956539154
train_iter_loss: 0.03871701657772064
train_iter_loss: 0.11721867322921753
train_iter_loss: 0.1401117593050003
train_iter_loss: 0.088406503200531
train_iter_loss: 0.17429940402507782
train_iter_loss: 0.271712988615036
train_iter_loss: 0.09164009243249893
train_iter_loss: 0.0888543650507927
train_iter_loss: 0.07422716915607452
train_iter_loss: 0.14179940521717072
train_iter_loss: 0.20631565153598785
train_iter_loss: 0.1817927062511444
train_iter_loss: 0.17904648184776306
train_iter_loss: 0.2887633442878723
train_iter_loss: 0.17626149952411652
train_iter_loss: 0.12507879734039307
train_iter_loss: 0.14507697522640228
train_iter_loss: 0.2154059261083603
train_iter_loss: 0.34187236428260803
train_iter_loss: 0.08986490964889526
train_iter_loss: 0.1571342796087265
train_iter_loss: 0.21014611423015594
train_iter_loss: 0.2373989075422287
train_iter_loss: 0.10374695807695389
train_iter_loss: 0.07067183405160904
train_iter_loss: 0.12720440328121185
train_iter_loss: 0.21351031959056854
train_iter_loss: 0.13896851241588593
train_iter_loss: 0.1568591147661209
train_iter_loss: 0.16732823848724365
train_iter_loss: 0.22654205560684204
train_iter_loss: 0.10857781022787094
train_iter_loss: 0.17876115441322327
train_iter_loss: 0.07216500490903854
train_iter_loss: 0.16189400851726532
train_iter_loss: 0.13919469714164734
train_iter_loss: 0.19075101613998413
train_iter_loss: 0.1826467216014862
train_iter_loss: 0.16125482320785522
train_iter_loss: 0.0754069834947586
train_iter_loss: 0.09904827177524567
train_iter_loss: 0.07367632538080215
train_iter_loss: 0.16584962606430054
train_iter_loss: 0.13148461282253265
train_iter_loss: 0.15414565801620483
train_iter_loss: 0.3710486888885498
train_iter_loss: 0.2958376109600067
train loss :0.1580
---------------------
Validation seg loss: 0.2161238132448832 at epoch 579
epoch =    580/  1000, exp = train
train_iter_loss: 0.18534110486507416
train_iter_loss: 0.13325029611587524
train_iter_loss: 0.16156005859375
train_iter_loss: 0.12365768104791641
train_iter_loss: 0.11157851666212082
train_iter_loss: 0.18544310331344604
train_iter_loss: 0.17428359389305115
train_iter_loss: 0.17637979984283447
train_iter_loss: 0.15133970975875854
train_iter_loss: 0.15458594262599945
train_iter_loss: 0.0979529544711113
train_iter_loss: 0.13557375967502594
train_iter_loss: 0.14302602410316467
train_iter_loss: 0.16552016139030457
train_iter_loss: 0.09921608120203018
train_iter_loss: 0.1809438169002533
train_iter_loss: 0.12583154439926147
train_iter_loss: 0.2705758213996887
train_iter_loss: 0.19995346665382385
train_iter_loss: 0.15876483917236328
train_iter_loss: 0.22032848000526428
train_iter_loss: 0.027199098840355873
train_iter_loss: 0.16096851229667664
train_iter_loss: 0.12705881893634796
train_iter_loss: 0.07683875411748886
train_iter_loss: 0.13941070437431335
train_iter_loss: 0.18413491547107697
train_iter_loss: 0.17427563667297363
train_iter_loss: 0.2262430638074875
train_iter_loss: 0.20654484629631042
train_iter_loss: 0.14653629064559937
train_iter_loss: 0.14070938527584076
train_iter_loss: 0.19281509518623352
train_iter_loss: 0.0645260438323021
train_iter_loss: 0.12161245197057724
train_iter_loss: 0.21797960996627808
train_iter_loss: 0.11874252557754517
train_iter_loss: 0.10461486876010895
train_iter_loss: 0.15856286883354187
train_iter_loss: 0.13525837659835815
train_iter_loss: 0.15708860754966736
train_iter_loss: 0.1897181272506714
train_iter_loss: 0.19953618943691254
train_iter_loss: 0.12187079340219498
train_iter_loss: 0.18865033984184265
train_iter_loss: 0.12431488931179047
train_iter_loss: 0.106996551156044
train_iter_loss: 0.18579599261283875
train_iter_loss: 0.15165558457374573
train_iter_loss: 0.08736063539981842
train_iter_loss: 0.108446404337883
train_iter_loss: 0.17609059810638428
train_iter_loss: 0.07531090080738068
train_iter_loss: 0.18826916813850403
train_iter_loss: 0.1339881420135498
train_iter_loss: 0.12075476348400116
train_iter_loss: 0.21901869773864746
train_iter_loss: 0.24321496486663818
train_iter_loss: 0.12085283547639847
train_iter_loss: 0.1222538873553276
train_iter_loss: 0.12045418471097946
train_iter_loss: 0.09666673094034195
train_iter_loss: 0.11680614203214645
train_iter_loss: 0.11375175416469574
train_iter_loss: 0.15097929537296295
train_iter_loss: 0.17014183104038239
train_iter_loss: 0.31443485617637634
train_iter_loss: 0.1377980262041092
train_iter_loss: 0.08694090694189072
train_iter_loss: 0.2796304225921631
train_iter_loss: 0.08885825425386429
train_iter_loss: 0.1330396980047226
train_iter_loss: 0.19308672845363617
train_iter_loss: 0.15417872369289398
train_iter_loss: 0.05224287882447243
train_iter_loss: 0.16046330332756042
train_iter_loss: 0.14471377432346344
train_iter_loss: 0.19159448146820068
train_iter_loss: 0.08505197614431381
train_iter_loss: 0.04156038165092468
train_iter_loss: 0.14358267188072205
train_iter_loss: 0.10982774198055267
train_iter_loss: 0.3913174867630005
train_iter_loss: 0.10633596777915955
train_iter_loss: 0.20430490374565125
train_iter_loss: 0.04604030027985573
train_iter_loss: 0.1310911476612091
train_iter_loss: 0.2624690532684326
train_iter_loss: 0.1259220391511917
train_iter_loss: 0.20431798696517944
train_iter_loss: 0.1921786218881607
train_iter_loss: 0.17708732187747955
train_iter_loss: 0.15555867552757263
train_iter_loss: 0.21484076976776123
train_iter_loss: 0.2590092420578003
train_iter_loss: 0.08034830540418625
train_iter_loss: 0.1914849430322647
train_iter_loss: 0.18178994953632355
train_iter_loss: 0.17790646851062775
train_iter_loss: 0.10909166187047958
train loss :0.1556
---------------------
Validation seg loss: 0.21802735268050488 at epoch 580
epoch =    581/  1000, exp = train
train_iter_loss: 0.06605540961027145
train_iter_loss: 0.13637880980968475
train_iter_loss: 0.1823710948228836
train_iter_loss: 0.24030625820159912
train_iter_loss: 0.27619925141334534
train_iter_loss: 0.07175290584564209
train_iter_loss: 0.0698237195611
train_iter_loss: 0.19222711026668549
train_iter_loss: 0.23687437176704407
train_iter_loss: 0.15155984461307526
train_iter_loss: 0.1154896691441536
train_iter_loss: 0.13563156127929688
train_iter_loss: 0.17717207968235016
train_iter_loss: 0.10419152677059174
train_iter_loss: 0.13482099771499634
train_iter_loss: 0.18326681852340698
train_iter_loss: 0.29316508769989014
train_iter_loss: 0.12837804853916168
train_iter_loss: 0.21091866493225098
train_iter_loss: 0.12699973583221436
train_iter_loss: 0.16498732566833496
train_iter_loss: 0.26576387882232666
train_iter_loss: 0.1313670426607132
train_iter_loss: 0.10691186785697937
train_iter_loss: 0.13024842739105225
train_iter_loss: 0.09546992927789688
train_iter_loss: 0.1694234311580658
train_iter_loss: 0.17066548764705658
train_iter_loss: 0.08941346406936646
train_iter_loss: 0.14239732921123505
train_iter_loss: 0.14137707650661469
train_iter_loss: 0.15538854897022247
train_iter_loss: 0.09686540812253952
train_iter_loss: 0.18668682873249054
train_iter_loss: 0.17993853986263275
train_iter_loss: 0.11231229454278946
train_iter_loss: 0.0880691260099411
train_iter_loss: 0.12859275937080383
train_iter_loss: 0.046707797795534134
train_iter_loss: 0.1674044132232666
train_iter_loss: 0.1283658742904663
train_iter_loss: 0.13815023005008698
train_iter_loss: 0.06112650781869888
train_iter_loss: 0.09952285140752792
train_iter_loss: 0.2960667610168457
train_iter_loss: 0.15185464918613434
train_iter_loss: 0.1161249428987503
train_iter_loss: 0.16711369156837463
train_iter_loss: 0.152920201420784
train_iter_loss: 0.059386901557445526
train_iter_loss: 0.14893251657485962
train_iter_loss: 0.19434016942977905
train_iter_loss: 0.1872532218694687
train_iter_loss: 0.05703306943178177
train_iter_loss: 0.20656925439834595
train_iter_loss: 0.10133247077465057
train_iter_loss: 0.19353380799293518
train_iter_loss: 0.1705138236284256
train_iter_loss: 0.1088232770562172
train_iter_loss: 0.105650894343853
train_iter_loss: 0.12402331829071045
train_iter_loss: 0.16178113222122192
train_iter_loss: 0.1483650505542755
train_iter_loss: 0.2273218184709549
train_iter_loss: 0.1497512310743332
train_iter_loss: 0.10616590827703476
train_iter_loss: 0.07245863974094391
train_iter_loss: 0.12009813636541367
train_iter_loss: 0.16029459238052368
train_iter_loss: 0.17698714137077332
train_iter_loss: 0.17583328485488892
train_iter_loss: 0.22313269972801208
train_iter_loss: 0.24380651116371155
train_iter_loss: 0.12499058991670609
train_iter_loss: 0.12963025271892548
train_iter_loss: 0.1882535219192505
train_iter_loss: 0.10842414200305939
train_iter_loss: 0.12787184119224548
train_iter_loss: 0.2242552787065506
train_iter_loss: 0.10982353240251541
train_iter_loss: 0.2564910650253296
train_iter_loss: 0.07445355504751205
train_iter_loss: 0.24684850871562958
train_iter_loss: 0.20850348472595215
train_iter_loss: 0.3055034577846527
train_iter_loss: 0.1596090942621231
train_iter_loss: 0.20434747636318207
train_iter_loss: 0.16218248009681702
train_iter_loss: 0.10571987926959991
train_iter_loss: 0.18738648295402527
train_iter_loss: 0.23897524178028107
train_iter_loss: 0.2166614532470703
train_iter_loss: 0.12120161205530167
train_iter_loss: 0.16179992258548737
train_iter_loss: 0.10768977552652359
train_iter_loss: 0.17515195906162262
train_iter_loss: 0.2184731662273407
train_iter_loss: 0.1653091162443161
train_iter_loss: 0.13901996612548828
train_iter_loss: 0.20779724419116974
train loss :0.1577
---------------------
Validation seg loss: 0.21781503142929584 at epoch 581
epoch =    582/  1000, exp = train
train_iter_loss: 0.13808971643447876
train_iter_loss: 0.05292108282446861
train_iter_loss: 0.11778273433446884
train_iter_loss: 0.1681678295135498
train_iter_loss: 0.1172611191868782
train_iter_loss: 0.0834728553891182
train_iter_loss: 0.269546240568161
train_iter_loss: 0.1125473603606224
train_iter_loss: 0.15033145248889923
train_iter_loss: 0.11706531792879105
train_iter_loss: 0.261400043964386
train_iter_loss: 0.06287810206413269
train_iter_loss: 0.07887742668390274
train_iter_loss: 0.14703142642974854
train_iter_loss: 0.14473554491996765
train_iter_loss: 0.07925578951835632
train_iter_loss: 0.32974618673324585
train_iter_loss: 0.0895366370677948
train_iter_loss: 0.1400275081396103
train_iter_loss: 0.09056564420461655
train_iter_loss: 0.09868859499692917
train_iter_loss: 0.1627701222896576
train_iter_loss: 0.19927991926670074
train_iter_loss: 0.15745294094085693
train_iter_loss: 0.1770499050617218
train_iter_loss: 0.24373385310173035
train_iter_loss: 0.19381912052631378
train_iter_loss: 0.13489684462547302
train_iter_loss: 0.1396358609199524
train_iter_loss: 0.15323691070079803
train_iter_loss: 0.16322964429855347
train_iter_loss: 0.276578426361084
train_iter_loss: 0.09350412338972092
train_iter_loss: 0.1514025330543518
train_iter_loss: 0.10679800808429718
train_iter_loss: 0.10447383671998978
train_iter_loss: 0.23031772673130035
train_iter_loss: 0.15269705653190613
train_iter_loss: 0.18432281911373138
train_iter_loss: 0.08585510402917862
train_iter_loss: 0.13268673419952393
train_iter_loss: 0.2903507351875305
train_iter_loss: 0.15152820944786072
train_iter_loss: 0.05514282360672951
train_iter_loss: 0.15585173666477203
train_iter_loss: 0.136220782995224
train_iter_loss: 0.12018043547868729
train_iter_loss: 0.15446804463863373
train_iter_loss: 0.06570856273174286
train_iter_loss: 0.19788439571857452
train_iter_loss: 0.25580108165740967
train_iter_loss: 0.13386131823062897
train_iter_loss: 0.1147305816411972
train_iter_loss: 0.12798233330249786
train_iter_loss: 0.06967227905988693
train_iter_loss: 0.19989582896232605
train_iter_loss: 0.2712079584598541
train_iter_loss: 0.051116738468408585
train_iter_loss: 0.10161839425563812
train_iter_loss: 0.22451573610305786
train_iter_loss: 0.18449318408966064
train_iter_loss: 0.21882714331150055
train_iter_loss: 0.22428296506404877
train_iter_loss: 0.342377245426178
train_iter_loss: 0.1452198475599289
train_iter_loss: 0.14828385412693024
train_iter_loss: 0.10774166882038116
train_iter_loss: 0.10781220346689224
train_iter_loss: 0.06942596286535263
train_iter_loss: 0.10029459744691849
train_iter_loss: 0.15473975241184235
train_iter_loss: 0.10431349277496338
train_iter_loss: 0.10363172739744186
train_iter_loss: 0.13253268599510193
train_iter_loss: 0.14228665828704834
train_iter_loss: 0.349454790353775
train_iter_loss: 0.18890531361103058
train_iter_loss: 0.19690299034118652
train_iter_loss: 0.15916547179222107
train_iter_loss: 0.15169553458690643
train_iter_loss: 0.24043118953704834
train_iter_loss: 0.16586032509803772
train_iter_loss: 0.1299709528684616
train_iter_loss: 0.20001623034477234
train_iter_loss: 0.03844187781214714
train_iter_loss: 0.2637762129306793
train_iter_loss: 0.10438564419746399
train_iter_loss: 0.148292675614357
train_iter_loss: 0.2537495493888855
train_iter_loss: 0.1412840187549591
train_iter_loss: 0.11972862482070923
train_iter_loss: 0.20949916541576385
train_iter_loss: 0.11808032542467117
train_iter_loss: 0.09577981382608414
train_iter_loss: 0.15060915052890778
train_iter_loss: 0.28292301297187805
train_iter_loss: 0.07579660415649414
train_iter_loss: 0.14530044794082642
train_iter_loss: 0.12859483063220978
train_iter_loss: 0.19833990931510925
train loss :0.1567
---------------------
Validation seg loss: 0.2204033592364417 at epoch 582
epoch =    583/  1000, exp = train
train_iter_loss: 0.21080030500888824
train_iter_loss: 0.21009816229343414
train_iter_loss: 0.14229759573936462
train_iter_loss: 0.22552169859409332
train_iter_loss: 0.12076562643051147
train_iter_loss: 0.18408581614494324
train_iter_loss: 0.09291153401136398
train_iter_loss: 0.16665035486221313
train_iter_loss: 0.10571324080228806
train_iter_loss: 0.13120724260807037
train_iter_loss: 0.16977781057357788
train_iter_loss: 0.25226736068725586
train_iter_loss: 0.15015073120594025
train_iter_loss: 0.15974651277065277
train_iter_loss: 0.22088034451007843
train_iter_loss: 0.11925727128982544
train_iter_loss: 0.15800759196281433
train_iter_loss: 0.23031753301620483
train_iter_loss: 0.10885541886091232
train_iter_loss: 0.12860967218875885
train_iter_loss: 0.08843345195055008
train_iter_loss: 0.14486350119113922
train_iter_loss: 0.13016819953918457
train_iter_loss: 0.06505123525857925
train_iter_loss: 0.07539782673120499
train_iter_loss: 0.1417241394519806
train_iter_loss: 0.23299947381019592
train_iter_loss: 0.14109082520008087
train_iter_loss: 0.15824735164642334
train_iter_loss: 0.09296715259552002
train_iter_loss: 0.1519637256860733
train_iter_loss: 0.14590685069561005
train_iter_loss: 0.17556621134281158
train_iter_loss: 0.08046877384185791
train_iter_loss: 0.1786319613456726
train_iter_loss: 0.15380138158798218
train_iter_loss: 0.12944850325584412
train_iter_loss: 0.22082296013832092
train_iter_loss: 0.22883263230323792
train_iter_loss: 0.24605484306812286
train_iter_loss: 0.13054339587688446
train_iter_loss: 0.21580985188484192
train_iter_loss: 0.12252537906169891
train_iter_loss: 0.09332432597875595
train_iter_loss: 0.08018013834953308
train_iter_loss: 0.22605659067630768
train_iter_loss: 0.09680785238742828
train_iter_loss: 0.11445532739162445
train_iter_loss: 0.05130189657211304
train_iter_loss: 0.21713624894618988
train_iter_loss: 0.15319710969924927
train_iter_loss: 0.0917443335056305
train_iter_loss: 0.148392453789711
train_iter_loss: 0.1432134211063385
train_iter_loss: 0.18033483624458313
train_iter_loss: 0.08719882369041443
train_iter_loss: 0.16211362183094025
train_iter_loss: 0.12249314039945602
train_iter_loss: 0.3873778283596039
train_iter_loss: 0.13292944431304932
train_iter_loss: 0.13164636492729187
train_iter_loss: 0.147332563996315
train_iter_loss: 0.10146740823984146
train_iter_loss: 0.10637306421995163
train_iter_loss: 0.08168508857488632
train_iter_loss: 0.20231424272060394
train_iter_loss: 0.1580994576215744
train_iter_loss: 0.17505376040935516
train_iter_loss: 0.12391349673271179
train_iter_loss: 0.18302729725837708
train_iter_loss: 0.06703221797943115
train_iter_loss: 0.12111398577690125
train_iter_loss: 0.10804443806409836
train_iter_loss: 0.11254528164863586
train_iter_loss: 0.0482184961438179
train_iter_loss: 0.09895148873329163
train_iter_loss: 0.17999747395515442
train_iter_loss: 0.19802933931350708
train_iter_loss: 0.1625407487154007
train_iter_loss: 0.07642839103937149
train_iter_loss: 0.30323049426078796
train_iter_loss: 0.1827820986509323
train_iter_loss: 0.1920449584722519
train_iter_loss: 0.28580424189567566
train_iter_loss: 0.07178696990013123
train_iter_loss: 0.1414201408624649
train_iter_loss: 0.208690345287323
train_iter_loss: 0.09202971309423447
train_iter_loss: 0.10972239077091217
train_iter_loss: 0.2002543956041336
train_iter_loss: 0.2162097692489624
train_iter_loss: 0.19396716356277466
train_iter_loss: 0.13654883205890656
train_iter_loss: 0.19312217831611633
train_iter_loss: 0.28398236632347107
train_iter_loss: 0.17284062504768372
train_iter_loss: 0.38015660643577576
train_iter_loss: 0.14584466814994812
train_iter_loss: 0.20371674001216888
train_iter_loss: 0.16334272921085358
train loss :0.1585
---------------------
Validation seg loss: 0.22136964262375292 at epoch 583
epoch =    584/  1000, exp = train
train_iter_loss: 0.1981482058763504
train_iter_loss: 0.14724811911582947
train_iter_loss: 0.07441984117031097
train_iter_loss: 0.11297226697206497
train_iter_loss: 0.11242615431547165
train_iter_loss: 0.1047208234667778
train_iter_loss: 0.24933263659477234
train_iter_loss: 0.10481474548578262
train_iter_loss: 0.10644791275262833
train_iter_loss: 0.0960412248969078
train_iter_loss: 0.18470075726509094
train_iter_loss: 0.11443165689706802
train_iter_loss: 0.2549149990081787
train_iter_loss: 0.18789885938167572
train_iter_loss: 0.13156867027282715
train_iter_loss: 0.21677012741565704
train_iter_loss: 0.13198168575763702
train_iter_loss: 0.08602016419172287
train_iter_loss: 0.10756248980760574
train_iter_loss: 0.144100621342659
train_iter_loss: 0.15793345868587494
train_iter_loss: 0.0815289169549942
train_iter_loss: 0.1434047520160675
train_iter_loss: 0.09051927924156189
train_iter_loss: 0.06547237932682037
train_iter_loss: 0.09602387249469757
train_iter_loss: 0.20347155630588531
train_iter_loss: 0.13528451323509216
train_iter_loss: 0.1474786102771759
train_iter_loss: 0.10674081742763519
train_iter_loss: 0.14678260684013367
train_iter_loss: 0.17006102204322815
train_iter_loss: 0.2059127688407898
train_iter_loss: 0.13383984565734863
train_iter_loss: 0.14416052401065826
train_iter_loss: 0.07313348352909088
train_iter_loss: 0.09253980219364166
train_iter_loss: 0.16332511603832245
train_iter_loss: 0.13523021340370178
train_iter_loss: 0.22199203073978424
train_iter_loss: 0.12912821769714355
train_iter_loss: 0.11829246580600739
train_iter_loss: 0.18276576697826385
train_iter_loss: 0.09847107529640198
train_iter_loss: 0.20256933569908142
train_iter_loss: 0.09344179928302765
train_iter_loss: 0.1252908557653427
train_iter_loss: 0.11249445378780365
train_iter_loss: 0.10305396467447281
train_iter_loss: 0.22782102227210999
train_iter_loss: 0.16216346621513367
train_iter_loss: 0.1281018853187561
train_iter_loss: 0.18228410184383392
train_iter_loss: 0.09675601869821548
train_iter_loss: 0.08610190451145172
train_iter_loss: 0.08403634279966354
train_iter_loss: 0.06762321293354034
train_iter_loss: 0.10696035623550415
train_iter_loss: 0.19289663434028625
train_iter_loss: 0.3678992688655853
train_iter_loss: 0.3035978078842163
train_iter_loss: 0.045440323650836945
train_iter_loss: 0.14742666482925415
train_iter_loss: 0.19105471670627594
train_iter_loss: 0.10862436145544052
train_iter_loss: 0.13333821296691895
train_iter_loss: 0.14292874932289124
train_iter_loss: 0.09253305196762085
train_iter_loss: 0.10736050456762314
train_iter_loss: 0.13330823183059692
train_iter_loss: 0.14888833463191986
train_iter_loss: 0.0855044275522232
train_iter_loss: 0.12638023495674133
train_iter_loss: 0.13911528885364532
train_iter_loss: 0.17746564745903015
train_iter_loss: 0.21301831305027008
train_iter_loss: 0.2078779637813568
train_iter_loss: 0.20325009524822235
train_iter_loss: 0.06473314762115479
train_iter_loss: 0.1330864429473877
train_iter_loss: 0.15235203504562378
train_iter_loss: 0.19632112979888916
train_iter_loss: 0.22094371914863586
train_iter_loss: 0.10045444220304489
train_iter_loss: 0.24464590847492218
train_iter_loss: 0.1099785789847374
train_iter_loss: 0.145757794380188
train_iter_loss: 0.14758646488189697
train_iter_loss: 0.19819015264511108
train_iter_loss: 0.1772746592760086
train_iter_loss: 0.40301334857940674
train_iter_loss: 0.22034624218940735
train_iter_loss: 0.1483028382062912
train_iter_loss: 0.11094598472118378
train_iter_loss: 0.08846873044967651
train_iter_loss: 0.22916708886623383
train_iter_loss: 0.1966705471277237
train_iter_loss: 0.12250471860170364
train_iter_loss: 0.08785820007324219
train_iter_loss: 0.19944614171981812
train loss :0.1501
---------------------
Validation seg loss: 0.21947067923281552 at epoch 584
epoch =    585/  1000, exp = train
train_iter_loss: 0.14401501417160034
train_iter_loss: 0.12224610894918442
train_iter_loss: 0.18511104583740234
train_iter_loss: 0.1341806948184967
train_iter_loss: 0.13751313090324402
train_iter_loss: 0.08330937474966049
train_iter_loss: 0.2151588499546051
train_iter_loss: 0.12043935805559158
train_iter_loss: 0.20808489620685577
train_iter_loss: 0.13661262392997742
train_iter_loss: 0.13804291188716888
train_iter_loss: 0.10703188925981522
train_iter_loss: 0.14949053525924683
train_iter_loss: 0.28969401121139526
train_iter_loss: 0.08740083128213882
train_iter_loss: 0.23166853189468384
train_iter_loss: 0.1985948234796524
train_iter_loss: 0.2040591686964035
train_iter_loss: 0.12218271940946579
train_iter_loss: 0.10620755702257156
train_iter_loss: 0.1613682210445404
train_iter_loss: 0.1932038962841034
train_iter_loss: 0.34133222699165344
train_iter_loss: 0.0659012496471405
train_iter_loss: 0.09659934788942337
train_iter_loss: 0.17795808613300323
train_iter_loss: 0.10751868784427643
train_iter_loss: 0.13994470238685608
train_iter_loss: 0.2407410740852356
train_iter_loss: 0.08369694650173187
train_iter_loss: 0.12431630492210388
train_iter_loss: 0.09748326987028122
train_iter_loss: 0.31180551648139954
train_iter_loss: 0.21001672744750977
train_iter_loss: 0.23509490489959717
train_iter_loss: 0.24166609346866608
train_iter_loss: 0.12856438755989075
train_iter_loss: 0.1266556829214096
train_iter_loss: 0.1533675342798233
train_iter_loss: 0.19220659136772156
train_iter_loss: 0.126860573887825
train_iter_loss: 0.2077476531267166
train_iter_loss: 0.23373381793498993
train_iter_loss: 0.12568825483322144
train_iter_loss: 0.10490310937166214
train_iter_loss: 0.19443604350090027
train_iter_loss: 0.1674814224243164
train_iter_loss: 0.126228928565979
train_iter_loss: 0.11347422748804092
train_iter_loss: 0.31230008602142334
train_iter_loss: 0.15578904747962952
train_iter_loss: 0.1149134635925293
train_iter_loss: 0.07086743414402008
train_iter_loss: 0.13141512870788574
train_iter_loss: 0.13877764344215393
train_iter_loss: 0.16003811359405518
train_iter_loss: 0.12607485055923462
train_iter_loss: 0.26988986134529114
train_iter_loss: 0.17798985540866852
train_iter_loss: 0.1837107539176941
train_iter_loss: 0.12539871037006378
train_iter_loss: 0.10206593573093414
train_iter_loss: 0.10343749821186066
train_iter_loss: 0.14049746096134186
train_iter_loss: 0.09842675924301147
train_iter_loss: 0.21612784266471863
train_iter_loss: 0.13088728487491608
train_iter_loss: 0.21065226197242737
train_iter_loss: 0.1405782401561737
train_iter_loss: 0.22806772589683533
train_iter_loss: 0.16141581535339355
train_iter_loss: 0.2317916452884674
train_iter_loss: 0.13170282542705536
train_iter_loss: 0.203194260597229
train_iter_loss: 0.13482148945331573
train_iter_loss: 0.18078266084194183
train_iter_loss: 0.22011293470859528
train_iter_loss: 0.16202481091022491
train_iter_loss: 0.0845780074596405
train_iter_loss: 0.11791340261697769
train_iter_loss: 0.18540295958518982
train_iter_loss: 0.15489362180233002
train_iter_loss: 0.18813727796077728
train_iter_loss: 0.1912996619939804
train_iter_loss: 0.27203282713890076
train_iter_loss: 0.23519569635391235
train_iter_loss: 0.06735940277576447
train_iter_loss: 0.1628061681985855
train_iter_loss: 0.14746327698230743
train_iter_loss: 0.11101852357387543
train_iter_loss: 0.06398151814937592
train_iter_loss: 0.22116026282310486
train_iter_loss: 0.1382768750190735
train_iter_loss: 0.17723149061203003
train_iter_loss: 0.09158138930797577
train_iter_loss: 0.16141553223133087
train_iter_loss: 0.16106337308883667
train_iter_loss: 0.1006777435541153
train_iter_loss: 0.07816800475120544
train_iter_loss: 0.23234513401985168
train loss :0.1621
---------------------
Validation seg loss: 0.21564174817769594 at epoch 585
epoch =    586/  1000, exp = train
train_iter_loss: 0.21176324784755707
train_iter_loss: 0.13249391317367554
train_iter_loss: 0.12014573812484741
train_iter_loss: 0.18634048104286194
train_iter_loss: 0.19142526388168335
train_iter_loss: 0.12708589434623718
train_iter_loss: 0.22050251066684723
train_iter_loss: 0.15062931180000305
train_iter_loss: 0.10021130740642548
train_iter_loss: 0.1727219969034195
train_iter_loss: 0.07204920053482056
train_iter_loss: 0.0645245611667633
train_iter_loss: 0.2453581541776657
train_iter_loss: 0.1873006820678711
train_iter_loss: 0.21407587826251984
train_iter_loss: 0.17251521348953247
train_iter_loss: 0.3216063380241394
train_iter_loss: 0.13048161566257477
train_iter_loss: 0.31949567794799805
train_iter_loss: 0.27498725056648254
train_iter_loss: 0.1815735548734665
train_iter_loss: 0.17107044160366058
train_iter_loss: 0.10042732208967209
train_iter_loss: 0.17094597220420837
train_iter_loss: 0.1773649901151657
train_iter_loss: 0.07098030298948288
train_iter_loss: 0.13214080035686493
train_iter_loss: 0.325801819562912
train_iter_loss: 0.2269047200679779
train_iter_loss: 0.2557603716850281
train_iter_loss: 0.2852463126182556
train_iter_loss: 0.07694052904844284
train_iter_loss: 0.09540434181690216
train_iter_loss: 0.12322818487882614
train_iter_loss: 0.10122577100992203
train_iter_loss: 0.13301265239715576
train_iter_loss: 0.15898624062538147
train_iter_loss: 0.1538015753030777
train_iter_loss: 0.16828356683254242
train_iter_loss: 0.16211192309856415
train_iter_loss: 0.16755786538124084
train_iter_loss: 0.17864300310611725
train_iter_loss: 0.13402394950389862
train_iter_loss: 0.052933406084775925
train_iter_loss: 0.13178598880767822
train_iter_loss: 0.12767188251018524
train_iter_loss: 0.03435075655579567
train_iter_loss: 0.20760507881641388
train_iter_loss: 0.06231534108519554
train_iter_loss: 0.19672013819217682
train_iter_loss: 0.02553643099963665
train_iter_loss: 0.17304125428199768
train_iter_loss: 0.24746696650981903
train_iter_loss: 0.12046954780817032
train_iter_loss: 0.1892976611852646
train_iter_loss: 0.20564374327659607
train_iter_loss: 0.10318484157323837
train_iter_loss: 0.07562258094549179
train_iter_loss: 0.2183200567960739
train_iter_loss: 0.05529475212097168
train_iter_loss: 0.2113766074180603
train_iter_loss: 0.17063532769680023
train_iter_loss: 0.11169885098934174
train_iter_loss: 0.10040255635976791
train_iter_loss: 0.18154753744602203
train_iter_loss: 0.14307492971420288
train_iter_loss: 0.1851370930671692
train_iter_loss: 0.21266962587833405
train_iter_loss: 0.24096055328845978
train_iter_loss: 0.13369180262088776
train_iter_loss: 0.12065201252698898
train_iter_loss: 0.13308535516262054
train_iter_loss: 0.1491163969039917
train_iter_loss: 0.074678435921669
train_iter_loss: 0.28697720170021057
train_iter_loss: 0.14146876335144043
train_iter_loss: 0.28438764810562134
train_iter_loss: 0.22606408596038818
train_iter_loss: 0.15598469972610474
train_iter_loss: 0.12003730237483978
train_iter_loss: 0.24032874405384064
train_iter_loss: 0.14944729208946228
train_iter_loss: 0.14726392924785614
train_iter_loss: 0.08766131848096848
train_iter_loss: 0.1604844331741333
train_iter_loss: 0.0876980572938919
train_iter_loss: 0.09445370733737946
train_iter_loss: 0.25835269689559937
train_iter_loss: 0.20798556506633759
train_iter_loss: 0.10610022395849228
train_iter_loss: 0.12116435170173645
train_iter_loss: 0.14280648529529572
train_iter_loss: 0.12405367195606232
train_iter_loss: 0.18092530965805054
train_iter_loss: 0.19955874979496002
train_iter_loss: 0.19163291156291962
train_iter_loss: 0.10988453030586243
train_iter_loss: 0.12047220766544342
train_iter_loss: 0.18893413245677948
train_iter_loss: 0.03190326318144798
train loss :0.1609
---------------------
Validation seg loss: 0.21537606435704906 at epoch 586
epoch =    587/  1000, exp = train
train_iter_loss: 0.2257169634103775
train_iter_loss: 0.11401252448558807
train_iter_loss: 0.06366723030805588
train_iter_loss: 0.14701971411705017
train_iter_loss: 0.17469504475593567
train_iter_loss: 0.1622163951396942
train_iter_loss: 0.20190127193927765
train_iter_loss: 0.275980144739151
train_iter_loss: 0.07975305616855621
train_iter_loss: 0.22642363607883453
train_iter_loss: 0.10163843631744385
train_iter_loss: 0.19177956879138947
train_iter_loss: 0.16780951619148254
train_iter_loss: 0.18624351918697357
train_iter_loss: 0.18119986355304718
train_iter_loss: 0.16504663228988647
train_iter_loss: 0.07294278591871262
train_iter_loss: 0.216451495885849
train_iter_loss: 0.19777171313762665
train_iter_loss: 0.21505948901176453
train_iter_loss: 0.2440277487039566
train_iter_loss: 0.27474069595336914
train_iter_loss: 0.12673845887184143
train_iter_loss: 0.23308780789375305
train_iter_loss: 0.10554296523332596
train_iter_loss: 0.17170636355876923
train_iter_loss: 0.07634041458368301
train_iter_loss: 0.12848341464996338
train_iter_loss: 0.1642063558101654
train_iter_loss: 0.15022744238376617
train_iter_loss: 0.2368856966495514
train_iter_loss: 0.04993738979101181
train_iter_loss: 0.2044425755739212
train_iter_loss: 0.15034513175487518
train_iter_loss: 0.03352535516023636
train_iter_loss: 0.14180918037891388
train_iter_loss: 0.29368823766708374
train_iter_loss: 0.1553778052330017
train_iter_loss: 0.13078972697257996
train_iter_loss: 0.1896980106830597
train_iter_loss: 0.10219207406044006
train_iter_loss: 0.1453574299812317
train_iter_loss: 0.12414371222257614
train_iter_loss: 0.2651267945766449
train_iter_loss: 0.07740280777215958
train_iter_loss: 0.1943550854921341
train_iter_loss: 0.16253891587257385
train_iter_loss: 0.23002377152442932
train_iter_loss: 0.13625961542129517
train_iter_loss: 0.13906295597553253
train_iter_loss: 0.04974137246608734
train_iter_loss: 0.18384170532226562
train_iter_loss: 0.1307135671377182
train_iter_loss: 0.08712530136108398
train_iter_loss: 0.2361472249031067
train_iter_loss: 0.16562296450138092
train_iter_loss: 0.11266554147005081
train_iter_loss: 0.16521331667900085
train_iter_loss: 0.1562443971633911
train_iter_loss: 0.10554663836956024
train_iter_loss: 0.19850392639636993
train_iter_loss: 0.08634237200021744
train_iter_loss: 0.2272815853357315
train_iter_loss: 0.07299819588661194
train_iter_loss: 0.3185456097126007
train_iter_loss: 0.06971649825572968
train_iter_loss: 0.09753919392824173
train_iter_loss: 0.35455742478370667
train_iter_loss: 0.22729434072971344
train_iter_loss: 0.1278057098388672
train_iter_loss: 0.16100120544433594
train_iter_loss: 0.20876426994800568
train_iter_loss: 0.12410902976989746
train_iter_loss: 0.19917944073677063
train_iter_loss: 0.09963249415159225
train_iter_loss: 0.0911819189786911
train_iter_loss: 0.13506121933460236
train_iter_loss: 0.16860908269882202
train_iter_loss: 0.14308390021324158
train_iter_loss: 0.1494496911764145
train_iter_loss: 0.16056834161281586
train_iter_loss: 0.08201827853918076
train_iter_loss: 0.10689304769039154
train_iter_loss: 0.21130047738552094
train_iter_loss: 0.2168126404285431
train_iter_loss: 0.1557626873254776
train_iter_loss: 0.15206359326839447
train_iter_loss: 0.1489204615354538
train_iter_loss: 0.16157501935958862
train_iter_loss: 0.1497446894645691
train_iter_loss: 0.1984129697084427
train_iter_loss: 0.14851532876491547
train_iter_loss: 0.12644478678703308
train_iter_loss: 0.2135087549686432
train_iter_loss: 0.10364104062318802
train_iter_loss: 0.13023659586906433
train_iter_loss: 0.08245653659105301
train_iter_loss: 0.1691102236509323
train_iter_loss: 0.0920044481754303
train_iter_loss: 0.14058765769004822
train loss :0.1597
---------------------
Validation seg loss: 0.2164751507675732 at epoch 587
epoch =    588/  1000, exp = train
train_iter_loss: 0.11052261292934418
train_iter_loss: 0.11394322663545609
train_iter_loss: 0.17424853146076202
train_iter_loss: 0.1316879391670227
train_iter_loss: 0.07699504494667053
train_iter_loss: 0.1765461564064026
train_iter_loss: 0.13714049756526947
train_iter_loss: 0.06361129879951477
train_iter_loss: 0.2042502462863922
train_iter_loss: 0.0945061594247818
train_iter_loss: 0.21632368862628937
train_iter_loss: 0.15487337112426758
train_iter_loss: 0.14608120918273926
train_iter_loss: 0.5137437582015991
train_iter_loss: 0.1144089549779892
train_iter_loss: 0.2071368247270584
train_iter_loss: 0.13282467424869537
train_iter_loss: 0.10949437320232391
train_iter_loss: 0.09517937153577805
train_iter_loss: 0.18254490196704865
train_iter_loss: 0.054540012031793594
train_iter_loss: 0.18981875479221344
train_iter_loss: 0.276176393032074
train_iter_loss: 0.07746139168739319
train_iter_loss: 0.13940836489200592
train_iter_loss: 0.12529125809669495
train_iter_loss: 0.21404977142810822
train_iter_loss: 0.11222831159830093
train_iter_loss: 0.08240199834108353
train_iter_loss: 0.07979559898376465
train_iter_loss: 0.2612546682357788
train_iter_loss: 0.14738430082798004
train_iter_loss: 0.07934712618589401
train_iter_loss: 0.2052692323923111
train_iter_loss: 0.06916528195142746
train_iter_loss: 0.15411131083965302
train_iter_loss: 0.12180101871490479
train_iter_loss: 0.16270096600055695
train_iter_loss: 0.17601872980594635
train_iter_loss: 0.25821393728256226
train_iter_loss: 0.1357560157775879
train_iter_loss: 0.1054254025220871
train_iter_loss: 0.09128298610448837
train_iter_loss: 0.11280659586191177
train_iter_loss: 0.09894067794084549
train_iter_loss: 0.1782444417476654
train_iter_loss: 0.2706611454486847
train_iter_loss: 0.1391770988702774
train_iter_loss: 0.2401181161403656
train_iter_loss: 0.1587255746126175
train_iter_loss: 0.19344796240329742
train_iter_loss: 0.18215414881706238
train_iter_loss: 0.2047889083623886
train_iter_loss: 0.16912095248699188
train_iter_loss: 0.21141882240772247
train_iter_loss: 0.19832171499729156
train_iter_loss: 0.10949265956878662
train_iter_loss: 0.17170396447181702
train_iter_loss: 0.0565495640039444
train_iter_loss: 0.19844967126846313
train_iter_loss: 0.12719960510730743
train_iter_loss: 0.12030574679374695
train_iter_loss: 0.36216822266578674
train_iter_loss: 0.1128530353307724
train_iter_loss: 0.17681746184825897
train_iter_loss: 0.23103655874729156
train_iter_loss: 0.12648425996303558
train_iter_loss: 0.15174899995326996
train_iter_loss: 0.12136519700288773
train_iter_loss: 0.1832251250743866
train_iter_loss: 0.19775694608688354
train_iter_loss: 0.2065631002187729
train_iter_loss: 0.13213104009628296
train_iter_loss: 0.10242079198360443
train_iter_loss: 0.13008785247802734
train_iter_loss: 0.13229969143867493
train_iter_loss: 0.18219412863254547
train_iter_loss: 0.24769406020641327
train_iter_loss: 0.2235385626554489
train_iter_loss: 0.09579338133335114
train_iter_loss: 0.2057177871465683
train_iter_loss: 0.18148475885391235
train_iter_loss: 0.19258394837379456
train_iter_loss: 0.0907888188958168
train_iter_loss: 0.07145179808139801
train_iter_loss: 0.11518816649913788
train_iter_loss: 0.1431599110364914
train_iter_loss: 0.1514241248369217
train_iter_loss: 0.20357678830623627
train_iter_loss: 0.20897088944911957
train_iter_loss: 0.16742068529129028
train_iter_loss: 0.1704709678888321
train_iter_loss: 0.19573533535003662
train_iter_loss: 0.18789397180080414
train_iter_loss: 0.10388019680976868
train_iter_loss: 0.09697507321834564
train_iter_loss: 0.17738820612430573
train_iter_loss: 0.2534169554710388
train_iter_loss: 0.12206277251243591
train_iter_loss: 0.10112059116363525
train loss :0.1603
---------------------
Validation seg loss: 0.21551680129091694 at epoch 588
epoch =    589/  1000, exp = train
train_iter_loss: 0.16288164258003235
train_iter_loss: 0.18088914453983307
train_iter_loss: 0.16315846145153046
train_iter_loss: 0.3032321631908417
train_iter_loss: 0.2141883373260498
train_iter_loss: 0.2529970109462738
train_iter_loss: 0.1034749448299408
train_iter_loss: 0.2466108649969101
train_iter_loss: 0.06719761341810226
train_iter_loss: 0.14434927701950073
train_iter_loss: 0.3083730638027191
train_iter_loss: 0.15633174777030945
train_iter_loss: 0.1159866526722908
train_iter_loss: 0.16904529929161072
train_iter_loss: 0.11547546088695526
train_iter_loss: 0.11839266121387482
train_iter_loss: 0.12092359364032745
train_iter_loss: 0.22518399357795715
train_iter_loss: 0.1099252700805664
train_iter_loss: 0.07332141697406769
train_iter_loss: 0.0675954669713974
train_iter_loss: 0.15406468510627747
train_iter_loss: 0.09864949434995651
train_iter_loss: 0.054914362728595734
train_iter_loss: 0.15351448953151703
train_iter_loss: 0.09775872528553009
train_iter_loss: 0.14282862842082977
train_iter_loss: 0.06498514115810394
train_iter_loss: 0.2736026346683502
train_iter_loss: 0.20290787518024445
train_iter_loss: 0.17436730861663818
train_iter_loss: 0.20774775743484497
train_iter_loss: 0.18743425607681274
train_iter_loss: 0.11701027303934097
train_iter_loss: 0.1422252058982849
train_iter_loss: 0.20315659046173096
train_iter_loss: 0.12018078565597534
train_iter_loss: 0.18426038324832916
train_iter_loss: 0.15080717206001282
train_iter_loss: 0.15388919413089752
train_iter_loss: 0.33155760169029236
train_iter_loss: 0.09461149573326111
train_iter_loss: 0.10185179114341736
train_iter_loss: 0.09981747716665268
train_iter_loss: 0.08046184480190277
train_iter_loss: 0.10833282023668289
train_iter_loss: 0.11819487065076828
train_iter_loss: 0.11523107439279556
train_iter_loss: 0.13512609899044037
train_iter_loss: 0.16211509704589844
train_iter_loss: 0.05501377955079079
train_iter_loss: 0.16629654169082642
train_iter_loss: 0.36447256803512573
train_iter_loss: 0.10765936970710754
train_iter_loss: 0.22327420115470886
train_iter_loss: 0.12224658578634262
train_iter_loss: 0.23929181694984436
train_iter_loss: 0.1141308844089508
train_iter_loss: 0.10286339372396469
train_iter_loss: 0.09902098029851913
train_iter_loss: 0.10798510164022446
train_iter_loss: 0.10155869275331497
train_iter_loss: 0.25081273913383484
train_iter_loss: 0.22910334169864655
train_iter_loss: 0.10781650245189667
train_iter_loss: 0.15405933558940887
train_iter_loss: 0.2032955139875412
train_iter_loss: 0.18202294409275055
train_iter_loss: 0.07754579931497574
train_iter_loss: 0.36615654826164246
train_iter_loss: 0.3503713011741638
train_iter_loss: 0.20709128677845
train_iter_loss: 0.11670605838298798
train_iter_loss: 0.07115955650806427
train_iter_loss: 0.24304889142513275
train_iter_loss: 0.14938534796237946
train_iter_loss: 0.13743628561496735
train_iter_loss: 0.10825936496257782
train_iter_loss: 0.5860211849212646
train_iter_loss: 0.21322980523109436
train_iter_loss: 0.22651919722557068
train_iter_loss: 0.21159972250461578
train_iter_loss: 0.1744101196527481
train_iter_loss: 0.10704667866230011
train_iter_loss: 0.22302204370498657
train_iter_loss: 0.2455887347459793
train_iter_loss: 0.11672616004943848
train_iter_loss: 0.06612791121006012
train_iter_loss: 0.1459076702594757
train_iter_loss: 0.20779062807559967
train_iter_loss: 0.13224215805530548
train_iter_loss: 0.091755710542202
train_iter_loss: 0.08526981621980667
train_iter_loss: 0.1497146040201187
train_iter_loss: 0.21507760882377625
train_iter_loss: 0.08996101468801498
train_iter_loss: 0.04935528337955475
train_iter_loss: 0.2841789424419403
train_iter_loss: 0.11743199080228806
train_iter_loss: 0.22215873003005981
train loss :0.1653
---------------------
Validation seg loss: 0.2139266242431301 at epoch 589
epoch =    590/  1000, exp = train
train_iter_loss: 0.14667390286922455
train_iter_loss: 0.14504216611385345
train_iter_loss: 0.13640360534191132
train_iter_loss: 0.1979490965604782
train_iter_loss: 0.21363866329193115
train_iter_loss: 0.14121463894844055
train_iter_loss: 0.1796908974647522
train_iter_loss: 0.2836231589317322
train_iter_loss: 0.07256471365690231
train_iter_loss: 0.06376508623361588
train_iter_loss: 0.12083720415830612
train_iter_loss: 0.09336880594491959
train_iter_loss: 0.11489864438772202
train_iter_loss: 0.17950187623500824
train_iter_loss: 0.13888226449489594
train_iter_loss: 0.1501156985759735
train_iter_loss: 0.2012740820646286
train_iter_loss: 0.22881852090358734
train_iter_loss: 0.2324628233909607
train_iter_loss: 0.08551705628633499
train_iter_loss: 0.21065635979175568
train_iter_loss: 0.14951464533805847
train_iter_loss: 0.09134023636579514
train_iter_loss: 0.07724133878946304
train_iter_loss: 0.08796937763690948
train_iter_loss: 0.23177801072597504
train_iter_loss: 0.15798063576221466
train_iter_loss: 0.19141483306884766
train_iter_loss: 0.18159428238868713
train_iter_loss: 0.12426506727933884
train_iter_loss: 0.12499017268419266
train_iter_loss: 0.18999136984348297
train_iter_loss: 0.06684412807226181
train_iter_loss: 0.17667865753173828
train_iter_loss: 0.16976864635944366
train_iter_loss: 0.18202172219753265
train_iter_loss: 0.18105676770210266
train_iter_loss: 0.0744948461651802
train_iter_loss: 0.2809625267982483
train_iter_loss: 0.26373225450515747
train_iter_loss: 0.18194982409477234
train_iter_loss: 0.12462396174669266
train_iter_loss: 0.057724032551050186
train_iter_loss: 0.1484617441892624
train_iter_loss: 0.18726792931556702
train_iter_loss: 0.2078397572040558
train_iter_loss: 0.14512330293655396
train_iter_loss: 0.15565255284309387
train_iter_loss: 0.33982565999031067
train_iter_loss: 0.0903225988149643
train_iter_loss: 0.1350586861371994
train_iter_loss: 0.2977454364299774
train_iter_loss: 0.1743129938840866
train_iter_loss: 0.1455441266298294
train_iter_loss: 0.2736489474773407
train_iter_loss: 0.14589159190654755
train_iter_loss: 0.07547078281641006
train_iter_loss: 0.06014767289161682
train_iter_loss: 0.08362996578216553
train_iter_loss: 0.07914935797452927
train_iter_loss: 0.11088041961193085
train_iter_loss: 0.16419506072998047
train_iter_loss: 0.20155379176139832
train_iter_loss: 0.1218077763915062
train_iter_loss: 0.1814284473657608
train_iter_loss: 0.2510906457901001
train_iter_loss: 0.17714710533618927
train_iter_loss: 0.12367548048496246
train_iter_loss: 0.10567130893468857
train_iter_loss: 0.06615027785301208
train_iter_loss: 0.17507433891296387
train_iter_loss: 0.17553149163722992
train_iter_loss: 0.08161762356758118
train_iter_loss: 0.01834491640329361
train_iter_loss: 0.13534563779830933
train_iter_loss: 0.2388545423746109
train_iter_loss: 0.21662995219230652
train_iter_loss: 0.27425330877304077
train_iter_loss: 0.2526634633541107
train_iter_loss: 0.16144025325775146
train_iter_loss: 0.23107384145259857
train_iter_loss: 0.14165537059307098
train_iter_loss: 0.3408719003200531
train_iter_loss: 0.22299394011497498
train_iter_loss: 0.15044750273227692
train_iter_loss: 0.16090421378612518
train_iter_loss: 0.02824247069656849
train_iter_loss: 0.1679677665233612
train_iter_loss: 0.12197169661521912
train_iter_loss: 0.16623950004577637
train_iter_loss: 0.20645935833454132
train_iter_loss: 0.12838760018348694
train_iter_loss: 0.15743939578533173
train_iter_loss: 0.13630267977714539
train_iter_loss: 0.146165668964386
train_iter_loss: 0.19021029770374298
train_iter_loss: 0.04035855829715729
train_iter_loss: 0.11409423500299454
train_iter_loss: 0.12187281250953674
train_iter_loss: 0.20475558936595917
train loss :0.1603
---------------------
Validation seg loss: 0.2172187814787733 at epoch 590
epoch =    591/  1000, exp = train
train_iter_loss: 0.1252061426639557
train_iter_loss: 0.0949862152338028
train_iter_loss: 0.08297034353017807
train_iter_loss: 0.11866635829210281
train_iter_loss: 0.34082746505737305
train_iter_loss: 0.11740075051784515
train_iter_loss: 0.09182225167751312
train_iter_loss: 0.16953285038471222
train_iter_loss: 0.06945997476577759
train_iter_loss: 0.21461014449596405
train_iter_loss: 0.19742213189601898
train_iter_loss: 0.12483220547437668
train_iter_loss: 0.10356535762548447
train_iter_loss: 0.19345755875110626
train_iter_loss: 0.269790381193161
train_iter_loss: 0.12623392045497894
train_iter_loss: 0.2118694931268692
train_iter_loss: 0.15917155146598816
train_iter_loss: 0.1482674777507782
train_iter_loss: 0.16817082464694977
train_iter_loss: 0.19913242757320404
train_iter_loss: 0.15763646364212036
train_iter_loss: 0.11849603056907654
train_iter_loss: 0.0795869305729866
train_iter_loss: 0.13308481872081757
train_iter_loss: 0.21257305145263672
train_iter_loss: 0.1362866312265396
train_iter_loss: 0.11838093400001526
train_iter_loss: 0.17099863290786743
train_iter_loss: 0.1144031286239624
train_iter_loss: 0.1756179928779602
train_iter_loss: 0.0929972231388092
train_iter_loss: 0.25832515954971313
train_iter_loss: 0.12745250761508942
train_iter_loss: 0.29476770758628845
train_iter_loss: 0.18876196444034576
train_iter_loss: 0.1868792176246643
train_iter_loss: 0.13828031718730927
train_iter_loss: 0.13563334941864014
train_iter_loss: 0.15398775041103363
train_iter_loss: 0.17697399854660034
train_iter_loss: 0.10716340690851212
train_iter_loss: 0.2804533541202545
train_iter_loss: 0.13785286247730255
train_iter_loss: 0.17315083742141724
train_iter_loss: 0.17064647376537323
train_iter_loss: 0.1614091843366623
train_iter_loss: 0.14848355948925018
train_iter_loss: 0.10380478203296661
train_iter_loss: 0.12086836993694305
train_iter_loss: 0.3346795439720154
train_iter_loss: 0.13008512556552887
train_iter_loss: 0.11191447079181671
train_iter_loss: 0.1554814875125885
train_iter_loss: 0.08863230049610138
train_iter_loss: 0.09506068378686905
train_iter_loss: 0.15033850073814392
train_iter_loss: 0.13806858658790588
train_iter_loss: 0.33904385566711426
train_iter_loss: 0.10863173753023148
train_iter_loss: 0.03638872131705284
train_iter_loss: 0.15122392773628235
train_iter_loss: 0.16099585592746735
train_iter_loss: 0.2241758555173874
train_iter_loss: 0.13787871599197388
train_iter_loss: 0.0993136540055275
train_iter_loss: 0.20430007576942444
train_iter_loss: 0.10897403955459595
train_iter_loss: 0.17300571501255035
train_iter_loss: 0.0906233936548233
train_iter_loss: 0.10268332064151764
train_iter_loss: 0.14487093687057495
train_iter_loss: 0.11573159694671631
train_iter_loss: 0.22569841146469116
train_iter_loss: 0.08118542283773422
train_iter_loss: 0.11763402819633484
train_iter_loss: 0.14535202085971832
train_iter_loss: 0.1367262452840805
train_iter_loss: 0.22902736067771912
train_iter_loss: 0.18016108870506287
train_iter_loss: 0.11825159937143326
train_iter_loss: 0.1299472451210022
train_iter_loss: 0.1462976634502411
train_iter_loss: 0.160575732588768
train_iter_loss: 0.12149392813444138
train_iter_loss: 0.2981155216693878
train_iter_loss: 0.098967544734478
train_iter_loss: 0.24258314073085785
train_iter_loss: 0.16780324280261993
train_iter_loss: 0.11682343482971191
train_iter_loss: 0.2346535474061966
train_iter_loss: 0.3367680311203003
train_iter_loss: 0.124568410217762
train_iter_loss: 0.11608876287937164
train_iter_loss: 0.15177150070667267
train_iter_loss: 0.13167566061019897
train_iter_loss: 0.15592187643051147
train_iter_loss: 0.1512220948934555
train_iter_loss: 0.2648186981678009
train_iter_loss: 0.1702079027891159
train loss :0.1602
---------------------
Validation seg loss: 0.220815087853105 at epoch 591
epoch =    592/  1000, exp = train
train_iter_loss: 0.3370148539543152
train_iter_loss: 0.32279857993125916
train_iter_loss: 0.11309780180454254
train_iter_loss: 0.024109620600938797
train_iter_loss: 0.1762448251247406
train_iter_loss: 0.10467634350061417
train_iter_loss: 0.056328240782022476
train_iter_loss: 0.21667054295539856
train_iter_loss: 0.12138821184635162
train_iter_loss: 0.13696756958961487
train_iter_loss: 0.24498555064201355
train_iter_loss: 0.09124267846345901
train_iter_loss: 0.07478363066911697
train_iter_loss: 0.14110949635505676
train_iter_loss: 0.09310755878686905
train_iter_loss: 0.18836253881454468
train_iter_loss: 0.2267899066209793
train_iter_loss: 0.20141717791557312
train_iter_loss: 0.09563048183917999
train_iter_loss: 0.0636797547340393
train_iter_loss: 0.24878054857254028
train_iter_loss: 0.17403554916381836
train_iter_loss: 0.3153970241546631
train_iter_loss: 0.19987571239471436
train_iter_loss: 0.14232386648654938
train_iter_loss: 0.19779948890209198
train_iter_loss: 0.15902626514434814
train_iter_loss: 0.16553723812103271
train_iter_loss: 0.20154064893722534
train_iter_loss: 0.07762875407934189
train_iter_loss: 0.18375496566295624
train_iter_loss: 0.13818621635437012
train_iter_loss: 0.18150953948497772
train_iter_loss: 0.22299900650978088
train_iter_loss: 0.08345915377140045
train_iter_loss: 0.08542490750551224
train_iter_loss: 0.07676748931407928
train_iter_loss: 0.07941081374883652
train_iter_loss: 0.18539747595787048
train_iter_loss: 0.07518579810857773
train_iter_loss: 0.23496143519878387
train_iter_loss: 0.09449922293424606
train_iter_loss: 0.09943407773971558
train_iter_loss: 0.18795602023601532
train_iter_loss: 0.16855159401893616
train_iter_loss: 0.20455491542816162
train_iter_loss: 0.13328950107097626
train_iter_loss: 0.25720107555389404
train_iter_loss: 0.19990812242031097
train_iter_loss: 0.17520391941070557
train_iter_loss: 0.14678548276424408
train_iter_loss: 0.1370970606803894
train_iter_loss: 0.18740031123161316
train_iter_loss: 0.21717849373817444
train_iter_loss: 0.044959865510463715
train_iter_loss: 0.21714811027050018
train_iter_loss: 0.09041385352611542
train_iter_loss: 0.12755891680717468
train_iter_loss: 0.06352126598358154
train_iter_loss: 0.12688873708248138
train_iter_loss: 0.09546110033988953
train_iter_loss: 0.09804780781269073
train_iter_loss: 0.11974568665027618
train_iter_loss: 0.14337918162345886
train_iter_loss: 0.1381809413433075
train_iter_loss: 0.15008118748664856
train_iter_loss: 0.08855677396059036
train_iter_loss: 0.29381123185157776
train_iter_loss: 0.18218058347702026
train_iter_loss: 0.15500685572624207
train_iter_loss: 0.2399362176656723
train_iter_loss: 0.09680460393428802
train_iter_loss: 0.10557593405246735
train_iter_loss: 0.1287188082933426
train_iter_loss: 0.1494160145521164
train_iter_loss: 0.21505965292453766
train_iter_loss: 0.08148368448019028
train_iter_loss: 0.08663538843393326
train_iter_loss: 0.12460512667894363
train_iter_loss: 0.14283064007759094
train_iter_loss: 0.14176718890666962
train_iter_loss: 0.2690720558166504
train_iter_loss: 0.18562175333499908
train_iter_loss: 0.17765019834041595
train_iter_loss: 0.24855972826480865
train_iter_loss: 0.047566577792167664
train_iter_loss: 0.17473457753658295
train_iter_loss: 0.14992621541023254
train_iter_loss: 0.16694137454032898
train_iter_loss: 0.08934681862592697
train_iter_loss: 0.1998821198940277
train_iter_loss: 0.2114877551794052
train_iter_loss: 0.12914171814918518
train_iter_loss: 0.26707881689071655
train_iter_loss: 0.35063090920448303
train_iter_loss: 0.1429399698972702
train_iter_loss: 0.13009046018123627
train_iter_loss: 0.13845619559288025
train_iter_loss: 0.2850232720375061
train_iter_loss: 0.20196335017681122
train loss :0.1604
---------------------
Validation seg loss: 0.2175727042299256 at epoch 592
epoch =    593/  1000, exp = train
train_iter_loss: 0.11448018252849579
train_iter_loss: 0.0867358073592186
train_iter_loss: 0.14882127940654755
train_iter_loss: 0.21128542721271515
train_iter_loss: 0.13940586149692535
train_iter_loss: 0.06250105798244476
train_iter_loss: 0.15090766549110413
train_iter_loss: 0.2291468381881714
train_iter_loss: 0.14757177233695984
train_iter_loss: 0.2676783502101898
train_iter_loss: 0.09763853251934052
train_iter_loss: 0.13853470981121063
train_iter_loss: 0.16767966747283936
train_iter_loss: 0.22833594679832458
train_iter_loss: 0.15409894287586212
train_iter_loss: 0.09093725681304932
train_iter_loss: 0.11421464383602142
train_iter_loss: 0.13302376866340637
train_iter_loss: 0.15251590311527252
train_iter_loss: 0.1736699789762497
train_iter_loss: 0.17118892073631287
train_iter_loss: 0.2099914252758026
train_iter_loss: 0.15368877351284027
train_iter_loss: 0.19256645441055298
train_iter_loss: 0.22962184250354767
train_iter_loss: 0.09023389965295792
train_iter_loss: 0.229216068983078
train_iter_loss: 0.10096221417188644
train_iter_loss: 0.1687179058790207
train_iter_loss: 0.14355379343032837
train_iter_loss: 0.120203398168087
train_iter_loss: 0.09804371744394302
train_iter_loss: 0.16852408647537231
train_iter_loss: 0.033750295639038086
train_iter_loss: 0.17949214577674866
train_iter_loss: 0.049867771565914154
train_iter_loss: 0.13284124433994293
train_iter_loss: 0.290269136428833
train_iter_loss: 0.1676892638206482
train_iter_loss: 0.10842616856098175
train_iter_loss: 0.07283459603786469
train_iter_loss: 0.23196326196193695
train_iter_loss: 0.1529291868209839
train_iter_loss: 0.09092000871896744
train_iter_loss: 0.11141980439424515
train_iter_loss: 0.09530097991228104
train_iter_loss: 0.11302591115236282
train_iter_loss: 0.13978256285190582
train_iter_loss: 0.12717780470848083
train_iter_loss: 0.3551798462867737
train_iter_loss: 0.060493916273117065
train_iter_loss: 0.3286658227443695
train_iter_loss: 0.1275681108236313
train_iter_loss: 0.20499084889888763
train_iter_loss: 0.21314996480941772
train_iter_loss: 0.10562466830015182
train_iter_loss: 0.29827526211738586
train_iter_loss: 0.22447755932807922
train_iter_loss: 0.12736386060714722
train_iter_loss: 0.16782577335834503
train_iter_loss: 0.09628380089998245
train_iter_loss: 0.21632923185825348
train_iter_loss: 0.13415689766407013
train_iter_loss: 0.17321895062923431
train_iter_loss: 0.16535700857639313
train_iter_loss: 0.19309860467910767
train_iter_loss: 0.0813717469573021
train_iter_loss: 0.15945908427238464
train_iter_loss: 0.1804160475730896
train_iter_loss: 0.1514883190393448
train_iter_loss: 0.17614693939685822
train_iter_loss: 0.2388211041688919
train_iter_loss: 0.1802419126033783
train_iter_loss: 0.2208833247423172
train_iter_loss: 0.13220736384391785
train_iter_loss: 0.22006504237651825
train_iter_loss: 0.16958977282047272
train_iter_loss: 0.24321423470973969
train_iter_loss: 0.10321266204118729
train_iter_loss: 0.15977197885513306
train_iter_loss: 0.17668890953063965
train_iter_loss: 0.07272106409072876
train_iter_loss: 0.1044682115316391
train_iter_loss: 0.23209130764007568
train_iter_loss: 0.29785576462745667
train_iter_loss: 0.12045565992593765
train_iter_loss: 0.130167156457901
train_iter_loss: 0.1878577172756195
train_iter_loss: 0.1667499840259552
train_iter_loss: 0.1398710012435913
train_iter_loss: 0.18739083409309387
train_iter_loss: 0.08321155607700348
train_iter_loss: 0.1842164397239685
train_iter_loss: 0.10951228439807892
train_iter_loss: 0.19347356259822845
train_iter_loss: 0.15198476612567902
train_iter_loss: 0.24860383570194244
train_iter_loss: 0.1662028729915619
train_iter_loss: 0.14477266371250153
train_iter_loss: 0.05748749524354935
train loss :0.1611
---------------------
Validation seg loss: 0.21788141843950692 at epoch 593
epoch =    594/  1000, exp = train
train_iter_loss: 0.11377333104610443
train_iter_loss: 0.2542629837989807
train_iter_loss: 0.15491235256195068
train_iter_loss: 0.0829581618309021
train_iter_loss: 0.2927643358707428
train_iter_loss: 0.1572481095790863
train_iter_loss: 0.18589462339878082
train_iter_loss: 0.15113647282123566
train_iter_loss: 0.11445498466491699
train_iter_loss: 0.13278847932815552
train_iter_loss: 0.1543540507555008
train_iter_loss: 0.1337941586971283
train_iter_loss: 0.07496201246976852
train_iter_loss: 0.0706143006682396
train_iter_loss: 0.2693633735179901
train_iter_loss: 0.14917545020580292
train_iter_loss: 0.10285940021276474
train_iter_loss: 0.1610497534275055
train_iter_loss: 0.2763727605342865
train_iter_loss: 0.11451280862092972
train_iter_loss: 0.09790070354938507
train_iter_loss: 0.05764096975326538
train_iter_loss: 0.12849563360214233
train_iter_loss: 0.05541419982910156
train_iter_loss: 0.17380395531654358
train_iter_loss: 0.16133685410022736
train_iter_loss: 0.09933152794837952
train_iter_loss: 0.16661591827869415
train_iter_loss: 0.18301184475421906
train_iter_loss: 0.12230234593153
train_iter_loss: 0.14904484152793884
train_iter_loss: 0.1546083688735962
train_iter_loss: 0.10209358483552933
train_iter_loss: 0.09494737535715103
train_iter_loss: 0.10967449098825455
train_iter_loss: 0.12952452898025513
train_iter_loss: 0.082244873046875
train_iter_loss: 0.15635044872760773
train_iter_loss: 0.25970396399497986
train_iter_loss: 0.06872489303350449
train_iter_loss: 0.1025666669011116
train_iter_loss: 0.31711703538894653
train_iter_loss: 0.15539707243442535
train_iter_loss: 0.2744136154651642
train_iter_loss: 0.34950387477874756
train_iter_loss: 0.09227146208286285
train_iter_loss: 0.17134585976600647
train_iter_loss: 0.22507748007774353
train_iter_loss: 0.13796502351760864
train_iter_loss: 0.09731153398752213
train_iter_loss: 0.07199372351169586
train_iter_loss: 0.17683833837509155
train_iter_loss: 0.17914864420890808
train_iter_loss: 0.1428777426481247
train_iter_loss: 0.2768152952194214
train_iter_loss: 0.25350409746170044
train_iter_loss: 0.08120367676019669
train_iter_loss: 0.1955220252275467
train_iter_loss: 0.17923559248447418
train_iter_loss: 0.16971102356910706
train_iter_loss: 0.04297586902976036
train_iter_loss: 0.16401757299900055
train_iter_loss: 0.17020782828330994
train_iter_loss: 0.1487046629190445
train_iter_loss: 0.1345171481370926
train_iter_loss: 0.13799670338630676
train_iter_loss: 0.16137762367725372
train_iter_loss: 0.15071076154708862
train_iter_loss: 0.2835627496242523
train_iter_loss: 0.0747670829296112
train_iter_loss: 0.16093850135803223
train_iter_loss: 0.08026871830224991
train_iter_loss: 0.06312242150306702
train_iter_loss: 0.14216768741607666
train_iter_loss: 0.14898276329040527
train_iter_loss: 0.16723337769508362
train_iter_loss: 0.19161245226860046
train_iter_loss: 0.16897866129875183
train_iter_loss: 0.10941213369369507
train_iter_loss: 0.11298937350511551
train_iter_loss: 0.2653464376926422
train_iter_loss: 0.10298358649015427
train_iter_loss: 0.14875707030296326
train_iter_loss: 0.13237866759300232
train_iter_loss: 0.10331391543149948
train_iter_loss: 0.12704655528068542
train_iter_loss: 0.1059843897819519
train_iter_loss: 0.12340667098760605
train_iter_loss: 0.1614554226398468
train_iter_loss: 0.22181740403175354
train_iter_loss: 0.14941227436065674
train_iter_loss: 0.19518893957138062
train_iter_loss: 0.09440682083368301
train_iter_loss: 0.16382339596748352
train_iter_loss: 0.22329138219356537
train_iter_loss: 0.14273646473884583
train_iter_loss: 0.19154801964759827
train_iter_loss: 0.10485388338565826
train_iter_loss: 0.17365430295467377
train_iter_loss: 0.17029647529125214
train loss :0.1543
---------------------
Validation seg loss: 0.21963581610646732 at epoch 594
epoch =    595/  1000, exp = train
train_iter_loss: 0.2559013366699219
train_iter_loss: 0.09363605082035065
train_iter_loss: 0.22175389528274536
train_iter_loss: 0.21023350954055786
train_iter_loss: 0.22350983321666718
train_iter_loss: 0.09770384430885315
train_iter_loss: 0.1413189023733139
train_iter_loss: 0.16850218176841736
train_iter_loss: 0.12870553135871887
train_iter_loss: 0.1861230731010437
train_iter_loss: 0.1320820152759552
train_iter_loss: 0.06333523988723755
train_iter_loss: 0.12158413231372833
train_iter_loss: 0.31719857454299927
train_iter_loss: 0.11785367876291275
train_iter_loss: 0.13279642164707184
train_iter_loss: 0.1138908788561821
train_iter_loss: 0.14116482436656952
train_iter_loss: 0.093623086810112
train_iter_loss: 0.17478711903095245
train_iter_loss: 0.11998245120048523
train_iter_loss: 0.15554627776145935
train_iter_loss: 0.206260085105896
train_iter_loss: 0.15416155755519867
train_iter_loss: 0.10786829143762589
train_iter_loss: 0.11957651376724243
train_iter_loss: 0.1502145379781723
train_iter_loss: 0.12639208137989044
train_iter_loss: 0.12332890927791595
train_iter_loss: 0.2247522920370102
train_iter_loss: 0.15506356954574585
train_iter_loss: 0.1181609258055687
train_iter_loss: 0.14591269195079803
train_iter_loss: 0.2528465688228607
train_iter_loss: 0.1437009572982788
train_iter_loss: 0.08910176903009415
train_iter_loss: 0.21967405080795288
train_iter_loss: 0.22667568922042847
train_iter_loss: 0.10061370581388474
train_iter_loss: 0.13802845776081085
train_iter_loss: 0.22919555008411407
train_iter_loss: 0.13727939128875732
train_iter_loss: 0.12725912034511566
train_iter_loss: 0.10189016163349152
train_iter_loss: 0.13390600681304932
train_iter_loss: 0.18223659694194794
train_iter_loss: 0.12987235188484192
train_iter_loss: 0.1632741391658783
train_iter_loss: 0.23090891540050507
train_iter_loss: 0.13488569855690002
train_iter_loss: 0.06653515249490738
train_iter_loss: 0.1051725372672081
train_iter_loss: 0.09836293756961823
train_iter_loss: 0.1384323537349701
train_iter_loss: 0.17097081243991852
train_iter_loss: 0.16460487246513367
train_iter_loss: 0.13957609236240387
train_iter_loss: 0.21291662752628326
train_iter_loss: 0.13554273545742035
train_iter_loss: 0.1142599880695343
train_iter_loss: 0.2717471420764923
train_iter_loss: 0.16199690103530884
train_iter_loss: 0.10834230482578278
train_iter_loss: 0.21588847041130066
train_iter_loss: 0.17543157935142517
train_iter_loss: 0.06364300101995468
train_iter_loss: 0.21666742861270905
train_iter_loss: 0.15161855518817902
train_iter_loss: 0.10638733953237534
train_iter_loss: 0.20804190635681152
train_iter_loss: 0.14541219174861908
train_iter_loss: 0.07552476227283478
train_iter_loss: 0.11786291748285294
train_iter_loss: 0.21926580369472504
train_iter_loss: 0.13183526694774628
train_iter_loss: 0.26145973801612854
train_iter_loss: 0.10997597128152847
train_iter_loss: 0.10697638988494873
train_iter_loss: 0.2252887636423111
train_iter_loss: 0.2129422128200531
train_iter_loss: 0.14440207183361053
train_iter_loss: 0.15155677497386932
train_iter_loss: 0.17502258718013763
train_iter_loss: 0.1607719212770462
train_iter_loss: 0.23764842748641968
train_iter_loss: 0.16108480095863342
train_iter_loss: 0.14293228089809418
train_iter_loss: 0.196632519364357
train_iter_loss: 0.14849579334259033
train_iter_loss: 0.1293574720621109
train_iter_loss: 0.16748644411563873
train_iter_loss: 0.18260903656482697
train_iter_loss: 0.07532013952732086
train_iter_loss: 0.12503311038017273
train_iter_loss: 0.10889483988285065
train_iter_loss: 0.22045527398586273
train_iter_loss: 0.13130336999893188
train_iter_loss: 0.07741744816303253
train_iter_loss: 0.19859762489795685
train_iter_loss: 0.1734289675951004
train loss :0.1568
---------------------
Validation seg loss: 0.21448694508663327 at epoch 595
epoch =    596/  1000, exp = train
train_iter_loss: 0.0399167537689209
train_iter_loss: 0.10133250802755356
train_iter_loss: 0.17058663070201874
train_iter_loss: 0.24303002655506134
train_iter_loss: 0.10341334342956543
train_iter_loss: 0.17813833057880402
train_iter_loss: 0.11134860664606094
train_iter_loss: 0.10232412815093994
train_iter_loss: 0.05395200848579407
train_iter_loss: 0.15524113178253174
train_iter_loss: 0.25244420766830444
train_iter_loss: 0.18818558752536774
train_iter_loss: 0.1605903059244156
train_iter_loss: 0.13738323748111725
train_iter_loss: 0.10129734128713608
train_iter_loss: 0.15642321109771729
train_iter_loss: 0.07921804487705231
train_iter_loss: 0.2287256270647049
train_iter_loss: 0.14533178508281708
train_iter_loss: 0.17234192788600922
train_iter_loss: 0.05461561679840088
train_iter_loss: 0.16953837871551514
train_iter_loss: 0.077040895819664
train_iter_loss: 0.1475202888250351
train_iter_loss: 0.09886889159679413
train_iter_loss: 0.17906896770000458
train_iter_loss: 0.28021615743637085
train_iter_loss: 0.13734176754951477
train_iter_loss: 0.19157147407531738
train_iter_loss: 0.0478610061109066
train_iter_loss: 0.19045129418373108
train_iter_loss: 0.1164582148194313
train_iter_loss: 0.024653945118188858
train_iter_loss: 0.16623806953430176
train_iter_loss: 0.21118144690990448
train_iter_loss: 0.11755331605672836
train_iter_loss: 0.08264818042516708
train_iter_loss: 0.12078212946653366
train_iter_loss: 0.17930130660533905
train_iter_loss: 0.1763925850391388
train_iter_loss: 0.1880691945552826
train_iter_loss: 0.24247987568378448
train_iter_loss: 0.1413465440273285
train_iter_loss: 0.12708182632923126
train_iter_loss: 0.1661517471075058
train_iter_loss: 0.07185281068086624
train_iter_loss: 0.17775604128837585
train_iter_loss: 0.09891930967569351
train_iter_loss: 0.09246865659952164
train_iter_loss: 0.2050323635339737
train_iter_loss: 0.17384083569049835
train_iter_loss: 0.082944355905056
train_iter_loss: 0.2141900360584259
train_iter_loss: 0.18433819711208344
train_iter_loss: 0.172117680311203
train_iter_loss: 0.08997088670730591
train_iter_loss: 0.1160358339548111
train_iter_loss: 0.19715803861618042
train_iter_loss: 0.18278591334819794
train_iter_loss: 0.21349063515663147
train_iter_loss: 0.28353333473205566
train_iter_loss: 0.0945652425289154
train_iter_loss: 0.13991884887218475
train_iter_loss: 0.1842988282442093
train_iter_loss: 0.10938487201929092
train_iter_loss: 0.13837017118930817
train_iter_loss: 0.2028101235628128
train_iter_loss: 0.3018164038658142
train_iter_loss: 0.1215367540717125
train_iter_loss: 0.16930243372917175
train_iter_loss: 0.20374557375907898
train_iter_loss: 0.18083707988262177
train_iter_loss: 0.3271988332271576
train_iter_loss: 0.142909973859787
train_iter_loss: 0.19108061492443085
train_iter_loss: 0.2837707996368408
train_iter_loss: 0.2048214077949524
train_iter_loss: 0.19533182680606842
train_iter_loss: 0.09211675822734833
train_iter_loss: 0.20216765999794006
train_iter_loss: 0.18176336586475372
train_iter_loss: 0.15648142993450165
train_iter_loss: 0.16322578489780426
train_iter_loss: 0.194121852517128
train_iter_loss: 0.23036304116249084
train_iter_loss: 0.09396570175886154
train_iter_loss: 0.17587848007678986
train_iter_loss: 0.14400433003902435
train_iter_loss: 0.16537967324256897
train_iter_loss: 0.2696854770183563
train_iter_loss: 0.21886499226093292
train_iter_loss: 0.11582975834608078
train_iter_loss: 0.22763973474502563
train_iter_loss: 0.12097946554422379
train_iter_loss: 0.1906971037387848
train_iter_loss: 0.14751310646533966
train_iter_loss: 0.26208600401878357
train_iter_loss: 0.223918154835701
train_iter_loss: 0.1513456255197525
train_iter_loss: 0.15182000398635864
train loss :0.1630
---------------------
Validation seg loss: 0.21343198242017403 at epoch 596
epoch =    597/  1000, exp = train
train_iter_loss: 0.14142104983329773
train_iter_loss: 0.10624188184738159
train_iter_loss: 0.1338948905467987
train_iter_loss: 0.17975643277168274
train_iter_loss: 0.14107973873615265
train_iter_loss: 0.22015692293643951
train_iter_loss: 0.09066688269376755
train_iter_loss: 0.09392432868480682
train_iter_loss: 0.08160930126905441
train_iter_loss: 0.17139266431331635
train_iter_loss: 0.04662619158625603
train_iter_loss: 0.14480742812156677
train_iter_loss: 0.1709931492805481
train_iter_loss: 0.19134506583213806
train_iter_loss: 0.20392899215221405
train_iter_loss: 0.1720387041568756
train_iter_loss: 0.07027510553598404
train_iter_loss: 0.19623708724975586
train_iter_loss: 0.25107765197753906
train_iter_loss: 0.14142724871635437
train_iter_loss: 0.13969609141349792
train_iter_loss: 0.12902586162090302
train_iter_loss: 0.13125386834144592
train_iter_loss: 0.08043539524078369
train_iter_loss: 0.129534050822258
train_iter_loss: 0.1326957643032074
train_iter_loss: 0.0885859951376915
train_iter_loss: 0.16189002990722656
train_iter_loss: 0.2259615957736969
train_iter_loss: 0.153364360332489
train_iter_loss: 0.12133960425853729
train_iter_loss: 0.13504086434841156
train_iter_loss: 0.04826093092560768
train_iter_loss: 0.35041725635528564
train_iter_loss: 0.08532661199569702
train_iter_loss: 0.1082005575299263
train_iter_loss: 0.058145634829998016
train_iter_loss: 0.15453672409057617
train_iter_loss: 0.2362607717514038
train_iter_loss: 0.38530415296554565
train_iter_loss: 0.19944477081298828
train_iter_loss: 0.15985417366027832
train_iter_loss: 0.08532435446977615
train_iter_loss: 0.1370602399110794
train_iter_loss: 0.2597370147705078
train_iter_loss: 0.26462581753730774
train_iter_loss: 0.09109640121459961
train_iter_loss: 0.1659461110830307
train_iter_loss: 0.18231062591075897
train_iter_loss: 0.14269661903381348
train_iter_loss: 0.17388689517974854
train_iter_loss: 0.14012417197227478
train_iter_loss: 0.12313932925462723
train_iter_loss: 0.12348505109548569
train_iter_loss: 0.21231423318386078
train_iter_loss: 0.12843966484069824
train_iter_loss: 0.14951515197753906
train_iter_loss: 0.11828107386827469
train_iter_loss: 0.23584820330142975
train_iter_loss: 0.15650685131549835
train_iter_loss: 0.14718808233737946
train_iter_loss: 0.27417418360710144
train_iter_loss: 0.13421514630317688
train_iter_loss: 0.28708016872406006
train_iter_loss: 0.11694569140672684
train_iter_loss: 0.13788318634033203
train_iter_loss: 0.26135650277137756
train_iter_loss: 0.11466145515441895
train_iter_loss: 0.2925744950771332
train_iter_loss: 0.15080688893795013
train_iter_loss: 0.09950926899909973
train_iter_loss: 0.13313055038452148
train_iter_loss: 0.16855226457118988
train_iter_loss: 0.0907936543226242
train_iter_loss: 0.20000533759593964
train_iter_loss: 0.04733194783329964
train_iter_loss: 0.18023008108139038
train_iter_loss: 0.13795727491378784
train_iter_loss: 0.082866370677948
train_iter_loss: 0.09559018909931183
train_iter_loss: 0.18310727179050446
train_iter_loss: 0.13228395581245422
train_iter_loss: 0.15822184085845947
train_iter_loss: 0.24391643702983856
train_iter_loss: 0.21824006736278534
train_iter_loss: 0.3640340566635132
train_iter_loss: 0.11163488030433655
train_iter_loss: 0.13303688168525696
train_iter_loss: 0.14138787984848022
train_iter_loss: 0.14886923134326935
train_iter_loss: 0.18070143461227417
train_iter_loss: 0.08333095163106918
train_iter_loss: 0.2777329087257385
train_iter_loss: 0.20085206627845764
train_iter_loss: 0.11404584348201752
train_iter_loss: 0.2575494945049286
train_iter_loss: 0.18010956048965454
train_iter_loss: 0.09237460792064667
train_iter_loss: 0.12881769239902496
train_iter_loss: 0.1822659820318222
train loss :0.1610
---------------------
Validation seg loss: 0.21655149321194808 at epoch 597
epoch =    598/  1000, exp = train
train_iter_loss: 0.16092562675476074
train_iter_loss: 0.16068483889102936
train_iter_loss: 0.19242337346076965
train_iter_loss: 0.07520822435617447
train_iter_loss: 0.495113730430603
train_iter_loss: 0.17609155178070068
train_iter_loss: 0.22909830510616302
train_iter_loss: 0.13267628848552704
train_iter_loss: 0.16582445800304413
train_iter_loss: 0.02004014141857624
train_iter_loss: 0.18276074528694153
train_iter_loss: 0.11351670324802399
train_iter_loss: 0.09940595924854279
train_iter_loss: 0.14560720324516296
train_iter_loss: 0.18269634246826172
train_iter_loss: 0.21318571269512177
train_iter_loss: 0.15058600902557373
train_iter_loss: 0.23944240808486938
train_iter_loss: 0.12391991913318634
train_iter_loss: 0.13571743667125702
train_iter_loss: 0.03774132952094078
train_iter_loss: 0.07095016539096832
train_iter_loss: 0.10850322991609573
train_iter_loss: 0.07998310029506683
train_iter_loss: 0.12555640935897827
train_iter_loss: 0.15243998169898987
train_iter_loss: 0.13155344128608704
train_iter_loss: 0.18285799026489258
train_iter_loss: 0.28096580505371094
train_iter_loss: 0.19081531465053558
train_iter_loss: 0.15704332292079926
train_iter_loss: 0.18549183011054993
train_iter_loss: 0.2364162802696228
train_iter_loss: 0.03159099444746971
train_iter_loss: 0.17073480784893036
train_iter_loss: 0.12989315390586853
train_iter_loss: 0.14697463810443878
train_iter_loss: 0.21139977872371674
train_iter_loss: 0.2701997458934784
train_iter_loss: 0.1586296707391739
train_iter_loss: 0.13243688642978668
train_iter_loss: 0.1455860733985901
train_iter_loss: 0.23980654776096344
train_iter_loss: 0.13051554560661316
train_iter_loss: 0.14285363256931305
train_iter_loss: 0.1527968943119049
train_iter_loss: 0.23729287087917328
train_iter_loss: 0.14829899370670319
train_iter_loss: 0.26796314120292664
train_iter_loss: 0.11365964263677597
train_iter_loss: 0.24441704154014587
train_iter_loss: 0.20946401357650757
train_iter_loss: 0.10419381409883499
train_iter_loss: 0.1272438019514084
train_iter_loss: 0.4164206385612488
train_iter_loss: 0.14395809173583984
train_iter_loss: 0.14851121604442596
train_iter_loss: 0.16335347294807434
train_iter_loss: 0.1018415242433548
train_iter_loss: 0.06249048560857773
train_iter_loss: 0.1966646909713745
train_iter_loss: 0.21751081943511963
train_iter_loss: 0.1761232614517212
train_iter_loss: 0.17841286957263947
train_iter_loss: 0.2600964307785034
train_iter_loss: 0.16178879141807556
train_iter_loss: 0.15922145545482635
train_iter_loss: 0.1082264706492424
train_iter_loss: 0.20242424309253693
train_iter_loss: 0.07623819261789322
train_iter_loss: 0.13844674825668335
train_iter_loss: 0.08681556582450867
train_iter_loss: 0.13680920004844666
train_iter_loss: 0.1788695752620697
train_iter_loss: 0.09200213104486465
train_iter_loss: 0.11488392949104309
train_iter_loss: 0.19492386281490326
train_iter_loss: 0.18470154702663422
train_iter_loss: 0.17766588926315308
train_iter_loss: 0.05068277567625046
train_iter_loss: 0.15790759027004242
train_iter_loss: 0.08087773621082306
train_iter_loss: 0.1144985482096672
train_iter_loss: 0.16969113051891327
train_iter_loss: 0.3181406259536743
train_iter_loss: 0.1641615331172943
train_iter_loss: 0.11481772363185883
train_iter_loss: 0.19911570847034454
train_iter_loss: 0.31167545914649963
train_iter_loss: 0.27116701006889343
train_iter_loss: 0.32124385237693787
train_iter_loss: 0.08749454468488693
train_iter_loss: 0.12696866691112518
train_iter_loss: 0.24243049323558807
train_iter_loss: 0.08624046295881271
train_iter_loss: 0.18523180484771729
train_iter_loss: 0.1301777958869934
train_iter_loss: 0.13748201727867126
train_iter_loss: 0.21163152158260345
train_iter_loss: 0.17480911314487457
train loss :0.1675
---------------------
Validation seg loss: 0.2169842194449508 at epoch 598
epoch =    599/  1000, exp = train
train_iter_loss: 0.13565033674240112
train_iter_loss: 0.12917585670948029
train_iter_loss: 0.06852581351995468
train_iter_loss: 0.10864370316267014
train_iter_loss: 0.26284879446029663
train_iter_loss: 0.2639937102794647
train_iter_loss: 0.10888782888650894
train_iter_loss: 0.16064710915088654
train_iter_loss: 0.15151028335094452
train_iter_loss: 0.0994248017668724
train_iter_loss: 0.104267917573452
train_iter_loss: 0.13752375543117523
train_iter_loss: 0.12486445158720016
train_iter_loss: 0.22811923921108246
train_iter_loss: 0.07911889255046844
train_iter_loss: 0.17279285192489624
train_iter_loss: 0.33016911149024963
train_iter_loss: 0.08878380060195923
train_iter_loss: 0.19312675297260284
train_iter_loss: 0.11878961324691772
train_iter_loss: 0.09210697561502457
train_iter_loss: 0.164889395236969
train_iter_loss: 0.09087721258401871
train_iter_loss: 0.1185113936662674
train_iter_loss: 0.12733349204063416
train_iter_loss: 0.26784881949424744
train_iter_loss: 0.1283862441778183
train_iter_loss: 0.08764199167490005
train_iter_loss: 0.13691574335098267
train_iter_loss: 0.12523317337036133
train_iter_loss: 0.06105508282780647
train_iter_loss: 0.15657426416873932
train_iter_loss: 0.13704144954681396
train_iter_loss: 0.2457801103591919
train_iter_loss: 0.18866945803165436
train_iter_loss: 0.11957143247127533
train_iter_loss: 0.15460486710071564
train_iter_loss: 0.13488462567329407
train_iter_loss: 0.2049569934606552
train_iter_loss: 0.06024638190865517
train_iter_loss: 0.24919164180755615
train_iter_loss: 0.10794668644666672
train_iter_loss: 0.1740010678768158
train_iter_loss: 0.11498044431209564
train_iter_loss: 0.12302998453378677
train_iter_loss: 0.13778677582740784
train_iter_loss: 0.2836027145385742
train_iter_loss: 0.14720435440540314
train_iter_loss: 0.30744683742523193
train_iter_loss: 0.10456530004739761
train_iter_loss: 0.18321435153484344
train_iter_loss: 0.07580624520778656
train_iter_loss: 0.20470182597637177
train_iter_loss: 0.16937589645385742
train_iter_loss: 0.2062958925962448
train_iter_loss: 0.2183026820421219
train_iter_loss: 0.15650367736816406
train_iter_loss: 0.1359330415725708
train_iter_loss: 0.31257233023643494
train_iter_loss: 0.3273666799068451
train_iter_loss: 0.167082279920578
train_iter_loss: 0.3644760847091675
train_iter_loss: 0.20451422035694122
train_iter_loss: 0.1371190845966339
train_iter_loss: 0.07466467469930649
train_iter_loss: 0.06273077428340912
train_iter_loss: 0.1467985361814499
train_iter_loss: 0.15828773379325867
train_iter_loss: 0.1389499008655548
train_iter_loss: 0.12350648641586304
train_iter_loss: 0.29450663924217224
train_iter_loss: 0.11621809005737305
train_iter_loss: 0.2126743048429489
train_iter_loss: 0.277052640914917
train_iter_loss: 0.15113480389118195
train_iter_loss: 0.09695044904947281
train_iter_loss: 0.08036551624536514
train_iter_loss: 0.3294893205165863
train_iter_loss: 0.17123471200466156
train_iter_loss: 0.17658910155296326
train_iter_loss: 0.12898729741573334
train_iter_loss: 0.13968393206596375
train_iter_loss: 0.16593796014785767
train_iter_loss: 0.10828352719545364
train_iter_loss: 0.12975408136844635
train_iter_loss: 0.1199067085981369
train_iter_loss: 0.06577818840742111
train_iter_loss: 0.08868004381656647
train_iter_loss: 0.17753395438194275
train_iter_loss: 0.18752770125865936
train_iter_loss: 0.07884624600410461
train_iter_loss: 0.20706072449684143
train_iter_loss: 0.11457501351833344
train_iter_loss: 0.2207820862531662
train_iter_loss: 0.21009708940982819
train_iter_loss: 0.16700880229473114
train_iter_loss: 0.1682198941707611
train_iter_loss: 0.34142738580703735
train_iter_loss: 0.09714994579553604
train_iter_loss: 0.14977528154850006
train loss :0.1633
---------------------
Validation seg loss: 0.2179222991602179 at epoch 599
epoch =    600/  1000, exp = train
train_iter_loss: 0.07708221673965454
train_iter_loss: 0.3362698256969452
train_iter_loss: 0.10689526051282883
train_iter_loss: 0.07871292531490326
train_iter_loss: 0.16214606165885925
train_iter_loss: 0.28238698840141296
train_iter_loss: 0.09611305594444275
train_iter_loss: 0.09520703554153442
train_iter_loss: 0.11981559544801712
train_iter_loss: 0.15462712943553925
train_iter_loss: 0.18565218150615692
train_iter_loss: 0.08063209056854248
train_iter_loss: 0.18093807995319366
train_iter_loss: 0.28153204917907715
train_iter_loss: 0.047040704637765884
train_iter_loss: 0.2116485983133316
train_iter_loss: 0.1311621218919754
train_iter_loss: 0.09922823309898376
train_iter_loss: 0.09818831086158752
train_iter_loss: 0.08766121417284012
train_iter_loss: 0.08701319992542267
train_iter_loss: 0.051924966275691986
train_iter_loss: 0.11643270403146744
train_iter_loss: 0.15891972184181213
train_iter_loss: 0.08959749341011047
train_iter_loss: 0.12865959107875824
train_iter_loss: 0.24466845393180847
train_iter_loss: 0.12143775075674057
train_iter_loss: 0.13786514103412628
train_iter_loss: 0.047817982733249664
train_iter_loss: 0.16638416051864624
train_iter_loss: 0.1290464699268341
train_iter_loss: 0.14916731417179108
train_iter_loss: 0.23662400245666504
train_iter_loss: 0.20330142974853516
train_iter_loss: 0.17948055267333984
train_iter_loss: 0.11349210888147354
train_iter_loss: 0.2648622989654541
train_iter_loss: 0.2219819724559784
train_iter_loss: 0.19790427386760712
train_iter_loss: 0.29940032958984375
train_iter_loss: 0.11461463570594788
train_iter_loss: 0.09241336584091187
train_iter_loss: 0.13383327424526215
train_iter_loss: 0.30033549666404724
train_iter_loss: 0.12356775999069214
train_iter_loss: 0.26281216740608215
train_iter_loss: 0.12719708681106567
train_iter_loss: 0.1205054447054863
train_iter_loss: 0.18111951649188995
train_iter_loss: 0.13963140547275543
train_iter_loss: 0.09962836652994156
train_iter_loss: 0.19031411409378052
train_iter_loss: 0.1546381264925003
train_iter_loss: 0.17042583227157593
train_iter_loss: 0.1460256427526474
train_iter_loss: 0.19131530821323395
train_iter_loss: 0.11194215714931488
train_iter_loss: 0.1258106827735901
train_iter_loss: 0.151051327586174
train_iter_loss: 0.1794075071811676
train_iter_loss: 0.13203026354312897
train_iter_loss: 0.16219750046730042
train_iter_loss: 0.0365702286362648
train_iter_loss: 0.14094951748847961
train_iter_loss: 0.10781611502170563
train_iter_loss: 0.20109990239143372
train_iter_loss: 0.26558566093444824
train_iter_loss: 0.14773832261562347
train_iter_loss: 0.09812368452548981
train_iter_loss: 0.20782478153705597
train_iter_loss: 0.2080313265323639
train_iter_loss: 0.32226529717445374
train_iter_loss: 0.11995406448841095
train_iter_loss: 0.2574058473110199
train_iter_loss: 0.09779368340969086
train_iter_loss: 0.1023596003651619
train_iter_loss: 0.25748685002326965
train_iter_loss: 0.17671877145767212
train_iter_loss: 0.164862260222435
train_iter_loss: 0.15352463722229004
train_iter_loss: 0.1535577028989792
train_iter_loss: 0.23732729256153107
train_iter_loss: 0.1973617821931839
train_iter_loss: 0.15159767866134644
train_iter_loss: 0.228458970785141
train_iter_loss: 0.13295447826385498
train_iter_loss: 0.09750350564718246
train_iter_loss: 0.1484583616256714
train_iter_loss: 0.06332869082689285
train_iter_loss: 0.09294663369655609
train_iter_loss: 0.1429700404405594
train_iter_loss: 0.19407962262630463
train_iter_loss: 0.21534575521945953
train_iter_loss: 0.37751686573028564
train_iter_loss: 0.11685830354690552
train_iter_loss: 0.10341320931911469
train_iter_loss: 0.31344911456108093
train_iter_loss: 0.1823742538690567
train_iter_loss: 0.22313834726810455
train loss :0.1627
---------------------
Validation seg loss: 0.21799553994019077 at epoch 600
epoch =    601/  1000, exp = train
train_iter_loss: 0.2018071711063385
train_iter_loss: 0.19121398031711578
train_iter_loss: 0.14586544036865234
train_iter_loss: 0.23316949605941772
train_iter_loss: 0.24496608972549438
train_iter_loss: 0.1424054354429245
train_iter_loss: 0.1811290979385376
train_iter_loss: 0.22345125675201416
train_iter_loss: 0.09368178993463516
train_iter_loss: 0.1895729899406433
train_iter_loss: 0.16106665134429932
train_iter_loss: 0.21495942771434784
train_iter_loss: 0.2924329936504364
train_iter_loss: 0.15772804617881775
train_iter_loss: 0.057334788143634796
train_iter_loss: 0.16133248805999756
train_iter_loss: 0.4457111954689026
train_iter_loss: 0.06351130455732346
train_iter_loss: 0.07488014549016953
train_iter_loss: 0.10523601621389389
train_iter_loss: 0.18666057288646698
train_iter_loss: 0.19977307319641113
train_iter_loss: 0.14205293357372284
train_iter_loss: 0.16351185739040375
train_iter_loss: 0.0898316279053688
train_iter_loss: 0.099676214158535
train_iter_loss: 0.10443811118602753
train_iter_loss: 0.0737924873828888
train_iter_loss: 0.18247535824775696
train_iter_loss: 0.18608954548835754
train_iter_loss: 0.15239205956459045
train_iter_loss: 0.09867184609174728
train_iter_loss: 0.2420119047164917
train_iter_loss: 0.24340713024139404
train_iter_loss: 0.14517635107040405
train_iter_loss: 0.10201100260019302
train_iter_loss: 0.0994434803724289
train_iter_loss: 0.19130118191242218
train_iter_loss: 0.15980082750320435
train_iter_loss: 0.17839644849300385
train_iter_loss: 0.21576252579689026
train_iter_loss: 0.24218182265758514
train_iter_loss: 0.12108083069324493
train_iter_loss: 0.21212337911128998
train_iter_loss: 0.06977558135986328
train_iter_loss: 0.14116574823856354
train_iter_loss: 0.1260676234960556
train_iter_loss: 0.1316424459218979
train_iter_loss: 0.05855337157845497
train_iter_loss: 0.1583845168352127
train_iter_loss: 0.18067580461502075
train_iter_loss: 0.25756803154945374
train_iter_loss: 0.09340596199035645
train_iter_loss: 0.14130456745624542
train_iter_loss: 0.07318060845136642
train_iter_loss: 0.1880592405796051
train_iter_loss: 0.2466149926185608
train_iter_loss: 0.17488202452659607
train_iter_loss: 0.17348289489746094
train_iter_loss: 0.17787611484527588
train_iter_loss: 0.21669122576713562
train_iter_loss: 0.07383609563112259
train_iter_loss: 0.1517099142074585
train_iter_loss: 0.22816146910190582
train_iter_loss: 0.14654333889484406
train_iter_loss: 0.17642223834991455
train_iter_loss: 0.16854561865329742
train_iter_loss: 0.149800106883049
train_iter_loss: 0.0866004154086113
train_iter_loss: 0.21258343756198883
train_iter_loss: 0.1705167442560196
train_iter_loss: 0.1926366090774536
train_iter_loss: 0.0820755586028099
train_iter_loss: 0.20693828165531158
train_iter_loss: 0.1648995727300644
train_iter_loss: 0.18241718411445618
train_iter_loss: 0.18452146649360657
train_iter_loss: 0.14526183903217316
train_iter_loss: 0.2117944061756134
train_iter_loss: 0.09935048967599869
train_iter_loss: 0.09348652511835098
train_iter_loss: 0.12575426697731018
train_iter_loss: 0.09658321738243103
train_iter_loss: 0.12264600396156311
train_iter_loss: 0.19561460614204407
train_iter_loss: 0.12795166671276093
train_iter_loss: 0.1912887990474701
train_iter_loss: 0.11932341754436493
train_iter_loss: 0.29414546489715576
train_iter_loss: 0.2058069407939911
train_iter_loss: 0.042335450649261475
train_iter_loss: 0.19449278712272644
train_iter_loss: 0.11716712266206741
train_iter_loss: 0.21790654957294464
train_iter_loss: 0.10251765698194504
train_iter_loss: 0.211739182472229
train_iter_loss: 0.14510878920555115
train_iter_loss: 0.15493474900722504
train_iter_loss: 0.4794505536556244
train_iter_loss: 0.09370314329862595
train loss :0.1655
---------------------
Validation seg loss: 0.2144807783693497 at epoch 601
epoch =    602/  1000, exp = train
train_iter_loss: 0.2532879114151001
train_iter_loss: 0.35578224062919617
train_iter_loss: 0.11229189485311508
train_iter_loss: 0.2212163209915161
train_iter_loss: 0.2375684529542923
train_iter_loss: 0.10973045229911804
train_iter_loss: 0.10795732587575912
train_iter_loss: 0.1777547150850296
train_iter_loss: 0.12362755089998245
train_iter_loss: 0.15414445102214813
train_iter_loss: 0.10964401066303253
train_iter_loss: 0.17331019043922424
train_iter_loss: 0.14994971454143524
train_iter_loss: 0.19327829778194427
train_iter_loss: 0.09578987956047058
train_iter_loss: 0.21026009321212769
train_iter_loss: 0.1055799126625061
train_iter_loss: 0.14074446260929108
train_iter_loss: 0.15212322771549225
train_iter_loss: 0.12188349664211273
train_iter_loss: 0.11530367285013199
train_iter_loss: 0.2883845567703247
train_iter_loss: 0.20403854548931122
train_iter_loss: 0.2530032992362976
train_iter_loss: 0.19527505338191986
train_iter_loss: 0.16859093308448792
train_iter_loss: 0.15996892750263214
train_iter_loss: 0.3505478501319885
train_iter_loss: 0.13355903327465057
train_iter_loss: 0.17174047231674194
train_iter_loss: 0.13750450313091278
train_iter_loss: 0.1419743448495865
train_iter_loss: 0.19231165945529938
train_iter_loss: 0.16524873673915863
train_iter_loss: 0.1929847002029419
train_iter_loss: 0.129401296377182
train_iter_loss: 0.05339331924915314
train_iter_loss: 0.16874106228351593
train_iter_loss: 0.16516131162643433
train_iter_loss: 0.11586778610944748
train_iter_loss: 0.14555755257606506
train_iter_loss: 0.1085161417722702
train_iter_loss: 0.18077728152275085
train_iter_loss: 0.107247494161129
train_iter_loss: 0.13456760346889496
train_iter_loss: 0.1420632004737854
train_iter_loss: 0.2608823776245117
train_iter_loss: 0.14362944662570953
train_iter_loss: 0.2855707108974457
train_iter_loss: 0.10471386462450027
train_iter_loss: 0.18609841167926788
train_iter_loss: 0.11628793925046921
train_iter_loss: 0.11331155151128769
train_iter_loss: 0.22380200028419495
train_iter_loss: 0.1578851193189621
train_iter_loss: 0.26438039541244507
train_iter_loss: 0.14216703176498413
train_iter_loss: 0.32565048336982727
train_iter_loss: 0.1246851310133934
train_iter_loss: 0.1153227761387825
train_iter_loss: 0.21054092049598694
train_iter_loss: 0.10949784517288208
train_iter_loss: 0.18104852735996246
train_iter_loss: 0.12784568965435028
train_iter_loss: 0.16395795345306396
train_iter_loss: 0.05176596716046333
train_iter_loss: 0.2909194529056549
train_iter_loss: 0.2700100839138031
train_iter_loss: 0.2695981562137604
train_iter_loss: 0.1924954354763031
train_iter_loss: 0.034916337579488754
train_iter_loss: 0.11807277798652649
train_iter_loss: 0.14631637930870056
train_iter_loss: 0.0934029147028923
train_iter_loss: 0.15001003444194794
train_iter_loss: 0.09920961409807205
train_iter_loss: 0.08117954432964325
train_iter_loss: 0.11710165441036224
train_iter_loss: 0.1346949189901352
train_iter_loss: 0.23951666057109833
train_iter_loss: 0.05853483825922012
train_iter_loss: 0.18166294693946838
train_iter_loss: 0.20030577480793
train_iter_loss: 0.19554190337657928
train_iter_loss: 0.10932659357786179
train_iter_loss: 0.23661942780017853
train_iter_loss: 0.1521114856004715
train_iter_loss: 0.17540541291236877
train_iter_loss: 0.1271256059408188
train_iter_loss: 0.15462499856948853
train_iter_loss: 0.16186274588108063
train_iter_loss: 0.18352378904819489
train_iter_loss: 0.12977401912212372
train_iter_loss: 0.10970480740070343
train_iter_loss: 0.14705157279968262
train_iter_loss: 0.07923494279384613
train_iter_loss: 0.11893435567617416
train_iter_loss: 0.09409864246845245
train_iter_loss: 0.2056119740009308
train_iter_loss: 0.21248389780521393
train loss :0.1647
---------------------
Validation seg loss: 0.21452436941447403 at epoch 602
epoch =    603/  1000, exp = train
train_iter_loss: 0.10816334933042526
train_iter_loss: 0.13916520774364471
train_iter_loss: 0.2821282744407654
train_iter_loss: 0.16489504277706146
train_iter_loss: 0.19290396571159363
train_iter_loss: 0.18772077560424805
train_iter_loss: 0.09092037379741669
train_iter_loss: 0.16003404557704926
train_iter_loss: 0.06832306832075119
train_iter_loss: 0.10054215788841248
train_iter_loss: 0.2011665254831314
train_iter_loss: 0.18592609465122223
train_iter_loss: 0.13576260209083557
train_iter_loss: 0.1821349561214447
train_iter_loss: 0.11858988553285599
train_iter_loss: 0.20091016590595245
train_iter_loss: 0.21539092063903809
train_iter_loss: 0.16286225616931915
train_iter_loss: 0.09039874374866486
train_iter_loss: 0.2308322787284851
train_iter_loss: 0.20145702362060547
train_iter_loss: 0.2379366159439087
train_iter_loss: 0.1284385770559311
train_iter_loss: 0.28308525681495667
train_iter_loss: 0.187229186296463
train_iter_loss: 0.1510956585407257
train_iter_loss: 0.13637520372867584
train_iter_loss: 0.2967125475406647
train_iter_loss: 0.0750444233417511
train_iter_loss: 0.13490454852581024
train_iter_loss: 0.10871552675962448
train_iter_loss: 0.12255000323057175
train_iter_loss: 0.17866726219654083
train_iter_loss: 0.1667589247226715
train_iter_loss: 0.09684853255748749
train_iter_loss: 0.14133326709270477
train_iter_loss: 0.12701816856861115
train_iter_loss: 0.09864983707666397
train_iter_loss: 0.09903331845998764
train_iter_loss: 0.15060515701770782
train_iter_loss: 0.16424253582954407
train_iter_loss: 0.18682458996772766
train_iter_loss: 0.15924330055713654
train_iter_loss: 0.2252955138683319
train_iter_loss: 0.15557554364204407
train_iter_loss: 0.11147807538509369
train_iter_loss: 0.16105403006076813
train_iter_loss: 0.17023146152496338
train_iter_loss: 0.17808464169502258
train_iter_loss: 0.17663367092609406
train_iter_loss: 0.054799631237983704
train_iter_loss: 0.22810150682926178
train_iter_loss: 0.062045130878686905
train_iter_loss: 0.15444082021713257
train_iter_loss: 0.19842679798603058
train_iter_loss: 0.16722235083580017
train_iter_loss: 0.1423189342021942
train_iter_loss: 0.06779344379901886
train_iter_loss: 0.14813143014907837
train_iter_loss: 0.1600913256406784
train_iter_loss: 0.16622722148895264
train_iter_loss: 0.09308787435293198
train_iter_loss: 0.16330279409885406
train_iter_loss: 0.12874510884284973
train_iter_loss: 0.1595158725976944
train_iter_loss: 0.15857376158237457
train_iter_loss: 0.22546760737895966
train_iter_loss: 0.15988755226135254
train_iter_loss: 0.10720700025558472
train_iter_loss: 0.0676349475979805
train_iter_loss: 0.08305447548627853
train_iter_loss: 0.08600737154483795
train_iter_loss: 0.11244571208953857
train_iter_loss: 0.19260263442993164
train_iter_loss: 0.30351144075393677
train_iter_loss: 0.09571730345487595
train_iter_loss: 0.1781446784734726
train_iter_loss: 0.16881759464740753
train_iter_loss: 0.1156257912516594
train_iter_loss: 0.17390042543411255
train_iter_loss: 0.19467619061470032
train_iter_loss: 0.13643577694892883
train_iter_loss: 0.1551571637392044
train_iter_loss: 0.1884395033121109
train_iter_loss: 0.14277403056621552
train_iter_loss: 0.18252119421958923
train_iter_loss: 0.14788714051246643
train_iter_loss: 0.14339594542980194
train_iter_loss: 0.11544495075941086
train_iter_loss: 0.3920929431915283
train_iter_loss: 0.1916055828332901
train_iter_loss: 0.25536012649536133
train_iter_loss: 0.043656062334775925
train_iter_loss: 0.07495374232530594
train_iter_loss: 0.11519056558609009
train_iter_loss: 0.0693342462182045
train_iter_loss: 0.10931825637817383
train_iter_loss: 0.11411581933498383
train_iter_loss: 0.3343586027622223
train_iter_loss: 0.1130216121673584
train loss :0.1574
---------------------
Validation seg loss: 0.2135825956872893 at epoch 603
epoch =    604/  1000, exp = train
train_iter_loss: 0.30711957812309265
train_iter_loss: 0.15520009398460388
train_iter_loss: 0.10356882959604263
train_iter_loss: 0.21031717956066132
train_iter_loss: 0.1427764892578125
train_iter_loss: 0.2097162902355194
train_iter_loss: 0.14127948880195618
train_iter_loss: 0.09635873883962631
train_iter_loss: 0.5605210661888123
train_iter_loss: 0.14569878578186035
train_iter_loss: 0.1776367723941803
train_iter_loss: 0.1311119794845581
train_iter_loss: 0.16743646562099457
train_iter_loss: 0.23293396830558777
train_iter_loss: 0.23362818360328674
train_iter_loss: 0.2013627588748932
train_iter_loss: 0.11661098897457123
train_iter_loss: 0.13906791806221008
train_iter_loss: 0.17685003578662872
train_iter_loss: 0.21754860877990723
train_iter_loss: 0.19359031319618225
train_iter_loss: 0.10326997935771942
train_iter_loss: 0.19276827573776245
train_iter_loss: 0.1914101243019104
train_iter_loss: 0.14742672443389893
train_iter_loss: 0.14288941025733948
train_iter_loss: 0.09447934478521347
train_iter_loss: 0.18600693345069885
train_iter_loss: 0.13765399158000946
train_iter_loss: 0.3113784193992615
train_iter_loss: 0.16379345953464508
train_iter_loss: 0.23549160361289978
train_iter_loss: 0.16189943253993988
train_iter_loss: 0.04234042018651962
train_iter_loss: 0.1735265851020813
train_iter_loss: 0.05326138436794281
train_iter_loss: 0.11712165176868439
train_iter_loss: 0.13102740049362183
train_iter_loss: 0.11133167892694473
train_iter_loss: 0.12327948212623596
train_iter_loss: 0.04758237674832344
train_iter_loss: 0.19970977306365967
train_iter_loss: 0.18069659173488617
train_iter_loss: 0.20131126046180725
train_iter_loss: 0.15242110192775726
train_iter_loss: 0.12634214758872986
train_iter_loss: 0.12488255649805069
train_iter_loss: 0.17558890581130981
train_iter_loss: 0.3501158356666565
train_iter_loss: 0.15628518164157867
train_iter_loss: 0.09348540008068085
train_iter_loss: 0.11364199966192245
train_iter_loss: 0.06037367507815361
train_iter_loss: 0.14213278889656067
train_iter_loss: 0.1819135993719101
train_iter_loss: 0.1255037486553192
train_iter_loss: 0.11473675072193146
train_iter_loss: 0.12099114805459976
train_iter_loss: 0.17944960296154022
train_iter_loss: 0.12680082023143768
train_iter_loss: 0.04072929546236992
train_iter_loss: 0.11042626202106476
train_iter_loss: 0.0904630795121193
train_iter_loss: 0.12064489722251892
train_iter_loss: 0.14299026131629944
train_iter_loss: 0.16896285116672516
train_iter_loss: 0.09847540408372879
train_iter_loss: 0.06015770882368088
train_iter_loss: 0.11369296163320541
train_iter_loss: 0.09603226184844971
train_iter_loss: 0.1503097265958786
train_iter_loss: 0.3485482633113861
train_iter_loss: 0.15630598366260529
train_iter_loss: 0.13107076287269592
train_iter_loss: 0.13217045366764069
train_iter_loss: 0.2865883409976959
train_iter_loss: 0.10928529500961304
train_iter_loss: 0.17901316285133362
train_iter_loss: 0.08678503334522247
train_iter_loss: 0.10908588021993637
train_iter_loss: 0.45353591442108154
train_iter_loss: 0.1822875589132309
train_iter_loss: 0.1922028362751007
train_iter_loss: 0.17011946439743042
train_iter_loss: 0.1133347898721695
train_iter_loss: 0.1254691183567047
train_iter_loss: 0.22412846982479095
train_iter_loss: 0.11014537513256073
train_iter_loss: 0.3297065496444702
train_iter_loss: 0.1939113885164261
train_iter_loss: 0.22653934359550476
train_iter_loss: 0.3377307951450348
train_iter_loss: 0.27109086513519287
train_iter_loss: 0.13531655073165894
train_iter_loss: 0.13654394447803497
train_iter_loss: 0.15696664154529572
train_iter_loss: 0.11889121681451797
train_iter_loss: 0.12691302597522736
train_iter_loss: 0.1620076596736908
train_iter_loss: 0.17662324011325836
train loss :0.1669
---------------------
Validation seg loss: 0.21458212786160832 at epoch 604
epoch =    605/  1000, exp = train
train_iter_loss: 0.12165156751871109
train_iter_loss: 0.11686225235462189
train_iter_loss: 0.06063631922006607
train_iter_loss: 0.07250101864337921
train_iter_loss: 0.23545436561107635
train_iter_loss: 0.1907782405614853
train_iter_loss: 0.20205160975456238
train_iter_loss: 0.17358560860157013
train_iter_loss: 0.2099149078130722
train_iter_loss: 0.19407106935977936
train_iter_loss: 0.11004488915205002
train_iter_loss: 0.1046222671866417
train_iter_loss: 0.1387416273355484
train_iter_loss: 0.14384125173091888
train_iter_loss: 0.12169419974088669
train_iter_loss: 0.30478551983833313
train_iter_loss: 0.14803916215896606
train_iter_loss: 0.23678697645664215
train_iter_loss: 0.06056937947869301
train_iter_loss: 0.09568054229021072
train_iter_loss: 0.19905215501785278
train_iter_loss: 0.1681244671344757
train_iter_loss: 0.1993504911661148
train_iter_loss: 0.12995532155036926
train_iter_loss: 0.18756066262722015
train_iter_loss: 0.08117279410362244
train_iter_loss: 0.1051664799451828
train_iter_loss: 0.14986582100391388
train_iter_loss: 0.21109099686145782
train_iter_loss: 0.18205443024635315
train_iter_loss: 0.07261095941066742
train_iter_loss: 0.23462854325771332
train_iter_loss: 0.12955884635448456
train_iter_loss: 0.09831418097019196
train_iter_loss: 0.12824048101902008
train_iter_loss: 0.18958693742752075
train_iter_loss: 0.1851506233215332
train_iter_loss: 0.2696693539619446
train_iter_loss: 0.12181232869625092
train_iter_loss: 0.3208761215209961
train_iter_loss: 0.15303710103034973
train_iter_loss: 0.2584717869758606
train_iter_loss: 0.14504340291023254
train_iter_loss: 0.04473667964339256
train_iter_loss: 0.1623830646276474
train_iter_loss: 0.1319253295660019
train_iter_loss: 0.1484033167362213
train_iter_loss: 0.1265621781349182
train_iter_loss: 0.20411592721939087
train_iter_loss: 0.25553515553474426
train_iter_loss: 0.1483267843723297
train_iter_loss: 0.2348938137292862
train_iter_loss: 0.26472070813179016
train_iter_loss: 0.1371026635169983
train_iter_loss: 0.09004446119070053
train_iter_loss: 0.31950104236602783
train_iter_loss: 0.11848978698253632
train_iter_loss: 0.05902613699436188
train_iter_loss: 0.27704745531082153
train_iter_loss: 0.14563868939876556
train_iter_loss: 0.19670763611793518
train_iter_loss: 0.04868824779987335
train_iter_loss: 0.09472285211086273
train_iter_loss: 0.09363092482089996
train_iter_loss: 0.101957768201828
train_iter_loss: 0.1233772337436676
train_iter_loss: 0.20357345044612885
train_iter_loss: 0.1145833283662796
train_iter_loss: 0.32626259326934814
train_iter_loss: 0.1708420068025589
train_iter_loss: 0.13895469903945923
train_iter_loss: 0.09790758788585663
train_iter_loss: 0.1699678748846054
train_iter_loss: 0.27682939171791077
train_iter_loss: 0.08288242667913437
train_iter_loss: 0.1949150711297989
train_iter_loss: 0.3613927364349365
train_iter_loss: 0.23288798332214355
train_iter_loss: 0.13304634392261505
train_iter_loss: 0.18459969758987427
train_iter_loss: 0.07288680225610733
train_iter_loss: 0.10466137528419495
train_iter_loss: 0.14957277476787567
train_iter_loss: 0.3462165296077728
train_iter_loss: 0.15428569912910461
train_iter_loss: 0.0488554909825325
train_iter_loss: 0.1213940680027008
train_iter_loss: 0.2563985288143158
train_iter_loss: 0.0788850486278534
train_iter_loss: 0.09468639642000198
train_iter_loss: 0.30098584294319153
train_iter_loss: 0.21081948280334473
train_iter_loss: 0.16410879790782928
train_iter_loss: 0.24536174535751343
train_iter_loss: 0.18355780839920044
train_iter_loss: 0.0958782285451889
train_iter_loss: 0.23827967047691345
train_iter_loss: 0.13238495588302612
train_iter_loss: 0.11292023956775665
train_iter_loss: 0.2680922746658325
train loss :0.1673
---------------------
Validation seg loss: 0.21722283505149326 at epoch 605
epoch =    606/  1000, exp = train
train_iter_loss: 0.28144553303718567
train_iter_loss: 0.13907937705516815
train_iter_loss: 0.11033151298761368
train_iter_loss: 0.17675639688968658
train_iter_loss: 0.06312662363052368
train_iter_loss: 0.08141765743494034
train_iter_loss: 0.16870883107185364
train_iter_loss: 0.14752116799354553
train_iter_loss: 0.40463680028915405
train_iter_loss: 0.11053134500980377
train_iter_loss: 0.17775927484035492
train_iter_loss: 0.14794163405895233
train_iter_loss: 0.2071649134159088
train_iter_loss: 0.19724684953689575
train_iter_loss: 0.10610651969909668
train_iter_loss: 0.1843612641096115
train_iter_loss: 0.11015946418046951
train_iter_loss: 0.13475990295410156
train_iter_loss: 0.10319298505783081
train_iter_loss: 0.1748717576265335
train_iter_loss: 0.2813517451286316
train_iter_loss: 0.19384193420410156
train_iter_loss: 0.10661806911230087
train_iter_loss: 0.10193994641304016
train_iter_loss: 0.0777926966547966
train_iter_loss: 0.11584705859422684
train_iter_loss: 0.18785741925239563
train_iter_loss: 0.14892935752868652
train_iter_loss: 0.326124370098114
train_iter_loss: 0.1090765967965126
train_iter_loss: 0.21555271744728088
train_iter_loss: 0.09504260867834091
train_iter_loss: 0.20502682030200958
train_iter_loss: 0.10051631182432175
train_iter_loss: 0.06157201528549194
train_iter_loss: 0.1795509159564972
train_iter_loss: 0.3964320123195648
train_iter_loss: 0.061924900859594345
train_iter_loss: 0.1581512838602066
train_iter_loss: 0.28016865253448486
train_iter_loss: 0.2069728672504425
train_iter_loss: 0.16982349753379822
train_iter_loss: 0.11283394694328308
train_iter_loss: 0.10610979050397873
train_iter_loss: 0.12872737646102905
train_iter_loss: 0.08198023587465286
train_iter_loss: 0.05922139063477516
train_iter_loss: 0.20207969844341278
train_iter_loss: 0.12511949241161346
train_iter_loss: 0.30017995834350586
train_iter_loss: 0.17151430249214172
train_iter_loss: 0.1886366903781891
train_iter_loss: 0.1527712047100067
train_iter_loss: 0.39285701513290405
train_iter_loss: 0.21347446739673615
train_iter_loss: 0.16475526988506317
train_iter_loss: 0.10114077478647232
train_iter_loss: 0.07838552445173264
train_iter_loss: 0.17107395827770233
train_iter_loss: 0.15431015193462372
train_iter_loss: 0.17229612171649933
train_iter_loss: 0.11015371978282928
train_iter_loss: 0.09046566486358643
train_iter_loss: 0.18863323330879211
train_iter_loss: 0.17591984570026398
train_iter_loss: 0.22917869687080383
train_iter_loss: 0.1407018005847931
train_iter_loss: 0.14960969984531403
train_iter_loss: 0.13756415247917175
train_iter_loss: 0.1865137368440628
train_iter_loss: 0.0832616314291954
train_iter_loss: 0.18742720782756805
train_iter_loss: 0.21362583339214325
train_iter_loss: 0.15649129450321198
train_iter_loss: 0.14953318238258362
train_iter_loss: 0.2401057928800583
train_iter_loss: 0.22705824673175812
train_iter_loss: 0.14311856031417847
train_iter_loss: 0.2147202491760254
train_iter_loss: 0.13460776209831238
train_iter_loss: 0.018047772347927094
train_iter_loss: 0.17402586340904236
train_iter_loss: 0.1185840293765068
train_iter_loss: 0.1573232263326645
train_iter_loss: 0.23596975207328796
train_iter_loss: 0.1986771523952484
train_iter_loss: 0.25353172421455383
train_iter_loss: 0.25422656536102295
train_iter_loss: 0.10138466954231262
train_iter_loss: 0.22153721749782562
train_iter_loss: 0.08576436340808868
train_iter_loss: 0.24439875781536102
train_iter_loss: 0.1810760498046875
train_iter_loss: 0.14557786285877228
train_iter_loss: 0.11602038145065308
train_iter_loss: 0.18902921676635742
train_iter_loss: 0.06309385597705841
train_iter_loss: 0.1807807981967926
train_iter_loss: 0.18965144455432892
train_iter_loss: 0.23863255977630615
train loss :0.1678
---------------------
Validation seg loss: 0.21402695753945494 at epoch 606
epoch =    607/  1000, exp = train
train_iter_loss: 0.16820372641086578
train_iter_loss: 0.17593176662921906
train_iter_loss: 0.1000954806804657
train_iter_loss: 0.1778860092163086
train_iter_loss: 0.24870868027210236
train_iter_loss: 0.20896555483341217
train_iter_loss: 0.09110966324806213
train_iter_loss: 0.08064009249210358
train_iter_loss: 0.15868695080280304
train_iter_loss: 0.29359862208366394
train_iter_loss: 0.06784434616565704
train_iter_loss: 0.19515417516231537
train_iter_loss: 0.19796203076839447
train_iter_loss: 0.2828996777534485
train_iter_loss: 0.22899889945983887
train_iter_loss: 0.343397319316864
train_iter_loss: 0.1947508305311203
train_iter_loss: 0.17118246853351593
train_iter_loss: 0.06980707496404648
train_iter_loss: 0.15334124863147736
train_iter_loss: 0.11698487401008606
train_iter_loss: 0.316617876291275
train_iter_loss: 0.13485905528068542
train_iter_loss: 0.11887956410646439
train_iter_loss: 0.16576412320137024
train_iter_loss: 0.1760397106409073
train_iter_loss: 0.17593331634998322
train_iter_loss: 0.2225494682788849
train_iter_loss: 0.10280431807041168
train_iter_loss: 0.10682836174964905
train_iter_loss: 0.11274007707834244
train_iter_loss: 0.13547229766845703
train_iter_loss: 0.18055863678455353
train_iter_loss: 0.13280487060546875
train_iter_loss: 0.1291864663362503
train_iter_loss: 0.147518128156662
train_iter_loss: 0.17912645637989044
train_iter_loss: 0.14946943521499634
train_iter_loss: 0.16682039201259613
train_iter_loss: 0.24231155216693878
train_iter_loss: 0.12690237164497375
train_iter_loss: 0.1502934694290161
train_iter_loss: 0.12956038117408752
train_iter_loss: 0.151248499751091
train_iter_loss: 0.1647997945547104
train_iter_loss: 0.09427393972873688
train_iter_loss: 0.17714038491249084
train_iter_loss: 0.0755881741642952
train_iter_loss: 0.06765337288379669
train_iter_loss: 0.0699019506573677
train_iter_loss: 0.17864708602428436
train_iter_loss: 0.1314927488565445
train_iter_loss: 0.18368171155452728
train_iter_loss: 0.14087164402008057
train_iter_loss: 0.16194722056388855
train_iter_loss: 0.13362739980220795
train_iter_loss: 0.1722533404827118
train_iter_loss: 0.16878560185432434
train_iter_loss: 0.08941752463579178
train_iter_loss: 0.18326057493686676
train_iter_loss: 0.1399279087781906
train_iter_loss: 0.13752484321594238
train_iter_loss: 0.2331579625606537
train_iter_loss: 0.17148379981517792
train_iter_loss: 0.17126113176345825
train_iter_loss: 0.13486546277999878
train_iter_loss: 0.0916203036904335
train_iter_loss: 0.13147667050361633
train_iter_loss: 0.1253902018070221
train_iter_loss: 0.12001549452543259
train_iter_loss: 0.10426193475723267
train_iter_loss: 0.1560892015695572
train_iter_loss: 0.1193704679608345
train_iter_loss: 0.17066125571727753
train_iter_loss: 0.13410022854804993
train_iter_loss: 0.11785748600959778
train_iter_loss: 0.13175541162490845
train_iter_loss: 0.1347704976797104
train_iter_loss: 0.20411424338817596
train_iter_loss: 0.1930866539478302
train_iter_loss: 0.29367855191230774
train_iter_loss: 0.3362746238708496
train_iter_loss: 0.2612144947052002
train_iter_loss: 0.14866232872009277
train_iter_loss: 0.10601378977298737
train_iter_loss: 0.08354649692773819
train_iter_loss: 0.1588454395532608
train_iter_loss: 0.1232452392578125
train_iter_loss: 0.15452951192855835
train_iter_loss: 0.10128884017467499
train_iter_loss: 0.09930442273616791
train_iter_loss: 0.13353019952774048
train_iter_loss: 0.10507451742887497
train_iter_loss: 0.1626909226179123
train_iter_loss: 0.12371452152729034
train_iter_loss: 0.10381364822387695
train_iter_loss: 0.09087371081113815
train_iter_loss: 0.12213309109210968
train_iter_loss: 0.16904421150684357
train_iter_loss: 0.3048715889453888
train loss :0.1585
---------------------
Validation seg loss: 0.21531407390984725 at epoch 607
epoch =    608/  1000, exp = train
train_iter_loss: 0.17140524089336395
train_iter_loss: 0.2020312398672104
train_iter_loss: 0.17823737859725952
train_iter_loss: 0.10591267049312592
train_iter_loss: 0.11067336797714233
train_iter_loss: 0.11629887670278549
train_iter_loss: 0.17944057285785675
train_iter_loss: 0.11660357564687729
train_iter_loss: 0.21506239473819733
train_iter_loss: 0.12532450258731842
train_iter_loss: 0.17739002406597137
train_iter_loss: 0.25555890798568726
train_iter_loss: 0.12568961083889008
train_iter_loss: 0.13625706732273102
train_iter_loss: 0.1692333072423935
train_iter_loss: 0.1397731602191925
train_iter_loss: 0.189495250582695
train_iter_loss: 0.12088751792907715
train_iter_loss: 0.1938694417476654
train_iter_loss: 0.2686763405799866
train_iter_loss: 0.1627567708492279
train_iter_loss: 0.14713728427886963
train_iter_loss: 0.19374309480190277
train_iter_loss: 0.1531132012605667
train_iter_loss: 0.1942649930715561
train_iter_loss: 0.07044535130262375
train_iter_loss: 0.14596277475357056
train_iter_loss: 0.13590027391910553
train_iter_loss: 0.1361827850341797
train_iter_loss: 0.11160489171743393
train_iter_loss: 0.12426301091909409
train_iter_loss: 0.19538834691047668
train_iter_loss: 0.08674623817205429
train_iter_loss: 0.14753051102161407
train_iter_loss: 0.14540454745292664
train_iter_loss: 0.12816183269023895
train_iter_loss: 0.1139095351099968
train_iter_loss: 0.13059702515602112
train_iter_loss: 0.2033216655254364
train_iter_loss: 0.23715248703956604
train_iter_loss: 0.12345016747713089
train_iter_loss: 0.0725589469075203
train_iter_loss: 0.11420956254005432
train_iter_loss: 0.13864940404891968
train_iter_loss: 0.13210037350654602
train_iter_loss: 0.18202003836631775
train_iter_loss: 0.13529521226882935
train_iter_loss: 0.17149469256401062
train_iter_loss: 0.09326585382223129
train_iter_loss: 0.09798604249954224
train_iter_loss: 0.12095954269170761
train_iter_loss: 0.09332069754600525
train_iter_loss: 0.05951365828514099
train_iter_loss: 0.12181251496076584
train_iter_loss: 0.14029091596603394
train_iter_loss: 0.1157253086566925
train_iter_loss: 0.1411461979150772
train_iter_loss: 0.14149799942970276
train_iter_loss: 0.1813162863254547
train_iter_loss: 0.06428354978561401
train_iter_loss: 0.1736707091331482
train_iter_loss: 0.06141425296664238
train_iter_loss: 0.16613788902759552
train_iter_loss: 0.1745568811893463
train_iter_loss: 0.0942656397819519
train_iter_loss: 0.1531129777431488
train_iter_loss: 0.09653676301240921
train_iter_loss: 0.23184552788734436
train_iter_loss: 0.20722214877605438
train_iter_loss: 0.07380005717277527
train_iter_loss: 0.20842739939689636
train_iter_loss: 0.2012442797422409
train_iter_loss: 0.15731315314769745
train_iter_loss: 0.15262286365032196
train_iter_loss: 0.1652759313583374
train_iter_loss: 0.12493810802698135
train_iter_loss: 0.11939065903425217
train_iter_loss: 0.1522505134344101
train_iter_loss: 0.1542302519083023
train_iter_loss: 0.14306297898292542
train_iter_loss: 0.19403906166553497
train_iter_loss: 0.17497123777866364
train_iter_loss: 0.17504039406776428
train_iter_loss: 0.10143615305423737
train_iter_loss: 0.15435758233070374
train_iter_loss: 0.2064821422100067
train_iter_loss: 0.10138050466775894
train_iter_loss: 0.07970287650823593
train_iter_loss: 0.13119730353355408
train_iter_loss: 0.2356763780117035
train_iter_loss: 0.18981094658374786
train_iter_loss: 0.20536887645721436
train_iter_loss: 0.14523014426231384
train_iter_loss: 0.26419004797935486
train_iter_loss: 0.0921679213643074
train_iter_loss: 0.19086851179599762
train_iter_loss: 0.18527665734291077
train_iter_loss: 0.10849650204181671
train_iter_loss: 0.14807219803333282
train_iter_loss: 0.21763168275356293
train loss :0.1517
---------------------
Validation seg loss: 0.21526865539017995 at epoch 608
epoch =    609/  1000, exp = train
train_iter_loss: 0.15876683592796326
train_iter_loss: 0.1934979110956192
train_iter_loss: 0.1878197342157364
train_iter_loss: 0.1278427541255951
train_iter_loss: 0.10283873975276947
train_iter_loss: 0.10688626766204834
train_iter_loss: 0.2143508791923523
train_iter_loss: 0.0847826600074768
train_iter_loss: 0.21263569593429565
train_iter_loss: 0.18169645965099335
train_iter_loss: 0.12076500803232193
train_iter_loss: 0.20327143371105194
train_iter_loss: 0.12845434248447418
train_iter_loss: 0.14582419395446777
train_iter_loss: 0.13437265157699585
train_iter_loss: 0.18248595297336578
train_iter_loss: 0.17698551714420319
train_iter_loss: 0.20279285311698914
train_iter_loss: 0.06558661162853241
train_iter_loss: 0.18702636659145355
train_iter_loss: 0.11364202201366425
train_iter_loss: 0.1791074126958847
train_iter_loss: 0.1001637727022171
train_iter_loss: 0.181173175573349
train_iter_loss: 0.14347757399082184
train_iter_loss: 0.19883188605308533
train_iter_loss: 0.0998014360666275
train_iter_loss: 0.19356116652488708
train_iter_loss: 0.19515050947666168
train_iter_loss: 0.23561504483222961
train_iter_loss: 0.11087889969348907
train_iter_loss: 0.177144393324852
train_iter_loss: 0.18655553460121155
train_iter_loss: 0.1173737496137619
train_iter_loss: 0.0653737485408783
train_iter_loss: 0.10130792111158371
train_iter_loss: 0.11115416884422302
train_iter_loss: 0.14959785342216492
train_iter_loss: 0.33570531010627747
train_iter_loss: 0.09009574353694916
train_iter_loss: 0.13599923253059387
train_iter_loss: 0.3658860921859741
train_iter_loss: 0.1195133626461029
train_iter_loss: 0.19819706678390503
train_iter_loss: 0.08983612805604935
train_iter_loss: 0.11784524470567703
train_iter_loss: 0.15280084311962128
train_iter_loss: 0.3218943774700165
train_iter_loss: 0.12244901061058044
train_iter_loss: 0.09310871362686157
train_iter_loss: 0.154992014169693
train_iter_loss: 0.23194102942943573
train_iter_loss: 0.1781432330608368
train_iter_loss: 0.2640382647514343
train_iter_loss: 0.15316779911518097
train_iter_loss: 0.0403389073908329
train_iter_loss: 0.10095582902431488
train_iter_loss: 0.10398154705762863
train_iter_loss: 0.11146867275238037
train_iter_loss: 0.20241385698318481
train_iter_loss: 0.19717134535312653
train_iter_loss: 0.1299697756767273
train_iter_loss: 0.14033442735671997
train_iter_loss: 0.17841556668281555
train_iter_loss: 0.16204267740249634
train_iter_loss: 0.19367411732673645
train_iter_loss: 0.07642555981874466
train_iter_loss: 0.17817789316177368
train_iter_loss: 0.24692703783512115
train_iter_loss: 0.21711711585521698
train_iter_loss: 0.13835522532463074
train_iter_loss: 0.1693304032087326
train_iter_loss: 0.11906109005212784
train_iter_loss: 0.13741184771060944
train_iter_loss: 0.17376728355884552
train_iter_loss: 0.13107535243034363
train_iter_loss: 0.18218286335468292
train_iter_loss: 0.12984995543956757
train_iter_loss: 0.10958705842494965
train_iter_loss: 0.1834731101989746
train_iter_loss: 0.19560426473617554
train_iter_loss: 0.16895170509815216
train_iter_loss: 0.22040829062461853
train_iter_loss: 0.17447783052921295
train_iter_loss: 0.1764019876718521
train_iter_loss: 0.20930315554141998
train_iter_loss: 0.11926945298910141
train_iter_loss: 0.20984260737895966
train_iter_loss: 0.15699300169944763
train_iter_loss: 0.1260974407196045
train_iter_loss: 0.13547900319099426
train_iter_loss: 0.0984129086136818
train_iter_loss: 0.1421268731355667
train_iter_loss: 0.13729774951934814
train_iter_loss: 0.08553027361631393
train_iter_loss: 0.11005159467458725
train_iter_loss: 0.2540431320667267
train_iter_loss: 0.12929396331310272
train_iter_loss: 0.1722216159105301
train_iter_loss: 0.1710047870874405
train loss :0.1600
---------------------
Validation seg loss: 0.21449419248954588 at epoch 609
epoch =    610/  1000, exp = train
train_iter_loss: 0.10880648344755173
train_iter_loss: 0.17562615871429443
train_iter_loss: 0.22333525121212006
train_iter_loss: 0.1931576430797577
train_iter_loss: 0.2967591881752014
train_iter_loss: 0.16200031340122223
train_iter_loss: 0.1275792121887207
train_iter_loss: 0.15091389417648315
train_iter_loss: 0.19373182952404022
train_iter_loss: 0.12260142713785172
train_iter_loss: 0.145600363612175
train_iter_loss: 0.15826232731342316
train_iter_loss: 0.08364327251911163
train_iter_loss: 0.255005806684494
train_iter_loss: 0.3427126407623291
train_iter_loss: 0.09429425001144409
train_iter_loss: 0.2895183861255646
train_iter_loss: 0.06967049092054367
train_iter_loss: 0.1755654215812683
train_iter_loss: 0.16478388011455536
train_iter_loss: 0.12906959652900696
train_iter_loss: 0.11143885552883148
train_iter_loss: 0.20446620881557465
train_iter_loss: 0.1660047322511673
train_iter_loss: 0.20630380511283875
train_iter_loss: 0.1538841426372528
train_iter_loss: 0.20153586566448212
train_iter_loss: 0.12387833744287491
train_iter_loss: 0.19679637253284454
train_iter_loss: 0.1212279349565506
train_iter_loss: 0.1347557157278061
train_iter_loss: 0.15254200994968414
train_iter_loss: 0.29210320115089417
train_iter_loss: 0.2262253761291504
train_iter_loss: 0.19217775762081146
train_iter_loss: 0.10479138046503067
train_iter_loss: 0.05081355571746826
train_iter_loss: 0.1124151423573494
train_iter_loss: 0.18102073669433594
train_iter_loss: 0.10272572934627533
train_iter_loss: 0.16258075833320618
train_iter_loss: 0.03249849006533623
train_iter_loss: 0.23748861253261566
train_iter_loss: 0.21779023110866547
train_iter_loss: 0.11897912621498108
train_iter_loss: 0.0900428518652916
train_iter_loss: 0.09247691929340363
train_iter_loss: 0.3262542188167572
train_iter_loss: 0.21554502844810486
train_iter_loss: 0.16391295194625854
train_iter_loss: 0.15192146599292755
train_iter_loss: 0.17689739167690277
train_iter_loss: 0.1875571608543396
train_iter_loss: 0.22387029230594635
train_iter_loss: 0.10365036129951477
train_iter_loss: 0.19640369713306427
train_iter_loss: 0.1885078251361847
train_iter_loss: 0.12284725904464722
train_iter_loss: 0.19881455600261688
train_iter_loss: 0.16295036673545837
train_iter_loss: 0.13465967774391174
train_iter_loss: 0.23441007733345032
train_iter_loss: 0.12099486589431763
train_iter_loss: 0.18700124323368073
train_iter_loss: 0.13960641622543335
train_iter_loss: 0.19814464449882507
train_iter_loss: 0.12263230979442596
train_iter_loss: 0.1326485127210617
train_iter_loss: 0.3270229399204254
train_iter_loss: 0.14766953885555267
train_iter_loss: 0.14121787250041962
train_iter_loss: 0.14628446102142334
train_iter_loss: 0.09145404398441315
train_iter_loss: 0.14774473011493683
train_iter_loss: 0.1830301731824875
train_iter_loss: 0.10257196426391602
train_iter_loss: 0.1716919094324112
train_iter_loss: 0.3053357005119324
train_iter_loss: 0.2089909166097641
train_iter_loss: 0.141429603099823
train_iter_loss: 0.16063664853572845
train_iter_loss: 0.042906057089567184
train_iter_loss: 0.0818449929356575
train_iter_loss: 0.14378105103969574
train_iter_loss: 0.11584211885929108
train_iter_loss: 0.15381230413913727
train_iter_loss: 0.08303796499967575
train_iter_loss: 0.16495604813098907
train_iter_loss: 0.12244565784931183
train_iter_loss: 0.15545664727687836
train_iter_loss: 0.25577977299690247
train_iter_loss: 0.1697244644165039
train_iter_loss: 0.07543022185564041
train_iter_loss: 0.18346303701400757
train_iter_loss: 0.2589111328125
train_iter_loss: 0.14989453554153442
train_iter_loss: 0.12103105336427689
train_iter_loss: 0.0915452241897583
train_iter_loss: 0.1168714165687561
train_iter_loss: 0.1082095354795456
train loss :0.1637
---------------------
Validation seg loss: 0.2139077490318637 at epoch 610
epoch =    611/  1000, exp = train
train_iter_loss: 0.06753802299499512
train_iter_loss: 0.17107625305652618
train_iter_loss: 0.1376558393239975
train_iter_loss: 0.18015946447849274
train_iter_loss: 0.11507981270551682
train_iter_loss: 0.20524165034294128
train_iter_loss: 0.1547832041978836
train_iter_loss: 0.24468903243541718
train_iter_loss: 0.13038228452205658
train_iter_loss: 0.13966353237628937
train_iter_loss: 0.17889384925365448
train_iter_loss: 0.1982802152633667
train_iter_loss: 0.11012057960033417
train_iter_loss: 0.18332086503505707
train_iter_loss: 0.2064879685640335
train_iter_loss: 0.17215214669704437
train_iter_loss: 0.08287164568901062
train_iter_loss: 0.16306641697883606
train_iter_loss: 0.21392583847045898
train_iter_loss: 0.06377654522657394
train_iter_loss: 0.06218337267637253
train_iter_loss: 0.14150476455688477
train_iter_loss: 0.1449839323759079
train_iter_loss: 0.13689061999320984
train_iter_loss: 0.09879117459058762
train_iter_loss: 0.23472794890403748
train_iter_loss: 0.19117367267608643
train_iter_loss: 0.24330411851406097
train_iter_loss: 0.14717786014080048
train_iter_loss: 0.13149003684520721
train_iter_loss: 0.0934009850025177
train_iter_loss: 0.17832234501838684
train_iter_loss: 0.07628317922353745
train_iter_loss: 0.17552320659160614
train_iter_loss: 0.14318114519119263
train_iter_loss: 0.14617790281772614
train_iter_loss: 0.2124914973974228
train_iter_loss: 0.12958094477653503
train_iter_loss: 0.06684371083974838
train_iter_loss: 0.15731579065322876
train_iter_loss: 0.20354115962982178
train_iter_loss: 0.05101068317890167
train_iter_loss: 0.08199189603328705
train_iter_loss: 0.12086819857358932
train_iter_loss: 0.1517668515443802
train_iter_loss: 0.05596622824668884
train_iter_loss: 0.23888419568538666
train_iter_loss: 0.18895328044891357
train_iter_loss: 0.38844946026802063
train_iter_loss: 0.15021046996116638
train_iter_loss: 0.0905771255493164
train_iter_loss: 0.06668654084205627
train_iter_loss: 0.14652997255325317
train_iter_loss: 0.09815099090337753
train_iter_loss: 0.14149971306324005
train_iter_loss: 0.1661585569381714
train_iter_loss: 0.13917264342308044
train_iter_loss: 0.1418319195508957
train_iter_loss: 0.1385374814271927
train_iter_loss: 0.1492200344800949
train_iter_loss: 0.14271843433380127
train_iter_loss: 0.17328457534313202
train_iter_loss: 0.04001273587346077
train_iter_loss: 0.269307404756546
train_iter_loss: 0.19874583184719086
train_iter_loss: 0.11538374423980713
train_iter_loss: 0.3241649568080902
train_iter_loss: 0.12293969839811325
train_iter_loss: 0.12116266041994095
train_iter_loss: 0.25131717324256897
train_iter_loss: 0.1688830554485321
train_iter_loss: 0.1544545590877533
train_iter_loss: 0.16685104370117188
train_iter_loss: 0.0832429751753807
train_iter_loss: 0.1989227831363678
train_iter_loss: 0.11583538353443146
train_iter_loss: 0.1485794186592102
train_iter_loss: 0.2228739708662033
train_iter_loss: 0.10415715724229813
train_iter_loss: 0.19417919218540192
train_iter_loss: 0.11391075700521469
train_iter_loss: 0.271301805973053
train_iter_loss: 0.21859271824359894
train_iter_loss: 0.1407301276922226
train_iter_loss: 0.2231597602367401
train_iter_loss: 0.18803057074546814
train_iter_loss: 0.18677590787410736
train_iter_loss: 0.08182253688573837
train_iter_loss: 0.13373780250549316
train_iter_loss: 0.23382405936717987
train_iter_loss: 0.24742618203163147
train_iter_loss: 0.07381626218557358
train_iter_loss: 0.09696675091981888
train_iter_loss: 0.21440765261650085
train_iter_loss: 0.294590026140213
train_iter_loss: 0.21283553540706635
train_iter_loss: 0.13418668508529663
train_iter_loss: 0.09417727589607239
train_iter_loss: 0.20799562335014343
train_iter_loss: 0.11192481219768524
train loss :0.1590
---------------------
Validation seg loss: 0.21711454266365968 at epoch 611
epoch =    612/  1000, exp = train
train_iter_loss: 0.24907232820987701
train_iter_loss: 0.049802474677562714
train_iter_loss: 0.11258462816476822
train_iter_loss: 0.14178036153316498
train_iter_loss: 0.141484797000885
train_iter_loss: 0.106563501060009
train_iter_loss: 0.11560148000717163
train_iter_loss: 0.1390896737575531
train_iter_loss: 0.1675543338060379
train_iter_loss: 0.14047832787036896
train_iter_loss: 0.26956039667129517
train_iter_loss: 0.0710030272603035
train_iter_loss: 0.08770708739757538
train_iter_loss: 0.08871129155158997
train_iter_loss: 0.06451217085123062
train_iter_loss: 0.17125843465328217
train_iter_loss: 0.31466493010520935
train_iter_loss: 0.06421815603971481
train_iter_loss: 0.06878470629453659
train_iter_loss: 0.21787439286708832
train_iter_loss: 0.2135026901960373
train_iter_loss: 0.30195152759552
train_iter_loss: 0.1427016705274582
train_iter_loss: 0.15989217162132263
train_iter_loss: 0.12696710228919983
train_iter_loss: 0.10456634312868118
train_iter_loss: 0.3437258005142212
train_iter_loss: 0.13419954478740692
train_iter_loss: 0.20281504094600677
train_iter_loss: 0.20655310153961182
train_iter_loss: 0.15455536544322968
train_iter_loss: 0.12207694351673126
train_iter_loss: 0.3240085244178772
train_iter_loss: 0.2057274878025055
train_iter_loss: 0.1063917949795723
train_iter_loss: 0.1963491588830948
train_iter_loss: 0.18653233349323273
train_iter_loss: 0.14074590802192688
train_iter_loss: 0.20029209554195404
train_iter_loss: 0.15695863962173462
train_iter_loss: 0.11792968958616257
train_iter_loss: 0.17146769165992737
train_iter_loss: 0.09893257915973663
train_iter_loss: 0.14296916127204895
train_iter_loss: 0.2423727959394455
train_iter_loss: 0.027796855196356773
train_iter_loss: 0.16095346212387085
train_iter_loss: 0.1952466070652008
train_iter_loss: 0.16319768130779266
train_iter_loss: 0.16954447329044342
train_iter_loss: 0.11133818328380585
train_iter_loss: 0.0934094786643982
train_iter_loss: 0.2548903226852417
train_iter_loss: 0.163946732878685
train_iter_loss: 0.12185918539762497
train_iter_loss: 0.12727196514606476
train_iter_loss: 0.1899486482143402
train_iter_loss: 0.18411442637443542
train_iter_loss: 0.15758410096168518
train_iter_loss: 0.07196538895368576
train_iter_loss: 0.27288052439689636
train_iter_loss: 0.19051627814769745
train_iter_loss: 0.16187377274036407
train_iter_loss: 0.1177029088139534
train_iter_loss: 0.2939285933971405
train_iter_loss: 0.20939920842647552
train_iter_loss: 0.1733599156141281
train_iter_loss: 0.10258983075618744
train_iter_loss: 0.17009012401103973
train_iter_loss: 0.1183369979262352
train_iter_loss: 0.18056941032409668
train_iter_loss: 0.12808935344219208
train_iter_loss: 0.23394405841827393
train_iter_loss: 0.09415575116872787
train_iter_loss: 0.1925518810749054
train_iter_loss: 0.1457652449607849
train_iter_loss: 0.17035287618637085
train_iter_loss: 0.09377001225948334
train_iter_loss: 0.23154443502426147
train_iter_loss: 0.13461890816688538
train_iter_loss: 0.08603502810001373
train_iter_loss: 0.09317602962255478
train_iter_loss: 0.16492949426174164
train_iter_loss: 0.2090459167957306
train_iter_loss: 0.13967937231063843
train_iter_loss: 0.21477223932743073
train_iter_loss: 0.256303608417511
train_iter_loss: 0.12299264222383499
train_iter_loss: 0.1446307897567749
train_iter_loss: 0.18160277605056763
train_iter_loss: 0.15560267865657806
train_iter_loss: 0.12437152117490768
train_iter_loss: 0.14576579630374908
train_iter_loss: 0.1265478879213333
train_iter_loss: 0.14111663401126862
train_iter_loss: 0.17136654257774353
train_iter_loss: 0.15389850735664368
train_iter_loss: 0.2100841999053955
train_iter_loss: 0.22497990727424622
train_iter_loss: 0.10859040170907974
train loss :0.1623
---------------------
Validation seg loss: 0.21353530902239792 at epoch 612
epoch =    613/  1000, exp = train
train_iter_loss: 0.25033891201019287
train_iter_loss: 0.15095102787017822
train_iter_loss: 0.172185018658638
train_iter_loss: 0.18503546714782715
train_iter_loss: 0.06852921843528748
train_iter_loss: 0.17477674782276154
train_iter_loss: 0.1299782693386078
train_iter_loss: 0.11451628059148788
train_iter_loss: 0.10105614364147186
train_iter_loss: 0.1448749601840973
train_iter_loss: 0.0695670023560524
train_iter_loss: 0.13735809922218323
train_iter_loss: 0.19329608976840973
train_iter_loss: 0.1468316614627838
train_iter_loss: 0.1208404004573822
train_iter_loss: 0.09572363644838333
train_iter_loss: 0.17418423295021057
train_iter_loss: 0.22291553020477295
train_iter_loss: 0.16553102433681488
train_iter_loss: 0.16505621373653412
train_iter_loss: 0.1345960795879364
train_iter_loss: 0.20739328861236572
train_iter_loss: 0.1998782455921173
train_iter_loss: 0.15600185096263885
train_iter_loss: 0.106545090675354
train_iter_loss: 0.14898322522640228
train_iter_loss: 0.18059241771697998
train_iter_loss: 0.2524343729019165
train_iter_loss: 0.07530197501182556
train_iter_loss: 0.12860441207885742
train_iter_loss: 0.12585125863552094
train_iter_loss: 0.05962754786014557
train_iter_loss: 0.1943083107471466
train_iter_loss: 0.09684697538614273
train_iter_loss: 0.16706186532974243
train_iter_loss: 0.17390033602714539
train_iter_loss: 0.1114799827337265
train_iter_loss: 0.17798030376434326
train_iter_loss: 0.21684399247169495
train_iter_loss: 0.056904830038547516
train_iter_loss: 0.15646876394748688
train_iter_loss: 0.20716266334056854
train_iter_loss: 0.17850062251091003
train_iter_loss: 0.1807333081960678
train_iter_loss: 0.12923136353492737
train_iter_loss: 0.20060767233371735
train_iter_loss: 0.12868115305900574
train_iter_loss: 0.20444880425930023
train_iter_loss: 0.16895070672035217
train_iter_loss: 0.2393910437822342
train_iter_loss: 0.23663859069347382
train_iter_loss: 0.19793187081813812
train_iter_loss: 0.16395188868045807
train_iter_loss: 0.1058361753821373
train_iter_loss: 0.2750411629676819
train_iter_loss: 0.14047542214393616
train_iter_loss: 0.1356261670589447
train_iter_loss: 0.12411079555749893
train_iter_loss: 0.09099259227514267
train_iter_loss: 0.168814554810524
train_iter_loss: 0.08609642088413239
train_iter_loss: 0.1366821676492691
train_iter_loss: 0.14507927000522614
train_iter_loss: 0.1070394217967987
train_iter_loss: 0.09224971383810043
train_iter_loss: 0.12315971404314041
train_iter_loss: 0.08669207245111465
train_iter_loss: 0.30589237809181213
train_iter_loss: 0.23231805860996246
train_iter_loss: 0.11546274274587631
train_iter_loss: 0.2775304913520813
train_iter_loss: 0.12447561323642731
train_iter_loss: 0.18012404441833496
train_iter_loss: 0.16986291110515594
train_iter_loss: 0.13104644417762756
train_iter_loss: 0.04719184711575508
train_iter_loss: 0.07760912925004959
train_iter_loss: 0.14668916165828705
train_iter_loss: 0.11914529651403427
train_iter_loss: 0.04255085438489914
train_iter_loss: 0.15424199402332306
train_iter_loss: 0.1663043349981308
train_iter_loss: 0.038585737347602844
train_iter_loss: 0.1415022760629654
train_iter_loss: 0.18493379652500153
train_iter_loss: 0.08656149357557297
train_iter_loss: 0.16219820082187653
train_iter_loss: 0.23092429339885712
train_iter_loss: 0.2698320746421814
train_iter_loss: 0.12921974062919617
train_iter_loss: 0.13338541984558105
train_iter_loss: 0.11572163552045822
train_iter_loss: 0.10653848946094513
train_iter_loss: 0.23694758117198944
train_iter_loss: 0.09626273810863495
train_iter_loss: 0.1376306712627411
train_iter_loss: 0.16145041584968567
train_iter_loss: 0.12022153288125992
train_iter_loss: 0.06531237810850143
train_iter_loss: 0.10488346964120865
train loss :0.1514
---------------------
Validation seg loss: 0.21619094316935483 at epoch 613
epoch =    614/  1000, exp = train
train_iter_loss: 0.14936783909797668
train_iter_loss: 0.15581102669239044
train_iter_loss: 0.22907914221286774
train_iter_loss: 0.14244352281093597
train_iter_loss: 0.1100514829158783
train_iter_loss: 0.11661683022975922
train_iter_loss: 0.23435412347316742
train_iter_loss: 0.08591245114803314
train_iter_loss: 0.17179737985134125
train_iter_loss: 0.3035261929035187
train_iter_loss: 0.13653571903705597
train_iter_loss: 0.3048737347126007
train_iter_loss: 0.11969562619924545
train_iter_loss: 0.17119871079921722
train_iter_loss: 0.1495860069990158
train_iter_loss: 0.06868799775838852
train_iter_loss: 0.08866282552480698
train_iter_loss: 0.18890637159347534
train_iter_loss: 0.07042595744132996
train_iter_loss: 0.11269110441207886
train_iter_loss: 0.5649132132530212
train_iter_loss: 0.08851553499698639
train_iter_loss: 0.04715504124760628
train_iter_loss: 0.18387629091739655
train_iter_loss: 0.11432570219039917
train_iter_loss: 0.2081349790096283
train_iter_loss: 0.13634136319160461
train_iter_loss: 0.14079876244068146
train_iter_loss: 0.07347947359085083
train_iter_loss: 0.12196677923202515
train_iter_loss: 0.12400993704795837
train_iter_loss: 0.16042907536029816
train_iter_loss: 0.10768678784370422
train_iter_loss: 0.12104173749685287
train_iter_loss: 0.13235971331596375
train_iter_loss: 0.20291317999362946
train_iter_loss: 0.19291181862354279
train_iter_loss: 0.14814923703670502
train_iter_loss: 0.24241171777248383
train_iter_loss: 0.14161570370197296
train_iter_loss: 0.1983398050069809
train_iter_loss: 0.21976333856582642
train_iter_loss: 0.14386020600795746
train_iter_loss: 0.03830694779753685
train_iter_loss: 0.23151901364326477
train_iter_loss: 0.1576477736234665
train_iter_loss: 0.22770556807518005
train_iter_loss: 0.12851223349571228
train_iter_loss: 0.1846533864736557
train_iter_loss: 0.2159041315317154
train_iter_loss: 0.12453790009021759
train_iter_loss: 0.19024434685707092
train_iter_loss: 0.1544143259525299
train_iter_loss: 0.22888945043087006
train_iter_loss: 0.17220227420330048
train_iter_loss: 0.2002330720424652
train_iter_loss: 0.13964755833148956
train_iter_loss: 0.1765296459197998
train_iter_loss: 0.08040037751197815
train_iter_loss: 0.1478002816438675
train_iter_loss: 0.2498972862958908
train_iter_loss: 0.08606668561697006
train_iter_loss: 0.17964217066764832
train_iter_loss: 0.04219375178217888
train_iter_loss: 0.19550247490406036
train_iter_loss: 0.19339600205421448
train_iter_loss: 0.2193356603384018
train_iter_loss: 0.2107420265674591
train_iter_loss: 0.16368941962718964
train_iter_loss: 0.14912891387939453
train_iter_loss: 0.030382400378584862
train_iter_loss: 0.1912480890750885
train_iter_loss: 0.1351211667060852
train_iter_loss: 0.1323397159576416
train_iter_loss: 0.14029018580913544
train_iter_loss: 0.15120550990104675
train_iter_loss: 0.11811166256666183
train_iter_loss: 0.1215054988861084
train_iter_loss: 0.14069877564907074
train_iter_loss: 0.14474545419216156
train_iter_loss: 0.1674451231956482
train_iter_loss: 0.08833912014961243
train_iter_loss: 0.166128471493721
train_iter_loss: 0.09713714569807053
train_iter_loss: 0.11907742917537689
train_iter_loss: 0.2898451089859009
train_iter_loss: 0.16707125306129456
train_iter_loss: 0.1440502107143402
train_iter_loss: 0.05944899097084999
train_iter_loss: 0.10773661732673645
train_iter_loss: 0.32767561078071594
train_iter_loss: 0.29901763796806335
train_iter_loss: 0.14866618812084198
train_iter_loss: 0.20456460118293762
train_iter_loss: 0.09381166845560074
train_iter_loss: 0.07827279716730118
train_iter_loss: 0.21658192574977875
train_iter_loss: 0.2047610878944397
train_iter_loss: 0.1302754133939743
train_iter_loss: 0.17410054802894592
train loss :0.1616
---------------------
Validation seg loss: 0.21649983867932604 at epoch 614
epoch =    615/  1000, exp = train
train_iter_loss: 0.17002582550048828
train_iter_loss: 0.12484674155712128
train_iter_loss: 0.15408386290073395
train_iter_loss: 0.1223556324839592
train_iter_loss: 0.274007648229599
train_iter_loss: 0.20271526277065277
train_iter_loss: 0.20119518041610718
train_iter_loss: 0.11757686734199524
train_iter_loss: 0.11571674793958664
train_iter_loss: 0.1357974261045456
train_iter_loss: 0.22800955176353455
train_iter_loss: 0.1632245033979416
train_iter_loss: 0.16715973615646362
train_iter_loss: 0.1879398375749588
train_iter_loss: 0.2546495795249939
train_iter_loss: 0.10614480823278427
train_iter_loss: 0.15190231800079346
train_iter_loss: 0.17195656895637512
train_iter_loss: 0.0932280495762825
train_iter_loss: 0.1328052431344986
train_iter_loss: 0.06337472796440125
train_iter_loss: 0.19611315429210663
train_iter_loss: 0.255267858505249
train_iter_loss: 0.09707652032375336
train_iter_loss: 0.227016419172287
train_iter_loss: 0.18932169675827026
train_iter_loss: 0.17917276918888092
train_iter_loss: 0.1911267191171646
train_iter_loss: 0.1514187753200531
train_iter_loss: 0.11605630815029144
train_iter_loss: 0.08858239650726318
train_iter_loss: 0.18890492618083954
train_iter_loss: 0.287554532289505
train_iter_loss: 0.12188787013292313
train_iter_loss: 0.1291021853685379
train_iter_loss: 0.154448002576828
train_iter_loss: 0.061867520213127136
train_iter_loss: 0.12053146958351135
train_iter_loss: 0.11144623160362244
train_iter_loss: 0.11311259865760803
train_iter_loss: 0.27246060967445374
train_iter_loss: 0.08320393413305283
train_iter_loss: 0.18425671756267548
train_iter_loss: 0.12263654917478561
train_iter_loss: 0.08617326617240906
train_iter_loss: 0.19557331502437592
train_iter_loss: 0.13966895639896393
train_iter_loss: 0.156686931848526
train_iter_loss: 0.11583389341831207
train_iter_loss: 0.11560819298028946
train_iter_loss: 0.12751390039920807
train_iter_loss: 0.12227629125118256
train_iter_loss: 0.11067861318588257
train_iter_loss: 0.09587203711271286
train_iter_loss: 0.05328361317515373
train_iter_loss: 0.23568475246429443
train_iter_loss: 0.1099676564335823
train_iter_loss: 0.1433965414762497
train_iter_loss: 0.12288694828748703
train_iter_loss: 0.08473430573940277
train_iter_loss: 0.11524529755115509
train_iter_loss: 0.151157945394516
train_iter_loss: 0.14497047662734985
train_iter_loss: 0.1462341547012329
train_iter_loss: 0.19500839710235596
train_iter_loss: 0.16927234828472137
train_iter_loss: 0.16497692465782166
train_iter_loss: 0.14003172516822815
train_iter_loss: 0.22814998030662537
train_iter_loss: 0.2295248955488205
train_iter_loss: 0.12042904645204544
train_iter_loss: 0.17149601876735687
train_iter_loss: 0.2426292598247528
train_iter_loss: 0.16074547171592712
train_iter_loss: 0.24145537614822388
train_iter_loss: 0.11678928881883621
train_iter_loss: 0.08811230957508087
train_iter_loss: 0.1314287781715393
train_iter_loss: 0.1468261480331421
train_iter_loss: 0.11352895200252533
train_iter_loss: 0.2407716065645218
train_iter_loss: 0.14159253239631653
train_iter_loss: 0.0772486999630928
train_iter_loss: 0.13001708686351776
train_iter_loss: 0.05469125509262085
train_iter_loss: 0.11013752967119217
train_iter_loss: 0.1613810658454895
train_iter_loss: 0.16604860126972198
train_iter_loss: 0.21409130096435547
train_iter_loss: 0.23251426219940186
train_iter_loss: 0.42950886487960815
train_iter_loss: 0.12740355730056763
train_iter_loss: 0.20018695294857025
train_iter_loss: 0.1873226761817932
train_iter_loss: 0.26749977469444275
train_iter_loss: 0.13874098658561707
train_iter_loss: 0.2288724184036255
train_iter_loss: 0.1648561805486679
train_iter_loss: 0.13820631802082062
train_iter_loss: 0.3428836762905121
train loss :0.1613
---------------------
Validation seg loss: 0.21618778231325295 at epoch 615
epoch =    616/  1000, exp = train
train_iter_loss: 0.20051230490207672
train_iter_loss: 0.14091911911964417
train_iter_loss: 0.3498750925064087
train_iter_loss: 0.18403533101081848
train_iter_loss: 0.09583785384893417
train_iter_loss: 0.11796625703573227
train_iter_loss: 0.23089154064655304
train_iter_loss: 0.08638753741979599
train_iter_loss: 0.07660514116287231
train_iter_loss: 0.27947404980659485
train_iter_loss: 0.1628868132829666
train_iter_loss: 0.2007990926504135
train_iter_loss: 0.1521781086921692
train_iter_loss: 0.10510312020778656
train_iter_loss: 0.13355271518230438
train_iter_loss: 0.15973952412605286
train_iter_loss: 0.1098337471485138
train_iter_loss: 0.22480325400829315
train_iter_loss: 0.10432964563369751
train_iter_loss: 0.264925479888916
train_iter_loss: 0.1929461807012558
train_iter_loss: 0.10344520956277847
train_iter_loss: 0.16226689517498016
train_iter_loss: 0.12657864391803741
train_iter_loss: 0.16070595383644104
train_iter_loss: 0.14222800731658936
train_iter_loss: 0.05621343106031418
train_iter_loss: 0.11806271970272064
train_iter_loss: 0.13771387934684753
train_iter_loss: 0.10710678994655609
train_iter_loss: 0.12130081653594971
train_iter_loss: 0.17658406496047974
train_iter_loss: 0.14051486551761627
train_iter_loss: 0.15678942203521729
train_iter_loss: 0.15989723801612854
train_iter_loss: 0.13942429423332214
train_iter_loss: 0.17993128299713135
train_iter_loss: 0.19316275417804718
train_iter_loss: 0.1272510439157486
train_iter_loss: 0.09891040623188019
train_iter_loss: 0.17540410161018372
train_iter_loss: 0.13024938106536865
train_iter_loss: 0.2101990133523941
train_iter_loss: 0.10218928754329681
train_iter_loss: 0.18327555060386658
train_iter_loss: 0.10994262993335724
train_iter_loss: 0.13452479243278503
train_iter_loss: 0.18279452621936798
train_iter_loss: 0.27359041571617126
train_iter_loss: 0.2631809413433075
train_iter_loss: 0.13460363447666168
train_iter_loss: 0.05223187059164047
train_iter_loss: 0.11280347406864166
train_iter_loss: 0.24435819685459137
train_iter_loss: 0.20617027580738068
train_iter_loss: 0.14683805406093597
train_iter_loss: 0.23733903467655182
train_iter_loss: 0.0821533128619194
train_iter_loss: 0.2870320975780487
train_iter_loss: 0.10085835307836533
train_iter_loss: 0.1172858476638794
train_iter_loss: 0.11768843978643417
train_iter_loss: 0.1177530363202095
train_iter_loss: 0.2623006999492645
train_iter_loss: 0.2208688110113144
train_iter_loss: 0.2316020429134369
train_iter_loss: 0.15114445984363556
train_iter_loss: 0.16046997904777527
train_iter_loss: 0.133540078997612
train_iter_loss: 0.09072242677211761
train_iter_loss: 0.15374119579792023
train_iter_loss: 0.07770532369613647
train_iter_loss: 0.19660568237304688
train_iter_loss: 0.15846678614616394
train_iter_loss: 0.12104861438274384
train_iter_loss: 0.17170511186122894
train_iter_loss: 0.15742544829845428
train_iter_loss: 0.20486971735954285
train_iter_loss: 0.2149154543876648
train_iter_loss: 0.07963470369577408
train_iter_loss: 0.13934335112571716
train_iter_loss: 0.20392043888568878
train_iter_loss: 0.20682378113269806
train_iter_loss: 0.13238827884197235
train_iter_loss: 0.3211146891117096
train_iter_loss: 0.15559644997119904
train_iter_loss: 0.1285540908575058
train_iter_loss: 0.15843017399311066
train_iter_loss: 0.13045579195022583
train_iter_loss: 0.22277787327766418
train_iter_loss: 0.1832410842180252
train_iter_loss: 0.13931967318058014
train_iter_loss: 0.1641831248998642
train_iter_loss: 0.1364542543888092
train_iter_loss: 0.058719195425510406
train_iter_loss: 0.06929322332143784
train_iter_loss: 0.13937442004680634
train_iter_loss: 0.11905879527330399
train_iter_loss: 0.3537404239177704
train_iter_loss: 0.2378063201904297
train loss :0.1625
---------------------
Validation seg loss: 0.21691008925191918 at epoch 616
epoch =    617/  1000, exp = train
train_iter_loss: 0.1711340993642807
train_iter_loss: 0.19013279676437378
train_iter_loss: 0.12028856575489044
train_iter_loss: 0.1869804561138153
train_iter_loss: 0.08585938066244125
train_iter_loss: 0.13222919404506683
train_iter_loss: 0.15617616474628448
train_iter_loss: 0.17865784466266632
train_iter_loss: 0.11768031865358353
train_iter_loss: 0.10729213058948517
train_iter_loss: 0.15783952176570892
train_iter_loss: 0.1502222865819931
train_iter_loss: 0.15167981386184692
train_iter_loss: 0.2607802450656891
train_iter_loss: 0.24001160264015198
train_iter_loss: 0.26257333159446716
train_iter_loss: 0.1623632162809372
train_iter_loss: 0.13148339092731476
train_iter_loss: 0.10560066252946854
train_iter_loss: 0.20964202284812927
train_iter_loss: 0.26981332898139954
train_iter_loss: 0.14939536154270172
train_iter_loss: 0.16280514001846313
train_iter_loss: 0.08046875149011612
train_iter_loss: 0.185128852725029
train_iter_loss: 0.2363298088312149
train_iter_loss: 0.08121699839830399
train_iter_loss: 0.12846669554710388
train_iter_loss: 0.10704164206981659
train_iter_loss: 0.07028452306985855
train_iter_loss: 0.0787385106086731
train_iter_loss: 0.18490658700466156
train_iter_loss: 0.1717706322669983
train_iter_loss: 0.11705725640058517
train_iter_loss: 0.17328482866287231
train_iter_loss: 0.13924293220043182
train_iter_loss: 0.12482675164937973
train_iter_loss: 0.0781623125076294
train_iter_loss: 0.04362952336668968
train_iter_loss: 0.15838736295700073
train_iter_loss: 0.1280648559331894
train_iter_loss: 0.1481839269399643
train_iter_loss: 0.2841178774833679
train_iter_loss: 0.11473316699266434
train_iter_loss: 0.12867814302444458
train_iter_loss: 0.1728137880563736
train_iter_loss: 0.12811318039894104
train_iter_loss: 0.3081544041633606
train_iter_loss: 0.19151322543621063
train_iter_loss: 0.1908389776945114
train_iter_loss: 0.22185532748699188
train_iter_loss: 0.2085016369819641
train_iter_loss: 0.10629726946353912
train_iter_loss: 0.07371090352535248
train_iter_loss: 0.08897928148508072
train_iter_loss: 0.37076759338378906
train_iter_loss: 0.1312011033296585
train_iter_loss: 0.16081669926643372
train_iter_loss: 0.14932045340538025
train_iter_loss: 0.06903789937496185
train_iter_loss: 0.08248627930879593
train_iter_loss: 0.20762747526168823
train_iter_loss: 0.260829359292984
train_iter_loss: 0.15018920600414276
train_iter_loss: 0.1180352047085762
train_iter_loss: 0.16378149390220642
train_iter_loss: 0.25792816281318665
train_iter_loss: 0.09812502562999725
train_iter_loss: 0.14030519127845764
train_iter_loss: 0.10035556554794312
train_iter_loss: 0.23113931715488434
train_iter_loss: 0.21672789752483368
train_iter_loss: 0.1564643681049347
train_iter_loss: 0.22577384114265442
train_iter_loss: 0.15241609513759613
train_iter_loss: 0.11156126856803894
train_iter_loss: 0.13057754933834076
train_iter_loss: 0.24815474450588226
train_iter_loss: 0.07749582827091217
train_iter_loss: 0.12541401386260986
train_iter_loss: 0.15065471827983856
train_iter_loss: 0.19900453090667725
train_iter_loss: 0.2625485062599182
train_iter_loss: 0.12509292364120483
train_iter_loss: 0.18934933841228485
train_iter_loss: 0.15316371619701385
train_iter_loss: 0.11702346801757812
train_iter_loss: 0.10149000585079193
train_iter_loss: 0.15960372984409332
train_iter_loss: 0.1356772482395172
train_iter_loss: 0.22196784615516663
train_iter_loss: 0.06812794506549835
train_iter_loss: 0.15447084605693817
train_iter_loss: 0.12852059304714203
train_iter_loss: 0.14568758010864258
train_iter_loss: 0.17780020833015442
train_iter_loss: 0.0627695694565773
train_iter_loss: 0.09218090027570724
train_iter_loss: 0.16858384013175964
train_iter_loss: 0.15827322006225586
train loss :0.1579
---------------------
Validation seg loss: 0.21266037884677919 at epoch 617
epoch =    618/  1000, exp = train
train_iter_loss: 0.13966770470142365
train_iter_loss: 0.15053434669971466
train_iter_loss: 0.18271955847740173
train_iter_loss: 0.19300952553749084
train_iter_loss: 0.10851109027862549
train_iter_loss: 0.08656390756368637
train_iter_loss: 0.22374123334884644
train_iter_loss: 0.10049712657928467
train_iter_loss: 0.1946578025817871
train_iter_loss: 0.2414793074131012
train_iter_loss: 0.18247674405574799
train_iter_loss: 0.12942369282245636
train_iter_loss: 0.20825603604316711
train_iter_loss: 0.07607581466436386
train_iter_loss: 0.3013482987880707
train_iter_loss: 0.12301319092512131
train_iter_loss: 0.19440622627735138
train_iter_loss: 0.11456047743558884
train_iter_loss: 0.17249543964862823
train_iter_loss: 0.19365566968917847
train_iter_loss: 0.11767292767763138
train_iter_loss: 0.16381141543388367
train_iter_loss: 0.15611615777015686
train_iter_loss: 0.15429255366325378
train_iter_loss: 0.12416121363639832
train_iter_loss: 0.13933083415031433
train_iter_loss: 0.14052432775497437
train_iter_loss: 0.08881154656410217
train_iter_loss: 0.13584432005882263
train_iter_loss: 0.31522253155708313
train_iter_loss: 0.09271425008773804
train_iter_loss: 0.27958735823631287
train_iter_loss: 0.11326223611831665
train_iter_loss: 0.18745173513889313
train_iter_loss: 0.09339504688978195
train_iter_loss: 0.14135824143886566
train_iter_loss: 0.13127632439136505
train_iter_loss: 0.15013688802719116
train_iter_loss: 0.16739122569561005
train_iter_loss: 0.07382000237703323
train_iter_loss: 0.1884872168302536
train_iter_loss: 0.20690879225730896
train_iter_loss: 0.10032007843255997
train_iter_loss: 0.12996694445610046
train_iter_loss: 0.2702939808368683
train_iter_loss: 0.18802745640277863
train_iter_loss: 0.08857222646474838
train_iter_loss: 0.1959543526172638
train_iter_loss: 0.22885502874851227
train_iter_loss: 0.140667125582695
train_iter_loss: 0.23168355226516724
train_iter_loss: 0.24233798682689667
train_iter_loss: 0.11283333599567413
train_iter_loss: 0.17565006017684937
train_iter_loss: 0.06612949073314667
train_iter_loss: 0.12606722116470337
train_iter_loss: 0.16658297181129456
train_iter_loss: 0.13961149752140045
train_iter_loss: 0.2234874665737152
train_iter_loss: 0.12305909395217896
train_iter_loss: 0.1443449705839157
train_iter_loss: 0.14261260628700256
train_iter_loss: 0.21199947595596313
train_iter_loss: 0.23384137451648712
train_iter_loss: 0.2869197130203247
train_iter_loss: 0.14725618064403534
train_iter_loss: 0.13120383024215698
train_iter_loss: 0.4370560646057129
train_iter_loss: 0.14547811448574066
train_iter_loss: 0.14375947415828705
train_iter_loss: 0.16453129053115845
train_iter_loss: 0.12745490670204163
train_iter_loss: 0.17248371243476868
train_iter_loss: 0.14832481741905212
train_iter_loss: 0.10209642350673676
train_iter_loss: 0.18941165506839752
train_iter_loss: 0.14715692400932312
train_iter_loss: 0.2954065501689911
train_iter_loss: 0.05906185880303383
train_iter_loss: 0.0859210267663002
train_iter_loss: 0.15183740854263306
train_iter_loss: 0.05063565447926521
train_iter_loss: 0.11940930783748627
train_iter_loss: 0.06732363253831863
train_iter_loss: 0.22218582034111023
train_iter_loss: 0.22949641942977905
train_iter_loss: 0.211758092045784
train_iter_loss: 0.213837131857872
train_iter_loss: 0.16310274600982666
train_iter_loss: 0.265018105506897
train_iter_loss: 0.09710733592510223
train_iter_loss: 0.1955862194299698
train_iter_loss: 0.12493596971035004
train_iter_loss: 0.07434431463479996
train_iter_loss: 0.2777901887893677
train_iter_loss: 0.15673135221004486
train_iter_loss: 0.08435683697462082
train_iter_loss: 0.13136179745197296
train_iter_loss: 0.14188556373119354
train_iter_loss: 0.12754569947719574
train loss :0.1641
---------------------
Validation seg loss: 0.2141673590478329 at epoch 618
epoch =    619/  1000, exp = train
train_iter_loss: 0.15101203322410583
train_iter_loss: 0.11752258986234665
train_iter_loss: 0.17006896436214447
train_iter_loss: 0.2411615252494812
train_iter_loss: 0.1640610545873642
train_iter_loss: 0.26932379603385925
train_iter_loss: 0.12491229921579361
train_iter_loss: 0.19492247700691223
train_iter_loss: 0.1481708437204361
train_iter_loss: 0.16355624794960022
train_iter_loss: 0.1588299721479416
train_iter_loss: 0.24666786193847656
train_iter_loss: 0.08336203545331955
train_iter_loss: 0.10576420277357101
train_iter_loss: 0.1305345743894577
train_iter_loss: 0.17198096215724945
train_iter_loss: 0.10619745403528214
train_iter_loss: 0.08379559218883514
train_iter_loss: 0.3449019491672516
train_iter_loss: 0.18411751091480255
train_iter_loss: 0.13988247513771057
train_iter_loss: 0.0833258256316185
train_iter_loss: 0.1313963234424591
train_iter_loss: 0.19008314609527588
train_iter_loss: 0.0820474699139595
train_iter_loss: 0.19724969565868378
train_iter_loss: 0.11094512790441513
train_iter_loss: 0.3299081027507782
train_iter_loss: 0.1821572184562683
train_iter_loss: 0.15339897572994232
train_iter_loss: 0.2402811199426651
train_iter_loss: 0.34424299001693726
train_iter_loss: 0.14289899170398712
train_iter_loss: 0.07870998978614807
train_iter_loss: 0.1998707503080368
train_iter_loss: 0.18029390275478363
train_iter_loss: 0.1168927252292633
train_iter_loss: 0.15842017531394958
train_iter_loss: 0.09920436888933182
train_iter_loss: 0.12370255589485168
train_iter_loss: 0.13659539818763733
train_iter_loss: 0.06075242906808853
train_iter_loss: 0.17980948090553284
train_iter_loss: 0.12241636216640472
train_iter_loss: 0.1510539948940277
train_iter_loss: 0.15663832426071167
train_iter_loss: 0.1316605806350708
train_iter_loss: 0.20423884689807892
train_iter_loss: 0.06320276856422424
train_iter_loss: 0.1679396778345108
train_iter_loss: 0.14585711061954498
train_iter_loss: 0.11630238592624664
train_iter_loss: 0.0841117650270462
train_iter_loss: 0.15842550992965698
train_iter_loss: 0.10046030580997467
train_iter_loss: 0.07332883030176163
train_iter_loss: 0.20014438033103943
train_iter_loss: 0.16413062810897827
train_iter_loss: 0.08034300059080124
train_iter_loss: 0.13060756027698517
train_iter_loss: 0.09249947965145111
train_iter_loss: 0.1576378494501114
train_iter_loss: 0.1332002729177475
train_iter_loss: 0.17067934572696686
train_iter_loss: 0.14715944230556488
train_iter_loss: 0.076214499771595
train_iter_loss: 0.24027591943740845
train_iter_loss: 0.1518716961145401
train_iter_loss: 0.2633216381072998
train_iter_loss: 0.12453122437000275
train_iter_loss: 0.10509241372346878
train_iter_loss: 0.2538280189037323
train_iter_loss: 0.1991845965385437
train_iter_loss: 0.18612632155418396
train_iter_loss: 0.18227337300777435
train_iter_loss: 0.08040384948253632
train_iter_loss: 0.20261162519454956
train_iter_loss: 0.12674082815647125
train_iter_loss: 0.1472921222448349
train_iter_loss: 0.16820915043354034
train_iter_loss: 0.17584338784217834
train_iter_loss: 0.0784057080745697
train_iter_loss: 0.22326208651065826
train_iter_loss: 0.14593246579170227
train_iter_loss: 0.16002555191516876
train_iter_loss: 0.15776586532592773
train_iter_loss: 0.11928324401378632
train_iter_loss: 0.15827172994613647
train_iter_loss: 0.09022972732782364
train_iter_loss: 0.15426118671894073
train_iter_loss: 0.12565872073173523
train_iter_loss: 0.2069673389196396
train_iter_loss: 0.10565939545631409
train_iter_loss: 0.11756766587495804
train_iter_loss: 0.121640145778656
train_iter_loss: 0.1752096265554428
train_iter_loss: 0.11772466450929642
train_iter_loss: 0.21757788956165314
train_iter_loss: 0.1421113759279251
train_iter_loss: 0.06932956725358963
train loss :0.1551
---------------------
Validation seg loss: 0.2156245512272811 at epoch 619
epoch =    620/  1000, exp = train
train_iter_loss: 0.1378856897354126
train_iter_loss: 0.23858101665973663
train_iter_loss: 0.09012870490550995
train_iter_loss: 0.09087962657213211
train_iter_loss: 0.0942036360502243
train_iter_loss: 0.19883882999420166
train_iter_loss: 0.12040159851312637
train_iter_loss: 0.3659588098526001
train_iter_loss: 0.19112156331539154
train_iter_loss: 0.1667262464761734
train_iter_loss: 0.13056634366512299
train_iter_loss: 0.10481688380241394
train_iter_loss: 0.10082654654979706
train_iter_loss: 0.3290741443634033
train_iter_loss: 0.15801165997982025
train_iter_loss: 0.12127634882926941
train_iter_loss: 0.15830156207084656
train_iter_loss: 0.24699684977531433
train_iter_loss: 0.1436282843351364
train_iter_loss: 0.02931901067495346
train_iter_loss: 0.09165922552347183
train_iter_loss: 0.20206639170646667
train_iter_loss: 0.17582976818084717
train_iter_loss: 0.11590586602687836
train_iter_loss: 0.1740233600139618
train_iter_loss: 0.19993215799331665
train_iter_loss: 0.2244311422109604
train_iter_loss: 0.17868131399154663
train_iter_loss: 0.15429186820983887
train_iter_loss: 0.19807149469852448
train_iter_loss: 0.14871551096439362
train_iter_loss: 0.11316408216953278
train_iter_loss: 0.23611564934253693
train_iter_loss: 0.09946221113204956
train_iter_loss: 0.13492169976234436
train_iter_loss: 0.2234249860048294
train_iter_loss: 0.1910061240196228
train_iter_loss: 0.11754929274320602
train_iter_loss: 0.3265571892261505
train_iter_loss: 0.08312352001667023
train_iter_loss: 0.10455389320850372
train_iter_loss: 0.18418611586093903
train_iter_loss: 0.07055921107530594
train_iter_loss: 0.18054065108299255
train_iter_loss: 0.1600736677646637
train_iter_loss: 0.13755908608436584
train_iter_loss: 0.13712525367736816
train_iter_loss: 0.24248626828193665
train_iter_loss: 0.1605502814054489
train_iter_loss: 0.1778842657804489
train_iter_loss: 0.12681327760219574
train_iter_loss: 0.10056669265031815
train_iter_loss: 0.21375387907028198
train_iter_loss: 0.07938830554485321
train_iter_loss: 0.08814738690853119
train_iter_loss: 0.16198958456516266
train_iter_loss: 0.2074592262506485
train_iter_loss: 0.17523734271526337
train_iter_loss: 0.19002772867679596
train_iter_loss: 0.10765843838453293
train_iter_loss: 0.1825382113456726
train_iter_loss: 0.10997984558343887
train_iter_loss: 0.16562199592590332
train_iter_loss: 0.23547424376010895
train_iter_loss: 0.16809581220149994
train_iter_loss: 0.07733900099992752
train_iter_loss: 0.051965806633234024
train_iter_loss: 0.09455344825983047
train_iter_loss: 0.07602740079164505
train_iter_loss: 0.12391798943281174
train_iter_loss: 0.11024629324674606
train_iter_loss: 0.12807409465312958
train_iter_loss: 0.06942974776029587
train_iter_loss: 0.0582621805369854
train_iter_loss: 0.1770889163017273
train_iter_loss: 0.12479878216981888
train_iter_loss: 0.0724577009677887
train_iter_loss: 0.16803088784217834
train_iter_loss: 0.1969970166683197
train_iter_loss: 0.14714016020298004
train_iter_loss: 0.19702136516571045
train_iter_loss: 0.17364078760147095
train_iter_loss: 0.10178585350513458
train_iter_loss: 0.23228202760219574
train_iter_loss: 0.22808951139450073
train_iter_loss: 0.20518726110458374
train_iter_loss: 0.12876354157924652
train_iter_loss: 0.2533363997936249
train_iter_loss: 0.1931995153427124
train_iter_loss: 0.12888579070568085
train_iter_loss: 0.20029519498348236
train_iter_loss: 0.19065026938915253
train_iter_loss: 0.11972774565219879
train_iter_loss: 0.21920131146907806
train_iter_loss: 0.14424319565296173
train_iter_loss: 0.245575949549675
train_iter_loss: 0.22066839039325714
train_iter_loss: 0.09772932529449463
train_iter_loss: 0.15550021827220917
train_iter_loss: 0.08665898442268372
train loss :0.1583
---------------------
Validation seg loss: 0.21920764769585627 at epoch 620
epoch =    621/  1000, exp = train
train_iter_loss: 0.11217275261878967
train_iter_loss: 0.15207920968532562
train_iter_loss: 0.11787519603967667
train_iter_loss: 0.15642482042312622
train_iter_loss: 0.1534532606601715
train_iter_loss: 0.15160217881202698
train_iter_loss: 0.06084860861301422
train_iter_loss: 0.25324392318725586
train_iter_loss: 0.10811691731214523
train_iter_loss: 0.19687414169311523
train_iter_loss: 0.06903301179409027
train_iter_loss: 0.1499139964580536
train_iter_loss: 0.10945605486631393
train_iter_loss: 0.08597510308027267
train_iter_loss: 0.17298589646816254
train_iter_loss: 0.31094783544540405
train_iter_loss: 0.13177885115146637
train_iter_loss: 0.026542553678154945
train_iter_loss: 0.22860513627529144
train_iter_loss: 0.07793255150318146
train_iter_loss: 0.2359553873538971
train_iter_loss: 0.12813401222229004
train_iter_loss: 0.09603927284479141
train_iter_loss: 0.1120648980140686
train_iter_loss: 0.1782725602388382
train_iter_loss: 0.1251230090856552
train_iter_loss: 0.21803085505962372
train_iter_loss: 0.17700260877609253
train_iter_loss: 0.2660725712776184
train_iter_loss: 0.14516142010688782
train_iter_loss: 0.1122579425573349
train_iter_loss: 0.2221231460571289
train_iter_loss: 0.1987798810005188
train_iter_loss: 0.07178647071123123
train_iter_loss: 0.13889846205711365
train_iter_loss: 0.1903422474861145
train_iter_loss: 0.1485299915075302
train_iter_loss: 0.17196428775787354
train_iter_loss: 0.2745877504348755
train_iter_loss: 0.2031550258398056
train_iter_loss: 0.11988245695829391
train_iter_loss: 0.25133079290390015
train_iter_loss: 0.11848293244838715
train_iter_loss: 0.1948193460702896
train_iter_loss: 0.12436553835868835
train_iter_loss: 0.24684718251228333
train_iter_loss: 0.230255126953125
train_iter_loss: 0.21885138750076294
train_iter_loss: 0.06556878983974457
train_iter_loss: 0.0855141133069992
train_iter_loss: 0.07015661150217056
train_iter_loss: 0.2850538194179535
train_iter_loss: 0.1955614686012268
train_iter_loss: 0.11627037823200226
train_iter_loss: 0.08896107226610184
train_iter_loss: 0.3255934715270996
train_iter_loss: 0.1568586230278015
train_iter_loss: 0.22704283893108368
train_iter_loss: 0.09754634648561478
train_iter_loss: 0.1982661485671997
train_iter_loss: 0.18443500995635986
train_iter_loss: 0.15578892827033997
train_iter_loss: 0.20281127095222473
train_iter_loss: 0.11068513989448547
train_iter_loss: 0.14054611325263977
train_iter_loss: 0.15874595940113068
train_iter_loss: 0.17729336023330688
train_iter_loss: 0.08452990651130676
train_iter_loss: 0.17129264771938324
train_iter_loss: 0.18932026624679565
train_iter_loss: 0.14197395741939545
train_iter_loss: 0.08328767865896225
train_iter_loss: 0.22864657640457153
train_iter_loss: 0.15096284449100494
train_iter_loss: 0.15330199897289276
train_iter_loss: 0.21904893219470978
train_iter_loss: 0.19675032794475555
train_iter_loss: 0.21763408184051514
train_iter_loss: 0.08394373953342438
train_iter_loss: 0.12613418698310852
train_iter_loss: 0.16749288141727448
train_iter_loss: 0.1598435640335083
train_iter_loss: 0.12922248244285583
train_iter_loss: 0.13905885815620422
train_iter_loss: 0.10893160104751587
train_iter_loss: 0.1401256024837494
train_iter_loss: 0.07105724513530731
train_iter_loss: 0.16318069398403168
train_iter_loss: 0.0428871288895607
train_iter_loss: 0.14356625080108643
train_iter_loss: 0.1587180644273758
train_iter_loss: 0.13232344388961792
train_iter_loss: 0.22066625952720642
train_iter_loss: 0.08413190394639969
train_iter_loss: 0.11482571065425873
train_iter_loss: 0.17946091294288635
train_iter_loss: 0.09393971413373947
train_iter_loss: 0.13985338807106018
train_iter_loss: 0.12900392711162567
train_iter_loss: 0.13616356253623962
train loss :0.1565
---------------------
Validation seg loss: 0.21364107496931023 at epoch 621
epoch =    622/  1000, exp = train
train_iter_loss: 0.19142365455627441
train_iter_loss: 0.18366841971874237
train_iter_loss: 0.043587666004896164
train_iter_loss: 0.15868479013442993
train_iter_loss: 0.08047211915254593
train_iter_loss: 0.2160654366016388
train_iter_loss: 0.11539573967456818
train_iter_loss: 0.09959714859724045
train_iter_loss: 0.17021358013153076
train_iter_loss: 0.23215454816818237
train_iter_loss: 0.21361318230628967
train_iter_loss: 0.11258932948112488
train_iter_loss: 0.19011078774929047
train_iter_loss: 0.14863573014736176
train_iter_loss: 0.22265353798866272
train_iter_loss: 0.18433482944965363
train_iter_loss: 0.23258976638317108
train_iter_loss: 0.10144791752099991
train_iter_loss: 0.15404777228832245
train_iter_loss: 0.11153624206781387
train_iter_loss: 0.13432346284389496
train_iter_loss: 0.15172094106674194
train_iter_loss: 0.07316169887781143
train_iter_loss: 0.16942459344863892
train_iter_loss: 0.12251581996679306
train_iter_loss: 0.054980382323265076
train_iter_loss: 0.28066959977149963
train_iter_loss: 0.10944768041372299
train_iter_loss: 0.09642922878265381
train_iter_loss: 0.2103525698184967
train_iter_loss: 0.13194024562835693
train_iter_loss: 0.2991710603237152
train_iter_loss: 0.31667324900627136
train_iter_loss: 0.228949174284935
train_iter_loss: 0.1033783107995987
train_iter_loss: 0.23454220592975616
train_iter_loss: 0.11085832118988037
train_iter_loss: 0.15024779736995697
train_iter_loss: 0.12414586544036865
train_iter_loss: 0.24501819908618927
train_iter_loss: 0.13053008913993835
train_iter_loss: 0.17028987407684326
train_iter_loss: 0.08449697494506836
train_iter_loss: 0.11181169003248215
train_iter_loss: 0.1928318291902542
train_iter_loss: 0.15930646657943726
train_iter_loss: 0.10083025693893433
train_iter_loss: 0.06667493283748627
train_iter_loss: 0.2479712814092636
train_iter_loss: 0.1355302631855011
train_iter_loss: 0.2676628530025482
train_iter_loss: 0.11087408661842346
train_iter_loss: 0.14045386016368866
train_iter_loss: 0.22177258133888245
train_iter_loss: 0.18127508461475372
train_iter_loss: 0.2881920337677002
train_iter_loss: 0.08499924093484879
train_iter_loss: 0.164907306432724
train_iter_loss: 0.14844483137130737
train_iter_loss: 0.32522818446159363
train_iter_loss: 0.14906981587409973
train_iter_loss: 0.23324409127235413
train_iter_loss: 0.10182342678308487
train_iter_loss: 0.047118186950683594
train_iter_loss: 0.16939352452754974
train_iter_loss: 0.12317356467247009
train_iter_loss: 0.0984712690114975
train_iter_loss: 0.17127563059329987
train_iter_loss: 0.19333887100219727
train_iter_loss: 0.1465771198272705
train_iter_loss: 0.15125638246536255
train_iter_loss: 0.19331927597522736
train_iter_loss: 0.18796797096729279
train_iter_loss: 0.12959256768226624
train_iter_loss: 0.2241768091917038
train_iter_loss: 0.19233791530132294
train_iter_loss: 0.12119263410568237
train_iter_loss: 0.1694609671831131
train_iter_loss: 0.15444648265838623
train_iter_loss: 0.2572329044342041
train_iter_loss: 0.11229320615530014
train_iter_loss: 0.0799027532339096
train_iter_loss: 0.2968176007270813
train_iter_loss: 0.06527573615312576
train_iter_loss: 0.11730553954839706
train_iter_loss: 0.050080932676792145
train_iter_loss: 0.12212817370891571
train_iter_loss: 0.30582717061042786
train_iter_loss: 0.21751368045806885
train_iter_loss: 0.1294483095407486
train_iter_loss: 0.11502474546432495
train_iter_loss: 0.14588823914527893
train_iter_loss: 0.18063955008983612
train_iter_loss: 0.16314934194087982
train_iter_loss: 0.06614472717046738
train_iter_loss: 0.14386387169361115
train_iter_loss: 0.16173990070819855
train_iter_loss: 0.1340629905462265
train_iter_loss: 0.04320717602968216
train_iter_loss: 0.11169455200433731
train loss :0.1598
---------------------
Validation seg loss: 0.21781350540454095 at epoch 622
epoch =    623/  1000, exp = train
train_iter_loss: 0.10133644938468933
train_iter_loss: 0.27250978350639343
train_iter_loss: 0.09355775266885757
train_iter_loss: 0.1973576843738556
train_iter_loss: 0.10385064780712128
train_iter_loss: 0.2911330759525299
train_iter_loss: 0.2737356722354889
train_iter_loss: 0.1419769674539566
train_iter_loss: 0.09336157143115997
train_iter_loss: 0.21221426129341125
train_iter_loss: 0.06276719272136688
train_iter_loss: 0.15532991290092468
train_iter_loss: 0.18096935749053955
train_iter_loss: 0.13042649626731873
train_iter_loss: 0.12893237173557281
train_iter_loss: 0.14785465598106384
train_iter_loss: 0.2391669899225235
train_iter_loss: 0.16695131361484528
train_iter_loss: 0.08221358060836792
train_iter_loss: 0.3111407160758972
train_iter_loss: 0.1442209631204605
train_iter_loss: 0.04630403593182564
train_iter_loss: 0.110176220536232
train_iter_loss: 0.15330767631530762
train_iter_loss: 0.10876712948083878
train_iter_loss: 0.06940770149230957
train_iter_loss: 0.15611054003238678
train_iter_loss: 0.07407521456480026
train_iter_loss: 0.1368701308965683
train_iter_loss: 0.12823154032230377
train_iter_loss: 0.10635509341955185
train_iter_loss: 0.11363529413938522
train_iter_loss: 0.11567602306604385
train_iter_loss: 0.22144363820552826
train_iter_loss: 0.13832668960094452
train_iter_loss: 0.10449770092964172
train_iter_loss: 0.10669878870248795
train_iter_loss: 0.1538989245891571
train_iter_loss: 0.2295588105916977
train_iter_loss: 0.08880775421857834
train_iter_loss: 0.3207800090312958
train_iter_loss: 0.12936024367809296
train_iter_loss: 0.19203045964241028
train_iter_loss: 0.04248037934303284
train_iter_loss: 0.150858074426651
train_iter_loss: 0.0673293024301529
train_iter_loss: 0.13083170354366302
train_iter_loss: 0.15029500424861908
train_iter_loss: 0.17236241698265076
train_iter_loss: 0.24425667524337769
train_iter_loss: 0.2555426061153412
train_iter_loss: 0.09390265494585037
train_iter_loss: 0.1639178842306137
train_iter_loss: 0.07891583442687988
train_iter_loss: 0.15932409465312958
train_iter_loss: 0.09979148954153061
train_iter_loss: 0.10380679368972778
train_iter_loss: 0.1348802000284195
train_iter_loss: 0.14435601234436035
train_iter_loss: 0.19737228751182556
train_iter_loss: 0.37255001068115234
train_iter_loss: 0.20169655978679657
train_iter_loss: 0.2722409665584564
train_iter_loss: 0.260499507188797
train_iter_loss: 0.12956362962722778
train_iter_loss: 0.13883264362812042
train_iter_loss: 0.16257217526435852
train_iter_loss: 0.0974317193031311
train_iter_loss: 0.09061483293771744
train_iter_loss: 0.17176522314548492
train_iter_loss: 0.09509995579719543
train_iter_loss: 0.15684708952903748
train_iter_loss: 0.1826123595237732
train_iter_loss: 0.1639639139175415
train_iter_loss: 0.318631649017334
train_iter_loss: 0.08368445187807083
train_iter_loss: 0.17316199839115143
train_iter_loss: 0.10883613675832748
train_iter_loss: 0.18654954433441162
train_iter_loss: 0.10117053985595703
train_iter_loss: 0.14701150357723236
train_iter_loss: 0.13738584518432617
train_iter_loss: 0.1796530932188034
train_iter_loss: 0.24011671543121338
train_iter_loss: 0.17676055431365967
train_iter_loss: 0.14486385881900787
train_iter_loss: 0.14508923888206482
train_iter_loss: 0.10722527652978897
train_iter_loss: 0.09063427150249481
train_iter_loss: 0.5240659713745117
train_iter_loss: 0.16039788722991943
train_iter_loss: 0.11629015952348709
train_iter_loss: 0.1937631368637085
train_iter_loss: 0.24263891577720642
train_iter_loss: 0.24352297186851501
train_iter_loss: 0.14254158735275269
train_iter_loss: 0.16336441040039062
train_iter_loss: 0.05630170553922653
train_iter_loss: 0.2846951484680176
train_iter_loss: 0.13906000554561615
train loss :0.1619
---------------------
Validation seg loss: 0.21604541963282622 at epoch 623
epoch =    624/  1000, exp = train
train_iter_loss: 0.23409825563430786
train_iter_loss: 0.1236470639705658
train_iter_loss: 0.19180439412593842
train_iter_loss: 0.12951533496379852
train_iter_loss: 0.17909272015094757
train_iter_loss: 0.1429748386144638
train_iter_loss: 0.2323872447013855
train_iter_loss: 0.18888026475906372
train_iter_loss: 0.06442764401435852
train_iter_loss: 0.16802740097045898
train_iter_loss: 0.18375001847743988
train_iter_loss: 0.20084045827388763
train_iter_loss: 0.17279618978500366
train_iter_loss: 0.11535856872797012
train_iter_loss: 0.09856284409761429
train_iter_loss: 0.14744548499584198
train_iter_loss: 0.29536738991737366
train_iter_loss: 0.16554029285907745
train_iter_loss: 0.09425564110279083
train_iter_loss: 0.07601208984851837
train_iter_loss: 0.0669526532292366
train_iter_loss: 0.18125420808792114
train_iter_loss: 0.2677413821220398
train_iter_loss: 0.25144922733306885
train_iter_loss: 0.12179545313119888
train_iter_loss: 0.2487553358078003
train_iter_loss: 0.18343470990657806
train_iter_loss: 0.11449111253023148
train_iter_loss: 0.16117358207702637
train_iter_loss: 0.09841348230838776
train_iter_loss: 0.16991303861141205
train_iter_loss: 0.07151875644922256
train_iter_loss: 0.14885421097278595
train_iter_loss: 0.1505814641714096
train_iter_loss: 0.06943459063768387
train_iter_loss: 0.1506408005952835
train_iter_loss: 0.15016627311706543
train_iter_loss: 0.22764906287193298
train_iter_loss: 0.17614243924617767
train_iter_loss: 0.08921848982572556
train_iter_loss: 0.16564728319644928
train_iter_loss: 0.07806351035833359
train_iter_loss: 0.13681596517562866
train_iter_loss: 0.15836000442504883
train_iter_loss: 0.13808241486549377
train_iter_loss: 0.26648879051208496
train_iter_loss: 0.19092905521392822
train_iter_loss: 0.19365650415420532
train_iter_loss: 0.1584705114364624
train_iter_loss: 0.1965034157037735
train_iter_loss: 0.06061514467000961
train_iter_loss: 0.17789779603481293
train_iter_loss: 0.10925603657960892
train_iter_loss: 0.11592104285955429
train_iter_loss: 0.10103941708803177
train_iter_loss: 0.11622525006532669
train_iter_loss: 0.15744774043560028
train_iter_loss: 0.12665243446826935
train_iter_loss: 0.2312064915895462
train_iter_loss: 0.1346440613269806
train_iter_loss: 0.08048944920301437
train_iter_loss: 0.1354888379573822
train_iter_loss: 0.17584913969039917
train_iter_loss: 0.11014574021100998
train_iter_loss: 0.08914864808320999
train_iter_loss: 0.16705863177776337
train_iter_loss: 0.12313621491193771
train_iter_loss: 0.07426583766937256
train_iter_loss: 0.22780023515224457
train_iter_loss: 0.13796862959861755
train_iter_loss: 0.12160247564315796
train_iter_loss: 0.17717690765857697
train_iter_loss: 0.14358916878700256
train_iter_loss: 0.13050618767738342
train_iter_loss: 0.17078204452991486
train_iter_loss: 0.2394944280385971
train_iter_loss: 0.07823275774717331
train_iter_loss: 0.1918177455663681
train_iter_loss: 0.1758100688457489
train_iter_loss: 0.3531422019004822
train_iter_loss: 0.17573289573192596
train_iter_loss: 0.10005319863557816
train_iter_loss: 0.08540015667676926
train_iter_loss: 0.16957513988018036
train_iter_loss: 0.16156615316867828
train_iter_loss: 0.3256944715976715
train_iter_loss: 0.10420002788305283
train_iter_loss: 0.1694178432226181
train_iter_loss: 0.041917260736227036
train_iter_loss: 0.243667334318161
train_iter_loss: 0.2081986516714096
train_iter_loss: 0.14021803438663483
train_iter_loss: 0.168328657746315
train_iter_loss: 0.235892653465271
train_iter_loss: 0.07876935601234436
train_iter_loss: 0.16945698857307434
train_iter_loss: 0.2945844233036041
train_iter_loss: 0.11551922559738159
train_iter_loss: 0.20686496794223785
train_iter_loss: 0.07318305969238281
train loss :0.1585
---------------------
Validation seg loss: 0.21383483647280987 at epoch 624
epoch =    625/  1000, exp = train
train_iter_loss: 0.19486065208911896
train_iter_loss: 0.1496652215719223
train_iter_loss: 0.13237906992435455
train_iter_loss: 0.1196945533156395
train_iter_loss: 0.2133360505104065
train_iter_loss: 0.1622278392314911
train_iter_loss: 0.14574208855628967
train_iter_loss: 0.18642476201057434
train_iter_loss: 0.16796617209911346
train_iter_loss: 0.23103314638137817
train_iter_loss: 0.1451873928308487
train_iter_loss: 0.08278775215148926
train_iter_loss: 0.05769335851073265
train_iter_loss: 0.11260851472616196
train_iter_loss: 0.1677674800157547
train_iter_loss: 0.06588137149810791
train_iter_loss: 0.12115270644426346
train_iter_loss: 0.19999179244041443
train_iter_loss: 0.22290174663066864
train_iter_loss: 0.20689578354358673
train_iter_loss: 0.164154514670372
train_iter_loss: 0.13723786175251007
train_iter_loss: 0.06409299373626709
train_iter_loss: 0.2045394480228424
train_iter_loss: 0.2922394871711731
train_iter_loss: 0.10450582206249237
train_iter_loss: 0.21146011352539062
train_iter_loss: 0.055219411849975586
train_iter_loss: 0.21447788178920746
train_iter_loss: 0.11395400762557983
train_iter_loss: 0.12296120822429657
train_iter_loss: 0.15296508371829987
train_iter_loss: 0.1018669530749321
train_iter_loss: 0.19347086548805237
train_iter_loss: 0.14971254765987396
train_iter_loss: 0.04830007627606392
train_iter_loss: 0.31752490997314453
train_iter_loss: 0.09734275192022324
train_iter_loss: 0.060329630970954895
train_iter_loss: 0.30894798040390015
train_iter_loss: 0.14786040782928467
train_iter_loss: 0.17495666444301605
train_iter_loss: 0.1274907886981964
train_iter_loss: 0.1496008038520813
train_iter_loss: 0.11841980367898941
train_iter_loss: 0.22277472913265228
train_iter_loss: 0.14701329171657562
train_iter_loss: 0.1683700531721115
train_iter_loss: 0.28391751646995544
train_iter_loss: 0.13963982462882996
train_iter_loss: 0.19008569419384003
train_iter_loss: 0.09432496875524521
train_iter_loss: 0.15665966272354126
train_iter_loss: 0.12003804743289948
train_iter_loss: 0.09897637367248535
train_iter_loss: 0.1973414123058319
train_iter_loss: 0.1740148365497589
train_iter_loss: 0.17046213150024414
train_iter_loss: 0.1552237570285797
train_iter_loss: 0.056086450815200806
train_iter_loss: 0.159270241856575
train_iter_loss: 0.20433597266674042
train_iter_loss: 0.15405431389808655
train_iter_loss: 0.09655942022800446
train_iter_loss: 0.3226768672466278
train_iter_loss: 0.1803019791841507
train_iter_loss: 0.13209925591945648
train_iter_loss: 0.07590731978416443
train_iter_loss: 0.21690312027931213
train_iter_loss: 0.15913982689380646
train_iter_loss: 0.04432421550154686
train_iter_loss: 0.15345454216003418
train_iter_loss: 0.17792361974716187
train_iter_loss: 0.19332996010780334
train_iter_loss: 0.08430688828229904
train_iter_loss: 0.31039661169052124
train_iter_loss: 0.10798561573028564
train_iter_loss: 0.08773891627788544
train_iter_loss: 0.08508206158876419
train_iter_loss: 0.1623586118221283
train_iter_loss: 0.20178766548633575
train_iter_loss: 0.1782686561346054
train_iter_loss: 0.18986660242080688
train_iter_loss: 0.21514073014259338
train_iter_loss: 0.14759129285812378
train_iter_loss: 0.12313438206911087
train_iter_loss: 0.16654783487319946
train_iter_loss: 0.1407133787870407
train_iter_loss: 0.09886987507343292
train_iter_loss: 0.2838299572467804
train_iter_loss: 0.05716642737388611
train_iter_loss: 0.1386755108833313
train_iter_loss: 0.1844116598367691
train_iter_loss: 0.15229959785938263
train_iter_loss: 0.19601963460445404
train_iter_loss: 0.20755834877490997
train_iter_loss: 0.1393330991268158
train_iter_loss: 0.15836544334888458
train_iter_loss: 0.1220385953783989
train_iter_loss: 0.2664794325828552
train loss :0.1593
---------------------
Validation seg loss: 0.21765646476404005 at epoch 625
epoch =    626/  1000, exp = train
train_iter_loss: 0.1683214157819748
train_iter_loss: 0.10469041019678116
train_iter_loss: 0.24075616896152496
train_iter_loss: 0.08722862601280212
train_iter_loss: 0.15190598368644714
train_iter_loss: 0.12810342013835907
train_iter_loss: 0.14290691912174225
train_iter_loss: 0.14701056480407715
train_iter_loss: 0.20312368869781494
train_iter_loss: 0.12600640952587128
train_iter_loss: 0.12265051901340485
train_iter_loss: 0.1170385405421257
train_iter_loss: 0.3231019675731659
train_iter_loss: 0.3463131785392761
train_iter_loss: 0.12851616740226746
train_iter_loss: 0.15388867259025574
train_iter_loss: 0.1829373687505722
train_iter_loss: 0.2028900682926178
train_iter_loss: 0.2263006865978241
train_iter_loss: 0.11167078465223312
train_iter_loss: 0.08856132626533508
train_iter_loss: 0.1072065681219101
train_iter_loss: 0.1602189838886261
train_iter_loss: 0.281625896692276
train_iter_loss: 0.07868780940771103
train_iter_loss: 0.31194767355918884
train_iter_loss: 0.09408827126026154
train_iter_loss: 0.20638985931873322
train_iter_loss: 0.1337393969297409
train_iter_loss: 0.18776656687259674
train_iter_loss: 0.10194656997919083
train_iter_loss: 0.13000589609146118
train_iter_loss: 0.11660613119602203
train_iter_loss: 0.13215482234954834
train_iter_loss: 0.18438749015331268
train_iter_loss: 0.24584339559078217
train_iter_loss: 0.29661795496940613
train_iter_loss: 0.13147157430648804
train_iter_loss: 0.16930526494979858
train_iter_loss: 0.07448917627334595
train_iter_loss: 0.10035397112369537
train_iter_loss: 0.10328080505132675
train_iter_loss: 0.10876813530921936
train_iter_loss: 0.11790905892848969
train_iter_loss: 0.14066541194915771
train_iter_loss: 0.18026745319366455
train_iter_loss: 0.09900294244289398
train_iter_loss: 0.32559260725975037
train_iter_loss: 0.16116194427013397
train_iter_loss: 0.34579193592071533
train_iter_loss: 0.10358845442533493
train_iter_loss: 0.20952408015727997
train_iter_loss: 0.11385230720043182
train_iter_loss: 0.16323277354240417
train_iter_loss: 0.16094300150871277
train_iter_loss: 0.17195305228233337
train_iter_loss: 0.1949722319841385
train_iter_loss: 0.20925785601139069
train_iter_loss: 0.042351916432380676
train_iter_loss: 0.12068074196577072
train_iter_loss: 0.19960211217403412
train_iter_loss: 0.11605584621429443
train_iter_loss: 0.16026104986667633
train_iter_loss: 0.17782224714756012
train_iter_loss: 0.24681031703948975
train_iter_loss: 0.08908823132514954
train_iter_loss: 0.056984882801771164
train_iter_loss: 0.1804560422897339
train_iter_loss: 0.0847049131989479
train_iter_loss: 0.1521519422531128
train_iter_loss: 0.12783828377723694
train_iter_loss: 0.1529517024755478
train_iter_loss: 0.15442612767219543
train_iter_loss: 0.06400203704833984
train_iter_loss: 0.11447439342737198
train_iter_loss: 0.14557749032974243
train_iter_loss: 0.23447933793067932
train_iter_loss: 0.23617544770240784
train_iter_loss: 0.31810909509658813
train_iter_loss: 0.13385042548179626
train_iter_loss: 0.10119928419589996
train_iter_loss: 0.098822221159935
train_iter_loss: 0.10790730267763138
train_iter_loss: 0.17220985889434814
train_iter_loss: 0.146597757935524
train_iter_loss: 0.1833646297454834
train_iter_loss: 0.17658324539661407
train_iter_loss: 0.11814181506633759
train_iter_loss: 0.10244148969650269
train_iter_loss: 0.12997421622276306
train_iter_loss: 0.08915548026561737
train_iter_loss: 0.1993958055973053
train_iter_loss: 0.18707668781280518
train_iter_loss: 0.09704779088497162
train_iter_loss: 0.22142815589904785
train_iter_loss: 0.11826979368925095
train_iter_loss: 0.13571272790431976
train_iter_loss: 0.08242830634117126
train_iter_loss: 0.14417380094528198
train_iter_loss: 0.10993922501802444
train loss :0.1585
---------------------
Validation seg loss: 0.2142733619636241 at epoch 626
epoch =    627/  1000, exp = train
train_iter_loss: 0.0693037137389183
train_iter_loss: 0.07886717468500137
train_iter_loss: 0.2133684754371643
train_iter_loss: 0.07875558733940125
train_iter_loss: 0.2790337800979614
train_iter_loss: 0.0324675552546978
train_iter_loss: 0.13929887115955353
train_iter_loss: 0.16352559626102448
train_iter_loss: 0.17505638301372528
train_iter_loss: 0.07901595532894135
train_iter_loss: 0.148667573928833
train_iter_loss: 0.14665420353412628
train_iter_loss: 0.0832693874835968
train_iter_loss: 0.1267061084508896
train_iter_loss: 0.09773436188697815
train_iter_loss: 0.14298570156097412
train_iter_loss: 0.28578031063079834
train_iter_loss: 0.11890353262424469
train_iter_loss: 0.16522814333438873
train_iter_loss: 0.09948475658893585
train_iter_loss: 0.3586251735687256
train_iter_loss: 0.13280290365219116
train_iter_loss: 0.1249239593744278
train_iter_loss: 0.15520089864730835
train_iter_loss: 0.14414465427398682
train_iter_loss: 0.18234314024448395
train_iter_loss: 0.311046838760376
train_iter_loss: 0.18394018709659576
train_iter_loss: 0.1161917969584465
train_iter_loss: 0.08338741213083267
train_iter_loss: 0.12079542875289917
train_iter_loss: 0.1384873390197754
train_iter_loss: 0.16003598272800446
train_iter_loss: 0.11844754219055176
train_iter_loss: 0.12727221846580505
train_iter_loss: 0.14102818071842194
train_iter_loss: 0.10378698259592056
train_iter_loss: 0.14303629100322723
train_iter_loss: 0.17174744606018066
train_iter_loss: 0.17222952842712402
train_iter_loss: 0.17248398065567017
train_iter_loss: 0.23722127079963684
train_iter_loss: 0.18714822828769684
train_iter_loss: 0.19803963601589203
train_iter_loss: 0.18519870936870575
train_iter_loss: 0.07854675501585007
train_iter_loss: 0.19880616664886475
train_iter_loss: 0.18295370042324066
train_iter_loss: 0.14757874608039856
train_iter_loss: 0.1383712738752365
train_iter_loss: 0.09939958900213242
train_iter_loss: 0.21950271725654602
train_iter_loss: 0.34489428997039795
train_iter_loss: 0.14336161315441132
train_iter_loss: 0.26659491658210754
train_iter_loss: 0.11870655417442322
train_iter_loss: 0.08926108479499817
train_iter_loss: 0.1516064703464508
train_iter_loss: 0.1697193831205368
train_iter_loss: 0.07483164221048355
train_iter_loss: 0.16096818447113037
train_iter_loss: 0.15323901176452637
train_iter_loss: 0.09777775406837463
train_iter_loss: 0.09091849625110626
train_iter_loss: 0.13928328454494476
train_iter_loss: 0.10084965825080872
train_iter_loss: 0.21620813012123108
train_iter_loss: 0.20493705570697784
train_iter_loss: 0.07187364995479584
train_iter_loss: 0.2102845013141632
train_iter_loss: 0.12789969146251678
train_iter_loss: 0.21923166513442993
train_iter_loss: 0.1136128380894661
train_iter_loss: 0.19086988270282745
train_iter_loss: 0.16616123914718628
train_iter_loss: 0.23043254017829895
train_iter_loss: 0.11171973496675491
train_iter_loss: 0.2204839140176773
train_iter_loss: 0.17000757157802582
train_iter_loss: 0.11209195852279663
train_iter_loss: 0.1429390013217926
train_iter_loss: 0.11036475747823715
train_iter_loss: 0.0751490369439125
train_iter_loss: 0.13044142723083496
train_iter_loss: 0.17245489358901978
train_iter_loss: 0.24711591005325317
train_iter_loss: 0.08078048378229141
train_iter_loss: 0.15256085991859436
train_iter_loss: 0.13968661427497864
train_iter_loss: 0.19085046648979187
train_iter_loss: 0.28272342681884766
train_iter_loss: 0.061379265040159225
train_iter_loss: 0.08423729240894318
train_iter_loss: 0.23878882825374603
train_iter_loss: 0.2082241028547287
train_iter_loss: 0.22436656057834625
train_iter_loss: 0.29308411478996277
train_iter_loss: 0.2447051703929901
train_iter_loss: 0.11643846333026886
train_iter_loss: 0.19658918678760529
train loss :0.1598
---------------------
Validation seg loss: 0.21549016419130396 at epoch 627
epoch =    628/  1000, exp = train
train_iter_loss: 0.2867514491081238
train_iter_loss: 0.14613030850887299
train_iter_loss: 0.3267967402935028
train_iter_loss: 0.12184669822454453
train_iter_loss: 0.18535657227039337
train_iter_loss: 0.10248998552560806
train_iter_loss: 0.10038730502128601
train_iter_loss: 0.10135024040937424
train_iter_loss: 0.217531219124794
train_iter_loss: 0.15173040330410004
train_iter_loss: 0.10788575559854507
train_iter_loss: 0.2097131758928299
train_iter_loss: 0.21330484747886658
train_iter_loss: 0.20719502866268158
train_iter_loss: 0.05034330114722252
train_iter_loss: 0.15746371448040009
train_iter_loss: 0.21480688452720642
train_iter_loss: 0.027997829020023346
train_iter_loss: 0.2035556584596634
train_iter_loss: 0.2019975483417511
train_iter_loss: 0.14239636063575745
train_iter_loss: 0.1584080308675766
train_iter_loss: 0.18851269781589508
train_iter_loss: 0.18976014852523804
train_iter_loss: 0.26567476987838745
train_iter_loss: 0.17072448134422302
train_iter_loss: 0.09760604053735733
train_iter_loss: 0.22562246024608612
train_iter_loss: 0.22251088917255402
train_iter_loss: 0.06716597080230713
train_iter_loss: 0.11751341819763184
train_iter_loss: 0.07964350283145905
train_iter_loss: 0.21916578710079193
train_iter_loss: 0.13736264407634735
train_iter_loss: 0.09524547308683395
train_iter_loss: 0.12239973247051239
train_iter_loss: 0.11663766950368881
train_iter_loss: 0.2975505292415619
train_iter_loss: 0.24288344383239746
train_iter_loss: 0.10247579962015152
train_iter_loss: 0.3313082456588745
train_iter_loss: 0.31251344084739685
train_iter_loss: 0.17891252040863037
train_iter_loss: 0.28733110427856445
train_iter_loss: 0.12853257358074188
train_iter_loss: 0.084328293800354
train_iter_loss: 0.24698445200920105
train_iter_loss: 0.0928594246506691
train_iter_loss: 0.07759272307157516
train_iter_loss: 0.2629169523715973
train_iter_loss: 0.12887990474700928
train_iter_loss: 0.17288269102573395
train_iter_loss: 0.15791739523410797
train_iter_loss: 0.11190303415060043
train_iter_loss: 0.08443871140480042
train_iter_loss: 0.034879572689533234
train_iter_loss: 0.13625547289848328
train_iter_loss: 0.16258671879768372
train_iter_loss: 0.12956519424915314
train_iter_loss: 0.24572324752807617
train_iter_loss: 0.2098091095685959
train_iter_loss: 0.05876416712999344
train_iter_loss: 0.12636113166809082
train_iter_loss: 0.03711043670773506
train_iter_loss: 0.16046401858329773
train_iter_loss: 0.14307798445224762
train_iter_loss: 0.21473774313926697
train_iter_loss: 0.22444899380207062
train_iter_loss: 0.12893429398536682
train_iter_loss: 0.08762044459581375
train_iter_loss: 0.10866594314575195
train_iter_loss: 0.19425444304943085
train_iter_loss: 0.25941744446754456
train_iter_loss: 0.13917168974876404
train_iter_loss: 0.178262397646904
train_iter_loss: 0.10535167902708054
train_iter_loss: 0.0942937359213829
train_iter_loss: 0.17294879257678986
train_iter_loss: 0.16364993155002594
train_iter_loss: 0.1355668157339096
train_iter_loss: 0.23728100955486298
train_iter_loss: 0.0732928141951561
train_iter_loss: 0.17236794531345367
train_iter_loss: 0.14419160783290863
train_iter_loss: 0.1631137728691101
train_iter_loss: 0.07207247614860535
train_iter_loss: 0.16681937873363495
train_iter_loss: 0.13988392055034637
train_iter_loss: 0.125421404838562
train_iter_loss: 0.14858901500701904
train_iter_loss: 0.11469550430774689
train_iter_loss: 0.18423080444335938
train_iter_loss: 0.14107777178287506
train_iter_loss: 0.1003117635846138
train_iter_loss: 0.16624699532985687
train_iter_loss: 0.15550817549228668
train_iter_loss: 0.22570772469043732
train_iter_loss: 0.22725693881511688
train_iter_loss: 0.28073781728744507
train_iter_loss: 0.13677561283111572
train loss :0.1624
---------------------
Validation seg loss: 0.21227864648725064 at epoch 628
epoch =    629/  1000, exp = train
train_iter_loss: 0.6101586818695068
train_iter_loss: 0.16387365758419037
train_iter_loss: 0.10999585688114166
train_iter_loss: 0.30159929394721985
train_iter_loss: 0.08106030523777008
train_iter_loss: 0.15405507385730743
train_iter_loss: 0.17919309437274933
train_iter_loss: 0.2347014844417572
train_iter_loss: 0.21357202529907227
train_iter_loss: 0.1209922581911087
train_iter_loss: 0.14044056832790375
train_iter_loss: 0.12347406893968582
train_iter_loss: 0.21332377195358276
train_iter_loss: 0.039333026856184006
train_iter_loss: 0.164994478225708
train_iter_loss: 0.10007975995540619
train_iter_loss: 0.15209552645683289
train_iter_loss: 0.06426342576742172
train_iter_loss: 0.23927201330661774
train_iter_loss: 0.15017691254615784
train_iter_loss: 0.17174482345581055
train_iter_loss: 0.12923197448253632
train_iter_loss: 0.0803208276629448
train_iter_loss: 0.2814315855503082
train_iter_loss: 0.1979535073041916
train_iter_loss: 0.11270999163389206
train_iter_loss: 0.2343306541442871
train_iter_loss: 0.18252608180046082
train_iter_loss: 0.21153628826141357
train_iter_loss: 0.24179662764072418
train_iter_loss: 0.14831528067588806
train_iter_loss: 0.11858157068490982
train_iter_loss: 0.19807344675064087
train_iter_loss: 0.278876930475235
train_iter_loss: 0.06990071386098862
train_iter_loss: 0.08577551692724228
train_iter_loss: 0.16793213784694672
train_iter_loss: 0.10835321247577667
train_iter_loss: 0.19261226058006287
train_iter_loss: 0.16335293650627136
train_iter_loss: 0.06672035157680511
train_iter_loss: 0.11099980026483536
train_iter_loss: 0.16497230529785156
train_iter_loss: 0.0884096771478653
train_iter_loss: 0.1395365595817566
train_iter_loss: 0.06924499571323395
train_iter_loss: 0.1072884202003479
train_iter_loss: 0.1503521203994751
train_iter_loss: 0.10652864724397659
train_iter_loss: 0.25760507583618164
train_iter_loss: 0.07631366699934006
train_iter_loss: 0.1939963549375534
train_iter_loss: 0.30346816778182983
train_iter_loss: 0.1214543953537941
train_iter_loss: 0.20755700767040253
train_iter_loss: 0.180711030960083
train_iter_loss: 0.2628127634525299
train_iter_loss: 0.1458650827407837
train_iter_loss: 0.22547867894172668
train_iter_loss: 0.1551467627286911
train_iter_loss: 0.13697408139705658
train_iter_loss: 0.06562569737434387
train_iter_loss: 0.21184754371643066
train_iter_loss: 0.14652658998966217
train_iter_loss: 0.25694015622138977
train_iter_loss: 0.10582628101110458
train_iter_loss: 0.06693019717931747
train_iter_loss: 0.1376810073852539
train_iter_loss: 0.17143480479717255
train_iter_loss: 0.11753915995359421
train_iter_loss: 0.23059266805648804
train_iter_loss: 0.1648647040128708
train_iter_loss: 0.199320986866951
train_iter_loss: 0.02227775938808918
train_iter_loss: 0.2840932011604309
train_iter_loss: 0.14233657717704773
train_iter_loss: 0.14157603681087494
train_iter_loss: 0.15264253318309784
train_iter_loss: 0.11046844720840454
train_iter_loss: 0.1951545774936676
train_iter_loss: 0.1520501971244812
train_iter_loss: 0.1301034688949585
train_iter_loss: 0.1464451253414154
train_iter_loss: 0.15047305822372437
train_iter_loss: 0.13360540568828583
train_iter_loss: 0.12883375585079193
train_iter_loss: 0.14255358278751373
train_iter_loss: 0.043465666472911835
train_iter_loss: 0.10608214884996414
train_iter_loss: 0.07257973402738571
train_iter_loss: 0.31116408109664917
train_iter_loss: 0.10820285975933075
train_iter_loss: 0.1299663782119751
train_iter_loss: 0.2425171583890915
train_iter_loss: 0.1253863275051117
train_iter_loss: 0.10315696895122528
train_iter_loss: 0.07494768500328064
train_iter_loss: 0.10624740272760391
train_iter_loss: 0.22981464862823486
train_iter_loss: 0.13009266555309296
train loss :0.1606
---------------------
Validation seg loss: 0.21425852665396514 at epoch 629
epoch =    630/  1000, exp = train
train_iter_loss: 0.0921986922621727
train_iter_loss: 0.1289954036474228
train_iter_loss: 0.1965213418006897
train_iter_loss: 0.32208433747291565
train_iter_loss: 0.3417448103427887
train_iter_loss: 0.34374547004699707
train_iter_loss: 0.18359266221523285
train_iter_loss: 0.11618329584598541
train_iter_loss: 0.08429364860057831
train_iter_loss: 0.17564885318279266
train_iter_loss: 0.2098889946937561
train_iter_loss: 0.14042949676513672
train_iter_loss: 0.1065632700920105
train_iter_loss: 0.1551976203918457
train_iter_loss: 0.17852076888084412
train_iter_loss: 0.2254771590232849
train_iter_loss: 0.08140300959348679
train_iter_loss: 0.15994535386562347
train_iter_loss: 0.10307548195123672
train_iter_loss: 0.20627890527248383
train_iter_loss: 0.18715454638004303
train_iter_loss: 0.17208018898963928
train_iter_loss: 0.14598166942596436
train_iter_loss: 0.11318744719028473
train_iter_loss: 0.17330463230609894
train_iter_loss: 0.22113236784934998
train_iter_loss: 0.12947876751422882
train_iter_loss: 0.1528082638978958
train_iter_loss: 0.14268963038921356
train_iter_loss: 0.13526205718517303
train_iter_loss: 0.28288888931274414
train_iter_loss: 0.10288510471582413
train_iter_loss: 0.1734437495470047
train_iter_loss: 0.15591715276241302
train_iter_loss: 0.14821089804172516
train_iter_loss: 0.2975523769855499
train_iter_loss: 0.16253136098384857
train_iter_loss: 0.13213372230529785
train_iter_loss: 0.21662050485610962
train_iter_loss: 0.12136811763048172
train_iter_loss: 0.21854837238788605
train_iter_loss: 0.14415998756885529
train_iter_loss: 0.1663321554660797
train_iter_loss: 0.10593780875205994
train_iter_loss: 0.10420875251293182
train_iter_loss: 0.12776999175548553
train_iter_loss: 0.2381899505853653
train_iter_loss: 0.21755272150039673
train_iter_loss: 0.23320318758487701
train_iter_loss: 0.13592961430549622
train_iter_loss: 0.19746406376361847
train_iter_loss: 0.08467787504196167
train_iter_loss: 0.18651556968688965
train_iter_loss: 0.10357871651649475
train_iter_loss: 0.15675471723079681
train_iter_loss: 0.17972615361213684
train_iter_loss: 0.05545298755168915
train_iter_loss: 0.08097481727600098
train_iter_loss: 0.1637149155139923
train_iter_loss: 0.2009539008140564
train_iter_loss: 0.09512263536453247
train_iter_loss: 0.13355115056037903
train_iter_loss: 0.23229162395000458
train_iter_loss: 0.13435862958431244
train_iter_loss: 0.13790196180343628
train_iter_loss: 0.16790162026882172
train_iter_loss: 0.11835012584924698
train_iter_loss: 0.11412134766578674
train_iter_loss: 0.2776143550872803
train_iter_loss: 0.16761377453804016
train_iter_loss: 0.17351409792900085
train_iter_loss: 0.1499713808298111
train_iter_loss: 0.07109685987234116
train_iter_loss: 0.22775797545909882
train_iter_loss: 0.2703797519207001
train_iter_loss: 0.11623670905828476
train_iter_loss: 0.14779259264469147
train_iter_loss: 0.05571966618299484
train_iter_loss: 0.1524089127779007
train_iter_loss: 0.09008848667144775
train_iter_loss: 0.10000861436128616
train_iter_loss: 0.06788169592618942
train_iter_loss: 0.1361546814441681
train_iter_loss: 0.12901468575000763
train_iter_loss: 0.1782366931438446
train_iter_loss: 0.14278413355350494
train_iter_loss: 0.23732444643974304
train_iter_loss: 0.15838207304477692
train_iter_loss: 0.1678052395582199
train_iter_loss: 0.1313130259513855
train_iter_loss: 0.13711430132389069
train_iter_loss: 0.23565006256103516
train_iter_loss: 0.1024521142244339
train_iter_loss: 0.12129023671150208
train_iter_loss: 0.2990618050098419
train_iter_loss: 0.21443410217761993
train_iter_loss: 0.1323041319847107
train_iter_loss: 0.07972302287817001
train_iter_loss: 0.06851113587617874
train_iter_loss: 0.04094363749027252
train loss :0.1609
---------------------
Validation seg loss: 0.22097085381172737 at epoch 630
epoch =    631/  1000, exp = train
train_iter_loss: 0.17191141843795776
train_iter_loss: 0.12915579974651337
train_iter_loss: 0.1999257653951645
train_iter_loss: 0.1423165500164032
train_iter_loss: 0.16453486680984497
train_iter_loss: 0.19706109166145325
train_iter_loss: 0.16848105192184448
train_iter_loss: 0.1755656898021698
train_iter_loss: 0.1140117347240448
train_iter_loss: 0.14259155094623566
train_iter_loss: 0.07048507034778595
train_iter_loss: 0.10605766624212265
train_iter_loss: 0.17851968109607697
train_iter_loss: 0.07048109918832779
train_iter_loss: 0.1319611519575119
train_iter_loss: 0.1480843722820282
train_iter_loss: 0.09558628499507904
train_iter_loss: 0.1507192999124527
train_iter_loss: 0.10339643806219101
train_iter_loss: 0.04441099241375923
train_iter_loss: 0.08937988430261612
train_iter_loss: 0.08236038684844971
train_iter_loss: 0.21264977753162384
train_iter_loss: 0.11820638179779053
train_iter_loss: 0.18107739090919495
train_iter_loss: 0.13562914729118347
train_iter_loss: 0.15845473110675812
train_iter_loss: 0.1720643937587738
train_iter_loss: 0.25820934772491455
train_iter_loss: 0.09529624134302139
train_iter_loss: 0.08588594943284988
train_iter_loss: 0.19219006597995758
train_iter_loss: 0.17349588871002197
train_iter_loss: 0.06851500272750854
train_iter_loss: 0.11486713588237762
train_iter_loss: 0.059703532606363297
train_iter_loss: 0.1406969577074051
train_iter_loss: 0.03772570565342903
train_iter_loss: 0.28491348028182983
train_iter_loss: 0.1625768393278122
train_iter_loss: 0.071140356361866
train_iter_loss: 0.2586177587509155
train_iter_loss: 0.1823464334011078
train_iter_loss: 0.20310992002487183
train_iter_loss: 0.14139007031917572
train_iter_loss: 0.23594167828559875
train_iter_loss: 0.10295484215021133
train_iter_loss: 0.20395508408546448
train_iter_loss: 0.11553993076086044
train_iter_loss: 0.13287757337093353
train_iter_loss: 0.1765739470720291
train_iter_loss: 0.24605776369571686
train_iter_loss: 0.057667240500450134
train_iter_loss: 0.12232350558042526
train_iter_loss: 0.17012949287891388
train_iter_loss: 0.12714017927646637
train_iter_loss: 0.11132551729679108
train_iter_loss: 0.24268479645252228
train_iter_loss: 0.14277015626430511
train_iter_loss: 0.14556504786014557
train_iter_loss: 0.17264391481876373
train_iter_loss: 0.2122928947210312
train_iter_loss: 0.11910337209701538
train_iter_loss: 0.09653005003929138
train_iter_loss: 0.2707102596759796
train_iter_loss: 0.19513171911239624
train_iter_loss: 0.1305232048034668
train_iter_loss: 0.1566384881734848
train_iter_loss: 0.17253394424915314
train_iter_loss: 0.1428402215242386
train_iter_loss: 0.1418977975845337
train_iter_loss: 0.10561181604862213
train_iter_loss: 0.1336110681295395
train_iter_loss: 0.27252814173698425
train_iter_loss: 0.44507724046707153
train_iter_loss: 0.27382123470306396
train_iter_loss: 0.11462216079235077
train_iter_loss: 0.23096315562725067
train_iter_loss: 0.15029841661453247
train_iter_loss: 0.19862157106399536
train_iter_loss: 0.09882603585720062
train_iter_loss: 0.1362081617116928
train_iter_loss: 0.12548305094242096
train_iter_loss: 0.1623660922050476
train_iter_loss: 0.1676570326089859
train_iter_loss: 0.1980927288532257
train_iter_loss: 0.05721501260995865
train_iter_loss: 0.14905843138694763
train_iter_loss: 0.22183740139007568
train_iter_loss: 0.15266793966293335
train_iter_loss: 0.22515523433685303
train_iter_loss: 0.22303543984889984
train_iter_loss: 0.1685425490140915
train_iter_loss: 0.17452193796634674
train_iter_loss: 0.10946333408355713
train_iter_loss: 0.10854564607143402
train_iter_loss: 0.1880013644695282
train_iter_loss: 0.13949193060398102
train_iter_loss: 0.1774231195449829
train_iter_loss: 0.10947905480861664
train loss :0.1571
---------------------
Validation seg loss: 0.21652591340068095 at epoch 631
epoch =    632/  1000, exp = train
train_iter_loss: 0.24822582304477692
train_iter_loss: 0.08268023282289505
train_iter_loss: 0.051624100655317307
train_iter_loss: 0.15551228821277618
train_iter_loss: 0.0393545888364315
train_iter_loss: 0.14948713779449463
train_iter_loss: 0.1195092648267746
train_iter_loss: 0.1368904709815979
train_iter_loss: 0.2080691009759903
train_iter_loss: 0.20330196619033813
train_iter_loss: 0.066079281270504
train_iter_loss: 0.20860795676708221
train_iter_loss: 0.19292129576206207
train_iter_loss: 0.18854160606861115
train_iter_loss: 0.09275460243225098
train_iter_loss: 0.17179661989212036
train_iter_loss: 0.08823307603597641
train_iter_loss: 0.09550532698631287
train_iter_loss: 0.08011244982481003
train_iter_loss: 0.13870234787464142
train_iter_loss: 0.20349039137363434
train_iter_loss: 0.10040238499641418
train_iter_loss: 0.08793817460536957
train_iter_loss: 0.15970169007778168
train_iter_loss: 0.10014106333255768
train_iter_loss: 0.035817693918943405
train_iter_loss: 0.12135310471057892
train_iter_loss: 0.2662164568901062
train_iter_loss: 0.3329680562019348
train_iter_loss: 0.16489827632904053
train_iter_loss: 0.13880927860736847
train_iter_loss: 0.3976060748100281
train_iter_loss: 0.08937527239322662
train_iter_loss: 0.12446217238903046
train_iter_loss: 0.2447407841682434
train_iter_loss: 0.1646120697259903
train_iter_loss: 0.12232624739408493
train_iter_loss: 0.18481431901454926
train_iter_loss: 0.08555025607347488
train_iter_loss: 0.1896265745162964
train_iter_loss: 0.18663440644741058
train_iter_loss: 0.20278562605381012
train_iter_loss: 0.14284585416316986
train_iter_loss: 0.29387930035591125
train_iter_loss: 0.11555251479148865
train_iter_loss: 0.11703138053417206
train_iter_loss: 0.14920735359191895
train_iter_loss: 0.19598281383514404
train_iter_loss: 0.21377968788146973
train_iter_loss: 0.13368993997573853
train_iter_loss: 0.23354341089725494
train_iter_loss: 0.1946934163570404
train_iter_loss: 0.15383483469486237
train_iter_loss: 0.17144261300563812
train_iter_loss: 0.17111951112747192
train_iter_loss: 0.10738174617290497
train_iter_loss: 0.3537159264087677
train_iter_loss: 0.22114227712154388
train_iter_loss: 0.20896276831626892
train_iter_loss: 0.18507696688175201
train_iter_loss: 0.2388835996389389
train_iter_loss: 0.36951398849487305
train_iter_loss: 0.1978803128004074
train_iter_loss: 0.22337846457958221
train_iter_loss: 0.1369154155254364
train_iter_loss: 0.25867733359336853
train_iter_loss: 0.1474912017583847
train_iter_loss: 0.15106017887592316
train_iter_loss: 0.1471637636423111
train_iter_loss: 0.21152934432029724
train_iter_loss: 0.05737794190645218
train_iter_loss: 0.24457423388957977
train_iter_loss: 0.19364506006240845
train_iter_loss: 0.34777894616127014
train_iter_loss: 0.1540781855583191
train_iter_loss: 0.06608040630817413
train_iter_loss: 0.08196862041950226
train_iter_loss: 0.14448674023151398
train_iter_loss: 0.12446964532136917
train_iter_loss: 0.05832010507583618
train_iter_loss: 0.12298855930566788
train_iter_loss: 0.22066405415534973
train_iter_loss: 0.11544166505336761
train_iter_loss: 0.15368521213531494
train_iter_loss: 0.10539508610963821
train_iter_loss: 0.1507110446691513
train_iter_loss: 0.21813829243183136
train_iter_loss: 0.19976100325584412
train_iter_loss: 0.1484874188899994
train_iter_loss: 0.16918401420116425
train_iter_loss: 0.3746151030063629
train_iter_loss: 0.25528207421302795
train_iter_loss: 0.059671852737665176
train_iter_loss: 0.12507231533527374
train_iter_loss: 0.10458297282457352
train_iter_loss: 0.15467090904712677
train_iter_loss: 0.15751853585243225
train_iter_loss: 0.1505972295999527
train_iter_loss: 0.07462809234857559
train_iter_loss: 0.23134881258010864
train loss :0.1679
---------------------
Validation seg loss: 0.215871796187646 at epoch 632
epoch =    633/  1000, exp = train
train_iter_loss: 0.2271571010351181
train_iter_loss: 0.19746339321136475
train_iter_loss: 0.1334671974182129
train_iter_loss: 0.22615964710712433
train_iter_loss: 0.15410903096199036
train_iter_loss: 0.18459440767765045
train_iter_loss: 0.09434164315462112
train_iter_loss: 0.21706126630306244
train_iter_loss: 0.21094338595867157
train_iter_loss: 0.10847237706184387
train_iter_loss: 0.1798458695411682
train_iter_loss: 0.1208098977804184
train_iter_loss: 0.12479494512081146
train_iter_loss: 0.29336854815483093
train_iter_loss: 0.09103396534919739
train_iter_loss: 0.2218099981546402
train_iter_loss: 0.1765465885400772
train_iter_loss: 0.06888718903064728
train_iter_loss: 0.1017453595995903
train_iter_loss: 0.09149137884378433
train_iter_loss: 0.19872114062309265
train_iter_loss: 0.11660986393690109
train_iter_loss: 0.15006054937839508
train_iter_loss: 0.14374567568302155
train_iter_loss: 0.1572437584400177
train_iter_loss: 0.13065467774868011
train_iter_loss: 0.10957394540309906
train_iter_loss: 0.09424147009849548
train_iter_loss: 0.2694460451602936
train_iter_loss: 0.18619361519813538
train_iter_loss: 0.12185458838939667
train_iter_loss: 0.26695728302001953
train_iter_loss: 0.08513377606868744
train_iter_loss: 0.25211283564567566
train_iter_loss: 0.10776609182357788
train_iter_loss: 0.26514244079589844
train_iter_loss: 0.09205114841461182
train_iter_loss: 0.12893563508987427
train_iter_loss: 0.25698399543762207
train_iter_loss: 0.16133473813533783
train_iter_loss: 0.08624768257141113
train_iter_loss: 0.07519740611314774
train_iter_loss: 0.20900845527648926
train_iter_loss: 0.15883706510066986
train_iter_loss: 0.16645881533622742
train_iter_loss: 0.13366827368736267
train_iter_loss: 0.1561574637889862
train_iter_loss: 0.16313953697681427
train_iter_loss: 0.21816028654575348
train_iter_loss: 0.12634910643100739
train_iter_loss: 0.2248203605413437
train_iter_loss: 0.17142748832702637
train_iter_loss: 0.1739898920059204
train_iter_loss: 0.06206921115517616
train_iter_loss: 0.15006014704704285
train_iter_loss: 0.178311288356781
train_iter_loss: 0.25887835025787354
train_iter_loss: 0.13124850392341614
train_iter_loss: 0.07939787954092026
train_iter_loss: 0.07122033834457397
train_iter_loss: 0.13578133285045624
train_iter_loss: 0.09471247345209122
train_iter_loss: 0.2736321687698364
train_iter_loss: 0.24846263229846954
train_iter_loss: 0.11833732575178146
train_iter_loss: 0.1060510203242302
train_iter_loss: 0.10597249120473862
train_iter_loss: 0.08363884687423706
train_iter_loss: 0.1542886346578598
train_iter_loss: 0.1749693602323532
train_iter_loss: 0.12480179220438004
train_iter_loss: 0.06172432750463486
train_iter_loss: 0.23645177483558655
train_iter_loss: 0.15924566984176636
train_iter_loss: 0.3244568705558777
train_iter_loss: 0.10642888396978378
train_iter_loss: 0.2019139975309372
train_iter_loss: 0.10852722823619843
train_iter_loss: 0.09300099313259125
train_iter_loss: 0.17302200198173523
train_iter_loss: 0.15013375878334045
train_iter_loss: 0.1659213900566101
train_iter_loss: 0.12392570078372955
train_iter_loss: 0.09248929470777512
train_iter_loss: 0.14409884810447693
train_iter_loss: 0.13431642949581146
train_iter_loss: 0.16138134896755219
train_iter_loss: 0.12469815462827682
train_iter_loss: 0.1557634025812149
train_iter_loss: 0.05507790669798851
train_iter_loss: 0.31744182109832764
train_iter_loss: 0.08446706086397171
train_iter_loss: 0.18956907093524933
train_iter_loss: 0.07034792751073837
train_iter_loss: 0.09634590893983841
train_iter_loss: 0.22380366921424866
train_iter_loss: 0.19230633974075317
train_iter_loss: 0.09913204610347748
train_iter_loss: 0.18696503341197968
train_iter_loss: 0.1449422538280487
train loss :0.1568
---------------------
Validation seg loss: 0.21448710129403 at epoch 633
epoch =    634/  1000, exp = train
train_iter_loss: 0.0660829246044159
train_iter_loss: 0.1516953855752945
train_iter_loss: 0.17196018993854523
train_iter_loss: 0.14054559171199799
train_iter_loss: 0.1479511260986328
train_iter_loss: 0.16245850920677185
train_iter_loss: 0.19181068241596222
train_iter_loss: 0.10206998139619827
train_iter_loss: 0.20425842702388763
train_iter_loss: 0.15235355496406555
train_iter_loss: 0.028266284614801407
train_iter_loss: 0.11967338621616364
train_iter_loss: 0.10327877849340439
train_iter_loss: 0.15413622558116913
train_iter_loss: 0.07789500057697296
train_iter_loss: 0.1503710299730301
train_iter_loss: 0.1889353096485138
train_iter_loss: 0.18027475476264954
train_iter_loss: 0.11420293897390366
train_iter_loss: 0.18371263146400452
train_iter_loss: 0.16597308218479156
train_iter_loss: 0.16113443672657013
train_iter_loss: 0.12327759712934494
train_iter_loss: 0.23536443710327148
train_iter_loss: 0.1633223593235016
train_iter_loss: 0.19577336311340332
train_iter_loss: 0.143438458442688
train_iter_loss: 0.07486900687217712
train_iter_loss: 0.16251859068870544
train_iter_loss: 0.1959591954946518
train_iter_loss: 0.2312183976173401
train_iter_loss: 0.11201979219913483
train_iter_loss: 0.12264376878738403
train_iter_loss: 0.10045482963323593
train_iter_loss: 0.033984001725912094
train_iter_loss: 0.1848839670419693
train_iter_loss: 0.09943792968988419
train_iter_loss: 0.14443476498126984
train_iter_loss: 0.2446090131998062
train_iter_loss: 0.14672710001468658
train_iter_loss: 0.0890052318572998
train_iter_loss: 0.302153617143631
train_iter_loss: 0.192935049533844
train_iter_loss: 0.21796108782291412
train_iter_loss: 0.2399487942457199
train_iter_loss: 0.1697699874639511
train_iter_loss: 0.18566164374351501
train_iter_loss: 0.15522998571395874
train_iter_loss: 0.13854128122329712
train_iter_loss: 0.1903943419456482
train_iter_loss: 0.18396791815757751
train_iter_loss: 0.07727688550949097
train_iter_loss: 0.2274603396654129
train_iter_loss: 0.17446228861808777
train_iter_loss: 0.20804566144943237
train_iter_loss: 0.2285660207271576
train_iter_loss: 0.12334533035755157
train_iter_loss: 0.1909761130809784
train_iter_loss: 0.15619000792503357
train_iter_loss: 0.06489279121160507
train_iter_loss: 0.214853897690773
train_iter_loss: 0.09115219861268997
train_iter_loss: 0.18525899946689606
train_iter_loss: 0.2284022718667984
train_iter_loss: 0.13947132229804993
train_iter_loss: 0.1534978151321411
train_iter_loss: 0.1391829401254654
train_iter_loss: 0.2348932921886444
train_iter_loss: 0.31954556703567505
train_iter_loss: 0.10049176216125488
train_iter_loss: 0.2093573957681656
train_iter_loss: 0.0537894070148468
train_iter_loss: 0.057700321078300476
train_iter_loss: 0.10578001290559769
train_iter_loss: 0.12051677703857422
train_iter_loss: 0.33572816848754883
train_iter_loss: 0.22237560153007507
train_iter_loss: 0.11321157962083817
train_iter_loss: 0.1770349144935608
train_iter_loss: 0.12318293750286102
train_iter_loss: 0.14602164924144745
train_iter_loss: 0.1314067542552948
train_iter_loss: 0.10100208967924118
train_iter_loss: 0.15021997690200806
train_iter_loss: 0.18857498466968536
train_iter_loss: 0.21833181381225586
train_iter_loss: 0.0429321825504303
train_iter_loss: 0.183256134390831
train_iter_loss: 0.15264536440372467
train_iter_loss: 0.17337200045585632
train_iter_loss: 0.148617684841156
train_iter_loss: 0.2077292501926422
train_iter_loss: 0.25051480531692505
train_iter_loss: 0.0759197399020195
train_iter_loss: 0.12171126157045364
train_iter_loss: 0.13422013819217682
train_iter_loss: 0.13361665606498718
train_iter_loss: 0.3106943368911743
train_iter_loss: 0.1288645714521408
train_iter_loss: 0.21674947440624237
train loss :0.1605
---------------------
Validation seg loss: 0.22151416449649436 at epoch 634
epoch =    635/  1000, exp = train
train_iter_loss: 0.19783452153205872
train_iter_loss: 0.12523460388183594
train_iter_loss: 0.1258111149072647
train_iter_loss: 0.13778051733970642
train_iter_loss: 0.3234415650367737
train_iter_loss: 0.15093228220939636
train_iter_loss: 0.06817035377025604
train_iter_loss: 0.17705869674682617
train_iter_loss: 0.09223335981369019
train_iter_loss: 0.20345593988895416
train_iter_loss: 0.1076083779335022
train_iter_loss: 0.12703286111354828
train_iter_loss: 0.15778544545173645
train_iter_loss: 0.18484671413898468
train_iter_loss: 0.15752433240413666
train_iter_loss: 0.21691511571407318
train_iter_loss: 0.22394996881484985
train_iter_loss: 0.15247313678264618
train_iter_loss: 0.11976194381713867
train_iter_loss: 0.22105753421783447
train_iter_loss: 0.1337638795375824
train_iter_loss: 0.21126651763916016
train_iter_loss: 0.04802066460251808
train_iter_loss: 0.24220646917819977
train_iter_loss: 0.1494670957326889
train_iter_loss: 0.18826621770858765
train_iter_loss: 0.14958520233631134
train_iter_loss: 0.23881661891937256
train_iter_loss: 0.12297296524047852
train_iter_loss: 0.1281120777130127
train_iter_loss: 0.13851222395896912
train_iter_loss: 0.05224202573299408
train_iter_loss: 0.15064604580402374
train_iter_loss: 0.32945385575294495
train_iter_loss: 0.14594782888889313
train_iter_loss: 0.2602132558822632
train_iter_loss: 0.14725004136562347
train_iter_loss: 0.11461125314235687
train_iter_loss: 0.07234029471874237
train_iter_loss: 0.16775177419185638
train_iter_loss: 0.4111870527267456
train_iter_loss: 0.27494457364082336
train_iter_loss: 0.09169114381074905
train_iter_loss: 0.16166657209396362
train_iter_loss: 0.23136447370052338
train_iter_loss: 0.16591604053974152
train_iter_loss: 0.23445181548595428
train_iter_loss: 0.16165363788604736
train_iter_loss: 0.07546072453260422
train_iter_loss: 0.1501712203025818
train_iter_loss: 0.17120549082756042
train_iter_loss: 0.08438372611999512
train_iter_loss: 0.14108121395111084
train_iter_loss: 0.0952429249882698
train_iter_loss: 0.1954338699579239
train_iter_loss: 0.06821423023939133
train_iter_loss: 0.2806173861026764
train_iter_loss: 0.20682455599308014
train_iter_loss: 0.15329864621162415
train_iter_loss: 0.15774685144424438
train_iter_loss: 0.138619527220726
train_iter_loss: 0.2491176575422287
train_iter_loss: 0.15790748596191406
train_iter_loss: 0.13334377110004425
train_iter_loss: 0.13828356564044952
train_iter_loss: 0.09019780158996582
train_iter_loss: 0.14580297470092773
train_iter_loss: 0.09311313182115555
train_iter_loss: 0.1152147576212883
train_iter_loss: 0.14294032752513885
train_iter_loss: 0.17228734493255615
train_iter_loss: 0.0805983766913414
train_iter_loss: 0.16731274127960205
train_iter_loss: 0.20272907614707947
train_iter_loss: 0.21649540960788727
train_iter_loss: 0.17436736822128296
train_iter_loss: 0.16709257662296295
train_iter_loss: 0.1840566098690033
train_iter_loss: 0.07539060711860657
train_iter_loss: 0.16394446790218353
train_iter_loss: 0.16970403492450714
train_iter_loss: 0.06821796298027039
train_iter_loss: 0.21950754523277283
train_iter_loss: 0.17143259942531586
train_iter_loss: 0.09277365356683731
train_iter_loss: 0.16121231019496918
train_iter_loss: 0.12602448463439941
train_iter_loss: 0.1002868264913559
train_iter_loss: 0.12439265102148056
train_iter_loss: 0.20909647643566132
train_iter_loss: 0.11362309008836746
train_iter_loss: 0.18538300693035126
train_iter_loss: 0.06793061643838882
train_iter_loss: 0.07404188811779022
train_iter_loss: 0.2081431746482849
train_iter_loss: 0.11106390506029129
train_iter_loss: 0.21042585372924805
train_iter_loss: 0.13223518431186676
train_iter_loss: 0.12462888658046722
train_iter_loss: 0.23153480887413025
train loss :0.1605
---------------------
Validation seg loss: 0.21847002700252352 at epoch 635
epoch =    636/  1000, exp = train
train_iter_loss: 0.11595487594604492
train_iter_loss: 0.17842638492584229
train_iter_loss: 0.15906144678592682
train_iter_loss: 0.1689973920583725
train_iter_loss: 0.17860320210456848
train_iter_loss: 0.13827040791511536
train_iter_loss: 0.12278551608324051
train_iter_loss: 0.1660189926624298
train_iter_loss: 0.1883094608783722
train_iter_loss: 0.1418561041355133
train_iter_loss: 0.08034542948007584
train_iter_loss: 0.055417876690626144
train_iter_loss: 0.17851047217845917
train_iter_loss: 0.11350421607494354
train_iter_loss: 0.2209366410970688
train_iter_loss: 0.19588924944400787
train_iter_loss: 0.06628279387950897
train_iter_loss: 0.21927370131015778
train_iter_loss: 0.21592923998832703
train_iter_loss: 0.2502985894680023
train_iter_loss: 0.15599417686462402
train_iter_loss: 0.15858717262744904
train_iter_loss: 0.10981972515583038
train_iter_loss: 0.10566902160644531
train_iter_loss: 0.17004378139972687
train_iter_loss: 0.21251685917377472
train_iter_loss: 0.12707096338272095
train_iter_loss: 0.14520512521266937
train_iter_loss: 0.13634447753429413
train_iter_loss: 0.38286182284355164
train_iter_loss: 0.18217505514621735
train_iter_loss: 0.22689615190029144
train_iter_loss: 0.20360830426216125
train_iter_loss: 0.09387283772230148
train_iter_loss: 0.06861131638288498
train_iter_loss: 0.1696988195180893
train_iter_loss: 0.23122254014015198
train_iter_loss: 0.12474754452705383
train_iter_loss: 0.09813421219587326
train_iter_loss: 0.06225883960723877
train_iter_loss: 0.260863333940506
train_iter_loss: 0.1709313839673996
train_iter_loss: 0.13633641600608826
train_iter_loss: 0.08899629861116409
train_iter_loss: 0.13358263671398163
train_iter_loss: 0.0647067055106163
train_iter_loss: 0.16543501615524292
train_iter_loss: 0.1371469646692276
train_iter_loss: 0.20467214286327362
train_iter_loss: 0.13208802044391632
train_iter_loss: 0.14728225767612457
train_iter_loss: 0.138090118765831
train_iter_loss: 0.09455177187919617
train_iter_loss: 0.16537067294120789
train_iter_loss: 0.38608241081237793
train_iter_loss: 0.26281675696372986
train_iter_loss: 0.23893065750598907
train_iter_loss: 0.18553636968135834
train_iter_loss: 0.19048461318016052
train_iter_loss: 0.1049109399318695
train_iter_loss: 0.15507499873638153
train_iter_loss: 0.18717660009860992
train_iter_loss: 0.07203217595815659
train_iter_loss: 0.17278046905994415
train_iter_loss: 0.1850382387638092
train_iter_loss: 0.12432625144720078
train_iter_loss: 0.17446817457675934
train_iter_loss: 0.19972428679466248
train_iter_loss: 0.24069342017173767
train_iter_loss: 0.21631786227226257
train_iter_loss: 0.20225919783115387
train_iter_loss: 0.1768806129693985
train_iter_loss: 0.09133981168270111
train_iter_loss: 0.11063338816165924
train_iter_loss: 0.12594440579414368
train_iter_loss: 0.21257421374320984
train_iter_loss: 0.18594852089881897
train_iter_loss: 0.15661272406578064
train_iter_loss: 0.15649208426475525
train_iter_loss: 0.15046869218349457
train_iter_loss: 0.16352558135986328
train_iter_loss: 0.11742837727069855
train_iter_loss: 0.1462077796459198
train_iter_loss: 0.16714656352996826
train_iter_loss: 0.19146190583705902
train_iter_loss: 0.19657456874847412
train_iter_loss: 0.1818235069513321
train_iter_loss: 0.12368254363536835
train_iter_loss: 0.14814233779907227
train_iter_loss: 0.11742469668388367
train_iter_loss: 0.15590763092041016
train_iter_loss: 0.10394398868083954
train_iter_loss: 0.11374090611934662
train_iter_loss: 0.13187509775161743
train_iter_loss: 0.13941095769405365
train_iter_loss: 0.16656526923179626
train_iter_loss: 0.1364956945180893
train_iter_loss: 0.164270281791687
train_iter_loss: 0.10469810664653778
train_iter_loss: 0.07876306027173996
train loss :0.1604
---------------------
Validation seg loss: 0.2176802469733751 at epoch 636
epoch =    637/  1000, exp = train
train_iter_loss: 0.13536523282527924
train_iter_loss: 0.09794475138187408
train_iter_loss: 0.08866636455059052
train_iter_loss: 0.18102481961250305
train_iter_loss: 0.1268300861120224
train_iter_loss: 0.22595347464084625
train_iter_loss: 0.2212458848953247
train_iter_loss: 0.12380287796258926
train_iter_loss: 0.16162119805812836
train_iter_loss: 0.1608245074748993
train_iter_loss: 0.10446034371852875
train_iter_loss: 0.16880258917808533
train_iter_loss: 0.05038806051015854
train_iter_loss: 0.1289960741996765
train_iter_loss: 0.18049189448356628
train_iter_loss: 0.1782727688550949
train_iter_loss: 0.18078704178333282
train_iter_loss: 0.11245894432067871
train_iter_loss: 0.26224204897880554
train_iter_loss: 0.12286660820245743
train_iter_loss: 0.1614474207162857
train_iter_loss: 0.21666319668293
train_iter_loss: 0.10275215655565262
train_iter_loss: 0.04461036995053291
train_iter_loss: 0.09243332594633102
train_iter_loss: 0.20828941464424133
train_iter_loss: 0.11197932064533234
train_iter_loss: 0.1074296310544014
train_iter_loss: 0.16886845231056213
train_iter_loss: 0.17904888093471527
train_iter_loss: 0.14115257561206818
train_iter_loss: 0.1767769306898117
train_iter_loss: 0.24199657142162323
train_iter_loss: 0.1743527501821518
train_iter_loss: 0.274972528219223
train_iter_loss: 0.060458794236183167
train_iter_loss: 0.13355940580368042
train_iter_loss: 0.04646489769220352
train_iter_loss: 0.12235619872808456
train_iter_loss: 0.23124144971370697
train_iter_loss: 0.14953473210334778
train_iter_loss: 0.16121822595596313
train_iter_loss: 0.13678747415542603
train_iter_loss: 0.26126235723495483
train_iter_loss: 0.2359064370393753
train_iter_loss: 0.11261484771966934
train_iter_loss: 0.26785141229629517
train_iter_loss: 0.17772190272808075
train_iter_loss: 0.14611105620861053
train_iter_loss: 0.0962275043129921
train_iter_loss: 0.2307633012533188
train_iter_loss: 0.13505877554416656
train_iter_loss: 0.15633736550807953
train_iter_loss: 0.14853821694850922
train_iter_loss: 0.2572650909423828
train_iter_loss: 0.1998634934425354
train_iter_loss: 0.12586583197116852
train_iter_loss: 0.15431585907936096
train_iter_loss: 0.2405518740415573
train_iter_loss: 0.0930904969573021
train_iter_loss: 0.26596590876579285
train_iter_loss: 0.23812727630138397
train_iter_loss: 0.13827496767044067
train_iter_loss: 0.13360491394996643
train_iter_loss: 0.10075899958610535
train_iter_loss: 0.2844730615615845
train_iter_loss: 0.12092777341604233
train_iter_loss: 0.18460363149642944
train_iter_loss: 0.1703026294708252
train_iter_loss: 0.16756020486354828
train_iter_loss: 0.12840650975704193
train_iter_loss: 0.09234548360109329
train_iter_loss: 0.1559644192457199
train_iter_loss: 0.09445913136005402
train_iter_loss: 0.10901593416929245
train_iter_loss: 0.27765071392059326
train_iter_loss: 0.10278928279876709
train_iter_loss: 0.22675098478794098
train_iter_loss: 0.1571851521730423
train_iter_loss: 0.08842507004737854
train_iter_loss: 0.06573637574911118
train_iter_loss: 0.22012320160865784
train_iter_loss: 0.2411944717168808
train_iter_loss: 0.15393590927124023
train_iter_loss: 0.17755912244319916
train_iter_loss: 0.15644089877605438
train_iter_loss: 0.11080214381217957
train_iter_loss: 0.15383416414260864
train_iter_loss: 0.11724085360765457
train_iter_loss: 0.13671740889549255
train_iter_loss: 0.21216581761837006
train_iter_loss: 0.15974733233451843
train_iter_loss: 0.15973901748657227
train_iter_loss: 0.07616952806711197
train_iter_loss: 0.10167212039232254
train_iter_loss: 0.15944908559322357
train_iter_loss: 0.23120005428791046
train_iter_loss: 0.25498464703559875
train_iter_loss: 0.25009623169898987
train_iter_loss: 0.19830165803432465
train loss :0.1623
---------------------
Validation seg loss: 0.21504915464353166 at epoch 637
epoch =    638/  1000, exp = train
train_iter_loss: 0.22819174826145172
train_iter_loss: 0.06806634366512299
train_iter_loss: 0.10373358428478241
train_iter_loss: 0.17103692889213562
train_iter_loss: 0.14962609112262726
train_iter_loss: 0.06346922367811203
train_iter_loss: 0.10505031794309616
train_iter_loss: 0.10184232890605927
train_iter_loss: 0.23466476798057556
train_iter_loss: 0.2307061403989792
train_iter_loss: 0.18814490735530853
train_iter_loss: 0.10132511705160141
train_iter_loss: 0.17480191588401794
train_iter_loss: 0.1786089539527893
train_iter_loss: 0.1165202409029007
train_iter_loss: 0.20116189122200012
train_iter_loss: 0.22106051445007324
train_iter_loss: 0.11632133275270462
train_iter_loss: 0.17354340851306915
train_iter_loss: 0.22338338196277618
train_iter_loss: 0.14309318363666534
train_iter_loss: 0.04369881749153137
train_iter_loss: 0.14483438432216644
train_iter_loss: 0.1874261498451233
train_iter_loss: 0.23164325952529907
train_iter_loss: 0.17661026120185852
train_iter_loss: 0.14558736979961395
train_iter_loss: 0.2190752476453781
train_iter_loss: 0.15475152432918549
train_iter_loss: 0.17852063477039337
train_iter_loss: 0.09310577064752579
train_iter_loss: 0.23722468316555023
train_iter_loss: 0.0836106538772583
train_iter_loss: 0.06168833374977112
train_iter_loss: 0.39301827549934387
train_iter_loss: 0.19923943281173706
train_iter_loss: 0.18728503584861755
train_iter_loss: 0.057187099009752274
train_iter_loss: 0.20010273158550262
train_iter_loss: 0.11907052993774414
train_iter_loss: 0.21248169243335724
train_iter_loss: 0.2067752480506897
train_iter_loss: 0.11197331547737122
train_iter_loss: 0.11047905683517456
train_iter_loss: 0.10859223455190659
train_iter_loss: 0.07450711727142334
train_iter_loss: 0.1400492936372757
train_iter_loss: 0.13705290853977203
train_iter_loss: 0.1440376341342926
train_iter_loss: 0.22703410685062408
train_iter_loss: 0.22256645560264587
train_iter_loss: 0.14929072558879852
train_iter_loss: 0.2212660312652588
train_iter_loss: 0.18193885684013367
train_iter_loss: 0.1291993409395218
train_iter_loss: 0.1442391574382782
train_iter_loss: 0.10159891098737717
train_iter_loss: 0.1164138913154602
train_iter_loss: 0.17994044721126556
train_iter_loss: 0.15361909568309784
train_iter_loss: 0.16676999628543854
train_iter_loss: 0.1565743088722229
train_iter_loss: 0.09838510304689407
train_iter_loss: 0.17451730370521545
train_iter_loss: 0.18250039219856262
train_iter_loss: 0.14344845712184906
train_iter_loss: 0.09607230126857758
train_iter_loss: 0.2040044069290161
train_iter_loss: 0.1352512538433075
train_iter_loss: 0.11594764143228531
train_iter_loss: 0.12406112253665924
train_iter_loss: 0.07703377306461334
train_iter_loss: 0.22422921657562256
train_iter_loss: 0.08548469096422195
train_iter_loss: 0.20530937612056732
train_iter_loss: 0.17020943760871887
train_iter_loss: 0.25768452882766724
train_iter_loss: 0.15959911048412323
train_iter_loss: 0.2100452482700348
train_iter_loss: 0.1540384590625763
train_iter_loss: 0.07110294699668884
train_iter_loss: 0.46891921758651733
train_iter_loss: 0.1646849811077118
train_iter_loss: 0.12850360572338104
train_iter_loss: 0.15895909070968628
train_iter_loss: 0.1643909513950348
train_iter_loss: 0.16889387369155884
train_iter_loss: 0.1274843066930771
train_iter_loss: 0.18095114827156067
train_iter_loss: 0.08711181581020355
train_iter_loss: 0.1722198724746704
train_iter_loss: 0.14326567947864532
train_iter_loss: 0.13501110672950745
train_iter_loss: 0.17489507794380188
train_iter_loss: 0.22257983684539795
train_iter_loss: 0.158049076795578
train_iter_loss: 0.2705269455909729
train_iter_loss: 0.12620432674884796
train_iter_loss: 0.14376233518123627
train_iter_loss: 0.08776511996984482
train loss :0.1614
---------------------
Validation seg loss: 0.2183205380789795 at epoch 638
epoch =    639/  1000, exp = train
train_iter_loss: 0.17036408185958862
train_iter_loss: 0.09841421246528625
train_iter_loss: 0.1727166622877121
train_iter_loss: 0.1984599381685257
train_iter_loss: 0.2581510543823242
train_iter_loss: 0.4768959879875183
train_iter_loss: 0.13034364581108093
train_iter_loss: 0.14379705488681793
train_iter_loss: 0.2126576155424118
train_iter_loss: 0.1382821500301361
train_iter_loss: 0.1011316254734993
train_iter_loss: 0.1626761108636856
train_iter_loss: 0.20549128949642181
train_iter_loss: 0.18643313646316528
train_iter_loss: 0.11267894506454468
train_iter_loss: 0.13542906939983368
train_iter_loss: 0.19911383092403412
train_iter_loss: 0.12116803228855133
train_iter_loss: 0.15937578678131104
train_iter_loss: 0.15798558294773102
train_iter_loss: 0.153830423951149
train_iter_loss: 0.30972567200660706
train_iter_loss: 0.15658533573150635
train_iter_loss: 0.16072317957878113
train_iter_loss: 0.14616306126117706
train_iter_loss: 0.05461639165878296
train_iter_loss: 0.11786992847919464
train_iter_loss: 0.2732279896736145
train_iter_loss: 0.1467607319355011
train_iter_loss: 0.2718236744403839
train_iter_loss: 0.23704008758068085
train_iter_loss: 0.16734054684638977
train_iter_loss: 0.1604425460100174
train_iter_loss: 0.1376635879278183
train_iter_loss: 0.1027536615729332
train_iter_loss: 0.26429352164268494
train_iter_loss: 0.14188599586486816
train_iter_loss: 0.1330186277627945
train_iter_loss: 0.08613835275173187
train_iter_loss: 0.04130125418305397
train_iter_loss: 0.15646596252918243
train_iter_loss: 0.08858542889356613
train_iter_loss: 0.12314704805612564
train_iter_loss: 0.13445821404457092
train_iter_loss: 0.1693328320980072
train_iter_loss: 0.06716960668563843
train_iter_loss: 0.26898831129074097
train_iter_loss: 0.23189738392829895
train_iter_loss: 0.1172969862818718
train_iter_loss: 0.0692375972867012
train_iter_loss: 0.07955922931432724
train_iter_loss: 0.15382611751556396
train_iter_loss: 0.12025610357522964
train_iter_loss: 0.1418263018131256
train_iter_loss: 0.24170298874378204
train_iter_loss: 0.10288190096616745
train_iter_loss: 0.14383670687675476
train_iter_loss: 0.1686222404241562
train_iter_loss: 0.14992544054985046
train_iter_loss: 0.21883778274059296
train_iter_loss: 0.18765754997730255
train_iter_loss: 0.17348085343837738
train_iter_loss: 0.12074652314186096
train_iter_loss: 0.1203523501753807
train_iter_loss: 0.10305145382881165
train_iter_loss: 0.31964603066444397
train_iter_loss: 0.11558137834072113
train_iter_loss: 0.09605672210454941
train_iter_loss: 0.15613403916358948
train_iter_loss: 0.2523803412914276
train_iter_loss: 0.1634729653596878
train_iter_loss: 0.164765402674675
train_iter_loss: 0.15385155379772186
train_iter_loss: 0.1554555594921112
train_iter_loss: 0.05859876051545143
train_iter_loss: 0.23799824714660645
train_iter_loss: 0.12429720908403397
train_iter_loss: 0.17078764736652374
train_iter_loss: 0.19417428970336914
train_iter_loss: 0.21585223078727722
train_iter_loss: 0.15584543347358704
train_iter_loss: 0.11938250064849854
train_iter_loss: 0.1727275252342224
train_iter_loss: 0.059872668236494064
train_iter_loss: 0.08456137031316757
train_iter_loss: 0.18244214355945587
train_iter_loss: 0.05037571117281914
train_iter_loss: 0.13391099870204926
train_iter_loss: 0.0835275873541832
train_iter_loss: 0.18329916894435883
train_iter_loss: 0.17780466377735138
train_iter_loss: 0.16068293154239655
train_iter_loss: 0.13390856981277466
train_iter_loss: 0.15217262506484985
train_iter_loss: 0.14757832884788513
train_iter_loss: 0.14944368600845337
train_iter_loss: 0.1496792435646057
train_iter_loss: 0.09503474086523056
train_iter_loss: 0.1092536523938179
train_iter_loss: 0.1465648114681244
train loss :0.1585
---------------------
Validation seg loss: 0.21653599921121913 at epoch 639
epoch =    640/  1000, exp = train
train_iter_loss: 0.18040014803409576
train_iter_loss: 0.1693592518568039
train_iter_loss: 0.1404726803302765
train_iter_loss: 0.1555842161178589
train_iter_loss: 0.24970516562461853
train_iter_loss: 0.37569206953048706
train_iter_loss: 0.10734973847866058
train_iter_loss: 0.20932704210281372
train_iter_loss: 0.18288443982601166
train_iter_loss: 0.1117125079035759
train_iter_loss: 0.23733076453208923
train_iter_loss: 0.19310946762561798
train_iter_loss: 0.10779654234647751
train_iter_loss: 0.2049046754837036
train_iter_loss: 0.20963643491268158
train_iter_loss: 0.09543208032846451
train_iter_loss: 0.07873719185590744
train_iter_loss: 0.18938542902469635
train_iter_loss: 0.07950036227703094
train_iter_loss: 0.11094370484352112
train_iter_loss: 0.2087528556585312
train_iter_loss: 0.06468173116445541
train_iter_loss: 0.08441660553216934
train_iter_loss: 0.09804519265890121
train_iter_loss: 0.25710105895996094
train_iter_loss: 0.13703586161136627
train_iter_loss: 0.13754229247570038
train_iter_loss: 0.14025543630123138
train_iter_loss: 0.2660103142261505
train_iter_loss: 0.1287868171930313
train_iter_loss: 0.058661144226789474
train_iter_loss: 0.09511394798755646
train_iter_loss: 0.11063146591186523
train_iter_loss: 0.1428598016500473
train_iter_loss: 0.1688694953918457
train_iter_loss: 0.11604374647140503
train_iter_loss: 0.18995767831802368
train_iter_loss: 0.1981153041124344
train_iter_loss: 0.06812901049852371
train_iter_loss: 0.08553571254014969
train_iter_loss: 0.1624125838279724
train_iter_loss: 0.12193635106086731
train_iter_loss: 0.23906534910202026
train_iter_loss: 0.08222046494483948
train_iter_loss: 0.180051788687706
train_iter_loss: 0.21799373626708984
train_iter_loss: 0.11274388432502747
train_iter_loss: 0.11734971404075623
train_iter_loss: 0.1364607959985733
train_iter_loss: 0.0845705047249794
train_iter_loss: 0.13633032143115997
train_iter_loss: 0.17177355289459229
train_iter_loss: 0.21187478303909302
train_iter_loss: 0.08981352299451828
train_iter_loss: 0.046572793275117874
train_iter_loss: 0.14236441254615784
train_iter_loss: 0.158148393034935
train_iter_loss: 0.1712276041507721
train_iter_loss: 0.10069532692432404
train_iter_loss: 0.16074950993061066
train_iter_loss: 0.14625579118728638
train_iter_loss: 0.36775389313697815
train_iter_loss: 0.2175045609474182
train_iter_loss: 0.14899127185344696
train_iter_loss: 0.11414938420057297
train_iter_loss: 0.19433575868606567
train_iter_loss: 0.16416221857070923
train_iter_loss: 0.193979874253273
train_iter_loss: 0.2300734519958496
train_iter_loss: 0.0705297514796257
train_iter_loss: 0.11775720119476318
train_iter_loss: 0.12589122354984283
train_iter_loss: 0.20004811882972717
train_iter_loss: 0.08942681550979614
train_iter_loss: 0.1616590917110443
train_iter_loss: 0.15151230990886688
train_iter_loss: 0.07221643626689911
train_iter_loss: 0.115144282579422
train_iter_loss: 0.14577238261699677
train_iter_loss: 0.09896824508905411
train_iter_loss: 0.12672942876815796
train_iter_loss: 0.10163659602403641
train_iter_loss: 0.08174046874046326
train_iter_loss: 0.2527351677417755
train_iter_loss: 0.19937296211719513
train_iter_loss: 0.14195972681045532
train_iter_loss: 0.22335901856422424
train_iter_loss: 0.14780081808567047
train_iter_loss: 0.16508840024471283
train_iter_loss: 0.07134652882814407
train_iter_loss: 0.23698756098747253
train_iter_loss: 0.10588844865560532
train_iter_loss: 0.13771378993988037
train_iter_loss: 0.2334214597940445
train_iter_loss: 0.12865562736988068
train_iter_loss: 0.23062875866889954
train_iter_loss: 0.17725877463817596
train_iter_loss: 0.09045290946960449
train_iter_loss: 0.1757773756980896
train_iter_loss: 0.12040328979492188
train loss :0.1542
---------------------
Validation seg loss: 0.21667102451067208 at epoch 640
epoch =    641/  1000, exp = train
train_iter_loss: 0.2069663256406784
train_iter_loss: 0.11715713888406754
train_iter_loss: 0.1630936563014984
train_iter_loss: 0.1320473551750183
train_iter_loss: 0.23426295816898346
train_iter_loss: 0.2099025398492813
train_iter_loss: 0.0676753893494606
train_iter_loss: 0.20278877019882202
train_iter_loss: 0.062249135226011276
train_iter_loss: 0.12451313436031342
train_iter_loss: 0.18243123590946198
train_iter_loss: 0.2038392424583435
train_iter_loss: 0.14029596745967865
train_iter_loss: 0.1554805040359497
train_iter_loss: 0.31497496366500854
train_iter_loss: 0.17780911922454834
train_iter_loss: 0.14677612483501434
train_iter_loss: 0.11928892880678177
train_iter_loss: 0.15927019715309143
train_iter_loss: 0.13171470165252686
train_iter_loss: 0.22901713848114014
train_iter_loss: 0.19642049074172974
train_iter_loss: 0.1224123165011406
train_iter_loss: 0.17033040523529053
train_iter_loss: 0.2339349389076233
train_iter_loss: 0.20409131050109863
train_iter_loss: 0.09184924513101578
train_iter_loss: 0.07820981740951538
train_iter_loss: 0.10744383931159973
train_iter_loss: 0.17265631258487701
train_iter_loss: 0.09247897565364838
train_iter_loss: 0.20454268157482147
train_iter_loss: 0.14793919026851654
train_iter_loss: 0.11182281374931335
train_iter_loss: 0.20593997836112976
train_iter_loss: 0.17721454799175262
train_iter_loss: 0.39245760440826416
train_iter_loss: 0.09271339327096939
train_iter_loss: 0.24771180748939514
train_iter_loss: 0.12981776893138885
train_iter_loss: 0.15616677701473236
train_iter_loss: 0.12953858077526093
train_iter_loss: 0.14264217019081116
train_iter_loss: 0.0684172511100769
train_iter_loss: 0.1789526641368866
train_iter_loss: 0.07243762910366058
train_iter_loss: 0.09874195605516434
train_iter_loss: 0.33787259459495544
train_iter_loss: 0.05887274816632271
train_iter_loss: 0.15742191672325134
train_iter_loss: 0.12589286267757416
train_iter_loss: 0.11916609853506088
train_iter_loss: 0.27526766061782837
train_iter_loss: 0.1414397805929184
train_iter_loss: 0.13864728808403015
train_iter_loss: 0.12081751972436905
train_iter_loss: 0.1561547815799713
train_iter_loss: 0.16626670956611633
train_iter_loss: 0.2396748960018158
train_iter_loss: 0.21671293675899506
train_iter_loss: 0.19920849800109863
train_iter_loss: 0.24010607600212097
train_iter_loss: 0.12433921545743942
train_iter_loss: 0.12954358756542206
train_iter_loss: 0.16713838279247284
train_iter_loss: 0.23430763185024261
train_iter_loss: 0.14527961611747742
train_iter_loss: 0.12426576763391495
train_iter_loss: 0.1314650923013687
train_iter_loss: 0.12741629779338837
train_iter_loss: 0.12132012099027634
train_iter_loss: 0.07709614932537079
train_iter_loss: 0.2412509322166443
train_iter_loss: 0.08611837029457092
train_iter_loss: 0.1811717003583908
train_iter_loss: 0.11768773943185806
train_iter_loss: 0.23854243755340576
train_iter_loss: 0.11240284889936447
train_iter_loss: 0.08988863974809647
train_iter_loss: 0.11077938228845596
train_iter_loss: 0.16888751089572906
train_iter_loss: 0.11565493792295456
train_iter_loss: 0.1764274537563324
train_iter_loss: 0.1519087702035904
train_iter_loss: 0.1683526486158371
train_iter_loss: 0.12346222996711731
train_iter_loss: 0.22175514698028564
train_iter_loss: 0.3716016411781311
train_iter_loss: 0.10109344869852066
train_iter_loss: 0.10275597125291824
train_iter_loss: 0.16823382675647736
train_iter_loss: 0.10686611384153366
train_iter_loss: 0.16782262921333313
train_iter_loss: 0.1523263305425644
train_iter_loss: 0.1933203935623169
train_iter_loss: 0.16687820851802826
train_iter_loss: 0.2279275506734848
train_iter_loss: 0.14469824731349945
train_iter_loss: 0.16289252042770386
train_iter_loss: 0.09252920746803284
train loss :0.1620
---------------------
Validation seg loss: 0.21731023799698307 at epoch 641
epoch =    642/  1000, exp = train
train_iter_loss: 0.1893167346715927
train_iter_loss: 0.3080948293209076
train_iter_loss: 0.09451407939195633
train_iter_loss: 0.052633997052907944
train_iter_loss: 0.268960177898407
train_iter_loss: 0.1120489239692688
train_iter_loss: 0.17403991520404816
train_iter_loss: 0.17545606195926666
train_iter_loss: 0.11759990453720093
train_iter_loss: 0.07620133459568024
train_iter_loss: 0.10654249787330627
train_iter_loss: 0.12883912026882172
train_iter_loss: 0.14161911606788635
train_iter_loss: 0.1682124137878418
train_iter_loss: 0.09776778519153595
train_iter_loss: 0.1290600746870041
train_iter_loss: 0.10701197385787964
train_iter_loss: 0.2559220790863037
train_iter_loss: 0.1376843899488449
train_iter_loss: 0.14703311026096344
train_iter_loss: 0.18304836750030518
train_iter_loss: 0.11210859566926956
train_iter_loss: 0.09867441654205322
train_iter_loss: 0.1293220818042755
train_iter_loss: 0.28329038619995117
train_iter_loss: 0.11594316363334656
train_iter_loss: 0.12082123011350632
train_iter_loss: 0.16289009153842926
train_iter_loss: 0.1708248108625412
train_iter_loss: 0.07045110315084457
train_iter_loss: 0.2511216998100281
train_iter_loss: 0.0564088448882103
train_iter_loss: 0.2294020652770996
train_iter_loss: 0.2021706998348236
train_iter_loss: 0.21831651031970978
train_iter_loss: 0.15856818854808807
train_iter_loss: 0.15917405486106873
train_iter_loss: 0.07411592453718185
train_iter_loss: 0.09261888265609741
train_iter_loss: 0.07083490490913391
train_iter_loss: 0.19613617658615112
train_iter_loss: 0.3551284372806549
train_iter_loss: 0.26994800567626953
train_iter_loss: 0.1779356300830841
train_iter_loss: 0.15689633786678314
train_iter_loss: 0.2570090591907501
train_iter_loss: 0.08710768818855286
train_iter_loss: 0.1365814208984375
train_iter_loss: 0.12446774542331696
train_iter_loss: 0.19773568212985992
train_iter_loss: 0.25097787380218506
train_iter_loss: 0.1695040762424469
train_iter_loss: 0.15598668158054352
train_iter_loss: 0.10188285261392593
train_iter_loss: 0.06054455414414406
train_iter_loss: 0.1839662343263626
train_iter_loss: 0.11439545452594757
train_iter_loss: 0.20474030077457428
train_iter_loss: 0.21665428578853607
train_iter_loss: 0.15282635390758514
train_iter_loss: 0.29023095965385437
train_iter_loss: 0.15690875053405762
train_iter_loss: 0.20466741919517517
train_iter_loss: 0.07965810596942902
train_iter_loss: 0.2481774389743805
train_iter_loss: 0.38083165884017944
train_iter_loss: 0.1707676202058792
train_iter_loss: 0.14080551266670227
train_iter_loss: 0.19687815010547638
train_iter_loss: 0.12440839409828186
train_iter_loss: 0.052491698414087296
train_iter_loss: 0.1774352788925171
train_iter_loss: 0.20371609926223755
train_iter_loss: 0.24518902599811554
train_iter_loss: 0.13702303171157837
train_iter_loss: 0.08924952149391174
train_iter_loss: 0.2467699646949768
train_iter_loss: 0.15299612283706665
train_iter_loss: 0.13316957652568817
train_iter_loss: 0.1519988626241684
train_iter_loss: 0.12456126511096954
train_iter_loss: 0.44914188981056213
train_iter_loss: 0.15547217428684235
train_iter_loss: 0.12811318039894104
train_iter_loss: 0.0939023345708847
train_iter_loss: 0.17017322778701782
train_iter_loss: 0.1517895609140396
train_iter_loss: 0.13699793815612793
train_iter_loss: 0.1851748526096344
train_iter_loss: 0.18735799193382263
train_iter_loss: 0.30891454219818115
train_iter_loss: 0.09688054770231247
train_iter_loss: 0.23661549389362335
train_iter_loss: 0.17260293662548065
train_iter_loss: 0.2729390263557434
train_iter_loss: 0.12631221115589142
train_iter_loss: 0.182707741856575
train_iter_loss: 0.31440967321395874
train_iter_loss: 0.1472231149673462
train_iter_loss: 0.11472244560718536
train loss :0.1702
---------------------
Validation seg loss: 0.21690493929287735 at epoch 642
epoch =    643/  1000, exp = train
train_iter_loss: 0.11339662224054337
train_iter_loss: 0.19036221504211426
train_iter_loss: 0.05873919650912285
train_iter_loss: 0.11494741588830948
train_iter_loss: 0.18468527495861053
train_iter_loss: 0.21129442751407623
train_iter_loss: 0.20200175046920776
train_iter_loss: 0.24383141100406647
train_iter_loss: 0.08738847821950912
train_iter_loss: 0.18833407759666443
train_iter_loss: 0.10657881200313568
train_iter_loss: 0.10322485864162445
train_iter_loss: 0.17807699739933014
train_iter_loss: 0.16075454652309418
train_iter_loss: 0.21590499579906464
train_iter_loss: 0.12162648886442184
train_iter_loss: 0.12775693833827972
train_iter_loss: 0.25669366121292114
train_iter_loss: 0.18815305829048157
train_iter_loss: 0.20666146278381348
train_iter_loss: 0.09761001169681549
train_iter_loss: 0.2559664249420166
train_iter_loss: 0.12371332943439484
train_iter_loss: 0.1583545058965683
train_iter_loss: 0.2862815856933594
train_iter_loss: 0.12236752361059189
train_iter_loss: 0.16339942812919617
train_iter_loss: 0.34381479024887085
train_iter_loss: 0.0970398411154747
train_iter_loss: 0.1714407205581665
train_iter_loss: 0.1518131047487259
train_iter_loss: 0.21446073055267334
train_iter_loss: 0.14432093501091003
train_iter_loss: 0.1499136984348297
train_iter_loss: 0.2236735075712204
train_iter_loss: 0.24824115633964539
train_iter_loss: 0.28570517897605896
train_iter_loss: 0.09299278259277344
train_iter_loss: 0.1367577165365219
train_iter_loss: 0.24552187323570251
train_iter_loss: 0.06602608412504196
train_iter_loss: 0.18051587045192719
train_iter_loss: 0.09324607998132706
train_iter_loss: 0.2129402458667755
train_iter_loss: 0.10852503031492233
train_iter_loss: 0.13843120634555817
train_iter_loss: 0.12454277276992798
train_iter_loss: 0.279010146856308
train_iter_loss: 0.12856677174568176
train_iter_loss: 0.18810002505779266
train_iter_loss: 0.1466122269630432
train_iter_loss: 0.03317474573850632
train_iter_loss: 0.08612632751464844
train_iter_loss: 0.09653108566999435
train_iter_loss: 0.13703422248363495
train_iter_loss: 0.19487079977989197
train_iter_loss: 0.13332805037498474
train_iter_loss: 0.1433013379573822
train_iter_loss: 0.210730642080307
train_iter_loss: 0.17692629992961884
train_iter_loss: 0.20350408554077148
train_iter_loss: 0.04992647096514702
train_iter_loss: 0.16420169174671173
train_iter_loss: 0.17196519672870636
train_iter_loss: 0.17010609805583954
train_iter_loss: 0.17060565948486328
train_iter_loss: 0.1500902771949768
train_iter_loss: 0.14979606866836548
train_iter_loss: 0.40809306502342224
train_iter_loss: 0.1512375921010971
train_iter_loss: 0.13703741133213043
train_iter_loss: 0.13016091287136078
train_iter_loss: 0.097510926425457
train_iter_loss: 0.08200480043888092
train_iter_loss: 0.16292054951190948
train_iter_loss: 0.0720316618680954
train_iter_loss: 0.1970701664686203
train_iter_loss: 0.15071704983711243
train_iter_loss: 0.10954967886209488
train_iter_loss: 0.05700015276670456
train_iter_loss: 0.17761282622814178
train_iter_loss: 0.11106725037097931
train_iter_loss: 0.13337332010269165
train_iter_loss: 0.11754484474658966
train_iter_loss: 0.11529828608036041
train_iter_loss: 0.1437278687953949
train_iter_loss: 0.1258818805217743
train_iter_loss: 0.10517621785402298
train_iter_loss: 0.09291905164718628
train_iter_loss: 0.10928170382976532
train_iter_loss: 0.08600548654794693
train_iter_loss: 0.18327279388904572
train_iter_loss: 0.16273023188114166
train_iter_loss: 0.1275911033153534
train_iter_loss: 0.17345799505710602
train_iter_loss: 0.20025017857551575
train_iter_loss: 0.11525604128837585
train_iter_loss: 0.3806312084197998
train_iter_loss: 0.14333845674991608
train_iter_loss: 0.10326237231492996
train loss :0.1592
---------------------
Validation seg loss: 0.21885033515018393 at epoch 643
epoch =    644/  1000, exp = train
train_iter_loss: 0.10831283777952194
train_iter_loss: 0.13908584415912628
train_iter_loss: 0.23864789307117462
train_iter_loss: 0.23864218592643738
train_iter_loss: 0.14882272481918335
train_iter_loss: 0.11952366679906845
train_iter_loss: 0.11633896827697754
train_iter_loss: 0.1586294323205948
train_iter_loss: 0.1779167205095291
train_iter_loss: 0.14485569298267365
train_iter_loss: 0.14210975170135498
train_iter_loss: 0.16422612965106964
train_iter_loss: 0.07168103009462357
train_iter_loss: 0.14560586214065552
train_iter_loss: 0.20263366401195526
train_iter_loss: 0.09500138461589813
train_iter_loss: 0.0933738425374031
train_iter_loss: 0.12764449417591095
train_iter_loss: 0.15765628218650818
train_iter_loss: 0.14172683656215668
train_iter_loss: 0.13500846922397614
train_iter_loss: 0.17080989480018616
train_iter_loss: 0.07178322970867157
train_iter_loss: 0.1242651641368866
train_iter_loss: 0.2395608276128769
train_iter_loss: 0.20874260365962982
train_iter_loss: 0.10377674549818039
train_iter_loss: 0.12914934754371643
train_iter_loss: 0.15611408650875092
train_iter_loss: 0.20539768040180206
train_iter_loss: 0.1735834777355194
train_iter_loss: 0.08531695604324341
train_iter_loss: 0.1167069748044014
train_iter_loss: 0.1962256282567978
train_iter_loss: 0.1766022890806198
train_iter_loss: 0.1317259669303894
train_iter_loss: 0.0879250168800354
train_iter_loss: 0.16653034090995789
train_iter_loss: 0.12479904294013977
train_iter_loss: 0.1694585382938385
train_iter_loss: 0.2550232708454132
train_iter_loss: 0.10816287994384766
train_iter_loss: 0.23949484527111053
train_iter_loss: 0.09518880397081375
train_iter_loss: 0.15253618359565735
train_iter_loss: 0.07882045954465866
train_iter_loss: 0.1433403640985489
train_iter_loss: 0.11108697205781937
train_iter_loss: 0.1910439282655716
train_iter_loss: 0.08813454210758209
train_iter_loss: 0.3749963641166687
train_iter_loss: 0.11289002001285553
train_iter_loss: 0.15245237946510315
train_iter_loss: 0.11162498593330383
train_iter_loss: 0.1300441324710846
train_iter_loss: 0.2363099306821823
train_iter_loss: 0.1520814150571823
train_iter_loss: 0.061334747821092606
train_iter_loss: 0.20167668163776398
train_iter_loss: 0.17350473999977112
train_iter_loss: 0.1270129531621933
train_iter_loss: 0.243197962641716
train_iter_loss: 0.2521352469921112
train_iter_loss: 0.14117376506328583
train_iter_loss: 0.1828744113445282
train_iter_loss: 0.09522029757499695
train_iter_loss: 0.1696314513683319
train_iter_loss: 0.1960195153951645
train_iter_loss: 0.23513875901699066
train_iter_loss: 0.10915238410234451
train_iter_loss: 0.09089257568120956
train_iter_loss: 0.1458948701620102
train_iter_loss: 0.13918472826480865
train_iter_loss: 0.07571332901716232
train_iter_loss: 0.09668713808059692
train_iter_loss: 0.19671159982681274
train_iter_loss: 0.23386122286319733
train_iter_loss: 0.09132444858551025
train_iter_loss: 0.13074012100696564
train_iter_loss: 0.23548384010791779
train_iter_loss: 0.18629325926303864
train_iter_loss: 0.11530174314975739
train_iter_loss: 0.2070506364107132
train_iter_loss: 0.18349480628967285
train_iter_loss: 0.11618150025606155
train_iter_loss: 0.1631377935409546
train_iter_loss: 0.17887990176677704
train_iter_loss: 0.2029953896999359
train_iter_loss: 0.1433200240135193
train_iter_loss: 0.1835617870092392
train_iter_loss: 0.08362533897161484
train_iter_loss: 0.20550863444805145
train_iter_loss: 0.1623149961233139
train_iter_loss: 0.11230570077896118
train_iter_loss: 0.16220596432685852
train_iter_loss: 0.10129299759864807
train_iter_loss: 0.1867702603340149
train_iter_loss: 0.10131753236055374
train_iter_loss: 0.17155277729034424
train_iter_loss: 0.06539519131183624
train loss :0.1546
---------------------
Validation seg loss: 0.21754647811593594 at epoch 644
epoch =    645/  1000, exp = train
train_iter_loss: 0.14732104539871216
train_iter_loss: 0.21955683827400208
train_iter_loss: 0.25541192293167114
train_iter_loss: 0.137356236577034
train_iter_loss: 0.12339906394481659
train_iter_loss: 0.1413816511631012
train_iter_loss: 0.2281978279352188
train_iter_loss: 0.15828801691532135
train_iter_loss: 0.0785166323184967
train_iter_loss: 0.07797649502754211
train_iter_loss: 0.23030558228492737
train_iter_loss: 0.21222183108329773
train_iter_loss: 0.32037827372550964
train_iter_loss: 0.24750429391860962
train_iter_loss: 0.11998403817415237
train_iter_loss: 0.14990904927253723
train_iter_loss: 0.28576362133026123
train_iter_loss: 0.10392601788043976
train_iter_loss: 0.08537044376134872
train_iter_loss: 0.1766674816608429
train_iter_loss: 0.20734228193759918
train_iter_loss: 0.10718076676130295
train_iter_loss: 0.054607685655355453
train_iter_loss: 0.14721649885177612
train_iter_loss: 0.14601998031139374
train_iter_loss: 0.19263526797294617
train_iter_loss: 0.14807696640491486
train_iter_loss: 0.22174538671970367
train_iter_loss: 0.10021241754293442
train_iter_loss: 0.14293977618217468
train_iter_loss: 0.09075892716646194
train_iter_loss: 0.19062097370624542
train_iter_loss: 0.29676544666290283
train_iter_loss: 0.1756206601858139
train_iter_loss: 0.05592905357480049
train_iter_loss: 0.12149027734994888
train_iter_loss: 0.06793341785669327
train_iter_loss: 0.09202244877815247
train_iter_loss: 0.15285393595695496
train_iter_loss: 0.10175526887178421
train_iter_loss: 0.15769752860069275
train_iter_loss: 0.16908517479896545
train_iter_loss: 0.23235578835010529
train_iter_loss: 0.1895095407962799
train_iter_loss: 0.13862432539463043
train_iter_loss: 0.16746050119400024
train_iter_loss: 0.15258103609085083
train_iter_loss: 0.07577560096979141
train_iter_loss: 0.1030048131942749
train_iter_loss: 0.1824655532836914
train_iter_loss: 0.25984302163124084
train_iter_loss: 0.15364119410514832
train_iter_loss: 0.05473910644650459
train_iter_loss: 0.24538780748844147
train_iter_loss: 0.10258317738771439
train_iter_loss: 0.18447273969650269
train_iter_loss: 0.13231047987937927
train_iter_loss: 0.20576223731040955
train_iter_loss: 0.1775692254304886
train_iter_loss: 0.23253558576107025
train_iter_loss: 0.12595240771770477
train_iter_loss: 0.053987886756658554
train_iter_loss: 0.19883088767528534
train_iter_loss: 0.09072104841470718
train_iter_loss: 0.12111406773328781
train_iter_loss: 0.14888842403888702
train_iter_loss: 0.09266941249370575
train_iter_loss: 0.1654079705476761
train_iter_loss: 0.21274222433567047
train_iter_loss: 0.14678247272968292
train_iter_loss: 0.19594192504882812
train_iter_loss: 0.1563827097415924
train_iter_loss: 0.26779618859291077
train_iter_loss: 0.06986766308546066
train_iter_loss: 0.24480246007442474
train_iter_loss: 0.0873480886220932
train_iter_loss: 0.2967519760131836
train_iter_loss: 0.18992578983306885
train_iter_loss: 0.14011767506599426
train_iter_loss: 0.1857813447713852
train_iter_loss: 0.1277296394109726
train_iter_loss: 0.10074114054441452
train_iter_loss: 0.23762772977352142
train_iter_loss: 0.1256970465183258
train_iter_loss: 0.14857034385204315
train_iter_loss: 0.16214242577552795
train_iter_loss: 0.12417284399271011
train_iter_loss: 0.19477954506874084
train_iter_loss: 0.16222374141216278
train_iter_loss: 0.15496328473091125
train_iter_loss: 0.08982650190591812
train_iter_loss: 0.12686344981193542
train_iter_loss: 0.14078496396541595
train_iter_loss: 0.10612858086824417
train_iter_loss: 0.1524132937192917
train_iter_loss: 0.34899529814720154
train_iter_loss: 0.10664055496454239
train_iter_loss: 0.10471231490373611
train_iter_loss: 0.05035999417304993
train_iter_loss: 0.1387440711259842
train loss :0.1588
---------------------
Validation seg loss: 0.22040875371158966 at epoch 645
epoch =    646/  1000, exp = train
train_iter_loss: 0.24790510535240173
train_iter_loss: 0.3267589807510376
train_iter_loss: 0.22502222657203674
train_iter_loss: 0.1115979477763176
train_iter_loss: 0.12459766119718552
train_iter_loss: 0.11290781944990158
train_iter_loss: 0.06749354302883148
train_iter_loss: 0.13953609764575958
train_iter_loss: 0.14246197044849396
train_iter_loss: 0.1367017924785614
train_iter_loss: 0.11116966605186462
train_iter_loss: 0.14091534912586212
train_iter_loss: 0.1992127150297165
train_iter_loss: 0.08233094215393066
train_iter_loss: 0.08216916024684906
train_iter_loss: 0.2006538063287735
train_iter_loss: 0.1277085393667221
train_iter_loss: 0.21327601373195648
train_iter_loss: 0.12002415955066681
train_iter_loss: 0.26465171575546265
train_iter_loss: 0.079247385263443
train_iter_loss: 0.2568657696247101
train_iter_loss: 0.14232563972473145
train_iter_loss: 0.11579642444849014
train_iter_loss: 0.12322655320167542
train_iter_loss: 0.13239628076553345
train_iter_loss: 0.14829415082931519
train_iter_loss: 0.1557268500328064
train_iter_loss: 0.15478357672691345
train_iter_loss: 0.1382627636194229
train_iter_loss: 0.11792713403701782
train_iter_loss: 0.09759055823087692
train_iter_loss: 0.156685471534729
train_iter_loss: 0.06377049535512924
train_iter_loss: 0.2955493628978729
train_iter_loss: 0.08588282763957977
train_iter_loss: 0.15392443537712097
train_iter_loss: 0.0277192872017622
train_iter_loss: 0.21225929260253906
train_iter_loss: 0.17485307157039642
train_iter_loss: 0.15583816170692444
train_iter_loss: 0.09813354909420013
train_iter_loss: 0.20482973754405975
train_iter_loss: 0.28080305457115173
train_iter_loss: 0.1080433577299118
train_iter_loss: 0.4235292077064514
train_iter_loss: 0.09610243141651154
train_iter_loss: 0.1938573122024536
train_iter_loss: 0.3151588439941406
train_iter_loss: 0.20454896986484528
train_iter_loss: 0.16273152828216553
train_iter_loss: 0.11559442430734634
train_iter_loss: 0.09273207932710648
train_iter_loss: 0.12630489468574524
train_iter_loss: 0.17344436049461365
train_iter_loss: 0.09791361540555954
train_iter_loss: 0.20646429061889648
train_iter_loss: 0.16026970744132996
train_iter_loss: 0.11961741745471954
train_iter_loss: 0.16273440420627594
train_iter_loss: 0.14567311108112335
train_iter_loss: 0.2226058840751648
train_iter_loss: 0.06082819774746895
train_iter_loss: 0.1428023874759674
train_iter_loss: 0.15739959478378296
train_iter_loss: 0.22488349676132202
train_iter_loss: 0.04030750319361687
train_iter_loss: 0.29807281494140625
train_iter_loss: 0.09413695335388184
train_iter_loss: 0.09557238221168518
train_iter_loss: 0.09647613763809204
train_iter_loss: 0.13382677733898163
train_iter_loss: 0.10585574060678482
train_iter_loss: 0.10456759482622147
train_iter_loss: 0.11908247321844101
train_iter_loss: 0.10250242054462433
train_iter_loss: 0.11703800410032272
train_iter_loss: 0.18324148654937744
train_iter_loss: 0.16110071539878845
train_iter_loss: 0.37408995628356934
train_iter_loss: 0.14671452343463898
train_iter_loss: 0.12289437651634216
train_iter_loss: 0.23432379961013794
train_iter_loss: 0.1059807762503624
train_iter_loss: 0.3940870463848114
train_iter_loss: 0.08070695400238037
train_iter_loss: 0.10371555387973785
train_iter_loss: 0.06188041716814041
train_iter_loss: 0.19305075705051422
train_iter_loss: 0.19178982079029083
train_iter_loss: 0.11280481517314911
train_iter_loss: 0.11615699529647827
train_iter_loss: 0.11654943972826004
train_iter_loss: 0.2223537117242813
train_iter_loss: 0.11683277785778046
train_iter_loss: 0.12675148248672485
train_iter_loss: 0.2076238989830017
train_iter_loss: 0.17696957290172577
train_iter_loss: 0.10945327579975128
train_iter_loss: 0.17116297781467438
train loss :0.1576
---------------------
Validation seg loss: 0.21820869426822886 at epoch 646
epoch =    647/  1000, exp = train
train_iter_loss: 0.30021801590919495
train_iter_loss: 0.27868232131004333
train_iter_loss: 0.10510057955980301
train_iter_loss: 0.13501399755477905
train_iter_loss: 0.18620359897613525
train_iter_loss: 0.14428940415382385
train_iter_loss: 0.13443775475025177
train_iter_loss: 0.15351518988609314
train_iter_loss: 0.1465645432472229
train_iter_loss: 0.39451730251312256
train_iter_loss: 0.0991135835647583
train_iter_loss: 0.195450097322464
train_iter_loss: 0.09995884448289871
train_iter_loss: 0.1185028925538063
train_iter_loss: 0.14074906706809998
train_iter_loss: 0.1712455302476883
train_iter_loss: 0.08024809509515762
train_iter_loss: 0.09898371994495392
train_iter_loss: 0.1753196120262146
train_iter_loss: 0.23377478122711182
train_iter_loss: 0.293270468711853
train_iter_loss: 0.16549567878246307
train_iter_loss: 0.07501357048749924
train_iter_loss: 0.0955108255147934
train_iter_loss: 0.0856810212135315
train_iter_loss: 0.15175507962703705
train_iter_loss: 0.1936294138431549
train_iter_loss: 0.08452273160219193
train_iter_loss: 0.190717875957489
train_iter_loss: 0.13238227367401123
train_iter_loss: 0.16640208661556244
train_iter_loss: 0.13697555661201477
train_iter_loss: 0.142377570271492
train_iter_loss: 0.20733708143234253
train_iter_loss: 0.10940320789813995
train_iter_loss: 0.3153427243232727
train_iter_loss: 0.13030391931533813
train_iter_loss: 0.11461129039525986
train_iter_loss: 0.18011575937271118
train_iter_loss: 0.23170699179172516
train_iter_loss: 0.1963929682970047
train_iter_loss: 0.14249517023563385
train_iter_loss: 0.0893542692065239
train_iter_loss: 0.14950090646743774
train_iter_loss: 0.09703521430492401
train_iter_loss: 0.3002440631389618
train_iter_loss: 0.14268414676189423
train_iter_loss: 0.15418557822704315
train_iter_loss: 0.18590053915977478
train_iter_loss: 0.15408889949321747
train_iter_loss: 0.3400743901729584
train_iter_loss: 0.09565410763025284
train_iter_loss: 0.12047844380140305
train_iter_loss: 0.1462937295436859
train_iter_loss: 0.1160358190536499
train_iter_loss: 0.11332900077104568
train_iter_loss: 0.07439150661230087
train_iter_loss: 0.149186909198761
train_iter_loss: 0.2350141555070877
train_iter_loss: 0.20093807578086853
train_iter_loss: 0.15676230192184448
train_iter_loss: 0.2842581868171692
train_iter_loss: 0.15272067487239838
train_iter_loss: 0.09662149101495743
train_iter_loss: 0.14276684820652008
train_iter_loss: 0.1332903951406479
train_iter_loss: 0.15871278941631317
train_iter_loss: 0.217643603682518
train_iter_loss: 0.11906798183917999
train_iter_loss: 0.11425517499446869
train_iter_loss: 0.1415518969297409
train_iter_loss: 0.16717509925365448
train_iter_loss: 0.22577673196792603
train_iter_loss: 0.11304350942373276
train_iter_loss: 0.1946178674697876
train_iter_loss: 0.1784539818763733
train_iter_loss: 0.1004587784409523
train_iter_loss: 0.20059658586978912
train_iter_loss: 0.2065984308719635
train_iter_loss: 0.18566693365573883
train_iter_loss: 0.20784161984920502
train_iter_loss: 0.1719997525215149
train_iter_loss: 0.13789419829845428
train_iter_loss: 0.1621263474225998
train_iter_loss: 0.12687504291534424
train_iter_loss: 0.14194540679454803
train_iter_loss: 0.15856707096099854
train_iter_loss: 0.09976932406425476
train_iter_loss: 0.0845847800374031
train_iter_loss: 0.2152366191148758
train_iter_loss: 0.14402620494365692
train_iter_loss: 0.30559277534484863
train_iter_loss: 0.20262360572814941
train_iter_loss: 0.07556228339672089
train_iter_loss: 0.09792446345090866
train_iter_loss: 0.08626116812229156
train_iter_loss: 0.11236363649368286
train_iter_loss: 0.20071996748447418
train_iter_loss: 0.2682274878025055
train_iter_loss: 0.10112849622964859
train loss :0.1635
---------------------
Validation seg loss: 0.21966537834092412 at epoch 647
epoch =    648/  1000, exp = train
train_iter_loss: 0.11826306581497192
train_iter_loss: 0.19514483213424683
train_iter_loss: 0.1333753913640976
train_iter_loss: 0.12036900222301483
train_iter_loss: 0.19741789996623993
train_iter_loss: 0.10972723364830017
train_iter_loss: 0.09532949328422546
train_iter_loss: 0.12537267804145813
train_iter_loss: 0.20320558547973633
train_iter_loss: 0.12954869866371155
train_iter_loss: 0.12301033735275269
train_iter_loss: 0.1369291990995407
train_iter_loss: 0.11189953237771988
train_iter_loss: 0.14709410071372986
train_iter_loss: 0.17321933805942535
train_iter_loss: 0.13855576515197754
train_iter_loss: 0.15946708619594574
train_iter_loss: 0.1456383466720581
train_iter_loss: 0.1598498374223709
train_iter_loss: 0.094044029712677
train_iter_loss: 0.17425920069217682
train_iter_loss: 0.19264662265777588
train_iter_loss: 0.15946675837039948
train_iter_loss: 0.16215378046035767
train_iter_loss: 0.17521043121814728
train_iter_loss: 0.17217634618282318
train_iter_loss: 0.2039465755224228
train_iter_loss: 0.14732511341571808
train_iter_loss: 0.09847544878721237
train_iter_loss: 0.08424676954746246
train_iter_loss: 0.2577906548976898
train_iter_loss: 0.16181235015392303
train_iter_loss: 0.11108986288309097
train_iter_loss: 0.15643595159053802
train_iter_loss: 0.0972490981221199
train_iter_loss: 0.10282213985919952
train_iter_loss: 0.17238037288188934
train_iter_loss: 0.15459613502025604
train_iter_loss: 0.15198171138763428
train_iter_loss: 0.12624399363994598
train_iter_loss: 0.2527603805065155
train_iter_loss: 0.1387193500995636
train_iter_loss: 0.1394863873720169
train_iter_loss: 0.1494598388671875
train_iter_loss: 0.09047205001115799
train_iter_loss: 0.18136224150657654
train_iter_loss: 0.22313939034938812
train_iter_loss: 0.08606215566396713
train_iter_loss: 0.09955191612243652
train_iter_loss: 0.22414958477020264
train_iter_loss: 0.15352430939674377
train_iter_loss: 0.05988290160894394
train_iter_loss: 0.20732109248638153
train_iter_loss: 0.14523719251155853
train_iter_loss: 0.25711363554000854
train_iter_loss: 0.20717695355415344
train_iter_loss: 0.23187564313411713
train_iter_loss: 0.08135759085416794
train_iter_loss: 0.21155743300914764
train_iter_loss: 0.05658326297998428
train_iter_loss: 0.27414166927337646
train_iter_loss: 0.0751318484544754
train_iter_loss: 0.1303873509168625
train_iter_loss: 0.10488411039113998
train_iter_loss: 0.1849392056465149
train_iter_loss: 0.25076135993003845
train_iter_loss: 0.25257405638694763
train_iter_loss: 0.11630728095769882
train_iter_loss: 0.10054314136505127
train_iter_loss: 0.23611441254615784
train_iter_loss: 0.10476703941822052
train_iter_loss: 0.163031205534935
train_iter_loss: 0.03645211085677147
train_iter_loss: 0.21866214275360107
train_iter_loss: 0.3137541115283966
train_iter_loss: 0.311197966337204
train_iter_loss: 0.1700068861246109
train_iter_loss: 0.09146053344011307
train_iter_loss: 0.08933622390031815
train_iter_loss: 0.10536755621433258
train_iter_loss: 0.19122235476970673
train_iter_loss: 0.10630292445421219
train_iter_loss: 0.2771387994289398
train_iter_loss: 0.1471446305513382
train_iter_loss: 0.1456637680530548
train_iter_loss: 0.13144242763519287
train_iter_loss: 0.10003084689378738
train_iter_loss: 0.0723533108830452
train_iter_loss: 0.06534232944250107
train_iter_loss: 0.09749624133110046
train_iter_loss: 0.05448218435049057
train_iter_loss: 0.25304099917411804
train_iter_loss: 0.09817090630531311
train_iter_loss: 0.1222144067287445
train_iter_loss: 0.2734781801700592
train_iter_loss: 0.17764566838741302
train_iter_loss: 0.19621707499027252
train_iter_loss: 0.1851453334093094
train_iter_loss: 0.06294683367013931
train_iter_loss: 0.25750842690467834
train loss :0.1558
---------------------
Validation seg loss: 0.22107058763504028 at epoch 648
epoch =    649/  1000, exp = train
train_iter_loss: 0.06427491456270218
train_iter_loss: 0.1395711600780487
train_iter_loss: 0.03992973268032074
train_iter_loss: 0.11902812868356705
train_iter_loss: 0.14473648369312286
train_iter_loss: 0.36136820912361145
train_iter_loss: 0.2061426192522049
train_iter_loss: 0.09202736616134644
train_iter_loss: 0.2988412082195282
train_iter_loss: 0.14095859229564667
train_iter_loss: 0.22324657440185547
train_iter_loss: 0.09760952740907669
train_iter_loss: 0.14831849932670593
train_iter_loss: 0.18254274129867554
train_iter_loss: 0.08373867720365524
train_iter_loss: 0.19943033158779144
train_iter_loss: 0.2773243188858032
train_iter_loss: 0.16048280894756317
train_iter_loss: 0.09917226433753967
train_iter_loss: 0.15066097676753998
train_iter_loss: 0.13986359536647797
train_iter_loss: 0.14503441751003265
train_iter_loss: 0.25831717252731323
train_iter_loss: 0.13252021372318268
train_iter_loss: 0.18366719782352448
train_iter_loss: 0.1997387558221817
train_iter_loss: 0.18920327723026276
train_iter_loss: 0.05222982540726662
train_iter_loss: 0.22945888340473175
train_iter_loss: 0.27961108088493347
train_iter_loss: 0.08973623067140579
train_iter_loss: 0.1353498250246048
train_iter_loss: 0.17040081322193146
train_iter_loss: 0.13450057804584503
train_iter_loss: 0.07718633115291595
train_iter_loss: 0.141723170876503
train_iter_loss: 0.18959344923496246
train_iter_loss: 0.26398810744285583
train_iter_loss: 0.08288366347551346
train_iter_loss: 0.05768021196126938
train_iter_loss: 0.15350481867790222
train_iter_loss: 0.15260541439056396
train_iter_loss: 0.16938786208629608
train_iter_loss: 0.1615561544895172
train_iter_loss: 0.05727483332157135
train_iter_loss: 0.17617172002792358
train_iter_loss: 0.1303437054157257
train_iter_loss: 0.17201851308345795
train_iter_loss: 0.14770182967185974
train_iter_loss: 0.15447144210338593
train_iter_loss: 0.13534937798976898
train_iter_loss: 0.2485092431306839
train_iter_loss: 0.15898844599723816
train_iter_loss: 0.16927926242351532
train_iter_loss: 0.2931828200817108
train_iter_loss: 0.16630424559116364
train_iter_loss: 0.1470937281847
train_iter_loss: 0.20104317367076874
train_iter_loss: 0.11820992827415466
train_iter_loss: 0.10127691179513931
train_iter_loss: 0.1353430449962616
train_iter_loss: 0.11305695027112961
train_iter_loss: 0.12285150587558746
train_iter_loss: 0.15553422272205353
train_iter_loss: 0.09640206396579742
train_iter_loss: 0.21114517748355865
train_iter_loss: 0.0819162130355835
train_iter_loss: 0.2786402404308319
train_iter_loss: 0.2062491774559021
train_iter_loss: 0.17949278652668
train_iter_loss: 0.11522328108549118
train_iter_loss: 0.1575741320848465
train_iter_loss: 0.10464217513799667
train_iter_loss: 0.24284760653972626
train_iter_loss: 0.1726556420326233
train_iter_loss: 0.07880634814500809
train_iter_loss: 0.06967466324567795
train_iter_loss: 0.1781737208366394
train_iter_loss: 0.11526043713092804
train_iter_loss: 0.17790021002292633
train_iter_loss: 0.18258722126483917
train_iter_loss: 0.1598932296037674
train_iter_loss: 0.14335891604423523
train_iter_loss: 0.17514434456825256
train_iter_loss: 0.22052250802516937
train_iter_loss: 0.14473597705364227
train_iter_loss: 0.3277204632759094
train_iter_loss: 0.11560988426208496
train_iter_loss: 0.12189614772796631
train_iter_loss: 0.22243458032608032
train_iter_loss: 0.10967595875263214
train_iter_loss: 0.16189344227313995
train_iter_loss: 0.16939224302768707
train_iter_loss: 0.11394785344600677
train_iter_loss: 0.20891092717647552
train_iter_loss: 0.0726110190153122
train_iter_loss: 0.09668649733066559
train_iter_loss: 0.1873936653137207
train_iter_loss: 0.08346997946500778
train_iter_loss: 0.1202756017446518
train loss :0.1588
---------------------
Validation seg loss: 0.21417749370008987 at epoch 649
epoch =    650/  1000, exp = train
train_iter_loss: 0.10447169840335846
train_iter_loss: 0.14040395617485046
train_iter_loss: 0.2520696520805359
train_iter_loss: 0.14898955821990967
train_iter_loss: 0.22845196723937988
train_iter_loss: 0.21097280085086823
train_iter_loss: 0.12332476675510406
train_iter_loss: 0.08279454708099365
train_iter_loss: 0.12944242358207703
train_iter_loss: 0.20178888738155365
train_iter_loss: 0.2004249542951584
train_iter_loss: 0.14501087367534637
train_iter_loss: 0.14810650050640106
train_iter_loss: 0.08894170820713043
train_iter_loss: 0.3036130368709564
train_iter_loss: 0.2407766729593277
train_iter_loss: 0.20368137955665588
train_iter_loss: 0.10760147124528885
train_iter_loss: 0.1983085423707962
train_iter_loss: 0.20145438611507416
train_iter_loss: 0.12657414376735687
train_iter_loss: 0.10952075570821762
train_iter_loss: 0.18278075754642487
train_iter_loss: 0.17811954021453857
train_iter_loss: 0.23079192638397217
train_iter_loss: 0.1968044489622116
train_iter_loss: 0.2590152621269226
train_iter_loss: 0.12418782711029053
train_iter_loss: 0.08180578798055649
train_iter_loss: 0.03135814890265465
train_iter_loss: 0.18703095614910126
train_iter_loss: 0.119792640209198
train_iter_loss: 0.11934791505336761
train_iter_loss: 0.16261036694049835
train_iter_loss: 0.14758770167827606
train_iter_loss: 0.17633703351020813
train_iter_loss: 0.224028080701828
train_iter_loss: 0.11772875487804413
train_iter_loss: 0.17998157441616058
train_iter_loss: 0.3269328474998474
train_iter_loss: 0.14891348779201508
train_iter_loss: 0.1579156517982483
train_iter_loss: 0.23672500252723694
train_iter_loss: 0.13852331042289734
train_iter_loss: 0.2272881120443344
train_iter_loss: 0.17382070422172546
train_iter_loss: 0.18673743307590485
train_iter_loss: 0.1925128698348999
train_iter_loss: 0.14001347124576569
train_iter_loss: 0.15269148349761963
train_iter_loss: 0.10401877760887146
train_iter_loss: 0.2916063964366913
train_iter_loss: 0.13917872309684753
train_iter_loss: 0.04659966006875038
train_iter_loss: 0.22398696839809418
train_iter_loss: 0.2557143270969391
train_iter_loss: 0.1659730076789856
train_iter_loss: 0.28518351912498474
train_iter_loss: 0.09495792537927628
train_iter_loss: 0.18019895255565643
train_iter_loss: 0.1762382835149765
train_iter_loss: 0.18120276927947998
train_iter_loss: 0.17452897131443024
train_iter_loss: 0.193156898021698
train_iter_loss: 0.10959546267986298
train_iter_loss: 0.23171931505203247
train_iter_loss: 0.213038831949234
train_iter_loss: 0.11977896839380264
train_iter_loss: 0.15754376351833344
train_iter_loss: 0.09167157113552094
train_iter_loss: 0.13242274522781372
train_iter_loss: 0.11060736328363419
train_iter_loss: 0.12482073903083801
train_iter_loss: 0.06436865776777267
train_iter_loss: 0.11055967211723328
train_iter_loss: 0.12412435561418533
train_iter_loss: 0.1667044460773468
train_iter_loss: 0.15959849953651428
train_iter_loss: 0.13257285952568054
train_iter_loss: 0.21944516897201538
train_iter_loss: 0.07527774572372437
train_iter_loss: 0.28414031863212585
train_iter_loss: 0.09309843182563782
train_iter_loss: 0.3673293888568878
train_iter_loss: 0.13132458925247192
train_iter_loss: 0.13856805860996246
train_iter_loss: 0.21871593594551086
train_iter_loss: 0.14162051677703857
train_iter_loss: 0.18488746881484985
train_iter_loss: 0.12362129241228104
train_iter_loss: 0.19117051362991333
train_iter_loss: 0.0763322114944458
train_iter_loss: 0.10229754447937012
train_iter_loss: 0.1226751059293747
train_iter_loss: 0.149105042219162
train_iter_loss: 0.1904023140668869
train_iter_loss: 0.19354109466075897
train_iter_loss: 0.09438718110322952
train_iter_loss: 0.08670604228973389
train_iter_loss: 0.08479639887809753
train loss :0.1649
---------------------
Validation seg loss: 0.2162180879539898 at epoch 650
epoch =    651/  1000, exp = train
train_iter_loss: 0.1320381760597229
train_iter_loss: 0.13751432299613953
train_iter_loss: 0.13443931937217712
train_iter_loss: 0.15619570016860962
train_iter_loss: 0.16083775460720062
train_iter_loss: 0.09156838804483414
train_iter_loss: 0.13163213431835175
train_iter_loss: 0.2032402753829956
train_iter_loss: 0.10572203248739243
train_iter_loss: 0.18847863376140594
train_iter_loss: 0.07216642796993256
train_iter_loss: 0.13655327260494232
train_iter_loss: 0.11515183001756668
train_iter_loss: 0.14297209680080414
train_iter_loss: 0.12782137095928192
train_iter_loss: 0.1640980839729309
train_iter_loss: 0.1490924209356308
train_iter_loss: 0.18237651884555817
train_iter_loss: 0.11708204448223114
train_iter_loss: 0.4032640755176544
train_iter_loss: 0.14364951848983765
train_iter_loss: 0.2561485171318054
train_iter_loss: 0.24668218195438385
train_iter_loss: 0.1504087895154953
train_iter_loss: 0.17536400258541107
train_iter_loss: 0.10027693957090378
train_iter_loss: 0.19118182361125946
train_iter_loss: 0.15527094900608063
train_iter_loss: 0.12957261502742767
train_iter_loss: 0.11000247299671173
train_iter_loss: 0.13418187201023102
train_iter_loss: 0.1942085176706314
train_iter_loss: 0.1080678254365921
train_iter_loss: 0.1226755902171135
train_iter_loss: 0.11857772618532181
train_iter_loss: 0.16319388151168823
train_iter_loss: 0.32026833295822144
train_iter_loss: 0.06836428493261337
train_iter_loss: 0.12825463712215424
train_iter_loss: 0.1970420926809311
train_iter_loss: 0.11112313717603683
train_iter_loss: 0.24744951725006104
train_iter_loss: 0.1305452436208725
train_iter_loss: 0.07268630713224411
train_iter_loss: 0.19001968204975128
train_iter_loss: 0.09010307490825653
train_iter_loss: 0.19669224321842194
train_iter_loss: 0.15929031372070312
train_iter_loss: 0.4073367416858673
train_iter_loss: 0.14262966811656952
train_iter_loss: 0.3928900957107544
train_iter_loss: 0.08377742767333984
train_iter_loss: 0.18853873014450073
train_iter_loss: 0.07526012510061264
train_iter_loss: 0.17914557456970215
train_iter_loss: 0.11829836666584015
train_iter_loss: 0.11767325550317764
train_iter_loss: 0.31134259700775146
train_iter_loss: 0.21056689321994781
train_iter_loss: 0.07840771228075027
train_iter_loss: 0.11549904197454453
train_iter_loss: 0.11727843433618546
train_iter_loss: 0.19188754260540009
train_iter_loss: 0.0893859788775444
train_iter_loss: 0.11743292957544327
train_iter_loss: 0.1458185762166977
train_iter_loss: 0.1451745480298996
train_iter_loss: 0.2058710753917694
train_iter_loss: 0.2233593612909317
train_iter_loss: 0.15811286866664886
train_iter_loss: 0.14239710569381714
train_iter_loss: 0.07714232802391052
train_iter_loss: 0.17439855635166168
train_iter_loss: 0.17489805817604065
train_iter_loss: 0.11494255065917969
train_iter_loss: 0.20337316393852234
train_iter_loss: 0.07479004561901093
train_iter_loss: 0.09868757426738739
train_iter_loss: 0.14493556320667267
train_iter_loss: 0.14208203554153442
train_iter_loss: 0.15709443390369415
train_iter_loss: 0.050411026924848557
train_iter_loss: 0.13979212939739227
train_iter_loss: 0.14351648092269897
train_iter_loss: 0.11732544749975204
train_iter_loss: 0.14216850697994232
train_iter_loss: 0.12900222837924957
train_iter_loss: 0.18491674959659576
train_iter_loss: 0.15846218168735504
train_iter_loss: 0.11883772909641266
train_iter_loss: 0.2371649593114853
train_iter_loss: 0.15261195600032806
train_iter_loss: 0.17515569925308228
train_iter_loss: 0.19076742231845856
train_iter_loss: 0.14921574294567108
train_iter_loss: 0.23329029977321625
train_iter_loss: 0.1140357106924057
train_iter_loss: 0.17098036408424377
train_iter_loss: 0.0660485029220581
train_iter_loss: 0.12388602644205093
train loss :0.1575
---------------------
Validation seg loss: 0.21719911064566025 at epoch 651
epoch =    652/  1000, exp = train
train_iter_loss: 0.2046787142753601
train_iter_loss: 0.08517451584339142
train_iter_loss: 0.15300409495830536
train_iter_loss: 0.1751834899187088
train_iter_loss: 0.11187681555747986
train_iter_loss: 0.2616647481918335
train_iter_loss: 0.1303132176399231
train_iter_loss: 0.1612757295370102
train_iter_loss: 0.14597132802009583
train_iter_loss: 0.14100930094718933
train_iter_loss: 0.14685428142547607
train_iter_loss: 0.1506097912788391
train_iter_loss: 0.1400143951177597
train_iter_loss: 0.11344395577907562
train_iter_loss: 0.1917770802974701
train_iter_loss: 0.13700158894062042
train_iter_loss: 0.2396233230829239
train_iter_loss: 0.24402770400047302
train_iter_loss: 0.10378004610538483
train_iter_loss: 0.21238180994987488
train_iter_loss: 0.16297274827957153
train_iter_loss: 0.07043363898992538
train_iter_loss: 0.1378234624862671
train_iter_loss: 0.13312861323356628
train_iter_loss: 0.07302166521549225
train_iter_loss: 0.14839570224285126
train_iter_loss: 0.11142244935035706
train_iter_loss: 0.20931503176689148
train_iter_loss: 0.16718976199626923
train_iter_loss: 0.06612196564674377
train_iter_loss: 0.1396133005619049
train_iter_loss: 0.06100161746144295
train_iter_loss: 0.14477330446243286
train_iter_loss: 0.12874896824359894
train_iter_loss: 0.09184297919273376
train_iter_loss: 0.08977730572223663
train_iter_loss: 0.15278612077236176
train_iter_loss: 0.15538212656974792
train_iter_loss: 0.10545926541090012
train_iter_loss: 0.2768488824367523
train_iter_loss: 0.16545285284519196
train_iter_loss: 0.20700068771839142
train_iter_loss: 0.16167627274990082
train_iter_loss: 0.1619950532913208
train_iter_loss: 0.17675699293613434
train_iter_loss: 0.1232326403260231
train_iter_loss: 0.20394088327884674
train_iter_loss: 0.1378149390220642
train_iter_loss: 0.256997674703598
train_iter_loss: 0.13399899005889893
train_iter_loss: 0.17703506350517273
train_iter_loss: 0.10601844638586044
train_iter_loss: 0.13036702573299408
train_iter_loss: 0.11146300286054611
train_iter_loss: 0.2751544117927551
train_iter_loss: 0.14960554242134094
train_iter_loss: 0.17819428443908691
train_iter_loss: 0.12925879657268524
train_iter_loss: 0.19505192339420319
train_iter_loss: 0.13964751362800598
train_iter_loss: 0.21259167790412903
train_iter_loss: 0.13670028746128082
train_iter_loss: 0.15080028772354126
train_iter_loss: 0.16989271342754364
train_iter_loss: 0.14066609740257263
train_iter_loss: 0.14646901190280914
train_iter_loss: 0.08947142213582993
train_iter_loss: 0.1528327465057373
train_iter_loss: 0.19108915328979492
train_iter_loss: 0.12404261529445648
train_iter_loss: 0.1057865247130394
train_iter_loss: 0.0605846643447876
train_iter_loss: 0.12272299081087112
train_iter_loss: 0.09874685108661652
train_iter_loss: 0.23685726523399353
train_iter_loss: 0.20372498035430908
train_iter_loss: 0.2575051784515381
train_iter_loss: 0.3239152729511261
train_iter_loss: 0.11065559834241867
train_iter_loss: 0.07549218088388443
train_iter_loss: 0.040939245373010635
train_iter_loss: 0.13145729899406433
train_iter_loss: 0.045579634606838226
train_iter_loss: 0.19577661156654358
train_iter_loss: 0.2401876300573349
train_iter_loss: 0.031043093651533127
train_iter_loss: 0.23625703155994415
train_iter_loss: 0.07000146806240082
train_iter_loss: 0.12012700736522675
train_iter_loss: 0.28904300928115845
train_iter_loss: 0.1808108687400818
train_iter_loss: 0.13951627910137177
train_iter_loss: 0.22194865345954895
train_iter_loss: 0.19802916049957275
train_iter_loss: 0.12413612753152847
train_iter_loss: 0.1166030764579773
train_iter_loss: 0.39996638894081116
train_iter_loss: 0.17555582523345947
train_iter_loss: 0.11672983318567276
train_iter_loss: 0.2993328869342804
train loss :0.1584
---------------------
Validation seg loss: 0.21936151068130472 at epoch 652
epoch =    653/  1000, exp = train
train_iter_loss: 0.11306224763393402
train_iter_loss: 0.1284654289484024
train_iter_loss: 0.1316269040107727
train_iter_loss: 0.13324739038944244
train_iter_loss: 0.17260588705539703
train_iter_loss: 0.15843340754508972
train_iter_loss: 0.21029610931873322
train_iter_loss: 0.1603405475616455
train_iter_loss: 0.11539838463068008
train_iter_loss: 0.190750390291214
train_iter_loss: 0.09888499975204468
train_iter_loss: 0.1112547516822815
train_iter_loss: 0.2024727314710617
train_iter_loss: 0.22886447608470917
train_iter_loss: 0.15744277834892273
train_iter_loss: 0.11191069334745407
train_iter_loss: 0.20512230694293976
train_iter_loss: 0.14841850101947784
train_iter_loss: 0.11797754466533661
train_iter_loss: 0.1402973085641861
train_iter_loss: 0.06620689481496811
train_iter_loss: 0.19498613476753235
train_iter_loss: 0.14680549502372742
train_iter_loss: 0.16610272228717804
train_iter_loss: 0.2743836045265198
train_iter_loss: 0.3177470862865448
train_iter_loss: 0.10835114121437073
train_iter_loss: 0.2237785905599594
train_iter_loss: 0.21582499146461487
train_iter_loss: 0.1622226983308792
train_iter_loss: 0.14348599314689636
train_iter_loss: 0.11772830039262772
train_iter_loss: 0.2524264454841614
train_iter_loss: 0.18147428333759308
train_iter_loss: 0.26184412837028503
train_iter_loss: 0.18490195274353027
train_iter_loss: 0.15470927953720093
train_iter_loss: 0.07851963490247726
train_iter_loss: 0.32299110293388367
train_iter_loss: 0.11566471308469772
train_iter_loss: 0.1621202975511551
train_iter_loss: 0.16946300864219666
train_iter_loss: 0.22140002250671387
train_iter_loss: 0.07465781271457672
train_iter_loss: 0.24317406117916107
train_iter_loss: 0.10515569150447845
train_iter_loss: 0.17001976072788239
train_iter_loss: 0.09032995253801346
train_iter_loss: 0.07988937944173813
train_iter_loss: 0.0752405971288681
train_iter_loss: 0.09781018644571304
train_iter_loss: 0.1387636959552765
train_iter_loss: 0.23389141261577606
train_iter_loss: 0.1340872049331665
train_iter_loss: 0.18326370418071747
train_iter_loss: 0.10258887708187103
train_iter_loss: 0.2645642161369324
train_iter_loss: 0.12501482665538788
train_iter_loss: 0.17340216040611267
train_iter_loss: 0.0948677733540535
train_iter_loss: 0.23537851870059967
train_iter_loss: 0.30798661708831787
train_iter_loss: 0.1362488567829132
train_iter_loss: 0.10693060606718063
train_iter_loss: 0.04799138754606247
train_iter_loss: 0.11923396587371826
train_iter_loss: 0.11052582412958145
train_iter_loss: 0.23617418110370636
train_iter_loss: 0.11244481056928635
train_iter_loss: 0.07627265155315399
train_iter_loss: 0.1282908171415329
train_iter_loss: 0.15410088002681732
train_iter_loss: 0.10191026329994202
train_iter_loss: 0.17610350251197815
train_iter_loss: 0.27522802352905273
train_iter_loss: 0.12025473266839981
train_iter_loss: 0.28860726952552795
train_iter_loss: 0.13154824078083038
train_iter_loss: 0.15773867070674896
train_iter_loss: 0.16932587325572968
train_iter_loss: 0.12017697840929031
train_iter_loss: 0.37025800347328186
train_iter_loss: 0.150259330868721
train_iter_loss: 0.1176806390285492
train_iter_loss: 0.0853305533528328
train_iter_loss: 0.21338234841823578
train_iter_loss: 0.2144174873828888
train_iter_loss: 0.10174739360809326
train_iter_loss: 0.1707451343536377
train_iter_loss: 0.10980911552906036
train_iter_loss: 0.20544779300689697
train_iter_loss: 0.21667039394378662
train_iter_loss: 0.19070585072040558
train_iter_loss: 0.17920008301734924
train_iter_loss: 0.08165533095598221
train_iter_loss: 0.08297528326511383
train_iter_loss: 0.11748097836971283
train_iter_loss: 0.07498335838317871
train_iter_loss: 0.07757430523633957
train_iter_loss: 0.13342097401618958
train loss :0.1603
---------------------
Validation seg loss: 0.22038034124756758 at epoch 653
epoch =    654/  1000, exp = train
train_iter_loss: 0.31750497221946716
train_iter_loss: 0.11144112795591354
train_iter_loss: 0.1777065545320511
train_iter_loss: 0.14754155278205872
train_iter_loss: 0.1572849452495575
train_iter_loss: 0.11565842479467392
train_iter_loss: 0.1014217883348465
train_iter_loss: 0.08602765202522278
train_iter_loss: 0.1742102950811386
train_iter_loss: 0.13783106207847595
train_iter_loss: 0.14826038479804993
train_iter_loss: 0.18723201751708984
train_iter_loss: 0.21173255145549774
train_iter_loss: 0.14676900207996368
train_iter_loss: 0.15042702853679657
train_iter_loss: 0.08012749999761581
train_iter_loss: 0.232882559299469
train_iter_loss: 0.17370052635669708
train_iter_loss: 0.18498900532722473
train_iter_loss: 0.07995539903640747
train_iter_loss: 0.23379629850387573
train_iter_loss: 0.1450459361076355
train_iter_loss: 0.2883952558040619
train_iter_loss: 0.14524175226688385
train_iter_loss: 0.16774526238441467
train_iter_loss: 0.12111292779445648
train_iter_loss: 0.1385228931903839
train_iter_loss: 0.19983908534049988
train_iter_loss: 0.12755921483039856
train_iter_loss: 0.0793902650475502
train_iter_loss: 0.10368970036506653
train_iter_loss: 0.1659081131219864
train_iter_loss: 0.18854336440563202
train_iter_loss: 0.19747298955917358
train_iter_loss: 0.06581640988588333
train_iter_loss: 0.10590609908103943
train_iter_loss: 0.1667482703924179
train_iter_loss: 0.26229751110076904
train_iter_loss: 0.13910329341888428
train_iter_loss: 0.2161741703748703
train_iter_loss: 0.2340465933084488
train_iter_loss: 0.08864879608154297
train_iter_loss: 0.17138272523880005
train_iter_loss: 0.05541926249861717
train_iter_loss: 0.1461094319820404
train_iter_loss: 0.1708616018295288
train_iter_loss: 0.0932767316699028
train_iter_loss: 0.08717901259660721
train_iter_loss: 0.1680193841457367
train_iter_loss: 0.16816990077495575
train_iter_loss: 0.11508473008871078
train_iter_loss: 0.043639667332172394
train_iter_loss: 0.11525992304086685
train_iter_loss: 0.15895205736160278
train_iter_loss: 0.1732337474822998
train_iter_loss: 0.19222816824913025
train_iter_loss: 0.15569116175174713
train_iter_loss: 0.1627185344696045
train_iter_loss: 0.1509018838405609
train_iter_loss: 0.13741515576839447
train_iter_loss: 0.2101261466741562
train_iter_loss: 0.18975810706615448
train_iter_loss: 0.16953501105308533
train_iter_loss: 0.19779101014137268
train_iter_loss: 0.13573773205280304
train_iter_loss: 0.2122625708580017
train_iter_loss: 0.107498899102211
train_iter_loss: 0.17624862492084503
train_iter_loss: 0.18214847147464752
train_iter_loss: 0.1201220452785492
train_iter_loss: 0.19361339509487152
train_iter_loss: 0.09364300966262817
train_iter_loss: 0.1618853062391281
train_iter_loss: 0.24366414546966553
train_iter_loss: 0.2420344352722168
train_iter_loss: 0.17562100291252136
train_iter_loss: 0.2143860161304474
train_iter_loss: 0.1662077009677887
train_iter_loss: 0.18268071115016937
train_iter_loss: 0.10969673097133636
train_iter_loss: 0.15356004238128662
train_iter_loss: 0.16390460729599
train_iter_loss: 0.16804753243923187
train_iter_loss: 0.12329431623220444
train_iter_loss: 0.2942766547203064
train_iter_loss: 0.15384554862976074
train_iter_loss: 0.17052072286605835
train_iter_loss: 0.15624292194843292
train_iter_loss: 0.09120269119739532
train_iter_loss: 0.1381339132785797
train_iter_loss: 0.11288708448410034
train_iter_loss: 0.11779443919658661
train_iter_loss: 0.14780457317829132
train_iter_loss: 0.17475874722003937
train_iter_loss: 0.1420658826828003
train_iter_loss: 0.08633037656545639
train_iter_loss: 0.22595623135566711
train_iter_loss: 0.15550470352172852
train_iter_loss: 0.24376463890075684
train_iter_loss: 0.20365987718105316
train loss :0.1604
---------------------
Validation seg loss: 0.21271035752593065 at epoch 654
epoch =    655/  1000, exp = train
train_iter_loss: 0.12551742792129517
train_iter_loss: 0.17958468198776245
train_iter_loss: 0.19842633605003357
train_iter_loss: 0.11926592886447906
train_iter_loss: 0.1641232669353485
train_iter_loss: 0.1685132086277008
train_iter_loss: 0.13565696775913239
train_iter_loss: 0.19936439394950867
train_iter_loss: 0.23270168900489807
train_iter_loss: 0.1657000184059143
train_iter_loss: 0.09396872669458389
train_iter_loss: 0.0678427591919899
train_iter_loss: 0.21480175852775574
train_iter_loss: 0.15286600589752197
train_iter_loss: 0.15317542850971222
train_iter_loss: 0.187067911028862
train_iter_loss: 0.13359536230564117
train_iter_loss: 0.0801512598991394
train_iter_loss: 0.1272427886724472
train_iter_loss: 0.13399307429790497
train_iter_loss: 0.16972441971302032
train_iter_loss: 0.16155143082141876
train_iter_loss: 0.10115215927362442
train_iter_loss: 0.24714317917823792
train_iter_loss: 0.1479673832654953
train_iter_loss: 0.07298644632101059
train_iter_loss: 0.1437993198633194
train_iter_loss: 0.18796175718307495
train_iter_loss: 0.24607132375240326
train_iter_loss: 0.17053274810314178
train_iter_loss: 0.11050215363502502
train_iter_loss: 0.24129381775856018
train_iter_loss: 0.15381701290607452
train_iter_loss: 0.3018973767757416
train_iter_loss: 0.24114076793193817
train_iter_loss: 0.17880026996135712
train_iter_loss: 0.10143687576055527
train_iter_loss: 0.23757003247737885
train_iter_loss: 0.27434396743774414
train_iter_loss: 0.06967081129550934
train_iter_loss: 0.17495965957641602
train_iter_loss: 0.14743000268936157
train_iter_loss: 0.21063633263111115
train_iter_loss: 0.14334549009799957
train_iter_loss: 0.17075352370738983
train_iter_loss: 0.19139523804187775
train_iter_loss: 0.09071923792362213
train_iter_loss: 0.07176938652992249
train_iter_loss: 0.08923114836215973
train_iter_loss: 0.20058871805667877
train_iter_loss: 0.09346379339694977
train_iter_loss: 0.32004234194755554
train_iter_loss: 0.14054273068904877
train_iter_loss: 0.2131139039993286
train_iter_loss: 0.35148146748542786
train_iter_loss: 0.08307933062314987
train_iter_loss: 0.2244267463684082
train_iter_loss: 0.17096370458602905
train_iter_loss: 0.1506395787000656
train_iter_loss: 0.13393273949623108
train_iter_loss: 0.22287647426128387
train_iter_loss: 0.11841882765293121
train_iter_loss: 0.17787779867649078
train_iter_loss: 0.15614759922027588
train_iter_loss: 0.14651434123516083
train_iter_loss: 0.1187727078795433
train_iter_loss: 0.1640283465385437
train_iter_loss: 0.10750342905521393
train_iter_loss: 0.133111372590065
train_iter_loss: 0.06991177797317505
train_iter_loss: 0.33642274141311646
train_iter_loss: 0.2064737230539322
train_iter_loss: 0.20110437273979187
train_iter_loss: 0.13860401511192322
train_iter_loss: 0.1667756289243698
train_iter_loss: 0.11022071540355682
train_iter_loss: 0.17411048710346222
train_iter_loss: 0.0645010694861412
train_iter_loss: 0.22072531282901764
train_iter_loss: 0.17784252762794495
train_iter_loss: 0.18881765007972717
train_iter_loss: 0.1528206765651703
train_iter_loss: 0.10574786365032196
train_iter_loss: 0.16475297510623932
train_iter_loss: 0.12856799364089966
train_iter_loss: 0.10223226994276047
train_iter_loss: 0.12344644963741302
train_iter_loss: 0.09964769333600998
train_iter_loss: 0.21769402921199799
train_iter_loss: 0.21560490131378174
train_iter_loss: 0.11975237727165222
train_iter_loss: 0.11735492944717407
train_iter_loss: 0.145237997174263
train_iter_loss: 0.17440681159496307
train_iter_loss: 0.10460849106311798
train_iter_loss: 0.0698004886507988
train_iter_loss: 0.1650029718875885
train_iter_loss: 0.07167746126651764
train_iter_loss: 0.19383858144283295
train_iter_loss: 0.20716525614261627
train loss :0.1621
---------------------
Validation seg loss: 0.2183076855287237 at epoch 655
epoch =    656/  1000, exp = train
train_iter_loss: 0.11174032837152481
train_iter_loss: 0.16654400527477264
train_iter_loss: 0.12994840741157532
train_iter_loss: 0.2554290294647217
train_iter_loss: 0.20404846966266632
train_iter_loss: 0.2133287936449051
train_iter_loss: 0.15557074546813965
train_iter_loss: 0.16336283087730408
train_iter_loss: 0.18251582980155945
train_iter_loss: 0.09751810133457184
train_iter_loss: 0.14011821150779724
train_iter_loss: 0.12410832941532135
train_iter_loss: 0.15659727156162262
train_iter_loss: 0.16442538797855377
train_iter_loss: 0.23718275129795074
train_iter_loss: 0.2855818271636963
train_iter_loss: 0.08858706802129745
train_iter_loss: 0.026444198563694954
train_iter_loss: 0.18932393193244934
train_iter_loss: 0.10579811781644821
train_iter_loss: 0.1428488790988922
train_iter_loss: 0.07756856083869934
train_iter_loss: 0.09850382804870605
train_iter_loss: 0.16695962846279144
train_iter_loss: 0.11341730505228043
train_iter_loss: 0.29258742928504944
train_iter_loss: 0.16528400778770447
train_iter_loss: 0.18456770479679108
train_iter_loss: 0.15510383248329163
train_iter_loss: 0.10738135129213333
train_iter_loss: 0.18363916873931885
train_iter_loss: 0.24071542918682098
train_iter_loss: 0.040886156260967255
train_iter_loss: 0.05531368404626846
train_iter_loss: 0.11224205046892166
train_iter_loss: 0.10729073733091354
train_iter_loss: 0.06893399357795715
train_iter_loss: 0.16878432035446167
train_iter_loss: 0.11087358742952347
train_iter_loss: 0.11590854823589325
train_iter_loss: 0.06834287196397781
train_iter_loss: 0.17433877289295197
train_iter_loss: 0.16178645193576813
train_iter_loss: 0.14680153131484985
train_iter_loss: 0.1204296201467514
train_iter_loss: 0.09603036195039749
train_iter_loss: 0.17066733539104462
train_iter_loss: 0.028690865263342857
train_iter_loss: 0.21079055964946747
train_iter_loss: 0.22426412999629974
train_iter_loss: 0.17187772691249847
train_iter_loss: 0.12586408853530884
train_iter_loss: 0.3622385859489441
train_iter_loss: 0.2442186325788498
train_iter_loss: 0.18472175300121307
train_iter_loss: 0.13511091470718384
train_iter_loss: 0.2503329813480377
train_iter_loss: 0.15878787636756897
train_iter_loss: 0.07621091604232788
train_iter_loss: 0.053931597620248795
train_iter_loss: 0.13005377352237701
train_iter_loss: 0.12420053035020828
train_iter_loss: 0.12851183116436005
train_iter_loss: 0.08576098084449768
train_iter_loss: 0.18117783963680267
train_iter_loss: 0.09144336730241776
train_iter_loss: 0.228590190410614
train_iter_loss: 0.26957741379737854
train_iter_loss: 0.10933293402194977
train_iter_loss: 0.10257146507501602
train_iter_loss: 0.07951415330171585
train_iter_loss: 0.17843961715698242
train_iter_loss: 0.14206743240356445
train_iter_loss: 0.15316253900527954
train_iter_loss: 0.13107457756996155
train_iter_loss: 0.21815837919712067
train_iter_loss: 0.09013298153877258
train_iter_loss: 0.250254362821579
train_iter_loss: 0.09756854921579361
train_iter_loss: 0.18810883164405823
train_iter_loss: 0.2602323293685913
train_iter_loss: 0.1443207859992981
train_iter_loss: 0.09088465571403503
train_iter_loss: 0.3289428949356079
train_iter_loss: 0.1036086305975914
train_iter_loss: 0.15578560531139374
train_iter_loss: 0.17840518057346344
train_iter_loss: 0.13525569438934326
train_iter_loss: 0.1640949249267578
train_iter_loss: 0.1670398712158203
train_iter_loss: 0.18061630427837372
train_iter_loss: 0.13850072026252747
train_iter_loss: 0.14822758734226227
train_iter_loss: 0.03636235371232033
train_iter_loss: 0.17246918380260468
train_iter_loss: 0.2967691421508789
train_iter_loss: 0.160856693983078
train_iter_loss: 0.1752341091632843
train_iter_loss: 0.14627356827259064
train_iter_loss: 0.15962910652160645
train loss :0.1556
---------------------
Validation seg loss: 0.2165917356428251 at epoch 656
epoch =    657/  1000, exp = train
train_iter_loss: 0.23498862981796265
train_iter_loss: 0.08712069690227509
train_iter_loss: 0.2693563401699066
train_iter_loss: 0.11337835341691971
train_iter_loss: 0.11649899929761887
train_iter_loss: 0.0997595340013504
train_iter_loss: 0.07209408283233643
train_iter_loss: 0.17690421640872955
train_iter_loss: 0.16892224550247192
train_iter_loss: 0.17486634850502014
train_iter_loss: 0.1870390772819519
train_iter_loss: 0.10052052140235901
train_iter_loss: 0.08026660978794098
train_iter_loss: 0.22770631313323975
train_iter_loss: 0.29802578687667847
train_iter_loss: 0.08172132074832916
train_iter_loss: 0.11478429287672043
train_iter_loss: 0.0635945126414299
train_iter_loss: 0.08269853889942169
train_iter_loss: 0.23979061841964722
train_iter_loss: 0.2637214660644531
train_iter_loss: 0.1260117143392563
train_iter_loss: 0.14842112362384796
train_iter_loss: 0.16529196500778198
train_iter_loss: 0.09395884722471237
train_iter_loss: 0.12390170246362686
train_iter_loss: 0.11087903380393982
train_iter_loss: 0.13511638343334198
train_iter_loss: 0.17155873775482178
train_iter_loss: 0.0985817015171051
train_iter_loss: 0.19657589495182037
train_iter_loss: 0.23366254568099976
train_iter_loss: 0.11520639806985855
train_iter_loss: 0.08756855130195618
train_iter_loss: 0.16146109998226166
train_iter_loss: 0.13264276087284088
train_iter_loss: 0.07661879062652588
train_iter_loss: 0.15498365461826324
train_iter_loss: 0.17328912019729614
train_iter_loss: 0.104496069252491
train_iter_loss: 0.12960870563983917
train_iter_loss: 0.11374425888061523
train_iter_loss: 0.23313072323799133
train_iter_loss: 0.2205386608839035
train_iter_loss: 0.13112637400627136
train_iter_loss: 0.14472247660160065
train_iter_loss: 0.19296132028102875
train_iter_loss: 0.22739484906196594
train_iter_loss: 0.19272856414318085
train_iter_loss: 0.1808282881975174
train_iter_loss: 0.15616393089294434
train_iter_loss: 0.11036935448646545
train_iter_loss: 0.17396019399166107
train_iter_loss: 0.1919669806957245
train_iter_loss: 0.14675453305244446
train_iter_loss: 0.11220402270555496
train_iter_loss: 0.06984210014343262
train_iter_loss: 0.10664615780115128
train_iter_loss: 0.21327045559883118
train_iter_loss: 0.17196424305438995
train_iter_loss: 0.1305297464132309
train_iter_loss: 0.2120533436536789
train_iter_loss: 0.2076871544122696
train_iter_loss: 0.2506508529186249
train_iter_loss: 0.1444583684206009
train_iter_loss: 0.16092003881931305
train_iter_loss: 0.08225159347057343
train_iter_loss: 0.17937318980693817
train_iter_loss: 0.19273929297924042
train_iter_loss: 0.23481237888336182
train_iter_loss: 0.03704655542969704
train_iter_loss: 0.309309720993042
train_iter_loss: 0.13637366890907288
train_iter_loss: 0.08394370973110199
train_iter_loss: 0.10243713855743408
train_iter_loss: 0.23274894058704376
train_iter_loss: 0.3592472970485687
train_iter_loss: 0.18652506172657013
train_iter_loss: 0.3262542486190796
train_iter_loss: 0.18187089264392853
train_iter_loss: 0.1480766087770462
train_iter_loss: 0.1760864555835724
train_iter_loss: 0.10469213128089905
train_iter_loss: 0.2153330147266388
train_iter_loss: 0.1250508725643158
train_iter_loss: 0.08566215634346008
train_iter_loss: 0.16728900372982025
train_iter_loss: 0.11175516992807388
train_iter_loss: 0.13147073984146118
train_iter_loss: 0.18124058842658997
train_iter_loss: 0.16994261741638184
train_iter_loss: 0.09718790650367737
train_iter_loss: 0.11595451086759567
train_iter_loss: 0.09645690023899078
train_iter_loss: 0.05914982408285141
train_iter_loss: 0.15305881202220917
train_iter_loss: 0.24133393168449402
train_iter_loss: 0.16276705265045166
train_iter_loss: 0.27189379930496216
train_iter_loss: 0.19516383111476898
train loss :0.1600
---------------------
Validation seg loss: 0.21607066521350787 at epoch 657
epoch =    658/  1000, exp = train
train_iter_loss: 0.14458727836608887
train_iter_loss: 0.1453752964735031
train_iter_loss: 0.100649893283844
train_iter_loss: 0.05268241465091705
train_iter_loss: 0.09996873885393143
train_iter_loss: 0.05391965061426163
train_iter_loss: 0.16490910947322845
train_iter_loss: 0.14506876468658447
train_iter_loss: 0.14747729897499084
train_iter_loss: 0.2776930630207062
train_iter_loss: 0.07017583400011063
train_iter_loss: 0.19893811643123627
train_iter_loss: 0.1770765334367752
train_iter_loss: 0.21179115772247314
train_iter_loss: 0.20155520737171173
train_iter_loss: 0.20639075338840485
train_iter_loss: 0.13647472858428955
train_iter_loss: 0.08449538797140121
train_iter_loss: 0.12976041436195374
train_iter_loss: 0.20788493752479553
train_iter_loss: 0.23658747971057892
train_iter_loss: 0.16900412738323212
train_iter_loss: 0.24275167286396027
train_iter_loss: 0.18120288848876953
train_iter_loss: 0.08631797134876251
train_iter_loss: 0.19131597876548767
train_iter_loss: 0.08804214000701904
train_iter_loss: 0.30010101199150085
train_iter_loss: 0.1339908093214035
train_iter_loss: 0.14827118813991547
train_iter_loss: 0.1926383227109909
train_iter_loss: 0.0726262629032135
train_iter_loss: 0.15170781314373016
train_iter_loss: 0.07338888943195343
train_iter_loss: 0.14076067507266998
train_iter_loss: 0.08345822244882584
train_iter_loss: 0.14007842540740967
train_iter_loss: 0.17963646352291107
train_iter_loss: 0.08690984547138214
train_iter_loss: 0.20270539820194244
train_iter_loss: 0.09676899760961533
train_iter_loss: 0.09229420125484467
train_iter_loss: 0.18509748578071594
train_iter_loss: 0.22605298459529877
train_iter_loss: 0.17491617798805237
train_iter_loss: 0.09452256560325623
train_iter_loss: 0.09461692720651627
train_iter_loss: 0.2589806914329529
train_iter_loss: 0.09170495718717575
train_iter_loss: 0.12645423412322998
train_iter_loss: 0.4265989065170288
train_iter_loss: 0.10625921189785004
train_iter_loss: 0.2927020788192749
train_iter_loss: 0.19858627021312714
train_iter_loss: 0.30294179916381836
train_iter_loss: 0.15453194081783295
train_iter_loss: 0.10288576036691666
train_iter_loss: 0.09492131322622299
train_iter_loss: 0.17958302795886993
train_iter_loss: 0.1235072910785675
train_iter_loss: 0.08377125859260559
train_iter_loss: 0.1410997062921524
train_iter_loss: 0.1900218427181244
train_iter_loss: 0.16676795482635498
train_iter_loss: 0.09896209090948105
train_iter_loss: 0.16114339232444763
train_iter_loss: 0.184981569647789
train_iter_loss: 0.23623764514923096
train_iter_loss: 0.22738903760910034
train_iter_loss: 0.15974652767181396
train_iter_loss: 0.14956387877464294
train_iter_loss: 0.1964002549648285
train_iter_loss: 0.23940515518188477
train_iter_loss: 0.12823952734470367
train_iter_loss: 0.21233542263507843
train_iter_loss: 0.2232850193977356
train_iter_loss: 0.07889547199010849
train_iter_loss: 0.12764786183834076
train_iter_loss: 0.15544496476650238
train_iter_loss: 0.17604686319828033
train_iter_loss: 0.1312667429447174
train_iter_loss: 0.1266176551580429
train_iter_loss: 0.10329265892505646
train_iter_loss: 0.217714324593544
train_iter_loss: 0.2356244921684265
train_iter_loss: 0.07073568552732468
train_iter_loss: 0.09587904810905457
train_iter_loss: 0.13866586983203888
train_iter_loss: 0.15586495399475098
train_iter_loss: 0.36939430236816406
train_iter_loss: 0.14145155251026154
train_iter_loss: 0.1403059959411621
train_iter_loss: 0.22307682037353516
train_iter_loss: 0.17838867008686066
train_iter_loss: 0.2315976321697235
train_iter_loss: 0.14344587922096252
train_iter_loss: 0.13970910012722015
train_iter_loss: 0.06407754123210907
train_iter_loss: 0.13151448965072632
train_iter_loss: 0.18621762096881866
train loss :0.1620
---------------------
Validation seg loss: 0.21929384353307058 at epoch 658
epoch =    659/  1000, exp = train
train_iter_loss: 0.15542887151241302
train_iter_loss: 0.13547901809215546
train_iter_loss: 0.05427619442343712
train_iter_loss: 0.12930838763713837
train_iter_loss: 0.19183231890201569
train_iter_loss: 0.22610600292682648
train_iter_loss: 0.15077418088912964
train_iter_loss: 0.11951349675655365
train_iter_loss: 0.08432131260633469
train_iter_loss: 0.17685778439044952
train_iter_loss: 0.24600809812545776
train_iter_loss: 0.2192995548248291
train_iter_loss: 0.1340303272008896
train_iter_loss: 0.22307509183883667
train_iter_loss: 0.1532624065876007
train_iter_loss: 0.11383569985628128
train_iter_loss: 0.14758440852165222
train_iter_loss: 0.16802850365638733
train_iter_loss: 0.0634562224149704
train_iter_loss: 0.15921664237976074
train_iter_loss: 0.1684298813343048
train_iter_loss: 0.23797114193439484
train_iter_loss: 0.22071807086467743
train_iter_loss: 0.14689838886260986
train_iter_loss: 0.1404632329940796
train_iter_loss: 0.16693945229053497
train_iter_loss: 0.07759194821119308
train_iter_loss: 0.15772691369056702
train_iter_loss: 0.11594371497631073
train_iter_loss: 0.15522953867912292
train_iter_loss: 0.1249099150300026
train_iter_loss: 0.1659034937620163
train_iter_loss: 0.1874171793460846
train_iter_loss: 0.20363876223564148
train_iter_loss: 0.0452747605741024
train_iter_loss: 0.18981917202472687
train_iter_loss: 0.10406981408596039
train_iter_loss: 0.1689199060201645
train_iter_loss: 0.1636204570531845
train_iter_loss: 0.2088346928358078
train_iter_loss: 0.06608138978481293
train_iter_loss: 0.21889659762382507
train_iter_loss: 0.12656238675117493
train_iter_loss: 0.22820749878883362
train_iter_loss: 0.15214914083480835
train_iter_loss: 0.16461822390556335
train_iter_loss: 0.26690545678138733
train_iter_loss: 0.16158676147460938
train_iter_loss: 0.08529014885425568
train_iter_loss: 0.17703242599964142
train_iter_loss: 0.10181857645511627
train_iter_loss: 0.09973428398370743
train_iter_loss: 0.1404886543750763
train_iter_loss: 0.1201690211892128
train_iter_loss: 0.13708427548408508
train_iter_loss: 0.15656715631484985
train_iter_loss: 0.17111067473888397
train_iter_loss: 0.029175663366913795
train_iter_loss: 0.15337924659252167
train_iter_loss: 0.15552696585655212
train_iter_loss: 0.11187033355236053
train_iter_loss: 0.2431197613477707
train_iter_loss: 0.2025265246629715
train_iter_loss: 0.09732536971569061
train_iter_loss: 0.2273436039686203
train_iter_loss: 0.15113867819309235
train_iter_loss: 0.09298640489578247
train_iter_loss: 0.11133738607168198
train_iter_loss: 0.17141060531139374
train_iter_loss: 0.11878561973571777
train_iter_loss: 0.16276052594184875
train_iter_loss: 0.17063070833683014
train_iter_loss: 0.17970561981201172
train_iter_loss: 0.29246196150779724
train_iter_loss: 0.11872115731239319
train_iter_loss: 0.0756419450044632
train_iter_loss: 0.09030915796756744
train_iter_loss: 0.10130996257066727
train_iter_loss: 0.16238759458065033
train_iter_loss: 0.1212131530046463
train_iter_loss: 0.2827904224395752
train_iter_loss: 0.16461530327796936
train_iter_loss: 0.16521809995174408
train_iter_loss: 0.3298559784889221
train_iter_loss: 0.19003456830978394
train_iter_loss: 0.10914932936429977
train_iter_loss: 0.1684691458940506
train_iter_loss: 0.13201908767223358
train_iter_loss: 0.1654524803161621
train_iter_loss: 0.1256721317768097
train_iter_loss: 0.1411881148815155
train_iter_loss: 0.14818473160266876
train_iter_loss: 0.061129242181777954
train_iter_loss: 0.16809360682964325
train_iter_loss: 0.13755102455615997
train_iter_loss: 0.1808335930109024
train_iter_loss: 0.18183818459510803
train_iter_loss: 0.14223793148994446
train_iter_loss: 0.15789563953876495
train_iter_loss: 0.14061889052391052
train loss :0.1557
---------------------
Validation seg loss: 0.2180592582192061 at epoch 659
epoch =    660/  1000, exp = train
train_iter_loss: 0.06758217513561249
train_iter_loss: 0.14378541707992554
train_iter_loss: 0.06784117221832275
train_iter_loss: 0.17090928554534912
train_iter_loss: 0.19281360507011414
train_iter_loss: 0.3501046895980835
train_iter_loss: 0.14543473720550537
train_iter_loss: 0.06791254132986069
train_iter_loss: 0.23688127100467682
train_iter_loss: 0.13269385695457458
train_iter_loss: 0.15015949308872223
train_iter_loss: 0.1888907253742218
train_iter_loss: 0.05129305645823479
train_iter_loss: 0.14970016479492188
train_iter_loss: 0.2829214632511139
train_iter_loss: 0.09790719300508499
train_iter_loss: 0.23317070305347443
train_iter_loss: 0.22361278533935547
train_iter_loss: 0.17469894886016846
train_iter_loss: 0.23524470627307892
train_iter_loss: 0.10523942112922668
train_iter_loss: 0.12350359559059143
train_iter_loss: 0.1216154620051384
train_iter_loss: 0.18573670089244843
train_iter_loss: 0.1602712720632553
train_iter_loss: 0.3019239902496338
train_iter_loss: 0.1392776519060135
train_iter_loss: 0.42222559452056885
train_iter_loss: 0.17580179870128632
train_iter_loss: 0.11899302154779434
train_iter_loss: 0.22233577072620392
train_iter_loss: 0.1381731778383255
train_iter_loss: 0.1431746482849121
train_iter_loss: 0.06386087834835052
train_iter_loss: 0.14025238156318665
train_iter_loss: 0.11838866025209427
train_iter_loss: 0.18085335195064545
train_iter_loss: 0.06544176489114761
train_iter_loss: 0.20646682381629944
train_iter_loss: 0.20995637774467468
train_iter_loss: 0.22214044630527496
train_iter_loss: 0.216976135969162
train_iter_loss: 0.09668994694948196
train_iter_loss: 0.19284383952617645
train_iter_loss: 0.16178995370864868
train_iter_loss: 0.19028177857398987
train_iter_loss: 0.1286664456129074
train_iter_loss: 0.1540217250585556
train_iter_loss: 0.19466255605220795
train_iter_loss: 0.23543523252010345
train_iter_loss: 0.1238996610045433
train_iter_loss: 0.11991676688194275
train_iter_loss: 0.16020339727401733
train_iter_loss: 0.16205580532550812
train_iter_loss: 0.10879220813512802
train_iter_loss: 0.11033916473388672
train_iter_loss: 0.3112090229988098
train_iter_loss: 0.13147872686386108
train_iter_loss: 0.09361587464809418
train_iter_loss: 0.057733140885829926
train_iter_loss: 0.15029509365558624
train_iter_loss: 0.0889768898487091
train_iter_loss: 0.11510581523180008
train_iter_loss: 0.1171083152294159
train_iter_loss: 0.11883299052715302
train_iter_loss: 0.1792701929807663
train_iter_loss: 0.1553155481815338
train_iter_loss: 0.2708291709423065
train_iter_loss: 0.15243537724018097
train_iter_loss: 0.18537335097789764
train_iter_loss: 0.07585404813289642
train_iter_loss: 0.07696780562400818
train_iter_loss: 0.19855239987373352
train_iter_loss: 0.20944473147392273
train_iter_loss: 0.11742309480905533
train_iter_loss: 0.07991087436676025
train_iter_loss: 0.14893290400505066
train_iter_loss: 0.15593312680721283
train_iter_loss: 0.08137352019548416
train_iter_loss: 0.10072397440671921
train_iter_loss: 0.1691187471151352
train_iter_loss: 0.15299038589000702
train_iter_loss: 0.15525898337364197
train_iter_loss: 0.07719915360212326
train_iter_loss: 0.16292697191238403
train_iter_loss: 0.12554311752319336
train_iter_loss: 0.14756572246551514
train_iter_loss: 0.20486323535442352
train_iter_loss: 0.08783207088708878
train_iter_loss: 0.07196596264839172
train_iter_loss: 0.06826838105916977
train_iter_loss: 0.1550515741109848
train_iter_loss: 0.0928880050778389
train_iter_loss: 0.3283541798591614
train_iter_loss: 0.16490766406059265
train_iter_loss: 0.09644637256860733
train_iter_loss: 0.19487124681472778
train_iter_loss: 0.18670552968978882
train_iter_loss: 0.16620594263076782
train_iter_loss: 0.19511443376541138
train loss :0.1582
---------------------
Validation seg loss: 0.21923835565915928 at epoch 660
epoch =    661/  1000, exp = train
train_iter_loss: 0.16556435823440552
train_iter_loss: 0.14193163812160492
train_iter_loss: 0.12029159069061279
train_iter_loss: 0.1247619092464447
train_iter_loss: 0.09926126897335052
train_iter_loss: 0.175734743475914
train_iter_loss: 0.0754309892654419
train_iter_loss: 0.23172882199287415
train_iter_loss: 0.1986130326986313
train_iter_loss: 0.11675555258989334
train_iter_loss: 0.12698999047279358
train_iter_loss: 0.09254154562950134
train_iter_loss: 0.10818538069725037
train_iter_loss: 0.13216020166873932
train_iter_loss: 0.11889273673295975
train_iter_loss: 0.049720995128154755
train_iter_loss: 0.23328739404678345
train_iter_loss: 0.20754504203796387
train_iter_loss: 0.1423240303993225
train_iter_loss: 0.2704360783100128
train_iter_loss: 0.16900241374969482
train_iter_loss: 0.389103502035141
train_iter_loss: 0.18290629982948303
train_iter_loss: 0.11126973479986191
train_iter_loss: 0.21307559311389923
train_iter_loss: 0.15978191792964935
train_iter_loss: 0.097410187125206
train_iter_loss: 0.09185916930437088
train_iter_loss: 0.18444231152534485
train_iter_loss: 0.10157518088817596
train_iter_loss: 0.11452513188123703
train_iter_loss: 0.22494050860404968
train_iter_loss: 0.08202551305294037
train_iter_loss: 0.12401719391345978
train_iter_loss: 0.3154200315475464
train_iter_loss: 0.157820925116539
train_iter_loss: 0.14000539481639862
train_iter_loss: 0.09018213301897049
train_iter_loss: 0.18675260245800018
train_iter_loss: 0.1280910074710846
train_iter_loss: 0.12256423383951187
train_iter_loss: 0.30773502588272095
train_iter_loss: 0.10530775040388107
train_iter_loss: 0.15336810052394867
train_iter_loss: 0.28258830308914185
train_iter_loss: 0.10660351067781448
train_iter_loss: 0.06591533124446869
train_iter_loss: 0.26646384596824646
train_iter_loss: 0.2900082767009735
train_iter_loss: 0.21865007281303406
train_iter_loss: 0.16481918096542358
train_iter_loss: 0.05434243381023407
train_iter_loss: 0.10926664620637894
train_iter_loss: 0.1188482716679573
train_iter_loss: 0.13994787633419037
train_iter_loss: 0.09119641780853271
train_iter_loss: 0.20343448221683502
train_iter_loss: 0.18742525577545166
train_iter_loss: 0.06553533673286438
train_iter_loss: 0.19294820725917816
train_iter_loss: 0.12547914683818817
train_iter_loss: 0.04045334830880165
train_iter_loss: 0.23259247839450836
train_iter_loss: 0.14764323830604553
train_iter_loss: 0.1466953158378601
train_iter_loss: 0.29758134484291077
train_iter_loss: 0.15275751054286957
train_iter_loss: 0.18187692761421204
train_iter_loss: 0.09541359543800354
train_iter_loss: 0.12958405911922455
train_iter_loss: 0.15957771241664886
train_iter_loss: 0.17307165265083313
train_iter_loss: 0.12697410583496094
train_iter_loss: 0.28356677293777466
train_iter_loss: 0.16639256477355957
train_iter_loss: 0.06412745267152786
train_iter_loss: 0.16439510881900787
train_iter_loss: 0.16065163910388947
train_iter_loss: 0.26235896348953247
train_iter_loss: 0.09500040858983994
train_iter_loss: 0.5456370711326599
train_iter_loss: 0.14659123122692108
train_iter_loss: 0.13245534896850586
train_iter_loss: 0.12052883207798004
train_iter_loss: 0.2170802801847458
train_iter_loss: 0.10804763436317444
train_iter_loss: 0.16860049962997437
train_iter_loss: 0.2018616944551468
train_iter_loss: 0.11430913209915161
train_iter_loss: 0.16720841825008392
train_iter_loss: 0.1535005271434784
train_iter_loss: 0.20517966151237488
train_iter_loss: 0.15235711634159088
train_iter_loss: 0.1621726006269455
train_iter_loss: 0.10385801643133163
train_iter_loss: 0.09033356606960297
train_iter_loss: 0.15247608721256256
train_iter_loss: 0.11874831467866898
train_iter_loss: 0.06932301819324493
train_iter_loss: 0.16300243139266968
train loss :0.1610
---------------------
Validation seg loss: 0.21491069691079967 at epoch 661
epoch =    662/  1000, exp = train
train_iter_loss: 0.1274293214082718
train_iter_loss: 0.169582337141037
train_iter_loss: 0.14353623986244202
train_iter_loss: 0.23996542394161224
train_iter_loss: 0.09926234185695648
train_iter_loss: 0.14948232471942902
train_iter_loss: 0.12618355453014374
train_iter_loss: 0.08539009094238281
train_iter_loss: 0.10457375645637512
train_iter_loss: 0.1147744357585907
train_iter_loss: 0.11534558236598969
train_iter_loss: 0.2724163234233856
train_iter_loss: 0.09571047127246857
train_iter_loss: 0.13502095639705658
train_iter_loss: 0.26125338673591614
train_iter_loss: 0.27705493569374084
train_iter_loss: 0.21668997406959534
train_iter_loss: 0.11358050256967545
train_iter_loss: 0.19062909483909607
train_iter_loss: 0.14062106609344482
train_iter_loss: 0.0742778405547142
train_iter_loss: 0.17087703943252563
train_iter_loss: 0.1999051719903946
train_iter_loss: 0.10033950954675674
train_iter_loss: 0.43132641911506653
train_iter_loss: 0.053321365267038345
train_iter_loss: 0.14967145025730133
train_iter_loss: 0.11265060305595398
train_iter_loss: 0.11947943270206451
train_iter_loss: 0.17227573692798615
train_iter_loss: 0.10311400890350342
train_iter_loss: 0.10026979446411133
train_iter_loss: 0.24128274619579315
train_iter_loss: 0.25012528896331787
train_iter_loss: 0.0628117024898529
train_iter_loss: 0.23603564500808716
train_iter_loss: 0.16959071159362793
train_iter_loss: 0.07805188000202179
train_iter_loss: 0.06054264307022095
train_iter_loss: 0.14663907885551453
train_iter_loss: 0.11932563036680222
train_iter_loss: 0.15469329059123993
train_iter_loss: 0.07220947742462158
train_iter_loss: 0.211618572473526
train_iter_loss: 0.11144141852855682
train_iter_loss: 0.07100960612297058
train_iter_loss: 0.18303652107715607
train_iter_loss: 0.16941408812999725
train_iter_loss: 0.203014075756073
train_iter_loss: 0.18821899592876434
train_iter_loss: 0.20591618120670319
train_iter_loss: 0.20619826018810272
train_iter_loss: 0.08019039779901505
train_iter_loss: 0.1493101418018341
train_iter_loss: 0.11490952968597412
train_iter_loss: 0.1366046965122223
train_iter_loss: 0.17069146037101746
train_iter_loss: 0.19136805832386017
train_iter_loss: 0.11665907502174377
train_iter_loss: 0.1116107925772667
train_iter_loss: 0.1413325071334839
train_iter_loss: 0.10462195426225662
train_iter_loss: 0.1891735941171646
train_iter_loss: 0.10114453732967377
train_iter_loss: 0.19239920377731323
train_iter_loss: 0.14001525938510895
train_iter_loss: 0.24032063782215118
train_iter_loss: 0.21983103454113007
train_iter_loss: 0.11067891865968704
train_iter_loss: 0.08184970170259476
train_iter_loss: 0.15803970396518707
train_iter_loss: 0.25734037160873413
train_iter_loss: 0.17771582305431366
train_iter_loss: 0.08627402782440186
train_iter_loss: 0.15412329137325287
train_iter_loss: 0.17376700043678284
train_iter_loss: 0.09539414942264557
train_iter_loss: 0.16961349546909332
train_iter_loss: 0.17214389145374298
train_iter_loss: 0.10401216149330139
train_iter_loss: 0.20079074800014496
train_iter_loss: 0.15424029529094696
train_iter_loss: 0.09032125771045685
train_iter_loss: 0.10916191339492798
train_iter_loss: 0.22217664122581482
train_iter_loss: 0.12036379426717758
train_iter_loss: 0.13285931944847107
train_iter_loss: 0.11049814522266388
train_iter_loss: 0.229059100151062
train_iter_loss: 0.16280631721019745
train_iter_loss: 0.12750278413295746
train_iter_loss: 0.2841205596923828
train_iter_loss: 0.28050872683525085
train_iter_loss: 0.05955947935581207
train_iter_loss: 0.08572550117969513
train_iter_loss: 0.31195583939552307
train_iter_loss: 0.17727257311344147
train_iter_loss: 0.0819229930639267
train_iter_loss: 0.048319969326257706
train_iter_loss: 0.18840746581554413
train loss :0.1559
---------------------
Validation seg loss: 0.22072704153943737 at epoch 662
epoch =    663/  1000, exp = train
train_iter_loss: 0.10307924449443817
train_iter_loss: 0.08456168323755264
train_iter_loss: 0.08936035633087158
train_iter_loss: 0.17633488774299622
train_iter_loss: 0.06880258768796921
train_iter_loss: 0.1340680718421936
train_iter_loss: 0.1834937334060669
train_iter_loss: 0.12404391169548035
train_iter_loss: 0.2580049932003021
train_iter_loss: 0.11108586192131042
train_iter_loss: 0.11231090128421783
train_iter_loss: 0.06632312387228012
train_iter_loss: 0.12766480445861816
train_iter_loss: 0.13655667006969452
train_iter_loss: 0.13346907496452332
train_iter_loss: 0.2581445574760437
train_iter_loss: 0.11746136844158173
train_iter_loss: 0.25738638639450073
train_iter_loss: 0.09142061322927475
train_iter_loss: 0.27483227849006653
train_iter_loss: 0.1416023075580597
train_iter_loss: 0.07387590408325195
train_iter_loss: 0.1599844992160797
train_iter_loss: 0.2342127114534378
train_iter_loss: 0.17424647510051727
train_iter_loss: 0.19400297105312347
train_iter_loss: 0.11504638195037842
train_iter_loss: 0.1521252542734146
train_iter_loss: 0.08334915339946747
train_iter_loss: 0.18621262907981873
train_iter_loss: 0.22826269268989563
train_iter_loss: 0.14941111207008362
train_iter_loss: 0.16382697224617004
train_iter_loss: 0.0975266769528389
train_iter_loss: 0.1728052943944931
train_iter_loss: 0.09870801120996475
train_iter_loss: 0.16957832872867584
train_iter_loss: 0.1997242271900177
train_iter_loss: 0.16710619628429413
train_iter_loss: 0.16989508271217346
train_iter_loss: 0.21909181773662567
train_iter_loss: 0.10203561931848526
train_iter_loss: 0.20866648852825165
train_iter_loss: 0.22525843977928162
train_iter_loss: 0.08296023309230804
train_iter_loss: 0.08069344609975815
train_iter_loss: 0.16980837285518646
train_iter_loss: 0.12844882905483246
train_iter_loss: 0.13850753009319305
train_iter_loss: 0.2006475329399109
train_iter_loss: 0.10166005045175552
train_iter_loss: 0.1921805739402771
train_iter_loss: 0.05863583832979202
train_iter_loss: 0.24958136677742004
train_iter_loss: 0.17398443818092346
train_iter_loss: 0.15383461117744446
train_iter_loss: 0.05265676602721214
train_iter_loss: 0.2167699933052063
train_iter_loss: 0.18694137036800385
train_iter_loss: 0.17595578730106354
train_iter_loss: 0.13779498636722565
train_iter_loss: 0.2761349081993103
train_iter_loss: 0.1861657351255417
train_iter_loss: 0.22453217208385468
train_iter_loss: 0.13963322341442108
train_iter_loss: 0.10200783610343933
train_iter_loss: 0.11960941553115845
train_iter_loss: 0.16803400218486786
train_iter_loss: 0.07175996154546738
train_iter_loss: 0.1318511664867401
train_iter_loss: 0.2944093346595764
train_iter_loss: 0.36911341547966003
train_iter_loss: 0.19091469049453735
train_iter_loss: 0.14009425044059753
train_iter_loss: 0.14241012930870056
train_iter_loss: 0.17911489307880402
train_iter_loss: 0.07598859816789627
train_iter_loss: 0.17992885410785675
train_iter_loss: 0.14530842006206512
train_iter_loss: 0.02339216321706772
train_iter_loss: 0.11496385186910629
train_iter_loss: 0.20190207660198212
train_iter_loss: 0.059527598321437836
train_iter_loss: 0.1622324287891388
train_iter_loss: 0.25778740644454956
train_iter_loss: 0.09800393134355545
train_iter_loss: 0.21944604814052582
train_iter_loss: 0.09236245602369308
train_iter_loss: 0.15073637664318085
train_iter_loss: 0.17190490663051605
train_iter_loss: 0.22556854784488678
train_iter_loss: 0.23811142146587372
train_iter_loss: 0.13400061428546906
train_iter_loss: 0.2372199296951294
train_iter_loss: 0.13603238761425018
train_iter_loss: 0.17154894769191742
train_iter_loss: 0.14372099936008453
train_iter_loss: 0.16323235630989075
train_iter_loss: 0.10636945813894272
train_iter_loss: 0.17595259845256805
train loss :0.1587
---------------------
Validation seg loss: 0.2191501603163076 at epoch 663
epoch =    664/  1000, exp = train
train_iter_loss: 0.25486233830451965
train_iter_loss: 0.21158011257648468
train_iter_loss: 0.12035093456506729
train_iter_loss: 0.14269253611564636
train_iter_loss: 0.21991904079914093
train_iter_loss: 0.11003472656011581
train_iter_loss: 0.12202426791191101
train_iter_loss: 0.1534876972436905
train_iter_loss: 0.1913856565952301
train_iter_loss: 0.1331385374069214
train_iter_loss: 0.2056911289691925
train_iter_loss: 0.09315650910139084
train_iter_loss: 0.15329134464263916
train_iter_loss: 0.16627730429172516
train_iter_loss: 0.18143804371356964
train_iter_loss: 0.09083174914121628
train_iter_loss: 0.08680778741836548
train_iter_loss: 0.10464861243963242
train_iter_loss: 0.13821181654930115
train_iter_loss: 0.08386529982089996
train_iter_loss: 0.1216355487704277
train_iter_loss: 0.23845238983631134
train_iter_loss: 0.12484465539455414
train_iter_loss: 0.07320697605609894
train_iter_loss: 0.15519572794437408
train_iter_loss: 0.19685785472393036
train_iter_loss: 0.200166717171669
train_iter_loss: 0.1573002189397812
train_iter_loss: 0.29744282364845276
train_iter_loss: 0.1530180126428604
train_iter_loss: 0.15965904295444489
train_iter_loss: 0.24290667474269867
train_iter_loss: 0.13470180332660675
train_iter_loss: 0.06910745054483414
train_iter_loss: 0.16659854352474213
train_iter_loss: 0.2138943374156952
train_iter_loss: 0.14244788885116577
train_iter_loss: 0.23832347989082336
train_iter_loss: 0.3222295343875885
train_iter_loss: 0.16798093914985657
train_iter_loss: 0.052274689078330994
train_iter_loss: 0.08533962815999985
train_iter_loss: 0.09499960392713547
train_iter_loss: 0.25495070219039917
train_iter_loss: 0.13321742415428162
train_iter_loss: 0.15353427827358246
train_iter_loss: 0.26808369159698486
train_iter_loss: 0.231853187084198
train_iter_loss: 0.2057071030139923
train_iter_loss: 0.2145499289035797
train_iter_loss: 0.36073267459869385
train_iter_loss: 0.09050401300191879
train_iter_loss: 0.19074250757694244
train_iter_loss: 0.23142726719379425
train_iter_loss: 0.07891976833343506
train_iter_loss: 0.17127743363380432
train_iter_loss: 0.12241285294294357
train_iter_loss: 0.09283622354269028
train_iter_loss: 0.10304225981235504
train_iter_loss: 0.07906332612037659
train_iter_loss: 0.09905719757080078
train_iter_loss: 0.1443648785352707
train_iter_loss: 0.12858323752880096
train_iter_loss: 0.21446926891803741
train_iter_loss: 0.10307657718658447
train_iter_loss: 0.24498626589775085
train_iter_loss: 0.21807870268821716
train_iter_loss: 0.09491909295320511
train_iter_loss: 0.08056899905204773
train_iter_loss: 0.14315113425254822
train_iter_loss: 0.08999795466661453
train_iter_loss: 0.15647336840629578
train_iter_loss: 0.10521877557039261
train_iter_loss: 0.12931479513645172
train_iter_loss: 0.1175885796546936
train_iter_loss: 0.07499317079782486
train_iter_loss: 0.12741120159626007
train_iter_loss: 0.13491356372833252
train_iter_loss: 0.2204042226076126
train_iter_loss: 0.18109910190105438
train_iter_loss: 0.20609642565250397
train_iter_loss: 0.17325083911418915
train_iter_loss: 0.1487550288438797
train_iter_loss: 0.04235653951764107
train_iter_loss: 0.20531104505062103
train_iter_loss: 0.1119832769036293
train_iter_loss: 0.1504230946302414
train_iter_loss: 0.14487230777740479
train_iter_loss: 0.13517683744430542
train_iter_loss: 0.29348137974739075
train_iter_loss: 0.3080446720123291
train_iter_loss: 0.22402046620845795
train_iter_loss: 0.22715036571025848
train_iter_loss: 0.09886303544044495
train_iter_loss: 0.12498002499341965
train_iter_loss: 0.18010489642620087
train_iter_loss: 0.22738683223724365
train_iter_loss: 0.08601740747690201
train_iter_loss: 0.14834405481815338
train_iter_loss: 0.2023330181837082
train loss :0.1616
---------------------
Validation seg loss: 0.22045441200288962 at epoch 664
epoch =    665/  1000, exp = train
train_iter_loss: 0.18664126098155975
train_iter_loss: 0.21209268271923065
train_iter_loss: 0.017895318567752838
train_iter_loss: 0.06461497396230698
train_iter_loss: 0.12412421405315399
train_iter_loss: 0.18974126875400543
train_iter_loss: 0.2562658488750458
train_iter_loss: 0.15820932388305664
train_iter_loss: 0.08471855521202087
train_iter_loss: 0.18119458854198456
train_iter_loss: 0.20727947354316711
train_iter_loss: 0.14558622241020203
train_iter_loss: 0.17989221215248108
train_iter_loss: 0.07893827557563782
train_iter_loss: 0.20970630645751953
train_iter_loss: 0.10879228264093399
train_iter_loss: 0.24991318583488464
train_iter_loss: 0.17902813851833344
train_iter_loss: 0.12277602404356003
train_iter_loss: 0.10400793701410294
train_iter_loss: 0.3266512155532837
train_iter_loss: 0.1128295287489891
train_iter_loss: 0.12548474967479706
train_iter_loss: 0.10891135782003403
train_iter_loss: 0.2929019331932068
train_iter_loss: 0.11970840394496918
train_iter_loss: 0.1741640567779541
train_iter_loss: 0.11737468838691711
train_iter_loss: 0.1049981564283371
train_iter_loss: 0.17734295129776
train_iter_loss: 0.13502617180347443
train_iter_loss: 0.16394153237342834
train_iter_loss: 0.14837782084941864
train_iter_loss: 0.13669390976428986
train_iter_loss: 0.09224465489387512
train_iter_loss: 0.20035818219184875
train_iter_loss: 0.2742055356502533
train_iter_loss: 0.11169078201055527
train_iter_loss: 0.1793994903564453
train_iter_loss: 0.2078782618045807
train_iter_loss: 0.21634820103645325
train_iter_loss: 0.08256309479475021
train_iter_loss: 0.1506079137325287
train_iter_loss: 0.13307149708271027
train_iter_loss: 0.25514331459999084
train_iter_loss: 0.14785216748714447
train_iter_loss: 0.1610180288553238
train_iter_loss: 0.30530843138694763
train_iter_loss: 0.1405278891324997
train_iter_loss: 0.0339316800236702
train_iter_loss: 0.15330949425697327
train_iter_loss: 0.14305523037910461
train_iter_loss: 0.14606580138206482
train_iter_loss: 0.12175389379262924
train_iter_loss: 0.13944028317928314
train_iter_loss: 0.10528809577226639
train_iter_loss: 0.23119257390499115
train_iter_loss: 0.08271006494760513
train_iter_loss: 0.065586157143116
train_iter_loss: 0.10844366252422333
train_iter_loss: 0.1515284925699234
train_iter_loss: 0.1631542444229126
train_iter_loss: 0.18777358531951904
train_iter_loss: 0.15958760678768158
train_iter_loss: 0.11393482983112335
train_iter_loss: 0.1756894439458847
train_iter_loss: 0.1873365342617035
train_iter_loss: 0.16850481927394867
train_iter_loss: 0.6462410688400269
train_iter_loss: 0.21691647171974182
train_iter_loss: 0.12087924033403397
train_iter_loss: 0.15764537453651428
train_iter_loss: 0.18230414390563965
train_iter_loss: 0.2085317075252533
train_iter_loss: 0.12696614861488342
train_iter_loss: 0.15609905123710632
train_iter_loss: 0.2158489227294922
train_iter_loss: 0.11024238914251328
train_iter_loss: 0.1677095741033554
train_iter_loss: 0.09498702734708786
train_iter_loss: 0.4291236102581024
train_iter_loss: 0.1303599327802658
train_iter_loss: 0.1529238522052765
train_iter_loss: 0.0801156535744667
train_iter_loss: 0.14945004880428314
train_iter_loss: 0.2035573571920395
train_iter_loss: 0.1809702068567276
train_iter_loss: 0.21899081766605377
train_iter_loss: 0.1463029533624649
train_iter_loss: 0.27043816447257996
train_iter_loss: 0.20606766641139984
train_iter_loss: 0.15835461020469666
train_iter_loss: 0.04134664312005043
train_iter_loss: 0.17724600434303284
train_iter_loss: 0.14678196609020233
train_iter_loss: 0.2162974327802658
train_iter_loss: 0.20246565341949463
train_iter_loss: 0.09007368981838226
train_iter_loss: 0.12120141834020615
train_iter_loss: 0.17435674369335175
train loss :0.1666
---------------------
Validation seg loss: 0.21875001643872205 at epoch 665
epoch =    666/  1000, exp = train
train_iter_loss: 0.07899493724107742
train_iter_loss: 0.12449692189693451
train_iter_loss: 0.3032858967781067
train_iter_loss: 0.11955318599939346
train_iter_loss: 0.1149703785777092
train_iter_loss: 0.11142221093177795
train_iter_loss: 0.12462504208087921
train_iter_loss: 0.20133735239505768
train_iter_loss: 0.1840488314628601
train_iter_loss: 0.08316652476787567
train_iter_loss: 0.13043466210365295
train_iter_loss: 0.05883999913930893
train_iter_loss: 0.10722558945417404
train_iter_loss: 0.1653774380683899
train_iter_loss: 0.2951565086841583
train_iter_loss: 0.07930980622768402
train_iter_loss: 0.22105348110198975
train_iter_loss: 0.07395906001329422
train_iter_loss: 0.18879570066928864
train_iter_loss: 0.3261006772518158
train_iter_loss: 0.05592164397239685
train_iter_loss: 0.15285524725914001
train_iter_loss: 0.1854386031627655
train_iter_loss: 0.09301077574491501
train_iter_loss: 0.2154163420200348
train_iter_loss: 0.12466411292552948
train_iter_loss: 0.17641393840312958
train_iter_loss: 0.3105655312538147
train_iter_loss: 0.18414875864982605
train_iter_loss: 0.2143660932779312
train_iter_loss: 0.20117925107479095
train_iter_loss: 0.13800570368766785
train_iter_loss: 0.18236885964870453
train_iter_loss: 0.1385641098022461
train_iter_loss: 0.15445014834403992
train_iter_loss: 0.2863824963569641
train_iter_loss: 0.1515035629272461
train_iter_loss: 0.11250820010900497
train_iter_loss: 0.11033785343170166
train_iter_loss: 0.22046969830989838
train_iter_loss: 0.25727352499961853
train_iter_loss: 0.07684342563152313
train_iter_loss: 0.13613811135292053
train_iter_loss: 0.2382357269525528
train_iter_loss: 0.27353501319885254
train_iter_loss: 0.09072990715503693
train_iter_loss: 0.1879437118768692
train_iter_loss: 0.24162325263023376
train_iter_loss: 0.17791086435317993
train_iter_loss: 0.20878809690475464
train_iter_loss: 0.1338074654340744
train_iter_loss: 0.04969213530421257
train_iter_loss: 0.16261063516139984
train_iter_loss: 0.1748218685388565
train_iter_loss: 0.13988858461380005
train_iter_loss: 0.20053574442863464
train_iter_loss: 0.2273055464029312
train_iter_loss: 0.15412144362926483
train_iter_loss: 0.12206298857927322
train_iter_loss: 0.31821900606155396
train_iter_loss: 0.1152544915676117
train_iter_loss: 0.06918855756521225
train_iter_loss: 0.19692294299602509
train_iter_loss: 0.191242054104805
train_iter_loss: 0.10770929604768753
train_iter_loss: 0.21152324974536896
train_iter_loss: 0.1103329285979271
train_iter_loss: 0.14616426825523376
train_iter_loss: 0.2753039598464966
train_iter_loss: 0.15437696874141693
train_iter_loss: 0.06723310798406601
train_iter_loss: 0.15948422253131866
train_iter_loss: 0.16232632100582123
train_iter_loss: 0.15088996291160583
train_iter_loss: 0.07415919750928879
train_iter_loss: 0.2569330036640167
train_iter_loss: 0.0992744043469429
train_iter_loss: 0.13713136315345764
train_iter_loss: 0.28140348196029663
train_iter_loss: 0.13737058639526367
train_iter_loss: 0.13274027407169342
train_iter_loss: 0.12323493510484695
train_iter_loss: 0.09031788259744644
train_iter_loss: 0.14782582223415375
train_iter_loss: 0.09990647435188293
train_iter_loss: 0.1180553138256073
train_iter_loss: 0.07877759635448456
train_iter_loss: 0.11493504047393799
train_iter_loss: 0.21352463960647583
train_iter_loss: 0.07973227649927139
train_iter_loss: 0.1422131359577179
train_iter_loss: 0.13126009702682495
train_iter_loss: 0.14610044658184052
train_iter_loss: 0.28261882066726685
train_iter_loss: 0.15887127816677094
train_iter_loss: 0.1768035888671875
train_iter_loss: 0.16108644008636475
train_iter_loss: 0.09157023578882217
train_iter_loss: 0.16071243584156036
train_iter_loss: 0.2731771469116211
train loss :0.1627
---------------------
Validation seg loss: 0.21562113096269797 at epoch 666
epoch =    667/  1000, exp = train
train_iter_loss: 0.11912491172552109
train_iter_loss: 0.14308620989322662
train_iter_loss: 0.16255901753902435
train_iter_loss: 0.12184784561395645
train_iter_loss: 0.14084254205226898
train_iter_loss: 0.12203002721071243
train_iter_loss: 0.11941076070070267
train_iter_loss: 0.14048373699188232
train_iter_loss: 0.06107756122946739
train_iter_loss: 0.1741628646850586
train_iter_loss: 0.19194549322128296
train_iter_loss: 0.2410651445388794
train_iter_loss: 0.1437198668718338
train_iter_loss: 0.13711759448051453
train_iter_loss: 0.12234880030155182
train_iter_loss: 0.28197339177131653
train_iter_loss: 0.1840333640575409
train_iter_loss: 0.11365818232297897
train_iter_loss: 0.1601877510547638
train_iter_loss: 0.17271198332309723
train_iter_loss: 0.16595883667469025
train_iter_loss: 0.281083345413208
train_iter_loss: 0.06905432790517807
train_iter_loss: 0.1700560450553894
train_iter_loss: 0.14708422124385834
train_iter_loss: 0.24812309443950653
train_iter_loss: 0.16697324812412262
train_iter_loss: 0.16487713158130646
train_iter_loss: 0.25088635087013245
train_iter_loss: 0.1948329657316208
train_iter_loss: 0.1723349541425705
train_iter_loss: 0.1746208220720291
train_iter_loss: 0.1383514106273651
train_iter_loss: 0.12647348642349243
train_iter_loss: 0.035238634794950485
train_iter_loss: 0.11805622279644012
train_iter_loss: 0.17165030539035797
train_iter_loss: 0.14312881231307983
train_iter_loss: 0.24173884093761444
train_iter_loss: 0.2257409691810608
train_iter_loss: 0.23212799429893494
train_iter_loss: 0.028330521658062935
train_iter_loss: 0.36264169216156006
train_iter_loss: 0.0918053463101387
train_iter_loss: 0.11968491971492767
train_iter_loss: 0.10831265151500702
train_iter_loss: 0.12984201312065125
train_iter_loss: 0.2070264220237732
train_iter_loss: 0.08261322230100632
train_iter_loss: 0.07505423575639725
train_iter_loss: 0.14587408304214478
train_iter_loss: 0.05591854825615883
train_iter_loss: 0.07939736545085907
train_iter_loss: 0.3449251353740692
train_iter_loss: 0.20812556147575378
train_iter_loss: 0.12332044541835785
train_iter_loss: 0.12997815012931824
train_iter_loss: 0.1308610439300537
train_iter_loss: 0.07604110985994339
train_iter_loss: 0.10955072194337845
train_iter_loss: 0.14658261835575104
train_iter_loss: 0.1478516161441803
train_iter_loss: 0.12181200832128525
train_iter_loss: 0.13993339240550995
train_iter_loss: 0.09321485459804535
train_iter_loss: 0.219276562333107
train_iter_loss: 0.10281545668840408
train_iter_loss: 0.2294275462627411
train_iter_loss: 0.1896451860666275
train_iter_loss: 0.12563645839691162
train_iter_loss: 0.09625612944364548
train_iter_loss: 0.20527788996696472
train_iter_loss: 0.19120393693447113
train_iter_loss: 0.12252174317836761
train_iter_loss: 0.303819864988327
train_iter_loss: 0.1351044923067093
train_iter_loss: 0.16405431926250458
train_iter_loss: 0.12531745433807373
train_iter_loss: 0.030117416754364967
train_iter_loss: 0.1925758421421051
train_iter_loss: 0.09855465590953827
train_iter_loss: 0.1838480532169342
train_iter_loss: 0.257732093334198
train_iter_loss: 0.20255671441555023
train_iter_loss: 0.10186199843883514
train_iter_loss: 0.13292895257472992
train_iter_loss: 0.14849475026130676
train_iter_loss: 0.0861065536737442
train_iter_loss: 0.13218547403812408
train_iter_loss: 0.2674250304698944
train_iter_loss: 0.18751800060272217
train_iter_loss: 0.2089996039867401
train_iter_loss: 0.10505449771881104
train_iter_loss: 0.17589375376701355
train_iter_loss: 0.4942694902420044
train_iter_loss: 0.10561792552471161
train_iter_loss: 0.146301731467247
train_iter_loss: 0.24397866427898407
train_iter_loss: 0.16559109091758728
train_iter_loss: 0.1458292156457901
train loss :0.1614
---------------------
Validation seg loss: 0.21330628377157 at epoch 667
epoch =    668/  1000, exp = train
train_iter_loss: 0.18410718441009521
train_iter_loss: 0.09366539865732193
train_iter_loss: 0.10742849111557007
train_iter_loss: 0.20359012484550476
train_iter_loss: 0.14315563440322876
train_iter_loss: 0.08799917995929718
train_iter_loss: 0.16623790562152863
train_iter_loss: 0.19660677015781403
train_iter_loss: 0.05088329315185547
train_iter_loss: 0.12686000764369965
train_iter_loss: 0.10001760721206665
train_iter_loss: 0.1897982954978943
train_iter_loss: 0.12610547244548798
train_iter_loss: 0.1949026882648468
train_iter_loss: 0.0800376608967781
train_iter_loss: 0.11229169368743896
train_iter_loss: 0.12829406559467316
train_iter_loss: 0.13608436286449432
train_iter_loss: 0.12263020128011703
train_iter_loss: 0.1962481588125229
train_iter_loss: 0.389032244682312
train_iter_loss: 0.24643321335315704
train_iter_loss: 0.1269650161266327
train_iter_loss: 0.12905636429786682
train_iter_loss: 0.18642817437648773
train_iter_loss: 0.15251532196998596
train_iter_loss: 0.18384042382240295
train_iter_loss: 0.10899477452039719
train_iter_loss: 0.13333521783351898
train_iter_loss: 0.13959957659244537
train_iter_loss: 0.1730264276266098
train_iter_loss: 0.22677898406982422
train_iter_loss: 0.13143478333950043
train_iter_loss: 0.18393324315547943
train_iter_loss: 0.12801887094974518
train_iter_loss: 0.17453725636005402
train_iter_loss: 0.4405801296234131
train_iter_loss: 0.19119197130203247
train_iter_loss: 0.08140753209590912
train_iter_loss: 0.1711822897195816
train_iter_loss: 0.1707879900932312
train_iter_loss: 0.06111830100417137
train_iter_loss: 0.1669672429561615
train_iter_loss: 0.06842144578695297
train_iter_loss: 0.09955126792192459
train_iter_loss: 0.15807662904262543
train_iter_loss: 0.12723547220230103
train_iter_loss: 0.16924849152565002
train_iter_loss: 0.09140515327453613
train_iter_loss: 0.1364508867263794
train_iter_loss: 0.16066782176494598
train_iter_loss: 0.14026613533496857
train_iter_loss: 0.20887747406959534
train_iter_loss: 0.15838506817817688
train_iter_loss: 0.17651022970676422
train_iter_loss: 0.06176019459962845
train_iter_loss: 0.1780320703983307
train_iter_loss: 0.16183368861675262
train_iter_loss: 0.18676240742206573
train_iter_loss: 0.35137489438056946
train_iter_loss: 0.13380567729473114
train_iter_loss: 0.22241608798503876
train_iter_loss: 0.11467169225215912
train_iter_loss: 0.17574797570705414
train_iter_loss: 0.1106935515999794
train_iter_loss: 0.10932917892932892
train_iter_loss: 0.32811233401298523
train_iter_loss: 0.09237159043550491
train_iter_loss: 0.17301005125045776
train_iter_loss: 0.16093866527080536
train_iter_loss: 0.0721980407834053
train_iter_loss: 0.1270122081041336
train_iter_loss: 0.29393312335014343
train_iter_loss: 0.18198975920677185
train_iter_loss: 0.21021845936775208
train_iter_loss: 0.26559364795684814
train_iter_loss: 0.13153205811977386
train_iter_loss: 0.027584370225667953
train_iter_loss: 0.067954421043396
train_iter_loss: 0.1471337229013443
train_iter_loss: 0.2269253134727478
train_iter_loss: 0.17353853583335876
train_iter_loss: 0.11748276650905609
train_iter_loss: 0.09537559747695923
train_iter_loss: 0.07364970445632935
train_iter_loss: 0.1477499157190323
train_iter_loss: 0.21067780256271362
train_iter_loss: 0.22280187904834747
train_iter_loss: 0.113609679043293
train_iter_loss: 0.10531417280435562
train_iter_loss: 0.11761175841093063
train_iter_loss: 0.11540612578392029
train_iter_loss: 0.14939484000205994
train_iter_loss: 0.229786679148674
train_iter_loss: 0.11754459887742996
train_iter_loss: 0.06641406565904617
train_iter_loss: 0.3002904951572418
train_iter_loss: 0.08669605106115341
train_iter_loss: 0.0715341791510582
train_iter_loss: 0.15141397714614868
train loss :0.1568
---------------------
Validation seg loss: 0.21741374167350103 at epoch 668
epoch =    669/  1000, exp = train
train_iter_loss: 0.22055599093437195
train_iter_loss: 0.07370210438966751
train_iter_loss: 0.14796610176563263
train_iter_loss: 0.15152257680892944
train_iter_loss: 0.10006394982337952
train_iter_loss: 0.1740339994430542
train_iter_loss: 0.19024348258972168
train_iter_loss: 0.11879048496484756
train_iter_loss: 0.1493135541677475
train_iter_loss: 0.269084095954895
train_iter_loss: 0.13449294865131378
train_iter_loss: 0.09298405796289444
train_iter_loss: 0.12579037249088287
train_iter_loss: 0.2858622968196869
train_iter_loss: 0.16632014513015747
train_iter_loss: 0.30097800493240356
train_iter_loss: 0.13915692269802094
train_iter_loss: 0.07900995761156082
train_iter_loss: 0.11173231154680252
train_iter_loss: 0.2243291288614273
train_iter_loss: 0.2665052115917206
train_iter_loss: 0.08241641521453857
train_iter_loss: 0.12399107962846756
train_iter_loss: 0.14412519335746765
train_iter_loss: 0.1470434069633484
train_iter_loss: 0.1349462866783142
train_iter_loss: 0.17094764113426208
train_iter_loss: 0.14050689339637756
train_iter_loss: 0.08535436540842056
train_iter_loss: 0.12061387300491333
train_iter_loss: 0.18812784552574158
train_iter_loss: 0.12950539588928223
train_iter_loss: 0.10612601041793823
train_iter_loss: 0.1782616227865219
train_iter_loss: 0.07807973772287369
train_iter_loss: 0.17857353389263153
train_iter_loss: 0.14503294229507446
train_iter_loss: 0.1099788025021553
train_iter_loss: 0.1176571324467659
train_iter_loss: 0.14699901640415192
train_iter_loss: 0.156295508146286
train_iter_loss: 0.2367272526025772
train_iter_loss: 0.16901235282421112
train_iter_loss: 0.16104315221309662
train_iter_loss: 0.16181038320064545
train_iter_loss: 0.2081167846918106
train_iter_loss: 0.14663664996623993
train_iter_loss: 0.16158007085323334
train_iter_loss: 0.18860457837581635
train_iter_loss: 0.12133701145648956
train_iter_loss: 0.34763190150260925
train_iter_loss: 0.0520457960665226
train_iter_loss: 0.14916984736919403
train_iter_loss: 0.16390368342399597
train_iter_loss: 0.1431000828742981
train_iter_loss: 0.11249873042106628
train_iter_loss: 0.22535783052444458
train_iter_loss: 0.1649058312177658
train_iter_loss: 0.17419905960559845
train_iter_loss: 0.17360204458236694
train_iter_loss: 0.08510231971740723
train_iter_loss: 0.19840069115161896
train_iter_loss: 0.19083376228809357
train_iter_loss: 0.09978874027729034
train_iter_loss: 0.10466983914375305
train_iter_loss: 0.10450316965579987
train_iter_loss: 0.13397613167762756
train_iter_loss: 0.2208396941423416
train_iter_loss: 0.20948822796344757
train_iter_loss: 0.2675878405570984
train_iter_loss: 0.27510929107666016
train_iter_loss: 0.15320801734924316
train_iter_loss: 0.1377607136964798
train_iter_loss: 0.16299889981746674
train_iter_loss: 0.17492732405662537
train_iter_loss: 0.22687147557735443
train_iter_loss: 0.16245430707931519
train_iter_loss: 0.11469267308712006
train_iter_loss: 0.19157245755195618
train_iter_loss: 0.17025445401668549
train_iter_loss: 0.14531812071800232
train_iter_loss: 0.19905723631381989
train_iter_loss: 0.09172460436820984
train_iter_loss: 0.18616223335266113
train_iter_loss: 0.21671180427074432
train_iter_loss: 0.10589359700679779
train_iter_loss: 0.14509041607379913
train_iter_loss: 0.1352853626012802
train_iter_loss: 0.2870102524757385
train_iter_loss: 0.18755018711090088
train_iter_loss: 0.18349117040634155
train_iter_loss: 0.19832471013069153
train_iter_loss: 0.07929090410470963
train_iter_loss: 0.16354389488697052
train_iter_loss: 0.16220806539058685
train_iter_loss: 0.15649376809597015
train_iter_loss: 0.13563697040081024
train_iter_loss: 0.13876518607139587
train_iter_loss: 0.040888722985982895
train_iter_loss: 0.15519745647907257
train loss :0.1612
---------------------
Validation seg loss: 0.21029424955541232 at epoch 669
********************
best_val_epoch_loss:  0.21029424955541232
MODEL UPDATED
epoch =    670/  1000, exp = train
train_iter_loss: 0.07426023483276367
train_iter_loss: 0.15013547241687775
train_iter_loss: 0.05634189769625664
train_iter_loss: 0.057227034121751785
train_iter_loss: 0.18029743432998657
train_iter_loss: 0.041482675820589066
train_iter_loss: 0.18290802836418152
train_iter_loss: 0.19487786293029785
train_iter_loss: 0.22139102220535278
train_iter_loss: 0.16235075891017914
train_iter_loss: 0.12435443699359894
train_iter_loss: 0.23112010955810547
train_iter_loss: 0.13826721906661987
train_iter_loss: 0.10276317596435547
train_iter_loss: 0.3155031204223633
train_iter_loss: 0.13264913856983185
train_iter_loss: 0.09431236982345581
train_iter_loss: 0.11184000968933105
train_iter_loss: 0.06606478989124298
train_iter_loss: 0.12134576588869095
train_iter_loss: 0.24430763721466064
train_iter_loss: 0.13134996592998505
train_iter_loss: 0.22222180664539337
train_iter_loss: 0.16988375782966614
train_iter_loss: 0.16939815878868103
train_iter_loss: 0.12005028128623962
train_iter_loss: 0.1936286836862564
train_iter_loss: 0.15077146887779236
train_iter_loss: 0.1903478056192398
train_iter_loss: 0.08568897098302841
train_iter_loss: 0.19468967616558075
train_iter_loss: 0.046320606023073196
train_iter_loss: 0.19875936210155487
train_iter_loss: 0.21934367716312408
train_iter_loss: 0.084438756108284
train_iter_loss: 0.37793388962745667
train_iter_loss: 0.10458839684724808
train_iter_loss: 0.13153669238090515
train_iter_loss: 0.15361052751541138
train_iter_loss: 0.10300462692975998
train_iter_loss: 0.10952604562044144
train_iter_loss: 0.1318095326423645
train_iter_loss: 0.09452307969331741
train_iter_loss: 0.31395888328552246
train_iter_loss: 0.18831215798854828
train_iter_loss: 0.18936461210250854
train_iter_loss: 0.20852771401405334
train_iter_loss: 0.12492303550243378
train_iter_loss: 0.16370408236980438
train_iter_loss: 0.2985101342201233
train_iter_loss: 0.10523752123117447
train_iter_loss: 0.18824894726276398
train_iter_loss: 0.05706745386123657
train_iter_loss: 0.1875845491886139
train_iter_loss: 0.20835833251476288
train_iter_loss: 0.13383160531520844
train_iter_loss: 0.05392749607563019
train_iter_loss: 0.22215355932712555
train_iter_loss: 0.18957288563251495
train_iter_loss: 0.16430097818374634
train_iter_loss: 0.1303262710571289
train_iter_loss: 0.2920280396938324
train_iter_loss: 0.12497924268245697
train_iter_loss: 0.11208660900592804
train_iter_loss: 0.06646129488945007
train_iter_loss: 0.1825615018606186
train_iter_loss: 0.059970833361148834
train_iter_loss: 0.1388641893863678
train_iter_loss: 0.07840698212385178
train_iter_loss: 0.13187769055366516
train_iter_loss: 0.1850501447916031
train_iter_loss: 0.17919921875
train_iter_loss: 0.1560831069946289
train_iter_loss: 0.28025293350219727
train_iter_loss: 0.10800263285636902
train_iter_loss: 0.18903185427188873
train_iter_loss: 0.22778069972991943
train_iter_loss: 0.14660051465034485
train_iter_loss: 0.1412518173456192
train_iter_loss: 0.05047876387834549
train_iter_loss: 0.2424798160791397
train_iter_loss: 0.11211203038692474
train_iter_loss: 0.10017052292823792
train_iter_loss: 0.11381716281175613
train_iter_loss: 0.21340608596801758
train_iter_loss: 0.2180166095495224
train_iter_loss: 0.153159499168396
train_iter_loss: 0.14036166667938232
train_iter_loss: 0.0614805743098259
train_iter_loss: 0.23351575434207916
train_iter_loss: 0.08705660700798035
train_iter_loss: 0.1205291599035263
train_iter_loss: 0.28875091671943665
train_iter_loss: 0.14620105922222137
train_iter_loss: 0.10117664188146591
train_iter_loss: 0.21266166865825653
train_iter_loss: 0.13029362261295319
train_iter_loss: 0.14494581520557404
train_iter_loss: 0.3760325014591217
train_iter_loss: 0.23459260165691376
train loss :0.1596
---------------------
Validation seg loss: 0.21678544425423135 at epoch 670
epoch =    671/  1000, exp = train
train_iter_loss: 0.09229642897844315
train_iter_loss: 0.19909603893756866
train_iter_loss: 0.06637021154165268
train_iter_loss: 0.10442253202199936
train_iter_loss: 0.19533222913742065
train_iter_loss: 0.1325937956571579
train_iter_loss: 0.127289816737175
train_iter_loss: 0.1137784942984581
train_iter_loss: 0.12641888856887817
train_iter_loss: 0.21819885075092316
train_iter_loss: 0.2676152288913727
train_iter_loss: 0.17076249420642853
train_iter_loss: 0.22388187050819397
train_iter_loss: 0.09133520722389221
train_iter_loss: 0.19817286729812622
train_iter_loss: 0.1296098232269287
train_iter_loss: 0.19729703664779663
train_iter_loss: 0.21443498134613037
train_iter_loss: 0.17671672999858856
train_iter_loss: 0.17503714561462402
train_iter_loss: 0.12590274214744568
train_iter_loss: 0.11191918700933456
train_iter_loss: 0.18750730156898499
train_iter_loss: 0.168217733502388
train_iter_loss: 0.1999998539686203
train_iter_loss: 0.2155051976442337
train_iter_loss: 0.09088268131017685
train_iter_loss: 0.19898155331611633
train_iter_loss: 0.09818658232688904
train_iter_loss: 0.06799980998039246
train_iter_loss: 0.2979651689529419
train_iter_loss: 0.07795480638742447
train_iter_loss: 0.08768796175718307
train_iter_loss: 0.08944229781627655
train_iter_loss: 0.1018558144569397
train_iter_loss: 0.06457112729549408
train_iter_loss: 0.1637992560863495
train_iter_loss: 0.24480611085891724
train_iter_loss: 0.14339138567447662
train_iter_loss: 0.10808926820755005
train_iter_loss: 0.18781335651874542
train_iter_loss: 0.06194206327199936
train_iter_loss: 0.12398990988731384
train_iter_loss: 0.16512514650821686
train_iter_loss: 0.2361682653427124
train_iter_loss: 0.10482137650251389
train_iter_loss: 0.14111486077308655
train_iter_loss: 0.12003244459629059
train_iter_loss: 0.17339760065078735
train_iter_loss: 0.1499210000038147
train_iter_loss: 0.25868043303489685
train_iter_loss: 0.12943050265312195
train_iter_loss: 0.11883297562599182
train_iter_loss: 0.18611568212509155
train_iter_loss: 0.1373922973871231
train_iter_loss: 0.19713151454925537
train_iter_loss: 0.159513920545578
train_iter_loss: 0.22069169580936432
train_iter_loss: 0.29030418395996094
train_iter_loss: 0.18168486654758453
train_iter_loss: 0.2563912570476532
train_iter_loss: 0.1245635598897934
train_iter_loss: 0.15492814779281616
train_iter_loss: 0.1911783069372177
train_iter_loss: 0.11944080889225006
train_iter_loss: 0.15388154983520508
train_iter_loss: 0.11013736575841904
train_iter_loss: 0.08848325908184052
train_iter_loss: 0.10034442692995071
train_iter_loss: 0.22579598426818848
train_iter_loss: 0.13071425259113312
train_iter_loss: 0.21325068175792694
train_iter_loss: 0.21104899048805237
train_iter_loss: 0.17746484279632568
train_iter_loss: 0.18549740314483643
train_iter_loss: 0.11361915618181229
train_iter_loss: 0.19587647914886475
train_iter_loss: 0.09230530261993408
train_iter_loss: 0.14778758585453033
train_iter_loss: 0.13907060027122498
train_iter_loss: 0.15135203301906586
train_iter_loss: 0.09239937365055084
train_iter_loss: 0.23542256653308868
train_iter_loss: 0.15191452205181122
train_iter_loss: 0.2387719303369522
train_iter_loss: 0.1782250553369522
train_iter_loss: 0.11316339671611786
train_iter_loss: 0.059165116399526596
train_iter_loss: 0.077792227268219
train_iter_loss: 0.08011367172002792
train_iter_loss: 0.2179645448923111
train_iter_loss: 0.1830361783504486
train_iter_loss: 0.173930361866951
train_iter_loss: 0.18733316659927368
train_iter_loss: 0.2080310881137848
train_iter_loss: 0.28948694467544556
train_iter_loss: 0.1487426459789276
train_iter_loss: 0.21101459860801697
train_iter_loss: 0.17713548243045807
train_iter_loss: 0.08621322363615036
train loss :0.1596
---------------------
Validation seg loss: 0.2206372943935248 at epoch 671
epoch =    672/  1000, exp = train
train_iter_loss: 0.1591676026582718
train_iter_loss: 0.12062551081180573
train_iter_loss: 0.1312425583600998
train_iter_loss: 0.1478092521429062
train_iter_loss: 0.1509198397397995
train_iter_loss: 0.13456688821315765
train_iter_loss: 0.20885953307151794
train_iter_loss: 0.20532788336277008
train_iter_loss: 0.12158922106027603
train_iter_loss: 0.1849808394908905
train_iter_loss: 0.20578579604625702
train_iter_loss: 0.15685509145259857
train_iter_loss: 0.20722942054271698
train_iter_loss: 0.13531097769737244
train_iter_loss: 0.14749979972839355
train_iter_loss: 0.2706471085548401
train_iter_loss: 0.11988838762044907
train_iter_loss: 0.15314604341983795
train_iter_loss: 0.1374279111623764
train_iter_loss: 0.05715100094676018
train_iter_loss: 0.29465731978416443
train_iter_loss: 0.14441664516925812
train_iter_loss: 0.17316289246082306
train_iter_loss: 0.07283490151166916
train_iter_loss: 0.1827392429113388
train_iter_loss: 0.25323954224586487
train_iter_loss: 0.12573890388011932
train_iter_loss: 0.1843756139278412
train_iter_loss: 0.11988178640604019
train_iter_loss: 0.26054468750953674
train_iter_loss: 0.10501720756292343
train_iter_loss: 0.19122202694416046
train_iter_loss: 0.15536785125732422
train_iter_loss: 0.19571134448051453
train_iter_loss: 0.1803220957517624
train_iter_loss: 0.12307700514793396
train_iter_loss: 0.16863593459129333
train_iter_loss: 0.11739421635866165
train_iter_loss: 0.09267925471067429
train_iter_loss: 0.1845741868019104
train_iter_loss: 0.12930823862552643
train_iter_loss: 0.13100659847259521
train_iter_loss: 0.17781494557857513
train_iter_loss: 0.12054948508739471
train_iter_loss: 0.11933551728725433
train_iter_loss: 0.20170858502388
train_iter_loss: 0.2484031319618225
train_iter_loss: 0.327841579914093
train_iter_loss: 0.14253345131874084
train_iter_loss: 0.10892578214406967
train_iter_loss: 0.27302461862564087
train_iter_loss: 0.21237611770629883
train_iter_loss: 0.14267800748348236
train_iter_loss: 0.13006380200386047
train_iter_loss: 0.15151941776275635
train_iter_loss: 0.1342400461435318
train_iter_loss: 0.11201083660125732
train_iter_loss: 0.1338670402765274
train_iter_loss: 0.18217891454696655
train_iter_loss: 0.1559412032365799
train_iter_loss: 0.19533562660217285
train_iter_loss: 0.06756778806447983
train_iter_loss: 0.2272057682275772
train_iter_loss: 0.1866634041070938
train_iter_loss: 0.1690177321434021
train_iter_loss: 0.41084617376327515
train_iter_loss: 0.21501567959785461
train_iter_loss: 0.11977235972881317
train_iter_loss: 0.21792249381542206
train_iter_loss: 0.12015372514724731
train_iter_loss: 0.12454768270254135
train_iter_loss: 0.18150301277637482
train_iter_loss: 0.07616349309682846
train_iter_loss: 0.1960465908050537
train_iter_loss: 0.1870730221271515
train_iter_loss: 0.13940566778182983
train_iter_loss: 0.16139313578605652
train_iter_loss: 0.14412835240364075
train_iter_loss: 0.1756981760263443
train_iter_loss: 0.2531338036060333
train_iter_loss: 0.11790048331022263
train_iter_loss: 0.24305766820907593
train_iter_loss: 0.1240311712026596
train_iter_loss: 0.13393646478652954
train_iter_loss: 0.21492576599121094
train_iter_loss: 0.2065439671278
train_iter_loss: 0.07608330994844437
train_iter_loss: 0.3109626770019531
train_iter_loss: 0.14062747359275818
train_iter_loss: 0.2671809494495392
train_iter_loss: 0.1597636640071869
train_iter_loss: 0.16423167288303375
train_iter_loss: 0.10054034739732742
train_iter_loss: 0.09081420302391052
train_iter_loss: 0.10551171004772186
train_iter_loss: 0.08227372169494629
train_iter_loss: 0.21913711726665497
train_iter_loss: 0.1120549663901329
train_iter_loss: 0.0884251818060875
train_iter_loss: 0.11832506209611893
train loss :0.1662
---------------------
Validation seg loss: 0.2134029227572511 at epoch 672
epoch =    673/  1000, exp = train
train_iter_loss: 0.18087397515773773
train_iter_loss: 0.12973909080028534
train_iter_loss: 0.20607303082942963
train_iter_loss: 0.16250598430633545
train_iter_loss: 0.024434836581349373
train_iter_loss: 0.15443377196788788
train_iter_loss: 0.13783416152000427
train_iter_loss: 0.13693884015083313
train_iter_loss: 0.22672152519226074
train_iter_loss: 0.27862194180488586
train_iter_loss: 0.26366519927978516
train_iter_loss: 0.1582181602716446
train_iter_loss: 0.16880039870738983
train_iter_loss: 0.06662488728761673
train_iter_loss: 0.09575794637203217
train_iter_loss: 0.35445213317871094
train_iter_loss: 0.15465860068798065
train_iter_loss: 0.09509055316448212
train_iter_loss: 0.1474945843219757
train_iter_loss: 0.13229094445705414
train_iter_loss: 0.11388282477855682
train_iter_loss: 0.11299565434455872
train_iter_loss: 0.19485080242156982
train_iter_loss: 0.15116457641124725
train_iter_loss: 0.09860842674970627
train_iter_loss: 0.16021810472011566
train_iter_loss: 0.09725670516490936
train_iter_loss: 0.37427639961242676
train_iter_loss: 0.22693729400634766
train_iter_loss: 0.13990484178066254
train_iter_loss: 0.2542724907398224
train_iter_loss: 0.11982599645853043
train_iter_loss: 0.1002752035856247
train_iter_loss: 0.09872978925704956
train_iter_loss: 0.047050684690475464
train_iter_loss: 0.1284801959991455
train_iter_loss: 0.19933688640594482
train_iter_loss: 0.1971767693758011
train_iter_loss: 0.18247191607952118
train_iter_loss: 0.09472092241048813
train_iter_loss: 0.1989443302154541
train_iter_loss: 0.21129386126995087
train_iter_loss: 0.24213750660419464
train_iter_loss: 0.18883581459522247
train_iter_loss: 0.1255401372909546
train_iter_loss: 0.39285922050476074
train_iter_loss: 0.21875259280204773
train_iter_loss: 0.14031483232975006
train_iter_loss: 0.17085051536560059
train_iter_loss: 0.10141588747501373
train_iter_loss: 0.14893151819705963
train_iter_loss: 0.15657533705234528
train_iter_loss: 0.3377135992050171
train_iter_loss: 0.11887115985155106
train_iter_loss: 0.26964011788368225
train_iter_loss: 0.12542934715747833
train_iter_loss: 0.22619707882404327
train_iter_loss: 0.2234773188829422
train_iter_loss: 0.1137680783867836
train_iter_loss: 0.07732133567333221
train_iter_loss: 0.09203387796878815
train_iter_loss: 0.1665121465921402
train_iter_loss: 0.2957669198513031
train_iter_loss: 0.1474332958459854
train_iter_loss: 0.11160273104906082
train_iter_loss: 0.1748810112476349
train_iter_loss: 0.08167002350091934
train_iter_loss: 0.146511048078537
train_iter_loss: 0.31156080961227417
train_iter_loss: 0.13096867501735687
train_iter_loss: 0.23092831671237946
train_iter_loss: 0.09913560003042221
train_iter_loss: 0.12567898631095886
train_iter_loss: 0.1558842957019806
train_iter_loss: 0.14468616247177124
train_iter_loss: 0.09420366585254669
train_iter_loss: 0.2691350281238556
train_iter_loss: 0.1191977933049202
train_iter_loss: 0.16194288432598114
train_iter_loss: 0.09509485214948654
train_iter_loss: 0.16352766752243042
train_iter_loss: 0.18658378720283508
train_iter_loss: 0.15401914715766907
train_iter_loss: 0.15445809066295624
train_iter_loss: 0.1497832089662552
train_iter_loss: 0.16345368325710297
train_iter_loss: 0.22054143249988556
train_iter_loss: 0.13941434025764465
train_iter_loss: 0.08825795352458954
train_iter_loss: 0.20291870832443237
train_iter_loss: 0.1916552633047104
train_iter_loss: 0.13344135880470276
train_iter_loss: 0.07952415943145752
train_iter_loss: 0.119228795170784
train_iter_loss: 0.13295546174049377
train_iter_loss: 0.09944082051515579
train_iter_loss: 0.10784498602151871
train_iter_loss: 0.11860667169094086
train_iter_loss: 0.19440627098083496
train_iter_loss: 0.19141057133674622
train loss :0.1647
---------------------
Validation seg loss: 0.21428712255339016 at epoch 673
epoch =    674/  1000, exp = train
train_iter_loss: 0.11452052742242813
train_iter_loss: 0.2468026727437973
train_iter_loss: 0.08161073923110962
train_iter_loss: 0.17662446200847626
train_iter_loss: 0.03229023516178131
train_iter_loss: 0.13944053649902344
train_iter_loss: 0.2204020470380783
train_iter_loss: 0.15535509586334229
train_iter_loss: 0.27220797538757324
train_iter_loss: 0.20338024199008942
train_iter_loss: 0.31158870458602905
train_iter_loss: 0.11224798858165741
train_iter_loss: 0.2319406121969223
train_iter_loss: 0.1651434451341629
train_iter_loss: 0.09036263823509216
train_iter_loss: 0.1367991715669632
train_iter_loss: 0.3116828203201294
train_iter_loss: 0.10992663353681564
train_iter_loss: 0.08897858113050461
train_iter_loss: 0.15910491347312927
train_iter_loss: 0.2174031138420105
train_iter_loss: 0.11735108494758606
train_iter_loss: 0.10523343831300735
train_iter_loss: 0.20198801159858704
train_iter_loss: 0.10073886811733246
train_iter_loss: 0.1350153386592865
train_iter_loss: 0.18395045399665833
train_iter_loss: 0.05539809539914131
train_iter_loss: 0.22910694777965546
train_iter_loss: 0.1532098650932312
train_iter_loss: 0.16565223038196564
train_iter_loss: 0.18933133780956268
train_iter_loss: 0.1806683987379074
train_iter_loss: 0.08586651086807251
train_iter_loss: 0.23942679166793823
train_iter_loss: 0.1880432814359665
train_iter_loss: 0.4902750551700592
train_iter_loss: 0.21440546214580536
train_iter_loss: 0.1975051909685135
train_iter_loss: 0.05324310436844826
train_iter_loss: 0.14595508575439453
train_iter_loss: 0.14909696578979492
train_iter_loss: 0.1502162516117096
train_iter_loss: 0.19079768657684326
train_iter_loss: 0.14844830334186554
train_iter_loss: 0.14792409539222717
train_iter_loss: 0.08139688521623611
train_iter_loss: 0.12841255962848663
train_iter_loss: 0.162253737449646
train_iter_loss: 0.15832418203353882
train_iter_loss: 0.1317383348941803
train_iter_loss: 0.21113760769367218
train_iter_loss: 0.1774878054857254
train_iter_loss: 0.12991061806678772
train_iter_loss: 0.12457377463579178
train_iter_loss: 0.15734721720218658
train_iter_loss: 0.06363522261381149
train_iter_loss: 0.22166334092617035
train_iter_loss: 0.1143028661608696
train_iter_loss: 0.13147035241127014
train_iter_loss: 0.10722927004098892
train_iter_loss: 0.17876970767974854
train_iter_loss: 0.03879975154995918
train_iter_loss: 0.18202534317970276
train_iter_loss: 0.24277839064598083
train_iter_loss: 0.08863012492656708
train_iter_loss: 0.20614774525165558
train_iter_loss: 0.22490008175373077
train_iter_loss: 0.1283789873123169
train_iter_loss: 0.138800248503685
train_iter_loss: 0.22384075820446014
train_iter_loss: 0.1413888782262802
train_iter_loss: 0.07321450859308243
train_iter_loss: 0.18969543278217316
train_iter_loss: 0.09471970796585083
train_iter_loss: 0.11743234097957611
train_iter_loss: 0.06590381264686584
train_iter_loss: 0.2389778345823288
train_iter_loss: 0.22907647490501404
train_iter_loss: 0.14728617668151855
train_iter_loss: 0.11844140291213989
train_iter_loss: 0.19810271263122559
train_iter_loss: 0.10779917240142822
train_iter_loss: 0.09875611215829849
train_iter_loss: 0.12493129074573517
train_iter_loss: 0.19313085079193115
train_iter_loss: 0.16884134709835052
train_iter_loss: 0.18247759342193604
train_iter_loss: 0.15613791346549988
train_iter_loss: 0.12519411742687225
train_iter_loss: 0.1609809398651123
train_iter_loss: 0.1683018058538437
train_iter_loss: 0.06989013403654099
train_iter_loss: 0.27875298261642456
train_iter_loss: 0.16116245090961456
train_iter_loss: 0.16374145448207855
train_iter_loss: 0.17171649634838104
train_iter_loss: 0.09853045642375946
train_iter_loss: 0.17352387309074402
train_iter_loss: 0.23035530745983124
train loss :0.1616
---------------------
Validation seg loss: 0.21543726708986005 at epoch 674
epoch =    675/  1000, exp = train
train_iter_loss: 0.13892538845539093
train_iter_loss: 0.22602598369121552
train_iter_loss: 0.16529841721057892
train_iter_loss: 0.1435096710920334
train_iter_loss: 0.08766964823007584
train_iter_loss: 0.18747402727603912
train_iter_loss: 0.17242617905139923
train_iter_loss: 0.14289891719818115
train_iter_loss: 0.2549162805080414
train_iter_loss: 0.2182552069425583
train_iter_loss: 0.028002846986055374
train_iter_loss: 0.1756327897310257
train_iter_loss: 0.11745007336139679
train_iter_loss: 0.24247045814990997
train_iter_loss: 0.17030709981918335
train_iter_loss: 0.17990250885486603
train_iter_loss: 0.14779049158096313
train_iter_loss: 0.2502773106098175
train_iter_loss: 0.19452567398548126
train_iter_loss: 0.07521938532590866
train_iter_loss: 0.16617099940776825
train_iter_loss: 0.27219951152801514
train_iter_loss: 0.2286040186882019
train_iter_loss: 0.1312606781721115
train_iter_loss: 0.20973096787929535
train_iter_loss: 0.12447867542505264
train_iter_loss: 0.09565680474042892
train_iter_loss: 0.0842171236872673
train_iter_loss: 0.22106003761291504
train_iter_loss: 0.11201952397823334
train_iter_loss: 0.15159641206264496
train_iter_loss: 0.10106164216995239
train_iter_loss: 0.06259957700967789
train_iter_loss: 0.3185509145259857
train_iter_loss: 0.17754454910755157
train_iter_loss: 0.1172836646437645
train_iter_loss: 0.2854573726654053
train_iter_loss: 0.1797720491886139
train_iter_loss: 0.1939287632703781
train_iter_loss: 0.1116228848695755
train_iter_loss: 0.20258842408657074
train_iter_loss: 0.07900119572877884
train_iter_loss: 0.14639364182949066
train_iter_loss: 0.19924868643283844
train_iter_loss: 0.1529514640569687
train_iter_loss: 0.23970894515514374
train_iter_loss: 0.1825639009475708
train_iter_loss: 0.09833299368619919
train_iter_loss: 0.20204198360443115
train_iter_loss: 0.12404675036668777
train_iter_loss: 0.09733585268259048
train_iter_loss: 0.11875993013381958
train_iter_loss: 0.09263554215431213
train_iter_loss: 0.1426379233598709
train_iter_loss: 0.1457502841949463
train_iter_loss: 0.1473744660615921
train_iter_loss: 0.08282700926065445
train_iter_loss: 0.18101465702056885
train_iter_loss: 0.09807570278644562
train_iter_loss: 0.19311851263046265
train_iter_loss: 0.16670668125152588
train_iter_loss: 0.17557621002197266
train_iter_loss: 0.10399507731199265
train_iter_loss: 0.18107837438583374
train_iter_loss: 0.19986574351787567
train_iter_loss: 0.1810290813446045
train_iter_loss: 0.24842092394828796
train_iter_loss: 0.17773044109344482
train_iter_loss: 0.1003340631723404
train_iter_loss: 0.14184124767780304
train_iter_loss: 0.15488551557064056
train_iter_loss: 0.05174778774380684
train_iter_loss: 0.19907276332378387
train_iter_loss: 0.14728960394859314
train_iter_loss: 0.1060136929154396
train_iter_loss: 0.08517366647720337
train_iter_loss: 0.14536349475383759
train_iter_loss: 0.14423781633377075
train_iter_loss: 0.15286630392074585
train_iter_loss: 0.17998799681663513
train_iter_loss: 0.22322750091552734
train_iter_loss: 0.09182456135749817
train_iter_loss: 0.14141903817653656
train_iter_loss: 0.14674882590770721
train_iter_loss: 0.2736701965332031
train_iter_loss: 0.13260474801063538
train_iter_loss: 0.10573790967464447
train_iter_loss: 0.06599462032318115
train_iter_loss: 0.05778805911540985
train_iter_loss: 0.14670483767986298
train_iter_loss: 0.21355846524238586
train_iter_loss: 0.2028883695602417
train_iter_loss: 0.18851996958255768
train_iter_loss: 0.1679629534482956
train_iter_loss: 0.11386313289403915
train_iter_loss: 0.17069710791110992
train_iter_loss: 0.24711774289608002
train_iter_loss: 0.08657453954219818
train_iter_loss: 0.09846556186676025
train_iter_loss: 0.08098557591438293
train loss :0.1575
---------------------
Validation seg loss: 0.21801973486600337 at epoch 675
epoch =    676/  1000, exp = train
train_iter_loss: 0.10213866829872131
train_iter_loss: 0.16568376123905182
train_iter_loss: 0.25526660680770874
train_iter_loss: 0.19702483713626862
train_iter_loss: 0.15375539660453796
train_iter_loss: 0.13253429532051086
train_iter_loss: 0.3675311803817749
train_iter_loss: 0.044495172798633575
train_iter_loss: 0.14334064722061157
train_iter_loss: 0.08960684388875961
train_iter_loss: 0.14489394426345825
train_iter_loss: 0.12166301161050797
train_iter_loss: 0.21210606396198273
train_iter_loss: 0.26955217123031616
train_iter_loss: 0.11561129987239838
train_iter_loss: 0.07588434219360352
train_iter_loss: 0.17665797472000122
train_iter_loss: 0.1667982041835785
train_iter_loss: 0.17355011403560638
train_iter_loss: 0.1476062536239624
train_iter_loss: 0.15657635033130646
train_iter_loss: 0.2562284469604492
train_iter_loss: 0.12994255125522614
train_iter_loss: 0.12331758439540863
train_iter_loss: 0.10822290182113647
train_iter_loss: 0.22999237477779388
train_iter_loss: 0.2291550487279892
train_iter_loss: 0.04368022456765175
train_iter_loss: 0.13341297209262848
train_iter_loss: 0.13767415285110474
train_iter_loss: 0.17439012229442596
train_iter_loss: 0.19785697758197784
train_iter_loss: 0.17635825276374817
train_iter_loss: 0.1143861636519432
train_iter_loss: 0.21658459305763245
train_iter_loss: 0.20155976712703705
train_iter_loss: 0.056447237730026245
train_iter_loss: 0.10759023576974869
train_iter_loss: 0.15793032944202423
train_iter_loss: 0.16122815012931824
train_iter_loss: 0.235473170876503
train_iter_loss: 0.10330066829919815
train_iter_loss: 0.1656162589788437
train_iter_loss: 0.16373182833194733
train_iter_loss: 0.10415995866060257
train_iter_loss: 0.03704448416829109
train_iter_loss: 0.22170422971248627
train_iter_loss: 0.1042243242263794
train_iter_loss: 0.23216232657432556
train_iter_loss: 0.22437644004821777
train_iter_loss: 0.1616663932800293
train_iter_loss: 0.11537820845842361
train_iter_loss: 0.13472987711429596
train_iter_loss: 0.08423210680484772
train_iter_loss: 0.12285477668046951
train_iter_loss: 0.15935802459716797
train_iter_loss: 0.12459482997655869
train_iter_loss: 0.04537658765912056
train_iter_loss: 0.12503553926944733
train_iter_loss: 0.1155998632311821
train_iter_loss: 0.10502590239048004
train_iter_loss: 0.17031945288181305
train_iter_loss: 0.17991074919700623
train_iter_loss: 0.16654706001281738
train_iter_loss: 0.17850692570209503
train_iter_loss: 0.1662084460258484
train_iter_loss: 0.13635016977787018
train_iter_loss: 0.17226429283618927
train_iter_loss: 0.1537533402442932
train_iter_loss: 0.17900536954402924
train_iter_loss: 0.28358906507492065
train_iter_loss: 0.24479089677333832
train_iter_loss: 0.15658767521381378
train_iter_loss: 0.098089799284935
train_iter_loss: 0.1290649175643921
train_iter_loss: 0.05575684458017349
train_iter_loss: 0.09243369847536087
train_iter_loss: 0.14219240844249725
train_iter_loss: 0.24856552481651306
train_iter_loss: 0.2071811854839325
train_iter_loss: 0.1971604973077774
train_iter_loss: 0.1354035586118698
train_iter_loss: 0.1347152143716812
train_iter_loss: 0.1170724406838417
train_iter_loss: 0.17611804604530334
train_iter_loss: 0.18946939706802368
train_iter_loss: 0.40359872579574585
train_iter_loss: 0.08775325119495392
train_iter_loss: 0.059614699333906174
train_iter_loss: 0.1353130340576172
train_iter_loss: 0.26143041253089905
train_iter_loss: 0.11224745213985443
train_iter_loss: 0.23883450031280518
train_iter_loss: 0.17036041617393494
train_iter_loss: 0.09990415722131729
train_iter_loss: 0.2930763065814972
train_iter_loss: 0.1268925815820694
train_iter_loss: 0.12467703223228455
train_iter_loss: 0.14259874820709229
train_iter_loss: 0.1541721522808075
train loss :0.1593
---------------------
Validation seg loss: 0.21450009043240603 at epoch 676
epoch =    677/  1000, exp = train
train_iter_loss: 0.19299815595149994
train_iter_loss: 0.2116621881723404
train_iter_loss: 0.1360790878534317
train_iter_loss: 0.14322982728481293
train_iter_loss: 0.14358828961849213
train_iter_loss: 0.17932578921318054
train_iter_loss: 0.2024732083082199
train_iter_loss: 0.07087714225053787
train_iter_loss: 0.22894001007080078
train_iter_loss: 0.2143556922674179
train_iter_loss: 0.1853851079940796
train_iter_loss: 0.1928093582391739
train_iter_loss: 0.17532047629356384
train_iter_loss: 0.23357151448726654
train_iter_loss: 0.12096156179904938
train_iter_loss: 0.14463891088962555
train_iter_loss: 0.22916105389595032
train_iter_loss: 0.12861430644989014
train_iter_loss: 0.15368570387363434
train_iter_loss: 0.20308290421962738
train_iter_loss: 0.13951098918914795
train_iter_loss: 0.3559529185295105
train_iter_loss: 0.19679228961467743
train_iter_loss: 0.13642001152038574
train_iter_loss: 0.08497221022844315
train_iter_loss: 0.10479705780744553
train_iter_loss: 0.13724225759506226
train_iter_loss: 0.11420074850320816
train_iter_loss: 0.11125441640615463
train_iter_loss: 0.3217551112174988
train_iter_loss: 0.12983515858650208
train_iter_loss: 0.22354689240455627
train_iter_loss: 0.14067795872688293
train_iter_loss: 0.21639521420001984
train_iter_loss: 0.10645922273397446
train_iter_loss: 0.13214223086833954
train_iter_loss: 0.244528129696846
train_iter_loss: 0.10412263870239258
train_iter_loss: 0.08093221485614777
train_iter_loss: 0.12354198843240738
train_iter_loss: 0.11559529602527618
train_iter_loss: 0.09272466599941254
train_iter_loss: 0.11202949285507202
train_iter_loss: 0.3439245820045471
train_iter_loss: 0.20747120678424835
train_iter_loss: 0.1377456933259964
train_iter_loss: 0.11837926506996155
train_iter_loss: 0.13026964664459229
train_iter_loss: 0.1616230309009552
train_iter_loss: 0.24380551278591156
train_iter_loss: 0.16110409796237946
train_iter_loss: 0.39081817865371704
train_iter_loss: 0.17163750529289246
train_iter_loss: 0.22435225546360016
train_iter_loss: 0.10272993892431259
train_iter_loss: 0.13660478591918945
train_iter_loss: 0.1601586937904358
train_iter_loss: 0.19566139578819275
train_iter_loss: 0.2634647786617279
train_iter_loss: 0.2521006464958191
train_iter_loss: 0.12747491896152496
train_iter_loss: 0.14783503115177155
train_iter_loss: 0.07525280863046646
train_iter_loss: 0.09985210001468658
train_iter_loss: 0.13667917251586914
train_iter_loss: 0.18458044528961182
train_iter_loss: 0.17802448570728302
train_iter_loss: 0.20994538068771362
train_iter_loss: 0.04762497916817665
train_iter_loss: 0.2217293530702591
train_iter_loss: 0.21124708652496338
train_iter_loss: 0.10684902220964432
train_iter_loss: 0.11427507549524307
train_iter_loss: 0.09517980366945267
train_iter_loss: 0.16594740748405457
train_iter_loss: 0.1877502053976059
train_iter_loss: 0.2640322148799896
train_iter_loss: 0.17398996651172638
train_iter_loss: 0.1325632631778717
train_iter_loss: 0.13190409541130066
train_iter_loss: 0.15176838636398315
train_iter_loss: 0.1441778540611267
train_iter_loss: 0.08006839454174042
train_iter_loss: 0.07633799314498901
train_iter_loss: 0.23179437220096588
train_iter_loss: 0.15632835030555725
train_iter_loss: 0.18117892742156982
train_iter_loss: 0.18761470913887024
train_iter_loss: 0.1779799461364746
train_iter_loss: 0.17028233408927917
train_iter_loss: 0.15505601465702057
train_iter_loss: 0.09312108159065247
train_iter_loss: 0.1502230018377304
train_iter_loss: 0.15908892452716827
train_iter_loss: 0.19011865556240082
train_iter_loss: 0.11166895925998688
train_iter_loss: 0.20198601484298706
train_iter_loss: 0.17338858544826508
train_iter_loss: 0.11722888797521591
train_iter_loss: 0.08132950961589813
train loss :0.1658
---------------------
Validation seg loss: 0.2157648224735035 at epoch 677
epoch =    678/  1000, exp = train
train_iter_loss: 0.18939581513404846
train_iter_loss: 0.09113039076328278
train_iter_loss: 0.10838887840509415
train_iter_loss: 0.09713402390480042
train_iter_loss: 0.12137124687433243
train_iter_loss: 0.10650711506605148
train_iter_loss: 0.17545852065086365
train_iter_loss: 0.12777331471443176
train_iter_loss: 0.14343294501304626
train_iter_loss: 0.12704911828041077
train_iter_loss: 0.11515533924102783
train_iter_loss: 0.1651352196931839
train_iter_loss: 0.1376725286245346
train_iter_loss: 0.036319781094789505
train_iter_loss: 0.12826776504516602
train_iter_loss: 0.168907031416893
train_iter_loss: 0.2087155282497406
train_iter_loss: 0.11511901021003723
train_iter_loss: 0.3035190999507904
train_iter_loss: 0.16014136373996735
train_iter_loss: 0.10790332406759262
train_iter_loss: 0.2543099522590637
train_iter_loss: 0.07837289571762085
train_iter_loss: 0.12691082060337067
train_iter_loss: 0.18663953244686127
train_iter_loss: 0.20081399381160736
train_iter_loss: 0.19149599969387054
train_iter_loss: 0.09231090545654297
train_iter_loss: 0.1739242523908615
train_iter_loss: 0.27756696939468384
train_iter_loss: 0.283373087644577
train_iter_loss: 0.13368874788284302
train_iter_loss: 0.09915062040090561
train_iter_loss: 0.23462754487991333
train_iter_loss: 0.10077917575836182
train_iter_loss: 0.19561076164245605
train_iter_loss: 0.12327487766742706
train_iter_loss: 0.09588300436735153
train_iter_loss: 0.2319820076227188
train_iter_loss: 0.12757162749767303
train_iter_loss: 0.16723091900348663
train_iter_loss: 0.10751853883266449
train_iter_loss: 0.21055279672145844
train_iter_loss: 0.19054438173770905
train_iter_loss: 0.18034207820892334
train_iter_loss: 0.15440531075000763
train_iter_loss: 0.09290362894535065
train_iter_loss: 0.1503800004720688
train_iter_loss: 0.15837788581848145
train_iter_loss: 0.08448605239391327
train_iter_loss: 0.10865194350481033
train_iter_loss: 0.10412762314081192
train_iter_loss: 0.18768367171287537
train_iter_loss: 0.11172357946634293
train_iter_loss: 0.10234174132347107
train_iter_loss: 0.09218352288007736
train_iter_loss: 0.2563210725784302
train_iter_loss: 0.08208306133747101
train_iter_loss: 0.08279933780431747
train_iter_loss: 0.28893887996673584
train_iter_loss: 0.12443708628416061
train_iter_loss: 0.11908140778541565
train_iter_loss: 0.19598518311977386
train_iter_loss: 0.11222964525222778
train_iter_loss: 0.06236289069056511
train_iter_loss: 0.2622765302658081
train_iter_loss: 0.1513681411743164
train_iter_loss: 0.1809382289648056
train_iter_loss: 0.06813991814851761
train_iter_loss: 0.24001765251159668
train_iter_loss: 0.23137694597244263
train_iter_loss: 0.10575933009386063
train_iter_loss: 0.21153350174427032
train_iter_loss: 0.2701041102409363
train_iter_loss: 0.15374813973903656
train_iter_loss: 0.0936988964676857
train_iter_loss: 0.3233391344547272
train_iter_loss: 0.16754718124866486
train_iter_loss: 0.14282165467739105
train_iter_loss: 0.1475261002779007
train_iter_loss: 0.11730388551950455
train_iter_loss: 0.24959883093833923
train_iter_loss: 0.22252094745635986
train_iter_loss: 0.09305856376886368
train_iter_loss: 0.15279951691627502
train_iter_loss: 0.12087559700012207
train_iter_loss: 0.12582182884216309
train_iter_loss: 0.15552876889705658
train_iter_loss: 0.20188412070274353
train_iter_loss: 0.08666897565126419
train_iter_loss: 0.202223002910614
train_iter_loss: 0.14996066689491272
train_iter_loss: 0.20485757291316986
train_iter_loss: 0.16708967089653015
train_iter_loss: 0.13689132034778595
train_iter_loss: 0.1925288587808609
train_iter_loss: 0.16810202598571777
train_iter_loss: 0.1343984752893448
train_iter_loss: 0.16534772515296936
train_iter_loss: 0.21845485270023346
train loss :0.1585
---------------------
Validation seg loss: 0.21735249570447882 at epoch 678
epoch =    679/  1000, exp = train
train_iter_loss: 0.15887421369552612
train_iter_loss: 0.19743895530700684
train_iter_loss: 0.12617705762386322
train_iter_loss: 0.18967671692371368
train_iter_loss: 0.1872415542602539
train_iter_loss: 0.16175104677677155
train_iter_loss: 0.12712062895298004
train_iter_loss: 0.04464496299624443
train_iter_loss: 0.22256675362586975
train_iter_loss: 0.1308899074792862
train_iter_loss: 0.13711556792259216
train_iter_loss: 0.08345616608858109
train_iter_loss: 0.12961508333683014
train_iter_loss: 0.186520054936409
train_iter_loss: 0.043787725269794464
train_iter_loss: 0.1317552924156189
train_iter_loss: 0.15931056439876556
train_iter_loss: 0.1849779337644577
train_iter_loss: 0.14190007746219635
train_iter_loss: 0.0979827344417572
train_iter_loss: 0.21311958134174347
train_iter_loss: 0.15237826108932495
train_iter_loss: 0.16769178211688995
train_iter_loss: 0.12320701032876968
train_iter_loss: 0.18452534079551697
train_iter_loss: 0.12243805825710297
train_iter_loss: 0.12298285216093063
train_iter_loss: 0.15277878940105438
train_iter_loss: 0.23924727737903595
train_iter_loss: 0.13623888790607452
train_iter_loss: 0.05730648338794708
train_iter_loss: 0.15546958148479462
train_iter_loss: 0.069408118724823
train_iter_loss: 0.12215102463960648
train_iter_loss: 0.16875627636909485
train_iter_loss: 0.21839575469493866
train_iter_loss: 0.1555079221725464
train_iter_loss: 0.16776549816131592
train_iter_loss: 0.32536640763282776
train_iter_loss: 0.11487457901239395
train_iter_loss: 0.11941362172365189
train_iter_loss: 0.11713842302560806
train_iter_loss: 0.16822168231010437
train_iter_loss: 0.10536275804042816
train_iter_loss: 0.1322271078824997
train_iter_loss: 0.13340754806995392
train_iter_loss: 0.18726666271686554
train_iter_loss: 0.11948814988136292
train_iter_loss: 0.17231285572052002
train_iter_loss: 0.1778833121061325
train_iter_loss: 0.12384133785963058
train_iter_loss: 0.18462133407592773
train_iter_loss: 0.16679394245147705
train_iter_loss: 0.21682168543338776
train_iter_loss: 0.2432641237974167
train_iter_loss: 0.0993633046746254
train_iter_loss: 0.12131824344396591
train_iter_loss: 0.23999442160129547
train_iter_loss: 0.11790057271718979
train_iter_loss: 0.2678901255130768
train_iter_loss: 0.08836423605680466
train_iter_loss: 0.122461698949337
train_iter_loss: 0.10707566142082214
train_iter_loss: 0.10107704252004623
train_iter_loss: 0.27797451615333557
train_iter_loss: 0.08518223464488983
train_iter_loss: 0.07090023905038834
train_iter_loss: 0.2104130983352661
train_iter_loss: 0.17190051078796387
train_iter_loss: 0.19226780533790588
train_iter_loss: 0.18718665838241577
train_iter_loss: 0.08614782989025116
train_iter_loss: 0.18967410922050476
train_iter_loss: 0.1657525897026062
train_iter_loss: 0.14783670008182526
train_iter_loss: 0.17335925996303558
train_iter_loss: 0.12131433188915253
train_iter_loss: 0.1123242974281311
train_iter_loss: 0.13676221668720245
train_iter_loss: 0.21788455545902252
train_iter_loss: 0.2071835696697235
train_iter_loss: 0.2359250783920288
train_iter_loss: 0.11160062998533249
train_iter_loss: 0.13909853994846344
train_iter_loss: 0.141435444355011
train_iter_loss: 0.14919857680797577
train_iter_loss: 0.06531278789043427
train_iter_loss: 0.08644773811101913
train_iter_loss: 0.12235163897275925
train_iter_loss: 0.3252594470977783
train_iter_loss: 0.21606718003749847
train_iter_loss: 0.2433200627565384
train_iter_loss: 0.06820058077573776
train_iter_loss: 0.16499118506908417
train_iter_loss: 0.10027966648340225
train_iter_loss: 0.20855337381362915
train_iter_loss: 0.26062312722206116
train_iter_loss: 0.1708022803068161
train_iter_loss: 0.1648835688829422
train_iter_loss: 0.2578705847263336
train loss :0.1579
---------------------
Validation seg loss: 0.21964954599773265 at epoch 679
epoch =    680/  1000, exp = train
train_iter_loss: 0.16974540054798126
train_iter_loss: 0.19796007871627808
train_iter_loss: 0.19526606798171997
train_iter_loss: 0.07630698382854462
train_iter_loss: 0.16636568307876587
train_iter_loss: 0.16099698841571808
train_iter_loss: 0.13281604647636414
train_iter_loss: 0.32448768615722656
train_iter_loss: 0.1481655091047287
train_iter_loss: 0.0802149772644043
train_iter_loss: 0.1335969716310501
train_iter_loss: 0.05875847116112709
train_iter_loss: 0.13633522391319275
train_iter_loss: 0.21070487797260284
train_iter_loss: 0.19862650334835052
train_iter_loss: 0.16842004656791687
train_iter_loss: 0.13536109030246735
train_iter_loss: 0.1569518893957138
train_iter_loss: 0.3400041162967682
train_iter_loss: 0.17829535901546478
train_iter_loss: 0.25258317589759827
train_iter_loss: 0.1241011917591095
train_iter_loss: 0.11982239037752151
train_iter_loss: 0.12878258526325226
train_iter_loss: 0.06878239661455154
train_iter_loss: 0.10957440733909607
train_iter_loss: 0.15906886756420135
train_iter_loss: 0.12961292266845703
train_iter_loss: 0.1089131310582161
train_iter_loss: 0.08978643268346786
train_iter_loss: 0.15857766568660736
train_iter_loss: 0.17088089883327484
train_iter_loss: 0.22432732582092285
train_iter_loss: 0.1481705904006958
train_iter_loss: 0.14097175002098083
train_iter_loss: 0.21189308166503906
train_iter_loss: 0.11758474260568619
train_iter_loss: 0.1283860057592392
train_iter_loss: 0.24776393175125122
train_iter_loss: 0.18165116012096405
train_iter_loss: 0.08972974121570587
train_iter_loss: 0.19063283503055573
train_iter_loss: 0.1452474445104599
train_iter_loss: 0.15031248331069946
train_iter_loss: 0.1073862761259079
train_iter_loss: 0.09914925694465637
train_iter_loss: 0.13439108431339264
train_iter_loss: 0.11409950256347656
train_iter_loss: 0.14572428166866302
train_iter_loss: 0.21632687747478485
train_iter_loss: 0.10138098895549774
train_iter_loss: 0.16061696410179138
train_iter_loss: 0.13514770567417145
train_iter_loss: 0.20243829488754272
train_iter_loss: 0.1866375356912613
train_iter_loss: 0.12461541593074799
train_iter_loss: 0.27753859758377075
train_iter_loss: 0.197077676653862
train_iter_loss: 0.19173288345336914
train_iter_loss: 0.1574479341506958
train_iter_loss: 0.133818119764328
train_iter_loss: 0.09921913594007492
train_iter_loss: 0.26334869861602783
train_iter_loss: 0.1402345597743988
train_iter_loss: 0.1639982908964157
train_iter_loss: 0.12243866175413132
train_iter_loss: 0.1549777090549469
train_iter_loss: 0.19388729333877563
train_iter_loss: 0.17338450253009796
train_iter_loss: 0.15702317655086517
train_iter_loss: 0.1493651419878006
train_iter_loss: 0.1540144383907318
train_iter_loss: 0.08634297549724579
train_iter_loss: 0.15637940168380737
train_iter_loss: 0.15814030170440674
train_iter_loss: 0.17826806008815765
train_iter_loss: 0.08920256793498993
train_iter_loss: 0.18674902617931366
train_iter_loss: 0.1439330130815506
train_iter_loss: 0.1570577770471573
train_iter_loss: 0.09141413122415543
train_iter_loss: 0.08868792653083801
train_iter_loss: 0.14276205003261566
train_iter_loss: 0.09359782189130783
train_iter_loss: 0.1216040849685669
train_iter_loss: 0.12077662348747253
train_iter_loss: 0.11679992824792862
train_iter_loss: 0.17707054316997528
train_iter_loss: 0.1282990723848343
train_iter_loss: 0.20674802362918854
train_iter_loss: 0.31587526202201843
train_iter_loss: 0.1672326624393463
train_iter_loss: 0.10558538883924484
train_iter_loss: 0.11475258320569992
train_iter_loss: 0.20602145791053772
train_iter_loss: 0.19414779543876648
train_iter_loss: 0.09697078913450241
train_iter_loss: 0.14661049842834473
train_iter_loss: 0.2188684344291687
train_iter_loss: 0.21205030381679535
train loss :0.1580
---------------------
Validation seg loss: 0.21853366692743773 at epoch 680
epoch =    681/  1000, exp = train
train_iter_loss: 0.18276061117649078
train_iter_loss: 0.27635395526885986
train_iter_loss: 0.25560662150382996
train_iter_loss: 0.19486580789089203
train_iter_loss: 0.1660529226064682
train_iter_loss: 0.22329050302505493
train_iter_loss: 0.17672942578792572
train_iter_loss: 0.20041978359222412
train_iter_loss: 0.15519091486930847
train_iter_loss: 0.09238328784704208
train_iter_loss: 0.08406373113393784
train_iter_loss: 0.15557019412517548
train_iter_loss: 0.0776207447052002
train_iter_loss: 0.14879712462425232
train_iter_loss: 0.1956390142440796
train_iter_loss: 0.13316862285137177
train_iter_loss: 0.1779773235321045
train_iter_loss: 0.2844860255718231
train_iter_loss: 0.3009764850139618
train_iter_loss: 0.27337971329689026
train_iter_loss: 0.11753581464290619
train_iter_loss: 0.20206792652606964
train_iter_loss: 0.29029300808906555
train_iter_loss: 0.13726718723773956
train_iter_loss: 0.17493613064289093
train_iter_loss: 0.17921388149261475
train_iter_loss: 0.1044711023569107
train_iter_loss: 0.18881098926067352
train_iter_loss: 0.1750289499759674
train_iter_loss: 0.19992218911647797
train_iter_loss: 0.15453849732875824
train_iter_loss: 0.15740138292312622
train_iter_loss: 0.16661307215690613
train_iter_loss: 0.12424499541521072
train_iter_loss: 0.25876447558403015
train_iter_loss: 0.2211630791425705
train_iter_loss: 0.15031085908412933
train_iter_loss: 0.1316862255334854
train_iter_loss: 0.1596432775259018
train_iter_loss: 0.12356986850500107
train_iter_loss: 0.14298728108406067
train_iter_loss: 0.14173701405525208
train_iter_loss: 0.046263374388217926
train_iter_loss: 0.2517109811306
train_iter_loss: 0.16375477612018585
train_iter_loss: 0.10790589451789856
train_iter_loss: 0.1194806694984436
train_iter_loss: 0.09469006955623627
train_iter_loss: 0.052528783679008484
train_iter_loss: 0.2234153151512146
train_iter_loss: 0.11154381930828094
train_iter_loss: 0.10542508959770203
train_iter_loss: 0.16283750534057617
train_iter_loss: 0.14593419432640076
train_iter_loss: 0.1463901698589325
train_iter_loss: 0.17449285089969635
train_iter_loss: 0.2775925099849701
train_iter_loss: 0.12962138652801514
train_iter_loss: 0.22759470343589783
train_iter_loss: 0.1624661386013031
train_iter_loss: 0.1622113436460495
train_iter_loss: 0.1513388305902481
train_iter_loss: 0.14483508467674255
train_iter_loss: 0.10948013514280319
train_iter_loss: 0.06152333691716194
train_iter_loss: 0.1827215552330017
train_iter_loss: 0.12490037828683853
train_iter_loss: 0.0787055566906929
train_iter_loss: 0.21433164179325104
train_iter_loss: 0.15080152451992035
train_iter_loss: 0.07641871273517609
train_iter_loss: 0.17580264806747437
train_iter_loss: 0.09498632699251175
train_iter_loss: 0.1360226720571518
train_iter_loss: 0.1137603372335434
train_iter_loss: 0.11900912970304489
train_iter_loss: 0.14509157836437225
train_iter_loss: 0.059453558176755905
train_iter_loss: 0.14694754779338837
train_iter_loss: 0.17283856868743896
train_iter_loss: 0.1389046460390091
train_iter_loss: 0.13870085775852203
train_iter_loss: 0.0991826057434082
train_iter_loss: 0.15716467797756195
train_iter_loss: 0.24285857379436493
train_iter_loss: 0.14377133548259735
train_iter_loss: 0.1763957142829895
train_iter_loss: 0.19518950581550598
train_iter_loss: 0.1256009191274643
train_iter_loss: 0.15531650185585022
train_iter_loss: 0.09929080307483673
train_iter_loss: 0.1912282109260559
train_iter_loss: 0.15789338946342468
train_iter_loss: 0.12290622293949127
train_iter_loss: 0.1456097811460495
train_iter_loss: 0.12622037529945374
train_iter_loss: 0.09992683678865433
train_iter_loss: 0.16307765245437622
train_iter_loss: 0.17835469543933868
train_iter_loss: 0.30335676670074463
train loss :0.1610
---------------------
Validation seg loss: 0.21696139148981222 at epoch 681
epoch =    682/  1000, exp = train
train_iter_loss: 0.19872407615184784
train_iter_loss: 0.09072320908308029
train_iter_loss: 0.11606325954198837
train_iter_loss: 0.12422379106283188
train_iter_loss: 0.2647121250629425
train_iter_loss: 0.2756565809249878
train_iter_loss: 0.19905444979667664
train_iter_loss: 0.08253910392522812
train_iter_loss: 0.15732234716415405
train_iter_loss: 0.04797610640525818
train_iter_loss: 0.09342749416828156
train_iter_loss: 0.16464802622795105
train_iter_loss: 0.19921740889549255
train_iter_loss: 0.25645408034324646
train_iter_loss: 0.2591092586517334
train_iter_loss: 0.1783379465341568
train_iter_loss: 0.04306040704250336
train_iter_loss: 0.2148723006248474
train_iter_loss: 0.12072785198688507
train_iter_loss: 0.10730331391096115
train_iter_loss: 0.09604153037071228
train_iter_loss: 0.13636226952075958
train_iter_loss: 0.08601612597703934
train_iter_loss: 0.35308757424354553
train_iter_loss: 0.09217295795679092
train_iter_loss: 0.19828209280967712
train_iter_loss: 0.27010759711265564
train_iter_loss: 0.15675121545791626
train_iter_loss: 0.19965852797031403
train_iter_loss: 0.22769615054130554
train_iter_loss: 0.07865004241466522
train_iter_loss: 0.058607205748558044
train_iter_loss: 0.13945838809013367
train_iter_loss: 0.11698875576257706
train_iter_loss: 0.10366477072238922
train_iter_loss: 0.16259407997131348
train_iter_loss: 0.150020569562912
train_iter_loss: 0.11679144203662872
train_iter_loss: 0.2672885060310364
train_iter_loss: 0.05033014714717865
train_iter_loss: 0.09137408435344696
train_iter_loss: 0.14338761568069458
train_iter_loss: 0.1631889045238495
train_iter_loss: 0.1810138076543808
train_iter_loss: 0.11260110139846802
train_iter_loss: 0.1033700630068779
train_iter_loss: 0.3140307068824768
train_iter_loss: 0.21050399541854858
train_iter_loss: 0.1815989464521408
train_iter_loss: 0.11852308362722397
train_iter_loss: 0.07563789188861847
train_iter_loss: 0.1647430956363678
train_iter_loss: 0.21877777576446533
train_iter_loss: 0.06976817548274994
train_iter_loss: 0.20169615745544434
train_iter_loss: 0.1661187708377838
train_iter_loss: 0.12616029381752014
train_iter_loss: 0.09741180390119553
train_iter_loss: 0.2179039567708969
train_iter_loss: 0.08065738528966904
train_iter_loss: 0.13274990022182465
train_iter_loss: 0.14858271181583405
train_iter_loss: 0.06983361393213272
train_iter_loss: 0.1627981960773468
train_iter_loss: 0.14707285165786743
train_iter_loss: 0.08790857344865799
train_iter_loss: 0.18491190671920776
train_iter_loss: 0.14194729924201965
train_iter_loss: 0.3208799362182617
train_iter_loss: 0.11492574959993362
train_iter_loss: 0.032250285148620605
train_iter_loss: 0.09845681488513947
train_iter_loss: 0.14775194227695465
train_iter_loss: 0.3138815462589264
train_iter_loss: 0.16050998866558075
train_iter_loss: 0.09383168816566467
train_iter_loss: 0.21071204543113708
train_iter_loss: 0.1754053682088852
train_iter_loss: 0.15213848650455475
train_iter_loss: 0.10676400363445282
train_iter_loss: 0.1262109875679016
train_iter_loss: 0.14653708040714264
train_iter_loss: 0.13371188938617706
train_iter_loss: 0.12999944388866425
train_iter_loss: 0.06826706230640411
train_iter_loss: 0.21846595406532288
train_iter_loss: 0.4160417914390564
train_iter_loss: 0.15835949778556824
train_iter_loss: 0.2651520073413849
train_iter_loss: 0.2269679605960846
train_iter_loss: 0.1857953518629074
train_iter_loss: 0.07416532188653946
train_iter_loss: 0.09844154119491577
train_iter_loss: 0.12621548771858215
train_iter_loss: 0.1214090883731842
train_iter_loss: 0.19021263718605042
train_iter_loss: 0.16957342624664307
train_iter_loss: 0.2683511972427368
train_iter_loss: 0.11888366937637329
train_iter_loss: 0.1733623445034027
train loss :0.1587
---------------------
Validation seg loss: 0.21535601556511982 at epoch 682
epoch =    683/  1000, exp = train
train_iter_loss: 0.13731743395328522
train_iter_loss: 0.04223896935582161
train_iter_loss: 0.19046561419963837
train_iter_loss: 0.24824373424053192
train_iter_loss: 0.09332019090652466
train_iter_loss: 0.1716078817844391
train_iter_loss: 0.13031616806983948
train_iter_loss: 0.11025919020175934
train_iter_loss: 0.14447611570358276
train_iter_loss: 0.10923246294260025
train_iter_loss: 0.21888378262519836
train_iter_loss: 0.05042164400219917
train_iter_loss: 0.0853387638926506
train_iter_loss: 0.13891445100307465
train_iter_loss: 0.07584422081708908
train_iter_loss: 0.15196353197097778
train_iter_loss: 0.1384424865245819
train_iter_loss: 0.21556875109672546
train_iter_loss: 0.14091072976589203
train_iter_loss: 0.21901552379131317
train_iter_loss: 0.1697378009557724
train_iter_loss: 0.18726135790348053
train_iter_loss: 0.1401938945055008
train_iter_loss: 0.3498396873474121
train_iter_loss: 0.11542001366615295
train_iter_loss: 0.13680849969387054
train_iter_loss: 0.24720478057861328
train_iter_loss: 0.10699136555194855
train_iter_loss: 0.09907807409763336
train_iter_loss: 0.13112269341945648
train_iter_loss: 0.1414467692375183
train_iter_loss: 0.1826050728559494
train_iter_loss: 0.15978054702281952
train_iter_loss: 0.09218192845582962
train_iter_loss: 0.1951480507850647
train_iter_loss: 0.1553955376148224
train_iter_loss: 0.1412687450647354
train_iter_loss: 0.19305925071239471
train_iter_loss: 0.1465003490447998
train_iter_loss: 0.08255897462368011
train_iter_loss: 0.1962525099515915
train_iter_loss: 0.1407981812953949
train_iter_loss: 0.18450039625167847
train_iter_loss: 0.08317741751670837
train_iter_loss: 0.14807389676570892
train_iter_loss: 0.13590551912784576
train_iter_loss: 0.12421447038650513
train_iter_loss: 0.09304076433181763
train_iter_loss: 0.09729946404695511
train_iter_loss: 0.24551574885845184
train_iter_loss: 0.20732033252716064
train_iter_loss: 0.11457917839288712
train_iter_loss: 0.15446053445339203
train_iter_loss: 0.1680144965648651
train_iter_loss: 0.21923720836639404
train_iter_loss: 0.1315169483423233
train_iter_loss: 0.15438733994960785
train_iter_loss: 0.17996326088905334
train_iter_loss: 0.16942095756530762
train_iter_loss: 0.08650434017181396
train_iter_loss: 0.1125168651342392
train_iter_loss: 0.1200016438961029
train_iter_loss: 0.260663777589798
train_iter_loss: 0.1708442121744156
train_iter_loss: 0.2287050485610962
train_iter_loss: 0.26023364067077637
train_iter_loss: 0.21424277126789093
train_iter_loss: 0.1252107471227646
train_iter_loss: 0.1139581948518753
train_iter_loss: 0.14619332551956177
train_iter_loss: 0.07191166281700134
train_iter_loss: 0.24622155725955963
train_iter_loss: 0.1559986174106598
train_iter_loss: 0.08719845116138458
train_iter_loss: 0.1652912199497223
train_iter_loss: 0.11680661141872406
train_iter_loss: 0.0850057527422905
train_iter_loss: 0.1366918832063675
train_iter_loss: 0.1732925921678543
train_iter_loss: 0.2141522467136383
train_iter_loss: 0.13764771819114685
train_iter_loss: 0.2508418560028076
train_iter_loss: 0.18880273401737213
train_iter_loss: 0.2045268565416336
train_iter_loss: 0.24840952455997467
train_iter_loss: 0.2349517047405243
train_iter_loss: 0.12177814543247223
train_iter_loss: 0.08841138333082199
train_iter_loss: 0.0969490110874176
train_iter_loss: 0.26527130603790283
train_iter_loss: 0.07544588297605515
train_iter_loss: 0.08739563077688217
train_iter_loss: 0.082495778799057
train_iter_loss: 0.13353043794631958
train_iter_loss: 0.11055263131856918
train_iter_loss: 0.10466019064188004
train_iter_loss: 0.15760938823223114
train_iter_loss: 0.19647563993930817
train_iter_loss: 0.11097051948308945
train_iter_loss: 0.1197492703795433
train loss :0.1540
---------------------
Validation seg loss: 0.21565364112304347 at epoch 683
epoch =    684/  1000, exp = train
train_iter_loss: 0.10351767390966415
train_iter_loss: 0.1373913437128067
train_iter_loss: 0.2408326119184494
train_iter_loss: 0.1779770851135254
train_iter_loss: 0.17649415135383606
train_iter_loss: 0.11455519497394562
train_iter_loss: 0.14249157905578613
train_iter_loss: 0.14882872998714447
train_iter_loss: 0.08587587624788284
train_iter_loss: 0.17678700387477875
train_iter_loss: 0.14790533483028412
train_iter_loss: 0.08757057040929794
train_iter_loss: 0.1682700365781784
train_iter_loss: 0.07139160484075546
train_iter_loss: 0.10866454988718033
train_iter_loss: 0.26529940962791443
train_iter_loss: 0.12177141010761261
train_iter_loss: 0.13809172809123993
train_iter_loss: 0.1294497698545456
train_iter_loss: 0.0801805779337883
train_iter_loss: 0.12196461111307144
train_iter_loss: 0.11315367370843887
train_iter_loss: 0.14451679587364197
train_iter_loss: 0.07996067404747009
train_iter_loss: 0.05288295820355415
train_iter_loss: 0.1953047811985016
train_iter_loss: 0.16071432828903198
train_iter_loss: 0.18372072279453278
train_iter_loss: 0.13387973606586456
train_iter_loss: 0.18093591928482056
train_iter_loss: 0.10405855625867844
train_iter_loss: 0.22530074417591095
train_iter_loss: 0.1512126475572586
train_iter_loss: 0.1727588027715683
train_iter_loss: 0.15018731355667114
train_iter_loss: 0.32080304622650146
train_iter_loss: 0.16064922511577606
train_iter_loss: 0.16354788839817047
train_iter_loss: 0.14019808173179626
train_iter_loss: 0.14295020699501038
train_iter_loss: 0.24009063839912415
train_iter_loss: 0.04035116359591484
train_iter_loss: 0.088054358959198
train_iter_loss: 0.1938847005367279
train_iter_loss: 0.11389430612325668
train_iter_loss: 0.1097344309091568
train_iter_loss: 0.1030750498175621
train_iter_loss: 0.14656470715999603
train_iter_loss: 0.13828007876873016
train_iter_loss: 0.18172208964824677
train_iter_loss: 0.10643153637647629
train_iter_loss: 0.09938061982393265
train_iter_loss: 0.11464045196771622
train_iter_loss: 0.07561136037111282
train_iter_loss: 0.19020211696624756
train_iter_loss: 0.0835382267832756
train_iter_loss: 0.04216747730970383
train_iter_loss: 0.21068507432937622
train_iter_loss: 0.2274351418018341
train_iter_loss: 0.16946712136268616
train_iter_loss: 0.166240394115448
train_iter_loss: 0.17348617315292358
train_iter_loss: 0.13394299149513245
train_iter_loss: 0.21216091513633728
train_iter_loss: 0.11294561624526978
train_iter_loss: 0.1629539281129837
train_iter_loss: 0.14145149290561676
train_iter_loss: 0.24848896265029907
train_iter_loss: 0.1603793203830719
train_iter_loss: 0.1417718529701233
train_iter_loss: 0.17613749206066132
train_iter_loss: 0.2474105954170227
train_iter_loss: 0.07903235405683517
train_iter_loss: 0.33847352862358093
train_iter_loss: 0.2458498477935791
train_iter_loss: 0.1919575184583664
train_iter_loss: 0.17229723930358887
train_iter_loss: 0.08690013736486435
train_iter_loss: 0.1528218537569046
train_iter_loss: 0.12908460199832916
train_iter_loss: 0.16690976917743683
train_iter_loss: 0.08852844685316086
train_iter_loss: 0.06942242383956909
train_iter_loss: 0.1312854290008545
train_iter_loss: 0.1757451742887497
train_iter_loss: 0.12045163661241531
train_iter_loss: 0.13028459250926971
train_iter_loss: 0.26722320914268494
train_iter_loss: 0.13444359600543976
train_iter_loss: 0.15797625482082367
train_iter_loss: 0.18070237338542938
train_iter_loss: 0.17564274370670319
train_iter_loss: 0.1520610749721527
train_iter_loss: 0.20273186266422272
train_iter_loss: 0.18196366727352142
train_iter_loss: 0.07159695029258728
train_iter_loss: 0.2574950158596039
train_iter_loss: 0.1990833878517151
train_iter_loss: 0.13307377696037292
train_iter_loss: 0.17587992548942566
train loss :0.1540
---------------------
Validation seg loss: 0.21691999295374975 at epoch 684
epoch =    685/  1000, exp = train
train_iter_loss: 0.11001716554164886
train_iter_loss: 0.07748297601938248
train_iter_loss: 0.12368789315223694
train_iter_loss: 0.23602990806102753
train_iter_loss: 0.13080403208732605
train_iter_loss: 0.15854673087596893
train_iter_loss: 0.14009855687618256
train_iter_loss: 0.06265660375356674
train_iter_loss: 0.10209017246961594
train_iter_loss: 0.1496499925851822
train_iter_loss: 0.1354464739561081
train_iter_loss: 0.12429286539554596
train_iter_loss: 0.15196232497692108
train_iter_loss: 0.12088318169116974
train_iter_loss: 0.15200480818748474
train_iter_loss: 0.16494785249233246
train_iter_loss: 0.12580104172229767
train_iter_loss: 0.0869143158197403
train_iter_loss: 0.2689485549926758
train_iter_loss: 0.14753837883472443
train_iter_loss: 0.1649162918329239
train_iter_loss: 0.12334413081407547
train_iter_loss: 0.1272844523191452
train_iter_loss: 0.049909740686416626
train_iter_loss: 0.2868887186050415
train_iter_loss: 0.1741105169057846
train_iter_loss: 0.14480890333652496
train_iter_loss: 0.14938341081142426
train_iter_loss: 0.18594807386398315
train_iter_loss: 0.16951650381088257
train_iter_loss: 0.14378733932971954
train_iter_loss: 0.2274189591407776
train_iter_loss: 0.10217873752117157
train_iter_loss: 0.14622531831264496
train_iter_loss: 0.3476473391056061
train_iter_loss: 0.0689663216471672
train_iter_loss: 0.10713846236467361
train_iter_loss: 0.09591879695653915
train_iter_loss: 0.24013015627861023
train_iter_loss: 0.19978749752044678
train_iter_loss: 0.10928788036108017
train_iter_loss: 0.20869813859462738
train_iter_loss: 0.27967631816864014
train_iter_loss: 0.13533735275268555
train_iter_loss: 0.13074059784412384
train_iter_loss: 0.17908433079719543
train_iter_loss: 0.1840953528881073
train_iter_loss: 0.027962032705545425
train_iter_loss: 0.13801738619804382
train_iter_loss: 0.24384744465351105
train_iter_loss: 0.145516499876976
train_iter_loss: 0.1805654764175415
train_iter_loss: 0.12260469049215317
train_iter_loss: 0.20148718357086182
train_iter_loss: 0.20022611320018768
train_iter_loss: 0.17700910568237305
train_iter_loss: 0.36557483673095703
train_iter_loss: 0.15903864800930023
train_iter_loss: 0.22948268055915833
train_iter_loss: 0.15732203423976898
train_iter_loss: 0.0916544646024704
train_iter_loss: 0.16424739360809326
train_iter_loss: 0.10581204295158386
train_iter_loss: 0.10882692784070969
train_iter_loss: 0.1669500172138214
train_iter_loss: 0.052319567650556564
train_iter_loss: 0.2290392965078354
train_iter_loss: 0.22178740799427032
train_iter_loss: 0.22512498497962952
train_iter_loss: 0.14084111154079437
train_iter_loss: 0.27562499046325684
train_iter_loss: 0.05899829789996147
train_iter_loss: 0.1012658029794693
train_iter_loss: 0.2443922758102417
train_iter_loss: 0.18840758502483368
train_iter_loss: 0.20582716166973114
train_iter_loss: 0.1932039111852646
train_iter_loss: 0.08165375888347626
train_iter_loss: 0.16484712064266205
train_iter_loss: 0.2519964873790741
train_iter_loss: 0.04996555298566818
train_iter_loss: 0.1515883356332779
train_iter_loss: 0.1513783037662506
train_iter_loss: 0.07731010764837265
train_iter_loss: 0.06584363430738449
train_iter_loss: 0.16823320090770721
train_iter_loss: 0.12051250040531158
train_iter_loss: 0.21241062879562378
train_iter_loss: 0.11399834603071213
train_iter_loss: 0.19669730961322784
train_iter_loss: 0.06964974105358124
train_iter_loss: 0.1345035433769226
train_iter_loss: 0.12356259673833847
train_iter_loss: 0.13716363906860352
train_iter_loss: 0.18812264502048492
train_iter_loss: 0.033645547926425934
train_iter_loss: 0.05935291200876236
train_iter_loss: 0.13962994515895844
train_iter_loss: 0.2309398055076599
train_iter_loss: 0.18312329053878784
train loss :0.1564
---------------------
Validation seg loss: 0.21508242855867687 at epoch 685
epoch =    686/  1000, exp = train
train_iter_loss: 0.10359292477369308
train_iter_loss: 0.1491057425737381
train_iter_loss: 0.2660483419895172
train_iter_loss: 0.26719099283218384
train_iter_loss: 0.17132113873958588
train_iter_loss: 0.09469819813966751
train_iter_loss: 0.25109511613845825
train_iter_loss: 0.13592486083507538
train_iter_loss: 0.1763100028038025
train_iter_loss: 0.1326788067817688
train_iter_loss: 0.07706312835216522
train_iter_loss: 0.13045385479927063
train_iter_loss: 0.24422621726989746
train_iter_loss: 0.14063119888305664
train_iter_loss: 0.20373235642910004
train_iter_loss: 0.1447916179895401
train_iter_loss: 0.1991714984178543
train_iter_loss: 0.08259504288434982
train_iter_loss: 0.14605210721492767
train_iter_loss: 0.1650553047657013
train_iter_loss: 0.23549392819404602
train_iter_loss: 0.16542309522628784
train_iter_loss: 0.2750310003757477
train_iter_loss: 0.09580445289611816
train_iter_loss: 0.07409707456827164
train_iter_loss: 0.22202490270137787
train_iter_loss: 0.1759108304977417
train_iter_loss: 0.1252073049545288
train_iter_loss: 0.19979354739189148
train_iter_loss: 0.3862893283367157
train_iter_loss: 0.1805996298789978
train_iter_loss: 0.14709360897541046
train_iter_loss: 0.18179820477962494
train_iter_loss: 0.08259521424770355
train_iter_loss: 0.07950951904058456
train_iter_loss: 0.09226317703723907
train_iter_loss: 0.14922329783439636
train_iter_loss: 0.15360838174819946
train_iter_loss: 0.1276874840259552
train_iter_loss: 0.27190539240837097
train_iter_loss: 0.16523146629333496
train_iter_loss: 0.13472115993499756
train_iter_loss: 0.17766173183918
train_iter_loss: 0.25474807620048523
train_iter_loss: 0.20394660532474518
train_iter_loss: 0.10647649317979813
train_iter_loss: 0.19828081130981445
train_iter_loss: 0.10443861037492752
train_iter_loss: 0.2008075714111328
train_iter_loss: 0.16893048584461212
train_iter_loss: 0.13621316850185394
train_iter_loss: 0.09322603791952133
train_iter_loss: 0.10254887491464615
train_iter_loss: 0.26563572883605957
train_iter_loss: 0.24928703904151917
train_iter_loss: 0.09647046029567719
train_iter_loss: 0.23987990617752075
train_iter_loss: 0.2804518938064575
train_iter_loss: 0.3137492537498474
train_iter_loss: 0.08304910361766815
train_iter_loss: 0.15947312116622925
train_iter_loss: 0.08965586870908737
train_iter_loss: 0.06696388125419617
train_iter_loss: 0.1670781522989273
train_iter_loss: 0.09954652935266495
train_iter_loss: 0.11259964853525162
train_iter_loss: 0.18089896440505981
train_iter_loss: 0.1420528143644333
train_iter_loss: 0.15367642045021057
train_iter_loss: 0.18620553612709045
train_iter_loss: 0.17804308235645294
train_iter_loss: 0.25824838876724243
train_iter_loss: 0.23074796795845032
train_iter_loss: 0.12506285309791565
train_iter_loss: 0.19442981481552124
train_iter_loss: 0.1265786588191986
train_iter_loss: 0.1037609651684761
train_iter_loss: 0.17073772847652435
train_iter_loss: 0.370025098323822
train_iter_loss: 0.1402607560157776
train_iter_loss: 0.18018488585948944
train_iter_loss: 0.11082279682159424
train_iter_loss: 0.31014958024024963
train_iter_loss: 0.14130574464797974
train_iter_loss: 0.26069551706314087
train_iter_loss: 0.10767284780740738
train_iter_loss: 0.1301955282688141
train_iter_loss: 0.062390755861997604
train_iter_loss: 0.13292521238327026
train_iter_loss: 0.10051079839468002
train_iter_loss: 0.17882464826107025
train_iter_loss: 0.1567472666501999
train_iter_loss: 0.1751413345336914
train_iter_loss: 0.07036084681749344
train_iter_loss: 0.059673842042684555
train_iter_loss: 0.2270248979330063
train_iter_loss: 0.11463328450918198
train_iter_loss: 0.04992601275444031
train_iter_loss: 0.11110110580921173
train_iter_loss: 0.1992824226617813
train loss :0.1657
---------------------
Validation seg loss: 0.2168120217404135 at epoch 686
epoch =    687/  1000, exp = train
train_iter_loss: 0.2057335376739502
train_iter_loss: 0.17295333743095398
train_iter_loss: 0.18250474333763123
train_iter_loss: 0.10985434800386429
train_iter_loss: 0.13368834555149078
train_iter_loss: 0.11672544479370117
train_iter_loss: 0.12553836405277252
train_iter_loss: 0.10206536203622818
train_iter_loss: 0.2160247266292572
train_iter_loss: 0.13844752311706543
train_iter_loss: 0.3135925829410553
train_iter_loss: 0.16664299368858337
train_iter_loss: 0.1279660016298294
train_iter_loss: 0.22733013331890106
train_iter_loss: 0.2117559313774109
train_iter_loss: 0.10833197832107544
train_iter_loss: 0.16031473875045776
train_iter_loss: 0.16949768364429474
train_iter_loss: 0.11907926201820374
train_iter_loss: 0.13899539411067963
train_iter_loss: 0.1351235806941986
train_iter_loss: 0.17269934713840485
train_iter_loss: 0.18599103391170502
train_iter_loss: 0.21372747421264648
train_iter_loss: 0.22387206554412842
train_iter_loss: 0.030960682779550552
train_iter_loss: 0.0658935159444809
train_iter_loss: 0.10844816267490387
train_iter_loss: 0.0770685076713562
train_iter_loss: 0.2680176794528961
train_iter_loss: 0.10253597050905228
train_iter_loss: 0.10094995051622391
train_iter_loss: 0.12846782803535461
train_iter_loss: 0.2049194872379303
train_iter_loss: 0.19626781344413757
train_iter_loss: 0.10074818879365921
train_iter_loss: 0.15150186419487
train_iter_loss: 0.05339180678129196
train_iter_loss: 0.20444346964359283
train_iter_loss: 0.1283683180809021
train_iter_loss: 0.20948530733585358
train_iter_loss: 0.2708001732826233
train_iter_loss: 0.14659610390663147
train_iter_loss: 0.2228952944278717
train_iter_loss: 0.1745878905057907
train_iter_loss: 0.1645008772611618
train_iter_loss: 0.08806179463863373
train_iter_loss: 0.08631011098623276
train_iter_loss: 0.05756927654147148
train_iter_loss: 0.19827735424041748
train_iter_loss: 0.1704035848379135
train_iter_loss: 0.29020437598228455
train_iter_loss: 0.08120942860841751
train_iter_loss: 0.15663693845272064
train_iter_loss: 0.20269282162189484
train_iter_loss: 0.06711313128471375
train_iter_loss: 0.16938018798828125
train_iter_loss: 0.29704049229621887
train_iter_loss: 0.12164478749036789
train_iter_loss: 0.12856917083263397
train_iter_loss: 0.11358729004859924
train_iter_loss: 0.1328701376914978
train_iter_loss: 0.2050642967224121
train_iter_loss: 0.1622084528207779
train_iter_loss: 0.18608547747135162
train_iter_loss: 0.262077271938324
train_iter_loss: 0.19879238307476044
train_iter_loss: 0.11540459096431732
train_iter_loss: 0.09729384630918503
train_iter_loss: 0.14093329012393951
train_iter_loss: 0.20972013473510742
train_iter_loss: 0.16116765141487122
train_iter_loss: 0.18218404054641724
train_iter_loss: 0.15293055772781372
train_iter_loss: 0.1749725639820099
train_iter_loss: 0.2511563003063202
train_iter_loss: 0.09565337002277374
train_iter_loss: 0.13027402758598328
train_iter_loss: 0.10087497532367706
train_iter_loss: 0.16107390820980072
train_iter_loss: 0.1645651012659073
train_iter_loss: 0.19062596559524536
train_iter_loss: 0.1628471463918686
train_iter_loss: 0.10416250675916672
train_iter_loss: 0.3073715567588806
train_iter_loss: 0.07747727632522583
train_iter_loss: 0.06748731434345245
train_iter_loss: 0.16475386917591095
train_iter_loss: 0.24412497878074646
train_iter_loss: 0.20460304617881775
train_iter_loss: 0.15828533470630646
train_iter_loss: 0.18926385045051575
train_iter_loss: 0.12827996909618378
train_iter_loss: 0.18461908400058746
train_iter_loss: 0.10758089274168015
train_iter_loss: 0.14939239621162415
train_iter_loss: 0.1086997538805008
train_iter_loss: 0.08665270358324051
train_iter_loss: 0.11179089546203613
train_iter_loss: 0.08311561495065689
train loss :0.1580
---------------------
Validation seg loss: 0.21368303558490467 at epoch 687
epoch =    688/  1000, exp = train
train_iter_loss: 0.13149644434452057
train_iter_loss: 0.17284448444843292
train_iter_loss: 0.1324281245470047
train_iter_loss: 0.0898311510682106
train_iter_loss: 0.09749355912208557
train_iter_loss: 0.12283236533403397
train_iter_loss: 0.11878642439842224
train_iter_loss: 0.08433734625577927
train_iter_loss: 0.08757016062736511
train_iter_loss: 0.12921272218227386
train_iter_loss: 0.09505409002304077
train_iter_loss: 0.13255763053894043
train_iter_loss: 0.046566564589738846
train_iter_loss: 0.12650829553604126
train_iter_loss: 0.09403390437364578
train_iter_loss: 0.20245392620563507
train_iter_loss: 0.15967419743537903
train_iter_loss: 0.21313105523586273
train_iter_loss: 0.33999890089035034
train_iter_loss: 0.09014490991830826
train_iter_loss: 0.37649086117744446
train_iter_loss: 0.16216245293617249
train_iter_loss: 0.19359663128852844
train_iter_loss: 0.24047818779945374
train_iter_loss: 0.07506467401981354
train_iter_loss: 0.15158481895923615
train_iter_loss: 0.23498445749282837
train_iter_loss: 0.13481339812278748
train_iter_loss: 0.17328007519245148
train_iter_loss: 0.07952103018760681
train_iter_loss: 0.15318992733955383
train_iter_loss: 0.21471677720546722
train_iter_loss: 0.12693780660629272
train_iter_loss: 0.08217430114746094
train_iter_loss: 0.05418218672275543
train_iter_loss: 0.1136004775762558
train_iter_loss: 0.18271394073963165
train_iter_loss: 0.12997028231620789
train_iter_loss: 0.12051013857126236
train_iter_loss: 0.14543074369430542
train_iter_loss: 0.06389036029577255
train_iter_loss: 0.12335309386253357
train_iter_loss: 0.117152638733387
train_iter_loss: 0.09970042109489441
train_iter_loss: 0.16887865960597992
train_iter_loss: 0.14179037511348724
train_iter_loss: 0.3456342816352844
train_iter_loss: 0.23696762323379517
train_iter_loss: 0.1099458560347557
train_iter_loss: 0.13421162962913513
train_iter_loss: 0.09270063042640686
train_iter_loss: 0.17268218100070953
train_iter_loss: 0.20913171768188477
train_iter_loss: 0.19153353571891785
train_iter_loss: 0.10087580233812332
train_iter_loss: 0.1672411412000656
train_iter_loss: 0.14541617035865784
train_iter_loss: 0.1169777363538742
train_iter_loss: 0.06500305980443954
train_iter_loss: 0.10442157089710236
train_iter_loss: 0.1710386425256729
train_iter_loss: 0.20849908888339996
train_iter_loss: 0.24697960913181305
train_iter_loss: 0.08514466136693954
train_iter_loss: 0.2112163007259369
train_iter_loss: 0.26696789264678955
train_iter_loss: 0.12698149681091309
train_iter_loss: 0.132350355386734
train_iter_loss: 0.13157320022583008
train_iter_loss: 0.1577109843492508
train_iter_loss: 0.19596023857593536
train_iter_loss: 0.24198313057422638
train_iter_loss: 0.14899279177188873
train_iter_loss: 0.09815573692321777
train_iter_loss: 0.15971951186656952
train_iter_loss: 0.08781670033931732
train_iter_loss: 0.23205557465553284
train_iter_loss: 0.09475192427635193
train_iter_loss: 0.15116143226623535
train_iter_loss: 0.23408518731594086
train_iter_loss: 0.1164972335100174
train_iter_loss: 0.10279685258865356
train_iter_loss: 0.17471995949745178
train_iter_loss: 0.15172651410102844
train_iter_loss: 0.1747271567583084
train_iter_loss: 0.2106575071811676
train_iter_loss: 0.12936265766620636
train_iter_loss: 0.21703962981700897
train_iter_loss: 0.0891074538230896
train_iter_loss: 0.2000724822282791
train_iter_loss: 0.20308314263820648
train_iter_loss: 0.11063932627439499
train_iter_loss: 0.17767155170440674
train_iter_loss: 0.11865074932575226
train_iter_loss: 0.1205684095621109
train_iter_loss: 0.09444885700941086
train_iter_loss: 0.156539648771286
train_iter_loss: 0.1348671019077301
train_iter_loss: 0.2198277711868286
train_iter_loss: 0.19292622804641724
train loss :0.1536
---------------------
Validation seg loss: 0.21338038446100535 at epoch 688
epoch =    689/  1000, exp = train
train_iter_loss: 0.14510703086853027
train_iter_loss: 0.31089869141578674
train_iter_loss: 0.09677859395742416
train_iter_loss: 0.2057969570159912
train_iter_loss: 0.23092126846313477
train_iter_loss: 0.2707090973854065
train_iter_loss: 0.1113128736615181
train_iter_loss: 0.385376900434494
train_iter_loss: 0.14429643750190735
train_iter_loss: 0.21439312398433685
train_iter_loss: 0.23948507010936737
train_iter_loss: 0.21880832314491272
train_iter_loss: 0.22351008653640747
train_iter_loss: 0.14534996449947357
train_iter_loss: 0.17348332703113556
train_iter_loss: 0.24469508230686188
train_iter_loss: 0.09556188434362411
train_iter_loss: 0.11944610625505447
train_iter_loss: 0.15478631854057312
train_iter_loss: 0.19041240215301514
train_iter_loss: 0.19782328605651855
train_iter_loss: 0.1461969017982483
train_iter_loss: 0.2118922621011734
train_iter_loss: 0.25499939918518066
train_iter_loss: 0.1602725088596344
train_iter_loss: 0.134413480758667
train_iter_loss: 0.11421359330415726
train_iter_loss: 0.16927359998226166
train_iter_loss: 0.11328021436929703
train_iter_loss: 0.0976695865392685
train_iter_loss: 0.1173592284321785
train_iter_loss: 0.13702966272830963
train_iter_loss: 0.14659997820854187
train_iter_loss: 0.15288865566253662
train_iter_loss: 0.1537211835384369
train_iter_loss: 0.06190604344010353
train_iter_loss: 0.11632735282182693
train_iter_loss: 0.2667975425720215
train_iter_loss: 0.2973850965499878
train_iter_loss: 0.01898343861103058
train_iter_loss: 0.05064873769879341
train_iter_loss: 0.17880462110042572
train_iter_loss: 0.2117890566587448
train_iter_loss: 0.21778343617916107
train_iter_loss: 0.11556114256381989
train_iter_loss: 0.15367302298545837
train_iter_loss: 0.08749811351299286
train_iter_loss: 0.13939006626605988
train_iter_loss: 0.07090650498867035
train_iter_loss: 0.23040543496608734
train_iter_loss: 0.24919940531253815
train_iter_loss: 0.10056118667125702
train_iter_loss: 0.13197925686836243
train_iter_loss: 0.12979452311992645
train_iter_loss: 0.27910688519477844
train_iter_loss: 0.11744268983602524
train_iter_loss: 0.20485617220401764
train_iter_loss: 0.10904980450868607
train_iter_loss: 0.10327036678791046
train_iter_loss: 0.11868782341480255
train_iter_loss: 0.11878985911607742
train_iter_loss: 0.13490384817123413
train_iter_loss: 0.14874809980392456
train_iter_loss: 0.10478489845991135
train_iter_loss: 0.17323824763298035
train_iter_loss: 0.18592411279678345
train_iter_loss: 0.1427908092737198
train_iter_loss: 0.16496261954307556
train_iter_loss: 0.12992072105407715
train_iter_loss: 0.1335359364748001
train_iter_loss: 0.18945565819740295
train_iter_loss: 0.19434323906898499
train_iter_loss: 0.18741996586322784
train_iter_loss: 0.11079982668161392
train_iter_loss: 0.11696097254753113
train_iter_loss: 0.10057985782623291
train_iter_loss: 0.11112780123949051
train_iter_loss: 0.14939150214195251
train_iter_loss: 0.16237974166870117
train_iter_loss: 0.10759439319372177
train_iter_loss: 0.09905193001031876
train_iter_loss: 0.2115679532289505
train_iter_loss: 0.21126022934913635
train_iter_loss: 0.18440845608711243
train_iter_loss: 0.22266286611557007
train_iter_loss: 0.20606359839439392
train_iter_loss: 0.3075666129589081
train_iter_loss: 0.08101321011781693
train_iter_loss: 0.08440026640892029
train_iter_loss: 0.1283903867006302
train_iter_loss: 0.1321604698896408
train_iter_loss: 0.0690835639834404
train_iter_loss: 0.1966722458600998
train_iter_loss: 0.25544941425323486
train_iter_loss: 0.1003156453371048
train_iter_loss: 0.19927473366260529
train_iter_loss: 0.4215090870857239
train_iter_loss: 0.24109935760498047
train_iter_loss: 0.10553310066461563
train_iter_loss: 0.15024356544017792
train loss :0.1662
---------------------
Validation seg loss: 0.21310456109426493 at epoch 689
epoch =    690/  1000, exp = train
train_iter_loss: 0.17196938395500183
train_iter_loss: 0.10965511947870255
train_iter_loss: 0.19450047612190247
train_iter_loss: 0.07814037799835205
train_iter_loss: 0.08318094164133072
train_iter_loss: 0.36171966791152954
train_iter_loss: 0.19746538996696472
train_iter_loss: 0.21919210255146027
train_iter_loss: 0.2060427963733673
train_iter_loss: 0.09120697528123856
train_iter_loss: 0.12806077301502228
train_iter_loss: 0.19017009437084198
train_iter_loss: 0.12913884222507477
train_iter_loss: 0.12886960804462433
train_iter_loss: 0.2254653424024582
train_iter_loss: 0.1015983521938324
train_iter_loss: 0.08977067470550537
train_iter_loss: 0.18758219480514526
train_iter_loss: 0.23991529643535614
train_iter_loss: 0.13958562910556793
train_iter_loss: 0.23193837702274323
train_iter_loss: 0.19169096648693085
train_iter_loss: 0.10689514875411987
train_iter_loss: 0.13457077741622925
train_iter_loss: 0.1265728920698166
train_iter_loss: 0.1991436928510666
train_iter_loss: 0.036486390978097916
train_iter_loss: 0.13777634501457214
train_iter_loss: 0.07877292484045029
train_iter_loss: 0.18637964129447937
train_iter_loss: 0.10123477131128311
train_iter_loss: 0.22729164361953735
train_iter_loss: 0.22180840373039246
train_iter_loss: 0.17041540145874023
train_iter_loss: 0.15678799152374268
train_iter_loss: 0.06317980587482452
train_iter_loss: 0.18335793912410736
train_iter_loss: 0.16750553250312805
train_iter_loss: 0.24270768463611603
train_iter_loss: 0.10921046882867813
train_iter_loss: 0.19408120214939117
train_iter_loss: 0.1750302016735077
train_iter_loss: 0.15088218450546265
train_iter_loss: 0.08387011289596558
train_iter_loss: 0.229531392455101
train_iter_loss: 0.11118857562541962
train_iter_loss: 0.15620851516723633
train_iter_loss: 0.16226913034915924
train_iter_loss: 0.09070321917533875
train_iter_loss: 0.12274203449487686
train_iter_loss: 0.07736026495695114
train_iter_loss: 0.1359570175409317
train_iter_loss: 0.14985717833042145
train_iter_loss: 0.15985919535160065
train_iter_loss: 0.17391647398471832
train_iter_loss: 0.07509880512952805
train_iter_loss: 0.09472806006669998
train_iter_loss: 0.12114336341619492
train_iter_loss: 0.13152183592319489
train_iter_loss: 0.163808673620224
train_iter_loss: 0.11861942708492279
train_iter_loss: 0.1283678412437439
train_iter_loss: 0.12688343226909637
train_iter_loss: 0.12349016964435577
train_iter_loss: 0.22155794501304626
train_iter_loss: 0.10511916130781174
train_iter_loss: 0.24344681203365326
train_iter_loss: 0.1143282800912857
train_iter_loss: 0.13988502323627472
train_iter_loss: 0.15824030339717865
train_iter_loss: 0.04193625971674919
train_iter_loss: 0.19504614174365997
train_iter_loss: 0.057429347187280655
train_iter_loss: 0.08857537806034088
train_iter_loss: 0.14322522282600403
train_iter_loss: 0.08811639994382858
train_iter_loss: 0.11100627481937408
train_iter_loss: 0.15705470740795135
train_iter_loss: 0.1281433254480362
train_iter_loss: 0.12462351471185684
train_iter_loss: 0.07642723619937897
train_iter_loss: 0.133535698056221
train_iter_loss: 0.23826931416988373
train_iter_loss: 0.38818448781967163
train_iter_loss: 0.15428899228572845
train_iter_loss: 0.2384227067232132
train_iter_loss: 0.16925230622291565
train_iter_loss: 0.0994153618812561
train_iter_loss: 0.2081308364868164
train_iter_loss: 0.18388228118419647
train_iter_loss: 0.09112093597650528
train_iter_loss: 0.17660698294639587
train_iter_loss: 0.1857128143310547
train_iter_loss: 0.19593746960163116
train_iter_loss: 0.17133645713329315
train_iter_loss: 0.21330571174621582
train_iter_loss: 0.21284855902194977
train_iter_loss: 0.19698074460029602
train_iter_loss: 0.14737185835838318
train_iter_loss: 0.08928530663251877
train loss :0.1546
---------------------
Validation seg loss: 0.21475010983786494 at epoch 690
epoch =    691/  1000, exp = train
train_iter_loss: 0.14582030475139618
train_iter_loss: 0.09377531707286835
train_iter_loss: 0.1726895570755005
train_iter_loss: 0.31648391485214233
train_iter_loss: 0.21571865677833557
train_iter_loss: 0.27584367990493774
train_iter_loss: 0.07276567816734314
train_iter_loss: 0.10923872888088226
train_iter_loss: 0.1377560794353485
train_iter_loss: 0.153082937002182
train_iter_loss: 0.11394627392292023
train_iter_loss: 0.17998924851417542
train_iter_loss: 0.22492413222789764
train_iter_loss: 0.14501816034317017
train_iter_loss: 0.07796607911586761
train_iter_loss: 0.1229008361697197
train_iter_loss: 0.08386765420436859
train_iter_loss: 0.1275261640548706
train_iter_loss: 0.07084326446056366
train_iter_loss: 0.1950083076953888
train_iter_loss: 0.0989217683672905
train_iter_loss: 0.0520058274269104
train_iter_loss: 0.123912513256073
train_iter_loss: 0.2757073938846588
train_iter_loss: 0.12821930646896362
train_iter_loss: 0.1505223959684372
train_iter_loss: 0.12328745424747467
train_iter_loss: 0.05946622043848038
train_iter_loss: 0.11927490681409836
train_iter_loss: 0.08899001777172089
train_iter_loss: 0.2511306405067444
train_iter_loss: 0.36975133419036865
train_iter_loss: 0.08315781503915787
train_iter_loss: 0.11503573507070541
train_iter_loss: 0.15897920727729797
train_iter_loss: 0.19319209456443787
train_iter_loss: 0.20052185654640198
train_iter_loss: 0.1599418818950653
train_iter_loss: 0.33836448192596436
train_iter_loss: 0.09091167896986008
train_iter_loss: 0.2305886596441269
train_iter_loss: 0.260524183511734
train_iter_loss: 0.07580087333917618
train_iter_loss: 0.12037435919046402
train_iter_loss: 0.15364417433738708
train_iter_loss: 0.22831399738788605
train_iter_loss: 0.1622932404279709
train_iter_loss: 0.11285887658596039
train_iter_loss: 0.13732759654521942
train_iter_loss: 0.161921888589859
train_iter_loss: 0.15541507303714752
train_iter_loss: 0.05392836034297943
train_iter_loss: 0.23699821531772614
train_iter_loss: 0.2664005756378174
train_iter_loss: 0.08124540746212006
train_iter_loss: 0.24160030484199524
train_iter_loss: 0.12328704446554184
train_iter_loss: 0.1031133309006691
train_iter_loss: 0.15445302426815033
train_iter_loss: 0.12772676348686218
train_iter_loss: 0.0771537572145462
train_iter_loss: 0.1383468359708786
train_iter_loss: 0.221159890294075
train_iter_loss: 0.04494450241327286
train_iter_loss: 0.13212959468364716
train_iter_loss: 0.20901688933372498
train_iter_loss: 0.12816651165485382
train_iter_loss: 0.09297904372215271
train_iter_loss: 0.08775807172060013
train_iter_loss: 0.22273406386375427
train_iter_loss: 0.1731501966714859
train_iter_loss: 0.21963797509670258
train_iter_loss: 0.10267530381679535
train_iter_loss: 0.14434805512428284
train_iter_loss: 0.17511853575706482
train_iter_loss: 0.09135141223669052
train_iter_loss: 0.16096124053001404
train_iter_loss: 0.1828889399766922
train_iter_loss: 0.10265734791755676
train_iter_loss: 0.08199851959943771
train_iter_loss: 0.2425914853811264
train_iter_loss: 0.1207815632224083
train_iter_loss: 0.17938274145126343
train_iter_loss: 0.17689892649650574
train_iter_loss: 0.15993593633174896
train_iter_loss: 0.3545823097229004
train_iter_loss: 0.10902097076177597
train_iter_loss: 0.2465638518333435
train_iter_loss: 0.17601646482944489
train_iter_loss: 0.12105756998062134
train_iter_loss: 0.1085081696510315
train_iter_loss: 0.23631811141967773
train_iter_loss: 0.13277584314346313
train_iter_loss: 0.1430801898241043
train_iter_loss: 0.19131048023700714
train_iter_loss: 0.14279738068580627
train_iter_loss: 0.09569624066352844
train_iter_loss: 0.1356973648071289
train_iter_loss: 0.12324696034193039
train_iter_loss: 0.1720694750547409
train loss :0.1572
---------------------
Validation seg loss: 0.2177237315841441 at epoch 691
epoch =    692/  1000, exp = train
train_iter_loss: 0.2768184244632721
train_iter_loss: 0.13687960803508759
train_iter_loss: 0.2079111635684967
train_iter_loss: 0.14150938391685486
train_iter_loss: 0.11693035811185837
train_iter_loss: 0.11218267679214478
train_iter_loss: 0.09861863404512405
train_iter_loss: 0.25635460019111633
train_iter_loss: 0.13527880609035492
train_iter_loss: 0.2667420506477356
train_iter_loss: 0.06499648094177246
train_iter_loss: 0.134116068482399
train_iter_loss: 0.39351338148117065
train_iter_loss: 0.1005953773856163
train_iter_loss: 0.08153167366981506
train_iter_loss: 0.1109246239066124
train_iter_loss: 0.11873985826969147
train_iter_loss: 0.11900985240936279
train_iter_loss: 0.16538569331169128
train_iter_loss: 0.1773105412721634
train_iter_loss: 0.06612682342529297
train_iter_loss: 0.19020810723304749
train_iter_loss: 0.13928520679473877
train_iter_loss: 0.15137335658073425
train_iter_loss: 0.16840066015720367
train_iter_loss: 0.1520688384771347
train_iter_loss: 0.10770276933908463
train_iter_loss: 0.10248418897390366
train_iter_loss: 0.10404648631811142
train_iter_loss: 0.16051720082759857
train_iter_loss: 0.14126425981521606
train_iter_loss: 0.019877463579177856
train_iter_loss: 0.14939329028129578
train_iter_loss: 0.1601632535457611
train_iter_loss: 0.18857358396053314
train_iter_loss: 0.051108378916978836
train_iter_loss: 0.07227566838264465
train_iter_loss: 0.1569753736257553
train_iter_loss: 0.13891556859016418
train_iter_loss: 0.1578339785337448
train_iter_loss: 0.15115807950496674
train_iter_loss: 0.190854012966156
train_iter_loss: 0.23166455328464508
train_iter_loss: 0.11981882899999619
train_iter_loss: 0.149754136800766
train_iter_loss: 0.05145838111639023
train_iter_loss: 0.18458040058612823
train_iter_loss: 0.23351383209228516
train_iter_loss: 0.19066597521305084
train_iter_loss: 0.14081428945064545
train_iter_loss: 0.18352417647838593
train_iter_loss: 0.08341068029403687
train_iter_loss: 0.18934392929077148
train_iter_loss: 0.12968812882900238
train_iter_loss: 0.151855006814003
train_iter_loss: 0.21942809224128723
train_iter_loss: 0.16325774788856506
train_iter_loss: 0.1539413183927536
train_iter_loss: 0.04948178678750992
train_iter_loss: 0.20091789960861206
train_iter_loss: 0.18617978692054749
train_iter_loss: 0.21058672666549683
train_iter_loss: 0.1044250950217247
train_iter_loss: 0.1780920773744583
train_iter_loss: 0.17164529860019684
train_iter_loss: 0.09688656777143478
train_iter_loss: 0.3505515158176422
train_iter_loss: 0.17465606331825256
train_iter_loss: 0.11628014594316483
train_iter_loss: 0.10222051292657852
train_iter_loss: 0.08693365007638931
train_iter_loss: 0.12107031047344208
train_iter_loss: 0.30426084995269775
train_iter_loss: 0.14120931923389435
train_iter_loss: 0.2390093207359314
train_iter_loss: 0.13449978828430176
train_iter_loss: 0.1628372222185135
train_iter_loss: 0.238141268491745
train_iter_loss: 0.1436041295528412
train_iter_loss: 0.126333549618721
train_iter_loss: 0.08250213414430618
train_iter_loss: 0.06830266863107681
train_iter_loss: 0.17567306756973267
train_iter_loss: 0.07815153151750565
train_iter_loss: 0.1458907276391983
train_iter_loss: 0.1289573758840561
train_iter_loss: 0.3753329813480377
train_iter_loss: 0.21347199380397797
train_iter_loss: 0.1476193517446518
train_iter_loss: 0.1543554663658142
train_iter_loss: 0.17985087633132935
train_iter_loss: 0.3313070237636566
train_iter_loss: 0.15376944839954376
train_iter_loss: 0.2329200804233551
train_iter_loss: 0.11689432710409164
train_iter_loss: 0.12752975523471832
train_iter_loss: 0.12154802680015564
train_iter_loss: 0.1511358916759491
train_iter_loss: 0.08707110583782196
train_iter_loss: 0.15417839586734772
train loss :0.1574
---------------------
Validation seg loss: 0.22210550864385264 at epoch 692
epoch =    693/  1000, exp = train
train_iter_loss: 0.1899285614490509
train_iter_loss: 0.1398412436246872
train_iter_loss: 0.14272646605968475
train_iter_loss: 0.18906988203525543
train_iter_loss: 0.11945204436779022
train_iter_loss: 0.16495643556118011
train_iter_loss: 0.1918555498123169
train_iter_loss: 0.23365475237369537
train_iter_loss: 0.19799654185771942
train_iter_loss: 0.07607759535312653
train_iter_loss: 0.09771359711885452
train_iter_loss: 0.18279175460338593
train_iter_loss: 0.14323678612709045
train_iter_loss: 0.10365793108940125
train_iter_loss: 0.11951978504657745
train_iter_loss: 0.22559227049350739
train_iter_loss: 0.19163447618484497
train_iter_loss: 0.1400812268257141
train_iter_loss: 0.07849357277154922
train_iter_loss: 0.10681000351905823
train_iter_loss: 0.165037140250206
train_iter_loss: 0.22497494518756866
train_iter_loss: 0.2020612508058548
train_iter_loss: 0.05597279593348503
train_iter_loss: 0.22374612092971802
train_iter_loss: 0.07413776218891144
train_iter_loss: 0.17358021438121796
train_iter_loss: 0.07471080869436264
train_iter_loss: 0.1177501454949379
train_iter_loss: 0.16504904627799988
train_iter_loss: 0.09005599468946457
train_iter_loss: 0.09183944016695023
train_iter_loss: 0.2229604572057724
train_iter_loss: 0.2796548008918762
train_iter_loss: 0.09702358394861221
train_iter_loss: 0.1942681223154068
train_iter_loss: 0.12803958356380463
train_iter_loss: 0.16617904603481293
train_iter_loss: 0.08881206065416336
train_iter_loss: 0.12365978211164474
train_iter_loss: 0.20721963047981262
train_iter_loss: 0.11487738788127899
train_iter_loss: 0.2610214650630951
train_iter_loss: 0.18058593571186066
train_iter_loss: 0.2704501748085022
train_iter_loss: 0.1535571664571762
train_iter_loss: 0.12826579809188843
train_iter_loss: 0.10012724250555038
train_iter_loss: 0.13771429657936096
train_iter_loss: 0.20520377159118652
train_iter_loss: 0.14049017429351807
train_iter_loss: 0.09752572327852249
train_iter_loss: 0.15485318005084991
train_iter_loss: 0.12644177675247192
train_iter_loss: 0.05174117162823677
train_iter_loss: 0.10985488444566727
train_iter_loss: 0.15092775225639343
train_iter_loss: 0.2233046293258667
train_iter_loss: 0.14771148562431335
train_iter_loss: 0.1430903971195221
train_iter_loss: 0.1680186688899994
train_iter_loss: 0.2289707213640213
train_iter_loss: 0.1259164810180664
train_iter_loss: 0.16498050093650818
train_iter_loss: 0.2409965991973877
train_iter_loss: 0.11540434509515762
train_iter_loss: 0.14874659478664398
train_iter_loss: 0.1733800768852234
train_iter_loss: 0.07186546176671982
train_iter_loss: 0.13971872627735138
train_iter_loss: 0.15347236394882202
train_iter_loss: 0.16299477219581604
train_iter_loss: 0.12538130581378937
train_iter_loss: 0.11007648706436157
train_iter_loss: 0.15372321009635925
train_iter_loss: 0.1868416965007782
train_iter_loss: 0.15643464028835297
train_iter_loss: 0.13977734744548798
train_iter_loss: 0.19812390208244324
train_iter_loss: 0.036386799067258835
train_iter_loss: 0.22316502034664154
train_iter_loss: 0.1433665156364441
train_iter_loss: 0.12552392482757568
train_iter_loss: 0.14759406447410583
train_iter_loss: 0.13915161788463593
train_iter_loss: 0.07347089797258377
train_iter_loss: 0.11249874532222748
train_iter_loss: 0.11675824970006943
train_iter_loss: 0.11736292392015457
train_iter_loss: 0.13112355768680573
train_iter_loss: 0.13341499865055084
train_iter_loss: 0.11466600745916367
train_iter_loss: 0.22924001514911652
train_iter_loss: 0.04682290926575661
train_iter_loss: 0.18523597717285156
train_iter_loss: 0.11279682070016861
train_iter_loss: 0.20009617507457733
train_iter_loss: 0.13779260218143463
train_iter_loss: 0.1219252347946167
train_iter_loss: 0.1579059660434723
train loss :0.1503
---------------------
Validation seg loss: 0.21715062463058615 at epoch 693
epoch =    694/  1000, exp = train
train_iter_loss: 0.09448786079883575
train_iter_loss: 0.2412143349647522
train_iter_loss: 0.12073732167482376
train_iter_loss: 0.10196620225906372
train_iter_loss: 0.2525072693824768
train_iter_loss: 0.2234603613615036
train_iter_loss: 0.2549755573272705
train_iter_loss: 0.2732042968273163
train_iter_loss: 0.16797587275505066
train_iter_loss: 0.10690055042505264
train_iter_loss: 0.13971886038780212
train_iter_loss: 0.1577250063419342
train_iter_loss: 0.1409701704978943
train_iter_loss: 0.07045481353998184
train_iter_loss: 0.12779492139816284
train_iter_loss: 0.17921684682369232
train_iter_loss: 0.20362010598182678
train_iter_loss: 0.10450227558612823
train_iter_loss: 0.20959728956222534
train_iter_loss: 0.13095983862876892
train_iter_loss: 0.14336906373500824
train_iter_loss: 0.10686106234788895
train_iter_loss: 0.20883694291114807
train_iter_loss: 0.09402891248464584
train_iter_loss: 0.21602663397789001
train_iter_loss: 0.1053955927491188
train_iter_loss: 0.11864690482616425
train_iter_loss: 0.17979465425014496
train_iter_loss: 0.1666014939546585
train_iter_loss: 0.09718327224254608
train_iter_loss: 0.14396609365940094
train_iter_loss: 0.12766613066196442
train_iter_loss: 0.13271668553352356
train_iter_loss: 0.10404835641384125
train_iter_loss: 0.14259427785873413
train_iter_loss: 0.10539981722831726
train_iter_loss: 0.12784476578235626
train_iter_loss: 0.12334456294775009
train_iter_loss: 0.22424179315567017
train_iter_loss: 0.09420695155858994
train_iter_loss: 0.15825112164020538
train_iter_loss: 0.2111918181180954
train_iter_loss: 0.1617925614118576
train_iter_loss: 0.07799525558948517
train_iter_loss: 0.29096680879592896
train_iter_loss: 0.22526872158050537
train_iter_loss: 0.20342057943344116
train_iter_loss: 0.1499377191066742
train_iter_loss: 0.14570406079292297
train_iter_loss: 0.166899174451828
train_iter_loss: 0.11792460829019547
train_iter_loss: 0.09096626937389374
train_iter_loss: 0.10477028042078018
train_iter_loss: 0.11072225123643875
train_iter_loss: 0.15720656514167786
train_iter_loss: 0.19673064351081848
train_iter_loss: 0.22014681994915009
train_iter_loss: 0.1654624193906784
train_iter_loss: 0.07664564996957779
train_iter_loss: 0.17947475612163544
train_iter_loss: 0.2224692851305008
train_iter_loss: 0.13096149265766144
train_iter_loss: 0.10718685388565063
train_iter_loss: 0.20152606070041656
train_iter_loss: 0.2053045630455017
train_iter_loss: 0.21886709332466125
train_iter_loss: 0.14891867339611053
train_iter_loss: 0.1538587361574173
train_iter_loss: 0.12733669579029083
train_iter_loss: 0.19551178812980652
train_iter_loss: 0.08071069419384003
train_iter_loss: 0.11246361583471298
train_iter_loss: 0.13244768977165222
train_iter_loss: 0.17475856840610504
train_iter_loss: 0.13495446741580963
train_iter_loss: 0.09341034293174744
train_iter_loss: 0.16947898268699646
train_iter_loss: 0.13914625346660614
train_iter_loss: 0.1759084016084671
train_iter_loss: 0.1651199907064438
train_iter_loss: 0.33281832933425903
train_iter_loss: 0.2819053828716278
train_iter_loss: 0.20687662065029144
train_iter_loss: 0.18638195097446442
train_iter_loss: 0.14558784663677216
train_iter_loss: 0.12126167118549347
train_iter_loss: 0.2221468836069107
train_iter_loss: 0.1725177764892578
train_iter_loss: 0.16002622246742249
train_iter_loss: 0.1566503494977951
train_iter_loss: 0.047279179096221924
train_iter_loss: 0.12818317115306854
train_iter_loss: 0.08126088976860046
train_iter_loss: 0.3625694215297699
train_iter_loss: 0.2014322280883789
train_iter_loss: 0.15601582825183868
train_iter_loss: 0.073884978890419
train_iter_loss: 0.17692509293556213
train_iter_loss: 0.28451618552207947
train_iter_loss: 0.057034626603126526
train loss :0.1607
---------------------
Validation seg loss: 0.2225690545373649 at epoch 694
epoch =    695/  1000, exp = train
train_iter_loss: 0.26836901903152466
train_iter_loss: 0.09960239380598068
train_iter_loss: 0.1554853320121765
train_iter_loss: 0.19129955768585205
train_iter_loss: 0.0650205984711647
train_iter_loss: 0.15119384229183197
train_iter_loss: 0.4000721275806427
train_iter_loss: 0.167019322514534
train_iter_loss: 0.1561189442873001
train_iter_loss: 0.08034476637840271
train_iter_loss: 0.2887159585952759
train_iter_loss: 0.10483470559120178
train_iter_loss: 0.16808590292930603
train_iter_loss: 0.25228482484817505
train_iter_loss: 0.1293860226869583
train_iter_loss: 0.1692253202199936
train_iter_loss: 0.13084115087985992
train_iter_loss: 0.16985760629177094
train_iter_loss: 0.14378540217876434
train_iter_loss: 0.048105932772159576
train_iter_loss: 0.09428092837333679
train_iter_loss: 0.21211743354797363
train_iter_loss: 0.19013051688671112
train_iter_loss: 0.14031098783016205
train_iter_loss: 0.15094813704490662
train_iter_loss: 0.15850979089736938
train_iter_loss: 0.3151983916759491
train_iter_loss: 0.11424454301595688
train_iter_loss: 0.11607543379068375
train_iter_loss: 0.12563960254192352
train_iter_loss: 0.21212126314640045
train_iter_loss: 0.05163680389523506
train_iter_loss: 0.11659113317728043
train_iter_loss: 0.09114019572734833
train_iter_loss: 0.19581498205661774
train_iter_loss: 0.18122629821300507
train_iter_loss: 0.14902731776237488
train_iter_loss: 0.19743262231349945
train_iter_loss: 0.10409925878047943
train_iter_loss: 0.1343879997730255
train_iter_loss: 0.10501579195261002
train_iter_loss: 0.11094308644533157
train_iter_loss: 0.14023619890213013
train_iter_loss: 0.1706690490245819
train_iter_loss: 0.1460246592760086
train_iter_loss: 0.11333821713924408
train_iter_loss: 0.15326319634914398
train_iter_loss: 0.11855348944664001
train_iter_loss: 0.0822959691286087
train_iter_loss: 0.19556203484535217
train_iter_loss: 0.11338409036397934
train_iter_loss: 0.15599507093429565
train_iter_loss: 0.12247850745916367
train_iter_loss: 0.10695529729127884
train_iter_loss: 0.1716790646314621
train_iter_loss: 0.0700901448726654
train_iter_loss: 0.1791219562292099
train_iter_loss: 0.14452973008155823
train_iter_loss: 0.13544340431690216
train_iter_loss: 0.1605963110923767
train_iter_loss: 0.18992812931537628
train_iter_loss: 0.15898211300373077
train_iter_loss: 0.2597162127494812
train_iter_loss: 0.27587053179740906
train_iter_loss: 0.12023179978132248
train_iter_loss: 0.02286706306040287
train_iter_loss: 0.2763342261314392
train_iter_loss: 0.2203441560268402
train_iter_loss: 0.07447304576635361
train_iter_loss: 0.12973544001579285
train_iter_loss: 0.16929461061954498
train_iter_loss: 0.13946962356567383
train_iter_loss: 0.18231292068958282
train_iter_loss: 0.18346351385116577
train_iter_loss: 0.23453804850578308
train_iter_loss: 0.18320001661777496
train_iter_loss: 0.2342534363269806
train_iter_loss: 0.11989856511354446
train_iter_loss: 0.1634819060564041
train_iter_loss: 0.09752054512500763
train_iter_loss: 0.12695294618606567
train_iter_loss: 0.08801406621932983
train_iter_loss: 0.151075541973114
train_iter_loss: 0.11464646458625793
train_iter_loss: 0.2916601896286011
train_iter_loss: 0.059402961283922195
train_iter_loss: 0.09789208322763443
train_iter_loss: 0.23794324696063995
train_iter_loss: 0.11339715868234634
train_iter_loss: 0.26496878266334534
train_iter_loss: 0.1565357744693756
train_iter_loss: 0.16476254165172577
train_iter_loss: 0.20441630482673645
train_iter_loss: 0.17466211318969727
train_iter_loss: 0.21663112938404083
train_iter_loss: 0.08980289101600647
train_iter_loss: 0.21770279109477997
train_iter_loss: 0.09713172167539597
train_iter_loss: 0.10776858776807785
train_iter_loss: 0.28433138132095337
train loss :0.1594
---------------------
Validation seg loss: 0.2171110003167447 at epoch 695
epoch =    696/  1000, exp = train
train_iter_loss: 0.22989784181118011
train_iter_loss: 0.15028846263885498
train_iter_loss: 0.21036340296268463
train_iter_loss: 0.2268969714641571
train_iter_loss: 0.19818289577960968
train_iter_loss: 0.06476553529500961
train_iter_loss: 0.2547537088394165
train_iter_loss: 0.12759798765182495
train_iter_loss: 0.2502385079860687
train_iter_loss: 0.021020909771323204
train_iter_loss: 0.07806117832660675
train_iter_loss: 0.2699767053127289
train_iter_loss: 0.2767234444618225
train_iter_loss: 0.19170454144477844
train_iter_loss: 0.053232789039611816
train_iter_loss: 0.2696631848812103
train_iter_loss: 0.21407806873321533
train_iter_loss: 0.27974337339401245
train_iter_loss: 0.18876628577709198
train_iter_loss: 0.1757882982492447
train_iter_loss: 0.13049408793449402
train_iter_loss: 0.17101497948169708
train_iter_loss: 0.16355517506599426
train_iter_loss: 0.0634981021285057
train_iter_loss: 0.15973004698753357
train_iter_loss: 0.08546198904514313
train_iter_loss: 0.19533652067184448
train_iter_loss: 0.12662602961063385
train_iter_loss: 0.11649240553379059
train_iter_loss: 0.12308899313211441
train_iter_loss: 0.07473354786634445
train_iter_loss: 0.12748771905899048
train_iter_loss: 0.140084907412529
train_iter_loss: 0.14016230404376984
train_iter_loss: 0.32763054966926575
train_iter_loss: 0.11486973613500595
train_iter_loss: 0.15971556305885315
train_iter_loss: 0.15295018255710602
train_iter_loss: 0.2255731225013733
train_iter_loss: 0.10003481060266495
train_iter_loss: 0.09708821028470993
train_iter_loss: 0.1469927281141281
train_iter_loss: 0.2805195748806
train_iter_loss: 0.141293466091156
train_iter_loss: 0.24696560204029083
train_iter_loss: 0.1732141077518463
train_iter_loss: 0.08369550108909607
train_iter_loss: 0.23323854804039001
train_iter_loss: 0.11026175320148468
train_iter_loss: 0.11720628291368484
train_iter_loss: 0.11676043272018433
train_iter_loss: 0.1545000523328781
train_iter_loss: 0.213212788105011
train_iter_loss: 0.10369817167520523
train_iter_loss: 0.11957072466611862
train_iter_loss: 0.09626132249832153
train_iter_loss: 0.1753084808588028
train_iter_loss: 0.07772360742092133
train_iter_loss: 0.17135921120643616
train_iter_loss: 0.17649738490581512
train_iter_loss: 0.148422509431839
train_iter_loss: 0.12913791835308075
train_iter_loss: 0.09351236373186111
train_iter_loss: 0.08092709630727768
train_iter_loss: 0.19888611137866974
train_iter_loss: 0.15356864035129547
train_iter_loss: 0.1263044774532318
train_iter_loss: 0.14518971741199493
train_iter_loss: 0.1860746145248413
train_iter_loss: 0.22395038604736328
train_iter_loss: 0.13000044226646423
train_iter_loss: 0.1466653048992157
train_iter_loss: 0.18976126611232758
train_iter_loss: 0.10749396681785583
train_iter_loss: 0.20214907824993134
train_iter_loss: 0.14313168823719025
train_iter_loss: 0.16833698749542236
train_iter_loss: 0.12879571318626404
train_iter_loss: 0.10401816666126251
train_iter_loss: 0.0995686948299408
train_iter_loss: 0.19056667387485504
train_iter_loss: 0.28147462010383606
train_iter_loss: 0.2219228297472
train_iter_loss: 0.159237340092659
train_iter_loss: 0.0817192941904068
train_iter_loss: 0.17265397310256958
train_iter_loss: 0.08476975560188293
train_iter_loss: 0.10887125879526138
train_iter_loss: 0.04458090662956238
train_iter_loss: 0.09751499444246292
train_iter_loss: 0.2233607918024063
train_iter_loss: 0.20888134837150574
train_iter_loss: 0.0893862396478653
train_iter_loss: 0.19715304672718048
train_iter_loss: 0.19403736293315887
train_iter_loss: 0.18797123432159424
train_iter_loss: 0.10477443784475327
train_iter_loss: 0.19402213394641876
train_iter_loss: 0.15534865856170654
train_iter_loss: 0.08341581374406815
train loss :0.1581
---------------------
Validation seg loss: 0.21737648579204139 at epoch 696
epoch =    697/  1000, exp = train
train_iter_loss: 0.1803058236837387
train_iter_loss: 0.0965464785695076
train_iter_loss: 0.2086368352174759
train_iter_loss: 0.16374127566814423
train_iter_loss: 0.0704680010676384
train_iter_loss: 0.22653689980506897
train_iter_loss: 0.1093461886048317
train_iter_loss: 0.13261349499225616
train_iter_loss: 0.19828060269355774
train_iter_loss: 0.24273547530174255
train_iter_loss: 0.14966078102588654
train_iter_loss: 0.14315494894981384
train_iter_loss: 0.13580858707427979
train_iter_loss: 0.10517136007547379
train_iter_loss: 0.07757667452096939
train_iter_loss: 0.184867262840271
train_iter_loss: 0.09311238676309586
train_iter_loss: 0.04964050278067589
train_iter_loss: 0.12853845953941345
train_iter_loss: 0.22352570295333862
train_iter_loss: 0.12357933074235916
train_iter_loss: 0.14780141413211823
train_iter_loss: 0.22346514463424683
train_iter_loss: 0.10937504470348358
train_iter_loss: 0.08808677643537521
train_iter_loss: 0.1630711555480957
train_iter_loss: 0.15814843773841858
train_iter_loss: 0.05600631237030029
train_iter_loss: 0.08431105315685272
train_iter_loss: 0.0580584891140461
train_iter_loss: 0.15380215644836426
train_iter_loss: 0.20066429674625397
train_iter_loss: 0.2262875735759735
train_iter_loss: 0.13045182824134827
train_iter_loss: 0.18295630812644958
train_iter_loss: 0.1299472600221634
train_iter_loss: 0.07629747688770294
train_iter_loss: 0.11002502590417862
train_iter_loss: 0.13988728821277618
train_iter_loss: 0.08323147892951965
train_iter_loss: 0.11903251707553864
train_iter_loss: 0.2369653880596161
train_iter_loss: 0.37459224462509155
train_iter_loss: 0.26949283480644226
train_iter_loss: 0.16451974213123322
train_iter_loss: 0.07229142636060715
train_iter_loss: 0.11465658247470856
train_iter_loss: 0.16503243148326874
train_iter_loss: 0.18183737993240356
train_iter_loss: 0.23204582929611206
train_iter_loss: 0.11493310332298279
train_iter_loss: 0.10785878449678421
train_iter_loss: 0.2050318568944931
train_iter_loss: 0.1961006373167038
train_iter_loss: 0.13257507979869843
train_iter_loss: 0.10973720997571945
train_iter_loss: 0.15393389761447906
train_iter_loss: 0.19046777486801147
train_iter_loss: 0.1607600748538971
train_iter_loss: 0.2054603546857834
train_iter_loss: 0.08778799325227737
train_iter_loss: 0.3245934844017029
train_iter_loss: 0.201764777302742
train_iter_loss: 0.20304618775844574
train_iter_loss: 0.18287058174610138
train_iter_loss: 0.11551116406917572
train_iter_loss: 0.1411798596382141
train_iter_loss: 0.10534904897212982
train_iter_loss: 0.17976708710193634
train_iter_loss: 0.30333423614501953
train_iter_loss: 0.15663093328475952
train_iter_loss: 0.21280580759048462
train_iter_loss: 0.21453528106212616
train_iter_loss: 0.12089494615793228
train_iter_loss: 0.1671750694513321
train_iter_loss: 0.1159740537405014
train_iter_loss: 0.18630875647068024
train_iter_loss: 0.14491897821426392
train_iter_loss: 0.2691443860530853
train_iter_loss: 0.1078147292137146
train_iter_loss: 0.2115880399942398
train_iter_loss: 0.21225598454475403
train_iter_loss: 0.07552435994148254
train_iter_loss: 0.1585136353969574
train_iter_loss: 0.2632490396499634
train_iter_loss: 0.10219831764698029
train_iter_loss: 0.06648597121238708
train_iter_loss: 0.09792424738407135
train_iter_loss: 0.19060301780700684
train_iter_loss: 0.11949113011360168
train_iter_loss: 0.20862850546836853
train_iter_loss: 0.1878836303949356
train_iter_loss: 0.12545792758464813
train_iter_loss: 0.1016402542591095
train_iter_loss: 0.21684281527996063
train_iter_loss: 0.06769845634698868
train_iter_loss: 0.14890047907829285
train_iter_loss: 0.348509281873703
train_iter_loss: 0.08083163946866989
train_iter_loss: 0.17388159036636353
train loss :0.1593
---------------------
Validation seg loss: 0.21491103188820043 at epoch 697
epoch =    698/  1000, exp = train
train_iter_loss: 0.22143423557281494
train_iter_loss: 0.21114908158779144
train_iter_loss: 0.25644850730895996
train_iter_loss: 0.2924632132053375
train_iter_loss: 0.17862635850906372
train_iter_loss: 0.4447042644023895
train_iter_loss: 0.12583909928798676
train_iter_loss: 0.10691660642623901
train_iter_loss: 0.21389126777648926
train_iter_loss: 0.0902365893125534
train_iter_loss: 0.11136206984519958
train_iter_loss: 0.10633430629968643
train_iter_loss: 0.17172156274318695
train_iter_loss: 0.2780381441116333
train_iter_loss: 0.10379400849342346
train_iter_loss: 0.0872253030538559
train_iter_loss: 0.18556520342826843
train_iter_loss: 0.13055109977722168
train_iter_loss: 0.09545508772134781
train_iter_loss: 0.21379172801971436
train_iter_loss: 0.15200136601924896
train_iter_loss: 0.24022068083286285
train_iter_loss: 0.1406775265932083
train_iter_loss: 0.12018094211816788
train_iter_loss: 0.18890875577926636
train_iter_loss: 0.08411107212305069
train_iter_loss: 0.11216139793395996
train_iter_loss: 0.15634432435035706
train_iter_loss: 0.11168010532855988
train_iter_loss: 0.12262565642595291
train_iter_loss: 0.12804844975471497
train_iter_loss: 0.15566371381282806
train_iter_loss: 0.12657377123832703
train_iter_loss: 0.15662287175655365
train_iter_loss: 0.19376304745674133
train_iter_loss: 0.12289991974830627
train_iter_loss: 0.09203246980905533
train_iter_loss: 0.07712794840335846
train_iter_loss: 0.11327642947435379
train_iter_loss: 0.31778091192245483
train_iter_loss: 0.16605477035045624
train_iter_loss: 0.11649806797504425
train_iter_loss: 0.1761326938867569
train_iter_loss: 0.07505986094474792
train_iter_loss: 0.12036717683076859
train_iter_loss: 0.14741063117980957
train_iter_loss: 0.1643957942724228
train_iter_loss: 0.2569189965724945
train_iter_loss: 0.12175766378641129
train_iter_loss: 0.13240888714790344
train_iter_loss: 0.195706307888031
train_iter_loss: 0.20390556752681732
train_iter_loss: 0.15518876910209656
train_iter_loss: 0.25143542885780334
train_iter_loss: 0.13682518899440765
train_iter_loss: 0.09536130726337433
train_iter_loss: 0.17843319475650787
train_iter_loss: 0.05478934943675995
train_iter_loss: 0.1399378478527069
train_iter_loss: 0.12134137749671936
train_iter_loss: 0.10457611829042435
train_iter_loss: 0.15888971090316772
train_iter_loss: 0.16524286568164825
train_iter_loss: 0.18896637856960297
train_iter_loss: 0.17408953607082367
train_iter_loss: 0.14241346716880798
train_iter_loss: 0.1121523305773735
train_iter_loss: 0.1326490342617035
train_iter_loss: 0.10887191444635391
train_iter_loss: 0.16964833438396454
train_iter_loss: 0.16770173609256744
train_iter_loss: 0.22232836484909058
train_iter_loss: 0.20025742053985596
train_iter_loss: 0.10778848826885223
train_iter_loss: 0.16327977180480957
train_iter_loss: 0.23208549618721008
train_iter_loss: 0.12031690031290054
train_iter_loss: 0.12509071826934814
train_iter_loss: 0.19496077299118042
train_iter_loss: 0.24345329403877258
train_iter_loss: 0.14060433208942413
train_iter_loss: 0.13467524945735931
train_iter_loss: 0.15726961195468903
train_iter_loss: 0.1151496097445488
train_iter_loss: 0.0950712040066719
train_iter_loss: 0.0775616392493248
train_iter_loss: 0.2780180275440216
train_iter_loss: 0.1626625657081604
train_iter_loss: 0.2754665017127991
train_iter_loss: 0.20607268810272217
train_iter_loss: 0.16216765344142914
train_iter_loss: 0.12034387141466141
train_iter_loss: 0.16720040142536163
train_iter_loss: 0.21687611937522888
train_iter_loss: 0.08759645372629166
train_iter_loss: 0.19232742488384247
train_iter_loss: 0.12784400582313538
train_iter_loss: 0.25695937871932983
train_iter_loss: 0.23185835778713226
train_iter_loss: 0.27682557702064514
train loss :0.1642
---------------------
Validation seg loss: 0.22438668529942352 at epoch 698
epoch =    699/  1000, exp = train
train_iter_loss: 0.14266835153102875
train_iter_loss: 0.10308651626110077
train_iter_loss: 0.15839949250221252
train_iter_loss: 0.15801799297332764
train_iter_loss: 0.10996221750974655
train_iter_loss: 0.05974840000271797
train_iter_loss: 0.1563376486301422
train_iter_loss: 0.0695839673280716
train_iter_loss: 0.10598233342170715
train_iter_loss: 0.2302769422531128
train_iter_loss: 0.14571426808834076
train_iter_loss: 0.12526196241378784
train_iter_loss: 0.16272763907909393
train_iter_loss: 0.1752697378396988
train_iter_loss: 0.1789502203464508
train_iter_loss: 0.14571282267570496
train_iter_loss: 0.14857202768325806
train_iter_loss: 0.1895950585603714
train_iter_loss: 0.2203715741634369
train_iter_loss: 0.1659994125366211
train_iter_loss: 0.17029699683189392
train_iter_loss: 0.17199581861495972
train_iter_loss: 0.2426280379295349
train_iter_loss: 0.11659535020589828
train_iter_loss: 0.13441991806030273
train_iter_loss: 0.2120174616575241
train_iter_loss: 0.11863754689693451
train_iter_loss: 0.20706026256084442
train_iter_loss: 0.2597970962524414
train_iter_loss: 0.12128991633653641
train_iter_loss: 0.11035668849945068
train_iter_loss: 0.19717943668365479
train_iter_loss: 0.17000006139278412
train_iter_loss: 0.2199016660451889
train_iter_loss: 0.04871254414319992
train_iter_loss: 0.18233323097229004
train_iter_loss: 0.06692450493574142
train_iter_loss: 0.12756112217903137
train_iter_loss: 0.027697745710611343
train_iter_loss: 0.07872560620307922
train_iter_loss: 0.08653483539819717
train_iter_loss: 0.06463176757097244
train_iter_loss: 0.19236430525779724
train_iter_loss: 0.10886140912771225
train_iter_loss: 0.22376061975955963
train_iter_loss: 0.16837315261363983
train_iter_loss: 0.1538640707731247
train_iter_loss: 0.2255333513021469
train_iter_loss: 0.1464003324508667
train_iter_loss: 0.1361551433801651
train_iter_loss: 0.17370913922786713
train_iter_loss: 0.3792198598384857
train_iter_loss: 0.18942765891551971
train_iter_loss: 0.20660904049873352
train_iter_loss: 0.14649055898189545
train_iter_loss: 0.15188567340373993
train_iter_loss: 0.3215040862560272
train_iter_loss: 0.16533802449703217
train_iter_loss: 0.1375238597393036
train_iter_loss: 0.28314876556396484
train_iter_loss: 0.13035228848457336
train_iter_loss: 0.17577023804187775
train_iter_loss: 0.1518671214580536
train_iter_loss: 0.1403089463710785
train_iter_loss: 0.1480076164007187
train_iter_loss: 0.17464351654052734
train_iter_loss: 0.15560191869735718
train_iter_loss: 0.10937748104333878
train_iter_loss: 0.17198626697063446
train_iter_loss: 0.3114023804664612
train_iter_loss: 0.14516043663024902
train_iter_loss: 0.04955943673849106
train_iter_loss: 0.10230551660060883
train_iter_loss: 0.07158198207616806
train_iter_loss: 0.09225071966648102
train_iter_loss: 0.1600046306848526
train_iter_loss: 0.13514594733715057
train_iter_loss: 0.11359468102455139
train_iter_loss: 0.21750220656394958
train_iter_loss: 0.2272031456232071
train_iter_loss: 0.13081178069114685
train_iter_loss: 0.15236593782901764
train_iter_loss: 0.172573983669281
train_iter_loss: 0.3646843135356903
train_iter_loss: 0.1030087098479271
train_iter_loss: 0.21248677372932434
train_iter_loss: 0.20975668728351593
train_iter_loss: 0.10060656070709229
train_iter_loss: 0.2015874832868576
train_iter_loss: 0.1296881139278412
train_iter_loss: 0.1340453177690506
train_iter_loss: 0.13447611033916473
train_iter_loss: 0.1846127063035965
train_iter_loss: 0.14678359031677246
train_iter_loss: 0.10610593110322952
train_iter_loss: 0.1599700152873993
train_iter_loss: 0.11859087646007538
train_iter_loss: 0.09454113245010376
train_iter_loss: 0.2507507801055908
train_iter_loss: 0.15870806574821472
train loss :0.1601
---------------------
Validation seg loss: 0.21506915339883767 at epoch 699
epoch =    700/  1000, exp = train
train_iter_loss: 0.11712795495986938
train_iter_loss: 0.22828947007656097
train_iter_loss: 0.12818075716495514
train_iter_loss: 0.19905170798301697
train_iter_loss: 0.13666124641895294
train_iter_loss: 0.20114436745643616
train_iter_loss: 0.02562056854367256
train_iter_loss: 0.17375560104846954
train_iter_loss: 0.24053454399108887
train_iter_loss: 0.09993238747119904
train_iter_loss: 0.11265479028224945
train_iter_loss: 0.15259136259555817
train_iter_loss: 0.08979480713605881
train_iter_loss: 0.16254861652851105
train_iter_loss: 0.1601346880197525
train_iter_loss: 0.12322908639907837
train_iter_loss: 0.19125357270240784
train_iter_loss: 0.10630301386117935
train_iter_loss: 0.18832512199878693
train_iter_loss: 0.1107320785522461
train_iter_loss: 0.17006634175777435
train_iter_loss: 0.12915192544460297
train_iter_loss: 0.11118756234645844
train_iter_loss: 0.14359277486801147
train_iter_loss: 0.210860013961792
train_iter_loss: 0.28070688247680664
train_iter_loss: 0.1798923760652542
train_iter_loss: 0.07494726777076721
train_iter_loss: 0.1338229924440384
train_iter_loss: 0.10585559159517288
train_iter_loss: 0.20952607691287994
train_iter_loss: 0.31372788548469543
train_iter_loss: 0.18837851285934448
train_iter_loss: 0.22998973727226257
train_iter_loss: 0.19961875677108765
train_iter_loss: 0.16116265952587128
train_iter_loss: 0.3183644413948059
train_iter_loss: 0.19684307277202606
train_iter_loss: 0.0881364643573761
train_iter_loss: 0.1425369679927826
train_iter_loss: 0.1362849324941635
train_iter_loss: 0.055686116218566895
train_iter_loss: 0.17971397936344147
train_iter_loss: 0.14144554734230042
train_iter_loss: 0.1281844824552536
train_iter_loss: 0.1485934555530548
train_iter_loss: 0.031791169196367264
train_iter_loss: 0.08329523354768753
train_iter_loss: 0.21850846707820892
train_iter_loss: 0.22111371159553528
train_iter_loss: 0.12309658527374268
train_iter_loss: 0.2508944571018219
train_iter_loss: 0.252505362033844
train_iter_loss: 0.18095076084136963
train_iter_loss: 0.12024872750043869
train_iter_loss: 0.14800506830215454
train_iter_loss: 0.12176347523927689
train_iter_loss: 0.08010683953762054
train_iter_loss: 0.06396391242742538
train_iter_loss: 0.18151482939720154
train_iter_loss: 0.13657507300376892
train_iter_loss: 0.21165348589420319
train_iter_loss: 0.27141815423965454
train_iter_loss: 0.18700812757015228
train_iter_loss: 0.12238792330026627
train_iter_loss: 0.0788709968328476
train_iter_loss: 0.16928817331790924
train_iter_loss: 0.14950427412986755
train_iter_loss: 0.21206967532634735
train_iter_loss: 0.18992756307125092
train_iter_loss: 0.13632437586784363
train_iter_loss: 0.2517344653606415
train_iter_loss: 0.17192135751247406
train_iter_loss: 0.17019221186637878
train_iter_loss: 0.1165618821978569
train_iter_loss: 0.17706528306007385
train_iter_loss: 0.15231558680534363
train_iter_loss: 0.25457650423049927
train_iter_loss: 0.20806193351745605
train_iter_loss: 0.2175244241952896
train_iter_loss: 0.1431565284729004
train_iter_loss: 0.12348413467407227
train_iter_loss: 0.06868914514780045
train_iter_loss: 0.1209992989897728
train_iter_loss: 0.11303416639566422
train_iter_loss: 0.21831151843070984
train_iter_loss: 0.06249748915433884
train_iter_loss: 0.134832963347435
train_iter_loss: 0.06949100643396378
train_iter_loss: 0.1527218520641327
train_iter_loss: 0.08233225345611572
train_iter_loss: 0.043774694204330444
train_iter_loss: 0.10718226432800293
train_iter_loss: 0.08817300200462341
train_iter_loss: 0.14069344103336334
train_iter_loss: 0.062163542956113815
train_iter_loss: 0.09319324046373367
train_iter_loss: 0.15005111694335938
train_iter_loss: 0.16055609285831451
train_iter_loss: 0.17000067234039307
train loss :0.1545
---------------------
Validation seg loss: 0.21162380275474685 at epoch 700
epoch =    701/  1000, exp = train
train_iter_loss: 0.19795893132686615
train_iter_loss: 0.23765860497951508
train_iter_loss: 0.13258780539035797
train_iter_loss: 0.028327791020274162
train_iter_loss: 0.12036672979593277
train_iter_loss: 0.14274008572101593
train_iter_loss: 0.15028095245361328
train_iter_loss: 0.23119325935840607
train_iter_loss: 0.14216110110282898
train_iter_loss: 0.11573682725429535
train_iter_loss: 0.16331176459789276
train_iter_loss: 0.24448895454406738
train_iter_loss: 0.16177432239055634
train_iter_loss: 0.1547367125749588
train_iter_loss: 0.11486715078353882
train_iter_loss: 0.1599930077791214
train_iter_loss: 0.252367228269577
train_iter_loss: 0.13187727332115173
train_iter_loss: 0.16764873266220093
train_iter_loss: 0.11145445704460144
train_iter_loss: 0.14778442680835724
train_iter_loss: 0.24571794271469116
train_iter_loss: 0.1464507281780243
train_iter_loss: 0.1683902144432068
train_iter_loss: 0.2072129100561142
train_iter_loss: 0.08715902268886566
train_iter_loss: 0.1252664178609848
train_iter_loss: 0.15375152230262756
train_iter_loss: 0.15405674278736115
train_iter_loss: 0.255725622177124
train_iter_loss: 0.1511363536119461
train_iter_loss: 0.13936102390289307
train_iter_loss: 0.2106236070394516
train_iter_loss: 0.11882852762937546
train_iter_loss: 0.203903466463089
train_iter_loss: 0.11748097091913223
train_iter_loss: 0.13792774081230164
train_iter_loss: 0.09219297766685486
train_iter_loss: 0.15698474645614624
train_iter_loss: 0.22487306594848633
train_iter_loss: 0.2018209844827652
train_iter_loss: 0.09417799115180969
train_iter_loss: 0.16374021768569946
train_iter_loss: 0.1381547898054123
train_iter_loss: 0.1948445737361908
train_iter_loss: 0.09548262506723404
train_iter_loss: 0.14112013578414917
train_iter_loss: 0.16433337330818176
train_iter_loss: 0.21388454735279083
train_iter_loss: 0.08951571583747864
train_iter_loss: 0.13763685524463654
train_iter_loss: 0.19394567608833313
train_iter_loss: 0.07318639010190964
train_iter_loss: 0.0898151844739914
train_iter_loss: 0.23804610967636108
train_iter_loss: 0.12472397089004517
train_iter_loss: 0.267754465341568
train_iter_loss: 0.14674383401870728
train_iter_loss: 0.2150525003671646
train_iter_loss: 0.12283211946487427
train_iter_loss: 0.3278599977493286
train_iter_loss: 0.1459338366985321
train_iter_loss: 0.175613671541214
train_iter_loss: 0.1541254073381424
train_iter_loss: 0.12217739969491959
train_iter_loss: 0.09813103079795837
train_iter_loss: 0.03918666020035744
train_iter_loss: 0.40082478523254395
train_iter_loss: 0.2788585424423218
train_iter_loss: 0.3186386823654175
train_iter_loss: 0.1569502353668213
train_iter_loss: 0.07120183110237122
train_iter_loss: 0.07980769872665405
train_iter_loss: 0.09987282007932663
train_iter_loss: 0.1869259625673294
train_iter_loss: 0.16521073877811432
train_iter_loss: 0.057843443006277084
train_iter_loss: 0.07548338919878006
train_iter_loss: 0.04353504255414009
train_iter_loss: 0.10206124931573868
train_iter_loss: 0.14618343114852905
train_iter_loss: 0.1090797632932663
train_iter_loss: 0.2042650729417801
train_iter_loss: 0.21776843070983887
train_iter_loss: 0.10206769406795502
train_iter_loss: 0.12178844958543777
train_iter_loss: 0.020695991814136505
train_iter_loss: 0.270713746547699
train_iter_loss: 0.17497913539409637
train_iter_loss: 0.112406425178051
train_iter_loss: 0.19719663262367249
train_iter_loss: 0.1256871372461319
train_iter_loss: 0.1343410760164261
train_iter_loss: 0.27865836024284363
train_iter_loss: 0.17336469888687134
train_iter_loss: 0.07329805940389633
train_iter_loss: 0.16113024950027466
train_iter_loss: 0.12790584564208984
train_iter_loss: 0.12330766022205353
train_iter_loss: 0.19030573964118958
train loss :0.1583
---------------------
Validation seg loss: 0.21884160132129798 at epoch 701
epoch =    702/  1000, exp = train
train_iter_loss: 0.11047821491956711
train_iter_loss: 0.15636968612670898
train_iter_loss: 0.10511123389005661
train_iter_loss: 0.289357453584671
train_iter_loss: 0.0876249447464943
train_iter_loss: 0.1669817715883255
train_iter_loss: 0.2460002601146698
train_iter_loss: 0.19544176757335663
train_iter_loss: 0.08611243963241577
train_iter_loss: 0.13356059789657593
train_iter_loss: 0.11237531900405884
train_iter_loss: 0.1027296707034111
train_iter_loss: 0.19888311624526978
train_iter_loss: 0.2233313024044037
train_iter_loss: 0.18818062543869019
train_iter_loss: 0.11042642593383789
train_iter_loss: 0.21856233477592468
train_iter_loss: 0.18489223718643188
train_iter_loss: 0.13072344660758972
train_iter_loss: 0.13353751599788666
train_iter_loss: 0.17770831286907196
train_iter_loss: 0.08282742649316788
train_iter_loss: 0.14427368342876434
train_iter_loss: 0.15378668904304504
train_iter_loss: 0.17734360694885254
train_iter_loss: 0.10514476895332336
train_iter_loss: 0.10818710178136826
train_iter_loss: 0.14055860042572021
train_iter_loss: 0.09552649408578873
train_iter_loss: 0.3543091118335724
train_iter_loss: 0.11727112531661987
train_iter_loss: 0.15078134834766388
train_iter_loss: 0.14797614514827728
train_iter_loss: 0.11591965705156326
train_iter_loss: 0.1800578087568283
train_iter_loss: 0.18294976651668549
train_iter_loss: 0.23112359642982483
train_iter_loss: 0.08259136974811554
train_iter_loss: 0.2741915285587311
train_iter_loss: 0.11699675768613815
train_iter_loss: 0.16578581929206848
train_iter_loss: 0.1424264907836914
train_iter_loss: 0.200395405292511
train_iter_loss: 0.2662842571735382
train_iter_loss: 0.18735264241695404
train_iter_loss: 0.09103453904390335
train_iter_loss: 0.12036529928445816
train_iter_loss: 0.24083875119686127
train_iter_loss: 0.1575412154197693
train_iter_loss: 0.14436864852905273
train_iter_loss: 0.11224003881216049
train_iter_loss: 0.08424961566925049
train_iter_loss: 0.135857954621315
train_iter_loss: 0.17333029210567474
train_iter_loss: 0.1139940470457077
train_iter_loss: 0.12020069360733032
train_iter_loss: 0.3229160010814667
train_iter_loss: 0.17309878766536713
train_iter_loss: 0.19729404151439667
train_iter_loss: 0.23390530049800873
train_iter_loss: 0.18688908219337463
train_iter_loss: 0.15373575687408447
train_iter_loss: 0.12915191054344177
train_iter_loss: 0.15218080580234528
train_iter_loss: 0.07169974595308304
train_iter_loss: 0.1941623091697693
train_iter_loss: 0.15009911358356476
train_iter_loss: 0.12746666371822357
train_iter_loss: 0.09636207669973373
train_iter_loss: 0.2107781022787094
train_iter_loss: 0.24691247940063477
train_iter_loss: 0.10988511890172958
train_iter_loss: 0.09952476620674133
train_iter_loss: 0.18814478814601898
train_iter_loss: 0.24906869232654572
train_iter_loss: 0.08146809041500092
train_iter_loss: 0.23218683898448944
train_iter_loss: 0.11675462126731873
train_iter_loss: 0.19556300342082977
train_iter_loss: 0.16595859825611115
train_iter_loss: 0.08058130741119385
train_iter_loss: 0.15522339940071106
train_iter_loss: 0.03703901544213295
train_iter_loss: 0.10455594211816788
train_iter_loss: 0.07536573708057404
train_iter_loss: 0.1472579538822174
train_iter_loss: 0.3577478528022766
train_iter_loss: 0.10137293487787247
train_iter_loss: 0.1678071767091751
train_iter_loss: 0.11721034348011017
train_iter_loss: 0.2069913148880005
train_iter_loss: 0.140360027551651
train_iter_loss: 0.06342285871505737
train_iter_loss: 0.1591508388519287
train_iter_loss: 0.10017470270395279
train_iter_loss: 0.1202012225985527
train_iter_loss: 0.1369587630033493
train_iter_loss: 0.22349178791046143
train_iter_loss: 0.1829434186220169
train_iter_loss: 0.15521040558815002
train loss :0.1585
---------------------
Validation seg loss: 0.21428315690279287 at epoch 702
epoch =    703/  1000, exp = train
train_iter_loss: 0.07250311970710754
train_iter_loss: 0.08541977405548096
train_iter_loss: 0.2779944837093353
train_iter_loss: 0.12604345381259918
train_iter_loss: 0.19019737839698792
train_iter_loss: 0.29987776279449463
train_iter_loss: 0.21068555116653442
train_iter_loss: 0.18347066640853882
train_iter_loss: 0.2100706696510315
train_iter_loss: 0.22254908084869385
train_iter_loss: 0.05269463732838631
train_iter_loss: 0.11988134682178497
train_iter_loss: 0.2156648486852646
train_iter_loss: 0.14099010825157166
train_iter_loss: 0.11913996934890747
train_iter_loss: 0.23760075867176056
train_iter_loss: 0.16666994988918304
train_iter_loss: 0.09535707533359528
train_iter_loss: 0.13230279088020325
train_iter_loss: 0.18406763672828674
train_iter_loss: 0.16994696855545044
train_iter_loss: 0.23690186440944672
train_iter_loss: 0.1778290569782257
train_iter_loss: 0.14868955314159393
train_iter_loss: 0.12773650884628296
train_iter_loss: 0.18985794484615326
train_iter_loss: 0.13298259675502777
train_iter_loss: 0.13022004067897797
train_iter_loss: 0.10530880838632584
train_iter_loss: 0.17418678104877472
train_iter_loss: 0.15714505314826965
train_iter_loss: 0.05522531270980835
train_iter_loss: 0.1440274566411972
train_iter_loss: 0.15310069918632507
train_iter_loss: 0.20143325626850128
train_iter_loss: 0.10196568816900253
train_iter_loss: 0.04193348065018654
train_iter_loss: 0.2052270621061325
train_iter_loss: 0.2409733533859253
train_iter_loss: 0.1501641869544983
train_iter_loss: 0.33256199955940247
train_iter_loss: 0.017892824485898018
train_iter_loss: 0.2979106307029724
train_iter_loss: 0.11183162778615952
train_iter_loss: 0.17206457257270813
train_iter_loss: 0.07844746112823486
train_iter_loss: 0.31334686279296875
train_iter_loss: 0.2232002168893814
train_iter_loss: 0.12992948293685913
train_iter_loss: 0.16124272346496582
train_iter_loss: 0.1833462417125702
train_iter_loss: 0.0943310409784317
train_iter_loss: 0.0775958001613617
train_iter_loss: 0.12507937848567963
train_iter_loss: 0.11356624960899353
train_iter_loss: 0.16124911606311798
train_iter_loss: 0.16859430074691772
train_iter_loss: 0.13046546280384064
train_iter_loss: 0.17410196363925934
train_iter_loss: 0.12036760151386261
train_iter_loss: 0.1753881275653839
train_iter_loss: 0.21853992342948914
train_iter_loss: 0.16331729292869568
train_iter_loss: 0.1019439697265625
train_iter_loss: 0.16063041985034943
train_iter_loss: 0.07252431660890579
train_iter_loss: 0.2202567309141159
train_iter_loss: 0.20839916169643402
train_iter_loss: 0.15509895980358124
train_iter_loss: 0.1604078859090805
train_iter_loss: 0.12087748944759369
train_iter_loss: 0.14391644299030304
train_iter_loss: 0.17482106387615204
train_iter_loss: 0.11954758316278458
train_iter_loss: 0.08434171974658966
train_iter_loss: 0.13259662687778473
train_iter_loss: 0.14841127395629883
train_iter_loss: 0.08973756432533264
train_iter_loss: 0.14746205508708954
train_iter_loss: 0.1709621101617813
train_iter_loss: 0.18750017881393433
train_iter_loss: 0.13823509216308594
train_iter_loss: 0.15678870677947998
train_iter_loss: 0.083524189889431
train_iter_loss: 0.1136309951543808
train_iter_loss: 0.19090454280376434
train_iter_loss: 0.10288304835557938
train_iter_loss: 0.2492980808019638
train_iter_loss: 0.04569881036877632
train_iter_loss: 0.13654263317584991
train_iter_loss: 0.10315476357936859
train_iter_loss: 0.1702233999967575
train_iter_loss: 0.18336789309978485
train_iter_loss: 0.17045719921588898
train_iter_loss: 0.14575329422950745
train_iter_loss: 0.2091677337884903
train_iter_loss: 0.09670896828174591
train_iter_loss: 0.0939192920923233
train_iter_loss: 0.0950993075966835
train_iter_loss: 0.12285153567790985
train loss :0.1550
---------------------
Validation seg loss: 0.21716658491641283 at epoch 703
epoch =    704/  1000, exp = train
train_iter_loss: 0.09791186451911926
train_iter_loss: 0.12912490963935852
train_iter_loss: 0.15896660089492798
train_iter_loss: 0.2018081396818161
train_iter_loss: 0.15130434930324554
train_iter_loss: 0.09729191660881042
train_iter_loss: 0.29434946179389954
train_iter_loss: 0.09716369211673737
train_iter_loss: 0.15244457125663757
train_iter_loss: 0.15079842507839203
train_iter_loss: 0.26516425609588623
train_iter_loss: 0.07618189603090286
train_iter_loss: 0.12374955415725708
train_iter_loss: 0.27601975202560425
train_iter_loss: 0.14331184327602386
train_iter_loss: 0.1320808231830597
train_iter_loss: 0.11893071234226227
train_iter_loss: 0.11859896779060364
train_iter_loss: 0.17374174296855927
train_iter_loss: 0.13926954567432404
train_iter_loss: 0.1362583488225937
train_iter_loss: 0.21330784261226654
train_iter_loss: 0.2206113487482071
train_iter_loss: 0.12473529577255249
train_iter_loss: 0.1794700026512146
train_iter_loss: 0.08688666671514511
train_iter_loss: 0.11949458718299866
train_iter_loss: 0.12331527471542358
train_iter_loss: 0.29034942388534546
train_iter_loss: 0.2333879917860031
train_iter_loss: 0.1318875402212143
train_iter_loss: 0.16129912436008453
train_iter_loss: 0.20608824491500854
train_iter_loss: 0.10994607955217361
train_iter_loss: 0.11417053639888763
train_iter_loss: 0.10146461427211761
train_iter_loss: 0.3792470693588257
train_iter_loss: 0.11016012728214264
train_iter_loss: 0.1843055933713913
train_iter_loss: 0.08381638675928116
train_iter_loss: 0.17990641295909882
train_iter_loss: 0.03990916162729263
train_iter_loss: 0.12631453573703766
train_iter_loss: 0.035513415932655334
train_iter_loss: 0.12630914151668549
train_iter_loss: 0.05289342626929283
train_iter_loss: 0.07786912471055984
train_iter_loss: 0.15473970770835876
train_iter_loss: 0.2846524119377136
train_iter_loss: 0.08933363854885101
train_iter_loss: 0.06173160672187805
train_iter_loss: 0.1961040198802948
train_iter_loss: 0.19008521735668182
train_iter_loss: 0.08009061962366104
train_iter_loss: 0.05211826041340828
train_iter_loss: 0.21919097006320953
train_iter_loss: 0.24080322682857513
train_iter_loss: 0.052340518683195114
train_iter_loss: 0.34331008791923523
train_iter_loss: 0.08601678907871246
train_iter_loss: 0.10252011567354202
train_iter_loss: 0.11460179835557938
train_iter_loss: 0.08772771060466766
train_iter_loss: 0.23384392261505127
train_iter_loss: 0.17563576996326447
train_iter_loss: 0.18496891856193542
train_iter_loss: 0.34923431277275085
train_iter_loss: 0.22506193816661835
train_iter_loss: 0.1162424311041832
train_iter_loss: 0.14749504625797272
train_iter_loss: 0.06265931576490402
train_iter_loss: 0.15499725937843323
train_iter_loss: 0.16626249253749847
train_iter_loss: 0.0908866599202156
train_iter_loss: 0.18293045461177826
train_iter_loss: 0.17794792354106903
train_iter_loss: 0.15472328662872314
train_iter_loss: 0.0841517448425293
train_iter_loss: 0.09627068042755127
train_iter_loss: 0.10982397943735123
train_iter_loss: 0.12982909381389618
train_iter_loss: 0.2047959417104721
train_iter_loss: 0.24917007982730865
train_iter_loss: 0.10305174440145493
train_iter_loss: 0.12602044641971588
train_iter_loss: 0.11452091485261917
train_iter_loss: 0.12007669359445572
train_iter_loss: 0.23784610629081726
train_iter_loss: 0.2566716969013214
train_iter_loss: 0.11865273863077164
train_iter_loss: 0.0925145223736763
train_iter_loss: 0.20026731491088867
train_iter_loss: 0.21146522462368011
train_iter_loss: 0.14798732101917267
train_iter_loss: 0.1477956473827362
train_iter_loss: 0.11894791573286057
train_iter_loss: 0.08766618371009827
train_iter_loss: 0.10134977847337723
train_iter_loss: 0.2051730453968048
train_iter_loss: 0.15249815583229065
train loss :0.1540
---------------------
Validation seg loss: 0.21825718865642008 at epoch 704
epoch =    705/  1000, exp = train
train_iter_loss: 0.14507180452346802
train_iter_loss: 0.1895318329334259
train_iter_loss: 0.154448464512825
train_iter_loss: 0.21507558226585388
train_iter_loss: 0.31384462118148804
train_iter_loss: 0.30350616574287415
train_iter_loss: 0.14060497283935547
train_iter_loss: 0.1200336217880249
train_iter_loss: 0.13897369801998138
train_iter_loss: 0.23873209953308105
train_iter_loss: 0.1455429345369339
train_iter_loss: 0.18320970237255096
train_iter_loss: 0.18240141868591309
train_iter_loss: 0.17211271822452545
train_iter_loss: 0.2969864308834076
train_iter_loss: 0.1829075813293457
train_iter_loss: 0.12366191297769547
train_iter_loss: 0.2140064388513565
train_iter_loss: 0.13713069260120392
train_iter_loss: 0.10340164601802826
train_iter_loss: 0.09773938357830048
train_iter_loss: 0.08536214381456375
train_iter_loss: 0.14241863787174225
train_iter_loss: 0.1019042432308197
train_iter_loss: 0.0569838210940361
train_iter_loss: 0.21127015352249146
train_iter_loss: 0.12040188908576965
train_iter_loss: 0.1626504808664322
train_iter_loss: 0.08714758604764938
train_iter_loss: 0.1045304536819458
train_iter_loss: 0.12048526853322983
train_iter_loss: 0.04570690169930458
train_iter_loss: 0.23186363279819489
train_iter_loss: 0.20121830701828003
train_iter_loss: 0.1647733747959137
train_iter_loss: 0.09005364775657654
train_iter_loss: 0.18652471899986267
train_iter_loss: 0.252645879983902
train_iter_loss: 0.09869712591171265
train_iter_loss: 0.24358464777469635
train_iter_loss: 0.2275472730398178
train_iter_loss: 0.16052544116973877
train_iter_loss: 0.12927410006523132
train_iter_loss: 0.40460535883903503
train_iter_loss: 0.13936379551887512
train_iter_loss: 0.15453794598579407
train_iter_loss: 0.09323704242706299
train_iter_loss: 0.09857770800590515
train_iter_loss: 0.15668052434921265
train_iter_loss: 0.1523815393447876
train_iter_loss: 0.17204101383686066
train_iter_loss: 0.16614210605621338
train_iter_loss: 0.16570067405700684
train_iter_loss: 0.15771909058094025
train_iter_loss: 0.14400260150432587
train_iter_loss: 0.20541399717330933
train_iter_loss: 0.1260436773300171
train_iter_loss: 0.20647652447223663
train_iter_loss: 0.04278026893734932
train_iter_loss: 0.24527885019779205
train_iter_loss: 0.1286754608154297
train_iter_loss: 0.12947392463684082
train_iter_loss: 0.1082751676440239
train_iter_loss: 0.09828203171491623
train_iter_loss: 0.2003687024116516
train_iter_loss: 0.1565370112657547
train_iter_loss: 0.08163675665855408
train_iter_loss: 0.27218344807624817
train_iter_loss: 0.19250869750976562
train_iter_loss: 0.158187597990036
train_iter_loss: 0.184156134724617
train_iter_loss: 0.15170948207378387
train_iter_loss: 0.1517775058746338
train_iter_loss: 0.20609921216964722
train_iter_loss: 0.20111525058746338
train_iter_loss: 0.14708800613880157
train_iter_loss: 0.14325061440467834
train_iter_loss: 0.08317822962999344
train_iter_loss: 0.20285020768642426
train_iter_loss: 0.14457282423973083
train_iter_loss: 0.19185326993465424
train_iter_loss: 0.1274862140417099
train_iter_loss: 0.1303178071975708
train_iter_loss: 0.20801179111003876
train_iter_loss: 0.2307630032300949
train_iter_loss: 0.04168566316366196
train_iter_loss: 0.22934995591640472
train_iter_loss: 0.07781998068094254
train_iter_loss: 0.09431511163711548
train_iter_loss: 0.19597408175468445
train_iter_loss: 0.17828026413917542
train_iter_loss: 0.20964522659778595
train_iter_loss: 0.11978580802679062
train_iter_loss: 0.14929881691932678
train_iter_loss: 0.20444883406162262
train_iter_loss: 0.19606149196624756
train_iter_loss: 0.16837860643863678
train_iter_loss: 0.10668785125017166
train_iter_loss: 0.16252078115940094
train_iter_loss: 0.12993508577346802
train loss :0.1630
---------------------
Validation seg loss: 0.21587086091134347 at epoch 705
epoch =    706/  1000, exp = train
train_iter_loss: 0.19496312737464905
train_iter_loss: 0.038243964314460754
train_iter_loss: 0.18542680144309998
train_iter_loss: 0.19881980121135712
train_iter_loss: 0.1436944603919983
train_iter_loss: 0.16937685012817383
train_iter_loss: 0.26489323377609253
train_iter_loss: 0.20353184640407562
train_iter_loss: 0.09531490504741669
train_iter_loss: 0.18509213626384735
train_iter_loss: 0.06824523210525513
train_iter_loss: 0.13668905198574066
train_iter_loss: 0.15133962035179138
train_iter_loss: 0.11933878064155579
train_iter_loss: 0.20132653415203094
train_iter_loss: 0.18125581741333008
train_iter_loss: 0.18222743272781372
train_iter_loss: 0.10127396881580353
train_iter_loss: 0.17094194889068604
train_iter_loss: 0.12288576364517212
train_iter_loss: 0.10688213258981705
train_iter_loss: 0.34756115078926086
train_iter_loss: 0.12580114603042603
train_iter_loss: 0.15566879510879517
train_iter_loss: 0.28180384635925293
train_iter_loss: 0.15051934123039246
train_iter_loss: 0.14024992287158966
train_iter_loss: 0.05892086401581764
train_iter_loss: 0.17089025676250458
train_iter_loss: 0.16829250752925873
train_iter_loss: 0.11152984201908112
train_iter_loss: 0.07989644259214401
train_iter_loss: 0.1108672246336937
train_iter_loss: 0.14044509828090668
train_iter_loss: 0.09856102615594864
train_iter_loss: 0.15514002740383148
train_iter_loss: 0.059470031410455704
train_iter_loss: 0.16851550340652466
train_iter_loss: 0.06447406113147736
train_iter_loss: 0.18551670014858246
train_iter_loss: 0.12586550414562225
train_iter_loss: 0.1291094869375229
train_iter_loss: 0.09266737848520279
train_iter_loss: 0.214642733335495
train_iter_loss: 0.13673187792301178
train_iter_loss: 0.18308141827583313
train_iter_loss: 0.1471528857946396
train_iter_loss: 0.13111130893230438
train_iter_loss: 0.10950332134962082
train_iter_loss: 0.138088658452034
train_iter_loss: 0.14071796834468842
train_iter_loss: 0.15038494765758514
train_iter_loss: 0.21035829186439514
train_iter_loss: 0.15640297532081604
train_iter_loss: 0.09136497229337692
train_iter_loss: 0.11282879114151001
train_iter_loss: 0.26552483439445496
train_iter_loss: 0.18002165853977203
train_iter_loss: 0.1331729292869568
train_iter_loss: 0.21520493924617767
train_iter_loss: 0.10974036902189255
train_iter_loss: 0.23691751062870026
train_iter_loss: 0.15043359994888306
train_iter_loss: 0.08835924416780472
train_iter_loss: 0.13685071468353271
train_iter_loss: 0.31353431940078735
train_iter_loss: 0.17886358499526978
train_iter_loss: 0.15070629119873047
train_iter_loss: 0.14494869112968445
train_iter_loss: 0.16461318731307983
train_iter_loss: 0.19430799782276154
train_iter_loss: 0.17874324321746826
train_iter_loss: 0.12718580663204193
train_iter_loss: 0.2727561295032501
train_iter_loss: 0.17727866768836975
train_iter_loss: 0.27169570326805115
train_iter_loss: 0.12649622559547424
train_iter_loss: 0.20263491570949554
train_iter_loss: 0.1914975792169571
train_iter_loss: 0.07778208702802658
train_iter_loss: 0.08950496464967728
train_iter_loss: 0.10203928500413895
train_iter_loss: 0.1437733918428421
train_iter_loss: 0.11931827664375305
train_iter_loss: 0.05579162761569023
train_iter_loss: 0.206085667014122
train_iter_loss: 0.24049146473407745
train_iter_loss: 0.1404160112142563
train_iter_loss: 0.11377764493227005
train_iter_loss: 0.14560295641422272
train_iter_loss: 0.1688854992389679
train_iter_loss: 0.06003988906741142
train_iter_loss: 0.16234458982944489
train_iter_loss: 0.13099326193332672
train_iter_loss: 0.1399707943201065
train_iter_loss: 0.16815011203289032
train_iter_loss: 0.10618875920772552
train_iter_loss: 0.1291058212518692
train_iter_loss: 0.09049922972917557
train_iter_loss: 0.13651463389396667
train loss :0.1537
---------------------
Validation seg loss: 0.21490499490590873 at epoch 706
epoch =    707/  1000, exp = train
train_iter_loss: 0.19718162715435028
train_iter_loss: 0.21688033640384674
train_iter_loss: 0.17741715908050537
train_iter_loss: 0.19740664958953857
train_iter_loss: 0.2090439349412918
train_iter_loss: 0.24919641017913818
train_iter_loss: 0.1681438386440277
train_iter_loss: 0.08509557694196701
train_iter_loss: 0.15606406331062317
train_iter_loss: 0.19111725687980652
train_iter_loss: 0.22937507927417755
train_iter_loss: 0.09178824722766876
train_iter_loss: 0.05671892315149307
train_iter_loss: 0.16719649732112885
train_iter_loss: 0.12002527713775635
train_iter_loss: 0.24652142822742462
train_iter_loss: 0.14203450083732605
train_iter_loss: 0.20221468806266785
train_iter_loss: 0.13941779732704163
train_iter_loss: 0.021981067955493927
train_iter_loss: 0.14815698564052582
train_iter_loss: 0.15233387053012848
train_iter_loss: 0.10654562711715698
train_iter_loss: 0.16758522391319275
train_iter_loss: 0.15604838728904724
train_iter_loss: 0.12490513920783997
train_iter_loss: 0.13496942818164825
train_iter_loss: 0.4150892496109009
train_iter_loss: 0.1847148835659027
train_iter_loss: 0.11985529214143753
train_iter_loss: 0.11280763894319534
train_iter_loss: 0.13503390550613403
train_iter_loss: 0.1374635100364685
train_iter_loss: 0.19391271471977234
train_iter_loss: 0.1298706978559494
train_iter_loss: 0.09763093292713165
train_iter_loss: 0.264245867729187
train_iter_loss: 0.23514853417873383
train_iter_loss: 0.12719039618968964
train_iter_loss: 0.16689155995845795
train_iter_loss: 0.06656447798013687
train_iter_loss: 0.11346577852964401
train_iter_loss: 0.10531549155712128
train_iter_loss: 0.18645739555358887
train_iter_loss: 0.38201233744621277
train_iter_loss: 0.16546709835529327
train_iter_loss: 0.2703576982021332
train_iter_loss: 0.12956008315086365
train_iter_loss: 0.15059688687324524
train_iter_loss: 0.07307178527116776
train_iter_loss: 0.2357012778520584
train_iter_loss: 0.06899189949035645
train_iter_loss: 0.12369970232248306
train_iter_loss: 0.07801506668329239
train_iter_loss: 0.20125465095043182
train_iter_loss: 0.10938271880149841
train_iter_loss: 0.10537316650152206
train_iter_loss: 0.07457264512777328
train_iter_loss: 0.19974611699581146
train_iter_loss: 0.20902936160564423
train_iter_loss: 0.07451332360506058
train_iter_loss: 0.1887592375278473
train_iter_loss: 0.2772594094276428
train_iter_loss: 0.1831674575805664
train_iter_loss: 0.19323354959487915
train_iter_loss: 0.14819103479385376
train_iter_loss: 0.11008360981941223
train_iter_loss: 0.07885842770338058
train_iter_loss: 0.1392548382282257
train_iter_loss: 0.09436777979135513
train_iter_loss: 0.16605094075202942
train_iter_loss: 0.23436550796031952
train_iter_loss: 0.11893539875745773
train_iter_loss: 0.12984049320220947
train_iter_loss: 0.21037937700748444
train_iter_loss: 0.13236355781555176
train_iter_loss: 0.2149401158094406
train_iter_loss: 0.12732462584972382
train_iter_loss: 0.1089465320110321
train_iter_loss: 0.1510886251926422
train_iter_loss: 0.1945899873971939
train_iter_loss: 0.10762261599302292
train_iter_loss: 0.2051190882921219
train_iter_loss: 0.2101120799779892
train_iter_loss: 0.14924123883247375
train_iter_loss: 0.20500808954238892
train_iter_loss: 0.11523423343896866
train_iter_loss: 0.23047694563865662
train_iter_loss: 0.08368638902902603
train_iter_loss: 0.091600701212883
train_iter_loss: 0.14243897795677185
train_iter_loss: 0.10439711809158325
train_iter_loss: 0.10468465834856033
train_iter_loss: 0.15364989638328552
train_iter_loss: 0.35050731897354126
train_iter_loss: 0.12242822349071503
train_iter_loss: 0.13053348660469055
train_iter_loss: 0.22667984664440155
train_iter_loss: 0.19168397784233093
train_iter_loss: 0.20684221386909485
train loss :0.1619
---------------------
Validation seg loss: 0.21725988414419709 at epoch 707
epoch =    708/  1000, exp = train
train_iter_loss: 0.07590804249048233
train_iter_loss: 0.21156664192676544
train_iter_loss: 0.16125887632369995
train_iter_loss: 0.08605506271123886
train_iter_loss: 0.13967271149158478
train_iter_loss: 0.15682172775268555
train_iter_loss: 0.131814643740654
train_iter_loss: 0.1502964198589325
train_iter_loss: 0.22114728391170502
train_iter_loss: 0.1814119815826416
train_iter_loss: 0.15157227218151093
train_iter_loss: 0.12664347887039185
train_iter_loss: 0.14761625230312347
train_iter_loss: 0.023503975942730904
train_iter_loss: 0.11805248260498047
train_iter_loss: 0.07643958926200867
train_iter_loss: 0.11340419203042984
train_iter_loss: 0.03721094876527786
train_iter_loss: 0.12969517707824707
train_iter_loss: 0.09904485195875168
train_iter_loss: 0.1871645450592041
train_iter_loss: 0.09696610271930695
train_iter_loss: 0.12597331404685974
train_iter_loss: 0.2650390863418579
train_iter_loss: 0.09608291834592819
train_iter_loss: 0.2455475628376007
train_iter_loss: 0.05672605335712433
train_iter_loss: 0.11856000870466232
train_iter_loss: 0.16488127410411835
train_iter_loss: 0.1046682596206665
train_iter_loss: 0.22190137207508087
train_iter_loss: 0.14850471913814545
train_iter_loss: 0.18080176413059235
train_iter_loss: 0.14746280014514923
train_iter_loss: 0.17714302241802216
train_iter_loss: 0.19653189182281494
train_iter_loss: 0.22649234533309937
train_iter_loss: 0.1877196729183197
train_iter_loss: 0.18509308993816376
train_iter_loss: 0.22270898520946503
train_iter_loss: 0.093153215944767
train_iter_loss: 0.1644940823316574
train_iter_loss: 0.28462129831314087
train_iter_loss: 0.14115369319915771
train_iter_loss: 0.08604084700345993
train_iter_loss: 0.18024373054504395
train_iter_loss: 0.15449431538581848
train_iter_loss: 0.10936751216650009
train_iter_loss: 0.12258229404687881
train_iter_loss: 0.1137862280011177
train_iter_loss: 0.1477477103471756
train_iter_loss: 0.08398573100566864
train_iter_loss: 0.1528174877166748
train_iter_loss: 0.3314252495765686
train_iter_loss: 0.24879240989685059
train_iter_loss: 0.099140465259552
train_iter_loss: 0.1460045427083969
train_iter_loss: 0.1273525506258011
train_iter_loss: 0.09746518731117249
train_iter_loss: 0.1446397751569748
train_iter_loss: 0.21331249177455902
train_iter_loss: 0.1337731033563614
train_iter_loss: 0.1314588338136673
train_iter_loss: 0.22805769741535187
train_iter_loss: 0.15746839344501495
train_iter_loss: 0.2530631721019745
train_iter_loss: 0.12903112173080444
train_iter_loss: 0.3060126006603241
train_iter_loss: 0.1218283548951149
train_iter_loss: 0.08459517359733582
train_iter_loss: 0.21082161366939545
train_iter_loss: 0.1500415802001953
train_iter_loss: 0.1387689858675003
train_iter_loss: 0.18507401645183563
train_iter_loss: 0.1294727623462677
train_iter_loss: 0.37297511100769043
train_iter_loss: 0.09866300225257874
train_iter_loss: 0.08453469723463058
train_iter_loss: 0.1151367723941803
train_iter_loss: 0.179064080119133
train_iter_loss: 0.2578229606151581
train_iter_loss: 0.19416143000125885
train_iter_loss: 0.15337689220905304
train_iter_loss: 0.08517149090766907
train_iter_loss: 0.06273500621318817
train_iter_loss: 0.2633059024810791
train_iter_loss: 0.1147172749042511
train_iter_loss: 0.11502713710069656
train_iter_loss: 0.20487180352210999
train_iter_loss: 0.1084546446800232
train_iter_loss: 0.1661660373210907
train_iter_loss: 0.20688796043395996
train_iter_loss: 0.10085979104042053
train_iter_loss: 0.12275896966457367
train_iter_loss: 0.125250905752182
train_iter_loss: 0.12078097462654114
train_iter_loss: 0.07920491695404053
train_iter_loss: 0.15646147727966309
train_iter_loss: 0.0628209039568901
train_iter_loss: 0.09254468977451324
train loss :0.1530
---------------------
Validation seg loss: 0.21273643295895658 at epoch 708
epoch =    709/  1000, exp = train
train_iter_loss: 0.10974790155887604
train_iter_loss: 0.0883045643568039
train_iter_loss: 0.10641802102327347
train_iter_loss: 0.24266858398914337
train_iter_loss: 0.24549444019794464
train_iter_loss: 0.07363488525152206
train_iter_loss: 0.18075640499591827
train_iter_loss: 0.1316392719745636
train_iter_loss: 0.13962724804878235
train_iter_loss: 0.20875048637390137
train_iter_loss: 0.11928966641426086
train_iter_loss: 0.08316751569509506
train_iter_loss: 0.19769860804080963
train_iter_loss: 0.10112548619508743
train_iter_loss: 0.05448191612958908
train_iter_loss: 0.14132185280323029
train_iter_loss: 0.088199682533741
train_iter_loss: 0.2132544219493866
train_iter_loss: 0.22974054515361786
train_iter_loss: 0.11084697395563126
train_iter_loss: 0.2984957993030548
train_iter_loss: 0.18482902646064758
train_iter_loss: 0.12916868925094604
train_iter_loss: 0.2955538332462311
train_iter_loss: 0.23648697137832642
train_iter_loss: 0.1358213871717453
train_iter_loss: 0.18146444857120514
train_iter_loss: 0.1534220427274704
train_iter_loss: 0.2552747428417206
train_iter_loss: 0.14845982193946838
train_iter_loss: 0.15800848603248596
train_iter_loss: 0.12921889126300812
train_iter_loss: 0.12568782269954681
train_iter_loss: 0.18577708303928375
train_iter_loss: 0.15630556643009186
train_iter_loss: 0.10486587136983871
train_iter_loss: 0.15389162302017212
train_iter_loss: 0.18270480632781982
train_iter_loss: 0.12325561791658401
train_iter_loss: 0.1523127555847168
train_iter_loss: 0.11470989882946014
train_iter_loss: 0.116813063621521
train_iter_loss: 0.08851741999387741
train_iter_loss: 0.14485065639019012
train_iter_loss: 0.1410050392150879
train_iter_loss: 0.14210154116153717
train_iter_loss: 0.06306503713130951
train_iter_loss: 0.15334881842136383
train_iter_loss: 0.18039321899414062
train_iter_loss: 0.11231967061758041
train_iter_loss: 0.06123547628521919
train_iter_loss: 0.05693725124001503
train_iter_loss: 0.2653208076953888
train_iter_loss: 0.275623083114624
train_iter_loss: 0.1503109484910965
train_iter_loss: 0.17155294120311737
train_iter_loss: 0.1267509013414383
train_iter_loss: 0.20616987347602844
train_iter_loss: 0.2679368257522583
train_iter_loss: 0.17335467040538788
train_iter_loss: 0.12956401705741882
train_iter_loss: 0.09183088690042496
train_iter_loss: 0.06976321339607239
train_iter_loss: 0.12325652688741684
train_iter_loss: 0.11634282022714615
train_iter_loss: 0.30999448895454407
train_iter_loss: 0.23051728308200836
train_iter_loss: 0.16258370876312256
train_iter_loss: 0.12284494936466217
train_iter_loss: 0.146986722946167
train_iter_loss: 0.10991407185792923
train_iter_loss: 0.14984671771526337
train_iter_loss: 0.20201008021831512
train_iter_loss: 0.23444686830043793
train_iter_loss: 0.05058363825082779
train_iter_loss: 0.14090986549854279
train_iter_loss: 0.12955963611602783
train_iter_loss: 0.13711729645729065
train_iter_loss: 0.08582089096307755
train_iter_loss: 0.3125401735305786
train_iter_loss: 0.10796749591827393
train_iter_loss: 0.29963862895965576
train_iter_loss: 0.11721587926149368
train_iter_loss: 0.2759433388710022
train_iter_loss: 0.08655253052711487
train_iter_loss: 0.2325000911951065
train_iter_loss: 0.12493980675935745
train_iter_loss: 0.10699822753667831
train_iter_loss: 0.2855263948440552
train_iter_loss: 0.16704422235488892
train_iter_loss: 0.15262794494628906
train_iter_loss: 0.19660146534442902
train_iter_loss: 0.17987868189811707
train_iter_loss: 0.16456757485866547
train_iter_loss: 0.14403553307056427
train_iter_loss: 0.0718660056591034
train_iter_loss: 0.13279801607131958
train_iter_loss: 0.18914280831813812
train_iter_loss: 0.18715547025203705
train_iter_loss: 0.21728134155273438
train loss :0.1602
---------------------
Validation seg loss: 0.2159830629825592 at epoch 709
epoch =    710/  1000, exp = train
train_iter_loss: 0.09370359778404236
train_iter_loss: 0.08464284986257553
train_iter_loss: 0.1542549431324005
train_iter_loss: 0.16588880121707916
train_iter_loss: 0.07290766388177872
train_iter_loss: 0.11078303307294846
train_iter_loss: 0.05735636129975319
train_iter_loss: 0.12328978627920151
train_iter_loss: 0.2066260278224945
train_iter_loss: 0.06502319127321243
train_iter_loss: 0.10652228444814682
train_iter_loss: 0.12888500094413757
train_iter_loss: 0.199140727519989
train_iter_loss: 0.2002498209476471
train_iter_loss: 0.08762248605489731
train_iter_loss: 0.18842177093029022
train_iter_loss: 0.14814278483390808
train_iter_loss: 0.1318216472864151
train_iter_loss: 0.11308640241622925
train_iter_loss: 0.17171762883663177
train_iter_loss: 0.0846727043390274
train_iter_loss: 0.06871303170919418
train_iter_loss: 0.14456407725811005
train_iter_loss: 0.13728979229927063
train_iter_loss: 0.08331503719091415
train_iter_loss: 0.09990130364894867
train_iter_loss: 0.1011257991194725
train_iter_loss: 0.2653231620788574
train_iter_loss: 0.24460914731025696
train_iter_loss: 0.1267114281654358
train_iter_loss: 0.1303740292787552
train_iter_loss: 0.1582249253988266
train_iter_loss: 0.08992186188697815
train_iter_loss: 0.22630837559700012
train_iter_loss: 0.15213125944137573
train_iter_loss: 0.17014168202877045
train_iter_loss: 0.17804738879203796
train_iter_loss: 0.0844227522611618
train_iter_loss: 0.43489402532577515
train_iter_loss: 0.19515511393547058
train_iter_loss: 0.09920217841863632
train_iter_loss: 0.10934782773256302
train_iter_loss: 0.12202728539705276
train_iter_loss: 0.12048543244600296
train_iter_loss: 0.10024339705705643
train_iter_loss: 0.19898204505443573
train_iter_loss: 0.19582900404930115
train_iter_loss: 0.11883166432380676
train_iter_loss: 0.26622283458709717
train_iter_loss: 0.19376379251480103
train_iter_loss: 0.19404809176921844
train_iter_loss: 0.10452946275472641
train_iter_loss: 0.16744942963123322
train_iter_loss: 0.16191963851451874
train_iter_loss: 0.1387428343296051
train_iter_loss: 0.21059028804302216
train_iter_loss: 0.14465264976024628
train_iter_loss: 0.12319499999284744
train_iter_loss: 0.17521587014198303
train_iter_loss: 0.15798799693584442
train_iter_loss: 0.1196984276175499
train_iter_loss: 0.17737925052642822
train_iter_loss: 0.25515860319137573
train_iter_loss: 0.09994836151599884
train_iter_loss: 0.15179358422756195
train_iter_loss: 0.07201416045427322
train_iter_loss: 0.14640848338603973
train_iter_loss: 0.1254308670759201
train_iter_loss: 0.1702362596988678
train_iter_loss: 0.2531985342502594
train_iter_loss: 0.18827207386493683
train_iter_loss: 0.2566802501678467
train_iter_loss: 0.19591039419174194
train_iter_loss: 0.11578772217035294
train_iter_loss: 0.11819114536046982
train_iter_loss: 0.1862635314464569
train_iter_loss: 0.11866016685962677
train_iter_loss: 0.13135182857513428
train_iter_loss: 0.1629226803779602
train_iter_loss: 0.14035438001155853
train_iter_loss: 0.050152357667684555
train_iter_loss: 0.15882287919521332
train_iter_loss: 0.26054292917251587
train_iter_loss: 0.13283437490463257
train_iter_loss: 0.20442990958690643
train_iter_loss: 0.17631076276302338
train_iter_loss: 0.10552974045276642
train_iter_loss: 0.1923585832118988
train_iter_loss: 0.17307305335998535
train_iter_loss: 0.19839085638523102
train_iter_loss: 0.0872921571135521
train_iter_loss: 0.1858435869216919
train_iter_loss: 0.26597338914871216
train_iter_loss: 0.06678096204996109
train_iter_loss: 0.14362859725952148
train_iter_loss: 0.12817128002643585
train_iter_loss: 0.34484103322029114
train_iter_loss: 0.20262017846107483
train_iter_loss: 0.07390653342008591
train_iter_loss: 0.06495336443185806
train loss :0.1545
---------------------
Validation seg loss: 0.21921767789940788 at epoch 710
epoch =    711/  1000, exp = train
train_iter_loss: 0.26914891600608826
train_iter_loss: 0.08280077576637268
train_iter_loss: 0.09754044562578201
train_iter_loss: 0.1790391355752945
train_iter_loss: 0.15338952839374542
train_iter_loss: 0.10594668239355087
train_iter_loss: 0.15558066964149475
train_iter_loss: 0.15336129069328308
train_iter_loss: 0.14491179585456848
train_iter_loss: 0.1648372858762741
train_iter_loss: 0.3624494671821594
train_iter_loss: 0.06677499413490295
train_iter_loss: 0.11449474841356277
train_iter_loss: 0.06716097891330719
train_iter_loss: 0.13323140144348145
train_iter_loss: 0.04703400656580925
train_iter_loss: 0.08680279552936554
train_iter_loss: 0.16070082783699036
train_iter_loss: 0.15252158045768738
train_iter_loss: 0.11711222678422928
train_iter_loss: 0.12136543542146683
train_iter_loss: 0.2835652828216553
train_iter_loss: 0.19812986254692078
train_iter_loss: 0.18866829574108124
train_iter_loss: 0.26461753249168396
train_iter_loss: 0.0980910211801529
train_iter_loss: 0.2283860445022583
train_iter_loss: 0.17612624168395996
train_iter_loss: 0.07118626683950424
train_iter_loss: 0.1434619426727295
train_iter_loss: 0.15651243925094604
train_iter_loss: 0.11359499394893646
train_iter_loss: 0.12380778044462204
train_iter_loss: 0.07519815117120743
train_iter_loss: 0.11498457938432693
train_iter_loss: 0.16049230098724365
train_iter_loss: 0.1367083191871643
train_iter_loss: 0.15553729236125946
train_iter_loss: 0.09535138309001923
train_iter_loss: 0.2104075402021408
train_iter_loss: 0.1934286653995514
train_iter_loss: 0.17749705910682678
train_iter_loss: 0.11004742234945297
train_iter_loss: 0.30994197726249695
train_iter_loss: 0.19333431124687195
train_iter_loss: 0.21477776765823364
train_iter_loss: 0.15381941199302673
train_iter_loss: 0.18919417262077332
train_iter_loss: 0.20518717169761658
train_iter_loss: 0.19811366498470306
train_iter_loss: 0.0750277191400528
train_iter_loss: 0.12836720049381256
train_iter_loss: 0.09154251962900162
train_iter_loss: 0.16065053641796112
train_iter_loss: 0.14345881342887878
train_iter_loss: 0.11468470841646194
train_iter_loss: 0.134518563747406
train_iter_loss: 0.14145256578922272
train_iter_loss: 0.1440284699201584
train_iter_loss: 0.11494507640600204
train_iter_loss: 0.28794965147972107
train_iter_loss: 0.22643956542015076
train_iter_loss: 0.22703775763511658
train_iter_loss: 0.08277295529842377
train_iter_loss: 0.16305436193943024
train_iter_loss: 0.17733968794345856
train_iter_loss: 0.20243827998638153
train_iter_loss: 0.19214241206645966
train_iter_loss: 0.10889655351638794
train_iter_loss: 0.0969005599617958
train_iter_loss: 0.17082345485687256
train_iter_loss: 0.10475266724824905
train_iter_loss: 0.08928106725215912
train_iter_loss: 0.1000136062502861
train_iter_loss: 0.11225248873233795
train_iter_loss: 0.27662256360054016
train_iter_loss: 0.09088163822889328
train_iter_loss: 0.24694642424583435
train_iter_loss: 0.23447397351264954
train_iter_loss: 0.0771593451499939
train_iter_loss: 0.042489226907491684
train_iter_loss: 0.1459490805864334
train_iter_loss: 0.10834912210702896
train_iter_loss: 0.2346981167793274
train_iter_loss: 0.10861114412546158
train_iter_loss: 0.2145898938179016
train_iter_loss: 0.18284814059734344
train_iter_loss: 0.12683744728565216
train_iter_loss: 0.1644066721200943
train_iter_loss: 0.29438281059265137
train_iter_loss: 0.09269239753484726
train_iter_loss: 0.1385621726512909
train_iter_loss: 0.1884167343378067
train_iter_loss: 0.09447091072797775
train_iter_loss: 0.10036586225032806
train_iter_loss: 0.12251225113868713
train_iter_loss: 0.12700290977954865
train_iter_loss: 0.11961590498685837
train_iter_loss: 0.10189447551965714
train_iter_loss: 0.1685899794101715
train loss :0.1543
---------------------
Validation seg loss: 0.2151323089657246 at epoch 711
epoch =    712/  1000, exp = train
train_iter_loss: 0.2503277063369751
train_iter_loss: 0.14017736911773682
train_iter_loss: 0.3767288327217102
train_iter_loss: 0.15079781413078308
train_iter_loss: 0.15163148939609528
train_iter_loss: 0.08381111174821854
train_iter_loss: 0.14610502123832703
train_iter_loss: 0.24036365747451782
train_iter_loss: 0.07013427466154099
train_iter_loss: 0.10076528787612915
train_iter_loss: 0.15610042214393616
train_iter_loss: 0.1295912265777588
train_iter_loss: 0.1411939561367035
train_iter_loss: 0.16202257573604584
train_iter_loss: 0.1113012507557869
train_iter_loss: 0.1390799582004547
train_iter_loss: 0.04943448305130005
train_iter_loss: 0.13532432913780212
train_iter_loss: 0.10082176327705383
train_iter_loss: 0.1492668092250824
train_iter_loss: 0.185644268989563
train_iter_loss: 0.14366403222084045
train_iter_loss: 0.2220638245344162
train_iter_loss: 0.12578146159648895
train_iter_loss: 0.32033997774124146
train_iter_loss: 0.16614492237567902
train_iter_loss: 0.22089408338069916
train_iter_loss: 0.0720398873090744
train_iter_loss: 0.07000670582056046
train_iter_loss: 0.16803479194641113
train_iter_loss: 0.07009609043598175
train_iter_loss: 0.12345098704099655
train_iter_loss: 0.16156959533691406
train_iter_loss: 0.1332424283027649
train_iter_loss: 0.11509323865175247
train_iter_loss: 0.12329376488924026
train_iter_loss: 0.20068219304084778
train_iter_loss: 0.13287542760372162
train_iter_loss: 0.3874965012073517
train_iter_loss: 0.07214606553316116
train_iter_loss: 0.0879918709397316
train_iter_loss: 0.14158578217029572
train_iter_loss: 0.21491724252700806
train_iter_loss: 0.0663696750998497
train_iter_loss: 0.12770934402942657
train_iter_loss: 0.11164158582687378
train_iter_loss: 0.09646393358707428
train_iter_loss: 0.17952358722686768
train_iter_loss: 0.07112523913383484
train_iter_loss: 0.1317860335111618
train_iter_loss: 0.1302022486925125
train_iter_loss: 0.23630006611347198
train_iter_loss: 0.11271599680185318
train_iter_loss: 0.0789288878440857
train_iter_loss: 0.19852334260940552
train_iter_loss: 0.08158580958843231
train_iter_loss: 0.17858164012432098
train_iter_loss: 0.20991575717926025
train_iter_loss: 0.2195233702659607
train_iter_loss: 0.1461995393037796
train_iter_loss: 0.1331259161233902
train_iter_loss: 0.17785561084747314
train_iter_loss: 0.0880710557103157
train_iter_loss: 0.16646838188171387
train_iter_loss: 0.22174230217933655
train_iter_loss: 0.08477544784545898
train_iter_loss: 0.15946079790592194
train_iter_loss: 0.19073113799095154
train_iter_loss: 0.06660469621419907
train_iter_loss: 0.3127620220184326
train_iter_loss: 0.13510355353355408
train_iter_loss: 0.1877722442150116
train_iter_loss: 0.09763681888580322
train_iter_loss: 0.17032162845134735
train_iter_loss: 0.20988453924655914
train_iter_loss: 0.2069435864686966
train_iter_loss: 0.12128215283155441
train_iter_loss: 0.3117210865020752
train_iter_loss: 0.2467687577009201
train_iter_loss: 0.17264051735401154
train_iter_loss: 0.12348777800798416
train_iter_loss: 0.09290698915719986
train_iter_loss: 0.13683314621448517
train_iter_loss: 0.2201838344335556
train_iter_loss: 0.21158775687217712
train_iter_loss: 0.19623856246471405
train_iter_loss: 0.10626373440027237
train_iter_loss: 0.07096489518880844
train_iter_loss: 0.2966075539588928
train_iter_loss: 0.12515711784362793
train_iter_loss: 0.1925985962152481
train_iter_loss: 0.205997496843338
train_iter_loss: 0.11447407305240631
train_iter_loss: 0.14912840723991394
train_iter_loss: 0.08574635535478592
train_iter_loss: 0.13102032244205475
train_iter_loss: 0.13678300380706787
train_iter_loss: 0.2004667967557907
train_iter_loss: 0.17410551011562347
train_iter_loss: 0.24702605605125427
train loss :0.1585
---------------------
Validation seg loss: 0.21533804591569417 at epoch 712
epoch =    713/  1000, exp = train
train_iter_loss: 0.09934482723474503
train_iter_loss: 0.15729664266109467
train_iter_loss: 0.2660956382751465
train_iter_loss: 0.21620914340019226
train_iter_loss: 0.1660139113664627
train_iter_loss: 0.3627117872238159
train_iter_loss: 0.1779319792985916
train_iter_loss: 0.08752017468214035
train_iter_loss: 0.19348777830600739
train_iter_loss: 0.16443359851837158
train_iter_loss: 0.0951218381524086
train_iter_loss: 0.2234366536140442
train_iter_loss: 0.10381433367729187
train_iter_loss: 0.14095833897590637
train_iter_loss: 0.08914462476968765
train_iter_loss: 0.1325220912694931
train_iter_loss: 0.12378626316785812
train_iter_loss: 0.07162202149629593
train_iter_loss: 0.09335984289646149
train_iter_loss: 0.13785777986049652
train_iter_loss: 0.21090587973594666
train_iter_loss: 0.1968393325805664
train_iter_loss: 0.21261122822761536
train_iter_loss: 0.10004764050245285
train_iter_loss: 0.09659235179424286
train_iter_loss: 0.3203252851963043
train_iter_loss: 0.2954687178134918
train_iter_loss: 0.1867292821407318
train_iter_loss: 0.19176466763019562
train_iter_loss: 0.22498181462287903
train_iter_loss: 0.1512186974287033
train_iter_loss: 0.151421919465065
train_iter_loss: 0.08450861275196075
train_iter_loss: 0.1616656482219696
train_iter_loss: 0.13827349245548248
train_iter_loss: 0.14389444887638092
train_iter_loss: 0.17737503349781036
train_iter_loss: 0.11561716347932816
train_iter_loss: 0.14756760001182556
train_iter_loss: 0.19880934059619904
train_iter_loss: 0.14520743489265442
train_iter_loss: 0.12840470671653748
train_iter_loss: 0.1883564442396164
train_iter_loss: 0.12824410200119019
train_iter_loss: 0.14212337136268616
train_iter_loss: 0.14744307100772858
train_iter_loss: 0.1045675203204155
train_iter_loss: 0.12598970532417297
train_iter_loss: 0.2119167596101761
train_iter_loss: 0.10966173559427261
train_iter_loss: 0.08675288408994675
train_iter_loss: 0.04571811482310295
train_iter_loss: 0.1382688283920288
train_iter_loss: 0.18439160287380219
train_iter_loss: 0.10606591403484344
train_iter_loss: 0.11489921808242798
train_iter_loss: 0.17961350083351135
train_iter_loss: 0.1318429857492447
train_iter_loss: 0.13476286828517914
train_iter_loss: 0.15009190142154694
train_iter_loss: 0.09163739532232285
train_iter_loss: 0.14100532233715057
train_iter_loss: 0.15770724415779114
train_iter_loss: 0.12488698214292526
train_iter_loss: 0.19219142198562622
train_iter_loss: 0.13801148533821106
train_iter_loss: 0.10679201036691666
train_iter_loss: 0.10668234527111053
train_iter_loss: 0.17686507105827332
train_iter_loss: 0.3930334746837616
train_iter_loss: 0.05669030547142029
train_iter_loss: 0.1384667605161667
train_iter_loss: 0.13769887387752533
train_iter_loss: 0.07234127819538116
train_iter_loss: 0.08309247344732285
train_iter_loss: 0.10822443664073944
train_iter_loss: 0.1601105034351349
train_iter_loss: 0.15314285457134247
train_iter_loss: 0.3258150517940521
train_iter_loss: 0.17667078971862793
train_iter_loss: 0.10362528264522552
train_iter_loss: 0.1184249073266983
train_iter_loss: 0.0976911336183548
train_iter_loss: 0.13776268064975739
train_iter_loss: 0.23043619096279144
train_iter_loss: 0.18347765505313873
train_iter_loss: 0.1249338686466217
train_iter_loss: 0.08506444841623306
train_iter_loss: 0.18779462575912476
train_iter_loss: 0.20468197762966156
train_iter_loss: 0.10498827695846558
train_iter_loss: 0.21176627278327942
train_iter_loss: 0.13363689184188843
train_iter_loss: 0.16988497972488403
train_iter_loss: 0.07421707361936569
train_iter_loss: 0.11696426570415497
train_iter_loss: 0.14664432406425476
train_iter_loss: 0.24313350021839142
train_iter_loss: 0.11599934101104736
train_iter_loss: 0.2772739827632904
train loss :0.1558
---------------------
Validation seg loss: 0.21736587058612197 at epoch 713
epoch =    714/  1000, exp = train
train_iter_loss: 0.12343785911798477
train_iter_loss: 0.14931979775428772
train_iter_loss: 0.07824473083019257
train_iter_loss: 0.12897050380706787
train_iter_loss: 0.11555507034063339
train_iter_loss: 0.18568243086338043
train_iter_loss: 0.21732370555400848
train_iter_loss: 0.08471619337797165
train_iter_loss: 0.20022010803222656
train_iter_loss: 0.12211646139621735
train_iter_loss: 0.20760995149612427
train_iter_loss: 0.16814248263835907
train_iter_loss: 0.07022448629140854
train_iter_loss: 0.1722870022058487
train_iter_loss: 0.11998123675584793
train_iter_loss: 0.1578194499015808
train_iter_loss: 0.07641304284334183
train_iter_loss: 0.15039418637752533
train_iter_loss: 0.19732433557510376
train_iter_loss: 0.06092663109302521
train_iter_loss: 0.24106581509113312
train_iter_loss: 0.13687023520469666
train_iter_loss: 0.2789788842201233
train_iter_loss: 0.07316498458385468
train_iter_loss: 0.11997922509908676
train_iter_loss: 0.18050068616867065
train_iter_loss: 0.13706116378307343
train_iter_loss: 0.08928787708282471
train_iter_loss: 0.09102123230695724
train_iter_loss: 0.14729538559913635
train_iter_loss: 0.09993714094161987
train_iter_loss: 0.15964891016483307
train_iter_loss: 0.1843082457780838
train_iter_loss: 0.15710566937923431
train_iter_loss: 0.08761607110500336
train_iter_loss: 0.18784566223621368
train_iter_loss: 0.1026548221707344
train_iter_loss: 0.14317093789577484
train_iter_loss: 0.1230585053563118
train_iter_loss: 0.21091608703136444
train_iter_loss: 0.1560819149017334
train_iter_loss: 0.09837798774242401
train_iter_loss: 0.32078760862350464
train_iter_loss: 0.0662333145737648
train_iter_loss: 0.11999336630105972
train_iter_loss: 0.14231139421463013
train_iter_loss: 0.09288529306650162
train_iter_loss: 0.154994398355484
train_iter_loss: 0.21289075911045074
train_iter_loss: 0.13056595623493195
train_iter_loss: 0.1480245143175125
train_iter_loss: 0.15019862353801727
train_iter_loss: 0.13306550681591034
train_iter_loss: 0.16104699671268463
train_iter_loss: 0.26079121232032776
train_iter_loss: 0.29706764221191406
train_iter_loss: 0.30009379982948303
train_iter_loss: 0.08932609856128693
train_iter_loss: 0.3383689224720001
train_iter_loss: 0.18395526707172394
train_iter_loss: 0.0650479719042778
train_iter_loss: 0.10820140689611435
train_iter_loss: 0.11596108973026276
train_iter_loss: 0.1962226778268814
train_iter_loss: 0.1516619622707367
train_iter_loss: 0.10526806861162186
train_iter_loss: 0.0884939655661583
train_iter_loss: 0.1679053157567978
train_iter_loss: 0.21588203310966492
train_iter_loss: 0.1698843240737915
train_iter_loss: 0.15671439468860626
train_iter_loss: 0.10616946965456009
train_iter_loss: 0.10737747699022293
train_iter_loss: 0.2096884548664093
train_iter_loss: 0.147627592086792
train_iter_loss: 0.15869498252868652
train_iter_loss: 0.17834095656871796
train_iter_loss: 0.0978013277053833
train_iter_loss: 0.16369087994098663
train_iter_loss: 0.10923831164836884
train_iter_loss: 0.13077795505523682
train_iter_loss: 0.21668514609336853
train_iter_loss: 0.16172432899475098
train_iter_loss: 0.24549967050552368
train_iter_loss: 0.09736855328083038
train_iter_loss: 0.18064235150814056
train_iter_loss: 0.19104820489883423
train_iter_loss: 0.1339867115020752
train_iter_loss: 0.0706554502248764
train_iter_loss: 0.07491908222436905
train_iter_loss: 0.15427154302597046
train_iter_loss: 0.18410786986351013
train_iter_loss: 0.18296144902706146
train_iter_loss: 0.20219917595386505
train_iter_loss: 0.09906654804944992
train_iter_loss: 0.16728690266609192
train_iter_loss: 0.17659534513950348
train_iter_loss: 0.13482262194156647
train_iter_loss: 0.13067109882831573
train_iter_loss: 0.2846405506134033
train loss :0.1549
---------------------
Validation seg loss: 0.21749162939289268 at epoch 714
epoch =    715/  1000, exp = train
train_iter_loss: 0.08266602456569672
train_iter_loss: 0.1314334124326706
train_iter_loss: 0.14396609365940094
train_iter_loss: 0.08289463073015213
train_iter_loss: 0.14137756824493408
train_iter_loss: 0.1431758999824524
train_iter_loss: 0.09015225619077682
train_iter_loss: 0.15538059175014496
train_iter_loss: 0.18589745461940765
train_iter_loss: 0.17231586575508118
train_iter_loss: 0.1558324545621872
train_iter_loss: 0.09700815379619598
train_iter_loss: 0.30540552735328674
train_iter_loss: 0.11920548230409622
train_iter_loss: 0.10227149724960327
train_iter_loss: 0.13122479617595673
train_iter_loss: 0.08608735352754593
train_iter_loss: 0.053980641067028046
train_iter_loss: 0.1560574322938919
train_iter_loss: 0.11550497263669968
train_iter_loss: 0.18973836302757263
train_iter_loss: 0.07225864380598068
train_iter_loss: 0.11965310573577881
train_iter_loss: 0.20392665266990662
train_iter_loss: 0.14982537925243378
train_iter_loss: 0.28604742884635925
train_iter_loss: 0.08287964761257172
train_iter_loss: 0.09772948175668716
train_iter_loss: 0.3731098175048828
train_iter_loss: 0.17411744594573975
train_iter_loss: 0.23710821568965912
train_iter_loss: 0.0945504680275917
train_iter_loss: 0.14003770053386688
train_iter_loss: 0.14106005430221558
train_iter_loss: 0.1519552767276764
train_iter_loss: 0.09201020747423172
train_iter_loss: 0.13849575817584991
train_iter_loss: 0.10266321897506714
train_iter_loss: 0.17015090584754944
train_iter_loss: 0.15931308269500732
train_iter_loss: 0.29497817158699036
train_iter_loss: 0.15466269850730896
train_iter_loss: 0.19368568062782288
train_iter_loss: 0.15148936212062836
train_iter_loss: 0.18113727867603302
train_iter_loss: 0.10597507655620575
train_iter_loss: 0.14111115038394928
train_iter_loss: 0.0800560936331749
train_iter_loss: 0.05693233385682106
train_iter_loss: 0.16211749613285065
train_iter_loss: 0.12119881063699722
train_iter_loss: 0.18746121227741241
train_iter_loss: 0.11459621787071228
train_iter_loss: 0.20399217307567596
train_iter_loss: 0.1290055215358734
train_iter_loss: 0.12521067261695862
train_iter_loss: 0.20272068679332733
train_iter_loss: 0.2508765459060669
train_iter_loss: 0.22357404232025146
train_iter_loss: 0.1546027809381485
train_iter_loss: 0.05586123466491699
train_iter_loss: 0.1502360701560974
train_iter_loss: 0.15611957013607025
train_iter_loss: 0.1971725970506668
train_iter_loss: 0.25805461406707764
train_iter_loss: 0.1674492210149765
train_iter_loss: 0.15247225761413574
train_iter_loss: 0.31395435333251953
train_iter_loss: 0.12339846044778824
train_iter_loss: 0.1970844268798828
train_iter_loss: 0.14528250694274902
train_iter_loss: 0.1309269666671753
train_iter_loss: 0.12153606116771698
train_iter_loss: 0.18263454735279083
train_iter_loss: 0.0799512043595314
train_iter_loss: 0.1838247925043106
train_iter_loss: 0.14452743530273438
train_iter_loss: 0.1267525553703308
train_iter_loss: 0.0986078605055809
train_iter_loss: 0.15552391111850739
train_iter_loss: 0.09242241829633713
train_iter_loss: 0.2708054482936859
train_iter_loss: 0.13771875202655792
train_iter_loss: 0.18819238245487213
train_iter_loss: 0.27083349227905273
train_iter_loss: 0.3030225336551666
train_iter_loss: 0.10417679697275162
train_iter_loss: 0.2286415845155716
train_iter_loss: 0.2320963442325592
train_iter_loss: 0.12248421460390091
train_iter_loss: 0.15769709646701813
train_iter_loss: 0.22550299763679504
train_iter_loss: 0.11604318767786026
train_iter_loss: 0.13335683941841125
train_iter_loss: 0.13036176562309265
train_iter_loss: 0.24100057780742645
train_iter_loss: 0.15135541558265686
train_iter_loss: 0.055915672332048416
train_iter_loss: 0.12877638638019562
train_iter_loss: 0.2994745671749115
train loss :0.1595
---------------------
Validation seg loss: 0.21534617567645772 at epoch 715
epoch =    716/  1000, exp = train
train_iter_loss: 0.11181829124689102
train_iter_loss: 0.16054880619049072
train_iter_loss: 0.25837528705596924
train_iter_loss: 0.11549074202775955
train_iter_loss: 0.14005893468856812
train_iter_loss: 0.10962904244661331
train_iter_loss: 0.23002491891384125
train_iter_loss: 0.08057666569948196
train_iter_loss: 0.2916828393936157
train_iter_loss: 0.0754828229546547
train_iter_loss: 0.24519291520118713
train_iter_loss: 0.09079977124929428
train_iter_loss: 0.21620585024356842
train_iter_loss: 0.19751814007759094
train_iter_loss: 0.23921458423137665
train_iter_loss: 0.13315150141716003
train_iter_loss: 0.13577215373516083
train_iter_loss: 0.1469482034444809
train_iter_loss: 0.13222536444664001
train_iter_loss: 0.23950308561325073
train_iter_loss: 0.11741255223751068
train_iter_loss: 0.12432177364826202
train_iter_loss: 0.2694272994995117
train_iter_loss: 0.13219569623470306
train_iter_loss: 0.29660120606422424
train_iter_loss: 0.1122589111328125
train_iter_loss: 0.10608893632888794
train_iter_loss: 0.15040133893489838
train_iter_loss: 0.07402966916561127
train_iter_loss: 0.27811238169670105
train_iter_loss: 0.08888979256153107
train_iter_loss: 0.20141366124153137
train_iter_loss: 0.1615791916847229
train_iter_loss: 0.11206873506307602
train_iter_loss: 0.05887492001056671
train_iter_loss: 0.15165546536445618
train_iter_loss: 0.09307786077260971
train_iter_loss: 0.150329127907753
train_iter_loss: 0.11852406710386276
train_iter_loss: 0.16251784563064575
train_iter_loss: 0.12253909558057785
train_iter_loss: 0.14754220843315125
train_iter_loss: 0.16945044696331024
train_iter_loss: 0.08612654358148575
train_iter_loss: 0.23767217993736267
train_iter_loss: 0.06934082508087158
train_iter_loss: 0.12357910722494125
train_iter_loss: 0.23390187323093414
train_iter_loss: 0.1029965877532959
train_iter_loss: 0.16153478622436523
train_iter_loss: 0.17104478180408478
train_iter_loss: 0.29746562242507935
train_iter_loss: 0.2658625543117523
train_iter_loss: 0.2647302746772766
train_iter_loss: 0.13990774750709534
train_iter_loss: 0.12080394476652145
train_iter_loss: 0.21538057923316956
train_iter_loss: 0.1613505631685257
train_iter_loss: 0.14379991590976715
train_iter_loss: 0.09171151369810104
train_iter_loss: 0.04941360652446747
train_iter_loss: 0.1617153435945511
train_iter_loss: 0.06773104518651962
train_iter_loss: 0.12336360663175583
train_iter_loss: 0.11293207108974457
train_iter_loss: 0.17540587484836578
train_iter_loss: 0.183807834982872
train_iter_loss: 0.24867020547389984
train_iter_loss: 0.10053519904613495
train_iter_loss: 0.23141320049762726
train_iter_loss: 0.0433792918920517
train_iter_loss: 0.15084798634052277
train_iter_loss: 0.1890116184949875
train_iter_loss: 0.1617332249879837
train_iter_loss: 0.11609089374542236
train_iter_loss: 0.2085651457309723
train_iter_loss: 0.0960153341293335
train_iter_loss: 0.37449324131011963
train_iter_loss: 0.13729484379291534
train_iter_loss: 0.18488748371601105
train_iter_loss: 0.07835426926612854
train_iter_loss: 0.255462646484375
train_iter_loss: 0.15540200471878052
train_iter_loss: 0.22206202149391174
train_iter_loss: 0.04015961289405823
train_iter_loss: 0.12106833606958389
train_iter_loss: 0.05698837712407112
train_iter_loss: 0.17207330465316772
train_iter_loss: 0.2224915772676468
train_iter_loss: 0.08964041620492935
train_iter_loss: 0.21640120446681976
train_iter_loss: 0.1436128467321396
train_iter_loss: 0.20504042506217957
train_iter_loss: 0.0765085443854332
train_iter_loss: 0.12468303740024567
train_iter_loss: 0.13239248096942902
train_iter_loss: 0.10406298190355301
train_iter_loss: 0.10009422898292542
train_iter_loss: 0.07333826273679733
train_iter_loss: 0.19824381172657013
train loss :0.1570
---------------------
Validation seg loss: 0.21482235692300886 at epoch 716
epoch =    717/  1000, exp = train
train_iter_loss: 0.15056179463863373
train_iter_loss: 0.14801622927188873
train_iter_loss: 0.1141863763332367
train_iter_loss: 0.1282808631658554
train_iter_loss: 0.08401943743228912
train_iter_loss: 0.2663734555244446
train_iter_loss: 0.1730012595653534
train_iter_loss: 0.12791568040847778
train_iter_loss: 0.12366721779108047
train_iter_loss: 0.042233873158693314
train_iter_loss: 0.17458012700080872
train_iter_loss: 0.16285136342048645
train_iter_loss: 0.12958413362503052
train_iter_loss: 0.16569314897060394
train_iter_loss: 0.18747098743915558
train_iter_loss: 0.26137784123420715
train_iter_loss: 0.18163596093654633
train_iter_loss: 0.1859021633863449
train_iter_loss: 0.16347846388816833
train_iter_loss: 0.16354766488075256
train_iter_loss: 0.10514742136001587
train_iter_loss: 0.2273692786693573
train_iter_loss: 0.16459248960018158
train_iter_loss: 0.21057993173599243
train_iter_loss: 0.08212917298078537
train_iter_loss: 0.21361654996871948
train_iter_loss: 0.24157926440238953
train_iter_loss: 0.14616185426712036
train_iter_loss: 0.18111738562583923
train_iter_loss: 0.03019566833972931
train_iter_loss: 0.13791100680828094
train_iter_loss: 0.18708950281143188
train_iter_loss: 0.2153978794813156
train_iter_loss: 0.20194193720817566
train_iter_loss: 0.14726422727108002
train_iter_loss: 0.17116181552410126
train_iter_loss: 0.13077861070632935
train_iter_loss: 0.10875013470649719
train_iter_loss: 0.123444564640522
train_iter_loss: 0.11436080932617188
train_iter_loss: 0.07308081537485123
train_iter_loss: 0.13741615414619446
train_iter_loss: 0.268954873085022
train_iter_loss: 0.11233223974704742
train_iter_loss: 0.16012662649154663
train_iter_loss: 0.05052271485328674
train_iter_loss: 0.08524100482463837
train_iter_loss: 0.08229049295186996
train_iter_loss: 0.09691265970468521
train_iter_loss: 0.042145851999521255
train_iter_loss: 0.19831472635269165
train_iter_loss: 0.2223031222820282
train_iter_loss: 0.24625732004642487
train_iter_loss: 0.18416275084018707
train_iter_loss: 0.1767933964729309
train_iter_loss: 0.03735506534576416
train_iter_loss: 0.16111917793750763
train_iter_loss: 0.19325537979602814
train_iter_loss: 0.10908818244934082
train_iter_loss: 0.1141931489109993
train_iter_loss: 0.25138580799102783
train_iter_loss: 0.25292161107063293
train_iter_loss: 0.07394885271787643
train_iter_loss: 0.09558402001857758
train_iter_loss: 0.1733016073703766
train_iter_loss: 0.19105228781700134
train_iter_loss: 0.022876782342791557
train_iter_loss: 0.19655027985572815
train_iter_loss: 0.13557212054729462
train_iter_loss: 0.09262407571077347
train_iter_loss: 0.21278530359268188
train_iter_loss: 0.12299063056707382
train_iter_loss: 0.13684916496276855
train_iter_loss: 0.28815582394599915
train_iter_loss: 0.19610591232776642
train_iter_loss: 0.14982424676418304
train_iter_loss: 0.11983012408018112
train_iter_loss: 0.14899931848049164
train_iter_loss: 0.13479779660701752
train_iter_loss: 0.23609662055969238
train_iter_loss: 0.1466829478740692
train_iter_loss: 0.20553070306777954
train_iter_loss: 0.23240773379802704
train_iter_loss: 0.20423516631126404
train_iter_loss: 0.08006350696086884
train_iter_loss: 0.26738494634628296
train_iter_loss: 0.03976081684231758
train_iter_loss: 0.14182181656360626
train_iter_loss: 0.15805619955062866
train_iter_loss: 0.1409129947423935
train_iter_loss: 0.12035775929689407
train_iter_loss: 0.11731661856174469
train_iter_loss: 0.14799678325653076
train_iter_loss: 0.14561167359352112
train_iter_loss: 0.05520900711417198
train_iter_loss: 0.20198163390159607
train_iter_loss: 0.24941422045230865
train_iter_loss: 0.17925488948822021
train_iter_loss: 0.08992665261030197
train_iter_loss: 0.07186055183410645
train loss :0.1541
---------------------
Validation seg loss: 0.21377785003937358 at epoch 717
epoch =    718/  1000, exp = train
train_iter_loss: 0.11168244481086731
train_iter_loss: 0.12676119804382324
train_iter_loss: 0.2695424258708954
train_iter_loss: 0.13066545128822327
train_iter_loss: 0.1811486780643463
train_iter_loss: 0.13306409120559692
train_iter_loss: 0.1675388067960739
train_iter_loss: 0.2142365276813507
train_iter_loss: 0.16048866510391235
train_iter_loss: 0.0951356589794159
train_iter_loss: 0.2452528327703476
train_iter_loss: 0.18484798073768616
train_iter_loss: 0.14430442452430725
train_iter_loss: 0.1410810649394989
train_iter_loss: 0.1266539841890335
train_iter_loss: 0.1513749212026596
train_iter_loss: 0.13917644321918488
train_iter_loss: 0.1082896888256073
train_iter_loss: 0.14764507114887238
train_iter_loss: 0.15519124269485474
train_iter_loss: 0.1601043939590454
train_iter_loss: 0.15717482566833496
train_iter_loss: 0.15452826023101807
train_iter_loss: 0.15956546366214752
train_iter_loss: 0.13261905312538147
train_iter_loss: 0.1747765690088272
train_iter_loss: 0.1311805248260498
train_iter_loss: 0.2045818567276001
train_iter_loss: 0.1603493094444275
train_iter_loss: 0.2498365342617035
train_iter_loss: 0.1212792918086052
train_iter_loss: 0.09615688025951385
train_iter_loss: 0.08204634487628937
train_iter_loss: 0.06470271944999695
train_iter_loss: 0.12874405086040497
train_iter_loss: 0.09149318188428879
train_iter_loss: 0.13888418674468994
train_iter_loss: 0.12882375717163086
train_iter_loss: 0.14498035609722137
train_iter_loss: 0.17681491374969482
train_iter_loss: 0.1535278856754303
train_iter_loss: 0.11171878129243851
train_iter_loss: 0.1327565461397171
train_iter_loss: 0.12687177956104279
train_iter_loss: 0.21499815583229065
train_iter_loss: 0.12236301600933075
train_iter_loss: 0.06683804094791412
train_iter_loss: 0.1535646915435791
train_iter_loss: 0.13307513296604156
train_iter_loss: 0.15791766345500946
train_iter_loss: 0.13710664212703705
train_iter_loss: 0.21699127554893494
train_iter_loss: 0.06544654071331024
train_iter_loss: 0.2656315863132477
train_iter_loss: 0.19610944390296936
train_iter_loss: 0.2201981246471405
train_iter_loss: 0.05347398295998573
train_iter_loss: 0.16189976036548615
train_iter_loss: 0.09529278427362442
train_iter_loss: 0.2783864736557007
train_iter_loss: 0.22132335603237152
train_iter_loss: 0.13334979116916656
train_iter_loss: 0.18192769587039948
train_iter_loss: 0.1770150363445282
train_iter_loss: 0.1628028005361557
train_iter_loss: 0.16654300689697266
train_iter_loss: 0.24782513082027435
train_iter_loss: 0.22002220153808594
train_iter_loss: 0.1820559948682785
train_iter_loss: 0.12082937359809875
train_iter_loss: 0.19499224424362183
train_iter_loss: 0.1256987303495407
train_iter_loss: 0.09310559183359146
train_iter_loss: 0.2000509351491928
train_iter_loss: 0.20246021449565887
train_iter_loss: 0.19980716705322266
train_iter_loss: 0.1814420521259308
train_iter_loss: 0.11625438183546066
train_iter_loss: 0.15676242113113403
train_iter_loss: 0.18921467661857605
train_iter_loss: 0.10623053461313248
train_iter_loss: 0.10349918901920319
train_iter_loss: 0.10128092765808105
train_iter_loss: 0.3242458403110504
train_iter_loss: 0.15388819575309753
train_iter_loss: 0.1580909937620163
train_iter_loss: 0.14994443953037262
train_iter_loss: 0.08181414008140564
train_iter_loss: 0.09061241894960403
train_iter_loss: 0.17054957151412964
train_iter_loss: 0.08664962649345398
train_iter_loss: 0.24266786873340607
train_iter_loss: 0.14927208423614502
train_iter_loss: 0.04060538858175278
train_iter_loss: 0.20796436071395874
train_iter_loss: 0.13285093009471893
train_iter_loss: 0.15219180285930634
train_iter_loss: 0.1427929699420929
train_iter_loss: 0.05269261449575424
train_iter_loss: 0.13689406216144562
train loss :0.1547
---------------------
Validation seg loss: 0.22024140177505477 at epoch 718
epoch =    719/  1000, exp = train
train_iter_loss: 0.1944553405046463
train_iter_loss: 0.03869526833295822
train_iter_loss: 0.11007566004991531
train_iter_loss: 0.08104057610034943
train_iter_loss: 0.06349506229162216
train_iter_loss: 0.11422266066074371
train_iter_loss: 0.12070493400096893
train_iter_loss: 0.08682650327682495
train_iter_loss: 0.14491936564445496
train_iter_loss: 0.21143735945224762
train_iter_loss: 0.20117048919200897
train_iter_loss: 0.12186840176582336
train_iter_loss: 0.18315163254737854
train_iter_loss: 0.10773041844367981
train_iter_loss: 0.15621298551559448
train_iter_loss: 0.084513820707798
train_iter_loss: 0.08944999426603317
train_iter_loss: 0.3956059515476227
train_iter_loss: 0.1668189913034439
train_iter_loss: 0.176142618060112
train_iter_loss: 0.1983661949634552
train_iter_loss: 0.12265599519014359
train_iter_loss: 0.12489628046751022
train_iter_loss: 0.16372931003570557
train_iter_loss: 0.11711326241493225
train_iter_loss: 0.056776005774736404
train_iter_loss: 0.08595529943704605
train_iter_loss: 0.14465674757957458
train_iter_loss: 0.09204678237438202
train_iter_loss: 0.24497106671333313
train_iter_loss: 0.17343614995479584
train_iter_loss: 0.22999101877212524
train_iter_loss: 0.15684790909290314
train_iter_loss: 0.07572541385889053
train_iter_loss: 0.1144239753484726
train_iter_loss: 0.19604292511940002
train_iter_loss: 0.19693849980831146
train_iter_loss: 0.16315172612667084
train_iter_loss: 0.24058294296264648
train_iter_loss: 0.20888137817382812
train_iter_loss: 0.13449300825595856
train_iter_loss: 0.10103119164705276
train_iter_loss: 0.18263016641139984
train_iter_loss: 0.08536262810230255
train_iter_loss: 0.17015893757343292
train_iter_loss: 0.20014408230781555
train_iter_loss: 0.14339053630828857
train_iter_loss: 0.18017034232616425
train_iter_loss: 0.16697759926319122
train_iter_loss: 0.30806174874305725
train_iter_loss: 0.13497793674468994
train_iter_loss: 0.30657339096069336
train_iter_loss: 0.3380531668663025
train_iter_loss: 0.07383087277412415
train_iter_loss: 0.14052161574363708
train_iter_loss: 0.1375487893819809
train_iter_loss: 0.17765310406684875
train_iter_loss: 0.2423863708972931
train_iter_loss: 0.16734403371810913
train_iter_loss: 0.2088964730501175
train_iter_loss: 0.13973984122276306
train_iter_loss: 0.16718055307865143
train_iter_loss: 0.15951694548130035
train_iter_loss: 0.19581037759780884
train_iter_loss: 0.1426767259836197
train_iter_loss: 0.07697640359401703
train_iter_loss: 0.30852651596069336
train_iter_loss: 0.16289173066616058
train_iter_loss: 0.2509894371032715
train_iter_loss: 0.08178885281085968
train_iter_loss: 0.09535180032253265
train_iter_loss: 0.1326858401298523
train_iter_loss: 0.1623576581478119
train_iter_loss: 0.19619706273078918
train_iter_loss: 0.09061773121356964
train_iter_loss: 0.20150162279605865
train_iter_loss: 0.244611456990242
train_iter_loss: 0.1368035078048706
train_iter_loss: 0.25625723600387573
train_iter_loss: 0.18876339495182037
train_iter_loss: 0.16145862638950348
train_iter_loss: 0.19197164475917816
train_iter_loss: 0.1359764039516449
train_iter_loss: 0.1868983954191208
train_iter_loss: 0.16308176517486572
train_iter_loss: 0.12935452163219452
train_iter_loss: 0.18537980318069458
train_iter_loss: 0.10148518532514572
train_iter_loss: 0.18724879622459412
train_iter_loss: 0.1776963770389557
train_iter_loss: 0.08018182963132858
train_iter_loss: 0.10151147097349167
train_iter_loss: 0.219975546002388
train_iter_loss: 0.2528061866760254
train_iter_loss: 0.2188745141029358
train_iter_loss: 0.17992162704467773
train_iter_loss: 0.14603783190250397
train_iter_loss: 0.2688341736793518
train_iter_loss: 0.1027190163731575
train_iter_loss: 0.17484630644321442
train loss :0.1647
---------------------
Validation seg loss: 0.2159155653438197 at epoch 719
epoch =    720/  1000, exp = train
train_iter_loss: 0.22685456275939941
train_iter_loss: 0.08450246602296829
train_iter_loss: 0.13138720393180847
train_iter_loss: 0.11929791420698166
train_iter_loss: 0.19294743239879608
train_iter_loss: 0.10317905992269516
train_iter_loss: 0.1890222579240799
train_iter_loss: 0.25069770216941833
train_iter_loss: 0.09088870882987976
train_iter_loss: 0.2073124498128891
train_iter_loss: 0.13552600145339966
train_iter_loss: 0.03755899518728256
train_iter_loss: 0.1107000857591629
train_iter_loss: 0.12206430733203888
train_iter_loss: 0.15050232410430908
train_iter_loss: 0.15956246852874756
train_iter_loss: 0.19819891452789307
train_iter_loss: 0.1993708312511444
train_iter_loss: 0.28621649742126465
train_iter_loss: 0.17015007138252258
train_iter_loss: 0.2426440715789795
train_iter_loss: 0.14979462325572968
train_iter_loss: 0.14713402092456818
train_iter_loss: 0.14515626430511475
train_iter_loss: 0.30047839879989624
train_iter_loss: 0.08131968230009079
train_iter_loss: 0.28535521030426025
train_iter_loss: 0.05112830549478531
train_iter_loss: 0.1001097708940506
train_iter_loss: 0.1986950784921646
train_iter_loss: 0.33884209394454956
train_iter_loss: 0.2024116963148117
train_iter_loss: 0.11329948157072067
train_iter_loss: 0.09997132420539856
train_iter_loss: 0.1137365847826004
train_iter_loss: 0.17281979322433472
train_iter_loss: 0.11622987687587738
train_iter_loss: 0.13419851660728455
train_iter_loss: 0.1069570928812027
train_iter_loss: 0.2189880758523941
train_iter_loss: 0.078697569668293
train_iter_loss: 0.17467886209487915
train_iter_loss: 0.1901676207780838
train_iter_loss: 0.08537173271179199
train_iter_loss: 0.10508985072374344
train_iter_loss: 0.07150618731975555
train_iter_loss: 0.19138458371162415
train_iter_loss: 0.1342487335205078
train_iter_loss: 0.10582707077264786
train_iter_loss: 0.07375892996788025
train_iter_loss: 0.04220450669527054
train_iter_loss: 0.20880165696144104
train_iter_loss: 0.13996213674545288
train_iter_loss: 0.14952239394187927
train_iter_loss: 0.2181079387664795
train_iter_loss: 0.14342057704925537
train_iter_loss: 0.141156405210495
train_iter_loss: 0.3418324887752533
train_iter_loss: 0.10141082108020782
train_iter_loss: 0.20581045746803284
train_iter_loss: 0.18476150929927826
train_iter_loss: 0.19258354604244232
train_iter_loss: 0.10315678268671036
train_iter_loss: 0.10390408337116241
train_iter_loss: 0.17616143822669983
train_iter_loss: 0.12941552698612213
train_iter_loss: 0.09628233313560486
train_iter_loss: 0.194600448012352
train_iter_loss: 0.19407834112644196
train_iter_loss: 0.11672508716583252
train_iter_loss: 0.14256735146045685
train_iter_loss: 0.13368038833141327
train_iter_loss: 0.1446758657693863
train_iter_loss: 0.16678975522518158
train_iter_loss: 0.27022403478622437
train_iter_loss: 0.15474916994571686
train_iter_loss: 0.07898899912834167
train_iter_loss: 0.21183030307292938
train_iter_loss: 0.2091178148984909
train_iter_loss: 0.08799567073583603
train_iter_loss: 0.16662131249904633
train_iter_loss: 0.09140030294656754
train_iter_loss: 0.1600109487771988
train_iter_loss: 0.29863569140434265
train_iter_loss: 0.10695336759090424
train_iter_loss: 0.13723716139793396
train_iter_loss: 0.13831205666065216
train_iter_loss: 0.20062440633773804
train_iter_loss: 0.11227346211671829
train_iter_loss: 0.15555407106876373
train_iter_loss: 0.15698136389255524
train_iter_loss: 0.29379749298095703
train_iter_loss: 0.23934465646743774
train_iter_loss: 0.1587117463350296
train_iter_loss: 0.13916370272636414
train_iter_loss: 0.23197562992572784
train_iter_loss: 0.18974152207374573
train_iter_loss: 0.07142242789268494
train_iter_loss: 0.181050643324852
train_iter_loss: 0.07133054733276367
train loss :0.1595
---------------------
Validation seg loss: 0.21488658156913687 at epoch 720
epoch =    721/  1000, exp = train
train_iter_loss: 0.13932855427265167
train_iter_loss: 0.12400878965854645
train_iter_loss: 0.14013773202896118
train_iter_loss: 0.10362021625041962
train_iter_loss: 0.17555567622184753
train_iter_loss: 0.09083914011716843
train_iter_loss: 0.037814054638147354
train_iter_loss: 0.2185133546590805
train_iter_loss: 0.23692964017391205
train_iter_loss: 0.044258471578359604
train_iter_loss: 0.22134608030319214
train_iter_loss: 0.11110953241586685
train_iter_loss: 0.09564676880836487
train_iter_loss: 0.1567399650812149
train_iter_loss: 0.13033436238765717
train_iter_loss: 0.08514364808797836
train_iter_loss: 0.10520131886005402
train_iter_loss: 0.16204558312892914
train_iter_loss: 0.1431243121623993
train_iter_loss: 0.16504181921482086
train_iter_loss: 0.2077229619026184
train_iter_loss: 0.08385372906923294
train_iter_loss: 0.15784168243408203
train_iter_loss: 0.17500562965869904
train_iter_loss: 0.11479134112596512
train_iter_loss: 0.10211940109729767
train_iter_loss: 0.18011990189552307
train_iter_loss: 0.22426795959472656
train_iter_loss: 0.20054180920124054
train_iter_loss: 0.1513856202363968
train_iter_loss: 0.17280879616737366
train_iter_loss: 0.19874358177185059
train_iter_loss: 0.3155643939971924
train_iter_loss: 0.12401390820741653
train_iter_loss: 0.1086486428976059
train_iter_loss: 0.17818331718444824
train_iter_loss: 0.11563123762607574
train_iter_loss: 0.1535302698612213
train_iter_loss: 0.14809495210647583
train_iter_loss: 0.13168825209140778
train_iter_loss: 0.1921330988407135
train_iter_loss: 0.12745773792266846
train_iter_loss: 0.17720894515514374
train_iter_loss: 0.2048209309577942
train_iter_loss: 0.0907253697514534
train_iter_loss: 0.13351069390773773
train_iter_loss: 0.11866907775402069
train_iter_loss: 0.207112655043602
train_iter_loss: 0.26049160957336426
train_iter_loss: 0.1308872401714325
train_iter_loss: 0.28775474429130554
train_iter_loss: 0.14048722386360168
train_iter_loss: 0.17903245985507965
train_iter_loss: 0.14896737039089203
train_iter_loss: 0.2750547528266907
train_iter_loss: 0.11332032829523087
train_iter_loss: 0.15372830629348755
train_iter_loss: 0.15061083436012268
train_iter_loss: 0.16575759649276733
train_iter_loss: 0.1715354472398758
train_iter_loss: 0.06325832009315491
train_iter_loss: 0.17623203992843628
train_iter_loss: 0.17205515503883362
train_iter_loss: 0.18447479605674744
train_iter_loss: 0.23394961655139923
train_iter_loss: 0.305489718914032
train_iter_loss: 0.09179393202066422
train_iter_loss: 0.152364581823349
train_iter_loss: 0.20230984687805176
train_iter_loss: 0.07200683653354645
train_iter_loss: 0.11236708611249924
train_iter_loss: 0.18946564197540283
train_iter_loss: 0.22059336304664612
train_iter_loss: 0.09046482294797897
train_iter_loss: 0.19150185585021973
train_iter_loss: 0.1325666755437851
train_iter_loss: 0.10416163504123688
train_iter_loss: 0.14618580043315887
train_iter_loss: 0.1337609738111496
train_iter_loss: 0.13149546086788177
train_iter_loss: 0.2349965125322342
train_iter_loss: 0.24929389357566833
train_iter_loss: 0.2448950707912445
train_iter_loss: 0.11855490505695343
train_iter_loss: 0.1589205265045166
train_iter_loss: 0.15470357239246368
train_iter_loss: 0.15324914455413818
train_iter_loss: 0.09101979434490204
train_iter_loss: 0.19861434400081635
train_iter_loss: 0.20747649669647217
train_iter_loss: 0.12069898098707199
train_iter_loss: 0.18862542510032654
train_iter_loss: 0.2120489925146103
train_iter_loss: 0.31653618812561035
train_iter_loss: 0.04915235564112663
train_iter_loss: 0.13849574327468872
train_iter_loss: 0.15074026584625244
train_iter_loss: 0.3058808147907257
train_iter_loss: 0.07027759402990341
train_iter_loss: 0.07248961925506592
train loss :0.1606
---------------------
Validation seg loss: 0.22378178122716974 at epoch 721
epoch =    722/  1000, exp = train
train_iter_loss: 0.2230682075023651
train_iter_loss: 0.14064939320087433
train_iter_loss: 0.06144372746348381
train_iter_loss: 0.11636465042829514
train_iter_loss: 0.1776745617389679
train_iter_loss: 0.2113090455532074
train_iter_loss: 0.2008010447025299
train_iter_loss: 0.2845800817012787
train_iter_loss: 0.08587514609098434
train_iter_loss: 0.17789238691329956
train_iter_loss: 0.050829749554395676
train_iter_loss: 0.0859263464808464
train_iter_loss: 0.22346611320972443
train_iter_loss: 0.2168630212545395
train_iter_loss: 0.13653619587421417
train_iter_loss: 0.16645905375480652
train_iter_loss: 0.127804234623909
train_iter_loss: 0.15569572150707245
train_iter_loss: 0.16139326989650726
train_iter_loss: 0.15888769924640656
train_iter_loss: 0.13183893263339996
train_iter_loss: 0.1256491094827652
train_iter_loss: 0.1283133178949356
train_iter_loss: 0.24860049784183502
train_iter_loss: 0.12166819721460342
train_iter_loss: 0.19105640053749084
train_iter_loss: 0.17573006451129913
train_iter_loss: 0.16488730907440186
train_iter_loss: 0.15985436737537384
train_iter_loss: 0.12888042628765106
train_iter_loss: 0.10519789159297943
train_iter_loss: 0.1026415228843689
train_iter_loss: 0.1322726011276245
train_iter_loss: 0.1141449362039566
train_iter_loss: 0.17124903202056885
train_iter_loss: 0.12323630601167679
train_iter_loss: 0.10597075521945953
train_iter_loss: 0.24129709601402283
train_iter_loss: 0.14108726382255554
train_iter_loss: 0.16639584302902222
train_iter_loss: 0.14560949802398682
train_iter_loss: 0.13501957058906555
train_iter_loss: 0.21221616864204407
train_iter_loss: 0.19552047550678253
train_iter_loss: 0.12875622510910034
train_iter_loss: 0.19554735720157623
train_iter_loss: 0.12847891449928284
train_iter_loss: 0.08310187608003616
train_iter_loss: 0.16673128306865692
train_iter_loss: 0.21619659662246704
train_iter_loss: 0.16559268534183502
train_iter_loss: 0.11906213313341141
train_iter_loss: 0.08872979134321213
train_iter_loss: 0.11032408475875854
train_iter_loss: 0.17564649879932404
train_iter_loss: 0.30897730588912964
train_iter_loss: 0.21697324514389038
train_iter_loss: 0.210191011428833
train_iter_loss: 0.25775831937789917
train_iter_loss: 0.23579655587673187
train_iter_loss: 0.12997855246067047
train_iter_loss: 0.16332097351551056
train_iter_loss: 0.09414111822843552
train_iter_loss: 0.1340618133544922
train_iter_loss: 0.14291797578334808
train_iter_loss: 0.13343346118927002
train_iter_loss: 0.16808609664440155
train_iter_loss: 0.04636985436081886
train_iter_loss: 0.06429506093263626
train_iter_loss: 0.07034818083047867
train_iter_loss: 0.1326458901166916
train_iter_loss: 0.09597236663103104
train_iter_loss: 0.1918245106935501
train_iter_loss: 0.17987529933452606
train_iter_loss: 0.1748267561197281
train_iter_loss: 0.1275264471769333
train_iter_loss: 0.15491065382957458
train_iter_loss: 0.15294897556304932
train_iter_loss: 0.197897270321846
train_iter_loss: 0.1642882525920868
train_iter_loss: 0.30211934447288513
train_iter_loss: 0.23099388182163239
train_iter_loss: 0.09612961113452911
train_iter_loss: 0.2183316946029663
train_iter_loss: 0.1667640209197998
train_iter_loss: 0.05654201656579971
train_iter_loss: 0.09909388422966003
train_iter_loss: 0.05273430049419403
train_iter_loss: 0.15666836500167847
train_iter_loss: 0.11933284997940063
train_iter_loss: 0.17938587069511414
train_iter_loss: 0.18898294866085052
train_iter_loss: 0.12379024177789688
train_iter_loss: 0.0824916660785675
train_iter_loss: 0.31294625997543335
train_iter_loss: 0.15538005530834198
train_iter_loss: 0.07508416473865509
train_iter_loss: 0.2227737307548523
train_iter_loss: 0.08657009154558182
train_iter_loss: 0.197078675031662
train loss :0.1564
---------------------
Validation seg loss: 0.218104537576437 at epoch 722
epoch =    723/  1000, exp = train
train_iter_loss: 0.07928478717803955
train_iter_loss: 0.2510562539100647
train_iter_loss: 0.11390791833400726
train_iter_loss: 0.02338554710149765
train_iter_loss: 0.22674952447414398
train_iter_loss: 0.07885973900556564
train_iter_loss: 0.06608356535434723
train_iter_loss: 0.133114293217659
train_iter_loss: 0.05878139287233353
train_iter_loss: 0.10804519802331924
train_iter_loss: 0.10790430754423141
train_iter_loss: 0.11395195126533508
train_iter_loss: 0.1421518474817276
train_iter_loss: 0.10626406222581863
train_iter_loss: 0.026006802916526794
train_iter_loss: 0.18054330348968506
train_iter_loss: 0.15790687501430511
train_iter_loss: 0.23273096978664398
train_iter_loss: 0.255798876285553
train_iter_loss: 0.10321240872144699
train_iter_loss: 0.20912224054336548
train_iter_loss: 0.24322089552879333
train_iter_loss: 0.12905023992061615
train_iter_loss: 0.09929624944925308
train_iter_loss: 0.19594553112983704
train_iter_loss: 0.2648458480834961
train_iter_loss: 0.22848686575889587
train_iter_loss: 0.1362491399049759
train_iter_loss: 0.17068789899349213
train_iter_loss: 0.18861287832260132
train_iter_loss: 0.20357206463813782
train_iter_loss: 0.21231885254383087
train_iter_loss: 0.15912966430187225
train_iter_loss: 0.12045345455408096
train_iter_loss: 0.15153852105140686
train_iter_loss: 0.11577663570642471
train_iter_loss: 0.2326672524213791
train_iter_loss: 0.07428360730409622
train_iter_loss: 0.12384140491485596
train_iter_loss: 0.2149873822927475
train_iter_loss: 0.14579589664936066
train_iter_loss: 0.11810871958732605
train_iter_loss: 0.12831757962703705
train_iter_loss: 0.10967305302619934
train_iter_loss: 0.17535798251628876
train_iter_loss: 0.14826394617557526
train_iter_loss: 0.27163687348365784
train_iter_loss: 0.23132890462875366
train_iter_loss: 0.1392514854669571
train_iter_loss: 0.15209509432315826
train_iter_loss: 0.11457745730876923
train_iter_loss: 0.1037004217505455
train_iter_loss: 0.2597995698451996
train_iter_loss: 0.19199258089065552
train_iter_loss: 0.29596471786499023
train_iter_loss: 0.2750735580921173
train_iter_loss: 0.13243326544761658
train_iter_loss: 0.09404116868972778
train_iter_loss: 0.2250121384859085
train_iter_loss: 0.17097607254981995
train_iter_loss: 0.13962921500205994
train_iter_loss: 0.18439197540283203
train_iter_loss: 0.1002812311053276
train_iter_loss: 0.15422114729881287
train_iter_loss: 0.15248966217041016
train_iter_loss: 0.11209329217672348
train_iter_loss: 0.10750745981931686
train_iter_loss: 0.2583896815776825
train_iter_loss: 0.22389823198318481
train_iter_loss: 0.17529821395874023
train_iter_loss: 0.10707469284534454
train_iter_loss: 0.09970322996377945
train_iter_loss: 0.2614435851573944
train_iter_loss: 0.17483249306678772
train_iter_loss: 0.12740130722522736
train_iter_loss: 0.43852758407592773
train_iter_loss: 0.25255316495895386
train_iter_loss: 0.16777054965496063
train_iter_loss: 0.12669791281223297
train_iter_loss: 0.1150510311126709
train_iter_loss: 0.10798663645982742
train_iter_loss: 0.22095708549022675
train_iter_loss: 0.2048797756433487
train_iter_loss: 0.10776036977767944
train_iter_loss: 0.19947460293769836
train_iter_loss: 0.16589519381523132
train_iter_loss: 0.30803781747817993
train_iter_loss: 0.09331441670656204
train_iter_loss: 0.14850091934204102
train_iter_loss: 0.210599884390831
train_iter_loss: 0.08861730247735977
train_iter_loss: 0.1416560560464859
train_iter_loss: 0.13645359873771667
train_iter_loss: 0.05330590531229973
train_iter_loss: 0.14554743468761444
train_iter_loss: 0.1298397332429886
train_iter_loss: 0.2254520058631897
train_iter_loss: 0.14964620769023895
train_iter_loss: 0.11442071944475174
train_iter_loss: 0.167197585105896
train loss :0.1628
---------------------
Validation seg loss: 0.21104720763791845 at epoch 723
epoch =    724/  1000, exp = train
train_iter_loss: 0.09615875035524368
train_iter_loss: 0.17875947058200836
train_iter_loss: 0.11921360343694687
train_iter_loss: 0.1945401132106781
train_iter_loss: 0.17244437336921692
train_iter_loss: 0.21927917003631592
train_iter_loss: 0.05763429030776024
train_iter_loss: 0.21785537898540497
train_iter_loss: 0.1134297326207161
train_iter_loss: 0.15422622859477997
train_iter_loss: 0.1393921673297882
train_iter_loss: 0.06213987246155739
train_iter_loss: 0.08764175325632095
train_iter_loss: 0.059134986251592636
train_iter_loss: 0.2501717209815979
train_iter_loss: 0.23079152405261993
train_iter_loss: 0.18598659336566925
train_iter_loss: 0.19454587996006012
train_iter_loss: 0.25105535984039307
train_iter_loss: 0.1503281593322754
train_iter_loss: 0.21882379055023193
train_iter_loss: 0.1317359358072281
train_iter_loss: 0.15477654337882996
train_iter_loss: 0.096685491502285
train_iter_loss: 0.07166071981191635
train_iter_loss: 0.17037692666053772
train_iter_loss: 0.3029647171497345
train_iter_loss: 0.09914588183164597
train_iter_loss: 0.10667338967323303
train_iter_loss: 0.1846453994512558
train_iter_loss: 0.14238356053829193
train_iter_loss: 0.14443466067314148
train_iter_loss: 0.20591382682323456
train_iter_loss: 0.2916964292526245
train_iter_loss: 0.03295418992638588
train_iter_loss: 0.11277085542678833
train_iter_loss: 0.06479440629482269
train_iter_loss: 0.07142022997140884
train_iter_loss: 0.27904438972473145
train_iter_loss: 0.1762157678604126
train_iter_loss: 0.10559187829494476
train_iter_loss: 0.10958058387041092
train_iter_loss: 0.1181328222155571
train_iter_loss: 0.10874135047197342
train_iter_loss: 0.12001732736825943
train_iter_loss: 0.061655279248952866
train_iter_loss: 0.09068047255277634
train_iter_loss: 0.04961581900715828
train_iter_loss: 0.15279391407966614
train_iter_loss: 0.3944033980369568
train_iter_loss: 0.11665727198123932
train_iter_loss: 0.05832703411579132
train_iter_loss: 0.1315404772758484
train_iter_loss: 0.1841547042131424
train_iter_loss: 0.22322233021259308
train_iter_loss: 0.13512925803661346
train_iter_loss: 0.1213892251253128
train_iter_loss: 0.24729107320308685
train_iter_loss: 0.27537938952445984
train_iter_loss: 0.18375220894813538
train_iter_loss: 0.22182445228099823
train_iter_loss: 0.11339329183101654
train_iter_loss: 0.1722804307937622
train_iter_loss: 0.16541635990142822
train_iter_loss: 0.32739949226379395
train_iter_loss: 0.11855501681566238
train_iter_loss: 0.26998192071914673
train_iter_loss: 0.11881192028522491
train_iter_loss: 0.20157679915428162
train_iter_loss: 0.13146086037158966
train_iter_loss: 0.1589515060186386
train_iter_loss: 0.0753701776266098
train_iter_loss: 0.13850688934326172
train_iter_loss: 0.10377959161996841
train_iter_loss: 0.15805849432945251
train_iter_loss: 0.13491269946098328
train_iter_loss: 0.22856810688972473
train_iter_loss: 0.07051162421703339
train_iter_loss: 0.06352046877145767
train_iter_loss: 0.14838719367980957
train_iter_loss: 0.16773349046707153
train_iter_loss: 0.17323824763298035
train_iter_loss: 0.14310483634471893
train_iter_loss: 0.32427477836608887
train_iter_loss: 0.17491178214550018
train_iter_loss: 0.1473434716463089
train_iter_loss: 0.1935472935438156
train_iter_loss: 0.13768790662288666
train_iter_loss: 0.0933205634355545
train_iter_loss: 0.16712090373039246
train_iter_loss: 0.20592613518238068
train_iter_loss: 0.2077440321445465
train_iter_loss: 0.30207499861717224
train_iter_loss: 0.06966251879930496
train_iter_loss: 0.1188393160700798
train_iter_loss: 0.14434143900871277
train_iter_loss: 0.2153037041425705
train_iter_loss: 0.18407335877418518
train_iter_loss: 0.20090876519680023
train_iter_loss: 0.16980217397212982
train loss :0.1601
---------------------
Validation seg loss: 0.2160639159012375 at epoch 724
epoch =    725/  1000, exp = train
train_iter_loss: 0.14730249345302582
train_iter_loss: 0.14070621132850647
train_iter_loss: 0.15824362635612488
train_iter_loss: 0.11696171015501022
train_iter_loss: 0.15017135441303253
train_iter_loss: 0.11752840131521225
train_iter_loss: 0.11878477036952972
train_iter_loss: 0.15121564269065857
train_iter_loss: 0.5746217966079712
train_iter_loss: 0.16150318086147308
train_iter_loss: 0.19214189052581787
train_iter_loss: 0.124643474817276
train_iter_loss: 0.0857752338051796
train_iter_loss: 0.198496475815773
train_iter_loss: 0.2533285915851593
train_iter_loss: 0.1122291311621666
train_iter_loss: 0.09746167808771133
train_iter_loss: 0.09799366444349289
train_iter_loss: 0.0766390934586525
train_iter_loss: 0.20383043587207794
train_iter_loss: 0.03700299933552742
train_iter_loss: 0.1938437521457672
train_iter_loss: 0.18838536739349365
train_iter_loss: 0.21223057806491852
train_iter_loss: 0.12640118598937988
train_iter_loss: 0.18126416206359863
train_iter_loss: 0.16547968983650208
train_iter_loss: 0.1884034425020218
train_iter_loss: 0.07225406914949417
train_iter_loss: 0.13973794877529144
train_iter_loss: 0.1186353787779808
train_iter_loss: 0.04326740279793739
train_iter_loss: 0.1558827906847
train_iter_loss: 0.07912146300077438
train_iter_loss: 0.09944603592157364
train_iter_loss: 0.13993750512599945
train_iter_loss: 0.23834410309791565
train_iter_loss: 0.4143091142177582
train_iter_loss: 0.09413931518793106
train_iter_loss: 0.2698768675327301
train_iter_loss: 0.1382959932088852
train_iter_loss: 0.1454073041677475
train_iter_loss: 0.2070195972919464
train_iter_loss: 0.14994296431541443
train_iter_loss: 0.16791750490665436
train_iter_loss: 0.2109043449163437
train_iter_loss: 0.19937796890735626
train_iter_loss: 0.08391892164945602
train_iter_loss: 0.15463703870773315
train_iter_loss: 0.12652109563350677
train_iter_loss: 0.16082067787647247
train_iter_loss: 0.10265037417411804
train_iter_loss: 0.09007758647203445
train_iter_loss: 0.18398119509220123
train_iter_loss: 0.16198323667049408
train_iter_loss: 0.07813103497028351
train_iter_loss: 0.19918455183506012
train_iter_loss: 0.2385263293981552
train_iter_loss: 0.11442648619413376
train_iter_loss: 0.2930442690849304
train_iter_loss: 0.25032567977905273
train_iter_loss: 0.25355786085128784
train_iter_loss: 0.10577217489480972
train_iter_loss: 0.09473660588264465
train_iter_loss: 0.0920175090432167
train_iter_loss: 0.0820879340171814
train_iter_loss: 0.11244235187768936
train_iter_loss: 0.1319388449192047
train_iter_loss: 0.1430712193250656
train_iter_loss: 0.4508688449859619
train_iter_loss: 0.12888464331626892
train_iter_loss: 0.22466608881950378
train_iter_loss: 0.15641285479068756
train_iter_loss: 0.17907702922821045
train_iter_loss: 0.12634499371051788
train_iter_loss: 0.16592352092266083
train_iter_loss: 0.13069956004619598
train_iter_loss: 0.11840717494487762
train_iter_loss: 0.2231075018644333
train_iter_loss: 0.21157608926296234
train_iter_loss: 0.07697644084692001
train_iter_loss: 0.1740354746580124
train_iter_loss: 0.22019639611244202
train_iter_loss: 0.21671627461910248
train_iter_loss: 0.21689468622207642
train_iter_loss: 0.07837159931659698
train_iter_loss: 0.15392950177192688
train_iter_loss: 0.07931796461343765
train_iter_loss: 0.13903042674064636
train_iter_loss: 0.08589117974042892
train_iter_loss: 0.14329726994037628
train_iter_loss: 0.2628387212753296
train_iter_loss: 0.13717615604400635
train_iter_loss: 0.15443459153175354
train_iter_loss: 0.21097950637340546
train_iter_loss: 0.27503326535224915
train_iter_loss: 0.21173226833343506
train_iter_loss: 0.14397595822811127
train_iter_loss: 0.07635944336652756
train_iter_loss: 0.2580866515636444
train loss :0.1650
---------------------
Validation seg loss: 0.2128779090027202 at epoch 725
epoch =    726/  1000, exp = train
train_iter_loss: 0.34941476583480835
train_iter_loss: 0.1734156459569931
train_iter_loss: 0.1991885006427765
train_iter_loss: 0.10872919857501984
train_iter_loss: 0.0748278871178627
train_iter_loss: 0.13099995255470276
train_iter_loss: 0.1794699877500534
train_iter_loss: 0.23368249833583832
train_iter_loss: 0.22946949303150177
train_iter_loss: 0.16706915199756622
train_iter_loss: 0.15580447018146515
train_iter_loss: 0.08389504998922348
train_iter_loss: 0.11513346433639526
train_iter_loss: 0.14337940514087677
train_iter_loss: 0.1217515841126442
train_iter_loss: 0.12961532175540924
train_iter_loss: 0.07931837439537048
train_iter_loss: 0.2608819901943207
train_iter_loss: 0.14836668968200684
train_iter_loss: 0.4470784664154053
train_iter_loss: 0.13982313871383667
train_iter_loss: 0.21236887574195862
train_iter_loss: 0.14542272686958313
train_iter_loss: 0.11608666926622391
train_iter_loss: 0.17060357332229614
train_iter_loss: 0.1976868063211441
train_iter_loss: 0.13864737749099731
train_iter_loss: 0.12124957144260406
train_iter_loss: 0.18995454907417297
train_iter_loss: 0.15019331872463226
train_iter_loss: 0.14945530891418457
train_iter_loss: 0.229008287191391
train_iter_loss: 0.2197350263595581
train_iter_loss: 0.17970870435237885
train_iter_loss: 0.11243907362222672
train_iter_loss: 0.11932674795389175
train_iter_loss: 0.14553683996200562
train_iter_loss: 0.14259234070777893
train_iter_loss: 0.0289726834744215
train_iter_loss: 0.1698007732629776
train_iter_loss: 0.1463165283203125
train_iter_loss: 0.11713522672653198
train_iter_loss: 0.19163580238819122
train_iter_loss: 0.20609769225120544
train_iter_loss: 0.321854829788208
train_iter_loss: 0.15257097780704498
train_iter_loss: 0.11635138839483261
train_iter_loss: 0.20251625776290894
train_iter_loss: 0.1184573769569397
train_iter_loss: 0.19375298917293549
train_iter_loss: 0.1175512745976448
train_iter_loss: 0.44556882977485657
train_iter_loss: 0.14330922067165375
train_iter_loss: 0.11065178364515305
train_iter_loss: 0.17921504378318787
train_iter_loss: 0.15486599504947662
train_iter_loss: 0.11984289437532425
train_iter_loss: 0.1444069743156433
train_iter_loss: 0.09581592679023743
train_iter_loss: 0.17034068703651428
train_iter_loss: 0.09371799230575562
train_iter_loss: 0.2343742549419403
train_iter_loss: 0.23002037405967712
train_iter_loss: 0.1260252594947815
train_iter_loss: 0.4395884573459625
train_iter_loss: 0.10114011913537979
train_iter_loss: 0.305998295545578
train_iter_loss: 0.14990898966789246
train_iter_loss: 0.10360785573720932
train_iter_loss: 0.12800255417823792
train_iter_loss: 0.15441960096359253
train_iter_loss: 0.14226509630680084
train_iter_loss: 0.026326294988393784
train_iter_loss: 0.09152761846780777
train_iter_loss: 0.09678521007299423
train_iter_loss: 0.29139405488967896
train_iter_loss: 0.26374855637550354
train_iter_loss: 0.15178513526916504
train_iter_loss: 0.26691803336143494
train_iter_loss: 0.20215262472629547
train_iter_loss: 0.1420173943042755
train_iter_loss: 0.09503167867660522
train_iter_loss: 0.1547633409500122
train_iter_loss: 0.13621951639652252
train_iter_loss: 0.05830622836947441
train_iter_loss: 0.1954336017370224
train_iter_loss: 0.1462354063987732
train_iter_loss: 0.09420689940452576
train_iter_loss: 0.13782961666584015
train_iter_loss: 0.14532078802585602
train_iter_loss: 0.10966019332408905
train_iter_loss: 0.12075381726026535
train_iter_loss: 0.09382262080907822
train_iter_loss: 0.07945853471755981
train_iter_loss: 0.23369048535823822
train_iter_loss: 0.2024127095937729
train_iter_loss: 0.28228673338890076
train_iter_loss: 0.06777532398700714
train_iter_loss: 0.07236338406801224
train_iter_loss: 0.21648606657981873
train loss :0.1661
---------------------
Validation seg loss: 0.21492328643271663 at epoch 726
epoch =    727/  1000, exp = train
train_iter_loss: 0.2328837811946869
train_iter_loss: 0.15590651333332062
train_iter_loss: 0.15673880279064178
train_iter_loss: 0.15861079096794128
train_iter_loss: 0.1085830107331276
train_iter_loss: 0.07093604654073715
train_iter_loss: 0.1278427541255951
train_iter_loss: 0.11819899827241898
train_iter_loss: 0.23036834597587585
train_iter_loss: 0.13063852488994598
train_iter_loss: 0.26212722063064575
train_iter_loss: 0.1034083217382431
train_iter_loss: 0.12174626439809799
train_iter_loss: 0.18582691252231598
train_iter_loss: 0.10128352791070938
train_iter_loss: 0.22880913317203522
train_iter_loss: 0.14172548055648804
train_iter_loss: 0.25760698318481445
train_iter_loss: 0.1320309191942215
train_iter_loss: 0.19433042407035828
train_iter_loss: 0.14450229704380035
train_iter_loss: 0.07206837832927704
train_iter_loss: 0.1912139356136322
train_iter_loss: 0.08458234369754791
train_iter_loss: 0.291471391916275
train_iter_loss: 0.11441212892532349
train_iter_loss: 0.3294385075569153
train_iter_loss: 0.2731800973415375
train_iter_loss: 0.10596828162670135
train_iter_loss: 0.26931363344192505
train_iter_loss: 0.35567429661750793
train_iter_loss: 0.11989010870456696
train_iter_loss: 0.10725590586662292
train_iter_loss: 0.25056588649749756
train_iter_loss: 0.17783497273921967
train_iter_loss: 0.19650165736675262
train_iter_loss: 0.07152805477380753
train_iter_loss: 0.2026384323835373
train_iter_loss: 0.07813695818185806
train_iter_loss: 0.14456620812416077
train_iter_loss: 0.226258784532547
train_iter_loss: 0.07973471283912659
train_iter_loss: 0.04348257929086685
train_iter_loss: 0.10409174114465714
train_iter_loss: 0.14928275346755981
train_iter_loss: 0.16475272178649902
train_iter_loss: 0.1137901097536087
train_iter_loss: 0.17183314263820648
train_iter_loss: 0.17341911792755127
train_iter_loss: 0.1499858945608139
train_iter_loss: 0.1144140437245369
train_iter_loss: 0.12502095103263855
train_iter_loss: 0.21109721064567566
train_iter_loss: 0.21304172277450562
train_iter_loss: 0.09572652727365494
train_iter_loss: 0.12905578315258026
train_iter_loss: 0.10122406482696533
train_iter_loss: 0.12938831746578217
train_iter_loss: 0.3011782765388489
train_iter_loss: 0.1564166247844696
train_iter_loss: 0.20320816338062286
train_iter_loss: 0.06931401044130325
train_iter_loss: 0.13226951658725739
train_iter_loss: 0.1613650918006897
train_iter_loss: 0.15422086417675018
train_iter_loss: 0.13467808067798615
train_iter_loss: 0.11954034119844437
train_iter_loss: 0.10823594778776169
train_iter_loss: 0.21198591589927673
train_iter_loss: 0.2224486768245697
train_iter_loss: 0.14631985127925873
train_iter_loss: 0.22730228304862976
train_iter_loss: 0.2336922436952591
train_iter_loss: 0.09364701807498932
train_iter_loss: 0.22802895307540894
train_iter_loss: 0.1298074573278427
train_iter_loss: 0.09866562485694885
train_iter_loss: 0.2368505895137787
train_iter_loss: 0.2592411935329437
train_iter_loss: 0.1831534504890442
train_iter_loss: 0.25562289357185364
train_iter_loss: 0.17845065891742706
train_iter_loss: 0.15346179902553558
train_iter_loss: 0.08951084315776825
train_iter_loss: 0.10721413791179657
train_iter_loss: 0.1536930948495865
train_iter_loss: 0.4162261188030243
train_iter_loss: 0.13484731316566467
train_iter_loss: 0.19600510597229004
train_iter_loss: 0.1852278858423233
train_iter_loss: 0.08748842030763626
train_iter_loss: 0.12271641194820404
train_iter_loss: 0.11855500936508179
train_iter_loss: 0.17458923161029816
train_iter_loss: 0.06273557245731354
train_iter_loss: 0.17666703462600708
train_iter_loss: 0.15917375683784485
train_iter_loss: 0.1181151494383812
train_iter_loss: 0.1854887157678604
train_iter_loss: 0.06438150256872177
train loss :0.1641
---------------------
Validation seg loss: 0.21516800763190919 at epoch 727
epoch =    728/  1000, exp = train
train_iter_loss: 0.1680888533592224
train_iter_loss: 0.20986159145832062
train_iter_loss: 0.10442370176315308
train_iter_loss: 0.20731590688228607
train_iter_loss: 0.09682200103998184
train_iter_loss: 0.17060160636901855
train_iter_loss: 0.11434969305992126
train_iter_loss: 0.09007315337657928
train_iter_loss: 0.15867196023464203
train_iter_loss: 0.11223858594894409
train_iter_loss: 0.2083895206451416
train_iter_loss: 0.08314566314220428
train_iter_loss: 0.14076076447963715
train_iter_loss: 0.08573200553655624
train_iter_loss: 0.17800232768058777
train_iter_loss: 0.18811847269535065
train_iter_loss: 0.16321830451488495
train_iter_loss: 0.1337394416332245
train_iter_loss: 0.18052229285240173
train_iter_loss: 0.17904776334762573
train_iter_loss: 0.13691134750843048
train_iter_loss: 0.1857030987739563
train_iter_loss: 0.1596110314130783
train_iter_loss: 0.11100099235773087
train_iter_loss: 0.24456438422203064
train_iter_loss: 0.1724197268486023
train_iter_loss: 0.14605608582496643
train_iter_loss: 0.11267868429422379
train_iter_loss: 0.18666480481624603
train_iter_loss: 0.2246466726064682
train_iter_loss: 0.10740701854228973
train_iter_loss: 0.20926372706890106
train_iter_loss: 0.11424696445465088
train_iter_loss: 0.17760175466537476
train_iter_loss: 0.2122034728527069
train_iter_loss: 0.17129267752170563
train_iter_loss: 0.0597684271633625
train_iter_loss: 0.09745442867279053
train_iter_loss: 0.13278613984584808
train_iter_loss: 0.19228684902191162
train_iter_loss: 0.18984773755073547
train_iter_loss: 0.127708300948143
train_iter_loss: 0.18999755382537842
train_iter_loss: 0.1645113080739975
train_iter_loss: 0.08809098601341248
train_iter_loss: 0.2017156332731247
train_iter_loss: 0.14339689910411835
train_iter_loss: 0.1928834766149521
train_iter_loss: 0.1315138339996338
train_iter_loss: 0.1318640261888504
train_iter_loss: 0.11675042659044266
train_iter_loss: 0.2148575335741043
train_iter_loss: 0.06969863921403885
train_iter_loss: 0.21522095799446106
train_iter_loss: 0.21429644525051117
train_iter_loss: 0.09357300400733948
train_iter_loss: 0.1798042505979538
train_iter_loss: 0.15500830113887787
train_iter_loss: 0.24959051609039307
train_iter_loss: 0.05473924055695534
train_iter_loss: 0.10888347029685974
train_iter_loss: 0.23678956925868988
train_iter_loss: 0.21855607628822327
train_iter_loss: 0.06287863105535507
train_iter_loss: 0.10152696818113327
train_iter_loss: 0.10410556942224503
train_iter_loss: 0.2403322160243988
train_iter_loss: 0.20119020342826843
train_iter_loss: 0.13276563584804535
train_iter_loss: 0.16733363270759583
train_iter_loss: 0.11923115700483322
train_iter_loss: 0.17855939269065857
train_iter_loss: 0.1155594065785408
train_iter_loss: 0.05350446701049805
train_iter_loss: 0.0560966432094574
train_iter_loss: 0.11532005667686462
train_iter_loss: 0.05859380215406418
train_iter_loss: 0.11662480980157852
train_iter_loss: 0.1837884783744812
train_iter_loss: 0.2441689372062683
train_iter_loss: 0.14165061712265015
train_iter_loss: 0.10377993434667587
train_iter_loss: 0.053051311522722244
train_iter_loss: 0.0789756178855896
train_iter_loss: 0.11675725877285004
train_iter_loss: 0.16306914389133453
train_iter_loss: 0.16925975680351257
train_iter_loss: 0.22072145342826843
train_iter_loss: 0.17819814383983612
train_iter_loss: 0.1423163264989853
train_iter_loss: 0.0953880101442337
train_iter_loss: 0.0835317000746727
train_iter_loss: 0.22069725394248962
train_iter_loss: 0.18556703627109528
train_iter_loss: 0.17315231263637543
train_iter_loss: 0.2298799306154251
train_iter_loss: 0.21448858082294464
train_iter_loss: 0.02220166102051735
train_iter_loss: 0.1488247662782669
train_iter_loss: 0.19217823445796967
train loss :0.1517
---------------------
Validation seg loss: 0.2207743798488013 at epoch 728
epoch =    729/  1000, exp = train
train_iter_loss: 0.18789328634738922
train_iter_loss: 0.18578915297985077
train_iter_loss: 0.16376498341560364
train_iter_loss: 0.15188546478748322
train_iter_loss: 0.13563019037246704
train_iter_loss: 0.12629015743732452
train_iter_loss: 0.16330373287200928
train_iter_loss: 0.0914573073387146
train_iter_loss: 0.14179627597332
train_iter_loss: 0.07234162092208862
train_iter_loss: 0.04784237965941429
train_iter_loss: 0.1472896784543991
train_iter_loss: 0.17060275375843048
train_iter_loss: 0.10513044148683548
train_iter_loss: 0.1020725816488266
train_iter_loss: 0.13858339190483093
train_iter_loss: 0.09382275491952896
train_iter_loss: 0.11109250783920288
train_iter_loss: 0.2396388053894043
train_iter_loss: 0.08898445963859558
train_iter_loss: 0.1216840073466301
train_iter_loss: 0.12783531844615936
train_iter_loss: 0.3103885352611542
train_iter_loss: 0.22776135802268982
train_iter_loss: 0.14851221442222595
train_iter_loss: 0.16342398524284363
train_iter_loss: 0.14045439660549164
train_iter_loss: 0.05967169627547264
train_iter_loss: 0.13881894946098328
train_iter_loss: 0.2093876153230667
train_iter_loss: 0.08060804009437561
train_iter_loss: 0.1232493668794632
train_iter_loss: 0.25572553277015686
train_iter_loss: 0.2321338653564453
train_iter_loss: 0.15022850036621094
train_iter_loss: 0.21774034202098846
train_iter_loss: 0.19942575693130493
train_iter_loss: 0.2464171200990677
train_iter_loss: 0.23486930131912231
train_iter_loss: 0.18853561580181122
train_iter_loss: 0.22216270864009857
train_iter_loss: 0.114589162170887
train_iter_loss: 0.1900465190410614
train_iter_loss: 0.17667940258979797
train_iter_loss: 0.11983494460582733
train_iter_loss: 0.15722116827964783
train_iter_loss: 0.22422650456428528
train_iter_loss: 0.13947483897209167
train_iter_loss: 0.5688663721084595
train_iter_loss: 0.15579093992710114
train_iter_loss: 0.12113167345523834
train_iter_loss: 0.21539321541786194
train_iter_loss: 0.14543427526950836
train_iter_loss: 0.0751613900065422
train_iter_loss: 0.2314499169588089
train_iter_loss: 0.12653514742851257
train_iter_loss: 0.21631404757499695
train_iter_loss: 0.1344311386346817
train_iter_loss: 0.3024199604988098
train_iter_loss: 0.12108762562274933
train_iter_loss: 0.18387988209724426
train_iter_loss: 0.14735807478427887
train_iter_loss: 0.1779809296131134
train_iter_loss: 0.1314050704240799
train_iter_loss: 0.12285257130861282
train_iter_loss: 0.11083105206489563
train_iter_loss: 0.3675065338611603
train_iter_loss: 0.12243671715259552
train_iter_loss: 0.26075589656829834
train_iter_loss: 0.1292002946138382
train_iter_loss: 0.19832168519496918
train_iter_loss: 0.16285713016986847
train_iter_loss: 0.0906502977013588
train_iter_loss: 0.10739000886678696
train_iter_loss: 0.11377716809511185
train_iter_loss: 0.24486103653907776
train_iter_loss: 0.10212136805057526
train_iter_loss: 0.19366976618766785
train_iter_loss: 0.19996210932731628
train_iter_loss: 0.08675862103700638
train_iter_loss: 0.2730799615383148
train_iter_loss: 0.1764364391565323
train_iter_loss: 0.10148017108440399
train_iter_loss: 0.19465580582618713
train_iter_loss: 0.108847975730896
train_iter_loss: 0.130573108792305
train_iter_loss: 0.09691951423883438
train_iter_loss: 0.1745244711637497
train_iter_loss: 0.23877152800559998
train_iter_loss: 0.22925201058387756
train_iter_loss: 0.16654296219348907
train_iter_loss: 0.13368099927902222
train_iter_loss: 0.09908454865217209
train_iter_loss: 0.14880213141441345
train_iter_loss: 0.17041781544685364
train_iter_loss: 0.1260407567024231
train_iter_loss: 0.10791970789432526
train_iter_loss: 0.12482058256864548
train_iter_loss: 0.13863253593444824
train_iter_loss: 0.0822560116648674
train loss :0.1643
---------------------
Validation seg loss: 0.21466234161184644 at epoch 729
epoch =    730/  1000, exp = train
train_iter_loss: 0.11854061484336853
train_iter_loss: 0.2650783658027649
train_iter_loss: 0.26343291997909546
train_iter_loss: 0.18094690144062042
train_iter_loss: 0.2046913057565689
train_iter_loss: 0.1868053823709488
train_iter_loss: 0.1547788828611374
train_iter_loss: 0.07677596807479858
train_iter_loss: 0.1436636745929718
train_iter_loss: 0.43913134932518005
train_iter_loss: 0.12554621696472168
train_iter_loss: 0.12246570736169815
train_iter_loss: 0.13680116832256317
train_iter_loss: 0.17574961483478546
train_iter_loss: 0.050126004964113235
train_iter_loss: 0.19038228690624237
train_iter_loss: 0.09619467705488205
train_iter_loss: 0.10288601368665695
train_iter_loss: 0.09520084410905838
train_iter_loss: 0.17166562378406525
train_iter_loss: 0.08266732096672058
train_iter_loss: 0.16560788452625275
train_iter_loss: 0.1253868192434311
train_iter_loss: 0.152235746383667
train_iter_loss: 0.1208135187625885
train_iter_loss: 0.17955005168914795
train_iter_loss: 0.08461212366819382
train_iter_loss: 0.19752749800682068
train_iter_loss: 0.23138192296028137
train_iter_loss: 0.12327559292316437
train_iter_loss: 0.15922893583774567
train_iter_loss: 0.13978025317192078
train_iter_loss: 0.1375235766172409
train_iter_loss: 0.14245368540287018
train_iter_loss: 0.05295887216925621
train_iter_loss: 0.05392470210790634
train_iter_loss: 0.12853898108005524
train_iter_loss: 0.17738507688045502
train_iter_loss: 0.1799716204404831
train_iter_loss: 0.16139911115169525
train_iter_loss: 0.12743797898292542
train_iter_loss: 0.06205587089061737
train_iter_loss: 0.07189685851335526
train_iter_loss: 0.11304532736539841
train_iter_loss: 0.183151975274086
train_iter_loss: 0.1589677333831787
train_iter_loss: 0.19831599295139313
train_iter_loss: 0.09671445935964584
train_iter_loss: 0.2096167355775833
train_iter_loss: 0.1359989047050476
train_iter_loss: 0.14753393828868866
train_iter_loss: 0.14215755462646484
train_iter_loss: 0.1574230194091797
train_iter_loss: 0.19190774857997894
train_iter_loss: 0.20872430503368378
train_iter_loss: 0.12260925769805908
train_iter_loss: 0.12798789143562317
train_iter_loss: 0.11864530295133591
train_iter_loss: 0.17210127413272858
train_iter_loss: 0.18650586903095245
train_iter_loss: 0.13465653359889984
train_iter_loss: 0.13869842886924744
train_iter_loss: 0.16820187866687775
train_iter_loss: 0.235458105802536
train_iter_loss: 0.18514156341552734
train_iter_loss: 0.06683898717164993
train_iter_loss: 0.16733720898628235
train_iter_loss: 0.2781909108161926
train_iter_loss: 0.25875744223594666
train_iter_loss: 0.12216779589653015
train_iter_loss: 0.12354863435029984
train_iter_loss: 0.17057792842388153
train_iter_loss: 0.24080166220664978
train_iter_loss: 0.15444056689739227
train_iter_loss: 0.10080517828464508
train_iter_loss: 0.2593950033187866
train_iter_loss: 0.0783185064792633
train_iter_loss: 0.18697817623615265
train_iter_loss: 0.07527617365121841
train_iter_loss: 0.078526072204113
train_iter_loss: 0.13426454365253448
train_iter_loss: 0.2780223786830902
train_iter_loss: 0.15776808559894562
train_iter_loss: 0.10677304118871689
train_iter_loss: 0.21671177446842194
train_iter_loss: 0.18251053988933563
train_iter_loss: 0.22969260811805725
train_iter_loss: 0.11284160614013672
train_iter_loss: 0.12271743267774582
train_iter_loss: 0.11723891645669937
train_iter_loss: 0.21198590099811554
train_iter_loss: 0.20334507524967194
train_iter_loss: 0.1202269122004509
train_iter_loss: 0.11012742668390274
train_iter_loss: 0.1972537487745285
train_iter_loss: 0.15751796960830688
train_iter_loss: 0.06421218067407608
train_iter_loss: 0.18737991154193878
train_iter_loss: 0.2456766963005066
train_iter_loss: 0.29108864068984985
train loss :0.1587
---------------------
Validation seg loss: 0.2139635083335891 at epoch 730
epoch =    731/  1000, exp = train
train_iter_loss: 0.08053749054670334
train_iter_loss: 0.14293575286865234
train_iter_loss: 0.1723858118057251
train_iter_loss: 0.19314146041870117
train_iter_loss: 0.23905611038208008
train_iter_loss: 0.21118244528770447
train_iter_loss: 0.11940157413482666
train_iter_loss: 0.11174684017896652
train_iter_loss: 0.2168750911951065
train_iter_loss: 0.14215318858623505
train_iter_loss: 0.11754761636257172
train_iter_loss: 0.10182604193687439
train_iter_loss: 0.12725068628787994
train_iter_loss: 0.16397272050380707
train_iter_loss: 0.184148371219635
train_iter_loss: 0.13806092739105225
train_iter_loss: 0.3014337718486786
train_iter_loss: 0.1420457512140274
train_iter_loss: 0.15609753131866455
train_iter_loss: 0.19794471561908722
train_iter_loss: 0.524458110332489
train_iter_loss: 0.06673543900251389
train_iter_loss: 0.38464030623435974
train_iter_loss: 0.16096992790699005
train_iter_loss: 0.1986827701330185
train_iter_loss: 0.18608258664608002
train_iter_loss: 0.09590618312358856
train_iter_loss: 0.16053034365177155
train_iter_loss: 0.11903351545333862
train_iter_loss: 0.1900210827589035
train_iter_loss: 0.16564327478408813
train_iter_loss: 0.1610085666179657
train_iter_loss: 0.13692493736743927
train_iter_loss: 0.22703756392002106
train_iter_loss: 0.15982292592525482
train_iter_loss: 0.13591627776622772
train_iter_loss: 0.17650346457958221
train_iter_loss: 0.12818561494350433
train_iter_loss: 0.20624788105487823
train_iter_loss: 0.20589444041252136
train_iter_loss: 0.09642884880304337
train_iter_loss: 0.1692422777414322
train_iter_loss: 0.09616222977638245
train_iter_loss: 0.09975803643465042
train_iter_loss: 0.10814791917800903
train_iter_loss: 0.1709323227405548
train_iter_loss: 0.17987364530563354
train_iter_loss: 0.10649994015693665
train_iter_loss: 0.1548243761062622
train_iter_loss: 0.08485973626375198
train_iter_loss: 0.20477773249149323
train_iter_loss: 0.16380703449249268
train_iter_loss: 0.14837276935577393
train_iter_loss: 0.256591796875
train_iter_loss: 0.1889045089483261
train_iter_loss: 0.1080065444111824
train_iter_loss: 0.19784951210021973
train_iter_loss: 0.1695997565984726
train_iter_loss: 0.21844424307346344
train_iter_loss: 0.04025685042142868
train_iter_loss: 0.13703957200050354
train_iter_loss: 0.06882043182849884
train_iter_loss: 0.07657402753829956
train_iter_loss: 0.11918905377388
train_iter_loss: 0.19246798753738403
train_iter_loss: 0.14466744661331177
train_iter_loss: 0.23683452606201172
train_iter_loss: 0.08864030241966248
train_iter_loss: 0.14245213568210602
train_iter_loss: 0.19624002277851105
train_iter_loss: 0.11793690174818039
train_iter_loss: 0.11085227131843567
train_iter_loss: 0.1701836735010147
train_iter_loss: 0.1196296364068985
train_iter_loss: 0.096949502825737
train_iter_loss: 0.1931866854429245
train_iter_loss: 0.20885764062404633
train_iter_loss: 0.18041937053203583
train_iter_loss: 0.04319622367620468
train_iter_loss: 0.09458474069833755
train_iter_loss: 0.2605663239955902
train_iter_loss: 0.05210838094353676
train_iter_loss: 0.0757053792476654
train_iter_loss: 0.14724647998809814
train_iter_loss: 0.14138585329055786
train_iter_loss: 0.13410453498363495
train_iter_loss: 0.0994952917098999
train_iter_loss: 0.15624350309371948
train_iter_loss: 0.2310059666633606
train_iter_loss: 0.14654897153377533
train_iter_loss: 0.1894380897283554
train_iter_loss: 0.13651297986507416
train_iter_loss: 0.24801646173000336
train_iter_loss: 0.11483647674322128
train_iter_loss: 0.200813427567482
train_iter_loss: 0.12539608776569366
train_iter_loss: 0.13895824551582336
train_iter_loss: 0.09377563744783401
train_iter_loss: 0.20366548001766205
train_iter_loss: 0.08318176865577698
train loss :0.1589
---------------------
Validation seg loss: 0.2207240597081353 at epoch 731
epoch =    732/  1000, exp = train
train_iter_loss: 0.12374041974544525
train_iter_loss: 0.165922611951828
train_iter_loss: 0.09254993498325348
train_iter_loss: 0.22906148433685303
train_iter_loss: 0.08082921802997589
train_iter_loss: 0.11677394062280655
train_iter_loss: 0.23200717568397522
train_iter_loss: 0.1401292383670807
train_iter_loss: 0.2398349642753601
train_iter_loss: 0.1324489861726761
train_iter_loss: 0.2921215891838074
train_iter_loss: 0.17440302670001984
train_iter_loss: 0.1446036845445633
train_iter_loss: 0.1619720757007599
train_iter_loss: 0.13631926476955414
train_iter_loss: 0.1760290563106537
train_iter_loss: 0.16246671974658966
train_iter_loss: 0.1409653127193451
train_iter_loss: 0.11366532742977142
train_iter_loss: 0.04267568141222
train_iter_loss: 0.1611906737089157
train_iter_loss: 0.1375269740819931
train_iter_loss: 0.14257781207561493
train_iter_loss: 0.17535535991191864
train_iter_loss: 0.18599246442317963
train_iter_loss: 0.19281162321567535
train_iter_loss: 0.15121524035930634
train_iter_loss: 0.10217485576868057
train_iter_loss: 0.1246221736073494
train_iter_loss: 0.16625970602035522
train_iter_loss: 0.18523409962654114
train_iter_loss: 0.18545903265476227
train_iter_loss: 0.18595296144485474
train_iter_loss: 0.1464724987745285
train_iter_loss: 0.04983337223529816
train_iter_loss: 0.2083575576543808
train_iter_loss: 0.14003150165081024
train_iter_loss: 0.17979347705841064
train_iter_loss: 0.16075780987739563
train_iter_loss: 0.0972810685634613
train_iter_loss: 0.16429585218429565
train_iter_loss: 0.09311656653881073
train_iter_loss: 0.1566930115222931
train_iter_loss: 0.27972158789634705
train_iter_loss: 0.18730665743350983
train_iter_loss: 0.10752353817224503
train_iter_loss: 0.15239496529102325
train_iter_loss: 0.15792685747146606
train_iter_loss: 0.09515591710805893
train_iter_loss: 0.2177853286266327
train_iter_loss: 0.22628073394298553
train_iter_loss: 0.1586664617061615
train_iter_loss: 0.07775838673114777
train_iter_loss: 0.3031983971595764
train_iter_loss: 0.15137366950511932
train_iter_loss: 0.299254447221756
train_iter_loss: 0.06499280035495758
train_iter_loss: 0.2346823513507843
train_iter_loss: 0.24137547612190247
train_iter_loss: 0.12031228095293045
train_iter_loss: 0.07071938365697861
train_iter_loss: 0.18394088745117188
train_iter_loss: 0.0498737096786499
train_iter_loss: 0.09579578787088394
train_iter_loss: 0.1332552433013916
train_iter_loss: 0.07332693785429001
train_iter_loss: 0.1917249858379364
train_iter_loss: 0.12462277710437775
train_iter_loss: 0.22566474974155426
train_iter_loss: 0.2968311905860901
train_iter_loss: 0.15312470495700836
train_iter_loss: 0.11757516860961914
train_iter_loss: 0.13729313015937805
train_iter_loss: 0.1281934678554535
train_iter_loss: 0.3446506857872009
train_iter_loss: 0.11901675164699554
train_iter_loss: 0.1726396381855011
train_iter_loss: 0.14771482348442078
train_iter_loss: 0.18619109690189362
train_iter_loss: 0.15370123088359833
train_iter_loss: 0.19357416033744812
train_iter_loss: 0.09120533615350723
train_iter_loss: 0.20632554590702057
train_iter_loss: 0.07160137593746185
train_iter_loss: 0.2236616313457489
train_iter_loss: 0.21364116668701172
train_iter_loss: 0.12167965620756149
train_iter_loss: 0.20459666848182678
train_iter_loss: 0.1729532927274704
train_iter_loss: 0.1755819022655487
train_iter_loss: 0.313760370016098
train_iter_loss: 0.10089698433876038
train_iter_loss: 0.1249256283044815
train_iter_loss: 0.08643009513616562
train_iter_loss: 0.21273039281368256
train_iter_loss: 0.19945648312568665
train_iter_loss: 0.11260715872049332
train_iter_loss: 0.1661350131034851
train_iter_loss: 0.17887231707572937
train_iter_loss: 0.17880423367023468
train loss :0.1628
---------------------
Validation seg loss: 0.21673413345871387 at epoch 732
epoch =    733/  1000, exp = train
train_iter_loss: 0.24997064471244812
train_iter_loss: 0.30474013090133667
train_iter_loss: 0.25702279806137085
train_iter_loss: 0.12617704272270203
train_iter_loss: 0.13973461091518402
train_iter_loss: 0.09720508009195328
train_iter_loss: 0.1604185700416565
train_iter_loss: 0.1334414929151535
train_iter_loss: 0.15648244321346283
train_iter_loss: 0.14164632558822632
train_iter_loss: 0.14794112741947174
train_iter_loss: 0.06796807795763016
train_iter_loss: 0.14490553736686707
train_iter_loss: 0.2359125018119812
train_iter_loss: 0.24544334411621094
train_iter_loss: 0.08251290023326874
train_iter_loss: 0.056814003735780716
train_iter_loss: 0.11447005718946457
train_iter_loss: 0.1691315770149231
train_iter_loss: 0.0852949246764183
train_iter_loss: 0.3110571801662445
train_iter_loss: 0.13469292223453522
train_iter_loss: 0.11420523375272751
train_iter_loss: 0.15393126010894775
train_iter_loss: 0.1284414827823639
train_iter_loss: 0.1516391485929489
train_iter_loss: 0.07586927711963654
train_iter_loss: 0.11461010575294495
train_iter_loss: 0.14055421948432922
train_iter_loss: 0.19636081159114838
train_iter_loss: 0.1363016664981842
train_iter_loss: 0.15170592069625854
train_iter_loss: 0.10387633740901947
train_iter_loss: 0.20611271262168884
train_iter_loss: 0.09599786251783371
train_iter_loss: 0.22406771779060364
train_iter_loss: 0.17291736602783203
train_iter_loss: 0.43338826298713684
train_iter_loss: 0.11134979873895645
train_iter_loss: 0.0989476889371872
train_iter_loss: 0.1359027475118637
train_iter_loss: 0.04148925840854645
train_iter_loss: 0.18822968006134033
train_iter_loss: 0.26610860228538513
train_iter_loss: 0.20285454392433167
train_iter_loss: 0.2130846083164215
train_iter_loss: 0.1964523047208786
train_iter_loss: 0.1477627158164978
train_iter_loss: 0.17298901081085205
train_iter_loss: 0.11229062080383301
train_iter_loss: 0.18029309809207916
train_iter_loss: 0.13547317683696747
train_iter_loss: 0.06906040012836456
train_iter_loss: 0.25794848799705505
train_iter_loss: 0.1818944215774536
train_iter_loss: 0.1448514461517334
train_iter_loss: 0.15819014608860016
train_iter_loss: 0.16074970364570618
train_iter_loss: 0.15030121803283691
train_iter_loss: 0.06548375636339188
train_iter_loss: 0.2615167498588562
train_iter_loss: 0.13385321199893951
train_iter_loss: 0.10347724705934525
train_iter_loss: 0.160223126411438
train_iter_loss: 0.2364562302827835
train_iter_loss: 0.13282814621925354
train_iter_loss: 0.1286703497171402
train_iter_loss: 0.17947784066200256
train_iter_loss: 0.14890894293785095
train_iter_loss: 0.20509088039398193
train_iter_loss: 0.10692708194255829
train_iter_loss: 0.2402459979057312
train_iter_loss: 0.2185441106557846
train_iter_loss: 0.06892207264900208
train_iter_loss: 0.16807317733764648
train_iter_loss: 0.09955211728811264
train_iter_loss: 0.04497232660651207
train_iter_loss: 0.09338489919900894
train_iter_loss: 0.10383573919534683
train_iter_loss: 0.13083520531654358
train_iter_loss: 0.1797516793012619
train_iter_loss: 0.16506829857826233
train_iter_loss: 0.08921154588460922
train_iter_loss: 0.11871691048145294
train_iter_loss: 0.15600240230560303
train_iter_loss: 0.07133904844522476
train_iter_loss: 0.28029799461364746
train_iter_loss: 0.11011267453432083
train_iter_loss: 0.17638713121414185
train_iter_loss: 0.12292017042636871
train_iter_loss: 0.13081927597522736
train_iter_loss: 0.08483205735683441
train_iter_loss: 0.35988980531692505
train_iter_loss: 0.19628514349460602
train_iter_loss: 0.08587351441383362
train_iter_loss: 0.2577568292617798
train_iter_loss: 0.25061675906181335
train_iter_loss: 0.14257881045341492
train_iter_loss: 0.08312160521745682
train_iter_loss: 0.17677466571331024
train loss :0.1592
---------------------
Validation seg loss: 0.21963052028122376 at epoch 733
epoch =    734/  1000, exp = train
train_iter_loss: 0.17048779129981995
train_iter_loss: 0.35858625173568726
train_iter_loss: 0.21526826918125153
train_iter_loss: 0.13533496856689453
train_iter_loss: 0.17491863667964935
train_iter_loss: 0.1719353348016739
train_iter_loss: 0.18389835953712463
train_iter_loss: 0.23419404029846191
train_iter_loss: 0.1577407270669937
train_iter_loss: 0.22243636846542358
train_iter_loss: 0.23252439498901367
train_iter_loss: 0.14510652422904968
train_iter_loss: 0.12792757153511047
train_iter_loss: 0.20660512149333954
train_iter_loss: 0.3670186698436737
train_iter_loss: 0.1685357391834259
train_iter_loss: 0.2231290638446808
train_iter_loss: 0.05732038989663124
train_iter_loss: 0.20352624356746674
train_iter_loss: 0.10883323848247528
train_iter_loss: 0.05885983631014824
train_iter_loss: 0.12927304208278656
train_iter_loss: 0.1080489382147789
train_iter_loss: 0.14879946410655975
train_iter_loss: 0.18891428411006927
train_iter_loss: 0.09950076043605804
train_iter_loss: 0.1953800469636917
train_iter_loss: 0.17723968625068665
train_iter_loss: 0.1292543262243271
train_iter_loss: 0.12249541282653809
train_iter_loss: 0.13914847373962402
train_iter_loss: 0.13498865067958832
train_iter_loss: 0.23506484925746918
train_iter_loss: 0.12309514731168747
train_iter_loss: 0.10138507187366486
train_iter_loss: 0.3237290680408478
train_iter_loss: 0.32384076714515686
train_iter_loss: 0.12185677886009216
train_iter_loss: 0.09460122138261795
train_iter_loss: 0.1307630091905594
train_iter_loss: 0.20992924273014069
train_iter_loss: 0.2774735689163208
train_iter_loss: 0.24818867444992065
train_iter_loss: 0.21012690663337708
train_iter_loss: 0.10783807188272476
train_iter_loss: 0.25458401441574097
train_iter_loss: 0.1341199278831482
train_iter_loss: 0.10000598430633545
train_iter_loss: 0.1600327044725418
train_iter_loss: 0.1725461184978485
train_iter_loss: 0.08075776696205139
train_iter_loss: 0.2881488502025604
train_iter_loss: 0.15123485028743744
train_iter_loss: 0.21296997368335724
train_iter_loss: 0.1537138968706131
train_iter_loss: 0.10683126747608185
train_iter_loss: 0.44754868745803833
train_iter_loss: 0.2341010570526123
train_iter_loss: 0.12667395174503326
train_iter_loss: 0.12003180384635925
train_iter_loss: 0.018448354676365852
train_iter_loss: 0.18901270627975464
train_iter_loss: 0.09659874439239502
train_iter_loss: 0.10002987086772919
train_iter_loss: 0.14228571951389313
train_iter_loss: 0.2647213637828827
train_iter_loss: 0.06596153229475021
train_iter_loss: 0.19185060262680054
train_iter_loss: 0.09048391878604889
train_iter_loss: 0.19759197533130646
train_iter_loss: 0.13555696606636047
train_iter_loss: 0.10102065652608871
train_iter_loss: 0.13930825889110565
train_iter_loss: 0.07744673639535904
train_iter_loss: 0.10797503590583801
train_iter_loss: 0.09795703738927841
train_iter_loss: 0.06432216614484787
train_iter_loss: 0.07566666603088379
train_iter_loss: 0.131709486246109
train_iter_loss: 0.18449953198432922
train_iter_loss: 0.10495603084564209
train_iter_loss: 0.1831047236919403
train_iter_loss: 0.30460208654403687
train_iter_loss: 0.09130144119262695
train_iter_loss: 0.12485267221927643
train_iter_loss: 0.1349799931049347
train_iter_loss: 0.13717202842235565
train_iter_loss: 0.11920612305402756
train_iter_loss: 0.19743788242340088
train_iter_loss: 0.14968521893024445
train_iter_loss: 0.18556001782417297
train_iter_loss: 0.13412800431251526
train_iter_loss: 0.133646160364151
train_iter_loss: 0.040720805525779724
train_iter_loss: 0.18791359663009644
train_iter_loss: 0.16940684616565704
train_iter_loss: 0.08439642190933228
train_iter_loss: 0.19610367715358734
train_iter_loss: 0.12295620143413544
train_iter_loss: 0.08227287232875824
train loss :0.1626
---------------------
Validation seg loss: 0.21706795859379027 at epoch 734
epoch =    735/  1000, exp = train
train_iter_loss: 0.21540799736976624
train_iter_loss: 0.17752602696418762
train_iter_loss: 0.2875475585460663
train_iter_loss: 0.10659376531839371
train_iter_loss: 0.1905902475118637
train_iter_loss: 0.1718924194574356
train_iter_loss: 0.1402348428964615
train_iter_loss: 0.14342740178108215
train_iter_loss: 0.22075073421001434
train_iter_loss: 0.20135436952114105
train_iter_loss: 0.15383048355579376
train_iter_loss: 0.1862080693244934
train_iter_loss: 0.17074482142925262
train_iter_loss: 0.19556018710136414
train_iter_loss: 0.15418456494808197
train_iter_loss: 0.22221238911151886
train_iter_loss: 0.150151327252388
train_iter_loss: 0.1220657080411911
train_iter_loss: 0.16427868604660034
train_iter_loss: 0.23507770895957947
train_iter_loss: 0.1987249255180359
train_iter_loss: 0.052924808114767075
train_iter_loss: 0.19925832748413086
train_iter_loss: 0.1978830099105835
train_iter_loss: 0.12535201013088226
train_iter_loss: 0.12192957103252411
train_iter_loss: 0.15897442400455475
train_iter_loss: 0.10241638123989105
train_iter_loss: 0.13846828043460846
train_iter_loss: 0.13482551276683807
train_iter_loss: 0.2732938826084137
train_iter_loss: 0.16838368773460388
train_iter_loss: 0.03777702525258064
train_iter_loss: 0.11286109685897827
train_iter_loss: 0.28963425755500793
train_iter_loss: 0.07237959653139114
train_iter_loss: 0.20695364475250244
train_iter_loss: 0.1800270676612854
train_iter_loss: 0.16594845056533813
train_iter_loss: 0.12581679224967957
train_iter_loss: 0.13583813607692719
train_iter_loss: 0.22336815297603607
train_iter_loss: 0.12083239108324051
train_iter_loss: 0.12816528975963593
train_iter_loss: 0.07644669711589813
train_iter_loss: 0.25789183378219604
train_iter_loss: 0.07731786370277405
train_iter_loss: 0.12005553394556046
train_iter_loss: 0.14970798790454865
train_iter_loss: 0.13355007767677307
train_iter_loss: 0.18062812089920044
train_iter_loss: 0.07983171939849854
train_iter_loss: 0.1308278739452362
train_iter_loss: 0.0554087869822979
train_iter_loss: 0.10424740612506866
train_iter_loss: 0.094956174492836
train_iter_loss: 0.12762580811977386
train_iter_loss: 0.1681959182024002
train_iter_loss: 0.1601918488740921
train_iter_loss: 0.14001311361789703
train_iter_loss: 0.11907411366701126
train_iter_loss: 0.17925553023815155
train_iter_loss: 0.19284148514270782
train_iter_loss: 0.09874224662780762
train_iter_loss: 0.10824241489171982
train_iter_loss: 0.14255790412425995
train_iter_loss: 0.14711099863052368
train_iter_loss: 0.1407802253961563
train_iter_loss: 0.05652216076850891
train_iter_loss: 0.1445821225643158
train_iter_loss: 0.12849099934101105
train_iter_loss: 0.15185575187206268
train_iter_loss: 0.20743140578269958
train_iter_loss: 0.11964050680398941
train_iter_loss: 0.2677653729915619
train_iter_loss: 0.11143940687179565
train_iter_loss: 0.09780658781528473
train_iter_loss: 0.1268114596605301
train_iter_loss: 0.2218630164861679
train_iter_loss: 0.1518772393465042
train_iter_loss: 0.17540958523750305
train_iter_loss: 0.3105716407299042
train_iter_loss: 0.20959247648715973
train_iter_loss: 0.14942193031311035
train_iter_loss: 0.16406039893627167
train_iter_loss: 0.11263258755207062
train_iter_loss: 0.1496661901473999
train_iter_loss: 0.13261987268924713
train_iter_loss: 0.09041183441877365
train_iter_loss: 0.23635022342205048
train_iter_loss: 0.05364318564534187
train_iter_loss: 0.04539205878973007
train_iter_loss: 0.16819021105766296
train_iter_loss: 0.15871833264827728
train_iter_loss: 0.10957527905702591
train_iter_loss: 0.17764024436473846
train_iter_loss: 0.18863289058208466
train_iter_loss: 0.09679485112428665
train_iter_loss: 0.11841578781604767
train_iter_loss: 0.09758280217647552
train loss :0.1536
---------------------
Validation seg loss: 0.2137171370504459 at epoch 735
epoch =    736/  1000, exp = train
train_iter_loss: 0.18269656598567963
train_iter_loss: 0.10415279865264893
train_iter_loss: 0.1394532024860382
train_iter_loss: 0.14969629049301147
train_iter_loss: 0.07054199278354645
train_iter_loss: 0.095151387155056
train_iter_loss: 0.09660564363002777
train_iter_loss: 0.18874168395996094
train_iter_loss: 0.20100080966949463
train_iter_loss: 0.22374984622001648
train_iter_loss: 0.16852736473083496
train_iter_loss: 0.16301152110099792
train_iter_loss: 0.12045177072286606
train_iter_loss: 0.11538571864366531
train_iter_loss: 0.22979529201984406
train_iter_loss: 0.059235721826553345
train_iter_loss: 0.15898960828781128
train_iter_loss: 0.19068647921085358
train_iter_loss: 0.09559579938650131
train_iter_loss: 0.10395849496126175
train_iter_loss: 0.18893173336982727
train_iter_loss: 0.11327209323644638
train_iter_loss: 0.17980867624282837
train_iter_loss: 0.11997007578611374
train_iter_loss: 0.2314772605895996
train_iter_loss: 0.32124996185302734
train_iter_loss: 0.09261608123779297
train_iter_loss: 0.1737598329782486
train_iter_loss: 0.15009553730487823
train_iter_loss: 0.16114559769630432
train_iter_loss: 0.1583079844713211
train_iter_loss: 0.15540841221809387
train_iter_loss: 0.15883678197860718
train_iter_loss: 0.1513381153345108
train_iter_loss: 0.1583160012960434
train_iter_loss: 0.13114437460899353
train_iter_loss: 0.1437503844499588
train_iter_loss: 0.08133260905742645
train_iter_loss: 0.11145485192537308
train_iter_loss: 0.1543274223804474
train_iter_loss: 0.23302876949310303
train_iter_loss: 0.12367787212133408
train_iter_loss: 0.21479082107543945
train_iter_loss: 0.0786752924323082
train_iter_loss: 0.15382680296897888
train_iter_loss: 0.19959339499473572
train_iter_loss: 0.046002961695194244
train_iter_loss: 0.10769490152597427
train_iter_loss: 0.2342178076505661
train_iter_loss: 0.12238854914903641
train_iter_loss: 0.12702426314353943
train_iter_loss: 0.13409000635147095
train_iter_loss: 0.13395830988883972
train_iter_loss: 0.13938084244728088
train_iter_loss: 0.2054213136434555
train_iter_loss: 0.13380904495716095
train_iter_loss: 0.10834179073572159
train_iter_loss: 0.2691324055194855
train_iter_loss: 0.10450972616672516
train_iter_loss: 0.16797910630702972
train_iter_loss: 0.07057325541973114
train_iter_loss: 0.18505650758743286
train_iter_loss: 0.0764889121055603
train_iter_loss: 0.1743178367614746
train_iter_loss: 0.10167305916547775
train_iter_loss: 0.17445071041584015
train_iter_loss: 0.14705388247966766
train_iter_loss: 0.13580010831356049
train_iter_loss: 0.06771984696388245
train_iter_loss: 0.13740380108356476
train_iter_loss: 0.22525466978549957
train_iter_loss: 0.1385868936777115
train_iter_loss: 0.08018077909946442
train_iter_loss: 0.17971785366535187
train_iter_loss: 0.04045305401086807
train_iter_loss: 0.17336808145046234
train_iter_loss: 0.11280778795480728
train_iter_loss: 0.15169759094715118
train_iter_loss: 0.1594199389219284
train_iter_loss: 0.14616626501083374
train_iter_loss: 0.09972880780696869
train_iter_loss: 0.14418752491474152
train_iter_loss: 0.11281279474496841
train_iter_loss: 0.16566704213619232
train_iter_loss: 0.1473265141248703
train_iter_loss: 0.19445669651031494
train_iter_loss: 0.13226476311683655
train_iter_loss: 0.13183407485485077
train_iter_loss: 0.05830513685941696
train_iter_loss: 0.27707529067993164
train_iter_loss: 0.14570941030979156
train_iter_loss: 0.06432275474071503
train_iter_loss: 0.37288159132003784
train_iter_loss: 0.19645093381404877
train_iter_loss: 0.33715739846229553
train_iter_loss: 0.310344398021698
train_iter_loss: 0.32264870405197144
train_iter_loss: 0.2460314929485321
train_iter_loss: 0.18695126473903656
train_iter_loss: 0.17719468474388123
train loss :0.1569
---------------------
Validation seg loss: 0.2166204724179686 at epoch 736
epoch =    737/  1000, exp = train
train_iter_loss: 0.14920496940612793
train_iter_loss: 0.11564938724040985
train_iter_loss: 0.1521189659833908
train_iter_loss: 0.12888531386852264
train_iter_loss: 0.14922811090946198
train_iter_loss: 0.09490544348955154
train_iter_loss: 0.2722024917602539
train_iter_loss: 0.21615244448184967
train_iter_loss: 0.1395466923713684
train_iter_loss: 0.226926788687706
train_iter_loss: 0.1620320975780487
train_iter_loss: 0.09934371709823608
train_iter_loss: 0.17991423606872559
train_iter_loss: 0.06940481811761856
train_iter_loss: 0.08910410851240158
train_iter_loss: 0.020794536918401718
train_iter_loss: 0.13129174709320068
train_iter_loss: 0.2951814830303192
train_iter_loss: 0.24424035847187042
train_iter_loss: 0.1388130486011505
train_iter_loss: 0.17672207951545715
train_iter_loss: 0.1775166541337967
train_iter_loss: 0.2537476122379303
train_iter_loss: 0.11196114867925644
train_iter_loss: 0.20884136855602264
train_iter_loss: 0.11593153327703476
train_iter_loss: 0.3252776265144348
train_iter_loss: 0.27955710887908936
train_iter_loss: 0.20445245504379272
train_iter_loss: 0.1999714970588684
train_iter_loss: 0.16237621009349823
train_iter_loss: 0.1462220698595047
train_iter_loss: 0.15626004338264465
train_iter_loss: 0.16440069675445557
train_iter_loss: 0.1673445701599121
train_iter_loss: 0.24569468200206757
train_iter_loss: 0.10703539848327637
train_iter_loss: 0.09380762279033661
train_iter_loss: 0.1253848373889923
train_iter_loss: 0.1737855225801468
train_iter_loss: 0.16028307378292084
train_iter_loss: 0.13846194744110107
train_iter_loss: 0.22605586051940918
train_iter_loss: 0.07774552702903748
train_iter_loss: 0.19817087054252625
train_iter_loss: 0.19348470866680145
train_iter_loss: 0.15154105424880981
train_iter_loss: 0.19564129412174225
train_iter_loss: 0.09101570397615433
train_iter_loss: 0.1345115453004837
train_iter_loss: 0.17349155247211456
train_iter_loss: 0.31544655561447144
train_iter_loss: 0.20763252675533295
train_iter_loss: 0.11289773881435394
train_iter_loss: 0.1373012810945511
train_iter_loss: 0.2797095477581024
train_iter_loss: 0.15372434258460999
train_iter_loss: 0.14763951301574707
train_iter_loss: 0.12112663686275482
train_iter_loss: 0.1094975695014
train_iter_loss: 0.23192736506462097
train_iter_loss: 0.06503268331289291
train_iter_loss: 0.18768537044525146
train_iter_loss: 0.1439032256603241
train_iter_loss: 0.11463803052902222
train_iter_loss: 0.11394281685352325
train_iter_loss: 0.08877430111169815
train_iter_loss: 0.11910665780305862
train_iter_loss: 0.09637552499771118
train_iter_loss: 0.3514421284198761
train_iter_loss: 0.12274318933486938
train_iter_loss: 0.2153247445821762
train_iter_loss: 0.13017109036445618
train_iter_loss: 0.17038710415363312
train_iter_loss: 0.10352311283349991
train_iter_loss: 0.1319417804479599
train_iter_loss: 0.14012537896633148
train_iter_loss: 0.25812962651252747
train_iter_loss: 0.11350151151418686
train_iter_loss: 0.21951450407505035
train_iter_loss: 0.06197317689657211
train_iter_loss: 0.4108292758464813
train_iter_loss: 0.07820935547351837
train_iter_loss: 0.14601992070674896
train_iter_loss: 0.05251740291714668
train_iter_loss: 0.09728613495826721
train_iter_loss: 0.14360085129737854
train_iter_loss: 0.05575386807322502
train_iter_loss: 0.21513499319553375
train_iter_loss: 0.1690654456615448
train_iter_loss: 0.12284483760595322
train_iter_loss: 0.12731361389160156
train_iter_loss: 0.026359383016824722
train_iter_loss: 0.2025899440050125
train_iter_loss: 0.15245147049427032
train_iter_loss: 0.3429001569747925
train_iter_loss: 0.1477043330669403
train_iter_loss: 0.34239280223846436
train_iter_loss: 0.2883695662021637
train_iter_loss: 0.07540003955364227
train loss :0.1653
---------------------
Validation seg loss: 0.22171373401750932 at epoch 737
epoch =    738/  1000, exp = train
train_iter_loss: 0.13845764100551605
train_iter_loss: 0.12304651737213135
train_iter_loss: 0.09673700481653214
train_iter_loss: 0.17037583887577057
train_iter_loss: 0.11262927204370499
train_iter_loss: 0.1238437071442604
train_iter_loss: 0.11488647758960724
train_iter_loss: 0.12470352649688721
train_iter_loss: 0.14321434497833252
train_iter_loss: 0.13590207695960999
train_iter_loss: 0.20079703629016876
train_iter_loss: 0.17392897605895996
train_iter_loss: 0.06728222966194153
train_iter_loss: 0.1409892737865448
train_iter_loss: 0.14786700904369354
train_iter_loss: 0.1014680340886116
train_iter_loss: 0.09201847016811371
train_iter_loss: 0.2043212205171585
train_iter_loss: 0.22773532569408417
train_iter_loss: 0.13745464384555817
train_iter_loss: 0.11552919447422028
train_iter_loss: 0.088858962059021
train_iter_loss: 0.14742286503314972
train_iter_loss: 0.2312716543674469
train_iter_loss: 0.12068253010511398
train_iter_loss: 0.2974551022052765
train_iter_loss: 0.07438270002603531
train_iter_loss: 0.2089235931634903
train_iter_loss: 0.12837637960910797
train_iter_loss: 0.14238576591014862
train_iter_loss: 0.13951264321804047
train_iter_loss: 0.02757306396961212
train_iter_loss: 0.11472217738628387
train_iter_loss: 0.21796616911888123
train_iter_loss: 0.05983700230717659
train_iter_loss: 0.1791745275259018
train_iter_loss: 0.2513555586338043
train_iter_loss: 0.2442570924758911
train_iter_loss: 0.136167973279953
train_iter_loss: 0.19382648169994354
train_iter_loss: 0.21175456047058105
train_iter_loss: 0.05409906059503555
train_iter_loss: 0.3319994807243347
train_iter_loss: 0.1293037086725235
train_iter_loss: 0.15699540078639984
train_iter_loss: 0.09946997463703156
train_iter_loss: 0.06548240780830383
train_iter_loss: 0.06552908569574356
train_iter_loss: 0.1846126914024353
train_iter_loss: 0.09902049601078033
train_iter_loss: 0.09442023932933807
train_iter_loss: 0.12174174189567566
train_iter_loss: 0.1010332852602005
train_iter_loss: 0.08506521582603455
train_iter_loss: 0.15453609824180603
train_iter_loss: 0.2636745870113373
train_iter_loss: 0.16882652044296265
train_iter_loss: 0.2321809083223343
train_iter_loss: 0.23414745926856995
train_iter_loss: 0.29423797130584717
train_iter_loss: 0.13460299372673035
train_iter_loss: 0.182805597782135
train_iter_loss: 0.08937608450651169
train_iter_loss: 0.11397042125463486
train_iter_loss: 0.18199831247329712
train_iter_loss: 0.17482411861419678
train_iter_loss: 0.08690532296895981
train_iter_loss: 0.08489483594894409
train_iter_loss: 0.2918345332145691
train_iter_loss: 0.13032418489456177
train_iter_loss: 0.08519183844327927
train_iter_loss: 0.2942759394645691
train_iter_loss: 0.17857827246189117
train_iter_loss: 0.1765890270471573
train_iter_loss: 0.17187149822711945
train_iter_loss: 0.06522258371114731
train_iter_loss: 0.1262575387954712
train_iter_loss: 0.21063055098056793
train_iter_loss: 0.1321302354335785
train_iter_loss: 0.23615452647209167
train_iter_loss: 0.07867471873760223
train_iter_loss: 0.18370765447616577
train_iter_loss: 0.07537104934453964
train_iter_loss: 0.12849007546901703
train_iter_loss: 0.3079076111316681
train_iter_loss: 0.09714512526988983
train_iter_loss: 0.12520606815814972
train_iter_loss: 0.20152996480464935
train_iter_loss: 0.11145088076591492
train_iter_loss: 0.10104503482580185
train_iter_loss: 0.3246549665927887
train_iter_loss: 0.1569174975156784
train_iter_loss: 0.12743787467479706
train_iter_loss: 0.16024565696716309
train_iter_loss: 0.15890981256961823
train_iter_loss: 0.348032683134079
train_iter_loss: 0.17863646149635315
train_iter_loss: 0.11387479305267334
train_iter_loss: 0.1354052871465683
train_iter_loss: 0.09135392308235168
train loss :0.1557
---------------------
Validation seg loss: 0.21786531759425998 at epoch 738
epoch =    739/  1000, exp = train
train_iter_loss: 0.19376292824745178
train_iter_loss: 0.1644333004951477
train_iter_loss: 0.22224079072475433
train_iter_loss: 0.14903110265731812
train_iter_loss: 0.1548846811056137
train_iter_loss: 0.09734123200178146
train_iter_loss: 0.12488207966089249
train_iter_loss: 0.16159841418266296
train_iter_loss: 0.2539280652999878
train_iter_loss: 0.13023202121257782
train_iter_loss: 0.1403970569372177
train_iter_loss: 0.1373146027326584
train_iter_loss: 0.13612012565135956
train_iter_loss: 0.20826905965805054
train_iter_loss: 0.07822823524475098
train_iter_loss: 0.08378239721059799
train_iter_loss: 0.1524004340171814
train_iter_loss: 0.06354633718729019
train_iter_loss: 0.21807584166526794
train_iter_loss: 0.10230173170566559
train_iter_loss: 0.3230225443840027
train_iter_loss: 0.2492343634366989
train_iter_loss: 0.1461130976676941
train_iter_loss: 0.13574634492397308
train_iter_loss: 0.18223868310451508
train_iter_loss: 0.09013669192790985
train_iter_loss: 0.18783706426620483
train_iter_loss: 0.01947830803692341
train_iter_loss: 0.1336330771446228
train_iter_loss: 0.19750063121318817
train_iter_loss: 0.13335144519805908
train_iter_loss: 0.2037692815065384
train_iter_loss: 0.15067775547504425
train_iter_loss: 0.22350646555423737
train_iter_loss: 0.1569596379995346
train_iter_loss: 0.19891870021820068
train_iter_loss: 0.12403973191976547
train_iter_loss: 0.15392033755779266
train_iter_loss: 0.2070765495300293
train_iter_loss: 0.18855682015419006
train_iter_loss: 0.17874173820018768
train_iter_loss: 0.1411081999540329
train_iter_loss: 0.10758458077907562
train_iter_loss: 0.1460326761007309
train_iter_loss: 0.11109005659818649
train_iter_loss: 0.3212474286556244
train_iter_loss: 0.18120010197162628
train_iter_loss: 0.13582807779312134
train_iter_loss: 0.15039929747581482
train_iter_loss: 0.17121227085590363
train_iter_loss: 0.13846153020858765
train_iter_loss: 0.15617123246192932
train_iter_loss: 0.31952574849128723
train_iter_loss: 0.12100309133529663
train_iter_loss: 0.12322384119033813
train_iter_loss: 0.11383604258298874
train_iter_loss: 0.0757737085223198
train_iter_loss: 0.11374878138303757
train_iter_loss: 0.1418106108903885
train_iter_loss: 0.28201964497566223
train_iter_loss: 0.15111611783504486
train_iter_loss: 0.10644721239805222
train_iter_loss: 0.069454625248909
train_iter_loss: 0.14084100723266602
train_iter_loss: 0.1762458235025406
train_iter_loss: 0.1785503476858139
train_iter_loss: 0.1967797577381134
train_iter_loss: 0.2029515653848648
train_iter_loss: 0.1848948895931244
train_iter_loss: 0.11383717507123947
train_iter_loss: 0.13190652430057526
train_iter_loss: 0.0904625952243805
train_iter_loss: 0.16711045801639557
train_iter_loss: 0.18621738255023956
train_iter_loss: 0.1601773351430893
train_iter_loss: 0.08205434679985046
train_iter_loss: 0.1345391571521759
train_iter_loss: 0.2542073726654053
train_iter_loss: 0.13679441809654236
train_iter_loss: 0.13071714341640472
train_iter_loss: 0.13803333044052124
train_iter_loss: 0.12207262963056564
train_iter_loss: 0.14860451221466064
train_iter_loss: 0.12959817051887512
train_iter_loss: 0.22265881299972534
train_iter_loss: 0.06013040617108345
train_iter_loss: 0.28046226501464844
train_iter_loss: 0.18020324409008026
train_iter_loss: 0.1392737478017807
train_iter_loss: 0.11965705454349518
train_iter_loss: 0.19337080419063568
train_iter_loss: 0.09620141237974167
train_iter_loss: 0.25876277685165405
train_iter_loss: 0.2139279544353485
train_iter_loss: 0.10390515625476837
train_iter_loss: 0.17996090650558472
train_iter_loss: 0.06151319667696953
train_iter_loss: 0.1567823439836502
train_iter_loss: 0.17535734176635742
train_iter_loss: 0.23605719208717346
train loss :0.1598
---------------------
Validation seg loss: 0.22047559452190432 at epoch 739
epoch =    740/  1000, exp = train
train_iter_loss: 0.1743621528148651
train_iter_loss: 0.1916206181049347
train_iter_loss: 0.23083031177520752
train_iter_loss: 0.15106342732906342
train_iter_loss: 0.23047441244125366
train_iter_loss: 0.15994997322559357
train_iter_loss: 0.09644493460655212
train_iter_loss: 0.15258769690990448
train_iter_loss: 0.16939350962638855
train_iter_loss: 0.12448867410421371
train_iter_loss: 0.3083840310573578
train_iter_loss: 0.10744887590408325
train_iter_loss: 0.1415073126554489
train_iter_loss: 0.1274477243423462
train_iter_loss: 0.15543155372142792
train_iter_loss: 0.20403240621089935
train_iter_loss: 0.16135984659194946
train_iter_loss: 0.23060646653175354
train_iter_loss: 0.21166874468326569
train_iter_loss: 0.13684292137622833
train_iter_loss: 0.13769382238388062
train_iter_loss: 0.16510067880153656
train_iter_loss: 0.046109218150377274
train_iter_loss: 0.21481376886367798
train_iter_loss: 0.10572677850723267
train_iter_loss: 0.14704950153827667
train_iter_loss: 0.1708003133535385
train_iter_loss: 0.14689038693904877
train_iter_loss: 0.049155622720718384
train_iter_loss: 0.1953669786453247
train_iter_loss: 0.10101921111345291
train_iter_loss: 0.13743801414966583
train_iter_loss: 0.16706642508506775
train_iter_loss: 0.19498586654663086
train_iter_loss: 0.1718737632036209
train_iter_loss: 0.18401950597763062
train_iter_loss: 0.11967083811759949
train_iter_loss: 0.17986245453357697
train_iter_loss: 0.2413567304611206
train_iter_loss: 0.14347529411315918
train_iter_loss: 0.1418280303478241
train_iter_loss: 0.12161973118782043
train_iter_loss: 0.1878490298986435
train_iter_loss: 0.15718793869018555
train_iter_loss: 0.22327151894569397
train_iter_loss: 0.16491545736789703
train_iter_loss: 0.17111597955226898
train_iter_loss: 0.0865006297826767
train_iter_loss: 0.08157684653997421
train_iter_loss: 0.1856587529182434
train_iter_loss: 0.14429187774658203
train_iter_loss: 0.07304759323596954
train_iter_loss: 0.32739755511283875
train_iter_loss: 0.16047117114067078
train_iter_loss: 0.10145227611064911
train_iter_loss: 0.1600787341594696
train_iter_loss: 0.10028283298015594
train_iter_loss: 0.11244174093008041
train_iter_loss: 0.27891477942466736
train_iter_loss: 0.1311710923910141
train_iter_loss: 0.19905157387256622
train_iter_loss: 0.2388812005519867
train_iter_loss: 0.057837583124637604
train_iter_loss: 0.17402076721191406
train_iter_loss: 0.16300104558467865
train_iter_loss: 0.10779675841331482
train_iter_loss: 0.0960739403963089
train_iter_loss: 0.19582323729991913
train_iter_loss: 0.07386215776205063
train_iter_loss: 0.17875634133815765
train_iter_loss: 0.10400956124067307
train_iter_loss: 0.08800791949033737
train_iter_loss: 0.126334086060524
train_iter_loss: 0.213527113199234
train_iter_loss: 0.1440768837928772
train_iter_loss: 0.09438588470220566
train_iter_loss: 0.12221214175224304
train_iter_loss: 0.20349878072738647
train_iter_loss: 0.18762940168380737
train_iter_loss: 0.19525988399982452
train_iter_loss: 0.21333704888820648
train_iter_loss: 0.2333042472600937
train_iter_loss: 0.1395929902791977
train_iter_loss: 0.09050148725509644
train_iter_loss: 0.05351311340928078
train_iter_loss: 0.11299193650484085
train_iter_loss: 0.1611337512731552
train_iter_loss: 0.12409306317567825
train_iter_loss: 0.17713722586631775
train_iter_loss: 0.29880577325820923
train_iter_loss: 0.1151127815246582
train_iter_loss: 0.2444431185722351
train_iter_loss: 0.07416625320911407
train_iter_loss: 0.10608116537332535
train_iter_loss: 0.1885639876127243
train_iter_loss: 0.09328974038362503
train_iter_loss: 0.10248960554599762
train_iter_loss: 0.10193776339292526
train_iter_loss: 0.1240038052201271
train_iter_loss: 0.12172159552574158
train loss :0.1560
---------------------
Validation seg loss: 0.21655347353761206 at epoch 740
epoch =    741/  1000, exp = train
train_iter_loss: 0.08281178027391434
train_iter_loss: 0.1960277408361435
train_iter_loss: 0.11577467620372772
train_iter_loss: 0.13203305006027222
train_iter_loss: 0.13632649183273315
train_iter_loss: 0.17125044763088226
train_iter_loss: 0.1818649023771286
train_iter_loss: 0.1899828314781189
train_iter_loss: 0.15944857895374298
train_iter_loss: 0.06525503844022751
train_iter_loss: 0.16489896178245544
train_iter_loss: 0.19724933803081512
train_iter_loss: 0.14494718611240387
train_iter_loss: 0.1795101910829544
train_iter_loss: 0.0683220848441124
train_iter_loss: 0.12202048301696777
train_iter_loss: 0.12395873665809631
train_iter_loss: 0.1073322594165802
train_iter_loss: 0.16585320234298706
train_iter_loss: 0.1187555268406868
train_iter_loss: 0.05223026126623154
train_iter_loss: 0.1576307862997055
train_iter_loss: 0.03751140460371971
train_iter_loss: 0.1422368884086609
train_iter_loss: 0.0355282686650753
train_iter_loss: 0.14534632861614227
train_iter_loss: 0.2596583068370819
train_iter_loss: 0.22660388052463531
train_iter_loss: 0.17991167306900024
train_iter_loss: 0.3710050582885742
train_iter_loss: 0.1467035561800003
train_iter_loss: 0.08957783877849579
train_iter_loss: 0.1572955697774887
train_iter_loss: 0.15078189969062805
train_iter_loss: 0.1605944037437439
train_iter_loss: 0.15000666677951813
train_iter_loss: 0.2343783974647522
train_iter_loss: 0.07400469481945038
train_iter_loss: 0.18600694835186005
train_iter_loss: 0.1096884161233902
train_iter_loss: 0.0904797613620758
train_iter_loss: 0.14571137726306915
train_iter_loss: 0.061856482177972794
train_iter_loss: 0.12883104383945465
train_iter_loss: 0.14453664422035217
train_iter_loss: 0.14662522077560425
train_iter_loss: 0.11300159245729446
train_iter_loss: 0.17515695095062256
train_iter_loss: 0.20820342004299164
train_iter_loss: 0.050855498760938644
train_iter_loss: 0.11002398282289505
train_iter_loss: 0.13237929344177246
train_iter_loss: 0.11799893528223038
train_iter_loss: 0.2433040291070938
train_iter_loss: 0.2520764172077179
train_iter_loss: 0.11742355674505234
train_iter_loss: 0.2267230600118637
train_iter_loss: 0.13097599148750305
train_iter_loss: 0.09599564969539642
train_iter_loss: 0.20359085500240326
train_iter_loss: 0.08222346007823944
train_iter_loss: 0.16371002793312073
train_iter_loss: 0.19491298496723175
train_iter_loss: 0.18572503328323364
train_iter_loss: 0.2364949733018875
train_iter_loss: 0.13463135063648224
train_iter_loss: 0.10386169701814651
train_iter_loss: 0.4528999924659729
train_iter_loss: 0.11004944890737534
train_iter_loss: 0.18483181297779083
train_iter_loss: 0.22341759502887726
train_iter_loss: 0.10326753556728363
train_iter_loss: 0.08165526390075684
train_iter_loss: 0.06256205588579178
train_iter_loss: 0.17791467905044556
train_iter_loss: 0.15795402228832245
train_iter_loss: 0.19599251449108124
train_iter_loss: 0.11104399710893631
train_iter_loss: 0.1371251493692398
train_iter_loss: 0.20049409568309784
train_iter_loss: 0.09032135456800461
train_iter_loss: 0.13334877789020538
train_iter_loss: 0.175591841340065
train_iter_loss: 0.213993638753891
train_iter_loss: 0.14831028878688812
train_iter_loss: 0.1765875369310379
train_iter_loss: 0.27443939447402954
train_iter_loss: 0.11910314857959747
train_iter_loss: 0.17552460730075836
train_iter_loss: 0.12832705676555634
train_iter_loss: 0.16516940295696259
train_iter_loss: 0.1799146831035614
train_iter_loss: 0.22556588053703308
train_iter_loss: 0.18933725357055664
train_iter_loss: 0.02544427663087845
train_iter_loss: 0.07343465089797974
train_iter_loss: 0.15898187458515167
train_iter_loss: 0.25609090924263
train_iter_loss: 0.1263185441493988
train_iter_loss: 0.11642288416624069
train loss :0.1540
---------------------
Validation seg loss: 0.21170810639928533 at epoch 741
epoch =    742/  1000, exp = train
train_iter_loss: 0.13462455570697784
train_iter_loss: 0.24655036628246307
train_iter_loss: 0.17308339476585388
train_iter_loss: 0.2389189749956131
train_iter_loss: 0.09322761744260788
train_iter_loss: 0.10734729468822479
train_iter_loss: 0.08567061275243759
train_iter_loss: 0.08781546354293823
train_iter_loss: 0.109705351293087
train_iter_loss: 0.21980775892734528
train_iter_loss: 0.15826313197612762
train_iter_loss: 0.10298943519592285
train_iter_loss: 0.25860556960105896
train_iter_loss: 0.1650875061750412
train_iter_loss: 0.14246563613414764
train_iter_loss: 0.09272652864456177
train_iter_loss: 0.038534581661224365
train_iter_loss: 0.13559551537036896
train_iter_loss: 0.17654038965702057
train_iter_loss: 0.13242438435554504
train_iter_loss: 0.08921467512845993
train_iter_loss: 0.12538161873817444
train_iter_loss: 0.2308616191148758
train_iter_loss: 0.16799066960811615
train_iter_loss: 0.27259519696235657
train_iter_loss: 0.07541579008102417
train_iter_loss: 0.21764115989208221
train_iter_loss: 0.13199841976165771
train_iter_loss: 0.11430081725120544
train_iter_loss: 0.2115936279296875
train_iter_loss: 0.0786205604672432
train_iter_loss: 0.12287537753582001
train_iter_loss: 0.07299038767814636
train_iter_loss: 0.06215984746813774
train_iter_loss: 0.1528007537126541
train_iter_loss: 0.3095085918903351
train_iter_loss: 0.22086089849472046
train_iter_loss: 0.08117859810590744
train_iter_loss: 0.11156080663204193
train_iter_loss: 0.07414348423480988
train_iter_loss: 0.17471367120742798
train_iter_loss: 0.057197704911231995
train_iter_loss: 0.494872510433197
train_iter_loss: 0.09697889536619186
train_iter_loss: 0.262370228767395
train_iter_loss: 0.18972410261631012
train_iter_loss: 0.176712304353714
train_iter_loss: 0.2482774704694748
train_iter_loss: 0.2252325415611267
train_iter_loss: 0.13876937329769135
train_iter_loss: 0.14991536736488342
train_iter_loss: 0.21053197979927063
train_iter_loss: 0.1547473967075348
train_iter_loss: 0.13850460946559906
train_iter_loss: 0.22712883353233337
train_iter_loss: 0.12476105242967606
train_iter_loss: 0.12128198891878128
train_iter_loss: 0.10169102251529694
train_iter_loss: 0.1605689525604248
train_iter_loss: 0.19623807072639465
train_iter_loss: 0.2140357792377472
train_iter_loss: 0.14164507389068604
train_iter_loss: 0.19454170763492584
train_iter_loss: 0.11127442866563797
train_iter_loss: 0.0972037985920906
train_iter_loss: 0.12673641741275787
train_iter_loss: 0.24107597768306732
train_iter_loss: 0.10855162143707275
train_iter_loss: 0.13152630627155304
train_iter_loss: 0.2091037631034851
train_iter_loss: 0.15901221334934235
train_iter_loss: 0.253836989402771
train_iter_loss: 0.10455980151891708
train_iter_loss: 0.09141159802675247
train_iter_loss: 0.12587577104568481
train_iter_loss: 0.21243299543857574
train_iter_loss: 0.17243421077728271
train_iter_loss: 0.08888687938451767
train_iter_loss: 0.25401005148887634
train_iter_loss: 0.19111351668834686
train_iter_loss: 0.23714129626750946
train_iter_loss: 0.14498789608478546
train_iter_loss: 0.09416021406650543
train_iter_loss: 0.14234347641468048
train_iter_loss: 0.2033499777317047
train_iter_loss: 0.18864162266254425
train_iter_loss: 0.08065634220838547
train_iter_loss: 0.1559390425682068
train_iter_loss: 0.17806445062160492
train_iter_loss: 0.13097555935382843
train_iter_loss: 0.45501700043678284
train_iter_loss: 0.09707291424274445
train_iter_loss: 0.14304855465888977
train_iter_loss: 0.17009179294109344
train_iter_loss: 0.14187295734882355
train_iter_loss: 0.2051306515932083
train_iter_loss: 0.1665874570608139
train_iter_loss: 0.17331507802009583
train_iter_loss: 0.25139129161834717
train_iter_loss: 0.1192360371351242
train loss :0.1634
---------------------
Validation seg loss: 0.2224417096970357 at epoch 742
epoch =    743/  1000, exp = train
train_iter_loss: 0.13512197136878967
train_iter_loss: 0.06566905975341797
train_iter_loss: 0.1066117063164711
train_iter_loss: 0.21050992608070374
train_iter_loss: 0.15713119506835938
train_iter_loss: 0.16283480823040009
train_iter_loss: 0.1072324812412262
train_iter_loss: 0.12236271798610687
train_iter_loss: 0.19275246560573578
train_iter_loss: 0.1519954353570938
train_iter_loss: 0.22346557676792145
train_iter_loss: 0.20718882977962494
train_iter_loss: 0.09300626814365387
train_iter_loss: 0.08497218042612076
train_iter_loss: 0.1309540718793869
train_iter_loss: 0.2160550355911255
train_iter_loss: 0.2325325757265091
train_iter_loss: 0.16715556383132935
train_iter_loss: 0.12233085930347443
train_iter_loss: 0.10134128481149673
train_iter_loss: 0.053370244801044464
train_iter_loss: 0.16436396539211273
train_iter_loss: 0.10597006231546402
train_iter_loss: 0.12532252073287964
train_iter_loss: 0.11281798034906387
train_iter_loss: 0.08258957415819168
train_iter_loss: 0.07276681810617447
train_iter_loss: 0.1361324042081833
train_iter_loss: 0.17575958371162415
train_iter_loss: 0.06620784848928452
train_iter_loss: 0.16174980998039246
train_iter_loss: 0.09461675584316254
train_iter_loss: 0.2115854024887085
train_iter_loss: 0.09520256519317627
train_iter_loss: 0.15018609166145325
train_iter_loss: 0.07975169271230698
train_iter_loss: 0.18085330724716187
train_iter_loss: 0.1634521633386612
train_iter_loss: 0.15792855620384216
train_iter_loss: 0.1538725048303604
train_iter_loss: 0.1301707923412323
train_iter_loss: 0.21594026684761047
train_iter_loss: 0.17277830839157104
train_iter_loss: 0.1485729068517685
train_iter_loss: 0.13778866827487946
train_iter_loss: 0.12359246611595154
train_iter_loss: 0.05588841438293457
train_iter_loss: 0.20058831572532654
train_iter_loss: 0.2439253181219101
train_iter_loss: 0.16853384673595428
train_iter_loss: 0.20885300636291504
train_iter_loss: 0.16186963021755219
train_iter_loss: 0.20240168273448944
train_iter_loss: 0.3025011718273163
train_iter_loss: 0.273428350687027
train_iter_loss: 0.10607372224330902
train_iter_loss: 0.2188519984483719
train_iter_loss: 0.15103290975093842
train_iter_loss: 0.1198735162615776
train_iter_loss: 0.2718384861946106
train_iter_loss: 0.14208972454071045
train_iter_loss: 0.1725004017353058
train_iter_loss: 0.31508001685142517
train_iter_loss: 0.13933782279491425
train_iter_loss: 0.15547412633895874
train_iter_loss: 0.11962957680225372
train_iter_loss: 0.15506084263324738
train_iter_loss: 0.12746916711330414
train_iter_loss: 0.10678235441446304
train_iter_loss: 0.10099794715642929
train_iter_loss: 0.1263761818408966
train_iter_loss: 0.2092183381319046
train_iter_loss: 0.22956939041614532
train_iter_loss: 0.1620407998561859
train_iter_loss: 0.14552174508571625
train_iter_loss: 0.1169062927365303
train_iter_loss: 0.17381614446640015
train_iter_loss: 0.19731871783733368
train_iter_loss: 0.11344124376773834
train_iter_loss: 0.14735464751720428
train_iter_loss: 0.16976836323738098
train_iter_loss: 0.1294429451227188
train_iter_loss: 0.2515752613544464
train_iter_loss: 0.13397996127605438
train_iter_loss: 0.10608065128326416
train_iter_loss: 0.10404039919376373
train_iter_loss: 0.15244163572788239
train_iter_loss: 0.15491561591625214
train_iter_loss: 0.17011675238609314
train_iter_loss: 0.19237114489078522
train_iter_loss: 0.11241581290960312
train_iter_loss: 0.1341707408428192
train_iter_loss: 0.15372850000858307
train_iter_loss: 0.11326105892658234
train_iter_loss: 0.20779922604560852
train_iter_loss: 0.38162320852279663
train_iter_loss: 0.11694616824388504
train_iter_loss: 0.08329208940267563
train_iter_loss: 0.08436574041843414
train_iter_loss: 0.1727328896522522
train loss :0.1559
---------------------
Validation seg loss: 0.21169561475409931 at epoch 743
epoch =    744/  1000, exp = train
train_iter_loss: 0.08543618768453598
train_iter_loss: 0.19638890027999878
train_iter_loss: 0.1233166754245758
train_iter_loss: 0.1996941715478897
train_iter_loss: 0.18147696554660797
train_iter_loss: 0.11301212757825851
train_iter_loss: 0.18449316918849945
train_iter_loss: 0.1640433818101883
train_iter_loss: 0.12608417868614197
train_iter_loss: 0.1807018667459488
train_iter_loss: 0.1095278263092041
train_iter_loss: 0.11768871545791626
train_iter_loss: 0.17278537154197693
train_iter_loss: 0.16221393644809723
train_iter_loss: 0.1183461844921112
train_iter_loss: 0.05861935764551163
train_iter_loss: 0.20165491104125977
train_iter_loss: 0.12484118342399597
train_iter_loss: 0.2123551070690155
train_iter_loss: 0.22868292033672333
train_iter_loss: 0.1673816293478012
train_iter_loss: 0.14456956088542938
train_iter_loss: 0.09232931584119797
train_iter_loss: 0.24722103774547577
train_iter_loss: 0.1324353963136673
train_iter_loss: 0.16476057469844818
train_iter_loss: 0.050779249519109726
train_iter_loss: 0.14172106981277466
train_iter_loss: 0.14104580879211426
train_iter_loss: 0.2087644338607788
train_iter_loss: 0.09827285259962082
train_iter_loss: 0.152145117521286
train_iter_loss: 0.27974608540534973
train_iter_loss: 0.1713094413280487
train_iter_loss: 0.14656247198581696
train_iter_loss: 0.1660843789577484
train_iter_loss: 0.2762015759944916
train_iter_loss: 0.1392078846693039
train_iter_loss: 0.27021288871765137
train_iter_loss: 0.0958818793296814
train_iter_loss: 0.16618020832538605
train_iter_loss: 0.10773526877164841
train_iter_loss: 0.1929924637079239
train_iter_loss: 0.1321364790201187
train_iter_loss: 0.19438548386096954
train_iter_loss: 0.11731468886137009
train_iter_loss: 0.21360093355178833
train_iter_loss: 0.1282397210597992
train_iter_loss: 0.20074015855789185
train_iter_loss: 0.10046501457691193
train_iter_loss: 0.24120399355888367
train_iter_loss: 0.2757216989994049
train_iter_loss: 0.1585250049829483
train_iter_loss: 0.13102105259895325
train_iter_loss: 0.07755086570978165
train_iter_loss: 0.1072443425655365
train_iter_loss: 0.1888098120689392
train_iter_loss: 0.12540718913078308
train_iter_loss: 0.15015345811843872
train_iter_loss: 0.12463639676570892
train_iter_loss: 0.10926079005002975
train_iter_loss: 0.06225592643022537
train_iter_loss: 0.20598748326301575
train_iter_loss: 0.16844570636749268
train_iter_loss: 0.1307165026664734
train_iter_loss: 0.16744327545166016
train_iter_loss: 0.12478524446487427
train_iter_loss: 0.15073297917842865
train_iter_loss: 0.16098566353321075
train_iter_loss: 0.307729572057724
train_iter_loss: 0.1031707227230072
train_iter_loss: 0.09578091651201248
train_iter_loss: 0.051999546587467194
train_iter_loss: 0.13568294048309326
train_iter_loss: 0.11545436084270477
train_iter_loss: 0.38660943508148193
train_iter_loss: 0.1494917869567871
train_iter_loss: 0.1314961165189743
train_iter_loss: 0.12349008768796921
train_iter_loss: 0.14970168471336365
train_iter_loss: 0.10488100349903107
train_iter_loss: 0.10198348015546799
train_iter_loss: 0.10524274408817291
train_iter_loss: 0.13508062064647675
train_iter_loss: 0.16803359985351562
train_iter_loss: 0.07908424735069275
train_iter_loss: 0.10827484726905823
train_iter_loss: 0.20213600993156433
train_iter_loss: 0.1681281477212906
train_iter_loss: 0.09144126623868942
train_iter_loss: 0.3325543999671936
train_iter_loss: 0.27925774455070496
train_iter_loss: 0.1930212378501892
train_iter_loss: 0.13457220792770386
train_iter_loss: 0.15077762305736542
train_iter_loss: 0.12948933243751526
train_iter_loss: 0.07544513791799545
train_iter_loss: 0.12898483872413635
train_iter_loss: 0.1726912260055542
train_iter_loss: 0.18567316234111786
train loss :0.1572
---------------------
Validation seg loss: 0.21819068225241214 at epoch 744
epoch =    745/  1000, exp = train
train_iter_loss: 0.226898193359375
train_iter_loss: 0.15668781101703644
train_iter_loss: 0.16488440334796906
train_iter_loss: 0.243718683719635
train_iter_loss: 0.2324606031179428
train_iter_loss: 0.06951828300952911
train_iter_loss: 0.19523628056049347
train_iter_loss: 0.13740354776382446
train_iter_loss: 0.3322415351867676
train_iter_loss: 0.047818057239055634
train_iter_loss: 0.10282330960035324
train_iter_loss: 0.2905983030796051
train_iter_loss: 0.2858157753944397
train_iter_loss: 0.09510932862758636
train_iter_loss: 0.18770644068717957
train_iter_loss: 0.09414344280958176
train_iter_loss: 0.1305508017539978
train_iter_loss: 0.16500279307365417
train_iter_loss: 0.20351195335388184
train_iter_loss: 0.17783130705356598
train_iter_loss: 0.29583021998405457
train_iter_loss: 0.20132528245449066
train_iter_loss: 0.1245230883359909
train_iter_loss: 0.21835216879844666
train_iter_loss: 0.1451086699962616
train_iter_loss: 0.24950388073921204
train_iter_loss: 0.091853566467762
train_iter_loss: 0.11454081535339355
train_iter_loss: 0.2071157693862915
train_iter_loss: 0.16611404716968536
train_iter_loss: 0.14678385853767395
train_iter_loss: 0.11716281622648239
train_iter_loss: 0.1531786024570465
train_iter_loss: 0.14549893140792847
train_iter_loss: 0.24185341596603394
train_iter_loss: 0.22434860467910767
train_iter_loss: 0.22199377417564392
train_iter_loss: 0.09896895289421082
train_iter_loss: 0.08883635699748993
train_iter_loss: 0.2559798061847687
train_iter_loss: 0.12308388948440552
train_iter_loss: 0.09257379919290543
train_iter_loss: 0.16281825304031372
train_iter_loss: 0.12507092952728271
train_iter_loss: 0.12507547438144684
train_iter_loss: 0.11603952944278717
train_iter_loss: 0.10147640109062195
train_iter_loss: 0.12184754759073257
train_iter_loss: 0.2213555872440338
train_iter_loss: 0.20521096885204315
train_iter_loss: 0.15593889355659485
train_iter_loss: 0.20432984828948975
train_iter_loss: 0.06699216365814209
train_iter_loss: 0.10915732383728027
train_iter_loss: 0.10205042362213135
train_iter_loss: 0.13114292919635773
train_iter_loss: 0.09592317789793015
train_iter_loss: 0.19732089340686798
train_iter_loss: 0.1288401335477829
train_iter_loss: 0.1499365270137787
train_iter_loss: 0.08526585251092911
train_iter_loss: 0.07005605101585388
train_iter_loss: 0.1418580263853073
train_iter_loss: 0.23402360081672668
train_iter_loss: 0.15624696016311646
train_iter_loss: 0.3068433105945587
train_iter_loss: 0.16060945391654968
train_iter_loss: 0.15933369100093842
train_iter_loss: 0.1667475551366806
train_iter_loss: 0.2018793374300003
train_iter_loss: 0.16005700826644897
train_iter_loss: 0.1112561896443367
train_iter_loss: 0.1723422408103943
train_iter_loss: 0.16959522664546967
train_iter_loss: 0.22270330786705017
train_iter_loss: 0.09042122215032578
train_iter_loss: 0.10346576571464539
train_iter_loss: 0.14299915730953217
train_iter_loss: 0.17951035499572754
train_iter_loss: 0.11059371381998062
train_iter_loss: 0.12668198347091675
train_iter_loss: 0.11016536504030228
train_iter_loss: 0.1113528236746788
train_iter_loss: 0.2571388781070709
train_iter_loss: 0.19091904163360596
train_iter_loss: 0.2030411660671234
train_iter_loss: 0.04811545088887215
train_iter_loss: 0.21777014434337616
train_iter_loss: 0.07721760123968124
train_iter_loss: 0.1836739331483841
train_iter_loss: 0.2382727712392807
train_iter_loss: 0.1199684739112854
train_iter_loss: 0.13927139341831207
train_iter_loss: 0.09216740727424622
train_iter_loss: 0.08382551372051239
train_iter_loss: 0.20742003619670868
train_iter_loss: 0.04179392382502556
train_iter_loss: 0.13678869605064392
train_iter_loss: 0.20122848451137543
train_iter_loss: 0.3006930351257324
train loss :0.1618
---------------------
Validation seg loss: 0.21819572236810653 at epoch 745
epoch =    746/  1000, exp = train
train_iter_loss: 0.1956656277179718
train_iter_loss: 0.187364861369133
train_iter_loss: 0.06190870329737663
train_iter_loss: 0.1213131919503212
train_iter_loss: 0.10907041281461716
train_iter_loss: 0.10329243540763855
train_iter_loss: 0.22234541177749634
train_iter_loss: 0.16469301283359528
train_iter_loss: 0.16502714157104492
train_iter_loss: 0.12593920528888702
train_iter_loss: 0.15177978575229645
train_iter_loss: 0.14254522323608398
train_iter_loss: 0.16398681700229645
train_iter_loss: 0.2539529800415039
train_iter_loss: 0.14868204295635223
train_iter_loss: 0.10547018051147461
train_iter_loss: 0.12880770862102509
train_iter_loss: 0.13695290684700012
train_iter_loss: 0.25055113434791565
train_iter_loss: 0.09333691000938416
train_iter_loss: 0.1821713000535965
train_iter_loss: 0.10838901251554489
train_iter_loss: 0.21350222826004028
train_iter_loss: 0.12255319207906723
train_iter_loss: 0.20132920145988464
train_iter_loss: 0.07655103504657745
train_iter_loss: 0.1979752480983734
train_iter_loss: 0.07856430113315582
train_iter_loss: 0.10586948692798615
train_iter_loss: 0.11175724118947983
train_iter_loss: 0.16832777857780457
train_iter_loss: 0.09990212321281433
train_iter_loss: 0.2317637801170349
train_iter_loss: 0.13949771225452423
train_iter_loss: 0.05590108036994934
train_iter_loss: 0.1083337739109993
train_iter_loss: 0.18203948438167572
train_iter_loss: 0.1561582088470459
train_iter_loss: 0.2109496295452118
train_iter_loss: 0.1729959398508072
train_iter_loss: 0.053701963275671005
train_iter_loss: 0.21419057250022888
train_iter_loss: 0.18763400614261627
train_iter_loss: 0.21910572052001953
train_iter_loss: 0.1798650324344635
train_iter_loss: 0.21115681529045105
train_iter_loss: 0.5137779712677002
train_iter_loss: 0.1072670966386795
train_iter_loss: 0.2328372746706009
train_iter_loss: 0.26303547620773315
train_iter_loss: 0.14304527640342712
train_iter_loss: 0.15804772078990936
train_iter_loss: 0.16931213438510895
train_iter_loss: 0.16291390359401703
train_iter_loss: 0.16121773421764374
train_iter_loss: 0.17942281067371368
train_iter_loss: 0.09749667346477509
train_iter_loss: 0.036342814564704895
train_iter_loss: 0.17576120793819427
train_iter_loss: 0.09598612785339355
train_iter_loss: 0.17359913885593414
train_iter_loss: 0.1750868856906891
train_iter_loss: 0.11426261067390442
train_iter_loss: 0.20832312107086182
train_iter_loss: 0.08731861412525177
train_iter_loss: 0.10978621989488602
train_iter_loss: 0.14638274908065796
train_iter_loss: 0.11261763423681259
train_iter_loss: 0.09952883422374725
train_iter_loss: 0.10700912773609161
train_iter_loss: 0.13476482033729553
train_iter_loss: 0.278848797082901
train_iter_loss: 0.14748655259609222
train_iter_loss: 0.11523159593343735
train_iter_loss: 0.28029707074165344
train_iter_loss: 0.0934869572520256
train_iter_loss: 0.34696778655052185
train_iter_loss: 0.15764567255973816
train_iter_loss: 0.07642462104558945
train_iter_loss: 0.17075562477111816
train_iter_loss: 0.13845263421535492
train_iter_loss: 0.17998823523521423
train_iter_loss: 0.3005799353122711
train_iter_loss: 0.15133877098560333
train_iter_loss: 0.10366296023130417
train_iter_loss: 0.17807139456272125
train_iter_loss: 0.10155447572469711
train_iter_loss: 0.22573617100715637
train_iter_loss: 0.12019951641559601
train_iter_loss: 0.2638382315635681
train_iter_loss: 0.26390567421913147
train_iter_loss: 0.12768027186393738
train_iter_loss: 0.12539660930633545
train_iter_loss: 0.09556437283754349
train_iter_loss: 0.28909140825271606
train_iter_loss: 0.13528592884540558
train_iter_loss: 0.1288139373064041
train_iter_loss: 0.08477066457271576
train_iter_loss: 0.14028243720531464
train_iter_loss: 0.1150187999010086
train loss :0.1609
---------------------
Validation seg loss: 0.2136531001361052 at epoch 746
epoch =    747/  1000, exp = train
train_iter_loss: 0.1738526076078415
train_iter_loss: 0.09778163582086563
train_iter_loss: 0.14267270267009735
train_iter_loss: 0.18654567003250122
train_iter_loss: 0.03784489259123802
train_iter_loss: 0.09620381146669388
train_iter_loss: 0.12381921708583832
train_iter_loss: 0.13352416455745697
train_iter_loss: 0.13846760988235474
train_iter_loss: 0.10847394168376923
train_iter_loss: 0.08887899667024612
train_iter_loss: 0.15688170492649078
train_iter_loss: 0.18510498106479645
train_iter_loss: 0.14095015823841095
train_iter_loss: 0.19411493837833405
train_iter_loss: 0.15924213826656342
train_iter_loss: 0.15457464754581451
train_iter_loss: 0.34779423475265503
train_iter_loss: 0.09626274555921555
train_iter_loss: 0.2264692336320877
train_iter_loss: 0.15406286716461182
train_iter_loss: 0.13181284070014954
train_iter_loss: 0.14225037395954132
train_iter_loss: 0.12928979098796844
train_iter_loss: 0.09784539043903351
train_iter_loss: 0.11021769046783447
train_iter_loss: 0.17258493602275848
train_iter_loss: 0.2498687505722046
train_iter_loss: 0.16032527387142181
train_iter_loss: 0.18922652304172516
train_iter_loss: 0.1996915489435196
train_iter_loss: 0.11272814869880676
train_iter_loss: 0.0679025650024414
train_iter_loss: 0.11161970347166061
train_iter_loss: 0.1801750212907791
train_iter_loss: 0.1912299245595932
train_iter_loss: 0.11199656873941422
train_iter_loss: 0.13090789318084717
train_iter_loss: 0.17475952208042145
train_iter_loss: 0.18747475743293762
train_iter_loss: 0.1645577847957611
train_iter_loss: 0.14740820229053497
train_iter_loss: 0.12848205864429474
train_iter_loss: 0.30492159724235535
train_iter_loss: 0.11412639170885086
train_iter_loss: 0.27549487352371216
train_iter_loss: 0.15226827561855316
train_iter_loss: 0.1264190375804901
train_iter_loss: 0.11229661107063293
train_iter_loss: 0.08233631402254105
train_iter_loss: 0.1707906872034073
train_iter_loss: 0.163288414478302
train_iter_loss: 0.08196127414703369
train_iter_loss: 0.16532941162586212
train_iter_loss: 0.21890044212341309
train_iter_loss: 0.11091600358486176
train_iter_loss: 0.15673285722732544
train_iter_loss: 0.13617002964019775
train_iter_loss: 0.18567444384098053
train_iter_loss: 0.21229614317417145
train_iter_loss: 0.12958382070064545
train_iter_loss: 0.07919357717037201
train_iter_loss: 0.18505552411079407
train_iter_loss: 0.19922098517417908
train_iter_loss: 0.1633840799331665
train_iter_loss: 0.06939774006605148
train_iter_loss: 0.2467484325170517
train_iter_loss: 0.10429386794567108
train_iter_loss: 0.15154413878917694
train_iter_loss: 0.0868176743388176
train_iter_loss: 0.1829461008310318
train_iter_loss: 0.17474150657653809
train_iter_loss: 0.1635192632675171
train_iter_loss: 0.03993791714310646
train_iter_loss: 0.2010110765695572
train_iter_loss: 0.11993224918842316
train_iter_loss: 0.15483225882053375
train_iter_loss: 0.2824174761772156
train_iter_loss: 0.14231649041175842
train_iter_loss: 0.2122698724269867
train_iter_loss: 0.1914418488740921
train_iter_loss: 0.252030611038208
train_iter_loss: 0.15216991305351257
train_iter_loss: 0.18419717252254486
train_iter_loss: 0.22469565272331238
train_iter_loss: 0.1085195392370224
train_iter_loss: 0.10541734844446182
train_iter_loss: 0.08892083913087845
train_iter_loss: 0.15463855862617493
train_iter_loss: 0.1166725903749466
train_iter_loss: 0.14905565977096558
train_iter_loss: 0.2453795075416565
train_iter_loss: 0.18657268583774567
train_iter_loss: 0.2038809359073639
train_iter_loss: 0.04906764253973961
train_iter_loss: 0.1355096846818924
train_iter_loss: 0.0946909636259079
train_iter_loss: 0.2696741819381714
train_iter_loss: 0.15366436541080475
train_iter_loss: 0.18543873727321625
train loss :0.1571
---------------------
Validation seg loss: 0.21282928170776874 at epoch 747
epoch =    748/  1000, exp = train
train_iter_loss: 0.15045835077762604
train_iter_loss: 0.13201965391635895
train_iter_loss: 0.15203961730003357
train_iter_loss: 0.11298279464244843
train_iter_loss: 0.0876857340335846
train_iter_loss: 0.07823094725608826
train_iter_loss: 0.22411836683750153
train_iter_loss: 0.1704082190990448
train_iter_loss: 0.10768970102071762
train_iter_loss: 0.19186528027057648
train_iter_loss: 0.1305406540632248
train_iter_loss: 0.11361818760633469
train_iter_loss: 0.26306697726249695
train_iter_loss: 0.133409783244133
train_iter_loss: 0.11861539632081985
train_iter_loss: 0.2580493092536926
train_iter_loss: 0.11529572308063507
train_iter_loss: 0.12048794329166412
train_iter_loss: 0.14034079015254974
train_iter_loss: 0.2720913290977478
train_iter_loss: 0.17050157487392426
train_iter_loss: 0.09595254063606262
train_iter_loss: 0.1770762950181961
train_iter_loss: 0.1988673210144043
train_iter_loss: 0.17525562644004822
train_iter_loss: 0.09464506059885025
train_iter_loss: 0.11192437261343002
train_iter_loss: 0.21678879857063293
train_iter_loss: 0.24364162981510162
train_iter_loss: 0.11877300590276718
train_iter_loss: 0.16721884906291962
train_iter_loss: 0.15429309010505676
train_iter_loss: 0.15408706665039062
train_iter_loss: 0.10833222419023514
train_iter_loss: 0.11697982996702194
train_iter_loss: 0.07558000832796097
train_iter_loss: 0.18339811265468597
train_iter_loss: 0.1931326985359192
train_iter_loss: 0.1491416096687317
train_iter_loss: 0.0502689965069294
train_iter_loss: 0.11228931695222855
train_iter_loss: 0.1943226009607315
train_iter_loss: 0.08207272738218307
train_iter_loss: 0.2622675597667694
train_iter_loss: 0.1415325552225113
train_iter_loss: 0.13744951784610748
train_iter_loss: 0.14853258430957794
train_iter_loss: 0.08971648663282394
train_iter_loss: 0.10002905875444412
train_iter_loss: 0.09243674576282501
train_iter_loss: 0.171485036611557
train_iter_loss: 0.19678744673728943
train_iter_loss: 0.2261829823255539
train_iter_loss: 0.06733307242393494
train_iter_loss: 0.19915303587913513
train_iter_loss: 0.1079508438706398
train_iter_loss: 0.21829642355442047
train_iter_loss: 0.1654275506734848
train_iter_loss: 0.22920256853103638
train_iter_loss: 0.17935912311077118
train_iter_loss: 0.14849816262722015
train_iter_loss: 0.24225755035877228
train_iter_loss: 0.20141872763633728
train_iter_loss: 0.1340305656194687
train_iter_loss: 0.06441548466682434
train_iter_loss: 0.13748182356357574
train_iter_loss: 0.12840750813484192
train_iter_loss: 0.18748833239078522
train_iter_loss: 0.04970397800207138
train_iter_loss: 0.2750492990016937
train_iter_loss: 0.19054116308689117
train_iter_loss: 0.15096674859523773
train_iter_loss: 0.0820775255560875
train_iter_loss: 0.4560289680957794
train_iter_loss: 0.1763617992401123
train_iter_loss: 0.11662619560956955
train_iter_loss: 0.20831526815891266
train_iter_loss: 0.17912541329860687
train_iter_loss: 0.17654843628406525
train_iter_loss: 0.0689043253660202
train_iter_loss: 0.1842103749513626
train_iter_loss: 0.1462409794330597
train_iter_loss: 0.24001914262771606
train_iter_loss: 0.19755291938781738
train_iter_loss: 0.2666138708591461
train_iter_loss: 0.1520693302154541
train_iter_loss: 0.12828074395656586
train_iter_loss: 0.11591607332229614
train_iter_loss: 0.14034713804721832
train_iter_loss: 0.10654912143945694
train_iter_loss: 0.11844377964735031
train_iter_loss: 0.11018694937229156
train_iter_loss: 0.3198239207267761
train_iter_loss: 0.11430986225605011
train_iter_loss: 0.11138927936553955
train_iter_loss: 0.11811290681362152
train_iter_loss: 0.15784886479377747
train_iter_loss: 0.09988334029912949
train_iter_loss: 0.15324391424655914
train_iter_loss: 0.2293539047241211
train loss :0.1589
---------------------
Validation seg loss: 0.21157255691458593 at epoch 748
epoch =    749/  1000, exp = train
train_iter_loss: 0.0976342186331749
train_iter_loss: 0.12273479998111725
train_iter_loss: 0.10829013586044312
train_iter_loss: 0.1350773274898529
train_iter_loss: 0.12230478972196579
train_iter_loss: 0.20048008859157562
train_iter_loss: 0.2076081484556198
train_iter_loss: 0.13009285926818848
train_iter_loss: 0.17945639789104462
train_iter_loss: 0.14418040215969086
train_iter_loss: 0.15724250674247742
train_iter_loss: 0.15456192195415497
train_iter_loss: 0.10325543582439423
train_iter_loss: 0.1626880019903183
train_iter_loss: 0.10811027139425278
train_iter_loss: 0.1753457486629486
train_iter_loss: 0.28103500604629517
train_iter_loss: 0.07208827137947083
train_iter_loss: 0.1364990621805191
train_iter_loss: 0.11298803985118866
train_iter_loss: 0.27516308426856995
train_iter_loss: 0.16514424979686737
train_iter_loss: 0.24162468314170837
train_iter_loss: 0.11816735565662384
train_iter_loss: 0.17811425030231476
train_iter_loss: 0.23337964713573456
train_iter_loss: 0.20201557874679565
train_iter_loss: 0.11234518140554428
train_iter_loss: 0.12168072164058685
train_iter_loss: 0.13743671774864197
train_iter_loss: 0.09376461803913116
train_iter_loss: 0.11952418833971024
train_iter_loss: 0.19025146961212158
train_iter_loss: 0.17959579825401306
train_iter_loss: 0.13629470765590668
train_iter_loss: 0.18736062943935394
train_iter_loss: 0.16720853745937347
train_iter_loss: 0.15238934755325317
train_iter_loss: 0.15903834998607635
train_iter_loss: 0.3009752631187439
train_iter_loss: 0.10314645618200302
train_iter_loss: 0.12982861697673798
train_iter_loss: 0.18804967403411865
train_iter_loss: 0.15940985083580017
train_iter_loss: 0.13050656020641327
train_iter_loss: 0.16170965135097504
train_iter_loss: 0.14111800491809845
train_iter_loss: 0.07563569396734238
train_iter_loss: 0.20840519666671753
train_iter_loss: 0.14394918084144592
train_iter_loss: 0.11541485786437988
train_iter_loss: 0.24268858134746552
train_iter_loss: 0.08233948051929474
train_iter_loss: 0.1610250622034073
train_iter_loss: 0.1995401829481125
train_iter_loss: 0.11520533263683319
train_iter_loss: 0.16410815715789795
train_iter_loss: 0.09050016105175018
train_iter_loss: 0.18632180988788605
train_iter_loss: 0.16342104971408844
train_iter_loss: 0.18994960188865662
train_iter_loss: 0.08649122714996338
train_iter_loss: 0.20996232330799103
train_iter_loss: 0.17179696261882782
train_iter_loss: 0.2319883555173874
train_iter_loss: 0.13032491505146027
train_iter_loss: 0.1822003871202469
train_iter_loss: 0.1860789805650711
train_iter_loss: 0.1414119303226471
train_iter_loss: 0.16185730695724487
train_iter_loss: 0.0707557201385498
train_iter_loss: 0.10637134313583374
train_iter_loss: 0.05869299918413162
train_iter_loss: 0.1259808987379074
train_iter_loss: 0.18140165507793427
train_iter_loss: 0.13282164931297302
train_iter_loss: 0.11098665744066238
train_iter_loss: 0.057807374745607376
train_iter_loss: 0.0710415318608284
train_iter_loss: 0.17281407117843628
train_iter_loss: 0.14144392311573029
train_iter_loss: 0.2244514673948288
train_iter_loss: 0.18180042505264282
train_iter_loss: 0.13058054447174072
train_iter_loss: 0.1355222910642624
train_iter_loss: 0.17808420956134796
train_iter_loss: 0.07839387655258179
train_iter_loss: 0.07500627636909485
train_iter_loss: 0.5703269243240356
train_iter_loss: 0.14951643347740173
train_iter_loss: 0.12727226316928864
train_iter_loss: 0.22924725711345673
train_iter_loss: 0.11257152259349823
train_iter_loss: 0.10805552452802658
train_iter_loss: 0.23636125028133392
train_iter_loss: 0.18054762482643127
train_iter_loss: 0.15458093583583832
train_iter_loss: 0.14944002032279968
train_iter_loss: 0.14294472336769104
train_iter_loss: 0.12792740762233734
train loss :0.1572
---------------------
Validation seg loss: 0.21185476308301934 at epoch 749
epoch =    750/  1000, exp = train
train_iter_loss: 0.2094409167766571
train_iter_loss: 0.3661670386791229
train_iter_loss: 0.1748514324426651
train_iter_loss: 0.10517199337482452
train_iter_loss: 0.20960144698619843
train_iter_loss: 0.21773454546928406
train_iter_loss: 0.3650764226913452
train_iter_loss: 0.1868002712726593
train_iter_loss: 0.057871513068675995
train_iter_loss: 0.40114232897758484
train_iter_loss: 0.13996751606464386
train_iter_loss: 0.16093364357948303
train_iter_loss: 0.1476738452911377
train_iter_loss: 0.14404363930225372
train_iter_loss: 0.10935072600841522
train_iter_loss: 0.3544776737689972
train_iter_loss: 0.06814809143543243
train_iter_loss: 0.21548807621002197
train_iter_loss: 0.26330602169036865
train_iter_loss: 0.17550526559352875
train_iter_loss: 0.10222455114126205
train_iter_loss: 0.2125711292028427
train_iter_loss: 0.2390253096818924
train_iter_loss: 0.12726661562919617
train_iter_loss: 0.07574929296970367
train_iter_loss: 0.12165786325931549
train_iter_loss: 0.118632473051548
train_iter_loss: 0.18882863223552704
train_iter_loss: 0.12316884100437164
train_iter_loss: 0.15650661289691925
train_iter_loss: 0.42552420496940613
train_iter_loss: 0.0767178013920784
train_iter_loss: 0.10578139126300812
train_iter_loss: 0.15710870921611786
train_iter_loss: 0.10901059955358505
train_iter_loss: 0.17312026023864746
train_iter_loss: 0.22341342270374298
train_iter_loss: 0.1618959605693817
train_iter_loss: 0.21841758489608765
train_iter_loss: 0.1292613297700882
train_iter_loss: 0.0643843412399292
train_iter_loss: 0.13142332434654236
train_iter_loss: 0.17072263360023499
train_iter_loss: 0.14156706631183624
train_iter_loss: 0.09814365953207016
train_iter_loss: 0.11009058356285095
train_iter_loss: 0.12927496433258057
train_iter_loss: 0.10626323521137238
train_iter_loss: 0.13205544650554657
train_iter_loss: 0.08756382018327713
train_iter_loss: 0.1707928478717804
train_iter_loss: 0.19371215999126434
train_iter_loss: 0.08840259164571762
train_iter_loss: 0.0889410749077797
train_iter_loss: 0.09156083315610886
train_iter_loss: 0.20156154036521912
train_iter_loss: 0.10961604118347168
train_iter_loss: 0.14381572604179382
train_iter_loss: 0.16852103173732758
train_iter_loss: 0.12396536767482758
train_iter_loss: 0.09858636558055878
train_iter_loss: 0.2186271846294403
train_iter_loss: 0.13269974291324615
train_iter_loss: 0.055639106780290604
train_iter_loss: 0.11604482680559158
train_iter_loss: 0.131063312292099
train_iter_loss: 0.2635611295700073
train_iter_loss: 0.12395056337118149
train_iter_loss: 0.1534470021724701
train_iter_loss: 0.1294633150100708
train_iter_loss: 0.34278926253318787
train_iter_loss: 0.1854378879070282
train_iter_loss: 0.14346565306186676
train_iter_loss: 0.1519550234079361
train_iter_loss: 0.14214910566806793
train_iter_loss: 0.13660882413387299
train_iter_loss: 0.1596960872411728
train_iter_loss: 0.18227817118167877
train_iter_loss: 0.18795834481716156
train_iter_loss: 0.1857713907957077
train_iter_loss: 0.21686482429504395
train_iter_loss: 0.09349902719259262
train_iter_loss: 0.22882600128650665
train_iter_loss: 0.1777377724647522
train_iter_loss: 0.3022700548171997
train_iter_loss: 0.09849239140748978
train_iter_loss: 0.32097288966178894
train_iter_loss: 0.1873781830072403
train_iter_loss: 0.09638314694166183
train_iter_loss: 0.052990835160017014
train_iter_loss: 0.21168865263462067
train_iter_loss: 0.11536911875009537
train_iter_loss: 0.11890271306037903
train_iter_loss: 0.10098101943731308
train_iter_loss: 0.19262123107910156
train_iter_loss: 0.10343572497367859
train_iter_loss: 0.26876235008239746
train_iter_loss: 0.10281641036272049
train_iter_loss: 0.16425131261348724
train_iter_loss: 0.12496891617774963
train loss :0.1655
---------------------
Validation seg loss: 0.21322463428215035 at epoch 750
epoch =    751/  1000, exp = train
train_iter_loss: 0.1200762391090393
train_iter_loss: 0.23803505301475525
train_iter_loss: 0.1909146010875702
train_iter_loss: 0.3507571220397949
train_iter_loss: 0.22161608934402466
train_iter_loss: 0.14068269729614258
train_iter_loss: 0.10448732227087021
train_iter_loss: 0.16712133586406708
train_iter_loss: 0.10211827605962753
train_iter_loss: 0.1053130105137825
train_iter_loss: 0.09361296892166138
train_iter_loss: 0.19552819430828094
train_iter_loss: 0.15605506300926208
train_iter_loss: 0.26665452122688293
train_iter_loss: 0.1613399237394333
train_iter_loss: 0.08891713619232178
train_iter_loss: 0.22272923588752747
train_iter_loss: 0.02719573676586151
train_iter_loss: 0.13424329459667206
train_iter_loss: 0.10873812437057495
train_iter_loss: 0.18524962663650513
train_iter_loss: 0.18117506802082062
train_iter_loss: 0.11319711059331894
train_iter_loss: 0.22847266495227814
train_iter_loss: 0.07593027502298355
train_iter_loss: 0.2352902591228485
train_iter_loss: 0.2132141888141632
train_iter_loss: 0.10449690371751785
train_iter_loss: 0.2224515676498413
train_iter_loss: 0.11110337823629379
train_iter_loss: 0.11913980543613434
train_iter_loss: 0.1541319191455841
train_iter_loss: 0.24306176602840424
train_iter_loss: 0.13900919258594513
train_iter_loss: 0.47853508591651917
train_iter_loss: 0.14503991603851318
train_iter_loss: 0.15508703887462616
train_iter_loss: 0.20120000839233398
train_iter_loss: 0.08822864294052124
train_iter_loss: 0.032564375549554825
train_iter_loss: 0.1943153291940689
train_iter_loss: 0.21983708441257477
train_iter_loss: 0.24926918745040894
train_iter_loss: 0.09608012437820435
train_iter_loss: 0.09949533641338348
train_iter_loss: 0.18400722742080688
train_iter_loss: 0.23396041989326477
train_iter_loss: 0.1563185155391693
train_iter_loss: 0.10675527900457382
train_iter_loss: 0.12307345122098923
train_iter_loss: 0.14238886535167694
train_iter_loss: 0.10733982175588608
train_iter_loss: 0.12694670259952545
train_iter_loss: 0.16119173169136047
train_iter_loss: 0.23622171580791473
train_iter_loss: 0.34394848346710205
train_iter_loss: 0.12063460052013397
train_iter_loss: 0.1482093185186386
train_iter_loss: 0.06369791179895401
train_iter_loss: 0.10767354816198349
train_iter_loss: 0.1909986436367035
train_iter_loss: 0.09194155037403107
train_iter_loss: 0.17853739857673645
train_iter_loss: 0.050364769995212555
train_iter_loss: 0.1875499188899994
train_iter_loss: 0.07936064153909683
train_iter_loss: 0.16334956884384155
train_iter_loss: 0.10331439971923828
train_iter_loss: 0.19675859808921814
train_iter_loss: 0.20897501707077026
train_iter_loss: 0.07318872213363647
train_iter_loss: 0.10376404970884323
train_iter_loss: 0.13333189487457275
train_iter_loss: 0.1080828383564949
train_iter_loss: 0.15956400334835052
train_iter_loss: 0.18434672057628632
train_iter_loss: 0.04407788813114166
train_iter_loss: 0.09044131636619568
train_iter_loss: 0.11925087124109268
train_iter_loss: 0.1587008237838745
train_iter_loss: 0.18504294753074646
train_iter_loss: 0.17724710702896118
train_iter_loss: 0.1698235720396042
train_iter_loss: 0.20318925380706787
train_iter_loss: 0.22658324241638184
train_iter_loss: 0.1510145217180252
train_iter_loss: 0.21572409570217133
train_iter_loss: 0.1957799345254898
train_iter_loss: 0.2679089903831482
train_iter_loss: 0.13206124305725098
train_iter_loss: 0.14977587759494781
train_iter_loss: 0.09630799293518066
train_iter_loss: 0.13199231028556824
train_iter_loss: 0.11371234059333801
train_iter_loss: 0.12369035929441452
train_iter_loss: 0.10202904790639877
train_iter_loss: 0.17520380020141602
train_iter_loss: 0.13767367601394653
train_iter_loss: 0.5243878960609436
train_iter_loss: 0.17127439379692078
train loss :0.1629
---------------------
Validation seg loss: 0.22093885508805233 at epoch 751
epoch =    752/  1000, exp = train
train_iter_loss: 0.2826848328113556
train_iter_loss: 0.1719389110803604
train_iter_loss: 0.14366701245307922
train_iter_loss: 0.2602427899837494
train_iter_loss: 0.176121786236763
train_iter_loss: 0.14395850896835327
train_iter_loss: 0.25861018896102905
train_iter_loss: 0.22783561050891876
train_iter_loss: 0.15235193073749542
train_iter_loss: 0.13643036782741547
train_iter_loss: 0.16731783747673035
train_iter_loss: 0.14692328870296478
train_iter_loss: 0.224609375
train_iter_loss: 0.2287352830171585
train_iter_loss: 0.24302004277706146
train_iter_loss: 0.09698648005723953
train_iter_loss: 0.15014824271202087
train_iter_loss: 0.33498144149780273
train_iter_loss: 0.11782454699277878
train_iter_loss: 0.15843558311462402
train_iter_loss: 0.11729410290718079
train_iter_loss: 0.09794818609952927
train_iter_loss: 0.08986981213092804
train_iter_loss: 0.07013190537691116
train_iter_loss: 0.06697837263345718
train_iter_loss: 0.2912326753139496
train_iter_loss: 0.07632355391979218
train_iter_loss: 0.22113174200057983
train_iter_loss: 0.06675664335489273
train_iter_loss: 0.13695234060287476
train_iter_loss: 0.1923609972000122
train_iter_loss: 0.15530769526958466
train_iter_loss: 0.0784442126750946
train_iter_loss: 0.1506018340587616
train_iter_loss: 0.11705705523490906
train_iter_loss: 0.1655210256576538
train_iter_loss: 0.18598107993602753
train_iter_loss: 0.2394835650920868
train_iter_loss: 0.20985248684883118
train_iter_loss: 0.10604029893875122
train_iter_loss: 0.15185433626174927
train_iter_loss: 0.1323438286781311
train_iter_loss: 0.1943698674440384
train_iter_loss: 0.13351191580295563
train_iter_loss: 0.10991865396499634
train_iter_loss: 0.1585322469472885
train_iter_loss: 0.37900030612945557
train_iter_loss: 0.21581058204174042
train_iter_loss: 0.19885456562042236
train_iter_loss: 0.14189095795154572
train_iter_loss: 0.10683885961771011
train_iter_loss: 0.17267853021621704
train_iter_loss: 0.08638082444667816
train_iter_loss: 0.06203807145357132
train_iter_loss: 0.06647136807441711
train_iter_loss: 0.13262496888637543
train_iter_loss: 0.15616904199123383
train_iter_loss: 0.0957312136888504
train_iter_loss: 0.09710101783275604
train_iter_loss: 0.22283753752708435
train_iter_loss: 0.14160564541816711
train_iter_loss: 0.0796220451593399
train_iter_loss: 0.126974418759346
train_iter_loss: 0.09737618267536163
train_iter_loss: 0.09510589390993118
train_iter_loss: 0.13073639571666718
train_iter_loss: 0.12067145854234695
train_iter_loss: 0.1765095740556717
train_iter_loss: 0.13055892288684845
train_iter_loss: 0.2653331160545349
train_iter_loss: 0.18674664199352264
train_iter_loss: 0.21212519705295563
train_iter_loss: 0.07169303297996521
train_iter_loss: 0.25288453698158264
train_iter_loss: 0.15494193136692047
train_iter_loss: 0.08742564171552658
train_iter_loss: 0.26633796095848083
train_iter_loss: 0.0609562061727047
train_iter_loss: 0.0803060457110405
train_iter_loss: 0.1585758924484253
train_iter_loss: 0.12068063765764236
train_iter_loss: 0.17079319059848785
train_iter_loss: 0.18538393080234528
train_iter_loss: 0.26812809705734253
train_iter_loss: 0.0648435652256012
train_iter_loss: 0.1362186223268509
train_iter_loss: 0.095433309674263
train_iter_loss: 0.1288701444864273
train_iter_loss: 0.23066890239715576
train_iter_loss: 0.0847606211900711
train_iter_loss: 0.158841073513031
train_iter_loss: 0.15552176535129547
train_iter_loss: 0.14501558244228363
train_iter_loss: 0.11457730084657669
train_iter_loss: 0.24012848734855652
train_iter_loss: 0.16464033722877502
train_iter_loss: 0.25203683972358704
train_iter_loss: 0.2248399257659912
train_iter_loss: 0.11508809030056
train_iter_loss: 0.1307026892900467
train loss :0.1592
---------------------
Validation seg loss: 0.2172841106295445 at epoch 752
epoch =    753/  1000, exp = train
train_iter_loss: 0.07606598734855652
train_iter_loss: 0.15069977939128876
train_iter_loss: 0.13092456758022308
train_iter_loss: 0.17035099864006042
train_iter_loss: 0.07297737151384354
train_iter_loss: 0.08500643819570541
train_iter_loss: 0.15166260302066803
train_iter_loss: 0.1319803148508072
train_iter_loss: 0.0874512642621994
train_iter_loss: 0.21314233541488647
train_iter_loss: 0.0943206250667572
train_iter_loss: 0.1814609318971634
train_iter_loss: 0.18386152386665344
train_iter_loss: 0.07516872137784958
train_iter_loss: 0.10195085406303406
train_iter_loss: 0.1805088371038437
train_iter_loss: 0.02862321026623249
train_iter_loss: 0.08497914671897888
train_iter_loss: 0.18049739301204681
train_iter_loss: 0.12371919304132462
train_iter_loss: 0.23400497436523438
train_iter_loss: 0.12285567820072174
train_iter_loss: 0.16191889345645905
train_iter_loss: 0.13939379155635834
train_iter_loss: 0.12931519746780396
train_iter_loss: 0.19457858800888062
train_iter_loss: 0.09422233700752258
train_iter_loss: 0.14212454855442047
train_iter_loss: 0.13150450587272644
train_iter_loss: 0.11582092195749283
train_iter_loss: 0.23016206920146942
train_iter_loss: 0.07785315811634064
train_iter_loss: 0.1528792828321457
train_iter_loss: 0.26786041259765625
train_iter_loss: 0.20707793533802032
train_iter_loss: 0.2531389892101288
train_iter_loss: 0.1675761640071869
train_iter_loss: 0.23380199074745178
train_iter_loss: 0.12370497733354568
train_iter_loss: 0.13559626042842865
train_iter_loss: 0.3046943247318268
train_iter_loss: 0.16505543887615204
train_iter_loss: 0.13698743283748627
train_iter_loss: 0.13198059797286987
train_iter_loss: 0.12367909401655197
train_iter_loss: 0.08789506554603577
train_iter_loss: 0.14315223693847656
train_iter_loss: 0.183893084526062
train_iter_loss: 0.1519911140203476
train_iter_loss: 0.14250357449054718
train_iter_loss: 0.19715481996536255
train_iter_loss: 0.17864254117012024
train_iter_loss: 0.16606026887893677
train_iter_loss: 0.1382579803466797
train_iter_loss: 0.18219299614429474
train_iter_loss: 0.13005271553993225
train_iter_loss: 0.13442867994308472
train_iter_loss: 0.24735085666179657
train_iter_loss: 0.11176234483718872
train_iter_loss: 0.07813006639480591
train_iter_loss: 0.46223247051239014
train_iter_loss: 0.06258327513933182
train_iter_loss: 0.16623441874980927
train_iter_loss: 0.10544268786907196
train_iter_loss: 0.2723582983016968
train_iter_loss: 0.2365771383047104
train_iter_loss: 0.03877386823296547
train_iter_loss: 0.29183757305145264
train_iter_loss: 0.08512105792760849
train_iter_loss: 0.10478829592466354
train_iter_loss: 0.19829371571540833
train_iter_loss: 0.13059009611606598
train_iter_loss: 0.05476949363946915
train_iter_loss: 0.1451766937971115
train_iter_loss: 0.1862882375717163
train_iter_loss: 0.2658403813838959
train_iter_loss: 0.13025780022144318
train_iter_loss: 0.12097609788179398
train_iter_loss: 0.09898867458105087
train_iter_loss: 0.11652854084968567
train_iter_loss: 0.22302301228046417
train_iter_loss: 0.090253084897995
train_iter_loss: 0.13485901057720184
train_iter_loss: 0.1758841872215271
train_iter_loss: 0.21204052865505219
train_iter_loss: 0.12822914123535156
train_iter_loss: 0.2331506907939911
train_iter_loss: 0.17230436205863953
train_iter_loss: 0.14569005370140076
train_iter_loss: 0.1701454371213913
train_iter_loss: 0.24099232256412506
train_iter_loss: 0.19095320999622345
train_iter_loss: 0.10497035086154938
train_iter_loss: 0.1558205634355545
train_iter_loss: 0.1773516833782196
train_iter_loss: 0.12276896834373474
train_iter_loss: 0.184663787484169
train_iter_loss: 0.19192707538604736
train_iter_loss: 0.07594502717256546
train_iter_loss: 0.17971408367156982
train loss :0.1570
---------------------
Validation seg loss: 0.21869002224631467 at epoch 753
epoch =    754/  1000, exp = train
train_iter_loss: 0.2440222203731537
train_iter_loss: 0.23636184632778168
train_iter_loss: 0.19514040648937225
train_iter_loss: 0.18432004749774933
train_iter_loss: 0.20732992887496948
train_iter_loss: 0.1943608969449997
train_iter_loss: 0.05856435373425484
train_iter_loss: 0.11817425489425659
train_iter_loss: 0.16716790199279785
train_iter_loss: 0.14858505129814148
train_iter_loss: 0.08072472363710403
train_iter_loss: 0.19857542216777802
train_iter_loss: 0.12212925404310226
train_iter_loss: 0.07659388333559036
train_iter_loss: 0.13663093745708466
train_iter_loss: 0.15413688123226166
train_iter_loss: 0.16155259311199188
train_iter_loss: 0.21736614406108856
train_iter_loss: 0.17845110595226288
train_iter_loss: 0.22861172258853912
train_iter_loss: 0.23100773990154266
train_iter_loss: 0.13686685264110565
train_iter_loss: 0.19308039546012878
train_iter_loss: 0.05933425575494766
train_iter_loss: 0.15416274964809418
train_iter_loss: 0.0931127518415451
train_iter_loss: 0.09736986458301544
train_iter_loss: 0.08543956279754639
train_iter_loss: 0.15072676539421082
train_iter_loss: 0.30090782046318054
train_iter_loss: 0.16348636150360107
train_iter_loss: 0.06697987765073776
train_iter_loss: 0.035532739013433456
train_iter_loss: 0.19243045151233673
train_iter_loss: 0.1690569818019867
train_iter_loss: 0.163871631026268
train_iter_loss: 0.08553434163331985
train_iter_loss: 0.1923842877149582
train_iter_loss: 0.14120887219905853
train_iter_loss: 0.17570412158966064
train_iter_loss: 0.26021599769592285
train_iter_loss: 0.1434987485408783
train_iter_loss: 0.1530764251947403
train_iter_loss: 0.21294203400611877
train_iter_loss: 0.20202043652534485
train_iter_loss: 0.1741306483745575
train_iter_loss: 0.34469130635261536
train_iter_loss: 0.10500697791576385
train_iter_loss: 0.05333989858627319
train_iter_loss: 0.11901655793190002
train_iter_loss: 0.09086586534976959
train_iter_loss: 0.16474591195583344
train_iter_loss: 0.09506805986166
train_iter_loss: 0.19669167697429657
train_iter_loss: 0.20890814065933228
train_iter_loss: 0.19075794517993927
train_iter_loss: 0.0828932523727417
train_iter_loss: 0.1757482886314392
train_iter_loss: 0.1391012817621231
train_iter_loss: 0.10006526857614517
train_iter_loss: 0.17984873056411743
train_iter_loss: 0.17600588500499725
train_iter_loss: 0.11028648167848587
train_iter_loss: 0.14354638755321503
train_iter_loss: 0.20099158585071564
train_iter_loss: 0.08730967342853546
train_iter_loss: 0.11500825732946396
train_iter_loss: 0.2077566683292389
train_iter_loss: 0.2580227553844452
train_iter_loss: 0.16315209865570068
train_iter_loss: 0.16249775886535645
train_iter_loss: 0.10343112051486969
train_iter_loss: 0.09086236357688904
train_iter_loss: 0.08635516464710236
train_iter_loss: 0.12331955134868622
train_iter_loss: 0.18719711899757385
train_iter_loss: 0.19912831485271454
train_iter_loss: 0.23528333008289337
train_iter_loss: 0.13802386820316315
train_iter_loss: 0.16691991686820984
train_iter_loss: 0.11712717264890671
train_iter_loss: 0.2253628969192505
train_iter_loss: 0.22276058793067932
train_iter_loss: 0.16002899408340454
train_iter_loss: 0.1545751690864563
train_iter_loss: 0.10743007063865662
train_iter_loss: 0.09754053503274918
train_iter_loss: 0.09342914819717407
train_iter_loss: 0.1621442586183548
train_iter_loss: 0.2632300853729248
train_iter_loss: 0.13010580837726593
train_iter_loss: 0.2085759937763214
train_iter_loss: 0.0514829158782959
train_iter_loss: 0.1351282000541687
train_iter_loss: 0.14166200160980225
train_iter_loss: 0.16405698657035828
train_iter_loss: 0.13900572061538696
train_iter_loss: 0.08078742772340775
train_iter_loss: 0.14177599549293518
train_iter_loss: 0.284961998462677
train loss :0.1578
---------------------
Validation seg loss: 0.21232145972866215 at epoch 754
epoch =    755/  1000, exp = train
train_iter_loss: 0.15288637578487396
train_iter_loss: 0.13657070696353912
train_iter_loss: 0.07101379334926605
train_iter_loss: 0.15928025543689728
train_iter_loss: 0.1225772500038147
train_iter_loss: 0.20821522176265717
train_iter_loss: 0.19505545496940613
train_iter_loss: 0.13745398819446564
train_iter_loss: 0.22766545414924622
train_iter_loss: 0.1648293286561966
train_iter_loss: 0.10282228887081146
train_iter_loss: 0.3520890772342682
train_iter_loss: 0.4008713662624359
train_iter_loss: 0.10431777685880661
train_iter_loss: 0.15782122313976288
train_iter_loss: 0.08165881782770157
train_iter_loss: 0.19733862578868866
train_iter_loss: 0.12426452338695526
train_iter_loss: 0.08184859901666641
train_iter_loss: 0.11830876767635345
train_iter_loss: 0.11799776554107666
train_iter_loss: 0.14872623980045319
train_iter_loss: 0.30546486377716064
train_iter_loss: 0.11560371518135071
train_iter_loss: 0.07006193697452545
train_iter_loss: 0.14243586361408234
train_iter_loss: 0.03860968351364136
train_iter_loss: 0.12642386555671692
train_iter_loss: 0.0756295844912529
train_iter_loss: 0.16398362815380096
train_iter_loss: 0.16408520936965942
train_iter_loss: 0.24932970106601715
train_iter_loss: 0.19712497293949127
train_iter_loss: 0.12702684104442596
train_iter_loss: 0.15625399351119995
train_iter_loss: 0.1240936890244484
train_iter_loss: 0.2075280398130417
train_iter_loss: 0.10543999075889587
train_iter_loss: 0.0355411060154438
train_iter_loss: 0.2165139764547348
train_iter_loss: 0.08492180705070496
train_iter_loss: 0.1413891315460205
train_iter_loss: 0.07441436499357224
train_iter_loss: 0.06954984366893768
train_iter_loss: 0.10294324159622192
train_iter_loss: 0.1373899281024933
train_iter_loss: 0.15985676646232605
train_iter_loss: 0.11002268642187119
train_iter_loss: 0.12274222820997238
train_iter_loss: 0.18536189198493958
train_iter_loss: 0.13989987969398499
train_iter_loss: 0.1695714294910431
train_iter_loss: 0.10245441645383835
train_iter_loss: 0.17381347715854645
train_iter_loss: 0.14057177305221558
train_iter_loss: 0.14080680906772614
train_iter_loss: 0.31232425570487976
train_iter_loss: 0.07371040433645248
train_iter_loss: 0.06073853373527527
train_iter_loss: 0.23929139971733093
train_iter_loss: 0.19035755097866058
train_iter_loss: 0.15934047102928162
train_iter_loss: 0.14847834408283234
train_iter_loss: 0.22750940918922424
train_iter_loss: 0.10225159674882889
train_iter_loss: 0.0472998283803463
train_iter_loss: 0.0981588363647461
train_iter_loss: 0.22215372323989868
train_iter_loss: 0.3708057105541229
train_iter_loss: 0.10654750466346741
train_iter_loss: 0.3430498540401459
train_iter_loss: 0.15857285261154175
train_iter_loss: 0.20166541635990143
train_iter_loss: 0.2911752760410309
train_iter_loss: 0.17579969763755798
train_iter_loss: 0.1394275426864624
train_iter_loss: 0.12362787127494812
train_iter_loss: 0.11383874714374542
train_iter_loss: 0.21677254140377045
train_iter_loss: 0.1438259780406952
train_iter_loss: 0.18241699039936066
train_iter_loss: 0.06756964325904846
train_iter_loss: 0.17346765100955963
train_iter_loss: 0.13483262062072754
train_iter_loss: 0.11995755881071091
train_iter_loss: 0.17556974291801453
train_iter_loss: 0.10527023673057556
train_iter_loss: 0.17336530983448029
train_iter_loss: 0.17382410168647766
train_iter_loss: 0.10832563042640686
train_iter_loss: 0.07966993004083633
train_iter_loss: 0.18750201165676117
train_iter_loss: 0.05950671434402466
train_iter_loss: 0.09929509460926056
train_iter_loss: 0.1847633421421051
train_iter_loss: 0.2018786519765854
train_iter_loss: 0.15066079795360565
train_iter_loss: 0.21000653505325317
train_iter_loss: 0.12265379726886749
train_iter_loss: 0.23194727301597595
train loss :0.1561
---------------------
Validation seg loss: 0.21764517315434959 at epoch 755
epoch =    756/  1000, exp = train
train_iter_loss: 0.18945038318634033
train_iter_loss: 0.11459770053625107
train_iter_loss: 0.17507126927375793
train_iter_loss: 0.06882507354021072
train_iter_loss: 0.2201358824968338
train_iter_loss: 0.0910470113158226
train_iter_loss: 0.10275118798017502
train_iter_loss: 0.15449845790863037
train_iter_loss: 0.18377935886383057
train_iter_loss: 0.10850445181131363
train_iter_loss: 0.09165089577436447
train_iter_loss: 0.16449733078479767
train_iter_loss: 0.0882721021771431
train_iter_loss: 0.3008882701396942
train_iter_loss: 0.07533717155456543
train_iter_loss: 0.07648148387670517
train_iter_loss: 0.15868403017520905
train_iter_loss: 0.11048958450555801
train_iter_loss: 0.12781161069869995
train_iter_loss: 0.0833108127117157
train_iter_loss: 0.06702451407909393
train_iter_loss: 0.1285177320241928
train_iter_loss: 0.10054441541433334
train_iter_loss: 0.21095335483551025
train_iter_loss: 0.2050972729921341
train_iter_loss: 0.15048469603061676
train_iter_loss: 0.09558092802762985
train_iter_loss: 0.11541301012039185
train_iter_loss: 0.18904617428779602
train_iter_loss: 0.24511881172657013
train_iter_loss: 0.03704635053873062
train_iter_loss: 0.20795542001724243
train_iter_loss: 0.10378728806972504
train_iter_loss: 0.15216903388500214
train_iter_loss: 0.0984032154083252
train_iter_loss: 0.21128450334072113
train_iter_loss: 0.24730786681175232
train_iter_loss: 0.04030107706785202
train_iter_loss: 0.3184213936328888
train_iter_loss: 0.05353078618645668
train_iter_loss: 0.13998159766197205
train_iter_loss: 0.1288762241601944
train_iter_loss: 0.10827644914388657
train_iter_loss: 0.25772833824157715
train_iter_loss: 0.1625470519065857
train_iter_loss: 0.23413243889808655
train_iter_loss: 0.0922410860657692
train_iter_loss: 0.2971195578575134
train_iter_loss: 0.17575612664222717
train_iter_loss: 0.14166298508644104
train_iter_loss: 0.10865934193134308
train_iter_loss: 0.12787336111068726
train_iter_loss: 0.04884816333651543
train_iter_loss: 0.2944542169570923
train_iter_loss: 0.15099942684173584
train_iter_loss: 0.14937609434127808
train_iter_loss: 0.20051419734954834
train_iter_loss: 0.08748525381088257
train_iter_loss: 0.15898752212524414
train_iter_loss: 0.14816132187843323
train_iter_loss: 0.23352880775928497
train_iter_loss: 0.14443063735961914
train_iter_loss: 0.14004285633563995
train_iter_loss: 0.24330198764801025
train_iter_loss: 0.20032231509685516
train_iter_loss: 0.1678839772939682
train_iter_loss: 0.22328941524028778
train_iter_loss: 0.05801642686128616
train_iter_loss: 0.1399979293346405
train_iter_loss: 0.15595853328704834
train_iter_loss: 0.15260764956474304
train_iter_loss: 0.30444952845573425
train_iter_loss: 0.2125149667263031
train_iter_loss: 0.12803316116333008
train_iter_loss: 0.1505730003118515
train_iter_loss: 0.31015387177467346
train_iter_loss: 0.10631460696458817
train_iter_loss: 0.13272488117218018
train_iter_loss: 0.2578348219394684
train_iter_loss: 0.06367545574903488
train_iter_loss: 0.08752618730068207
train_iter_loss: 0.11799347400665283
train_iter_loss: 0.20472414791584015
train_iter_loss: 0.0792224258184433
train_iter_loss: 0.1025920882821083
train_iter_loss: 0.10261566191911697
train_iter_loss: 0.10450856387615204
train_iter_loss: 0.12099668383598328
train_iter_loss: 0.18019308149814606
train_iter_loss: 0.2032431811094284
train_iter_loss: 0.27280211448669434
train_iter_loss: 0.2390296906232834
train_iter_loss: 0.09616246074438095
train_iter_loss: 0.16563233733177185
train_iter_loss: 0.13045451045036316
train_iter_loss: 0.21090067923069
train_iter_loss: 0.13629232347011566
train_iter_loss: 0.18041399121284485
train_iter_loss: 0.20821799337863922
train_iter_loss: 0.15877151489257812
train loss :0.1566
---------------------
Validation seg loss: 0.21622361026634024 at epoch 756
epoch =    757/  1000, exp = train
train_iter_loss: 0.20044098794460297
train_iter_loss: 0.13662905991077423
train_iter_loss: 0.15904951095581055
train_iter_loss: 0.22118370234966278
train_iter_loss: 0.08154373615980148
train_iter_loss: 0.11838220804929733
train_iter_loss: 0.06304166465997696
train_iter_loss: 0.2444811463356018
train_iter_loss: 0.20678609609603882
train_iter_loss: 0.04525984823703766
train_iter_loss: 0.10241855680942535
train_iter_loss: 0.10095777362585068
train_iter_loss: 0.21104790270328522
train_iter_loss: 0.12187419831752777
train_iter_loss: 0.2702558636665344
train_iter_loss: 0.22797493636608124
train_iter_loss: 0.11298321932554245
train_iter_loss: 0.08738218992948532
train_iter_loss: 0.1401643306016922
train_iter_loss: 0.1920567750930786
train_iter_loss: 0.1752040982246399
train_iter_loss: 0.18918383121490479
train_iter_loss: 0.11018531769514084
train_iter_loss: 0.20173196494579315
train_iter_loss: 0.14567521214485168
train_iter_loss: 0.07793066650629044
train_iter_loss: 0.1446293294429779
train_iter_loss: 0.19534580409526825
train_iter_loss: 0.12788523733615875
train_iter_loss: 0.056679531931877136
train_iter_loss: 0.2713572084903717
train_iter_loss: 0.10098111629486084
train_iter_loss: 0.10389083623886108
train_iter_loss: 0.16511011123657227
train_iter_loss: 0.10143990814685822
train_iter_loss: 0.49804770946502686
train_iter_loss: 0.21484138071537018
train_iter_loss: 0.11710315197706223
train_iter_loss: 0.2382117211818695
train_iter_loss: 0.19818924367427826
train_iter_loss: 0.31991493701934814
train_iter_loss: 0.22240981459617615
train_iter_loss: 0.11099469661712646
train_iter_loss: 0.2003146857023239
train_iter_loss: 0.22257572412490845
train_iter_loss: 0.09395547956228256
train_iter_loss: 0.23147301375865936
train_iter_loss: 0.10254087299108505
train_iter_loss: 0.20881588757038116
train_iter_loss: 0.1283816695213318
train_iter_loss: 0.1747632622718811
train_iter_loss: 0.11204987019300461
train_iter_loss: 0.1408320665359497
train_iter_loss: 0.1855584979057312
train_iter_loss: 0.22354690730571747
train_iter_loss: 0.31675031781196594
train_iter_loss: 0.12316805124282837
train_iter_loss: 0.08012732118368149
train_iter_loss: 0.10155974328517914
train_iter_loss: 0.1551344096660614
train_iter_loss: 0.1606328785419464
train_iter_loss: 0.19829364120960236
train_iter_loss: 0.1314382702112198
train_iter_loss: 0.16886886954307556
train_iter_loss: 0.17896544933319092
train_iter_loss: 0.2358904927968979
train_iter_loss: 0.19675405323505402
train_iter_loss: 0.19844549894332886
train_iter_loss: 0.19113637506961823
train_iter_loss: 0.14407922327518463
train_iter_loss: 0.15607450902462006
train_iter_loss: 0.0988551452755928
train_iter_loss: 0.06284613907337189
train_iter_loss: 0.2170315831899643
train_iter_loss: 0.30436044931411743
train_iter_loss: 0.10649466514587402
train_iter_loss: 0.170021653175354
train_iter_loss: 0.10493118315935135
train_iter_loss: 0.13255037367343903
train_iter_loss: 0.15191145241260529
train_iter_loss: 0.15824921429157257
train_iter_loss: 0.08438163995742798
train_iter_loss: 0.19329077005386353
train_iter_loss: 0.1508876383304596
train_iter_loss: 0.10873030871152878
train_iter_loss: 0.22973161935806274
train_iter_loss: 0.18588809669017792
train_iter_loss: 0.0991142988204956
train_iter_loss: 0.18843679130077362
train_iter_loss: 0.13931506872177124
train_iter_loss: 0.04499266669154167
train_iter_loss: 0.345513254404068
train_iter_loss: 0.21573090553283691
train_iter_loss: 0.05964848771691322
train_iter_loss: 0.10519491136074066
train_iter_loss: 0.11574333906173706
train_iter_loss: 0.1793862134218216
train_iter_loss: 0.15319477021694183
train_iter_loss: 0.07443676888942719
train_iter_loss: 0.12458448857069016
train loss :0.1636
---------------------
Validation seg loss: 0.21388213910197593 at epoch 757
epoch =    758/  1000, exp = train
train_iter_loss: 0.12190715968608856
train_iter_loss: 0.10356597602367401
train_iter_loss: 0.35293135046958923
train_iter_loss: 0.18820373713970184
train_iter_loss: 0.18058158457279205
train_iter_loss: 0.09582803398370743
train_iter_loss: 0.09532851725816727
train_iter_loss: 0.12704585492610931
train_iter_loss: 0.18416865170001984
train_iter_loss: 0.14429882168769836
train_iter_loss: 0.0851643830537796
train_iter_loss: 0.13630995154380798
train_iter_loss: 0.10032688081264496
train_iter_loss: 0.04802635684609413
train_iter_loss: 0.13717563450336456
train_iter_loss: 0.11592072993516922
train_iter_loss: 0.07989831268787384
train_iter_loss: 0.14467623829841614
train_iter_loss: 0.16071709990501404
train_iter_loss: 0.2909982204437256
train_iter_loss: 0.17621393501758575
train_iter_loss: 0.12709274888038635
train_iter_loss: 0.1369047313928604
train_iter_loss: 0.2831995487213135
train_iter_loss: 0.06869591027498245
train_iter_loss: 0.26817837357521057
train_iter_loss: 0.15504229068756104
train_iter_loss: 0.17163118720054626
train_iter_loss: 0.15347735583782196
train_iter_loss: 0.1826830804347992
train_iter_loss: 0.0817965641617775
train_iter_loss: 0.04412315785884857
train_iter_loss: 0.14749641716480255
train_iter_loss: 0.13645868003368378
train_iter_loss: 0.08072647452354431
train_iter_loss: 0.09757492691278458
train_iter_loss: 0.21220248937606812
train_iter_loss: 0.21094472706317902
train_iter_loss: 0.11184029281139374
train_iter_loss: 0.25789400935173035
train_iter_loss: 0.2571292519569397
train_iter_loss: 0.16767385601997375
train_iter_loss: 0.10643592476844788
train_iter_loss: 0.12582042813301086
train_iter_loss: 0.10657627135515213
train_iter_loss: 0.137203648686409
train_iter_loss: 0.16337543725967407
train_iter_loss: 0.12600639462471008
train_iter_loss: 0.1931551694869995
train_iter_loss: 0.16400550305843353
train_iter_loss: 0.117484450340271
train_iter_loss: 0.20633447170257568
train_iter_loss: 0.3383670747280121
train_iter_loss: 0.21067385375499725
train_iter_loss: 0.06475843489170074
train_iter_loss: 0.1652434766292572
train_iter_loss: 0.1298019289970398
train_iter_loss: 0.17179575562477112
train_iter_loss: 0.10470777750015259
train_iter_loss: 0.19583195447921753
train_iter_loss: 0.2292199432849884
train_iter_loss: 0.11242026835680008
train_iter_loss: 0.13664525747299194
train_iter_loss: 0.37193533778190613
train_iter_loss: 0.09486186504364014
train_iter_loss: 0.07011666893959045
train_iter_loss: 0.09476590901613235
train_iter_loss: 0.13732343912124634
train_iter_loss: 0.15671266615390778
train_iter_loss: 0.14499440789222717
train_iter_loss: 0.07926113158464432
train_iter_loss: 0.18732744455337524
train_iter_loss: 0.24602168798446655
train_iter_loss: 0.08676441758871078
train_iter_loss: 0.2674728035926819
train_iter_loss: 0.09171043336391449
train_iter_loss: 0.1481238603591919
train_iter_loss: 0.0738607794046402
train_iter_loss: 0.12654584646224976
train_iter_loss: 0.11308891326189041
train_iter_loss: 0.13390810787677765
train_iter_loss: 0.16400764882564545
train_iter_loss: 0.15078188478946686
train_iter_loss: 0.15496303141117096
train_iter_loss: 0.17223408818244934
train_iter_loss: 0.13820289075374603
train_iter_loss: 0.18157343566417694
train_iter_loss: 0.17227716743946075
train_iter_loss: 0.19506900012493134
train_iter_loss: 0.3789384365081787
train_iter_loss: 0.06632834672927856
train_iter_loss: 0.2499673217535019
train_iter_loss: 0.17281199991703033
train_iter_loss: 0.16669636964797974
train_iter_loss: 0.06966450065374374
train_iter_loss: 0.0790843516588211
train_iter_loss: 0.1697147935628891
train_iter_loss: 0.1593303233385086
train_iter_loss: 0.18329362571239471
train_iter_loss: 0.08270597457885742
train loss :0.1562
---------------------
Validation seg loss: 0.21856367544112904 at epoch 758
epoch =    759/  1000, exp = train
train_iter_loss: 0.04350680857896805
train_iter_loss: 0.10046632587909698
train_iter_loss: 0.35403409600257874
train_iter_loss: 0.17746470868587494
train_iter_loss: 0.09759408980607986
train_iter_loss: 0.11137333512306213
train_iter_loss: 0.3482351303100586
train_iter_loss: 0.1431485265493393
train_iter_loss: 0.14469636976718903
train_iter_loss: 0.14196820557117462
train_iter_loss: 0.17197047173976898
train_iter_loss: 0.1734989881515503
train_iter_loss: 0.1343957483768463
train_iter_loss: 0.23074570298194885
train_iter_loss: 0.14644747972488403
train_iter_loss: 0.10734810680150986
train_iter_loss: 0.3739359974861145
train_iter_loss: 0.1540035605430603
train_iter_loss: 0.07027070969343185
train_iter_loss: 0.18932311236858368
train_iter_loss: 0.1646856665611267
train_iter_loss: 0.22428588569164276
train_iter_loss: 0.06595712900161743
train_iter_loss: 0.1539630889892578
train_iter_loss: 0.1272558718919754
train_iter_loss: 0.11702646315097809
train_iter_loss: 0.09422378987073898
train_iter_loss: 0.23969489336013794
train_iter_loss: 0.11028973758220673
train_iter_loss: 0.13388821482658386
train_iter_loss: 0.03847827762365341
train_iter_loss: 0.17542698979377747
train_iter_loss: 0.2499537467956543
train_iter_loss: 0.15041504800319672
train_iter_loss: 0.12347405403852463
train_iter_loss: 0.2922661304473877
train_iter_loss: 0.12160257995128632
train_iter_loss: 0.17990761995315552
train_iter_loss: 0.18544495105743408
train_iter_loss: 0.19436416029930115
train_iter_loss: 0.11995362490415573
train_iter_loss: 0.27195441722869873
train_iter_loss: 0.17475390434265137
train_iter_loss: 0.09056199342012405
train_iter_loss: 0.08897842466831207
train_iter_loss: 0.18739724159240723
train_iter_loss: 0.07099723070859909
train_iter_loss: 0.0923827588558197
train_iter_loss: 0.22547580301761627
train_iter_loss: 0.11619972437620163
train_iter_loss: 0.0775170773267746
train_iter_loss: 0.15296489000320435
train_iter_loss: 0.17957744002342224
train_iter_loss: 0.16094376146793365
train_iter_loss: 0.03850851207971573
train_iter_loss: 0.05409886687994003
train_iter_loss: 0.17342446744441986
train_iter_loss: 0.13814371824264526
train_iter_loss: 0.12601585686206818
train_iter_loss: 0.27233490347862244
train_iter_loss: 0.3068926930427551
train_iter_loss: 0.10197611153125763
train_iter_loss: 0.08992506563663483
train_iter_loss: 0.20558087527751923
train_iter_loss: 0.10121741145849228
train_iter_loss: 0.16997791826725006
train_iter_loss: 0.05076945945620537
train_iter_loss: 0.17739622294902802
train_iter_loss: 0.2434006929397583
train_iter_loss: 0.2418251782655716
train_iter_loss: 0.23163168132305145
train_iter_loss: 0.15628042817115784
train_iter_loss: 0.17290346324443817
train_iter_loss: 0.1596381515264511
train_iter_loss: 0.259272962808609
train_iter_loss: 0.1604641079902649
train_iter_loss: 0.07697482407093048
train_iter_loss: 0.1821172684431076
train_iter_loss: 0.19613762199878693
train_iter_loss: 0.17734724283218384
train_iter_loss: 0.16742223501205444
train_iter_loss: 0.19702044129371643
train_iter_loss: 0.19021248817443848
train_iter_loss: 0.10711678117513657
train_iter_loss: 0.10264282673597336
train_iter_loss: 0.2368779331445694
train_iter_loss: 0.18058471381664276
train_iter_loss: 0.18285010755062103
train_iter_loss: 0.14625096321105957
train_iter_loss: 0.11458582431077957
train_iter_loss: 0.17286108434200287
train_iter_loss: 0.1675182729959488
train_iter_loss: 0.09054304659366608
train_iter_loss: 0.11205758899450302
train_iter_loss: 0.16839717328548431
train_iter_loss: 0.3022536337375641
train_iter_loss: 0.07094070315361023
train_iter_loss: 0.20574647188186646
train_iter_loss: 0.13023364543914795
train_iter_loss: 0.08739925920963287
train loss :0.1612
---------------------
Validation seg loss: 0.21200086030546786 at epoch 759
epoch =    760/  1000, exp = train
train_iter_loss: 0.07389739900827408
train_iter_loss: 0.11336427927017212
train_iter_loss: 0.13262446224689484
train_iter_loss: 0.1579974889755249
train_iter_loss: 0.10490049421787262
train_iter_loss: 0.19259928166866302
train_iter_loss: 0.11214933544397354
train_iter_loss: 0.18831121921539307
train_iter_loss: 0.13637183606624603
train_iter_loss: 0.13372445106506348
train_iter_loss: 0.03798540681600571
train_iter_loss: 0.151169553399086
train_iter_loss: 0.2057962715625763
train_iter_loss: 0.18369066715240479
train_iter_loss: 0.18768741190433502
train_iter_loss: 0.16755613684654236
train_iter_loss: 0.2255822867155075
train_iter_loss: 0.13362886011600494
train_iter_loss: 0.19227196276187897
train_iter_loss: 0.16839659214019775
train_iter_loss: 0.26436281204223633
train_iter_loss: 0.1281200349330902
train_iter_loss: 0.2626359164714813
train_iter_loss: 0.06325334310531616
train_iter_loss: 0.19200892746448517
train_iter_loss: 0.16764099895954132
train_iter_loss: 0.22090168297290802
train_iter_loss: 0.1270192414522171
train_iter_loss: 0.17707857489585876
train_iter_loss: 0.13556614518165588
train_iter_loss: 0.1323922723531723
train_iter_loss: 0.10586274415254593
train_iter_loss: 0.18160764873027802
train_iter_loss: 0.109784796833992
train_iter_loss: 0.10421948879957199
train_iter_loss: 0.3171555995941162
train_iter_loss: 0.24235647916793823
train_iter_loss: 0.13207148015499115
train_iter_loss: 0.1610860675573349
train_iter_loss: 0.2756507694721222
train_iter_loss: 0.08464272320270538
train_iter_loss: 0.16779577732086182
train_iter_loss: 0.1339448094367981
train_iter_loss: 0.09581400454044342
train_iter_loss: 0.2693115770816803
train_iter_loss: 0.17474211752414703
train_iter_loss: 0.16489800810813904
train_iter_loss: 0.15882983803749084
train_iter_loss: 0.3079642951488495
train_iter_loss: 0.06542228162288666
train_iter_loss: 0.12369582056999207
train_iter_loss: 0.16682741045951843
train_iter_loss: 0.22327573597431183
train_iter_loss: 0.2819427251815796
train_iter_loss: 0.12285064160823822
train_iter_loss: 0.13535408675670624
train_iter_loss: 0.12963898479938507
train_iter_loss: 0.159804105758667
train_iter_loss: 0.09876741468906403
train_iter_loss: 0.1351042240858078
train_iter_loss: 0.1867559552192688
train_iter_loss: 0.0926542580127716
train_iter_loss: 0.48312753438949585
train_iter_loss: 0.08395359665155411
train_iter_loss: 0.07029548287391663
train_iter_loss: 0.15670529007911682
train_iter_loss: 0.1673567295074463
train_iter_loss: 0.16705270111560822
train_iter_loss: 0.11554766446352005
train_iter_loss: 0.1402006894350052
train_iter_loss: 0.36126798391342163
train_iter_loss: 0.2065291553735733
train_iter_loss: 0.1637365221977234
train_iter_loss: 0.22476725280284882
train_iter_loss: 0.0795111432671547
train_iter_loss: 0.046479351818561554
train_iter_loss: 0.05058218911290169
train_iter_loss: 0.03638199344277382
train_iter_loss: 0.19363659620285034
train_iter_loss: 0.19201934337615967
train_iter_loss: 0.2005203515291214
train_iter_loss: 0.1887366622686386
train_iter_loss: 0.17627528309822083
train_iter_loss: 0.13554830849170685
train_iter_loss: 0.3039845824241638
train_iter_loss: 0.12192806601524353
train_iter_loss: 0.04918239265680313
train_iter_loss: 0.25562822818756104
train_iter_loss: 0.03761988878250122
train_iter_loss: 0.06753616780042648
train_iter_loss: 0.09268177300691605
train_iter_loss: 0.17193825542926788
train_iter_loss: 0.22745758295059204
train_iter_loss: 0.09934674948453903
train_iter_loss: 0.19772273302078247
train_iter_loss: 0.1668325662612915
train_iter_loss: 0.07616765052080154
train_iter_loss: 0.14615562558174133
train_iter_loss: 0.0990101620554924
train_iter_loss: 0.19140377640724182
train loss :0.1609
---------------------
Validation seg loss: 0.21609546835446414 at epoch 760
epoch =    761/  1000, exp = train
train_iter_loss: 0.2062390297651291
train_iter_loss: 0.14044828712940216
train_iter_loss: 0.09350919723510742
train_iter_loss: 0.17549823224544525
train_iter_loss: 0.167899027466774
train_iter_loss: 0.1592831164598465
train_iter_loss: 0.12523500621318817
train_iter_loss: 0.17403057217597961
train_iter_loss: 0.11892709881067276
train_iter_loss: 0.18489862978458405
train_iter_loss: 0.10624554753303528
train_iter_loss: 0.12248308211565018
train_iter_loss: 0.10645900666713715
train_iter_loss: 0.08273965865373611
train_iter_loss: 0.12059323489665985
train_iter_loss: 0.10839155316352844
train_iter_loss: 0.10401365160942078
train_iter_loss: 0.2146690934896469
train_iter_loss: 0.19937646389007568
train_iter_loss: 0.1607503592967987
train_iter_loss: 0.04918695613741875
train_iter_loss: 0.119831882417202
train_iter_loss: 0.13393442332744598
train_iter_loss: 0.1457364410161972
train_iter_loss: 0.3002505600452423
train_iter_loss: 0.07182393223047256
train_iter_loss: 0.17619067430496216
train_iter_loss: 0.17177465558052063
train_iter_loss: 0.18291345238685608
train_iter_loss: 0.15942716598510742
train_iter_loss: 0.057925909757614136
train_iter_loss: 0.35572561621665955
train_iter_loss: 0.09546317905187607
train_iter_loss: 0.07836950570344925
train_iter_loss: 0.1585237681865692
train_iter_loss: 0.19301173090934753
train_iter_loss: 0.16898420453071594
train_iter_loss: 0.01753253862261772
train_iter_loss: 0.15430209040641785
train_iter_loss: 0.11814693361520767
train_iter_loss: 0.11042095720767975
train_iter_loss: 0.26175621151924133
train_iter_loss: 0.048570435494184494
train_iter_loss: 0.2755739986896515
train_iter_loss: 0.1205444261431694
train_iter_loss: 0.0871163085103035
train_iter_loss: 0.11958625912666321
train_iter_loss: 0.2175045758485794
train_iter_loss: 0.08196459710597992
train_iter_loss: 0.27972280979156494
train_iter_loss: 0.20685704052448273
train_iter_loss: 0.11976192146539688
train_iter_loss: 0.1307283490896225
train_iter_loss: 0.35457614064216614
train_iter_loss: 0.12991273403167725
train_iter_loss: 0.1439765989780426
train_iter_loss: 0.22593222558498383
train_iter_loss: 0.20749244093894958
train_iter_loss: 0.08797619491815567
train_iter_loss: 0.15041515231132507
train_iter_loss: 0.324956476688385
train_iter_loss: 0.2624688744544983
train_iter_loss: 0.14867332577705383
train_iter_loss: 0.04515727981925011
train_iter_loss: 0.17703919112682343
train_iter_loss: 0.16415870189666748
train_iter_loss: 0.11525419354438782
train_iter_loss: 0.10083113610744476
train_iter_loss: 0.19583100080490112
train_iter_loss: 0.17578960955142975
train_iter_loss: 0.13827240467071533
train_iter_loss: 0.25333458185195923
train_iter_loss: 0.07049866765737534
train_iter_loss: 0.16939327120780945
train_iter_loss: 0.2169708013534546
train_iter_loss: 0.24933385848999023
train_iter_loss: 0.03538098186254501
train_iter_loss: 0.15525831282138824
train_iter_loss: 0.3095601201057434
train_iter_loss: 0.27660542726516724
train_iter_loss: 0.26774951815605164
train_iter_loss: 0.15056663751602173
train_iter_loss: 0.211076021194458
train_iter_loss: 0.22338992357254028
train_iter_loss: 0.07076018303632736
train_iter_loss: 0.2860487401485443
train_iter_loss: 0.1701781004667282
train_iter_loss: 0.13677343726158142
train_iter_loss: 0.17597341537475586
train_iter_loss: 0.20636165142059326
train_iter_loss: 0.13189299404621124
train_iter_loss: 0.23572395741939545
train_iter_loss: 0.06526105850934982
train_iter_loss: 0.1098194494843483
train_iter_loss: 0.13983839750289917
train_iter_loss: 0.21818308532238007
train_iter_loss: 0.23161934316158295
train_iter_loss: 0.16407433152198792
train_iter_loss: 0.2193056344985962
train_iter_loss: 0.10893286764621735
train loss :0.1641
---------------------
Validation seg loss: 0.2150703007882496 at epoch 761
epoch =    762/  1000, exp = train
train_iter_loss: 0.1061473861336708
train_iter_loss: 0.14869461953639984
train_iter_loss: 0.32093456387519836
train_iter_loss: 0.08424534648656845
train_iter_loss: 0.17596442997455597
train_iter_loss: 0.13606618344783783
train_iter_loss: 0.1377280205488205
train_iter_loss: 0.15274013578891754
train_iter_loss: 0.08081672340631485
train_iter_loss: 0.1025979295372963
train_iter_loss: 0.09042172133922577
train_iter_loss: 0.13508020341396332
train_iter_loss: 0.12872380018234253
train_iter_loss: 0.2698768973350525
train_iter_loss: 0.10352730005979538
train_iter_loss: 0.16174422204494476
train_iter_loss: 0.2911667823791504
train_iter_loss: 0.1324312835931778
train_iter_loss: 0.137004092335701
train_iter_loss: 0.0878654196858406
train_iter_loss: 0.1589929163455963
train_iter_loss: 0.16044791042804718
train_iter_loss: 0.14871346950531006
train_iter_loss: 0.24402157962322235
train_iter_loss: 0.2140980362892151
train_iter_loss: 0.19771212339401245
train_iter_loss: 0.13179391622543335
train_iter_loss: 0.10991620272397995
train_iter_loss: 0.26162806153297424
train_iter_loss: 0.09934543073177338
train_iter_loss: 0.20137956738471985
train_iter_loss: 0.17758910357952118
train_iter_loss: 0.13873931765556335
train_iter_loss: 0.2436095029115677
train_iter_loss: 0.13699036836624146
train_iter_loss: 0.1085207387804985
train_iter_loss: 0.08546987175941467
train_iter_loss: 0.10833939909934998
train_iter_loss: 0.1323934942483902
train_iter_loss: 0.11036490648984909
train_iter_loss: 0.22194437682628632
train_iter_loss: 0.08360034972429276
train_iter_loss: 0.17693763971328735
train_iter_loss: 0.10892550647258759
train_iter_loss: 0.07585705816745758
train_iter_loss: 0.13949640095233917
train_iter_loss: 0.12736448645591736
train_iter_loss: 0.10545385628938675
train_iter_loss: 0.25965800881385803
train_iter_loss: 0.10699151456356049
train_iter_loss: 0.17395663261413574
train_iter_loss: 0.2451391965150833
train_iter_loss: 0.21362967789173126
train_iter_loss: 0.09194884449243546
train_iter_loss: 0.2556469142436981
train_iter_loss: 0.08484016358852386
train_iter_loss: 0.06525574624538422
train_iter_loss: 0.198287695646286
train_iter_loss: 0.06913549453020096
train_iter_loss: 0.11606822907924652
train_iter_loss: 0.28739133477211
train_iter_loss: 0.12106897681951523
train_iter_loss: 0.2619088888168335
train_iter_loss: 0.1443749964237213
train_iter_loss: 0.1358916163444519
train_iter_loss: 0.1465269774198532
train_iter_loss: 0.15016363561153412
train_iter_loss: 0.072175994515419
train_iter_loss: 0.22155159711837769
train_iter_loss: 0.17184151709079742
train_iter_loss: 0.14760726690292358
train_iter_loss: 0.1884501427412033
train_iter_loss: 0.28248295187950134
train_iter_loss: 0.1317661553621292
train_iter_loss: 0.12251050025224686
train_iter_loss: 0.07360132038593292
train_iter_loss: 0.1553603857755661
train_iter_loss: 0.12608186900615692
train_iter_loss: 0.2618831992149353
train_iter_loss: 0.13642112910747528
train_iter_loss: 0.141147181391716
train_iter_loss: 0.2366180419921875
train_iter_loss: 0.08908618986606598
train_iter_loss: 0.24765101075172424
train_iter_loss: 0.2815484404563904
train_iter_loss: 0.14330442249774933
train_iter_loss: 0.19002340734004974
train_iter_loss: 0.15248101949691772
train_iter_loss: 0.1878577470779419
train_iter_loss: 0.13387431204319
train_iter_loss: 0.13160252571105957
train_iter_loss: 0.19894109666347504
train_iter_loss: 0.22219792008399963
train_iter_loss: 0.33542564511299133
train_iter_loss: 0.16966785490512848
train_iter_loss: 0.1974300742149353
train_iter_loss: 0.1720578670501709
train_iter_loss: 0.19909407198429108
train_iter_loss: 0.14314493536949158
train_iter_loss: 0.1763962060213089
train loss :0.1633
---------------------
Validation seg loss: 0.21594095318841766 at epoch 762
epoch =    763/  1000, exp = train
train_iter_loss: 0.17292985320091248
train_iter_loss: 0.12211748212575912
train_iter_loss: 0.10131417214870453
train_iter_loss: 0.16366614401340485
train_iter_loss: 0.16552168130874634
train_iter_loss: 0.15057818591594696
train_iter_loss: 0.2229737937450409
train_iter_loss: 0.05923568457365036
train_iter_loss: 0.08642593771219254
train_iter_loss: 0.2069210708141327
train_iter_loss: 0.11767955124378204
train_iter_loss: 0.2865131199359894
train_iter_loss: 0.15739841759204865
train_iter_loss: 0.10074961930513382
train_iter_loss: 0.15591154992580414
train_iter_loss: 0.18850181996822357
train_iter_loss: 0.07762856036424637
train_iter_loss: 0.17506209015846252
train_iter_loss: 0.09497461467981339
train_iter_loss: 0.16257691383361816
train_iter_loss: 0.2193337082862854
train_iter_loss: 0.07384085655212402
train_iter_loss: 0.1052030399441719
train_iter_loss: 0.15291064977645874
train_iter_loss: 0.1478527933359146
train_iter_loss: 0.13473865389823914
train_iter_loss: 0.0629797950387001
train_iter_loss: 0.16652421653270721
train_iter_loss: 0.09317854046821594
train_iter_loss: 0.2025548666715622
train_iter_loss: 0.1050255224108696
train_iter_loss: 0.2200290560722351
train_iter_loss: 0.14964693784713745
train_iter_loss: 0.14051759243011475
train_iter_loss: 0.08952515572309494
train_iter_loss: 0.12816481292247772
train_iter_loss: 0.13159652054309845
train_iter_loss: 0.1825292408466339
train_iter_loss: 0.16644419729709625
train_iter_loss: 0.11370306462049484
train_iter_loss: 0.11374431848526001
train_iter_loss: 0.22821423411369324
train_iter_loss: 0.07335805147886276
train_iter_loss: 0.11559069901704788
train_iter_loss: 0.16163314878940582
train_iter_loss: 0.13653500378131866
train_iter_loss: 0.21066734194755554
train_iter_loss: 0.19975964725017548
train_iter_loss: 0.13627542555332184
train_iter_loss: 0.10362159460783005
train_iter_loss: 0.11326336860656738
train_iter_loss: 0.2576923072338104
train_iter_loss: 0.15636064112186432
train_iter_loss: 0.09542746096849442
train_iter_loss: 0.1972985863685608
train_iter_loss: 0.19126299023628235
train_iter_loss: 0.09897077828645706
train_iter_loss: 0.1219680905342102
train_iter_loss: 0.21432439982891083
train_iter_loss: 0.09379739314317703
train_iter_loss: 0.09899549931287766
train_iter_loss: 0.18060727417469025
train_iter_loss: 0.09338948130607605
train_iter_loss: 0.12869198620319366
train_iter_loss: 0.1753825843334198
train_iter_loss: 0.08080999553203583
train_iter_loss: 0.12621818482875824
train_iter_loss: 0.1637314260005951
train_iter_loss: 0.15736711025238037
train_iter_loss: 0.09544399380683899
train_iter_loss: 0.18916717171669006
train_iter_loss: 0.2889591157436371
train_iter_loss: 0.1896163523197174
train_iter_loss: 0.17109620571136475
train_iter_loss: 0.18856322765350342
train_iter_loss: 0.14147166907787323
train_iter_loss: 0.1854257732629776
train_iter_loss: 0.10936807841062546
train_iter_loss: 0.17902961373329163
train_iter_loss: 0.16243156790733337
train_iter_loss: 0.1069786325097084
train_iter_loss: 0.07244878262281418
train_iter_loss: 0.11652795970439911
train_iter_loss: 0.11358810216188431
train_iter_loss: 0.0727534145116806
train_iter_loss: 0.1410326212644577
train_iter_loss: 0.2959775924682617
train_iter_loss: 0.1358013153076172
train_iter_loss: 0.27831408381462097
train_iter_loss: 0.1366625875234604
train_iter_loss: 0.10988486558198929
train_iter_loss: 0.29228249192237854
train_iter_loss: 0.09841205924749374
train_iter_loss: 0.21621622145175934
train_iter_loss: 0.2999669909477234
train_iter_loss: 0.1377030462026596
train_iter_loss: 0.11701827496290207
train_iter_loss: 0.0582568421959877
train_iter_loss: 0.1055912971496582
train_iter_loss: 0.13962119817733765
train loss :0.1509
---------------------
Validation seg loss: 0.21638830555251465 at epoch 763
epoch =    764/  1000, exp = train
train_iter_loss: 0.18890899419784546
train_iter_loss: 0.2939451038837433
train_iter_loss: 0.15677860379219055
train_iter_loss: 0.06052656099200249
train_iter_loss: 0.20230989158153534
train_iter_loss: 0.11933109164237976
train_iter_loss: 0.12691757082939148
train_iter_loss: 0.050862524658441544
train_iter_loss: 0.17942297458648682
train_iter_loss: 0.1411260962486267
train_iter_loss: 0.13140463829040527
train_iter_loss: 0.14927735924720764
train_iter_loss: 0.0687987431883812
train_iter_loss: 0.11639418452978134
train_iter_loss: 0.2093391716480255
train_iter_loss: 0.0566759817302227
train_iter_loss: 0.0638272613286972
train_iter_loss: 0.20591121912002563
train_iter_loss: 0.2369505763053894
train_iter_loss: 0.16430671513080597
train_iter_loss: 0.23090194165706635
train_iter_loss: 0.15835951268672943
train_iter_loss: 0.17975616455078125
train_iter_loss: 0.15009190142154694
train_iter_loss: 0.03347332775592804
train_iter_loss: 0.201200932264328
train_iter_loss: 0.11102530360221863
train_iter_loss: 0.061938900500535965
train_iter_loss: 0.1412540078163147
train_iter_loss: 0.0777030810713768
train_iter_loss: 0.10904553532600403
train_iter_loss: 0.13022832572460175
train_iter_loss: 0.23134222626686096
train_iter_loss: 0.2265373170375824
train_iter_loss: 0.07567096501588821
train_iter_loss: 0.14075618982315063
train_iter_loss: 0.140451118350029
train_iter_loss: 0.11855877190828323
train_iter_loss: 0.14245551824569702
train_iter_loss: 0.261214017868042
train_iter_loss: 0.11039117723703384
train_iter_loss: 0.1631590873003006
train_iter_loss: 0.13003359735012054
train_iter_loss: 0.08966153860092163
train_iter_loss: 0.10887130349874496
train_iter_loss: 0.02347441203892231
train_iter_loss: 0.07323719561100006
train_iter_loss: 0.10759902000427246
train_iter_loss: 0.17812569439411163
train_iter_loss: 0.08310206979513168
train_iter_loss: 0.14065410196781158
train_iter_loss: 0.10057806968688965
train_iter_loss: 0.17536742985248566
train_iter_loss: 0.13984188437461853
train_iter_loss: 0.1169721931219101
train_iter_loss: 0.1450015753507614
train_iter_loss: 0.16953028738498688
train_iter_loss: 0.15604080259799957
train_iter_loss: 0.3904671370983124
train_iter_loss: 0.12510809302330017
train_iter_loss: 0.12629839777946472
train_iter_loss: 0.2612634301185608
train_iter_loss: 0.10304290056228638
train_iter_loss: 0.22815458476543427
train_iter_loss: 0.08489371091127396
train_iter_loss: 0.2854328453540802
train_iter_loss: 0.060151174664497375
train_iter_loss: 0.14119607210159302
train_iter_loss: 0.21454346179962158
train_iter_loss: 0.0806349441409111
train_iter_loss: 0.19521701335906982
train_iter_loss: 0.12820300459861755
train_iter_loss: 0.07615697383880615
train_iter_loss: 0.18093430995941162
train_iter_loss: 0.2332611382007599
train_iter_loss: 0.17444273829460144
train_iter_loss: 0.16359244287014008
train_iter_loss: 0.11066218465566635
train_iter_loss: 0.03675120696425438
train_iter_loss: 0.15803001821041107
train_iter_loss: 0.18805132806301117
train_iter_loss: 0.10866505652666092
train_iter_loss: 0.3229951858520508
train_iter_loss: 0.16885294020175934
train_iter_loss: 0.17235249280929565
train_iter_loss: 0.19748929142951965
train_iter_loss: 0.14567412436008453
train_iter_loss: 0.049827758222818375
train_iter_loss: 0.15989066660404205
train_iter_loss: 0.16697269678115845
train_iter_loss: 0.1705838441848755
train_iter_loss: 0.2299829125404358
train_iter_loss: 0.11127045005559921
train_iter_loss: 0.1223234087228775
train_iter_loss: 0.2713777422904968
train_iter_loss: 0.1912005990743637
train_iter_loss: 0.28748834133148193
train_iter_loss: 0.10130470246076584
train_iter_loss: 0.06052578240633011
train_iter_loss: 0.17830690741539001
train loss :0.1517
---------------------
Validation seg loss: 0.21865936175410478 at epoch 764
epoch =    765/  1000, exp = train
train_iter_loss: 0.12389679998159409
train_iter_loss: 0.11719094961881638
train_iter_loss: 0.09188461303710938
train_iter_loss: 0.23407910764217377
train_iter_loss: 0.3614085614681244
train_iter_loss: 0.22325634956359863
train_iter_loss: 0.03699077293276787
train_iter_loss: 0.13300979137420654
train_iter_loss: 0.2382434606552124
train_iter_loss: 0.11684734374284744
train_iter_loss: 0.11604722589254379
train_iter_loss: 0.18346387147903442
train_iter_loss: 0.10231126844882965
train_iter_loss: 0.14135944843292236
train_iter_loss: 0.2282353788614273
train_iter_loss: 0.1783374398946762
train_iter_loss: 0.11292368173599243
train_iter_loss: 0.12866702675819397
train_iter_loss: 0.11387866735458374
train_iter_loss: 0.1094748005270958
train_iter_loss: 0.19064924120903015
train_iter_loss: 0.19777663052082062
train_iter_loss: 0.16342926025390625
train_iter_loss: 0.1466316133737564
train_iter_loss: 0.1296810805797577
train_iter_loss: 0.16753830015659332
train_iter_loss: 0.2643851637840271
train_iter_loss: 0.1397918313741684
train_iter_loss: 0.14920371770858765
train_iter_loss: 0.09474820643663406
train_iter_loss: 0.11630381643772125
train_iter_loss: 0.17797617614269257
train_iter_loss: 0.1780785322189331
train_iter_loss: 0.2268948256969452
train_iter_loss: 0.158906489610672
train_iter_loss: 0.14447222650051117
train_iter_loss: 0.32217156887054443
train_iter_loss: 0.1268126219511032
train_iter_loss: 0.0864446759223938
train_iter_loss: 0.039278045296669006
train_iter_loss: 0.15124687552452087
train_iter_loss: 0.20245392620563507
train_iter_loss: 0.12331850081682205
train_iter_loss: 0.08948854357004166
train_iter_loss: 0.17716500163078308
train_iter_loss: 0.24485060572624207
train_iter_loss: 0.06634698063135147
train_iter_loss: 0.26143887639045715
train_iter_loss: 0.1068168357014656
train_iter_loss: 0.1440100371837616
train_iter_loss: 0.19010375440120697
train_iter_loss: 0.1710658222436905
train_iter_loss: 0.12633994221687317
train_iter_loss: 0.14907468855381012
train_iter_loss: 0.11431659758090973
train_iter_loss: 0.2626866102218628
train_iter_loss: 0.07597213983535767
train_iter_loss: 0.11537256836891174
train_iter_loss: 0.09905754029750824
train_iter_loss: 0.16646261513233185
train_iter_loss: 0.21174560487270355
train_iter_loss: 0.041316546499729156
train_iter_loss: 0.13533151149749756
train_iter_loss: 0.16585339605808258
train_iter_loss: 0.1421559751033783
train_iter_loss: 0.25393134355545044
train_iter_loss: 0.1937139332294464
train_iter_loss: 0.11912044882774353
train_iter_loss: 0.075924351811409
train_iter_loss: 0.2668447196483612
train_iter_loss: 0.2492935061454773
train_iter_loss: 0.14884871244430542
train_iter_loss: 0.10161557048559189
train_iter_loss: 0.09269405901432037
train_iter_loss: 0.09993115812540054
train_iter_loss: 0.18823054432868958
train_iter_loss: 0.20299983024597168
train_iter_loss: 0.20435567200183868
train_iter_loss: 0.20356661081314087
train_iter_loss: 0.10190238058567047
train_iter_loss: 0.07948899269104004
train_iter_loss: 0.1653541773557663
train_iter_loss: 0.06912727653980255
train_iter_loss: 0.1460168957710266
train_iter_loss: 0.1745244860649109
train_iter_loss: 0.4091469347476959
train_iter_loss: 0.09372538328170776
train_iter_loss: 0.15981417894363403
train_iter_loss: 0.13505688309669495
train_iter_loss: 0.15237419307231903
train_iter_loss: 0.204120472073555
train_iter_loss: 0.12597039341926575
train_iter_loss: 0.14461158215999603
train_iter_loss: 0.1558951586484909
train_iter_loss: 0.19469888508319855
train_iter_loss: 0.10258159786462784
train_iter_loss: 0.3290857970714569
train_iter_loss: 0.08366258442401886
train_iter_loss: 0.16881701350212097
train_iter_loss: 0.17136317491531372
train loss :0.1597
---------------------
Validation seg loss: 0.22314431074620136 at epoch 765
epoch =    766/  1000, exp = train
train_iter_loss: 0.15343120694160461
train_iter_loss: 0.16009964048862457
train_iter_loss: 0.09926710277795792
train_iter_loss: 0.13447017967700958
train_iter_loss: 0.06930433213710785
train_iter_loss: 0.26719939708709717
train_iter_loss: 0.2305791974067688
train_iter_loss: 0.1230592131614685
train_iter_loss: 0.11729636788368225
train_iter_loss: 0.12000495940446854
train_iter_loss: 0.1670287400484085
train_iter_loss: 0.18512408435344696
train_iter_loss: 0.07357030361890793
train_iter_loss: 0.11291835457086563
train_iter_loss: 0.16599582135677338
train_iter_loss: 0.12820707261562347
train_iter_loss: 0.09770023077726364
train_iter_loss: 0.18160668015480042
train_iter_loss: 0.19246774911880493
train_iter_loss: 0.20445267856121063
train_iter_loss: 0.18778115510940552
train_iter_loss: 0.13371294736862183
train_iter_loss: 0.17150619626045227
train_iter_loss: 0.10501372814178467
train_iter_loss: 0.15284186601638794
train_iter_loss: 0.16123425960540771
train_iter_loss: 0.278591126203537
train_iter_loss: 0.1059817522764206
train_iter_loss: 0.11920236796140671
train_iter_loss: 0.21685653924942017
train_iter_loss: 0.1409102976322174
train_iter_loss: 0.16846734285354614
train_iter_loss: 0.13214635848999023
train_iter_loss: 0.11706376820802689
train_iter_loss: 0.10316918790340424
train_iter_loss: 0.14954279363155365
train_iter_loss: 0.13616780936717987
train_iter_loss: 0.18374928832054138
train_iter_loss: 0.11195793747901917
train_iter_loss: 0.07179881632328033
train_iter_loss: 0.17679497599601746
train_iter_loss: 0.12429417669773102
train_iter_loss: 0.14597612619400024
train_iter_loss: 0.11473295092582703
train_iter_loss: 0.17134742438793182
train_iter_loss: 0.11658737063407898
train_iter_loss: 0.09594899415969849
train_iter_loss: 0.11739497631788254
train_iter_loss: 0.058205556124448776
train_iter_loss: 0.19840288162231445
train_iter_loss: 0.13661311566829681
train_iter_loss: 0.09504915028810501
train_iter_loss: 0.15559060871601105
train_iter_loss: 0.14152713119983673
train_iter_loss: 0.15760405361652374
train_iter_loss: 0.16165027022361755
train_iter_loss: 0.08905645459890366
train_iter_loss: 0.13121581077575684
train_iter_loss: 0.09857144206762314
train_iter_loss: 0.09085973352193832
train_iter_loss: 0.14940983057022095
train_iter_loss: 0.13893435895442963
train_iter_loss: 0.11656192690134048
train_iter_loss: 0.2182820588350296
train_iter_loss: 0.06008676812052727
train_iter_loss: 0.14991681277751923
train_iter_loss: 0.12266281247138977
train_iter_loss: 0.08291693031787872
train_iter_loss: 0.11250757426023483
train_iter_loss: 0.1220928356051445
train_iter_loss: 0.06588645279407501
train_iter_loss: 0.11821847409009933
train_iter_loss: 0.1264224350452423
train_iter_loss: 0.1390821486711502
train_iter_loss: 0.16162070631980896
train_iter_loss: 0.13683797419071198
train_iter_loss: 0.1107000857591629
train_iter_loss: 0.1744433343410492
train_iter_loss: 0.11486271768808365
train_iter_loss: 0.1561482846736908
train_iter_loss: 0.14827105402946472
train_iter_loss: 0.22477878630161285
train_iter_loss: 0.21035821735858917
train_iter_loss: 0.19829703867435455
train_iter_loss: 0.16316241025924683
train_iter_loss: 0.17385277152061462
train_iter_loss: 0.1901630163192749
train_iter_loss: 0.1528637409210205
train_iter_loss: 0.2118947058916092
train_iter_loss: 0.09402542561292648
train_iter_loss: 0.2272609919309616
train_iter_loss: 0.12170115113258362
train_iter_loss: 0.18581005930900574
train_iter_loss: 0.08324410021305084
train_iter_loss: 0.12693025171756744
train_iter_loss: 0.243482768535614
train_iter_loss: 0.31624600291252136
train_iter_loss: 0.16373546421527863
train_iter_loss: 0.30576515197753906
train_iter_loss: 0.13971172273159027
train loss :0.1493
---------------------
Validation seg loss: 0.2199925678378006 at epoch 766
epoch =    767/  1000, exp = train
train_iter_loss: 0.11914555728435516
train_iter_loss: 0.11757101863622665
train_iter_loss: 0.1643390655517578
train_iter_loss: 0.188351109623909
train_iter_loss: 0.10791115462779999
train_iter_loss: 0.08165103942155838
train_iter_loss: 0.152835413813591
train_iter_loss: 0.07087689638137817
train_iter_loss: 0.13405562937259674
train_iter_loss: 0.1896379292011261
train_iter_loss: 0.13997171819210052
train_iter_loss: 0.11755269765853882
train_iter_loss: 0.1511310636997223
train_iter_loss: 0.1612206995487213
train_iter_loss: 0.06385107338428497
train_iter_loss: 0.17128317058086395
train_iter_loss: 0.18037578463554382
train_iter_loss: 0.07029876857995987
train_iter_loss: 0.16557936370372772
train_iter_loss: 0.16448257863521576
train_iter_loss: 0.13462285697460175
train_iter_loss: 0.17570391297340393
train_iter_loss: 0.12569163739681244
train_iter_loss: 0.11744830012321472
train_iter_loss: 0.11699081212282181
train_iter_loss: 0.0604039691388607
train_iter_loss: 0.2221847027540207
train_iter_loss: 0.12592709064483643
train_iter_loss: 0.18231911957263947
train_iter_loss: 0.1463010162115097
train_iter_loss: 0.1839679777622223
train_iter_loss: 0.147031769156456
train_iter_loss: 0.0752362310886383
train_iter_loss: 0.4162040054798126
train_iter_loss: 0.12048441171646118
train_iter_loss: 0.07389293611049652
train_iter_loss: 0.15029771625995636
train_iter_loss: 0.09744688868522644
train_iter_loss: 0.059222932904958725
train_iter_loss: 0.12476446479558945
train_iter_loss: 0.15235422551631927
train_iter_loss: 0.28037896752357483
train_iter_loss: 0.1263401061296463
train_iter_loss: 0.12946796417236328
train_iter_loss: 0.12287115305662155
train_iter_loss: 0.10708822309970856
train_iter_loss: 0.20389173924922943
train_iter_loss: 0.11904200166463852
train_iter_loss: 0.12982460856437683
train_iter_loss: 0.16192154586315155
train_iter_loss: 0.1292807161808014
train_iter_loss: 0.0456179715692997
train_iter_loss: 0.11254722625017166
train_iter_loss: 0.16523732244968414
train_iter_loss: 0.1515648514032364
train_iter_loss: 0.1856256127357483
train_iter_loss: 0.21577608585357666
train_iter_loss: 0.13871122896671295
train_iter_loss: 0.17978398501873016
train_iter_loss: 0.13855215907096863
train_iter_loss: 0.1701027750968933
train_iter_loss: 0.3348800539970398
train_iter_loss: 0.13019828498363495
train_iter_loss: 0.20199096202850342
train_iter_loss: 0.16631175577640533
train_iter_loss: 0.07295778393745422
train_iter_loss: 0.25395092368125916
train_iter_loss: 0.2766449451446533
train_iter_loss: 0.1887211948633194
train_iter_loss: 0.2003657966852188
train_iter_loss: 0.11707901209592819
train_iter_loss: 0.08949530124664307
train_iter_loss: 0.13273008167743683
train_iter_loss: 0.27848151326179504
train_iter_loss: 0.04806341230869293
train_iter_loss: 0.25151950120925903
train_iter_loss: 0.11470138281583786
train_iter_loss: 0.2406153529882431
train_iter_loss: 0.17365270853042603
train_iter_loss: 0.14444364607334137
train_iter_loss: 0.12036024779081345
train_iter_loss: 0.11274439841508865
train_iter_loss: 0.13482770323753357
train_iter_loss: 0.14362841844558716
train_iter_loss: 0.12243833392858505
train_iter_loss: 0.047988612204790115
train_iter_loss: 0.2164367139339447
train_iter_loss: 0.04769553244113922
train_iter_loss: 0.14565443992614746
train_iter_loss: 0.2979625165462494
train_iter_loss: 0.2772063910961151
train_iter_loss: 0.21330644190311432
train_iter_loss: 0.17613066732883453
train_iter_loss: 0.2065393030643463
train_iter_loss: 0.08280874788761139
train_iter_loss: 0.2855990529060364
train_iter_loss: 0.06811846047639847
train_iter_loss: 0.049137137830257416
train_iter_loss: 0.1150800809264183
train_iter_loss: 0.140976220369339
train loss :0.1530
---------------------
Validation seg loss: 0.21932816717176223 at epoch 767
epoch =    768/  1000, exp = train
train_iter_loss: 0.14861516654491425
train_iter_loss: 0.2927475571632385
train_iter_loss: 0.27807581424713135
train_iter_loss: 0.12167114019393921
train_iter_loss: 0.2176341861486435
train_iter_loss: 0.07911166548728943
train_iter_loss: 0.23251406848430634
train_iter_loss: 0.1896885186433792
train_iter_loss: 0.12327605485916138
train_iter_loss: 0.12746144831180573
train_iter_loss: 0.1693943440914154
train_iter_loss: 0.16081401705741882
train_iter_loss: 0.1933358609676361
train_iter_loss: 0.12231148034334183
train_iter_loss: 0.14868786931037903
train_iter_loss: 0.07432004809379578
train_iter_loss: 0.10408000648021698
train_iter_loss: 0.12430526316165924
train_iter_loss: 0.18662352859973907
train_iter_loss: 0.11356443911790848
train_iter_loss: 0.09097036719322205
train_iter_loss: 0.14568141102790833
train_iter_loss: 0.24301859736442566
train_iter_loss: 0.22156253457069397
train_iter_loss: 0.16306112706661224
train_iter_loss: 0.12207949161529541
train_iter_loss: 0.16052033007144928
train_iter_loss: 0.10524721443653107
train_iter_loss: 0.2231186479330063
train_iter_loss: 0.10214666277170181
train_iter_loss: 0.18949361145496368
train_iter_loss: 0.16732777655124664
train_iter_loss: 0.08148090541362762
train_iter_loss: 0.14520491659641266
train_iter_loss: 0.14137756824493408
train_iter_loss: 0.19351960718631744
train_iter_loss: 0.1901090443134308
train_iter_loss: 0.16292743384838104
train_iter_loss: 0.12510307133197784
train_iter_loss: 0.10086307674646378
train_iter_loss: 0.2637541890144348
train_iter_loss: 0.11849910020828247
train_iter_loss: 0.2614879608154297
train_iter_loss: 0.08959116786718369
train_iter_loss: 0.13433833420276642
train_iter_loss: 0.25837576389312744
train_iter_loss: 0.22319893538951874
train_iter_loss: 0.1767193078994751
train_iter_loss: 0.08307892084121704
train_iter_loss: 0.12617987394332886
train_iter_loss: 0.15449969470500946
train_iter_loss: 0.17753858864307404
train_iter_loss: 0.18200065195560455
train_iter_loss: 0.19157114624977112
train_iter_loss: 0.16583459079265594
train_iter_loss: 0.054133255034685135
train_iter_loss: 0.20460525155067444
train_iter_loss: 0.1469779908657074
train_iter_loss: 0.13122029602527618
train_iter_loss: 0.22313250601291656
train_iter_loss: 0.1540566384792328
train_iter_loss: 0.24439601600170135
train_iter_loss: 0.20109234750270844
train_iter_loss: 0.1223519891500473
train_iter_loss: 0.06777412444353104
train_iter_loss: 0.0748196467757225
train_iter_loss: 0.15903493762016296
train_iter_loss: 0.1794310212135315
train_iter_loss: 0.13576564192771912
train_iter_loss: 0.10380140691995621
train_iter_loss: 0.1863522082567215
train_iter_loss: 0.10958926379680634
train_iter_loss: 0.06906590610742569
train_iter_loss: 0.0826389491558075
train_iter_loss: 0.17569993436336517
train_iter_loss: 0.07144110649824142
train_iter_loss: 0.18523050844669342
train_iter_loss: 0.07733979821205139
train_iter_loss: 0.10690697282552719
train_iter_loss: 0.20699554681777954
train_iter_loss: 0.0721709355711937
train_iter_loss: 0.23121587932109833
train_iter_loss: 0.1948493868112564
train_iter_loss: 0.070110984146595
train_iter_loss: 0.15690498054027557
train_iter_loss: 0.11648531258106232
train_iter_loss: 0.19489248096942902
train_iter_loss: 0.31296536326408386
train_iter_loss: 0.08886028081178665
train_iter_loss: 0.1375502496957779
train_iter_loss: 0.13722431659698486
train_iter_loss: 0.14335408806800842
train_iter_loss: 0.15804488956928253
train_iter_loss: 0.19606594741344452
train_iter_loss: 0.15633633732795715
train_iter_loss: 0.21701514720916748
train_iter_loss: 0.21099312603473663
train_iter_loss: 0.12800011038780212
train_iter_loss: 0.049956630915403366
train_iter_loss: 0.16392870247364044
train loss :0.1565
---------------------
Validation seg loss: 0.2120711351217667 at epoch 768
epoch =    769/  1000, exp = train
train_iter_loss: 0.25502511858940125
train_iter_loss: 0.06096551939845085
train_iter_loss: 0.04864049330353737
train_iter_loss: 0.09444523602724075
train_iter_loss: 0.33098915219306946
train_iter_loss: 0.120262011885643
train_iter_loss: 0.1786874383687973
train_iter_loss: 0.12552990019321442
train_iter_loss: 0.12150926887989044
train_iter_loss: 0.15159399807453156
train_iter_loss: 0.08704548329114914
train_iter_loss: 0.11107300966978073
train_iter_loss: 0.22541378438472748
train_iter_loss: 0.14057596027851105
train_iter_loss: 0.14585956931114197
train_iter_loss: 0.22082726657390594
train_iter_loss: 0.12276338785886765
train_iter_loss: 0.25547492504119873
train_iter_loss: 0.1515175998210907
train_iter_loss: 0.13622958958148956
train_iter_loss: 0.21466729044914246
train_iter_loss: 0.12360258400440216
train_iter_loss: 0.18963788449764252
train_iter_loss: 0.24090807139873505
train_iter_loss: 0.16077886521816254
train_iter_loss: 0.18792448937892914
train_iter_loss: 0.1485789716243744
train_iter_loss: 0.12557290494441986
train_iter_loss: 0.07075200229883194
train_iter_loss: 0.11442249268293381
train_iter_loss: 0.16116657853126526
train_iter_loss: 0.06347569078207016
train_iter_loss: 0.12355254590511322
train_iter_loss: 0.14606846868991852
train_iter_loss: 0.11784044653177261
train_iter_loss: 0.22689779102802277
train_iter_loss: 0.16158810257911682
train_iter_loss: 0.1358482837677002
train_iter_loss: 0.11644317209720612
train_iter_loss: 0.1555667668581009
train_iter_loss: 0.12458828836679459
train_iter_loss: 0.12097635865211487
train_iter_loss: 0.15847906470298767
train_iter_loss: 0.18747487664222717
train_iter_loss: 0.25392043590545654
train_iter_loss: 0.18436644971370697
train_iter_loss: 0.2399766743183136
train_iter_loss: 0.12681987881660461
train_iter_loss: 0.30032414197921753
train_iter_loss: 0.04649611935019493
train_iter_loss: 0.12103869020938873
train_iter_loss: 0.15778861939907074
train_iter_loss: 0.13063795864582062
train_iter_loss: 0.0952589139342308
train_iter_loss: 0.14545117318630219
train_iter_loss: 0.15120331943035126
train_iter_loss: 0.16314876079559326
train_iter_loss: 0.1512988805770874
train_iter_loss: 0.25614386796951294
train_iter_loss: 0.19791261851787567
train_iter_loss: 0.14466698467731476
train_iter_loss: 0.18711774051189423
train_iter_loss: 0.15621782839298248
train_iter_loss: 0.24642126262187958
train_iter_loss: 0.19321174919605255
train_iter_loss: 0.12374860793352127
train_iter_loss: 0.1611873358488083
train_iter_loss: 0.2562764883041382
train_iter_loss: 0.07747401297092438
train_iter_loss: 0.2009424865245819
train_iter_loss: 0.2158229798078537
train_iter_loss: 0.22070099413394928
train_iter_loss: 0.13333864510059357
train_iter_loss: 0.1618260145187378
train_iter_loss: 0.08358846604824066
train_iter_loss: 0.18078021705150604
train_iter_loss: 0.10834323614835739
train_iter_loss: 0.13124272227287292
train_iter_loss: 0.13964970409870148
train_iter_loss: 0.14985117316246033
train_iter_loss: 0.19214250147342682
train_iter_loss: 0.13522528111934662
train_iter_loss: 0.13028861582279205
train_iter_loss: 0.09914650768041611
train_iter_loss: 0.20746739208698273
train_iter_loss: 0.1887872964143753
train_iter_loss: 0.19975809752941132
train_iter_loss: 0.18792100250720978
train_iter_loss: 0.202924445271492
train_iter_loss: 0.054597705602645874
train_iter_loss: 0.12817788124084473
train_iter_loss: 0.13588398694992065
train_iter_loss: 0.19159714877605438
train_iter_loss: 0.17867517471313477
train_iter_loss: 0.08493874222040176
train_iter_loss: 0.1488238424062729
train_iter_loss: 0.1234632357954979
train_iter_loss: 0.15577827394008636
train_iter_loss: 0.0525314137339592
train_iter_loss: 0.04668833315372467
train loss :0.1566
---------------------
Validation seg loss: 0.21268075116787036 at epoch 769
epoch =    770/  1000, exp = train
train_iter_loss: 0.0947292298078537
train_iter_loss: 0.12329768389463425
train_iter_loss: 0.22452335059642792
train_iter_loss: 0.08589101582765579
train_iter_loss: 0.14556065201759338
train_iter_loss: 0.1375824511051178
train_iter_loss: 0.18736223876476288
train_iter_loss: 0.14817571640014648
train_iter_loss: 0.0706389918923378
train_iter_loss: 0.13296040892601013
train_iter_loss: 0.12125487625598907
train_iter_loss: 0.1039852499961853
train_iter_loss: 0.02902926877140999
train_iter_loss: 0.18074536323547363
train_iter_loss: 0.18092431128025055
train_iter_loss: 0.20730949938297272
train_iter_loss: 0.15740452706813812
train_iter_loss: 0.12725159525871277
train_iter_loss: 0.11525857448577881
train_iter_loss: 0.16178710758686066
train_iter_loss: 0.1545134335756302
train_iter_loss: 0.13759970664978027
train_iter_loss: 0.07775869220495224
train_iter_loss: 0.15910905599594116
train_iter_loss: 0.12958645820617676
train_iter_loss: 0.10541962087154388
train_iter_loss: 0.1502697616815567
train_iter_loss: 0.06915528327226639
train_iter_loss: 0.07744694501161575
train_iter_loss: 0.13240569829940796
train_iter_loss: 0.1523638516664505
train_iter_loss: 0.09194609522819519
train_iter_loss: 0.18676263093948364
train_iter_loss: 0.23393845558166504
train_iter_loss: 0.3322892487049103
train_iter_loss: 0.10708042234182358
train_iter_loss: 0.22253891825675964
train_iter_loss: 0.21498319506645203
train_iter_loss: 0.26630303263664246
train_iter_loss: 0.06679410487413406
train_iter_loss: 0.09898234158754349
train_iter_loss: 0.111933134496212
train_iter_loss: 0.2282501757144928
train_iter_loss: 0.05480078607797623
train_iter_loss: 0.26500633358955383
train_iter_loss: 0.12626633048057556
train_iter_loss: 0.11412344127893448
train_iter_loss: 0.16217707097530365
train_iter_loss: 0.16590899229049683
train_iter_loss: 0.2616124451160431
train_iter_loss: 0.11225737631320953
train_iter_loss: 0.14044228196144104
train_iter_loss: 0.14897459745407104
train_iter_loss: 0.1210341826081276
train_iter_loss: 0.41759926080703735
train_iter_loss: 0.08113342523574829
train_iter_loss: 0.2810373902320862
train_iter_loss: 0.0919010117650032
train_iter_loss: 0.225619375705719
train_iter_loss: 0.11976079642772675
train_iter_loss: 0.11785665899515152
train_iter_loss: 0.1734519600868225
train_iter_loss: 0.09809271991252899
train_iter_loss: 0.1256549060344696
train_iter_loss: 0.2127673476934433
train_iter_loss: 0.13803482055664062
train_iter_loss: 0.07615530490875244
train_iter_loss: 0.07047328352928162
train_iter_loss: 0.15771964192390442
train_iter_loss: 0.20049965381622314
train_iter_loss: 0.20423941314220428
train_iter_loss: 0.10165402293205261
train_iter_loss: 0.043363459408283234
train_iter_loss: 0.14886289834976196
train_iter_loss: 0.16471539437770844
train_iter_loss: 0.15051360428333282
train_iter_loss: 0.08814152330160141
train_iter_loss: 0.10419005155563354
train_iter_loss: 0.13976097106933594
train_iter_loss: 0.08593259006738663
train_iter_loss: 0.2019716203212738
train_iter_loss: 0.12022357434034348
train_iter_loss: 0.12071393430233002
train_iter_loss: 0.14651328325271606
train_iter_loss: 0.16856735944747925
train_iter_loss: 0.32946062088012695
train_iter_loss: 0.14706532657146454
train_iter_loss: 0.14007200300693512
train_iter_loss: 0.09342316538095474
train_iter_loss: 0.13917575776576996
train_iter_loss: 0.23073911666870117
train_iter_loss: 0.18391253054141998
train_iter_loss: 0.22794252634048462
train_iter_loss: 0.22951090335845947
train_iter_loss: 0.20567217469215393
train_iter_loss: 0.1412927359342575
train_iter_loss: 0.15397924184799194
train_iter_loss: 0.12617237865924835
train_iter_loss: 0.25320056080818176
train_iter_loss: 0.16804158687591553
train loss :0.1542
---------------------
Validation seg loss: 0.21391549397189663 at epoch 770
epoch =    771/  1000, exp = train
train_iter_loss: 0.1348588913679123
train_iter_loss: 0.12803122401237488
train_iter_loss: 0.10645145922899246
train_iter_loss: 0.18449881672859192
train_iter_loss: 0.130263552069664
train_iter_loss: 0.16221870481967926
train_iter_loss: 0.1059437170624733
train_iter_loss: 0.19208458065986633
train_iter_loss: 0.12286840379238129
train_iter_loss: 0.18654115498065948
train_iter_loss: 0.28135254979133606
train_iter_loss: 0.15442736446857452
train_iter_loss: 0.07358768582344055
train_iter_loss: 0.12179146707057953
train_iter_loss: 0.14882782101631165
train_iter_loss: 0.13517114520072937
train_iter_loss: 0.08538755029439926
train_iter_loss: 0.2557576298713684
train_iter_loss: 0.20924441516399384
train_iter_loss: 0.09398627281188965
train_iter_loss: 0.14558440446853638
train_iter_loss: 0.2782800793647766
train_iter_loss: 0.1340140551328659
train_iter_loss: 0.2103799730539322
train_iter_loss: 0.22950740158557892
train_iter_loss: 0.17348487675189972
train_iter_loss: 0.11806879937648773
train_iter_loss: 0.16921167075634003
train_iter_loss: 0.1982664316892624
train_iter_loss: 0.253078818321228
train_iter_loss: 0.12220317870378494
train_iter_loss: 0.19927449524402618
train_iter_loss: 0.08325675129890442
train_iter_loss: 0.2406327873468399
train_iter_loss: 0.16586102545261383
train_iter_loss: 0.20010696351528168
train_iter_loss: 0.2251741737127304
train_iter_loss: 0.1405082792043686
train_iter_loss: 0.2103467881679535
train_iter_loss: 0.21714118123054504
train_iter_loss: 0.22234463691711426
train_iter_loss: 0.11672185361385345
train_iter_loss: 0.15082968771457672
train_iter_loss: 0.10773008316755295
train_iter_loss: 0.2372448891401291
train_iter_loss: 0.10269170254468918
train_iter_loss: 0.1231348067522049
train_iter_loss: 0.14070294797420502
train_iter_loss: 0.09043729305267334
train_iter_loss: 0.1343671828508377
train_iter_loss: 0.1384803056716919
train_iter_loss: 0.14267268776893616
train_iter_loss: 0.1471259891986847
train_iter_loss: 0.14583106338977814
train_iter_loss: 0.14789512753486633
train_iter_loss: 0.06199006736278534
train_iter_loss: 0.1566847711801529
train_iter_loss: 0.139430433511734
train_iter_loss: 0.22072596848011017
train_iter_loss: 0.06859556585550308
train_iter_loss: 0.1302042007446289
train_iter_loss: 0.12696203589439392
train_iter_loss: 0.14877869188785553
train_iter_loss: 0.22276534140110016
train_iter_loss: 0.13709348440170288
train_iter_loss: 0.13501881062984467
train_iter_loss: 0.14303083717823029
train_iter_loss: 0.1408408284187317
train_iter_loss: 0.2432706505060196
train_iter_loss: 0.1519181877374649
train_iter_loss: 0.20070941746234894
train_iter_loss: 0.22605638206005096
train_iter_loss: 0.40957316756248474
train_iter_loss: 0.06555762887001038
train_iter_loss: 0.2202572375535965
train_iter_loss: 0.11564779281616211
train_iter_loss: 0.1978183537721634
train_iter_loss: 0.17872284352779388
train_iter_loss: 0.15559126436710358
train_iter_loss: 0.14811216294765472
train_iter_loss: 0.12480513751506805
train_iter_loss: 0.12946096062660217
train_iter_loss: 0.17390772700309753
train_iter_loss: 0.07440192997455597
train_iter_loss: 0.16304154694080353
train_iter_loss: 0.18557901680469513
train_iter_loss: 0.18686775863170624
train_iter_loss: 0.04379983991384506
train_iter_loss: 0.10657535493373871
train_iter_loss: 0.11689198762178421
train_iter_loss: 0.20546087622642517
train_iter_loss: 0.034540899097919464
train_iter_loss: 0.1906641721725464
train_iter_loss: 0.16730950772762299
train_iter_loss: 0.16057188808918
train_iter_loss: 0.24141867458820343
train_iter_loss: 0.07099993526935577
train_iter_loss: 0.1522926539182663
train_iter_loss: 0.1262313425540924
train_iter_loss: 0.14006507396697998
train loss :0.1597
---------------------
Validation seg loss: 0.21679647303766236 at epoch 771
epoch =    772/  1000, exp = train
train_iter_loss: 0.1633947193622589
train_iter_loss: 0.10852275043725967
train_iter_loss: 0.2612433433532715
train_iter_loss: 0.12266407907009125
train_iter_loss: 0.1783914715051651
train_iter_loss: 0.1486690640449524
train_iter_loss: 0.12182550132274628
train_iter_loss: 0.0935223177075386
train_iter_loss: 0.1230873093008995
train_iter_loss: 0.19770961999893188
train_iter_loss: 0.16446921229362488
train_iter_loss: 0.17862626910209656
train_iter_loss: 0.20345108211040497
train_iter_loss: 0.14139127731323242
train_iter_loss: 0.18302419781684875
train_iter_loss: 0.18188434839248657
train_iter_loss: 0.11814982444047928
train_iter_loss: 0.03361038863658905
train_iter_loss: 0.18357250094413757
train_iter_loss: 0.15033113956451416
train_iter_loss: 0.2958593964576721
train_iter_loss: 0.07932166010141373
train_iter_loss: 0.20400361716747284
train_iter_loss: 0.2675580382347107
train_iter_loss: 0.2355177104473114
train_iter_loss: 0.15767264366149902
train_iter_loss: 0.1616743803024292
train_iter_loss: 0.17727549374103546
train_iter_loss: 0.16028258204460144
train_iter_loss: 0.20961135625839233
train_iter_loss: 0.23024412989616394
train_iter_loss: 0.14275528490543365
train_iter_loss: 0.10877227038145065
train_iter_loss: 0.20813319087028503
train_iter_loss: 0.22958096861839294
train_iter_loss: 0.1888134479522705
train_iter_loss: 0.06202436238527298
train_iter_loss: 0.14466062188148499
train_iter_loss: 0.18273067474365234
train_iter_loss: 0.11876724660396576
train_iter_loss: 0.12318189442157745
train_iter_loss: 0.10205831378698349
train_iter_loss: 0.32279667258262634
train_iter_loss: 0.1215476468205452
train_iter_loss: 0.3294227123260498
train_iter_loss: 0.191769540309906
train_iter_loss: 0.24731366336345673
train_iter_loss: 0.15279798209667206
train_iter_loss: 0.13295035064220428
train_iter_loss: 0.1265811026096344
train_iter_loss: 0.12612241506576538
train_iter_loss: 0.13889171183109283
train_iter_loss: 0.07181913405656815
train_iter_loss: 0.1459716260433197
train_iter_loss: 0.14933249354362488
train_iter_loss: 0.09861543774604797
train_iter_loss: 0.258999228477478
train_iter_loss: 0.15135304629802704
train_iter_loss: 0.16576555371284485
train_iter_loss: 0.13691477477550507
train_iter_loss: 0.1953248381614685
train_iter_loss: 0.1309528648853302
train_iter_loss: 0.10106402635574341
train_iter_loss: 0.16369272768497467
train_iter_loss: 0.13084028661251068
train_iter_loss: 0.2214871346950531
train_iter_loss: 0.1845577359199524
train_iter_loss: 0.09969152510166168
train_iter_loss: 0.06311528384685516
train_iter_loss: 0.19979703426361084
train_iter_loss: 0.11361102759838104
train_iter_loss: 0.13684186339378357
train_iter_loss: 0.04272792488336563
train_iter_loss: 0.22077754139900208
train_iter_loss: 0.4999407231807709
train_iter_loss: 0.17456375062465668
train_iter_loss: 0.1344062238931656
train_iter_loss: 0.10713203251361847
train_iter_loss: 0.1645396649837494
train_iter_loss: 0.12032914906740189
train_iter_loss: 0.08818759024143219
train_iter_loss: 0.15485338866710663
train_iter_loss: 0.15616899728775024
train_iter_loss: 0.1763107031583786
train_iter_loss: 0.05978149175643921
train_iter_loss: 0.10611344873905182
train_iter_loss: 0.17037612199783325
train_iter_loss: 0.0643889307975769
train_iter_loss: 0.05792359262704849
train_iter_loss: 0.15980641543865204
train_iter_loss: 0.15806864202022552
train_iter_loss: 0.18241307139396667
train_iter_loss: 0.08191992342472076
train_iter_loss: 0.17857231199741364
train_iter_loss: 0.07127247005701065
train_iter_loss: 0.1722540557384491
train_iter_loss: 0.2001020908355713
train_iter_loss: 0.12781427800655365
train_iter_loss: 0.14180707931518555
train_iter_loss: 0.22502392530441284
train loss :0.1602
---------------------
Validation seg loss: 0.21459160727571766 at epoch 772
epoch =    773/  1000, exp = train
train_iter_loss: 0.09287126362323761
train_iter_loss: 0.14963993430137634
train_iter_loss: 0.15887099504470825
train_iter_loss: 0.10635773837566376
train_iter_loss: 0.07629675418138504
train_iter_loss: 0.16886179149150848
train_iter_loss: 0.1476605236530304
train_iter_loss: 0.18924422562122345
train_iter_loss: 0.1197735071182251
train_iter_loss: 0.12924659252166748
train_iter_loss: 0.08319184929132462
train_iter_loss: 0.21146799623966217
train_iter_loss: 0.08353668451309204
train_iter_loss: 0.2079595923423767
train_iter_loss: 0.18791359663009644
train_iter_loss: 0.19717617332935333
train_iter_loss: 0.14531075954437256
train_iter_loss: 0.24417147040367126
train_iter_loss: 0.20645016431808472
train_iter_loss: 0.11920768022537231
train_iter_loss: 0.05179470404982567
train_iter_loss: 0.18187807500362396
train_iter_loss: 0.16233596205711365
train_iter_loss: 0.09584890305995941
train_iter_loss: 0.2935807704925537
train_iter_loss: 0.19813963770866394
train_iter_loss: 0.18603235483169556
train_iter_loss: 0.2070707231760025
train_iter_loss: 0.19015729427337646
train_iter_loss: 0.056290216743946075
train_iter_loss: 0.09145537763834
train_iter_loss: 0.14074820280075073
train_iter_loss: 0.07670760899782181
train_iter_loss: 0.08694233000278473
train_iter_loss: 0.2066393941640854
train_iter_loss: 0.15262320637702942
train_iter_loss: 0.08870277553796768
train_iter_loss: 0.1681373119354248
train_iter_loss: 0.15264523029327393
train_iter_loss: 0.12183128297328949
train_iter_loss: 0.14605744183063507
train_iter_loss: 0.22570839524269104
train_iter_loss: 0.23063793778419495
train_iter_loss: 0.12010964751243591
train_iter_loss: 0.12563715875148773
train_iter_loss: 0.07179766148328781
train_iter_loss: 0.22105848789215088
train_iter_loss: 0.16899089515209198
train_iter_loss: 0.1439913511276245
train_iter_loss: 0.1451292335987091
train_iter_loss: 0.059195999056100845
train_iter_loss: 0.21357034146785736
train_iter_loss: 0.16287674009799957
train_iter_loss: 0.14752529561519623
train_iter_loss: 0.2278125286102295
train_iter_loss: 0.1509733498096466
train_iter_loss: 0.05951791629195213
train_iter_loss: 0.13678516447544098
train_iter_loss: 0.18285924196243286
train_iter_loss: 0.12099496275186539
train_iter_loss: 0.10219330340623856
train_iter_loss: 0.11755197495222092
train_iter_loss: 0.1518426239490509
train_iter_loss: 0.31836339831352234
train_iter_loss: 0.20119608938694
train_iter_loss: 0.2072661966085434
train_iter_loss: 0.1880868375301361
train_iter_loss: 0.18219678103923798
train_iter_loss: 0.10347329080104828
train_iter_loss: 0.18266256153583527
train_iter_loss: 0.11641864478588104
train_iter_loss: 0.14357081055641174
train_iter_loss: 0.08399376273155212
train_iter_loss: 0.25370439887046814
train_iter_loss: 0.07834479212760925
train_iter_loss: 0.1955804079771042
train_iter_loss: 0.25923165678977966
train_iter_loss: 0.1788194328546524
train_iter_loss: 0.14589138329029083
train_iter_loss: 0.10841740667819977
train_iter_loss: 0.27733519673347473
train_iter_loss: 0.08749722689390182
train_iter_loss: 0.1305065155029297
train_iter_loss: 0.21373681724071503
train_iter_loss: 0.089667908847332
train_iter_loss: 0.20582056045532227
train_iter_loss: 0.14897529780864716
train_iter_loss: 0.1003919392824173
train_iter_loss: 0.2352667599916458
train_iter_loss: 0.13585638999938965
train_iter_loss: 0.16410230100154877
train_iter_loss: 0.20291204750537872
train_iter_loss: 0.12067539244890213
train_iter_loss: 0.08038648217916489
train_iter_loss: 0.11779114603996277
train_iter_loss: 0.20108559727668762
train_iter_loss: 0.23221075534820557
train_iter_loss: 0.13332228362560272
train_iter_loss: 0.25238147377967834
train_iter_loss: 0.03127443790435791
train loss :0.1564
---------------------
Validation seg loss: 0.21139564751735274 at epoch 773
epoch =    774/  1000, exp = train
train_iter_loss: 0.06781098991632462
train_iter_loss: 0.06202707067131996
train_iter_loss: 0.13668571412563324
train_iter_loss: 0.13542650640010834
train_iter_loss: 0.20432695746421814
train_iter_loss: 0.14111612737178802
train_iter_loss: 0.2694689929485321
train_iter_loss: 0.13442401587963104
train_iter_loss: 0.1747441440820694
train_iter_loss: 0.07333032041788101
train_iter_loss: 0.1206255778670311
train_iter_loss: 0.10733450204133987
train_iter_loss: 0.12979774177074432
train_iter_loss: 0.2164798229932785
train_iter_loss: 0.10926470905542374
train_iter_loss: 0.12584397196769714
train_iter_loss: 0.19990788400173187
train_iter_loss: 0.07908884435892105
train_iter_loss: 0.08299406617879868
train_iter_loss: 0.2361115664243698
train_iter_loss: 0.13095727562904358
train_iter_loss: 0.1445951759815216
train_iter_loss: 0.23618969321250916
train_iter_loss: 0.18575510382652283
train_iter_loss: 0.10148558765649796
train_iter_loss: 0.10962242633104324
train_iter_loss: 0.14837755262851715
train_iter_loss: 0.22137707471847534
train_iter_loss: 0.07855363190174103
train_iter_loss: 0.15226441621780396
train_iter_loss: 0.13985086977481842
train_iter_loss: 0.10678993910551071
train_iter_loss: 0.20435571670532227
train_iter_loss: 0.10310657322406769
train_iter_loss: 0.13196413218975067
train_iter_loss: 0.25382140278816223
train_iter_loss: 0.16570481657981873
train_iter_loss: 0.18576814234256744
train_iter_loss: 0.15388870239257812
train_iter_loss: 0.06965403258800507
train_iter_loss: 0.21119116246700287
train_iter_loss: 0.2889978587627411
train_iter_loss: 0.1570442169904709
train_iter_loss: 0.21628378331661224
train_iter_loss: 0.36062952876091003
train_iter_loss: 0.10108243674039841
train_iter_loss: 0.1678456962108612
train_iter_loss: 0.1636268049478531
train_iter_loss: 0.1376604586839676
train_iter_loss: 0.1773000806570053
train_iter_loss: 0.08125786483287811
train_iter_loss: 0.31576359272003174
train_iter_loss: 0.12647733092308044
train_iter_loss: 0.09012600779533386
train_iter_loss: 0.10916987806558609
train_iter_loss: 0.19101561605930328
train_iter_loss: 0.22357966005802155
train_iter_loss: 0.1659276783466339
train_iter_loss: 0.11897208541631699
train_iter_loss: 0.06594794988632202
train_iter_loss: 0.13813331723213196
train_iter_loss: 0.09108363837003708
train_iter_loss: 0.1675594002008438
train_iter_loss: 0.07337501645088196
train_iter_loss: 0.17123658955097198
train_iter_loss: 0.23371295630931854
train_iter_loss: 0.14634473621845245
train_iter_loss: 0.13589273393154144
train_iter_loss: 0.10535240918397903
train_iter_loss: 0.14093764126300812
train_iter_loss: 0.18153226375579834
train_iter_loss: 0.1932104080915451
train_iter_loss: 0.10486773401498795
train_iter_loss: 0.15484045445919037
train_iter_loss: 0.2733384966850281
train_iter_loss: 0.09528479725122452
train_iter_loss: 0.17639613151550293
train_iter_loss: 0.1684478372335434
train_iter_loss: 0.14995421469211578
train_iter_loss: 0.13838954269886017
train_iter_loss: 0.1495104432106018
train_iter_loss: 0.12580730020999908
train_iter_loss: 0.09131234884262085
train_iter_loss: 0.14001084864139557
train_iter_loss: 0.12636244297027588
train_iter_loss: 0.1346365064382553
train_iter_loss: 0.152986541390419
train_iter_loss: 0.18242543935775757
train_iter_loss: 0.22729884088039398
train_iter_loss: 0.08991512656211853
train_iter_loss: 0.09011054784059525
train_iter_loss: 0.1525706797838211
train_iter_loss: 0.08308448642492294
train_iter_loss: 0.14295151829719543
train_iter_loss: 0.19039037823677063
train_iter_loss: 0.15802831947803497
train_iter_loss: 0.06062743067741394
train_iter_loss: 0.17680633068084717
train_iter_loss: 0.2199995219707489
train_iter_loss: 0.13384954631328583
train loss :0.1533
---------------------
Validation seg loss: 0.213908229427377 at epoch 774
epoch =    775/  1000, exp = train
train_iter_loss: 0.23161613941192627
train_iter_loss: 0.10301746428012848
train_iter_loss: 0.20087175071239471
train_iter_loss: 0.18823064863681793
train_iter_loss: 0.1507340520620346
train_iter_loss: 0.1470934897661209
train_iter_loss: 0.1042449101805687
train_iter_loss: 0.12264885753393173
train_iter_loss: 0.10999488085508347
train_iter_loss: 0.25889959931373596
train_iter_loss: 0.14666487276554108
train_iter_loss: 0.1719108670949936
train_iter_loss: 0.20017178356647491
train_iter_loss: 0.09776147454977036
train_iter_loss: 0.028230998665094376
train_iter_loss: 0.1903168261051178
train_iter_loss: 0.07744931429624557
train_iter_loss: 0.21681299805641174
train_iter_loss: 0.30303239822387695
train_iter_loss: 0.11633487045764923
train_iter_loss: 0.1309286653995514
train_iter_loss: 0.1730031669139862
train_iter_loss: 0.11887408792972565
train_iter_loss: 0.09730273485183716
train_iter_loss: 0.173658549785614
train_iter_loss: 0.15304522216320038
train_iter_loss: 0.13967479765415192
train_iter_loss: 0.18145185708999634
train_iter_loss: 0.12826837599277496
train_iter_loss: 0.08982039988040924
train_iter_loss: 0.15178510546684265
train_iter_loss: 0.07082127034664154
train_iter_loss: 0.12308811396360397
train_iter_loss: 0.099399134516716
train_iter_loss: 0.3339298665523529
train_iter_loss: 0.1088523268699646
train_iter_loss: 0.16109010577201843
train_iter_loss: 0.09433671087026596
train_iter_loss: 0.11831178516149521
train_iter_loss: 0.17595703899860382
train_iter_loss: 0.16456085443496704
train_iter_loss: 0.13531017303466797
train_iter_loss: 0.19421669840812683
train_iter_loss: 0.19161154329776764
train_iter_loss: 0.13456787168979645
train_iter_loss: 0.09675559401512146
train_iter_loss: 0.16623853147029877
train_iter_loss: 0.13505412638187408
train_iter_loss: 0.2109432965517044
train_iter_loss: 0.15467049181461334
train_iter_loss: 0.04510161280632019
train_iter_loss: 0.29120832681655884
train_iter_loss: 0.3167850375175476
train_iter_loss: 0.16525331139564514
train_iter_loss: 0.20443186163902283
train_iter_loss: 0.17733673751354218
train_iter_loss: 0.153662770986557
train_iter_loss: 0.17935128509998322
train_iter_loss: 0.19154341518878937
train_iter_loss: 0.18376930058002472
train_iter_loss: 0.21218939125537872
train_iter_loss: 0.26042571663856506
train_iter_loss: 0.12406224012374878
train_iter_loss: 0.12379500269889832
train_iter_loss: 0.1312239170074463
train_iter_loss: 0.1686186045408249
train_iter_loss: 0.23884466290473938
train_iter_loss: 0.08741067349910736
train_iter_loss: 0.1511741280555725
train_iter_loss: 0.14060944318771362
train_iter_loss: 0.1069735512137413
train_iter_loss: 0.15911605954170227
train_iter_loss: 0.1542254537343979
train_iter_loss: 0.11223747581243515
train_iter_loss: 0.0742800384759903
train_iter_loss: 0.12249305099248886
train_iter_loss: 0.23756246268749237
train_iter_loss: 0.19618889689445496
train_iter_loss: 0.11417382955551147
train_iter_loss: 0.14594890177249908
train_iter_loss: 0.12959454953670502
train_iter_loss: 0.11086822301149368
train_iter_loss: 0.07870547473430634
train_iter_loss: 0.11166033148765564
train_iter_loss: 0.06721112877130508
train_iter_loss: 0.14661134779453278
train_iter_loss: 0.08785248547792435
train_iter_loss: 0.10026392340660095
train_iter_loss: 0.14028550684452057
train_iter_loss: 0.09139877557754517
train_iter_loss: 0.11763729155063629
train_iter_loss: 0.20206190645694733
train_iter_loss: 0.14007100462913513
train_iter_loss: 0.07314547151327133
train_iter_loss: 0.1183924525976181
train_iter_loss: 0.1769886314868927
train_iter_loss: 0.15527775883674622
train_iter_loss: 0.17384518682956696
train_iter_loss: 0.17350205779075623
train_iter_loss: 0.2247982919216156
train loss :0.1532
---------------------
Validation seg loss: 0.2199248176700664 at epoch 775
epoch =    776/  1000, exp = train
train_iter_loss: 0.12415044009685516
train_iter_loss: 0.2522726356983185
train_iter_loss: 0.2600877285003662
train_iter_loss: 0.06197100505232811
train_iter_loss: 0.23173144459724426
train_iter_loss: 0.15005291998386383
train_iter_loss: 0.11526916921138763
train_iter_loss: 0.11513613909482956
train_iter_loss: 0.11229835450649261
train_iter_loss: 0.14396609365940094
train_iter_loss: 0.0789843425154686
train_iter_loss: 0.16357524693012238
train_iter_loss: 0.20724932849407196
train_iter_loss: 0.11134076118469238
train_iter_loss: 0.19696097075939178
train_iter_loss: 0.10708226263523102
train_iter_loss: 0.09753493964672089
train_iter_loss: 0.056738726794719696
train_iter_loss: 0.31287500262260437
train_iter_loss: 0.1349879652261734
train_iter_loss: 0.1831575334072113
train_iter_loss: 0.1082068532705307
train_iter_loss: 0.14443257451057434
train_iter_loss: 0.14103661477565765
train_iter_loss: 0.07937794923782349
train_iter_loss: 0.21921752393245697
train_iter_loss: 0.24549393355846405
train_iter_loss: 0.14605289697647095
train_iter_loss: 0.09181829541921616
train_iter_loss: 0.20707537233829498
train_iter_loss: 0.1742163598537445
train_iter_loss: 0.2878834009170532
train_iter_loss: 0.1881016343832016
train_iter_loss: 0.11922798305749893
train_iter_loss: 0.4231168329715729
train_iter_loss: 0.12198936939239502
train_iter_loss: 0.10912402719259262
train_iter_loss: 0.15019114315509796
train_iter_loss: 0.17725074291229248
train_iter_loss: 0.24495573341846466
train_iter_loss: 0.0928477942943573
train_iter_loss: 0.1955018937587738
train_iter_loss: 0.16533337533473969
train_iter_loss: 0.0868053212761879
train_iter_loss: 0.15578512847423553
train_iter_loss: 0.19178950786590576
train_iter_loss: 0.12323062866926193
train_iter_loss: 0.1201135516166687
train_iter_loss: 0.1306539922952652
train_iter_loss: 0.16647207736968994
train_iter_loss: 0.1358146369457245
train_iter_loss: 0.24049696326255798
train_iter_loss: 0.07307948172092438
train_iter_loss: 0.09250379353761673
train_iter_loss: 0.09011179208755493
train_iter_loss: 0.11084810644388199
train_iter_loss: 0.2942463159561157
train_iter_loss: 0.12737521529197693
train_iter_loss: 0.10311686247587204
train_iter_loss: 0.08500328660011292
train_iter_loss: 0.10794240236282349
train_iter_loss: 0.1452294886112213
train_iter_loss: 0.09766880422830582
train_iter_loss: 0.2281966656446457
train_iter_loss: 0.21958045661449432
train_iter_loss: 0.30250614881515503
train_iter_loss: 0.14060159027576447
train_iter_loss: 0.18978865444660187
train_iter_loss: 0.12115224450826645
train_iter_loss: 0.11185456067323685
train_iter_loss: 0.11745118349790573
train_iter_loss: 0.08285757154226303
train_iter_loss: 0.19219623506069183
train_iter_loss: 0.07642777264118195
train_iter_loss: 0.1129046380519867
train_iter_loss: 0.16971585154533386
train_iter_loss: 0.1446949690580368
train_iter_loss: 0.09518495202064514
train_iter_loss: 0.17498645186424255
train_iter_loss: 0.18411731719970703
train_iter_loss: 0.14131073653697968
train_iter_loss: 0.13177838921546936
train_iter_loss: 0.1527942568063736
train_iter_loss: 0.10564665496349335
train_iter_loss: 0.08106648921966553
train_iter_loss: 0.1886967122554779
train_iter_loss: 0.12382438033819199
train_iter_loss: 0.08301712572574615
train_iter_loss: 0.1975395828485489
train_iter_loss: 0.17653556168079376
train_iter_loss: 0.17017574608325958
train_iter_loss: 0.1557696908712387
train_iter_loss: 0.276692658662796
train_iter_loss: 0.12964041531085968
train_iter_loss: 0.1972397267818451
train_iter_loss: 0.1556800752878189
train_iter_loss: 0.06155814602971077
train_iter_loss: 0.06972279399633408
train_iter_loss: 0.2154354304075241
train_iter_loss: 0.27860742807388306
train loss :0.1564
---------------------
Validation seg loss: 0.21593678503666283 at epoch 776
epoch =    777/  1000, exp = train
train_iter_loss: 0.2780172526836395
train_iter_loss: 0.08257799595594406
train_iter_loss: 0.12946940958499908
train_iter_loss: 0.1372506320476532
train_iter_loss: 0.05015425756573677
train_iter_loss: 0.210087388753891
train_iter_loss: 0.16267138719558716
train_iter_loss: 0.11834246665239334
train_iter_loss: 0.11519016325473785
train_iter_loss: 0.13945606350898743
train_iter_loss: 0.15269005298614502
train_iter_loss: 0.06626816093921661
train_iter_loss: 0.19057948887348175
train_iter_loss: 0.11979741603136063
train_iter_loss: 0.24315054714679718
train_iter_loss: 0.15563179552555084
train_iter_loss: 0.1643533855676651
train_iter_loss: 0.21227210760116577
train_iter_loss: 0.13585646450519562
train_iter_loss: 0.16805420815944672
train_iter_loss: 0.15441963076591492
train_iter_loss: 0.0976233258843422
train_iter_loss: 0.1566084921360016
train_iter_loss: 0.1105148121714592
train_iter_loss: 0.2283627688884735
train_iter_loss: 0.16362637281417847
train_iter_loss: 0.2505907416343689
train_iter_loss: 0.25508561730384827
train_iter_loss: 0.2622780501842499
train_iter_loss: 0.23971232771873474
train_iter_loss: 0.23782838881015778
train_iter_loss: 0.22276365756988525
train_iter_loss: 0.162456676363945
train_iter_loss: 0.11069129407405853
train_iter_loss: 0.09008930623531342
train_iter_loss: 0.047140706330537796
train_iter_loss: 0.11936736106872559
train_iter_loss: 0.10129214823246002
train_iter_loss: 0.07582362741231918
train_iter_loss: 0.16945023834705353
train_iter_loss: 0.14118126034736633
train_iter_loss: 0.09306881576776505
train_iter_loss: 0.10393649339675903
train_iter_loss: 0.18107040226459503
train_iter_loss: 0.12709613144397736
train_iter_loss: 0.14113397896289825
train_iter_loss: 0.12450232356786728
train_iter_loss: 0.1002148985862732
train_iter_loss: 0.09775716811418533
train_iter_loss: 0.24860075116157532
train_iter_loss: 0.21753564476966858
train_iter_loss: 0.13432984054088593
train_iter_loss: 0.08513926714658737
train_iter_loss: 0.2702285647392273
train_iter_loss: 0.10318682342767715
train_iter_loss: 0.20784436166286469
train_iter_loss: 0.15660779178142548
train_iter_loss: 0.08284860849380493
train_iter_loss: 0.09751226007938385
train_iter_loss: 0.17335495352745056
train_iter_loss: 0.21578772366046906
train_iter_loss: 0.13155490159988403
train_iter_loss: 0.20167987048625946
train_iter_loss: 0.048165544867515564
train_iter_loss: 0.05072138085961342
train_iter_loss: 0.20737776160240173
train_iter_loss: 0.13054479658603668
train_iter_loss: 0.08815954625606537
train_iter_loss: 0.14213651418685913
train_iter_loss: 0.16501906514167786
train_iter_loss: 0.22057466208934784
train_iter_loss: 0.25399360060691833
train_iter_loss: 0.2348105013370514
train_iter_loss: 0.1208873838186264
train_iter_loss: 0.2200387865304947
train_iter_loss: 0.16598497331142426
train_iter_loss: 0.16332630813121796
train_iter_loss: 0.18695184588432312
train_iter_loss: 0.13584831357002258
train_iter_loss: 0.23453058302402496
train_iter_loss: 0.13077479600906372
train_iter_loss: 0.09953828901052475
train_iter_loss: 0.22700953483581543
train_iter_loss: 0.08148404210805893
train_iter_loss: 0.18045419454574585
train_iter_loss: 0.05575898289680481
train_iter_loss: 0.1012740433216095
train_iter_loss: 0.13675528764724731
train_iter_loss: 0.15882696211338043
train_iter_loss: 0.07923807948827744
train_iter_loss: 0.16530683636665344
train_iter_loss: 0.055144984275102615
train_iter_loss: 0.10764990746974945
train_iter_loss: 0.1688651740550995
train_iter_loss: 0.36086538434028625
train_iter_loss: 0.1634671539068222
train_iter_loss: 0.1770252287387848
train_iter_loss: 0.15171830356121063
train_iter_loss: 0.10736211389303207
train_iter_loss: 0.12191571295261383
train loss :0.1548
---------------------
Validation seg loss: 0.21657195449191727 at epoch 777
epoch =    778/  1000, exp = train
train_iter_loss: 0.10516120493412018
train_iter_loss: 0.10619210451841354
train_iter_loss: 0.20663511753082275
train_iter_loss: 0.22088582813739777
train_iter_loss: 0.14519013464450836
train_iter_loss: 0.1056654304265976
train_iter_loss: 0.13639004528522491
train_iter_loss: 0.16233809292316437
train_iter_loss: 0.0724654570221901
train_iter_loss: 0.17461258172988892
train_iter_loss: 0.1078743040561676
train_iter_loss: 0.11460844427347183
train_iter_loss: 0.3455088436603546
train_iter_loss: 0.1814010739326477
train_iter_loss: 0.17997758090496063
train_iter_loss: 0.12915293872356415
train_iter_loss: 0.15482012927532196
train_iter_loss: 0.08019758015871048
train_iter_loss: 0.21512502431869507
train_iter_loss: 0.22215265035629272
train_iter_loss: 0.2710743844509125
train_iter_loss: 0.16325391829013824
train_iter_loss: 0.07809018343687057
train_iter_loss: 0.07682864367961884
train_iter_loss: 0.23132403194904327
train_iter_loss: 0.03566429391503334
train_iter_loss: 0.06119690462946892
train_iter_loss: 0.1668936312198639
train_iter_loss: 0.13758975267410278
train_iter_loss: 0.197587251663208
train_iter_loss: 0.1307336837053299
train_iter_loss: 0.1589282900094986
train_iter_loss: 0.3385085463523865
train_iter_loss: 0.16526159644126892
train_iter_loss: 0.09897623211145401
train_iter_loss: 0.272070974111557
train_iter_loss: 0.14507322013378143
train_iter_loss: 0.2028825730085373
train_iter_loss: 0.13796330988407135
train_iter_loss: 0.13612934947013855
train_iter_loss: 0.09885262697935104
train_iter_loss: 0.074577696621418
train_iter_loss: 0.11907727271318436
train_iter_loss: 0.2216496765613556
train_iter_loss: 0.0980808287858963
train_iter_loss: 0.1816663146018982
train_iter_loss: 0.20022647082805634
train_iter_loss: 0.2795971632003784
train_iter_loss: 0.14381000399589539
train_iter_loss: 0.15971706807613373
train_iter_loss: 0.20032882690429688
train_iter_loss: 0.1843900829553604
train_iter_loss: 0.2160186618566513
train_iter_loss: 0.20683544874191284
train_iter_loss: 0.07746388018131256
train_iter_loss: 0.1881384551525116
train_iter_loss: 0.2745039165019989
train_iter_loss: 0.15748317539691925
train_iter_loss: 0.09968208521604538
train_iter_loss: 0.11867082118988037
train_iter_loss: 0.157899409532547
train_iter_loss: 0.24303960800170898
train_iter_loss: 0.138731449842453
train_iter_loss: 0.042777758091688156
train_iter_loss: 0.18491116166114807
train_iter_loss: 0.10782161355018616
train_iter_loss: 0.1337115615606308
train_iter_loss: 0.14771060645580292
train_iter_loss: 0.08353666961193085
train_iter_loss: 0.02412293292582035
train_iter_loss: 0.09721607714891434
train_iter_loss: 0.2022046446800232
train_iter_loss: 0.16423925757408142
train_iter_loss: 0.20982356369495392
train_iter_loss: 0.16522230207920074
train_iter_loss: 0.1434323638677597
train_iter_loss: 0.17092303931713104
train_iter_loss: 0.20005956292152405
train_iter_loss: 0.13332748413085938
train_iter_loss: 0.1471441388130188
train_iter_loss: 0.14435169100761414
train_iter_loss: 0.07047343999147415
train_iter_loss: 0.3736083507537842
train_iter_loss: 0.15212520956993103
train_iter_loss: 0.13713288307189941
train_iter_loss: 0.20817206799983978
train_iter_loss: 0.13385102152824402
train_iter_loss: 0.15340106189250946
train_iter_loss: 0.15633001923561096
train_iter_loss: 0.2736511528491974
train_iter_loss: 0.1745595782995224
train_iter_loss: 0.06265238672494888
train_iter_loss: 0.0958670899271965
train_iter_loss: 0.06453222036361694
train_iter_loss: 0.1403769850730896
train_iter_loss: 0.19237245619297028
train_iter_loss: 0.12536568939685822
train_iter_loss: 0.056639883667230606
train_iter_loss: 0.13162082433700562
train_iter_loss: 0.20173002779483795
train loss :0.1577
---------------------
Validation seg loss: 0.2203601940693158 at epoch 778
epoch =    779/  1000, exp = train
train_iter_loss: 0.2370794266462326
train_iter_loss: 0.1087333932518959
train_iter_loss: 0.11362310498952866
train_iter_loss: 0.12105459719896317
train_iter_loss: 0.21198710799217224
train_iter_loss: 0.07681360095739365
train_iter_loss: 0.09567812085151672
train_iter_loss: 0.11813121289014816
train_iter_loss: 0.14685878157615662
train_iter_loss: 0.19759692251682281
train_iter_loss: 0.16358162462711334
train_iter_loss: 0.2028687298297882
train_iter_loss: 0.13941511511802673
train_iter_loss: 0.13076387345790863
train_iter_loss: 0.26212745904922485
train_iter_loss: 0.1269000917673111
train_iter_loss: 0.24449968338012695
train_iter_loss: 0.16054625809192657
train_iter_loss: 0.1352294534444809
train_iter_loss: 0.2894101142883301
train_iter_loss: 0.08691040426492691
train_iter_loss: 0.1691703349351883
train_iter_loss: 0.11415313929319382
train_iter_loss: 0.26521632075309753
train_iter_loss: 0.13157448172569275
train_iter_loss: 0.13902878761291504
train_iter_loss: 0.2818381190299988
train_iter_loss: 0.16527120769023895
train_iter_loss: 0.14764255285263062
train_iter_loss: 0.15140920877456665
train_iter_loss: 0.20749570429325104
train_iter_loss: 0.13174332678318024
train_iter_loss: 0.1284102350473404
train_iter_loss: 0.1846068948507309
train_iter_loss: 0.1916923075914383
train_iter_loss: 0.13177241384983063
train_iter_loss: 0.07855190336704254
train_iter_loss: 0.05403408035635948
train_iter_loss: 0.08439214527606964
train_iter_loss: 0.042905911803245544
train_iter_loss: 0.10953022539615631
train_iter_loss: 0.15549780428409576
train_iter_loss: 0.19072827696800232
train_iter_loss: 0.15508081018924713
train_iter_loss: 0.08203195780515671
train_iter_loss: 0.21697722375392914
train_iter_loss: 0.14026521146297455
train_iter_loss: 0.21315091848373413
train_iter_loss: 0.2005079686641693
train_iter_loss: 0.1300967037677765
train_iter_loss: 0.08656488358974457
train_iter_loss: 0.28141340613365173
train_iter_loss: 0.2674873173236847
train_iter_loss: 0.1365213394165039
train_iter_loss: 0.25846055150032043
train_iter_loss: 0.12504246830940247
train_iter_loss: 0.11791945993900299
train_iter_loss: 0.17393381893634796
train_iter_loss: 0.08599743992090225
train_iter_loss: 0.12441977113485336
train_iter_loss: 0.07328469306230545
train_iter_loss: 0.17295722663402557
train_iter_loss: 0.11106612533330917
train_iter_loss: 0.198873832821846
train_iter_loss: 0.16646450757980347
train_iter_loss: 0.17618541419506073
train_iter_loss: 0.1558888703584671
train_iter_loss: 0.07276307791471481
train_iter_loss: 0.09290118515491486
train_iter_loss: 0.19758766889572144
train_iter_loss: 0.3313017189502716
train_iter_loss: 0.10720986127853394
train_iter_loss: 0.2005367875099182
train_iter_loss: 0.18257997930049896
train_iter_loss: 0.08558089286088943
train_iter_loss: 0.08668158203363419
train_iter_loss: 0.19850920140743256
train_iter_loss: 0.09665889292955399
train_iter_loss: 0.2168286144733429
train_iter_loss: 0.09704902768135071
train_iter_loss: 0.19337455928325653
train_iter_loss: 0.13509580492973328
train_iter_loss: 0.16485565900802612
train_iter_loss: 0.14286094903945923
train_iter_loss: 0.14387960731983185
train_iter_loss: 0.16057263314723969
train_iter_loss: 0.32059985399246216
train_iter_loss: 0.10596191138029099
train_iter_loss: 0.14042577147483826
train_iter_loss: 0.13569049537181854
train_iter_loss: 0.13417159020900726
train_iter_loss: 0.17874421179294586
train_iter_loss: 0.1377972513437271
train_iter_loss: 0.15129904448986053
train_iter_loss: 0.093647301197052
train_iter_loss: 0.19157269597053528
train_iter_loss: 0.09137384593486786
train_iter_loss: 0.15832853317260742
train_iter_loss: 0.13600777089595795
train_iter_loss: 0.272053062915802
train loss :0.1579
---------------------
Validation seg loss: 0.21327021421934916 at epoch 779
epoch =    780/  1000, exp = train
train_iter_loss: 0.12149650603532791
train_iter_loss: 0.09062056243419647
train_iter_loss: 0.2766549289226532
train_iter_loss: 0.23038345575332642
train_iter_loss: 0.16737349331378937
train_iter_loss: 0.09792477637529373
train_iter_loss: 0.10934599488973618
train_iter_loss: 0.15737617015838623
train_iter_loss: 0.10179124027490616
train_iter_loss: 0.08400881290435791
train_iter_loss: 0.1843092292547226
train_iter_loss: 0.11959526687860489
train_iter_loss: 0.14631275832653046
train_iter_loss: 0.10446019470691681
train_iter_loss: 0.16809049248695374
train_iter_loss: 0.18542395532131195
train_iter_loss: 0.10655415803194046
train_iter_loss: 0.15260089933872223
train_iter_loss: 0.16988946497440338
train_iter_loss: 0.19399632513523102
train_iter_loss: 0.14264748990535736
train_iter_loss: 0.10618305206298828
train_iter_loss: 0.16382360458374023
train_iter_loss: 0.1701216697692871
train_iter_loss: 0.1150788813829422
train_iter_loss: 0.1093752458691597
train_iter_loss: 0.09743485599756241
train_iter_loss: 0.14898298680782318
train_iter_loss: 0.17402775585651398
train_iter_loss: 0.1650315523147583
train_iter_loss: 0.2114167958498001
train_iter_loss: 0.22770725190639496
train_iter_loss: 0.06591737270355225
train_iter_loss: 0.17641060054302216
train_iter_loss: 0.19475072622299194
train_iter_loss: 0.0438249371945858
train_iter_loss: 0.19391308724880219
train_iter_loss: 0.1109331026673317
train_iter_loss: 0.18289826810359955
train_iter_loss: 0.17624631524085999
train_iter_loss: 0.16176703572273254
train_iter_loss: 0.07660427689552307
train_iter_loss: 0.09625754505395889
train_iter_loss: 0.2125380039215088
train_iter_loss: 0.1296239197254181
train_iter_loss: 0.1487407088279724
train_iter_loss: 0.0863434225320816
train_iter_loss: 0.20747844874858856
train_iter_loss: 0.24513541162014008
train_iter_loss: 0.12463688850402832
train_iter_loss: 0.15656514465808868
train_iter_loss: 0.2923138439655304
train_iter_loss: 0.18376724421977997
train_iter_loss: 0.1619555652141571
train_iter_loss: 0.12789537012577057
train_iter_loss: 0.12919698655605316
train_iter_loss: 0.11901121586561203
train_iter_loss: 0.2964819073677063
train_iter_loss: 0.18468226492404938
train_iter_loss: 0.20237357914447784
train_iter_loss: 0.08803661167621613
train_iter_loss: 0.1338881403207779
train_iter_loss: 0.09986135363578796
train_iter_loss: 0.19904404878616333
train_iter_loss: 0.15774133801460266
train_iter_loss: 0.18385587632656097
train_iter_loss: 0.06954395771026611
train_iter_loss: 0.13142402470111847
train_iter_loss: 0.1927422285079956
train_iter_loss: 0.04403259605169296
train_iter_loss: 0.12772110104560852
train_iter_loss: 0.19333933293819427
train_iter_loss: 0.1495254635810852
train_iter_loss: 0.08206772804260254
train_iter_loss: 0.2087584286928177
train_iter_loss: 0.1032358929514885
train_iter_loss: 0.10507720708847046
train_iter_loss: 0.18125654757022858
train_iter_loss: 0.05121900886297226
train_iter_loss: 0.11823622137308121
train_iter_loss: 0.18276768922805786
train_iter_loss: 0.17751476168632507
train_iter_loss: 0.1570853292942047
train_iter_loss: 0.06771007925271988
train_iter_loss: 0.08352456986904144
train_iter_loss: 0.1300513744354248
train_iter_loss: 0.20927751064300537
train_iter_loss: 0.06139169633388519
train_iter_loss: 0.23749348521232605
train_iter_loss: 0.2550743520259857
train_iter_loss: 0.1557006537914276
train_iter_loss: 0.2746925950050354
train_iter_loss: 0.14299866557121277
train_iter_loss: 0.12935204803943634
train_iter_loss: 0.1540677845478058
train_iter_loss: 0.13438351452350616
train_iter_loss: 0.0854601114988327
train_iter_loss: 0.1388973593711853
train_iter_loss: 0.260314404964447
train_iter_loss: 0.09430139511823654
train loss :0.1520
---------------------
Validation seg loss: 0.21726831198968696 at epoch 780
epoch =    781/  1000, exp = train
train_iter_loss: 0.2965489327907562
train_iter_loss: 0.16365878283977509
train_iter_loss: 0.09269368648529053
train_iter_loss: 0.1080022007226944
train_iter_loss: 0.15604481101036072
train_iter_loss: 0.12892666459083557
train_iter_loss: 0.28923681378364563
train_iter_loss: 0.1362665891647339
train_iter_loss: 0.15673178434371948
train_iter_loss: 0.1672762930393219
train_iter_loss: 0.17342117428779602
train_iter_loss: 0.07060181349515915
train_iter_loss: 0.1386447697877884
train_iter_loss: 0.09091038256883621
train_iter_loss: 0.16418729722499847
train_iter_loss: 0.12161733955144882
train_iter_loss: 0.23657624423503876
train_iter_loss: 0.1282770186662674
train_iter_loss: 0.24005721509456635
train_iter_loss: 0.15411531925201416
train_iter_loss: 0.19873733818531036
train_iter_loss: 0.07964082807302475
train_iter_loss: 0.13270632922649384
train_iter_loss: 0.17454233765602112
train_iter_loss: 0.11426710337400436
train_iter_loss: 0.10565122961997986
train_iter_loss: 0.1505209356546402
train_iter_loss: 0.17784573137760162
train_iter_loss: 0.1815919280052185
train_iter_loss: 0.15413710474967957
train_iter_loss: 0.13238438963890076
train_iter_loss: 0.16234922409057617
train_iter_loss: 0.2830929756164551
train_iter_loss: 0.11920370161533356
train_iter_loss: 0.11217819154262543
train_iter_loss: 0.11636397987604141
train_iter_loss: 0.13364943861961365
train_iter_loss: 0.1770051270723343
train_iter_loss: 0.17960090935230255
train_iter_loss: 0.12015196681022644
train_iter_loss: 0.0972011536359787
train_iter_loss: 0.1073022410273552
train_iter_loss: 0.19046229124069214
train_iter_loss: 0.35037437081336975
train_iter_loss: 0.20103953778743744
train_iter_loss: 0.14038346707820892
train_iter_loss: 0.22264356911182404
train_iter_loss: 0.14914412796497345
train_iter_loss: 0.1387358009815216
train_iter_loss: 0.14669948816299438
train_iter_loss: 0.11928112059831619
train_iter_loss: 0.09188197553157806
train_iter_loss: 0.13711637258529663
train_iter_loss: 0.15955449640750885
train_iter_loss: 0.20858319103717804
train_iter_loss: 0.2108565866947174
train_iter_loss: 0.10647036135196686
train_iter_loss: 0.16919289529323578
train_iter_loss: 0.21695944666862488
train_iter_loss: 0.14676985144615173
train_iter_loss: 0.16806146502494812
train_iter_loss: 0.16729950904846191
train_iter_loss: 0.15212726593017578
train_iter_loss: 0.19459109008312225
train_iter_loss: 0.11326204240322113
train_iter_loss: 0.19383454322814941
train_iter_loss: 0.18580037355422974
train_iter_loss: 0.23525439202785492
train_iter_loss: 0.17944879829883575
train_iter_loss: 0.2555381953716278
train_iter_loss: 0.0978606641292572
train_iter_loss: 0.10285350680351257
train_iter_loss: 0.14711545407772064
train_iter_loss: 0.11389753967523575
train_iter_loss: 0.2696167528629303
train_iter_loss: 0.11924120038747787
train_iter_loss: 0.26239004731178284
train_iter_loss: 0.11653438955545425
train_iter_loss: 0.2009386569261551
train_iter_loss: 0.10523855686187744
train_iter_loss: 0.13434851169586182
train_iter_loss: 0.07683499902486801
train_iter_loss: 0.21161484718322754
train_iter_loss: 0.094902403652668
train_iter_loss: 0.14567939937114716
train_iter_loss: 0.1567997932434082
train_iter_loss: 0.1900937557220459
train_iter_loss: 0.09399744123220444
train_iter_loss: 0.13529974222183228
train_iter_loss: 0.12473215907812119
train_iter_loss: 0.14725756645202637
train_iter_loss: 0.18698158860206604
train_iter_loss: 0.13060888648033142
train_iter_loss: 0.19843320548534393
train_iter_loss: 0.1540813446044922
train_iter_loss: 0.21921290457248688
train_iter_loss: 0.117125503718853
train_iter_loss: 0.0782860741019249
train_iter_loss: 0.21216994524002075
train_iter_loss: 0.19127652049064636
train loss :0.1606
---------------------
Validation seg loss: 0.22065385641038138 at epoch 781
epoch =    782/  1000, exp = train
train_iter_loss: 0.26396098732948303
train_iter_loss: 0.12654541432857513
train_iter_loss: 0.1785050481557846
train_iter_loss: 0.23736441135406494
train_iter_loss: 0.09132052958011627
train_iter_loss: 0.11027375608682632
train_iter_loss: 0.13221091032028198
train_iter_loss: 0.18694984912872314
train_iter_loss: 0.2021343857049942
train_iter_loss: 0.17761598527431488
train_iter_loss: 0.19108206033706665
train_iter_loss: 0.1437062919139862
train_iter_loss: 0.11140558868646622
train_iter_loss: 0.2886984944343567
train_iter_loss: 0.1555001437664032
train_iter_loss: 0.16551274061203003
train_iter_loss: 0.12066324800252914
train_iter_loss: 0.06268573552370071
train_iter_loss: 0.32520022988319397
train_iter_loss: 0.21098412573337555
train_iter_loss: 0.26015013456344604
train_iter_loss: 0.115016870200634
train_iter_loss: 0.08165470510721207
train_iter_loss: 0.16642752289772034
train_iter_loss: 0.20863598585128784
train_iter_loss: 0.27315735816955566
train_iter_loss: 0.056860215961933136
train_iter_loss: 0.13785400986671448
train_iter_loss: 0.08618736267089844
train_iter_loss: 0.13059605658054352
train_iter_loss: 0.07926715165376663
train_iter_loss: 0.08793669939041138
train_iter_loss: 0.17824414372444153
train_iter_loss: 0.13407602906227112
train_iter_loss: 0.07268174737691879
train_iter_loss: 0.13770325481891632
train_iter_loss: 0.05023400858044624
train_iter_loss: 0.07969535887241364
train_iter_loss: 0.13393816351890564
train_iter_loss: 0.2055208534002304
train_iter_loss: 0.12870343029499054
train_iter_loss: 0.22329643368721008
train_iter_loss: 0.23358026146888733
train_iter_loss: 0.1363053321838379
train_iter_loss: 0.09869930893182755
train_iter_loss: 0.2877558171749115
train_iter_loss: 0.14566990733146667
train_iter_loss: 0.15217219293117523
train_iter_loss: 0.14719462394714355
train_iter_loss: 0.1069662943482399
train_iter_loss: 0.11246420443058014
train_iter_loss: 0.11636128276586533
train_iter_loss: 0.06561963260173798
train_iter_loss: 0.2213175892829895
train_iter_loss: 0.08158392459154129
train_iter_loss: 0.16727100312709808
train_iter_loss: 0.1160711720585823
train_iter_loss: 0.08828555792570114
train_iter_loss: 0.3087408244609833
train_iter_loss: 0.08569042384624481
train_iter_loss: 0.14162471890449524
train_iter_loss: 0.1222219467163086
train_iter_loss: 0.2218354046344757
train_iter_loss: 0.10779415816068649
train_iter_loss: 0.10157081484794617
train_iter_loss: 0.16246876120567322
train_iter_loss: 0.3709544837474823
train_iter_loss: 0.12657789885997772
train_iter_loss: 0.1905418485403061
train_iter_loss: 0.22680512070655823
train_iter_loss: 0.15177516639232635
train_iter_loss: 0.1782892346382141
train_iter_loss: 0.3082933723926544
train_iter_loss: 0.19120673835277557
train_iter_loss: 0.10632941871881485
train_iter_loss: 0.08725631982088089
train_iter_loss: 0.28086692094802856
train_iter_loss: 0.1723424345254898
train_iter_loss: 0.1076325848698616
train_iter_loss: 0.19452042877674103
train_iter_loss: 0.06597728282213211
train_iter_loss: 0.19661612808704376
train_iter_loss: 0.1612502932548523
train_iter_loss: 0.29185155034065247
train_iter_loss: 0.18336760997772217
train_iter_loss: 0.21922335028648376
train_iter_loss: 0.13356050848960876
train_iter_loss: 0.11491524428129196
train_iter_loss: 0.26128751039505005
train_iter_loss: 0.3057457208633423
train_iter_loss: 0.08163823187351227
train_iter_loss: 0.13873247802257538
train_iter_loss: 0.1553920954465866
train_iter_loss: 0.14339730143547058
train_iter_loss: 0.16625343263149261
train_iter_loss: 0.08835861831903458
train_iter_loss: 0.12893913686275482
train_iter_loss: 0.12139473855495453
train_iter_loss: 0.11611298471689224
train_iter_loss: 0.17981551587581635
train loss :0.1615
---------------------
Validation seg loss: 0.21441463133285069 at epoch 782
epoch =    783/  1000, exp = train
train_iter_loss: 0.11269693821668625
train_iter_loss: 0.12896712124347687
train_iter_loss: 0.13143090903759003
train_iter_loss: 0.22952896356582642
train_iter_loss: 0.1370289921760559
train_iter_loss: 0.14785444736480713
train_iter_loss: 0.15662887692451477
train_iter_loss: 0.14282265305519104
train_iter_loss: 0.0848689079284668
train_iter_loss: 0.14509189128875732
train_iter_loss: 0.11718732118606567
train_iter_loss: 0.13710476458072662
train_iter_loss: 0.09932901710271835
train_iter_loss: 0.15934105217456818
train_iter_loss: 0.12492860853672028
train_iter_loss: 0.13268043100833893
train_iter_loss: 0.2104421705007553
train_iter_loss: 0.16535861790180206
train_iter_loss: 0.16849111020565033
train_iter_loss: 0.22540387511253357
train_iter_loss: 0.1638730764389038
train_iter_loss: 0.21954825520515442
train_iter_loss: 0.1655917912721634
train_iter_loss: 0.2578967809677124
train_iter_loss: 0.21752065420150757
train_iter_loss: 0.16700869798660278
train_iter_loss: 0.21369807422161102
train_iter_loss: 0.16590067744255066
train_iter_loss: 0.13973398506641388
train_iter_loss: 0.1193406954407692
train_iter_loss: 0.1192072406411171
train_iter_loss: 0.17663364112377167
train_iter_loss: 0.13954703509807587
train_iter_loss: 0.16668134927749634
train_iter_loss: 0.07869530469179153
train_iter_loss: 0.22853650152683258
train_iter_loss: 0.08927401900291443
train_iter_loss: 0.2084357738494873
train_iter_loss: 0.26378101110458374
train_iter_loss: 0.17834459245204926
train_iter_loss: 0.13916030526161194
train_iter_loss: 0.10951509326696396
train_iter_loss: 0.18721282482147217
train_iter_loss: 0.13100665807724
train_iter_loss: 0.14974312484264374
train_iter_loss: 0.3373773992061615
train_iter_loss: 0.07666664570569992
train_iter_loss: 0.04113810881972313
train_iter_loss: 0.20597650110721588
train_iter_loss: 0.1453075408935547
train_iter_loss: 0.20861609280109406
train_iter_loss: 0.1751101315021515
train_iter_loss: 0.1273132562637329
train_iter_loss: 0.26685017347335815
train_iter_loss: 0.1198476031422615
train_iter_loss: 0.179399311542511
train_iter_loss: 0.2649230360984802
train_iter_loss: 0.19317077100276947
train_iter_loss: 0.14585046470165253
train_iter_loss: 0.17807744443416595
train_iter_loss: 0.14652684330940247
train_iter_loss: 0.05170028284192085
train_iter_loss: 0.18330660462379456
train_iter_loss: 0.10278710722923279
train_iter_loss: 0.15221267938613892
train_iter_loss: 0.11082902550697327
train_iter_loss: 0.11303368955850601
train_iter_loss: 0.1191382110118866
train_iter_loss: 0.13755007088184357
train_iter_loss: 0.11624220758676529
train_iter_loss: 0.16144974529743195
train_iter_loss: 0.11644870042800903
train_iter_loss: 0.17883238196372986
train_iter_loss: 0.2789539396762848
train_iter_loss: 0.10382083058357239
train_iter_loss: 0.17897507548332214
train_iter_loss: 0.19405944645404816
train_iter_loss: 0.06815991550683975
train_iter_loss: 0.1096508726477623
train_iter_loss: 0.16240032017230988
train_iter_loss: 0.17134074866771698
train_iter_loss: 0.03833030164241791
train_iter_loss: 0.18123359978199005
train_iter_loss: 0.2239162027835846
train_iter_loss: 0.17504964768886566
train_iter_loss: 0.28634828329086304
train_iter_loss: 0.23026669025421143
train_iter_loss: 0.14327676594257355
train_iter_loss: 0.12649005651474
train_iter_loss: 0.1404607594013214
train_iter_loss: 0.1586487740278244
train_iter_loss: 0.3071804642677307
train_iter_loss: 0.2183779776096344
train_iter_loss: 0.049959003925323486
train_iter_loss: 0.26840740442276
train_iter_loss: 0.14660362899303436
train_iter_loss: 0.2655055820941925
train_iter_loss: 0.06985421478748322
train_iter_loss: 0.06777474284172058
train_iter_loss: 0.21105512976646423
train loss :0.1625
---------------------
Validation seg loss: 0.21526476941159312 at epoch 783
epoch =    784/  1000, exp = train
train_iter_loss: 0.16383129358291626
train_iter_loss: 0.1117800623178482
train_iter_loss: 0.10883349925279617
train_iter_loss: 0.17210030555725098
train_iter_loss: 0.20931370556354523
train_iter_loss: 0.14247913658618927
train_iter_loss: 0.17692036926746368
train_iter_loss: 0.1701279729604721
train_iter_loss: 0.17428776621818542
train_iter_loss: 0.1454867273569107
train_iter_loss: 0.16324849426746368
train_iter_loss: 0.09674215316772461
train_iter_loss: 0.10704188793897629
train_iter_loss: 0.17067885398864746
train_iter_loss: 0.17477834224700928
train_iter_loss: 0.160294309258461
train_iter_loss: 0.15893793106079102
train_iter_loss: 0.23801545798778534
train_iter_loss: 0.15128757059574127
train_iter_loss: 0.08407047390937805
train_iter_loss: 0.14715830981731415
train_iter_loss: 0.1022883951663971
train_iter_loss: 0.2115720957517624
train_iter_loss: 0.07125476747751236
train_iter_loss: 0.1533033549785614
train_iter_loss: 0.14727866649627686
train_iter_loss: 0.1293180286884308
train_iter_loss: 0.13335685431957245
train_iter_loss: 0.22770440578460693
train_iter_loss: 0.19049301743507385
train_iter_loss: 0.054575685411691666
train_iter_loss: 0.17255623638629913
train_iter_loss: 0.1822412759065628
train_iter_loss: 0.11053094267845154
train_iter_loss: 0.23965239524841309
train_iter_loss: 0.2802771329879761
train_iter_loss: 0.27600982785224915
train_iter_loss: 0.24620309472084045
train_iter_loss: 0.07651461660861969
train_iter_loss: 0.13544286787509918
train_iter_loss: 0.2247912734746933
train_iter_loss: 0.10337794572114944
train_iter_loss: 0.10906190425157547
train_iter_loss: 0.23437055945396423
train_iter_loss: 0.088522769510746
train_iter_loss: 0.23954267799854279
train_iter_loss: 0.17710943520069122
train_iter_loss: 0.20253722369670868
train_iter_loss: 0.22430984675884247
train_iter_loss: 0.12107249349355698
train_iter_loss: 0.08728582412004471
train_iter_loss: 0.07393983006477356
train_iter_loss: 0.12571163475513458
train_iter_loss: 0.14606639742851257
train_iter_loss: 0.17458753287792206
train_iter_loss: 0.08069019019603729
train_iter_loss: 0.13856054842472076
train_iter_loss: 0.15004566311836243
train_iter_loss: 0.17494964599609375
train_iter_loss: 0.11959824711084366
train_iter_loss: 0.09469553083181381
train_iter_loss: 0.03811464086174965
train_iter_loss: 0.16685286164283752
train_iter_loss: 0.10808292031288147
train_iter_loss: 0.08925269544124603
train_iter_loss: 0.12177509069442749
train_iter_loss: 0.1741066426038742
train_iter_loss: 0.1755693554878235
train_iter_loss: 0.06623636931180954
train_iter_loss: 0.13977141678333282
train_iter_loss: 0.20525117218494415
train_iter_loss: 0.12182974070310593
train_iter_loss: 0.3397867977619171
train_iter_loss: 0.12913867831230164
train_iter_loss: 0.2145458459854126
train_iter_loss: 0.15275518596172333
train_iter_loss: 0.17725597321987152
train_iter_loss: 0.1863430142402649
train_iter_loss: 0.18118980526924133
train_iter_loss: 0.06586107611656189
train_iter_loss: 0.06775053590536118
train_iter_loss: 0.10254666209220886
train_iter_loss: 0.15809673070907593
train_iter_loss: 0.0818190947175026
train_iter_loss: 0.22470121085643768
train_iter_loss: 0.07665583491325378
train_iter_loss: 0.19679199159145355
train_iter_loss: 0.1432499885559082
train_iter_loss: 0.039732806384563446
train_iter_loss: 0.18264897167682648
train_iter_loss: 0.1193109080195427
train_iter_loss: 0.2120620608329773
train_iter_loss: 0.1544598937034607
train_iter_loss: 0.09201215207576752
train_iter_loss: 0.34077316522598267
train_iter_loss: 0.1986893266439438
train_iter_loss: 0.09242216497659683
train_iter_loss: 0.1523120105266571
train_iter_loss: 0.16587534546852112
train_iter_loss: 0.2550061345100403
train loss :0.1553
---------------------
Validation seg loss: 0.214499933759348 at epoch 784
epoch =    785/  1000, exp = train
train_iter_loss: 0.10212593525648117
train_iter_loss: 0.2883712649345398
train_iter_loss: 0.12366271764039993
train_iter_loss: 0.12678317725658417
train_iter_loss: 0.024482514709234238
train_iter_loss: 0.10708037763834
train_iter_loss: 0.15360359847545624
train_iter_loss: 0.24622516334056854
train_iter_loss: 0.11038030683994293
train_iter_loss: 0.13349612057209015
train_iter_loss: 0.1698608249425888
train_iter_loss: 0.14591479301452637
train_iter_loss: 0.14691542088985443
train_iter_loss: 0.1244313046336174
train_iter_loss: 0.18144333362579346
train_iter_loss: 0.1680697202682495
train_iter_loss: 0.06159234046936035
train_iter_loss: 0.09793399274349213
train_iter_loss: 0.10997968912124634
train_iter_loss: 0.06378481537103653
train_iter_loss: 0.14058172702789307
train_iter_loss: 0.1805548220872879
train_iter_loss: 0.1148824468255043
train_iter_loss: 0.15101075172424316
train_iter_loss: 0.12353847175836563
train_iter_loss: 0.15251223742961884
train_iter_loss: 0.09168002009391785
train_iter_loss: 0.15245874226093292
train_iter_loss: 0.12525014579296112
train_iter_loss: 0.08236415684223175
train_iter_loss: 0.28186333179473877
train_iter_loss: 0.038714028894901276
train_iter_loss: 0.19882111251354218
train_iter_loss: 0.1474444419145584
train_iter_loss: 0.136656254529953
train_iter_loss: 0.1335585117340088
train_iter_loss: 0.12351466715335846
train_iter_loss: 0.09372303634881973
train_iter_loss: 0.13615547120571136
train_iter_loss: 0.27572277188301086
train_iter_loss: 0.06946928054094315
train_iter_loss: 0.16878099739551544
train_iter_loss: 0.2083306908607483
train_iter_loss: 0.11963602155447006
train_iter_loss: 0.17230917513370514
train_iter_loss: 0.07882405072450638
train_iter_loss: 0.07360304892063141
train_iter_loss: 0.18231084942817688
train_iter_loss: 0.11651129275560379
train_iter_loss: 0.10891406983137131
train_iter_loss: 0.1066846027970314
train_iter_loss: 0.037769656628370285
train_iter_loss: 0.11306585371494293
train_iter_loss: 0.2786724865436554
train_iter_loss: 0.17418840527534485
train_iter_loss: 0.21926464140415192
train_iter_loss: 0.12144245207309723
train_iter_loss: 0.22083671391010284
train_iter_loss: 0.24028517305850983
train_iter_loss: 0.1435827612876892
train_iter_loss: 0.3252430856227875
train_iter_loss: 0.21605736017227173
train_iter_loss: 0.18265236914157867
train_iter_loss: 0.19877517223358154
train_iter_loss: 0.2784103751182556
train_iter_loss: 0.16049492359161377
train_iter_loss: 0.1589798927307129
train_iter_loss: 0.15284252166748047
train_iter_loss: 0.19186094403266907
train_iter_loss: 0.18031425774097443
train_iter_loss: 0.11404638737440109
train_iter_loss: 0.18751230835914612
train_iter_loss: 0.20721711218357086
train_iter_loss: 0.1311376392841339
train_iter_loss: 0.3103025555610657
train_iter_loss: 0.2111596018075943
train_iter_loss: 0.10850048810243607
train_iter_loss: 0.10784713178873062
train_iter_loss: 0.1021532267332077
train_iter_loss: 0.11026587337255478
train_iter_loss: 0.11174027621746063
train_iter_loss: 0.23520882427692413
train_iter_loss: 0.12452073395252228
train_iter_loss: 0.1591273695230484
train_iter_loss: 0.2625773847103119
train_iter_loss: 0.0650426596403122
train_iter_loss: 0.08838792145252228
train_iter_loss: 0.24449442327022552
train_iter_loss: 0.19854789972305298
train_iter_loss: 0.08028427511453629
train_iter_loss: 0.1643722951412201
train_iter_loss: 0.21815146505832672
train_iter_loss: 0.15207187831401825
train_iter_loss: 0.17413799464702606
train_iter_loss: 0.15197168290615082
train_iter_loss: 0.07491843402385712
train_iter_loss: 0.1488938182592392
train_iter_loss: 0.20527571439743042
train_iter_loss: 0.07753968238830566
train_iter_loss: 0.1773281693458557
train loss :0.1543
---------------------
Validation seg loss: 0.21976937696267412 at epoch 785
epoch =    786/  1000, exp = train
train_iter_loss: 0.07248058915138245
train_iter_loss: 0.14638271927833557
train_iter_loss: 0.16721156239509583
train_iter_loss: 0.20150308310985565
train_iter_loss: 0.17660893499851227
train_iter_loss: 0.05631093308329582
train_iter_loss: 0.2361978441476822
train_iter_loss: 0.1266501247882843
train_iter_loss: 0.08401384204626083
train_iter_loss: 0.0795232281088829
train_iter_loss: 0.09529870748519897
train_iter_loss: 0.21018086373806
train_iter_loss: 0.2522095739841461
train_iter_loss: 0.1714772880077362
train_iter_loss: 0.07151348888874054
train_iter_loss: 0.17062830924987793
train_iter_loss: 0.16143429279327393
train_iter_loss: 0.14796902239322662
train_iter_loss: 0.12312594056129456
train_iter_loss: 0.1454513818025589
train_iter_loss: 0.16714774072170258
train_iter_loss: 0.15912729501724243
train_iter_loss: 0.1482122242450714
train_iter_loss: 0.1393723487854004
train_iter_loss: 0.14417503774166107
train_iter_loss: 0.11972959339618683
train_iter_loss: 0.26392093300819397
train_iter_loss: 0.06644150614738464
train_iter_loss: 0.1758282333612442
train_iter_loss: 0.17418116331100464
train_iter_loss: 0.19593419134616852
train_iter_loss: 0.26143768429756165
train_iter_loss: 0.11595594137907028
train_iter_loss: 0.20370657742023468
train_iter_loss: 0.20561060309410095
train_iter_loss: 0.2748553454875946
train_iter_loss: 0.09535674005746841
train_iter_loss: 0.17217198014259338
train_iter_loss: 0.0954374149441719
train_iter_loss: 0.2012101709842682
train_iter_loss: 0.08152294903993607
train_iter_loss: 0.1613953560590744
train_iter_loss: 0.47087612748146057
train_iter_loss: 0.09076084941625595
train_iter_loss: 0.1523245871067047
train_iter_loss: 0.15970726311206818
train_iter_loss: 0.13601548969745636
train_iter_loss: 0.18422101438045502
train_iter_loss: 0.10920630395412445
train_iter_loss: 0.17797328531742096
train_iter_loss: 0.08864067494869232
train_iter_loss: 0.2519262731075287
train_iter_loss: 0.17776799201965332
train_iter_loss: 0.2320733219385147
train_iter_loss: 0.15156833827495575
train_iter_loss: 0.10960101336240768
train_iter_loss: 0.07306048274040222
train_iter_loss: 0.2696310877799988
train_iter_loss: 0.22443702816963196
train_iter_loss: 0.06068292632699013
train_iter_loss: 0.16753999888896942
train_iter_loss: 0.09182709455490112
train_iter_loss: 0.14215004444122314
train_iter_loss: 0.10937301069498062
train_iter_loss: 0.13060002028942108
train_iter_loss: 0.12845243513584137
train_iter_loss: 0.09695908427238464
train_iter_loss: 0.09440203011035919
train_iter_loss: 0.10201779007911682
train_iter_loss: 0.0803840160369873
train_iter_loss: 0.056771911680698395
train_iter_loss: 0.25365346670150757
train_iter_loss: 0.10613194853067398
train_iter_loss: 0.1872478723526001
train_iter_loss: 0.24823680520057678
train_iter_loss: 0.1129283607006073
train_iter_loss: 0.16039970517158508
train_iter_loss: 0.14064446091651917
train_iter_loss: 0.2825513184070587
train_iter_loss: 0.12084384262561798
train_iter_loss: 0.1890246421098709
train_iter_loss: 0.10195382684469223
train_iter_loss: 0.1697736233472824
train_iter_loss: 0.07666938006877899
train_iter_loss: 0.1907896101474762
train_iter_loss: 0.15052148699760437
train_iter_loss: 0.07908862829208374
train_iter_loss: 0.22758595645427704
train_iter_loss: 0.3330211341381073
train_iter_loss: 0.10705843567848206
train_iter_loss: 0.2898109555244446
train_iter_loss: 0.10571713745594025
train_iter_loss: 0.17769713699817657
train_iter_loss: 0.07006552070379257
train_iter_loss: 0.15150034427642822
train_iter_loss: 0.14067333936691284
train_iter_loss: 0.08656369894742966
train_iter_loss: 0.15878522396087646
train_iter_loss: 0.09343643486499786
train_iter_loss: 0.15071284770965576
train loss :0.1566
---------------------
Validation seg loss: 0.21438479952443884 at epoch 786
epoch =    787/  1000, exp = train
train_iter_loss: 0.1377016007900238
train_iter_loss: 0.1860463172197342
train_iter_loss: 0.14689718186855316
train_iter_loss: 0.18910107016563416
train_iter_loss: 0.09451497346162796
train_iter_loss: 0.06445427238941193
train_iter_loss: 0.12434529513120651
train_iter_loss: 0.2468913495540619
train_iter_loss: 0.220070943236351
train_iter_loss: 0.15655241906642914
train_iter_loss: 0.1645493358373642
train_iter_loss: 0.09098018705844879
train_iter_loss: 0.07519333809614182
train_iter_loss: 0.19920529425144196
train_iter_loss: 0.13395416736602783
train_iter_loss: 0.20514672994613647
train_iter_loss: 0.10835682600736618
train_iter_loss: 0.12289862334728241
train_iter_loss: 0.17020082473754883
train_iter_loss: 0.11669314652681351
train_iter_loss: 0.12653310596942902
train_iter_loss: 0.17443770170211792
train_iter_loss: 0.11821231991052628
train_iter_loss: 0.13527479767799377
train_iter_loss: 0.08978535979986191
train_iter_loss: 0.13203924894332886
train_iter_loss: 0.22275373339653015
train_iter_loss: 0.24994079768657684
train_iter_loss: 0.1270672231912613
train_iter_loss: 0.09443490952253342
train_iter_loss: 0.08105714619159698
train_iter_loss: 0.10573871433734894
train_iter_loss: 0.09826159477233887
train_iter_loss: 0.17011065781116486
train_iter_loss: 0.14196528494358063
train_iter_loss: 0.14037735760211945
train_iter_loss: 0.15255111455917358
train_iter_loss: 0.15318720042705536
train_iter_loss: 0.23242013156414032
train_iter_loss: 0.07053089141845703
train_iter_loss: 0.16969147324562073
train_iter_loss: 0.14167830348014832
train_iter_loss: 0.08383234590291977
train_iter_loss: 0.1911843717098236
train_iter_loss: 0.06115976348519325
train_iter_loss: 0.11473089456558228
train_iter_loss: 0.18242554366588593
train_iter_loss: 0.1282932162284851
train_iter_loss: 0.22526995837688446
train_iter_loss: 0.14588618278503418
train_iter_loss: 0.11487938463687897
train_iter_loss: 0.06753531098365784
train_iter_loss: 0.12444289028644562
train_iter_loss: 0.12788119912147522
train_iter_loss: 0.15168185532093048
train_iter_loss: 0.2119796723127365
train_iter_loss: 0.08998233824968338
train_iter_loss: 0.07868301123380661
train_iter_loss: 0.13377472758293152
train_iter_loss: 0.1053151786327362
train_iter_loss: 0.3002689778804779
train_iter_loss: 0.10317996889352798
train_iter_loss: 0.22254467010498047
train_iter_loss: 0.1176547035574913
train_iter_loss: 0.06422360241413116
train_iter_loss: 0.1717517375946045
train_iter_loss: 0.24528366327285767
train_iter_loss: 0.05351095274090767
train_iter_loss: 0.14104169607162476
train_iter_loss: 0.12627571821212769
train_iter_loss: 0.21912206709384918
train_iter_loss: 0.19380289316177368
train_iter_loss: 0.11913701891899109
train_iter_loss: 0.17350679636001587
train_iter_loss: 0.3676321506500244
train_iter_loss: 0.1548173427581787
train_iter_loss: 0.16534364223480225
train_iter_loss: 0.10466523468494415
train_iter_loss: 0.40659961104393005
train_iter_loss: 0.10044404864311218
train_iter_loss: 0.16251923143863678
train_iter_loss: 0.219618558883667
train_iter_loss: 0.3906392753124237
train_iter_loss: 0.2841145396232605
train_iter_loss: 0.11787791550159454
train_iter_loss: 0.11334014683961868
train_iter_loss: 0.15807592868804932
train_iter_loss: 0.1173424944281578
train_iter_loss: 0.149293914437294
train_iter_loss: 0.2102726548910141
train_iter_loss: 0.08992274105548859
train_iter_loss: 0.12890386581420898
train_iter_loss: 0.1634162813425064
train_iter_loss: 0.2270059585571289
train_iter_loss: 0.23988056182861328
train_iter_loss: 0.22245734930038452
train_iter_loss: 0.07857584953308105
train_iter_loss: 0.19059079885482788
train_iter_loss: 0.12486519664525986
train_iter_loss: 0.09775729477405548
train loss :0.1561
---------------------
Validation seg loss: 0.21131205092236963 at epoch 787
epoch =    788/  1000, exp = train
train_iter_loss: 0.14946503937244415
train_iter_loss: 0.18549896776676178
train_iter_loss: 0.15413713455200195
train_iter_loss: 0.17971587181091309
train_iter_loss: 0.12259528040885925
train_iter_loss: 0.08083157986402512
train_iter_loss: 0.17975455522537231
train_iter_loss: 0.2696513533592224
train_iter_loss: 0.06276682764291763
train_iter_loss: 0.11501128226518631
train_iter_loss: 0.14605270326137543
train_iter_loss: 0.12904806435108185
train_iter_loss: 0.08321923762559891
train_iter_loss: 0.09644075483083725
train_iter_loss: 0.23679734766483307
train_iter_loss: 0.18975946307182312
train_iter_loss: 0.15568223595619202
train_iter_loss: 0.14550785720348358
train_iter_loss: 0.09280047565698624
train_iter_loss: 0.09910295903682709
train_iter_loss: 0.11261380463838577
train_iter_loss: 0.06366202980279922
train_iter_loss: 0.2183903157711029
train_iter_loss: 0.1567956507205963
train_iter_loss: 0.12288565933704376
train_iter_loss: 0.21290837228298187
train_iter_loss: 0.1658804714679718
train_iter_loss: 0.15821900963783264
train_iter_loss: 0.22291424870491028
train_iter_loss: 0.2009754627943039
train_iter_loss: 0.11675114184617996
train_iter_loss: 0.1614319384098053
train_iter_loss: 0.12151066958904266
train_iter_loss: 0.120660200715065
train_iter_loss: 0.14607051014900208
train_iter_loss: 0.19850589334964752
train_iter_loss: 0.22219626605510712
train_iter_loss: 0.2239706814289093
train_iter_loss: 0.07775566726922989
train_iter_loss: 0.20164072513580322
train_iter_loss: 0.13422752916812897
train_iter_loss: 0.16158343851566315
train_iter_loss: 0.18102185428142548
train_iter_loss: 0.1037512794137001
train_iter_loss: 0.1463053822517395
train_iter_loss: 0.17439565062522888
train_iter_loss: 0.14087136089801788
train_iter_loss: 0.04781834036111832
train_iter_loss: 0.17741891741752625
train_iter_loss: 0.2652446925640106
train_iter_loss: 0.16706806421279907
train_iter_loss: 0.10780126601457596
train_iter_loss: 0.1389150470495224
train_iter_loss: 0.15465332567691803
train_iter_loss: 0.11397223174571991
train_iter_loss: 0.15872491896152496
train_iter_loss: 0.21884585916996002
train_iter_loss: 0.15235380828380585
train_iter_loss: 0.18955552577972412
train_iter_loss: 0.2005460560321808
train_iter_loss: 0.14850926399230957
train_iter_loss: 0.18216818571090698
train_iter_loss: 0.167907252907753
train_iter_loss: 0.12019724398851395
train_iter_loss: 0.14507213234901428
train_iter_loss: 0.041788533329963684
train_iter_loss: 0.29515528678894043
train_iter_loss: 0.14307498931884766
train_iter_loss: 0.18415910005569458
train_iter_loss: 0.3258706033229828
train_iter_loss: 0.20616687834262848
train_iter_loss: 0.06035085394978523
train_iter_loss: 0.08989642560482025
train_iter_loss: 0.255532830953598
train_iter_loss: 0.2196870893239975
train_iter_loss: 0.17787499725818634
train_iter_loss: 0.08119199424982071
train_iter_loss: 0.1361263394355774
train_iter_loss: 0.13934490084648132
train_iter_loss: 0.10238545387983322
train_iter_loss: 0.15469202399253845
train_iter_loss: 0.23820137977600098
train_iter_loss: 0.1828235536813736
train_iter_loss: 0.07189977169036865
train_iter_loss: 0.14792148768901825
train_iter_loss: 0.09248298406600952
train_iter_loss: 0.10471480339765549
train_iter_loss: 0.23871473968029022
train_iter_loss: 0.1181022971868515
train_iter_loss: 0.28968149423599243
train_iter_loss: 0.1409919559955597
train_iter_loss: 0.10282569378614426
train_iter_loss: 0.15574049949645996
train_iter_loss: 0.05862957239151001
train_iter_loss: 0.11952944099903107
train_iter_loss: 0.11417337507009506
train_iter_loss: 0.21979869902133942
train_iter_loss: 0.22830528020858765
train_iter_loss: 0.1097865030169487
train_iter_loss: 0.13942161202430725
train loss :0.1564
---------------------
Validation seg loss: 0.2134310940438706 at epoch 788
epoch =    789/  1000, exp = train
train_iter_loss: 0.10475894808769226
train_iter_loss: 0.23153124749660492
train_iter_loss: 0.130482017993927
train_iter_loss: 0.1644945591688156
train_iter_loss: 0.253981351852417
train_iter_loss: 0.11928901821374893
train_iter_loss: 0.14631760120391846
train_iter_loss: 0.23021095991134644
train_iter_loss: 0.1382008194923401
train_iter_loss: 0.2576107680797577
train_iter_loss: 0.1919020414352417
train_iter_loss: 0.13203224539756775
train_iter_loss: 0.196772962808609
train_iter_loss: 0.12076744437217712
train_iter_loss: 0.1675957292318344
train_iter_loss: 0.08036600053310394
train_iter_loss: 0.2117975354194641
train_iter_loss: 0.09366068243980408
train_iter_loss: 0.11394975334405899
train_iter_loss: 0.13859964907169342
train_iter_loss: 0.11732980608940125
train_iter_loss: 0.09978138655424118
train_iter_loss: 0.09049122780561447
train_iter_loss: 0.1982162445783615
train_iter_loss: 0.16261231899261475
train_iter_loss: 0.11567384004592896
train_iter_loss: 0.04431753233075142
train_iter_loss: 0.143571138381958
train_iter_loss: 0.1814521849155426
train_iter_loss: 0.20356416702270508
train_iter_loss: 0.37038853764533997
train_iter_loss: 0.23561756312847137
train_iter_loss: 0.13518105447292328
train_iter_loss: 0.21591469645500183
train_iter_loss: 0.21309857070446014
train_iter_loss: 0.07027910649776459
train_iter_loss: 0.0832422748208046
train_iter_loss: 0.15338732302188873
train_iter_loss: 0.20718416571617126
train_iter_loss: 0.13334956765174866
train_iter_loss: 0.10956713557243347
train_iter_loss: 0.10418315976858139
train_iter_loss: 0.16576997935771942
train_iter_loss: 0.1280793696641922
train_iter_loss: 0.07041598111391068
train_iter_loss: 0.06457492709159851
train_iter_loss: 0.11899928003549576
train_iter_loss: 0.08388455212116241
train_iter_loss: 0.17905156314373016
train_iter_loss: 0.16156189143657684
train_iter_loss: 0.1305845081806183
train_iter_loss: 0.18808603286743164
train_iter_loss: 0.1477050483226776
train_iter_loss: 0.16059467196464539
train_iter_loss: 0.10918094962835312
train_iter_loss: 0.1847117394208908
train_iter_loss: 0.1631062626838684
train_iter_loss: 0.1298162192106247
train_iter_loss: 0.08649257570505142
train_iter_loss: 0.13698796927928925
train_iter_loss: 0.14694415032863617
train_iter_loss: 0.1751234531402588
train_iter_loss: 0.1484927237033844
train_iter_loss: 0.18538148701190948
train_iter_loss: 0.28272944688796997
train_iter_loss: 0.09362917393445969
train_iter_loss: 0.17219460010528564
train_iter_loss: 0.12234178185462952
train_iter_loss: 0.1514625996351242
train_iter_loss: 0.11660956591367722
train_iter_loss: 0.13042666018009186
train_iter_loss: 0.1885349154472351
train_iter_loss: 0.15813574194908142
train_iter_loss: 0.14493969082832336
train_iter_loss: 0.20689238607883453
train_iter_loss: 0.14354842901229858
train_iter_loss: 0.07621131092309952
train_iter_loss: 0.061026543378829956
train_iter_loss: 0.16340750455856323
train_iter_loss: 0.09850398451089859
train_iter_loss: 0.22657033801078796
train_iter_loss: 0.24910835921764374
train_iter_loss: 0.14447571337223053
train_iter_loss: 0.18156202137470245
train_iter_loss: 0.10275785624980927
train_iter_loss: 0.15884780883789062
train_iter_loss: 0.1458454728126526
train_iter_loss: 0.13032667338848114
train_iter_loss: 0.04693862050771713
train_iter_loss: 0.13701048493385315
train_iter_loss: 0.344230979681015
train_iter_loss: 0.1518312245607376
train_iter_loss: 0.13492974638938904
train_iter_loss: 0.1793603152036667
train_iter_loss: 0.14020664989948273
train_iter_loss: 0.18105056881904602
train_iter_loss: 0.15034744143486023
train_iter_loss: 0.08801740407943726
train_iter_loss: 0.08411464840173721
train_iter_loss: 0.15870679914951324
train loss :0.1528
---------------------
Validation seg loss: 0.21542341752364388 at epoch 789
epoch =    790/  1000, exp = train
train_iter_loss: 0.32134416699409485
train_iter_loss: 0.17209291458129883
train_iter_loss: 0.19655822217464447
train_iter_loss: 0.3134603500366211
train_iter_loss: 0.18156826496124268
train_iter_loss: 0.14165860414505005
train_iter_loss: 0.1404031366109848
train_iter_loss: 0.11785805225372314
train_iter_loss: 0.22782182693481445
train_iter_loss: 0.16870421171188354
train_iter_loss: 0.09517515450716019
train_iter_loss: 0.2100117951631546
train_iter_loss: 0.3589758574962616
train_iter_loss: 0.13283482193946838
train_iter_loss: 0.16390852630138397
train_iter_loss: 0.11783420294523239
train_iter_loss: 0.14397922158241272
train_iter_loss: 0.2170686274766922
train_iter_loss: 0.10467147827148438
train_iter_loss: 0.18519385159015656
train_iter_loss: 0.1182725727558136
train_iter_loss: 0.16454647481441498
train_iter_loss: 0.1326572597026825
train_iter_loss: 0.12298800051212311
train_iter_loss: 0.22921431064605713
train_iter_loss: 0.10595172643661499
train_iter_loss: 0.16923314332962036
train_iter_loss: 0.10670699179172516
train_iter_loss: 0.07548259198665619
train_iter_loss: 0.09825921058654785
train_iter_loss: 0.13653114438056946
train_iter_loss: 0.1128244549036026
train_iter_loss: 0.11463505029678345
train_iter_loss: 0.11023685336112976
train_iter_loss: 0.13568761944770813
train_iter_loss: 0.09166043251752853
train_iter_loss: 0.12870170176029205
train_iter_loss: 0.24376054108142853
train_iter_loss: 0.18808825314044952
train_iter_loss: 0.21979539096355438
train_iter_loss: 0.19912327826023102
train_iter_loss: 0.19005750119686127
train_iter_loss: 0.11656539887189865
train_iter_loss: 0.11005068570375443
train_iter_loss: 0.17790907621383667
train_iter_loss: 0.22244144976139069
train_iter_loss: 0.1516709327697754
train_iter_loss: 0.3085204064846039
train_iter_loss: 0.2907865047454834
train_iter_loss: 0.05110438913106918
train_iter_loss: 0.18700508773326874
train_iter_loss: 0.10863010585308075
train_iter_loss: 0.269756019115448
train_iter_loss: 0.04804953932762146
train_iter_loss: 0.06579441577196121
train_iter_loss: 0.2553655803203583
train_iter_loss: 0.1334482729434967
train_iter_loss: 0.1516827940940857
train_iter_loss: 0.2098056674003601
train_iter_loss: 0.0658004954457283
train_iter_loss: 0.043855272233486176
train_iter_loss: 0.17014119029045105
train_iter_loss: 0.19750748574733734
train_iter_loss: 0.15764819085597992
train_iter_loss: 0.2530575096607208
train_iter_loss: 0.08696126192808151
train_iter_loss: 0.25488001108169556
train_iter_loss: 0.14073999226093292
train_iter_loss: 0.1802666336297989
train_iter_loss: 0.17558901011943817
train_iter_loss: 0.1957179307937622
train_iter_loss: 0.1280493140220642
train_iter_loss: 0.0807594507932663
train_iter_loss: 0.10546112805604935
train_iter_loss: 0.1633533388376236
train_iter_loss: 0.14425267279148102
train_iter_loss: 0.07164821773767471
train_iter_loss: 0.10811880230903625
train_iter_loss: 0.18508906662464142
train_iter_loss: 0.2121119350194931
train_iter_loss: 0.07650886476039886
train_iter_loss: 0.0984223410487175
train_iter_loss: 0.1392045021057129
train_iter_loss: 0.22320836782455444
train_iter_loss: 0.13154034316539764
train_iter_loss: 0.21205195784568787
train_iter_loss: 0.10550180822610855
train_iter_loss: 0.10367286205291748
train_iter_loss: 0.0945676863193512
train_iter_loss: 0.1832273304462433
train_iter_loss: 0.1389504224061966
train_iter_loss: 0.09104073792695999
train_iter_loss: 0.10873105376958847
train_iter_loss: 0.1437087506055832
train_iter_loss: 0.16437190771102905
train_iter_loss: 0.04667903110384941
train_iter_loss: 0.18767327070236206
train_iter_loss: 0.1424318552017212
train_iter_loss: 0.1305420994758606
train_iter_loss: 0.1113061010837555
train loss :0.1567
---------------------
Validation seg loss: 0.21585867908906262 at epoch 790
epoch =    791/  1000, exp = train
train_iter_loss: 0.058921195566654205
train_iter_loss: 0.2915484309196472
train_iter_loss: 0.12602104246616364
train_iter_loss: 0.1343081146478653
train_iter_loss: 0.062069911509752274
train_iter_loss: 0.2079164981842041
train_iter_loss: 0.2690863609313965
train_iter_loss: 0.13395921885967255
train_iter_loss: 0.16685672104358673
train_iter_loss: 0.2630923390388489
train_iter_loss: 0.11313354969024658
train_iter_loss: 0.15135428309440613
train_iter_loss: 0.10656281560659409
train_iter_loss: 0.06294896453619003
train_iter_loss: 0.16298380494117737
train_iter_loss: 0.09319379925727844
train_iter_loss: 0.1203266829252243
train_iter_loss: 0.1515624076128006
train_iter_loss: 0.1309981346130371
train_iter_loss: 0.09743717312812805
train_iter_loss: 0.22559890151023865
train_iter_loss: 0.21347558498382568
train_iter_loss: 0.13869088888168335
train_iter_loss: 0.24308748543262482
train_iter_loss: 0.1666802316904068
train_iter_loss: 0.11447981745004654
train_iter_loss: 0.04405571147799492
train_iter_loss: 0.3403697907924652
train_iter_loss: 0.24455447494983673
train_iter_loss: 0.1374884396791458
train_iter_loss: 0.22759054601192474
train_iter_loss: 0.13265027105808258
train_iter_loss: 0.10884805023670197
train_iter_loss: 0.19881963729858398
train_iter_loss: 0.22233708202838898
train_iter_loss: 0.07747385650873184
train_iter_loss: 0.07306395471096039
train_iter_loss: 0.21480636298656464
train_iter_loss: 0.19564995169639587
train_iter_loss: 0.17150898277759552
train_iter_loss: 0.17109742760658264
train_iter_loss: 0.17250163853168488
train_iter_loss: 0.07862156629562378
train_iter_loss: 0.13135474920272827
train_iter_loss: 0.14096012711524963
train_iter_loss: 0.19787690043449402
train_iter_loss: 0.19356171786785126
train_iter_loss: 0.16385017335414886
train_iter_loss: 0.10658163577318192
train_iter_loss: 0.13952630758285522
train_iter_loss: 0.055518098175525665
train_iter_loss: 0.23404787480831146
train_iter_loss: 0.15802034735679626
train_iter_loss: 0.15242117643356323
train_iter_loss: 0.09185058623552322
train_iter_loss: 0.12109683454036713
train_iter_loss: 0.10946344584226608
train_iter_loss: 0.1457677185535431
train_iter_loss: 0.13736294209957123
train_iter_loss: 0.11441797763109207
train_iter_loss: 0.09089883416891098
train_iter_loss: 0.11540496349334717
train_iter_loss: 0.13245035707950592
train_iter_loss: 0.21777111291885376
train_iter_loss: 0.13459497690200806
train_iter_loss: 0.1821526288986206
train_iter_loss: 0.1691872775554657
train_iter_loss: 0.203206405043602
train_iter_loss: 0.0736766830086708
train_iter_loss: 0.20739048719406128
train_iter_loss: 0.11822901666164398
train_iter_loss: 0.17944233119487762
train_iter_loss: 0.02096744440495968
train_iter_loss: 0.11479711532592773
train_iter_loss: 0.12228353321552277
train_iter_loss: 0.21734437346458435
train_iter_loss: 0.140970841050148
train_iter_loss: 0.2230835109949112
train_iter_loss: 0.13880977034568787
train_iter_loss: 0.19718657433986664
train_iter_loss: 0.24139763414859772
train_iter_loss: 0.07935062050819397
train_iter_loss: 0.1686229109764099
train_iter_loss: 0.20773309469223022
train_iter_loss: 0.16210079193115234
train_iter_loss: 0.12980204820632935
train_iter_loss: 0.11812105774879456
train_iter_loss: 0.17774081230163574
train_iter_loss: 0.16136434674263
train_iter_loss: 0.11371084302663803
train_iter_loss: 0.15844951570034027
train_iter_loss: 0.10035175085067749
train_iter_loss: 0.08673664927482605
train_iter_loss: 0.21367758512496948
train_iter_loss: 0.08285857737064362
train_iter_loss: 0.26259922981262207
train_iter_loss: 0.0723438486456871
train_iter_loss: 0.20044200122356415
train_iter_loss: 0.3562430143356323
train_iter_loss: 0.1612495481967926
train loss :0.1562
---------------------
Validation seg loss: 0.2181501965706219 at epoch 791
epoch =    792/  1000, exp = train
train_iter_loss: 0.1126592755317688
train_iter_loss: 0.08279023319482803
train_iter_loss: 0.16474634408950806
train_iter_loss: 0.11016321927309036
train_iter_loss: 0.09785979241132736
train_iter_loss: 0.10497644543647766
train_iter_loss: 0.3122256100177765
train_iter_loss: 0.1729069948196411
train_iter_loss: 0.1286921203136444
train_iter_loss: 0.11533419787883759
train_iter_loss: 0.12682001292705536
train_iter_loss: 0.1401696354150772
train_iter_loss: 0.16695977747440338
train_iter_loss: 0.04987373203039169
train_iter_loss: 0.16621960699558258
train_iter_loss: 0.16340801119804382
train_iter_loss: 0.13676036894321442
train_iter_loss: 0.20653720200061798
train_iter_loss: 0.16641175746917725
train_iter_loss: 0.08888877928256989
train_iter_loss: 0.05060979723930359
train_iter_loss: 0.09491637349128723
train_iter_loss: 0.028367474675178528
train_iter_loss: 0.18639805912971497
train_iter_loss: 0.07067962735891342
train_iter_loss: 0.13540780544281006
train_iter_loss: 0.14243203401565552
train_iter_loss: 0.0547274574637413
train_iter_loss: 0.1453588753938675
train_iter_loss: 0.1457555890083313
train_iter_loss: 0.20872721076011658
train_iter_loss: 0.12543287873268127
train_iter_loss: 0.2374361753463745
train_iter_loss: 0.2010578066110611
train_iter_loss: 0.302543967962265
train_iter_loss: 0.21204809844493866
train_iter_loss: 0.11224910616874695
train_iter_loss: 0.09116362780332565
train_iter_loss: 0.13829611241817474
train_iter_loss: 0.2239527851343155
train_iter_loss: 0.1494598239660263
train_iter_loss: 0.11685977876186371
train_iter_loss: 0.23629292845726013
train_iter_loss: 0.1680191308259964
train_iter_loss: 0.10330004245042801
train_iter_loss: 0.23802775144577026
train_iter_loss: 0.16784408688545227
train_iter_loss: 0.16106447577476501
train_iter_loss: 0.10064056515693665
train_iter_loss: 0.10177458822727203
train_iter_loss: 0.18120211362838745
train_iter_loss: 0.2134743332862854
train_iter_loss: 0.09361610561609268
train_iter_loss: 0.09168630838394165
train_iter_loss: 0.10548704862594604
train_iter_loss: 0.11916033923625946
train_iter_loss: 0.15494218468666077
train_iter_loss: 0.15063315629959106
train_iter_loss: 0.11097919940948486
train_iter_loss: 0.17505036294460297
train_iter_loss: 0.11019688099622726
train_iter_loss: 0.13371887803077698
train_iter_loss: 0.16749219596385956
train_iter_loss: 0.11333569139242172
train_iter_loss: 0.0837218165397644
train_iter_loss: 0.24091510474681854
train_iter_loss: 0.1677623689174652
train_iter_loss: 0.09146545827388763
train_iter_loss: 0.21268680691719055
train_iter_loss: 0.15800601243972778
train_iter_loss: 0.24314916133880615
train_iter_loss: 0.2975623905658722
train_iter_loss: 0.07655960321426392
train_iter_loss: 0.09686456620693207
train_iter_loss: 0.12962041795253754
train_iter_loss: 0.20511354506015778
train_iter_loss: 0.04678281769156456
train_iter_loss: 0.16584251821041107
train_iter_loss: 0.10586380213499069
train_iter_loss: 0.18292218446731567
train_iter_loss: 0.1257842481136322
train_iter_loss: 0.27622342109680176
train_iter_loss: 0.10457093268632889
train_iter_loss: 0.07813216745853424
train_iter_loss: 0.2060491144657135
train_iter_loss: 0.20009920001029968
train_iter_loss: 0.21630489826202393
train_iter_loss: 0.1816573590040207
train_iter_loss: 0.10910668969154358
train_iter_loss: 0.16489526629447937
train_iter_loss: 0.1493823081254959
train_iter_loss: 0.15085728466510773
train_iter_loss: 0.09464439749717712
train_iter_loss: 0.3060607612133026
train_iter_loss: 0.11223714798688889
train_iter_loss: 0.1215364933013916
train_iter_loss: 0.07782463729381561
train_iter_loss: 0.2541183531284332
train_iter_loss: 0.1805836260318756
train_iter_loss: 0.14511099457740784
train loss :0.1510
---------------------
Validation seg loss: 0.2211126546147016 at epoch 792
epoch =    793/  1000, exp = train
train_iter_loss: 0.15506814420223236
train_iter_loss: 0.07379657030105591
train_iter_loss: 0.13654981553554535
train_iter_loss: 0.1430606245994568
train_iter_loss: 0.21070122718811035
train_iter_loss: 0.05083181709051132
train_iter_loss: 0.116961270570755
train_iter_loss: 0.16996265947818756
train_iter_loss: 0.09807564318180084
train_iter_loss: 0.2163911908864975
train_iter_loss: 0.11342356353998184
train_iter_loss: 0.06650964170694351
train_iter_loss: 0.1275811493396759
train_iter_loss: 0.11893431097269058
train_iter_loss: 0.11297708004713058
train_iter_loss: 0.14650128781795502
train_iter_loss: 0.3100546598434448
train_iter_loss: 0.125131756067276
train_iter_loss: 0.14813460409641266
train_iter_loss: 0.1004200354218483
train_iter_loss: 0.15821698307991028
train_iter_loss: 0.09774894267320633
train_iter_loss: 0.2043508142232895
train_iter_loss: 0.1618015021085739
train_iter_loss: 0.1601581871509552
train_iter_loss: 0.14157845079898834
train_iter_loss: 0.12662522494792938
train_iter_loss: 0.11372452974319458
train_iter_loss: 0.14891108870506287
train_iter_loss: 0.10871151834726334
train_iter_loss: 0.13913404941558838
train_iter_loss: 0.09118270128965378
train_iter_loss: 0.22827638685703278
train_iter_loss: 0.09583182632923126
train_iter_loss: 0.18561409413814545
train_iter_loss: 0.10710969567298889
train_iter_loss: 0.07797624915838242
train_iter_loss: 0.17741768062114716
train_iter_loss: 0.2013322114944458
train_iter_loss: 0.1166844591498375
train_iter_loss: 0.08496316522359848
train_iter_loss: 0.12814851105213165
train_iter_loss: 0.14377816021442413
train_iter_loss: 0.1929449737071991
train_iter_loss: 0.16609187424182892
train_iter_loss: 0.11796052753925323
train_iter_loss: 0.1605965942144394
train_iter_loss: 0.0931360051035881
train_iter_loss: 0.13283155858516693
train_iter_loss: 0.19904662668704987
train_iter_loss: 0.23041386902332306
train_iter_loss: 0.1565626710653305
train_iter_loss: 0.18946857750415802
train_iter_loss: 0.1557954102754593
train_iter_loss: 0.3090415298938751
train_iter_loss: 0.18631401658058167
train_iter_loss: 0.14405378699302673
train_iter_loss: 0.20908918976783752
train_iter_loss: 0.24151983857154846
train_iter_loss: 0.2015136331319809
train_iter_loss: 0.17131155729293823
train_iter_loss: 0.04972796514630318
train_iter_loss: 0.21165728569030762
train_iter_loss: 0.3179525136947632
train_iter_loss: 0.17328377068042755
train_iter_loss: 0.17396633327007294
train_iter_loss: 0.181046262383461
train_iter_loss: 0.17071768641471863
train_iter_loss: 0.15262165665626526
train_iter_loss: 0.24843528866767883
train_iter_loss: 0.1098804846405983
train_iter_loss: 0.19648705422878265
train_iter_loss: 0.12770234048366547
train_iter_loss: 0.14363780617713928
train_iter_loss: 0.10015915334224701
train_iter_loss: 0.10197760164737701
train_iter_loss: 0.20725955069065094
train_iter_loss: 0.11294282227754593
train_iter_loss: 0.06756948679685593
train_iter_loss: 0.2165576070547104
train_iter_loss: 0.35579416155815125
train_iter_loss: 0.1483960747718811
train_iter_loss: 0.05951176956295967
train_iter_loss: 0.19535386562347412
train_iter_loss: 0.18469524383544922
train_iter_loss: 0.16561634838581085
train_iter_loss: 0.17568808794021606
train_iter_loss: 0.1941024661064148
train_iter_loss: 0.10804089903831482
train_iter_loss: 0.10335622727870941
train_iter_loss: 0.16996115446090698
train_iter_loss: 0.09299877285957336
train_iter_loss: 0.210006445646286
train_iter_loss: 0.2117115706205368
train_iter_loss: 0.14307630062103271
train_iter_loss: 0.1316576600074768
train_iter_loss: 0.15550822019577026
train_iter_loss: 0.14576642215251923
train_iter_loss: 0.2396097481250763
train_iter_loss: 0.1563863903284073
train loss :0.1576
---------------------
Validation seg loss: 0.2192940675337219 at epoch 793
epoch =    794/  1000, exp = train
train_iter_loss: 0.2119937390089035
train_iter_loss: 0.26275143027305603
train_iter_loss: 0.10718782991170883
train_iter_loss: 0.11432524770498276
train_iter_loss: 0.09423483163118362
train_iter_loss: 0.16299201548099518
train_iter_loss: 0.12028757482767105
train_iter_loss: 0.18537861108779907
train_iter_loss: 0.17676477134227753
train_iter_loss: 0.1614585518836975
train_iter_loss: 0.16767975687980652
train_iter_loss: 0.1239655390381813
train_iter_loss: 0.12461160868406296
train_iter_loss: 0.1544169932603836
train_iter_loss: 0.20035074651241302
train_iter_loss: 0.0977320522069931
train_iter_loss: 0.26253804564476013
train_iter_loss: 0.0705750361084938
train_iter_loss: 0.2973729074001312
train_iter_loss: 0.21878498792648315
train_iter_loss: 0.07971660792827606
train_iter_loss: 0.10925599932670593
train_iter_loss: 0.07969044893980026
train_iter_loss: 0.16975915431976318
train_iter_loss: 0.16641278564929962
train_iter_loss: 0.1061578243970871
train_iter_loss: 0.24666793644428253
train_iter_loss: 0.13592597842216492
train_iter_loss: 0.12014401704072952
train_iter_loss: 0.1525857001543045
train_iter_loss: 0.35570377111434937
train_iter_loss: 0.18151403963565826
train_iter_loss: 0.09888835996389389
train_iter_loss: 0.13345138728618622
train_iter_loss: 0.1358111947774887
train_iter_loss: 0.08334393799304962
train_iter_loss: 0.09478979557752609
train_iter_loss: 0.11028516292572021
train_iter_loss: 0.05118497833609581
train_iter_loss: 0.17628495395183563
train_iter_loss: 0.15328267216682434
train_iter_loss: 0.18545353412628174
train_iter_loss: 0.08451678603887558
train_iter_loss: 0.09581752866506577
train_iter_loss: 0.17263247072696686
train_iter_loss: 0.19052079319953918
train_iter_loss: 0.10156465321779251
train_iter_loss: 0.10388411581516266
train_iter_loss: 0.11875057220458984
train_iter_loss: 0.1321551501750946
train_iter_loss: 0.2402203530073166
train_iter_loss: 0.2243567854166031
train_iter_loss: 0.11542745679616928
train_iter_loss: 0.16499437391757965
train_iter_loss: 0.11035443097352982
train_iter_loss: 0.20778010785579681
train_iter_loss: 0.13024769723415375
train_iter_loss: 0.10726302862167358
train_iter_loss: 0.10126318037509918
train_iter_loss: 0.1709030121564865
train_iter_loss: 0.19292493164539337
train_iter_loss: 0.13706806302070618
train_iter_loss: 0.1501147300004959
train_iter_loss: 0.12655650079250336
train_iter_loss: 0.1439870446920395
train_iter_loss: 0.11267071962356567
train_iter_loss: 0.12355112284421921
train_iter_loss: 0.38016438484191895
train_iter_loss: 0.2604750990867615
train_iter_loss: 0.20771583914756775
train_iter_loss: 0.1084992066025734
train_iter_loss: 0.08990345895290375
train_iter_loss: 0.13505838811397552
train_iter_loss: 0.09629213064908981
train_iter_loss: 0.1240677610039711
train_iter_loss: 0.11858181655406952
train_iter_loss: 0.07953108102083206
train_iter_loss: 0.09232859313488007
train_iter_loss: 0.12253042310476303
train_iter_loss: 0.1174076721072197
train_iter_loss: 0.17032402753829956
train_iter_loss: 0.1243392676115036
train_iter_loss: 0.2222338169813156
train_iter_loss: 0.2067938596010208
train_iter_loss: 0.18978078663349152
train_iter_loss: 0.16417676210403442
train_iter_loss: 0.24438366293907166
train_iter_loss: 0.10241954028606415
train_iter_loss: 0.2503165304660797
train_iter_loss: 0.08176033943891525
train_iter_loss: 0.18900519609451294
train_iter_loss: 0.08372003585100174
train_iter_loss: 0.16163785755634308
train_iter_loss: 0.09001724421977997
train_iter_loss: 0.23939965665340424
train_iter_loss: 0.11355210840702057
train_iter_loss: 0.2634504437446594
train_iter_loss: 0.12175235897302628
train_iter_loss: 0.13501152396202087
train_iter_loss: 0.07429209351539612
train loss :0.1532
---------------------
Validation seg loss: 0.21862003505054228 at epoch 794
epoch =    795/  1000, exp = train
train_iter_loss: 0.09377903491258621
train_iter_loss: 0.15995264053344727
train_iter_loss: 0.08347427845001221
train_iter_loss: 0.07834774255752563
train_iter_loss: 0.17135964334011078
train_iter_loss: 0.13640443980693817
train_iter_loss: 0.15590211749076843
train_iter_loss: 0.04443870484828949
train_iter_loss: 0.15083621442317963
train_iter_loss: 0.12216980010271072
train_iter_loss: 0.1320330649614334
train_iter_loss: 0.11880145967006683
train_iter_loss: 0.1260303407907486
train_iter_loss: 0.1421363204717636
train_iter_loss: 0.1493288278579712
train_iter_loss: 0.17491967976093292
train_iter_loss: 0.1489844173192978
train_iter_loss: 0.18574176728725433
train_iter_loss: 0.06461644917726517
train_iter_loss: 0.16190217435359955
train_iter_loss: 0.19006302952766418
train_iter_loss: 0.1587146520614624
train_iter_loss: 0.2562846839427948
train_iter_loss: 0.07932667434215546
train_iter_loss: 0.1222062036395073
train_iter_loss: 0.35289064049720764
train_iter_loss: 0.150370791554451
train_iter_loss: 0.2615560293197632
train_iter_loss: 0.19403702020645142
train_iter_loss: 0.12838080525398254
train_iter_loss: 0.17836636304855347
train_iter_loss: 0.2195620983839035
train_iter_loss: 0.1965198665857315
train_iter_loss: 0.053155556321144104
train_iter_loss: 0.12025823444128036
train_iter_loss: 0.1737048178911209
train_iter_loss: 0.2698696255683899
train_iter_loss: 0.15450705587863922
train_iter_loss: 0.07161519676446915
train_iter_loss: 0.13994719088077545
train_iter_loss: 0.05916683375835419
train_iter_loss: 0.1155179888010025
train_iter_loss: 0.16914822161197662
train_iter_loss: 0.25165724754333496
train_iter_loss: 0.10008686780929565
train_iter_loss: 0.17858348786830902
train_iter_loss: 0.29128769040107727
train_iter_loss: 0.12262531369924545
train_iter_loss: 0.20175603032112122
train_iter_loss: 0.11734316498041153
train_iter_loss: 0.18353289365768433
train_iter_loss: 0.182367742061615
train_iter_loss: 0.18793514370918274
train_iter_loss: 0.1405644714832306
train_iter_loss: 0.1611301153898239
train_iter_loss: 0.19908460974693298
train_iter_loss: 0.16872790455818176
train_iter_loss: 0.29799798130989075
train_iter_loss: 0.22647704184055328
train_iter_loss: 0.15124496817588806
train_iter_loss: 0.0694766491651535
train_iter_loss: 0.1761186271905899
train_iter_loss: 0.09540723264217377
train_iter_loss: 0.08759541809558868
train_iter_loss: 0.2056073546409607
train_iter_loss: 0.13182681798934937
train_iter_loss: 0.07465896755456924
train_iter_loss: 0.13308173418045044
train_iter_loss: 0.14923536777496338
train_iter_loss: 0.16940191388130188
train_iter_loss: 0.17004024982452393
train_iter_loss: 0.09119614213705063
train_iter_loss: 0.3763209283351898
train_iter_loss: 0.13760705292224884
train_iter_loss: 0.1224508211016655
train_iter_loss: 0.17672842741012573
train_iter_loss: 0.22225791215896606
train_iter_loss: 0.13241294026374817
train_iter_loss: 0.11738613247871399
train_iter_loss: 0.1830374151468277
train_iter_loss: 0.13986746966838837
train_iter_loss: 0.2996594309806824
train_iter_loss: 0.07859969139099121
train_iter_loss: 0.08879067748785019
train_iter_loss: 0.1313413679599762
train_iter_loss: 0.14685827493667603
train_iter_loss: 0.15605193376541138
train_iter_loss: 0.08917034417390823
train_iter_loss: 0.11003091186285019
train_iter_loss: 0.1327206790447235
train_iter_loss: 0.19561131298542023
train_iter_loss: 0.1497807651758194
train_iter_loss: 0.1417667269706726
train_iter_loss: 0.24216210842132568
train_iter_loss: 0.2870050370693207
train_iter_loss: 0.2824735641479492
train_iter_loss: 0.14490672945976257
train_iter_loss: 0.0647931769490242
train_iter_loss: 0.19681933522224426
train_iter_loss: 0.23402541875839233
train loss :0.1607
---------------------
Validation seg loss: 0.21966085988887638 at epoch 795
epoch =    796/  1000, exp = train
train_iter_loss: 0.12032334506511688
train_iter_loss: 0.09856265038251877
train_iter_loss: 0.2475711852312088
train_iter_loss: 0.13888049125671387
train_iter_loss: 0.16383661329746246
train_iter_loss: 0.16301791369915009
train_iter_loss: 0.15061065554618835
train_iter_loss: 0.11016322672367096
train_iter_loss: 0.2768741250038147
train_iter_loss: 0.21364839375019073
train_iter_loss: 0.2455916851758957
train_iter_loss: 0.1214163601398468
train_iter_loss: 0.14088255167007446
train_iter_loss: 0.1993921995162964
train_iter_loss: 0.2107696682214737
train_iter_loss: 0.2537185847759247
train_iter_loss: 0.1959882378578186
train_iter_loss: 0.1259903609752655
train_iter_loss: 0.13133451342582703
train_iter_loss: 0.0834885761141777
train_iter_loss: 0.2107018083333969
train_iter_loss: 0.1304032802581787
train_iter_loss: 0.1027650311589241
train_iter_loss: 0.1573307365179062
train_iter_loss: 0.08766744285821915
train_iter_loss: 0.2916700541973114
train_iter_loss: 0.0812729150056839
train_iter_loss: 0.1535954624414444
train_iter_loss: 0.1333077996969223
train_iter_loss: 0.11775828152894974
train_iter_loss: 0.0638713464140892
train_iter_loss: 0.11145826429128647
train_iter_loss: 0.08172987401485443
train_iter_loss: 0.09158339351415634
train_iter_loss: 0.20385555922985077
train_iter_loss: 0.13065309822559357
train_iter_loss: 0.3438210189342499
train_iter_loss: 0.24561800062656403
train_iter_loss: 0.1449316293001175
train_iter_loss: 0.2010965645313263
train_iter_loss: 0.18134795129299164
train_iter_loss: 0.14666748046875
train_iter_loss: 0.0815216526389122
train_iter_loss: 0.10934990644454956
train_iter_loss: 0.0675327330827713
train_iter_loss: 0.14468742907047272
train_iter_loss: 0.09535148739814758
train_iter_loss: 0.130974680185318
train_iter_loss: 0.15484285354614258
train_iter_loss: 0.14743784070014954
train_iter_loss: 0.1369616836309433
train_iter_loss: 0.1877068132162094
train_iter_loss: 0.13033127784729004
train_iter_loss: 0.2037537395954132
train_iter_loss: 0.21898578107357025
train_iter_loss: 0.12010093033313751
train_iter_loss: 0.12103402614593506
train_iter_loss: 0.04997488111257553
train_iter_loss: 0.17135393619537354
train_iter_loss: 0.1629352867603302
train_iter_loss: 0.05470634624361992
train_iter_loss: 0.19835545122623444
train_iter_loss: 0.19894534349441528
train_iter_loss: 0.08560796082019806
train_iter_loss: 0.17288266122341156
train_iter_loss: 0.21749389171600342
train_iter_loss: 0.17551034688949585
train_iter_loss: 0.12017932534217834
train_iter_loss: 0.13971956074237823
train_iter_loss: 0.3630179762840271
train_iter_loss: 0.1255488395690918
train_iter_loss: 0.08322691917419434
train_iter_loss: 0.22996333241462708
train_iter_loss: 0.2230015993118286
train_iter_loss: 0.147624209523201
train_iter_loss: 0.12034954130649567
train_iter_loss: 0.2099183201789856
train_iter_loss: 0.1553158164024353
train_iter_loss: 0.12947142124176025
train_iter_loss: 0.2014928162097931
train_iter_loss: 0.22710685431957245
train_iter_loss: 0.08266172558069229
train_iter_loss: 0.13352654874324799
train_iter_loss: 0.16146691143512726
train_iter_loss: 0.06145947054028511
train_iter_loss: 0.18086719512939453
train_iter_loss: 0.266340047121048
train_iter_loss: 0.21901315450668335
train_iter_loss: 0.11795736849308014
train_iter_loss: 0.1675357073545456
train_iter_loss: 0.20656809210777283
train_iter_loss: 0.1936502903699875
train_iter_loss: 0.21490974724292755
train_iter_loss: 0.18291325867176056
train_iter_loss: 0.12867535650730133
train_iter_loss: 0.05393478274345398
train_iter_loss: 0.08295342326164246
train_iter_loss: 0.16177789866924286
train_iter_loss: 0.2251884490251541
train_iter_loss: 0.19004955887794495
train loss :0.1601
---------------------
Validation seg loss: 0.21607788524783725 at epoch 796
epoch =    797/  1000, exp = train
train_iter_loss: 0.09312254190444946
train_iter_loss: 0.1453012228012085
train_iter_loss: 0.08265158534049988
train_iter_loss: 0.13106182217597961
train_iter_loss: 0.11344297975301743
train_iter_loss: 0.17780905961990356
train_iter_loss: 0.10754019767045975
train_iter_loss: 0.16149386763572693
train_iter_loss: 0.05829055234789848
train_iter_loss: 0.0873769000172615
train_iter_loss: 0.21308201551437378
train_iter_loss: 0.43907544016838074
train_iter_loss: 0.14696229994297028
train_iter_loss: 0.15858745574951172
train_iter_loss: 0.10037106275558472
train_iter_loss: 0.0851331278681755
train_iter_loss: 0.24985001981258392
train_iter_loss: 0.1139892041683197
train_iter_loss: 0.17294716835021973
train_iter_loss: 0.1823827177286148
train_iter_loss: 0.1668166220188141
train_iter_loss: 0.11387418955564499
train_iter_loss: 0.09019257128238678
train_iter_loss: 0.33540239930152893
train_iter_loss: 0.11510232836008072
train_iter_loss: 0.0780845507979393
train_iter_loss: 0.14473038911819458
train_iter_loss: 0.14074082672595978
train_iter_loss: 0.15893007814884186
train_iter_loss: 0.07944390922784805
train_iter_loss: 0.1576475352048874
train_iter_loss: 0.09264450520277023
train_iter_loss: 0.1694263070821762
train_iter_loss: 0.09002263098955154
train_iter_loss: 0.17502987384796143
train_iter_loss: 0.08593472838401794
train_iter_loss: 0.26087725162506104
train_iter_loss: 0.126946821808815
train_iter_loss: 0.12154282629489899
train_iter_loss: 0.17985408008098602
train_iter_loss: 0.08027521520853043
train_iter_loss: 0.09957963228225708
train_iter_loss: 0.17094574868679047
train_iter_loss: 0.17490078508853912
train_iter_loss: 0.2446741759777069
train_iter_loss: 0.0909709632396698
train_iter_loss: 0.17194606363773346
train_iter_loss: 0.11734270304441452
train_iter_loss: 0.14605192840099335
train_iter_loss: 0.11731788516044617
train_iter_loss: 0.13926056027412415
train_iter_loss: 0.17146119475364685
train_iter_loss: 0.3409401476383209
train_iter_loss: 0.11880230158567429
train_iter_loss: 0.1474040150642395
train_iter_loss: 0.22582010924816132
train_iter_loss: 0.2271634340286255
train_iter_loss: 0.12304183840751648
train_iter_loss: 0.16975724697113037
train_iter_loss: 0.2134023755788803
train_iter_loss: 0.3226461112499237
train_iter_loss: 0.14853785932064056
train_iter_loss: 0.07752129435539246
train_iter_loss: 0.11277108639478683
train_iter_loss: 0.2594153881072998
train_iter_loss: 0.06618760526180267
train_iter_loss: 0.14111031591892242
train_iter_loss: 0.30046263337135315
train_iter_loss: 0.1823294758796692
train_iter_loss: 0.16179470717906952
train_iter_loss: 0.22881214320659637
train_iter_loss: 0.15161557495594025
train_iter_loss: 0.13329409062862396
train_iter_loss: 0.18867044150829315
train_iter_loss: 0.13502463698387146
train_iter_loss: 0.05523507297039032
train_iter_loss: 0.19352062046527863
train_iter_loss: 0.06866559386253357
train_iter_loss: 0.11872298270463943
train_iter_loss: 0.19883814454078674
train_iter_loss: 0.21587300300598145
train_iter_loss: 0.17991521954536438
train_iter_loss: 0.21553076803684235
train_iter_loss: 0.11585938930511475
train_iter_loss: 0.19737280905246735
train_iter_loss: 0.07003787159919739
train_iter_loss: 0.18561041355133057
train_iter_loss: 0.1546122282743454
train_iter_loss: 0.19060979783535004
train_iter_loss: 0.19871334731578827
train_iter_loss: 0.2594412565231323
train_iter_loss: 0.07571600377559662
train_iter_loss: 0.22076208889484406
train_iter_loss: 0.1478278934955597
train_iter_loss: 0.249087855219841
train_iter_loss: 0.1260109543800354
train_iter_loss: 0.1581311672925949
train_iter_loss: 0.14583706855773926
train_iter_loss: 0.13443554937839508
train_iter_loss: 0.13265389204025269
train loss :0.1597
---------------------
Validation seg loss: 0.20983686513390462 at epoch 797
********************
best_val_epoch_loss:  0.20983686513390462
MODEL UPDATED
epoch =    798/  1000, exp = train
train_iter_loss: 0.17407530546188354
train_iter_loss: 0.07525701820850372
train_iter_loss: 0.05707917734980583
train_iter_loss: 0.24285520613193512
train_iter_loss: 0.14021560549736023
train_iter_loss: 0.07887468487024307
train_iter_loss: 0.13783879578113556
train_iter_loss: 0.08304156363010406
train_iter_loss: 0.1107148826122284
train_iter_loss: 0.15003080666065216
train_iter_loss: 0.21518051624298096
train_iter_loss: 0.2430257797241211
train_iter_loss: 0.18732625246047974
train_iter_loss: 0.11482816189527512
train_iter_loss: 0.08722160011529922
train_iter_loss: 0.15295536816120148
train_iter_loss: 0.1477043330669403
train_iter_loss: 0.10327586531639099
train_iter_loss: 0.06553199142217636
train_iter_loss: 0.1414843499660492
train_iter_loss: 0.0882725641131401
train_iter_loss: 0.18768605589866638
train_iter_loss: 0.12752363085746765
train_iter_loss: 0.0801037922501564
train_iter_loss: 0.20757262408733368
train_iter_loss: 0.24956035614013672
train_iter_loss: 0.20696060359477997
train_iter_loss: 0.24011120200157166
train_iter_loss: 0.11936689168214798
train_iter_loss: 0.0846138447523117
train_iter_loss: 0.17490240931510925
train_iter_loss: 0.19586674869060516
train_iter_loss: 0.14665587246418
train_iter_loss: 0.19914790987968445
train_iter_loss: 0.13160215318202972
train_iter_loss: 0.22164452075958252
train_iter_loss: 0.23788340389728546
train_iter_loss: 0.13318488001823425
train_iter_loss: 0.09296423941850662
train_iter_loss: 0.10171782970428467
train_iter_loss: 0.1519080549478531
train_iter_loss: 0.1793977916240692
train_iter_loss: 0.17487962543964386
train_iter_loss: 0.1998046189546585
train_iter_loss: 0.17094995081424713
train_iter_loss: 0.09225244075059891
train_iter_loss: 0.14815165102481842
train_iter_loss: 0.19368666410446167
train_iter_loss: 0.1450578272342682
train_iter_loss: 0.11482911556959152
train_iter_loss: 0.16776196658611298
train_iter_loss: 0.16013112664222717
train_iter_loss: 0.13653691112995148
train_iter_loss: 0.32739290595054626
train_iter_loss: 0.18146035075187683
train_iter_loss: 0.15348902344703674
train_iter_loss: 0.12798169255256653
train_iter_loss: 0.08769704401493073
train_iter_loss: 0.11889784038066864
train_iter_loss: 0.07986900955438614
train_iter_loss: 0.15654130280017853
train_iter_loss: 0.09070173650979996
train_iter_loss: 0.238326758146286
train_iter_loss: 0.16062583029270172
train_iter_loss: 0.06497855484485626
train_iter_loss: 0.04083091765642166
train_iter_loss: 0.1395377218723297
train_iter_loss: 0.34508180618286133
train_iter_loss: 0.16235841810703278
train_iter_loss: 0.16378004848957062
train_iter_loss: 0.2158159464597702
train_iter_loss: 0.11339458078145981
train_iter_loss: 0.26481956243515015
train_iter_loss: 0.06937799602746964
train_iter_loss: 0.14756491780281067
train_iter_loss: 0.16995607316493988
train_iter_loss: 0.1397034227848053
train_iter_loss: 0.14671972393989563
train_iter_loss: 0.15197576582431793
train_iter_loss: 0.16468021273612976
train_iter_loss: 0.1915406733751297
train_iter_loss: 0.2454673796892166
train_iter_loss: 0.09701814502477646
train_iter_loss: 0.22276055812835693
train_iter_loss: 0.17456063628196716
train_iter_loss: 0.15064509212970734
train_iter_loss: 0.1057790219783783
train_iter_loss: 0.1232021301984787
train_iter_loss: 0.09530704468488693
train_iter_loss: 0.2889269292354584
train_iter_loss: 0.18617527186870575
train_iter_loss: 0.23933424055576324
train_iter_loss: 0.35350272059440613
train_iter_loss: 0.1368694305419922
train_iter_loss: 0.09968601167201996
train_iter_loss: 0.11637783795595169
train_iter_loss: 0.1130790263414383
train_iter_loss: 0.10670973360538483
train_iter_loss: 0.2134254425764084
train_iter_loss: 0.1343434602022171
train loss :0.1575
---------------------
Validation seg loss: 0.21681478468455234 at epoch 798
epoch =    799/  1000, exp = train
train_iter_loss: 0.11498119682073593
train_iter_loss: 0.16382206976413727
train_iter_loss: 0.16720715165138245
train_iter_loss: 0.18352356553077698
train_iter_loss: 0.14540933072566986
train_iter_loss: 0.17874714732170105
train_iter_loss: 0.132301926612854
train_iter_loss: 0.16120783984661102
train_iter_loss: 0.18060818314552307
train_iter_loss: 0.3636612594127655
train_iter_loss: 0.17385205626487732
train_iter_loss: 0.18632033467292786
train_iter_loss: 0.206941619515419
train_iter_loss: 0.2145994007587433
train_iter_loss: 0.06133873760700226
train_iter_loss: 0.13459338247776031
train_iter_loss: 0.1882736086845398
train_iter_loss: 0.2345590740442276
train_iter_loss: 0.12329883873462677
train_iter_loss: 0.10263271629810333
train_iter_loss: 0.08882663398981094
train_iter_loss: 0.08370399475097656
train_iter_loss: 0.25918954610824585
train_iter_loss: 0.2958604395389557
train_iter_loss: 0.208547443151474
train_iter_loss: 0.13503225147724152
train_iter_loss: 0.2783239483833313
train_iter_loss: 0.13079756498336792
train_iter_loss: 0.15016333758831024
train_iter_loss: 0.06665576994419098
train_iter_loss: 0.1188180074095726
train_iter_loss: 0.12832990288734436
train_iter_loss: 0.10794737935066223
train_iter_loss: 0.13584445416927338
train_iter_loss: 0.1914861798286438
train_iter_loss: 0.08192524313926697
train_iter_loss: 0.1793028861284256
train_iter_loss: 0.08813240379095078
train_iter_loss: 0.14959904551506042
train_iter_loss: 0.1463279277086258
train_iter_loss: 0.11797664314508438
train_iter_loss: 0.1005128026008606
train_iter_loss: 0.13933226466178894
train_iter_loss: 0.14508302509784698
train_iter_loss: 0.04537314176559448
train_iter_loss: 0.09886673092842102
train_iter_loss: 0.28515708446502686
train_iter_loss: 0.2064775675535202
train_iter_loss: 0.19009487330913544
train_iter_loss: 0.1678065061569214
train_iter_loss: 0.1790335774421692
train_iter_loss: 0.153866246342659
train_iter_loss: 0.09379389882087708
train_iter_loss: 0.08682779222726822
train_iter_loss: 0.20697759091854095
train_iter_loss: 0.08059421181678772
train_iter_loss: 0.14544740319252014
train_iter_loss: 0.15953600406646729
train_iter_loss: 0.12964820861816406
train_iter_loss: 0.14436164498329163
train_iter_loss: 0.20156070590019226
train_iter_loss: 0.2375091314315796
train_iter_loss: 0.12374470382928848
train_iter_loss: 0.13230493664741516
train_iter_loss: 0.11733759939670563
train_iter_loss: 0.2203671783208847
train_iter_loss: 0.12413832545280457
train_iter_loss: 0.1286913901567459
train_iter_loss: 0.10530193895101547
train_iter_loss: 0.18238358199596405
train_iter_loss: 0.14595402777194977
train_iter_loss: 0.24392655491828918
train_iter_loss: 0.16912546753883362
train_iter_loss: 0.10353054106235504
train_iter_loss: 0.11746139824390411
train_iter_loss: 0.08950662612915039
train_iter_loss: 0.06850074976682663
train_iter_loss: 0.10803235322237015
train_iter_loss: 0.10487345606088638
train_iter_loss: 0.17197787761688232
train_iter_loss: 0.23982121050357819
train_iter_loss: 0.17913073301315308
train_iter_loss: 0.10936633497476578
train_iter_loss: 0.09380767494440079
train_iter_loss: 0.10077422857284546
train_iter_loss: 0.14877229928970337
train_iter_loss: 0.2199619859457016
train_iter_loss: 0.1255011111497879
train_iter_loss: 0.17738309502601624
train_iter_loss: 0.10767365247011185
train_iter_loss: 0.1693265289068222
train_iter_loss: 0.1095152273774147
train_iter_loss: 0.15429814159870148
train_iter_loss: 0.13330814242362976
train_iter_loss: 0.11621502786874771
train_iter_loss: 0.06074368208646774
train_iter_loss: 0.28305742144584656
train_iter_loss: 0.11031712591648102
train_iter_loss: 0.22703184187412262
train_iter_loss: 0.09869367629289627
train loss :0.1534
---------------------
Validation seg loss: 0.21473518290715116 at epoch 799
epoch =    800/  1000, exp = train
train_iter_loss: 0.1734374612569809
train_iter_loss: 0.13907285034656525
train_iter_loss: 0.17787866294384003
train_iter_loss: 0.11502014845609665
train_iter_loss: 0.12081748247146606
train_iter_loss: 0.11513463407754898
train_iter_loss: 0.26879802346229553
train_iter_loss: 0.15737493336200714
train_iter_loss: 0.14575931429862976
train_iter_loss: 0.14256690442562103
train_iter_loss: 0.14302290976047516
train_iter_loss: 0.09505890309810638
train_iter_loss: 0.06212576851248741
train_iter_loss: 0.1334964483976364
train_iter_loss: 0.1228087991476059
train_iter_loss: 0.16375426948070526
train_iter_loss: 0.2240300476551056
train_iter_loss: 0.10743684321641922
train_iter_loss: 0.06648554652929306
train_iter_loss: 0.20487743616104126
train_iter_loss: 0.17999392747879028
train_iter_loss: 0.2149149477481842
train_iter_loss: 0.18898262083530426
train_iter_loss: 0.045890819281339645
train_iter_loss: 0.1914433091878891
train_iter_loss: 0.18405847251415253
train_iter_loss: 0.19832825660705566
train_iter_loss: 0.05810040608048439
train_iter_loss: 0.21946346759796143
train_iter_loss: 0.13766011595726013
train_iter_loss: 0.10351982712745667
train_iter_loss: 0.1382354497909546
train_iter_loss: 0.03654107823967934
train_iter_loss: 0.2609473764896393
train_iter_loss: 0.2213844656944275
train_iter_loss: 0.07095873355865479
train_iter_loss: 0.19689899682998657
train_iter_loss: 0.05976041033864021
train_iter_loss: 0.19609147310256958
train_iter_loss: 0.16484147310256958
train_iter_loss: 0.2284730225801468
train_iter_loss: 0.14238598942756653
train_iter_loss: 0.21967609226703644
train_iter_loss: 0.15228641033172607
train_iter_loss: 0.18341661989688873
train_iter_loss: 0.03966878727078438
train_iter_loss: 0.21527379751205444
train_iter_loss: 0.03556467965245247
train_iter_loss: 0.14246688783168793
train_iter_loss: 0.22230148315429688
train_iter_loss: 0.17933914065361023
train_iter_loss: 0.18392126262187958
train_iter_loss: 0.09360896795988083
train_iter_loss: 0.2022387832403183
train_iter_loss: 0.17982113361358643
train_iter_loss: 0.15442803502082825
train_iter_loss: 0.06900519877672195
train_iter_loss: 0.16535578668117523
train_iter_loss: 0.18896780908107758
train_iter_loss: 0.13560710847377777
train_iter_loss: 0.20311838388442993
train_iter_loss: 0.09425471723079681
train_iter_loss: 0.07504983991384506
train_iter_loss: 0.1367524415254593
train_iter_loss: 0.1593254953622818
train_iter_loss: 0.17457419633865356
train_iter_loss: 0.18355606496334076
train_iter_loss: 0.14790397882461548
train_iter_loss: 0.17624084651470184
train_iter_loss: 0.18040627241134644
train_iter_loss: 0.19436758756637573
train_iter_loss: 0.2333141416311264
train_iter_loss: 0.18779534101486206
train_iter_loss: 0.13105331361293793
train_iter_loss: 0.1604207158088684
train_iter_loss: 0.376958429813385
train_iter_loss: 0.2566687762737274
train_iter_loss: 0.1309797465801239
train_iter_loss: 0.04107047617435455
train_iter_loss: 0.17748340964317322
train_iter_loss: 0.12690934538841248
train_iter_loss: 0.05814717337489128
train_iter_loss: 0.06557145714759827
train_iter_loss: 0.09278726577758789
train_iter_loss: 0.20471210777759552
train_iter_loss: 0.18502545356750488
train_iter_loss: 0.07713144272565842
train_iter_loss: 0.21618415415287018
train_iter_loss: 0.14395645260810852
train_iter_loss: 0.20485913753509521
train_iter_loss: 0.2043384462594986
train_iter_loss: 0.10784856975078583
train_iter_loss: 0.1188727468252182
train_iter_loss: 0.14909924566745758
train_iter_loss: 0.27557700872421265
train_iter_loss: 0.1638970822095871
train_iter_loss: 0.04818445071578026
train_iter_loss: 0.2533344626426697
train_iter_loss: 0.08248019963502884
train_iter_loss: 0.2193867266178131
train loss :0.1565
---------------------
Validation seg loss: 0.2177942139535861 at epoch 800
epoch =    801/  1000, exp = train
train_iter_loss: 0.3181517720222473
train_iter_loss: 0.070644810795784
train_iter_loss: 0.11681003868579865
train_iter_loss: 0.1613915115594864
train_iter_loss: 0.2103973925113678
train_iter_loss: 0.16937562823295593
train_iter_loss: 0.11380282789468765
train_iter_loss: 0.11272996664047241
train_iter_loss: 0.17519304156303406
train_iter_loss: 0.07338094711303711
train_iter_loss: 0.1920498013496399
train_iter_loss: 0.19269828498363495
train_iter_loss: 0.15079732239246368
train_iter_loss: 0.13681918382644653
train_iter_loss: 0.21719205379486084
train_iter_loss: 0.18314430117607117
train_iter_loss: 0.17397376894950867
train_iter_loss: 0.18370147049427032
train_iter_loss: 0.11341964453458786
train_iter_loss: 0.061605021357536316
train_iter_loss: 0.0924454927444458
train_iter_loss: 0.20251092314720154
train_iter_loss: 0.23604607582092285
train_iter_loss: 0.07194259762763977
train_iter_loss: 0.3476026654243469
train_iter_loss: 0.13627642393112183
train_iter_loss: 0.13370554149150848
train_iter_loss: 0.20937074720859528
train_iter_loss: 0.15663178265094757
train_iter_loss: 0.07292916625738144
train_iter_loss: 0.27592194080352783
train_iter_loss: 0.06652902066707611
train_iter_loss: 0.11849959194660187
train_iter_loss: 0.12189999222755432
train_iter_loss: 0.13764318823814392
train_iter_loss: 0.2555380165576935
train_iter_loss: 0.1815584897994995
train_iter_loss: 0.11749957501888275
train_iter_loss: 0.11762472987174988
train_iter_loss: 0.11043824255466461
train_iter_loss: 0.07566972076892853
train_iter_loss: 0.10521877557039261
train_iter_loss: 0.12126529216766357
train_iter_loss: 0.1802579015493393
train_iter_loss: 0.22424809634685516
train_iter_loss: 0.12275777012109756
train_iter_loss: 0.14315681159496307
train_iter_loss: 0.1947096437215805
train_iter_loss: 0.08639463037252426
train_iter_loss: 0.09709086269140244
train_iter_loss: 0.14742697775363922
train_iter_loss: 0.3513760566711426
train_iter_loss: 0.10474149882793427
train_iter_loss: 0.20198291540145874
train_iter_loss: 0.2576957046985626
train_iter_loss: 0.14796136319637299
train_iter_loss: 0.20823217928409576
train_iter_loss: 0.09454796463251114
train_iter_loss: 0.16924217343330383
train_iter_loss: 0.5269707441329956
train_iter_loss: 0.23304741084575653
train_iter_loss: 0.22365881502628326
train_iter_loss: 0.09908691048622131
train_iter_loss: 0.06869009137153625
train_iter_loss: 0.06247666850686073
train_iter_loss: 0.1379799097776413
train_iter_loss: 0.10954366624355316
train_iter_loss: 0.38595882058143616
train_iter_loss: 0.09725207835435867
train_iter_loss: 0.04564473032951355
train_iter_loss: 0.08583236485719681
train_iter_loss: 0.14081119000911713
train_iter_loss: 0.15239191055297852
train_iter_loss: 0.2802398204803467
train_iter_loss: 0.08958157896995544
train_iter_loss: 0.13727563619613647
train_iter_loss: 0.2540329396724701
train_iter_loss: 0.19547580182552338
train_iter_loss: 0.14394661784172058
train_iter_loss: 0.1188531294465065
train_iter_loss: 0.26602786779403687
train_iter_loss: 0.14664073288440704
train_iter_loss: 0.19476938247680664
train_iter_loss: 0.1541852355003357
train_iter_loss: 0.2820482552051544
train_iter_loss: 0.10400479286909103
train_iter_loss: 0.26524341106414795
train_iter_loss: 0.07876460254192352
train_iter_loss: 0.16271556913852692
train_iter_loss: 0.09185358881950378
train_iter_loss: 0.11913307756185532
train_iter_loss: 0.12528640031814575
train_iter_loss: 0.1798233687877655
train_iter_loss: 0.21223102509975433
train_iter_loss: 0.07353939116001129
train_iter_loss: 0.13650469481945038
train_iter_loss: 0.06579763442277908
train_iter_loss: 0.07540656626224518
train_iter_loss: 0.17573750019073486
train_iter_loss: 0.21525034308433533
train loss :0.1619
---------------------
Validation seg loss: 0.21350423507568128 at epoch 801
epoch =    802/  1000, exp = train
train_iter_loss: 0.2211240977048874
train_iter_loss: 0.2524198889732361
train_iter_loss: 0.18991900980472565
train_iter_loss: 0.19167256355285645
train_iter_loss: 0.12148842215538025
train_iter_loss: 0.15158234536647797
train_iter_loss: 0.1382095068693161
train_iter_loss: 0.12321653217077255
train_iter_loss: 0.12567737698554993
train_iter_loss: 0.13069677352905273
train_iter_loss: 0.1774739921092987
train_iter_loss: 0.23811130225658417
train_iter_loss: 0.23021574318408966
train_iter_loss: 0.17904356122016907
train_iter_loss: 0.10539212822914124
train_iter_loss: 0.10573995113372803
train_iter_loss: 0.12694427371025085
train_iter_loss: 0.11022533476352692
train_iter_loss: 0.1800370216369629
train_iter_loss: 0.031378280371427536
train_iter_loss: 0.22519773244857788
train_iter_loss: 0.1240425780415535
train_iter_loss: 0.09140969812870026
train_iter_loss: 0.07423809915781021
train_iter_loss: 0.12793360650539398
train_iter_loss: 0.16707538068294525
train_iter_loss: 0.11046690493822098
train_iter_loss: 0.09347861260175705
train_iter_loss: 0.16360777616500854
train_iter_loss: 0.5109682083129883
train_iter_loss: 0.26969629526138306
train_iter_loss: 0.10838763415813446
train_iter_loss: 0.14622390270233154
train_iter_loss: 0.24254433810710907
train_iter_loss: 0.04219904914498329
train_iter_loss: 0.0916261225938797
train_iter_loss: 0.2822786867618561
train_iter_loss: 0.10781040042638779
train_iter_loss: 0.14087951183319092
train_iter_loss: 0.11022445559501648
train_iter_loss: 0.2263779491186142
train_iter_loss: 0.1807720810174942
train_iter_loss: 0.1537778377532959
train_iter_loss: 0.22759276628494263
train_iter_loss: 0.11798833310604095
train_iter_loss: 0.1571178436279297
train_iter_loss: 0.21238218247890472
train_iter_loss: 0.24320843815803528
train_iter_loss: 0.18865659832954407
train_iter_loss: 0.21760942041873932
train_iter_loss: 0.24609066545963287
train_iter_loss: 0.1434917449951172
train_iter_loss: 0.13840153813362122
train_iter_loss: 0.10752709209918976
train_iter_loss: 0.27791547775268555
train_iter_loss: 0.1636580228805542
train_iter_loss: 0.1645517796278
train_iter_loss: 0.168161079287529
train_iter_loss: 0.12970051169395447
train_iter_loss: 0.1741369664669037
train_iter_loss: 0.12716808915138245
train_iter_loss: 0.17829583585262299
train_iter_loss: 0.1862965226173401
train_iter_loss: 0.1830264776945114
train_iter_loss: 0.20846806466579437
train_iter_loss: 0.14367467164993286
train_iter_loss: 0.17119887471199036
train_iter_loss: 0.07748281210660934
train_iter_loss: 0.08893983066082001
train_iter_loss: 0.15625332295894623
train_iter_loss: 0.12099096924066544
train_iter_loss: 0.18486914038658142
train_iter_loss: 0.1653009057044983
train_iter_loss: 0.10920654982328415
train_iter_loss: 0.14688169956207275
train_iter_loss: 0.07668402791023254
train_iter_loss: 0.19767136871814728
train_iter_loss: 0.2782156765460968
train_iter_loss: 0.14024588465690613
train_iter_loss: 0.15369568765163422
train_iter_loss: 0.0772116556763649
train_iter_loss: 0.08269007503986359
train_iter_loss: 0.11771678924560547
train_iter_loss: 0.2215995490550995
train_iter_loss: 0.10354418307542801
train_iter_loss: 0.1905318647623062
train_iter_loss: 0.22129219770431519
train_iter_loss: 0.21623778343200684
train_iter_loss: 0.14038488268852234
train_iter_loss: 0.2758044898509979
train_iter_loss: 0.1777338832616806
train_iter_loss: 0.15000107884407043
train_iter_loss: 0.07148431241512299
train_iter_loss: 0.16702963411808014
train_iter_loss: 0.12403427064418793
train_iter_loss: 0.19240862131118774
train_iter_loss: 0.12767073512077332
train_iter_loss: 0.13248012959957123
train_iter_loss: 0.21131287515163422
train_iter_loss: 0.08493556082248688
train loss :0.1634
---------------------
Validation seg loss: 0.2253859298532161 at epoch 802
epoch =    803/  1000, exp = train
train_iter_loss: 0.16541628539562225
train_iter_loss: 0.17334619164466858
train_iter_loss: 0.12465120106935501
train_iter_loss: 0.43811288475990295
train_iter_loss: 0.1099681481719017
train_iter_loss: 0.1133551076054573
train_iter_loss: 0.3329724669456482
train_iter_loss: 0.09740445762872696
train_iter_loss: 0.2079576849937439
train_iter_loss: 0.15534789860248566
train_iter_loss: 0.12073341012001038
train_iter_loss: 0.08772595226764679
train_iter_loss: 0.08065854758024216
train_iter_loss: 0.22059716284275055
train_iter_loss: 0.07728612422943115
train_iter_loss: 0.1268879920244217
train_iter_loss: 0.1219223290681839
train_iter_loss: 0.26069197058677673
train_iter_loss: 0.1856708824634552
train_iter_loss: 0.1069679856300354
train_iter_loss: 0.06496812403202057
train_iter_loss: 0.12667100131511688
train_iter_loss: 0.16594113409519196
train_iter_loss: 0.26637163758277893
train_iter_loss: 0.20472845435142517
train_iter_loss: 0.03854508697986603
train_iter_loss: 0.15587103366851807
train_iter_loss: 0.2084226757287979
train_iter_loss: 0.11135410517454147
train_iter_loss: 0.12798753380775452
train_iter_loss: 0.12455230206251144
train_iter_loss: 0.14221246540546417
train_iter_loss: 0.12478722631931305
train_iter_loss: 0.11131197959184647
train_iter_loss: 0.22816692292690277
train_iter_loss: 0.09995920211076736
train_iter_loss: 0.11580660194158554
train_iter_loss: 0.13938140869140625
train_iter_loss: 0.1654336303472519
train_iter_loss: 0.151258647441864
train_iter_loss: 0.35006242990493774
train_iter_loss: 0.12542316317558289
train_iter_loss: 0.12436677515506744
train_iter_loss: 0.13217957317829132
train_iter_loss: 0.1477799117565155
train_iter_loss: 0.15510344505310059
train_iter_loss: 0.12865878641605377
train_iter_loss: 0.24173107743263245
train_iter_loss: 0.22460977733135223
train_iter_loss: 0.1714925318956375
train_iter_loss: 0.20603947341442108
train_iter_loss: 0.17618902027606964
train_iter_loss: 0.3120657205581665
train_iter_loss: 0.1403784155845642
train_iter_loss: 0.1777738779783249
train_iter_loss: 0.2048458755016327
train_iter_loss: 0.2758483588695526
train_iter_loss: 0.19939255714416504
train_iter_loss: 0.10689794272184372
train_iter_loss: 0.1560259908437729
train_iter_loss: 0.06501615792512894
train_iter_loss: 0.24026548862457275
train_iter_loss: 0.17881925404071808
train_iter_loss: 0.3346545398235321
train_iter_loss: 0.2147105485200882
train_iter_loss: 0.14344026148319244
train_iter_loss: 0.10372304916381836
train_iter_loss: 0.10414636880159378
train_iter_loss: 0.10146281868219376
train_iter_loss: 0.10211630165576935
train_iter_loss: 0.2136988490819931
train_iter_loss: 0.16872945427894592
train_iter_loss: 0.10837024450302124
train_iter_loss: 0.3842966854572296
train_iter_loss: 0.10152937471866608
train_iter_loss: 0.1156366765499115
train_iter_loss: 0.18227210640907288
train_iter_loss: 0.1739315539598465
train_iter_loss: 0.18063883483409882
train_iter_loss: 0.25832730531692505
train_iter_loss: 0.15873517096042633
train_iter_loss: 0.05411047860980034
train_iter_loss: 0.2214847058057785
train_iter_loss: 0.08405835926532745
train_iter_loss: 0.06606027483940125
train_iter_loss: 0.10368464142084122
train_iter_loss: 0.10417646169662476
train_iter_loss: 0.06926143169403076
train_iter_loss: 0.1358451247215271
train_iter_loss: 0.13364127278327942
train_iter_loss: 0.2599717676639557
train_iter_loss: 0.19094575941562653
train_iter_loss: 0.19824360311031342
train_iter_loss: 0.2010546326637268
train_iter_loss: 0.14915841817855835
train_iter_loss: 0.049698878079652786
train_iter_loss: 0.10761357843875885
train_iter_loss: 0.24556155502796173
train_iter_loss: 0.10265988111495972
train_iter_loss: 0.12302134186029434
train loss :0.1634
---------------------
Validation seg loss: 0.21996316278599343 at epoch 803
epoch =    804/  1000, exp = train
train_iter_loss: 0.12409990280866623
train_iter_loss: 0.2727355360984802
train_iter_loss: 0.10129951685667038
train_iter_loss: 0.19026990234851837
train_iter_loss: 0.18977317214012146
train_iter_loss: 0.11859109997749329
train_iter_loss: 0.15098445117473602
train_iter_loss: 0.06046280264854431
train_iter_loss: 0.1816236525774002
train_iter_loss: 0.13169659674167633
train_iter_loss: 0.17564336955547333
train_iter_loss: 0.11113119125366211
train_iter_loss: 0.11092337220907211
train_iter_loss: 0.12610505521297455
train_iter_loss: 0.17062075436115265
train_iter_loss: 0.2427787333726883
train_iter_loss: 0.08348743617534637
train_iter_loss: 0.10052582621574402
train_iter_loss: 0.07451296597719193
train_iter_loss: 0.2293374389410019
train_iter_loss: 0.04404336214065552
train_iter_loss: 0.194386288523674
train_iter_loss: 0.1498282253742218
train_iter_loss: 0.1461046040058136
train_iter_loss: 0.1447235494852066
train_iter_loss: 0.16296006739139557
train_iter_loss: 0.15899169445037842
train_iter_loss: 0.11381331831216812
train_iter_loss: 0.17002816498279572
train_iter_loss: 0.13984985649585724
train_iter_loss: 0.14064867794513702
train_iter_loss: 0.1868741512298584
train_iter_loss: 0.08011189103126526
train_iter_loss: 0.03746481612324715
train_iter_loss: 0.15870565176010132
train_iter_loss: 0.08839064091444016
train_iter_loss: 0.1442059874534607
train_iter_loss: 0.1442302167415619
train_iter_loss: 0.1861584335565567
train_iter_loss: 0.32731568813323975
train_iter_loss: 0.1695970892906189
train_iter_loss: 0.06376833468675613
train_iter_loss: 0.20145267248153687
train_iter_loss: 0.03782808780670166
train_iter_loss: 0.17379166185855865
train_iter_loss: 0.28541406989097595
train_iter_loss: 0.10083901137113571
train_iter_loss: 0.056493036448955536
train_iter_loss: 0.09063924849033356
train_iter_loss: 0.13783280551433563
train_iter_loss: 0.11468461155891418
train_iter_loss: 0.15625827014446259
train_iter_loss: 0.15374816954135895
train_iter_loss: 0.3264763355255127
train_iter_loss: 0.16314321756362915
train_iter_loss: 0.10743554681539536
train_iter_loss: 0.18238690495491028
train_iter_loss: 0.08752232789993286
train_iter_loss: 0.13681486248970032
train_iter_loss: 0.16003914177417755
train_iter_loss: 0.14135901629924774
train_iter_loss: 0.06521762907505035
train_iter_loss: 0.2018968015909195
train_iter_loss: 0.16019207239151
train_iter_loss: 0.1112338975071907
train_iter_loss: 0.20223985612392426
train_iter_loss: 0.23750771582126617
train_iter_loss: 0.2386743575334549
train_iter_loss: 0.20965155959129333
train_iter_loss: 0.052264824509620667
train_iter_loss: 0.11692465096712112
train_iter_loss: 0.1254776269197464
train_iter_loss: 0.14458945393562317
train_iter_loss: 0.21285289525985718
train_iter_loss: 0.10174582898616791
train_iter_loss: 0.17435027658939362
train_iter_loss: 0.2562256157398224
train_iter_loss: 0.2786839008331299
train_iter_loss: 0.15318627655506134
train_iter_loss: 0.15162894129753113
train_iter_loss: 0.33915188908576965
train_iter_loss: 0.10019342601299286
train_iter_loss: 0.18992742896080017
train_iter_loss: 0.11555929481983185
train_iter_loss: 0.2045980989933014
train_iter_loss: 0.2556021809577942
train_iter_loss: 0.2391798347234726
train_iter_loss: 0.06499571353197098
train_iter_loss: 0.17129398882389069
train_iter_loss: 0.07511411607265472
train_iter_loss: 0.22621774673461914
train_iter_loss: 0.1617761105298996
train_iter_loss: 0.12449625879526138
train_iter_loss: 0.08400312066078186
train_iter_loss: 0.15255576372146606
train_iter_loss: 0.19746921956539154
train_iter_loss: 0.1188894584774971
train_iter_loss: 0.3846087157726288
train_iter_loss: 0.13245312869548798
train_iter_loss: 0.17145486176013947
train loss :0.1578
---------------------
Validation seg loss: 0.211962526119402 at epoch 804
epoch =    805/  1000, exp = train
train_iter_loss: 0.30349013209342957
train_iter_loss: 0.15635357797145844
train_iter_loss: 0.09782461076974869
train_iter_loss: 0.10544971376657486
train_iter_loss: 0.16021694242954254
train_iter_loss: 0.18321682512760162
train_iter_loss: 0.23029392957687378
train_iter_loss: 0.33447787165641785
train_iter_loss: 0.13222429156303406
train_iter_loss: 0.10317014902830124
train_iter_loss: 0.14921869337558746
train_iter_loss: 0.1003260537981987
train_iter_loss: 0.06539994478225708
train_iter_loss: 0.13449318706989288
train_iter_loss: 0.18920327723026276
train_iter_loss: 0.14817441999912262
train_iter_loss: 0.1325097680091858
train_iter_loss: 0.1850735992193222
train_iter_loss: 0.183806911110878
train_iter_loss: 0.09665319323539734
train_iter_loss: 0.22393037378787994
train_iter_loss: 0.17900888621807098
train_iter_loss: 0.2394162267446518
train_iter_loss: 0.21123343706130981
train_iter_loss: 0.029559729620814323
train_iter_loss: 0.13745428621768951
train_iter_loss: 0.17798765003681183
train_iter_loss: 0.03633136674761772
train_iter_loss: 0.2252383530139923
train_iter_loss: 0.13483664393424988
train_iter_loss: 0.11316119134426117
train_iter_loss: 0.11677920073270798
train_iter_loss: 0.14037179946899414
train_iter_loss: 0.15522204339504242
train_iter_loss: 0.19904635846614838
train_iter_loss: 0.19579564034938812
train_iter_loss: 0.12002798169851303
train_iter_loss: 0.05671539157629013
train_iter_loss: 0.1600344479084015
train_iter_loss: 0.05874009430408478
train_iter_loss: 0.2538101077079773
train_iter_loss: 0.2477412223815918
train_iter_loss: 0.10963627696037292
train_iter_loss: 0.1438491940498352
train_iter_loss: 0.20488859713077545
train_iter_loss: 0.1911739706993103
train_iter_loss: 0.133334219455719
train_iter_loss: 0.2518403232097626
train_iter_loss: 0.1799461841583252
train_iter_loss: 0.15090855956077576
train_iter_loss: 0.21792304515838623
train_iter_loss: 0.1301523596048355
train_iter_loss: 0.029149895533919334
train_iter_loss: 0.12811459600925446
train_iter_loss: 0.17542137205600739
train_iter_loss: 0.19012121856212616
train_iter_loss: 0.09585119038820267
train_iter_loss: 0.184297576546669
train_iter_loss: 0.22154618799686432
train_iter_loss: 0.15466561913490295
train_iter_loss: 0.23973947763442993
train_iter_loss: 0.23619680106639862
train_iter_loss: 0.2574014663696289
train_iter_loss: 0.03551053628325462
train_iter_loss: 0.13995613157749176
train_iter_loss: 0.10264165699481964
train_iter_loss: 0.2951304316520691
train_iter_loss: 0.13782744109630585
train_iter_loss: 0.13769124448299408
train_iter_loss: 0.19062019884586334
train_iter_loss: 0.07066680490970612
train_iter_loss: 0.18219289183616638
train_iter_loss: 0.10014155507087708
train_iter_loss: 0.12846019864082336
train_iter_loss: 0.11304309964179993
train_iter_loss: 0.22551308572292328
train_iter_loss: 0.1597987860441208
train_iter_loss: 0.17351141571998596
train_iter_loss: 0.12855200469493866
train_iter_loss: 0.22140586376190186
train_iter_loss: 0.21475568413734436
train_iter_loss: 0.1378898322582245
train_iter_loss: 0.10142616182565689
train_iter_loss: 0.09858887642621994
train_iter_loss: 0.18026067316532135
train_iter_loss: 0.20799869298934937
train_iter_loss: 0.11388450860977173
train_iter_loss: 0.14273139834403992
train_iter_loss: 0.08771604299545288
train_iter_loss: 0.10834383964538574
train_iter_loss: 0.1829359233379364
train_iter_loss: 0.08376352488994598
train_iter_loss: 0.10723099857568741
train_iter_loss: 0.10794633626937866
train_iter_loss: 0.11552941799163818
train_iter_loss: 0.19810576736927032
train_iter_loss: 0.14030835032463074
train_iter_loss: 0.16936057806015015
train_iter_loss: 0.11212995648384094
train_iter_loss: 0.15759281814098358
train loss :0.1570
---------------------
Validation seg loss: 0.2220701658746825 at epoch 805
epoch =    806/  1000, exp = train
train_iter_loss: 0.11246825754642487
train_iter_loss: 0.14281456172466278
train_iter_loss: 0.19134478271007538
train_iter_loss: 0.1255103349685669
train_iter_loss: 0.20816297829151154
train_iter_loss: 0.12560978531837463
train_iter_loss: 0.17751890420913696
train_iter_loss: 0.13708162307739258
train_iter_loss: 0.32541775703430176
train_iter_loss: 0.14758126437664032
train_iter_loss: 0.08951462060213089
train_iter_loss: 0.1358507126569748
train_iter_loss: 0.06692519038915634
train_iter_loss: 0.08897972851991653
train_iter_loss: 0.14064748585224152
train_iter_loss: 0.11497167497873306
train_iter_loss: 0.1268385648727417
train_iter_loss: 0.11945925652980804
train_iter_loss: 0.10765080899000168
train_iter_loss: 0.16050131618976593
train_iter_loss: 0.14976748824119568
train_iter_loss: 0.129922017455101
train_iter_loss: 0.13216863572597504
train_iter_loss: 0.19596929848194122
train_iter_loss: 0.015072761103510857
train_iter_loss: 0.1462976187467575
train_iter_loss: 0.1863705813884735
train_iter_loss: 0.113009013235569
train_iter_loss: 0.06332564353942871
train_iter_loss: 0.09136250615119934
train_iter_loss: 0.23618559539318085
train_iter_loss: 0.10538434982299805
train_iter_loss: 0.18148666620254517
train_iter_loss: 0.21035349369049072
train_iter_loss: 0.24630893766880035
train_iter_loss: 0.2022075206041336
train_iter_loss: 0.0857027918100357
train_iter_loss: 0.09939233213663101
train_iter_loss: 0.20252305269241333
train_iter_loss: 0.09438811987638474
train_iter_loss: 0.032269228249788284
train_iter_loss: 0.2841532230377197
train_iter_loss: 0.06515932828187943
train_iter_loss: 0.12285928428173065
train_iter_loss: 0.050036683678627014
train_iter_loss: 0.07809759676456451
train_iter_loss: 0.07288886606693268
train_iter_loss: 0.10888344049453735
train_iter_loss: 0.1207004114985466
train_iter_loss: 0.18840254843235016
train_iter_loss: 0.11061687022447586
train_iter_loss: 0.1800888180732727
train_iter_loss: 0.2813031077384949
train_iter_loss: 0.25158753991127014
train_iter_loss: 0.17191217839717865
train_iter_loss: 0.17391198873519897
train_iter_loss: 0.14551541209220886
train_iter_loss: 0.23349666595458984
train_iter_loss: 0.19265824556350708
train_iter_loss: 0.08458555489778519
train_iter_loss: 0.11634667217731476
train_iter_loss: 0.11185850203037262
train_iter_loss: 0.16720010340213776
train_iter_loss: 0.08174238353967667
train_iter_loss: 0.11199790984392166
train_iter_loss: 0.08872806280851364
train_iter_loss: 0.19180388748645782
train_iter_loss: 0.13068035244941711
train_iter_loss: 0.1876215785741806
train_iter_loss: 0.2243218570947647
train_iter_loss: 0.14318664371967316
train_iter_loss: 0.2161487340927124
train_iter_loss: 0.18031419813632965
train_iter_loss: 0.2966814935207367
train_iter_loss: 0.13866055011749268
train_iter_loss: 0.2555552124977112
train_iter_loss: 0.10993020236492157
train_iter_loss: 0.18955136835575104
train_iter_loss: 0.246947780251503
train_iter_loss: 0.07751911878585815
train_iter_loss: 0.24188321828842163
train_iter_loss: 0.10024481266736984
train_iter_loss: 0.018978755921125412
train_iter_loss: 0.18801896274089813
train_iter_loss: 0.15598958730697632
train_iter_loss: 0.19602930545806885
train_iter_loss: 0.1092969998717308
train_iter_loss: 0.17319683730602264
train_iter_loss: 0.14462493360042572
train_iter_loss: 0.08771000802516937
train_iter_loss: 0.21632236242294312
train_iter_loss: 0.08713281899690628
train_iter_loss: 0.19105100631713867
train_iter_loss: 0.13476447761058807
train_iter_loss: 0.16949181258678436
train_iter_loss: 0.17289119958877563
train_iter_loss: 0.24089829623699188
train_iter_loss: 0.23594720661640167
train_iter_loss: 0.1304505169391632
train_iter_loss: 0.05956962704658508
train loss :0.1516
---------------------
Validation seg loss: 0.21899323204953997 at epoch 806
epoch =    807/  1000, exp = train
train_iter_loss: 0.1942872554063797
train_iter_loss: 0.14084677398204803
train_iter_loss: 0.19072797894477844
train_iter_loss: 0.17954586446285248
train_iter_loss: 0.2158416211605072
train_iter_loss: 0.08135660737752914
train_iter_loss: 0.1121838167309761
train_iter_loss: 0.1465282291173935
train_iter_loss: 0.2529362738132477
train_iter_loss: 0.21600349247455597
train_iter_loss: 0.1101449504494667
train_iter_loss: 0.2345205694437027
train_iter_loss: 0.1193528100848198
train_iter_loss: 0.08698582649230957
train_iter_loss: 0.2138092815876007
train_iter_loss: 0.07443659007549286
train_iter_loss: 0.10835019499063492
train_iter_loss: 0.16618238389492035
train_iter_loss: 0.180993914604187
train_iter_loss: 0.07844823598861694
train_iter_loss: 0.2002522051334381
train_iter_loss: 0.158980593085289
train_iter_loss: 0.08991722017526627
train_iter_loss: 0.15163449943065643
train_iter_loss: 0.1713196486234665
train_iter_loss: 0.12427198886871338
train_iter_loss: 0.04691516235470772
train_iter_loss: 0.2581077218055725
train_iter_loss: 0.16044312715530396
train_iter_loss: 0.08860383182764053
train_iter_loss: 0.1843252032995224
train_iter_loss: 0.2122109979391098
train_iter_loss: 0.12159664183855057
train_iter_loss: 0.09983821958303452
train_iter_loss: 0.12498249113559723
train_iter_loss: 0.1418159157037735
train_iter_loss: 0.1853833794593811
train_iter_loss: 0.1257125288248062
train_iter_loss: 0.11610021442174911
train_iter_loss: 0.11828520148992538
train_iter_loss: 0.11119011044502258
train_iter_loss: 0.05351726710796356
train_iter_loss: 0.10010866075754166
train_iter_loss: 0.10449425876140594
train_iter_loss: 0.21865448355674744
train_iter_loss: 0.1865248680114746
train_iter_loss: 0.12998616695404053
train_iter_loss: 0.17005304992198944
train_iter_loss: 0.15770088136196136
train_iter_loss: 0.06687985360622406
train_iter_loss: 0.17305995523929596
train_iter_loss: 0.12262190133333206
train_iter_loss: 0.16913540661334991
train_iter_loss: 0.23923571407794952
train_iter_loss: 0.25814446806907654
train_iter_loss: 0.08774854242801666
train_iter_loss: 0.1779918521642685
train_iter_loss: 0.1560516208410263
train_iter_loss: 0.05406717211008072
train_iter_loss: 0.14342685043811798
train_iter_loss: 0.42550885677337646
train_iter_loss: 0.19937719404697418
train_iter_loss: 0.14677557349205017
train_iter_loss: 0.15281547605991364
train_iter_loss: 0.0707956999540329
train_iter_loss: 0.17708949744701385
train_iter_loss: 0.32383325695991516
train_iter_loss: 0.24372413754463196
train_iter_loss: 0.20300397276878357
train_iter_loss: 0.11330239474773407
train_iter_loss: 0.055710211396217346
train_iter_loss: 0.12339217960834503
train_iter_loss: 0.17959097027778625
train_iter_loss: 0.13452334702014923
train_iter_loss: 0.17928430438041687
train_iter_loss: 0.07481411844491959
train_iter_loss: 0.07110364735126495
train_iter_loss: 0.1395779550075531
train_iter_loss: 0.04194311425089836
train_iter_loss: 0.12490886449813843
train_iter_loss: 0.06519681960344315
train_iter_loss: 0.3275538384914398
train_iter_loss: 0.10728306323289871
train_iter_loss: 0.19252704083919525
train_iter_loss: 0.20503082871437073
train_iter_loss: 0.22909826040267944
train_iter_loss: 0.09739433228969574
train_iter_loss: 0.22057794034481049
train_iter_loss: 0.24410763382911682
train_iter_loss: 0.1009918674826622
train_iter_loss: 0.18312197923660278
train_iter_loss: 0.09984468668699265
train_iter_loss: 0.10321572422981262
train_iter_loss: 0.19551561772823334
train_iter_loss: 0.3152849078178406
train_iter_loss: 0.06434158235788345
train_iter_loss: 0.06497850269079208
train_iter_loss: 0.18389660120010376
train_iter_loss: 0.21758565306663513
train_iter_loss: 0.05787698179483414
train loss :0.1547
---------------------
Validation seg loss: 0.21895272232029797 at epoch 807
epoch =    808/  1000, exp = train
train_iter_loss: 0.2185061126947403
train_iter_loss: 0.13622337579727173
train_iter_loss: 0.12365216761827469
train_iter_loss: 0.16813968122005463
train_iter_loss: 0.19204196333885193
train_iter_loss: 0.17881423234939575
train_iter_loss: 0.19529227912425995
train_iter_loss: 0.11778398603200912
train_iter_loss: 0.13355445861816406
train_iter_loss: 0.1260857880115509
train_iter_loss: 0.09264733642339706
train_iter_loss: 0.10672339797019958
train_iter_loss: 0.16684512794017792
train_iter_loss: 0.16530735790729523
train_iter_loss: 0.0477287657558918
train_iter_loss: 0.23515750467777252
train_iter_loss: 0.23065811395645142
train_iter_loss: 0.13195985555648804
train_iter_loss: 0.14864632487297058
train_iter_loss: 0.026579460129141808
train_iter_loss: 0.2498331218957901
train_iter_loss: 0.09040456265211105
train_iter_loss: 0.08078321069478989
train_iter_loss: 0.1418868452310562
train_iter_loss: 0.19371046125888824
train_iter_loss: 0.15725691616535187
train_iter_loss: 0.17627453804016113
train_iter_loss: 0.1297963261604309
train_iter_loss: 0.13581106066703796
train_iter_loss: 0.05275339260697365
train_iter_loss: 0.1192571371793747
train_iter_loss: 0.09870155155658722
train_iter_loss: 0.15871571004390717
train_iter_loss: 0.14145700633525848
train_iter_loss: 0.1141972616314888
train_iter_loss: 0.1524655818939209
train_iter_loss: 0.19726340472698212
train_iter_loss: 0.10636533796787262
train_iter_loss: 0.11995772272348404
train_iter_loss: 0.21230600774288177
train_iter_loss: 0.13772182166576385
train_iter_loss: 0.1682073026895523
train_iter_loss: 0.19586506485939026
train_iter_loss: 0.13602232933044434
train_iter_loss: 0.188120499253273
train_iter_loss: 0.21987628936767578
train_iter_loss: 0.13816328346729279
train_iter_loss: 0.35433077812194824
train_iter_loss: 0.10881627351045609
train_iter_loss: 0.12610118091106415
train_iter_loss: 0.18874993920326233
train_iter_loss: 0.16094212234020233
train_iter_loss: 0.19699013233184814
train_iter_loss: 0.1650320142507553
train_iter_loss: 0.10781507939100266
train_iter_loss: 0.11581834405660629
train_iter_loss: 0.10189052671194077
train_iter_loss: 0.19076840579509735
train_iter_loss: 0.11400210112333298
train_iter_loss: 0.12303522974252701
train_iter_loss: 0.11647612601518631
train_iter_loss: 0.12516561150550842
train_iter_loss: 0.31123021245002747
train_iter_loss: 0.12558002769947052
train_iter_loss: 0.16277159750461578
train_iter_loss: 0.13907484710216522
train_iter_loss: 0.13527193665504456
train_iter_loss: 0.21693195402622223
train_iter_loss: 0.20752325654029846
train_iter_loss: 0.2445504367351532
train_iter_loss: 0.3438953459262848
train_iter_loss: 0.13002245128154755
train_iter_loss: 0.09599627554416656
train_iter_loss: 0.13900500535964966
train_iter_loss: 0.07588350027799606
train_iter_loss: 0.2767520844936371
train_iter_loss: 0.12814387679100037
train_iter_loss: 0.18088793754577637
train_iter_loss: 0.1566496193408966
train_iter_loss: 0.17877544462680817
train_iter_loss: 0.23559972643852234
train_iter_loss: 0.08399302512407303
train_iter_loss: 0.156352698802948
train_iter_loss: 0.1528187096118927
train_iter_loss: 0.15466724336147308
train_iter_loss: 0.16659803688526154
train_iter_loss: 0.05614199861884117
train_iter_loss: 0.10188572853803635
train_iter_loss: 0.13995255529880524
train_iter_loss: 0.08547362685203552
train_iter_loss: 0.1954490691423416
train_iter_loss: 0.14251908659934998
train_iter_loss: 0.14947804808616638
train_iter_loss: 0.20554876327514648
train_iter_loss: 0.1916312873363495
train_iter_loss: 0.15754666924476624
train_iter_loss: 0.1226058080792427
train_iter_loss: 0.19586332142353058
train_iter_loss: 0.10158050805330276
train_iter_loss: 0.12423675507307053
train loss :0.1558
---------------------
Validation seg loss: 0.21290960087317903 at epoch 808
epoch =    809/  1000, exp = train
train_iter_loss: 0.11537402123212814
train_iter_loss: 0.14791105687618256
train_iter_loss: 0.10645497590303421
train_iter_loss: 0.19906862080097198
train_iter_loss: 0.11205317080020905
train_iter_loss: 0.20881831645965576
train_iter_loss: 0.19044211506843567
train_iter_loss: 0.167294442653656
train_iter_loss: 0.13216601312160492
train_iter_loss: 0.14975422620773315
train_iter_loss: 0.20983435213565826
train_iter_loss: 0.1259758472442627
train_iter_loss: 0.15222319960594177
train_iter_loss: 0.1638677418231964
train_iter_loss: 0.07915352284908295
train_iter_loss: 0.19093483686447144
train_iter_loss: 0.1573658138513565
train_iter_loss: 0.08607087284326553
train_iter_loss: 0.2753119468688965
train_iter_loss: 0.07021984457969666
train_iter_loss: 0.12970153987407684
train_iter_loss: 0.16200979053974152
train_iter_loss: 0.12886618077754974
train_iter_loss: 0.16390301287174225
train_iter_loss: 0.10722586512565613
train_iter_loss: 0.1465129405260086
train_iter_loss: 0.19787132740020752
train_iter_loss: 0.16263432800769806
train_iter_loss: 0.08617047965526581
train_iter_loss: 0.11396026611328125
train_iter_loss: 0.17722974717617035
train_iter_loss: 0.10359769314527512
train_iter_loss: 0.15143495798110962
train_iter_loss: 0.2120760977268219
train_iter_loss: 0.19656412303447723
train_iter_loss: 0.05769159644842148
train_iter_loss: 0.13648805022239685
train_iter_loss: 0.11541645228862762
train_iter_loss: 0.1646983027458191
train_iter_loss: 0.18486090004444122
train_iter_loss: 0.2070831060409546
train_iter_loss: 0.1074000671505928
train_iter_loss: 0.16487383842468262
train_iter_loss: 0.13198859989643097
train_iter_loss: 0.22506506741046906
train_iter_loss: 0.13004755973815918
train_iter_loss: 0.02211770974099636
train_iter_loss: 0.16428008675575256
train_iter_loss: 0.15904578566551208
train_iter_loss: 0.1930088996887207
train_iter_loss: 0.11401397734880447
train_iter_loss: 0.10651543736457825
train_iter_loss: 0.11524773389101028
train_iter_loss: 0.06840068101882935
train_iter_loss: 0.08441909402608871
train_iter_loss: 0.13684162497520447
train_iter_loss: 0.08173248171806335
train_iter_loss: 0.1391940861940384
train_iter_loss: 0.22864271700382233
train_iter_loss: 0.10424207895994186
train_iter_loss: 0.13774913549423218
train_iter_loss: 0.20403535664081573
train_iter_loss: 0.5220964550971985
train_iter_loss: 0.0827701985836029
train_iter_loss: 0.15608260035514832
train_iter_loss: 0.10893222689628601
train_iter_loss: 0.07562123239040375
train_iter_loss: 0.08854096382856369
train_iter_loss: 0.1293710470199585
train_iter_loss: 0.12876221537590027
train_iter_loss: 0.12479691952466965
train_iter_loss: 0.20846670866012573
train_iter_loss: 0.19957047700881958
train_iter_loss: 0.0813366249203682
train_iter_loss: 0.16063837707042694
train_iter_loss: 0.15519359707832336
train_iter_loss: 0.19289736449718475
train_iter_loss: 0.1198134645819664
train_iter_loss: 0.14779551327228546
train_iter_loss: 0.07261204719543457
train_iter_loss: 0.1544889509677887
train_iter_loss: 0.10950580984354019
train_iter_loss: 0.21313202381134033
train_iter_loss: 0.2415619045495987
train_iter_loss: 0.14444738626480103
train_iter_loss: 0.09263391047716141
train_iter_loss: 0.18377220630645752
train_iter_loss: 0.2299802601337433
train_iter_loss: 0.15694595873355865
train_iter_loss: 0.21248693764209747
train_iter_loss: 0.18128974735736847
train_iter_loss: 0.21266765892505646
train_iter_loss: 0.13458643853664398
train_iter_loss: 0.08776523172855377
train_iter_loss: 0.06092893332242966
train_iter_loss: 0.20158708095550537
train_iter_loss: 0.17987416684627533
train_iter_loss: 0.2947022318840027
train_iter_loss: 0.22123247385025024
train_iter_loss: 0.1555115133523941
train loss :0.1535
---------------------
Validation seg loss: 0.2174206749922684 at epoch 809
epoch =    810/  1000, exp = train
train_iter_loss: 0.16955143213272095
train_iter_loss: 0.14167241752147675
train_iter_loss: 0.035951945930719376
train_iter_loss: 0.22423122823238373
train_iter_loss: 0.17861859500408173
train_iter_loss: 0.19210906326770782
train_iter_loss: 0.29025816917419434
train_iter_loss: 0.1533173769712448
train_iter_loss: 0.029420500621199608
train_iter_loss: 0.13010850548744202
train_iter_loss: 0.17805302143096924
train_iter_loss: 0.15864530205726624
train_iter_loss: 0.13061833381652832
train_iter_loss: 0.15619462728500366
train_iter_loss: 0.15799620747566223
train_iter_loss: 0.1544962227344513
train_iter_loss: 0.14264975488185883
train_iter_loss: 0.12817060947418213
train_iter_loss: 0.240995392203331
train_iter_loss: 0.18536818027496338
train_iter_loss: 0.09190845489501953
train_iter_loss: 0.13536977767944336
train_iter_loss: 0.14102257788181305
train_iter_loss: 0.13595955073833466
train_iter_loss: 0.062194786965847015
train_iter_loss: 0.1794280856847763
train_iter_loss: 0.1623641550540924
train_iter_loss: 0.12074192613363266
train_iter_loss: 0.13199292123317719
train_iter_loss: 0.11325197666883469
train_iter_loss: 0.13285058736801147
train_iter_loss: 0.2025277316570282
train_iter_loss: 0.2572215497493744
train_iter_loss: 0.36974284052848816
train_iter_loss: 0.05428840592503548
train_iter_loss: 0.20723973214626312
train_iter_loss: 0.10389111936092377
train_iter_loss: 0.18377000093460083
train_iter_loss: 0.20651531219482422
train_iter_loss: 0.27008897066116333
train_iter_loss: 0.15722320973873138
train_iter_loss: 0.14691835641860962
train_iter_loss: 0.11733734607696533
train_iter_loss: 0.12285099178552628
train_iter_loss: 0.0518040731549263
train_iter_loss: 0.07550077885389328
train_iter_loss: 0.11451633274555206
train_iter_loss: 0.08363635092973709
train_iter_loss: 0.13524718582630157
train_iter_loss: 0.25062328577041626
train_iter_loss: 0.20033062994480133
train_iter_loss: 0.09490851312875748
train_iter_loss: 0.18009412288665771
train_iter_loss: 0.088640958070755
train_iter_loss: 0.1427851766347885
train_iter_loss: 0.1700229048728943
train_iter_loss: 0.12843188643455505
train_iter_loss: 0.16976630687713623
train_iter_loss: 0.12971606850624084
train_iter_loss: 0.12384729087352753
train_iter_loss: 0.20679877698421478
train_iter_loss: 0.21078988909721375
train_iter_loss: 0.13605831563472748
train_iter_loss: 0.11509402841329575
train_iter_loss: 0.18481892347335815
train_iter_loss: 0.12930327653884888
train_iter_loss: 0.18643319606781006
train_iter_loss: 0.13862086832523346
train_iter_loss: 0.2853696346282959
train_iter_loss: 0.1663341522216797
train_iter_loss: 0.1829153150320053
train_iter_loss: 0.07580054551362991
train_iter_loss: 0.17912748456001282
train_iter_loss: 0.09688131511211395
train_iter_loss: 0.11187777668237686
train_iter_loss: 0.09274529665708542
train_iter_loss: 0.17043565213680267
train_iter_loss: 0.128700390458107
train_iter_loss: 0.15962515771389008
train_iter_loss: 0.12169148772954941
train_iter_loss: 0.185642272233963
train_iter_loss: 0.13106118142604828
train_iter_loss: 0.12324746698141098
train_iter_loss: 0.16871321201324463
train_iter_loss: 0.21598777174949646
train_iter_loss: 0.16301769018173218
train_iter_loss: 0.10830055177211761
train_iter_loss: 0.18211060762405396
train_iter_loss: 0.15102441608905792
train_iter_loss: 0.2053881138563156
train_iter_loss: 0.10215884447097778
train_iter_loss: 0.3044918477535248
train_iter_loss: 0.15697605907917023
train_iter_loss: 0.126380056142807
train_iter_loss: 0.12456806749105453
train_iter_loss: 0.12068783491849899
train_iter_loss: 0.13549867272377014
train_iter_loss: 0.17746487259864807
train_iter_loss: 0.08141135424375534
train_iter_loss: 0.11067891865968704
train loss :0.1543
---------------------
Validation seg loss: 0.2172444888021586 at epoch 810
epoch =    811/  1000, exp = train
train_iter_loss: 0.20659877359867096
train_iter_loss: 0.08716851472854614
train_iter_loss: 0.1308182030916214
train_iter_loss: 0.12068186700344086
train_iter_loss: 0.0864669680595398
train_iter_loss: 0.1469089686870575
train_iter_loss: 0.23181822896003723
train_iter_loss: 0.1017349436879158
train_iter_loss: 0.1563846915960312
train_iter_loss: 0.07801270484924316
train_iter_loss: 0.15984028577804565
train_iter_loss: 0.14511173963546753
train_iter_loss: 0.25818678736686707
train_iter_loss: 0.23392947018146515
train_iter_loss: 0.26480334997177124
train_iter_loss: 0.16063982248306274
train_iter_loss: 0.2139783501625061
train_iter_loss: 0.24595826864242554
train_iter_loss: 0.13084067404270172
train_iter_loss: 0.16438867151737213
train_iter_loss: 0.1382964551448822
train_iter_loss: 0.18682371079921722
train_iter_loss: 0.12268122285604477
train_iter_loss: 0.1131061464548111
train_iter_loss: 0.09380725026130676
train_iter_loss: 0.16718880832195282
train_iter_loss: 0.06433434784412384
train_iter_loss: 0.1519359052181244
train_iter_loss: 0.10091456770896912
train_iter_loss: 0.1066327691078186
train_iter_loss: 0.3006294071674347
train_iter_loss: 0.24991874396800995
train_iter_loss: 0.16146120429039001
train_iter_loss: 0.17974524199962616
train_iter_loss: 0.09670528769493103
train_iter_loss: 0.15574689209461212
train_iter_loss: 0.12878325581550598
train_iter_loss: 0.11961988359689713
train_iter_loss: 0.20732787251472473
train_iter_loss: 0.21041339635849
train_iter_loss: 0.060118429362773895
train_iter_loss: 0.12608957290649414
train_iter_loss: 0.18135856091976166
train_iter_loss: 0.09742377698421478
train_iter_loss: 0.14245061576366425
train_iter_loss: 0.09957699477672577
train_iter_loss: 0.196237713098526
train_iter_loss: 0.17101913690567017
train_iter_loss: 0.09020321071147919
train_iter_loss: 0.07806471735239029
train_iter_loss: 0.05695914849638939
train_iter_loss: 0.2077934741973877
train_iter_loss: 0.3122316300868988
train_iter_loss: 0.1067083477973938
train_iter_loss: 0.1766122728586197
train_iter_loss: 0.15787599980831146
train_iter_loss: 0.3909429609775543
train_iter_loss: 0.13407017290592194
train_iter_loss: 0.07396771758794785
train_iter_loss: 0.12149221450090408
train_iter_loss: 0.19539456069469452
train_iter_loss: 0.13519106805324554
train_iter_loss: 0.21384119987487793
train_iter_loss: 0.09921656548976898
train_iter_loss: 0.20639289915561676
train_iter_loss: 0.11547365039587021
train_iter_loss: 0.12228020280599594
train_iter_loss: 0.3991045355796814
train_iter_loss: 0.18845266103744507
train_iter_loss: 0.09376370161771774
train_iter_loss: 0.0847596749663353
train_iter_loss: 0.20653703808784485
train_iter_loss: 0.06534615904092789
train_iter_loss: 0.13616161048412323
train_iter_loss: 0.17139583826065063
train_iter_loss: 0.09660325199365616
train_iter_loss: 0.1626225858926773
train_iter_loss: 0.24765466153621674
train_iter_loss: 0.09594989567995071
train_iter_loss: 0.11663073301315308
train_iter_loss: 0.3215884864330292
train_iter_loss: 0.23266683518886566
train_iter_loss: 0.09482821822166443
train_iter_loss: 0.1138620376586914
train_iter_loss: 0.11217383295297623
train_iter_loss: 0.11446911096572876
train_iter_loss: 0.21194231510162354
train_iter_loss: 0.26006144285202026
train_iter_loss: 0.10878441482782364
train_iter_loss: 0.12684501707553864
train_iter_loss: 0.0668192058801651
train_iter_loss: 0.09940574318170547
train_iter_loss: 0.1789754182100296
train_iter_loss: 0.08564237505197525
train_iter_loss: 0.056116022169589996
train_iter_loss: 0.23152336478233337
train_iter_loss: 0.08959420025348663
train_iter_loss: 0.12757080793380737
train_iter_loss: 0.16984349489212036
train_iter_loss: 0.1750994771718979
train loss :0.1565
---------------------
Validation seg loss: 0.21212152295025452 at epoch 811
epoch =    812/  1000, exp = train
train_iter_loss: 0.1290866881608963
train_iter_loss: 0.14305110275745392
train_iter_loss: 0.15088427066802979
train_iter_loss: 0.13173522055149078
train_iter_loss: 0.09300576150417328
train_iter_loss: 0.29886287450790405
train_iter_loss: 0.1228836253285408
train_iter_loss: 0.3387909531593323
train_iter_loss: 0.03363010287284851
train_iter_loss: 0.16228055953979492
train_iter_loss: 0.09084613621234894
train_iter_loss: 0.05265315994620323
train_iter_loss: 0.08833099901676178
train_iter_loss: 0.12017086893320084
train_iter_loss: 0.1242755800485611
train_iter_loss: 0.138905867934227
train_iter_loss: 0.2450721561908722
train_iter_loss: 0.16746072471141815
train_iter_loss: 0.24405382573604584
train_iter_loss: 0.15275870263576508
train_iter_loss: 0.16232438385486603
train_iter_loss: 0.1452988088130951
train_iter_loss: 0.10935816913843155
train_iter_loss: 0.12097723037004471
train_iter_loss: 0.13852855563163757
train_iter_loss: 0.1075192540884018
train_iter_loss: 0.15038950741291046
train_iter_loss: 0.13179045915603638
train_iter_loss: 0.08141901344060898
train_iter_loss: 0.11138792335987091
train_iter_loss: 0.11211792379617691
train_iter_loss: 0.09947893768548965
train_iter_loss: 0.09913171827793121
train_iter_loss: 0.043614476919174194
train_iter_loss: 0.12332667410373688
train_iter_loss: 0.13834013044834137
train_iter_loss: 0.13038215041160583
train_iter_loss: 0.14129284024238586
train_iter_loss: 0.15741637349128723
train_iter_loss: 0.15691731870174408
train_iter_loss: 0.14821535348892212
train_iter_loss: 0.2124832421541214
train_iter_loss: 0.15918061137199402
train_iter_loss: 0.14878354966640472
train_iter_loss: 0.1194193959236145
train_iter_loss: 0.04961482435464859
train_iter_loss: 0.1914704144001007
train_iter_loss: 0.13755641877651215
train_iter_loss: 0.20246443152427673
train_iter_loss: 0.116798996925354
train_iter_loss: 0.1450674831867218
train_iter_loss: 0.16352632641792297
train_iter_loss: 0.19736771285533905
train_iter_loss: 0.24410352110862732
train_iter_loss: 0.1673683375120163
train_iter_loss: 0.20053531229496002
train_iter_loss: 0.09777796268463135
train_iter_loss: 0.2439308613538742
train_iter_loss: 0.08541882038116455
train_iter_loss: 0.15930034220218658
train_iter_loss: 0.04438445344567299
train_iter_loss: 0.17388540506362915
train_iter_loss: 0.11675869673490524
train_iter_loss: 0.09224696457386017
train_iter_loss: 0.16923120617866516
train_iter_loss: 0.11674255132675171
train_iter_loss: 0.08524166792631149
train_iter_loss: 0.2910988926887512
train_iter_loss: 0.08451734483242035
train_iter_loss: 0.18724879622459412
train_iter_loss: 0.11768274009227753
train_iter_loss: 0.10894610732793808
train_iter_loss: 0.1519506275653839
train_iter_loss: 0.08282743394374847
train_iter_loss: 0.17380671203136444
train_iter_loss: 0.15069089829921722
train_iter_loss: 0.13623739778995514
train_iter_loss: 0.1267850399017334
train_iter_loss: 0.2024044692516327
train_iter_loss: 0.31342947483062744
train_iter_loss: 0.1341060996055603
train_iter_loss: 0.2294735312461853
train_iter_loss: 0.37137332558631897
train_iter_loss: 0.190649151802063
train_iter_loss: 0.18821899592876434
train_iter_loss: 0.11579662561416626
train_iter_loss: 0.2077227532863617
train_iter_loss: 0.1611718088388443
train_iter_loss: 0.10503730922937393
train_iter_loss: 0.1733989715576172
train_iter_loss: 0.25234806537628174
train_iter_loss: 0.21942564845085144
train_iter_loss: 0.15302790701389313
train_iter_loss: 0.17980827391147614
train_iter_loss: 0.12379178404808044
train_iter_loss: 0.2394578903913498
train_iter_loss: 0.16466589272022247
train_iter_loss: 0.1383877396583557
train_iter_loss: 0.09060417860746384
train_iter_loss: 0.18412595987319946
train loss :0.1539
---------------------
Validation seg loss: 0.20897693516475693 at epoch 812
********************
best_val_epoch_loss:  0.20897693516475693
MODEL UPDATED
epoch =    813/  1000, exp = train
train_iter_loss: 0.13323621451854706
train_iter_loss: 0.13486455380916595
train_iter_loss: 0.174076110124588
train_iter_loss: 0.17787499725818634
train_iter_loss: 0.08051321655511856
train_iter_loss: 0.19665035605430603
train_iter_loss: 0.1138717457652092
train_iter_loss: 0.17941103875637054
train_iter_loss: 0.15814737975597382
train_iter_loss: 0.12834672629833221
train_iter_loss: 0.15660303831100464
train_iter_loss: 0.17091283202171326
train_iter_loss: 0.13448862731456757
train_iter_loss: 0.20842349529266357
train_iter_loss: 0.1241302639245987
train_iter_loss: 0.16617265343666077
train_iter_loss: 0.07362983375787735
train_iter_loss: 0.18131917715072632
train_iter_loss: 0.10736870020627975
train_iter_loss: 0.07672084122896194
train_iter_loss: 0.13854312896728516
train_iter_loss: 0.09103935211896896
train_iter_loss: 0.13986924290657043
train_iter_loss: 0.18824900686740875
train_iter_loss: 0.16692319512367249
train_iter_loss: 0.24983271956443787
train_iter_loss: 0.08205277472734451
train_iter_loss: 0.13274838030338287
train_iter_loss: 0.18868963420391083
train_iter_loss: 0.16459806263446808
train_iter_loss: 0.2498290091753006
train_iter_loss: 0.18223419785499573
train_iter_loss: 0.2226858139038086
train_iter_loss: 0.12719136476516724
train_iter_loss: 0.12379289418458939
train_iter_loss: 0.06967990100383759
train_iter_loss: 0.2216997593641281
train_iter_loss: 0.1516815572977066
train_iter_loss: 0.1442171186208725
train_iter_loss: 0.11811000853776932
train_iter_loss: 0.26898136734962463
train_iter_loss: 0.11822161078453064
train_iter_loss: 0.0668615773320198
train_iter_loss: 0.1707073599100113
train_iter_loss: 0.09822782874107361
train_iter_loss: 0.18046337366104126
train_iter_loss: 0.17989157140254974
train_iter_loss: 0.0808817520737648
train_iter_loss: 0.12919948995113373
train_iter_loss: 0.10788332670927048
train_iter_loss: 0.17425788938999176
train_iter_loss: 0.0687623992562294
train_iter_loss: 0.07761445641517639
train_iter_loss: 0.017406519502401352
train_iter_loss: 0.40059781074523926
train_iter_loss: 0.22264724969863892
train_iter_loss: 0.04662366583943367
train_iter_loss: 0.08259811252355576
train_iter_loss: 0.24933026731014252
train_iter_loss: 0.2049550712108612
train_iter_loss: 0.11012991517782211
train_iter_loss: 0.1695621907711029
train_iter_loss: 0.09718529880046844
train_iter_loss: 0.12054514139890671
train_iter_loss: 0.3106997311115265
train_iter_loss: 0.11918295919895172
train_iter_loss: 0.24703006446361542
train_iter_loss: 0.09817175567150116
train_iter_loss: 0.12801365554332733
train_iter_loss: 0.18116389214992523
train_iter_loss: 0.22679392993450165
train_iter_loss: 0.1252172440290451
train_iter_loss: 0.1260988712310791
train_iter_loss: 0.19822534918785095
train_iter_loss: 0.11915598809719086
train_iter_loss: 0.1093817949295044
train_iter_loss: 0.11695502698421478
train_iter_loss: 0.10104614496231079
train_iter_loss: 0.05231309309601784
train_iter_loss: 0.21261879801750183
train_iter_loss: 0.2089136689901352
train_iter_loss: 0.2314208447933197
train_iter_loss: 0.21011579036712646
train_iter_loss: 0.1798812299966812
train_iter_loss: 0.22296957671642303
train_iter_loss: 0.11930342018604279
train_iter_loss: 0.1278788149356842
train_iter_loss: 0.2702149748802185
train_iter_loss: 0.05463315546512604
train_iter_loss: 0.22437454760074615
train_iter_loss: 0.4721003770828247
train_iter_loss: 0.1617184430360794
train_iter_loss: 0.10271710902452469
train_iter_loss: 0.16795067489147186
train_iter_loss: 0.2428353875875473
train_iter_loss: 0.14019818603992462
train_iter_loss: 0.18212084472179413
train_iter_loss: 0.1583472639322281
train_iter_loss: 0.2811526656150818
train_iter_loss: 0.21511496603488922
train loss :0.1607
---------------------
Validation seg loss: 0.2130872022606573 at epoch 813
epoch =    814/  1000, exp = train
train_iter_loss: 0.2503305673599243
train_iter_loss: 0.10582536458969116
train_iter_loss: 0.14442096650600433
train_iter_loss: 0.16617529094219208
train_iter_loss: 0.1373591125011444
train_iter_loss: 0.14342361688613892
train_iter_loss: 0.1147003173828125
train_iter_loss: 0.25998562574386597
train_iter_loss: 0.16815906763076782
train_iter_loss: 0.054156843572854996
train_iter_loss: 0.09994083642959595
train_iter_loss: 0.1464281976222992
train_iter_loss: 0.14980487525463104
train_iter_loss: 0.2185700237751007
train_iter_loss: 0.15053343772888184
train_iter_loss: 0.11243642121553421
train_iter_loss: 0.16782411932945251
train_iter_loss: 0.2504059672355652
train_iter_loss: 0.16699351370334625
train_iter_loss: 0.10669391602277756
train_iter_loss: 0.1537238508462906
train_iter_loss: 0.16583143174648285
train_iter_loss: 0.16837498545646667
train_iter_loss: 0.19201719760894775
train_iter_loss: 0.2647005021572113
train_iter_loss: 0.17624926567077637
train_iter_loss: 0.07717563211917877
train_iter_loss: 0.09787038713693619
train_iter_loss: 0.09377902001142502
train_iter_loss: 0.1854020208120346
train_iter_loss: 0.11964257061481476
train_iter_loss: 0.14325399696826935
train_iter_loss: 0.10070635378360748
train_iter_loss: 0.14730964601039886
train_iter_loss: 0.14713561534881592
train_iter_loss: 0.16928145289421082
train_iter_loss: 0.21438202261924744
train_iter_loss: 0.160038560628891
train_iter_loss: 0.1567048281431198
train_iter_loss: 0.1980794370174408
train_iter_loss: 0.18997295200824738
train_iter_loss: 0.23693017661571503
train_iter_loss: 0.028128160163760185
train_iter_loss: 0.11180611699819565
train_iter_loss: 0.1440722495317459
train_iter_loss: 0.17787180840969086
train_iter_loss: 0.13886988162994385
train_iter_loss: 0.0661020576953888
train_iter_loss: 0.3953365087509155
train_iter_loss: 0.11598861962556839
train_iter_loss: 0.10753384232521057
train_iter_loss: 0.15647439658641815
train_iter_loss: 0.09930440038442612
train_iter_loss: 0.21831703186035156
train_iter_loss: 0.12458125501871109
train_iter_loss: 0.07938316464424133
train_iter_loss: 0.18339663743972778
train_iter_loss: 0.1451111137866974
train_iter_loss: 0.14849938452243805
train_iter_loss: 0.1419774442911148
train_iter_loss: 0.2762617766857147
train_iter_loss: 0.17266570031642914
train_iter_loss: 0.13560272753238678
train_iter_loss: 0.2693972885608673
train_iter_loss: 0.09769989550113678
train_iter_loss: 0.15825597941875458
train_iter_loss: 0.23366515338420868
train_iter_loss: 0.13497532904148102
train_iter_loss: 0.21739506721496582
train_iter_loss: 0.24132229387760162
train_iter_loss: 0.2620454728603363
train_iter_loss: 0.2416425496339798
train_iter_loss: 0.32758328318595886
train_iter_loss: 0.14759573340415955
train_iter_loss: 0.0731298178434372
train_iter_loss: 0.19263389706611633
train_iter_loss: 0.16264905035495758
train_iter_loss: 0.13901585340499878
train_iter_loss: 0.08520591259002686
train_iter_loss: 0.07032955437898636
train_iter_loss: 0.21511045098304749
train_iter_loss: 0.15945981442928314
train_iter_loss: 0.12468266487121582
train_iter_loss: 0.14415329694747925
train_iter_loss: 0.16796328127384186
train_iter_loss: 0.11584781110286713
train_iter_loss: 0.1320311725139618
train_iter_loss: 0.08635932207107544
train_iter_loss: 0.09610099345445633
train_iter_loss: 0.11571410298347473
train_iter_loss: 0.0538046695291996
train_iter_loss: 0.16855385899543762
train_iter_loss: 0.10112649947404861
train_iter_loss: 0.2590144872665405
train_iter_loss: 0.1188342496752739
train_iter_loss: 0.11900055408477783
train_iter_loss: 0.1616828590631485
train_iter_loss: 0.14381663501262665
train_iter_loss: 0.06819960474967957
train_iter_loss: 0.13486269116401672
train loss :0.1574
---------------------
Validation seg loss: 0.214617520992486 at epoch 814
epoch =    815/  1000, exp = train
train_iter_loss: 0.16017593443393707
train_iter_loss: 0.33530086278915405
train_iter_loss: 0.08486491441726685
train_iter_loss: 0.18496069312095642
train_iter_loss: 0.17034724354743958
train_iter_loss: 0.15210957825183868
train_iter_loss: 0.1588844656944275
train_iter_loss: 0.14550359547138214
train_iter_loss: 0.16353349387645721
train_iter_loss: 0.13182179629802704
train_iter_loss: 0.11856546252965927
train_iter_loss: 0.21070514619350433
train_iter_loss: 0.14522108435630798
train_iter_loss: 0.12614424526691437
train_iter_loss: 0.194819837808609
train_iter_loss: 0.12255734950304031
train_iter_loss: 0.13368557393550873
train_iter_loss: 0.12826210260391235
train_iter_loss: 0.0801963210105896
train_iter_loss: 0.03455840051174164
train_iter_loss: 0.24478061497211456
train_iter_loss: 0.16249868273735046
train_iter_loss: 0.07907666265964508
train_iter_loss: 0.09274740517139435
train_iter_loss: 0.056335609406232834
train_iter_loss: 0.3391052186489105
train_iter_loss: 0.2761508822441101
train_iter_loss: 0.17882056534290314
train_iter_loss: 0.22157633304595947
train_iter_loss: 0.07671782374382019
train_iter_loss: 0.11491387337446213
train_iter_loss: 0.16845069825649261
train_iter_loss: 0.10724352300167084
train_iter_loss: 0.11991159617900848
train_iter_loss: 0.19165709614753723
train_iter_loss: 0.18970036506652832
train_iter_loss: 0.11241133511066437
train_iter_loss: 0.22742269933223724
train_iter_loss: 0.045056529343128204
train_iter_loss: 0.2633999288082123
train_iter_loss: 0.15526120364665985
train_iter_loss: 0.241715207695961
train_iter_loss: 0.1841169148683548
train_iter_loss: 0.19360367953777313
train_iter_loss: 0.2257072925567627
train_iter_loss: 0.22103066742420197
train_iter_loss: 0.19250425696372986
train_iter_loss: 0.06724066287279129
train_iter_loss: 0.18347948789596558
train_iter_loss: 0.14424096047878265
train_iter_loss: 0.2116754949092865
train_iter_loss: 0.15893958508968353
train_iter_loss: 0.16531097888946533
train_iter_loss: 0.19105100631713867
train_iter_loss: 0.17129242420196533
train_iter_loss: 0.08514892309904099
train_iter_loss: 0.18358497321605682
train_iter_loss: 0.06449424475431442
train_iter_loss: 0.2815662622451782
train_iter_loss: 0.13123111426830292
train_iter_loss: 0.09168678522109985
train_iter_loss: 0.1502525806427002
train_iter_loss: 0.30391359329223633
train_iter_loss: 0.17994260787963867
train_iter_loss: 0.19330869615077972
train_iter_loss: 0.16625690460205078
train_iter_loss: 0.12455511838197708
train_iter_loss: 0.13733911514282227
train_iter_loss: 0.253779798746109
train_iter_loss: 0.13093915581703186
train_iter_loss: 0.09261627495288849
train_iter_loss: 0.06387200206518173
train_iter_loss: 0.09390715509653091
train_iter_loss: 0.22299478948116302
train_iter_loss: 0.143782377243042
train_iter_loss: 0.1848216950893402
train_iter_loss: 0.04033518582582474
train_iter_loss: 0.11575613170862198
train_iter_loss: 0.12752816081047058
train_iter_loss: 0.13738025724887848
train_iter_loss: 0.1500667929649353
train_iter_loss: 0.12517496943473816
train_iter_loss: 0.11983329057693481
train_iter_loss: 0.1741853803396225
train_iter_loss: 0.1804705113172531
train_iter_loss: 0.13654707372188568
train_iter_loss: 0.16429594159126282
train_iter_loss: 0.18018491566181183
train_iter_loss: 0.15295778214931488
train_iter_loss: 0.1541435569524765
train_iter_loss: 0.13697409629821777
train_iter_loss: 0.14277668297290802
train_iter_loss: 0.10289214551448822
train_iter_loss: 0.1289513260126114
train_iter_loss: 0.1712314486503601
train_iter_loss: 0.12958811223506927
train_iter_loss: 0.172069251537323
train_iter_loss: 0.20351235568523407
train_iter_loss: 0.14144960045814514
train_iter_loss: 0.09573163837194443
train loss :0.1578
---------------------
Validation seg loss: 0.2207129190342044 at epoch 815
epoch =    816/  1000, exp = train
train_iter_loss: 0.11435755342245102
train_iter_loss: 0.2662603557109833
train_iter_loss: 0.16618773341178894
train_iter_loss: 0.131755068898201
train_iter_loss: 0.25023776292800903
train_iter_loss: 0.23639114201068878
train_iter_loss: 0.04432949423789978
train_iter_loss: 0.06623835116624832
train_iter_loss: 0.2235836535692215
train_iter_loss: 0.16888052225112915
train_iter_loss: 0.08789310604333878
train_iter_loss: 0.15234576165676117
train_iter_loss: 0.2865949273109436
train_iter_loss: 0.18517927825450897
train_iter_loss: 0.05974189564585686
train_iter_loss: 0.12593738734722137
train_iter_loss: 0.14436717331409454
train_iter_loss: 0.194380983710289
train_iter_loss: 0.1018514409661293
train_iter_loss: 0.17076559364795685
train_iter_loss: 0.10773857682943344
train_iter_loss: 0.10521116852760315
train_iter_loss: 0.1682775467634201
train_iter_loss: 0.130877286195755
train_iter_loss: 0.1852380335330963
train_iter_loss: 0.20420733094215393
train_iter_loss: 0.1987650841474533
train_iter_loss: 0.1673063039779663
train_iter_loss: 0.10416853427886963
train_iter_loss: 0.18200956284999847
train_iter_loss: 0.07263528555631638
train_iter_loss: 0.10969116538763046
train_iter_loss: 0.24845781922340393
train_iter_loss: 0.04867251589894295
train_iter_loss: 0.14776025712490082
train_iter_loss: 0.10086609423160553
train_iter_loss: 0.148463174700737
train_iter_loss: 0.2797650992870331
train_iter_loss: 0.11878672987222672
train_iter_loss: 0.11413343995809555
train_iter_loss: 0.12019850313663483
train_iter_loss: 0.24689459800720215
train_iter_loss: 0.10761504620313644
train_iter_loss: 0.10971991717815399
train_iter_loss: 0.20776283740997314
train_iter_loss: 0.14401748776435852
train_iter_loss: 0.1694502830505371
train_iter_loss: 0.2094530314207077
train_iter_loss: 0.1244904026389122
train_iter_loss: 0.20903152227401733
train_iter_loss: 0.15086349844932556
train_iter_loss: 0.13544638454914093
train_iter_loss: 0.28539523482322693
train_iter_loss: 0.28918105363845825
train_iter_loss: 0.059320833534002304
train_iter_loss: 0.1774986982345581
train_iter_loss: 0.11862067133188248
train_iter_loss: 0.26970404386520386
train_iter_loss: 0.07308127731084824
train_iter_loss: 0.19679205119609833
train_iter_loss: 0.14961624145507812
train_iter_loss: 0.06990116089582443
train_iter_loss: 0.13046547770500183
train_iter_loss: 0.1173754557967186
train_iter_loss: 0.1124739795923233
train_iter_loss: 0.21932312846183777
train_iter_loss: 0.09292648732662201
train_iter_loss: 0.09646638482809067
train_iter_loss: 0.125661239027977
train_iter_loss: 0.16690324246883392
train_iter_loss: 0.15229497849941254
train_iter_loss: 0.09306798875331879
train_iter_loss: 0.12657950818538666
train_iter_loss: 0.19546052813529968
train_iter_loss: 0.13945609331130981
train_iter_loss: 0.17737628519535065
train_iter_loss: 0.11279004067182541
train_iter_loss: 0.12446239590644836
train_iter_loss: 0.25113898515701294
train_iter_loss: 0.24161319434642792
train_iter_loss: 0.1948322355747223
train_iter_loss: 0.1641562581062317
train_iter_loss: 0.09117689728736877
train_iter_loss: 0.10755439102649689
train_iter_loss: 0.12387270480394363
train_iter_loss: 0.0956641212105751
train_iter_loss: 0.2709732949733734
train_iter_loss: 0.11643172055482864
train_iter_loss: 0.10645997524261475
train_iter_loss: 0.279654324054718
train_iter_loss: 0.10815559327602386
train_iter_loss: 0.1273498684167862
train_iter_loss: 0.10782206058502197
train_iter_loss: 0.1567697823047638
train_iter_loss: 0.20840169489383698
train_iter_loss: 0.19415785372257233
train_iter_loss: 0.17757417261600494
train_iter_loss: 0.08948279917240143
train_iter_loss: 0.17058053612709045
train_iter_loss: 0.19713294506072998
train loss :0.1567
---------------------
Validation seg loss: 0.2144463996069049 at epoch 816
epoch =    817/  1000, exp = train
train_iter_loss: 0.08819495141506195
train_iter_loss: 0.18478266894817352
train_iter_loss: 0.04052455723285675
train_iter_loss: 0.18181169033050537
train_iter_loss: 0.11396082490682602
train_iter_loss: 0.12324358522891998
train_iter_loss: 0.19567357003688812
train_iter_loss: 0.2903333902359009
train_iter_loss: 0.07400689274072647
train_iter_loss: 0.13422101736068726
train_iter_loss: 0.1568981558084488
train_iter_loss: 0.1625225841999054
train_iter_loss: 0.1694880723953247
train_iter_loss: 0.13435469567775726
train_iter_loss: 0.14640554785728455
train_iter_loss: 0.15205591917037964
train_iter_loss: 0.16500811278820038
train_iter_loss: 0.19485285878181458
train_iter_loss: 0.16905537247657776
train_iter_loss: 0.0999927893280983
train_iter_loss: 0.14505331218242645
train_iter_loss: 0.11889977008104324
train_iter_loss: 0.14247287809848785
train_iter_loss: 0.16203665733337402
train_iter_loss: 0.060582857578992844
train_iter_loss: 0.177577406167984
train_iter_loss: 0.10842794924974442
train_iter_loss: 0.2060660719871521
train_iter_loss: 0.10244951397180557
train_iter_loss: 0.1486961543560028
train_iter_loss: 0.14627601206302643
train_iter_loss: 0.14836937189102173
train_iter_loss: 0.21610724925994873
train_iter_loss: 0.1721699833869934
train_iter_loss: 0.23873721063137054
train_iter_loss: 0.17969605326652527
train_iter_loss: 0.06217946112155914
train_iter_loss: 0.14064334332942963
train_iter_loss: 0.2242397516965866
train_iter_loss: 0.16603417694568634
train_iter_loss: 0.1745147705078125
train_iter_loss: 0.0869998037815094
train_iter_loss: 0.1768629550933838
train_iter_loss: 0.13404357433319092
train_iter_loss: 0.18024247884750366
train_iter_loss: 0.16205886006355286
train_iter_loss: 0.2061532735824585
train_iter_loss: 0.20097286999225616
train_iter_loss: 0.12592770159244537
train_iter_loss: 0.15764883160591125
train_iter_loss: 0.12731772661209106
train_iter_loss: 0.1302175372838974
train_iter_loss: 0.16275307536125183
train_iter_loss: 0.10739845782518387
train_iter_loss: 0.1903001219034195
train_iter_loss: 0.23427288234233856
train_iter_loss: 0.2668186128139496
train_iter_loss: 0.1643676608800888
train_iter_loss: 0.21389341354370117
train_iter_loss: 0.17334607243537903
train_iter_loss: 0.10816816985607147
train_iter_loss: 0.19293035566806793
train_iter_loss: 0.1468544602394104
train_iter_loss: 0.0894751325249672
train_iter_loss: 0.20149856805801392
train_iter_loss: 0.17898902297019958
train_iter_loss: 0.09993840754032135
train_iter_loss: 0.07226840406656265
train_iter_loss: 0.18355730175971985
train_iter_loss: 0.2839542627334595
train_iter_loss: 0.13464444875717163
train_iter_loss: 0.16837045550346375
train_iter_loss: 0.2683815062046051
train_iter_loss: 0.10443112999200821
train_iter_loss: 0.11334028095006943
train_iter_loss: 0.17256595194339752
train_iter_loss: 0.24156050384044647
train_iter_loss: 0.25206711888313293
train_iter_loss: 0.13268978893756866
train_iter_loss: 0.17722606658935547
train_iter_loss: 0.10798946022987366
train_iter_loss: 0.08199774473905563
train_iter_loss: 0.1420264095067978
train_iter_loss: 0.1952618807554245
train_iter_loss: 0.2074214518070221
train_iter_loss: 0.11806096881628036
train_iter_loss: 0.1469794511795044
train_iter_loss: 0.15359750390052795
train_iter_loss: 0.23176607489585876
train_iter_loss: 0.09755246341228485
train_iter_loss: 0.16026020050048828
train_iter_loss: 0.2110159993171692
train_iter_loss: 0.10369211435317993
train_iter_loss: 0.13718192279338837
train_iter_loss: 0.1333659291267395
train_iter_loss: 0.07865449786186218
train_iter_loss: 0.1394207626581192
train_iter_loss: 0.07429733127355576
train_iter_loss: 0.23166967928409576
train_iter_loss: 0.15844941139221191
train loss :0.1581
---------------------
Validation seg loss: 0.21324909239445092 at epoch 817
epoch =    818/  1000, exp = train
train_iter_loss: 0.21294374763965607
train_iter_loss: 0.21266596019268036
train_iter_loss: 0.24828609824180603
train_iter_loss: 0.15929517149925232
train_iter_loss: 0.14751125872135162
train_iter_loss: 0.28073808550834656
train_iter_loss: 0.20017488300800323
train_iter_loss: 0.1327146291732788
train_iter_loss: 0.07215488702058792
train_iter_loss: 0.2209196835756302
train_iter_loss: 0.05914149060845375
train_iter_loss: 0.23320643603801727
train_iter_loss: 0.251780241727829
train_iter_loss: 0.14945435523986816
train_iter_loss: 0.17021971940994263
train_iter_loss: 0.2212631106376648
train_iter_loss: 0.1902301162481308
train_iter_loss: 0.08975814282894135
train_iter_loss: 0.15847955644130707
train_iter_loss: 0.05976760387420654
train_iter_loss: 0.12153390794992447
train_iter_loss: 0.03018086589872837
train_iter_loss: 0.1715550273656845
train_iter_loss: 0.09429721534252167
train_iter_loss: 0.10604061931371689
train_iter_loss: 0.16511018574237823
train_iter_loss: 0.20055440068244934
train_iter_loss: 0.19360239803791046
train_iter_loss: 0.19777347147464752
train_iter_loss: 0.13772296905517578
train_iter_loss: 0.13199138641357422
train_iter_loss: 0.1383223831653595
train_iter_loss: 0.10764662176370621
train_iter_loss: 0.22900515794754028
train_iter_loss: 0.12622883915901184
train_iter_loss: 0.10100346803665161
train_iter_loss: 0.155970498919487
train_iter_loss: 0.21750983595848083
train_iter_loss: 0.11083252727985382
train_iter_loss: 0.13333363831043243
train_iter_loss: 0.16728703677654266
train_iter_loss: 0.23037569224834442
train_iter_loss: 0.2543407082557678
train_iter_loss: 0.06531013548374176
train_iter_loss: 0.12002149969339371
train_iter_loss: 0.268555611371994
train_iter_loss: 0.20068900287151337
train_iter_loss: 0.12773464620113373
train_iter_loss: 0.184846892952919
train_iter_loss: 0.09944550693035126
train_iter_loss: 0.13495582342147827
train_iter_loss: 0.183059424161911
train_iter_loss: 0.18022526800632477
train_iter_loss: 0.07341042906045914
train_iter_loss: 0.16070586442947388
train_iter_loss: 0.1757804900407791
train_iter_loss: 0.08894408494234085
train_iter_loss: 0.25655195116996765
train_iter_loss: 0.14942243695259094
train_iter_loss: 0.22018562257289886
train_iter_loss: 0.11323054134845734
train_iter_loss: 0.11766944825649261
train_iter_loss: 0.13959644734859467
train_iter_loss: 0.26746970415115356
train_iter_loss: 0.11477723717689514
train_iter_loss: 0.34620630741119385
train_iter_loss: 0.24312523007392883
train_iter_loss: 0.12061373144388199
train_iter_loss: 0.1978117674589157
train_iter_loss: 0.20486029982566833
train_iter_loss: 0.17310184240341187
train_iter_loss: 0.0948464646935463
train_iter_loss: 0.10742560774087906
train_iter_loss: 0.0595422200858593
train_iter_loss: 0.21238328516483307
train_iter_loss: 0.21403111517429352
train_iter_loss: 0.09233038127422333
train_iter_loss: 0.19303719699382782
train_iter_loss: 0.09198230504989624
train_iter_loss: 0.13959433138370514
train_iter_loss: 0.12029831856489182
train_iter_loss: 0.154077410697937
train_iter_loss: 0.15059830248355865
train_iter_loss: 0.40137264132499695
train_iter_loss: 0.149419903755188
train_iter_loss: 0.2646799385547638
train_iter_loss: 0.07429693639278412
train_iter_loss: 0.2689365744590759
train_iter_loss: 0.11595922708511353
train_iter_loss: 0.10214175283908844
train_iter_loss: 0.07266122102737427
train_iter_loss: 0.14102104306221008
train_iter_loss: 0.12183637917041779
train_iter_loss: 0.12184328585863113
train_iter_loss: 0.13554497063159943
train_iter_loss: 0.1638416349887848
train_iter_loss: 0.18672239780426025
train_iter_loss: 0.17100264132022858
train_iter_loss: 0.2526555061340332
train_iter_loss: 0.04738717898726463
train loss :0.1630
---------------------
Validation seg loss: 0.2207167275109381 at epoch 818
epoch =    819/  1000, exp = train
train_iter_loss: 0.09247229248285294
train_iter_loss: 0.09466732293367386
train_iter_loss: 0.11118591576814651
train_iter_loss: 0.11261767894029617
train_iter_loss: 0.12540023028850555
train_iter_loss: 0.21308733522891998
train_iter_loss: 0.11512933671474457
train_iter_loss: 0.14141546189785004
train_iter_loss: 0.12674494087696075
train_iter_loss: 0.08417364209890366
train_iter_loss: 0.1769351214170456
train_iter_loss: 0.11853931844234467
train_iter_loss: 0.2524014115333557
train_iter_loss: 0.1669614166021347
train_iter_loss: 0.26640820503234863
train_iter_loss: 0.0643082931637764
train_iter_loss: 0.3317375183105469
train_iter_loss: 0.33213019371032715
train_iter_loss: 0.1963215172290802
train_iter_loss: 0.20836900174617767
train_iter_loss: 0.1190173327922821
train_iter_loss: 0.15563751757144928
train_iter_loss: 0.12477823346853256
train_iter_loss: 0.14629124104976654
train_iter_loss: 0.07927402853965759
train_iter_loss: 0.15634027123451233
train_iter_loss: 0.19843937456607819
train_iter_loss: 0.18662448227405548
train_iter_loss: 0.16164159774780273
train_iter_loss: 0.18453386425971985
train_iter_loss: 0.11514262109994888
train_iter_loss: 0.17447873950004578
train_iter_loss: 0.12371327728033066
train_iter_loss: 0.10241302102804184
train_iter_loss: 0.1681099534034729
train_iter_loss: 0.11129055172204971
train_iter_loss: 0.20599637925624847
train_iter_loss: 0.1807226836681366
train_iter_loss: 0.16641126573085785
train_iter_loss: 0.17924435436725616
train_iter_loss: 0.13803404569625854
train_iter_loss: 0.13553911447525024
train_iter_loss: 0.2259453386068344
train_iter_loss: 0.14026480913162231
train_iter_loss: 0.27901574969291687
train_iter_loss: 0.09264809638261795
train_iter_loss: 0.1498958319425583
train_iter_loss: 0.2951154410839081
train_iter_loss: 0.11966598033905029
train_iter_loss: 0.1879619061946869
train_iter_loss: 0.038424935191869736
train_iter_loss: 0.17729632556438446
train_iter_loss: 0.108442023396492
train_iter_loss: 0.14955639839172363
train_iter_loss: 0.207393616437912
train_iter_loss: 0.1662285476922989
train_iter_loss: 0.1437380015850067
train_iter_loss: 0.15400926768779755
train_iter_loss: 0.13405992090702057
train_iter_loss: 0.1966966837644577
train_iter_loss: 0.274126797914505
train_iter_loss: 0.25272461771965027
train_iter_loss: 0.12617257237434387
train_iter_loss: 0.13910892605781555
train_iter_loss: 0.08414988219738007
train_iter_loss: 0.24063937366008759
train_iter_loss: 0.2577371299266815
train_iter_loss: 0.1101607158780098
train_iter_loss: 0.14489534497261047
train_iter_loss: 0.08592256158590317
train_iter_loss: 0.12076685577630997
train_iter_loss: 0.1017809733748436
train_iter_loss: 0.2020053118467331
train_iter_loss: 0.16764695942401886
train_iter_loss: 0.06768045574426651
train_iter_loss: 0.07589160650968552
train_iter_loss: 0.1709229201078415
train_iter_loss: 0.25979170203208923
train_iter_loss: 0.08479954302310944
train_iter_loss: 0.12162688374519348
train_iter_loss: 0.11820551753044128
train_iter_loss: 0.06877847760915756
train_iter_loss: 0.07667192071676254
train_iter_loss: 0.16027437150478363
train_iter_loss: 0.10770701617002487
train_iter_loss: 0.08496192842721939
train_iter_loss: 0.13937844336032867
train_iter_loss: 0.14470921456813812
train_iter_loss: 0.2038411945104599
train_iter_loss: 0.2078884094953537
train_iter_loss: 0.17784185707569122
train_iter_loss: 0.15427778661251068
train_iter_loss: 0.26459893584251404
train_iter_loss: 0.020516490563750267
train_iter_loss: 0.12595510482788086
train_iter_loss: 0.23239079117774963
train_iter_loss: 0.11448667198419571
train_iter_loss: 0.14521656930446625
train_iter_loss: 0.13111603260040283
train_iter_loss: 0.19423726201057434
train loss :0.1574
---------------------
Validation seg loss: 0.21446961351617608 at epoch 819
epoch =    820/  1000, exp = train
train_iter_loss: 0.0706586018204689
train_iter_loss: 0.1401030421257019
train_iter_loss: 0.1453063189983368
train_iter_loss: 0.0560774952173233
train_iter_loss: 0.02666468359529972
train_iter_loss: 0.2042408436536789
train_iter_loss: 0.27937668561935425
train_iter_loss: 0.20040187239646912
train_iter_loss: 0.18826128542423248
train_iter_loss: 0.1412658542394638
train_iter_loss: 0.17993618547916412
train_iter_loss: 0.12113750725984573
train_iter_loss: 0.1829521209001541
train_iter_loss: 0.0956903025507927
train_iter_loss: 0.07524572312831879
train_iter_loss: 0.0891270861029625
train_iter_loss: 0.2549254894256592
train_iter_loss: 0.2290440797805786
train_iter_loss: 0.2830103635787964
train_iter_loss: 0.10616723448038101
train_iter_loss: 0.20405323803424835
train_iter_loss: 0.07121255993843079
train_iter_loss: 0.15754462778568268
train_iter_loss: 0.05851943418383598
train_iter_loss: 0.060680776834487915
train_iter_loss: 0.11470948159694672
train_iter_loss: 0.16160722076892853
train_iter_loss: 0.23990969359874725
train_iter_loss: 0.13042400777339935
train_iter_loss: 0.09727279096841812
train_iter_loss: 0.17422987520694733
train_iter_loss: 0.212472066283226
train_iter_loss: 0.11363859474658966
train_iter_loss: 0.1101081445813179
train_iter_loss: 0.15701162815093994
train_iter_loss: 0.25567197799682617
train_iter_loss: 0.17778266966342926
train_iter_loss: 0.07110864669084549
train_iter_loss: 0.17875851690769196
train_iter_loss: 0.10856103897094727
train_iter_loss: 0.10752978175878525
train_iter_loss: 0.173831969499588
train_iter_loss: 0.08529172837734222
train_iter_loss: 0.16773125529289246
train_iter_loss: 0.17920567095279694
train_iter_loss: 0.11773207038640976
train_iter_loss: 0.20149296522140503
train_iter_loss: 0.14290428161621094
train_iter_loss: 0.12745298445224762
train_iter_loss: 0.14245176315307617
train_iter_loss: 0.17673170566558838
train_iter_loss: 0.22069525718688965
train_iter_loss: 0.035485539585351944
train_iter_loss: 0.19039681553840637
train_iter_loss: 0.16543884575366974
train_iter_loss: 0.1340555101633072
train_iter_loss: 0.07707962393760681
train_iter_loss: 0.18786010146141052
train_iter_loss: 0.29002249240875244
train_iter_loss: 0.17810645699501038
train_iter_loss: 0.22404006123542786
train_iter_loss: 0.22106793522834778
train_iter_loss: 0.14779792726039886
train_iter_loss: 0.16525129973888397
train_iter_loss: 0.2743188738822937
train_iter_loss: 0.20177416503429413
train_iter_loss: 0.11737879365682602
train_iter_loss: 0.15500769019126892
train_iter_loss: 0.1789989322423935
train_iter_loss: 0.14707422256469727
train_iter_loss: 0.12561163306236267
train_iter_loss: 0.2203644961118698
train_iter_loss: 0.13889920711517334
train_iter_loss: 0.1544644683599472
train_iter_loss: 0.17230048775672913
train_iter_loss: 0.16832834482192993
train_iter_loss: 0.2223219871520996
train_iter_loss: 0.12360099703073502
train_iter_loss: 0.12173699587583542
train_iter_loss: 0.16141104698181152
train_iter_loss: 0.13680949807167053
train_iter_loss: 0.23275402188301086
train_iter_loss: 0.2094273567199707
train_iter_loss: 0.08631905913352966
train_iter_loss: 0.1357511281967163
train_iter_loss: 0.20516172051429749
train_iter_loss: 0.041911788284778595
train_iter_loss: 0.17621761560440063
train_iter_loss: 0.1889232099056244
train_iter_loss: 0.2779172360897064
train_iter_loss: 0.13983088731765747
train_iter_loss: 0.11259577423334122
train_iter_loss: 0.14707987010478973
train_iter_loss: 0.11816155165433884
train_iter_loss: 0.04930438473820686
train_iter_loss: 0.12077242136001587
train_iter_loss: 0.15449149906635284
train_iter_loss: 0.1406809389591217
train_iter_loss: 0.22048529982566833
train_iter_loss: 0.06915242224931717
train loss :0.1559
---------------------
Validation seg loss: 0.22713483975462193 at epoch 820
epoch =    821/  1000, exp = train
train_iter_loss: 0.23180446028709412
train_iter_loss: 0.03009066917002201
train_iter_loss: 0.290496289730072
train_iter_loss: 0.11168532073497772
train_iter_loss: 0.17549920082092285
train_iter_loss: 0.06449904292821884
train_iter_loss: 0.14040108025074005
train_iter_loss: 0.1882110983133316
train_iter_loss: 0.10294169187545776
train_iter_loss: 0.12670600414276123
train_iter_loss: 0.16542035341262817
train_iter_loss: 0.21405504643917084
train_iter_loss: 0.1028904840350151
train_iter_loss: 0.11200837790966034
train_iter_loss: 0.1988881230354309
train_iter_loss: 0.06688641011714935
train_iter_loss: 0.2744092643260956
train_iter_loss: 0.2148488610982895
train_iter_loss: 0.046849966049194336
train_iter_loss: 0.1442268192768097
train_iter_loss: 0.08027053624391556
train_iter_loss: 0.0771251693367958
train_iter_loss: 0.06588060408830643
train_iter_loss: 0.13437150418758392
train_iter_loss: 0.17741058766841888
train_iter_loss: 0.192825585603714
train_iter_loss: 0.1971806287765503
train_iter_loss: 0.1050921306014061
train_iter_loss: 0.15176910161972046
train_iter_loss: 0.14746509492397308
train_iter_loss: 0.14494474232196808
train_iter_loss: 0.16688168048858643
train_iter_loss: 0.2787497341632843
train_iter_loss: 0.06969644874334335
train_iter_loss: 0.1654571145772934
train_iter_loss: 0.0665658637881279
train_iter_loss: 0.16090911626815796
train_iter_loss: 0.18814504146575928
train_iter_loss: 0.15776441991329193
train_iter_loss: 0.22368262708187103
train_iter_loss: 0.21549338102340698
train_iter_loss: 0.10254291445016861
train_iter_loss: 0.28849926590919495
train_iter_loss: 0.2272038608789444
train_iter_loss: 0.15884935855865479
train_iter_loss: 0.1046672835946083
train_iter_loss: 0.2055150866508484
train_iter_loss: 0.09637245535850525
train_iter_loss: 0.1400221884250641
train_iter_loss: 0.31225037574768066
train_iter_loss: 0.13961102068424225
train_iter_loss: 0.11111640930175781
train_iter_loss: 0.10098537802696228
train_iter_loss: 0.16382341086864471
train_iter_loss: 0.20472949743270874
train_iter_loss: 0.09221398830413818
train_iter_loss: 0.10067987442016602
train_iter_loss: 0.09960892796516418
train_iter_loss: 0.15306957066059113
train_iter_loss: 0.1926216334104538
train_iter_loss: 0.1528014838695526
train_iter_loss: 0.10989876091480255
train_iter_loss: 0.21663065254688263
train_iter_loss: 0.08598243445158005
train_iter_loss: 0.17298699915409088
train_iter_loss: 0.07527150213718414
train_iter_loss: 0.09529481083154678
train_iter_loss: 0.2631170153617859
train_iter_loss: 0.1274581104516983
train_iter_loss: 0.14073529839515686
train_iter_loss: 0.18325595557689667
train_iter_loss: 0.21847432851791382
train_iter_loss: 0.23189420998096466
train_iter_loss: 0.1593346893787384
train_iter_loss: 0.16010791063308716
train_iter_loss: 0.21411092579364777
train_iter_loss: 0.2295476198196411
train_iter_loss: 0.2378329485654831
train_iter_loss: 0.13536106050014496
train_iter_loss: 0.17444486916065216
train_iter_loss: 0.19777266681194305
train_iter_loss: 0.13417106866836548
train_iter_loss: 0.0833919495344162
train_iter_loss: 0.1283997744321823
train_iter_loss: 0.13467101752758026
train_iter_loss: 0.1358317881822586
train_iter_loss: 0.02833806350827217
train_iter_loss: 0.07732808589935303
train_iter_loss: 0.18887992203235626
train_iter_loss: 0.22215759754180908
train_iter_loss: 0.24838665127754211
train_iter_loss: 0.09851548820734024
train_iter_loss: 0.1614161729812622
train_iter_loss: 0.16190063953399658
train_iter_loss: 0.10360751301050186
train_iter_loss: 0.1905587911605835
train_iter_loss: 0.1668071746826172
train_iter_loss: 0.07040458172559738
train_iter_loss: 0.14514729380607605
train_iter_loss: 0.07495725899934769
train loss :0.1546
---------------------
Validation seg loss: 0.21652336110237916 at epoch 821
epoch =    822/  1000, exp = train
train_iter_loss: 0.17678895592689514
train_iter_loss: 0.1499931812286377
train_iter_loss: 0.09630171954631805
train_iter_loss: 0.14275497198104858
train_iter_loss: 0.18270017206668854
train_iter_loss: 0.16428667306900024
train_iter_loss: 0.10393721610307693
train_iter_loss: 0.16223867237567902
train_iter_loss: 0.1477738916873932
train_iter_loss: 0.14517588913440704
train_iter_loss: 0.1370643526315689
train_iter_loss: 0.2719632685184479
train_iter_loss: 0.15177679061889648
train_iter_loss: 0.13832218945026398
train_iter_loss: 0.16133154928684235
train_iter_loss: 0.11875195056200027
train_iter_loss: 0.0886974111199379
train_iter_loss: 0.22468917071819305
train_iter_loss: 0.19355754554271698
train_iter_loss: 0.06678128242492676
train_iter_loss: 0.2023429274559021
train_iter_loss: 0.2017378807067871
train_iter_loss: 0.11404028534889221
train_iter_loss: 0.14554697275161743
train_iter_loss: 0.18383702635765076
train_iter_loss: 0.20505325496196747
train_iter_loss: 0.172816202044487
train_iter_loss: 0.15254822373390198
train_iter_loss: 0.2256702333688736
train_iter_loss: 0.17804525792598724
train_iter_loss: 0.07244787365198135
train_iter_loss: 0.1252015382051468
train_iter_loss: 0.03675461933016777
train_iter_loss: 0.3925698399543762
train_iter_loss: 0.13389021158218384
train_iter_loss: 0.20492038130760193
train_iter_loss: 0.08479105681180954
train_iter_loss: 0.15067027509212494
train_iter_loss: 0.22326934337615967
train_iter_loss: 0.1593618243932724
train_iter_loss: 0.15363171696662903
train_iter_loss: 0.10810770839452744
train_iter_loss: 0.12166429311037064
train_iter_loss: 0.08786895871162415
train_iter_loss: 0.19560369849205017
train_iter_loss: 0.07993483543395996
train_iter_loss: 0.08466269820928574
train_iter_loss: 0.08177386969327927
train_iter_loss: 0.2709411382675171
train_iter_loss: 0.06648421287536621
train_iter_loss: 0.1443624347448349
train_iter_loss: 0.20906227827072144
train_iter_loss: 0.2001878023147583
train_iter_loss: 0.08280610293149948
train_iter_loss: 0.19334585964679718
train_iter_loss: 0.1474601775407791
train_iter_loss: 0.11318548768758774
train_iter_loss: 0.25427550077438354
train_iter_loss: 0.2774314880371094
train_iter_loss: 0.20747004449367523
train_iter_loss: 0.15580573678016663
train_iter_loss: 0.19015836715698242
train_iter_loss: 0.09391289949417114
train_iter_loss: 0.35668182373046875
train_iter_loss: 0.1603059023618698
train_iter_loss: 0.1549776941537857
train_iter_loss: 0.17866235971450806
train_iter_loss: 0.09895163774490356
train_iter_loss: 0.10733328014612198
train_iter_loss: 0.0696718767285347
train_iter_loss: 0.12326283007860184
train_iter_loss: 0.182023286819458
train_iter_loss: 0.14526498317718506
train_iter_loss: 0.1499001383781433
train_iter_loss: 0.15630173683166504
train_iter_loss: 0.10071815550327301
train_iter_loss: 0.15954729914665222
train_iter_loss: 0.14902746677398682
train_iter_loss: 0.5213075876235962
train_iter_loss: 0.3683323562145233
train_iter_loss: 0.09733996540307999
train_iter_loss: 0.10305158793926239
train_iter_loss: 0.056192249059677124
train_iter_loss: 0.10296954959630966
train_iter_loss: 0.3715059459209442
train_iter_loss: 0.13748539984226227
train_iter_loss: 0.07234333455562592
train_iter_loss: 0.2241816520690918
train_iter_loss: 0.17685438692569733
train_iter_loss: 0.09245317429304123
train_iter_loss: 0.17721405625343323
train_iter_loss: 0.10318665951490402
train_iter_loss: 0.27637779712677
train_iter_loss: 0.09618887305259705
train_iter_loss: 0.13768352568149567
train_iter_loss: 0.14988838136196136
train_iter_loss: 0.0492246076464653
train_iter_loss: 0.18187493085861206
train_iter_loss: 0.21313852071762085
train_iter_loss: 0.1070437878370285
train loss :0.1617
---------------------
Validation seg loss: 0.21588959224205814 at epoch 822
epoch =    823/  1000, exp = train
train_iter_loss: 0.12059733271598816
train_iter_loss: 0.13077718019485474
train_iter_loss: 0.22542303800582886
train_iter_loss: 0.18935038149356842
train_iter_loss: 0.310830682516098
train_iter_loss: 0.10246744006872177
train_iter_loss: 0.11084878444671631
train_iter_loss: 0.07735736668109894
train_iter_loss: 0.18984746932983398
train_iter_loss: 0.16793780028820038
train_iter_loss: 0.27062973380088806
train_iter_loss: 0.05955547094345093
train_iter_loss: 0.2903895676136017
train_iter_loss: 0.09090103954076767
train_iter_loss: 0.16812624037265778
train_iter_loss: 0.11093821376562119
train_iter_loss: 0.18769149482250214
train_iter_loss: 0.23094817996025085
train_iter_loss: 0.14286963641643524
train_iter_loss: 0.1596217304468155
train_iter_loss: 0.1465732604265213
train_iter_loss: 0.16620470583438873
train_iter_loss: 0.09855791926383972
train_iter_loss: 0.2042991816997528
train_iter_loss: 0.04124460741877556
train_iter_loss: 0.07980019599199295
train_iter_loss: 0.07411643117666245
train_iter_loss: 0.1907697319984436
train_iter_loss: 0.04987979680299759
train_iter_loss: 0.021494554355740547
train_iter_loss: 0.14540842175483704
train_iter_loss: 0.059688374400138855
train_iter_loss: 0.23836620151996613
train_iter_loss: 0.14703354239463806
train_iter_loss: 0.23429778218269348
train_iter_loss: 0.12344702333211899
train_iter_loss: 0.31433284282684326
train_iter_loss: 0.16717511415481567
train_iter_loss: 0.1482997089624405
train_iter_loss: 0.23829962313175201
train_iter_loss: 0.15138231217861176
train_iter_loss: 0.21839725971221924
train_iter_loss: 0.18560558557510376
train_iter_loss: 0.26751992106437683
train_iter_loss: 0.17658275365829468
train_iter_loss: 0.18249186873435974
train_iter_loss: 0.12558497488498688
train_iter_loss: 0.18329888582229614
train_iter_loss: 0.1462448686361313
train_iter_loss: 0.3184731602668762
train_iter_loss: 0.08988501131534576
train_iter_loss: 0.23602256178855896
train_iter_loss: 0.15420444309711456
train_iter_loss: 0.07836351543664932
train_iter_loss: 0.15562619268894196
train_iter_loss: 0.18848130106925964
train_iter_loss: 0.07399997115135193
train_iter_loss: 0.0955149456858635
train_iter_loss: 0.0912349596619606
train_iter_loss: 0.12867632508277893
train_iter_loss: 0.08595401793718338
train_iter_loss: 0.07607630640268326
train_iter_loss: 0.09183662384748459
train_iter_loss: 0.0657692477107048
train_iter_loss: 0.06661514192819595
train_iter_loss: 0.27012568712234497
train_iter_loss: 0.1481056809425354
train_iter_loss: 0.1124231368303299
train_iter_loss: 0.12322068959474564
train_iter_loss: 0.10623260587453842
train_iter_loss: 0.1980164647102356
train_iter_loss: 0.3205340802669525
train_iter_loss: 0.24290916323661804
train_iter_loss: 0.051831990480422974
train_iter_loss: 0.10626328736543655
train_iter_loss: 0.13303051888942719
train_iter_loss: 0.09023232012987137
train_iter_loss: 0.1980428546667099
train_iter_loss: 0.14986330270767212
train_iter_loss: 0.1068507507443428
train_iter_loss: 0.14486263692378998
train_iter_loss: 0.17900335788726807
train_iter_loss: 0.1519341766834259
train_iter_loss: 0.12686215341091156
train_iter_loss: 0.17878659069538116
train_iter_loss: 0.1398962438106537
train_iter_loss: 0.25538507103919983
train_iter_loss: 0.09333735704421997
train_iter_loss: 0.2514919936656952
train_iter_loss: 0.11626636236906052
train_iter_loss: 0.1812056005001068
train_iter_loss: 0.0784665122628212
train_iter_loss: 0.18969742953777313
train_iter_loss: 0.18108819425106049
train_iter_loss: 0.18787804245948792
train_iter_loss: 0.11817734688520432
train_iter_loss: 0.07809197902679443
train_iter_loss: 0.16243569552898407
train_iter_loss: 0.04730139300227165
train_iter_loss: 0.22979716956615448
train loss :0.1547
---------------------
Validation seg loss: 0.2141597367260816 at epoch 823
epoch =    824/  1000, exp = train
train_iter_loss: 0.104995496571064
train_iter_loss: 0.14939093589782715
train_iter_loss: 0.1886259764432907
train_iter_loss: 0.11341120302677155
train_iter_loss: 0.11135288327932358
train_iter_loss: 0.191873699426651
train_iter_loss: 0.20723362267017365
train_iter_loss: 0.11138264089822769
train_iter_loss: 0.11618936061859131
train_iter_loss: 0.05680094286799431
train_iter_loss: 0.10992833226919174
train_iter_loss: 0.17508426308631897
train_iter_loss: 0.12410961091518402
train_iter_loss: 0.07857511192560196
train_iter_loss: 0.06846482306718826
train_iter_loss: 0.13706877827644348
train_iter_loss: 0.16239050030708313
train_iter_loss: 0.0941953957080841
train_iter_loss: 0.2312404066324234
train_iter_loss: 0.15355464816093445
train_iter_loss: 0.17128236591815948
train_iter_loss: 0.17821303009986877
train_iter_loss: 0.1479087471961975
train_iter_loss: 0.1874578446149826
train_iter_loss: 0.16848818957805634
train_iter_loss: 0.03340651094913483
train_iter_loss: 0.1626282036304474
train_iter_loss: 0.3090929687023163
train_iter_loss: 0.19521205127239227
train_iter_loss: 0.10685668140649796
train_iter_loss: 0.1397881656885147
train_iter_loss: 0.13398507237434387
train_iter_loss: 0.15378013253211975
train_iter_loss: 0.10226615518331528
train_iter_loss: 0.142556831240654
train_iter_loss: 0.18096084892749786
train_iter_loss: 0.16496694087982178
train_iter_loss: 0.07131721824407578
train_iter_loss: 0.09551356732845306
train_iter_loss: 0.05464966967701912
train_iter_loss: 0.26795679330825806
train_iter_loss: 0.12308771163225174
train_iter_loss: 0.1934012621641159
train_iter_loss: 0.13524100184440613
train_iter_loss: 0.1203448697924614
train_iter_loss: 0.1337101310491562
train_iter_loss: 0.16482746601104736
train_iter_loss: 0.13344639539718628
train_iter_loss: 0.1617724597454071
train_iter_loss: 0.08457674831151962
train_iter_loss: 0.15564808249473572
train_iter_loss: 0.11020273715257645
train_iter_loss: 0.352889746427536
train_iter_loss: 0.12962572276592255
train_iter_loss: 0.18401533365249634
train_iter_loss: 0.12386325001716614
train_iter_loss: 0.18475857377052307
train_iter_loss: 0.08475740253925323
train_iter_loss: 0.1241031214594841
train_iter_loss: 0.17717023193836212
train_iter_loss: 0.1377728432416916
train_iter_loss: 0.26617810130119324
train_iter_loss: 0.2801991403102875
train_iter_loss: 0.2441464066505432
train_iter_loss: 0.17537187039852142
train_iter_loss: 0.1915281116962433
train_iter_loss: 0.09026647359132767
train_iter_loss: 0.20592676103115082
train_iter_loss: 0.11321017146110535
train_iter_loss: 0.12259280681610107
train_iter_loss: 0.33344465494155884
train_iter_loss: 0.17073172330856323
train_iter_loss: 0.09688249230384827
train_iter_loss: 0.18619059026241302
train_iter_loss: 0.15842191874980927
train_iter_loss: 0.10046599060297012
train_iter_loss: 0.344757080078125
train_iter_loss: 0.15507130324840546
train_iter_loss: 0.11399320513010025
train_iter_loss: 0.23274482786655426
train_iter_loss: 0.21541482210159302
train_iter_loss: 0.13034188747406006
train_iter_loss: 0.14208705723285675
train_iter_loss: 0.124555803835392
train_iter_loss: 0.17989413440227509
train_iter_loss: 0.16404224932193756
train_iter_loss: 0.1688462197780609
train_iter_loss: 0.13832320272922516
train_iter_loss: 0.16950485110282898
train_iter_loss: 0.09441371262073517
train_iter_loss: 0.2373562455177307
train_iter_loss: 0.20640304684638977
train_iter_loss: 0.19281414151191711
train_iter_loss: 0.1961742490530014
train_iter_loss: 0.10905158519744873
train_iter_loss: 0.10421933978796005
train_iter_loss: 0.1556748002767563
train_iter_loss: 0.19257527589797974
train_iter_loss: 0.2093176543712616
train_iter_loss: 0.21907685697078705
train loss :0.1596
---------------------
Validation seg loss: 0.21381846036902577 at epoch 824
epoch =    825/  1000, exp = train
train_iter_loss: 0.215872123837471
train_iter_loss: 0.0648481547832489
train_iter_loss: 0.18163490295410156
train_iter_loss: 0.13673733174800873
train_iter_loss: 0.1552387773990631
train_iter_loss: 0.18454857170581818
train_iter_loss: 0.17916734516620636
train_iter_loss: 0.15615807473659515
train_iter_loss: 0.1910056322813034
train_iter_loss: 0.10252944380044937
train_iter_loss: 0.2488769292831421
train_iter_loss: 0.13736110925674438
train_iter_loss: 0.17299784719944
train_iter_loss: 0.13032500445842743
train_iter_loss: 0.10380537062883377
train_iter_loss: 0.0805133730173111
train_iter_loss: 0.1457982361316681
train_iter_loss: 0.09589658677577972
train_iter_loss: 0.20133733749389648
train_iter_loss: 0.2450045496225357
train_iter_loss: 0.07429095357656479
train_iter_loss: 0.08617539703845978
train_iter_loss: 0.1398911327123642
train_iter_loss: 0.1114518791437149
train_iter_loss: 0.20028461515903473
train_iter_loss: 0.0529104545712471
train_iter_loss: 0.26267698407173157
train_iter_loss: 0.1374640315771103
train_iter_loss: 0.0479285754263401
train_iter_loss: 0.10955142229795456
train_iter_loss: 0.36169755458831787
train_iter_loss: 0.1831439882516861
train_iter_loss: 0.10731489956378937
train_iter_loss: 0.13902752101421356
train_iter_loss: 0.11110427975654602
train_iter_loss: 0.25459611415863037
train_iter_loss: 0.231783926486969
train_iter_loss: 0.23253268003463745
train_iter_loss: 0.20988672971725464
train_iter_loss: 0.13318155705928802
train_iter_loss: 0.22207075357437134
train_iter_loss: 0.15080556273460388
train_iter_loss: 0.11727116256952286
train_iter_loss: 0.09686335921287537
train_iter_loss: 0.16943304240703583
train_iter_loss: 0.1677851676940918
train_iter_loss: 0.060431353747844696
train_iter_loss: 0.07057663798332214
train_iter_loss: 0.09184093773365021
train_iter_loss: 0.07961966842412949
train_iter_loss: 0.0742667019367218
train_iter_loss: 0.1213010922074318
train_iter_loss: 0.15008780360221863
train_iter_loss: 0.08980512619018555
train_iter_loss: 0.17128309607505798
train_iter_loss: 0.13951675593852997
train_iter_loss: 0.187323659658432
train_iter_loss: 0.23514819145202637
train_iter_loss: 0.25092706084251404
train_iter_loss: 0.20716002583503723
train_iter_loss: 0.09336923062801361
train_iter_loss: 0.13084135949611664
train_iter_loss: 0.056101441383361816
train_iter_loss: 0.32461977005004883
train_iter_loss: 0.17856250703334808
train_iter_loss: 0.12106701731681824
train_iter_loss: 0.33927643299102783
train_iter_loss: 0.12734799087047577
train_iter_loss: 0.1440652757883072
train_iter_loss: 0.13411647081375122
train_iter_loss: 0.20323704183101654
train_iter_loss: 0.1850823312997818
train_iter_loss: 0.17760708928108215
train_iter_loss: 0.2320854514837265
train_iter_loss: 0.13107049465179443
train_iter_loss: 0.26749187707901
train_iter_loss: 0.09838022291660309
train_iter_loss: 0.17077578604221344
train_iter_loss: 0.20393496751785278
train_iter_loss: 0.12984561920166016
train_iter_loss: 0.1577002853155136
train_iter_loss: 0.3072187304496765
train_iter_loss: 0.18571867048740387
train_iter_loss: 0.13784419000148773
train_iter_loss: 0.19742749631404877
train_iter_loss: 0.20061711966991425
train_iter_loss: 0.15567812323570251
train_iter_loss: 0.1501225382089615
train_iter_loss: 0.15382039546966553
train_iter_loss: 0.12171552330255508
train_iter_loss: 0.15861596167087555
train_iter_loss: 0.14349369704723358
train_iter_loss: 0.11726031452417374
train_iter_loss: 0.3206787407398224
train_iter_loss: 0.19061943888664246
train_iter_loss: 0.10546610504388809
train_iter_loss: 0.06351363658905029
train_iter_loss: 0.10703261196613312
train_iter_loss: 0.1471405029296875
train_iter_loss: 0.1308312863111496
train loss :0.1603
---------------------
Validation seg loss: 0.21315937028881513 at epoch 825
epoch =    826/  1000, exp = train
train_iter_loss: 0.07081926614046097
train_iter_loss: 0.131021186709404
train_iter_loss: 0.17202046513557434
train_iter_loss: 0.18799132108688354
train_iter_loss: 0.12768390774726868
train_iter_loss: 0.16543273627758026
train_iter_loss: 0.20843498408794403
train_iter_loss: 0.10673970729112625
train_iter_loss: 0.1181882694363594
train_iter_loss: 0.11781271547079086
train_iter_loss: 0.21547260880470276
train_iter_loss: 0.15276367962360382
train_iter_loss: 0.19992703199386597
train_iter_loss: 0.1193322092294693
train_iter_loss: 0.11654213070869446
train_iter_loss: 0.07656504213809967
train_iter_loss: 0.17268426716327667
train_iter_loss: 0.13080066442489624
train_iter_loss: 0.23441189527511597
train_iter_loss: 0.10205182433128357
train_iter_loss: 0.12896326184272766
train_iter_loss: 0.24718555808067322
train_iter_loss: 0.16760554909706116
train_iter_loss: 0.16872484982013702
train_iter_loss: 0.14400388300418854
train_iter_loss: 0.1584036499261856
train_iter_loss: 0.10212858021259308
train_iter_loss: 0.16838762164115906
train_iter_loss: 0.06806447356939316
train_iter_loss: 0.1500103920698166
train_iter_loss: 0.1725517064332962
train_iter_loss: 0.1336999237537384
train_iter_loss: 0.0790882259607315
train_iter_loss: 0.15583480894565582
train_iter_loss: 0.15910255908966064
train_iter_loss: 0.0796266496181488
train_iter_loss: 0.06553278863430023
train_iter_loss: 0.08782212436199188
train_iter_loss: 0.13974884152412415
train_iter_loss: 0.12904009222984314
train_iter_loss: 0.2142130732536316
train_iter_loss: 0.1611076146364212
train_iter_loss: 0.17249388992786407
train_iter_loss: 0.11370865255594254
train_iter_loss: 0.1325072944164276
train_iter_loss: 0.13313548266887665
train_iter_loss: 0.2338765412569046
train_iter_loss: 0.1570175737142563
train_iter_loss: 0.2030486762523651
train_iter_loss: 0.145935520529747
train_iter_loss: 0.07934160530567169
train_iter_loss: 0.1357075273990631
train_iter_loss: 0.09785615652799606
train_iter_loss: 0.13371846079826355
train_iter_loss: 0.12417206913232803
train_iter_loss: 0.11718428134918213
train_iter_loss: 0.12975603342056274
train_iter_loss: 0.0936485156416893
train_iter_loss: 0.12131715565919876
train_iter_loss: 0.49114683270454407
train_iter_loss: 0.26649680733680725
train_iter_loss: 0.11046590656042099
train_iter_loss: 0.1300492286682129
train_iter_loss: 0.13886816799640656
train_iter_loss: 0.2541213631629944
train_iter_loss: 0.14798282086849213
train_iter_loss: 0.14733609557151794
train_iter_loss: 0.1522582322359085
train_iter_loss: 0.2106664776802063
train_iter_loss: 0.12994052469730377
train_iter_loss: 0.1526821255683899
train_iter_loss: 0.12589895725250244
train_iter_loss: 0.3353452980518341
train_iter_loss: 0.22177380323410034
train_iter_loss: 0.1688169240951538
train_iter_loss: 0.14268513023853302
train_iter_loss: 0.254606693983078
train_iter_loss: 0.3004041016101837
train_iter_loss: 0.13123491406440735
train_iter_loss: 0.2772107720375061
train_iter_loss: 0.12959745526313782
train_iter_loss: 0.260140597820282
train_iter_loss: 0.1405356377363205
train_iter_loss: 0.12999479472637177
train_iter_loss: 0.1256842166185379
train_iter_loss: 0.06812426447868347
train_iter_loss: 0.24605578184127808
train_iter_loss: 0.048660758882761
train_iter_loss: 0.17512869834899902
train_iter_loss: 0.18721555173397064
train_iter_loss: 0.04451090469956398
train_iter_loss: 0.21611161530017853
train_iter_loss: 0.19953973591327667
train_iter_loss: 0.18805809319019318
train_iter_loss: 0.10092663019895554
train_iter_loss: 0.09839016199111938
train_iter_loss: 0.2509818971157074
train_iter_loss: 0.09927396476268768
train_iter_loss: 0.13441964983940125
train_iter_loss: 0.12442898750305176
train loss :0.1575
---------------------
Validation seg loss: 0.2152805294015638 at epoch 826
epoch =    827/  1000, exp = train
train_iter_loss: 0.043702878057956696
train_iter_loss: 0.15929913520812988
train_iter_loss: 0.16508910059928894
train_iter_loss: 0.13604427874088287
train_iter_loss: 0.23585902154445648
train_iter_loss: 0.11119493097066879
train_iter_loss: 0.15938729047775269
train_iter_loss: 0.05878901109099388
train_iter_loss: 0.13783995807170868
train_iter_loss: 0.16054317355155945
train_iter_loss: 0.17342904210090637
train_iter_loss: 0.1808798462152481
train_iter_loss: 0.16158874332904816
train_iter_loss: 0.16524054110050201
train_iter_loss: 0.1224471777677536
train_iter_loss: 0.2515369951725006
train_iter_loss: 0.062456514686346054
train_iter_loss: 0.07563500106334686
train_iter_loss: 0.13630755245685577
train_iter_loss: 0.06838075816631317
train_iter_loss: 0.08052700012922287
train_iter_loss: 0.25005441904067993
train_iter_loss: 0.10665589570999146
train_iter_loss: 0.13814300298690796
train_iter_loss: 0.2364124357700348
train_iter_loss: 0.1321960687637329
train_iter_loss: 0.07258304208517075
train_iter_loss: 0.09779893606901169
train_iter_loss: 0.10318583995103836
train_iter_loss: 0.1546754091978073
train_iter_loss: 0.13737860321998596
train_iter_loss: 0.20850597321987152
train_iter_loss: 0.09052897989749908
train_iter_loss: 0.12531352043151855
train_iter_loss: 0.11579608917236328
train_iter_loss: 0.20807260274887085
train_iter_loss: 0.09584493190050125
train_iter_loss: 0.18774010241031647
train_iter_loss: 0.10119885206222534
train_iter_loss: 0.17653010785579681
train_iter_loss: 0.13432364165782928
train_iter_loss: 0.11826134473085403
train_iter_loss: 0.1765120029449463
train_iter_loss: 0.18145723640918732
train_iter_loss: 0.20805266499519348
train_iter_loss: 0.11399022489786148
train_iter_loss: 0.060731399804353714
train_iter_loss: 0.12316207587718964
train_iter_loss: 0.21406900882720947
train_iter_loss: 0.08380205184221268
train_iter_loss: 0.18027426302433014
train_iter_loss: 0.09196588397026062
train_iter_loss: 0.1501712203025818
train_iter_loss: 0.18757474422454834
train_iter_loss: 0.10866707563400269
train_iter_loss: 0.14702339470386505
train_iter_loss: 0.1178564801812172
train_iter_loss: 0.29452064633369446
train_iter_loss: 0.18566597998142242
train_iter_loss: 0.1269790679216385
train_iter_loss: 0.2063717544078827
train_iter_loss: 0.12193145602941513
train_iter_loss: 0.13013681769371033
train_iter_loss: 0.06887748837471008
train_iter_loss: 0.0712740421295166
train_iter_loss: 0.12026440352201462
train_iter_loss: 0.12649127840995789
train_iter_loss: 0.16766111552715302
train_iter_loss: 0.2561442255973816
train_iter_loss: 0.12492963671684265
train_iter_loss: 0.16423480212688446
train_iter_loss: 0.05032244324684143
train_iter_loss: 0.15869390964508057
train_iter_loss: 0.3536863327026367
train_iter_loss: 0.1643928736448288
train_iter_loss: 0.10657774657011032
train_iter_loss: 0.08711472898721695
train_iter_loss: 0.25428661704063416
train_iter_loss: 0.24750164151191711
train_iter_loss: 0.269150972366333
train_iter_loss: 0.141836017370224
train_iter_loss: 0.2583286464214325
train_iter_loss: 0.1421011984348297
train_iter_loss: 0.12943196296691895
train_iter_loss: 0.15953774750232697
train_iter_loss: 0.22979213297367096
train_iter_loss: 0.13068149983882904
train_iter_loss: 0.11207389831542969
train_iter_loss: 0.07741083204746246
train_iter_loss: 0.26143553853034973
train_iter_loss: 0.3018227517604828
train_iter_loss: 0.14703194797039032
train_iter_loss: 0.15197905898094177
train_iter_loss: 0.09292822331190109
train_iter_loss: 0.2074253261089325
train_iter_loss: 0.0759182944893837
train_iter_loss: 0.18262141942977905
train_iter_loss: 0.07360086590051651
train_iter_loss: 0.199346125125885
train_iter_loss: 0.26942330598831177
train loss :0.1534
---------------------
Validation seg loss: 0.21186591507339814 at epoch 827
epoch =    828/  1000, exp = train
train_iter_loss: 0.09095652401447296
train_iter_loss: 0.21732833981513977
train_iter_loss: 0.16686564683914185
train_iter_loss: 0.1529962718486786
train_iter_loss: 0.14913509786128998
train_iter_loss: 0.16580963134765625
train_iter_loss: 0.06720501184463501
train_iter_loss: 0.09901800751686096
train_iter_loss: 0.13614289462566376
train_iter_loss: 0.13620340824127197
train_iter_loss: 0.14978988468647003
train_iter_loss: 0.12395617365837097
train_iter_loss: 0.16065986454486847
train_iter_loss: 0.17255933582782745
train_iter_loss: 0.14122655987739563
train_iter_loss: 0.2050643265247345
train_iter_loss: 0.06755903363227844
train_iter_loss: 0.1642758846282959
train_iter_loss: 0.24723289906978607
train_iter_loss: 0.23608237504959106
train_iter_loss: 0.081342414021492
train_iter_loss: 0.08375213295221329
train_iter_loss: 0.18839578330516815
train_iter_loss: 0.12171631306409836
train_iter_loss: 0.14879482984542847
train_iter_loss: 0.23645663261413574
train_iter_loss: 0.23675262928009033
train_iter_loss: 0.3161083161830902
train_iter_loss: 0.1829376369714737
train_iter_loss: 0.278647243976593
train_iter_loss: 0.1881125420331955
train_iter_loss: 0.1775316596031189
train_iter_loss: 0.08362918347120285
train_iter_loss: 0.1968417912721634
train_iter_loss: 0.17881061136722565
train_iter_loss: 0.1976642906665802
train_iter_loss: 0.05573292821645737
train_iter_loss: 0.1314391791820526
train_iter_loss: 0.1074899211525917
train_iter_loss: 0.13411366939544678
train_iter_loss: 0.19439662992954254
train_iter_loss: 0.0947931557893753
train_iter_loss: 0.15046361088752747
train_iter_loss: 0.16329407691955566
train_iter_loss: 0.1529792696237564
train_iter_loss: 0.14190208911895752
train_iter_loss: 0.14705292880535126
train_iter_loss: 0.18929415941238403
train_iter_loss: 0.19248050451278687
train_iter_loss: 0.18940307199954987
train_iter_loss: 0.07955338805913925
train_iter_loss: 0.23971128463745117
train_iter_loss: 0.10986638814210892
train_iter_loss: 0.12473958730697632
train_iter_loss: 0.1682617962360382
train_iter_loss: 0.09861292690038681
train_iter_loss: 0.10760408639907837
train_iter_loss: 0.10106100142002106
train_iter_loss: 0.18420985341072083
train_iter_loss: 0.07403788715600967
train_iter_loss: 0.10783696919679642
train_iter_loss: 0.2502414286136627
train_iter_loss: 0.17952539026737213
train_iter_loss: 0.1679333597421646
train_iter_loss: 0.1145862564444542
train_iter_loss: 0.0769006684422493
train_iter_loss: 0.15687300264835358
train_iter_loss: 0.13325265049934387
train_iter_loss: 0.13225795328617096
train_iter_loss: 0.10320455580949783
train_iter_loss: 0.16521303355693817
train_iter_loss: 0.23264087736606598
train_iter_loss: 0.035118069499731064
train_iter_loss: 0.07976561039686203
train_iter_loss: 0.23271013796329498
train_iter_loss: 0.14267149567604065
train_iter_loss: 0.0952162966132164
train_iter_loss: 0.10496298968791962
train_iter_loss: 0.07543323934078217
train_iter_loss: 0.11440896987915039
train_iter_loss: 0.1524273008108139
train_iter_loss: 0.16746360063552856
train_iter_loss: 0.1097637191414833
train_iter_loss: 0.2110981047153473
train_iter_loss: 0.1353904753923416
train_iter_loss: 0.09767493605613708
train_iter_loss: 0.0841813012957573
train_iter_loss: 0.1610420197248459
train_iter_loss: 0.12450273334980011
train_iter_loss: 0.20792256295681
train_iter_loss: 0.1463925838470459
train_iter_loss: 0.20809221267700195
train_iter_loss: 0.16023920476436615
train_iter_loss: 0.17303848266601562
train_iter_loss: 0.14627808332443237
train_iter_loss: 0.06463218480348587
train_iter_loss: 0.13008691370487213
train_iter_loss: 0.16182497143745422
train_iter_loss: 0.16066521406173706
train_iter_loss: 0.09540373831987381
train loss :0.1503
---------------------
Validation seg loss: 0.21353860237231514 at epoch 828
epoch =    829/  1000, exp = train
train_iter_loss: 0.13299621641635895
train_iter_loss: 0.04166597127914429
train_iter_loss: 0.19196799397468567
train_iter_loss: 0.19148015975952148
train_iter_loss: 0.09051255136728287
train_iter_loss: 0.2609535753726959
train_iter_loss: 0.15123678743839264
train_iter_loss: 0.18935221433639526
train_iter_loss: 0.23027421534061432
train_iter_loss: 0.18426451086997986
train_iter_loss: 0.12647181749343872
train_iter_loss: 0.10857326537370682
train_iter_loss: 0.08124642074108124
train_iter_loss: 0.09045323729515076
train_iter_loss: 0.17263257503509521
train_iter_loss: 0.11271514743566513
train_iter_loss: 0.14503487944602966
train_iter_loss: 0.14662356674671173
train_iter_loss: 0.15632414817810059
train_iter_loss: 0.1610848754644394
train_iter_loss: 0.14535637199878693
train_iter_loss: 0.11241206526756287
train_iter_loss: 0.11094462126493454
train_iter_loss: 0.1458732932806015
train_iter_loss: 0.18588413298130035
train_iter_loss: 0.08377313613891602
train_iter_loss: 0.183318093419075
train_iter_loss: 0.13519808650016785
train_iter_loss: 0.06061282753944397
train_iter_loss: 0.06540261209011078
train_iter_loss: 0.11935528367757797
train_iter_loss: 0.16879339516162872
train_iter_loss: 0.202090322971344
train_iter_loss: 0.07438680529594421
train_iter_loss: 0.0686430037021637
train_iter_loss: 0.21462927758693695
train_iter_loss: 0.14050570130348206
train_iter_loss: 0.10812519490718842
train_iter_loss: 0.3233877122402191
train_iter_loss: 0.09929066896438599
train_iter_loss: 0.13214747607707977
train_iter_loss: 0.11182046681642532
train_iter_loss: 0.16031427681446075
train_iter_loss: 0.13808497786521912
train_iter_loss: 0.17992685735225677
train_iter_loss: 0.0931985005736351
train_iter_loss: 0.31521841883659363
train_iter_loss: 0.1623571217060089
train_iter_loss: 0.08979310095310211
train_iter_loss: 0.18737630546092987
train_iter_loss: 0.08813932538032532
train_iter_loss: 0.216775044798851
train_iter_loss: 0.1686168611049652
train_iter_loss: 0.0905924141407013
train_iter_loss: 0.1783975511789322
train_iter_loss: 0.1825181245803833
train_iter_loss: 0.07862359285354614
train_iter_loss: 0.29680532217025757
train_iter_loss: 0.2018311321735382
train_iter_loss: 0.137539342045784
train_iter_loss: 0.1927463710308075
train_iter_loss: 0.13913704454898834
train_iter_loss: 0.16245649755001068
train_iter_loss: 0.1392677128314972
train_iter_loss: 0.18621735274791718
train_iter_loss: 0.11507440358400345
train_iter_loss: 0.13823816180229187
train_iter_loss: 0.09880689531564713
train_iter_loss: 0.08518543839454651
train_iter_loss: 0.13207337260246277
train_iter_loss: 0.19950497150421143
train_iter_loss: 0.12914909422397614
train_iter_loss: 0.12262582033872604
train_iter_loss: 0.14080800116062164
train_iter_loss: 0.13351012766361237
train_iter_loss: 0.11531643569469452
train_iter_loss: 0.10736504197120667
train_iter_loss: 0.13976818323135376
train_iter_loss: 0.10250822454690933
train_iter_loss: 0.1775738149881363
train_iter_loss: 0.08087784051895142
train_iter_loss: 0.10649067163467407
train_iter_loss: 0.10605887323617935
train_iter_loss: 0.20009547472000122
train_iter_loss: 0.10967926681041718
train_iter_loss: 0.20895661413669586
train_iter_loss: 0.1167406365275383
train_iter_loss: 0.23749405145645142
train_iter_loss: 0.11518566310405731
train_iter_loss: 0.1283082664012909
train_iter_loss: 0.19343964755535126
train_iter_loss: 0.10551054775714874
train_iter_loss: 0.07807611674070358
train_iter_loss: 0.18343256413936615
train_iter_loss: 0.12062503397464752
train_iter_loss: 0.12081753462553024
train_iter_loss: 0.1871025413274765
train_iter_loss: 0.1301102489233017
train_iter_loss: 0.18577872216701508
train_iter_loss: 0.2065848559141159
train loss :0.1475
---------------------
Validation seg loss: 0.216400423842781 at epoch 829
epoch =    830/  1000, exp = train
train_iter_loss: 0.19864484667778015
train_iter_loss: 0.2139091044664383
train_iter_loss: 0.09826961904764175
train_iter_loss: 0.1107683926820755
train_iter_loss: 0.1968473494052887
train_iter_loss: 0.29106009006500244
train_iter_loss: 0.15680551528930664
train_iter_loss: 0.03077707439661026
train_iter_loss: 0.2582319676876068
train_iter_loss: 0.12106650322675705
train_iter_loss: 0.16274170577526093
train_iter_loss: 0.17188239097595215
train_iter_loss: 0.059823911637067795
train_iter_loss: 0.09091390669345856
train_iter_loss: 0.16251155734062195
train_iter_loss: 0.2923656404018402
train_iter_loss: 0.144916370511055
train_iter_loss: 0.21937230229377747
train_iter_loss: 0.1069004014134407
train_iter_loss: 0.17326059937477112
train_iter_loss: 0.33971577882766724
train_iter_loss: 0.10386601090431213
train_iter_loss: 0.0674348771572113
train_iter_loss: 0.18565420806407928
train_iter_loss: 0.22805461287498474
train_iter_loss: 0.12168130278587341
train_iter_loss: 0.11995436996221542
train_iter_loss: 0.2538454234600067
train_iter_loss: 0.07377295196056366
train_iter_loss: 0.18843898177146912
train_iter_loss: 0.24535247683525085
train_iter_loss: 0.1449798196554184
train_iter_loss: 0.21048307418823242
train_iter_loss: 0.11255458742380142
train_iter_loss: 0.17232096195220947
train_iter_loss: 0.08321715891361237
train_iter_loss: 0.15153543651103973
train_iter_loss: 0.25590646266937256
train_iter_loss: 0.3578275442123413
train_iter_loss: 0.05646472051739693
train_iter_loss: 0.15428079664707184
train_iter_loss: 0.18092581629753113
train_iter_loss: 0.23339413106441498
train_iter_loss: 0.1961677521467209
train_iter_loss: 0.19215027987957
train_iter_loss: 0.1829567700624466
train_iter_loss: 0.1752033531665802
train_iter_loss: 0.11748557537794113
train_iter_loss: 0.059155888855457306
train_iter_loss: 0.0860278308391571
train_iter_loss: 0.1432439386844635
train_iter_loss: 0.12176594138145447
train_iter_loss: 0.16814303398132324
train_iter_loss: 0.13691295683383942
train_iter_loss: 0.21830987930297852
train_iter_loss: 0.14515408873558044
train_iter_loss: 0.13645289838314056
train_iter_loss: 0.1345444619655609
train_iter_loss: 0.16889025270938873
train_iter_loss: 0.06638111919164658
train_iter_loss: 0.12838812172412872
train_iter_loss: 0.29704442620277405
train_iter_loss: 0.10783408582210541
train_iter_loss: 0.17509767413139343
train_iter_loss: 0.15214920043945312
train_iter_loss: 0.07538618892431259
train_iter_loss: 0.12157060950994492
train_iter_loss: 0.19295597076416016
train_iter_loss: 0.22826321423053741
train_iter_loss: 0.12101022154092789
train_iter_loss: 0.1292349249124527
train_iter_loss: 0.10775049030780792
train_iter_loss: 0.07500075548887253
train_iter_loss: 0.0863671600818634
train_iter_loss: 0.10636957734823227
train_iter_loss: 0.14214864373207092
train_iter_loss: 0.1174294725060463
train_iter_loss: 0.1336885690689087
train_iter_loss: 0.13702955842018127
train_iter_loss: 0.0873854011297226
train_iter_loss: 0.289898544549942
train_iter_loss: 0.19842277467250824
train_iter_loss: 0.09379497915506363
train_iter_loss: 0.12603648006916046
train_iter_loss: 0.13389801979064941
train_iter_loss: 0.07479482144117355
train_iter_loss: 0.16172543168067932
train_iter_loss: 0.17588447034358978
train_iter_loss: 0.3209258019924164
train_iter_loss: 0.19970262050628662
train_iter_loss: 0.13385239243507385
train_iter_loss: 0.03983346000313759
train_iter_loss: 0.12696491181850433
train_iter_loss: 0.16726873815059662
train_iter_loss: 0.10850630700588226
train_iter_loss: 0.10964785516262054
train_iter_loss: 0.08290984481573105
train_iter_loss: 0.0649324357509613
train_iter_loss: 0.1397560089826584
train_iter_loss: 0.1608806997537613
train loss :0.1553
---------------------
Validation seg loss: 0.22066715245468999 at epoch 830
epoch =    831/  1000, exp = train
train_iter_loss: 0.08111415058374405
train_iter_loss: 0.1357143521308899
train_iter_loss: 0.17196859419345856
train_iter_loss: 0.1381531059741974
train_iter_loss: 0.16929316520690918
train_iter_loss: 0.2414342314004898
train_iter_loss: 0.27836233377456665
train_iter_loss: 0.12095145881175995
train_iter_loss: 0.09815492480993271
train_iter_loss: 0.12343292683362961
train_iter_loss: 0.2304198294878006
train_iter_loss: 0.13239789009094238
train_iter_loss: 0.06269942969083786
train_iter_loss: 0.12465368211269379
train_iter_loss: 0.07875297963619232
train_iter_loss: 0.18855074048042297
train_iter_loss: 0.2365085631608963
train_iter_loss: 0.16200262308120728
train_iter_loss: 0.21552403271198273
train_iter_loss: 0.1678697168827057
train_iter_loss: 0.1486593782901764
train_iter_loss: 0.09740255773067474
train_iter_loss: 0.08719691634178162
train_iter_loss: 0.10621273517608643
train_iter_loss: 0.1483602225780487
train_iter_loss: 0.14749041199684143
train_iter_loss: 0.13631471991539001
train_iter_loss: 0.14347025752067566
train_iter_loss: 0.18539175391197205
train_iter_loss: 0.1125202476978302
train_iter_loss: 0.1379115730524063
train_iter_loss: 0.22341862320899963
train_iter_loss: 0.2160651534795761
train_iter_loss: 0.046310603618621826
train_iter_loss: 0.1098107174038887
train_iter_loss: 0.09209989011287689
train_iter_loss: 0.16407564282417297
train_iter_loss: 0.028568487614393234
train_iter_loss: 0.14945068955421448
train_iter_loss: 0.11272920668125153
train_iter_loss: 0.22045786678791046
train_iter_loss: 0.06153366342186928
train_iter_loss: 0.193754181265831
train_iter_loss: 0.08255040645599365
train_iter_loss: 0.1937386393547058
train_iter_loss: 0.15148118138313293
train_iter_loss: 0.08353172242641449
train_iter_loss: 0.13841427862644196
train_iter_loss: 0.131764218211174
train_iter_loss: 0.21541926264762878
train_iter_loss: 0.08940920233726501
train_iter_loss: 0.23290619254112244
train_iter_loss: 0.08746632188558578
train_iter_loss: 0.07596457004547119
train_iter_loss: 0.1684241145849228
train_iter_loss: 0.19202837347984314
train_iter_loss: 0.2161208987236023
train_iter_loss: 0.28006404638290405
train_iter_loss: 0.1270751953125
train_iter_loss: 0.33415132761001587
train_iter_loss: 0.14557461440563202
train_iter_loss: 0.13359421491622925
train_iter_loss: 0.14246124029159546
train_iter_loss: 0.23688119649887085
train_iter_loss: 0.25768962502479553
train_iter_loss: 0.1669328808784485
train_iter_loss: 0.12271828204393387
train_iter_loss: 0.07737233489751816
train_iter_loss: 0.08129045367240906
train_iter_loss: 0.11765560507774353
train_iter_loss: 0.12597234547138214
train_iter_loss: 0.12104318290948868
train_iter_loss: 0.17890414595603943
train_iter_loss: 0.23276261985301971
train_iter_loss: 0.12745289504528046
train_iter_loss: 0.1795087605714798
train_iter_loss: 0.14425383508205414
train_iter_loss: 0.12204989045858383
train_iter_loss: 0.10986872017383575
train_iter_loss: 0.12490913271903992
train_iter_loss: 0.19959484040737152
train_iter_loss: 0.19512952864170074
train_iter_loss: 0.17205160856246948
train_iter_loss: 0.15304385125637054
train_iter_loss: 0.14687854051589966
train_iter_loss: 0.2038096785545349
train_iter_loss: 0.12321488559246063
train_iter_loss: 0.14629262685775757
train_iter_loss: 0.10727313905954361
train_iter_loss: 0.1790507733821869
train_iter_loss: 0.168082594871521
train_iter_loss: 0.09030142426490784
train_iter_loss: 0.19300143420696259
train_iter_loss: 0.2333890050649643
train_iter_loss: 0.15143151581287384
train_iter_loss: 0.2575113773345947
train_iter_loss: 0.09608284384012222
train_iter_loss: 0.07495726644992828
train_iter_loss: 0.060288455337285995
train_iter_loss: 0.12966980040073395
train loss :0.1521
---------------------
Validation seg loss: 0.21599129308893714 at epoch 831
epoch =    832/  1000, exp = train
train_iter_loss: 0.29702216386795044
train_iter_loss: 0.12217434495687485
train_iter_loss: 0.2055901288986206
train_iter_loss: 0.0966254249215126
train_iter_loss: 0.1473807394504547
train_iter_loss: 0.13679994642734528
train_iter_loss: 0.13410072028636932
train_iter_loss: 0.1411130428314209
train_iter_loss: 0.15670645236968994
train_iter_loss: 0.16070792078971863
train_iter_loss: 0.17522646486759186
train_iter_loss: 0.13742601871490479
train_iter_loss: 0.11464788019657135
train_iter_loss: 0.13186943531036377
train_iter_loss: 0.10458749532699585
train_iter_loss: 0.07491049915552139
train_iter_loss: 0.19718711078166962
train_iter_loss: 0.12982593476772308
train_iter_loss: 0.052051421254873276
train_iter_loss: 0.1482168585062027
train_iter_loss: 0.26466062664985657
train_iter_loss: 0.18416334688663483
train_iter_loss: 0.10871203243732452
train_iter_loss: 0.19398166239261627
train_iter_loss: 0.09915091842412949
train_iter_loss: 0.19685518741607666
train_iter_loss: 0.13833490014076233
train_iter_loss: 0.1792929321527481
train_iter_loss: 0.2691928744316101
train_iter_loss: 0.09262014925479889
train_iter_loss: 0.16957290470600128
train_iter_loss: 0.1016889438033104
train_iter_loss: 0.1374424397945404
train_iter_loss: 0.0424300916492939
train_iter_loss: 0.10401894152164459
train_iter_loss: 0.08493397384881973
train_iter_loss: 0.2647232115268707
train_iter_loss: 0.18830835819244385
train_iter_loss: 0.12345927208662033
train_iter_loss: 0.2188103348016739
train_iter_loss: 0.21693146228790283
train_iter_loss: 0.15777237713336945
train_iter_loss: 0.11805853247642517
train_iter_loss: 0.31104719638824463
train_iter_loss: 0.18111227452754974
train_iter_loss: 0.11151089519262314
train_iter_loss: 0.12317153811454773
train_iter_loss: 0.0205155648291111
train_iter_loss: 0.10251165181398392
train_iter_loss: 0.1229100301861763
train_iter_loss: 0.2147611528635025
train_iter_loss: 0.11137615144252777
train_iter_loss: 0.17496347427368164
train_iter_loss: 0.06225435063242912
train_iter_loss: 0.18270911276340485
train_iter_loss: 0.15232673287391663
train_iter_loss: 0.09779337793588638
train_iter_loss: 0.1113375723361969
train_iter_loss: 0.18786805868148804
train_iter_loss: 0.12595002353191376
train_iter_loss: 0.09014584124088287
train_iter_loss: 0.21399402618408203
train_iter_loss: 0.12766113877296448
train_iter_loss: 0.13965584337711334
train_iter_loss: 0.13313741981983185
train_iter_loss: 0.15170443058013916
train_iter_loss: 0.05758717656135559
train_iter_loss: 0.15017741918563843
train_iter_loss: 0.3415776789188385
train_iter_loss: 0.19193795323371887
train_iter_loss: 0.13825833797454834
train_iter_loss: 0.12891793251037598
train_iter_loss: 0.2267572432756424
train_iter_loss: 0.1853553056716919
train_iter_loss: 0.17340753972530365
train_iter_loss: 0.152741476893425
train_iter_loss: 0.050740886479616165
train_iter_loss: 0.23561757802963257
train_iter_loss: 0.3083438575267792
train_iter_loss: 0.22524264454841614
train_iter_loss: 0.09958718717098236
train_iter_loss: 0.11183009296655655
train_iter_loss: 0.20187623798847198
train_iter_loss: 0.09633415937423706
train_iter_loss: 0.26664772629737854
train_iter_loss: 0.0956738144159317
train_iter_loss: 0.14024752378463745
train_iter_loss: 0.2077460139989853
train_iter_loss: 0.3248685896396637
train_iter_loss: 0.12186966091394424
train_iter_loss: 0.21047639846801758
train_iter_loss: 0.08771882951259613
train_iter_loss: 0.14200648665428162
train_iter_loss: 0.13134577870368958
train_iter_loss: 0.18128566443920135
train_iter_loss: 0.11932657659053802
train_iter_loss: 0.17382857203483582
train_iter_loss: 0.1466054618358612
train_iter_loss: 0.15363050997257233
train_iter_loss: 0.14941328763961792
train loss :0.1565
---------------------
Validation seg loss: 0.2190147537896234 at epoch 832
epoch =    833/  1000, exp = train
train_iter_loss: 0.1885778158903122
train_iter_loss: 0.17600418627262115
train_iter_loss: 0.2077515572309494
train_iter_loss: 0.2076546996831894
train_iter_loss: 0.16613033413887024
train_iter_loss: 0.14363059401512146
train_iter_loss: 0.11331792175769806
train_iter_loss: 0.19919833540916443
train_iter_loss: 0.11346957087516785
train_iter_loss: 0.28386828303337097
train_iter_loss: 0.20843124389648438
train_iter_loss: 0.11153450608253479
train_iter_loss: 0.13195064663887024
train_iter_loss: 0.10872893780469894
train_iter_loss: 0.12497632205486298
train_iter_loss: 0.09636139869689941
train_iter_loss: 0.14124715328216553
train_iter_loss: 0.11486102640628815
train_iter_loss: 0.13889001309871674
train_iter_loss: 0.03621869906783104
train_iter_loss: 0.1757880002260208
train_iter_loss: 0.1964409351348877
train_iter_loss: 0.17887268960475922
train_iter_loss: 0.07093688100576401
train_iter_loss: 0.18404753506183624
train_iter_loss: 0.15691818296909332
train_iter_loss: 0.16753265261650085
train_iter_loss: 0.09061689674854279
train_iter_loss: 0.1378491073846817
train_iter_loss: 0.28555384278297424
train_iter_loss: 0.25778988003730774
train_iter_loss: 0.11357497423887253
train_iter_loss: 0.15499016642570496
train_iter_loss: 0.2247120887041092
train_iter_loss: 0.08222558349370956
train_iter_loss: 0.17136985063552856
train_iter_loss: 0.1510210633277893
train_iter_loss: 0.2261323183774948
train_iter_loss: 0.10162261128425598
train_iter_loss: 0.10360081493854523
train_iter_loss: 0.2774881422519684
train_iter_loss: 0.10257197171449661
train_iter_loss: 0.18168170750141144
train_iter_loss: 0.19566228985786438
train_iter_loss: 0.241094172000885
train_iter_loss: 0.3035348951816559
train_iter_loss: 0.11602981388568878
train_iter_loss: 0.20296330749988556
train_iter_loss: 0.0710846483707428
train_iter_loss: 0.2003764510154724
train_iter_loss: 0.11565937846899033
train_iter_loss: 0.09561130404472351
train_iter_loss: 0.24060674011707306
train_iter_loss: 0.11342733353376389
train_iter_loss: 0.14862722158432007
train_iter_loss: 0.14231766760349274
train_iter_loss: 0.1011425331234932
train_iter_loss: 0.19284844398498535
train_iter_loss: 0.08169765025377274
train_iter_loss: 0.1830974668264389
train_iter_loss: 0.08697399497032166
train_iter_loss: 0.23717375099658966
train_iter_loss: 0.11953112483024597
train_iter_loss: 0.17386411130428314
train_iter_loss: 0.16530372202396393
train_iter_loss: 0.17121225595474243
train_iter_loss: 0.20429858565330505
train_iter_loss: 0.16535189747810364
train_iter_loss: 0.19960491359233856
train_iter_loss: 0.1176915317773819
train_iter_loss: 0.16949082911014557
train_iter_loss: 0.22147761285305023
train_iter_loss: 0.12532061338424683
train_iter_loss: 0.11802324652671814
train_iter_loss: 0.05406293645501137
train_iter_loss: 0.19529767334461212
train_iter_loss: 0.20578691363334656
train_iter_loss: 0.12825219333171844
train_iter_loss: 0.14526879787445068
train_iter_loss: 0.13867513835430145
train_iter_loss: 0.09314636141061783
train_iter_loss: 0.1188240647315979
train_iter_loss: 0.11432405561208725
train_iter_loss: 0.18847687542438507
train_iter_loss: 0.12762707471847534
train_iter_loss: 0.2833721339702606
train_iter_loss: 0.09446188807487488
train_iter_loss: 0.11102105677127838
train_iter_loss: 0.08795851469039917
train_iter_loss: 0.26908057928085327
train_iter_loss: 0.179311603307724
train_iter_loss: 0.09295191615819931
train_iter_loss: 0.2004677802324295
train_iter_loss: 0.06920144706964493
train_iter_loss: 0.08644717186689377
train_iter_loss: 0.1207568421959877
train_iter_loss: 0.16972655057907104
train_iter_loss: 0.18681414425373077
train_iter_loss: 0.15946222841739655
train_iter_loss: 0.15402570366859436
train loss :0.1576
---------------------
Validation seg loss: 0.2173824471614833 at epoch 833
epoch =    834/  1000, exp = train
train_iter_loss: 0.1670776903629303
train_iter_loss: 0.15427090227603912
train_iter_loss: 0.2070685625076294
train_iter_loss: 0.19498491287231445
train_iter_loss: 0.15426690876483917
train_iter_loss: 0.17881867289543152
train_iter_loss: 0.1502533107995987
train_iter_loss: 0.18075180053710938
train_iter_loss: 0.1420586109161377
train_iter_loss: 0.30690300464630127
train_iter_loss: 0.22488464415073395
train_iter_loss: 0.11003148555755615
train_iter_loss: 0.17773208022117615
train_iter_loss: 0.14123934507369995
train_iter_loss: 0.11952777951955795
train_iter_loss: 0.1518869251012802
train_iter_loss: 0.2569284439086914
train_iter_loss: 0.0988595187664032
train_iter_loss: 0.1958695948123932
train_iter_loss: 0.2029094696044922
train_iter_loss: 0.10296638309955597
train_iter_loss: 0.13752023875713348
train_iter_loss: 0.10962684452533722
train_iter_loss: 0.1664808988571167
train_iter_loss: 0.17210909724235535
train_iter_loss: 0.15789784491062164
train_iter_loss: 0.2248118370771408
train_iter_loss: 0.19908422231674194
train_iter_loss: 0.12627890706062317
train_iter_loss: 0.1147339940071106
train_iter_loss: 0.06534365564584732
train_iter_loss: 0.08285435289144516
train_iter_loss: 0.18368548154830933
train_iter_loss: 0.10018384456634521
train_iter_loss: 0.11311987042427063
train_iter_loss: 0.15629099309444427
train_iter_loss: 0.14551064372062683
train_iter_loss: 0.17500120401382446
train_iter_loss: 0.07998985052108765
train_iter_loss: 0.17584334313869476
train_iter_loss: 0.14932851493358612
train_iter_loss: 0.1297321766614914
train_iter_loss: 0.09972633421421051
train_iter_loss: 0.15315957367420197
train_iter_loss: 0.09070470929145813
train_iter_loss: 0.16825465857982635
train_iter_loss: 0.14287959039211273
train_iter_loss: 0.16464848816394806
train_iter_loss: 0.1700705587863922
train_iter_loss: 0.147734135389328
train_iter_loss: 0.12516477704048157
train_iter_loss: 0.14363586902618408
train_iter_loss: 0.08719129860401154
train_iter_loss: 0.22232799232006073
train_iter_loss: 0.12517675757408142
train_iter_loss: 0.15436747670173645
train_iter_loss: 0.3236263394355774
train_iter_loss: 0.21961601078510284
train_iter_loss: 0.07727210968732834
train_iter_loss: 0.20438802242279053
train_iter_loss: 0.10860029608011246
train_iter_loss: 0.07580810785293579
train_iter_loss: 0.13182173669338226
train_iter_loss: 0.2579047679901123
train_iter_loss: 0.13269290328025818
train_iter_loss: 0.2566063404083252
train_iter_loss: 0.033303193747997284
train_iter_loss: 0.08542492240667343
train_iter_loss: 0.13230712711811066
train_iter_loss: 0.2120267003774643
train_iter_loss: 0.1565294712781906
train_iter_loss: 0.07041078060865402
train_iter_loss: 0.07685920596122742
train_iter_loss: 0.2747083008289337
train_iter_loss: 0.1700439453125
train_iter_loss: 0.12141992151737213
train_iter_loss: 0.13274264335632324
train_iter_loss: 0.15840351581573486
train_iter_loss: 0.1649610847234726
train_iter_loss: 0.024058839306235313
train_iter_loss: 0.12013527750968933
train_iter_loss: 0.16768428683280945
train_iter_loss: 0.07565151900053024
train_iter_loss: 0.3475935459136963
train_iter_loss: 0.14873161911964417
train_iter_loss: 0.21054290235042572
train_iter_loss: 0.14074283838272095
train_iter_loss: 0.16554978489875793
train_iter_loss: 0.18059152364730835
train_iter_loss: 0.18054050207138062
train_iter_loss: 0.15560036897659302
train_iter_loss: 0.15442633628845215
train_iter_loss: 0.1567981243133545
train_iter_loss: 0.23153354227542877
train_iter_loss: 0.057120949029922485
train_iter_loss: 0.17144028842449188
train_iter_loss: 0.20711781084537506
train_iter_loss: 0.12235917150974274
train_iter_loss: 0.26395851373672485
train_iter_loss: 0.07310076802968979
train loss :0.1567
---------------------
Validation seg loss: 0.21512088921131953 at epoch 834
epoch =    835/  1000, exp = train
train_iter_loss: 0.2502031624317169
train_iter_loss: 0.14542320370674133
train_iter_loss: 0.13775517046451569
train_iter_loss: 0.09479188174009323
train_iter_loss: 0.22982923686504364
train_iter_loss: 0.17874839901924133
train_iter_loss: 0.15477196872234344
train_iter_loss: 0.09759102016687393
train_iter_loss: 0.13787773251533508
train_iter_loss: 0.13689880073070526
train_iter_loss: 0.16421063244342804
train_iter_loss: 0.14205273985862732
train_iter_loss: 0.10895568132400513
train_iter_loss: 0.15233691036701202
train_iter_loss: 0.14266307651996613
train_iter_loss: 0.07614941895008087
train_iter_loss: 0.1278693825006485
train_iter_loss: 0.11747731268405914
train_iter_loss: 0.1427544504404068
train_iter_loss: 0.05036160722374916
train_iter_loss: 0.17579112946987152
train_iter_loss: 0.10068250447511673
train_iter_loss: 0.12746109068393707
train_iter_loss: 0.12410738319158554
train_iter_loss: 0.1574375033378601
train_iter_loss: 0.17813414335250854
train_iter_loss: 0.06323502212762833
train_iter_loss: 0.17499801516532898
train_iter_loss: 0.24599726498126984
train_iter_loss: 0.11689906567335129
train_iter_loss: 0.15561698377132416
train_iter_loss: 0.2733798921108246
train_iter_loss: 0.15030136704444885
train_iter_loss: 0.13702231645584106
train_iter_loss: 0.06674579530954361
train_iter_loss: 0.14642547070980072
train_iter_loss: 0.11333960294723511
train_iter_loss: 0.14250122010707855
train_iter_loss: 0.14792661368846893
train_iter_loss: 0.11638732999563217
train_iter_loss: 0.1844385266304016
train_iter_loss: 0.24900558590888977
train_iter_loss: 0.11261788010597229
train_iter_loss: 0.13478361070156097
train_iter_loss: 0.22937993705272675
train_iter_loss: 0.29741325974464417
train_iter_loss: 0.05187751352787018
train_iter_loss: 0.07068431377410889
train_iter_loss: 0.09931614249944687
train_iter_loss: 0.10292312502861023
train_iter_loss: 0.12645982205867767
train_iter_loss: 0.10638215392827988
train_iter_loss: 0.2109951674938202
train_iter_loss: 0.2740251421928406
train_iter_loss: 0.1644742339849472
train_iter_loss: 0.14670594036579132
train_iter_loss: 0.17133042216300964
train_iter_loss: 0.150658518075943
train_iter_loss: 0.21286270022392273
train_iter_loss: 0.13806819915771484
train_iter_loss: 0.04357723146677017
train_iter_loss: 0.07805807888507843
train_iter_loss: 0.14023785293102264
train_iter_loss: 0.13400845229625702
train_iter_loss: 0.11867785453796387
train_iter_loss: 0.1740245223045349
train_iter_loss: 0.0657731145620346
train_iter_loss: 0.12537157535552979
train_iter_loss: 0.16756242513656616
train_iter_loss: 0.16898174583911896
train_iter_loss: 0.3674579858779907
train_iter_loss: 0.16919460892677307
train_iter_loss: 0.25111857056617737
train_iter_loss: 0.17311054468154907
train_iter_loss: 0.1683027744293213
train_iter_loss: 0.17574480175971985
train_iter_loss: 0.10365022718906403
train_iter_loss: 0.1271822452545166
train_iter_loss: 0.14351096749305725
train_iter_loss: 0.12785451114177704
train_iter_loss: 0.19788514077663422
train_iter_loss: 0.17760159075260162
train_iter_loss: 0.11070328950881958
train_iter_loss: 0.14870020747184753
train_iter_loss: 0.11797094345092773
train_iter_loss: 0.11276724189519882
train_iter_loss: 0.13458546996116638
train_iter_loss: 0.2298080176115036
train_iter_loss: 0.18673664331436157
train_iter_loss: 0.14594389498233795
train_iter_loss: 0.3608646094799042
train_iter_loss: 0.144746795296669
train_iter_loss: 0.21000616252422333
train_iter_loss: 0.12232521921396255
train_iter_loss: 0.11677702516317368
train_iter_loss: 0.15864881873130798
train_iter_loss: 0.15356779098510742
train_iter_loss: 0.18244528770446777
train_iter_loss: 0.18555256724357605
train_iter_loss: 0.13658776879310608
train loss :0.1545
---------------------
Validation seg loss: 0.21425611421219865 at epoch 835
epoch =    836/  1000, exp = train
train_iter_loss: 0.12483111023902893
train_iter_loss: 0.2895317077636719
train_iter_loss: 0.1620747447013855
train_iter_loss: 0.2372363805770874
train_iter_loss: 0.11521873623132706
train_iter_loss: 0.12641583383083344
train_iter_loss: 0.08574575185775757
train_iter_loss: 0.10791068524122238
train_iter_loss: 0.1825004369020462
train_iter_loss: 0.25681617856025696
train_iter_loss: 0.149840846657753
train_iter_loss: 0.2292480319738388
train_iter_loss: 0.06475554406642914
train_iter_loss: 0.12395073473453522
train_iter_loss: 0.15054912865161896
train_iter_loss: 0.17123596370220184
train_iter_loss: 0.18150758743286133
train_iter_loss: 0.10839710384607315
train_iter_loss: 0.20547348260879517
train_iter_loss: 0.19808335602283478
train_iter_loss: 0.06957538425922394
train_iter_loss: 0.1485201120376587
train_iter_loss: 0.2869248390197754
train_iter_loss: 0.22643782198429108
train_iter_loss: 0.11412392556667328
train_iter_loss: 0.1372404843568802
train_iter_loss: 0.13211072981357574
train_iter_loss: 0.1134689450263977
train_iter_loss: 0.09428278356790543
train_iter_loss: 0.17157313227653503
train_iter_loss: 0.16642659902572632
train_iter_loss: 0.1814916878938675
train_iter_loss: 0.23759302496910095
train_iter_loss: 0.2881549894809723
train_iter_loss: 0.17451982200145721
train_iter_loss: 0.07804715633392334
train_iter_loss: 0.11307433992624283
train_iter_loss: 0.13938012719154358
train_iter_loss: 0.1304982453584671
train_iter_loss: 0.17041637003421783
train_iter_loss: 0.10612628608942032
train_iter_loss: 0.18216440081596375
train_iter_loss: 0.17931623756885529
train_iter_loss: 0.048654187470674515
train_iter_loss: 0.1546342521905899
train_iter_loss: 0.3102847933769226
train_iter_loss: 0.21359223127365112
train_iter_loss: 0.18675288558006287
train_iter_loss: 0.2724536955356598
train_iter_loss: 0.15903903543949127
train_iter_loss: 0.21327196061611176
train_iter_loss: 0.12622235715389252
train_iter_loss: 0.2201186716556549
train_iter_loss: 0.14370134472846985
train_iter_loss: 0.11425938457250595
train_iter_loss: 0.1993781477212906
train_iter_loss: 0.08076310902833939
train_iter_loss: 0.1522364318370819
train_iter_loss: 0.06092660874128342
train_iter_loss: 0.16218698024749756
train_iter_loss: 0.05190679803490639
train_iter_loss: 0.16375504434108734
train_iter_loss: 0.046146027743816376
train_iter_loss: 0.11963965743780136
train_iter_loss: 0.12150639295578003
train_iter_loss: 0.08854104578495026
train_iter_loss: 0.17132407426834106
train_iter_loss: 0.17863479256629944
train_iter_loss: 0.15827885270118713
train_iter_loss: 0.1540287584066391
train_iter_loss: 0.1373046338558197
train_iter_loss: 0.11833293735980988
train_iter_loss: 0.13450616598129272
train_iter_loss: 0.1626562476158142
train_iter_loss: 0.1615595519542694
train_iter_loss: 0.2377171665430069
train_iter_loss: 0.11410186439752579
train_iter_loss: 0.08359698951244354
train_iter_loss: 0.29773804545402527
train_iter_loss: 0.06524495780467987
train_iter_loss: 0.25260353088378906
train_iter_loss: 0.1298668533563614
train_iter_loss: 0.11325708031654358
train_iter_loss: 0.16272756457328796
train_iter_loss: 0.0679469108581543
train_iter_loss: 0.16301663219928741
train_iter_loss: 0.16359306871891022
train_iter_loss: 0.19563108682632446
train_iter_loss: 0.20587144792079926
train_iter_loss: 0.1134335994720459
train_iter_loss: 0.07594732195138931
train_iter_loss: 0.08957071602344513
train_iter_loss: 0.10654877126216888
train_iter_loss: 0.1684061884880066
train_iter_loss: 0.1195714920759201
train_iter_loss: 0.22725260257720947
train_iter_loss: 0.12179027497768402
train_iter_loss: 0.12747351825237274
train_iter_loss: 0.16213101148605347
train_iter_loss: 0.14510980248451233
train loss :0.1556
---------------------
Validation seg loss: 0.21816005173824587 at epoch 836
epoch =    837/  1000, exp = train
train_iter_loss: 0.28128713369369507
train_iter_loss: 0.1105915755033493
train_iter_loss: 0.18011873960494995
train_iter_loss: 0.21465809643268585
train_iter_loss: 0.10929039120674133
train_iter_loss: 0.06273648142814636
train_iter_loss: 0.12111283093690872
train_iter_loss: 0.1819135546684265
train_iter_loss: 0.231293722987175
train_iter_loss: 0.12869712710380554
train_iter_loss: 0.21997188031673431
train_iter_loss: 0.07861892133951187
train_iter_loss: 0.15871258080005646
train_iter_loss: 0.11893633008003235
train_iter_loss: 0.1368216723203659
train_iter_loss: 0.08951724320650101
train_iter_loss: 0.11636696755886078
train_iter_loss: 0.28425824642181396
train_iter_loss: 0.10082191228866577
train_iter_loss: 0.1419404149055481
train_iter_loss: 0.13343435525894165
train_iter_loss: 0.1280273050069809
train_iter_loss: 0.09574982523918152
train_iter_loss: 0.15886668860912323
train_iter_loss: 0.15529362857341766
train_iter_loss: 0.32085728645324707
train_iter_loss: 0.15797175467014313
train_iter_loss: 0.14417797327041626
train_iter_loss: 0.059334468096494675
train_iter_loss: 0.12735900282859802
train_iter_loss: 0.4991435110569
train_iter_loss: 0.0720834881067276
train_iter_loss: 0.18716542422771454
train_iter_loss: 0.15499094128608704
train_iter_loss: 0.05880909785628319
train_iter_loss: 0.19747234880924225
train_iter_loss: 0.31556418538093567
train_iter_loss: 0.11617836356163025
train_iter_loss: 0.14118340611457825
train_iter_loss: 0.14324067533016205
train_iter_loss: 0.1346978396177292
train_iter_loss: 0.10085821151733398
train_iter_loss: 0.06938939541578293
train_iter_loss: 0.11874152719974518
train_iter_loss: 0.33237138390541077
train_iter_loss: 0.40510720014572144
train_iter_loss: 0.26289641857147217
train_iter_loss: 0.11155007779598236
train_iter_loss: 0.17058990895748138
train_iter_loss: 0.11162615567445755
train_iter_loss: 0.1389630138874054
train_iter_loss: 0.1451290398836136
train_iter_loss: 0.1134428158402443
train_iter_loss: 0.13138557970523834
train_iter_loss: 0.10246079415082932
train_iter_loss: 0.17986775934696198
train_iter_loss: 0.25449028611183167
train_iter_loss: 0.2913765013217926
train_iter_loss: 0.13762983679771423
train_iter_loss: 0.11012955754995346
train_iter_loss: 0.21956278383731842
train_iter_loss: 0.10888803005218506
train_iter_loss: 0.16323190927505493
train_iter_loss: 0.11304015666246414
train_iter_loss: 0.15805314481258392
train_iter_loss: 0.12091983109712601
train_iter_loss: 0.10206500440835953
train_iter_loss: 0.11922624707221985
train_iter_loss: 0.049869898706674576
train_iter_loss: 0.10216815024614334
train_iter_loss: 0.08770295232534409
train_iter_loss: 0.2137160748243332
train_iter_loss: 0.2148842215538025
train_iter_loss: 0.14978058636188507
train_iter_loss: 0.15488441288471222
train_iter_loss: 0.1357150375843048
train_iter_loss: 0.17860712110996246
train_iter_loss: 0.24835442006587982
train_iter_loss: 0.26666709780693054
train_iter_loss: 0.3307018280029297
train_iter_loss: 0.12373653054237366
train_iter_loss: 0.1439557820558548
train_iter_loss: 0.08038367331027985
train_iter_loss: 0.19364066421985626
train_iter_loss: 0.2164863795042038
train_iter_loss: 0.10603540390729904
train_iter_loss: 0.13828109204769135
train_iter_loss: 0.14905127882957458
train_iter_loss: 0.20408329367637634
train_iter_loss: 0.09456247091293335
train_iter_loss: 0.08457648754119873
train_iter_loss: 0.1717325747013092
train_iter_loss: 0.09461744129657745
train_iter_loss: 0.08675303310155869
train_iter_loss: 0.08790424466133118
train_iter_loss: 0.0895451158285141
train_iter_loss: 0.1731317788362503
train_iter_loss: 0.13141418993473053
train_iter_loss: 0.14125658571720123
train_iter_loss: 0.17931562662124634
train loss :0.1594
---------------------
Validation seg loss: 0.2159616609262127 at epoch 837
epoch =    838/  1000, exp = train
train_iter_loss: 0.1182822436094284
train_iter_loss: 0.09715913236141205
train_iter_loss: 0.14738622307777405
train_iter_loss: 0.15442980825901031
train_iter_loss: 0.18158726394176483
train_iter_loss: 0.01992863230407238
train_iter_loss: 0.1162772849202156
train_iter_loss: 0.2628900408744812
train_iter_loss: 0.19935430586338043
train_iter_loss: 0.14857830107212067
train_iter_loss: 0.21363519132137299
train_iter_loss: 0.06628620624542236
train_iter_loss: 0.23027347028255463
train_iter_loss: 0.12423594295978546
train_iter_loss: 0.10577224940061569
train_iter_loss: 0.2291509062051773
train_iter_loss: 0.10464586317539215
train_iter_loss: 0.1138310506939888
train_iter_loss: 0.1701499968767166
train_iter_loss: 0.10055773705244064
train_iter_loss: 0.14409707486629486
train_iter_loss: 0.22224955260753632
train_iter_loss: 0.3048022389411926
train_iter_loss: 0.09236674755811691
train_iter_loss: 0.08171749114990234
train_iter_loss: 0.19336916506290436
train_iter_loss: 0.1782044619321823
train_iter_loss: 0.27035319805145264
train_iter_loss: 0.1629778891801834
train_iter_loss: 0.08916375786066055
train_iter_loss: 0.21612639725208282
train_iter_loss: 0.10247502475976944
train_iter_loss: 0.10412686318159103
train_iter_loss: 0.18195365369319916
train_iter_loss: 0.1808956116437912
train_iter_loss: 0.25599366426467896
train_iter_loss: 0.19781376421451569
train_iter_loss: 0.21672454476356506
train_iter_loss: 0.12172533571720123
train_iter_loss: 0.2756135165691376
train_iter_loss: 0.18177597224712372
train_iter_loss: 0.11729411035776138
train_iter_loss: 0.22762399911880493
train_iter_loss: 0.2186342030763626
train_iter_loss: 0.14122161269187927
train_iter_loss: 0.1690877377986908
train_iter_loss: 0.05578801780939102
train_iter_loss: 0.11177538335323334
train_iter_loss: 0.17037315666675568
train_iter_loss: 0.10967130213975906
train_iter_loss: 0.07139230519533157
train_iter_loss: 0.14756076037883759
train_iter_loss: 0.17530617117881775
train_iter_loss: 0.31462806463241577
train_iter_loss: 0.11562042683362961
train_iter_loss: 0.07333346456289291
train_iter_loss: 0.15024025738239288
train_iter_loss: 0.11464415490627289
train_iter_loss: 0.06352243572473526
train_iter_loss: 0.08693117648363113
train_iter_loss: 0.15268854796886444
train_iter_loss: 0.17283426225185394
train_iter_loss: 0.25705257058143616
train_iter_loss: 0.09933348745107651
train_iter_loss: 0.11327097564935684
train_iter_loss: 0.1441396176815033
train_iter_loss: 0.11816195398569107
train_iter_loss: 0.31705242395401
train_iter_loss: 0.08064422011375427
train_iter_loss: 0.12345913797616959
train_iter_loss: 0.1393040418624878
train_iter_loss: 0.19833651185035706
train_iter_loss: 0.08802204579114914
train_iter_loss: 0.14514856040477753
train_iter_loss: 0.08343505859375
train_iter_loss: 0.16768868267536163
train_iter_loss: 0.18934176862239838
train_iter_loss: 0.13051894307136536
train_iter_loss: 0.2704409956932068
train_iter_loss: 0.18986156582832336
train_iter_loss: 0.11725346744060516
train_iter_loss: 0.20395225286483765
train_iter_loss: 0.17307327687740326
train_iter_loss: 0.1922854483127594
train_iter_loss: 0.16163811087608337
train_iter_loss: 0.14783300459384918
train_iter_loss: 0.07172688841819763
train_iter_loss: 0.07116428017616272
train_iter_loss: 0.1338951587677002
train_iter_loss: 0.08058124035596848
train_iter_loss: 0.13953673839569092
train_iter_loss: 0.15458561480045319
train_iter_loss: 0.1915023773908615
train_iter_loss: 0.11263211071491241
train_iter_loss: 0.13119693100452423
train_iter_loss: 0.19622963666915894
train_iter_loss: 0.044564180076122284
train_iter_loss: 0.22594033181667328
train_iter_loss: 0.08360669016838074
train_iter_loss: 0.15385161340236664
train loss :0.1544
---------------------
Validation seg loss: 0.21165625664915116 at epoch 838
epoch =    839/  1000, exp = train
train_iter_loss: 0.22849228978157043
train_iter_loss: 0.14815214276313782
train_iter_loss: 0.12608970701694489
train_iter_loss: 0.1567579209804535
train_iter_loss: 0.10941916704177856
train_iter_loss: 0.34125301241874695
train_iter_loss: 0.10583384335041046
train_iter_loss: 0.36657360196113586
train_iter_loss: 0.16310489177703857
train_iter_loss: 0.18258586525917053
train_iter_loss: 0.1650988757610321
train_iter_loss: 0.19580109417438507
train_iter_loss: 0.13159410655498505
train_iter_loss: 0.1889052540063858
train_iter_loss: 0.09729526937007904
train_iter_loss: 0.14012444019317627
train_iter_loss: 0.18928755819797516
train_iter_loss: 0.12769533693790436
train_iter_loss: 0.12745104730129242
train_iter_loss: 0.1957504004240036
train_iter_loss: 0.1700059026479721
train_iter_loss: 0.1775233894586563
train_iter_loss: 0.14197924733161926
train_iter_loss: 0.24717654287815094
train_iter_loss: 0.17030103504657745
train_iter_loss: 0.14107584953308105
train_iter_loss: 0.12938761711120605
train_iter_loss: 0.10087469965219498
train_iter_loss: 0.27858594059944153
train_iter_loss: 0.1756838858127594
train_iter_loss: 0.11486473679542542
train_iter_loss: 0.2106095552444458
train_iter_loss: 0.20158417522907257
train_iter_loss: 0.13514789938926697
train_iter_loss: 0.18458257615566254
train_iter_loss: 0.21355360746383667
train_iter_loss: 0.11678725481033325
train_iter_loss: 0.14586320519447327
train_iter_loss: 0.32276225090026855
train_iter_loss: 0.119553342461586
train_iter_loss: 0.12973026931285858
train_iter_loss: 0.12385164946317673
train_iter_loss: 0.19658245146274567
train_iter_loss: 0.22353406250476837
train_iter_loss: 0.17322072386741638
train_iter_loss: 0.10429759323596954
train_iter_loss: 0.09857885539531708
train_iter_loss: 0.12100202590227127
train_iter_loss: 0.145116925239563
train_iter_loss: 0.16875611245632172
train_iter_loss: 0.13545915484428406
train_iter_loss: 0.18711237609386444
train_iter_loss: 0.12259843200445175
train_iter_loss: 0.13418686389923096
train_iter_loss: 0.11640343070030212
train_iter_loss: 0.09476044774055481
train_iter_loss: 0.15345324575901031
train_iter_loss: 0.1862073689699173
train_iter_loss: 0.11246147006750107
train_iter_loss: 0.15991418063640594
train_iter_loss: 0.13101914525032043
train_iter_loss: 0.06424711644649506
train_iter_loss: 0.16772279143333435
train_iter_loss: 0.09922880679368973
train_iter_loss: 0.08250253647565842
train_iter_loss: 0.1329379379749298
train_iter_loss: 0.20034927129745483
train_iter_loss: 0.14183799922466278
train_iter_loss: 0.1888115406036377
train_iter_loss: 0.15271876752376556
train_iter_loss: 0.17271584272384644
train_iter_loss: 0.18455550074577332
train_iter_loss: 0.10809649527072906
train_iter_loss: 0.2055000513792038
train_iter_loss: 0.05808958783745766
train_iter_loss: 0.07120626419782639
train_iter_loss: 0.09582389891147614
train_iter_loss: 0.16662834584712982
train_iter_loss: 0.12158966064453125
train_iter_loss: 0.12781685590744019
train_iter_loss: 0.17898106575012207
train_iter_loss: 0.13339656591415405
train_iter_loss: 0.2015594094991684
train_iter_loss: 0.131136953830719
train_iter_loss: 0.1059393510222435
train_iter_loss: 0.22750712931156158
train_iter_loss: 0.1606549322605133
train_iter_loss: 0.12530307471752167
train_iter_loss: 0.2414190024137497
train_iter_loss: 0.17707255482673645
train_iter_loss: 0.11231866478919983
train_iter_loss: 0.14339157938957214
train_iter_loss: 0.08574777096509933
train_iter_loss: 0.09464213252067566
train_iter_loss: 0.19092178344726562
train_iter_loss: 0.24505560100078583
train_iter_loss: 0.10051799565553665
train_iter_loss: 0.22160419821739197
train_iter_loss: 0.1087266057729721
train_iter_loss: 0.16777142882347107
train loss :0.1585
---------------------
Validation seg loss: 0.21630924994582837 at epoch 839
epoch =    840/  1000, exp = train
train_iter_loss: 0.11640218645334244
train_iter_loss: 0.16138970851898193
train_iter_loss: 0.16748745739459991
train_iter_loss: 0.06994389742612839
train_iter_loss: 0.12547753751277924
train_iter_loss: 0.2242368757724762
train_iter_loss: 0.10150330513715744
train_iter_loss: 0.124546118080616
train_iter_loss: 0.19416768848896027
train_iter_loss: 0.1568976193666458
train_iter_loss: 0.17649361491203308
train_iter_loss: 0.2018933743238449
train_iter_loss: 0.22349613904953003
train_iter_loss: 0.1933113932609558
train_iter_loss: 0.18873649835586548
train_iter_loss: 0.18865855038166046
train_iter_loss: 0.1934012621641159
train_iter_loss: 0.11815383285284042
train_iter_loss: 0.18439054489135742
train_iter_loss: 0.20253922045230865
train_iter_loss: 0.14222346246242523
train_iter_loss: 0.08292333036661148
train_iter_loss: 0.17631059885025024
train_iter_loss: 0.0685196965932846
train_iter_loss: 0.18445731699466705
train_iter_loss: 0.1731988787651062
train_iter_loss: 0.27325761318206787
train_iter_loss: 0.14216181635856628
train_iter_loss: 0.09921588748693466
train_iter_loss: 0.07571690529584885
train_iter_loss: 0.10887839645147324
train_iter_loss: 0.09153643995523453
train_iter_loss: 0.15953992307186127
train_iter_loss: 0.10111905634403229
train_iter_loss: 0.17270871996879578
train_iter_loss: 0.07913989573717117
train_iter_loss: 0.13427147269248962
train_iter_loss: 0.24378710985183716
train_iter_loss: 0.1535223424434662
train_iter_loss: 0.22973880171775818
train_iter_loss: 0.14518111944198608
train_iter_loss: 0.1503262221813202
train_iter_loss: 0.2454884946346283
train_iter_loss: 0.19420669972896576
train_iter_loss: 0.18318608403205872
train_iter_loss: 0.08086660504341125
train_iter_loss: 0.10008681565523148
train_iter_loss: 0.13157835602760315
train_iter_loss: 0.2210274636745453
train_iter_loss: 0.10711297392845154
train_iter_loss: 0.13957583904266357
train_iter_loss: 0.06416753679513931
train_iter_loss: 0.13599921762943268
train_iter_loss: 0.17219965159893036
train_iter_loss: 0.14621463418006897
train_iter_loss: 0.10721120983362198
train_iter_loss: 0.10828869789838791
train_iter_loss: 0.10329878330230713
train_iter_loss: 0.11870692670345306
train_iter_loss: 0.09002009779214859
train_iter_loss: 0.07521161437034607
train_iter_loss: 0.21579137444496155
train_iter_loss: 0.08810467272996902
train_iter_loss: 0.15377283096313477
train_iter_loss: 0.12093915045261383
train_iter_loss: 0.20206214487552643
train_iter_loss: 0.12659847736358643
train_iter_loss: 0.15880852937698364
train_iter_loss: 0.1619177609682083
train_iter_loss: 0.2076469361782074
train_iter_loss: 0.164993017911911
train_iter_loss: 0.3186708092689514
train_iter_loss: 0.18316802382469177
train_iter_loss: 0.07393760234117508
train_iter_loss: 0.10893430560827255
train_iter_loss: 0.18634304404258728
train_iter_loss: 0.10707500576972961
train_iter_loss: 0.11685837805271149
train_iter_loss: 0.08434712141752243
train_iter_loss: 0.16712959110736847
train_iter_loss: 0.13198676705360413
train_iter_loss: 0.2913416624069214
train_iter_loss: 0.14445289969444275
train_iter_loss: 0.12623193860054016
train_iter_loss: 0.1817903220653534
train_iter_loss: 0.15702007710933685
train_iter_loss: 0.3157157301902771
train_iter_loss: 0.1500217169523239
train_iter_loss: 0.1582856923341751
train_iter_loss: 0.19884493947029114
train_iter_loss: 0.1664789915084839
train_iter_loss: 0.10646092891693115
train_iter_loss: 0.24430115520954132
train_iter_loss: 0.15847627818584442
train_iter_loss: 0.09039554744958878
train_iter_loss: 0.21917079389095306
train_iter_loss: 0.10366398841142654
train_iter_loss: 0.18122142553329468
train_iter_loss: 0.17186811566352844
train_iter_loss: 0.11690624803304672
train loss :0.1555
---------------------
Validation seg loss: 0.21893374660526807 at epoch 840
epoch =    841/  1000, exp = train
train_iter_loss: 0.12373007088899612
train_iter_loss: 0.0803021788597107
train_iter_loss: 0.22336775064468384
train_iter_loss: 0.09054914116859436
train_iter_loss: 0.10605822503566742
train_iter_loss: 0.11621817201375961
train_iter_loss: 0.1082896813750267
train_iter_loss: 0.10848669707775116
train_iter_loss: 0.06231820210814476
train_iter_loss: 0.09129947423934937
train_iter_loss: 0.10780137032270432
train_iter_loss: 0.16677968204021454
train_iter_loss: 0.20568129420280457
train_iter_loss: 0.09933800995349884
train_iter_loss: 0.13579817116260529
train_iter_loss: 0.1560041457414627
train_iter_loss: 0.14562542736530304
train_iter_loss: 0.07301260530948639
train_iter_loss: 0.20169468224048615
train_iter_loss: 0.07759752124547958
train_iter_loss: 0.11234980076551437
train_iter_loss: 0.24025720357894897
train_iter_loss: 0.20902743935585022
train_iter_loss: 0.0891871452331543
train_iter_loss: 0.06782495975494385
train_iter_loss: 0.21198682487010956
train_iter_loss: 0.22306399047374725
train_iter_loss: 0.15451085567474365
train_iter_loss: 0.12715956568717957
train_iter_loss: 0.19197633862495422
train_iter_loss: 0.1594424992799759
train_iter_loss: 0.18395838141441345
train_iter_loss: 0.14991503953933716
train_iter_loss: 0.18527884781360626
train_iter_loss: 0.19814594089984894
train_iter_loss: 0.14555683732032776
train_iter_loss: 0.13735049962997437
train_iter_loss: 0.11059823632240295
train_iter_loss: 0.19087444245815277
train_iter_loss: 0.15623411536216736
train_iter_loss: 0.15003393590450287
train_iter_loss: 0.07628029584884644
train_iter_loss: 0.2860950827598572
train_iter_loss: 0.29168593883514404
train_iter_loss: 0.14372959733009338
train_iter_loss: 0.16789941489696503
train_iter_loss: 0.20211781561374664
train_iter_loss: 0.2768043279647827
train_iter_loss: 0.1254180520772934
train_iter_loss: 0.17550207674503326
train_iter_loss: 0.14701877534389496
train_iter_loss: 0.04840714484453201
train_iter_loss: 0.16129577159881592
train_iter_loss: 0.19248279929161072
train_iter_loss: 0.11194667220115662
train_iter_loss: 0.12302510440349579
train_iter_loss: 0.16971230506896973
train_iter_loss: 0.18254025280475616
train_iter_loss: 0.15553390979766846
train_iter_loss: 0.20542867481708527
train_iter_loss: 0.1700121909379959
train_iter_loss: 0.1443108171224594
train_iter_loss: 0.04765217378735542
train_iter_loss: 0.14710476994514465
train_iter_loss: 0.13146792352199554
train_iter_loss: 0.11309970915317535
train_iter_loss: 0.22841902077198029
train_iter_loss: 0.3405095934867859
train_iter_loss: 0.2312394231557846
train_iter_loss: 0.11718550324440002
train_iter_loss: 0.11240687966346741
train_iter_loss: 0.15564703941345215
train_iter_loss: 0.1759960651397705
train_iter_loss: 0.27172520756721497
train_iter_loss: 0.12513229250907898
train_iter_loss: 0.10575525462627411
train_iter_loss: 0.19811342656612396
train_iter_loss: 0.10876882076263428
train_iter_loss: 0.20876283943653107
train_iter_loss: 0.2743227481842041
train_iter_loss: 0.14452572166919708
train_iter_loss: 0.2621793746948242
train_iter_loss: 0.21530145406723022
train_iter_loss: 0.18522706627845764
train_iter_loss: 0.059318047016859055
train_iter_loss: 0.12484782934188843
train_iter_loss: 0.0903942883014679
train_iter_loss: 0.13648073375225067
train_iter_loss: 0.13708223402500153
train_iter_loss: 0.19849072396755219
train_iter_loss: 0.16126316785812378
train_iter_loss: 0.14757686853408813
train_iter_loss: 0.12181763350963593
train_iter_loss: 0.14442239701747894
train_iter_loss: 0.15916968882083893
train_iter_loss: 0.1705615222454071
train_iter_loss: 0.07682421803474426
train_iter_loss: 0.20059816539287567
train_iter_loss: 0.16280235350131989
train_iter_loss: 0.16960851848125458
train loss :0.1575
---------------------
Validation seg loss: 0.2124458255140849 at epoch 841
epoch =    842/  1000, exp = train
train_iter_loss: 0.18411386013031006
train_iter_loss: 0.1926111876964569
train_iter_loss: 0.19933511316776276
train_iter_loss: 0.10185768455266953
train_iter_loss: 0.12534339725971222
train_iter_loss: 0.2225208580493927
train_iter_loss: 0.1217554584145546
train_iter_loss: 0.10746991634368896
train_iter_loss: 0.15957511961460114
train_iter_loss: 0.24000954627990723
train_iter_loss: 0.28315404057502747
train_iter_loss: 0.15442371368408203
train_iter_loss: 0.09076017886400223
train_iter_loss: 0.14369647204875946
train_iter_loss: 0.11227157711982727
train_iter_loss: 0.042590249329805374
train_iter_loss: 0.10485809296369553
train_iter_loss: 0.051182910799980164
train_iter_loss: 0.1423395574092865
train_iter_loss: 0.349150151014328
train_iter_loss: 0.206825390458107
train_iter_loss: 0.21387748420238495
train_iter_loss: 0.11404208838939667
train_iter_loss: 0.2775709927082062
train_iter_loss: 0.15132981538772583
train_iter_loss: 0.16953086853027344
train_iter_loss: 0.16013416647911072
train_iter_loss: 0.1946619600057602
train_iter_loss: 0.11798489838838577
train_iter_loss: 0.0986914411187172
train_iter_loss: 0.22263909876346588
train_iter_loss: 0.19009000062942505
train_iter_loss: 0.11310194432735443
train_iter_loss: 0.2668275833129883
train_iter_loss: 0.2192659080028534
train_iter_loss: 0.268954873085022
train_iter_loss: 0.146243155002594
train_iter_loss: 0.08920377492904663
train_iter_loss: 0.10373862087726593
train_iter_loss: 0.09932523965835571
train_iter_loss: 0.11782195419073105
train_iter_loss: 0.07362831383943558
train_iter_loss: 0.1625419408082962
train_iter_loss: 0.06334317475557327
train_iter_loss: 0.06371766328811646
train_iter_loss: 0.1406295895576477
train_iter_loss: 0.106797955930233
train_iter_loss: 0.13280771672725677
train_iter_loss: 0.09928392618894577
train_iter_loss: 0.13532090187072754
train_iter_loss: 0.14556638896465302
train_iter_loss: 0.16857530176639557
train_iter_loss: 0.11793754249811172
train_iter_loss: 0.10106699168682098
train_iter_loss: 0.18924115598201752
train_iter_loss: 0.16579648852348328
train_iter_loss: 0.12959393858909607
train_iter_loss: 0.119620680809021
train_iter_loss: 0.23747518658638
train_iter_loss: 0.0873180627822876
train_iter_loss: 0.1011451929807663
train_iter_loss: 0.18407002091407776
train_iter_loss: 0.12840238213539124
train_iter_loss: 0.10934867709875107
train_iter_loss: 0.32198238372802734
train_iter_loss: 0.0935463160276413
train_iter_loss: 0.15839536488056183
train_iter_loss: 0.17753641307353973
train_iter_loss: 0.08766037970781326
train_iter_loss: 0.1993006318807602
train_iter_loss: 0.12159783393144608
train_iter_loss: 0.23110084235668182
train_iter_loss: 0.19647112488746643
train_iter_loss: 0.11503607034683228
train_iter_loss: 0.1738077700138092
train_iter_loss: 0.12093757092952728
train_iter_loss: 0.26288920640945435
train_iter_loss: 0.24316422641277313
train_iter_loss: 0.06770728528499603
train_iter_loss: 0.08232831954956055
train_iter_loss: 0.18469808995723724
train_iter_loss: 0.2943129539489746
train_iter_loss: 0.1275421679019928
train_iter_loss: 0.20774154365062714
train_iter_loss: 0.30752649903297424
train_iter_loss: 0.0956660732626915
train_iter_loss: 0.26274481415748596
train_iter_loss: 0.22578933835029602
train_iter_loss: 0.15300260484218597
train_iter_loss: 0.1748495101928711
train_iter_loss: 0.22126828134059906
train_iter_loss: 0.1069641187787056
train_iter_loss: 0.1463557332754135
train_iter_loss: 0.19780384004116058
train_iter_loss: 0.22426162660121918
train_iter_loss: 0.2277979701757431
train_iter_loss: 0.13167212903499603
train_iter_loss: 0.12661989033222198
train_iter_loss: 0.16381928324699402
train_iter_loss: 0.08315525203943253
train loss :0.1611
---------------------
Validation seg loss: 0.22143713703800766 at epoch 842
epoch =    843/  1000, exp = train
train_iter_loss: 0.1876337081193924
train_iter_loss: 0.18841978907585144
train_iter_loss: 0.3071947395801544
train_iter_loss: 0.12839490175247192
train_iter_loss: 0.17869091033935547
train_iter_loss: 0.1554507315158844
train_iter_loss: 0.22096498310565948
train_iter_loss: 0.21274466812610626
train_iter_loss: 0.16955167055130005
train_iter_loss: 0.16417790949344635
train_iter_loss: 0.13948042690753937
train_iter_loss: 0.2615063190460205
train_iter_loss: 0.16555804014205933
train_iter_loss: 0.22370374202728271
train_iter_loss: 0.08627471327781677
train_iter_loss: 0.08437556773424149
train_iter_loss: 0.12208005785942078
train_iter_loss: 0.16015483438968658
train_iter_loss: 0.05592113360762596
train_iter_loss: 0.048235516995191574
train_iter_loss: 0.08557900786399841
train_iter_loss: 0.12490098923444748
train_iter_loss: 0.11091583222150803
train_iter_loss: 0.20777451992034912
train_iter_loss: 0.1555989533662796
train_iter_loss: 0.11795660108327866
train_iter_loss: 0.20702797174453735
train_iter_loss: 0.053871039301157
train_iter_loss: 0.16848504543304443
train_iter_loss: 0.14370305836200714
train_iter_loss: 0.1504020094871521
train_iter_loss: 0.13913483917713165
train_iter_loss: 0.09546807408332825
train_iter_loss: 0.12468082457780838
train_iter_loss: 0.12264449894428253
train_iter_loss: 0.15129444003105164
train_iter_loss: 0.10237225145101547
train_iter_loss: 0.18972496688365936
train_iter_loss: 0.19248512387275696
train_iter_loss: 0.10594062507152557
train_iter_loss: 0.16337880492210388
train_iter_loss: 0.09898139536380768
train_iter_loss: 0.194889098405838
train_iter_loss: 0.1467117965221405
train_iter_loss: 0.15482747554779053
train_iter_loss: 0.14491020143032074
train_iter_loss: 0.1387171447277069
train_iter_loss: 0.0835796594619751
train_iter_loss: 0.1622937172651291
train_iter_loss: 0.21402575075626373
train_iter_loss: 0.09059296548366547
train_iter_loss: 0.10454012453556061
train_iter_loss: 0.02908511273562908
train_iter_loss: 0.18950213491916656
train_iter_loss: 0.21045882999897003
train_iter_loss: 0.20483198761940002
train_iter_loss: 0.13825300335884094
train_iter_loss: 0.24919822812080383
train_iter_loss: 0.07966157048940659
train_iter_loss: 0.1979646533727646
train_iter_loss: 0.11175896972417831
train_iter_loss: 0.2192872166633606
train_iter_loss: 0.2327560931444168
train_iter_loss: 0.1403796374797821
train_iter_loss: 0.3191066086292267
train_iter_loss: 0.10880293697118759
train_iter_loss: 0.1299070566892624
train_iter_loss: 0.142155721783638
train_iter_loss: 0.16624940931797028
train_iter_loss: 0.13573315739631653
train_iter_loss: 0.184668630361557
train_iter_loss: 0.2779357433319092
train_iter_loss: 0.09125686436891556
train_iter_loss: 0.07150980830192566
train_iter_loss: 0.16701284050941467
train_iter_loss: 0.08871793746948242
train_iter_loss: 0.13812357187271118
train_iter_loss: 0.17073063552379608
train_iter_loss: 0.052092067897319794
train_iter_loss: 0.19667424261569977
train_iter_loss: 0.22336983680725098
train_iter_loss: 0.07350672036409378
train_iter_loss: 0.20011526346206665
train_iter_loss: 0.18046452105045319
train_iter_loss: 0.12159004807472229
train_iter_loss: 0.1472676396369934
train_iter_loss: 0.35421082377433777
train_iter_loss: 0.09040171653032303
train_iter_loss: 0.10664625465869904
train_iter_loss: 0.17409522831439972
train_iter_loss: 0.08912593126296997
train_iter_loss: 0.22807538509368896
train_iter_loss: 0.21484248340129852
train_iter_loss: 0.07687339186668396
train_iter_loss: 0.10873871296644211
train_iter_loss: 0.12852352857589722
train_iter_loss: 0.11236563324928284
train_iter_loss: 0.14275777339935303
train_iter_loss: 0.1348399519920349
train_iter_loss: 0.11190865933895111
train loss :0.1533
---------------------
Validation seg loss: 0.2164716706171913 at epoch 843
epoch =    844/  1000, exp = train
train_iter_loss: 0.18839876353740692
train_iter_loss: 0.23164640367031097
train_iter_loss: 0.16598019003868103
train_iter_loss: 0.12539789080619812
train_iter_loss: 0.18893419206142426
train_iter_loss: 0.24444818496704102
train_iter_loss: 0.23356521129608154
train_iter_loss: 0.15548968315124512
train_iter_loss: 0.2904779314994812
train_iter_loss: 0.08374922722578049
train_iter_loss: 0.21673452854156494
train_iter_loss: 0.2091263234615326
train_iter_loss: 0.24704957008361816
train_iter_loss: 0.18961286544799805
train_iter_loss: 0.16210292279720306
train_iter_loss: 0.11183017492294312
train_iter_loss: 0.35491082072257996
train_iter_loss: 0.1045650839805603
train_iter_loss: 0.12554119527339935
train_iter_loss: 0.10547100752592087
train_iter_loss: 0.10151074081659317
train_iter_loss: 0.16738823056221008
train_iter_loss: 0.1683494746685028
train_iter_loss: 0.13204149901866913
train_iter_loss: 0.059871118515729904
train_iter_loss: 0.050186365842819214
train_iter_loss: 0.12807999551296234
train_iter_loss: 0.14361004531383514
train_iter_loss: 0.10991087555885315
train_iter_loss: 0.14786194264888763
train_iter_loss: 0.21611762046813965
train_iter_loss: 0.15936337411403656
train_iter_loss: 0.24344882369041443
train_iter_loss: 0.1683797985315323
train_iter_loss: 0.152798131108284
train_iter_loss: 0.14031055569648743
train_iter_loss: 0.10447989404201508
train_iter_loss: 0.16309639811515808
train_iter_loss: 0.33559709787368774
train_iter_loss: 0.23159514367580414
train_iter_loss: 0.11797897517681122
train_iter_loss: 0.20383740961551666
train_iter_loss: 0.06747874617576599
train_iter_loss: 0.13934223353862762
train_iter_loss: 0.16428515315055847
train_iter_loss: 0.4123379588127136
train_iter_loss: 0.10136367380619049
train_iter_loss: 0.06616401672363281
train_iter_loss: 0.17080050706863403
train_iter_loss: 0.10375829041004181
train_iter_loss: 0.1976792961359024
train_iter_loss: 0.08025437593460083
train_iter_loss: 0.14237633347511292
train_iter_loss: 0.1237470880150795
train_iter_loss: 0.03011925145983696
train_iter_loss: 0.14698781073093414
train_iter_loss: 0.06802063435316086
train_iter_loss: 0.15913890302181244
train_iter_loss: 0.12535351514816284
train_iter_loss: 0.2253246158361435
train_iter_loss: 0.23675550520420074
train_iter_loss: 0.11312172561883926
train_iter_loss: 0.11647170782089233
train_iter_loss: 0.09475230425596237
train_iter_loss: 0.1461942195892334
train_iter_loss: 0.13309061527252197
train_iter_loss: 0.20728248357772827
train_iter_loss: 0.1285841017961502
train_iter_loss: 0.16514474153518677
train_iter_loss: 0.1625543087720871
train_iter_loss: 0.12334807217121124
train_iter_loss: 0.14835646748542786
train_iter_loss: 0.1339869350194931
train_iter_loss: 0.165890634059906
train_iter_loss: 0.12725213170051575
train_iter_loss: 0.13238176703453064
train_iter_loss: 0.08107395470142365
train_iter_loss: 0.11776486039161682
train_iter_loss: 0.16982749104499817
train_iter_loss: 0.13604408502578735
train_iter_loss: 0.07787266373634338
train_iter_loss: 0.06311821192502975
train_iter_loss: 0.18967778980731964
train_iter_loss: 0.10289417207241058
train_iter_loss: 0.30048564076423645
train_iter_loss: 0.14469429850578308
train_iter_loss: 0.01599053107202053
train_iter_loss: 0.11132772266864777
train_iter_loss: 0.17992408573627472
train_iter_loss: 0.2429674118757248
train_iter_loss: 0.07526638358831406
train_iter_loss: 0.1618492156267166
train_iter_loss: 0.17773935198783875
train_iter_loss: 0.12098678201436996
train_iter_loss: 0.13556431233882904
train_iter_loss: 0.10617911070585251
train_iter_loss: 0.17989273369312286
train_iter_loss: 0.09287773817777634
train_iter_loss: 0.06024910509586334
train_iter_loss: 0.134438157081604
train loss :0.1537
---------------------
Validation seg loss: 0.2136855428282804 at epoch 844
epoch =    845/  1000, exp = train
train_iter_loss: 0.10994137078523636
train_iter_loss: 0.09763848036527634
train_iter_loss: 0.10760437697172165
train_iter_loss: 0.13601836562156677
train_iter_loss: 0.097784124314785
train_iter_loss: 0.15850995481014252
train_iter_loss: 0.08942294865846634
train_iter_loss: 0.17992143332958221
train_iter_loss: 0.10921541601419449
train_iter_loss: 0.15800870954990387
train_iter_loss: 0.15189982950687408
train_iter_loss: 0.16229493916034698
train_iter_loss: 0.16410957276821136
train_iter_loss: 0.08966676890850067
train_iter_loss: 0.10080856829881668
train_iter_loss: 0.2759382426738739
train_iter_loss: 0.17918772995471954
train_iter_loss: 0.027634410187602043
train_iter_loss: 0.1726573407649994
train_iter_loss: 0.05846106633543968
train_iter_loss: 0.10882098227739334
train_iter_loss: 0.10222776234149933
train_iter_loss: 0.16967305541038513
train_iter_loss: 0.2465926557779312
train_iter_loss: 0.058239273726940155
train_iter_loss: 0.17983189225196838
train_iter_loss: 0.0908871442079544
train_iter_loss: 0.2790151536464691
train_iter_loss: 0.23450495302677155
train_iter_loss: 0.09309825301170349
train_iter_loss: 0.16085165739059448
train_iter_loss: 0.1745288223028183
train_iter_loss: 0.19587258994579315
train_iter_loss: 0.15562446415424347
train_iter_loss: 0.20289058983325958
train_iter_loss: 0.17985431849956512
train_iter_loss: 0.14792855083942413
train_iter_loss: 0.09644687175750732
train_iter_loss: 0.08519679307937622
train_iter_loss: 0.14406058192253113
train_iter_loss: 0.29172372817993164
train_iter_loss: 0.04436968266963959
train_iter_loss: 0.15498939156532288
train_iter_loss: 0.25607603788375854
train_iter_loss: 0.15023688971996307
train_iter_loss: 0.08201861381530762
train_iter_loss: 0.11849676072597504
train_iter_loss: 0.4126260578632355
train_iter_loss: 0.2540443241596222
train_iter_loss: 0.17898494005203247
train_iter_loss: 0.1461201161146164
train_iter_loss: 0.12890547513961792
train_iter_loss: 0.14761944115161896
train_iter_loss: 0.1331673562526703
train_iter_loss: 0.15956129133701324
train_iter_loss: 0.1280297338962555
train_iter_loss: 0.09757471084594727
train_iter_loss: 0.1701706349849701
train_iter_loss: 0.20495545864105225
train_iter_loss: 0.1072230115532875
train_iter_loss: 0.204228937625885
train_iter_loss: 0.15158863365650177
train_iter_loss: 0.12239885330200195
train_iter_loss: 0.10944869369268417
train_iter_loss: 0.1684861034154892
train_iter_loss: 0.16933295130729675
train_iter_loss: 0.06922128796577454
train_iter_loss: 0.18497708439826965
train_iter_loss: 0.07621374726295471
train_iter_loss: 0.15921859443187714
train_iter_loss: 0.147848442196846
train_iter_loss: 0.15638020634651184
train_iter_loss: 0.13911207020282745
train_iter_loss: 0.1292111575603485
train_iter_loss: 0.1045878455042839
train_iter_loss: 0.15900053083896637
train_iter_loss: 0.14874325692653656
train_iter_loss: 0.1726696640253067
train_iter_loss: 0.15540987253189087
train_iter_loss: 0.11218494921922684
train_iter_loss: 0.18230366706848145
train_iter_loss: 0.14458699524402618
train_iter_loss: 0.1716291904449463
train_iter_loss: 0.13252416253089905
train_iter_loss: 0.13166476786136627
train_iter_loss: 0.16154246032238007
train_iter_loss: 0.2068798542022705
train_iter_loss: 0.1952964961528778
train_iter_loss: 0.07617854326963425
train_iter_loss: 0.18528561294078827
train_iter_loss: 0.12883302569389343
train_iter_loss: 0.14910058677196503
train_iter_loss: 0.10673970729112625
train_iter_loss: 0.09960760176181793
train_iter_loss: 0.10154001414775848
train_iter_loss: 0.2310086488723755
train_iter_loss: 0.1644735485315323
train_iter_loss: 0.21859729290008545
train_iter_loss: 0.1355510950088501
train_iter_loss: 0.23679831624031067
train loss :0.1525
---------------------
Validation seg loss: 0.21681846894394113 at epoch 845
epoch =    846/  1000, exp = train
train_iter_loss: 0.1942288875579834
train_iter_loss: 0.15003569424152374
train_iter_loss: 0.24369153380393982
train_iter_loss: 0.3147747218608856
train_iter_loss: 0.07571128755807877
train_iter_loss: 0.17771190404891968
train_iter_loss: 0.1668226271867752
train_iter_loss: 0.11254288256168365
train_iter_loss: 0.07220296561717987
train_iter_loss: 0.169118270277977
train_iter_loss: 0.1927005499601364
train_iter_loss: 0.2197459191083908
train_iter_loss: 0.15166603028774261
train_iter_loss: 0.19241365790367126
train_iter_loss: 0.11796707659959793
train_iter_loss: 0.16849610209465027
train_iter_loss: 0.2405911237001419
train_iter_loss: 0.16199332475662231
train_iter_loss: 0.15377360582351685
train_iter_loss: 0.18822532892227173
train_iter_loss: 0.056075647473335266
train_iter_loss: 0.1614285260438919
train_iter_loss: 0.1918729990720749
train_iter_loss: 0.14731553196907043
train_iter_loss: 0.2571597397327423
train_iter_loss: 0.17799825966358185
train_iter_loss: 0.10814427584409714
train_iter_loss: 0.18148326873779297
train_iter_loss: 0.13713519275188446
train_iter_loss: 0.25671806931495667
train_iter_loss: 0.07805557548999786
train_iter_loss: 0.1789589524269104
train_iter_loss: 0.15815594792366028
train_iter_loss: 0.14761701226234436
train_iter_loss: 0.15445053577423096
train_iter_loss: 0.12620124220848083
train_iter_loss: 0.1554299294948578
train_iter_loss: 0.08186379075050354
train_iter_loss: 0.14687667787075043
train_iter_loss: 0.09143567830324173
train_iter_loss: 0.23176629841327667
train_iter_loss: 0.2000490427017212
train_iter_loss: 0.12383601069450378
train_iter_loss: 0.10591139644384384
train_iter_loss: 0.13183461129665375
train_iter_loss: 0.19941526651382446
train_iter_loss: 0.12370903044939041
train_iter_loss: 0.1805747002363205
train_iter_loss: 0.1146959513425827
train_iter_loss: 0.1803693175315857
train_iter_loss: 0.2627706825733185
train_iter_loss: 0.12321493774652481
train_iter_loss: 0.10912483185529709
train_iter_loss: 0.14937373995780945
train_iter_loss: 0.07639512419700623
train_iter_loss: 0.22219590842723846
train_iter_loss: 0.07149937748908997
train_iter_loss: 0.18192870914936066
train_iter_loss: 0.1357511430978775
train_iter_loss: 0.10797880589962006
train_iter_loss: 0.16555193066596985
train_iter_loss: 0.07489560544490814
train_iter_loss: 0.13688194751739502
train_iter_loss: 0.1573256552219391
train_iter_loss: 0.1446136087179184
train_iter_loss: 0.1923631876707077
train_iter_loss: 0.19974246621131897
train_iter_loss: 0.10549549013376236
train_iter_loss: 0.12623363733291626
train_iter_loss: 0.14665357768535614
train_iter_loss: 0.11812812089920044
train_iter_loss: 0.15963123738765717
train_iter_loss: 0.19546087086200714
train_iter_loss: 0.06598217785358429
train_iter_loss: 0.23245570063591003
train_iter_loss: 0.11631549894809723
train_iter_loss: 0.16088509559631348
train_iter_loss: 0.2617230713367462
train_iter_loss: 0.05794243887066841
train_iter_loss: 0.23549863696098328
train_iter_loss: 0.202255517244339
train_iter_loss: 0.09510102868080139
train_iter_loss: 0.10288143157958984
train_iter_loss: 0.048602472990751266
train_iter_loss: 0.064594566822052
train_iter_loss: 0.20662571489810944
train_iter_loss: 0.3136410117149353
train_iter_loss: 0.27465203404426575
train_iter_loss: 0.19844107329845428
train_iter_loss: 0.18221963942050934
train_iter_loss: 0.17365559935569763
train_iter_loss: 0.22988682985305786
train_iter_loss: 0.1737547367811203
train_iter_loss: 0.11930373311042786
train_iter_loss: 0.06546881794929504
train_iter_loss: 0.17494070529937744
train_iter_loss: 0.15835221111774445
train_iter_loss: 0.0860762894153595
train_iter_loss: 0.1747216135263443
train_iter_loss: 0.08538266271352768
train loss :0.1583
---------------------
Validation seg loss: 0.21727767866983447 at epoch 846
epoch =    847/  1000, exp = train
train_iter_loss: 0.13669243454933167
train_iter_loss: 0.1030711755156517
train_iter_loss: 0.2286902517080307
train_iter_loss: 0.08766365051269531
train_iter_loss: 0.20460066199302673
train_iter_loss: 0.08977509289979935
train_iter_loss: 0.12727554142475128
train_iter_loss: 0.2190866768360138
train_iter_loss: 0.13066482543945312
train_iter_loss: 0.06627215445041656
train_iter_loss: 0.129424586892128
train_iter_loss: 0.17392654716968536
train_iter_loss: 0.013941772282123566
train_iter_loss: 0.1210768073797226
train_iter_loss: 0.1393287032842636
train_iter_loss: 0.12124412506818771
train_iter_loss: 0.17703120410442352
train_iter_loss: 0.14818637073040009
train_iter_loss: 0.3048395812511444
train_iter_loss: 0.10268431901931763
train_iter_loss: 0.21556025743484497
train_iter_loss: 0.1339605152606964
train_iter_loss: 0.12797215580940247
train_iter_loss: 0.23091581463813782
train_iter_loss: 0.2661813795566559
train_iter_loss: 0.106977179646492
train_iter_loss: 0.20844390988349915
train_iter_loss: 0.09029312431812286
train_iter_loss: 0.1938142627477646
train_iter_loss: 0.13403329253196716
train_iter_loss: 0.1279822736978531
train_iter_loss: 0.148616760969162
train_iter_loss: 0.15480276942253113
train_iter_loss: 0.10996145009994507
train_iter_loss: 0.16547146439552307
train_iter_loss: 0.2318640798330307
train_iter_loss: 0.12739112973213196
train_iter_loss: 0.14668971300125122
train_iter_loss: 0.07752136886119843
train_iter_loss: 0.1707903891801834
train_iter_loss: 0.21519650518894196
train_iter_loss: 0.07893331348896027
train_iter_loss: 0.130406454205513
train_iter_loss: 0.15900194644927979
train_iter_loss: 0.12776318192481995
train_iter_loss: 0.20286403596401215
train_iter_loss: 0.2553809583187103
train_iter_loss: 0.08500345051288605
train_iter_loss: 0.13738307356834412
train_iter_loss: 0.1421976536512375
train_iter_loss: 0.20542369782924652
train_iter_loss: 0.15320269763469696
train_iter_loss: 0.34679844975471497
train_iter_loss: 0.14537769556045532
train_iter_loss: 0.1348351389169693
train_iter_loss: 0.12283902615308762
train_iter_loss: 0.2098751664161682
train_iter_loss: 0.16539396345615387
train_iter_loss: 0.08237810432910919
train_iter_loss: 0.26709967851638794
train_iter_loss: 0.21651871502399445
train_iter_loss: 0.1389985829591751
train_iter_loss: 0.17763303220272064
train_iter_loss: 0.1713293194770813
train_iter_loss: 0.10255667567253113
train_iter_loss: 0.24435395002365112
train_iter_loss: 0.19004510343074799
train_iter_loss: 0.08995775133371353
train_iter_loss: 0.14379306137561798
train_iter_loss: 0.2056267112493515
train_iter_loss: 0.24586176872253418
train_iter_loss: 0.16819950938224792
train_iter_loss: 0.14594142138957977
train_iter_loss: 0.07956518232822418
train_iter_loss: 0.1644965261220932
train_iter_loss: 0.13224324584007263
train_iter_loss: 0.13571493327617645
train_iter_loss: 0.16264095902442932
train_iter_loss: 0.23149074614048004
train_iter_loss: 0.08170095831155777
train_iter_loss: 0.09799335896968842
train_iter_loss: 0.1009339764714241
train_iter_loss: 0.09517814964056015
train_iter_loss: 0.12221455574035645
train_iter_loss: 0.10664521902799606
train_iter_loss: 0.16393549740314484
train_iter_loss: 0.1518980860710144
train_iter_loss: 0.18902072310447693
train_iter_loss: 0.20932307839393616
train_iter_loss: 0.16877375543117523
train_iter_loss: 0.3576206862926483
train_iter_loss: 0.14326807856559753
train_iter_loss: 0.09588953852653503
train_iter_loss: 0.14790217578411102
train_iter_loss: 0.15184636414051056
train_iter_loss: 0.10009700059890747
train_iter_loss: 0.10331893712282181
train_iter_loss: 0.14932163059711456
train_iter_loss: 0.11364760994911194
train_iter_loss: 0.10119993984699249
train loss :0.1562
---------------------
Validation seg loss: 0.2162092278606065 at epoch 847
epoch =    848/  1000, exp = train
train_iter_loss: 0.1476900428533554
train_iter_loss: 0.15184321999549866
train_iter_loss: 0.14868201315402985
train_iter_loss: 0.1915447860956192
train_iter_loss: 0.14291828870773315
train_iter_loss: 0.15399201214313507
train_iter_loss: 0.07665052264928818
train_iter_loss: 0.02291279099881649
train_iter_loss: 0.12804503738880157
train_iter_loss: 0.1779184490442276
train_iter_loss: 0.18759019672870636
train_iter_loss: 0.06986132264137268
train_iter_loss: 0.22629737854003906
train_iter_loss: 0.1719672530889511
train_iter_loss: 0.2182617038488388
train_iter_loss: 0.17999710142612457
train_iter_loss: 0.18177886307239532
train_iter_loss: 0.2679426968097687
train_iter_loss: 0.16464491188526154
train_iter_loss: 0.07461343705654144
train_iter_loss: 0.2605428099632263
train_iter_loss: 0.05564376711845398
train_iter_loss: 0.11171577125787735
train_iter_loss: 0.09042567014694214
train_iter_loss: 0.13724711537361145
train_iter_loss: 0.05970325320959091
train_iter_loss: 0.12113308161497116
train_iter_loss: 0.1668270081281662
train_iter_loss: 0.37717100977897644
train_iter_loss: 0.0634620189666748
train_iter_loss: 0.05946032702922821
train_iter_loss: 0.1169075220823288
train_iter_loss: 0.21707452833652496
train_iter_loss: 0.22950094938278198
train_iter_loss: 0.09553264081478119
train_iter_loss: 0.08821041882038116
train_iter_loss: 0.11140701919794083
train_iter_loss: 0.1852889358997345
train_iter_loss: 0.11440279334783554
train_iter_loss: 0.1207408681511879
train_iter_loss: 0.17704865336418152
train_iter_loss: 0.08342770487070084
train_iter_loss: 0.12171746045351028
train_iter_loss: 0.10038715600967407
train_iter_loss: 0.11766175925731659
train_iter_loss: 0.2833344340324402
train_iter_loss: 0.1491193324327469
train_iter_loss: 0.13598281145095825
train_iter_loss: 0.23285602033138275
train_iter_loss: 0.1568848192691803
train_iter_loss: 0.12263500690460205
train_iter_loss: 0.17781636118888855
train_iter_loss: 0.1391318440437317
train_iter_loss: 0.20826703310012817
train_iter_loss: 0.14222034811973572
train_iter_loss: 0.21461904048919678
train_iter_loss: 0.12323778867721558
train_iter_loss: 0.10175631940364838
train_iter_loss: 0.1523437201976776
train_iter_loss: 0.1547173261642456
train_iter_loss: 0.084999218583107
train_iter_loss: 0.17497038841247559
train_iter_loss: 0.168140709400177
train_iter_loss: 0.18453127145767212
train_iter_loss: 0.21089603006839752
train_iter_loss: 0.15391680598258972
train_iter_loss: 0.16777731478214264
train_iter_loss: 0.09795462340116501
train_iter_loss: 0.1616661101579666
train_iter_loss: 0.0560917966067791
train_iter_loss: 0.1803915649652481
train_iter_loss: 0.0614931546151638
train_iter_loss: 0.18082700669765472
train_iter_loss: 0.095382921397686
train_iter_loss: 0.21634003520011902
train_iter_loss: 0.1986680030822754
train_iter_loss: 0.08754593878984451
train_iter_loss: 0.3094220459461212
train_iter_loss: 0.14138828217983246
train_iter_loss: 0.16467730700969696
train_iter_loss: 0.08121570944786072
train_iter_loss: 0.2528814971446991
train_iter_loss: 0.3020145297050476
train_iter_loss: 0.12110982835292816
train_iter_loss: 0.07788501679897308
train_iter_loss: 0.14356961846351624
train_iter_loss: 0.24154751002788544
train_iter_loss: 0.15325205028057098
train_iter_loss: 0.14909544587135315
train_iter_loss: 0.16200974583625793
train_iter_loss: 0.11574907600879669
train_iter_loss: 0.15997196733951569
train_iter_loss: 0.19318772852420807
train_iter_loss: 0.22262829542160034
train_iter_loss: 0.12583638727664948
train_iter_loss: 0.20020225644111633
train_iter_loss: 0.06339447945356369
train_iter_loss: 0.18870383501052856
train_iter_loss: 0.053789280354976654
train_iter_loss: 0.18245099484920502
train loss :0.1541
---------------------
Validation seg loss: 0.21491669005942796 at epoch 848
epoch =    849/  1000, exp = train
train_iter_loss: 0.16553081572055817
train_iter_loss: 0.09068126976490021
train_iter_loss: 0.11593727767467499
train_iter_loss: 0.23329447209835052
train_iter_loss: 0.19044281542301178
train_iter_loss: 0.1449759602546692
train_iter_loss: 0.19076107442378998
train_iter_loss: 0.09546729922294617
train_iter_loss: 0.23714236915111542
train_iter_loss: 0.11761359125375748
train_iter_loss: 0.1781294047832489
train_iter_loss: 0.12110201269388199
train_iter_loss: 0.17394113540649414
train_iter_loss: 0.09782469272613525
train_iter_loss: 0.05964642018079758
train_iter_loss: 0.3224920332431793
train_iter_loss: 0.1433020681142807
train_iter_loss: 0.14613954722881317
train_iter_loss: 0.11920488625764847
train_iter_loss: 0.13537153601646423
train_iter_loss: 0.12736234068870544
train_iter_loss: 0.14513494074344635
train_iter_loss: 0.12318333983421326
train_iter_loss: 0.1356065422296524
train_iter_loss: 0.17713147401809692
train_iter_loss: 0.23051778972148895
train_iter_loss: 0.23075896501541138
train_iter_loss: 0.25606027245521545
train_iter_loss: 0.14175644516944885
train_iter_loss: 0.05474330484867096
train_iter_loss: 0.14937050640583038
train_iter_loss: 0.11005330830812454
train_iter_loss: 0.14153118431568146
train_iter_loss: 0.13548539578914642
train_iter_loss: 0.14947251975536346
train_iter_loss: 0.13445866107940674
train_iter_loss: 0.22747187316417694
train_iter_loss: 0.2716529667377472
train_iter_loss: 0.13627231121063232
train_iter_loss: 0.24590106308460236
train_iter_loss: 0.1289294958114624
train_iter_loss: 0.1484885811805725
train_iter_loss: 0.13992580771446228
train_iter_loss: 0.15076272189617157
train_iter_loss: 0.06217313930392265
train_iter_loss: 0.11322736740112305
train_iter_loss: 0.06448014825582504
train_iter_loss: 0.2265431433916092
train_iter_loss: 0.09314281493425369
train_iter_loss: 0.09601554274559021
train_iter_loss: 0.08837533742189407
train_iter_loss: 0.16999031603336334
train_iter_loss: 0.12282493710517883
train_iter_loss: 0.1734713464975357
train_iter_loss: 0.1465330570936203
train_iter_loss: 0.11643945425748825
train_iter_loss: 0.22732718288898468
train_iter_loss: 0.14613443613052368
train_iter_loss: 0.15279996395111084
train_iter_loss: 0.16277649998664856
train_iter_loss: 0.17311467230319977
train_iter_loss: 0.10773296654224396
train_iter_loss: 0.10330526530742645
train_iter_loss: 0.12414691597223282
train_iter_loss: 0.11199405044317245
train_iter_loss: 0.38790982961654663
train_iter_loss: 0.10860029608011246
train_iter_loss: 0.16183951497077942
train_iter_loss: 0.15847063064575195
train_iter_loss: 0.1767980009317398
train_iter_loss: 0.19215744733810425
train_iter_loss: 0.1285686492919922
train_iter_loss: 0.16376331448554993
train_iter_loss: 0.09479581564664841
train_iter_loss: 0.11445153504610062
train_iter_loss: 0.1380971521139145
train_iter_loss: 0.2056010216474533
train_iter_loss: 0.11358088254928589
train_iter_loss: 0.18549296259880066
train_iter_loss: 0.15876103937625885
train_iter_loss: 0.22663621604442596
train_iter_loss: 0.14497500658035278
train_iter_loss: 0.08713696151971817
train_iter_loss: 0.15897998213768005
train_iter_loss: 0.14236979186534882
train_iter_loss: 0.12129394710063934
train_iter_loss: 0.2542060613632202
train_iter_loss: 0.15536707639694214
train_iter_loss: 0.26702749729156494
train_iter_loss: 0.06104790046811104
train_iter_loss: 0.12173570692539215
train_iter_loss: 0.12550638616085052
train_iter_loss: 0.13976000249385834
train_iter_loss: 0.059420324862003326
train_iter_loss: 0.06750665605068207
train_iter_loss: 0.09299590438604355
train_iter_loss: 0.13458187878131866
train_iter_loss: 0.11269764602184296
train_iter_loss: 0.16709309816360474
train_iter_loss: 0.227102592587471
train loss :0.1526
---------------------
Validation seg loss: 0.2205512122215949 at epoch 849
epoch =    850/  1000, exp = train
train_iter_loss: 0.21274150907993317
train_iter_loss: 0.14723904430866241
train_iter_loss: 0.15073959529399872
train_iter_loss: 0.1454358547925949
train_iter_loss: 0.07037126272916794
train_iter_loss: 0.12134788185358047
train_iter_loss: 0.14519275724887848
train_iter_loss: 0.10682754963636398
train_iter_loss: 0.15584105253219604
train_iter_loss: 0.05825568735599518
train_iter_loss: 0.09268658608198166
train_iter_loss: 0.2340802550315857
train_iter_loss: 0.1511852890253067
train_iter_loss: 0.1534561812877655
train_iter_loss: 0.20443584024906158
train_iter_loss: 0.18589037656784058
train_iter_loss: 0.16715234518051147
train_iter_loss: 0.2055862694978714
train_iter_loss: 0.19527097046375275
train_iter_loss: 0.2398931086063385
train_iter_loss: 0.1687277853488922
train_iter_loss: 0.09150810539722443
train_iter_loss: 0.08053143322467804
train_iter_loss: 0.19801488518714905
train_iter_loss: 0.13856276869773865
train_iter_loss: 0.37295931577682495
train_iter_loss: 0.12479051947593689
train_iter_loss: 0.24175196886062622
train_iter_loss: 0.19144918024539948
train_iter_loss: 0.11171897500753403
train_iter_loss: 0.11305950582027435
train_iter_loss: 0.3030950725078583
train_iter_loss: 0.1492166370153427
train_iter_loss: 0.21987593173980713
train_iter_loss: 0.1334429532289505
train_iter_loss: 0.23000645637512207
train_iter_loss: 0.09096583724021912
train_iter_loss: 0.11777667701244354
train_iter_loss: 0.10638900101184845
train_iter_loss: 0.19876642525196075
train_iter_loss: 0.07962252199649811
train_iter_loss: 0.10242637991905212
train_iter_loss: 0.040847890079021454
train_iter_loss: 0.18311969935894012
train_iter_loss: 0.03551385551691055
train_iter_loss: 0.13399530947208405
train_iter_loss: 0.09120888262987137
train_iter_loss: 0.15486644208431244
train_iter_loss: 0.10297646373510361
train_iter_loss: 0.14855225384235382
train_iter_loss: 0.13984160125255585
train_iter_loss: 0.2151489108800888
train_iter_loss: 0.25086402893066406
train_iter_loss: 0.10111155360937119
train_iter_loss: 0.1902310997247696
train_iter_loss: 0.14004623889923096
train_iter_loss: 0.2164074182510376
train_iter_loss: 0.1683478057384491
train_iter_loss: 0.18761897087097168
train_iter_loss: 0.1300479918718338
train_iter_loss: 0.09129466861486435
train_iter_loss: 0.09240343421697617
train_iter_loss: 0.2241133600473404
train_iter_loss: 0.16832031309604645
train_iter_loss: 0.22681598365306854
train_iter_loss: 0.16512466967105865
train_iter_loss: 0.19829769432544708
train_iter_loss: 0.08772892504930496
train_iter_loss: 0.13713079690933228
train_iter_loss: 0.08975032716989517
train_iter_loss: 0.12367337197065353
train_iter_loss: 0.12273116409778595
train_iter_loss: 0.22365619242191315
train_iter_loss: 0.22314925491809845
train_iter_loss: 0.17299582064151764
train_iter_loss: 0.14048443734645844
train_iter_loss: 0.06813817471265793
train_iter_loss: 0.20018863677978516
train_iter_loss: 0.07265592366456985
train_iter_loss: 0.13845676183700562
train_iter_loss: 0.13069139420986176
train_iter_loss: 0.22702708840370178
train_iter_loss: 0.1754833310842514
train_iter_loss: 0.22524084150791168
train_iter_loss: 0.11457610130310059
train_iter_loss: 0.12896370887756348
train_iter_loss: 0.16284510493278503
train_iter_loss: 0.15400169789791107
train_iter_loss: 0.1165083572268486
train_iter_loss: 0.09298583120107651
train_iter_loss: 0.18510444462299347
train_iter_loss: 0.20110878348350525
train_iter_loss: 0.12047811597585678
train_iter_loss: 0.05032750964164734
train_iter_loss: 0.3211504817008972
train_iter_loss: 0.10617347806692123
train_iter_loss: 0.10844814032316208
train_iter_loss: 0.147416353225708
train_iter_loss: 0.14119188487529755
train_iter_loss: 0.16328655183315277
train loss :0.1553
---------------------
Validation seg loss: 0.213965742348008 at epoch 850
epoch =    851/  1000, exp = train
train_iter_loss: 0.2722649574279785
train_iter_loss: 0.14779691398143768
train_iter_loss: 0.3335871994495392
train_iter_loss: 0.14852939546108246
train_iter_loss: 0.1005653664469719
train_iter_loss: 0.1623774617910385
train_iter_loss: 0.17145726084709167
train_iter_loss: 0.10464758425951004
train_iter_loss: 0.24497966468334198
train_iter_loss: 0.1357441395521164
train_iter_loss: 0.07278166711330414
train_iter_loss: 0.15854687988758087
train_iter_loss: 0.18166175484657288
train_iter_loss: 0.16391070187091827
train_iter_loss: 0.1580529659986496
train_iter_loss: 0.08507685363292694
train_iter_loss: 0.0655336081981659
train_iter_loss: 0.1454198956489563
train_iter_loss: 0.11001303046941757
train_iter_loss: 0.18056780099868774
train_iter_loss: 0.09488112479448318
train_iter_loss: 0.16963060200214386
train_iter_loss: 0.22520074248313904
train_iter_loss: 0.18733179569244385
train_iter_loss: 0.055683307349681854
train_iter_loss: 0.07155890762805939
train_iter_loss: 0.13336512446403503
train_iter_loss: 0.25696584582328796
train_iter_loss: 0.13129083812236786
train_iter_loss: 0.2895881235599518
train_iter_loss: 0.043471671640872955
train_iter_loss: 0.22189544141292572
train_iter_loss: 0.1306629180908203
train_iter_loss: 0.24578043818473816
train_iter_loss: 0.19401507079601288
train_iter_loss: 0.12026943266391754
train_iter_loss: 0.034690190106630325
train_iter_loss: 0.14679843187332153
train_iter_loss: 0.129717156291008
train_iter_loss: 0.17316514253616333
train_iter_loss: 0.22985099256038666
train_iter_loss: 0.1074148491024971
train_iter_loss: 0.24184571206569672
train_iter_loss: 0.0906502828001976
train_iter_loss: 0.19509242475032806
train_iter_loss: 0.18794693052768707
train_iter_loss: 0.11127982288599014
train_iter_loss: 0.15097829699516296
train_iter_loss: 0.09252384305000305
train_iter_loss: 0.18690098822116852
train_iter_loss: 0.19543574750423431
train_iter_loss: 0.11907071620225906
train_iter_loss: 0.2138836830854416
train_iter_loss: 0.13451164960861206
train_iter_loss: 0.1324222981929779
train_iter_loss: 0.12828262150287628
train_iter_loss: 0.16710931062698364
train_iter_loss: 0.09406968206167221
train_iter_loss: 0.09922720491886139
train_iter_loss: 0.1907280832529068
train_iter_loss: 0.16899257898330688
train_iter_loss: 0.08465150743722916
train_iter_loss: 0.23358310759067535
train_iter_loss: 0.2778136432170868
train_iter_loss: 0.22458025813102722
train_iter_loss: 0.08246259391307831
train_iter_loss: 0.1314605176448822
train_iter_loss: 0.15021775662899017
train_iter_loss: 0.09121568500995636
train_iter_loss: 0.2133295238018036
train_iter_loss: 0.1869356781244278
train_iter_loss: 0.016391253098845482
train_iter_loss: 0.21022948622703552
train_iter_loss: 0.07923747599124908
train_iter_loss: 0.14541444182395935
train_iter_loss: 0.2506299912929535
train_iter_loss: 0.13798971474170685
train_iter_loss: 0.13552826642990112
train_iter_loss: 0.23602691292762756
train_iter_loss: 0.3498730957508087
train_iter_loss: 0.13771212100982666
train_iter_loss: 0.08369695395231247
train_iter_loss: 0.13941647112369537
train_iter_loss: 0.17211462557315826
train_iter_loss: 0.13268692791461945
train_iter_loss: 0.200497567653656
train_iter_loss: 0.24250754714012146
train_iter_loss: 0.09585201740264893
train_iter_loss: 0.3167777359485626
train_iter_loss: 0.1310221254825592
train_iter_loss: 0.15945085883140564
train_iter_loss: 0.1389942467212677
train_iter_loss: 0.1724494993686676
train_iter_loss: 0.20061562955379486
train_iter_loss: 0.12137166410684586
train_iter_loss: 0.1474960297346115
train_iter_loss: 0.1611308753490448
train_iter_loss: 0.13209937512874603
train_iter_loss: 0.13124792277812958
train_iter_loss: 0.22078797221183777
train loss :0.1607
---------------------
Validation seg loss: 0.21336469330104454 at epoch 851
epoch =    852/  1000, exp = train
train_iter_loss: 0.06855218857526779
train_iter_loss: 0.19727720320224762
train_iter_loss: 0.2687714993953705
train_iter_loss: 0.08363740891218185
train_iter_loss: 0.13320842385292053
train_iter_loss: 0.2576032876968384
train_iter_loss: 0.1617947667837143
train_iter_loss: 0.1347787231206894
train_iter_loss: 0.07737831771373749
train_iter_loss: 0.10259423404932022
train_iter_loss: 0.16228586435317993
train_iter_loss: 0.11916860193014145
train_iter_loss: 0.1271546185016632
train_iter_loss: 0.17943602800369263
train_iter_loss: 0.1798313558101654
train_iter_loss: 0.4633505642414093
train_iter_loss: 0.1411469429731369
train_iter_loss: 0.10368344187736511
train_iter_loss: 0.19023072719573975
train_iter_loss: 0.10120910406112671
train_iter_loss: 0.15648306906223297
train_iter_loss: 0.2966575622558594
train_iter_loss: 0.1509130299091339
train_iter_loss: 0.1818205863237381
train_iter_loss: 0.10322444140911102
train_iter_loss: 0.2666027545928955
train_iter_loss: 0.07671648263931274
train_iter_loss: 0.16780419647693634
train_iter_loss: 0.1524890661239624
train_iter_loss: 0.22452665865421295
train_iter_loss: 0.13379140198230743
train_iter_loss: 0.1617806851863861
train_iter_loss: 0.0902862548828125
train_iter_loss: 0.15703220665454865
train_iter_loss: 0.17375223338603973
train_iter_loss: 0.09644769877195358
train_iter_loss: 0.15927740931510925
train_iter_loss: 0.06580717861652374
train_iter_loss: 0.1520843356847763
train_iter_loss: 0.1881738156080246
train_iter_loss: 0.1393965184688568
train_iter_loss: 0.07512938976287842
train_iter_loss: 0.13755904138088226
train_iter_loss: 0.350938618183136
train_iter_loss: 0.11865077912807465
train_iter_loss: 0.2495017796754837
train_iter_loss: 0.3086555004119873
train_iter_loss: 0.14807356894016266
train_iter_loss: 0.18275079131126404
train_iter_loss: 0.15776510536670685
train_iter_loss: 0.21170687675476074
train_iter_loss: 0.10008913278579712
train_iter_loss: 0.16277648508548737
train_iter_loss: 0.22147417068481445
train_iter_loss: 0.13495619595050812
train_iter_loss: 0.1206129640340805
train_iter_loss: 0.3090648055076599
train_iter_loss: 0.136598601937294
train_iter_loss: 0.11282067745923996
train_iter_loss: 0.15271781384944916
train_iter_loss: 0.0895807221531868
train_iter_loss: 0.15230801701545715
train_iter_loss: 0.1929427981376648
train_iter_loss: 0.14921976625919342
train_iter_loss: 0.14708659052848816
train_iter_loss: 0.17882944643497467
train_iter_loss: 0.11335926502943039
train_iter_loss: 0.18247441947460175
train_iter_loss: 0.14990432560443878
train_iter_loss: 0.09621148556470871
train_iter_loss: 0.060749176889657974
train_iter_loss: 0.10321677476167679
train_iter_loss: 0.1283312737941742
train_iter_loss: 0.1417263150215149
train_iter_loss: 0.10066308081150055
train_iter_loss: 0.1008286103606224
train_iter_loss: 0.13550737500190735
train_iter_loss: 0.16233205795288086
train_iter_loss: 0.18179841339588165
train_iter_loss: 0.07050494104623795
train_iter_loss: 0.17677077651023865
train_iter_loss: 0.22836235165596008
train_iter_loss: 0.10684383660554886
train_iter_loss: 0.08408850431442261
train_iter_loss: 0.08624513447284698
train_iter_loss: 0.14557614922523499
train_iter_loss: 0.1050974652171135
train_iter_loss: 0.17926062643527985
train_iter_loss: 0.11962371319532394
train_iter_loss: 0.16264161467552185
train_iter_loss: 0.24186274409294128
train_iter_loss: 0.1917225420475006
train_iter_loss: 0.14768758416175842
train_iter_loss: 0.15427356958389282
train_iter_loss: 0.24489335715770721
train_iter_loss: 0.13148343563079834
train_iter_loss: 0.22236910462379456
train_iter_loss: 0.06508319079875946
train_iter_loss: 0.11127518117427826
train_iter_loss: 0.23656624555587769
train loss :0.1588
---------------------
Validation seg loss: 0.2153821663034834 at epoch 852
epoch =    853/  1000, exp = train
train_iter_loss: 0.10193974524736404
train_iter_loss: 0.12172441929578781
train_iter_loss: 0.25159138441085815
train_iter_loss: 0.12967927753925323
train_iter_loss: 0.14232733845710754
train_iter_loss: 0.07664421945810318
train_iter_loss: 0.12717178463935852
train_iter_loss: 0.13678058981895447
train_iter_loss: 0.1517648696899414
train_iter_loss: 0.08733877539634705
train_iter_loss: 0.13259035348892212
train_iter_loss: 0.1814809888601303
train_iter_loss: 0.18548749387264252
train_iter_loss: 0.1253160983324051
train_iter_loss: 0.14322328567504883
train_iter_loss: 0.11211541295051575
train_iter_loss: 0.0842917189002037
train_iter_loss: 0.18673408031463623
train_iter_loss: 0.13603739440441132
train_iter_loss: 0.1760576218366623
train_iter_loss: 0.13732509315013885
train_iter_loss: 0.18234950304031372
train_iter_loss: 0.11674607545137405
train_iter_loss: 0.09605225920677185
train_iter_loss: 0.2068585306406021
train_iter_loss: 0.3190617561340332
train_iter_loss: 0.16439935564994812
train_iter_loss: 0.19128204882144928
train_iter_loss: 0.13698147237300873
train_iter_loss: 0.039465077221393585
train_iter_loss: 0.11011392623186111
train_iter_loss: 0.16990244388580322
train_iter_loss: 0.17633676528930664
train_iter_loss: 0.05890608951449394
train_iter_loss: 0.2329173982143402
train_iter_loss: 0.14372451603412628
train_iter_loss: 0.14165562391281128
train_iter_loss: 0.26837965846061707
train_iter_loss: 0.19939789175987244
train_iter_loss: 0.08734568953514099
train_iter_loss: 0.08375734090805054
train_iter_loss: 0.140573650598526
train_iter_loss: 0.13602451980113983
train_iter_loss: 0.09646444767713547
train_iter_loss: 0.14284716546535492
train_iter_loss: 0.1310494989156723
train_iter_loss: 0.11424417048692703
train_iter_loss: 0.1735411435365677
train_iter_loss: 0.05133773386478424
train_iter_loss: 0.18490393459796906
train_iter_loss: 0.09054204821586609
train_iter_loss: 0.09907427430152893
train_iter_loss: 0.1470043808221817
train_iter_loss: 0.15051034092903137
train_iter_loss: 0.040325965732336044
train_iter_loss: 0.10991397500038147
train_iter_loss: 0.1833711564540863
train_iter_loss: 0.14112088084220886
train_iter_loss: 0.11922243982553482
train_iter_loss: 0.13685323297977448
train_iter_loss: 0.2163846343755722
train_iter_loss: 0.18805725872516632
train_iter_loss: 0.09264445304870605
train_iter_loss: 0.21839332580566406
train_iter_loss: 0.17992804944515228
train_iter_loss: 0.22606666386127472
train_iter_loss: 0.1695941835641861
train_iter_loss: 0.2065093219280243
train_iter_loss: 0.23802442848682404
train_iter_loss: 0.16170261800289154
train_iter_loss: 0.18096934258937836
train_iter_loss: 0.14898936450481415
train_iter_loss: 0.13427665829658508
train_iter_loss: 0.15963633358478546
train_iter_loss: 0.07746408134698868
train_iter_loss: 0.13934050500392914
train_iter_loss: 0.14275279641151428
train_iter_loss: 0.3860815763473511
train_iter_loss: 0.08520336449146271
train_iter_loss: 0.1852169930934906
train_iter_loss: 0.19568674266338348
train_iter_loss: 0.21495221555233002
train_iter_loss: 0.14807020127773285
train_iter_loss: 0.08033981174230576
train_iter_loss: 0.05911913886666298
train_iter_loss: 0.15465284883975983
train_iter_loss: 0.02818058431148529
train_iter_loss: 0.47934576869010925
train_iter_loss: 0.13098016381263733
train_iter_loss: 0.09085264801979065
train_iter_loss: 0.15218253433704376
train_iter_loss: 0.2085631936788559
train_iter_loss: 0.23241420090198517
train_iter_loss: 0.1677614003419876
train_iter_loss: 0.07471238821744919
train_iter_loss: 0.15052345395088196
train_iter_loss: 0.12839002907276154
train_iter_loss: 0.09911300241947174
train_iter_loss: 0.08937224000692368
train_iter_loss: 0.14182284474372864
train loss :0.1517
---------------------
Validation seg loss: 0.21470780176866167 at epoch 853
epoch =    854/  1000, exp = train
train_iter_loss: 0.1861230581998825
train_iter_loss: 0.25592654943466187
train_iter_loss: 0.11014171689748764
train_iter_loss: 0.1138383224606514
train_iter_loss: 0.402597039937973
train_iter_loss: 0.2915293872356415
train_iter_loss: 0.15815651416778564
train_iter_loss: 0.19422875344753265
train_iter_loss: 0.07058098912239075
train_iter_loss: 0.1783410906791687
train_iter_loss: 0.19717814028263092
train_iter_loss: 0.1621217131614685
train_iter_loss: 0.1016959473490715
train_iter_loss: 0.19143427908420563
train_iter_loss: 0.06419260054826736
train_iter_loss: 0.12097883969545364
train_iter_loss: 0.04948592185974121
train_iter_loss: 0.25261080265045166
train_iter_loss: 0.16910918056964874
train_iter_loss: 0.11692117154598236
train_iter_loss: 0.2563994824886322
train_iter_loss: 0.18945695459842682
train_iter_loss: 0.2690204977989197
train_iter_loss: 0.12144690006971359
train_iter_loss: 0.08715897798538208
train_iter_loss: 0.1539602428674698
train_iter_loss: 0.1008111983537674
train_iter_loss: 0.17144817113876343
train_iter_loss: 0.1849060207605362
train_iter_loss: 0.20413407683372498
train_iter_loss: 0.08453585207462311
train_iter_loss: 0.15697474777698517
train_iter_loss: 0.2306617945432663
train_iter_loss: 0.06083244830369949
train_iter_loss: 0.17731958627700806
train_iter_loss: 0.20890535414218903
train_iter_loss: 0.18862435221672058
train_iter_loss: 0.2795697748661041
train_iter_loss: 0.16585752367973328
train_iter_loss: 0.08692945539951324
train_iter_loss: 0.32195842266082764
train_iter_loss: 0.14760242402553558
train_iter_loss: 0.11397235095500946
train_iter_loss: 0.12884727120399475
train_iter_loss: 0.23687101900577545
train_iter_loss: 0.23753562569618225
train_iter_loss: 0.23101073503494263
train_iter_loss: 0.05621892586350441
train_iter_loss: 0.10854832828044891
train_iter_loss: 0.16512487828731537
train_iter_loss: 0.09298069775104523
train_iter_loss: 0.18111425638198853
train_iter_loss: 0.17441053688526154
train_iter_loss: 0.09397753328084946
train_iter_loss: 0.09319759905338287
train_iter_loss: 0.2483855038881302
train_iter_loss: 0.2672562003135681
train_iter_loss: 0.25375720858573914
train_iter_loss: 0.17470937967300415
train_iter_loss: 0.11748731136322021
train_iter_loss: 0.22038719058036804
train_iter_loss: 0.13312095403671265
train_iter_loss: 0.07047104090452194
train_iter_loss: 0.13060808181762695
train_iter_loss: 0.05023328214883804
train_iter_loss: 0.10219511389732361
train_iter_loss: 0.07314282655715942
train_iter_loss: 0.19993580877780914
train_iter_loss: 0.3541299104690552
train_iter_loss: 0.1246466413140297
train_iter_loss: 0.17529790103435516
train_iter_loss: 0.17271387577056885
train_iter_loss: 0.14552897214889526
train_iter_loss: 0.20755209028720856
train_iter_loss: 0.09294220060110092
train_iter_loss: 0.17015568912029266
train_iter_loss: 0.08328106254339218
train_iter_loss: 0.1889764815568924
train_iter_loss: 0.1657939851284027
train_iter_loss: 0.18408913910388947
train_iter_loss: 0.23053281009197235
train_iter_loss: 0.1408683806657791
train_iter_loss: 0.12214724719524384
train_iter_loss: 0.1697017103433609
train_iter_loss: 0.10543351620435715
train_iter_loss: 0.09215471893548965
train_iter_loss: 0.2590416371822357
train_iter_loss: 0.1466357558965683
train_iter_loss: 0.14230096340179443
train_iter_loss: 0.024362849071621895
train_iter_loss: 0.18803337216377258
train_iter_loss: 0.15652306377887726
train_iter_loss: 0.1315270960330963
train_iter_loss: 0.13437984883785248
train_iter_loss: 0.165333092212677
train_iter_loss: 0.09306443482637405
train_iter_loss: 0.05641437694430351
train_iter_loss: 0.15437208116054535
train_iter_loss: 0.07699408382177353
train_iter_loss: 0.10362177342176437
train loss :0.1610
---------------------
Validation seg loss: 0.21763279635178032 at epoch 854
epoch =    855/  1000, exp = train
train_iter_loss: 0.15110638737678528
train_iter_loss: 0.09184732288122177
train_iter_loss: 0.14257103204727173
train_iter_loss: 0.17632260918617249
train_iter_loss: 0.12963613867759705
train_iter_loss: 0.1501396894454956
train_iter_loss: 0.12308936566114426
train_iter_loss: 0.046439699828624725
train_iter_loss: 0.10183653980493546
train_iter_loss: 0.31691065430641174
train_iter_loss: 0.1719972938299179
train_iter_loss: 0.1050884872674942
train_iter_loss: 0.20548087358474731
train_iter_loss: 0.13225653767585754
train_iter_loss: 0.03840012103319168
train_iter_loss: 0.15433378517627716
train_iter_loss: 0.08049976825714111
train_iter_loss: 0.22666257619857788
train_iter_loss: 0.12550953030586243
train_iter_loss: 0.043317925184965134
train_iter_loss: 0.13200071454048157
train_iter_loss: 0.11492396891117096
train_iter_loss: 0.0464429073035717
train_iter_loss: 0.22244355082511902
train_iter_loss: 0.1366080492734909
train_iter_loss: 0.09535368531942368
train_iter_loss: 0.07784934341907501
train_iter_loss: 0.21081246435642242
train_iter_loss: 0.09806116670370102
train_iter_loss: 0.16828928887844086
train_iter_loss: 0.17052140831947327
train_iter_loss: 0.12508034706115723
train_iter_loss: 0.24983011186122894
train_iter_loss: 0.10543981194496155
train_iter_loss: 0.12874819338321686
train_iter_loss: 0.0999423936009407
train_iter_loss: 0.09429295361042023
train_iter_loss: 0.12769165635108948
train_iter_loss: 0.1790655106306076
train_iter_loss: 0.13800136744976044
train_iter_loss: 0.33119022846221924
train_iter_loss: 0.20844219624996185
train_iter_loss: 0.26554813981056213
train_iter_loss: 0.18340736627578735
train_iter_loss: 0.10289566963911057
train_iter_loss: 0.09799769520759583
train_iter_loss: 0.07306920737028122
train_iter_loss: 0.28385892510414124
train_iter_loss: 0.1135144755244255
train_iter_loss: 0.19946914911270142
train_iter_loss: 0.10798382759094238
train_iter_loss: 0.1624254435300827
train_iter_loss: 0.24737651646137238
train_iter_loss: 0.10024669021368027
train_iter_loss: 0.1717783808708191
train_iter_loss: 0.2791483700275421
train_iter_loss: 0.07620446383953094
train_iter_loss: 0.08241204172372818
train_iter_loss: 0.17065361142158508
train_iter_loss: 0.18517708778381348
train_iter_loss: 0.071408711373806
train_iter_loss: 0.257725328207016
train_iter_loss: 0.23506735265254974
train_iter_loss: 0.08718985319137573
train_iter_loss: 0.2605849504470825
train_iter_loss: 0.23776307702064514
train_iter_loss: 0.07468278706073761
train_iter_loss: 0.11233463883399963
train_iter_loss: 0.19225870072841644
train_iter_loss: 0.05651524290442467
train_iter_loss: 0.19815343618392944
train_iter_loss: 0.13740123808383942
train_iter_loss: 0.15458433330059052
train_iter_loss: 0.20538078248500824
train_iter_loss: 0.13530464470386505
train_iter_loss: 0.26563122868537903
train_iter_loss: 0.3842689096927643
train_iter_loss: 0.1915331482887268
train_iter_loss: 0.21627365052700043
train_iter_loss: 0.0988517627120018
train_iter_loss: 0.10579445213079453
train_iter_loss: 0.08060802519321442
train_iter_loss: 0.12835495173931122
train_iter_loss: 0.1409735530614853
train_iter_loss: 0.1625276505947113
train_iter_loss: 0.21272559463977814
train_iter_loss: 0.14807765185832977
train_iter_loss: 0.1276499629020691
train_iter_loss: 0.1167227178812027
train_iter_loss: 0.11059343069791794
train_iter_loss: 0.11989468336105347
train_iter_loss: 0.08372946083545685
train_iter_loss: 0.08687494695186615
train_iter_loss: 0.08105719834566116
train_iter_loss: 0.12174873799085617
train_iter_loss: 0.1863408386707306
train_iter_loss: 0.26759013533592224
train_iter_loss: 0.10145600885152817
train_iter_loss: 0.14437833428382874
train_iter_loss: 0.2578393816947937
train loss :0.1540
---------------------
Validation seg loss: 0.21628254504417474 at epoch 855
epoch =    856/  1000, exp = train
train_iter_loss: 0.15862618386745453
train_iter_loss: 0.17000985145568848
train_iter_loss: 0.14212879538536072
train_iter_loss: 0.09928040951490402
train_iter_loss: 0.20739953219890594
train_iter_loss: 0.16093291342258453
train_iter_loss: 0.1696896106004715
train_iter_loss: 0.04596122354269028
train_iter_loss: 0.13290011882781982
train_iter_loss: 0.055806584656238556
train_iter_loss: 0.1064905971288681
train_iter_loss: 0.2021304816007614
train_iter_loss: 0.13042934238910675
train_iter_loss: 0.1323460191488266
train_iter_loss: 0.1195082888007164
train_iter_loss: 0.14599555730819702
train_iter_loss: 0.12755133211612701
train_iter_loss: 0.08958929777145386
train_iter_loss: 0.2274530827999115
train_iter_loss: 0.20985209941864014
train_iter_loss: 0.11674311757087708
train_iter_loss: 0.2444727122783661
train_iter_loss: 0.059505850076675415
train_iter_loss: 0.07303491234779358
train_iter_loss: 0.1789630949497223
train_iter_loss: 0.1532461941242218
train_iter_loss: 0.13682891428470612
train_iter_loss: 0.14187265932559967
train_iter_loss: 0.13210858404636383
train_iter_loss: 0.18892070651054382
train_iter_loss: 0.10709551721811295
train_iter_loss: 0.2382412999868393
train_iter_loss: 0.09908019751310349
train_iter_loss: 0.22609682381153107
train_iter_loss: 0.1601281762123108
train_iter_loss: 0.09553153067827225
train_iter_loss: 0.16868188977241516
train_iter_loss: 0.0807579755783081
train_iter_loss: 0.18727631866931915
train_iter_loss: 0.10889232903718948
train_iter_loss: 0.08624324202537537
train_iter_loss: 0.10739267617464066
train_iter_loss: 0.3691549301147461
train_iter_loss: 0.14985324442386627
train_iter_loss: 0.22369703650474548
train_iter_loss: 0.31680068373680115
train_iter_loss: 0.09688519686460495
train_iter_loss: 0.1647510677576065
train_iter_loss: 0.13268499076366425
train_iter_loss: 0.20185640454292297
train_iter_loss: 0.2616117000579834
train_iter_loss: 0.09232322126626968
train_iter_loss: 0.09984354674816132
train_iter_loss: 0.12817177176475525
train_iter_loss: 0.20648692548274994
train_iter_loss: 0.17040954530239105
train_iter_loss: 0.13289189338684082
train_iter_loss: 0.11502394825220108
train_iter_loss: 0.11032893508672714
train_iter_loss: 0.14765405654907227
train_iter_loss: 0.2085227370262146
train_iter_loss: 0.24492351710796356
train_iter_loss: 0.11942653357982635
train_iter_loss: 0.1376350075006485
train_iter_loss: 0.09650470316410065
train_iter_loss: 0.08207334578037262
train_iter_loss: 0.13765503466129303
train_iter_loss: 0.11590762436389923
train_iter_loss: 0.226810023188591
train_iter_loss: 0.11161737889051437
train_iter_loss: 0.1881227195262909
train_iter_loss: 0.14370989799499512
train_iter_loss: 0.1100873127579689
train_iter_loss: 0.05316030606627464
train_iter_loss: 0.17523792386054993
train_iter_loss: 0.16726069152355194
train_iter_loss: 0.3355039656162262
train_iter_loss: 0.1411161869764328
train_iter_loss: 0.27318012714385986
train_iter_loss: 0.14479421079158783
train_iter_loss: 0.09833879768848419
train_iter_loss: 0.2191770225763321
train_iter_loss: 0.13551676273345947
train_iter_loss: 0.10341387987136841
train_iter_loss: 0.25037431716918945
train_iter_loss: 0.07133281230926514
train_iter_loss: 0.2873988747596741
train_iter_loss: 0.19954875111579895
train_iter_loss: 0.16113927960395813
train_iter_loss: 0.10020312666893005
train_iter_loss: 0.1449071615934372
train_iter_loss: 0.13508421182632446
train_iter_loss: 0.16527320444583893
train_iter_loss: 0.09893780946731567
train_iter_loss: 0.1512661576271057
train_iter_loss: 0.07781334221363068
train_iter_loss: 0.1906013786792755
train_iter_loss: 0.2449374496936798
train_iter_loss: 0.2365362048149109
train_iter_loss: 0.12857753038406372
train loss :0.1561
---------------------
Validation seg loss: 0.21431790062946812 at epoch 856
epoch =    857/  1000, exp = train
train_iter_loss: 0.09240570664405823
train_iter_loss: 0.1673426479101181
train_iter_loss: 0.3300746977329254
train_iter_loss: 0.06790736317634583
train_iter_loss: 0.19658160209655762
train_iter_loss: 0.0942988395690918
train_iter_loss: 0.14222478866577148
train_iter_loss: 0.11981713026762009
train_iter_loss: 0.05921575054526329
train_iter_loss: 0.12001673877239227
train_iter_loss: 0.24476785957813263
train_iter_loss: 0.16621078550815582
train_iter_loss: 0.05010398104786873
train_iter_loss: 0.15102529525756836
train_iter_loss: 0.31265193223953247
train_iter_loss: 0.06678463518619537
train_iter_loss: 0.12300494313240051
train_iter_loss: 0.07932062447071075
train_iter_loss: 0.19644102454185486
train_iter_loss: 0.11909211426973343
train_iter_loss: 0.12700951099395752
train_iter_loss: 0.30148905515670776
train_iter_loss: 0.0987607091665268
train_iter_loss: 0.18240466713905334
train_iter_loss: 0.21080635488033295
train_iter_loss: 0.2317400574684143
train_iter_loss: 0.24370397627353668
train_iter_loss: 0.10194555670022964
train_iter_loss: 0.14262790977954865
train_iter_loss: 0.1387525498867035
train_iter_loss: 0.1749069094657898
train_iter_loss: 0.0499064177274704
train_iter_loss: 0.13098156452178955
train_iter_loss: 0.09397560358047485
train_iter_loss: 0.16156545281410217
train_iter_loss: 0.17509636282920837
train_iter_loss: 0.23647716641426086
train_iter_loss: 0.11503233760595322
train_iter_loss: 0.10819680988788605
train_iter_loss: 0.04290081933140755
train_iter_loss: 0.24847917258739471
train_iter_loss: 0.1723574995994568
train_iter_loss: 0.051761988550424576
train_iter_loss: 0.18420566618442535
train_iter_loss: 0.1532362550497055
train_iter_loss: 0.15769045054912567
train_iter_loss: 0.20974014699459076
train_iter_loss: 0.11826696246862411
train_iter_loss: 0.171338751912117
train_iter_loss: 0.0847938284277916
train_iter_loss: 0.07472135126590729
train_iter_loss: 0.18994590640068054
train_iter_loss: 0.21144753694534302
train_iter_loss: 0.17013928294181824
train_iter_loss: 0.18624980747699738
train_iter_loss: 0.17830848693847656
train_iter_loss: 0.17908819019794464
train_iter_loss: 0.16490666568279266
train_iter_loss: 0.11650442332029343
train_iter_loss: 0.19093483686447144
train_iter_loss: 0.11224420368671417
train_iter_loss: 0.17294368147850037
train_iter_loss: 0.13745808601379395
train_iter_loss: 0.11849791556596756
train_iter_loss: 0.1587541252374649
train_iter_loss: 0.25509271025657654
train_iter_loss: 0.1907404512166977
train_iter_loss: 0.08494197577238083
train_iter_loss: 0.1772053837776184
train_iter_loss: 0.2383125126361847
train_iter_loss: 0.14171335101127625
train_iter_loss: 0.08386870473623276
train_iter_loss: 0.1305200606584549
train_iter_loss: 0.11212261766195297
train_iter_loss: 0.10089606791734695
train_iter_loss: 0.16600137948989868
train_iter_loss: 0.23602280020713806
train_iter_loss: 0.10870185494422913
train_iter_loss: 0.16356419026851654
train_iter_loss: 0.15030261874198914
train_iter_loss: 0.17225223779678345
train_iter_loss: 0.08864345401525497
train_iter_loss: 0.23853625357151031
train_iter_loss: 0.06968062371015549
train_iter_loss: 0.07084490358829498
train_iter_loss: 0.19680634140968323
train_iter_loss: 0.2415458709001541
train_iter_loss: 0.07890365272760391
train_iter_loss: 0.17447243630886078
train_iter_loss: 0.1121002584695816
train_iter_loss: 0.1251046061515808
train_iter_loss: 0.21531687676906586
train_iter_loss: 0.15059734880924225
train_iter_loss: 0.10514181107282639
train_iter_loss: 0.10744726657867432
train_iter_loss: 0.159805029630661
train_iter_loss: 0.07913178950548172
train_iter_loss: 0.12571612000465393
train_iter_loss: 0.15088020265102386
train_iter_loss: 0.19400572776794434
train loss :0.1523
---------------------
Validation seg loss: 0.21516448956966963 at epoch 857
epoch =    858/  1000, exp = train
train_iter_loss: 0.12852473556995392
train_iter_loss: 0.18752546608448029
train_iter_loss: 0.14211592078208923
train_iter_loss: 0.19102106988430023
train_iter_loss: 0.17849420011043549
train_iter_loss: 0.14691784977912903
train_iter_loss: 0.24110250174999237
train_iter_loss: 0.2982315719127655
train_iter_loss: 0.1162489578127861
train_iter_loss: 0.13793893158435822
train_iter_loss: 0.18908949196338654
train_iter_loss: 0.1471257358789444
train_iter_loss: 0.20501920580863953
train_iter_loss: 0.26856374740600586
train_iter_loss: 0.1625380516052246
train_iter_loss: 0.16991417109966278
train_iter_loss: 0.25835561752319336
train_iter_loss: 0.23292973637580872
train_iter_loss: 0.15994055569171906
train_iter_loss: 0.10771224647760391
train_iter_loss: 0.0732850730419159
train_iter_loss: 0.08961358666419983
train_iter_loss: 0.31919142603874207
train_iter_loss: 0.14240030944347382
train_iter_loss: 0.11032132804393768
train_iter_loss: 0.28563377261161804
train_iter_loss: 0.20754729211330414
train_iter_loss: 0.17242996394634247
train_iter_loss: 0.2118314653635025
train_iter_loss: 0.14179499447345734
train_iter_loss: 0.15471357107162476
train_iter_loss: 0.09919271618127823
train_iter_loss: 0.17905159294605255
train_iter_loss: 0.07314358651638031
train_iter_loss: 0.11372874677181244
train_iter_loss: 0.08952772617340088
train_iter_loss: 0.17253506183624268
train_iter_loss: 0.0910220593214035
train_iter_loss: 0.14145703613758087
train_iter_loss: 0.208408921957016
train_iter_loss: 0.10869727283716202
train_iter_loss: 0.09184691309928894
train_iter_loss: 0.17262351512908936
train_iter_loss: 0.23804090917110443
train_iter_loss: 0.11006946861743927
train_iter_loss: 0.13112454116344452
train_iter_loss: 0.07520432025194168
train_iter_loss: 0.08496791124343872
train_iter_loss: 0.10499785840511322
train_iter_loss: 0.19137203693389893
train_iter_loss: 0.13322068750858307
train_iter_loss: 0.16115958988666534
train_iter_loss: 0.30260324478149414
train_iter_loss: 0.12419883161783218
train_iter_loss: 0.19562938809394836
train_iter_loss: 0.06836544722318649
train_iter_loss: 0.1683204025030136
train_iter_loss: 0.27768474817276
train_iter_loss: 0.13653650879859924
train_iter_loss: 0.1751527637243271
train_iter_loss: 0.13560490310192108
train_iter_loss: 0.10586097836494446
train_iter_loss: 0.15754470229148865
train_iter_loss: 0.15098977088928223
train_iter_loss: 0.14550353586673737
train_iter_loss: 0.1761256456375122
train_iter_loss: 0.09536904841661453
train_iter_loss: 0.2361549288034439
train_iter_loss: 0.09489817917346954
train_iter_loss: 0.23462659120559692
train_iter_loss: 0.11061659455299377
train_iter_loss: 0.14473886787891388
train_iter_loss: 0.053715359419584274
train_iter_loss: 0.2140422910451889
train_iter_loss: 0.2879149913787842
train_iter_loss: 0.12888826429843903
train_iter_loss: 0.059076905250549316
train_iter_loss: 0.11866041272878647
train_iter_loss: 0.14322392642498016
train_iter_loss: 0.21457476913928986
train_iter_loss: 0.10661852359771729
train_iter_loss: 0.08572712540626526
train_iter_loss: 0.17119638621807098
train_iter_loss: 0.13425074517726898
train_iter_loss: 0.19654622673988342
train_iter_loss: 0.11017904430627823
train_iter_loss: 0.1005399078130722
train_iter_loss: 0.09606342762708664
train_iter_loss: 0.09585914760828018
train_iter_loss: 0.2041810154914856
train_iter_loss: 0.043325718492269516
train_iter_loss: 0.18287144601345062
train_iter_loss: 0.16997958719730377
train_iter_loss: 0.17205236852169037
train_iter_loss: 0.0514366440474987
train_iter_loss: 0.11671532690525055
train_iter_loss: 0.09347759187221527
train_iter_loss: 0.24082137644290924
train_iter_loss: 0.10581935942173004
train_iter_loss: 0.12271817028522491
train loss :0.1556
---------------------
Validation seg loss: 0.21641770766099105 at epoch 858
epoch =    859/  1000, exp = train
train_iter_loss: 0.1258409470319748
train_iter_loss: 0.10808413475751877
train_iter_loss: 0.17856624722480774
train_iter_loss: 0.20787079632282257
train_iter_loss: 0.0796082392334938
train_iter_loss: 0.23412209749221802
train_iter_loss: 0.22229138016700745
train_iter_loss: 0.15954139828681946
train_iter_loss: 0.10266292840242386
train_iter_loss: 0.1811702847480774
train_iter_loss: 0.08521809428930283
train_iter_loss: 0.1387321501970291
train_iter_loss: 0.17907801270484924
train_iter_loss: 0.11168742924928665
train_iter_loss: 0.0908559113740921
train_iter_loss: 0.08786656707525253
train_iter_loss: 0.2383365035057068
train_iter_loss: 0.14954331517219543
train_iter_loss: 0.12784013152122498
train_iter_loss: 0.18550065159797668
train_iter_loss: 0.11561071872711182
train_iter_loss: 0.18447402119636536
train_iter_loss: 0.09684817492961884
train_iter_loss: 0.08128140866756439
train_iter_loss: 0.20391206443309784
train_iter_loss: 0.12401655316352844
train_iter_loss: 0.17019771039485931
train_iter_loss: 0.07695413380861282
train_iter_loss: 0.23655655980110168
train_iter_loss: 0.09670181572437286
train_iter_loss: 0.05962666496634483
train_iter_loss: 0.05393245071172714
train_iter_loss: 0.09459589421749115
train_iter_loss: 0.20064915716648102
train_iter_loss: 0.18677666783332825
train_iter_loss: 0.12948869168758392
train_iter_loss: 0.13019491732120514
train_iter_loss: 0.17362035810947418
train_iter_loss: 0.33188316226005554
train_iter_loss: 0.20882122218608856
train_iter_loss: 0.14061173796653748
train_iter_loss: 0.3488338589668274
train_iter_loss: 0.13247966766357422
train_iter_loss: 0.07970147579908371
train_iter_loss: 0.11971482634544373
train_iter_loss: 0.10527470707893372
train_iter_loss: 0.11975354701280594
train_iter_loss: 0.11277347058057785
train_iter_loss: 0.1070668026804924
train_iter_loss: 0.08690077811479568
train_iter_loss: 0.19460339844226837
train_iter_loss: 0.1554519534111023
train_iter_loss: 0.1967194527387619
train_iter_loss: 0.21534906327724457
train_iter_loss: 0.08209481090307236
train_iter_loss: 0.21356640756130219
train_iter_loss: 0.23108139634132385
train_iter_loss: 0.11985558271408081
train_iter_loss: 0.16399706900119781
train_iter_loss: 0.12700948119163513
train_iter_loss: 0.2607148289680481
train_iter_loss: 0.10811866074800491
train_iter_loss: 0.0941392257809639
train_iter_loss: 0.06811755895614624
train_iter_loss: 0.11231353133916855
train_iter_loss: 0.17742833495140076
train_iter_loss: 0.10380896180868149
train_iter_loss: 0.11558177322149277
train_iter_loss: 0.1558358073234558
train_iter_loss: 0.10430881381034851
train_iter_loss: 0.24136371910572052
train_iter_loss: 0.14620256423950195
train_iter_loss: 0.2028462439775467
train_iter_loss: 0.19140969216823578
train_iter_loss: 0.17607805132865906
train_iter_loss: 0.2146361619234085
train_iter_loss: 0.17646531760692596
train_iter_loss: 0.11253555119037628
train_iter_loss: 0.04618190601468086
train_iter_loss: 0.12940357625484467
train_iter_loss: 0.17652985453605652
train_iter_loss: 0.1548149734735489
train_iter_loss: 0.19682781398296356
train_iter_loss: 0.14846183359622955
train_iter_loss: 0.1411651372909546
train_iter_loss: 0.14002954959869385
train_iter_loss: 0.20695781707763672
train_iter_loss: 0.13027356564998627
train_iter_loss: 0.0961410403251648
train_iter_loss: 0.22431935369968414
train_iter_loss: 0.2856574058532715
train_iter_loss: 0.22275923192501068
train_iter_loss: 0.2404918372631073
train_iter_loss: 0.138663187623024
train_iter_loss: 0.19144891202449799
train_iter_loss: 0.17947711050510406
train_iter_loss: 0.11711113154888153
train_iter_loss: 0.053084198385477066
train_iter_loss: 0.13159552216529846
train_iter_loss: 0.15729089081287384
train loss :0.1543
---------------------
Validation seg loss: 0.2136110214446232 at epoch 859
epoch =    860/  1000, exp = train
train_iter_loss: 0.1437544971704483
train_iter_loss: 0.16998609900474548
train_iter_loss: 0.17662914097309113
train_iter_loss: 0.12778446078300476
train_iter_loss: 0.15191850066184998
train_iter_loss: 0.09958495944738388
train_iter_loss: 0.1636641025543213
train_iter_loss: 0.2769475281238556
train_iter_loss: 0.07536385208368301
train_iter_loss: 0.07999123632907867
train_iter_loss: 0.06703173369169235
train_iter_loss: 0.294310063123703
train_iter_loss: 0.14808301627635956
train_iter_loss: 0.27292174100875854
train_iter_loss: 0.23838064074516296
train_iter_loss: 0.14864133298397064
train_iter_loss: 0.1214200109243393
train_iter_loss: 0.10582038015127182
train_iter_loss: 0.1832638829946518
train_iter_loss: 0.07073849439620972
train_iter_loss: 0.09073799103498459
train_iter_loss: 0.10052710026502609
train_iter_loss: 0.2771587073802948
train_iter_loss: 0.10219136625528336
train_iter_loss: 0.04068915173411369
train_iter_loss: 0.15263502299785614
train_iter_loss: 0.1454920470714569
train_iter_loss: 0.1476960927248001
train_iter_loss: 0.23279380798339844
train_iter_loss: 0.0691760703921318
train_iter_loss: 0.16720722615718842
train_iter_loss: 0.12110106647014618
train_iter_loss: 0.14170973002910614
train_iter_loss: 0.2359854131937027
train_iter_loss: 0.11987003684043884
train_iter_loss: 0.22608107328414917
train_iter_loss: 0.0691143348813057
train_iter_loss: 0.17070597410202026
train_iter_loss: 0.15586835145950317
train_iter_loss: 0.14954839646816254
train_iter_loss: 0.07083361595869064
train_iter_loss: 0.2123737782239914
train_iter_loss: 0.10864978283643723
train_iter_loss: 0.12275062501430511
train_iter_loss: 0.10965514183044434
train_iter_loss: 0.08674033731222153
train_iter_loss: 0.267193078994751
train_iter_loss: 0.384834885597229
train_iter_loss: 0.10724737495183945
train_iter_loss: 0.2242860198020935
train_iter_loss: 0.1297803670167923
train_iter_loss: 0.10356312245130539
train_iter_loss: 0.11086022108793259
train_iter_loss: 0.19535648822784424
train_iter_loss: 0.28905773162841797
train_iter_loss: 0.04134019836783409
train_iter_loss: 0.07080603390932083
train_iter_loss: 0.13422240316867828
train_iter_loss: 0.09804915636777878
train_iter_loss: 0.18941833078861237
train_iter_loss: 0.12498054653406143
train_iter_loss: 0.1875801980495453
train_iter_loss: 0.20241384208202362
train_iter_loss: 0.05438718944787979
train_iter_loss: 0.10412934422492981
train_iter_loss: 0.13238778710365295
train_iter_loss: 0.20723553001880646
train_iter_loss: 0.14961008727550507
train_iter_loss: 0.1186337023973465
train_iter_loss: 0.06496205925941467
train_iter_loss: 0.09467621147632599
train_iter_loss: 0.21575245261192322
train_iter_loss: 0.08863043785095215
train_iter_loss: 0.14841893315315247
train_iter_loss: 0.251950740814209
train_iter_loss: 0.160379096865654
train_iter_loss: 0.16701649129390717
train_iter_loss: 0.1110500916838646
train_iter_loss: 0.30313071608543396
train_iter_loss: 0.12272465229034424
train_iter_loss: 0.13467338681221008
train_iter_loss: 0.13167564570903778
train_iter_loss: 0.22298020124435425
train_iter_loss: 0.12476228177547455
train_iter_loss: 0.11183137446641922
train_iter_loss: 0.11507616192102432
train_iter_loss: 0.18949487805366516
train_iter_loss: 0.1241781935095787
train_iter_loss: 0.13345657289028168
train_iter_loss: 0.042185574769973755
train_iter_loss: 0.1446152776479721
train_iter_loss: 0.11113889515399933
train_iter_loss: 0.18023698031902313
train_iter_loss: 0.11583992093801498
train_iter_loss: 0.2524922490119934
train_iter_loss: 0.2036798596382141
train_iter_loss: 0.20692706108093262
train_iter_loss: 0.05297888442873955
train_iter_loss: 0.038069047033786774
train_iter_loss: 0.0559205636382103
train loss :0.1495
---------------------
Validation seg loss: 0.2197182365552575 at epoch 860
epoch =    861/  1000, exp = train
train_iter_loss: 0.11923572421073914
train_iter_loss: 0.320423424243927
train_iter_loss: 0.09210890531539917
train_iter_loss: 0.10799641907215118
train_iter_loss: 0.1812048703432083
train_iter_loss: 0.1777428239583969
train_iter_loss: 0.11173497140407562
train_iter_loss: 0.15920747816562653
train_iter_loss: 0.15824390947818756
train_iter_loss: 0.07789484411478043
train_iter_loss: 0.1423238217830658
train_iter_loss: 0.1734766960144043
train_iter_loss: 0.11050973832607269
train_iter_loss: 0.12506003677845
train_iter_loss: 0.28211671113967896
train_iter_loss: 0.2980119287967682
train_iter_loss: 0.18660901486873627
train_iter_loss: 0.12054304778575897
train_iter_loss: 0.09349893033504486
train_iter_loss: 0.11390508711338043
train_iter_loss: 0.1288098245859146
train_iter_loss: 0.14280866086483002
train_iter_loss: 0.08511779457330704
train_iter_loss: 0.151563823223114
train_iter_loss: 0.13993625342845917
train_iter_loss: 0.1461450159549713
train_iter_loss: 0.084390789270401
train_iter_loss: 0.1431005597114563
train_iter_loss: 0.14470838010311127
train_iter_loss: 0.17583198845386505
train_iter_loss: 0.06853144615888596
train_iter_loss: 0.10478547215461731
train_iter_loss: 0.11926977336406708
train_iter_loss: 0.2384556233882904
train_iter_loss: 0.14894472062587738
train_iter_loss: 0.1267615258693695
train_iter_loss: 0.08464712649583817
train_iter_loss: 0.11970187723636627
train_iter_loss: 0.08551150560379028
train_iter_loss: 0.24657130241394043
train_iter_loss: 0.2503818869590759
train_iter_loss: 0.08071914315223694
train_iter_loss: 0.164580300450325
train_iter_loss: 0.09883043169975281
train_iter_loss: 0.34125277400016785
train_iter_loss: 0.09110351651906967
train_iter_loss: 0.23469489812850952
train_iter_loss: 0.17568299174308777
train_iter_loss: 0.1627214103937149
train_iter_loss: 0.1494177132844925
train_iter_loss: 0.12678980827331543
train_iter_loss: 0.0601312555372715
train_iter_loss: 0.15627430379390717
train_iter_loss: 0.17573098838329315
train_iter_loss: 0.131039097905159
train_iter_loss: 0.20772792398929596
train_iter_loss: 0.28011268377304077
train_iter_loss: 0.12964223325252533
train_iter_loss: 0.17744436860084534
train_iter_loss: 0.051414214074611664
train_iter_loss: 0.14982396364212036
train_iter_loss: 0.16392402350902557
train_iter_loss: 0.12592169642448425
train_iter_loss: 0.0918569564819336
train_iter_loss: 0.3816964626312256
train_iter_loss: 0.07169340550899506
train_iter_loss: 0.2553270757198334
train_iter_loss: 0.13026230037212372
train_iter_loss: 0.1781875342130661
train_iter_loss: 0.34399718046188354
train_iter_loss: 0.136795774102211
train_iter_loss: 0.0991782620549202
train_iter_loss: 0.22602130472660065
train_iter_loss: 0.2084413468837738
train_iter_loss: 0.14823226630687714
train_iter_loss: 0.057106826454401016
train_iter_loss: 0.1254826784133911
train_iter_loss: 0.1543409675359726
train_iter_loss: 0.08916114270687103
train_iter_loss: 0.20159319043159485
train_iter_loss: 0.14565354585647583
train_iter_loss: 0.05472254753112793
train_iter_loss: 0.10572301596403122
train_iter_loss: 0.22316014766693115
train_iter_loss: 0.1478249579668045
train_iter_loss: 0.17821668088436127
train_iter_loss: 0.1681092083454132
train_iter_loss: 0.10269751399755478
train_iter_loss: 0.17735005915164948
train_iter_loss: 0.12793667614459991
train_iter_loss: 0.1323096603155136
train_iter_loss: 0.083504818379879
train_iter_loss: 0.20409461855888367
train_iter_loss: 0.1515083611011505
train_iter_loss: 0.2449728399515152
train_iter_loss: 0.3196653425693512
train_iter_loss: 0.19393141567707062
train_iter_loss: 0.12315881252288818
train_iter_loss: 0.24841628968715668
train_iter_loss: 0.13475678861141205
train loss :0.1584
---------------------
Validation seg loss: 0.21863269056457113 at epoch 861
epoch =    862/  1000, exp = train
train_iter_loss: 0.08132771402597427
train_iter_loss: 0.12309733033180237
train_iter_loss: 0.16434231400489807
train_iter_loss: 0.11654882878065109
train_iter_loss: 0.06657075881958008
train_iter_loss: 0.1345294713973999
train_iter_loss: 0.11651284992694855
train_iter_loss: 0.17266350984573364
train_iter_loss: 0.1168484091758728
train_iter_loss: 0.07820738852024078
train_iter_loss: 0.12101024389266968
train_iter_loss: 0.22402355074882507
train_iter_loss: 0.059344351291656494
train_iter_loss: 0.20706243813037872
train_iter_loss: 0.14118917286396027
train_iter_loss: 0.37909892201423645
train_iter_loss: 0.07331281900405884
train_iter_loss: 0.21472986042499542
train_iter_loss: 0.18042981624603271
train_iter_loss: 0.11207807809114456
train_iter_loss: 0.1852322667837143
train_iter_loss: 0.09827978909015656
train_iter_loss: 0.19123385846614838
train_iter_loss: 0.0854354277253151
train_iter_loss: 0.3056338131427765
train_iter_loss: 0.11996341496706009
train_iter_loss: 0.24670778214931488
train_iter_loss: 0.27290764451026917
train_iter_loss: 0.29493874311447144
train_iter_loss: 0.12682950496673584
train_iter_loss: 0.06452658772468567
train_iter_loss: 0.20243945717811584
train_iter_loss: 0.2341012954711914
train_iter_loss: 0.08570606261491776
train_iter_loss: 0.17798437178134918
train_iter_loss: 0.2928505539894104
train_iter_loss: 0.23023253679275513
train_iter_loss: 0.24016325175762177
train_iter_loss: 0.0707581415772438
train_iter_loss: 0.22710080444812775
train_iter_loss: 0.053746793419122696
train_iter_loss: 0.1003514751791954
train_iter_loss: 0.18079179525375366
train_iter_loss: 0.15396329760551453
train_iter_loss: 0.24802426993846893
train_iter_loss: 0.18616284430027008
train_iter_loss: 0.1148819550871849
train_iter_loss: 0.13440436124801636
train_iter_loss: 0.18411441147327423
train_iter_loss: 0.11261171102523804
train_iter_loss: 0.15722385048866272
train_iter_loss: 0.18899665772914886
train_iter_loss: 0.0728335976600647
train_iter_loss: 0.09856820106506348
train_iter_loss: 0.19982801377773285
train_iter_loss: 0.10554409772157669
train_iter_loss: 0.04365340247750282
train_iter_loss: 0.14064045250415802
train_iter_loss: 0.11833499372005463
train_iter_loss: 0.1599501669406891
train_iter_loss: 0.11511370539665222
train_iter_loss: 0.18456539511680603
train_iter_loss: 0.1972220540046692
train_iter_loss: 0.10466469824314117
train_iter_loss: 0.08900158852338791
train_iter_loss: 0.12023874372243881
train_iter_loss: 0.0875881165266037
train_iter_loss: 0.14843885600566864
train_iter_loss: 0.10333599150180817
train_iter_loss: 0.18802425265312195
train_iter_loss: 0.2599112391471863
train_iter_loss: 0.07934147864580154
train_iter_loss: 0.1449842005968094
train_iter_loss: 0.123423270881176
train_iter_loss: 0.1725093126296997
train_iter_loss: 0.1809372901916504
train_iter_loss: 0.06599219888448715
train_iter_loss: 0.13773326575756073
train_iter_loss: 0.14054033160209656
train_iter_loss: 0.10933761298656464
train_iter_loss: 0.17888501286506653
train_iter_loss: 0.2521132826805115
train_iter_loss: 0.22086217999458313
train_iter_loss: 0.1336953490972519
train_iter_loss: 0.13288982212543488
train_iter_loss: 0.26772764325141907
train_iter_loss: 0.21197983622550964
train_iter_loss: 0.0824616551399231
train_iter_loss: 0.256657212972641
train_iter_loss: 0.12040943652391434
train_iter_loss: 0.10723765194416046
train_iter_loss: 0.16601912677288055
train_iter_loss: 0.07902121543884277
train_iter_loss: 0.29616719484329224
train_iter_loss: 0.10806519538164139
train_iter_loss: 0.17811045050621033
train_iter_loss: 0.06528596580028534
train_iter_loss: 0.15736863017082214
train_iter_loss: 0.1461350917816162
train_iter_loss: 0.2265016734600067
train loss :0.1569
---------------------
Validation seg loss: 0.21131041324433852 at epoch 862
epoch =    863/  1000, exp = train
train_iter_loss: 0.20763452351093292
train_iter_loss: 0.1480703502893448
train_iter_loss: 0.2468131184577942
train_iter_loss: 0.09321191906929016
train_iter_loss: 0.12105952948331833
train_iter_loss: 0.12633280456066132
train_iter_loss: 0.1198849231004715
train_iter_loss: 0.17078104615211487
train_iter_loss: 0.22674359381198883
train_iter_loss: 0.12733466923236847
train_iter_loss: 0.098122738301754
train_iter_loss: 0.14965033531188965
train_iter_loss: 0.13328847289085388
train_iter_loss: 0.20220035314559937
train_iter_loss: 0.23594507575035095
train_iter_loss: 0.12947478890419006
train_iter_loss: 0.1919461339712143
train_iter_loss: 0.10268790274858475
train_iter_loss: 0.1893126666545868
train_iter_loss: 0.06728705763816833
train_iter_loss: 0.16199426352977753
train_iter_loss: 0.4859868586063385
train_iter_loss: 0.15125052630901337
train_iter_loss: 0.09777296334505081
train_iter_loss: 0.17388568818569183
train_iter_loss: 0.17888571321964264
train_iter_loss: 0.11573328077793121
train_iter_loss: 0.17671072483062744
train_iter_loss: 0.11583660542964935
train_iter_loss: 0.1508091688156128
train_iter_loss: 0.10834761708974838
train_iter_loss: 0.14993345737457275
train_iter_loss: 0.06557095050811768
train_iter_loss: 0.29829347133636475
train_iter_loss: 0.29101887345314026
train_iter_loss: 0.14553667604923248
train_iter_loss: 0.14101354777812958
train_iter_loss: 0.19669903814792633
train_iter_loss: 0.2029656618833542
train_iter_loss: 0.1514248102903366
train_iter_loss: 0.14834079146385193
train_iter_loss: 0.21888932585716248
train_iter_loss: 0.14118000864982605
train_iter_loss: 0.05914648249745369
train_iter_loss: 0.16411103308200836
train_iter_loss: 0.1117592602968216
train_iter_loss: 0.10438578575849533
train_iter_loss: 0.10306927561759949
train_iter_loss: 0.17378298938274384
train_iter_loss: 0.23057691752910614
train_iter_loss: 0.13742870092391968
train_iter_loss: 0.1749967336654663
train_iter_loss: 0.13636977970600128
train_iter_loss: 0.23509584367275238
train_iter_loss: 0.08120813220739365
train_iter_loss: 0.09305266290903091
train_iter_loss: 0.12807460129261017
train_iter_loss: 0.4217212498188019
train_iter_loss: 0.09431831538677216
train_iter_loss: 0.16387136280536652
train_iter_loss: 0.14889585971832275
train_iter_loss: 0.13553176820278168
train_iter_loss: 0.1096116453409195
train_iter_loss: 0.19688576459884644
train_iter_loss: 0.07469525188207626
train_iter_loss: 0.14728716015815735
train_iter_loss: 0.19139498472213745
train_iter_loss: 0.12137758731842041
train_iter_loss: 0.14078818261623383
train_iter_loss: 0.17177972197532654
train_iter_loss: 0.2088732123374939
train_iter_loss: 0.19670340418815613
train_iter_loss: 0.1867084801197052
train_iter_loss: 0.08820923417806625
train_iter_loss: 0.23019124567508698
train_iter_loss: 0.08287010341882706
train_iter_loss: 0.07897291332483292
train_iter_loss: 0.17767183482646942
train_iter_loss: 0.19043345749378204
train_iter_loss: 0.13394637405872345
train_iter_loss: 0.16617324948310852
train_iter_loss: 0.16638235747814178
train_iter_loss: 0.16992348432540894
train_iter_loss: 0.16805499792099
train_iter_loss: 0.11501691490411758
train_iter_loss: 0.18407857418060303
train_iter_loss: 0.10920130461454391
train_iter_loss: 0.19295848906040192
train_iter_loss: 0.28017017245292664
train_iter_loss: 0.09120237827301025
train_iter_loss: 0.18778732419013977
train_iter_loss: 0.10534151643514633
train_iter_loss: 0.16565993428230286
train_iter_loss: 0.17899872362613678
train_iter_loss: 0.10934935510158539
train_iter_loss: 0.27215105295181274
train_iter_loss: 0.10609302669763565
train_iter_loss: 0.2875955104827881
train_iter_loss: 0.16386961936950684
train_iter_loss: 0.09378550201654434
train loss :0.1625
---------------------
Validation seg loss: 0.22218002885017754 at epoch 863
epoch =    864/  1000, exp = train
train_iter_loss: 0.23395192623138428
train_iter_loss: 0.19795389473438263
train_iter_loss: 0.14599931240081787
train_iter_loss: 0.057466231286525726
train_iter_loss: 0.13317783176898956
train_iter_loss: 0.15921807289123535
train_iter_loss: 0.09116639196872711
train_iter_loss: 0.1268329918384552
train_iter_loss: 0.18640877306461334
train_iter_loss: 0.11640560626983643
train_iter_loss: 0.14199991524219513
train_iter_loss: 0.20034384727478027
train_iter_loss: 0.18002139031887054
train_iter_loss: 0.1403137743473053
train_iter_loss: 0.23504282534122467
train_iter_loss: 0.16298578679561615
train_iter_loss: 0.1646682620048523
train_iter_loss: 0.1571837067604065
train_iter_loss: 0.18687251210212708
train_iter_loss: 0.1288093626499176
train_iter_loss: 0.08505357801914215
train_iter_loss: 0.1712133139371872
train_iter_loss: 0.12629473209381104
train_iter_loss: 0.035782597959041595
train_iter_loss: 0.07540832459926605
train_iter_loss: 0.10266103595495224
train_iter_loss: 0.20339824259281158
train_iter_loss: 0.33351844549179077
train_iter_loss: 0.12719963490962982
train_iter_loss: 0.20280392467975616
train_iter_loss: 0.2589840590953827
train_iter_loss: 0.10957015305757523
train_iter_loss: 0.09896153211593628
train_iter_loss: 0.19243066012859344
train_iter_loss: 0.03811245784163475
train_iter_loss: 0.11171115189790726
train_iter_loss: 0.1312050074338913
train_iter_loss: 0.1567297875881195
train_iter_loss: 0.21563264727592468
train_iter_loss: 0.14449690282344818
train_iter_loss: 0.1376553773880005
train_iter_loss: 0.10014287382364273
train_iter_loss: 0.12864406406879425
train_iter_loss: 0.22700484097003937
train_iter_loss: 0.09864652901887894
train_iter_loss: 0.1138944998383522
train_iter_loss: 0.10943270474672318
train_iter_loss: 0.2655433118343353
train_iter_loss: 0.10317213833332062
train_iter_loss: 0.17365680634975433
train_iter_loss: 0.1671844720840454
train_iter_loss: 0.21804043650627136
train_iter_loss: 0.14913871884346008
train_iter_loss: 0.1962660402059555
train_iter_loss: 0.2500319182872772
train_iter_loss: 0.1673865169286728
train_iter_loss: 0.09788601845502853
train_iter_loss: 0.13649585843086243
train_iter_loss: 0.1447449028491974
train_iter_loss: 0.10629012435674667
train_iter_loss: 0.1514211744070053
train_iter_loss: 0.10920648276805878
train_iter_loss: 0.17865777015686035
train_iter_loss: 0.13818727433681488
train_iter_loss: 0.22895409166812897
train_iter_loss: 0.350633829832077
train_iter_loss: 0.10229222476482391
train_iter_loss: 0.26353543996810913
train_iter_loss: 0.1161622554063797
train_iter_loss: 0.12424253672361374
train_iter_loss: 0.22094470262527466
train_iter_loss: 0.13464032113552094
train_iter_loss: 0.11267975717782974
train_iter_loss: 0.10201496630907059
train_iter_loss: 0.14971527457237244
train_iter_loss: 0.2525787949562073
train_iter_loss: 0.14976529777050018
train_iter_loss: 0.14920412003993988
train_iter_loss: 0.15743902325630188
train_iter_loss: 0.07435460388660431
train_iter_loss: 0.05221276730298996
train_iter_loss: 0.2697879672050476
train_iter_loss: 0.14511187374591827
train_iter_loss: 0.06953883916139603
train_iter_loss: 0.11851795017719269
train_iter_loss: 0.1922542303800583
train_iter_loss: 0.13130563497543335
train_iter_loss: 0.19608409702777863
train_iter_loss: 0.19640818238258362
train_iter_loss: 0.2902630865573883
train_iter_loss: 0.14281342923641205
train_iter_loss: 0.13214965164661407
train_iter_loss: 0.09287824481725693
train_iter_loss: 0.0953727513551712
train_iter_loss: 0.17591579258441925
train_iter_loss: 0.07455146312713623
train_iter_loss: 0.15708677470684052
train_iter_loss: 0.10057703405618668
train_iter_loss: 0.12410828471183777
train_iter_loss: 0.1833011955022812
train loss :0.1553
---------------------
Validation seg loss: 0.21549142696806844 at epoch 864
epoch =    865/  1000, exp = train
train_iter_loss: 0.12235649675130844
train_iter_loss: 0.15865689516067505
train_iter_loss: 0.20679669082164764
train_iter_loss: 0.1111573725938797
train_iter_loss: 0.2722291946411133
train_iter_loss: 0.07259110361337662
train_iter_loss: 0.20051299035549164
train_iter_loss: 0.18371525406837463
train_iter_loss: 0.23730547726154327
train_iter_loss: 0.12080244719982147
train_iter_loss: 0.19857662916183472
train_iter_loss: 0.13155615329742432
train_iter_loss: 0.17737188935279846
train_iter_loss: 0.15207163989543915
train_iter_loss: 0.22505584359169006
train_iter_loss: 0.2042388617992401
train_iter_loss: 0.15363165736198425
train_iter_loss: 0.21476006507873535
train_iter_loss: 0.06461770832538605
train_iter_loss: 0.3318280577659607
train_iter_loss: 0.20450863242149353
train_iter_loss: 0.17306718230247498
train_iter_loss: 0.3335842192173004
train_iter_loss: 0.18189719319343567
train_iter_loss: 0.10314873605966568
train_iter_loss: 0.20345067977905273
train_iter_loss: 0.11867142468690872
train_iter_loss: 0.21863839030265808
train_iter_loss: 0.07410725951194763
train_iter_loss: 0.15586277842521667
train_iter_loss: 0.16187050938606262
train_iter_loss: 0.18307895958423615
train_iter_loss: 0.1609833836555481
train_iter_loss: 0.1433580219745636
train_iter_loss: 0.14610005915164948
train_iter_loss: 0.04517659172415733
train_iter_loss: 0.18134759366512299
train_iter_loss: 0.24861961603164673
train_iter_loss: 0.11858745664358139
train_iter_loss: 0.10634179413318634
train_iter_loss: 0.1589958220720291
train_iter_loss: 0.08443091809749603
train_iter_loss: 0.1188090592622757
train_iter_loss: 0.19936539232730865
train_iter_loss: 0.21740928292274475
train_iter_loss: 0.10109405219554901
train_iter_loss: 0.12219413369894028
train_iter_loss: 0.19008968770503998
train_iter_loss: 0.07932562381029129
train_iter_loss: 0.18483886122703552
train_iter_loss: 0.08586373180150986
train_iter_loss: 0.13687054812908173
train_iter_loss: 0.02843531034886837
train_iter_loss: 0.13787774741649628
train_iter_loss: 0.21769823133945465
train_iter_loss: 0.18140588700771332
train_iter_loss: 0.09119897335767746
train_iter_loss: 0.14635254442691803
train_iter_loss: 0.09768277406692505
train_iter_loss: 0.10697901248931885
train_iter_loss: 0.07274676114320755
train_iter_loss: 0.2778984606266022
train_iter_loss: 0.12434807419776917
train_iter_loss: 0.16529028117656708
train_iter_loss: 0.0638827234506607
train_iter_loss: 0.09113117307424545
train_iter_loss: 0.1497708261013031
train_iter_loss: 0.245277538895607
train_iter_loss: 0.20102328062057495
train_iter_loss: 0.19999292492866516
train_iter_loss: 0.12450818717479706
train_iter_loss: 0.10051190853118896
train_iter_loss: 0.15175530314445496
train_iter_loss: 0.08666028082370758
train_iter_loss: 0.09921250492334366
train_iter_loss: 0.09962622821331024
train_iter_loss: 0.14539417624473572
train_iter_loss: 0.14962807297706604
train_iter_loss: 0.10890225321054459
train_iter_loss: 0.1720099002122879
train_iter_loss: 0.0988626703619957
train_iter_loss: 0.10294640809297562
train_iter_loss: 0.1191723495721817
train_iter_loss: 0.18912707269191742
train_iter_loss: 0.17075514793395996
train_iter_loss: 0.1784234344959259
train_iter_loss: 0.1137647032737732
train_iter_loss: 0.12217146158218384
train_iter_loss: 0.21988576650619507
train_iter_loss: 0.08796138316392899
train_iter_loss: 0.1683809757232666
train_iter_loss: 0.1447683572769165
train_iter_loss: 0.09625548869371414
train_iter_loss: 0.10512518882751465
train_iter_loss: 0.3102312982082367
train_iter_loss: 0.22275955975055695
train_iter_loss: 0.1229606494307518
train_iter_loss: 0.12265453487634659
train_iter_loss: 0.19562090933322906
train_iter_loss: 0.13804402947425842
train loss :0.1550
---------------------
Validation seg loss: 0.2160694402783125 at epoch 865
epoch =    866/  1000, exp = train
train_iter_loss: 0.1442616879940033
train_iter_loss: 0.20389065146446228
train_iter_loss: 0.1740981787443161
train_iter_loss: 0.1431528478860855
train_iter_loss: 0.15468347072601318
train_iter_loss: 0.10705367475748062
train_iter_loss: 0.11224670708179474
train_iter_loss: 0.13748177886009216
train_iter_loss: 0.1404537558555603
train_iter_loss: 0.10645715892314911
train_iter_loss: 0.11758136004209518
train_iter_loss: 0.20519359409809113
train_iter_loss: 0.1361481249332428
train_iter_loss: 0.10388945788145065
train_iter_loss: 0.12038193643093109
train_iter_loss: 0.16922606527805328
train_iter_loss: 0.12652891874313354
train_iter_loss: 0.044090066105127335
train_iter_loss: 0.28855422139167786
train_iter_loss: 0.1581292748451233
train_iter_loss: 0.16421549022197723
train_iter_loss: 0.15269649028778076
train_iter_loss: 0.11685096472501755
train_iter_loss: 0.14773455262184143
train_iter_loss: 0.08628269284963608
train_iter_loss: 0.1007378101348877
train_iter_loss: 0.1813371628522873
train_iter_loss: 0.10684958100318909
train_iter_loss: 0.18996483087539673
train_iter_loss: 0.1664752960205078
train_iter_loss: 0.19106155633926392
train_iter_loss: 0.18717940151691437
train_iter_loss: 0.13944044709205627
train_iter_loss: 0.30117106437683105
train_iter_loss: 0.08771783113479614
train_iter_loss: 0.10679052770137787
train_iter_loss: 0.13476736843585968
train_iter_loss: 0.25030940771102905
train_iter_loss: 0.2103288322687149
train_iter_loss: 0.1152992844581604
train_iter_loss: 0.13674187660217285
train_iter_loss: 0.20873552560806274
train_iter_loss: 0.07908910512924194
train_iter_loss: 0.10251224786043167
train_iter_loss: 0.16484956443309784
train_iter_loss: 0.14839066565036774
train_iter_loss: 0.20687173306941986
train_iter_loss: 0.0876082256436348
train_iter_loss: 0.22160454094409943
train_iter_loss: 0.1473669409751892
train_iter_loss: 0.17967544496059418
train_iter_loss: 0.268227756023407
train_iter_loss: 0.11637582629919052
train_iter_loss: 0.12210658192634583
train_iter_loss: 0.08844584971666336
train_iter_loss: 0.18275657296180725
train_iter_loss: 0.09428615123033524
train_iter_loss: 0.057742923498153687
train_iter_loss: 0.022617658600211143
train_iter_loss: 0.13286083936691284
train_iter_loss: 0.1945219188928604
train_iter_loss: 0.22341562807559967
train_iter_loss: 0.26158639788627625
train_iter_loss: 0.14558611810207367
train_iter_loss: 0.035856422036886215
train_iter_loss: 0.11073464900255203
train_iter_loss: 0.24342164397239685
train_iter_loss: 0.09651918709278107
train_iter_loss: 0.12408377975225449
train_iter_loss: 0.21166378259658813
train_iter_loss: 0.16852223873138428
train_iter_loss: 0.1412035971879959
train_iter_loss: 0.15731126070022583
train_iter_loss: 0.09434537589550018
train_iter_loss: 0.08562126010656357
train_iter_loss: 0.1687663048505783
train_iter_loss: 0.17265553772449493
train_iter_loss: 0.22432895004749298
train_iter_loss: 0.13372625410556793
train_iter_loss: 0.11782331019639969
train_iter_loss: 0.29052698612213135
train_iter_loss: 0.14699621498584747
train_iter_loss: 0.17404542863368988
train_iter_loss: 0.1739032119512558
train_iter_loss: 0.1749003827571869
train_iter_loss: 0.23391330242156982
train_iter_loss: 0.11372943967580795
train_iter_loss: 0.18780317902565002
train_iter_loss: 0.10613562911748886
train_iter_loss: 0.09879732877016068
train_iter_loss: 0.20757846534252167
train_iter_loss: 0.1493300497531891
train_iter_loss: 0.1284198760986328
train_iter_loss: 0.24077169597148895
train_iter_loss: 0.18004147708415985
train_iter_loss: 0.0874825119972229
train_iter_loss: 0.1823960691690445
train_iter_loss: 0.13591104745864868
train_iter_loss: 0.16940996050834656
train_iter_loss: 0.11968311667442322
train loss :0.1537
---------------------
Validation seg loss: 0.21605629857875547 at epoch 866
epoch =    867/  1000, exp = train
train_iter_loss: 0.11965013295412064
train_iter_loss: 0.18551447987556458
train_iter_loss: 0.24364817142486572
train_iter_loss: 0.1554448902606964
train_iter_loss: 0.09505178779363632
train_iter_loss: 0.25111547112464905
train_iter_loss: 0.1601531207561493
train_iter_loss: 0.1868969053030014
train_iter_loss: 0.18294109404087067
train_iter_loss: 0.14992402493953705
train_iter_loss: 0.07825522124767303
train_iter_loss: 0.057646166533231735
train_iter_loss: 0.16399478912353516
train_iter_loss: 0.19989000260829926
train_iter_loss: 0.15749618411064148
train_iter_loss: 0.18132291734218597
train_iter_loss: 0.08590931445360184
train_iter_loss: 0.200631245970726
train_iter_loss: 0.09214863926172256
train_iter_loss: 0.23684097826480865
train_iter_loss: 0.1285368651151657
train_iter_loss: 0.1109844222664833
train_iter_loss: 0.08499034494161606
train_iter_loss: 0.12322067469358444
train_iter_loss: 0.15466639399528503
train_iter_loss: 0.2547746002674103
train_iter_loss: 0.09510528296232224
train_iter_loss: 0.05544380471110344
train_iter_loss: 0.24229639768600464
train_iter_loss: 0.1509122997522354
train_iter_loss: 0.1596687287092209
train_iter_loss: 0.10889861732721329
train_iter_loss: 0.15168584883213043
train_iter_loss: 0.2854458689689636
train_iter_loss: 0.10378554463386536
train_iter_loss: 0.1352139562368393
train_iter_loss: 0.28871768712997437
train_iter_loss: 0.14208655059337616
train_iter_loss: 0.12520453333854675
train_iter_loss: 0.1287546306848526
train_iter_loss: 0.12423820793628693
train_iter_loss: 0.23495803773403168
train_iter_loss: 0.06395905464887619
train_iter_loss: 0.1984872668981552
train_iter_loss: 0.09424538165330887
train_iter_loss: 0.10477641224861145
train_iter_loss: 0.09973080456256866
train_iter_loss: 0.18052606284618378
train_iter_loss: 0.13741283118724823
train_iter_loss: 0.07788290828466415
train_iter_loss: 0.18449364602565765
train_iter_loss: 0.16779816150665283
train_iter_loss: 0.18245407938957214
train_iter_loss: 0.24124878644943237
train_iter_loss: 0.215949147939682
train_iter_loss: 0.1557360738515854
train_iter_loss: 0.13346236944198608
train_iter_loss: 0.18886058032512665
train_iter_loss: 0.11071723699569702
train_iter_loss: 0.12771302461624146
train_iter_loss: 0.09582843631505966
train_iter_loss: 0.10641197115182877
train_iter_loss: 0.21953801810741425
train_iter_loss: 0.16815990209579468
train_iter_loss: 0.13727688789367676
train_iter_loss: 0.25676411390304565
train_iter_loss: 0.12938982248306274
train_iter_loss: 0.037308719009160995
train_iter_loss: 0.22674718499183655
train_iter_loss: 0.06503942608833313
train_iter_loss: 0.20198197662830353
train_iter_loss: 0.23819223046302795
train_iter_loss: 0.1848730891942978
train_iter_loss: 0.275105744600296
train_iter_loss: 0.21120622754096985
train_iter_loss: 0.10315504670143127
train_iter_loss: 0.06387003511190414
train_iter_loss: 0.11736026406288147
train_iter_loss: 0.13342808187007904
train_iter_loss: 0.09382130205631256
train_iter_loss: 0.049761272966861725
train_iter_loss: 0.16232973337173462
train_iter_loss: 0.2541697323322296
train_iter_loss: 0.1792566180229187
train_iter_loss: 0.16834837198257446
train_iter_loss: 0.2814275026321411
train_iter_loss: 0.12427382171154022
train_iter_loss: 0.15111063420772552
train_iter_loss: 0.1176234781742096
train_iter_loss: 0.09520003199577332
train_iter_loss: 0.07927628606557846
train_iter_loss: 0.18939217925071716
train_iter_loss: 0.1756402999162674
train_iter_loss: 0.13539573550224304
train_iter_loss: 0.1746004968881607
train_iter_loss: 0.10598026216030121
train_iter_loss: 0.12017122656106949
train_iter_loss: 0.09738300740718842
train_iter_loss: 0.18010573089122772
train_iter_loss: 0.11740749329328537
train loss :0.1542
---------------------
Validation seg loss: 0.2146538277332372 at epoch 867
epoch =    868/  1000, exp = train
train_iter_loss: 0.1692652553319931
train_iter_loss: 0.18779395520687103
train_iter_loss: 0.1398904025554657
train_iter_loss: 0.09309311956167221
train_iter_loss: 0.15359029173851013
train_iter_loss: 0.1864951103925705
train_iter_loss: 0.20131903886795044
train_iter_loss: 0.15033642947673798
train_iter_loss: 0.18975888192653656
train_iter_loss: 0.15203584730625153
train_iter_loss: 0.09693500399589539
train_iter_loss: 0.14977525174617767
train_iter_loss: 0.15917296707630157
train_iter_loss: 0.18059469759464264
train_iter_loss: 0.16893208026885986
train_iter_loss: 0.14678005874156952
train_iter_loss: 0.18623150885105133
train_iter_loss: 0.10459071397781372
train_iter_loss: 0.29330602288246155
train_iter_loss: 0.12724077701568604
train_iter_loss: 0.11821497231721878
train_iter_loss: 0.12239249795675278
train_iter_loss: 0.22959934175014496
train_iter_loss: 0.09060612320899963
train_iter_loss: 0.09684064239263535
train_iter_loss: 0.11582232266664505
train_iter_loss: 0.16143988072872162
train_iter_loss: 0.10970204323530197
train_iter_loss: 0.1841384321451187
train_iter_loss: 0.14659754931926727
train_iter_loss: 0.1312515139579773
train_iter_loss: 0.19357067346572876
train_iter_loss: 0.19068187475204468
train_iter_loss: 0.22058798372745514
train_iter_loss: 0.06492026150226593
train_iter_loss: 0.14110180735588074
train_iter_loss: 0.14763320982456207
train_iter_loss: 0.215122789144516
train_iter_loss: 0.06657986342906952
train_iter_loss: 0.17990654706954956
train_iter_loss: 0.08549424260854721
train_iter_loss: 0.18336176872253418
train_iter_loss: 0.18121160566806793
train_iter_loss: 0.14011555910110474
train_iter_loss: 0.18256613612174988
train_iter_loss: 0.09934327006340027
train_iter_loss: 0.14827141165733337
train_iter_loss: 0.27592894434928894
train_iter_loss: 0.1455916464328766
train_iter_loss: 0.14264313876628876
train_iter_loss: 0.1322292536497116
train_iter_loss: 0.20089074969291687
train_iter_loss: 0.10435647517442703
train_iter_loss: 0.10047093778848648
train_iter_loss: 0.12793117761611938
train_iter_loss: 0.12504331767559052
train_iter_loss: 0.17744186520576477
train_iter_loss: 0.1523495316505432
train_iter_loss: 0.09790786355733871
train_iter_loss: 0.2005455046892166
train_iter_loss: 0.14065562188625336
train_iter_loss: 0.11801684647798538
train_iter_loss: 0.2289741337299347
train_iter_loss: 0.17834362387657166
train_iter_loss: 0.07695651799440384
train_iter_loss: 0.05786062777042389
train_iter_loss: 0.15326856076717377
train_iter_loss: 0.07750345766544342
train_iter_loss: 0.10358043760061264
train_iter_loss: 0.19633886218070984
train_iter_loss: 0.07585036754608154
train_iter_loss: 0.07684458792209625
train_iter_loss: 0.10227714478969574
train_iter_loss: 0.23924314975738525
train_iter_loss: 0.12815448641777039
train_iter_loss: 0.23707658052444458
train_iter_loss: 0.12274245917797089
train_iter_loss: 0.14558856189250946
train_iter_loss: 0.2629890441894531
train_iter_loss: 0.17044022679328918
train_iter_loss: 0.08561453223228455
train_iter_loss: 0.1661057025194168
train_iter_loss: 0.12161239236593246
train_iter_loss: 0.07012691348791122
train_iter_loss: 0.16112498939037323
train_iter_loss: 0.0810735747218132
train_iter_loss: 0.15771512687206268
train_iter_loss: 0.11680876463651657
train_iter_loss: 0.18695227801799774
train_iter_loss: 0.12213350832462311
train_iter_loss: 0.1299605667591095
train_iter_loss: 0.1972765028476715
train_iter_loss: 0.1491548866033554
train_iter_loss: 0.16570328176021576
train_iter_loss: 0.20062704384326935
train_iter_loss: 0.2608458399772644
train_iter_loss: 0.23682135343551636
train_iter_loss: 0.194926917552948
train_iter_loss: 0.14068472385406494
train_iter_loss: 0.13436925411224365
train loss :0.1529
---------------------
Validation seg loss: 0.21857998847855992 at epoch 868
epoch =    869/  1000, exp = train
train_iter_loss: 0.08197387307882309
train_iter_loss: 0.201773002743721
train_iter_loss: 0.19707834720611572
train_iter_loss: 0.26408299803733826
train_iter_loss: 0.016420481726527214
train_iter_loss: 0.200339674949646
train_iter_loss: 0.1639428734779358
train_iter_loss: 0.18546967208385468
train_iter_loss: 0.1769668161869049
train_iter_loss: 0.12484541535377502
train_iter_loss: 0.20203988254070282
train_iter_loss: 0.2439943552017212
train_iter_loss: 0.07959366589784622
train_iter_loss: 0.1612621396780014
train_iter_loss: 0.1497129648923874
train_iter_loss: 0.1226000189781189
train_iter_loss: 0.2234097421169281
train_iter_loss: 0.13481639325618744
train_iter_loss: 0.19621612131595612
train_iter_loss: 0.12440241873264313
train_iter_loss: 0.17726193368434906
train_iter_loss: 0.13989657163619995
train_iter_loss: 0.13037952780723572
train_iter_loss: 0.1935396045446396
train_iter_loss: 0.21845649182796478
train_iter_loss: 0.03587927296757698
train_iter_loss: 0.1435042768716812
train_iter_loss: 0.09157022088766098
train_iter_loss: 0.1046256273984909
train_iter_loss: 0.0940292701125145
train_iter_loss: 0.12074017524719238
train_iter_loss: 0.2095891386270523
train_iter_loss: 0.15540684759616852
train_iter_loss: 0.11370409280061722
train_iter_loss: 0.11012007296085358
train_iter_loss: 0.09321978688240051
train_iter_loss: 0.10869252681732178
train_iter_loss: 0.32137662172317505
train_iter_loss: 0.173201784491539
train_iter_loss: 0.051017146557569504
train_iter_loss: 0.10518132150173187
train_iter_loss: 0.22128398716449738
train_iter_loss: 0.14362066984176636
train_iter_loss: 0.12277978658676147
train_iter_loss: 0.23259611427783966
train_iter_loss: 0.1629597395658493
train_iter_loss: 0.1447419822216034
train_iter_loss: 0.17538093030452728
train_iter_loss: 0.1418648511171341
train_iter_loss: 0.10856932401657104
train_iter_loss: 0.21236944198608398
train_iter_loss: 0.1689223200082779
train_iter_loss: 0.08491441607475281
train_iter_loss: 0.11134640872478485
train_iter_loss: 0.195614293217659
train_iter_loss: 0.129287987947464
train_iter_loss: 0.1855134814977646
train_iter_loss: 0.16193756461143494
train_iter_loss: 0.16126053035259247
train_iter_loss: 0.19232021272182465
train_iter_loss: 0.13929159939289093
train_iter_loss: 0.11986228823661804
train_iter_loss: 0.07851039618253708
train_iter_loss: 0.20133140683174133
train_iter_loss: 0.06320276856422424
train_iter_loss: 0.08833882212638855
train_iter_loss: 0.12354536354541779
train_iter_loss: 0.3033590316772461
train_iter_loss: 0.112753726541996
train_iter_loss: 0.3821560740470886
train_iter_loss: 0.18142521381378174
train_iter_loss: 0.17483718693256378
train_iter_loss: 0.21677911281585693
train_iter_loss: 0.13950073719024658
train_iter_loss: 0.29330411553382874
train_iter_loss: 0.10779973864555359
train_iter_loss: 0.05987536907196045
train_iter_loss: 0.09487040340900421
train_iter_loss: 0.10874612629413605
train_iter_loss: 0.1871020793914795
train_iter_loss: 0.15174156427383423
train_iter_loss: 0.2560389041900635
train_iter_loss: 0.1260523498058319
train_iter_loss: 0.13348643481731415
train_iter_loss: 0.11743035912513733
train_iter_loss: 0.18500632047653198
train_iter_loss: 0.08842003345489502
train_iter_loss: 0.1190035492181778
train_iter_loss: 0.23609456419944763
train_iter_loss: 0.18728451430797577
train_iter_loss: 0.17313426733016968
train_iter_loss: 0.09718338400125504
train_iter_loss: 0.19146381318569183
train_iter_loss: 0.1648171991109848
train_iter_loss: 0.16278278827667236
train_iter_loss: 0.06612535566091537
train_iter_loss: 0.2597120404243469
train_iter_loss: 0.08420554548501968
train_iter_loss: 0.08859783411026001
train_iter_loss: 0.2546157240867615
train loss :0.1567
---------------------
Validation seg loss: 0.21598890362272286 at epoch 869
epoch =    870/  1000, exp = train
train_iter_loss: 0.10034321248531342
train_iter_loss: 0.184262216091156
train_iter_loss: 0.1921643316745758
train_iter_loss: 0.2025390863418579
train_iter_loss: 0.17146547138690948
train_iter_loss: 0.11146502941846848
train_iter_loss: 0.14979222416877747
train_iter_loss: 0.12062346935272217
train_iter_loss: 0.06409500539302826
train_iter_loss: 0.06710432469844818
train_iter_loss: 0.10372386127710342
train_iter_loss: 0.17676131427288055
train_iter_loss: 0.22084775567054749
train_iter_loss: 0.07366909086704254
train_iter_loss: 0.29060935974121094
train_iter_loss: 0.11989783495664597
train_iter_loss: 0.08470022678375244
train_iter_loss: 0.1450127810239792
train_iter_loss: 0.08283371478319168
train_iter_loss: 0.24700884521007538
train_iter_loss: 0.21027709543704987
train_iter_loss: 0.20079492032527924
train_iter_loss: 0.12046768516302109
train_iter_loss: 0.1924377679824829
train_iter_loss: 0.1356583684682846
train_iter_loss: 0.1543627679347992
train_iter_loss: 0.2433280050754547
train_iter_loss: 0.14242400228977203
train_iter_loss: 0.16892921924591064
train_iter_loss: 0.061451394110918045
train_iter_loss: 0.1380724310874939
train_iter_loss: 0.15491853654384613
train_iter_loss: 0.12771281599998474
train_iter_loss: 0.16008755564689636
train_iter_loss: 0.1514374315738678
train_iter_loss: 0.16177327930927277
train_iter_loss: 0.24613575637340546
train_iter_loss: 0.11993377655744553
train_iter_loss: 0.1677408069372177
train_iter_loss: 0.2846498489379883
train_iter_loss: 0.12758632004261017
train_iter_loss: 0.14895176887512207
train_iter_loss: 0.16406050324440002
train_iter_loss: 0.10763883590698242
train_iter_loss: 0.21274475753307343
train_iter_loss: 0.11041552573442459
train_iter_loss: 0.19859552383422852
train_iter_loss: 0.17906880378723145
train_iter_loss: 0.2002909779548645
train_iter_loss: 0.06773842126131058
train_iter_loss: 0.20698799192905426
train_iter_loss: 0.08887036144733429
train_iter_loss: 0.15921838581562042
train_iter_loss: 0.13580802083015442
train_iter_loss: 0.109665647149086
train_iter_loss: 0.10916034877300262
train_iter_loss: 0.16826249659061432
train_iter_loss: 0.08160188794136047
train_iter_loss: 0.09329915791749954
train_iter_loss: 0.12413270026445389
train_iter_loss: 0.12442151457071304
train_iter_loss: 0.10838895291090012
train_iter_loss: 0.11308857798576355
train_iter_loss: 0.20335637032985687
train_iter_loss: 0.07933544367551804
train_iter_loss: 0.08200111240148544
train_iter_loss: 0.12701408565044403
train_iter_loss: 0.44522762298583984
train_iter_loss: 0.13955120742321014
train_iter_loss: 0.15971726179122925
train_iter_loss: 0.17080926895141602
train_iter_loss: 0.19779735803604126
train_iter_loss: 0.13560689985752106
train_iter_loss: 0.20819002389907837
train_iter_loss: 0.12433238327503204
train_iter_loss: 0.14373904466629028
train_iter_loss: 0.06201871857047081
train_iter_loss: 0.2625981569290161
train_iter_loss: 0.3254895806312561
train_iter_loss: 0.14924591779708862
train_iter_loss: 0.09488584846258163
train_iter_loss: 0.1595308482646942
train_iter_loss: 0.28899481892585754
train_iter_loss: 0.1448386162519455
train_iter_loss: 0.052734795957803726
train_iter_loss: 0.15501026809215546
train_iter_loss: 0.250434011220932
train_iter_loss: 0.2384013682603836
train_iter_loss: 0.1505473107099533
train_iter_loss: 0.13032609224319458
train_iter_loss: 0.13211603462696075
train_iter_loss: 0.30156660079956055
train_iter_loss: 0.14884378015995026
train_iter_loss: 0.26267483830451965
train_iter_loss: 0.22795338928699493
train_iter_loss: 0.1623845398426056
train_iter_loss: 0.08066504448652267
train_iter_loss: 0.07172627002000809
train_iter_loss: 0.14078161120414734
train_iter_loss: 0.19567809998989105
train loss :0.1595
---------------------
Validation seg loss: 0.21461056725491048 at epoch 870
epoch =    871/  1000, exp = train
train_iter_loss: 0.23296204209327698
train_iter_loss: 0.07522673159837723
train_iter_loss: 0.15954916179180145
train_iter_loss: 0.23100322484970093
train_iter_loss: 0.13338805735111237
train_iter_loss: 0.13304419815540314
train_iter_loss: 0.05396376550197601
train_iter_loss: 0.10936688631772995
train_iter_loss: 0.24607492983341217
train_iter_loss: 0.1491800993680954
train_iter_loss: 0.1488439291715622
train_iter_loss: 0.18496586382389069
train_iter_loss: 0.07341741770505905
train_iter_loss: 0.15032602846622467
train_iter_loss: 0.10694299638271332
train_iter_loss: 0.11496156454086304
train_iter_loss: 0.06695153564214706
train_iter_loss: 0.10483299195766449
train_iter_loss: 0.12686346471309662
train_iter_loss: 0.20430226624011993
train_iter_loss: 0.16528008878231049
train_iter_loss: 0.24722687900066376
train_iter_loss: 0.1522682160139084
train_iter_loss: 0.1263968050479889
train_iter_loss: 0.2553591728210449
train_iter_loss: 0.06890421360731125
train_iter_loss: 0.15171030163764954
train_iter_loss: 0.22190167009830475
train_iter_loss: 0.09267735481262207
train_iter_loss: 0.07213278114795685
train_iter_loss: 0.16729754209518433
train_iter_loss: 0.08251700550317764
train_iter_loss: 0.10418519377708435
train_iter_loss: 0.1348886638879776
train_iter_loss: 0.14263971149921417
train_iter_loss: 0.10871641337871552
train_iter_loss: 0.09435432404279709
train_iter_loss: 0.19424624741077423
train_iter_loss: 0.09286314249038696
train_iter_loss: 0.1590263545513153
train_iter_loss: 0.2332664281129837
train_iter_loss: 0.17606893181800842
train_iter_loss: 0.0672856867313385
train_iter_loss: 0.23649579286575317
train_iter_loss: 0.11402502655982971
train_iter_loss: 0.16505315899848938
train_iter_loss: 0.143462672829628
train_iter_loss: 0.12436961382627487
train_iter_loss: 0.23455075919628143
train_iter_loss: 0.17968487739562988
train_iter_loss: 0.14268729090690613
train_iter_loss: 0.09342555701732635
train_iter_loss: 0.2000489979982376
train_iter_loss: 0.23850609362125397
train_iter_loss: 0.1864691525697708
train_iter_loss: 0.09512981027364731
train_iter_loss: 0.30461999773979187
train_iter_loss: 0.3367750644683838
train_iter_loss: 0.21390965580940247
train_iter_loss: 0.16725896298885345
train_iter_loss: 0.13818489015102386
train_iter_loss: 0.14182844758033752
train_iter_loss: 0.1103542298078537
train_iter_loss: 0.17913243174552917
train_iter_loss: 0.24691694974899292
train_iter_loss: 0.09901518374681473
train_iter_loss: 0.1757032573223114
train_iter_loss: 0.10439641028642654
train_iter_loss: 0.13887935876846313
train_iter_loss: 0.2572198510169983
train_iter_loss: 0.2924591600894928
train_iter_loss: 0.1831905096769333
train_iter_loss: 0.23449519276618958
train_iter_loss: 0.18929710984230042
train_iter_loss: 0.11251753568649292
train_iter_loss: 0.10903795063495636
train_iter_loss: 0.14555202424526215
train_iter_loss: 0.0842086523771286
train_iter_loss: 0.17471840977668762
train_iter_loss: 0.26733866333961487
train_iter_loss: 0.5531420707702637
train_iter_loss: 0.10698790848255157
train_iter_loss: 0.05853036791086197
train_iter_loss: 0.07219769060611725
train_iter_loss: 0.10866258293390274
train_iter_loss: 0.08078733086585999
train_iter_loss: 0.13516999781131744
train_iter_loss: 0.03172098472714424
train_iter_loss: 0.17544369399547577
train_iter_loss: 0.14816437661647797
train_iter_loss: 0.07751680165529251
train_iter_loss: 0.19307704269886017
train_iter_loss: 0.12384983152151108
train_iter_loss: 0.26468876004219055
train_iter_loss: 0.1266230344772339
train_iter_loss: 0.09046249836683273
train_iter_loss: 0.09831248968839645
train_iter_loss: 0.19142048060894012
train_iter_loss: 0.15141870081424713
train_iter_loss: 0.22439365088939667
train loss :0.1585
---------------------
Validation seg loss: 0.2198441801065544 at epoch 871
epoch =    872/  1000, exp = train
train_iter_loss: 0.0826701894402504
train_iter_loss: 0.17102408409118652
train_iter_loss: 0.19379492104053497
train_iter_loss: 0.09013291448354721
train_iter_loss: 0.06536861509084702
train_iter_loss: 0.2016119658946991
train_iter_loss: 0.180488720536232
train_iter_loss: 0.09882965683937073
train_iter_loss: 0.20342883467674255
train_iter_loss: 0.11270279437303543
train_iter_loss: 0.08835818618535995
train_iter_loss: 0.08719060570001602
train_iter_loss: 0.1281897872686386
train_iter_loss: 0.2694714069366455
train_iter_loss: 0.15603262186050415
train_iter_loss: 0.06934081017971039
train_iter_loss: 0.28704920411109924
train_iter_loss: 0.10283239930868149
train_iter_loss: 0.16630128026008606
train_iter_loss: 0.2237173318862915
train_iter_loss: 0.1564493328332901
train_iter_loss: 0.07770814001560211
train_iter_loss: 0.12566828727722168
train_iter_loss: 0.1778261810541153
train_iter_loss: 0.30796486139297485
train_iter_loss: 0.18504785001277924
train_iter_loss: 0.19009874761104584
train_iter_loss: 0.11402866989374161
train_iter_loss: 0.1700388491153717
train_iter_loss: 0.11094068735837936
train_iter_loss: 0.18640685081481934
train_iter_loss: 0.13013306260108948
train_iter_loss: 0.05954538285732269
train_iter_loss: 0.19052930176258087
train_iter_loss: 0.08455617725849152
train_iter_loss: 0.10854238271713257
train_iter_loss: 0.12377001345157623
train_iter_loss: 0.20963223278522491
train_iter_loss: 0.13274367153644562
train_iter_loss: 0.07118874788284302
train_iter_loss: 0.1997321993112564
train_iter_loss: 0.05873444303870201
train_iter_loss: 0.10483553260564804
train_iter_loss: 0.1489161103963852
train_iter_loss: 0.11759327352046967
train_iter_loss: 0.12495151907205582
train_iter_loss: 0.2287764698266983
train_iter_loss: 0.13429948687553406
train_iter_loss: 0.2126532346010208
train_iter_loss: 0.13576439023017883
train_iter_loss: 0.1713264435529709
train_iter_loss: 0.24401992559432983
train_iter_loss: 0.14644543826580048
train_iter_loss: 0.1813933551311493
train_iter_loss: 0.22073324024677277
train_iter_loss: 0.2334267646074295
train_iter_loss: 0.07777885347604752
train_iter_loss: 0.2259824424982071
train_iter_loss: 0.11349042505025864
train_iter_loss: 0.11715466529130936
train_iter_loss: 0.0781419426202774
train_iter_loss: 0.22641128301620483
train_iter_loss: 0.1110692173242569
train_iter_loss: 0.16579508781433105
train_iter_loss: 0.12886807322502136
train_iter_loss: 0.22772027552127838
train_iter_loss: 0.19813032448291779
train_iter_loss: 0.26654279232025146
train_iter_loss: 0.1304054856300354
train_iter_loss: 0.1361214965581894
train_iter_loss: 0.12172868847846985
train_iter_loss: 0.17542703449726105
train_iter_loss: 0.08162658661603928
train_iter_loss: 0.14113834500312805
train_iter_loss: 0.05396657437086105
train_iter_loss: 0.18386268615722656
train_iter_loss: 0.079023078083992
train_iter_loss: 0.2514271140098572
train_iter_loss: 0.10555063933134079
train_iter_loss: 0.1758364588022232
train_iter_loss: 0.3360232412815094
train_iter_loss: 0.1620037704706192
train_iter_loss: 0.08078229427337646
train_iter_loss: 0.17527702450752258
train_iter_loss: 0.16988861560821533
train_iter_loss: 0.1426229327917099
train_iter_loss: 0.2403247058391571
train_iter_loss: 0.04268008843064308
train_iter_loss: 0.1500614583492279
train_iter_loss: 0.1445179581642151
train_iter_loss: 0.07539021223783493
train_iter_loss: 0.14612184464931488
train_iter_loss: 0.09592381119728088
train_iter_loss: 0.223515123128891
train_iter_loss: 0.23215271532535553
train_iter_loss: 0.15587331354618073
train_iter_loss: 0.2600908577442169
train_iter_loss: 0.11614582687616348
train_iter_loss: 0.20371399819850922
train_iter_loss: 0.0787830576300621
train loss :0.1552
---------------------
Validation seg loss: 0.21517983362746407 at epoch 872
epoch =    873/  1000, exp = train
train_iter_loss: 0.18925929069519043
train_iter_loss: 0.14236439764499664
train_iter_loss: 0.13442157208919525
train_iter_loss: 0.10890841484069824
train_iter_loss: 0.13909149169921875
train_iter_loss: 0.15833628177642822
train_iter_loss: 0.11934597790241241
train_iter_loss: 0.07863036543130875
train_iter_loss: 0.0866628885269165
train_iter_loss: 0.1638859063386917
train_iter_loss: 0.26206091046333313
train_iter_loss: 0.23363356292247772
train_iter_loss: 0.14562106132507324
train_iter_loss: 0.10227075219154358
train_iter_loss: 0.24178053438663483
train_iter_loss: 0.5369396209716797
train_iter_loss: 0.14784587919712067
train_iter_loss: 0.08541271835565567
train_iter_loss: 0.09661058336496353
train_iter_loss: 0.12767331302165985
train_iter_loss: 0.12096685916185379
train_iter_loss: 0.14086592197418213
train_iter_loss: 0.164391428232193
train_iter_loss: 0.23262913525104523
train_iter_loss: 0.08986537903547287
train_iter_loss: 0.14967478811740875
train_iter_loss: 0.22027678787708282
train_iter_loss: 0.12863190472126007
train_iter_loss: 0.14870305359363556
train_iter_loss: 0.10809870809316635
train_iter_loss: 0.1260414570569992
train_iter_loss: 0.1475575715303421
train_iter_loss: 0.21642737090587616
train_iter_loss: 0.2233196347951889
train_iter_loss: 0.15011604130268097
train_iter_loss: 0.21499186754226685
train_iter_loss: 0.3093395531177521
train_iter_loss: 0.13468457758426666
train_iter_loss: 0.05072545260190964
train_iter_loss: 0.26783421635627747
train_iter_loss: 0.13409574329853058
train_iter_loss: 0.10490510612726212
train_iter_loss: 0.05501934885978699
train_iter_loss: 0.2578265964984894
train_iter_loss: 0.03799829259514809
train_iter_loss: 0.18204016983509064
train_iter_loss: 0.1552492380142212
train_iter_loss: 0.10156536847352982
train_iter_loss: 0.13999322056770325
train_iter_loss: 0.20538794994354248
train_iter_loss: 0.16415107250213623
train_iter_loss: 0.1869584023952484
train_iter_loss: 0.20451530814170837
train_iter_loss: 0.06277097016572952
train_iter_loss: 0.06072317808866501
train_iter_loss: 0.2590634822845459
train_iter_loss: 0.10472774505615234
train_iter_loss: 0.17645321786403656
train_iter_loss: 0.10086546838283539
train_iter_loss: 0.1351955235004425
train_iter_loss: 0.13042517006397247
train_iter_loss: 0.13115008175373077
train_iter_loss: 0.138295516371727
train_iter_loss: 0.2396044135093689
train_iter_loss: 0.1402914673089981
train_iter_loss: 0.16297200322151184
train_iter_loss: 0.16956685483455658
train_iter_loss: 0.17351034283638
train_iter_loss: 0.2845948338508606
train_iter_loss: 0.15743130445480347
train_iter_loss: 0.10909508168697357
train_iter_loss: 0.021148329600691795
train_iter_loss: 0.3539198637008667
train_iter_loss: 0.16406044363975525
train_iter_loss: 0.116053506731987
train_iter_loss: 0.06046869233250618
train_iter_loss: 0.256734699010849
train_iter_loss: 0.20933179557323456
train_iter_loss: 0.12905263900756836
train_iter_loss: 0.1286143958568573
train_iter_loss: 0.1527632474899292
train_iter_loss: 0.1457773596048355
train_iter_loss: 0.2261141538619995
train_iter_loss: 0.15835386514663696
train_iter_loss: 0.14175522327423096
train_iter_loss: 0.23601944744586945
train_iter_loss: 0.17361335456371307
train_iter_loss: 0.09960322827100754
train_iter_loss: 0.20112791657447815
train_iter_loss: 0.1274109184741974
train_iter_loss: 0.10852719098329544
train_iter_loss: 0.15064524114131927
train_iter_loss: 0.11683684587478638
train_iter_loss: 0.19473309814929962
train_iter_loss: 0.12457069009542465
train_iter_loss: 0.06512267142534256
train_iter_loss: 0.10456318408250809
train_iter_loss: 0.2543993294239044
train_iter_loss: 0.16897428035736084
train_iter_loss: 0.08028660714626312
train loss :0.1594
---------------------
Validation seg loss: 0.2149981906690266 at epoch 873
epoch =    874/  1000, exp = train
train_iter_loss: 0.11107462644577026
train_iter_loss: 0.11928363144397736
train_iter_loss: 0.17826306819915771
train_iter_loss: 0.17627882957458496
train_iter_loss: 0.176156684756279
train_iter_loss: 0.11081154644489288
train_iter_loss: 0.16451497375965118
train_iter_loss: 0.29646697640419006
train_iter_loss: 0.20131060481071472
train_iter_loss: 0.1358470469713211
train_iter_loss: 0.12376540899276733
train_iter_loss: 0.1497366726398468
train_iter_loss: 0.1367657631635666
train_iter_loss: 0.07641703635454178
train_iter_loss: 0.2670470178127289
train_iter_loss: 0.2254943996667862
train_iter_loss: 0.061826277524232864
train_iter_loss: 0.17505523562431335
train_iter_loss: 0.11928746849298477
train_iter_loss: 0.20128938555717468
train_iter_loss: 0.10785499960184097
train_iter_loss: 0.2055913656949997
train_iter_loss: 0.18998543918132782
train_iter_loss: 0.2084433138370514
train_iter_loss: 0.18377789855003357
train_iter_loss: 0.13341070711612701
train_iter_loss: 0.13806091248989105
train_iter_loss: 0.06991533935070038
train_iter_loss: 0.15456466376781464
train_iter_loss: 0.12065188586711884
train_iter_loss: 0.1940634399652481
train_iter_loss: 0.18801739811897278
train_iter_loss: 0.21045233309268951
train_iter_loss: 0.13115964829921722
train_iter_loss: 0.26338285207748413
train_iter_loss: 0.14490152895450592
train_iter_loss: 0.22858715057373047
train_iter_loss: 0.08435703814029694
train_iter_loss: 0.22760869562625885
train_iter_loss: 0.09982611238956451
train_iter_loss: 0.19548137485980988
train_iter_loss: 0.1264989972114563
train_iter_loss: 0.1431848108768463
train_iter_loss: 0.21900786459445953
train_iter_loss: 0.1308494359254837
train_iter_loss: 0.11608231067657471
train_iter_loss: 0.15580151975154877
train_iter_loss: 0.18020200729370117
train_iter_loss: 0.07509341090917587
train_iter_loss: 0.11175910383462906
train_iter_loss: 0.15896373987197876
train_iter_loss: 0.1785263866186142
train_iter_loss: 0.20278237760066986
train_iter_loss: 0.21961818635463715
train_iter_loss: 0.0772857815027237
train_iter_loss: 0.06505660712718964
train_iter_loss: 0.12654124200344086
train_iter_loss: 0.23086708784103394
train_iter_loss: 0.13721884787082672
train_iter_loss: 0.11400177329778671
train_iter_loss: 0.17602132260799408
train_iter_loss: 0.2067495584487915
train_iter_loss: 0.10543043166399002
train_iter_loss: 0.1570144146680832
train_iter_loss: 0.13386866450309753
train_iter_loss: 0.2638184428215027
train_iter_loss: 0.17307308316230774
train_iter_loss: 0.21162937581539154
train_iter_loss: 0.1225634440779686
train_iter_loss: 0.19487784802913666
train_iter_loss: 0.10611669719219208
train_iter_loss: 0.12319030612707138
train_iter_loss: 0.1966782659292221
train_iter_loss: 0.13003228604793549
train_iter_loss: 0.11506345123052597
train_iter_loss: 0.105831578373909
train_iter_loss: 0.1821885108947754
train_iter_loss: 0.18736061453819275
train_iter_loss: 0.099180206656456
train_iter_loss: 0.16501787304878235
train_iter_loss: 0.19179870188236237
train_iter_loss: 0.1080440804362297
train_iter_loss: 0.16657009720802307
train_iter_loss: 0.10811737924814224
train_iter_loss: 0.09757792949676514
train_iter_loss: 0.052844490855932236
train_iter_loss: 0.2599872946739197
train_iter_loss: 0.17175574600696564
train_iter_loss: 0.19471201300621033
train_iter_loss: 0.071928471326828
train_iter_loss: 0.11463294923305511
train_iter_loss: 0.14141811430454254
train_iter_loss: 0.14137238264083862
train_iter_loss: 0.15678711235523224
train_iter_loss: 0.08503692597150803
train_iter_loss: 0.1425325870513916
train_iter_loss: 0.14256791770458221
train_iter_loss: 0.15082111954689026
train_iter_loss: 0.17735888063907623
train_iter_loss: 0.038833510130643845
train loss :0.1549
---------------------
Validation seg loss: 0.21892479388043284 at epoch 874
epoch =    875/  1000, exp = train
train_iter_loss: 0.17426592111587524
train_iter_loss: 0.12599043548107147
train_iter_loss: 0.1676952838897705
train_iter_loss: 0.08066433668136597
train_iter_loss: 0.171853169798851
train_iter_loss: 0.3174571394920349
train_iter_loss: 0.17924728989601135
train_iter_loss: 0.06765925884246826
train_iter_loss: 0.11734339594841003
train_iter_loss: 0.5149725079536438
train_iter_loss: 0.20749443769454956
train_iter_loss: 0.2020699679851532
train_iter_loss: 0.18273362517356873
train_iter_loss: 0.18227127194404602
train_iter_loss: 0.2933619022369385
train_iter_loss: 0.2808390259742737
train_iter_loss: 0.09153863787651062
train_iter_loss: 0.0894530788064003
train_iter_loss: 0.04405023530125618
train_iter_loss: 0.2602647542953491
train_iter_loss: 0.04843373969197273
train_iter_loss: 0.08863642066717148
train_iter_loss: 0.09803550690412521
train_iter_loss: 0.14067412912845612
train_iter_loss: 0.2420376092195511
train_iter_loss: 0.3026963472366333
train_iter_loss: 0.1991673707962036
train_iter_loss: 0.2578093409538269
train_iter_loss: 0.1830705851316452
train_iter_loss: 0.16840572655200958
train_iter_loss: 0.19763414561748505
train_iter_loss: 0.13266436755657196
train_iter_loss: 0.14295881986618042
train_iter_loss: 0.1605684906244278
train_iter_loss: 0.1949198991060257
train_iter_loss: 0.17426110804080963
train_iter_loss: 0.10086660832166672
train_iter_loss: 0.11197856813669205
train_iter_loss: 0.15268374979496002
train_iter_loss: 0.09909169375896454
train_iter_loss: 0.1599845290184021
train_iter_loss: 0.21496328711509705
train_iter_loss: 0.1397419273853302
train_iter_loss: 0.1795201301574707
train_iter_loss: 0.06127640977501869
train_iter_loss: 0.1331254243850708
train_iter_loss: 0.18123716115951538
train_iter_loss: 0.09024869650602341
train_iter_loss: 0.09879712760448456
train_iter_loss: 0.2487957924604416
train_iter_loss: 0.12468545138835907
train_iter_loss: 0.08832074701786041
train_iter_loss: 0.13536563515663147
train_iter_loss: 0.15596593916416168
train_iter_loss: 0.21219030022621155
train_iter_loss: 0.11771367490291595
train_iter_loss: 0.11837694048881531
train_iter_loss: 0.17944052815437317
train_iter_loss: 0.09909997880458832
train_iter_loss: 0.11931078135967255
train_iter_loss: 0.16436155140399933
train_iter_loss: 0.05487675592303276
train_iter_loss: 0.08715056627988815
train_iter_loss: 0.22005246579647064
train_iter_loss: 0.17256951332092285
train_iter_loss: 0.10981576144695282
train_iter_loss: 0.2586463987827301
train_iter_loss: 0.13591653108596802
train_iter_loss: 0.10667067021131516
train_iter_loss: 0.13272686302661896
train_iter_loss: 0.16029493510723114
train_iter_loss: 0.20889194309711456
train_iter_loss: 0.17217640578746796
train_iter_loss: 0.345139741897583
train_iter_loss: 0.22058343887329102
train_iter_loss: 0.11160720139741898
train_iter_loss: 0.0692419558763504
train_iter_loss: 0.16636492311954498
train_iter_loss: 0.10665152966976166
train_iter_loss: 0.1320902407169342
train_iter_loss: 0.13370728492736816
train_iter_loss: 0.23087574541568756
train_iter_loss: 0.1030084490776062
train_iter_loss: 0.21010182797908783
train_iter_loss: 0.14958973228931427
train_iter_loss: 0.06632400304079056
train_iter_loss: 0.12843476235866547
train_iter_loss: 0.05503324419260025
train_iter_loss: 0.19484083354473114
train_iter_loss: 0.10362155735492706
train_iter_loss: 0.19913357496261597
train_iter_loss: 0.17936564981937408
train_iter_loss: 0.1590977907180786
train_iter_loss: 0.1887475997209549
train_iter_loss: 0.2374275177717209
train_iter_loss: 0.14033204317092896
train_iter_loss: 0.1113654226064682
train_iter_loss: 0.2410287857055664
train_iter_loss: 0.13129717111587524
train_iter_loss: 0.12285532057285309
train loss :0.1618
---------------------
Validation seg loss: 0.22001411171876034 at epoch 875
epoch =    876/  1000, exp = train
train_iter_loss: 0.15295107662677765
train_iter_loss: 0.11785448342561722
train_iter_loss: 0.294058233499527
train_iter_loss: 0.13179150223731995
train_iter_loss: 0.24044093489646912
train_iter_loss: 0.1096838191151619
train_iter_loss: 0.2792709171772003
train_iter_loss: 0.16727162897586823
train_iter_loss: 0.1430235207080841
train_iter_loss: 0.13691392540931702
train_iter_loss: 0.16684134304523468
train_iter_loss: 0.14111153781414032
train_iter_loss: 0.11027419567108154
train_iter_loss: 0.142985537648201
train_iter_loss: 0.0879708081483841
train_iter_loss: 0.19691401720046997
train_iter_loss: 0.12897086143493652
train_iter_loss: 0.11921478807926178
train_iter_loss: 0.22390513122081757
train_iter_loss: 0.2001023292541504
train_iter_loss: 0.1277884989976883
train_iter_loss: 0.2653442919254303
train_iter_loss: 0.14879055321216583
train_iter_loss: 0.2726091742515564
train_iter_loss: 0.11971153318881989
train_iter_loss: 0.10805917531251907
train_iter_loss: 0.16642822325229645
train_iter_loss: 0.10731793195009232
train_iter_loss: 0.20632357895374298
train_iter_loss: 0.2075934261083603
train_iter_loss: 0.07378670573234558
train_iter_loss: 0.1299051195383072
train_iter_loss: 0.12766315042972565
train_iter_loss: 0.15066495537757874
train_iter_loss: 0.10532065480947495
train_iter_loss: 0.14588341116905212
train_iter_loss: 0.13736021518707275
train_iter_loss: 0.1742771714925766
train_iter_loss: 0.14203611016273499
train_iter_loss: 0.15228773653507233
train_iter_loss: 0.13387227058410645
train_iter_loss: 0.08283932507038116
train_iter_loss: 0.09622860699892044
train_iter_loss: 0.207747220993042
train_iter_loss: 0.17781949043273926
train_iter_loss: 0.17734363675117493
train_iter_loss: 0.17786867916584015
train_iter_loss: 0.14057761430740356
train_iter_loss: 0.1517099142074585
train_iter_loss: 0.15865939855575562
train_iter_loss: 0.21295832097530365
train_iter_loss: 0.17275448143482208
train_iter_loss: 0.2325165867805481
train_iter_loss: 0.0855221375823021
train_iter_loss: 0.16867879033088684
train_iter_loss: 0.09640482068061829
train_iter_loss: 0.06342927366495132
train_iter_loss: 0.1555994749069214
train_iter_loss: 0.10596500337123871
train_iter_loss: 0.15965808928012848
train_iter_loss: 0.1720173954963684
train_iter_loss: 0.1126587986946106
train_iter_loss: 0.10778205841779709
train_iter_loss: 0.13033686578273773
train_iter_loss: 0.1856551468372345
train_iter_loss: 0.13831137120723724
train_iter_loss: 0.15024971961975098
train_iter_loss: 0.30289894342422485
train_iter_loss: 0.1530657261610031
train_iter_loss: 0.14680863916873932
train_iter_loss: 0.20640522241592407
train_iter_loss: 0.11774970591068268
train_iter_loss: 0.17155738174915314
train_iter_loss: 0.08844412863254547
train_iter_loss: 0.08314353972673416
train_iter_loss: 0.23507019877433777
train_iter_loss: 0.19055363535881042
train_iter_loss: 0.10548099130392075
train_iter_loss: 0.09517493098974228
train_iter_loss: 0.12109236419200897
train_iter_loss: 0.19402790069580078
train_iter_loss: 0.15834392607212067
train_iter_loss: 0.1465209573507309
train_iter_loss: 0.07267454266548157
train_iter_loss: 0.1393473595380783
train_iter_loss: 0.38262391090393066
train_iter_loss: 0.10285484790802002
train_iter_loss: 0.07863590866327286
train_iter_loss: 0.19554482400417328
train_iter_loss: 0.21440917253494263
train_iter_loss: 0.17629651725292206
train_iter_loss: 0.16874657571315765
train_iter_loss: 0.056733082979917526
train_iter_loss: 0.10126045346260071
train_iter_loss: 0.1353738009929657
train_iter_loss: 0.1968660056591034
train_iter_loss: 0.13270802795886993
train_iter_loss: 0.13044096529483795
train_iter_loss: 0.10145284235477448
train_iter_loss: 0.17625777423381805
train loss :0.1556
---------------------
Validation seg loss: 0.2141405708982416 at epoch 876
epoch =    877/  1000, exp = train
train_iter_loss: 0.2177858203649521
train_iter_loss: 0.13492795825004578
train_iter_loss: 0.15879084169864655
train_iter_loss: 0.0643322691321373
train_iter_loss: 0.07255658507347107
train_iter_loss: 0.09652043879032135
train_iter_loss: 0.08803994208574295
train_iter_loss: 0.05032151937484741
train_iter_loss: 0.14031822979450226
train_iter_loss: 0.1784939020872116
train_iter_loss: 0.21535512804985046
train_iter_loss: 0.11246080696582794
train_iter_loss: 0.08926701545715332
train_iter_loss: 0.08723527938127518
train_iter_loss: 0.12451304495334625
train_iter_loss: 0.08513181656599045
train_iter_loss: 0.21337100863456726
train_iter_loss: 0.10252636671066284
train_iter_loss: 0.10840252041816711
train_iter_loss: 0.15800516307353973
train_iter_loss: 0.09689315408468246
train_iter_loss: 0.14217644929885864
train_iter_loss: 0.1606835275888443
train_iter_loss: 0.1439867913722992
train_iter_loss: 0.07610654085874557
train_iter_loss: 0.0905989333987236
train_iter_loss: 0.3073214888572693
train_iter_loss: 0.15110896527767181
train_iter_loss: 0.1704981029033661
train_iter_loss: 0.331015020608902
train_iter_loss: 0.18050411343574524
train_iter_loss: 0.10300089418888092
train_iter_loss: 0.13789719343185425
train_iter_loss: 0.1648220270872116
train_iter_loss: 0.05594202131032944
train_iter_loss: 0.1902775913476944
train_iter_loss: 0.19861821830272675
train_iter_loss: 0.09571041166782379
train_iter_loss: 0.11916010826826096
train_iter_loss: 0.2655712068080902
train_iter_loss: 0.10223864763975143
train_iter_loss: 0.3587804138660431
train_iter_loss: 0.19778557121753693
train_iter_loss: 0.13317060470581055
train_iter_loss: 0.270821750164032
train_iter_loss: 0.06133973225951195
train_iter_loss: 0.18850263953208923
train_iter_loss: 0.11976159363985062
train_iter_loss: 0.21663962304592133
train_iter_loss: 0.10603128373622894
train_iter_loss: 0.12962231040000916
train_iter_loss: 0.04598403349518776
train_iter_loss: 0.17445053160190582
train_iter_loss: 0.11895439028739929
train_iter_loss: 0.14846910536289215
train_iter_loss: 0.18723800778388977
train_iter_loss: 0.09713511914014816
train_iter_loss: 0.13289378583431244
train_iter_loss: 0.17399057745933533
train_iter_loss: 0.16339004039764404
train_iter_loss: 0.0944417417049408
train_iter_loss: 0.16060882806777954
train_iter_loss: 0.17210522294044495
train_iter_loss: 0.2042890340089798
train_iter_loss: 0.1628859043121338
train_iter_loss: 0.13518494367599487
train_iter_loss: 0.13825827836990356
train_iter_loss: 0.20872840285301208
train_iter_loss: 0.04812658950686455
train_iter_loss: 0.16840589046478271
train_iter_loss: 0.10564868152141571
train_iter_loss: 0.16255760192871094
train_iter_loss: 0.19417430460453033
train_iter_loss: 0.10360618680715561
train_iter_loss: 0.18911010026931763
train_iter_loss: 0.11230306327342987
train_iter_loss: 0.12167271226644516
train_iter_loss: 0.1148233637213707
train_iter_loss: 0.15358299016952515
train_iter_loss: 0.2273062914609909
train_iter_loss: 0.12191156297922134
train_iter_loss: 0.042532507330179214
train_iter_loss: 0.15478353202342987
train_iter_loss: 0.13311715424060822
train_iter_loss: 0.16487111151218414
train_iter_loss: 0.23394636809825897
train_iter_loss: 0.1253083199262619
train_iter_loss: 0.24994125962257385
train_iter_loss: 0.3017498850822449
train_iter_loss: 0.14370602369308472
train_iter_loss: 0.18703849613666534
train_iter_loss: 0.12265177816152573
train_iter_loss: 0.19481687247753143
train_iter_loss: 0.052328769117593765
train_iter_loss: 0.15660133957862854
train_iter_loss: 0.13680312037467957
train_iter_loss: 0.1052825078368187
train_iter_loss: 0.14458106458187103
train_iter_loss: 0.28237563371658325
train_iter_loss: 0.21742407977581024
train loss :0.1521
---------------------
Validation seg loss: 0.21720813116375007 at epoch 877
epoch =    878/  1000, exp = train
train_iter_loss: 0.17038854956626892
train_iter_loss: 0.09034541249275208
train_iter_loss: 0.0773409977555275
train_iter_loss: 0.17682042717933655
train_iter_loss: 0.2724212408065796
train_iter_loss: 0.2068055272102356
train_iter_loss: 0.15447399020195007
train_iter_loss: 0.1650819331407547
train_iter_loss: 0.2268723100423813
train_iter_loss: 0.2800401449203491
train_iter_loss: 0.19694693386554718
train_iter_loss: 0.11282415688037872
train_iter_loss: 0.1708863377571106
train_iter_loss: 0.1142691969871521
train_iter_loss: 0.14241772890090942
train_iter_loss: 0.08414047211408615
train_iter_loss: 0.22153344750404358
train_iter_loss: 0.1334415227174759
train_iter_loss: 0.11216268688440323
train_iter_loss: 0.04993794485926628
train_iter_loss: 0.11927603930234909
train_iter_loss: 0.20361103117465973
train_iter_loss: 0.17827066779136658
train_iter_loss: 0.11087874323129654
train_iter_loss: 0.11595822870731354
train_iter_loss: 0.097218357026577
train_iter_loss: 0.12213027477264404
train_iter_loss: 0.19314642250537872
train_iter_loss: 0.15535491704940796
train_iter_loss: 0.18290600180625916
train_iter_loss: 0.08751928806304932
train_iter_loss: 0.11383811384439468
train_iter_loss: 0.14044760167598724
train_iter_loss: 0.17129218578338623
train_iter_loss: 0.2576053738594055
train_iter_loss: 0.12786126136779785
train_iter_loss: 0.17732763290405273
train_iter_loss: 0.11414545774459839
train_iter_loss: 0.1539352536201477
train_iter_loss: 0.06412060558795929
train_iter_loss: 0.25499433279037476
train_iter_loss: 0.13264764845371246
train_iter_loss: 0.0998341515660286
train_iter_loss: 0.12427117675542831
train_iter_loss: 0.15453335642814636
train_iter_loss: 0.12737900018692017
train_iter_loss: 0.13134711980819702
train_iter_loss: 0.306972473859787
train_iter_loss: 0.14025092124938965
train_iter_loss: 0.2364681512117386
train_iter_loss: 0.18231666088104248
train_iter_loss: 0.18146033585071564
train_iter_loss: 0.1484466791152954
train_iter_loss: 0.10182224214076996
train_iter_loss: 0.19047732651233673
train_iter_loss: 0.13360649347305298
train_iter_loss: 0.19340945780277252
train_iter_loss: 0.296862930059433
train_iter_loss: 0.09063877165317535
train_iter_loss: 0.1646977663040161
train_iter_loss: 0.15466132760047913
train_iter_loss: 0.10539056360721588
train_iter_loss: 0.14514018595218658
train_iter_loss: 0.11220469325780869
train_iter_loss: 0.17650152742862701
train_iter_loss: 0.1236523687839508
train_iter_loss: 0.127520352602005
train_iter_loss: 0.1023055911064148
train_iter_loss: 0.13637396693229675
train_iter_loss: 0.09849077463150024
train_iter_loss: 0.1646873652935028
train_iter_loss: 0.1551561802625656
train_iter_loss: 0.1602209359407425
train_iter_loss: 0.12293746322393417
train_iter_loss: 0.240122452378273
train_iter_loss: 0.2978667914867401
train_iter_loss: 0.24095465242862701
train_iter_loss: 0.09114211052656174
train_iter_loss: 0.14712578058242798
train_iter_loss: 0.20587606728076935
train_iter_loss: 0.06682561337947845
train_iter_loss: 0.19331322610378265
train_iter_loss: 0.1444430947303772
train_iter_loss: 0.09358110278844833
train_iter_loss: 0.15647609531879425
train_iter_loss: 0.160419300198555
train_iter_loss: 0.1982228010892868
train_iter_loss: 0.1374846249818802
train_iter_loss: 0.2811232805252075
train_iter_loss: 0.1007547378540039
train_iter_loss: 0.1284237653017044
train_iter_loss: 0.168138787150383
train_iter_loss: 0.09110283106565475
train_iter_loss: 0.19898313283920288
train_iter_loss: 0.2011353224515915
train_iter_loss: 0.1163758933544159
train_iter_loss: 0.12224135547876358
train_iter_loss: 0.0757664293050766
train_iter_loss: 0.2608717978000641
train_iter_loss: 0.09959279745817184
train loss :0.1569
---------------------
Validation seg loss: 0.21489241150348395 at epoch 878
epoch =    879/  1000, exp = train
train_iter_loss: 0.3297237455844879
train_iter_loss: 0.048098690807819366
train_iter_loss: 0.23108750581741333
train_iter_loss: 0.18458843231201172
train_iter_loss: 0.10506808012723923
train_iter_loss: 0.11455234885215759
train_iter_loss: 0.09866161644458771
train_iter_loss: 0.15418054163455963
train_iter_loss: 0.20120000839233398
train_iter_loss: 0.13395053148269653
train_iter_loss: 0.170099139213562
train_iter_loss: 0.14814938604831696
train_iter_loss: 0.16131503880023956
train_iter_loss: 0.03877777233719826
train_iter_loss: 0.24231746792793274
train_iter_loss: 0.26357337832450867
train_iter_loss: 0.09763360768556595
train_iter_loss: 0.15720263123512268
train_iter_loss: 0.12246507406234741
train_iter_loss: 0.04459792375564575
train_iter_loss: 0.11541907489299774
train_iter_loss: 0.13015876710414886
train_iter_loss: 0.20393551886081696
train_iter_loss: 0.04459022358059883
train_iter_loss: 0.16257065534591675
train_iter_loss: 0.15784254670143127
train_iter_loss: 0.19274528324604034
train_iter_loss: 0.3117767870426178
train_iter_loss: 0.3647434413433075
train_iter_loss: 0.13994728028774261
train_iter_loss: 0.17945148050785065
train_iter_loss: 0.08898716419935226
train_iter_loss: 0.1063600704073906
train_iter_loss: 0.25598520040512085
train_iter_loss: 0.17788538336753845
train_iter_loss: 0.04448574408888817
train_iter_loss: 0.17913289368152618
train_iter_loss: 0.12019335478544235
train_iter_loss: 0.16905909776687622
train_iter_loss: 0.15311075747013092
train_iter_loss: 0.099616639316082
train_iter_loss: 0.12491938471794128
train_iter_loss: 0.177268847823143
train_iter_loss: 0.26139649748802185
train_iter_loss: 0.12656792998313904
train_iter_loss: 0.1342126727104187
train_iter_loss: 0.1489410251379013
train_iter_loss: 0.14654135704040527
train_iter_loss: 0.1617114096879959
train_iter_loss: 0.18667079508304596
train_iter_loss: 0.07342075556516647
train_iter_loss: 0.13549256324768066
train_iter_loss: 0.13273373246192932
train_iter_loss: 0.21825791895389557
train_iter_loss: 0.0816056951880455
train_iter_loss: 0.20719552040100098
train_iter_loss: 0.10075211524963379
train_iter_loss: 0.11138693988323212
train_iter_loss: 0.13549451529979706
train_iter_loss: 0.2324569672346115
train_iter_loss: 0.27479979395866394
train_iter_loss: 0.08913376927375793
train_iter_loss: 0.025126690044999123
train_iter_loss: 0.19156447052955627
train_iter_loss: 0.1650785654783249
train_iter_loss: 0.11472612619400024
train_iter_loss: 0.20441459119319916
train_iter_loss: 0.20003065466880798
train_iter_loss: 0.1119508147239685
train_iter_loss: 0.09675605595111847
train_iter_loss: 0.2253887802362442
train_iter_loss: 0.1456363946199417
train_iter_loss: 0.10135310888290405
train_iter_loss: 0.0825502946972847
train_iter_loss: 0.10734736919403076
train_iter_loss: 0.32336241006851196
train_iter_loss: 0.13797660171985626
train_iter_loss: 0.1854984611272812
train_iter_loss: 0.15576468408107758
train_iter_loss: 0.16338595747947693
train_iter_loss: 0.2059972584247589
train_iter_loss: 0.10867960751056671
train_iter_loss: 0.22055374085903168
train_iter_loss: 0.22473198175430298
train_iter_loss: 0.20573543012142181
train_iter_loss: 0.24182480573654175
train_iter_loss: 0.20233845710754395
train_iter_loss: 0.10391229391098022
train_iter_loss: 0.1821913868188858
train_iter_loss: 0.11045348644256592
train_iter_loss: 0.1344626247882843
train_iter_loss: 0.18056294322013855
train_iter_loss: 0.06880628317594528
train_iter_loss: 0.20884917676448822
train_iter_loss: 0.23771153390407562
train_iter_loss: 0.22436271607875824
train_iter_loss: 0.184644877910614
train_iter_loss: 0.11558905243873596
train_iter_loss: 0.23125949501991272
train_iter_loss: 0.1723475456237793
train loss :0.1617
---------------------
Validation seg loss: 0.2156010728311848 at epoch 879
epoch =    880/  1000, exp = train
train_iter_loss: 0.06591939926147461
train_iter_loss: 0.4597812294960022
train_iter_loss: 0.11094745993614197
train_iter_loss: 0.29882562160491943
train_iter_loss: 0.13035908341407776
train_iter_loss: 0.11766243726015091
train_iter_loss: 0.1475972980260849
train_iter_loss: 0.21201032400131226
train_iter_loss: 0.11040499806404114
train_iter_loss: 0.19364860653877258
train_iter_loss: 0.06635957956314087
train_iter_loss: 0.1511898785829544
train_iter_loss: 0.14013251662254333
train_iter_loss: 0.20051416754722595
train_iter_loss: 0.06909685581922531
train_iter_loss: 0.16648590564727783
train_iter_loss: 0.11358445137739182
train_iter_loss: 0.16489963233470917
train_iter_loss: 0.10931367427110672
train_iter_loss: 0.11801189184188843
train_iter_loss: 0.1680193543434143
train_iter_loss: 0.19325251877307892
train_iter_loss: 0.08805911242961884
train_iter_loss: 0.09773693233728409
train_iter_loss: 0.14533664286136627
train_iter_loss: 0.1316041350364685
train_iter_loss: 0.08808540552854538
train_iter_loss: 0.09897178411483765
train_iter_loss: 0.19412602484226227
train_iter_loss: 0.15303485095500946
train_iter_loss: 0.1381719708442688
train_iter_loss: 0.06199810281395912
train_iter_loss: 0.15828754007816315
train_iter_loss: 0.27169445157051086
train_iter_loss: 0.19678130745887756
train_iter_loss: 0.27325335144996643
train_iter_loss: 0.11738931387662888
train_iter_loss: 0.3582497239112854
train_iter_loss: 0.12241538614034653
train_iter_loss: 0.17713269591331482
train_iter_loss: 0.09108024835586548
train_iter_loss: 0.20484353601932526
train_iter_loss: 0.1550477296113968
train_iter_loss: 0.1962389498949051
train_iter_loss: 0.14361296594142914
train_iter_loss: 0.16161669790744781
train_iter_loss: 0.20361672341823578
train_iter_loss: 0.15933267772197723
train_iter_loss: 0.11446011066436768
train_iter_loss: 0.31073033809661865
train_iter_loss: 0.12532241642475128
train_iter_loss: 0.21246279776096344
train_iter_loss: 0.19014932215213776
train_iter_loss: 0.10233313590288162
train_iter_loss: 0.19911643862724304
train_iter_loss: 0.16924187541007996
train_iter_loss: 0.18532468378543854
train_iter_loss: 0.12873415648937225
train_iter_loss: 0.06348279863595963
train_iter_loss: 0.12490574270486832
train_iter_loss: 0.12945327162742615
train_iter_loss: 0.15472574532032013
train_iter_loss: 0.09138373285531998
train_iter_loss: 0.12319327890872955
train_iter_loss: 0.21853674948215485
train_iter_loss: 0.16131223738193512
train_iter_loss: 0.0415225587785244
train_iter_loss: 0.20325075089931488
train_iter_loss: 0.07951861619949341
train_iter_loss: 0.05691835284233093
train_iter_loss: 0.1474210023880005
train_iter_loss: 0.08404773473739624
train_iter_loss: 0.1920582503080368
train_iter_loss: 0.2743288278579712
train_iter_loss: 0.12207517772912979
train_iter_loss: 0.16609570384025574
train_iter_loss: 0.09733924269676208
train_iter_loss: 0.2746548056602478
train_iter_loss: 0.07746989279985428
train_iter_loss: 0.16738176345825195
train_iter_loss: 0.20403634011745453
train_iter_loss: 0.1911594271659851
train_iter_loss: 0.22907499969005585
train_iter_loss: 0.16882143914699554
train_iter_loss: 0.022344352677464485
train_iter_loss: 0.24681325256824493
train_iter_loss: 0.0971735268831253
train_iter_loss: 0.2254428118467331
train_iter_loss: 0.14046400785446167
train_iter_loss: 0.18930953741073608
train_iter_loss: 0.08964883536100388
train_iter_loss: 0.14185814559459686
train_iter_loss: 0.11463375389575958
train_iter_loss: 0.24751266837120056
train_iter_loss: 0.12215033918619156
train_iter_loss: 0.10382673889398575
train_iter_loss: 0.17532099783420563
train_iter_loss: 0.22600948810577393
train_iter_loss: 0.18554943799972534
train_iter_loss: 0.1774386614561081
train loss :0.1594
---------------------
Validation seg loss: 0.2183521785070452 at epoch 880
epoch =    881/  1000, exp = train
train_iter_loss: 0.09556064754724503
train_iter_loss: 0.14413243532180786
train_iter_loss: 0.06082644313573837
train_iter_loss: 0.18880672752857208
train_iter_loss: 0.11832229048013687
train_iter_loss: 0.10211044549942017
train_iter_loss: 0.21975912153720856
train_iter_loss: 0.253042995929718
train_iter_loss: 0.13805481791496277
train_iter_loss: 0.3666295111179352
train_iter_loss: 0.11840525269508362
train_iter_loss: 0.08739414066076279
train_iter_loss: 0.17013055086135864
train_iter_loss: 0.14092710614204407
train_iter_loss: 0.11025910824537277
train_iter_loss: 0.0956028625369072
train_iter_loss: 0.08884783089160919
train_iter_loss: 0.16015255451202393
train_iter_loss: 0.1642153114080429
train_iter_loss: 0.1694408655166626
train_iter_loss: 0.1864936500787735
train_iter_loss: 0.122258760035038
train_iter_loss: 0.2655324339866638
train_iter_loss: 0.25880226492881775
train_iter_loss: 0.09948230534791946
train_iter_loss: 0.1328555941581726
train_iter_loss: 0.18548695743083954
train_iter_loss: 0.21947640180587769
train_iter_loss: 0.14827075600624084
train_iter_loss: 0.13742420077323914
train_iter_loss: 0.1783633828163147
train_iter_loss: 0.08919096738100052
train_iter_loss: 0.13162554800510406
train_iter_loss: 0.2527860701084137
train_iter_loss: 0.08443629741668701
train_iter_loss: 0.07171756029129028
train_iter_loss: 0.13224126398563385
train_iter_loss: 0.1810920387506485
train_iter_loss: 0.10717589408159256
train_iter_loss: 0.18395839631557465
train_iter_loss: 0.18677178025245667
train_iter_loss: 0.12988919019699097
train_iter_loss: 0.2067892700433731
train_iter_loss: 0.17668932676315308
train_iter_loss: 0.19626155495643616
train_iter_loss: 0.07470882683992386
train_iter_loss: 0.09698083996772766
train_iter_loss: 0.09658899158239365
train_iter_loss: 0.16043621301651
train_iter_loss: 0.17333388328552246
train_iter_loss: 0.15650954842567444
train_iter_loss: 0.11233989894390106
train_iter_loss: 0.2246953397989273
train_iter_loss: 0.13744817674160004
train_iter_loss: 0.0702790915966034
train_iter_loss: 0.16919691860675812
train_iter_loss: 0.24789847433567047
train_iter_loss: 0.0935061126947403
train_iter_loss: 0.10274624079465866
train_iter_loss: 0.2142084687948227
train_iter_loss: 0.11444162577390671
train_iter_loss: 0.09198657423257828
train_iter_loss: 0.08841635286808014
train_iter_loss: 0.14495597779750824
train_iter_loss: 0.10205182433128357
train_iter_loss: 0.116116002202034
train_iter_loss: 0.09809523075819016
train_iter_loss: 0.24422116577625275
train_iter_loss: 0.22088667750358582
train_iter_loss: 0.23314817249774933
train_iter_loss: 0.01264586765319109
train_iter_loss: 0.2248629331588745
train_iter_loss: 0.09570971131324768
train_iter_loss: 0.12679921090602875
train_iter_loss: 0.22153863310813904
train_iter_loss: 0.1055990606546402
train_iter_loss: 0.1260635256767273
train_iter_loss: 0.4708319306373596
train_iter_loss: 0.06871394068002701
train_iter_loss: 0.20147915184497833
train_iter_loss: 0.14073975384235382
train_iter_loss: 0.059261027723550797
train_iter_loss: 0.21429014205932617
train_iter_loss: 0.051672305911779404
train_iter_loss: 0.1281028687953949
train_iter_loss: 0.12785977125167847
train_iter_loss: 0.17585550248622894
train_iter_loss: 0.12610702216625214
train_iter_loss: 0.19363495707511902
train_iter_loss: 0.15805001556873322
train_iter_loss: 0.31349828839302063
train_iter_loss: 0.15247796475887299
train_iter_loss: 0.24888615310192108
train_iter_loss: 0.12033480405807495
train_iter_loss: 0.07245329022407532
train_iter_loss: 0.16457219421863556
train_iter_loss: 0.24834947288036346
train_iter_loss: 0.12267592549324036
train_iter_loss: 0.18285176157951355
train_iter_loss: 0.10575271397829056
train loss :0.1556
---------------------
Validation seg loss: 0.2182107762925608 at epoch 881
epoch =    882/  1000, exp = train
train_iter_loss: 0.11146419495344162
train_iter_loss: 0.24580776691436768
train_iter_loss: 0.2922317087650299
train_iter_loss: 0.2248484194278717
train_iter_loss: 0.11012445390224457
train_iter_loss: 0.13319368660449982
train_iter_loss: 0.13791312277317047
train_iter_loss: 0.3139333128929138
train_iter_loss: 0.1344548612833023
train_iter_loss: 0.26936429738998413
train_iter_loss: 0.2025228589773178
train_iter_loss: 0.108176089823246
train_iter_loss: 0.18328145146369934
train_iter_loss: 0.08192479610443115
train_iter_loss: 0.13542500138282776
train_iter_loss: 0.0948006734251976
train_iter_loss: 0.043014079332351685
train_iter_loss: 0.13842585682868958
train_iter_loss: 0.07328250259160995
train_iter_loss: 0.11875447630882263
train_iter_loss: 0.14563168585300446
train_iter_loss: 0.18960465490818024
train_iter_loss: 0.1877339482307434
train_iter_loss: 0.11325733363628387
train_iter_loss: 0.06156096234917641
train_iter_loss: 0.1447233110666275
train_iter_loss: 0.199383944272995
train_iter_loss: 0.06278403103351593
train_iter_loss: 0.1046856939792633
train_iter_loss: 0.2601127028465271
train_iter_loss: 0.14086563885211945
train_iter_loss: 0.15191763639450073
train_iter_loss: 0.23768453299999237
train_iter_loss: 0.17846816778182983
train_iter_loss: 0.10640718042850494
train_iter_loss: 0.1544967144727707
train_iter_loss: 0.0811537578701973
train_iter_loss: 0.18750926852226257
train_iter_loss: 0.13781285285949707
train_iter_loss: 0.057005658745765686
train_iter_loss: 0.09830021858215332
train_iter_loss: 0.09996841847896576
train_iter_loss: 0.20665496587753296
train_iter_loss: 0.15259486436843872
train_iter_loss: 0.09453600645065308
train_iter_loss: 0.11138046532869339
train_iter_loss: 0.2172546088695526
train_iter_loss: 0.1400877982378006
train_iter_loss: 0.0625784620642662
train_iter_loss: 0.2041565328836441
train_iter_loss: 0.1778530776500702
train_iter_loss: 0.1480332612991333
train_iter_loss: 0.21732057631015778
train_iter_loss: 0.16359126567840576
train_iter_loss: 0.24505724012851715
train_iter_loss: 0.2651301324367523
train_iter_loss: 0.09070208668708801
train_iter_loss: 0.14159563183784485
train_iter_loss: 0.2843213677406311
train_iter_loss: 0.14209268987178802
train_iter_loss: 0.06727639585733414
train_iter_loss: 0.0504131056368351
train_iter_loss: 0.23169709742069244
train_iter_loss: 0.2272568792104721
train_iter_loss: 0.1324307769536972
train_iter_loss: 0.10392758995294571
train_iter_loss: 0.12230323255062103
train_iter_loss: 0.13602186739444733
train_iter_loss: 0.10859383642673492
train_iter_loss: 0.10985767096281052
train_iter_loss: 0.16169261932373047
train_iter_loss: 0.0401909276843071
train_iter_loss: 0.1390829086303711
train_iter_loss: 0.28220638632774353
train_iter_loss: 0.17846032977104187
train_iter_loss: 0.17542822659015656
train_iter_loss: 0.20153310894966125
train_iter_loss: 0.2147493064403534
train_iter_loss: 0.2017117142677307
train_iter_loss: 0.22274555265903473
train_iter_loss: 0.15523859858512878
train_iter_loss: 0.1465444713830948
train_iter_loss: 0.0649460181593895
train_iter_loss: 0.11439353972673416
train_iter_loss: 0.1705942153930664
train_iter_loss: 0.1521551012992859
train_iter_loss: 0.13244742155075073
train_iter_loss: 0.1717025637626648
train_iter_loss: 0.18252462148666382
train_iter_loss: 0.13679997622966766
train_iter_loss: 0.3176521360874176
train_iter_loss: 0.1166779100894928
train_iter_loss: 0.2374790459871292
train_iter_loss: 0.12860088050365448
train_iter_loss: 0.08431665599346161
train_iter_loss: 0.28585103154182434
train_iter_loss: 0.14704963564872742
train_iter_loss: 0.08812540769577026
train_iter_loss: 0.10941774398088455
train_iter_loss: 0.05121845752000809
train loss :0.1558
---------------------
Validation seg loss: 0.218636452317027 at epoch 882
epoch =    883/  1000, exp = train
train_iter_loss: 0.08097005635499954
train_iter_loss: 0.23308198153972626
train_iter_loss: 0.18893617391586304
train_iter_loss: 0.16405721008777618
train_iter_loss: 0.09390591830015182
train_iter_loss: 0.3298816382884979
train_iter_loss: 0.21045944094657898
train_iter_loss: 0.11455171555280685
train_iter_loss: 0.07303538173437119
train_iter_loss: 0.2424379140138626
train_iter_loss: 0.09267322719097137
train_iter_loss: 0.1494952291250229
train_iter_loss: 0.14802709221839905
train_iter_loss: 0.1586136668920517
train_iter_loss: 0.1682189404964447
train_iter_loss: 0.20861417055130005
train_iter_loss: 0.1486167311668396
train_iter_loss: 0.07679559290409088
train_iter_loss: 0.18359827995300293
train_iter_loss: 0.07379049062728882
train_iter_loss: 0.11779014021158218
train_iter_loss: 0.09983285516500473
train_iter_loss: 0.18743784725666046
train_iter_loss: 0.18187953531742096
train_iter_loss: 0.13843271136283875
train_iter_loss: 0.25370338559150696
train_iter_loss: 0.1349228024482727
train_iter_loss: 0.12605105340480804
train_iter_loss: 0.128386989235878
train_iter_loss: 0.08097989857196808
train_iter_loss: 0.17374703288078308
train_iter_loss: 0.26723670959472656
train_iter_loss: 0.10765159130096436
train_iter_loss: 0.17000265419483185
train_iter_loss: 0.04838523268699646
train_iter_loss: 0.21567733585834503
train_iter_loss: 0.188592329621315
train_iter_loss: 0.19397352635860443
train_iter_loss: 0.135415717959404
train_iter_loss: 0.08047929406166077
train_iter_loss: 0.19513219594955444
train_iter_loss: 0.13626731932163239
train_iter_loss: 0.1768663078546524
train_iter_loss: 0.14519961178302765
train_iter_loss: 0.08085042983293533
train_iter_loss: 0.06338241696357727
train_iter_loss: 0.13325297832489014
train_iter_loss: 0.07502096146345139
train_iter_loss: 0.10500079393386841
train_iter_loss: 0.09408837556838989
train_iter_loss: 0.2532710134983063
train_iter_loss: 0.09043995290994644
train_iter_loss: 0.10470271855592728
train_iter_loss: 0.2583387494087219
train_iter_loss: 0.07703279703855515
train_iter_loss: 0.16687354445457458
train_iter_loss: 0.1472764015197754
train_iter_loss: 0.18247279524803162
train_iter_loss: 0.09602224826812744
train_iter_loss: 0.25985419750213623
train_iter_loss: 0.25585949420928955
train_iter_loss: 0.15969404578208923
train_iter_loss: 0.15495064854621887
train_iter_loss: 0.2400592714548111
train_iter_loss: 0.1291392743587494
train_iter_loss: 0.16672036051750183
train_iter_loss: 0.15449753403663635
train_iter_loss: 0.21907983720302582
train_iter_loss: 0.14574016630649567
train_iter_loss: 0.13758397102355957
train_iter_loss: 0.33201029896736145
train_iter_loss: 0.1128578782081604
train_iter_loss: 0.131016805768013
train_iter_loss: 0.2244223952293396
train_iter_loss: 0.27450671792030334
train_iter_loss: 0.05537816882133484
train_iter_loss: 0.13608317077159882
train_iter_loss: 0.14139039814472198
train_iter_loss: 0.1949780434370041
train_iter_loss: 0.15751856565475464
train_iter_loss: 0.11325909197330475
train_iter_loss: 0.039246343076229095
train_iter_loss: 0.14685992896556854
train_iter_loss: 0.21834895014762878
train_iter_loss: 0.16869835555553436
train_iter_loss: 0.10256379097700119
train_iter_loss: 0.07701961696147919
train_iter_loss: 0.23176631331443787
train_iter_loss: 0.07601520419120789
train_iter_loss: 0.12900586426258087
train_iter_loss: 0.21691665053367615
train_iter_loss: 0.07059334218502045
train_iter_loss: 0.30567795038223267
train_iter_loss: 0.09042344242334366
train_iter_loss: 0.15151692926883698
train_iter_loss: 0.17301209270954132
train_iter_loss: 0.11811818182468414
train_iter_loss: 0.257648766040802
train_iter_loss: 0.1025550588965416
train_iter_loss: 0.20106817781925201
train loss :0.1568
---------------------
Validation seg loss: 0.2160532357487476 at epoch 883
epoch =    884/  1000, exp = train
train_iter_loss: 0.30802568793296814
train_iter_loss: 0.09880301356315613
train_iter_loss: 0.1959281712770462
train_iter_loss: 0.22769750654697418
train_iter_loss: 0.10901180654764175
train_iter_loss: 0.17900533974170685
train_iter_loss: 0.1766432523727417
train_iter_loss: 0.21577560901641846
train_iter_loss: 0.09565095603466034
train_iter_loss: 0.08459003269672394
train_iter_loss: 0.17314495146274567
train_iter_loss: 0.08588149398565292
train_iter_loss: 0.16764463484287262
train_iter_loss: 0.14932207763195038
train_iter_loss: 0.19660818576812744
train_iter_loss: 0.24682419002056122
train_iter_loss: 0.1440051943063736
train_iter_loss: 0.04891527444124222
train_iter_loss: 0.08561557531356812
train_iter_loss: 0.2600049376487732
train_iter_loss: 0.18911783397197723
train_iter_loss: 0.13648472726345062
train_iter_loss: 0.12265592813491821
train_iter_loss: 0.19675226509571075
train_iter_loss: 0.13663357496261597
train_iter_loss: 0.14535075426101685
train_iter_loss: 0.09612015634775162
train_iter_loss: 0.08802533894777298
train_iter_loss: 0.08210041373968124
train_iter_loss: 0.1819288581609726
train_iter_loss: 0.0455983504652977
train_iter_loss: 0.1603764146566391
train_iter_loss: 0.17074009776115417
train_iter_loss: 0.16776058077812195
train_iter_loss: 0.1810160130262375
train_iter_loss: 0.12896139919757843
train_iter_loss: 0.19463548064231873
train_iter_loss: 0.108177550137043
train_iter_loss: 0.20014800131320953
train_iter_loss: 0.1635773479938507
train_iter_loss: 0.2653340697288513
train_iter_loss: 0.1278606355190277
train_iter_loss: 0.1734493225812912
train_iter_loss: 0.13419480621814728
train_iter_loss: 0.08548370748758316
train_iter_loss: 0.2001197189092636
train_iter_loss: 0.14865538477897644
train_iter_loss: 0.25363439321517944
train_iter_loss: 0.3098837733268738
train_iter_loss: 0.16763253509998322
train_iter_loss: 0.10109128057956696
train_iter_loss: 0.07424703985452652
train_iter_loss: 0.14774999022483826
train_iter_loss: 0.12984566390514374
train_iter_loss: 0.14493104815483093
train_iter_loss: 0.09698732197284698
train_iter_loss: 0.14574092626571655
train_iter_loss: 0.3467348515987396
train_iter_loss: 0.26520466804504395
train_iter_loss: 0.12934376299381256
train_iter_loss: 0.10370095819234848
train_iter_loss: 0.1599271446466446
train_iter_loss: 0.3033815622329712
train_iter_loss: 0.14154167473316193
train_iter_loss: 0.1087367981672287
train_iter_loss: 0.09081558138132095
train_iter_loss: 0.1471817046403885
train_iter_loss: 0.03806043043732643
train_iter_loss: 0.18967720866203308
train_iter_loss: 0.10429902374744415
train_iter_loss: 0.1489248424768448
train_iter_loss: 0.120060995221138
train_iter_loss: 0.07555339485406876
train_iter_loss: 0.18785874545574188
train_iter_loss: 0.14487101137638092
train_iter_loss: 0.17753402888774872
train_iter_loss: 0.12455543875694275
train_iter_loss: 0.08695829659700394
train_iter_loss: 0.08231145888566971
train_iter_loss: 0.2834945619106293
train_iter_loss: 0.14772646129131317
train_iter_loss: 0.3200770914554596
train_iter_loss: 0.19248560070991516
train_iter_loss: 0.062160469591617584
train_iter_loss: 0.27915310859680176
train_iter_loss: 0.11379013955593109
train_iter_loss: 0.1393129676580429
train_iter_loss: 0.08570615947246552
train_iter_loss: 0.09424880892038345
train_iter_loss: 0.14759790897369385
train_iter_loss: 0.22316817939281464
train_iter_loss: 0.1613115668296814
train_iter_loss: 0.15476730465888977
train_iter_loss: 0.1405731588602066
train_iter_loss: 0.20587466657161713
train_iter_loss: 0.1270599514245987
train_iter_loss: 0.1407623291015625
train_iter_loss: 0.17167995870113373
train_iter_loss: 0.26457273960113525
train_iter_loss: 0.14692850410938263
train loss :0.1589
---------------------
Validation seg loss: 0.21412041538201695 at epoch 884
epoch =    885/  1000, exp = train
train_iter_loss: 0.1048637330532074
train_iter_loss: 0.2323906570672989
train_iter_loss: 0.3574494421482086
train_iter_loss: 0.08092878758907318
train_iter_loss: 0.2108822911977768
train_iter_loss: 0.15267886221408844
train_iter_loss: 0.1966356635093689
train_iter_loss: 0.15982423722743988
train_iter_loss: 0.15372459590435028
train_iter_loss: 0.16027618944644928
train_iter_loss: 0.1279219388961792
train_iter_loss: 0.15356463193893433
train_iter_loss: 0.18893659114837646
train_iter_loss: 0.10956498235464096
train_iter_loss: 0.1324344426393509
train_iter_loss: 0.15561163425445557
train_iter_loss: 0.11978188902139664
train_iter_loss: 0.16593500971794128
train_iter_loss: 0.07169823348522186
train_iter_loss: 0.19207394123077393
train_iter_loss: 0.17189975082874298
train_iter_loss: 0.08649194985628128
train_iter_loss: 0.07400627434253693
train_iter_loss: 0.2049601823091507
train_iter_loss: 0.1522538810968399
train_iter_loss: 0.07637283205986023
train_iter_loss: 0.1615409106016159
train_iter_loss: 0.06652989238500595
train_iter_loss: 0.1915450394153595
train_iter_loss: 0.2030373364686966
train_iter_loss: 0.1771911084651947
train_iter_loss: 0.19110138714313507
train_iter_loss: 0.15413311123847961
train_iter_loss: 0.06065865233540535
train_iter_loss: 0.11139433085918427
train_iter_loss: 0.2292860448360443
train_iter_loss: 0.0502927303314209
train_iter_loss: 0.19390814006328583
train_iter_loss: 0.26607680320739746
train_iter_loss: 0.2585766911506653
train_iter_loss: 0.15419186651706696
train_iter_loss: 0.1898217499256134
train_iter_loss: 0.09646903723478317
train_iter_loss: 0.15869535505771637
train_iter_loss: 0.17114290595054626
train_iter_loss: 0.056025296449661255
train_iter_loss: 0.10331088304519653
train_iter_loss: 0.20054884254932404
train_iter_loss: 0.07410930097103119
train_iter_loss: 0.2708238959312439
train_iter_loss: 0.12772810459136963
train_iter_loss: 0.2119871973991394
train_iter_loss: 0.13421253859996796
train_iter_loss: 0.20568862557411194
train_iter_loss: 0.11064738780260086
train_iter_loss: 0.07779423147439957
train_iter_loss: 0.10628339648246765
train_iter_loss: 0.1266716718673706
train_iter_loss: 0.19426476955413818
train_iter_loss: 0.1066863164305687
train_iter_loss: 0.07885777950286865
train_iter_loss: 0.1330469697713852
train_iter_loss: 0.3824581503868103
train_iter_loss: 0.20079974830150604
train_iter_loss: 0.22840167582035065
train_iter_loss: 0.19330155849456787
train_iter_loss: 0.15652096271514893
train_iter_loss: 0.10691482573747635
train_iter_loss: 0.14345355331897736
train_iter_loss: 0.1857956051826477
train_iter_loss: 0.04417307674884796
train_iter_loss: 0.1794661283493042
train_iter_loss: 0.1656937599182129
train_iter_loss: 0.06623905897140503
train_iter_loss: 0.20449484884738922
train_iter_loss: 0.24962590634822845
train_iter_loss: 0.0571630597114563
train_iter_loss: 0.14633281528949738
train_iter_loss: 0.1749546080827713
train_iter_loss: 0.2598602771759033
train_iter_loss: 0.11940333992242813
train_iter_loss: 0.2311112880706787
train_iter_loss: 0.27865466475486755
train_iter_loss: 0.17871443927288055
train_iter_loss: 0.16185754537582397
train_iter_loss: 0.13590334355831146
train_iter_loss: 0.22469152510166168
train_iter_loss: 0.10585757344961166
train_iter_loss: 0.11223253607749939
train_iter_loss: 0.12982410192489624
train_iter_loss: 0.19740058481693268
train_iter_loss: 0.11764489114284515
train_iter_loss: 0.08332687616348267
train_iter_loss: 0.14608334004878998
train_iter_loss: 0.08174404501914978
train_iter_loss: 0.2508232295513153
train_iter_loss: 0.06418393552303314
train_iter_loss: 0.1990271955728531
train_iter_loss: 0.2118241935968399
train_iter_loss: 0.13372018933296204
train loss :0.1587
---------------------
Validation seg loss: 0.2158296472779563 at epoch 885
epoch =    886/  1000, exp = train
train_iter_loss: 0.11701744794845581
train_iter_loss: 0.14815020561218262
train_iter_loss: 0.17589467763900757
train_iter_loss: 0.07457652688026428
train_iter_loss: 0.1417936086654663
train_iter_loss: 0.12437005341053009
train_iter_loss: 0.12405163794755936
train_iter_loss: 0.06510265171527863
train_iter_loss: 0.12167385965585709
train_iter_loss: 0.11756259948015213
train_iter_loss: 0.16071879863739014
train_iter_loss: 0.153141051530838
train_iter_loss: 0.2159290611743927
train_iter_loss: 0.06982643157243729
train_iter_loss: 0.10057038813829422
train_iter_loss: 0.0919286459684372
train_iter_loss: 0.11931563913822174
train_iter_loss: 0.15151086449623108
train_iter_loss: 0.11978772282600403
train_iter_loss: 0.08815429359674454
train_iter_loss: 0.18537090718746185
train_iter_loss: 0.21142573654651642
train_iter_loss: 0.09788434952497482
train_iter_loss: 0.12366139888763428
train_iter_loss: 0.13250628113746643
train_iter_loss: 0.09871087968349457
train_iter_loss: 0.3499809205532074
train_iter_loss: 0.07015836983919144
train_iter_loss: 0.11827012151479721
train_iter_loss: 0.17882685363292694
train_iter_loss: 0.10754092037677765
train_iter_loss: 0.08916152268648148
train_iter_loss: 0.07272069901227951
train_iter_loss: 0.1162455826997757
train_iter_loss: 0.15975640714168549
train_iter_loss: 0.12524078786373138
train_iter_loss: 0.12351800501346588
train_iter_loss: 0.16966161131858826
train_iter_loss: 0.18416710197925568
train_iter_loss: 0.20043553411960602
train_iter_loss: 0.10279402136802673
train_iter_loss: 0.20976480841636658
train_iter_loss: 0.1795046329498291
train_iter_loss: 0.1465986669063568
train_iter_loss: 0.11071327328681946
train_iter_loss: 0.04162570834159851
train_iter_loss: 0.14239881932735443
train_iter_loss: 0.17469719052314758
train_iter_loss: 0.11350851505994797
train_iter_loss: 0.12244512140750885
train_iter_loss: 0.1399526447057724
train_iter_loss: 0.1769261658191681
train_iter_loss: 0.15063226222991943
train_iter_loss: 0.12947115302085876
train_iter_loss: 0.22002585232257843
train_iter_loss: 0.27985137701034546
train_iter_loss: 0.1656692922115326
train_iter_loss: 0.0701807364821434
train_iter_loss: 0.20826999843120575
train_iter_loss: 0.19573979079723358
train_iter_loss: 0.4060179889202118
train_iter_loss: 0.3278026580810547
train_iter_loss: 0.15545570850372314
train_iter_loss: 0.27525103092193604
train_iter_loss: 0.16696041822433472
train_iter_loss: 0.24844340980052948
train_iter_loss: 0.13432376086711884
train_iter_loss: 0.25339192152023315
train_iter_loss: 0.12194070965051651
train_iter_loss: 0.19923730194568634
train_iter_loss: 0.2488209307193756
train_iter_loss: 0.23057861626148224
train_iter_loss: 0.18289104104042053
train_iter_loss: 0.1518457531929016
train_iter_loss: 0.16629840433597565
train_iter_loss: 0.07284990698099136
train_iter_loss: 0.27596983313560486
train_iter_loss: 0.13321246206760406
train_iter_loss: 0.13501127064228058
train_iter_loss: 0.07476211339235306
train_iter_loss: 0.12366919219493866
train_iter_loss: 0.16405509412288666
train_iter_loss: 0.1773100644350052
train_iter_loss: 0.19518117606639862
train_iter_loss: 0.13106131553649902
train_iter_loss: 0.09355098754167557
train_iter_loss: 0.1362619549036026
train_iter_loss: 0.0728004202246666
train_iter_loss: 0.08841373026371002
train_iter_loss: 0.14995786547660828
train_iter_loss: 0.07445024698972702
train_iter_loss: 0.16516698896884918
train_iter_loss: 0.1720682680606842
train_iter_loss: 0.23444679379463196
train_iter_loss: 0.27442505955696106
train_iter_loss: 0.10903028398752213
train_iter_loss: 0.13182106614112854
train_iter_loss: 0.06862682849168777
train_iter_loss: 0.2109714299440384
train_iter_loss: 0.18270473182201385
train loss :0.1555
---------------------
Validation seg loss: 0.21595380706536882 at epoch 886
epoch =    887/  1000, exp = train
train_iter_loss: 0.10575752705335617
train_iter_loss: 0.08085198700428009
train_iter_loss: 0.07487504929304123
train_iter_loss: 0.1013805940747261
train_iter_loss: 0.0900028645992279
train_iter_loss: 0.11596619337797165
train_iter_loss: 0.06334851682186127
train_iter_loss: 0.05115941911935806
train_iter_loss: 0.12433025240898132
train_iter_loss: 0.16651961207389832
train_iter_loss: 0.08330724388360977
train_iter_loss: 0.21969345211982727
train_iter_loss: 0.14014330506324768
train_iter_loss: 0.13513106107711792
train_iter_loss: 0.20485709607601166
train_iter_loss: 0.07028034329414368
train_iter_loss: 0.268407940864563
train_iter_loss: 0.1730007827281952
train_iter_loss: 0.08542134612798691
train_iter_loss: 0.14808803796768188
train_iter_loss: 0.1772940456867218
train_iter_loss: 0.34605300426483154
train_iter_loss: 0.1751033216714859
train_iter_loss: 0.1173948124051094
train_iter_loss: 0.22862716019153595
train_iter_loss: 0.2895349860191345
train_iter_loss: 0.1350513994693756
train_iter_loss: 0.2142866849899292
train_iter_loss: 0.22796620428562164
train_iter_loss: 0.16745705902576447
train_iter_loss: 0.16239558160305023
train_iter_loss: 0.10009743273258209
train_iter_loss: 0.11271432787179947
train_iter_loss: 0.10190770775079727
train_iter_loss: 0.14601589739322662
train_iter_loss: 0.1472872793674469
train_iter_loss: 0.25415998697280884
train_iter_loss: 0.14520812034606934
train_iter_loss: 0.25017258524894714
train_iter_loss: 0.10339639335870743
train_iter_loss: 0.1861371099948883
train_iter_loss: 0.15137164294719696
train_iter_loss: 0.09130603075027466
train_iter_loss: 0.18427063524723053
train_iter_loss: 0.2275802493095398
train_iter_loss: 0.07981205731630325
train_iter_loss: 0.1296946108341217
train_iter_loss: 0.14111800491809845
train_iter_loss: 0.10816846787929535
train_iter_loss: 0.23063983023166656
train_iter_loss: 0.18590429425239563
train_iter_loss: 0.20044761896133423
train_iter_loss: 0.16445866227149963
train_iter_loss: 0.13052330911159515
train_iter_loss: 0.14296968281269073
train_iter_loss: 0.1587202399969101
train_iter_loss: 0.12172853201627731
train_iter_loss: 0.10996980220079422
train_iter_loss: 0.18923164904117584
train_iter_loss: 0.13393543660640717
train_iter_loss: 0.15621790289878845
train_iter_loss: 0.19191060960292816
train_iter_loss: 0.10330282151699066
train_iter_loss: 0.2729792296886444
train_iter_loss: 0.062062475830316544
train_iter_loss: 0.15982964634895325
train_iter_loss: 0.08099974691867828
train_iter_loss: 0.18756449222564697
train_iter_loss: 0.14057673513889313
train_iter_loss: 0.15901170670986176
train_iter_loss: 0.17907480895519257
train_iter_loss: 0.19303958117961884
train_iter_loss: 0.057988621294498444
train_iter_loss: 0.12523476779460907
train_iter_loss: 0.060685720294713974
train_iter_loss: 0.14744044840335846
train_iter_loss: 0.1491508185863495
train_iter_loss: 0.10350138694047928
train_iter_loss: 0.24064403772354126
train_iter_loss: 0.14701691269874573
train_iter_loss: 0.23102612793445587
train_iter_loss: 0.1744959056377411
train_iter_loss: 0.3867715299129486
train_iter_loss: 0.1552850902080536
train_iter_loss: 0.2593056559562683
train_iter_loss: 0.08798419684171677
train_iter_loss: 0.2774718105792999
train_iter_loss: 0.14699748158454895
train_iter_loss: 0.19769029319286346
train_iter_loss: 0.20432904362678528
train_iter_loss: 0.14501002430915833
train_iter_loss: 0.15562793612480164
train_iter_loss: 0.1618805229663849
train_iter_loss: 0.1383771002292633
train_iter_loss: 0.1413881778717041
train_iter_loss: 0.23022355139255524
train_iter_loss: 0.09125334024429321
train_iter_loss: 0.2934522032737732
train_iter_loss: 0.1252622902393341
train_iter_loss: 0.11749251186847687
train loss :0.1597
---------------------
Validation seg loss: 0.21224929307991602 at epoch 887
epoch =    888/  1000, exp = train
train_iter_loss: 0.14705407619476318
train_iter_loss: 0.25021296739578247
train_iter_loss: 0.16442738473415375
train_iter_loss: 0.17477408051490784
train_iter_loss: 0.21585191786289215
train_iter_loss: 0.11583413928747177
train_iter_loss: 0.1435239166021347
train_iter_loss: 0.1645895540714264
train_iter_loss: 0.07781245559453964
train_iter_loss: 0.16400346159934998
train_iter_loss: 0.09398645162582397
train_iter_loss: 0.22667717933654785
train_iter_loss: 0.10197104513645172
train_iter_loss: 0.140275239944458
train_iter_loss: 0.16691584885120392
train_iter_loss: 0.16265250742435455
train_iter_loss: 0.09865640848875046
train_iter_loss: 0.12298960983753204
train_iter_loss: 0.11481264233589172
train_iter_loss: 0.132672518491745
train_iter_loss: 0.1757757067680359
train_iter_loss: 0.08798030018806458
train_iter_loss: 0.2808215320110321
train_iter_loss: 0.1967097371816635
train_iter_loss: 0.18379731476306915
train_iter_loss: 0.2863621413707733
train_iter_loss: 0.20359215140342712
train_iter_loss: 0.1916554719209671
train_iter_loss: 0.13245552778244019
train_iter_loss: 0.07736910879611969
train_iter_loss: 0.1237148866057396
train_iter_loss: 0.14315137267112732
train_iter_loss: 0.2292012721300125
train_iter_loss: 0.16938428580760956
train_iter_loss: 0.07149548083543777
train_iter_loss: 0.2104528695344925
train_iter_loss: 0.2746843993663788
train_iter_loss: 0.10732459276914597
train_iter_loss: 0.13896454870700836
train_iter_loss: 0.12282171100378036
train_iter_loss: 0.28786274790763855
train_iter_loss: 0.11936497688293457
train_iter_loss: 0.04400178790092468
train_iter_loss: 0.2386002540588379
train_iter_loss: 0.2668329179286957
train_iter_loss: 0.2531528174877167
train_iter_loss: 0.12484988570213318
train_iter_loss: 0.18785876035690308
train_iter_loss: 0.14188048243522644
train_iter_loss: 0.10168817639350891
train_iter_loss: 0.2074207216501236
train_iter_loss: 0.15337681770324707
train_iter_loss: 0.0965564176440239
train_iter_loss: 0.22467179596424103
train_iter_loss: 0.12587615847587585
train_iter_loss: 0.0880812481045723
train_iter_loss: 0.1626584231853485
train_iter_loss: 0.11364268511533737
train_iter_loss: 0.11248502135276794
train_iter_loss: 0.14583832025527954
train_iter_loss: 0.1490364670753479
train_iter_loss: 0.20673809945583344
train_iter_loss: 0.15776179730892181
train_iter_loss: 0.0760185569524765
train_iter_loss: 0.1798211634159088
train_iter_loss: 0.15938641130924225
train_iter_loss: 0.19286301732063293
train_iter_loss: 0.16688433289527893
train_iter_loss: 0.17578493058681488
train_iter_loss: 0.20653465390205383
train_iter_loss: 0.13535018265247345
train_iter_loss: 0.1156168133020401
train_iter_loss: 0.2895514965057373
train_iter_loss: 0.18644094467163086
train_iter_loss: 0.1548577845096588
train_iter_loss: 0.12440341711044312
train_iter_loss: 0.12935054302215576
train_iter_loss: 0.09787934273481369
train_iter_loss: 0.07236228883266449
train_iter_loss: 0.15701648592948914
train_iter_loss: 0.21269415318965912
train_iter_loss: 0.19210195541381836
train_iter_loss: 0.10970612615346909
train_iter_loss: 0.17818091809749603
train_iter_loss: 0.12757134437561035
train_iter_loss: 0.12028432637453079
train_iter_loss: 0.20011359453201294
train_iter_loss: 0.15955063700675964
train_iter_loss: 0.1088695079088211
train_iter_loss: 0.1387627124786377
train_iter_loss: 0.2388763725757599
train_iter_loss: 0.14849953353405
train_iter_loss: 0.06353442370891571
train_iter_loss: 0.17011089622974396
train_iter_loss: 0.13846713304519653
train_iter_loss: 0.1447358876466751
train_iter_loss: 0.1072295680642128
train_iter_loss: 0.08381422609090805
train_iter_loss: 0.14348110556602478
train_iter_loss: 0.1192023828625679
train loss :0.1578
---------------------
Validation seg loss: 0.2154921516512503 at epoch 888
epoch =    889/  1000, exp = train
train_iter_loss: 0.152713805437088
train_iter_loss: 0.2000381350517273
train_iter_loss: 0.10304759442806244
train_iter_loss: 0.3377141058444977
train_iter_loss: 0.15803276002407074
train_iter_loss: 0.06197810173034668
train_iter_loss: 0.15069852769374847
train_iter_loss: 0.22716595232486725
train_iter_loss: 0.09396746009588242
train_iter_loss: 0.30397817492485046
train_iter_loss: 0.11014844477176666
train_iter_loss: 0.09816604852676392
train_iter_loss: 0.18288706243038177
train_iter_loss: 0.07989560067653656
train_iter_loss: 0.07080868631601334
train_iter_loss: 0.24816107749938965
train_iter_loss: 0.09835811704397202
train_iter_loss: 0.19768770039081573
train_iter_loss: 0.15051785111427307
train_iter_loss: 0.12393641471862793
train_iter_loss: 0.2012615203857422
train_iter_loss: 0.13268619775772095
train_iter_loss: 0.14537851512432098
train_iter_loss: 0.18919268250465393
train_iter_loss: 0.1371878683567047
train_iter_loss: 0.09253270924091339
train_iter_loss: 0.09389319270849228
train_iter_loss: 0.06617546826601028
train_iter_loss: 0.07687066495418549
train_iter_loss: 0.13145597279071808
train_iter_loss: 0.218355193734169
train_iter_loss: 0.1256864368915558
train_iter_loss: 0.18199817836284637
train_iter_loss: 0.1917475014925003
train_iter_loss: 0.07615945488214493
train_iter_loss: 0.13606861233711243
train_iter_loss: 0.12039502710103989
train_iter_loss: 0.1485450714826584
train_iter_loss: 0.03644660860300064
train_iter_loss: 0.1585756540298462
train_iter_loss: 0.1254611760377884
train_iter_loss: 0.06054675206542015
train_iter_loss: 0.09198172390460968
train_iter_loss: 0.22756817936897278
train_iter_loss: 0.21821798384189606
train_iter_loss: 0.3724743127822876
train_iter_loss: 0.16010849177837372
train_iter_loss: 0.19963841140270233
train_iter_loss: 0.05283374711871147
train_iter_loss: 0.07891961187124252
train_iter_loss: 0.12572886049747467
train_iter_loss: 0.2722424566745758
train_iter_loss: 0.13645854592323303
train_iter_loss: 0.11203684657812119
train_iter_loss: 0.18297243118286133
train_iter_loss: 0.2520333528518677
train_iter_loss: 0.21285533905029297
train_iter_loss: 0.11210755258798599
train_iter_loss: 0.24289685487747192
train_iter_loss: 0.16834203898906708
train_iter_loss: 0.12908051908016205
train_iter_loss: 0.06018668785691261
train_iter_loss: 0.07473811507225037
train_iter_loss: 0.20650610327720642
train_iter_loss: 0.16798806190490723
train_iter_loss: 0.2615576684474945
train_iter_loss: 0.2556307017803192
train_iter_loss: 0.19340239465236664
train_iter_loss: 0.23670923709869385
train_iter_loss: 0.06689903140068054
train_iter_loss: 0.16473537683486938
train_iter_loss: 0.19979795813560486
train_iter_loss: 0.2059442698955536
train_iter_loss: 0.162010058760643
train_iter_loss: 0.06701411306858063
train_iter_loss: 0.10983555018901825
train_iter_loss: 0.12399516254663467
train_iter_loss: 0.1127464547753334
train_iter_loss: 0.19565439224243164
train_iter_loss: 0.1615201234817505
train_iter_loss: 0.1845998615026474
train_iter_loss: 0.12734971940517426
train_iter_loss: 0.13462084531784058
train_iter_loss: 0.12933726608753204
train_iter_loss: 0.1864442229270935
train_iter_loss: 0.15958242118358612
train_iter_loss: 0.04669683426618576
train_iter_loss: 0.39694276452064514
train_iter_loss: 0.13219451904296875
train_iter_loss: 0.06470014899969101
train_iter_loss: 0.16443251073360443
train_iter_loss: 0.25637394189834595
train_iter_loss: 0.21550799906253815
train_iter_loss: 0.22607703506946564
train_iter_loss: 0.12169970571994781
train_iter_loss: 0.11151373386383057
train_iter_loss: 0.19240114092826843
train_iter_loss: 0.14522720873355865
train_iter_loss: 0.15852980315685272
train_iter_loss: 0.09961347281932831
train loss :0.1578
---------------------
Validation seg loss: 0.21610822031309582 at epoch 889
epoch =    890/  1000, exp = train
train_iter_loss: 0.09912928938865662
train_iter_loss: 0.18337352573871613
train_iter_loss: 0.09275618940591812
train_iter_loss: 0.08213318884372711
train_iter_loss: 0.07631471008062363
train_iter_loss: 0.15859726071357727
train_iter_loss: 0.10383780300617218
train_iter_loss: 0.18775634467601776
train_iter_loss: 0.1756797581911087
train_iter_loss: 0.1353384256362915
train_iter_loss: 0.14463776350021362
train_iter_loss: 0.1341402530670166
train_iter_loss: 0.15315717458724976
train_iter_loss: 0.08720358461141586
train_iter_loss: 0.15676090121269226
train_iter_loss: 0.11051087826490402
train_iter_loss: 0.23728586733341217
train_iter_loss: 0.2431863248348236
train_iter_loss: 0.07189096510410309
train_iter_loss: 0.20231488347053528
train_iter_loss: 0.14089800417423248
train_iter_loss: 0.15133650600910187
train_iter_loss: 0.09160378575325012
train_iter_loss: 0.216548353433609
train_iter_loss: 0.10767269134521484
train_iter_loss: 0.035596318542957306
train_iter_loss: 0.12935025990009308
train_iter_loss: 0.23545236885547638
train_iter_loss: 0.10033578425645828
train_iter_loss: 0.2929494380950928
train_iter_loss: 0.12352181226015091
train_iter_loss: 0.14026564359664917
train_iter_loss: 0.2396129071712494
train_iter_loss: 0.23690113425254822
train_iter_loss: 0.13105089962482452
train_iter_loss: 0.1331912875175476
train_iter_loss: 0.1769421547651291
train_iter_loss: 0.1313711702823639
train_iter_loss: 0.10464209318161011
train_iter_loss: 0.11037272214889526
train_iter_loss: 0.07466034591197968
train_iter_loss: 0.2143329232931137
train_iter_loss: 0.25600412487983704
train_iter_loss: 0.17908695340156555
train_iter_loss: 0.1712903082370758
train_iter_loss: 0.19813476502895355
train_iter_loss: 0.20552772283554077
train_iter_loss: 0.1324339658021927
train_iter_loss: 0.08237560838460922
train_iter_loss: 0.14408040046691895
train_iter_loss: 0.1912512630224228
train_iter_loss: 0.06927352398633957
train_iter_loss: 0.1617632806301117
train_iter_loss: 0.14340347051620483
train_iter_loss: 0.18626204133033752
train_iter_loss: 0.2562185227870941
train_iter_loss: 0.14472320675849915
train_iter_loss: 0.14304426312446594
train_iter_loss: 0.14962416887283325
train_iter_loss: 0.09557837247848511
train_iter_loss: 0.1412549912929535
train_iter_loss: 0.21214890480041504
train_iter_loss: 0.08709485083818436
train_iter_loss: 0.10998401045799255
train_iter_loss: 0.029104823246598244
train_iter_loss: 0.18408295512199402
train_iter_loss: 0.10868680477142334
train_iter_loss: 0.1422593742609024
train_iter_loss: 0.1439986526966095
train_iter_loss: 0.11626098304986954
train_iter_loss: 0.1727231740951538
train_iter_loss: 0.11289510875940323
train_iter_loss: 0.0838346928358078
train_iter_loss: 0.23165294528007507
train_iter_loss: 0.08291026949882507
train_iter_loss: 0.15761667490005493
train_iter_loss: 0.3379577398300171
train_iter_loss: 0.22939059138298035
train_iter_loss: 0.15527166426181793
train_iter_loss: 0.25865063071250916
train_iter_loss: 0.27320367097854614
train_iter_loss: 0.06236100196838379
train_iter_loss: 0.2794082760810852
train_iter_loss: 0.30906036496162415
train_iter_loss: 0.12413815408945084
train_iter_loss: 0.100894495844841
train_iter_loss: 0.16440650820732117
train_iter_loss: 0.2110973447561264
train_iter_loss: 0.14745694398880005
train_iter_loss: 0.11631543189287186
train_iter_loss: 0.255910187959671
train_iter_loss: 0.14627918601036072
train_iter_loss: 0.23128056526184082
train_iter_loss: 0.050096698105335236
train_iter_loss: 0.06641606986522675
train_iter_loss: 0.20188188552856445
train_iter_loss: 0.2874896824359894
train_iter_loss: 0.09476115554571152
train_iter_loss: 0.12901590764522552
train_iter_loss: 0.16256435215473175
train loss :0.1573
---------------------
Validation seg loss: 0.21508392732028128 at epoch 890
epoch =    891/  1000, exp = train
train_iter_loss: 0.09720122814178467
train_iter_loss: 0.11031027138233185
train_iter_loss: 0.2331019639968872
train_iter_loss: 0.11877819150686264
train_iter_loss: 0.060387060046195984
train_iter_loss: 0.02892732247710228
train_iter_loss: 0.09667018800973892
train_iter_loss: 0.10874225199222565
train_iter_loss: 0.10218781232833862
train_iter_loss: 0.17767280340194702
train_iter_loss: 0.22002048790454865
train_iter_loss: 0.09907899796962738
train_iter_loss: 0.34388619661331177
train_iter_loss: 0.150884211063385
train_iter_loss: 0.12872454524040222
train_iter_loss: 0.08543086051940918
train_iter_loss: 0.12413895130157471
train_iter_loss: 0.2549528479576111
train_iter_loss: 0.10130105167627335
train_iter_loss: 0.07051539421081543
train_iter_loss: 0.08533145487308502
train_iter_loss: 0.26231133937835693
train_iter_loss: 0.12009640783071518
train_iter_loss: 0.11569160223007202
train_iter_loss: 0.1228056326508522
train_iter_loss: 0.06753206253051758
train_iter_loss: 0.12294549494981766
train_iter_loss: 0.10967767983675003
train_iter_loss: 0.16767071187496185
train_iter_loss: 0.15158496797084808
train_iter_loss: 0.09880051761865616
train_iter_loss: 0.23686262965202332
train_iter_loss: 0.170499786734581
train_iter_loss: 0.21939963102340698
train_iter_loss: 0.09107556939125061
train_iter_loss: 0.11013078689575195
train_iter_loss: 0.169549360871315
train_iter_loss: 0.09575586766004562
train_iter_loss: 0.3030897080898285
train_iter_loss: 0.23624180257320404
train_iter_loss: 0.10910934954881668
train_iter_loss: 0.1763380914926529
train_iter_loss: 0.13878268003463745
train_iter_loss: 0.0934043750166893
train_iter_loss: 0.20629072189331055
train_iter_loss: 0.12555642426013947
train_iter_loss: 0.2889334261417389
train_iter_loss: 0.2773461639881134
train_iter_loss: 0.17793992161750793
train_iter_loss: 0.03714289888739586
train_iter_loss: 0.20938430726528168
train_iter_loss: 0.2957262098789215
train_iter_loss: 0.2017277330160141
train_iter_loss: 0.04539157450199127
train_iter_loss: 0.18333542346954346
train_iter_loss: 0.22360865771770477
train_iter_loss: 0.20009972155094147
train_iter_loss: 0.1430044025182724
train_iter_loss: 0.22053901851177216
train_iter_loss: 0.0847112312912941
train_iter_loss: 0.13642139732837677
train_iter_loss: 0.08898618817329407
train_iter_loss: 0.06449689716100693
train_iter_loss: 0.15064801275730133
train_iter_loss: 0.09788762778043747
train_iter_loss: 0.25728675723075867
train_iter_loss: 0.19641979038715363
train_iter_loss: 0.2056884616613388
train_iter_loss: 0.19826281070709229
train_iter_loss: 0.27187833189964294
train_iter_loss: 0.17898592352867126
train_iter_loss: 0.2905159592628479
train_iter_loss: 0.09386828541755676
train_iter_loss: 0.11447343975305557
train_iter_loss: 0.11105313152074814
train_iter_loss: 0.23679077625274658
train_iter_loss: 0.0909702479839325
train_iter_loss: 0.19574081897735596
train_iter_loss: 0.11314985156059265
train_iter_loss: 0.12609291076660156
train_iter_loss: 0.22127729654312134
train_iter_loss: 0.10618344694375992
train_iter_loss: 0.13811884820461273
train_iter_loss: 0.09576164931058884
train_iter_loss: 0.104607492685318
train_iter_loss: 0.1265885829925537
train_iter_loss: 0.3092362582683563
train_iter_loss: 0.0830577090382576
train_iter_loss: 0.14563918113708496
train_iter_loss: 0.15541969239711761
train_iter_loss: 0.24662409722805023
train_iter_loss: 0.16782331466674805
train_iter_loss: 0.0844949409365654
train_iter_loss: 0.19361692667007446
train_iter_loss: 0.1895958036184311
train_iter_loss: 0.13572867214679718
train_iter_loss: 0.11896193772554398
train_iter_loss: 0.19577531516551971
train_iter_loss: 0.07795730978250504
train_iter_loss: 0.11051934212446213
train loss :0.1559
---------------------
Validation seg loss: 0.21573417946555704 at epoch 891
epoch =    892/  1000, exp = train
train_iter_loss: 0.1715223491191864
train_iter_loss: 0.12839339673519135
train_iter_loss: 0.22308312356472015
train_iter_loss: 0.20538784563541412
train_iter_loss: 0.11634068191051483
train_iter_loss: 0.16296371817588806
train_iter_loss: 0.08256217837333679
train_iter_loss: 0.13540072739124298
train_iter_loss: 0.09053748846054077
train_iter_loss: 0.17420126497745514
train_iter_loss: 0.19804328680038452
train_iter_loss: 0.0848497822880745
train_iter_loss: 0.1329243928194046
train_iter_loss: 0.2374403476715088
train_iter_loss: 0.12781129777431488
train_iter_loss: 0.1723976582288742
train_iter_loss: 0.12173277139663696
train_iter_loss: 0.1467110812664032
train_iter_loss: 0.09109405428171158
train_iter_loss: 0.07264909893274307
train_iter_loss: 0.15408508479595184
train_iter_loss: 0.13189084827899933
train_iter_loss: 0.11435951292514801
train_iter_loss: 0.17245744168758392
train_iter_loss: 0.11478343605995178
train_iter_loss: 0.15563815832138062
train_iter_loss: 0.17431418597698212
train_iter_loss: 0.068336121737957
train_iter_loss: 0.18997907638549805
train_iter_loss: 0.23913276195526123
train_iter_loss: 0.11502466350793839
train_iter_loss: 0.21222944557666779
train_iter_loss: 0.14437247812747955
train_iter_loss: 0.09206893295049667
train_iter_loss: 0.15198148787021637
train_iter_loss: 0.1650974154472351
train_iter_loss: 0.12261492758989334
train_iter_loss: 0.19718652963638306
train_iter_loss: 0.17988955974578857
train_iter_loss: 0.11427280306816101
train_iter_loss: 0.06954652816057205
train_iter_loss: 0.10358721017837524
train_iter_loss: 0.2063228040933609
train_iter_loss: 0.19591687619686127
train_iter_loss: 0.1858355551958084
train_iter_loss: 0.20619449019432068
train_iter_loss: 0.11453624069690704
train_iter_loss: 0.3013450503349304
train_iter_loss: 0.049380749464035034
train_iter_loss: 0.061252519488334656
train_iter_loss: 0.08972813189029694
train_iter_loss: 0.344987154006958
train_iter_loss: 0.15120872855186462
train_iter_loss: 0.14553530514240265
train_iter_loss: 0.1958940476179123
train_iter_loss: 0.17947284877300262
train_iter_loss: 0.09229008853435516
train_iter_loss: 0.1686788648366928
train_iter_loss: 0.19200372695922852
train_iter_loss: 0.10088785737752914
train_iter_loss: 0.12213725596666336
train_iter_loss: 0.14729276299476624
train_iter_loss: 0.08012589812278748
train_iter_loss: 0.12388085573911667
train_iter_loss: 0.20633740723133087
train_iter_loss: 0.21359041333198547
train_iter_loss: 0.1794958859682083
train_iter_loss: 0.11960197240114212
train_iter_loss: 0.35837969183921814
train_iter_loss: 0.08694620430469513
train_iter_loss: 0.15996459126472473
train_iter_loss: 0.27429288625717163
train_iter_loss: 0.1939081996679306
train_iter_loss: 0.12901140749454498
train_iter_loss: 0.36449557542800903
train_iter_loss: 0.05838492512702942
train_iter_loss: 0.07244133204221725
train_iter_loss: 0.1254618614912033
train_iter_loss: 0.2206300050020218
train_iter_loss: 0.08753412216901779
train_iter_loss: 0.09894726425409317
train_iter_loss: 0.21476367115974426
train_iter_loss: 0.19079862534999847
train_iter_loss: 0.24144089221954346
train_iter_loss: 0.10980544239282608
train_iter_loss: 0.18743789196014404
train_iter_loss: 0.16650895774364471
train_iter_loss: 0.08726944774389267
train_iter_loss: 0.20090696215629578
train_iter_loss: 0.2191573530435562
train_iter_loss: 0.23539647459983826
train_iter_loss: 0.10845004767179489
train_iter_loss: 0.10339687764644623
train_iter_loss: 0.19338633120059967
train_iter_loss: 0.10079433023929596
train_iter_loss: 0.12429708987474442
train_iter_loss: 0.22087673842906952
train_iter_loss: 0.06222277507185936
train_iter_loss: 0.19218190014362335
train_iter_loss: 0.08781366050243378
train loss :0.1567
---------------------
Validation seg loss: 0.21281963402299947 at epoch 892
epoch =    893/  1000, exp = train
train_iter_loss: 0.14413541555404663
train_iter_loss: 0.07107198983430862
train_iter_loss: 0.053748298436403275
train_iter_loss: 0.11684256047010422
train_iter_loss: 0.07850408554077148
train_iter_loss: 0.1615721434354782
train_iter_loss: 0.19255052506923676
train_iter_loss: 0.09292954951524734
train_iter_loss: 0.09617576003074646
train_iter_loss: 0.15455153584480286
train_iter_loss: 0.1361832469701767
train_iter_loss: 0.06713796406984329
train_iter_loss: 0.11701422184705734
train_iter_loss: 0.14660052955150604
train_iter_loss: 0.2462250143289566
train_iter_loss: 0.17306751012802124
train_iter_loss: 0.14977186918258667
train_iter_loss: 0.10562192648649216
train_iter_loss: 0.3144204914569855
train_iter_loss: 0.13313519954681396
train_iter_loss: 0.17769446969032288
train_iter_loss: 0.17290253937244415
train_iter_loss: 0.11130973696708679
train_iter_loss: 0.1442217379808426
train_iter_loss: 0.14367015659809113
train_iter_loss: 0.1378113031387329
train_iter_loss: 0.09156234562397003
train_iter_loss: 0.14688752591609955
train_iter_loss: 0.1913660764694214
train_iter_loss: 0.1688508689403534
train_iter_loss: 0.2398662120103836
train_iter_loss: 0.16480803489685059
train_iter_loss: 0.06996525824069977
train_iter_loss: 0.12959006428718567
train_iter_loss: 0.18084518611431122
train_iter_loss: 0.1548975110054016
train_iter_loss: 0.22023043036460876
train_iter_loss: 0.035075679421424866
train_iter_loss: 0.20280855894088745
train_iter_loss: 0.1713864505290985
train_iter_loss: 0.15526211261749268
train_iter_loss: 0.1546303629875183
train_iter_loss: 0.13856850564479828
train_iter_loss: 0.08483818918466568
train_iter_loss: 0.21726758778095245
train_iter_loss: 0.18395593762397766
train_iter_loss: 0.12633328139781952
train_iter_loss: 0.13169315457344055
train_iter_loss: 0.13080371916294098
train_iter_loss: 0.09523996710777283
train_iter_loss: 0.170364111661911
train_iter_loss: 0.1723901778459549
train_iter_loss: 0.14978757500648499
train_iter_loss: 0.11542991548776627
train_iter_loss: 0.11439570784568787
train_iter_loss: 0.14802634716033936
train_iter_loss: 0.16617850959300995
train_iter_loss: 0.13175010681152344
train_iter_loss: 0.1573675125837326
train_iter_loss: 0.11682651937007904
train_iter_loss: 0.19136986136436462
train_iter_loss: 0.2471029907464981
train_iter_loss: 0.09254179149866104
train_iter_loss: 0.10194405913352966
train_iter_loss: 0.23686879873275757
train_iter_loss: 0.19110940396785736
train_iter_loss: 0.13881836831569672
train_iter_loss: 0.19468948245048523
train_iter_loss: 0.1439124345779419
train_iter_loss: 0.17298030853271484
train_iter_loss: 0.08326432853937149
train_iter_loss: 0.17456339299678802
train_iter_loss: 0.072766974568367
train_iter_loss: 0.34355446696281433
train_iter_loss: 0.3149169981479645
train_iter_loss: 0.12304861843585968
train_iter_loss: 0.12431738525629044
train_iter_loss: 0.04072200879454613
train_iter_loss: 0.2113977074623108
train_iter_loss: 0.12544400990009308
train_iter_loss: 0.07274514436721802
train_iter_loss: 0.2163202166557312
train_iter_loss: 0.2623840272426605
train_iter_loss: 0.09840263426303864
train_iter_loss: 0.13770867884159088
train_iter_loss: 0.18685361742973328
train_iter_loss: 0.19966168701648712
train_iter_loss: 0.15026400983333588
train_iter_loss: 0.12098221480846405
train_iter_loss: 0.3361378610134125
train_iter_loss: 0.0964050143957138
train_iter_loss: 0.07709018886089325
train_iter_loss: 0.1891706883907318
train_iter_loss: 0.13128648698329926
train_iter_loss: 0.1670694798231125
train_iter_loss: 0.2352205067873001
train_iter_loss: 0.1657923310995102
train_iter_loss: 0.07678905129432678
train_iter_loss: 0.37129512429237366
train_iter_loss: 0.12418531626462936
train loss :0.1557
---------------------
Validation seg loss: 0.2128454742882893 at epoch 893
epoch =    894/  1000, exp = train
train_iter_loss: 0.1644955277442932
train_iter_loss: 0.12428265064954758
train_iter_loss: 0.10816151648759842
train_iter_loss: 0.05572798103094101
train_iter_loss: 0.06533102691173553
train_iter_loss: 0.14620763063430786
train_iter_loss: 0.14019158482551575
train_iter_loss: 0.13862672448158264
train_iter_loss: 0.15737873315811157
train_iter_loss: 0.17432618141174316
train_iter_loss: 0.2048506885766983
train_iter_loss: 0.1445508897304535
train_iter_loss: 0.22619949281215668
train_iter_loss: 0.14015629887580872
train_iter_loss: 0.16362497210502625
train_iter_loss: 0.16583405435085297
train_iter_loss: 0.24380695819854736
train_iter_loss: 0.09709446877241135
train_iter_loss: 0.1646776795387268
train_iter_loss: 0.16146208345890045
train_iter_loss: 0.13571582734584808
train_iter_loss: 0.20276974141597748
train_iter_loss: 0.11399739235639572
train_iter_loss: 0.1363544911146164
train_iter_loss: 0.22285079956054688
train_iter_loss: 0.14816223084926605
train_iter_loss: 0.13267742097377777
train_iter_loss: 0.1365852952003479
train_iter_loss: 0.1209859624505043
train_iter_loss: 0.15571551024913788
train_iter_loss: 0.2955021858215332
train_iter_loss: 0.2729357182979584
train_iter_loss: 0.11808616667985916
train_iter_loss: 0.19609296321868896
train_iter_loss: 0.06360118836164474
train_iter_loss: 0.1746424436569214
train_iter_loss: 0.05801776424050331
train_iter_loss: 0.19837844371795654
train_iter_loss: 0.16879798471927643
train_iter_loss: 0.15732550621032715
train_iter_loss: 0.17679691314697266
train_iter_loss: 0.16889874637126923
train_iter_loss: 0.17558865249156952
train_iter_loss: 0.03881826996803284
train_iter_loss: 0.1416747123003006
train_iter_loss: 0.15987321734428406
train_iter_loss: 0.06832373142242432
train_iter_loss: 0.12388172745704651
train_iter_loss: 0.17580123245716095
train_iter_loss: 0.10709470510482788
train_iter_loss: 0.14582450687885284
train_iter_loss: 0.14595669507980347
train_iter_loss: 0.10615081340074539
train_iter_loss: 0.16040819883346558
train_iter_loss: 0.2190808802843094
train_iter_loss: 0.11648645251989365
train_iter_loss: 0.15505629777908325
train_iter_loss: 0.18111462891101837
train_iter_loss: 0.09657926857471466
train_iter_loss: 0.1631155163049698
train_iter_loss: 0.2226645052433014
train_iter_loss: 0.17706333100795746
train_iter_loss: 0.16995075345039368
train_iter_loss: 0.06229975447058678
train_iter_loss: 0.14992734789848328
train_iter_loss: 0.16146521270275116
train_iter_loss: 0.09499339014291763
train_iter_loss: 0.0953579992055893
train_iter_loss: 0.1006762683391571
train_iter_loss: 0.1118587851524353
train_iter_loss: 0.11511407792568207
train_iter_loss: 0.10909149050712585
train_iter_loss: 0.08896008133888245
train_iter_loss: 0.18188729882240295
train_iter_loss: 0.21013668179512024
train_iter_loss: 0.10835041105747223
train_iter_loss: 0.14408129453659058
train_iter_loss: 0.15106992423534393
train_iter_loss: 0.32413971424102783
train_iter_loss: 0.21261829137802124
train_iter_loss: 0.046746160835027695
train_iter_loss: 0.15454602241516113
train_iter_loss: 0.15308362245559692
train_iter_loss: 0.25984108448028564
train_iter_loss: 0.0706234872341156
train_iter_loss: 0.14725208282470703
train_iter_loss: 0.2119709700345993
train_iter_loss: 0.08800607174634933
train_iter_loss: 0.18252573907375336
train_iter_loss: 0.2056056708097458
train_iter_loss: 0.25793057680130005
train_iter_loss: 0.10366037487983704
train_iter_loss: 0.24434776604175568
train_iter_loss: 0.07624520361423492
train_iter_loss: 0.10961943864822388
train_iter_loss: 0.072752945125103
train_iter_loss: 0.18005549907684326
train_iter_loss: 0.09053952991962433
train_iter_loss: 0.14358402788639069
train_iter_loss: 0.11714675277471542
train loss :0.1509
---------------------
Validation seg loss: 0.21650300033495956 at epoch 894
epoch =    895/  1000, exp = train
train_iter_loss: 0.14526917040348053
train_iter_loss: 0.16720633208751678
train_iter_loss: 0.1871822625398636
train_iter_loss: 0.15947093069553375
train_iter_loss: 0.11984454840421677
train_iter_loss: 0.24942897260189056
train_iter_loss: 0.2290709763765335
train_iter_loss: 0.1967417597770691
train_iter_loss: 0.1253831386566162
train_iter_loss: 0.15482228994369507
train_iter_loss: 0.1970982551574707
train_iter_loss: 0.27679499983787537
train_iter_loss: 0.19523851573467255
train_iter_loss: 0.13187195360660553
train_iter_loss: 0.07693420350551605
train_iter_loss: 0.13092632591724396
train_iter_loss: 0.13940875232219696
train_iter_loss: 0.16926750540733337
train_iter_loss: 0.13451117277145386
train_iter_loss: 0.19863024353981018
train_iter_loss: 0.11340071260929108
train_iter_loss: 0.055753424763679504
train_iter_loss: 0.11797846108675003
train_iter_loss: 0.1501689851284027
train_iter_loss: 0.18819163739681244
train_iter_loss: 0.12782688438892365
train_iter_loss: 0.06555343419313431
train_iter_loss: 0.08426977694034576
train_iter_loss: 0.12268888205289841
train_iter_loss: 0.12892411649227142
train_iter_loss: 0.12061509490013123
train_iter_loss: 0.13278569281101227
train_iter_loss: 0.16296201944351196
train_iter_loss: 0.07469695061445236
train_iter_loss: 0.15195468068122864
train_iter_loss: 0.11644533276557922
train_iter_loss: 0.1987382173538208
train_iter_loss: 0.11842871457338333
train_iter_loss: 0.15715229511260986
train_iter_loss: 0.06110062822699547
train_iter_loss: 0.091178297996521
train_iter_loss: 0.15621668100357056
train_iter_loss: 0.13257162272930145
train_iter_loss: 0.17550286650657654
train_iter_loss: 0.18350893259048462
train_iter_loss: 0.21895535290241241
train_iter_loss: 0.14902344346046448
train_iter_loss: 0.12769672274589539
train_iter_loss: 0.17564953863620758
train_iter_loss: 0.14325377345085144
train_iter_loss: 0.126662015914917
train_iter_loss: 0.17152735590934753
train_iter_loss: 0.14735493063926697
train_iter_loss: 0.20551232993602753
train_iter_loss: 0.11674496531486511
train_iter_loss: 0.3146182596683502
train_iter_loss: 0.16938067972660065
train_iter_loss: 0.06643489748239517
train_iter_loss: 0.13937970995903015
train_iter_loss: 0.1317550092935562
train_iter_loss: 0.1432851254940033
train_iter_loss: 0.18620753288269043
train_iter_loss: 0.20805509388446808
train_iter_loss: 0.16776150465011597
train_iter_loss: 0.12659217417240143
train_iter_loss: 0.20249612629413605
train_iter_loss: 0.2280488908290863
train_iter_loss: 0.12893667817115784
train_iter_loss: 0.09155777841806412
train_iter_loss: 0.254619836807251
train_iter_loss: 0.23926673829555511
train_iter_loss: 0.1191297248005867
train_iter_loss: 0.16207532584667206
train_iter_loss: 0.14604637026786804
train_iter_loss: 0.209554523229599
train_iter_loss: 0.09237006306648254
train_iter_loss: 0.16806508600711823
train_iter_loss: 0.22648689150810242
train_iter_loss: 0.20414207875728607
train_iter_loss: 0.15336863696575165
train_iter_loss: 0.09074193984270096
train_iter_loss: 0.07194112986326218
train_iter_loss: 0.11606086790561676
train_iter_loss: 0.07510865479707718
train_iter_loss: 0.20001283288002014
train_iter_loss: 0.10772442817687988
train_iter_loss: 0.11386531591415405
train_iter_loss: 0.12123247236013412
train_iter_loss: 0.21598578989505768
train_iter_loss: 0.14087636768817902
train_iter_loss: 0.14111328125
train_iter_loss: 0.15718157589435577
train_iter_loss: 0.21673549711704254
train_iter_loss: 0.2632407248020172
train_iter_loss: 0.17275361716747284
train_iter_loss: 0.1176115870475769
train_iter_loss: 0.15680350363254547
train_iter_loss: 0.0891633927822113
train_iter_loss: 0.20920243859291077
train_iter_loss: 0.14870207011699677
train loss :0.1551
---------------------
Validation seg loss: 0.21593036078232922 at epoch 895
epoch =    896/  1000, exp = train
train_iter_loss: 0.2197190821170807
train_iter_loss: 0.15367405116558075
train_iter_loss: 0.14858579635620117
train_iter_loss: 0.09420198947191238
train_iter_loss: 0.12468916922807693
train_iter_loss: 0.13840411603450775
train_iter_loss: 0.1037890762090683
train_iter_loss: 0.14694884419441223
train_iter_loss: 0.06888334453105927
train_iter_loss: 0.22218438982963562
train_iter_loss: 0.19358040392398834
train_iter_loss: 0.08270750194787979
train_iter_loss: 0.11141020059585571
train_iter_loss: 0.2950388491153717
train_iter_loss: 0.06936269998550415
train_iter_loss: 0.14317262172698975
train_iter_loss: 0.15991944074630737
train_iter_loss: 0.18543320894241333
train_iter_loss: 0.25697025656700134
train_iter_loss: 0.10049320012331009
train_iter_loss: 0.09957945346832275
train_iter_loss: 0.0835920199751854
train_iter_loss: 0.12358958274126053
train_iter_loss: 0.16016431152820587
train_iter_loss: 0.11064974963665009
train_iter_loss: 0.09598089009523392
train_iter_loss: 0.24186588823795319
train_iter_loss: 0.16618607938289642
train_iter_loss: 0.20277468860149384
train_iter_loss: 0.265809029340744
train_iter_loss: 0.13772112131118774
train_iter_loss: 0.12275642156600952
train_iter_loss: 0.30305641889572144
train_iter_loss: 0.19436627626419067
train_iter_loss: 0.10075398534536362
train_iter_loss: 0.05580662563443184
train_iter_loss: 0.12102332711219788
train_iter_loss: 0.31574925780296326
train_iter_loss: 0.1361331194639206
train_iter_loss: 0.15743158757686615
train_iter_loss: 0.14272068440914154
train_iter_loss: 0.09021151065826416
train_iter_loss: 0.12330643087625504
train_iter_loss: 0.2925984561443329
train_iter_loss: 0.15283600986003876
train_iter_loss: 0.06021719425916672
train_iter_loss: 0.24520786106586456
train_iter_loss: 0.09422770142555237
train_iter_loss: 0.2241395115852356
train_iter_loss: 0.18393240869045258
train_iter_loss: 0.13651403784751892
train_iter_loss: 0.18254582583904266
train_iter_loss: 0.2001195251941681
train_iter_loss: 0.091953344643116
train_iter_loss: 0.35066452622413635
train_iter_loss: 0.1746205985546112
train_iter_loss: 0.10308979451656342
train_iter_loss: 0.21328353881835938
train_iter_loss: 0.17470811307430267
train_iter_loss: 0.09704874455928802
train_iter_loss: 0.3348286747932434
train_iter_loss: 0.1726585477590561
train_iter_loss: 0.1025015339255333
train_iter_loss: 0.11845874786376953
train_iter_loss: 0.31086018681526184
train_iter_loss: 0.049847312271595
train_iter_loss: 0.16801388561725616
train_iter_loss: 0.15071484446525574
train_iter_loss: 0.17903770506381989
train_iter_loss: 0.15972115099430084
train_iter_loss: 0.1977393925189972
train_iter_loss: 0.0933065190911293
train_iter_loss: 0.10885801166296005
train_iter_loss: 0.14591938257217407
train_iter_loss: 0.15673306584358215
train_iter_loss: 0.1600610762834549
train_iter_loss: 0.19264106452465057
train_iter_loss: 0.11535666137933731
train_iter_loss: 0.04782635718584061
train_iter_loss: 0.2359052300453186
train_iter_loss: 0.065474733710289
train_iter_loss: 0.3417825400829315
train_iter_loss: 0.14394620060920715
train_iter_loss: 0.14220066368579865
train_iter_loss: 0.18928733468055725
train_iter_loss: 0.15119999647140503
train_iter_loss: 0.13828754425048828
train_iter_loss: 0.11531905084848404
train_iter_loss: 0.07183226943016052
train_iter_loss: 0.15089204907417297
train_iter_loss: 0.17972639203071594
train_iter_loss: 0.20512829720973969
train_iter_loss: 0.12511169910430908
train_iter_loss: 0.24000978469848633
train_iter_loss: 0.1054343432188034
train_iter_loss: 0.0814034640789032
train_iter_loss: 0.1761404573917389
train_iter_loss: 0.16127151250839233
train_iter_loss: 0.2128504514694214
train_iter_loss: 0.17770294845104218
train loss :0.1608
---------------------
Validation seg loss: 0.21526328805517755 at epoch 896
epoch =    897/  1000, exp = train
train_iter_loss: 0.1289258450269699
train_iter_loss: 0.18687325716018677
train_iter_loss: 0.10834876447916031
train_iter_loss: 0.14967092871665955
train_iter_loss: 0.1617724597454071
train_iter_loss: 0.15082962810993195
train_iter_loss: 0.1501552015542984
train_iter_loss: 0.19680583477020264
train_iter_loss: 0.07405803352594376
train_iter_loss: 0.16386397182941437
train_iter_loss: 0.16970017552375793
train_iter_loss: 0.10828214883804321
train_iter_loss: 0.2878420352935791
train_iter_loss: 0.24582986533641815
train_iter_loss: 0.18872113525867462
train_iter_loss: 0.1058829054236412
train_iter_loss: 0.1738014817237854
train_iter_loss: 0.10549642145633698
train_iter_loss: 0.03970438987016678
train_iter_loss: 0.152577206492424
train_iter_loss: 0.052540503442287445
train_iter_loss: 0.1324070245027542
train_iter_loss: 0.5902197957038879
train_iter_loss: 0.10406377166509628
train_iter_loss: 0.14262078702449799
train_iter_loss: 0.2301017940044403
train_iter_loss: 0.14680346846580505
train_iter_loss: 0.2065267264842987
train_iter_loss: 0.13538749516010284
train_iter_loss: 0.1657920926809311
train_iter_loss: 0.1737053543329239
train_iter_loss: 0.16141726076602936
train_iter_loss: 0.11664435267448425
train_iter_loss: 0.1778949350118637
train_iter_loss: 0.11761797219514847
train_iter_loss: 0.09560054540634155
train_iter_loss: 0.25924453139305115
train_iter_loss: 0.21684055030345917
train_iter_loss: 0.16058234870433807
train_iter_loss: 0.12796688079833984
train_iter_loss: 0.10075545310974121
train_iter_loss: 0.0999593511223793
train_iter_loss: 0.18129561841487885
train_iter_loss: 0.10364091396331787
train_iter_loss: 0.1603429764509201
train_iter_loss: 0.0743478313088417
train_iter_loss: 0.1745588779449463
train_iter_loss: 0.0960453599691391
train_iter_loss: 0.16932980716228485
train_iter_loss: 0.11043251305818558
train_iter_loss: 0.07202693819999695
train_iter_loss: 0.29865556955337524
train_iter_loss: 0.14001506567001343
train_iter_loss: 0.2135189026594162
train_iter_loss: 0.16290757060050964
train_iter_loss: 0.10656069219112396
train_iter_loss: 0.24233625829219818
train_iter_loss: 0.0935736745595932
train_iter_loss: 0.1378859132528305
train_iter_loss: 0.3551185429096222
train_iter_loss: 0.18032419681549072
train_iter_loss: 0.11521638184785843
train_iter_loss: 0.14798252284526825
train_iter_loss: 0.12053290754556656
train_iter_loss: 0.1277134120464325
train_iter_loss: 0.1781315952539444
train_iter_loss: 0.18553905189037323
train_iter_loss: 0.1984979212284088
train_iter_loss: 0.29956313967704773
train_iter_loss: 0.15066690742969513
train_iter_loss: 0.1625593602657318
train_iter_loss: 0.1455000638961792
train_iter_loss: 0.31245189905166626
train_iter_loss: 0.07841774821281433
train_iter_loss: 0.1146240383386612
train_iter_loss: 0.22602440416812897
train_iter_loss: 0.20794999599456787
train_iter_loss: 0.14821399748325348
train_iter_loss: 0.09520278871059418
train_iter_loss: 0.1620018184185028
train_iter_loss: 0.09821536391973495
train_iter_loss: 0.2958645522594452
train_iter_loss: 0.17299386858940125
train_iter_loss: 0.10298432409763336
train_iter_loss: 0.10300390422344208
train_iter_loss: 0.17786185443401337
train_iter_loss: 0.17217621207237244
train_iter_loss: 0.22173352539539337
train_iter_loss: 0.1264856606721878
train_iter_loss: 0.07302643358707428
train_iter_loss: 0.1373780369758606
train_iter_loss: 0.12258706986904144
train_iter_loss: 0.18448801338672638
train_iter_loss: 0.20954260230064392
train_iter_loss: 0.2630734443664551
train_iter_loss: 0.11731833964586258
train_iter_loss: 0.1610708385705948
train_iter_loss: 0.16961199045181274
train_iter_loss: 0.12187756597995758
train_iter_loss: 0.1441727727651596
train loss :0.1635
---------------------
Validation seg loss: 0.2136353579666114 at epoch 897
epoch =    898/  1000, exp = train
train_iter_loss: 0.09667996317148209
train_iter_loss: 0.1833736151456833
train_iter_loss: 0.0669126957654953
train_iter_loss: 0.0791713297367096
train_iter_loss: 0.17541933059692383
train_iter_loss: 0.1685793101787567
train_iter_loss: 0.12503249943256378
train_iter_loss: 0.09047836065292358
train_iter_loss: 0.1284923255443573
train_iter_loss: 0.11573344469070435
train_iter_loss: 0.052808139473199844
train_iter_loss: 0.1457701027393341
train_iter_loss: 0.14902016520500183
train_iter_loss: 0.06537692993879318
train_iter_loss: 0.19334377348423004
train_iter_loss: 0.22332428395748138
train_iter_loss: 0.04685720056295395
train_iter_loss: 0.18015648424625397
train_iter_loss: 0.15547804534435272
train_iter_loss: 0.134153351187706
train_iter_loss: 0.11118441075086594
train_iter_loss: 0.20555594563484192
train_iter_loss: 0.1356508433818817
train_iter_loss: 0.08870583772659302
train_iter_loss: 0.17606040835380554
train_iter_loss: 0.0793820396065712
train_iter_loss: 0.10992258042097092
train_iter_loss: 0.2373061180114746
train_iter_loss: 0.13989217579364777
train_iter_loss: 0.10189994424581528
train_iter_loss: 0.1306668072938919
train_iter_loss: 0.11711886525154114
train_iter_loss: 0.2519947290420532
train_iter_loss: 0.059236008673906326
train_iter_loss: 0.11547112464904785
train_iter_loss: 0.09375794231891632
train_iter_loss: 0.16733363270759583
train_iter_loss: 0.2350088655948639
train_iter_loss: 0.1897190511226654
train_iter_loss: 0.28161248564720154
train_iter_loss: 0.10262956470251083
train_iter_loss: 0.09162414073944092
train_iter_loss: 0.1099424660205841
train_iter_loss: 0.22486712038516998
train_iter_loss: 0.08181468397378922
train_iter_loss: 0.2559954822063446
train_iter_loss: 0.13111837208271027
train_iter_loss: 0.17021381855010986
train_iter_loss: 0.20854666829109192
train_iter_loss: 0.16055232286453247
train_iter_loss: 0.06061374023556709
train_iter_loss: 0.1096448078751564
train_iter_loss: 0.07908282428979874
train_iter_loss: 0.1585363894701004
train_iter_loss: 0.10829653590917587
train_iter_loss: 0.19885680079460144
train_iter_loss: 0.21320495009422302
train_iter_loss: 0.2560572922229767
train_iter_loss: 0.06545063853263855
train_iter_loss: 0.21313773095607758
train_iter_loss: 0.1753907948732376
train_iter_loss: 0.1732722371816635
train_iter_loss: 0.08306306600570679
train_iter_loss: 0.29051563143730164
train_iter_loss: 0.38399437069892883
train_iter_loss: 0.09390110522508621
train_iter_loss: 0.11546147614717484
train_iter_loss: 0.28627872467041016
train_iter_loss: 0.12841425836086273
train_iter_loss: 0.14015378057956696
train_iter_loss: 0.20004957914352417
train_iter_loss: 0.23645640909671783
train_iter_loss: 0.17130699753761292
train_iter_loss: 0.12770619988441467
train_iter_loss: 0.18206550180912018
train_iter_loss: 0.19974306225776672
train_iter_loss: 0.16460949182510376
train_iter_loss: 0.15303650498390198
train_iter_loss: 0.08422277122735977
train_iter_loss: 0.24244293570518494
train_iter_loss: 0.08367908746004105
train_iter_loss: 0.18395733833312988
train_iter_loss: 0.12476468086242676
train_iter_loss: 0.1725059300661087
train_iter_loss: 0.10423710942268372
train_iter_loss: 0.10032698512077332
train_iter_loss: 0.19694426655769348
train_iter_loss: 0.1699654906988144
train_iter_loss: 0.2043173760175705
train_iter_loss: 0.0904146060347557
train_iter_loss: 0.12720373272895813
train_iter_loss: 0.14378799498081207
train_iter_loss: 0.15606917440891266
train_iter_loss: 0.25038665533065796
train_iter_loss: 0.1188449114561081
train_iter_loss: 0.08559245616197586
train_iter_loss: 0.14412710070610046
train_iter_loss: 0.11265377700328827
train_iter_loss: 0.24983716011047363
train_iter_loss: 0.15830513834953308
train loss :0.1538
---------------------
Validation seg loss: 0.21150269822375672 at epoch 898
epoch =    899/  1000, exp = train
train_iter_loss: 0.07878440618515015
train_iter_loss: 0.21561361849308014
train_iter_loss: 0.14096704125404358
train_iter_loss: 0.12165150791406631
train_iter_loss: 0.10525935888290405
train_iter_loss: 0.1335379034280777
train_iter_loss: 0.19020986557006836
train_iter_loss: 0.19088828563690186
train_iter_loss: 0.12264035642147064
train_iter_loss: 0.1791658103466034
train_iter_loss: 0.2016126662492752
train_iter_loss: 0.21486304700374603
train_iter_loss: 0.11089587956666946
train_iter_loss: 0.1236971765756607
train_iter_loss: 0.1787777543067932
train_iter_loss: 0.11669616401195526
train_iter_loss: 0.14263707399368286
train_iter_loss: 0.28030115365982056
train_iter_loss: 0.13750579953193665
train_iter_loss: 0.08708245307207108
train_iter_loss: 0.1187661737203598
train_iter_loss: 0.1289253979921341
train_iter_loss: 0.13971379399299622
train_iter_loss: 0.09869394451379776
train_iter_loss: 0.21727214753627777
train_iter_loss: 0.21408864855766296
train_iter_loss: 0.2894444763660431
train_iter_loss: 0.16486366093158722
train_iter_loss: 0.11570269614458084
train_iter_loss: 0.19822026789188385
train_iter_loss: 0.17187635600566864
train_iter_loss: 0.24871674180030823
train_iter_loss: 0.14092759788036346
train_iter_loss: 0.0698246955871582
train_iter_loss: 0.09273958206176758
train_iter_loss: 0.10353133082389832
train_iter_loss: 0.269955575466156
train_iter_loss: 0.1927596926689148
train_iter_loss: 0.08173118531703949
train_iter_loss: 0.2489887923002243
train_iter_loss: 0.12233583629131317
train_iter_loss: 0.21776390075683594
train_iter_loss: 0.07434609532356262
train_iter_loss: 0.1991444230079651
train_iter_loss: 0.05941564962267876
train_iter_loss: 0.09248291701078415
train_iter_loss: 0.0789162740111351
train_iter_loss: 0.17061853408813477
train_iter_loss: 0.13987012207508087
train_iter_loss: 0.2460143119096756
train_iter_loss: 0.160348579287529
train_iter_loss: 0.16508077085018158
train_iter_loss: 0.07754343003034592
train_iter_loss: 0.11938264966011047
train_iter_loss: 0.161763995885849
train_iter_loss: 0.16103658080101013
train_iter_loss: 0.2741922438144684
train_iter_loss: 0.04916691035032272
train_iter_loss: 0.19126197695732117
train_iter_loss: 0.07111159712076187
train_iter_loss: 0.09990156441926956
train_iter_loss: 0.244269460439682
train_iter_loss: 0.1256842017173767
train_iter_loss: 0.1426190286874771
train_iter_loss: 0.15562739968299866
train_iter_loss: 0.07860085368156433
train_iter_loss: 0.19266390800476074
train_iter_loss: 0.08971013128757477
train_iter_loss: 0.16369447112083435
train_iter_loss: 0.1924618035554886
train_iter_loss: 0.10271809250116348
train_iter_loss: 0.16065795719623566
train_iter_loss: 0.11370966583490372
train_iter_loss: 0.26635900139808655
train_iter_loss: 0.157795712351799
train_iter_loss: 0.17936715483665466
train_iter_loss: 0.24372179806232452
train_iter_loss: 0.08457015454769135
train_iter_loss: 0.17081405222415924
train_iter_loss: 0.09738188236951828
train_iter_loss: 0.2524426579475403
train_iter_loss: 0.15070737898349762
train_iter_loss: 0.10293423384428024
train_iter_loss: 0.11379865556955338
train_iter_loss: 0.13426049053668976
train_iter_loss: 0.13483713567256927
train_iter_loss: 0.07164064794778824
train_iter_loss: 0.13518530130386353
train_iter_loss: 0.05535261705517769
train_iter_loss: 0.1536421775817871
train_iter_loss: 0.23765720427036285
train_iter_loss: 0.09923852235078812
train_iter_loss: 0.16962897777557373
train_iter_loss: 0.18817418813705444
train_iter_loss: 0.11365102231502533
train_iter_loss: 0.08649690449237823
train_iter_loss: 0.12851251661777496
train_iter_loss: 0.1890987753868103
train_iter_loss: 0.057674262672662735
train_iter_loss: 0.3248506486415863
train loss :0.1533
---------------------
Validation seg loss: 0.213333830008951 at epoch 899
epoch =    900/  1000, exp = train
train_iter_loss: 0.15079781413078308
train_iter_loss: 0.11993127316236496
train_iter_loss: 0.12457112222909927
train_iter_loss: 0.09239877015352249
train_iter_loss: 0.12671557068824768
train_iter_loss: 0.12434939295053482
train_iter_loss: 0.14094328880310059
train_iter_loss: 0.07469954341650009
train_iter_loss: 0.15130968391895294
train_iter_loss: 0.14508305490016937
train_iter_loss: 0.12403783202171326
train_iter_loss: 0.07332786917686462
train_iter_loss: 0.09912792593240738
train_iter_loss: 0.13072292506694794
train_iter_loss: 0.17572127282619476
train_iter_loss: 0.19018146395683289
train_iter_loss: 0.16692690551280975
train_iter_loss: 0.07995758950710297
train_iter_loss: 0.12918931245803833
train_iter_loss: 0.07634612172842026
train_iter_loss: 0.03942485526204109
train_iter_loss: 0.20817145705223083
train_iter_loss: 0.2593655586242676
train_iter_loss: 0.08627182990312576
train_iter_loss: 0.1514512598514557
train_iter_loss: 0.1448744386434555
train_iter_loss: 0.10219450294971466
train_iter_loss: 0.12279794365167618
train_iter_loss: 0.1959012746810913
train_iter_loss: 0.09728746116161346
train_iter_loss: 0.19315852224826813
train_iter_loss: 0.11231542378664017
train_iter_loss: 0.17409752309322357
train_iter_loss: 0.07930866628885269
train_iter_loss: 0.21474875509738922
train_iter_loss: 0.16059038043022156
train_iter_loss: 0.10303245484828949
train_iter_loss: 0.09408047050237656
train_iter_loss: 0.2183586061000824
train_iter_loss: 0.25665777921676636
train_iter_loss: 0.1285766065120697
train_iter_loss: 0.29581189155578613
train_iter_loss: 0.14819608628749847
train_iter_loss: 0.3205193281173706
train_iter_loss: 0.21950587630271912
train_iter_loss: 0.2111455500125885
train_iter_loss: 0.11268367618322372
train_iter_loss: 0.1646587997674942
train_iter_loss: 0.1218973845243454
train_iter_loss: 0.07027774304151535
train_iter_loss: 0.08263561874628067
train_iter_loss: 0.25191783905029297
train_iter_loss: 0.07713756710290909
train_iter_loss: 0.27720585465431213
train_iter_loss: 0.2779793441295624
train_iter_loss: 0.07015261799097061
train_iter_loss: 0.1264341175556183
train_iter_loss: 0.13099654018878937
train_iter_loss: 0.1970689594745636
train_iter_loss: 0.12324729561805725
train_iter_loss: 0.22578062117099762
train_iter_loss: 0.10941394418478012
train_iter_loss: 0.08730896562337875
train_iter_loss: 0.12170735746622086
train_iter_loss: 0.18783721327781677
train_iter_loss: 0.09775640070438385
train_iter_loss: 0.03013230487704277
train_iter_loss: 0.25746285915374756
train_iter_loss: 0.05841377377510071
train_iter_loss: 0.157746821641922
train_iter_loss: 0.08030740916728973
train_iter_loss: 0.19910050928592682
train_iter_loss: 0.15041819214820862
train_iter_loss: 0.11375442147254944
train_iter_loss: 0.14536504447460175
train_iter_loss: 0.15007901191711426
train_iter_loss: 0.048104651272296906
train_iter_loss: 0.2170160859823227
train_iter_loss: 0.21154384315013885
train_iter_loss: 0.17817331850528717
train_iter_loss: 0.2316463738679886
train_iter_loss: 0.07821295410394669
train_iter_loss: 0.14808830618858337
train_iter_loss: 0.14130881428718567
train_iter_loss: 0.08049038797616959
train_iter_loss: 0.23948825895786285
train_iter_loss: 0.2312183976173401
train_iter_loss: 0.2134009450674057
train_iter_loss: 0.1449127197265625
train_iter_loss: 0.14489081501960754
train_iter_loss: 0.2293921262025833
train_iter_loss: 0.22750090062618256
train_iter_loss: 0.1452450007200241
train_iter_loss: 0.11945653706789017
train_iter_loss: 0.2926148772239685
train_iter_loss: 0.13096964359283447
train_iter_loss: 0.31185397505760193
train_iter_loss: 0.15963883697986603
train_iter_loss: 0.07544968277215958
train_iter_loss: 0.16364258527755737
train loss :0.1541
---------------------
Validation seg loss: 0.21340303247759365 at epoch 900
epoch =    901/  1000, exp = train
train_iter_loss: 0.10608059167861938
train_iter_loss: 0.157545804977417
train_iter_loss: 0.14108526706695557
train_iter_loss: 0.09141577780246735
train_iter_loss: 0.24332496523857117
train_iter_loss: 0.14429904520511627
train_iter_loss: 0.13989408314228058
train_iter_loss: 0.17132042348384857
train_iter_loss: 0.10736235231161118
train_iter_loss: 0.15883499383926392
train_iter_loss: 0.10877960920333862
train_iter_loss: 0.2213439792394638
train_iter_loss: 0.1765895038843155
train_iter_loss: 0.13293594121932983
train_iter_loss: 0.12835127115249634
train_iter_loss: 0.12874428927898407
train_iter_loss: 0.1571168750524521
train_iter_loss: 0.1792507916688919
train_iter_loss: 0.1421334147453308
train_iter_loss: 0.20388884842395782
train_iter_loss: 0.163117915391922
train_iter_loss: 0.0635078102350235
train_iter_loss: 0.07023167610168457
train_iter_loss: 0.04320915788412094
train_iter_loss: 0.130560502409935
train_iter_loss: 0.2928270995616913
train_iter_loss: 0.21822191774845123
train_iter_loss: 0.1463986039161682
train_iter_loss: 0.16891315579414368
train_iter_loss: 0.19019657373428345
train_iter_loss: 0.11444854736328125
train_iter_loss: 0.17380915582180023
train_iter_loss: 0.04722828045487404
train_iter_loss: 0.2303810566663742
train_iter_loss: 0.14075201749801636
train_iter_loss: 0.1267172247171402
train_iter_loss: 0.17339573800563812
train_iter_loss: 0.3282657861709595
train_iter_loss: 0.1816919893026352
train_iter_loss: 0.15684650838375092
train_iter_loss: 0.09445678442716599
train_iter_loss: 0.09616200625896454
train_iter_loss: 0.1548088639974594
train_iter_loss: 0.14569520950317383
train_iter_loss: 0.11502041667699814
train_iter_loss: 0.21664345264434814
train_iter_loss: 0.1764257401227951
train_iter_loss: 0.14842884242534637
train_iter_loss: 0.11987361311912537
train_iter_loss: 0.1268319934606552
train_iter_loss: 0.31831037998199463
train_iter_loss: 0.13277149200439453
train_iter_loss: 0.2337731122970581
train_iter_loss: 0.09230729937553406
train_iter_loss: 0.16032136976718903
train_iter_loss: 0.15371844172477722
train_iter_loss: 0.18981727957725525
train_iter_loss: 0.1457081139087677
train_iter_loss: 0.20038354396820068
train_iter_loss: 0.16491425037384033
train_iter_loss: 0.04767744243144989
train_iter_loss: 0.056181855499744415
train_iter_loss: 0.20113658905029297
train_iter_loss: 0.17365826666355133
train_iter_loss: 0.0774611234664917
train_iter_loss: 0.21618203818798065
train_iter_loss: 0.19531066715717316
train_iter_loss: 0.12368360906839371
train_iter_loss: 0.1083582192659378
train_iter_loss: 0.13645876944065094
train_iter_loss: 0.08249019086360931
train_iter_loss: 0.1435427963733673
train_iter_loss: 0.24498915672302246
train_iter_loss: 0.19409264624118805
train_iter_loss: 0.12375396490097046
train_iter_loss: 0.31060078740119934
train_iter_loss: 0.12972110509872437
train_iter_loss: 0.17918045818805695
train_iter_loss: 0.1929924488067627
train_iter_loss: 0.21248352527618408
train_iter_loss: 0.14268022775650024
train_iter_loss: 0.1318339705467224
train_iter_loss: 0.05123544856905937
train_iter_loss: 0.18062418699264526
train_iter_loss: 0.1349569410085678
train_iter_loss: 0.11048807948827744
train_iter_loss: 0.233534038066864
train_iter_loss: 0.19877320528030396
train_iter_loss: 0.11278218775987625
train_iter_loss: 0.1368304044008255
train_iter_loss: 0.056463681161403656
train_iter_loss: 0.14719636738300323
train_iter_loss: 0.14062058925628662
train_iter_loss: 0.15480102598667145
train_iter_loss: 0.1703958809375763
train_iter_loss: 0.12352652847766876
train_iter_loss: 0.14415249228477478
train_iter_loss: 0.2383449673652649
train_iter_loss: 0.09100335091352463
train_iter_loss: 0.14570224285125732
train loss :0.1554
---------------------
Validation seg loss: 0.2173466793226324 at epoch 901
epoch =    902/  1000, exp = train
train_iter_loss: 0.10967686027288437
train_iter_loss: 0.06244653835892677
train_iter_loss: 0.09276489168405533
train_iter_loss: 0.14964082837104797
train_iter_loss: 0.044486917555332184
train_iter_loss: 0.138301819562912
train_iter_loss: 0.14385844767093658
train_iter_loss: 0.1275576800107956
train_iter_loss: 0.19201916456222534
train_iter_loss: 0.17445595562458038
train_iter_loss: 0.22629855573177338
train_iter_loss: 0.11001687496900558
train_iter_loss: 0.1007939875125885
train_iter_loss: 0.11605530977249146
train_iter_loss: 0.20487309992313385
train_iter_loss: 0.1257186084985733
train_iter_loss: 0.1533871591091156
train_iter_loss: 0.11042855679988861
train_iter_loss: 0.07084262371063232
train_iter_loss: 0.1756029725074768
train_iter_loss: 0.24001526832580566
train_iter_loss: 0.17018941044807434
train_iter_loss: 0.15956351161003113
train_iter_loss: 0.22335685789585114
train_iter_loss: 0.15123072266578674
train_iter_loss: 0.11094042658805847
train_iter_loss: 0.11651162058115005
train_iter_loss: 0.10483895987272263
train_iter_loss: 0.19767054915428162
train_iter_loss: 0.09786365181207657
train_iter_loss: 0.20555169880390167
train_iter_loss: 0.11089200526475906
train_iter_loss: 0.08325285464525223
train_iter_loss: 0.04265200346708298
train_iter_loss: 0.1493465006351471
train_iter_loss: 0.13616442680358887
train_iter_loss: 0.17540964484214783
train_iter_loss: 0.12312690168619156
train_iter_loss: 0.10422634333372116
train_iter_loss: 0.12896846234798431
train_iter_loss: 0.17154274880886078
train_iter_loss: 0.1989058554172516
train_iter_loss: 0.17349527776241302
train_iter_loss: 0.2557806968688965
train_iter_loss: 0.1423865705728531
train_iter_loss: 0.1381056159734726
train_iter_loss: 0.015976544469594955
train_iter_loss: 0.1816905438899994
train_iter_loss: 0.11159617453813553
train_iter_loss: 0.11051353067159653
train_iter_loss: 0.16929003596305847
train_iter_loss: 0.10780321061611176
train_iter_loss: 0.1103869080543518
train_iter_loss: 0.1718982309103012
train_iter_loss: 0.14153309166431427
train_iter_loss: 0.1946166306734085
train_iter_loss: 0.16568788886070251
train_iter_loss: 0.286673903465271
train_iter_loss: 0.1423671692609787
train_iter_loss: 0.1333983689546585
train_iter_loss: 0.17829157412052155
train_iter_loss: 0.17555220425128937
train_iter_loss: 0.11303900182247162
train_iter_loss: 0.15755653381347656
train_iter_loss: 0.37579113245010376
train_iter_loss: 0.18880176544189453
train_iter_loss: 0.1195296049118042
train_iter_loss: 0.23726002871990204
train_iter_loss: 0.22560058534145355
train_iter_loss: 0.17003656923770905
train_iter_loss: 0.09446536004543304
train_iter_loss: 0.18631821870803833
train_iter_loss: 0.1518625020980835
train_iter_loss: 0.15019291639328003
train_iter_loss: 0.07568346709012985
train_iter_loss: 0.10905558615922928
train_iter_loss: 0.1006801500916481
train_iter_loss: 0.141933411359787
train_iter_loss: 0.09329669177532196
train_iter_loss: 0.06325957924127579
train_iter_loss: 0.2426513135433197
train_iter_loss: 0.19659508764743805
train_iter_loss: 0.12021176517009735
train_iter_loss: 0.12110982090234756
train_iter_loss: 0.10255686938762665
train_iter_loss: 0.1526746302843094
train_iter_loss: 0.1313314288854599
train_iter_loss: 0.1642916351556778
train_iter_loss: 0.1814754158258438
train_iter_loss: 0.2545698881149292
train_iter_loss: 0.2661750018596649
train_iter_loss: 0.06808186322450638
train_iter_loss: 0.14915655553340912
train_iter_loss: 0.16629430651664734
train_iter_loss: 0.21016764640808105
train_iter_loss: 0.19386856257915497
train_iter_loss: 0.1580861657857895
train_iter_loss: 0.14891529083251953
train_iter_loss: 0.11132442206144333
train_iter_loss: 0.25434932112693787
train loss :0.1524
---------------------
Validation seg loss: 0.21917052951834673 at epoch 902
epoch =    903/  1000, exp = train
train_iter_loss: 0.16233013570308685
train_iter_loss: 0.13949595391750336
train_iter_loss: 0.2051689624786377
train_iter_loss: 0.18250539898872375
train_iter_loss: 0.15500584244728088
train_iter_loss: 0.15302084386348724
train_iter_loss: 0.11032567918300629
train_iter_loss: 0.05170387774705887
train_iter_loss: 0.050138648599386215
train_iter_loss: 0.15370585024356842
train_iter_loss: 0.11296722292900085
train_iter_loss: 0.16262349486351013
train_iter_loss: 0.21937546133995056
train_iter_loss: 0.11785512417554855
train_iter_loss: 0.1724027842283249
train_iter_loss: 0.15596461296081543
train_iter_loss: 0.22862356901168823
train_iter_loss: 0.11830944567918777
train_iter_loss: 0.12091080844402313
train_iter_loss: 0.16699644923210144
train_iter_loss: 0.0940435454249382
train_iter_loss: 0.25955185294151306
train_iter_loss: 0.1626008152961731
train_iter_loss: 0.16990847885608673
train_iter_loss: 0.17230020463466644
train_iter_loss: 0.13749508559703827
train_iter_loss: 0.173924058675766
train_iter_loss: 0.18242405354976654
train_iter_loss: 0.13224729895591736
train_iter_loss: 0.11074330657720566
train_iter_loss: 0.23959636688232422
train_iter_loss: 0.1494249552488327
train_iter_loss: 0.2172224074602127
train_iter_loss: 0.21896976232528687
train_iter_loss: 0.15152250230312347
train_iter_loss: 0.12721213698387146
train_iter_loss: 0.1483658105134964
train_iter_loss: 0.13808588683605194
train_iter_loss: 0.2581530213356018
train_iter_loss: 0.18626843392848969
train_iter_loss: 0.16466401517391205
train_iter_loss: 0.18082720041275024
train_iter_loss: 0.1031147837638855
train_iter_loss: 0.22004275023937225
train_iter_loss: 0.15111644566059113
train_iter_loss: 0.21170300245285034
train_iter_loss: 0.2608472406864166
train_iter_loss: 0.13262023031711578
train_iter_loss: 0.061042122542858124
train_iter_loss: 0.1769995540380478
train_iter_loss: 0.17506378889083862
train_iter_loss: 0.12512628734111786
train_iter_loss: 0.2619568407535553
train_iter_loss: 0.09204120934009552
train_iter_loss: 0.1271064281463623
train_iter_loss: 0.08303353935480118
train_iter_loss: 0.08399220556020737
train_iter_loss: 0.12799175083637238
train_iter_loss: 0.14484620094299316
train_iter_loss: 0.2799997925758362
train_iter_loss: 0.10817078500986099
train_iter_loss: 0.16587728261947632
train_iter_loss: 0.24796825647354126
train_iter_loss: 0.09622066468000412
train_iter_loss: 0.0856131762266159
train_iter_loss: 0.1816004365682602
train_iter_loss: 0.15392877161502838
train_iter_loss: 0.23856686055660248
train_iter_loss: 0.3778343200683594
train_iter_loss: 0.23541486263275146
train_iter_loss: 0.14580844342708588
train_iter_loss: 0.10013261437416077
train_iter_loss: 0.21451693773269653
train_iter_loss: 0.15125267207622528
train_iter_loss: 0.2819644510746002
train_iter_loss: 0.140019029378891
train_iter_loss: 0.1457836925983429
train_iter_loss: 0.15016844868659973
train_iter_loss: 0.25292646884918213
train_iter_loss: 0.0940285250544548
train_iter_loss: 0.16094417870044708
train_iter_loss: 0.12833811342716217
train_iter_loss: 0.2158970683813095
train_iter_loss: 0.1292153000831604
train_iter_loss: 0.1378987729549408
train_iter_loss: 0.3848861753940582
train_iter_loss: 0.1578061878681183
train_iter_loss: 0.05852725729346275
train_iter_loss: 0.0632828027009964
train_iter_loss: 0.2940150201320648
train_iter_loss: 0.06640609353780746
train_iter_loss: 0.10614138096570969
train_iter_loss: 0.21743251383304596
train_iter_loss: 0.16522780060768127
train_iter_loss: 0.06928696483373642
train_iter_loss: 0.07623704522848129
train_iter_loss: 0.021504495292901993
train_iter_loss: 0.09359324723482132
train_iter_loss: 0.24078421294689178
train_iter_loss: 0.16582591831684113
train loss :0.1621
---------------------
Validation seg loss: 0.21072702561417278 at epoch 903
epoch =    904/  1000, exp = train
train_iter_loss: 0.17771442234516144
train_iter_loss: 0.0833115428686142
train_iter_loss: 0.16469290852546692
train_iter_loss: 0.13420848548412323
train_iter_loss: 0.18657100200653076
train_iter_loss: 0.13613636791706085
train_iter_loss: 0.13578413426876068
train_iter_loss: 0.2731996476650238
train_iter_loss: 0.3651825487613678
train_iter_loss: 0.1935749500989914
train_iter_loss: 0.20813120901584625
train_iter_loss: 0.100584477186203
train_iter_loss: 0.19007889926433563
train_iter_loss: 0.3020951449871063
train_iter_loss: 0.1418503075838089
train_iter_loss: 0.17738983035087585
train_iter_loss: 0.2002973109483719
train_iter_loss: 0.1828138679265976
train_iter_loss: 0.07779408991336823
train_iter_loss: 0.1000780239701271
train_iter_loss: 0.1506168097257614
train_iter_loss: 0.2594829201698303
train_iter_loss: 0.18444977700710297
train_iter_loss: 0.28427043557167053
train_iter_loss: 0.11925768107175827
train_iter_loss: 0.10550172626972198
train_iter_loss: 0.1341748833656311
train_iter_loss: 0.11233587563037872
train_iter_loss: 0.15484096109867096
train_iter_loss: 0.16450996696949005
train_iter_loss: 0.12155361473560333
train_iter_loss: 0.16786660254001617
train_iter_loss: 0.15933983027935028
train_iter_loss: 0.046608708798885345
train_iter_loss: 0.15279924869537354
train_iter_loss: 0.0880788117647171
train_iter_loss: 0.11492767930030823
train_iter_loss: 0.12813019752502441
train_iter_loss: 0.1817513257265091
train_iter_loss: 0.2584832012653351
train_iter_loss: 0.16542473435401917
train_iter_loss: 0.08606072515249252
train_iter_loss: 0.23944801092147827
train_iter_loss: 0.2070910483598709
train_iter_loss: 0.09467386454343796
train_iter_loss: 0.10779888182878494
train_iter_loss: 0.05353081598877907
train_iter_loss: 0.15066932141780853
train_iter_loss: 0.32086408138275146
train_iter_loss: 0.18031612038612366
train_iter_loss: 0.12155192345380783
train_iter_loss: 0.25360119342803955
train_iter_loss: 0.11470230668783188
train_iter_loss: 0.11755219101905823
train_iter_loss: 0.14428925514221191
train_iter_loss: 0.16464555263519287
train_iter_loss: 0.12779080867767334
train_iter_loss: 0.20366716384887695
train_iter_loss: 0.2194758802652359
train_iter_loss: 0.09718973189592361
train_iter_loss: 0.18653970956802368
train_iter_loss: 0.24107393622398376
train_iter_loss: 0.1287764608860016
train_iter_loss: 0.18603405356407166
train_iter_loss: 0.11525949090719223
train_iter_loss: 0.09587782621383667
train_iter_loss: 0.15893831849098206
train_iter_loss: 0.13005280494689941
train_iter_loss: 0.1998860090970993
train_iter_loss: 0.13817951083183289
train_iter_loss: 0.1607404351234436
train_iter_loss: 0.11626556515693665
train_iter_loss: 0.09309753775596619
train_iter_loss: 0.14757604897022247
train_iter_loss: 0.08138449490070343
train_iter_loss: 0.20932325720787048
train_iter_loss: 0.1585523933172226
train_iter_loss: 0.16272471845149994
train_iter_loss: 0.1371891051530838
train_iter_loss: 0.1352090686559677
train_iter_loss: 0.09816262125968933
train_iter_loss: 0.2873428761959076
train_iter_loss: 0.03718841075897217
train_iter_loss: 0.12646320462226868
train_iter_loss: 0.2725732922554016
train_iter_loss: 0.2889542877674103
train_iter_loss: 0.09561146795749664
train_iter_loss: 0.10437404364347458
train_iter_loss: 0.17604084312915802
train_iter_loss: 0.07484525442123413
train_iter_loss: 0.14027762413024902
train_iter_loss: 0.22558556497097015
train_iter_loss: 0.0896177738904953
train_iter_loss: 0.16287954151630402
train_iter_loss: 0.21959248185157776
train_iter_loss: 0.17109444737434387
train_iter_loss: 0.1438142955303192
train_iter_loss: 0.1573842614889145
train_iter_loss: 0.08253756165504456
train_iter_loss: 0.16631051898002625
train loss :0.1606
---------------------
Validation seg loss: 0.21960302573224566 at epoch 904
epoch =    905/  1000, exp = train

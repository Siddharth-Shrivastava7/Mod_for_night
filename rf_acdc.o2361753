===============================
2361753.pbshpc
vsky040.hpc.iitd.ac.in
===============================
/home/cse/phd/anz208849/Mod_for_night
wait
['so_run_btad.py']
Snapshot stored in: ../scratch/saved_models/acdc/train
                     note : train          
                    model : deeplab        
                    train : 1              
                 multigpu : 0              
                    fixbn : 0              
                 fix_seed : 1              
            learning_rate : 7.5e-05        
                num_steps : 5000           
                   epochs : 1000           
             weight_decay : 0.0005         
                 momentum : 0.9            
                    power : 0.9            
                    round : 6              
               print_freq : 372            
                save_freq : 372            
              tensorboard : 1              
                  neptune : 0              
                   screen : 1              
                      val : 1              
                 val_freq : 5              
                   source : acdc_train_rf  
                   target : acdc_val_rf    
                   worker : 4              
               batch_size : 8              
              num_classes : 2              
                input_src : 720            
                input_tgt : 720            
                 crop_src : 600            
                 crop_tgt : 600            
                   mirror : 1              
                scale_min : 0.5            
                scale_max : 1.5            
                      rec : 0              
              init_weight : ./save/model410_city_deeplabv2.pth
             restore_from : None           
                 snapshot : ../scratch/saved_models/acdc/train
                   result : ./miou_result/ 
                      log : ./log/         
                   plabel : ./plabel       
                       tb : ./log/train    

Mode --> Train
Lets ride...
epoch =      0/  1000, exp = train
train_iter_loss: 0.6866186261177063
train_iter_loss: 0.7129576802253723
train_iter_loss: 0.6958593130111694
train_iter_loss: 0.7691731452941895
train_iter_loss: 0.7274253964424133
train_iter_loss: 0.692768394947052
train_iter_loss: 0.7167750597000122
train_iter_loss: 0.6431381106376648
train_iter_loss: 0.6788495182991028
train_iter_loss: 0.6538536548614502
train_iter_loss: 0.665797770023346
train_iter_loss: 0.6448562741279602
train_iter_loss: 0.6117773652076721
train_iter_loss: 0.646661102771759
train_iter_loss: 0.6745931506156921
train_iter_loss: 0.633945643901825
train_iter_loss: 0.621906042098999
train_iter_loss: 0.6685090065002441
train_iter_loss: 0.6195660829544067
train_iter_loss: 0.6560198068618774
train_iter_loss: 0.6002108454704285
train_iter_loss: 0.6332830190658569
train_iter_loss: 0.6267750859260559
train_iter_loss: 0.601993978023529
train_iter_loss: 0.5915214419364929
train_iter_loss: 0.6134678721427917
train_iter_loss: 0.6043187975883484
train_iter_loss: 0.5860763788223267
train_iter_loss: 0.6328698992729187
train_iter_loss: 0.623383104801178
train_iter_loss: 0.6071054935455322
train_iter_loss: 0.6309151649475098
train_iter_loss: 0.6122684478759766
train_iter_loss: 0.5922159552574158
train_iter_loss: 0.5824229121208191
train_iter_loss: 0.5779149532318115
train_iter_loss: 0.5292296409606934
train_iter_loss: 0.5755919814109802
train_iter_loss: 0.6090846657752991
train_iter_loss: 0.655221164226532
train_iter_loss: 0.5656105875968933
train_iter_loss: 0.5812287926673889
train_iter_loss: 0.61818528175354
train_iter_loss: 0.5512596368789673
train_iter_loss: 0.5575509667396545
train_iter_loss: 0.5928609371185303
train_iter_loss: 0.6259760856628418
train_iter_loss: 0.5910598039627075
train_iter_loss: 0.5556830167770386
train_iter_loss: 0.5534721612930298
train_iter_loss: 0.537544846534729
train_iter_loss: 0.5511656999588013
train_iter_loss: 0.5565071702003479
train_iter_loss: 0.581757664680481
train_iter_loss: 0.5585169196128845
train_iter_loss: 0.5723999738693237
train_iter_loss: 0.5350006222724915
train_iter_loss: 0.6343136429786682
train_iter_loss: 0.5394418835639954
train_iter_loss: 0.5599483847618103
train_iter_loss: 0.5514533519744873
train_iter_loss: 0.5780614018440247
train_iter_loss: 0.5477294325828552
train_iter_loss: 0.540944516658783
train_iter_loss: 0.5401518940925598
train_iter_loss: 0.5723395347595215
train_iter_loss: 0.604047954082489
train_iter_loss: 0.5771644115447998
train_iter_loss: 0.5735436677932739
train_iter_loss: 0.5568242073059082
train_iter_loss: 0.5504049062728882
train_iter_loss: 0.5519669651985168
train_iter_loss: 0.5736421942710876
train_iter_loss: 0.6058791875839233
train_iter_loss: 0.5968643426895142
train_iter_loss: 0.5811249017715454
train_iter_loss: 0.5224661231040955
train_iter_loss: 0.533406138420105
train_iter_loss: 0.5067833065986633
train_iter_loss: 0.5115329623222351
train_iter_loss: 0.5256423354148865
train_iter_loss: 0.5761669278144836
train_iter_loss: 0.5840394496917725
train_iter_loss: 0.5361376404762268
train_iter_loss: 0.5717831254005432
train_iter_loss: 0.5474340915679932
train_iter_loss: 0.5461919903755188
train_iter_loss: 0.5258978605270386
train_iter_loss: 0.6113802790641785
train_iter_loss: 0.5749003291130066
train_iter_loss: 0.4528680741786957
train_iter_loss: 0.5170411467552185
train_iter_loss: 0.5738290548324585
train_iter_loss: 0.5723634362220764
train_iter_loss: 0.5171866416931152
train_iter_loss: 0.5262119174003601
train_iter_loss: 0.5517592430114746
train_iter_loss: 0.544560968875885
train_iter_loss: 0.5780566334724426
train_iter_loss: 0.5022585988044739
train loss :0.5904
---------------------
Validation seg loss: 0.5159074001998272 at epoch 0
********************
best_val_epoch_loss:  0.5159074001998272
MODEL UPDATED
epoch =      1/  1000, exp = train
train_iter_loss: 0.5672168731689453
train_iter_loss: 0.536832332611084
train_iter_loss: 0.5246663093566895
train_iter_loss: 0.5408475995063782
train_iter_loss: 0.543745219707489
train_iter_loss: 0.5580825209617615
train_iter_loss: 0.548299252986908
train_iter_loss: 0.5688511729240417
train_iter_loss: 0.5880905985832214
train_iter_loss: 0.5549428462982178
train_iter_loss: 0.5713886022567749
train_iter_loss: 0.5356557369232178
train_iter_loss: 0.48037904500961304
train_iter_loss: 0.46793341636657715
train_iter_loss: 0.532041072845459
train_iter_loss: 0.5417591333389282
train_iter_loss: 0.47445306181907654
train_iter_loss: 0.5231367349624634
train_iter_loss: 0.5139131546020508
train_iter_loss: 0.5688478946685791
train_iter_loss: 0.5143260359764099
train_iter_loss: 0.5390222668647766
train_iter_loss: 0.5411930084228516
train_iter_loss: 0.5378443598747253
train_iter_loss: 0.4930393099784851
train_iter_loss: 0.552255392074585
train_iter_loss: 0.535950779914856
train_iter_loss: 0.5451603531837463
train_iter_loss: 0.4925293028354645
train_iter_loss: 0.4916605055332184
train_iter_loss: 0.5926088094711304
train_iter_loss: 0.5717779994010925
train_iter_loss: 0.44211283326148987
train_iter_loss: 0.5182989239692688
train_iter_loss: 0.5118891596794128
train_iter_loss: 0.5576058030128479
train_iter_loss: 0.6013317108154297
train_iter_loss: 0.6216844320297241
train_iter_loss: 0.5187214612960815
train_iter_loss: 0.5271987915039062
train_iter_loss: 0.5435541272163391
train_iter_loss: 0.50137859582901
train_iter_loss: 0.5255656838417053
train_iter_loss: 0.48858487606048584
train_iter_loss: 0.5071730613708496
train_iter_loss: 0.5073182582855225
train_iter_loss: 0.5426255464553833
train_iter_loss: 0.5037404894828796
train_iter_loss: 0.5016217231750488
train_iter_loss: 0.5142144560813904
train_iter_loss: 0.5851358771324158
train_iter_loss: 0.5132392644882202
train_iter_loss: 0.5558503866195679
train_iter_loss: 0.5144007205963135
train_iter_loss: 0.5097652077674866
train_iter_loss: 0.5792903304100037
train_iter_loss: 0.5432209372520447
train_iter_loss: 0.5115600824356079
train_iter_loss: 0.4626220464706421
train_iter_loss: 0.46894174814224243
train_iter_loss: 0.5818633437156677
train_iter_loss: 0.4978232681751251
train_iter_loss: 0.5117126703262329
train_iter_loss: 0.6082004308700562
train_iter_loss: 0.5408140420913696
train_iter_loss: 0.4972679018974304
train_iter_loss: 0.5091969966888428
train_iter_loss: 0.4759962260723114
train_iter_loss: 0.5106989145278931
train_iter_loss: 0.5783148407936096
train_iter_loss: 0.48634815216064453
train_iter_loss: 0.5179978609085083
train_iter_loss: 0.4384109079837799
train_iter_loss: 0.5277600884437561
train_iter_loss: 0.5213133096694946
train_iter_loss: 0.5048801302909851
train_iter_loss: 0.4997054636478424
train_iter_loss: 0.507269561290741
train_iter_loss: 0.482191801071167
train_iter_loss: 0.48222410678863525
train_iter_loss: 0.4922086000442505
train_iter_loss: 0.4540770649909973
train_iter_loss: 0.5810412168502808
train_iter_loss: 0.4796641170978546
train_iter_loss: 0.5258251428604126
train_iter_loss: 0.5458511114120483
train_iter_loss: 0.5176340341567993
train_iter_loss: 0.5021020770072937
train_iter_loss: 0.5008454322814941
train_iter_loss: 0.4911733865737915
train_iter_loss: 0.5233879685401917
train_iter_loss: 0.4728272557258606
train_iter_loss: 0.46600306034088135
train_iter_loss: 0.46688398718833923
train_iter_loss: 0.46835505962371826
train_iter_loss: 0.5204941630363464
train_iter_loss: 0.4997321367263794
train_iter_loss: 0.5404295921325684
train_iter_loss: 0.4888869524002075
train_iter_loss: 0.5229095816612244
train loss :0.5276
---------------------
Validation seg loss: 0.5205669490233907 at epoch 1
epoch =      2/  1000, exp = train
train_iter_loss: 0.4884326756000519
train_iter_loss: 0.46267032623291016
train_iter_loss: 0.6134051084518433
train_iter_loss: 0.4940734803676605
train_iter_loss: 0.5363202095031738
train_iter_loss: 0.5163624882698059
train_iter_loss: 0.4996320605278015
train_iter_loss: 0.42798173427581787
train_iter_loss: 0.4875737428665161
train_iter_loss: 0.4502072036266327
train_iter_loss: 0.5357298254966736
train_iter_loss: 0.4501696825027466
train_iter_loss: 0.43177688121795654
train_iter_loss: 0.4540371894836426
train_iter_loss: 0.5336444973945618
train_iter_loss: 0.48273295164108276
train_iter_loss: 0.47728246450424194
train_iter_loss: 0.5153685808181763
train_iter_loss: 0.4963960647583008
train_iter_loss: 0.5117565989494324
train_iter_loss: 0.49067923426628113
train_iter_loss: 0.5262730121612549
train_iter_loss: 0.49233660101890564
train_iter_loss: 0.6354341506958008
train_iter_loss: 0.4421018660068512
train_iter_loss: 0.4589310884475708
train_iter_loss: 0.4401223957538605
train_iter_loss: 0.4500551223754883
train_iter_loss: 0.5096855163574219
train_iter_loss: 0.5274951457977295
train_iter_loss: 0.5202990174293518
train_iter_loss: 0.5547949075698853
train_iter_loss: 0.4602455198764801
train_iter_loss: 0.5049448609352112
train_iter_loss: 0.4765211045742035
train_iter_loss: 0.4710988402366638
train_iter_loss: 0.4983454942703247
train_iter_loss: 0.45125237107276917
train_iter_loss: 0.49634188413619995
train_iter_loss: 0.48360705375671387
train_iter_loss: 0.47001850605010986
train_iter_loss: 0.4671163856983185
train_iter_loss: 0.5584086179733276
train_iter_loss: 0.4282728433609009
train_iter_loss: 0.5674464702606201
train_iter_loss: 0.46962228417396545
train_iter_loss: 0.5009840726852417
train_iter_loss: 0.5176857113838196
train_iter_loss: 0.4235021471977234
train_iter_loss: 0.47205814719200134
train_iter_loss: 0.5186336040496826
train_iter_loss: 0.4501103162765503
train_iter_loss: 0.49613744020462036
train_iter_loss: 0.5233438611030579
train_iter_loss: 0.5010416507720947
train_iter_loss: 0.4997654855251312
train_iter_loss: 0.5229765176773071
train_iter_loss: 0.5499919652938843
train_iter_loss: 0.4680691361427307
train_iter_loss: 0.4461579918861389
train_iter_loss: 0.5443556308746338
train_iter_loss: 0.45284685492515564
train_iter_loss: 0.43546250462532043
train_iter_loss: 0.45201680064201355
train_iter_loss: 0.47778820991516113
train_iter_loss: 0.6144526600837708
train_iter_loss: 0.46928977966308594
train_iter_loss: 0.4486691951751709
train_iter_loss: 0.49809256196022034
train_iter_loss: 0.5287954807281494
train_iter_loss: 0.46895018219947815
train_iter_loss: 0.5103702545166016
train_iter_loss: 0.4661819040775299
train_iter_loss: 0.4329478442668915
train_iter_loss: 0.5020783543586731
train_iter_loss: 0.4719749689102173
train_iter_loss: 0.5395352840423584
train_iter_loss: 0.4867781698703766
train_iter_loss: 0.457547664642334
train_iter_loss: 0.485398530960083
train_iter_loss: 0.5513009428977966
train_iter_loss: 0.4463692903518677
train_iter_loss: 0.5013625621795654
train_iter_loss: 0.4190623462200165
train_iter_loss: 0.5129804611206055
train_iter_loss: 0.488319993019104
train_iter_loss: 0.46421173214912415
train_iter_loss: 0.48594486713409424
train_iter_loss: 0.5605144500732422
train_iter_loss: 0.49592888355255127
train_iter_loss: 0.41858214139938354
train_iter_loss: 0.6208979487419128
train_iter_loss: 0.49857455492019653
train_iter_loss: 0.444866806268692
train_iter_loss: 0.5143539309501648
train_iter_loss: 0.40270107984542847
train_iter_loss: 0.5845471620559692
train_iter_loss: 0.4361881613731384
train_iter_loss: 0.47125592827796936
train_iter_loss: 0.4989512860774994
train loss :0.4970
---------------------
Validation seg loss: 0.49916413229591444 at epoch 2
********************
best_val_epoch_loss:  0.49916413229591444
MODEL UPDATED
epoch =      3/  1000, exp = train
train_iter_loss: 0.4072608947753906
train_iter_loss: 0.4608808755874634
train_iter_loss: 0.5503152012825012
train_iter_loss: 0.4413285255432129
train_iter_loss: 0.47708284854888916
train_iter_loss: 0.505237340927124
train_iter_loss: 0.43821924924850464
train_iter_loss: 0.4420595169067383
train_iter_loss: 0.5413289070129395
train_iter_loss: 0.5694248676300049
train_iter_loss: 0.4897463619709015
train_iter_loss: 0.4199826419353485
train_iter_loss: 0.48880118131637573
train_iter_loss: 0.4608023464679718
train_iter_loss: 0.4605036675930023
train_iter_loss: 0.4172455966472626
train_iter_loss: 0.5225952863693237
train_iter_loss: 0.49193137884140015
train_iter_loss: 0.4550977051258087
train_iter_loss: 0.4617599546909332
train_iter_loss: 0.43813568353652954
train_iter_loss: 0.4966198801994324
train_iter_loss: 0.4660396873950958
train_iter_loss: 0.5265282988548279
train_iter_loss: 0.5328630208969116
train_iter_loss: 0.43071818351745605
train_iter_loss: 0.4996752142906189
train_iter_loss: 0.5326763987541199
train_iter_loss: 0.3900546431541443
train_iter_loss: 0.48478081822395325
train_iter_loss: 0.45715296268463135
train_iter_loss: 0.5444186329841614
train_iter_loss: 0.4451014995574951
train_iter_loss: 0.4240286946296692
train_iter_loss: 0.4871838688850403
train_iter_loss: 0.44422808289527893
train_iter_loss: 0.4719918668270111
train_iter_loss: 0.40145161747932434
train_iter_loss: 0.436299592256546
train_iter_loss: 0.5019083619117737
train_iter_loss: 0.45047900080680847
train_iter_loss: 0.4333672821521759
train_iter_loss: 0.4393736720085144
train_iter_loss: 0.5163500905036926
train_iter_loss: 0.5076605081558228
train_iter_loss: 0.5130417943000793
train_iter_loss: 0.44739216566085815
train_iter_loss: 0.4547293782234192
train_iter_loss: 0.4918985366821289
train_iter_loss: 0.43411603569984436
train_iter_loss: 0.5505986213684082
train_iter_loss: 0.41374242305755615
train_iter_loss: 0.499381422996521
train_iter_loss: 0.4357663094997406
train_iter_loss: 0.4520855247974396
train_iter_loss: 0.43565019965171814
train_iter_loss: 0.4896187484264374
train_iter_loss: 0.4863276481628418
train_iter_loss: 0.4715934097766876
train_iter_loss: 0.46099838614463806
train_iter_loss: 0.43037012219429016
train_iter_loss: 0.4346623718738556
train_iter_loss: 0.3950822055339813
train_iter_loss: 0.4318986237049103
train_iter_loss: 0.5103088617324829
train_iter_loss: 0.5575814843177795
train_iter_loss: 0.476568341255188
train_iter_loss: 0.372896671295166
train_iter_loss: 0.36390769481658936
train_iter_loss: 0.448231041431427
train_iter_loss: 0.47200730443000793
train_iter_loss: 0.5215641260147095
train_iter_loss: 0.46897387504577637
train_iter_loss: 0.4304314851760864
train_iter_loss: 0.4602695405483246
train_iter_loss: 0.4833361804485321
train_iter_loss: 0.4782302975654602
train_iter_loss: 0.40830788016319275
train_iter_loss: 0.46233639121055603
train_iter_loss: 0.42988112568855286
train_iter_loss: 0.5555789470672607
train_iter_loss: 0.445538729429245
train_iter_loss: 0.4465256333351135
train_iter_loss: 0.36961615085601807
train_iter_loss: 0.520971953868866
train_iter_loss: 0.5448555946350098
train_iter_loss: 0.4986993670463562
train_iter_loss: 0.394505113363266
train_iter_loss: 0.42143508791923523
train_iter_loss: 0.4588880240917206
train_iter_loss: 0.5669233798980713
train_iter_loss: 0.4108943045139313
train_iter_loss: 0.41272732615470886
train_iter_loss: 0.440600723028183
train_iter_loss: 0.45378226041793823
train_iter_loss: 0.3865571916103363
train_iter_loss: 0.4165179133415222
train_iter_loss: 0.5057755708694458
train_iter_loss: 0.4583011865615845
train_iter_loss: 0.3732514977455139
train loss :0.4691
---------------------
Validation seg loss: 0.4788174944806774 at epoch 3
********************
best_val_epoch_loss:  0.4788174944806774
MODEL UPDATED
epoch =      4/  1000, exp = train
train_iter_loss: 0.48547759652137756
train_iter_loss: 0.4595256447792053
train_iter_loss: 0.40422725677490234
train_iter_loss: 0.4271366000175476
train_iter_loss: 0.4061306118965149
train_iter_loss: 0.44895491003990173
train_iter_loss: 0.4539881944656372
train_iter_loss: 0.562969446182251
train_iter_loss: 0.45862776041030884
train_iter_loss: 0.41988831758499146
train_iter_loss: 0.42663246393203735
train_iter_loss: 0.4081791043281555
train_iter_loss: 0.5806697010993958
train_iter_loss: 0.4353243410587311
train_iter_loss: 0.3953770697116852
train_iter_loss: 0.5471288561820984
train_iter_loss: 0.5425401926040649
train_iter_loss: 0.40732571482658386
train_iter_loss: 0.45582109689712524
train_iter_loss: 0.4897902309894562
train_iter_loss: 0.5167253017425537
train_iter_loss: 0.5142069458961487
train_iter_loss: 0.37484437227249146
train_iter_loss: 0.47297829389572144
train_iter_loss: 0.44369179010391235
train_iter_loss: 0.4874340891838074
train_iter_loss: 0.4529777765274048
train_iter_loss: 0.4919257164001465
train_iter_loss: 0.4413160979747772
train_iter_loss: 0.44924095273017883
train_iter_loss: 0.534752607345581
train_iter_loss: 0.4453336000442505
train_iter_loss: 0.35077399015426636
train_iter_loss: 0.39679235219955444
train_iter_loss: 0.45083099603652954
train_iter_loss: 0.42675235867500305
train_iter_loss: 0.49707314372062683
train_iter_loss: 0.45782071352005005
train_iter_loss: 0.6606431603431702
train_iter_loss: 0.4029276967048645
train_iter_loss: 0.5036314725875854
train_iter_loss: 0.41952934861183167
train_iter_loss: 0.4505774676799774
train_iter_loss: 0.5335355401039124
train_iter_loss: 0.4377380609512329
train_iter_loss: 0.4433862268924713
train_iter_loss: 0.45034435391426086
train_iter_loss: 0.4082537293434143
train_iter_loss: 0.5309292078018188
train_iter_loss: 0.4961680471897125
train_iter_loss: 0.41850095987319946
train_iter_loss: 0.41969701647758484
train_iter_loss: 0.3823493421077728
train_iter_loss: 0.4252530336380005
train_iter_loss: 0.39795824885368347
train_iter_loss: 0.4233921468257904
train_iter_loss: 0.3444690406322479
train_iter_loss: 0.3726409375667572
train_iter_loss: 0.41225874423980713
train_iter_loss: 0.5153448581695557
train_iter_loss: 0.39656394720077515
train_iter_loss: 0.4669777750968933
train_iter_loss: 0.3890838921070099
train_iter_loss: 0.4267265498638153
train_iter_loss: 0.44188857078552246
train_iter_loss: 0.4468868374824524
train_iter_loss: 0.5561950206756592
train_iter_loss: 0.45821166038513184
train_iter_loss: 0.4527992010116577
train_iter_loss: 0.5706033706665039
train_iter_loss: 0.49871349334716797
train_iter_loss: 0.3747795522212982
train_iter_loss: 0.45068612694740295
train_iter_loss: 0.4060584306716919
train_iter_loss: 0.4634927809238434
train_iter_loss: 0.45614558458328247
train_iter_loss: 0.35165509581565857
train_iter_loss: 0.4270596206188202
train_iter_loss: 0.470601886510849
train_iter_loss: 0.45046868920326233
train_iter_loss: 0.4068726599216461
train_iter_loss: 0.413669228553772
train_iter_loss: 0.4670119881629944
train_iter_loss: 0.43536004424095154
train_iter_loss: 0.4465198814868927
train_iter_loss: 0.4117989242076874
train_iter_loss: 0.4307315945625305
train_iter_loss: 0.484695702791214
train_iter_loss: 0.3937411904335022
train_iter_loss: 0.5274388194084167
train_iter_loss: 0.4037153422832489
train_iter_loss: 0.4676358997821808
train_iter_loss: 0.4334477484226227
train_iter_loss: 0.40879443287849426
train_iter_loss: 0.38634979724884033
train_iter_loss: 0.3222304582595825
train_iter_loss: 0.43435075879096985
train_iter_loss: 0.41427159309387207
train_iter_loss: 0.4105609655380249
train_iter_loss: 0.4360981285572052
train loss :0.4523
---------------------
Validation seg loss: 0.4660040686996478 at epoch 4
********************
best_val_epoch_loss:  0.4660040686996478
MODEL UPDATED
epoch =      5/  1000, exp = train
train_iter_loss: 0.48170605301856995
train_iter_loss: 0.472179651260376
train_iter_loss: 0.5408074855804443
train_iter_loss: 0.39204204082489014
train_iter_loss: 0.44535934925079346
train_iter_loss: 0.4911627471446991
train_iter_loss: 0.4075589179992676
train_iter_loss: 0.4754573106765747
train_iter_loss: 0.4574446380138397
train_iter_loss: 0.4320578873157501
train_iter_loss: 0.4245278239250183
train_iter_loss: 0.39094647765159607
train_iter_loss: 0.447133332490921
train_iter_loss: 0.5206893682479858
train_iter_loss: 0.39749377965927124
train_iter_loss: 0.46175670623779297
train_iter_loss: 0.4522477984428406
train_iter_loss: 0.4699072241783142
train_iter_loss: 0.35371819138526917
train_iter_loss: 0.4329421818256378
train_iter_loss: 0.3734671175479889
train_iter_loss: 0.5011642575263977
train_iter_loss: 0.41484031081199646
train_iter_loss: 0.46264058351516724
train_iter_loss: 0.4765878915786743
train_iter_loss: 0.5125418305397034
train_iter_loss: 0.49019670486450195
train_iter_loss: 0.4101216793060303
train_iter_loss: 0.4490463137626648
train_iter_loss: 0.48054081201553345
train_iter_loss: 0.4922374188899994
train_iter_loss: 0.3591706454753876
train_iter_loss: 0.4362894296646118
train_iter_loss: 0.46243762969970703
train_iter_loss: 0.4343699514865875
train_iter_loss: 0.38388392329216003
train_iter_loss: 0.4226570129394531
train_iter_loss: 0.37900233268737793
train_iter_loss: 0.3710927367210388
train_iter_loss: 0.44192633032798767
train_iter_loss: 0.4511220157146454
train_iter_loss: 0.35464876890182495
train_iter_loss: 0.4457443654537201
train_iter_loss: 0.38283616304397583
train_iter_loss: 0.42415472865104675
train_iter_loss: 0.5122880935668945
train_iter_loss: 0.35718491673469543
train_iter_loss: 0.4846181571483612
train_iter_loss: 0.4447762966156006
train_iter_loss: 0.4533461034297943
train_iter_loss: 0.4074215590953827
train_iter_loss: 0.41130271553993225
train_iter_loss: 0.45097652077674866
train_iter_loss: 0.39997056126594543
train_iter_loss: 0.4212194085121155
train_iter_loss: 0.46274518966674805
train_iter_loss: 0.4026488959789276
train_iter_loss: 0.43261396884918213
train_iter_loss: 0.41834282875061035
train_iter_loss: 0.4361426532268524
train_iter_loss: 0.4004436433315277
train_iter_loss: 0.42558303475379944
train_iter_loss: 0.46036624908447266
train_iter_loss: 0.49932143092155457
train_iter_loss: 0.44201594591140747
train_iter_loss: 0.41880956292152405
train_iter_loss: 0.44486695528030396
train_iter_loss: 0.4374566674232483
train_iter_loss: 0.4988928437232971
train_iter_loss: 0.5084236264228821
train_iter_loss: 0.4289844334125519
train_iter_loss: 0.391214519739151
train_iter_loss: 0.5409550070762634
train_iter_loss: 0.42628875374794006
train_iter_loss: 0.4387786388397217
train_iter_loss: 0.4225422441959381
train_iter_loss: 0.39984163641929626
train_iter_loss: 0.3780103623867035
train_iter_loss: 0.35877031087875366
train_iter_loss: 0.4769546091556549
train_iter_loss: 0.24476777017116547
train_iter_loss: 0.43198516964912415
train_iter_loss: 0.3728732466697693
train_iter_loss: 0.5194253325462341
train_iter_loss: 0.5563457608222961
train_iter_loss: 0.45665764808654785
train_iter_loss: 0.46447649598121643
train_iter_loss: 0.48088598251342773
train_iter_loss: 0.546703577041626
train_iter_loss: 0.47343266010284424
train_iter_loss: 0.4089355170726776
train_iter_loss: 0.4640274941921234
train_iter_loss: 0.3718888461589813
train_iter_loss: 0.40967467427253723
train_iter_loss: 0.45011866092681885
train_iter_loss: 0.5094238519668579
train_iter_loss: 0.5706161260604858
train_iter_loss: 0.389056921005249
train_iter_loss: 0.41141578555107117
train_iter_loss: 0.3753525912761688
train loss :0.4441
---------------------
Validation seg loss: 0.4575743291456744 at epoch 5
********************
best_val_epoch_loss:  0.4575743291456744
MODEL UPDATED
epoch =      6/  1000, exp = train
train_iter_loss: 0.34734538197517395
train_iter_loss: 0.4688400626182556
train_iter_loss: 0.49375268816947937
train_iter_loss: 0.31673529744148254
train_iter_loss: 0.320117324590683
train_iter_loss: 0.36849722266197205
train_iter_loss: 0.4472310543060303
train_iter_loss: 0.35471999645233154
train_iter_loss: 0.3680799901485443
train_iter_loss: 0.515616774559021
train_iter_loss: 0.46352633833885193
train_iter_loss: 0.5189858078956604
train_iter_loss: 0.45013588666915894
train_iter_loss: 0.4379340410232544
train_iter_loss: 0.4021786153316498
train_iter_loss: 0.3797362446784973
train_iter_loss: 0.37404122948646545
train_iter_loss: 0.3841266632080078
train_iter_loss: 0.37974971532821655
train_iter_loss: 0.4901030361652374
train_iter_loss: 0.4267570972442627
train_iter_loss: 0.4605569839477539
train_iter_loss: 0.4907665252685547
train_iter_loss: 0.43310242891311646
train_iter_loss: 0.5469547510147095
train_iter_loss: 0.5383384823799133
train_iter_loss: 0.3565981090068817
train_iter_loss: 0.35359904170036316
train_iter_loss: 0.3841387927532196
train_iter_loss: 0.43286553025245667
train_iter_loss: 0.5001105070114136
train_iter_loss: 0.36188358068466187
train_iter_loss: 0.47977903485298157
train_iter_loss: 0.4109046161174774
train_iter_loss: 0.3838459253311157
train_iter_loss: 0.5163195729255676
train_iter_loss: 0.32692381739616394
train_iter_loss: 0.38247907161712646
train_iter_loss: 0.3603806793689728
train_iter_loss: 0.3670594394207001
train_iter_loss: 0.3525873124599457
train_iter_loss: 0.4601142108440399
train_iter_loss: 0.5057682991027832
train_iter_loss: 0.5163524150848389
train_iter_loss: 0.44513437151908875
train_iter_loss: 0.4401792287826538
train_iter_loss: 0.4614049792289734
train_iter_loss: 0.4737982451915741
train_iter_loss: 0.425978422164917
train_iter_loss: 0.3358355164527893
train_iter_loss: 0.4822981357574463
train_iter_loss: 0.48144686222076416
train_iter_loss: 0.4783172905445099
train_iter_loss: 0.43275898694992065
train_iter_loss: 0.4730900228023529
train_iter_loss: 0.43093186616897583
train_iter_loss: 0.3914625346660614
train_iter_loss: 0.4771154522895813
train_iter_loss: 0.3974519371986389
train_iter_loss: 0.4514596462249756
train_iter_loss: 0.37878647446632385
train_iter_loss: 0.4379618465900421
train_iter_loss: 0.48425689339637756
train_iter_loss: 0.403989315032959
train_iter_loss: 0.4342464208602905
train_iter_loss: 0.47553178668022156
train_iter_loss: 0.4060646891593933
train_iter_loss: 0.44289931654930115
train_iter_loss: 0.3770386576652527
train_iter_loss: 0.5205768346786499
train_iter_loss: 0.4021240472793579
train_iter_loss: 0.47055336833000183
train_iter_loss: 0.3867655098438263
train_iter_loss: 0.2995384633541107
train_iter_loss: 0.36083725094795227
train_iter_loss: 0.4312209188938141
train_iter_loss: 0.43674325942993164
train_iter_loss: 0.35700154304504395
train_iter_loss: 0.4087376296520233
train_iter_loss: 0.4309731423854828
train_iter_loss: 0.4670775532722473
train_iter_loss: 0.45218053460121155
train_iter_loss: 0.4766426384449005
train_iter_loss: 0.3983147144317627
train_iter_loss: 0.39521944522857666
train_iter_loss: 0.4144257605075836
train_iter_loss: 0.4488520622253418
train_iter_loss: 0.48291000723838806
train_iter_loss: 0.4511447548866272
train_iter_loss: 0.4842735528945923
train_iter_loss: 0.3697209060192108
train_iter_loss: 0.3975599408149719
train_iter_loss: 0.30982252955436707
train_iter_loss: 0.3400765657424927
train_iter_loss: 0.4071197509765625
train_iter_loss: 0.41435763239860535
train_iter_loss: 0.3840329349040985
train_iter_loss: 0.444767564535141
train_iter_loss: 0.5187543630599976
train_iter_loss: 0.3769751489162445
train loss :0.4293
---------------------
Validation seg loss: 0.44995575911312735 at epoch 6
********************
best_val_epoch_loss:  0.44995575911312735
MODEL UPDATED
epoch =      7/  1000, exp = train
train_iter_loss: 0.6546297669410706
train_iter_loss: 0.45059648156166077
train_iter_loss: 0.42708784341812134
train_iter_loss: 0.3289531469345093
train_iter_loss: 0.45314961671829224
train_iter_loss: 0.42827004194259644
train_iter_loss: 0.38879838585853577
train_iter_loss: 0.3458036780357361
train_iter_loss: 0.38434892892837524
train_iter_loss: 0.3899756669998169
train_iter_loss: 0.38995856046676636
train_iter_loss: 0.44721007347106934
train_iter_loss: 0.4711558222770691
train_iter_loss: 0.4379810690879822
train_iter_loss: 0.4195654094219208
train_iter_loss: 0.41239526867866516
train_iter_loss: 0.42724594473838806
train_iter_loss: 0.3735104501247406
train_iter_loss: 0.34700965881347656
train_iter_loss: 0.3299243152141571
train_iter_loss: 0.4258274734020233
train_iter_loss: 0.4463648200035095
train_iter_loss: 0.46850720047950745
train_iter_loss: 0.40205737948417664
train_iter_loss: 0.39417120814323425
train_iter_loss: 0.36443009972572327
train_iter_loss: 0.5561679005622864
train_iter_loss: 0.4180888533592224
train_iter_loss: 0.4268994629383087
train_iter_loss: 0.5045820474624634
train_iter_loss: 0.40402135252952576
train_iter_loss: 0.42061641812324524
train_iter_loss: 0.413282573223114
train_iter_loss: 0.35793212056159973
train_iter_loss: 0.4976649284362793
train_iter_loss: 0.3845960199832916
train_iter_loss: 0.4404212236404419
train_iter_loss: 0.3868089020252228
train_iter_loss: 0.3534475266933441
train_iter_loss: 0.4305640757083893
train_iter_loss: 0.4587193429470062
train_iter_loss: 0.4661562442779541
train_iter_loss: 0.4935111403465271
train_iter_loss: 0.5197654366493225
train_iter_loss: 0.40907159447669983
train_iter_loss: 0.46827957034111023
train_iter_loss: 0.35754093527793884
train_iter_loss: 0.3883512020111084
train_iter_loss: 0.4349374771118164
train_iter_loss: 0.4394543468952179
train_iter_loss: 0.3813681900501251
train_iter_loss: 0.43588364124298096
train_iter_loss: 0.4678354859352112
train_iter_loss: 0.5108891725540161
train_iter_loss: 0.5189031958580017
train_iter_loss: 0.41097700595855713
train_iter_loss: 0.34076040983200073
train_iter_loss: 0.38652658462524414
train_iter_loss: 0.3910803496837616
train_iter_loss: 0.41891321539878845
train_iter_loss: 0.3488105833530426
train_iter_loss: 0.35300904512405396
train_iter_loss: 0.4112905263900757
train_iter_loss: 0.35698893666267395
train_iter_loss: 0.5682923197746277
train_iter_loss: 0.42951831221580505
train_iter_loss: 0.3540427088737488
train_iter_loss: 0.4035053253173828
train_iter_loss: 0.4313431978225708
train_iter_loss: 0.36745068430900574
train_iter_loss: 0.4222814440727234
train_iter_loss: 0.4087231159210205
train_iter_loss: 0.36403733491897583
train_iter_loss: 0.3795011341571808
train_iter_loss: 0.34527984261512756
train_iter_loss: 0.4236196279525757
train_iter_loss: 0.44223731756210327
train_iter_loss: 0.3847750425338745
train_iter_loss: 0.39572805166244507
train_iter_loss: 0.4736804664134979
train_iter_loss: 0.4171907305717468
train_iter_loss: 0.6335154175758362
train_iter_loss: 0.45714160799980164
train_iter_loss: 0.32460424304008484
train_iter_loss: 0.44185349345207214
train_iter_loss: 0.40414097905158997
train_iter_loss: 0.38678187131881714
train_iter_loss: 0.4315488338470459
train_iter_loss: 0.4276368021965027
train_iter_loss: 0.44764116406440735
train_iter_loss: 0.36746951937675476
train_iter_loss: 0.35435840487480164
train_iter_loss: 0.4321705996990204
train_iter_loss: 0.3450968265533447
train_iter_loss: 0.35499662160873413
train_iter_loss: 0.40256452560424805
train_iter_loss: 0.5345715880393982
train_iter_loss: 0.4296358525753021
train_iter_loss: 0.42099007964134216
train_iter_loss: 0.3913559019565582
train loss :0.4240
---------------------
Validation seg loss: 0.45248535352776637 at epoch 7
epoch =      8/  1000, exp = train
train_iter_loss: 0.4030330181121826
train_iter_loss: 0.31647005677223206
train_iter_loss: 0.2878105044364929
train_iter_loss: 0.46897226572036743
train_iter_loss: 0.4715772867202759
train_iter_loss: 0.3693870007991791
train_iter_loss: 0.4481976330280304
train_iter_loss: 0.48179906606674194
train_iter_loss: 0.4342080354690552
train_iter_loss: 0.32028624415397644
train_iter_loss: 0.339162677526474
train_iter_loss: 0.4859910309314728
train_iter_loss: 0.49684232473373413
train_iter_loss: 0.33486396074295044
train_iter_loss: 0.34896284341812134
train_iter_loss: 0.5049930214881897
train_iter_loss: 0.2987658977508545
train_iter_loss: 0.36249321699142456
train_iter_loss: 0.4166051149368286
train_iter_loss: 0.4471494257450104
train_iter_loss: 0.5438711047172546
train_iter_loss: 0.4047866463661194
train_iter_loss: 0.4688107669353485
train_iter_loss: 0.4261942505836487
train_iter_loss: 0.3456100523471832
train_iter_loss: 0.3547114133834839
train_iter_loss: 0.4528656005859375
train_iter_loss: 0.39994484186172485
train_iter_loss: 0.3678065240383148
train_iter_loss: 0.3681609630584717
train_iter_loss: 0.32230859994888306
train_iter_loss: 0.4677940309047699
train_iter_loss: 0.42004987597465515
train_iter_loss: 0.3687025010585785
train_iter_loss: 0.37014102935791016
train_iter_loss: 0.3508281409740448
train_iter_loss: 0.44811907410621643
train_iter_loss: 0.42302945256233215
train_iter_loss: 0.4224817454814911
train_iter_loss: 0.29415225982666016
train_iter_loss: 0.3814317286014557
train_iter_loss: 0.37180250883102417
train_iter_loss: 0.42287135124206543
train_iter_loss: 0.3144285976886749
train_iter_loss: 0.5451575517654419
train_iter_loss: 0.3378625214099884
train_iter_loss: 0.3521786332130432
train_iter_loss: 0.4231702983379364
train_iter_loss: 0.38093146681785583
train_iter_loss: 0.35237574577331543
train_iter_loss: 0.3773213326931
train_iter_loss: 0.3813210129737854
train_iter_loss: 0.40920546650886536
train_iter_loss: 0.4162770211696625
train_iter_loss: 0.4386928081512451
train_iter_loss: 0.428333044052124
train_iter_loss: 0.4902815818786621
train_iter_loss: 0.45862507820129395
train_iter_loss: 0.4500540494918823
train_iter_loss: 0.36166471242904663
train_iter_loss: 0.3843466639518738
train_iter_loss: 0.4766678214073181
train_iter_loss: 0.3590112626552582
train_iter_loss: 0.4827498495578766
train_iter_loss: 0.4944096803665161
train_iter_loss: 0.39933234453201294
train_iter_loss: 0.4938757121562958
train_iter_loss: 0.3767731785774231
train_iter_loss: 0.3153918981552124
train_iter_loss: 0.3178322911262512
train_iter_loss: 0.3677483797073364
train_iter_loss: 0.4328877925872803
train_iter_loss: 0.4044235050678253
train_iter_loss: 0.42193302512168884
train_iter_loss: 0.3784576654434204
train_iter_loss: 0.457124263048172
train_iter_loss: 0.3598878085613251
train_iter_loss: 0.37352344393730164
train_iter_loss: 0.4671754240989685
train_iter_loss: 0.39238080382347107
train_iter_loss: 0.5112867951393127
train_iter_loss: 0.33972716331481934
train_iter_loss: 0.4286594092845917
train_iter_loss: 0.46643444895744324
train_iter_loss: 0.4539690613746643
train_iter_loss: 0.4489613175392151
train_iter_loss: 0.34530070424079895
train_iter_loss: 0.3911261260509491
train_iter_loss: 0.3474276065826416
train_iter_loss: 0.42436474561691284
train_iter_loss: 0.35885679721832275
train_iter_loss: 0.4019413888454437
train_iter_loss: 0.3835837244987488
train_iter_loss: 0.43383726477622986
train_iter_loss: 0.36800703406333923
train_iter_loss: 0.3494042754173279
train_iter_loss: 0.3191927373409271
train_iter_loss: 0.4044559895992279
train_iter_loss: 0.44417673349380493
train_iter_loss: 0.43212321400642395
train loss :0.4078
---------------------
Validation seg loss: 0.4419656479555481 at epoch 8
********************
best_val_epoch_loss:  0.4419656479555481
MODEL UPDATED
epoch =      9/  1000, exp = train
train_iter_loss: 0.3953274190425873
train_iter_loss: 0.295997679233551
train_iter_loss: 0.4483133554458618
train_iter_loss: 0.38482075929641724
train_iter_loss: 0.39099597930908203
train_iter_loss: 0.4304400682449341
train_iter_loss: 0.48177990317344666
train_iter_loss: 0.38106366991996765
train_iter_loss: 0.40743139386177063
train_iter_loss: 0.3866673707962036
train_iter_loss: 0.32811808586120605
train_iter_loss: 0.3861840069293976
train_iter_loss: 0.34557217359542847
train_iter_loss: 0.377392053604126
train_iter_loss: 0.4080028831958771
train_iter_loss: 0.3806072771549225
train_iter_loss: 0.3551552891731262
train_iter_loss: 0.4186004102230072
train_iter_loss: 0.39301133155822754
train_iter_loss: 0.34935081005096436
train_iter_loss: 0.43823105096817017
train_iter_loss: 0.3652375340461731
train_iter_loss: 0.3881387412548065
train_iter_loss: 0.42141449451446533
train_iter_loss: 0.4940449297428131
train_iter_loss: 0.6100122332572937
train_iter_loss: 0.32061100006103516
train_iter_loss: 0.28242823481559753
train_iter_loss: 0.34694087505340576
train_iter_loss: 0.2771110534667969
train_iter_loss: 0.37424159049987793
train_iter_loss: 0.32656535506248474
train_iter_loss: 0.42654716968536377
train_iter_loss: 0.7477010488510132
train_iter_loss: 0.31299081444740295
train_iter_loss: 0.33560848236083984
train_iter_loss: 0.4324784576892853
train_iter_loss: 0.39472612738609314
train_iter_loss: 0.3974551558494568
train_iter_loss: 0.43856680393218994
train_iter_loss: 0.426882803440094
train_iter_loss: 0.39910876750946045
train_iter_loss: 0.3651352822780609
train_iter_loss: 0.3896434009075165
train_iter_loss: 0.42095792293548584
train_iter_loss: 0.28607091307640076
train_iter_loss: 0.3237452208995819
train_iter_loss: 0.3608126938343048
train_iter_loss: 0.4892408847808838
train_iter_loss: 0.4002956748008728
train_iter_loss: 0.40925630927085876
train_iter_loss: 0.31419938802719116
train_iter_loss: 0.45607268810272217
train_iter_loss: 0.38064461946487427
train_iter_loss: 0.363788902759552
train_iter_loss: 0.38182035088539124
train_iter_loss: 0.49790358543395996
train_iter_loss: 0.35895147919654846
train_iter_loss: 0.423523873090744
train_iter_loss: 0.3783723711967468
train_iter_loss: 0.4433518946170807
train_iter_loss: 0.40652117133140564
train_iter_loss: 0.43821126222610474
train_iter_loss: 0.4899466335773468
train_iter_loss: 0.4104054570198059
train_iter_loss: 0.46539339423179626
train_iter_loss: 0.43774011731147766
train_iter_loss: 0.3902631402015686
train_iter_loss: 0.31337636709213257
train_iter_loss: 0.3693123459815979
train_iter_loss: 0.380243182182312
train_iter_loss: 0.4373781979084015
train_iter_loss: 0.48945802450180054
train_iter_loss: 0.4654538035392761
train_iter_loss: 0.38863661885261536
train_iter_loss: 0.43166065216064453
train_iter_loss: 0.3506632149219513
train_iter_loss: 0.3975472152233124
train_iter_loss: 0.41442057490348816
train_iter_loss: 0.46396204829216003
train_iter_loss: 0.3767280876636505
train_iter_loss: 0.39348676800727844
train_iter_loss: 0.3796897232532501
train_iter_loss: 0.35449740290641785
train_iter_loss: 0.4498109817504883
train_iter_loss: 0.38860267400741577
train_iter_loss: 0.3618610203266144
train_iter_loss: 0.430315762758255
train_iter_loss: 0.35786518454551697
train_iter_loss: 0.4216868579387665
train_iter_loss: 0.2685399353504181
train_iter_loss: 0.3690999150276184
train_iter_loss: 0.43716850876808167
train_iter_loss: 0.44350171089172363
train_iter_loss: 0.3693627417087555
train_iter_loss: 0.42529991269111633
train_iter_loss: 0.4558434784412384
train_iter_loss: 0.3665623962879181
train_iter_loss: 0.30950331687927246
train_iter_loss: 0.5189781188964844
train loss :0.4037
---------------------
Validation seg loss: 0.436676201463308 at epoch 9
********************
best_val_epoch_loss:  0.436676201463308
MODEL UPDATED
epoch =     10/  1000, exp = train
train_iter_loss: 0.42886677384376526
train_iter_loss: 0.38413238525390625
train_iter_loss: 0.48901113867759705
train_iter_loss: 0.39053863286972046
train_iter_loss: 0.39970022439956665
train_iter_loss: 0.35042691230773926
train_iter_loss: 0.47282037138938904
train_iter_loss: 0.33758682012557983
train_iter_loss: 0.38264328241348267
train_iter_loss: 0.33689597249031067
train_iter_loss: 0.28542110323905945
train_iter_loss: 0.37768611311912537
train_iter_loss: 0.31506237387657166
train_iter_loss: 0.32764679193496704
train_iter_loss: 0.3985307812690735
train_iter_loss: 0.37216827273368835
train_iter_loss: 0.5390089750289917
train_iter_loss: 0.3735086917877197
train_iter_loss: 0.4559495747089386
train_iter_loss: 0.41734257340431213
train_iter_loss: 0.33012622594833374
train_iter_loss: 0.42175063490867615
train_iter_loss: 0.4647233784198761
train_iter_loss: 0.3794134259223938
train_iter_loss: 0.4331427812576294
train_iter_loss: 0.5878370404243469
train_iter_loss: 0.5105273723602295
train_iter_loss: 0.2976672947406769
train_iter_loss: 0.38232922554016113
train_iter_loss: 0.3785156011581421
train_iter_loss: 0.47899505496025085
train_iter_loss: 0.3666708171367645
train_iter_loss: 0.35647788643836975
train_iter_loss: 0.34751126170158386
train_iter_loss: 0.33517956733703613
train_iter_loss: 0.507314145565033
train_iter_loss: 0.505688488483429
train_iter_loss: 0.3496377766132355
train_iter_loss: 0.3658703863620758
train_iter_loss: 0.41330772638320923
train_iter_loss: 0.3701670467853546
train_iter_loss: 0.48110106587409973
train_iter_loss: 0.48578011989593506
train_iter_loss: 0.4607628881931305
train_iter_loss: 0.4481121599674225
train_iter_loss: 0.45256340503692627
train_iter_loss: 0.3704010248184204
train_iter_loss: 0.4237670302391052
train_iter_loss: 0.3682628571987152
train_iter_loss: 0.4319816529750824
train_iter_loss: 0.3720032870769501
train_iter_loss: 0.38857942819595337
train_iter_loss: 0.38682177662849426
train_iter_loss: 0.27419513463974
train_iter_loss: 0.3314429819583893
train_iter_loss: 0.4486726224422455
train_iter_loss: 0.3856850564479828
train_iter_loss: 0.5213091373443604
train_iter_loss: 0.4552701413631439
train_iter_loss: 0.4120117425918579
train_iter_loss: 0.40264827013015747
train_iter_loss: 0.45135125517845154
train_iter_loss: 0.42539072036743164
train_iter_loss: 0.43852558732032776
train_iter_loss: 0.3608523905277252
train_iter_loss: 0.35004091262817383
train_iter_loss: 0.3549211323261261
train_iter_loss: 0.2832649350166321
train_iter_loss: 0.48716551065444946
train_iter_loss: 0.40440958738327026
train_iter_loss: 0.385961651802063
train_iter_loss: 0.4213239252567291
train_iter_loss: 0.36953967809677124
train_iter_loss: 0.4338025748729706
train_iter_loss: 0.3456604778766632
train_iter_loss: 0.3369322717189789
train_iter_loss: 0.31559810042381287
train_iter_loss: 0.3824459910392761
train_iter_loss: 0.39722925424575806
train_iter_loss: 0.2385939508676529
train_iter_loss: 0.30116006731987
train_iter_loss: 0.5151903033256531
train_iter_loss: 0.38772720098495483
train_iter_loss: 0.3741148114204407
train_iter_loss: 0.3941047191619873
train_iter_loss: 0.41286447644233704
train_iter_loss: 0.41996216773986816
train_iter_loss: 0.3673405647277832
train_iter_loss: 0.39313507080078125
train_iter_loss: 0.3262097239494324
train_iter_loss: 0.3377922773361206
train_iter_loss: 0.2917433977127075
train_iter_loss: 0.373819500207901
train_iter_loss: 0.5387494564056396
train_iter_loss: 0.3971485197544098
train_iter_loss: 0.4654236435890198
train_iter_loss: 0.3008977472782135
train_iter_loss: 0.36676016449928284
train_iter_loss: 0.4715116024017334
train_iter_loss: 0.35095933079719543
train loss :0.4009
---------------------
Validation seg loss: 0.4351373683872088 at epoch 10
********************
best_val_epoch_loss:  0.4351373683872088
MODEL UPDATED
epoch =     11/  1000, exp = train
train_iter_loss: 0.4583471715450287
train_iter_loss: 0.38043391704559326
train_iter_loss: 0.39742815494537354
train_iter_loss: 0.32965266704559326
train_iter_loss: 0.4730323553085327
train_iter_loss: 0.4280950427055359
train_iter_loss: 0.3562389314174652
train_iter_loss: 0.3778857886791229
train_iter_loss: 0.4572587013244629
train_iter_loss: 0.36044713854789734
train_iter_loss: 0.5499446392059326
train_iter_loss: 0.32582005858421326
train_iter_loss: 0.4082777500152588
train_iter_loss: 0.42268386483192444
train_iter_loss: 0.2588823735713959
train_iter_loss: 0.36091819405555725
train_iter_loss: 0.4590185582637787
train_iter_loss: 0.3195071816444397
train_iter_loss: 0.4003649950027466
train_iter_loss: 0.3320247232913971
train_iter_loss: 0.32875508069992065
train_iter_loss: 0.414948970079422
train_iter_loss: 0.39082902669906616
train_iter_loss: 0.429470419883728
train_iter_loss: 0.44796425104141235
train_iter_loss: 0.3611699640750885
train_iter_loss: 0.38504770398139954
train_iter_loss: 0.5507822632789612
train_iter_loss: 0.3677327334880829
train_iter_loss: 0.45756614208221436
train_iter_loss: 0.4032461941242218
train_iter_loss: 0.36101865768432617
train_iter_loss: 0.5177154541015625
train_iter_loss: 0.36001577973365784
train_iter_loss: 0.42577582597732544
train_iter_loss: 0.31745782494544983
train_iter_loss: 0.3718559443950653
train_iter_loss: 0.44100919365882874
train_iter_loss: 0.38210436701774597
train_iter_loss: 0.3904964327812195
train_iter_loss: 0.412828654050827
train_iter_loss: 0.370790958404541
train_iter_loss: 0.34537917375564575
train_iter_loss: 0.46608591079711914
train_iter_loss: 0.4635314643383026
train_iter_loss: 0.4527733027935028
train_iter_loss: 0.2903158366680145
train_iter_loss: 0.31329113245010376
train_iter_loss: 0.31076887249946594
train_iter_loss: 0.4576456844806671
train_iter_loss: 0.4993588626384735
train_iter_loss: 0.33440595865249634
train_iter_loss: 0.36587563157081604
train_iter_loss: 0.3957768976688385
train_iter_loss: 0.42010584473609924
train_iter_loss: 0.38944268226623535
train_iter_loss: 0.3911844789981842
train_iter_loss: 0.42210766673088074
train_iter_loss: 0.44034722447395325
train_iter_loss: 0.3792840838432312
train_iter_loss: 0.3715062439441681
train_iter_loss: 0.29430440068244934
train_iter_loss: 0.3555639684200287
train_iter_loss: 0.3407907485961914
train_iter_loss: 0.3580819368362427
train_iter_loss: 0.2877747118473053
train_iter_loss: 0.42850223183631897
train_iter_loss: 0.3603925108909607
train_iter_loss: 0.36975184082984924
train_iter_loss: 0.3035842478275299
train_iter_loss: 0.3335321545600891
train_iter_loss: 0.3348537087440491
train_iter_loss: 0.4240155518054962
train_iter_loss: 0.47164177894592285
train_iter_loss: 0.3606562912464142
train_iter_loss: 0.3648981750011444
train_iter_loss: 0.4257456958293915
train_iter_loss: 0.37995079159736633
train_iter_loss: 0.3831925392150879
train_iter_loss: 0.31492331624031067
train_iter_loss: 0.3947064280509949
train_iter_loss: 0.4120960235595703
train_iter_loss: 0.3925422430038452
train_iter_loss: 0.46140149235725403
train_iter_loss: 0.3151939809322357
train_iter_loss: 0.30214443802833557
train_iter_loss: 0.4190894067287445
train_iter_loss: 0.340396910905838
train_iter_loss: 0.3829405605792999
train_iter_loss: 0.454730361700058
train_iter_loss: 0.37484022974967957
train_iter_loss: 0.32814717292785645
train_iter_loss: 0.28620025515556335
train_iter_loss: 0.35774174332618713
train_iter_loss: 0.41759493947029114
train_iter_loss: 0.34639087319374084
train_iter_loss: 0.3704269230365753
train_iter_loss: 0.4421822130680084
train_iter_loss: 0.41461336612701416
train_iter_loss: 0.3899066746234894
train loss :0.3915
---------------------
Validation seg loss: 0.4416284199874356 at epoch 11
epoch =     12/  1000, exp = train
train_iter_loss: 0.42783135175704956
train_iter_loss: 0.4514322280883789
train_iter_loss: 0.3579707145690918
train_iter_loss: 0.49950748682022095
train_iter_loss: 0.3222147524356842
train_iter_loss: 0.5267988443374634
train_iter_loss: 0.47414055466651917
train_iter_loss: 0.42855843901634216
train_iter_loss: 0.41309064626693726
train_iter_loss: 0.3801642060279846
train_iter_loss: 0.3342306911945343
train_iter_loss: 0.2841133177280426
train_iter_loss: 0.29068946838378906
train_iter_loss: 0.420028954744339
train_iter_loss: 0.38791313767433167
train_iter_loss: 0.5178607702255249
train_iter_loss: 0.29323381185531616
train_iter_loss: 0.31514373421669006
train_iter_loss: 0.3129102289676666
train_iter_loss: 0.3877997398376465
train_iter_loss: 0.37314239144325256
train_iter_loss: 0.36641496419906616
train_iter_loss: 0.35694095492362976
train_iter_loss: 0.42033249139785767
train_iter_loss: 0.3786485195159912
train_iter_loss: 0.5345560908317566
train_iter_loss: 0.4278774559497833
train_iter_loss: 0.4152199625968933
train_iter_loss: 0.41084402799606323
train_iter_loss: 0.28647753596305847
train_iter_loss: 0.36181628704071045
train_iter_loss: 0.3111007809638977
train_iter_loss: 0.31914862990379333
train_iter_loss: 0.322767049074173
train_iter_loss: 0.38853690028190613
train_iter_loss: 0.289257287979126
train_iter_loss: 0.3209996521472931
train_iter_loss: 0.4480306804180145
train_iter_loss: 0.29457342624664307
train_iter_loss: 0.43891969323158264
train_iter_loss: 0.41992804408073425
train_iter_loss: 0.462890625
train_iter_loss: 0.39251789450645447
train_iter_loss: 0.5204386115074158
train_iter_loss: 0.4013593792915344
train_iter_loss: 0.4502467215061188
train_iter_loss: 0.4280174970626831
train_iter_loss: 0.3545863628387451
train_iter_loss: 0.4427526295185089
train_iter_loss: 0.42399662733078003
train_iter_loss: 0.38815030455589294
train_iter_loss: 0.3314191997051239
train_iter_loss: 0.3936167359352112
train_iter_loss: 0.5027021169662476
train_iter_loss: 0.47321024537086487
train_iter_loss: 0.5157246589660645
train_iter_loss: 0.26222220063209534
train_iter_loss: 0.40991872549057007
train_iter_loss: 0.4395487308502197
train_iter_loss: 0.4373786151409149
train_iter_loss: 0.43408361077308655
train_iter_loss: 0.41161081194877625
train_iter_loss: 0.345676064491272
train_iter_loss: 0.40428122878074646
train_iter_loss: 0.3620105981826782
train_iter_loss: 0.48268628120422363
train_iter_loss: 0.30829954147338867
train_iter_loss: 0.3433750867843628
train_iter_loss: 0.42334237694740295
train_iter_loss: 0.4250306785106659
train_iter_loss: 0.3013540804386139
train_iter_loss: 0.28402337431907654
train_iter_loss: 0.44103899598121643
train_iter_loss: 0.3482199013233185
train_iter_loss: 0.33870819211006165
train_iter_loss: 0.4385683536529541
train_iter_loss: 0.4562183916568756
train_iter_loss: 0.5174370408058167
train_iter_loss: 0.3430720269680023
train_iter_loss: 0.512917697429657
train_iter_loss: 0.5278737545013428
train_iter_loss: 0.33716434240341187
train_iter_loss: 0.5653496980667114
train_iter_loss: 0.43987753987312317
train_iter_loss: 0.46206802129745483
train_iter_loss: 0.35001009702682495
train_iter_loss: 0.35659945011138916
train_iter_loss: 0.48973819613456726
train_iter_loss: 0.3213440179824829
train_iter_loss: 0.35412630438804626
train_iter_loss: 0.3157670795917511
train_iter_loss: 0.47384747862815857
train_iter_loss: 0.21071945130825043
train_iter_loss: 0.42656421661376953
train_iter_loss: 0.3703850209712982
train_iter_loss: 0.37015801668167114
train_iter_loss: 0.32002902030944824
train_iter_loss: 0.4089309871196747
train_iter_loss: 0.3251359462738037
train_iter_loss: 0.37887847423553467
train loss :0.3986
---------------------
Validation seg loss: 0.43435510262003485 at epoch 12
********************
best_val_epoch_loss:  0.43435510262003485
MODEL UPDATED
epoch =     13/  1000, exp = train
train_iter_loss: 0.2819945812225342
train_iter_loss: 0.39818528294563293
train_iter_loss: 0.3407551050186157
train_iter_loss: 0.3946759104728699
train_iter_loss: 0.39688047766685486
train_iter_loss: 0.3536689579486847
train_iter_loss: 0.2589055895805359
train_iter_loss: 0.2987249493598938
train_iter_loss: 0.4515322744846344
train_iter_loss: 0.2986653745174408
train_iter_loss: 0.4455091953277588
train_iter_loss: 0.487965852022171
train_iter_loss: 0.37784841656684875
train_iter_loss: 0.43755248188972473
train_iter_loss: 0.3234238624572754
train_iter_loss: 0.3370724618434906
train_iter_loss: 0.34338587522506714
train_iter_loss: 0.3804861009120941
train_iter_loss: 0.39530396461486816
train_iter_loss: 0.3681885004043579
train_iter_loss: 0.40706729888916016
train_iter_loss: 0.38362905383110046
train_iter_loss: 0.42781877517700195
train_iter_loss: 0.39570435881614685
train_iter_loss: 0.36767616868019104
train_iter_loss: 0.48261839151382446
train_iter_loss: 0.33382847905158997
train_iter_loss: 0.3831653892993927
train_iter_loss: 0.35991528630256653
train_iter_loss: 0.35839909315109253
train_iter_loss: 0.46663931012153625
train_iter_loss: 0.384750634431839
train_iter_loss: 0.30087217688560486
train_iter_loss: 0.30935049057006836
train_iter_loss: 0.39962807297706604
train_iter_loss: 0.37128183245658875
train_iter_loss: 0.26866084337234497
train_iter_loss: 0.36346912384033203
train_iter_loss: 0.41999351978302
train_iter_loss: 0.3862440586090088
train_iter_loss: 0.45403796434402466
train_iter_loss: 0.3777427673339844
train_iter_loss: 0.3664659261703491
train_iter_loss: 0.312167227268219
train_iter_loss: 0.34592416882514954
train_iter_loss: 0.42466843128204346
train_iter_loss: 0.39628827571868896
train_iter_loss: 0.5141043663024902
train_iter_loss: 0.3222445547580719
train_iter_loss: 0.39336392283439636
train_iter_loss: 0.47592177987098694
train_iter_loss: 0.3951590955257416
train_iter_loss: 0.39921167492866516
train_iter_loss: 0.41869330406188965
train_iter_loss: 0.39612051844596863
train_iter_loss: 0.4044824540615082
train_iter_loss: 0.47668787837028503
train_iter_loss: 0.3959067463874817
train_iter_loss: 0.33766913414001465
train_iter_loss: 0.45457807183265686
train_iter_loss: 0.3376016914844513
train_iter_loss: 0.368333101272583
train_iter_loss: 0.3507309854030609
train_iter_loss: 0.36652445793151855
train_iter_loss: 0.4261775016784668
train_iter_loss: 0.3550586700439453
train_iter_loss: 0.41036948561668396
train_iter_loss: 0.38298866152763367
train_iter_loss: 0.5482412576675415
train_iter_loss: 0.42815181612968445
train_iter_loss: 0.27214303612709045
train_iter_loss: 0.42380091547966003
train_iter_loss: 0.3570925295352936
train_iter_loss: 0.3768317699432373
train_iter_loss: 0.34857550263404846
train_iter_loss: 0.36575236916542053
train_iter_loss: 0.3537088632583618
train_iter_loss: 0.406069278717041
train_iter_loss: 0.44481632113456726
train_iter_loss: 0.29057931900024414
train_iter_loss: 0.35747355222702026
train_iter_loss: 0.2717292904853821
train_iter_loss: 0.34353986382484436
train_iter_loss: 0.40939804911613464
train_iter_loss: 0.4582630693912506
train_iter_loss: 0.3865683972835541
train_iter_loss: 0.37269699573516846
train_iter_loss: 0.34804442524909973
train_iter_loss: 0.3492336571216583
train_iter_loss: 0.3294542133808136
train_iter_loss: 0.4169521927833557
train_iter_loss: 0.4800117313861847
train_iter_loss: 0.5172808170318604
train_iter_loss: 0.36816054582595825
train_iter_loss: 0.24973522126674652
train_iter_loss: 0.3930540680885315
train_iter_loss: 0.3371575176715851
train_iter_loss: 0.3247867524623871
train_iter_loss: 0.30080142617225647
train_iter_loss: 0.31790751218795776
train loss :0.3835
---------------------
Validation seg loss: 0.4316743127298805 at epoch 13
********************
best_val_epoch_loss:  0.4316743127298805
MODEL UPDATED
epoch =     14/  1000, exp = train
train_iter_loss: 0.3376603424549103
train_iter_loss: 0.21058519184589386
train_iter_loss: 0.3670772910118103
train_iter_loss: 0.3397388458251953
train_iter_loss: 0.5780572295188904
train_iter_loss: 0.44545263051986694
train_iter_loss: 0.2150314748287201
train_iter_loss: 0.4436088502407074
train_iter_loss: 0.39927443861961365
train_iter_loss: 0.3091267943382263
train_iter_loss: 0.438274621963501
train_iter_loss: 0.4058174788951874
train_iter_loss: 0.36567962169647217
train_iter_loss: 0.4419938027858734
train_iter_loss: 0.5433750152587891
train_iter_loss: 0.43726927042007446
train_iter_loss: 0.2866629362106323
train_iter_loss: 0.32501220703125
train_iter_loss: 0.3264954686164856
train_iter_loss: 0.4047619700431824
train_iter_loss: 0.32349035143852234
train_iter_loss: 0.2849951386451721
train_iter_loss: 0.41003403067588806
train_iter_loss: 0.3662048280239105
train_iter_loss: 0.44271358847618103
train_iter_loss: 0.4066716730594635
train_iter_loss: 0.2289932370185852
train_iter_loss: 0.30132564902305603
train_iter_loss: 0.33233270049095154
train_iter_loss: 0.2904168367385864
train_iter_loss: 0.333563894033432
train_iter_loss: 0.33726486563682556
train_iter_loss: 0.4281589984893799
train_iter_loss: 0.4081071615219116
train_iter_loss: 0.2921372950077057
train_iter_loss: 0.35920482873916626
train_iter_loss: 0.5427281856536865
train_iter_loss: 0.29605355858802795
train_iter_loss: 0.5552530288696289
train_iter_loss: 0.37641027569770813
train_iter_loss: 0.4047810733318329
train_iter_loss: 0.26526519656181335
train_iter_loss: 0.3016486167907715
train_iter_loss: 0.38780447840690613
train_iter_loss: 0.3035407066345215
train_iter_loss: 0.34611842036247253
train_iter_loss: 0.4344748556613922
train_iter_loss: 0.4078328013420105
train_iter_loss: 0.41121378540992737
train_iter_loss: 0.3649679124355316
train_iter_loss: 0.5851832032203674
train_iter_loss: 0.42061951756477356
train_iter_loss: 0.34727954864501953
train_iter_loss: 0.42908522486686707
train_iter_loss: 0.3603421151638031
train_iter_loss: 0.3699388802051544
train_iter_loss: 0.41651827096939087
train_iter_loss: 0.30874985456466675
train_iter_loss: 0.4865521192550659
train_iter_loss: 0.3647416830062866
train_iter_loss: 0.3429928123950958
train_iter_loss: 0.3575790524482727
train_iter_loss: 0.3775664269924164
train_iter_loss: 0.35045620799064636
train_iter_loss: 0.3205139935016632
train_iter_loss: 0.35403093695640564
train_iter_loss: 0.3684588670730591
train_iter_loss: 0.3484322130680084
train_iter_loss: 0.3794628977775574
train_iter_loss: 0.3512038290500641
train_iter_loss: 0.402864545583725
train_iter_loss: 0.2860526442527771
train_iter_loss: 0.5319644808769226
train_iter_loss: 0.35624226927757263
train_iter_loss: 0.38713401556015015
train_iter_loss: 0.33074867725372314
train_iter_loss: 0.412956565618515
train_iter_loss: 0.35575100779533386
train_iter_loss: 0.45091554522514343
train_iter_loss: 0.3740677237510681
train_iter_loss: 0.44048720598220825
train_iter_loss: 0.3100823163986206
train_iter_loss: 0.32615557312965393
train_iter_loss: 0.3481205403804779
train_iter_loss: 0.4181138277053833
train_iter_loss: 0.313113808631897
train_iter_loss: 0.33427122235298157
train_iter_loss: 0.4217941164970398
train_iter_loss: 0.4763301610946655
train_iter_loss: 0.42950499057769775
train_iter_loss: 0.36727678775787354
train_iter_loss: 0.33828142285346985
train_iter_loss: 0.3900461494922638
train_iter_loss: 0.347384512424469
train_iter_loss: 0.36518922448158264
train_iter_loss: 0.5234304666519165
train_iter_loss: 0.2993674576282501
train_iter_loss: 0.4210189878940582
train_iter_loss: 0.3963770270347595
train_iter_loss: 0.37679147720336914
train loss :0.3812
---------------------
Validation seg loss: 0.42552818807790865 at epoch 14
********************
best_val_epoch_loss:  0.42552818807790865
MODEL UPDATED
epoch =     15/  1000, exp = train
train_iter_loss: 0.43076926469802856
train_iter_loss: 0.5579954385757446
train_iter_loss: 0.3667512834072113
train_iter_loss: 0.3481525480747223
train_iter_loss: 0.46205613017082214
train_iter_loss: 0.34395551681518555
train_iter_loss: 0.3530203402042389
train_iter_loss: 0.31430166959762573
train_iter_loss: 0.28497037291526794
train_iter_loss: 0.4101209044456482
train_iter_loss: 0.33921587467193604
train_iter_loss: 0.36694061756134033
train_iter_loss: 0.2536442279815674
train_iter_loss: 0.4926043450832367
train_iter_loss: 0.28954848647117615
train_iter_loss: 0.3282349705696106
train_iter_loss: 0.30770343542099
train_iter_loss: 0.2887793183326721
train_iter_loss: 0.3180326521396637
train_iter_loss: 0.33560052514076233
train_iter_loss: 0.5268123745918274
train_iter_loss: 0.26392343640327454
train_iter_loss: 0.48282578587532043
train_iter_loss: 0.37766239047050476
train_iter_loss: 0.333835631608963
train_iter_loss: 0.3403119146823883
train_iter_loss: 0.36938413977622986
train_iter_loss: 0.2809530794620514
train_iter_loss: 0.300557404756546
train_iter_loss: 0.35808834433555603
train_iter_loss: 0.4010780155658722
train_iter_loss: 0.42873087525367737
train_iter_loss: 0.23059847950935364
train_iter_loss: 0.3973945081233978
train_iter_loss: 0.29298579692840576
train_iter_loss: 0.37509235739707947
train_iter_loss: 0.301594078540802
train_iter_loss: 0.3403905928134918
train_iter_loss: 0.379218190908432
train_iter_loss: 0.2838106155395508
train_iter_loss: 0.37312015891075134
train_iter_loss: 0.3602323830127716
train_iter_loss: 0.41667014360427856
train_iter_loss: 0.4392860531806946
train_iter_loss: 0.37092944979667664
train_iter_loss: 0.31763431429862976
train_iter_loss: 0.3879563510417938
train_iter_loss: 0.4295327663421631
train_iter_loss: 0.3504461944103241
train_iter_loss: 0.4115923345088959
train_iter_loss: 0.34147876501083374
train_iter_loss: 0.2929910123348236
train_iter_loss: 0.39035117626190186
train_iter_loss: 0.23810935020446777
train_iter_loss: 0.37302589416503906
train_iter_loss: 0.29397183656692505
train_iter_loss: 0.3402020335197449
train_iter_loss: 0.390183687210083
train_iter_loss: 0.4321609139442444
train_iter_loss: 0.3061884343624115
train_iter_loss: 0.3326272666454315
train_iter_loss: 0.27833449840545654
train_iter_loss: 0.4436783492565155
train_iter_loss: 0.4295940697193146
train_iter_loss: 0.3447810411453247
train_iter_loss: 0.32082539796829224
train_iter_loss: 0.5426377654075623
train_iter_loss: 0.380038321018219
train_iter_loss: 0.45050206780433655
train_iter_loss: 0.5735966563224792
train_iter_loss: 0.550391674041748
train_iter_loss: 0.29034191370010376
train_iter_loss: 0.3829568028450012
train_iter_loss: 0.33151477575302124
train_iter_loss: 0.29379090666770935
train_iter_loss: 0.28651610016822815
train_iter_loss: 0.4435614049434662
train_iter_loss: 0.3388446271419525
train_iter_loss: 0.3707594573497772
train_iter_loss: 0.4042815864086151
train_iter_loss: 0.45232048630714417
train_iter_loss: 0.39388710260391235
train_iter_loss: 0.36485055088996887
train_iter_loss: 0.31594961881637573
train_iter_loss: 0.4034840762615204
train_iter_loss: 0.37083834409713745
train_iter_loss: 0.40108010172843933
train_iter_loss: 0.33248305320739746
train_iter_loss: 0.3800887167453766
train_iter_loss: 0.4347585439682007
train_iter_loss: 0.3571871817111969
train_iter_loss: 0.4783061444759369
train_iter_loss: 0.34353265166282654
train_iter_loss: 0.4106716513633728
train_iter_loss: 0.38054946064949036
train_iter_loss: 0.279521644115448
train_iter_loss: 0.3602273166179657
train_iter_loss: 0.38770759105682373
train_iter_loss: 0.37678056955337524
train_iter_loss: 0.3760284185409546
train loss :0.3728
---------------------
Validation seg loss: 0.41725457473745886 at epoch 15
********************
best_val_epoch_loss:  0.41725457473745886
MODEL UPDATED
epoch =     16/  1000, exp = train
train_iter_loss: 0.392240434885025
train_iter_loss: 0.3907082676887512
train_iter_loss: 0.3613385260105133
train_iter_loss: 0.3284613788127899
train_iter_loss: 0.3973388671875
train_iter_loss: 0.4211537539958954
train_iter_loss: 0.40811657905578613
train_iter_loss: 0.28512802720069885
train_iter_loss: 0.3258249759674072
train_iter_loss: 0.3380778729915619
train_iter_loss: 0.339229941368103
train_iter_loss: 0.3913929760456085
train_iter_loss: 0.3135393261909485
train_iter_loss: 0.34567463397979736
train_iter_loss: 0.3564523458480835
train_iter_loss: 0.5103803277015686
train_iter_loss: 0.5319538116455078
train_iter_loss: 0.42562249302864075
train_iter_loss: 0.5115141272544861
train_iter_loss: 0.33363020420074463
train_iter_loss: 0.4877541959285736
train_iter_loss: 0.33243247866630554
train_iter_loss: 0.4336301386356354
train_iter_loss: 0.3718443512916565
train_iter_loss: 0.2796227037906647
train_iter_loss: 0.3005865812301636
train_iter_loss: 0.3438207805156708
train_iter_loss: 0.3819541037082672
train_iter_loss: 0.4707050323486328
train_iter_loss: 0.4556517004966736
train_iter_loss: 0.3836081624031067
train_iter_loss: 0.4218897819519043
train_iter_loss: 0.5490918159484863
train_iter_loss: 0.43561267852783203
train_iter_loss: 0.4846295118331909
train_iter_loss: 0.25255030393600464
train_iter_loss: 0.3404620289802551
train_iter_loss: 0.33070874214172363
train_iter_loss: 0.33816155791282654
train_iter_loss: 0.33521291613578796
train_iter_loss: 0.28883054852485657
train_iter_loss: 0.4028647243976593
train_iter_loss: 0.23459358513355255
train_iter_loss: 0.46902406215667725
train_iter_loss: 0.34918686747550964
train_iter_loss: 0.3670230209827423
train_iter_loss: 0.2719036340713501
train_iter_loss: 0.4744729697704315
train_iter_loss: 0.3944908678531647
train_iter_loss: 0.38920363783836365
train_iter_loss: 0.39204141497612
train_iter_loss: 0.3031214773654938
train_iter_loss: 0.45036351680755615
train_iter_loss: 0.31985512375831604
train_iter_loss: 0.4928596019744873
train_iter_loss: 0.42424172163009644
train_iter_loss: 0.33649948239326477
train_iter_loss: 0.34342166781425476
train_iter_loss: 0.4036704897880554
train_iter_loss: 0.38736557960510254
train_iter_loss: 0.3425450026988983
train_iter_loss: 0.34891819953918457
train_iter_loss: 0.41602933406829834
train_iter_loss: 0.407967746257782
train_iter_loss: 0.2546948790550232
train_iter_loss: 0.30354928970336914
train_iter_loss: 0.16804367303848267
train_iter_loss: 0.4316164255142212
train_iter_loss: 0.49705109000205994
train_iter_loss: 0.34086617827415466
train_iter_loss: 0.26463258266448975
train_iter_loss: 0.4443555176258087
train_iter_loss: 0.4565649628639221
train_iter_loss: 0.4029340147972107
train_iter_loss: 0.36167940497398376
train_iter_loss: 0.37354928255081177
train_iter_loss: 0.23964150249958038
train_iter_loss: 0.41327059268951416
train_iter_loss: 0.3830112814903259
train_iter_loss: 0.37351953983306885
train_iter_loss: 0.39641663432121277
train_iter_loss: 0.33895397186279297
train_iter_loss: 0.33283331990242004
train_iter_loss: 0.3778236210346222
train_iter_loss: 0.28351128101348877
train_iter_loss: 0.36150026321411133
train_iter_loss: 0.3446027636528015
train_iter_loss: 0.43482705950737
train_iter_loss: 0.4321940839290619
train_iter_loss: 0.3677966296672821
train_iter_loss: 0.35879501700401306
train_iter_loss: 0.3190876543521881
train_iter_loss: 0.29127058386802673
train_iter_loss: 0.3577377200126648
train_iter_loss: 0.33511883020401
train_iter_loss: 0.33715197443962097
train_iter_loss: 0.3332384526729584
train_iter_loss: 0.496315062046051
train_iter_loss: 0.36860427260398865
train_iter_loss: 0.37369999289512634
train loss :0.3777
---------------------
Validation seg loss: 0.43927608346039393 at epoch 16
epoch =     17/  1000, exp = train
train_iter_loss: 0.3961252272129059
train_iter_loss: 0.5240468978881836
train_iter_loss: 0.370658278465271
train_iter_loss: 0.2978487014770508
train_iter_loss: 0.4209628999233246
train_iter_loss: 0.40284839272499084
train_iter_loss: 0.36737579107284546
train_iter_loss: 0.3457116484642029
train_iter_loss: 0.3962322473526001
train_iter_loss: 0.31315934658050537
train_iter_loss: 0.3418720066547394
train_iter_loss: 0.4015370011329651
train_iter_loss: 0.39409610629081726
train_iter_loss: 0.4600125551223755
train_iter_loss: 0.362141489982605
train_iter_loss: 0.3172578811645508
train_iter_loss: 0.408503919839859
train_iter_loss: 0.3893485963344574
train_iter_loss: 0.36493343114852905
train_iter_loss: 0.3282508850097656
train_iter_loss: 0.32336270809173584
train_iter_loss: 0.34109294414520264
train_iter_loss: 0.2251928299665451
train_iter_loss: 0.34077510237693787
train_iter_loss: 0.3420950174331665
train_iter_loss: 0.4307352602481842
train_iter_loss: 0.42902442812919617
train_iter_loss: 0.41855281591415405
train_iter_loss: 0.3098611831665039
train_iter_loss: 0.37385785579681396
train_iter_loss: 0.30005091428756714
train_iter_loss: 0.2905053496360779
train_iter_loss: 0.3818023204803467
train_iter_loss: 0.3441537618637085
train_iter_loss: 0.3574978709220886
train_iter_loss: 0.5592377781867981
train_iter_loss: 0.3570685386657715
train_iter_loss: 0.3920794725418091
train_iter_loss: 0.48653754591941833
train_iter_loss: 0.3254184424877167
train_iter_loss: 0.43186140060424805
train_iter_loss: 0.39734283089637756
train_iter_loss: 0.2567717432975769
train_iter_loss: 0.4367077052593231
train_iter_loss: 0.36007416248321533
train_iter_loss: 0.3651708960533142
train_iter_loss: 0.2740671634674072
train_iter_loss: 0.3274560272693634
train_iter_loss: 0.34706854820251465
train_iter_loss: 0.45421302318573
train_iter_loss: 0.3386143445968628
train_iter_loss: 0.4153776168823242
train_iter_loss: 0.4322730600833893
train_iter_loss: 0.40348389744758606
train_iter_loss: 0.25668150186538696
train_iter_loss: 0.4167822003364563
train_iter_loss: 0.327148973941803
train_iter_loss: 0.20089980959892273
train_iter_loss: 0.37145960330963135
train_iter_loss: 0.430513858795166
train_iter_loss: 0.43864578008651733
train_iter_loss: 0.3956376910209656
train_iter_loss: 0.28106528520584106
train_iter_loss: 0.33875927329063416
train_iter_loss: 0.49074268341064453
train_iter_loss: 0.433883935213089
train_iter_loss: 0.29685258865356445
train_iter_loss: 0.37989360094070435
train_iter_loss: 0.32492756843566895
train_iter_loss: 0.2998751997947693
train_iter_loss: 0.2851368486881256
train_iter_loss: 0.4727913439273834
train_iter_loss: 0.3368552327156067
train_iter_loss: 0.2970479726791382
train_iter_loss: 0.37078776955604553
train_iter_loss: 0.3880581259727478
train_iter_loss: 0.35845813155174255
train_iter_loss: 0.4324387311935425
train_iter_loss: 0.3786409795284271
train_iter_loss: 0.3446415066719055
train_iter_loss: 0.38063350319862366
train_iter_loss: 0.3042803108692169
train_iter_loss: 0.5176858901977539
train_iter_loss: 0.3602044880390167
train_iter_loss: 0.4004403054714203
train_iter_loss: 0.27368566393852234
train_iter_loss: 0.27168843150138855
train_iter_loss: 0.2501744031906128
train_iter_loss: 0.32752230763435364
train_iter_loss: 0.30783164501190186
train_iter_loss: 0.36600232124328613
train_iter_loss: 0.31793028116226196
train_iter_loss: 0.44518494606018066
train_iter_loss: 0.4312015473842621
train_iter_loss: 0.30626368522644043
train_iter_loss: 0.32478126883506775
train_iter_loss: 0.3762032091617584
train_iter_loss: 0.4122885763645172
train_iter_loss: 0.5059155821800232
train_iter_loss: 0.3354475498199463
train loss :0.3711
---------------------
Validation seg loss: 0.41352869443735985 at epoch 17
********************
best_val_epoch_loss:  0.41352869443735985
MODEL UPDATED
epoch =     18/  1000, exp = train
train_iter_loss: 0.4074946939945221
train_iter_loss: 0.42360085248947144
train_iter_loss: 0.3920159339904785
train_iter_loss: 0.49887678027153015
train_iter_loss: 0.3693501949310303
train_iter_loss: 0.28259918093681335
train_iter_loss: 0.3101973831653595
train_iter_loss: 0.3895581066608429
train_iter_loss: 0.3405354917049408
train_iter_loss: 0.28854528069496155
train_iter_loss: 0.2480047196149826
train_iter_loss: 0.4687993824481964
train_iter_loss: 0.3729510009288788
train_iter_loss: 0.4127981960773468
train_iter_loss: 0.28568845987319946
train_iter_loss: 0.3524136543273926
train_iter_loss: 0.43939265608787537
train_iter_loss: 0.42212629318237305
train_iter_loss: 0.28334954380989075
train_iter_loss: 0.30154046416282654
train_iter_loss: 0.33215540647506714
train_iter_loss: 0.43804970383644104
train_iter_loss: 0.2836659848690033
train_iter_loss: 0.3406596779823303
train_iter_loss: 0.40456539392471313
train_iter_loss: 0.38437724113464355
train_iter_loss: 0.3558886647224426
train_iter_loss: 0.34561336040496826
train_iter_loss: 0.3148675858974457
train_iter_loss: 0.5228667855262756
train_iter_loss: 0.3451084494590759
train_iter_loss: 0.43040063977241516
train_iter_loss: 0.40835699439048767
train_iter_loss: 0.4805624783039093
train_iter_loss: 0.4043482542037964
train_iter_loss: 0.4385760426521301
train_iter_loss: 0.33477410674095154
train_iter_loss: 0.3935709595680237
train_iter_loss: 0.44524770975112915
train_iter_loss: 0.2864987850189209
train_iter_loss: 0.4355168342590332
train_iter_loss: 0.41023704409599304
train_iter_loss: 0.3252454698085785
train_iter_loss: 0.35014164447784424
train_iter_loss: 0.46016696095466614
train_iter_loss: 0.39185014367103577
train_iter_loss: 0.27646294236183167
train_iter_loss: 0.322292685508728
train_iter_loss: 0.3721989095211029
train_iter_loss: 0.3304649889469147
train_iter_loss: 0.3793116807937622
train_iter_loss: 0.29400819540023804
train_iter_loss: 0.43619316816329956
train_iter_loss: 0.35516953468322754
train_iter_loss: 0.42448166012763977
train_iter_loss: 0.5139963626861572
train_iter_loss: 0.2800103425979614
train_iter_loss: 0.43865662813186646
train_iter_loss: 0.29983222484588623
train_iter_loss: 0.33272939920425415
train_iter_loss: 0.25034570693969727
train_iter_loss: 0.3873659670352936
train_iter_loss: 0.30844539403915405
train_iter_loss: 0.36978965997695923
train_iter_loss: 0.30214008688926697
train_iter_loss: 0.4113430678844452
train_iter_loss: 0.5125323534011841
train_iter_loss: 0.3601949214935303
train_iter_loss: 0.3488050699234009
train_iter_loss: 0.33632132411003113
train_iter_loss: 0.29021528363227844
train_iter_loss: 0.29674142599105835
train_iter_loss: 0.3863235116004944
train_iter_loss: 0.2736353874206543
train_iter_loss: 0.27793624997138977
train_iter_loss: 0.40934082865715027
train_iter_loss: 0.33049365878105164
train_iter_loss: 0.3261256217956543
train_iter_loss: 0.36196863651275635
train_iter_loss: 0.24186120927333832
train_iter_loss: 0.3781239688396454
train_iter_loss: 0.360855370759964
train_iter_loss: 0.34960150718688965
train_iter_loss: 0.3985101282596588
train_iter_loss: 0.33460867404937744
train_iter_loss: 0.36466750502586365
train_iter_loss: 0.41949525475502014
train_iter_loss: 0.4980415105819702
train_iter_loss: 0.3564745783805847
train_iter_loss: 0.2850719690322876
train_iter_loss: 0.4158201217651367
train_iter_loss: 0.31664443016052246
train_iter_loss: 0.2970220446586609
train_iter_loss: 0.3513534367084503
train_iter_loss: 0.3223109841346741
train_iter_loss: 0.3725307583808899
train_iter_loss: 0.25442254543304443
train_iter_loss: 0.2968146502971649
train_iter_loss: 0.427104651927948
train_iter_loss: 0.29728391766548157
train loss :0.3666
---------------------
Validation seg loss: 0.4251678056733788 at epoch 18
epoch =     19/  1000, exp = train
train_iter_loss: 0.293217271566391
train_iter_loss: 0.3483261168003082
train_iter_loss: 0.31600677967071533
train_iter_loss: 0.5123468041419983
train_iter_loss: 0.31403249502182007
train_iter_loss: 0.35724639892578125
train_iter_loss: 0.3946690857410431
train_iter_loss: 0.37054991722106934
train_iter_loss: 0.36717352271080017
train_iter_loss: 0.2633485794067383
train_iter_loss: 0.2949307858943939
train_iter_loss: 0.2891894578933716
train_iter_loss: 0.40893614292144775
train_iter_loss: 0.4721280634403229
train_iter_loss: 0.4224682152271271
train_iter_loss: 0.34134936332702637
train_iter_loss: 0.3140360116958618
train_iter_loss: 0.421674519777298
train_iter_loss: 0.43729349970817566
train_iter_loss: 0.3488155007362366
train_iter_loss: 0.395860493183136
train_iter_loss: 0.38489511609077454
train_iter_loss: 0.3322226107120514
train_iter_loss: 0.2934190630912781
train_iter_loss: 0.39843863248825073
train_iter_loss: 0.4332653880119324
train_iter_loss: 0.41041460633277893
train_iter_loss: 0.4450678825378418
train_iter_loss: 0.3642156720161438
train_iter_loss: 0.30329209566116333
train_iter_loss: 0.4286506772041321
train_iter_loss: 0.696584165096283
train_iter_loss: 0.4630240797996521
train_iter_loss: 0.330909788608551
train_iter_loss: 0.30650460720062256
train_iter_loss: 0.3224082291126251
train_iter_loss: 0.3199937045574188
train_iter_loss: 0.24347111582756042
train_iter_loss: 0.5066118836402893
train_iter_loss: 0.28547897934913635
train_iter_loss: 0.28758305311203003
train_iter_loss: 0.40059664845466614
train_iter_loss: 0.28664329648017883
train_iter_loss: 0.3027268648147583
train_iter_loss: 0.2760116755962372
train_iter_loss: 0.36890920996665955
train_iter_loss: 0.503460705280304
train_iter_loss: 0.42185288667678833
train_iter_loss: 0.2635359466075897
train_iter_loss: 0.37778040766716003
train_iter_loss: 0.4057525098323822
train_iter_loss: 0.29058825969696045
train_iter_loss: 0.3160126507282257
train_iter_loss: 0.2495693564414978
train_iter_loss: 0.43255695700645447
train_iter_loss: 0.3106282949447632
train_iter_loss: 0.4192453622817993
train_iter_loss: 0.4527744650840759
train_iter_loss: 0.33715179562568665
train_iter_loss: 0.3396259844303131
train_iter_loss: 0.3123811185359955
train_iter_loss: 0.3514101207256317
train_iter_loss: 0.4533770680427551
train_iter_loss: 0.3897974193096161
train_iter_loss: 0.3548610806465149
train_iter_loss: 0.4398891031742096
train_iter_loss: 0.26473522186279297
train_iter_loss: 0.4144720137119293
train_iter_loss: 0.2886390686035156
train_iter_loss: 0.2538355886936188
train_iter_loss: 0.41153547167778015
train_iter_loss: 0.2790200710296631
train_iter_loss: 0.3888399600982666
train_iter_loss: 0.37073636054992676
train_iter_loss: 0.47137197852134705
train_iter_loss: 0.47719821333885193
train_iter_loss: 0.34125635027885437
train_iter_loss: 0.41575539112091064
train_iter_loss: 0.38741040229797363
train_iter_loss: 0.4753093123435974
train_iter_loss: 0.28118738532066345
train_iter_loss: 0.33863458037376404
train_iter_loss: 0.44557031989097595
train_iter_loss: 0.3238275647163391
train_iter_loss: 0.3396107852458954
train_iter_loss: 0.40983501076698303
train_iter_loss: 0.36981216073036194
train_iter_loss: 0.30603834986686707
train_iter_loss: 0.3878467381000519
train_iter_loss: 0.3563324809074402
train_iter_loss: 0.39322155714035034
train_iter_loss: 0.47204098105430603
train_iter_loss: 0.2632773816585541
train_iter_loss: 0.35543373227119446
train_iter_loss: 0.40169084072113037
train_iter_loss: 0.41035258769989014
train_iter_loss: 0.35106945037841797
train_iter_loss: 0.24686598777770996
train_iter_loss: 0.42872172594070435
train_iter_loss: 0.2782539129257202
train loss :0.3706
---------------------
Validation seg loss: 0.4091001325901949 at epoch 19
********************
best_val_epoch_loss:  0.4091001325901949
MODEL UPDATED
epoch =     20/  1000, exp = train
train_iter_loss: 0.30305275321006775
train_iter_loss: 0.28425946831703186
train_iter_loss: 0.5076178312301636
train_iter_loss: 0.35399553179740906
train_iter_loss: 0.28296563029289246
train_iter_loss: 0.29718101024627686
train_iter_loss: 0.47956037521362305
train_iter_loss: 0.30113688111305237
train_iter_loss: 0.3934026062488556
train_iter_loss: 0.3407801389694214
train_iter_loss: 0.29559314250946045
train_iter_loss: 0.36199477314949036
train_iter_loss: 0.3645855784416199
train_iter_loss: 0.4194038510322571
train_iter_loss: 0.3933592438697815
train_iter_loss: 0.33007490634918213
train_iter_loss: 0.3195621371269226
train_iter_loss: 0.31184878945350647
train_iter_loss: 0.2439866065979004
train_iter_loss: 0.38291406631469727
train_iter_loss: 0.33160650730133057
train_iter_loss: 0.4809437394142151
train_iter_loss: 0.4491090774536133
train_iter_loss: 0.3730328381061554
train_iter_loss: 0.4123961925506592
train_iter_loss: 0.33644235134124756
train_iter_loss: 0.5538635849952698
train_iter_loss: 0.4857376515865326
train_iter_loss: 0.20440836250782013
train_iter_loss: 0.29209238290786743
train_iter_loss: 0.31376913189888
train_iter_loss: 0.3085643947124481
train_iter_loss: 0.34915590286254883
train_iter_loss: 0.368879109621048
train_iter_loss: 0.347336083650589
train_iter_loss: 0.3002121150493622
train_iter_loss: 0.3139454126358032
train_iter_loss: 0.3331502676010132
train_iter_loss: 0.3307783305644989
train_iter_loss: 0.355252742767334
train_iter_loss: 0.2636890709400177
train_iter_loss: 0.30202633142471313
train_iter_loss: 0.4330677390098572
train_iter_loss: 0.3894442915916443
train_iter_loss: 0.3303576111793518
train_iter_loss: 0.5259348154067993
train_iter_loss: 0.31074562668800354
train_iter_loss: 0.3247452974319458
train_iter_loss: 0.31831446290016174
train_iter_loss: 0.3546868860721588
train_iter_loss: 0.4884588420391083
train_iter_loss: 0.3023826479911804
train_iter_loss: 0.37231433391571045
train_iter_loss: 0.31823551654815674
train_iter_loss: 0.569619357585907
train_iter_loss: 0.3662716746330261
train_iter_loss: 0.3729327321052551
train_iter_loss: 0.2817024886608124
train_iter_loss: 0.34582552313804626
train_iter_loss: 0.34438374638557434
train_iter_loss: 0.35336822271347046
train_iter_loss: 0.32707852125167847
train_iter_loss: 0.37639737129211426
train_iter_loss: 0.3410738408565521
train_iter_loss: 0.4089774191379547
train_iter_loss: 0.4422866702079773
train_iter_loss: 0.390511155128479
train_iter_loss: 0.3756493628025055
train_iter_loss: 0.35983118414878845
train_iter_loss: 0.3123956620693207
train_iter_loss: 0.2910415530204773
train_iter_loss: 0.37111181020736694
train_iter_loss: 0.33115577697753906
train_iter_loss: 0.4358750283718109
train_iter_loss: 0.3697754740715027
train_iter_loss: 0.4748130142688751
train_iter_loss: 0.3330152630805969
train_iter_loss: 0.43055489659309387
train_iter_loss: 0.39666491746902466
train_iter_loss: 0.34431740641593933
train_iter_loss: 0.46432313323020935
train_iter_loss: 0.2900008261203766
train_iter_loss: 0.3349318206310272
train_iter_loss: 0.40685439109802246
train_iter_loss: 0.4666566848754883
train_iter_loss: 0.3054356276988983
train_iter_loss: 0.4685192406177521
train_iter_loss: 0.35354921221733093
train_iter_loss: 0.3407958149909973
train_iter_loss: 0.31491169333457947
train_iter_loss: 0.2998407185077667
train_iter_loss: 0.44143205881118774
train_iter_loss: 0.33981165289878845
train_iter_loss: 0.3865794539451599
train_iter_loss: 0.33327266573905945
train_iter_loss: 0.39523518085479736
train_iter_loss: 0.35692670941352844
train_iter_loss: 0.3522525727748871
train_iter_loss: 0.3795788288116455
train_iter_loss: 0.403901606798172
train loss :0.3679
---------------------
Validation seg loss: 0.42053943464778504 at epoch 20
epoch =     21/  1000, exp = train
train_iter_loss: 0.337102472782135
train_iter_loss: 0.30913642048835754
train_iter_loss: 0.33921292424201965
train_iter_loss: 0.2414555847644806
train_iter_loss: 0.4757227599620819
train_iter_loss: 0.4759308993816376
train_iter_loss: 0.4849788248538971
train_iter_loss: 0.414466917514801
train_iter_loss: 0.4606880843639374
train_iter_loss: 0.34493422508239746
train_iter_loss: 0.40281450748443604
train_iter_loss: 0.32446885108947754
train_iter_loss: 0.3808387517929077
train_iter_loss: 0.34605202078819275
train_iter_loss: 0.3543879985809326
train_iter_loss: 0.3920190632343292
train_iter_loss: 0.42260849475860596
train_iter_loss: 0.4349932074546814
train_iter_loss: 0.345357745885849
train_iter_loss: 0.32152169942855835
train_iter_loss: 0.5239142179489136
train_iter_loss: 0.3362541198730469
train_iter_loss: 0.29497888684272766
train_iter_loss: 0.3081800043582916
train_iter_loss: 0.3198910653591156
train_iter_loss: 0.3035094141960144
train_iter_loss: 0.4147801995277405
train_iter_loss: 0.3276214301586151
train_iter_loss: 0.34155067801475525
train_iter_loss: 0.34809714555740356
train_iter_loss: 0.31090471148490906
train_iter_loss: 0.3426285684108734
train_iter_loss: 0.3785260021686554
train_iter_loss: 0.34922245144844055
train_iter_loss: 0.22266864776611328
train_iter_loss: 0.3661348521709442
train_iter_loss: 0.41859951615333557
train_iter_loss: 0.48463302850723267
train_iter_loss: 0.4791434109210968
train_iter_loss: 0.2831127345561981
train_iter_loss: 0.2647426724433899
train_iter_loss: 0.5172067284584045
train_iter_loss: 0.45551687479019165
train_iter_loss: 0.339028000831604
train_iter_loss: 0.3424170911312103
train_iter_loss: 0.3391337990760803
train_iter_loss: 0.44329532980918884
train_iter_loss: 0.3817143440246582
train_iter_loss: 0.37760451436042786
train_iter_loss: 0.4474906027317047
train_iter_loss: 0.47173306345939636
train_iter_loss: 0.42740598320961
train_iter_loss: 0.4881041347980499
train_iter_loss: 0.5068729519844055
train_iter_loss: 0.4114137589931488
train_iter_loss: 0.42846235632896423
train_iter_loss: 0.30248576402664185
train_iter_loss: 0.27108234167099
train_iter_loss: 0.3522314727306366
train_iter_loss: 0.38615188002586365
train_iter_loss: 0.3463602066040039
train_iter_loss: 0.3209047317504883
train_iter_loss: 0.4208184778690338
train_iter_loss: 0.34387850761413574
train_iter_loss: 0.34033843874931335
train_iter_loss: 0.34197285771369934
train_iter_loss: 0.1977306455373764
train_iter_loss: 0.5041308999061584
train_iter_loss: 0.3960336744785309
train_iter_loss: 0.3152628540992737
train_iter_loss: 0.4066201448440552
train_iter_loss: 0.27333545684814453
train_iter_loss: 0.36225733160972595
train_iter_loss: 0.4570848047733307
train_iter_loss: 0.38563230633735657
train_iter_loss: 0.38369953632354736
train_iter_loss: 0.2595701515674591
train_iter_loss: 0.3271259367465973
train_iter_loss: 0.2318781316280365
train_iter_loss: 0.27968424558639526
train_iter_loss: 0.38573211431503296
train_iter_loss: 0.2910368740558624
train_iter_loss: 0.3202686607837677
train_iter_loss: 0.3285217881202698
train_iter_loss: 0.21370625495910645
train_iter_loss: 0.4014119505882263
train_iter_loss: 0.22768867015838623
train_iter_loss: 0.44672155380249023
train_iter_loss: 0.4118434190750122
train_iter_loss: 0.21668052673339844
train_iter_loss: 0.3397751450538635
train_iter_loss: 0.4800054728984833
train_iter_loss: 0.35124462842941284
train_iter_loss: 0.4001179039478302
train_iter_loss: 0.3728110194206238
train_iter_loss: 0.2865056097507477
train_iter_loss: 0.35893726348876953
train_iter_loss: 0.3792407810688019
train_iter_loss: 0.4055222272872925
train_iter_loss: 0.325382798910141
train loss :0.3692
---------------------
Validation seg loss: 0.4183425152639173 at epoch 21
epoch =     22/  1000, exp = train
train_iter_loss: 0.3059842586517334
train_iter_loss: 0.3021751940250397
train_iter_loss: 0.4765925705432892
train_iter_loss: 0.2916308045387268
train_iter_loss: 0.3673801124095917
train_iter_loss: 0.36221712827682495
train_iter_loss: 0.34644412994384766
train_iter_loss: 0.32107412815093994
train_iter_loss: 0.27980977296829224
train_iter_loss: 0.3596569895744324
train_iter_loss: 0.4523879289627075
train_iter_loss: 0.31677258014678955
train_iter_loss: 0.38870009779930115
train_iter_loss: 0.35124972462654114
train_iter_loss: 0.36378100514411926
train_iter_loss: 0.2665673494338989
train_iter_loss: 0.3494642376899719
train_iter_loss: 0.23630723357200623
train_iter_loss: 0.3186722695827484
train_iter_loss: 0.3879598081111908
train_iter_loss: 0.33660241961479187
train_iter_loss: 0.2990615963935852
train_iter_loss: 0.4231218993663788
train_iter_loss: 0.3290735185146332
train_iter_loss: 0.42668336629867554
train_iter_loss: 0.3384305238723755
train_iter_loss: 0.3122836649417877
train_iter_loss: 0.2527061700820923
train_iter_loss: 0.252043753862381
train_iter_loss: 0.3172777593135834
train_iter_loss: 0.33299344778060913
train_iter_loss: 0.3401729464530945
train_iter_loss: 0.25674939155578613
train_iter_loss: 0.3339040279388428
train_iter_loss: 0.3672981262207031
train_iter_loss: 0.5282258987426758
train_iter_loss: 0.35640522837638855
train_iter_loss: 0.42873361706733704
train_iter_loss: 0.6427119374275208
train_iter_loss: 0.33074951171875
train_iter_loss: 0.31407278776168823
train_iter_loss: 0.49322572350502014
train_iter_loss: 0.39922624826431274
train_iter_loss: 0.37946030497550964
train_iter_loss: 0.5075373649597168
train_iter_loss: 0.4155357778072357
train_iter_loss: 0.3636917173862457
train_iter_loss: 0.3006724715232849
train_iter_loss: 0.35755816102027893
train_iter_loss: 0.4020344018936157
train_iter_loss: 0.27603787183761597
train_iter_loss: 0.32662510871887207
train_iter_loss: 0.3037375807762146
train_iter_loss: 0.31726205348968506
train_iter_loss: 0.3192019462585449
train_iter_loss: 0.27926498651504517
train_iter_loss: 0.4163559377193451
train_iter_loss: 0.3330709636211395
train_iter_loss: 0.37266167998313904
train_iter_loss: 0.28009262681007385
train_iter_loss: 0.3245299458503723
train_iter_loss: 0.37935733795166016
train_iter_loss: 0.4025079905986786
train_iter_loss: 0.36092445254325867
train_iter_loss: 0.48659634590148926
train_iter_loss: 0.32530564069747925
train_iter_loss: 0.39350637793540955
train_iter_loss: 0.34203973412513733
train_iter_loss: 0.14509274065494537
train_iter_loss: 0.2990352511405945
train_iter_loss: 0.4229677617549896
train_iter_loss: 0.37242087721824646
train_iter_loss: 0.28951165080070496
train_iter_loss: 0.37891003489494324
train_iter_loss: 0.3808607757091522
train_iter_loss: 0.40199604630470276
train_iter_loss: 0.41331636905670166
train_iter_loss: 0.43882328271865845
train_iter_loss: 0.2640245258808136
train_iter_loss: 0.2539009749889374
train_iter_loss: 0.35570284724235535
train_iter_loss: 0.4641941487789154
train_iter_loss: 0.35696232318878174
train_iter_loss: 0.5433632731437683
train_iter_loss: 0.4460239112377167
train_iter_loss: 0.39708155393600464
train_iter_loss: 0.39897504448890686
train_iter_loss: 0.35094326734542847
train_iter_loss: 0.3035426735877991
train_iter_loss: 0.3638610243797302
train_iter_loss: 0.4203665852546692
train_iter_loss: 0.27317002415657043
train_iter_loss: 0.3298884332180023
train_iter_loss: 0.38660499453544617
train_iter_loss: 0.4199376702308655
train_iter_loss: 0.3109593689441681
train_iter_loss: 0.39109233021736145
train_iter_loss: 0.3367588520050049
train_iter_loss: 0.4623982012271881
train_iter_loss: 0.3441757261753082
train loss :0.3631
---------------------
Validation seg loss: 0.41100358650228885 at epoch 22
epoch =     23/  1000, exp = train
train_iter_loss: 0.26933377981185913
train_iter_loss: 0.3244647681713104
train_iter_loss: 0.35706377029418945
train_iter_loss: 0.38436052203178406
train_iter_loss: 0.2939963638782501
train_iter_loss: 0.3343512713909149
train_iter_loss: 0.3323897421360016
train_iter_loss: 0.48308444023132324
train_iter_loss: 0.3602368235588074
train_iter_loss: 0.3170355558395386
train_iter_loss: 0.366918683052063
train_iter_loss: 0.28640255331993103
train_iter_loss: 0.4192523956298828
train_iter_loss: 0.38463059067726135
train_iter_loss: 0.29930567741394043
train_iter_loss: 0.3295508623123169
train_iter_loss: 0.37022432684898376
train_iter_loss: 0.42859986424446106
train_iter_loss: 0.2848852276802063
train_iter_loss: 0.40719014406204224
train_iter_loss: 0.37875837087631226
train_iter_loss: 0.2594221830368042
train_iter_loss: 0.6975608468055725
train_iter_loss: 0.39640364050865173
train_iter_loss: 0.4462369978427887
train_iter_loss: 0.29315364360809326
train_iter_loss: 0.33328989148139954
train_iter_loss: 0.4084636867046356
train_iter_loss: 0.29051491618156433
train_iter_loss: 0.3962481617927551
train_iter_loss: 0.4328422248363495
train_iter_loss: 0.46318143606185913
train_iter_loss: 0.30436667799949646
train_iter_loss: 0.30371055006980896
train_iter_loss: 0.3168361783027649
train_iter_loss: 0.3511958718299866
train_iter_loss: 0.32305416464805603
train_iter_loss: 0.33682480454444885
train_iter_loss: 0.37642794847488403
train_iter_loss: 0.3845011591911316
train_iter_loss: 0.4805772304534912
train_iter_loss: 0.42671218514442444
train_iter_loss: 0.34521788358688354
train_iter_loss: 0.39832213521003723
train_iter_loss: 0.3440394699573517
train_iter_loss: 0.3031559884548187
train_iter_loss: 0.291620671749115
train_iter_loss: 0.30830949544906616
train_iter_loss: 0.3645840287208557
train_iter_loss: 0.39985182881355286
train_iter_loss: 0.20065978169441223
train_iter_loss: 0.330129474401474
train_iter_loss: 0.2868388593196869
train_iter_loss: 0.21553638577461243
train_iter_loss: 0.40117353200912476
train_iter_loss: 0.35001248121261597
train_iter_loss: 0.4841395914554596
train_iter_loss: 0.32922762632369995
train_iter_loss: 0.3568454384803772
train_iter_loss: 0.30318573117256165
train_iter_loss: 0.41475823521614075
train_iter_loss: 0.34926357865333557
train_iter_loss: 0.4612794816493988
train_iter_loss: 0.4048035740852356
train_iter_loss: 0.30039602518081665
train_iter_loss: 0.26197922229766846
train_iter_loss: 0.35238587856292725
train_iter_loss: 0.31227248907089233
train_iter_loss: 0.26196691393852234
train_iter_loss: 0.3113362789154053
train_iter_loss: 0.4738306403160095
train_iter_loss: 0.24340607225894928
train_iter_loss: 0.3511931300163269
train_iter_loss: 0.2744746506214142
train_iter_loss: 0.33543235063552856
train_iter_loss: 0.3988798260688782
train_iter_loss: 0.3728540241718292
train_iter_loss: 0.3615182936191559
train_iter_loss: 0.41922685503959656
train_iter_loss: 0.37583592534065247
train_iter_loss: 0.284889817237854
train_iter_loss: 0.4175381660461426
train_iter_loss: 0.3358178734779358
train_iter_loss: 0.3378492295742035
train_iter_loss: 0.33587464690208435
train_iter_loss: 0.2683705985546112
train_iter_loss: 0.3269214332103729
train_iter_loss: 0.43468648195266724
train_iter_loss: 0.3349420726299286
train_iter_loss: 0.37255656719207764
train_iter_loss: 0.23591431975364685
train_iter_loss: 0.37569475173950195
train_iter_loss: 0.4449540376663208
train_iter_loss: 0.26432424783706665
train_iter_loss: 0.34147605299949646
train_iter_loss: 0.4154914915561676
train_iter_loss: 0.3090376555919647
train_iter_loss: 0.2765708565711975
train_iter_loss: 0.30555492639541626
train_iter_loss: 0.575922966003418
train loss :0.3584
---------------------
Validation seg loss: 0.4048654113738042 at epoch 23
********************
best_val_epoch_loss:  0.4048654113738042
MODEL UPDATED
epoch =     24/  1000, exp = train
train_iter_loss: 0.25492623448371887
train_iter_loss: 0.3728041350841522
train_iter_loss: 0.3566514551639557
train_iter_loss: 0.2861419916152954
train_iter_loss: 0.3041432201862335
train_iter_loss: 0.23938211798667908
train_iter_loss: 0.3783682584762573
train_iter_loss: 0.34236252307891846
train_iter_loss: 0.3817221224308014
train_iter_loss: 0.35276275873184204
train_iter_loss: 0.22761383652687073
train_iter_loss: 0.2991339862346649
train_iter_loss: 0.3285064995288849
train_iter_loss: 0.41761600971221924
train_iter_loss: 0.3672417402267456
train_iter_loss: 0.3928256332874298
train_iter_loss: 0.40929681062698364
train_iter_loss: 0.34773555397987366
train_iter_loss: 0.383672833442688
train_iter_loss: 0.34081393480300903
train_iter_loss: 0.3761715292930603
train_iter_loss: 0.2784934341907501
train_iter_loss: 0.2894488573074341
train_iter_loss: 0.25360938906669617
train_iter_loss: 0.3244088292121887
train_iter_loss: 0.313203364610672
train_iter_loss: 0.32294565439224243
train_iter_loss: 0.300091952085495
train_iter_loss: 0.3338744044303894
train_iter_loss: 0.36188170313835144
train_iter_loss: 0.3353513777256012
train_iter_loss: 0.40347298979759216
train_iter_loss: 0.4211042821407318
train_iter_loss: 0.2530069053173065
train_iter_loss: 0.2833327651023865
train_iter_loss: 0.2989121079444885
train_iter_loss: 0.41871994733810425
train_iter_loss: 0.44188234210014343
train_iter_loss: 0.49878960847854614
train_iter_loss: 0.4348337948322296
train_iter_loss: 0.4599030911922455
train_iter_loss: 0.2366514652967453
train_iter_loss: 0.368413507938385
train_iter_loss: 0.4351697266101837
train_iter_loss: 0.30509310960769653
train_iter_loss: 0.29869508743286133
train_iter_loss: 0.31517231464385986
train_iter_loss: 0.40059909224510193
train_iter_loss: 0.31704816222190857
train_iter_loss: 0.34307992458343506
train_iter_loss: 0.32793161273002625
train_iter_loss: 0.4143107533454895
train_iter_loss: 0.43293461203575134
train_iter_loss: 0.28452059626579285
train_iter_loss: 0.3172447383403778
train_iter_loss: 0.30691060423851013
train_iter_loss: 0.3653629422187805
train_iter_loss: 0.30249014496803284
train_iter_loss: 0.39366304874420166
train_iter_loss: 0.3023320436477661
train_iter_loss: 0.40299493074417114
train_iter_loss: 0.28941744565963745
train_iter_loss: 0.27914372086524963
train_iter_loss: 0.2859761714935303
train_iter_loss: 0.39946886897087097
train_iter_loss: 0.30858707427978516
train_iter_loss: 0.3013900816440582
train_iter_loss: 0.34129956364631653
train_iter_loss: 0.34593987464904785
train_iter_loss: 0.43223443627357483
train_iter_loss: 0.37502455711364746
train_iter_loss: 0.3042972981929779
train_iter_loss: 0.38520562648773193
train_iter_loss: 0.2932344973087311
train_iter_loss: 0.31153443455696106
train_iter_loss: 0.35687384009361267
train_iter_loss: 0.420060932636261
train_iter_loss: 0.39546412229537964
train_iter_loss: 0.3947485089302063
train_iter_loss: 0.45134657621383667
train_iter_loss: 0.24708259105682373
train_iter_loss: 0.4000586271286011
train_iter_loss: 0.34471574425697327
train_iter_loss: 0.5889493823051453
train_iter_loss: 0.2798590064048767
train_iter_loss: 0.40927040576934814
train_iter_loss: 0.29609790444374084
train_iter_loss: 0.28600117564201355
train_iter_loss: 0.3017686605453491
train_iter_loss: 0.24827264249324799
train_iter_loss: 0.3951844871044159
train_iter_loss: 0.29524263739585876
train_iter_loss: 0.4033951163291931
train_iter_loss: 0.42715930938720703
train_iter_loss: 0.3454382121562958
train_iter_loss: 0.3815973699092865
train_iter_loss: 0.3072216212749481
train_iter_loss: 0.4547460675239563
train_iter_loss: 0.4368814527988434
train_iter_loss: 0.3940320611000061
train loss :0.3533
---------------------
Validation seg loss: 0.4025519913730194 at epoch 24
********************
best_val_epoch_loss:  0.4025519913730194
MODEL UPDATED
epoch =     25/  1000, exp = train
train_iter_loss: 0.3821021616458893
train_iter_loss: 0.3934454321861267
train_iter_loss: 0.2950848937034607
train_iter_loss: 0.30783817172050476
train_iter_loss: 0.2803841531276703
train_iter_loss: 0.2941822409629822
train_iter_loss: 0.3355792164802551
train_iter_loss: 0.31688907742500305
train_iter_loss: 0.31043541431427
train_iter_loss: 0.27235865592956543
train_iter_loss: 0.31743642687797546
train_iter_loss: 0.2713356614112854
train_iter_loss: 0.4236070513725281
train_iter_loss: 0.3469623923301697
train_iter_loss: 0.318856805562973
train_iter_loss: 0.29768696427345276
train_iter_loss: 0.29435697197914124
train_iter_loss: 0.3815852403640747
train_iter_loss: 0.23835936188697815
train_iter_loss: 0.3186831772327423
train_iter_loss: 0.3060305416584015
train_iter_loss: 0.35662245750427246
train_iter_loss: 0.4128025770187378
train_iter_loss: 0.4431557357311249
train_iter_loss: 0.34188711643218994
train_iter_loss: 0.3409818708896637
train_iter_loss: 0.1901460886001587
train_iter_loss: 0.23801901936531067
train_iter_loss: 0.32310330867767334
train_iter_loss: 0.3092651963233948
train_iter_loss: 0.36657339334487915
train_iter_loss: 0.33570942282676697
train_iter_loss: 0.33160972595214844
train_iter_loss: 0.5525299310684204
train_iter_loss: 0.38799479603767395
train_iter_loss: 0.4348789155483246
train_iter_loss: 0.37481656670570374
train_iter_loss: 0.30585700273513794
train_iter_loss: 0.5688460469245911
train_iter_loss: 0.3932323455810547
train_iter_loss: 0.38269707560539246
train_iter_loss: 0.25032520294189453
train_iter_loss: 0.5316148400306702
train_iter_loss: 0.3970485329627991
train_iter_loss: 0.3309985399246216
train_iter_loss: 0.49210235476493835
train_iter_loss: 0.423748642206192
train_iter_loss: 0.43015769124031067
train_iter_loss: 0.3300148546695709
train_iter_loss: 0.29130712151527405
train_iter_loss: 0.24447771906852722
train_iter_loss: 0.3556798994541168
train_iter_loss: 0.34016966819763184
train_iter_loss: 0.4717791676521301
train_iter_loss: 0.3809199631214142
train_iter_loss: 0.47393834590911865
train_iter_loss: 0.3057219088077545
train_iter_loss: 0.28553304076194763
train_iter_loss: 0.25959348678588867
train_iter_loss: 0.25519490242004395
train_iter_loss: 0.4442676901817322
train_iter_loss: 0.31573864817619324
train_iter_loss: 0.3688308596611023
train_iter_loss: 0.33562126755714417
train_iter_loss: 0.3993280231952667
train_iter_loss: 0.30793777108192444
train_iter_loss: 0.3320855498313904
train_iter_loss: 0.38073253631591797
train_iter_loss: 0.3881920874118805
train_iter_loss: 0.3484777808189392
train_iter_loss: 0.36877232789993286
train_iter_loss: 0.2820640802383423
train_iter_loss: 0.35425058007240295
train_iter_loss: 0.20166625082492828
train_iter_loss: 0.3117663860321045
train_iter_loss: 0.28678861260414124
train_iter_loss: 0.3437095284461975
train_iter_loss: 0.28949010372161865
train_iter_loss: 0.363309383392334
train_iter_loss: 0.2805602252483368
train_iter_loss: 0.2602391541004181
train_iter_loss: 0.4781363308429718
train_iter_loss: 0.2626294791698456
train_iter_loss: 0.42356184124946594
train_iter_loss: 0.3373221457004547
train_iter_loss: 0.362737774848938
train_iter_loss: 0.4056093692779541
train_iter_loss: 0.38657116889953613
train_iter_loss: 0.43799859285354614
train_iter_loss: 0.29360076785087585
train_iter_loss: 0.38733813166618347
train_iter_loss: 0.3121694028377533
train_iter_loss: 0.31309735774993896
train_iter_loss: 0.4031941294670105
train_iter_loss: 0.39385977387428284
train_iter_loss: 0.33652403950691223
train_iter_loss: 0.3287448585033417
train_iter_loss: 0.4692322015762329
train_iter_loss: 0.4886994957923889
train_iter_loss: 0.3609931468963623
train loss :0.3544
---------------------
Validation seg loss: 0.40313015639219646 at epoch 25
epoch =     26/  1000, exp = train
train_iter_loss: 0.39640888571739197
train_iter_loss: 0.30673912167549133
train_iter_loss: 0.5169911980628967
train_iter_loss: 0.4854254424571991
train_iter_loss: 0.26461470127105713
train_iter_loss: 0.3084058165550232
train_iter_loss: 0.2923943102359772
train_iter_loss: 0.2991698980331421
train_iter_loss: 0.49178096652030945
train_iter_loss: 0.2453877478837967
train_iter_loss: 0.33725613355636597
train_iter_loss: 0.4010368883609772
train_iter_loss: 0.3390960395336151
train_iter_loss: 0.6105782985687256
train_iter_loss: 0.40219399333000183
train_iter_loss: 0.4394424855709076
train_iter_loss: 0.29506129026412964
train_iter_loss: 0.2278275191783905
train_iter_loss: 0.28969427943229675
train_iter_loss: 0.3854108452796936
train_iter_loss: 0.3050883114337921
train_iter_loss: 0.2968752384185791
train_iter_loss: 0.3262307345867157
train_iter_loss: 0.1821615993976593
train_iter_loss: 0.4112471640110016
train_iter_loss: 0.38732072710990906
train_iter_loss: 0.36519771814346313
train_iter_loss: 0.31541377305984497
train_iter_loss: 0.29030078649520874
train_iter_loss: 0.27606332302093506
train_iter_loss: 0.28864434361457825
train_iter_loss: 0.3046048879623413
train_iter_loss: 0.2574501037597656
train_iter_loss: 0.3410317003726959
train_iter_loss: 0.3704063296318054
train_iter_loss: 0.30229654908180237
train_iter_loss: 0.3738478124141693
train_iter_loss: 0.5196531414985657
train_iter_loss: 0.425014853477478
train_iter_loss: 0.34327250719070435
train_iter_loss: 0.27904266119003296
train_iter_loss: 0.3598770201206207
train_iter_loss: 0.3067111074924469
train_iter_loss: 0.3756423592567444
train_iter_loss: 0.42401835322380066
train_iter_loss: 0.32719120383262634
train_iter_loss: 0.13975606858730316
train_iter_loss: 0.26409709453582764
train_iter_loss: 0.4001975655555725
train_iter_loss: 0.3287738561630249
train_iter_loss: 0.3608650267124176
train_iter_loss: 0.41068607568740845
train_iter_loss: 0.36554181575775146
train_iter_loss: 0.2797883152961731
train_iter_loss: 0.3857812285423279
train_iter_loss: 0.48636671900749207
train_iter_loss: 0.358405739068985
train_iter_loss: 0.29874178767204285
train_iter_loss: 0.3625892400741577
train_iter_loss: 0.39139944314956665
train_iter_loss: 0.2816128730773926
train_iter_loss: 0.4802703261375427
train_iter_loss: 0.3314773440361023
train_iter_loss: 0.2059377133846283
train_iter_loss: 0.36529651284217834
train_iter_loss: 0.4037938714027405
train_iter_loss: 0.37387675046920776
train_iter_loss: 0.3067672550678253
train_iter_loss: 0.238181933760643
train_iter_loss: 0.3868301212787628
train_iter_loss: 0.22218716144561768
train_iter_loss: 0.34978944063186646
train_iter_loss: 0.4000762701034546
train_iter_loss: 0.3837898075580597
train_iter_loss: 0.5713793635368347
train_iter_loss: 0.37798571586608887
train_iter_loss: 0.2670626938343048
train_iter_loss: 0.3958222568035126
train_iter_loss: 0.3079455494880676
train_iter_loss: 0.33004823327064514
train_iter_loss: 0.4196401834487915
train_iter_loss: 0.39161092042922974
train_iter_loss: 0.3607892096042633
train_iter_loss: 0.3321894109249115
train_iter_loss: 0.35081323981285095
train_iter_loss: 0.3737115263938904
train_iter_loss: 0.27594050765037537
train_iter_loss: 0.3499077558517456
train_iter_loss: 0.2238852083683014
train_iter_loss: 0.3018255829811096
train_iter_loss: 0.3840804994106293
train_iter_loss: 0.21614758670330048
train_iter_loss: 0.3042920231819153
train_iter_loss: 0.442013680934906
train_iter_loss: 0.32112154364585876
train_iter_loss: 0.3386901617050171
train_iter_loss: 0.3973173201084137
train_iter_loss: 0.2840260863304138
train_iter_loss: 0.36521291732788086
train_iter_loss: 0.3257313668727875
train loss :0.3502
---------------------
Validation seg loss: 0.42833707906868096 at epoch 26
epoch =     27/  1000, exp = train
train_iter_loss: 0.42585504055023193
train_iter_loss: 0.2821303904056549
train_iter_loss: 0.38249972462654114
train_iter_loss: 0.42051932215690613
train_iter_loss: 0.41585609316825867
train_iter_loss: 0.36261337995529175
train_iter_loss: 0.3546278178691864
train_iter_loss: 0.31535595655441284
train_iter_loss: 0.49693208932876587
train_iter_loss: 0.34601742029190063
train_iter_loss: 0.31276822090148926
train_iter_loss: 0.24807371199131012
train_iter_loss: 0.34679025411605835
train_iter_loss: 0.4039979577064514
train_iter_loss: 0.25999799370765686
train_iter_loss: 0.3151967525482178
train_iter_loss: 0.37220701575279236
train_iter_loss: 0.20621758699417114
train_iter_loss: 0.4617355167865753
train_iter_loss: 0.3830326199531555
train_iter_loss: 0.37368810176849365
train_iter_loss: 0.5384908318519592
train_iter_loss: 0.32246315479278564
train_iter_loss: 0.3026096820831299
train_iter_loss: 0.40826740860939026
train_iter_loss: 0.28753021359443665
train_iter_loss: 0.3203849196434021
train_iter_loss: 0.34158045053482056
train_iter_loss: 0.2959439158439636
train_iter_loss: 0.3906950056552887
train_iter_loss: 0.38473179936408997
train_iter_loss: 0.46340078115463257
train_iter_loss: 0.4275124967098236
train_iter_loss: 0.5357212424278259
train_iter_loss: 0.2956729233264923
train_iter_loss: 0.32439881563186646
train_iter_loss: 0.2633993327617645
train_iter_loss: 0.2933824062347412
train_iter_loss: 0.30324503779411316
train_iter_loss: 0.274873286485672
train_iter_loss: 0.3473474681377411
train_iter_loss: 0.3483339846134186
train_iter_loss: 0.34780383110046387
train_iter_loss: 0.2988019585609436
train_iter_loss: 0.42044463753700256
train_iter_loss: 0.2689156234264374
train_iter_loss: 0.34672626852989197
train_iter_loss: 0.23830878734588623
train_iter_loss: 0.2874608635902405
train_iter_loss: 0.38717886805534363
train_iter_loss: 0.394094854593277
train_iter_loss: 0.30138489603996277
train_iter_loss: 0.6023421883583069
train_iter_loss: 0.3526475131511688
train_iter_loss: 0.2908480763435364
train_iter_loss: 0.2989087700843811
train_iter_loss: 0.2729485034942627
train_iter_loss: 0.3463056981563568
train_iter_loss: 0.4041178226470947
train_iter_loss: 0.28018414974212646
train_iter_loss: 0.38949909806251526
train_iter_loss: 0.2998034954071045
train_iter_loss: 0.32772818207740784
train_iter_loss: 0.29247093200683594
train_iter_loss: 0.42083147168159485
train_iter_loss: 0.24641218781471252
train_iter_loss: 0.4251277446746826
train_iter_loss: 0.2970491945743561
train_iter_loss: 0.46421870589256287
train_iter_loss: 0.2701316475868225
train_iter_loss: 0.481969952583313
train_iter_loss: 0.37119343876838684
train_iter_loss: 0.23738470673561096
train_iter_loss: 0.5242955088615417
train_iter_loss: 0.3990805149078369
train_iter_loss: 0.3589773178100586
train_iter_loss: 0.26680752635002136
train_iter_loss: 0.38258808851242065
train_iter_loss: 0.22733739018440247
train_iter_loss: 0.4196162819862366
train_iter_loss: 0.2044624388217926
train_iter_loss: 0.3639147877693176
train_iter_loss: 0.2801560163497925
train_iter_loss: 0.4513196647167206
train_iter_loss: 0.3773616552352905
train_iter_loss: 0.3548617660999298
train_iter_loss: 0.29919859766960144
train_iter_loss: 0.48427170515060425
train_iter_loss: 0.4153278172016144
train_iter_loss: 0.23369930684566498
train_iter_loss: 0.3951968252658844
train_iter_loss: 0.31163913011550903
train_iter_loss: 0.349518746137619
train_iter_loss: 0.3369109034538269
train_iter_loss: 0.340057909488678
train_iter_loss: 0.3008517324924469
train_iter_loss: 0.28773725032806396
train_iter_loss: 0.30945056676864624
train_iter_loss: 0.33503395318984985
train_iter_loss: 0.3858271539211273
train loss :0.3537
---------------------
Validation seg loss: 0.4074993457926332 at epoch 27
epoch =     28/  1000, exp = train
train_iter_loss: 0.2255144715309143
train_iter_loss: 0.19929739832878113
train_iter_loss: 0.228868305683136
train_iter_loss: 0.29136019945144653
train_iter_loss: 0.25782617926597595
train_iter_loss: 0.2805905342102051
train_iter_loss: 0.37325137853622437
train_iter_loss: 0.4034918248653412
train_iter_loss: 0.30996009707450867
train_iter_loss: 0.36907392740249634
train_iter_loss: 0.30167925357818604
train_iter_loss: 0.32661962509155273
train_iter_loss: 0.40535005927085876
train_iter_loss: 0.47605136036872864
train_iter_loss: 0.27768653631210327
train_iter_loss: 0.34517911076545715
train_iter_loss: 0.33925265073776245
train_iter_loss: 0.26814740896224976
train_iter_loss: 0.21509017050266266
train_iter_loss: 0.3734365403652191
train_iter_loss: 0.2698914706707001
train_iter_loss: 0.3136729598045349
train_iter_loss: 0.3846138119697571
train_iter_loss: 0.3790307641029358
train_iter_loss: 0.26789626479148865
train_iter_loss: 0.3907425105571747
train_iter_loss: 0.29628515243530273
train_iter_loss: 0.2576256990432739
train_iter_loss: 0.24628140032291412
train_iter_loss: 0.36969539523124695
train_iter_loss: 0.3395577371120453
train_iter_loss: 0.4039748013019562
train_iter_loss: 0.2789989411830902
train_iter_loss: 0.3871621787548065
train_iter_loss: 0.4223741590976715
train_iter_loss: 0.4910067319869995
train_iter_loss: 0.42934712767601013
train_iter_loss: 0.29812437295913696
train_iter_loss: 0.25734877586364746
train_iter_loss: 0.25824108719825745
train_iter_loss: 0.3586495816707611
train_iter_loss: 0.37815073132514954
train_iter_loss: 0.3522886335849762
train_iter_loss: 0.31675010919570923
train_iter_loss: 0.4087720215320587
train_iter_loss: 0.3363248407840729
train_iter_loss: 0.3490241467952728
train_iter_loss: 0.3929194509983063
train_iter_loss: 0.31612589955329895
train_iter_loss: 0.36787107586860657
train_iter_loss: 0.42398130893707275
train_iter_loss: 0.4288591742515564
train_iter_loss: 0.2512178421020508
train_iter_loss: 0.47014230489730835
train_iter_loss: 0.47334223985671997
train_iter_loss: 0.356149286031723
train_iter_loss: 0.3335507810115814
train_iter_loss: 0.3318302035331726
train_iter_loss: 0.41257068514823914
train_iter_loss: 0.4933217167854309
train_iter_loss: 0.35145774483680725
train_iter_loss: 0.32058173418045044
train_iter_loss: 0.33160048723220825
train_iter_loss: 0.4253185987472534
train_iter_loss: 0.2893248498439789
train_iter_loss: 0.37749698758125305
train_iter_loss: 0.27010688185691833
train_iter_loss: 0.3492050766944885
train_iter_loss: 0.47583484649658203
train_iter_loss: 0.2527851164340973
train_iter_loss: 0.30012694001197815
train_iter_loss: 0.25879645347595215
train_iter_loss: 0.41510719060897827
train_iter_loss: 0.2847387194633484
train_iter_loss: 0.350909948348999
train_iter_loss: 0.2743804156780243
train_iter_loss: 0.47256872057914734
train_iter_loss: 0.37059059739112854
train_iter_loss: 0.28435373306274414
train_iter_loss: 0.4217279851436615
train_iter_loss: 0.2679317891597748
train_iter_loss: 0.24139682948589325
train_iter_loss: 0.2896829843521118
train_iter_loss: 0.40218183398246765
train_iter_loss: 0.3745031952857971
train_iter_loss: 0.31067535281181335
train_iter_loss: 0.3064703047275543
train_iter_loss: 0.4114837944507599
train_iter_loss: 0.3668965995311737
train_iter_loss: 0.3891651928424835
train_iter_loss: 0.4034019410610199
train_iter_loss: 0.5161355137825012
train_iter_loss: 0.28285595774650574
train_iter_loss: 0.3202521800994873
train_iter_loss: 0.3431824743747711
train_iter_loss: 0.30209577083587646
train_iter_loss: 0.3499480187892914
train_iter_loss: 0.2992543578147888
train_iter_loss: 0.37782952189445496
train_iter_loss: 0.4732331931591034
train loss :0.3482
---------------------
Validation seg loss: 0.4110498637804445 at epoch 28
epoch =     29/  1000, exp = train
train_iter_loss: 0.49955976009368896
train_iter_loss: 0.2015790045261383
train_iter_loss: 0.28590816259384155
train_iter_loss: 0.3033905625343323
train_iter_loss: 0.39349326491355896
train_iter_loss: 0.32837411761283875
train_iter_loss: 0.2508734166622162
train_iter_loss: 0.3766289949417114
train_iter_loss: 0.25225725769996643
train_iter_loss: 0.4832015931606293
train_iter_loss: 0.24867884814739227
train_iter_loss: 0.386234849691391
train_iter_loss: 0.290277361869812
train_iter_loss: 0.22443990409374237
train_iter_loss: 0.4016351103782654
train_iter_loss: 0.21820135414600372
train_iter_loss: 0.32992565631866455
train_iter_loss: 0.2733156681060791
train_iter_loss: 0.36595600843429565
train_iter_loss: 0.3020322620868683
train_iter_loss: 0.291193425655365
train_iter_loss: 0.3157762587070465
train_iter_loss: 0.35062068700790405
train_iter_loss: 0.2882439196109772
train_iter_loss: 0.291496604681015
train_iter_loss: 0.33149513602256775
train_iter_loss: 0.36333736777305603
train_iter_loss: 0.2861860692501068
train_iter_loss: 0.26350921392440796
train_iter_loss: 0.3480425477027893
train_iter_loss: 0.3850909173488617
train_iter_loss: 0.3032477796077728
train_iter_loss: 0.27877599000930786
train_iter_loss: 0.36475419998168945
train_iter_loss: 0.27994266152381897
train_iter_loss: 0.2935725450515747
train_iter_loss: 0.27496904134750366
train_iter_loss: 0.41518551111221313
train_iter_loss: 0.3627932667732239
train_iter_loss: 0.36949804425239563
train_iter_loss: 0.4165057837963104
train_iter_loss: 0.30163872241973877
train_iter_loss: 0.28156694769859314
train_iter_loss: 0.3712156414985657
train_iter_loss: 0.3321184813976288
train_iter_loss: 0.33128127455711365
train_iter_loss: 0.4456138014793396
train_iter_loss: 0.5816027522087097
train_iter_loss: 0.3046434819698334
train_iter_loss: 0.3411845862865448
train_iter_loss: 0.450624018907547
train_iter_loss: 0.3667762577533722
train_iter_loss: 0.23366688191890717
train_iter_loss: 0.3561278283596039
train_iter_loss: 0.2852461636066437
train_iter_loss: 0.4182910919189453
train_iter_loss: 0.36344510316848755
train_iter_loss: 0.37886130809783936
train_iter_loss: 0.26314935088157654
train_iter_loss: 0.4395468235015869
train_iter_loss: 0.487912118434906
train_iter_loss: 0.24193696677684784
train_iter_loss: 0.28269219398498535
train_iter_loss: 0.30565759539604187
train_iter_loss: 0.2895084321498871
train_iter_loss: 0.34338903427124023
train_iter_loss: 0.4633229076862335
train_iter_loss: 0.42299848794937134
train_iter_loss: 0.5194811224937439
train_iter_loss: 0.2923651933670044
train_iter_loss: 0.4325379431247711
train_iter_loss: 0.31492477655410767
train_iter_loss: 0.40045902132987976
train_iter_loss: 0.5412521958351135
train_iter_loss: 0.3832889497280121
train_iter_loss: 0.3515404164791107
train_iter_loss: 0.25524574518203735
train_iter_loss: 0.6536897420883179
train_iter_loss: 0.2924689054489136
train_iter_loss: 0.4406838119029999
train_iter_loss: 0.35459384322166443
train_iter_loss: 0.2933932840824127
train_iter_loss: 0.47806376218795776
train_iter_loss: 0.3908838927745819
train_iter_loss: 0.3332221806049347
train_iter_loss: 0.2534943222999573
train_iter_loss: 0.3289330005645752
train_iter_loss: 0.37492987513542175
train_iter_loss: 0.37080931663513184
train_iter_loss: 0.3700072169303894
train_iter_loss: 0.2502249777317047
train_iter_loss: 0.29141828417778015
train_iter_loss: 0.29227179288864136
train_iter_loss: 0.3440614938735962
train_iter_loss: 0.2816769778728485
train_iter_loss: 0.3104768991470337
train_iter_loss: 0.3062188923358917
train_iter_loss: 0.4239497184753418
train_iter_loss: 0.31067389249801636
train_iter_loss: 0.33918195962905884
train loss :0.3490
---------------------
Validation seg loss: 0.39978011279314196 at epoch 29
********************
best_val_epoch_loss:  0.39978011279314196
MODEL UPDATED
epoch =     30/  1000, exp = train
train_iter_loss: 0.28413891792297363
train_iter_loss: 0.29318901896476746
train_iter_loss: 0.3195294141769409
train_iter_loss: 0.29698699712753296
train_iter_loss: 0.3331981897354126
train_iter_loss: 0.28928840160369873
train_iter_loss: 0.29453524947166443
train_iter_loss: 0.3333066701889038
train_iter_loss: 0.334980845451355
train_iter_loss: 0.30651745200157166
train_iter_loss: 0.27431175112724304
train_iter_loss: 0.3912275731563568
train_iter_loss: 0.4032125174999237
train_iter_loss: 0.3097812235355377
train_iter_loss: 0.29728037118911743
train_iter_loss: 0.2849051058292389
train_iter_loss: 0.40884679555892944
train_iter_loss: 0.3706248104572296
train_iter_loss: 0.2648184895515442
train_iter_loss: 0.3198590576648712
train_iter_loss: 0.3100331425666809
train_iter_loss: 0.22725819051265717
train_iter_loss: 0.345973402261734
train_iter_loss: 0.27568650245666504
train_iter_loss: 0.30609771609306335
train_iter_loss: 0.2079966962337494
train_iter_loss: 0.4111190140247345
train_iter_loss: 0.3272765874862671
train_iter_loss: 0.35045844316482544
train_iter_loss: 0.25935885310173035
train_iter_loss: 0.26005616784095764
train_iter_loss: 0.38612210750579834
train_iter_loss: 0.32909324765205383
train_iter_loss: 0.4559754729270935
train_iter_loss: 0.3829186260700226
train_iter_loss: 0.4074716567993164
train_iter_loss: 0.2577085494995117
train_iter_loss: 0.4347282350063324
train_iter_loss: 0.344567209482193
train_iter_loss: 0.4900410771369934
train_iter_loss: 0.4502297341823578
train_iter_loss: 0.40789666771888733
train_iter_loss: 0.4465913474559784
train_iter_loss: 0.5105655789375305
train_iter_loss: 0.31217947602272034
train_iter_loss: 0.3821312189102173
train_iter_loss: 0.3556976914405823
train_iter_loss: 0.3800227642059326
train_iter_loss: 0.3186141550540924
train_iter_loss: 0.3071850538253784
train_iter_loss: 0.3214459717273712
train_iter_loss: 0.31881025433540344
train_iter_loss: 0.2803240418434143
train_iter_loss: 0.36594823002815247
train_iter_loss: 0.40300095081329346
train_iter_loss: 0.34268906712532043
train_iter_loss: 0.3285161852836609
train_iter_loss: 0.27319058775901794
train_iter_loss: 0.28892216086387634
train_iter_loss: 0.2442198246717453
train_iter_loss: 0.32293614745140076
train_iter_loss: 0.31902825832366943
train_iter_loss: 0.4078732132911682
train_iter_loss: 0.4234775900840759
train_iter_loss: 0.45039668679237366
train_iter_loss: 0.2054186463356018
train_iter_loss: 0.23366953432559967
train_iter_loss: 0.3519057333469391
train_iter_loss: 0.3215864896774292
train_iter_loss: 0.42648446559906006
train_iter_loss: 0.2819032073020935
train_iter_loss: 0.4558970034122467
train_iter_loss: 0.29006701707839966
train_iter_loss: 0.3147448003292084
train_iter_loss: 0.2640613317489624
train_iter_loss: 0.321098268032074
train_iter_loss: 0.38509494066238403
train_iter_loss: 0.19967983663082123
train_iter_loss: 0.32820001244544983
train_iter_loss: 0.4159388542175293
train_iter_loss: 0.10908809304237366
train_iter_loss: 0.27546289563179016
train_iter_loss: 0.31061312556266785
train_iter_loss: 0.34904879331588745
train_iter_loss: 0.2944597601890564
train_iter_loss: 0.3826824426651001
train_iter_loss: 0.48524463176727295
train_iter_loss: 0.2921407222747803
train_iter_loss: 0.32981038093566895
train_iter_loss: 0.351283460855484
train_iter_loss: 0.35548415780067444
train_iter_loss: 0.6379696130752563
train_iter_loss: 0.2503657341003418
train_iter_loss: 0.31457480788230896
train_iter_loss: 0.3788357973098755
train_iter_loss: 0.35917893052101135
train_iter_loss: 0.5118512511253357
train_iter_loss: 0.4783344268798828
train_iter_loss: 0.4143791198730469
train_iter_loss: 0.3673626780509949
train loss :0.3454
---------------------
Validation seg loss: 0.42469213245752846 at epoch 30
epoch =     31/  1000, exp = train
train_iter_loss: 0.3509235084056854
train_iter_loss: 0.2814503312110901
train_iter_loss: 0.4168008267879486
train_iter_loss: 0.31505775451660156
train_iter_loss: 0.33664122223854065
train_iter_loss: 0.23756544291973114
train_iter_loss: 0.3990600109100342
train_iter_loss: 0.24485188722610474
train_iter_loss: 0.28083208203315735
train_iter_loss: 0.30788248777389526
train_iter_loss: 0.30651602149009705
train_iter_loss: 0.2498960793018341
train_iter_loss: 0.30857178568840027
train_iter_loss: 0.2986990809440613
train_iter_loss: 0.3681209683418274
train_iter_loss: 0.3149397373199463
train_iter_loss: 0.36862701177597046
train_iter_loss: 0.36565902829170227
train_iter_loss: 0.6049394011497498
train_iter_loss: 0.43235278129577637
train_iter_loss: 0.36101409792900085
train_iter_loss: 0.24919246137142181
train_iter_loss: 0.29711949825286865
train_iter_loss: 0.2944660484790802
train_iter_loss: 0.2512637674808502
train_iter_loss: 0.21306729316711426
train_iter_loss: 0.5117396116256714
train_iter_loss: 0.36455655097961426
train_iter_loss: 0.406853586435318
train_iter_loss: 0.3289068639278412
train_iter_loss: 0.3370440602302551
train_iter_loss: 0.4044424295425415
train_iter_loss: 0.40326863527297974
train_iter_loss: 0.23627567291259766
train_iter_loss: 0.3765079081058502
train_iter_loss: 0.2786749005317688
train_iter_loss: 0.23638439178466797
train_iter_loss: 0.2320035994052887
train_iter_loss: 0.36084556579589844
train_iter_loss: 0.41737744212150574
train_iter_loss: 0.3640602231025696
train_iter_loss: 0.3397696316242218
train_iter_loss: 0.1890779733657837
train_iter_loss: 0.3507651686668396
train_iter_loss: 0.33317092061042786
train_iter_loss: 0.3349200189113617
train_iter_loss: 0.26179927587509155
train_iter_loss: 0.4014410376548767
train_iter_loss: 0.4643537104129791
train_iter_loss: 0.29950976371765137
train_iter_loss: 0.4406087398529053
train_iter_loss: 0.390747606754303
train_iter_loss: 0.3903196454048157
train_iter_loss: 0.38131698966026306
train_iter_loss: 0.39173147082328796
train_iter_loss: 0.2472424954175949
train_iter_loss: 0.4122823476791382
train_iter_loss: 0.42109066247940063
train_iter_loss: 0.5395200252532959
train_iter_loss: 0.3372385799884796
train_iter_loss: 0.3572954833507538
train_iter_loss: 0.3759942054748535
train_iter_loss: 0.28118157386779785
train_iter_loss: 0.30895042419433594
train_iter_loss: 0.4549970030784607
train_iter_loss: 0.2509961426258087
train_iter_loss: 0.39723485708236694
train_iter_loss: 0.27904966473579407
train_iter_loss: 0.40901413559913635
train_iter_loss: 0.2545163035392761
train_iter_loss: 0.26632261276245117
train_iter_loss: 0.36791640520095825
train_iter_loss: 0.2720375657081604
train_iter_loss: 0.5889562964439392
train_iter_loss: 0.4125790596008301
train_iter_loss: 0.2728094756603241
train_iter_loss: 0.40407150983810425
train_iter_loss: 0.26636940240859985
train_iter_loss: 0.37527725100517273
train_iter_loss: 0.5353518128395081
train_iter_loss: 0.3964681923389435
train_iter_loss: 0.4214474558830261
train_iter_loss: 0.47677263617515564
train_iter_loss: 0.3533572256565094
train_iter_loss: 0.46618080139160156
train_iter_loss: 0.4206642806529999
train_iter_loss: 0.31864073872566223
train_iter_loss: 0.29726341366767883
train_iter_loss: 0.2749840319156647
train_iter_loss: 0.2777521312236786
train_iter_loss: 0.38598954677581787
train_iter_loss: 0.28440552949905396
train_iter_loss: 0.31188300251960754
train_iter_loss: 0.23520192503929138
train_iter_loss: 0.32029494643211365
train_iter_loss: 0.2320636808872223
train_iter_loss: 0.35269883275032043
train_iter_loss: 0.4928453862667084
train_iter_loss: 0.3302064836025238
train_iter_loss: 0.284145712852478
train loss :0.3505
---------------------
Validation seg loss: 0.4019975281970681 at epoch 31
epoch =     32/  1000, exp = train
train_iter_loss: 0.37371066212654114
train_iter_loss: 0.3068208396434784
train_iter_loss: 0.2831919193267822
train_iter_loss: 0.31790977716445923
train_iter_loss: 0.32794222235679626
train_iter_loss: 0.417377769947052
train_iter_loss: 0.30216559767723083
train_iter_loss: 0.3129933774471283
train_iter_loss: 0.30107995867729187
train_iter_loss: 0.2849767804145813
train_iter_loss: 0.26330000162124634
train_iter_loss: 0.3632763624191284
train_iter_loss: 0.2947237193584442
train_iter_loss: 0.33225446939468384
train_iter_loss: 0.4018832743167877
train_iter_loss: 0.2512052357196808
train_iter_loss: 0.40347033739089966
train_iter_loss: 0.4510062634944916
train_iter_loss: 0.3490367531776428
train_iter_loss: 0.30806663632392883
train_iter_loss: 0.2423637956380844
train_iter_loss: 0.3192340135574341
train_iter_loss: 0.4186684489250183
train_iter_loss: 0.2785416841506958
train_iter_loss: 0.25442200899124146
train_iter_loss: 0.16559796035289764
train_iter_loss: 0.37431371212005615
train_iter_loss: 0.3323727250099182
train_iter_loss: 0.3157893121242523
train_iter_loss: 0.34907805919647217
train_iter_loss: 0.287785142660141
train_iter_loss: 0.3868544399738312
train_iter_loss: 0.34931057691574097
train_iter_loss: 0.4182486832141876
train_iter_loss: 0.36244481801986694
train_iter_loss: 0.4401353597640991
train_iter_loss: 0.4376133978366852
train_iter_loss: 0.2657913267612457
train_iter_loss: 0.402202308177948
train_iter_loss: 0.4073915481567383
train_iter_loss: 0.2927352786064148
train_iter_loss: 0.42078691720962524
train_iter_loss: 0.3279251158237457
train_iter_loss: 0.37149128317832947
train_iter_loss: 0.39260104298591614
train_iter_loss: 0.3024205267429352
train_iter_loss: 0.4226321876049042
train_iter_loss: 0.2826509475708008
train_iter_loss: 0.34790661931037903
train_iter_loss: 0.3703203499317169
train_iter_loss: 0.25952136516571045
train_iter_loss: 0.3550368845462799
train_iter_loss: 0.27582797408103943
train_iter_loss: 0.5357944965362549
train_iter_loss: 0.5472238063812256
train_iter_loss: 0.30446767807006836
train_iter_loss: 0.3424724340438843
train_iter_loss: 0.22038288414478302
train_iter_loss: 0.27860715985298157
train_iter_loss: 0.5247029066085815
train_iter_loss: 0.2790701389312744
train_iter_loss: 0.4742225408554077
train_iter_loss: 0.38798820972442627
train_iter_loss: 0.33150020241737366
train_iter_loss: 0.3615449070930481
train_iter_loss: 0.2772747576236725
train_iter_loss: 0.2630499005317688
train_iter_loss: 0.3429887294769287
train_iter_loss: 0.28403279185295105
train_iter_loss: 0.31996360421180725
train_iter_loss: 0.30581703782081604
train_iter_loss: 0.5196409225463867
train_iter_loss: 0.27774232625961304
train_iter_loss: 0.24463075399398804
train_iter_loss: 0.34621256589889526
train_iter_loss: 0.40287697315216064
train_iter_loss: 0.20955483615398407
train_iter_loss: 0.39849939942359924
train_iter_loss: 0.218444362282753
train_iter_loss: 0.3053855895996094
train_iter_loss: 0.38403183221817017
train_iter_loss: 0.2745180130004883
train_iter_loss: 0.274058997631073
train_iter_loss: 0.336893230676651
train_iter_loss: 0.3785534203052521
train_iter_loss: 0.35123103857040405
train_iter_loss: 0.2754860520362854
train_iter_loss: 0.34716418385505676
train_iter_loss: 0.28726696968078613
train_iter_loss: 0.33211177587509155
train_iter_loss: 0.2873276174068451
train_iter_loss: 0.3397541046142578
train_iter_loss: 0.33810845017433167
train_iter_loss: 0.3116709887981415
train_iter_loss: 0.40260079503059387
train_iter_loss: 0.48395007848739624
train_iter_loss: 0.35845834016799927
train_iter_loss: 0.2612989842891693
train_iter_loss: 0.2311398684978485
train_iter_loss: 0.3840585947036743
train loss :0.3417
---------------------
Validation seg loss: 0.4112019236638861 at epoch 32
epoch =     33/  1000, exp = train
train_iter_loss: 0.3269907534122467
train_iter_loss: 0.36300337314605713
train_iter_loss: 0.46599650382995605
train_iter_loss: 0.4304708242416382
train_iter_loss: 0.38495945930480957
train_iter_loss: 0.3901137113571167
train_iter_loss: 0.28267166018486023
train_iter_loss: 0.2765236496925354
train_iter_loss: 0.41523119807243347
train_iter_loss: 0.29290348291397095
train_iter_loss: 0.29240620136260986
train_iter_loss: 0.35904523730278015
train_iter_loss: 0.4926729202270508
train_iter_loss: 0.37663331627845764
train_iter_loss: 0.24953970313072205
train_iter_loss: 0.4230769872665405
train_iter_loss: 0.3963209390640259
train_iter_loss: 0.3825995624065399
train_iter_loss: 0.46120861172676086
train_iter_loss: 0.28991085290908813
train_iter_loss: 0.3625667989253998
train_iter_loss: 0.3025265038013458
train_iter_loss: 0.34163159132003784
train_iter_loss: 0.3094456195831299
train_iter_loss: 0.36474961042404175
train_iter_loss: 0.30486395955085754
train_iter_loss: 0.2937052547931671
train_iter_loss: 0.4158586859703064
train_iter_loss: 0.2854856550693512
train_iter_loss: 0.19425921142101288
train_iter_loss: 0.37034323811531067
train_iter_loss: 0.3970080316066742
train_iter_loss: 0.31552061438560486
train_iter_loss: 0.4652503728866577
train_iter_loss: 0.3360246419906616
train_iter_loss: 0.26268085837364197
train_iter_loss: 0.3079569339752197
train_iter_loss: 0.3120881915092468
train_iter_loss: 0.26755502820014954
train_iter_loss: 0.3487361967563629
train_iter_loss: 0.244050994515419
train_iter_loss: 0.389153391122818
train_iter_loss: 0.3815089464187622
train_iter_loss: 0.32023873925209045
train_iter_loss: 0.46282681822776794
train_iter_loss: 0.3382783830165863
train_iter_loss: 0.3014674484729767
train_iter_loss: 0.3611980378627777
train_iter_loss: 0.27074211835861206
train_iter_loss: 0.34070077538490295
train_iter_loss: 0.32096609473228455
train_iter_loss: 0.2470979392528534
train_iter_loss: 0.2539052367210388
train_iter_loss: 0.36886918544769287
train_iter_loss: 0.3247532546520233
train_iter_loss: 0.3236822187900543
train_iter_loss: 0.34155911207199097
train_iter_loss: 0.322765588760376
train_iter_loss: 0.3414451479911804
train_iter_loss: 0.45754000544548035
train_iter_loss: 0.4182996451854706
train_iter_loss: 0.38649457693099976
train_iter_loss: 0.17319194972515106
train_iter_loss: 0.45459312200546265
train_iter_loss: 0.37476810812950134
train_iter_loss: 0.31214335560798645
train_iter_loss: 0.22711582481861115
train_iter_loss: 0.296791672706604
train_iter_loss: 0.46272408962249756
train_iter_loss: 0.3771379888057709
train_iter_loss: 0.2942203879356384
train_iter_loss: 0.28143876791000366
train_iter_loss: 0.44375506043434143
train_iter_loss: 0.28096452355384827
train_iter_loss: 0.2120579183101654
train_iter_loss: 0.27163615822792053
train_iter_loss: 0.40681594610214233
train_iter_loss: 0.38436999917030334
train_iter_loss: 0.38326171040534973
train_iter_loss: 0.4522283673286438
train_iter_loss: 0.25075793266296387
train_iter_loss: 0.1903659999370575
train_iter_loss: 0.5256913900375366
train_iter_loss: 0.27363157272338867
train_iter_loss: 0.27950847148895264
train_iter_loss: 0.26234737038612366
train_iter_loss: 0.33921393752098083
train_iter_loss: 0.2868078351020813
train_iter_loss: 0.2593573033809662
train_iter_loss: 0.29377561807632446
train_iter_loss: 0.32812413573265076
train_iter_loss: 0.3385528028011322
train_iter_loss: 0.32765305042266846
train_iter_loss: 0.2929569184780121
train_iter_loss: 0.2530617117881775
train_iter_loss: 0.28998810052871704
train_iter_loss: 0.32349950075149536
train_iter_loss: 0.37515145540237427
train_iter_loss: 0.36910179257392883
train_iter_loss: 0.3483434021472931
train loss :0.3407
---------------------
Validation seg loss: 0.4071295648321228 at epoch 33
epoch =     34/  1000, exp = train
train_iter_loss: 0.3362124264240265
train_iter_loss: 0.3764738440513611
train_iter_loss: 0.3137684762477875
train_iter_loss: 0.4752007722854614
train_iter_loss: 0.35030055046081543
train_iter_loss: 0.34576937556266785
train_iter_loss: 0.35167625546455383
train_iter_loss: 0.3580586612224579
train_iter_loss: 0.40013155341148376
train_iter_loss: 0.3079468905925751
train_iter_loss: 0.417307049036026
train_iter_loss: 0.298959344625473
train_iter_loss: 0.2715485095977783
train_iter_loss: 0.32340341806411743
train_iter_loss: 0.2563624978065491
train_iter_loss: 0.26402032375335693
train_iter_loss: 0.21938258409500122
train_iter_loss: 0.3977700173854828
train_iter_loss: 0.37617188692092896
train_iter_loss: 0.26053863763809204
train_iter_loss: 0.2698762118816376
train_iter_loss: 0.37971535325050354
train_iter_loss: 0.3383021652698517
train_iter_loss: 0.21385714411735535
train_iter_loss: 0.3571150302886963
train_iter_loss: 0.37445250153541565
train_iter_loss: 0.40039366483688354
train_iter_loss: 0.27088385820388794
train_iter_loss: 0.20780128240585327
train_iter_loss: 0.2964619994163513
train_iter_loss: 0.48518964648246765
train_iter_loss: 0.36044982075691223
train_iter_loss: 0.3065207898616791
train_iter_loss: 0.2834493815898895
train_iter_loss: 0.3678636848926544
train_iter_loss: 0.20962755382061005
train_iter_loss: 0.3640230596065521
train_iter_loss: 0.3718820810317993
train_iter_loss: 0.33901822566986084
train_iter_loss: 0.23127999901771545
train_iter_loss: 0.3593624532222748
train_iter_loss: 0.31262192130088806
train_iter_loss: 0.21679361164569855
train_iter_loss: 0.46674972772598267
train_iter_loss: 0.36401641368865967
train_iter_loss: 0.3187674283981323
train_iter_loss: 0.2784944474697113
train_iter_loss: 0.3437528610229492
train_iter_loss: 0.32180216908454895
train_iter_loss: 0.19647550582885742
train_iter_loss: 0.5383213758468628
train_iter_loss: 0.3162310719490051
train_iter_loss: 0.3381142020225525
train_iter_loss: 0.3056075870990753
train_iter_loss: 0.3111976385116577
train_iter_loss: 0.35757285356521606
train_iter_loss: 0.30147820711135864
train_iter_loss: 0.4508069157600403
train_iter_loss: 0.34212663769721985
train_iter_loss: 0.4053865969181061
train_iter_loss: 0.23446115851402283
train_iter_loss: 0.2891966700553894
train_iter_loss: 0.45495712757110596
train_iter_loss: 0.31588104367256165
train_iter_loss: 0.31972846388816833
train_iter_loss: 0.27320703864097595
train_iter_loss: 0.28375208377838135
train_iter_loss: 0.37776437401771545
train_iter_loss: 0.32600724697113037
train_iter_loss: 0.3890776038169861
train_iter_loss: 0.24056115746498108
train_iter_loss: 0.3529004454612732
train_iter_loss: 0.34004372358322144
train_iter_loss: 0.3642708361148834
train_iter_loss: 0.4533720016479492
train_iter_loss: 0.3029789626598358
train_iter_loss: 0.2533006966114044
train_iter_loss: 0.2598937153816223
train_iter_loss: 0.1901848465204239
train_iter_loss: 0.30627235770225525
train_iter_loss: 0.24903851747512817
train_iter_loss: 0.47426658868789673
train_iter_loss: 0.423144668340683
train_iter_loss: 0.3654857575893402
train_iter_loss: 0.3839089870452881
train_iter_loss: 0.5628140568733215
train_iter_loss: 0.2728266716003418
train_iter_loss: 0.4809581935405731
train_iter_loss: 0.14695611596107483
train_iter_loss: 0.4215688407421112
train_iter_loss: 0.36710137128829956
train_iter_loss: 0.45099884271621704
train_iter_loss: 0.45579689741134644
train_iter_loss: 0.290727436542511
train_iter_loss: 0.3645441234111786
train_iter_loss: 0.33307868242263794
train_iter_loss: 0.244632750749588
train_iter_loss: 0.2479807436466217
train_iter_loss: 0.42712077498435974
train_iter_loss: 0.3885473906993866
train loss :0.3399
---------------------
Validation seg loss: 0.41243891017335765 at epoch 34
epoch =     35/  1000, exp = train
train_iter_loss: 0.30639827251434326
train_iter_loss: 0.36586740612983704
train_iter_loss: 0.28576910495758057
train_iter_loss: 0.3199692666530609
train_iter_loss: 0.37470465898513794
train_iter_loss: 0.33325430750846863
train_iter_loss: 0.40537750720977783
train_iter_loss: 0.3595496118068695
train_iter_loss: 0.3728621006011963
train_iter_loss: 0.22982141375541687
train_iter_loss: 0.4373505413532257
train_iter_loss: 0.3501325845718384
train_iter_loss: 0.31869179010391235
train_iter_loss: 0.3191298246383667
train_iter_loss: 0.4005645215511322
train_iter_loss: 0.3071290850639343
train_iter_loss: 0.34332624077796936
train_iter_loss: 0.32359838485717773
train_iter_loss: 0.3507481813430786
train_iter_loss: 0.27219143509864807
train_iter_loss: 0.18726254999637604
train_iter_loss: 0.2419346272945404
train_iter_loss: 0.3368707597255707
train_iter_loss: 0.29003798961639404
train_iter_loss: 0.3875412344932556
train_iter_loss: 0.31692755222320557
train_iter_loss: 0.3304169476032257
train_iter_loss: 0.3369346261024475
train_iter_loss: 0.2798783481121063
train_iter_loss: 0.3179638385772705
train_iter_loss: 0.23550085723400116
train_iter_loss: 0.33010149002075195
train_iter_loss: 0.3230270445346832
train_iter_loss: 0.31247758865356445
train_iter_loss: 0.3827148675918579
train_iter_loss: 0.2894989252090454
train_iter_loss: 0.3726882338523865
train_iter_loss: 0.30141758918762207
train_iter_loss: 0.4597679078578949
train_iter_loss: 0.27000102400779724
train_iter_loss: 0.27704107761383057
train_iter_loss: 0.32965463399887085
train_iter_loss: 0.23492829501628876
train_iter_loss: 0.3935587704181671
train_iter_loss: 0.4052826166152954
train_iter_loss: 0.47696560621261597
train_iter_loss: 0.373101145029068
train_iter_loss: 0.29425108432769775
train_iter_loss: 0.5173694491386414
train_iter_loss: 0.40539222955703735
train_iter_loss: 0.30673450231552124
train_iter_loss: 0.31189534068107605
train_iter_loss: 0.43938925862312317
train_iter_loss: 0.2595120668411255
train_iter_loss: 0.26161453127861023
train_iter_loss: 0.36979880928993225
train_iter_loss: 0.2424643635749817
train_iter_loss: 0.3701399564743042
train_iter_loss: 0.3014790117740631
train_iter_loss: 0.2622888684272766
train_iter_loss: 0.4547193944454193
train_iter_loss: 0.3109535872936249
train_iter_loss: 0.4026980400085449
train_iter_loss: 0.27960988879203796
train_iter_loss: 0.31693950295448303
train_iter_loss: 0.3680117130279541
train_iter_loss: 0.38717156648635864
train_iter_loss: 0.23018544912338257
train_iter_loss: 0.3900368809700012
train_iter_loss: 0.30147066712379456
train_iter_loss: 0.27718856930732727
train_iter_loss: 0.26547273993492126
train_iter_loss: 0.38755232095718384
train_iter_loss: 0.30247315764427185
train_iter_loss: 0.34620043635368347
train_iter_loss: 0.3107272982597351
train_iter_loss: 0.36921030282974243
train_iter_loss: 0.40041548013687134
train_iter_loss: 0.3612191677093506
train_iter_loss: 0.27706843614578247
train_iter_loss: 0.33954548835754395
train_iter_loss: 0.36605361104011536
train_iter_loss: 0.30085238814353943
train_iter_loss: 0.6181109547615051
train_iter_loss: 0.4898780882358551
train_iter_loss: 0.40958210825920105
train_iter_loss: 0.3352237939834595
train_iter_loss: 0.4308152496814728
train_iter_loss: 0.41795092821121216
train_iter_loss: 0.36502954363822937
train_iter_loss: 0.3951440751552582
train_iter_loss: 0.25216513872146606
train_iter_loss: 0.24296484887599945
train_iter_loss: 0.5804258584976196
train_iter_loss: 0.2846744656562805
train_iter_loss: 0.36371660232543945
train_iter_loss: 0.33502382040023804
train_iter_loss: 0.41869643330574036
train_iter_loss: 0.24479034543037415
train_iter_loss: 0.24291372299194336
train loss :0.3443
---------------------
Validation seg loss: 0.39947292356277414 at epoch 35
********************
best_val_epoch_loss:  0.39947292356277414
MODEL UPDATED
epoch =     36/  1000, exp = train
train_iter_loss: 0.42614591121673584
train_iter_loss: 0.2995949983596802
train_iter_loss: 0.31668853759765625
train_iter_loss: 0.29792162775993347
train_iter_loss: 0.2539083659648895
train_iter_loss: 0.44897618889808655
train_iter_loss: 0.343128502368927
train_iter_loss: 0.34408700466156006
train_iter_loss: 0.3228932023048401
train_iter_loss: 0.3536139726638794
train_iter_loss: 0.3312300145626068
train_iter_loss: 0.34125250577926636
train_iter_loss: 0.30588647723197937
train_iter_loss: 0.46865010261535645
train_iter_loss: 0.35171735286712646
train_iter_loss: 0.3782825469970703
train_iter_loss: 0.31320062279701233
train_iter_loss: 0.45737162232398987
train_iter_loss: 0.355118989944458
train_iter_loss: 0.37245166301727295
train_iter_loss: 0.3229535222053528
train_iter_loss: 0.39005419611930847
train_iter_loss: 0.3883652091026306
train_iter_loss: 0.23185274004936218
train_iter_loss: 0.34149807691574097
train_iter_loss: 0.2920560836791992
train_iter_loss: 0.36194074153900146
train_iter_loss: 0.3229442238807678
train_iter_loss: 0.2623600363731384
train_iter_loss: 0.2818167507648468
train_iter_loss: 0.240397647023201
train_iter_loss: 0.41857588291168213
train_iter_loss: 0.380808562040329
train_iter_loss: 0.20570965111255646
train_iter_loss: 0.2606111764907837
train_iter_loss: 0.29748743772506714
train_iter_loss: 0.34479716420173645
train_iter_loss: 0.3555966913700104
train_iter_loss: 0.15340159833431244
train_iter_loss: 0.3050217032432556
train_iter_loss: 0.25880637764930725
train_iter_loss: 0.3902086913585663
train_iter_loss: 0.3096061944961548
train_iter_loss: 0.34459418058395386
train_iter_loss: 0.2311743199825287
train_iter_loss: 0.3997837007045746
train_iter_loss: 0.30895423889160156
train_iter_loss: 0.41475313901901245
train_iter_loss: 0.3803681433200836
train_iter_loss: 0.29303213953971863
train_iter_loss: 0.3653210997581482
train_iter_loss: 0.2903096377849579
train_iter_loss: 0.37603092193603516
train_iter_loss: 0.32247835397720337
train_iter_loss: 0.3051539957523346
train_iter_loss: 0.2842857837677002
train_iter_loss: 0.2815607786178589
train_iter_loss: 0.35085397958755493
train_iter_loss: 0.4028319716453552
train_iter_loss: 0.33820512890815735
train_iter_loss: 0.45537060499191284
train_iter_loss: 0.3482167720794678
train_iter_loss: 0.3734455108642578
train_iter_loss: 0.30515164136886597
train_iter_loss: 0.569564163684845
train_iter_loss: 0.20306891202926636
train_iter_loss: 0.31888553500175476
train_iter_loss: 0.3584945499897003
train_iter_loss: 0.34662315249443054
train_iter_loss: 0.2824924886226654
train_iter_loss: 0.26394227147102356
train_iter_loss: 0.2745433449745178
train_iter_loss: 0.38727879524230957
train_iter_loss: 0.3396393656730652
train_iter_loss: 0.2707967460155487
train_iter_loss: 0.36257433891296387
train_iter_loss: 0.34265273809432983
train_iter_loss: 0.48939988017082214
train_iter_loss: 0.30852818489074707
train_iter_loss: 0.3239463269710541
train_iter_loss: 0.2034967839717865
train_iter_loss: 0.32041364908218384
train_iter_loss: 0.2812236249446869
train_iter_loss: 0.5647296905517578
train_iter_loss: 0.2746768891811371
train_iter_loss: 0.3199504315853119
train_iter_loss: 0.38579636812210083
train_iter_loss: 0.3276755213737488
train_iter_loss: 0.35067999362945557
train_iter_loss: 0.2864159345626831
train_iter_loss: 0.3568324148654938
train_iter_loss: 0.40516453981399536
train_iter_loss: 0.241932213306427
train_iter_loss: 0.470836877822876
train_iter_loss: 0.3536123037338257
train_iter_loss: 0.3985827565193176
train_iter_loss: 0.3975282311439514
train_iter_loss: 0.4236357510089874
train_iter_loss: 0.4924885034561157
train_iter_loss: 0.2123917043209076
train loss :0.3423
---------------------
Validation seg loss: 0.4240070737125176 at epoch 36
epoch =     37/  1000, exp = train
train_iter_loss: 0.30936795473098755
train_iter_loss: 0.3123123347759247
train_iter_loss: 0.47658422589302063
train_iter_loss: 0.24103358387947083
train_iter_loss: 0.4079055190086365
train_iter_loss: 0.30971869826316833
train_iter_loss: 0.28599539399147034
train_iter_loss: 0.7155104875564575
train_iter_loss: 0.4517875909805298
train_iter_loss: 0.3599865436553955
train_iter_loss: 0.2955566346645355
train_iter_loss: 0.317707359790802
train_iter_loss: 0.3593411445617676
train_iter_loss: 0.2735694944858551
train_iter_loss: 0.2644767761230469
train_iter_loss: 0.3368108868598938
train_iter_loss: 0.2710181176662445
train_iter_loss: 0.3880857527256012
train_iter_loss: 0.4966605603694916
train_iter_loss: 0.45450612902641296
train_iter_loss: 0.2872253954410553
train_iter_loss: 0.3775438070297241
train_iter_loss: 0.2829914093017578
train_iter_loss: 0.21724851429462433
train_iter_loss: 0.26247549057006836
train_iter_loss: 0.36870822310447693
train_iter_loss: 0.28370001912117004
train_iter_loss: 0.3160179555416107
train_iter_loss: 0.263944149017334
train_iter_loss: 0.36685147881507874
train_iter_loss: 0.2879810929298401
train_iter_loss: 0.2903076708316803
train_iter_loss: 0.41147762537002563
train_iter_loss: 0.29809263348579407
train_iter_loss: 0.2696143090724945
train_iter_loss: 0.4459993541240692
train_iter_loss: 0.4521995186805725
train_iter_loss: 0.4707027077674866
train_iter_loss: 0.29957103729248047
train_iter_loss: 0.42734622955322266
train_iter_loss: 0.5101439952850342
train_iter_loss: 0.42502301931381226
train_iter_loss: 0.23110681772232056
train_iter_loss: 0.19270923733711243
train_iter_loss: 0.23241491615772247
train_iter_loss: 0.21369712054729462
train_iter_loss: 0.358204185962677
train_iter_loss: 0.4330175220966339
train_iter_loss: 0.39371830224990845
train_iter_loss: 0.26540079712867737
train_iter_loss: 0.3311769664287567
train_iter_loss: 0.3247228264808655
train_iter_loss: 0.34317880868911743
train_iter_loss: 0.36169591546058655
train_iter_loss: 0.4006909430027008
train_iter_loss: 0.3257743716239929
train_iter_loss: 0.2719683051109314
train_iter_loss: 0.4455064833164215
train_iter_loss: 0.3304380774497986
train_iter_loss: 0.2978055477142334
train_iter_loss: 0.3955940902233124
train_iter_loss: 0.4060022234916687
train_iter_loss: 0.3481227457523346
train_iter_loss: 0.3328564465045929
train_iter_loss: 0.33615878224372864
train_iter_loss: 0.4182511568069458
train_iter_loss: 0.3468359410762787
train_iter_loss: 0.3560205399990082
train_iter_loss: 0.31622159481048584
train_iter_loss: 0.2036958634853363
train_iter_loss: 0.2520756721496582
train_iter_loss: 0.41787609457969666
train_iter_loss: 0.24083611369132996
train_iter_loss: 0.28077101707458496
train_iter_loss: 0.2489718198776245
train_iter_loss: 0.1992824524641037
train_iter_loss: 0.2250746786594391
train_iter_loss: 0.16758663952350616
train_iter_loss: 0.2773325443267822
train_iter_loss: 0.48822057247161865
train_iter_loss: 0.37673115730285645
train_iter_loss: 0.3674054443836212
train_iter_loss: 0.30214646458625793
train_iter_loss: 0.3018632233142853
train_iter_loss: 0.32909876108169556
train_iter_loss: 0.32524573802948
train_iter_loss: 0.3195858597755432
train_iter_loss: 0.31933435797691345
train_iter_loss: 0.3182440400123596
train_iter_loss: 0.23560118675231934
train_iter_loss: 0.22333666682243347
train_iter_loss: 0.23525086045265198
train_iter_loss: 0.33848944306373596
train_iter_loss: 0.2808213233947754
train_iter_loss: 0.33410099148750305
train_iter_loss: 0.346968412399292
train_iter_loss: 0.38386616110801697
train_iter_loss: 0.32232388854026794
train_iter_loss: 0.3098836839199066
train_iter_loss: 0.19291552901268005
train loss :0.3346
---------------------
Validation seg loss: 0.39704894453708856 at epoch 37
********************
best_val_epoch_loss:  0.39704894453708856
MODEL UPDATED
epoch =     38/  1000, exp = train
train_iter_loss: 0.3630836009979248
train_iter_loss: 0.22681477665901184
train_iter_loss: 0.34110650420188904
train_iter_loss: 0.4224047064781189
train_iter_loss: 0.3240642249584198
train_iter_loss: 0.275382936000824
train_iter_loss: 0.279326856136322
train_iter_loss: 0.36835598945617676
train_iter_loss: 0.41412627696990967
train_iter_loss: 0.2227294147014618
train_iter_loss: 0.2916851341724396
train_iter_loss: 0.38742291927337646
train_iter_loss: 0.3243737816810608
train_iter_loss: 0.38153183460235596
train_iter_loss: 0.39457887411117554
train_iter_loss: 0.20490936934947968
train_iter_loss: 0.2030288130044937
train_iter_loss: 0.18145528435707092
train_iter_loss: 0.32760655879974365
train_iter_loss: 0.34684666991233826
train_iter_loss: 0.22283941507339478
train_iter_loss: 0.22707557678222656
train_iter_loss: 0.29849764704704285
train_iter_loss: 0.20027610659599304
train_iter_loss: 0.349147230386734
train_iter_loss: 0.4155576527118683
train_iter_loss: 0.3330393135547638
train_iter_loss: 0.5175278782844543
train_iter_loss: 0.32756009697914124
train_iter_loss: 0.2433205246925354
train_iter_loss: 0.3349079191684723
train_iter_loss: 0.48530009388923645
train_iter_loss: 0.3886864185333252
train_iter_loss: 0.343749463558197
train_iter_loss: 0.4736160933971405
train_iter_loss: 0.29012584686279297
train_iter_loss: 0.33097678422927856
train_iter_loss: 0.24219156801700592
train_iter_loss: 0.34934496879577637
train_iter_loss: 0.37855565547943115
train_iter_loss: 0.38140496611595154
train_iter_loss: 0.44510847330093384
train_iter_loss: 0.3183068633079529
train_iter_loss: 0.4183241128921509
train_iter_loss: 0.4398285150527954
train_iter_loss: 0.31593382358551025
train_iter_loss: 0.3047226667404175
train_iter_loss: 0.3973340392112732
train_iter_loss: 0.2948037385940552
train_iter_loss: 0.2833247780799866
train_iter_loss: 0.42464789748191833
train_iter_loss: 0.30651041865348816
train_iter_loss: 0.42169952392578125
train_iter_loss: 0.2928038537502289
train_iter_loss: 0.2731066346168518
train_iter_loss: 0.3539120554924011
train_iter_loss: 0.23047490417957306
train_iter_loss: 0.31043484807014465
train_iter_loss: 0.42829692363739014
train_iter_loss: 0.29181498289108276
train_iter_loss: 0.5313770771026611
train_iter_loss: 0.3469664454460144
train_iter_loss: 0.3951396942138672
train_iter_loss: 0.32779935002326965
train_iter_loss: 0.2659684717655182
train_iter_loss: 0.3487168252468109
train_iter_loss: 0.280443012714386
train_iter_loss: 0.2905343770980835
train_iter_loss: 0.401114821434021
train_iter_loss: 0.3734874129295349
train_iter_loss: 0.19018369913101196
train_iter_loss: 0.39001256227493286
train_iter_loss: 0.3620694875717163
train_iter_loss: 0.35230880975723267
train_iter_loss: 0.263540655374527
train_iter_loss: 0.31228578090667725
train_iter_loss: 0.3035113513469696
train_iter_loss: 0.2750592529773712
train_iter_loss: 0.314841628074646
train_iter_loss: 0.30621254444122314
train_iter_loss: 0.2247706800699234
train_iter_loss: 0.3963547945022583
train_iter_loss: 0.33697330951690674
train_iter_loss: 0.3139744997024536
train_iter_loss: 0.3452804982662201
train_iter_loss: 0.26911285519599915
train_iter_loss: 0.2381560504436493
train_iter_loss: 0.23568953573703766
train_iter_loss: 0.34216779470443726
train_iter_loss: 0.2894664406776428
train_iter_loss: 0.33741334080696106
train_iter_loss: 0.234893336892128
train_iter_loss: 0.36863213777542114
train_iter_loss: 0.4413767457008362
train_iter_loss: 0.35292255878448486
train_iter_loss: 0.3983268141746521
train_iter_loss: 0.4194648563861847
train_iter_loss: 0.4337056279182434
train_iter_loss: 0.4571985602378845
train_iter_loss: 0.2538726329803467
train loss :0.3362
---------------------
Validation seg loss: 0.39326126103834164 at epoch 38
********************
best_val_epoch_loss:  0.39326126103834164
MODEL UPDATED
epoch =     39/  1000, exp = train
train_iter_loss: 0.38445064425468445
train_iter_loss: 0.15384051203727722
train_iter_loss: 0.2913054823875427
train_iter_loss: 0.23886866867542267
train_iter_loss: 0.4446338713169098
train_iter_loss: 0.37445470690727234
train_iter_loss: 0.3748895823955536
train_iter_loss: 0.304335355758667
train_iter_loss: 0.38174644112586975
train_iter_loss: 0.3023512661457062
train_iter_loss: 0.2895177900791168
train_iter_loss: 0.3306426703929901
train_iter_loss: 0.40518373250961304
train_iter_loss: 0.23003573715686798
train_iter_loss: 0.34070178866386414
train_iter_loss: 0.26568982005119324
train_iter_loss: 0.3746316432952881
train_iter_loss: 0.38462620973587036
train_iter_loss: 0.394578754901886
train_iter_loss: 0.26007530093193054
train_iter_loss: 0.358673632144928
train_iter_loss: 0.3268035352230072
train_iter_loss: 0.3125162124633789
train_iter_loss: 0.21971628069877625
train_iter_loss: 0.369209885597229
train_iter_loss: 0.39789333939552307
train_iter_loss: 0.24542547762393951
train_iter_loss: 0.34282296895980835
train_iter_loss: 0.28118130564689636
train_iter_loss: 0.28811562061309814
train_iter_loss: 0.3408520221710205
train_iter_loss: 0.29315248131752014
train_iter_loss: 0.3275178372859955
train_iter_loss: 0.3418024182319641
train_iter_loss: 0.1951828896999359
train_iter_loss: 0.2532508969306946
train_iter_loss: 0.4136560261249542
train_iter_loss: 0.3213024139404297
train_iter_loss: 0.1896190345287323
train_iter_loss: 0.29711592197418213
train_iter_loss: 0.4519359767436981
train_iter_loss: 0.41573137044906616
train_iter_loss: 0.2953697144985199
train_iter_loss: 0.370434045791626
train_iter_loss: 0.3460015058517456
train_iter_loss: 0.38130301237106323
train_iter_loss: 0.3290448486804962
train_iter_loss: 0.3675765097141266
train_iter_loss: 0.4827652871608734
train_iter_loss: 0.2992972433567047
train_iter_loss: 0.3374689519405365
train_iter_loss: 0.2467149943113327
train_iter_loss: 0.21453170478343964
train_iter_loss: 0.3091241717338562
train_iter_loss: 0.4393810033798218
train_iter_loss: 0.19937966763973236
train_iter_loss: 0.22049303352832794
train_iter_loss: 0.338274210691452
train_iter_loss: 0.35991254448890686
train_iter_loss: 0.35505035519599915
train_iter_loss: 0.2560429573059082
train_iter_loss: 0.26382777094841003
train_iter_loss: 0.3642978370189667
train_iter_loss: 0.45195549726486206
train_iter_loss: 0.2612746059894562
train_iter_loss: 0.2893478274345398
train_iter_loss: 0.38574808835983276
train_iter_loss: 0.4063948094844818
train_iter_loss: 0.4055272340774536
train_iter_loss: 0.2135952264070511
train_iter_loss: 0.4521064758300781
train_iter_loss: 0.3308566212654114
train_iter_loss: 0.20371346175670624
train_iter_loss: 0.29242953658103943
train_iter_loss: 0.3984982669353485
train_iter_loss: 0.34859782457351685
train_iter_loss: 0.34473860263824463
train_iter_loss: 0.27270475029945374
train_iter_loss: 0.24446643888950348
train_iter_loss: 0.2943694591522217
train_iter_loss: 0.3312097489833832
train_iter_loss: 0.18763497471809387
train_iter_loss: 0.479828417301178
train_iter_loss: 0.4588368833065033
train_iter_loss: 0.4238271713256836
train_iter_loss: 0.42949575185775757
train_iter_loss: 0.3753412961959839
train_iter_loss: 0.406751811504364
train_iter_loss: 0.478962242603302
train_iter_loss: 0.4492962062358856
train_iter_loss: 0.2672196924686432
train_iter_loss: 0.2833271622657776
train_iter_loss: 0.33611229062080383
train_iter_loss: 0.4388021230697632
train_iter_loss: 0.4239378869533539
train_iter_loss: 0.3122888505458832
train_iter_loss: 0.361178994178772
train_iter_loss: 0.3977852761745453
train_iter_loss: 0.4006648063659668
train_iter_loss: 0.3286834955215454
train loss :0.3379
---------------------
Validation seg loss: 0.4102117120165308 at epoch 39
epoch =     40/  1000, exp = train
train_iter_loss: 0.3385716676712036
train_iter_loss: 0.3246263861656189
train_iter_loss: 0.3069748878479004
train_iter_loss: 0.371341347694397
train_iter_loss: 0.2649674117565155
train_iter_loss: 0.35556453466415405
train_iter_loss: 0.2869209349155426
train_iter_loss: 0.3328687846660614
train_iter_loss: 0.23005463182926178
train_iter_loss: 0.37216106057167053
train_iter_loss: 0.4571886360645294
train_iter_loss: 0.35603785514831543
train_iter_loss: 0.31687575578689575
train_iter_loss: 0.31359684467315674
train_iter_loss: 0.2940498888492584
train_iter_loss: 0.2697884738445282
train_iter_loss: 0.4518960416316986
train_iter_loss: 0.22866606712341309
train_iter_loss: 0.2532772421836853
train_iter_loss: 0.41112810373306274
train_iter_loss: 0.27553802728652954
train_iter_loss: 0.3172973394393921
train_iter_loss: 0.45441576838493347
train_iter_loss: 0.3084590435028076
train_iter_loss: 0.4459594786167145
train_iter_loss: 0.29109296202659607
train_iter_loss: 0.37794914841651917
train_iter_loss: 0.29246631264686584
train_iter_loss: 0.24404095113277435
train_iter_loss: 0.3587026596069336
train_iter_loss: 0.2636832892894745
train_iter_loss: 0.30754631757736206
train_iter_loss: 0.2657768726348877
train_iter_loss: 0.2771908640861511
train_iter_loss: 0.28845280408859253
train_iter_loss: 0.3864380121231079
train_iter_loss: 0.28772297501564026
train_iter_loss: 0.2470078021287918
train_iter_loss: 0.3398774564266205
train_iter_loss: 0.32376840710639954
train_iter_loss: 0.3545738756656647
train_iter_loss: 0.35205379128456116
train_iter_loss: 0.28872066736221313
train_iter_loss: 0.34895238280296326
train_iter_loss: 0.31429266929626465
train_iter_loss: 0.34794607758522034
train_iter_loss: 0.2628038227558136
train_iter_loss: 0.36245834827423096
train_iter_loss: 0.2455739825963974
train_iter_loss: 0.4837211072444916
train_iter_loss: 0.3990499973297119
train_iter_loss: 0.4733203053474426
train_iter_loss: 0.3026101589202881
train_iter_loss: 0.413241982460022
train_iter_loss: 0.23609942197799683
train_iter_loss: 0.3333395719528198
train_iter_loss: 0.38502055406570435
train_iter_loss: 0.23161673545837402
train_iter_loss: 0.4172004759311676
train_iter_loss: 0.5011209845542908
train_iter_loss: 0.3683576285839081
train_iter_loss: 0.2910114824771881
train_iter_loss: 0.38615530729293823
train_iter_loss: 0.29524341225624084
train_iter_loss: 0.32328087091445923
train_iter_loss: 0.40389734506607056
train_iter_loss: 0.4024800658226013
train_iter_loss: 0.368324875831604
train_iter_loss: 0.2833738923072815
train_iter_loss: 0.2545888423919678
train_iter_loss: 0.39263996481895447
train_iter_loss: 0.41086751222610474
train_iter_loss: 0.3299226760864258
train_iter_loss: 0.40752658247947693
train_iter_loss: 0.32025083899497986
train_iter_loss: 0.322459876537323
train_iter_loss: 0.35448190569877625
train_iter_loss: 0.3318915069103241
train_iter_loss: 0.27565300464630127
train_iter_loss: 0.40251147747039795
train_iter_loss: 0.20674993097782135
train_iter_loss: 0.30411297082901
train_iter_loss: 0.32668420672416687
train_iter_loss: 0.26902008056640625
train_iter_loss: 0.12925146520137787
train_iter_loss: 0.23674172163009644
train_iter_loss: 0.2866383492946625
train_iter_loss: 0.3147759735584259
train_iter_loss: 0.3434554636478424
train_iter_loss: 0.3260877728462219
train_iter_loss: 0.44768381118774414
train_iter_loss: 0.28442978858947754
train_iter_loss: 0.3166196942329407
train_iter_loss: 0.2904500365257263
train_iter_loss: 0.37958329916000366
train_iter_loss: 0.42173510789871216
train_iter_loss: 0.2195354849100113
train_iter_loss: 0.43566298484802246
train_iter_loss: 0.40934649109840393
train_iter_loss: 0.2710009813308716
train loss :0.3343
---------------------
Validation seg loss: 0.4019269645565523 at epoch 40
epoch =     41/  1000, exp = train
train_iter_loss: 0.529194712638855
train_iter_loss: 0.22239045798778534
train_iter_loss: 0.30558091402053833
train_iter_loss: 0.327729195356369
train_iter_loss: 0.3183682858943939
train_iter_loss: 0.4212397038936615
train_iter_loss: 0.28071486949920654
train_iter_loss: 0.22812619805335999
train_iter_loss: 0.3210650682449341
train_iter_loss: 0.23390619456768036
train_iter_loss: 0.3224966526031494
train_iter_loss: 0.32769572734832764
train_iter_loss: 0.32442033290863037
train_iter_loss: 0.45199841260910034
train_iter_loss: 0.23369741439819336
train_iter_loss: 0.3643133044242859
train_iter_loss: 0.2509995698928833
train_iter_loss: 0.2785792350769043
train_iter_loss: 0.20542502403259277
train_iter_loss: 0.33130943775177
train_iter_loss: 0.3349430561065674
train_iter_loss: 0.3218494653701782
train_iter_loss: 0.37498044967651367
train_iter_loss: 0.4228908121585846
train_iter_loss: 0.22112324833869934
train_iter_loss: 0.2556927800178528
train_iter_loss: 0.36721840500831604
train_iter_loss: 0.2586364448070526
train_iter_loss: 0.25254136323928833
train_iter_loss: 0.33112770318984985
train_iter_loss: 0.30539044737815857
train_iter_loss: 0.34200355410575867
train_iter_loss: 0.35927140712738037
train_iter_loss: 0.3148616850376129
train_iter_loss: 0.2841777503490448
train_iter_loss: 0.3757411539554596
train_iter_loss: 0.2722387909889221
train_iter_loss: 0.35745468735694885
train_iter_loss: 0.2801057994365692
train_iter_loss: 0.3177851140499115
train_iter_loss: 0.3104124069213867
train_iter_loss: 0.3139907419681549
train_iter_loss: 0.3312360942363739
train_iter_loss: 0.4107253849506378
train_iter_loss: 0.43901240825653076
train_iter_loss: 0.3025393486022949
train_iter_loss: 0.21651816368103027
train_iter_loss: 0.21609319746494293
train_iter_loss: 0.4578491151332855
train_iter_loss: 0.25293466448783875
train_iter_loss: 0.3239966630935669
train_iter_loss: 0.18728527426719666
train_iter_loss: 0.20753756165504456
train_iter_loss: 0.36641696095466614
train_iter_loss: 0.27434277534484863
train_iter_loss: 0.34624698758125305
train_iter_loss: 0.3105444312095642
train_iter_loss: 0.3172672986984253
train_iter_loss: 0.5668095350265503
train_iter_loss: 0.2639036774635315
train_iter_loss: 0.2710259258747101
train_iter_loss: 0.3189793527126312
train_iter_loss: 0.28642579913139343
train_iter_loss: 0.271397203207016
train_iter_loss: 0.25551944971084595
train_iter_loss: 0.21215684711933136
train_iter_loss: 0.480935662984848
train_iter_loss: 0.3553880751132965
train_iter_loss: 0.2633225619792938
train_iter_loss: 0.37996160984039307
train_iter_loss: 0.33905065059661865
train_iter_loss: 0.3003835678100586
train_iter_loss: 0.2275266945362091
train_iter_loss: 0.4584603011608124
train_iter_loss: 0.3394610583782196
train_iter_loss: 0.3774355351924896
train_iter_loss: 0.4546873867511749
train_iter_loss: 0.22573783993721008
train_iter_loss: 0.3871850073337555
train_iter_loss: 0.41386598348617554
train_iter_loss: 0.4768257439136505
train_iter_loss: 0.3736461102962494
train_iter_loss: 0.3601345717906952
train_iter_loss: 0.4508030116558075
train_iter_loss: 0.41360604763031006
train_iter_loss: 0.3481745719909668
train_iter_loss: 0.30532145500183105
train_iter_loss: 0.4152565896511078
train_iter_loss: 0.24665237963199615
train_iter_loss: 0.3365592956542969
train_iter_loss: 0.19925856590270996
train_iter_loss: 0.36518990993499756
train_iter_loss: 0.29189956188201904
train_iter_loss: 0.2821331024169922
train_iter_loss: 0.34779322147369385
train_iter_loss: 0.46257513761520386
train_iter_loss: 0.5513521432876587
train_iter_loss: 0.48313915729522705
train_iter_loss: 0.239662766456604
train_iter_loss: 0.23701560497283936
train loss :0.3319
---------------------
Validation seg loss: 0.39256506599485874 at epoch 41
********************
best_val_epoch_loss:  0.39256506599485874
MODEL UPDATED
epoch =     42/  1000, exp = train
train_iter_loss: 0.2716295123100281
train_iter_loss: 0.3514985740184784
train_iter_loss: 0.4213714599609375
train_iter_loss: 0.2769472002983093
train_iter_loss: 0.2964353859424591
train_iter_loss: 0.2272443175315857
train_iter_loss: 0.4579258859157562
train_iter_loss: 0.36038094758987427
train_iter_loss: 0.2752893567085266
train_iter_loss: 0.4650251269340515
train_iter_loss: 0.3769463002681732
train_iter_loss: 0.2583668529987335
train_iter_loss: 0.4027629494667053
train_iter_loss: 0.346759557723999
train_iter_loss: 0.4135872721672058
train_iter_loss: 0.3329147696495056
train_iter_loss: 0.42933928966522217
train_iter_loss: 0.28437018394470215
train_iter_loss: 0.2861604392528534
train_iter_loss: 0.29341673851013184
train_iter_loss: 0.3655276298522949
train_iter_loss: 0.28773078322410583
train_iter_loss: 0.19469252228736877
train_iter_loss: 0.34651073813438416
train_iter_loss: 0.24555522203445435
train_iter_loss: 0.30178073048591614
train_iter_loss: 0.22723214328289032
train_iter_loss: 0.2679765522480011
train_iter_loss: 0.3233024477958679
train_iter_loss: 0.27238649129867554
train_iter_loss: 0.2650914490222931
train_iter_loss: 0.32258519530296326
train_iter_loss: 0.34849801659584045
train_iter_loss: 0.21983130276203156
train_iter_loss: 0.288373738527298
train_iter_loss: 0.26878565549850464
train_iter_loss: 0.4066278338432312
train_iter_loss: 0.4174326956272125
train_iter_loss: 0.33857670426368713
train_iter_loss: 0.23579269647598267
train_iter_loss: 0.2850395739078522
train_iter_loss: 0.38920485973358154
train_iter_loss: 0.2847387194633484
train_iter_loss: 0.2969299554824829
train_iter_loss: 0.20726975798606873
train_iter_loss: 0.3042623996734619
train_iter_loss: 0.3286498486995697
train_iter_loss: 0.38429009914398193
train_iter_loss: 0.3473902940750122
train_iter_loss: 0.37123173475265503
train_iter_loss: 0.443372517824173
train_iter_loss: 0.2215520143508911
train_iter_loss: 0.2577493190765381
train_iter_loss: 0.3708770275115967
train_iter_loss: 0.48624539375305176
train_iter_loss: 0.3351813852787018
train_iter_loss: 0.28672561049461365
train_iter_loss: 0.21230962872505188
train_iter_loss: 0.312874972820282
train_iter_loss: 0.4677540063858032
train_iter_loss: 0.35719582438468933
train_iter_loss: 0.31123632192611694
train_iter_loss: 0.1670074760913849
train_iter_loss: 0.3055439889431
train_iter_loss: 0.30970388650894165
train_iter_loss: 0.5666442513465881
train_iter_loss: 0.3596160411834717
train_iter_loss: 0.34404581785202026
train_iter_loss: 0.4073598086833954
train_iter_loss: 0.27169761061668396
train_iter_loss: 0.2919984757900238
train_iter_loss: 0.32242387533187866
train_iter_loss: 0.2191801816225052
train_iter_loss: 0.2691075801849365
train_iter_loss: 0.43001803755760193
train_iter_loss: 0.3787837624549866
train_iter_loss: 0.3182433545589447
train_iter_loss: 0.32812121510505676
train_iter_loss: 0.47830072045326233
train_iter_loss: 0.24550558626651764
train_iter_loss: 0.42538702487945557
train_iter_loss: 0.4032984673976898
train_iter_loss: 0.4669113755226135
train_iter_loss: 0.4849178194999695
train_iter_loss: 0.3878951072692871
train_iter_loss: 0.3786419928073883
train_iter_loss: 0.3687319755554199
train_iter_loss: 0.31700149178504944
train_iter_loss: 0.23857074975967407
train_iter_loss: 0.2772643566131592
train_iter_loss: 0.21849749982357025
train_iter_loss: 0.4326919615268707
train_iter_loss: 0.26896050572395325
train_iter_loss: 0.5444782972335815
train_iter_loss: 0.28328174352645874
train_iter_loss: 0.16660654544830322
train_iter_loss: 0.5277787446975708
train_iter_loss: 0.3049200475215912
train_iter_loss: 0.2524835765361786
train_iter_loss: 0.3636922538280487
train loss :0.3349
---------------------
Validation seg loss: 0.39186186526181566 at epoch 42
********************
best_val_epoch_loss:  0.39186186526181566
MODEL UPDATED
epoch =     43/  1000, exp = train
train_iter_loss: 0.21484224498271942
train_iter_loss: 0.21082276105880737
train_iter_loss: 0.29896658658981323
train_iter_loss: 0.3952878415584564
train_iter_loss: 0.2107347697019577
train_iter_loss: 0.3088172972202301
train_iter_loss: 0.30037590861320496
train_iter_loss: 0.33155685663223267
train_iter_loss: 0.2487756460905075
train_iter_loss: 0.4201602339744568
train_iter_loss: 0.3950798809528351
train_iter_loss: 0.35888922214508057
train_iter_loss: 0.29795676469802856
train_iter_loss: 0.3695414960384369
train_iter_loss: 0.2988731861114502
train_iter_loss: 0.26101920008659363
train_iter_loss: 0.4763396680355072
train_iter_loss: 0.28771841526031494
train_iter_loss: 0.420213520526886
train_iter_loss: 0.5164368748664856
train_iter_loss: 0.3114277124404907
train_iter_loss: 0.24610954523086548
train_iter_loss: 0.29577040672302246
train_iter_loss: 0.2638873755931854
train_iter_loss: 0.4510814845561981
train_iter_loss: 0.3853103518486023
train_iter_loss: 0.29825976490974426
train_iter_loss: 0.22753079235553741
train_iter_loss: 0.26821011304855347
train_iter_loss: 0.1900244504213333
train_iter_loss: 0.4454554617404938
train_iter_loss: 0.2577826976776123
train_iter_loss: 0.46260735392570496
train_iter_loss: 0.2273283451795578
train_iter_loss: 0.40975794196128845
train_iter_loss: 0.3020848035812378
train_iter_loss: 0.413687527179718
train_iter_loss: 0.20962612330913544
train_iter_loss: 0.39086049795150757
train_iter_loss: 0.3354725241661072
train_iter_loss: 0.4085984230041504
train_iter_loss: 0.25001171231269836
train_iter_loss: 0.2878579795360565
train_iter_loss: 0.31256499886512756
train_iter_loss: 0.508167564868927
train_iter_loss: 0.35142484307289124
train_iter_loss: 0.31463372707366943
train_iter_loss: 0.279098242521286
train_iter_loss: 0.4301230013370514
train_iter_loss: 0.23614828288555145
train_iter_loss: 0.38269445300102234
train_iter_loss: 0.28625741600990295
train_iter_loss: 0.2841656804084778
train_iter_loss: 0.33579158782958984
train_iter_loss: 0.36738038063049316
train_iter_loss: 0.3531995713710785
train_iter_loss: 0.3807469308376312
train_iter_loss: 0.3406588137149811
train_iter_loss: 0.5374528169631958
train_iter_loss: 0.3188847601413727
train_iter_loss: 0.30568304657936096
train_iter_loss: 0.3962349593639374
train_iter_loss: 0.3206358850002289
train_iter_loss: 0.4390198588371277
train_iter_loss: 0.5127313733100891
train_iter_loss: 0.23421555757522583
train_iter_loss: 0.3171076476573944
train_iter_loss: 0.37752145528793335
train_iter_loss: 0.25073671340942383
train_iter_loss: 0.2843337953090668
train_iter_loss: 0.31810736656188965
train_iter_loss: 0.5001352429389954
train_iter_loss: 0.30125850439071655
train_iter_loss: 0.3176431357860565
train_iter_loss: 0.26010239124298096
train_iter_loss: 0.36964359879493713
train_iter_loss: 0.1788313239812851
train_iter_loss: 0.27602460980415344
train_iter_loss: 0.22899292409420013
train_iter_loss: 0.43676528334617615
train_iter_loss: 0.42486217617988586
train_iter_loss: 0.39395731687545776
train_iter_loss: 0.33768782019615173
train_iter_loss: 0.30288249254226685
train_iter_loss: 0.2667619585990906
train_iter_loss: 0.2762148380279541
train_iter_loss: 0.29345256090164185
train_iter_loss: 0.21501491963863373
train_iter_loss: 0.41991809010505676
train_iter_loss: 0.30205008387565613
train_iter_loss: 0.4902849793434143
train_iter_loss: 0.29520589113235474
train_iter_loss: 0.6043643355369568
train_iter_loss: 0.2729993164539337
train_iter_loss: 0.36281585693359375
train_iter_loss: 0.24263323843479156
train_iter_loss: 0.21971459686756134
train_iter_loss: 0.3118041753768921
train_iter_loss: 0.3693259358406067
train_iter_loss: 0.3676449954509735
train loss :0.3371
---------------------
Validation seg loss: 0.3826537495654709 at epoch 43
********************
best_val_epoch_loss:  0.3826537495654709
MODEL UPDATED
epoch =     44/  1000, exp = train
train_iter_loss: 0.2730906307697296
train_iter_loss: 0.27432259917259216
train_iter_loss: 0.3208438456058502
train_iter_loss: 0.3004175126552582
train_iter_loss: 0.2907312214374542
train_iter_loss: 0.3428029417991638
train_iter_loss: 0.3012683689594269
train_iter_loss: 0.4030466675758362
train_iter_loss: 0.22979292273521423
train_iter_loss: 0.27312007546424866
train_iter_loss: 0.33873873949050903
train_iter_loss: 0.450742244720459
train_iter_loss: 0.2274468094110489
train_iter_loss: 0.3276810944080353
train_iter_loss: 0.33972692489624023
train_iter_loss: 0.3538554310798645
train_iter_loss: 0.38582924008369446
train_iter_loss: 0.4673043489456177
train_iter_loss: 0.18102508783340454
train_iter_loss: 0.18524624407291412
train_iter_loss: 0.27746495604515076
train_iter_loss: 0.2534032166004181
train_iter_loss: 0.2982081174850464
train_iter_loss: 0.2807459533214569
train_iter_loss: 0.4472728967666626
train_iter_loss: 0.36350658535957336
train_iter_loss: 0.35693469643592834
train_iter_loss: 0.33474600315093994
train_iter_loss: 0.3225167393684387
train_iter_loss: 0.3159312307834625
train_iter_loss: 0.41706910729408264
train_iter_loss: 0.27436038851737976
train_iter_loss: 0.2798667252063751
train_iter_loss: 0.2486249804496765
train_iter_loss: 0.24407826364040375
train_iter_loss: 0.31576377153396606
train_iter_loss: 0.3768783509731293
train_iter_loss: 0.42659929394721985
train_iter_loss: 0.17598272860050201
train_iter_loss: 0.22960200905799866
train_iter_loss: 0.2705346345901489
train_iter_loss: 0.42176443338394165
train_iter_loss: 0.3346123993396759
train_iter_loss: 0.5407299995422363
train_iter_loss: 0.34018293023109436
train_iter_loss: 0.24292923510074615
train_iter_loss: 0.38610923290252686
train_iter_loss: 0.19824343919754028
train_iter_loss: 0.3149573802947998
train_iter_loss: 0.45289045572280884
train_iter_loss: 0.35253292322158813
train_iter_loss: 0.3024006187915802
train_iter_loss: 0.23153716325759888
train_iter_loss: 0.25248265266418457
train_iter_loss: 0.3299407958984375
train_iter_loss: 0.24509967863559723
train_iter_loss: 0.415310800075531
train_iter_loss: 0.30510425567626953
train_iter_loss: 0.25656312704086304
train_iter_loss: 0.31949126720428467
train_iter_loss: 0.33599817752838135
train_iter_loss: 0.45092204213142395
train_iter_loss: 0.3160450756549835
train_iter_loss: 0.3783103823661804
train_iter_loss: 0.3584088683128357
train_iter_loss: 0.3945975601673126
train_iter_loss: 0.3717465400695801
train_iter_loss: 0.38730764389038086
train_iter_loss: 0.3058318793773651
train_iter_loss: 0.27870088815689087
train_iter_loss: 0.26389357447624207
train_iter_loss: 0.27503740787506104
train_iter_loss: 0.23428486287593842
train_iter_loss: 0.22734636068344116
train_iter_loss: 0.3683876097202301
train_iter_loss: 0.22847963869571686
train_iter_loss: 0.30919092893600464
train_iter_loss: 0.2731761336326599
train_iter_loss: 0.38067707419395447
train_iter_loss: 0.3690183758735657
train_iter_loss: 0.41231805086135864
train_iter_loss: 0.3915722072124481
train_iter_loss: 0.29841041564941406
train_iter_loss: 0.29189175367355347
train_iter_loss: 0.3595145642757416
train_iter_loss: 0.3046359717845917
train_iter_loss: 0.3640128970146179
train_iter_loss: 0.2598661780357361
train_iter_loss: 0.2155948281288147
train_iter_loss: 0.34758564829826355
train_iter_loss: 0.42833027243614197
train_iter_loss: 0.2474983185529709
train_iter_loss: 0.30539047718048096
train_iter_loss: 0.35515159368515015
train_iter_loss: 0.323171466588974
train_iter_loss: 0.29910165071487427
train_iter_loss: 0.5373210310935974
train_iter_loss: 0.36121538281440735
train_iter_loss: 0.27529510855674744
train_iter_loss: 0.264982134103775
train loss :0.3251
---------------------
Validation seg loss: 0.3838246824782131 at epoch 44
epoch =     45/  1000, exp = train
train_iter_loss: 0.30822786688804626
train_iter_loss: 0.49979743361473083
train_iter_loss: 0.3568049371242523
train_iter_loss: 0.22987042367458344
train_iter_loss: 0.3625360429286957
train_iter_loss: 0.37354037165641785
train_iter_loss: 0.2718545198440552
train_iter_loss: 0.4550530016422272
train_iter_loss: 0.2948419153690338
train_iter_loss: 0.30084529519081116
train_iter_loss: 0.24168407917022705
train_iter_loss: 0.37766367197036743
train_iter_loss: 0.30071279406547546
train_iter_loss: 0.22082829475402832
train_iter_loss: 0.24844177067279816
train_iter_loss: 0.1943400651216507
train_iter_loss: 0.28728610277175903
train_iter_loss: 0.4352291524410248
train_iter_loss: 0.24710017442703247
train_iter_loss: 0.28577736020088196
train_iter_loss: 0.28019794821739197
train_iter_loss: 0.29542234539985657
train_iter_loss: 0.3283405900001526
train_iter_loss: 0.18736504018306732
train_iter_loss: 0.23254752159118652
train_iter_loss: 0.32738029956817627
train_iter_loss: 0.3487587869167328
train_iter_loss: 0.29711413383483887
train_iter_loss: 0.22778001427650452
train_iter_loss: 0.32618585228919983
train_iter_loss: 0.3522758483886719
train_iter_loss: 0.32962676882743835
train_iter_loss: 0.34567099809646606
train_iter_loss: 0.33343982696533203
train_iter_loss: 0.32728755474090576
train_iter_loss: 0.3022705316543579
train_iter_loss: 0.29237499833106995
train_iter_loss: 0.366422176361084
train_iter_loss: 0.28977668285369873
train_iter_loss: 0.34329813718795776
train_iter_loss: 0.3332221806049347
train_iter_loss: 0.2698017656803131
train_iter_loss: 0.5013382434844971
train_iter_loss: 0.338342547416687
train_iter_loss: 0.32668760418891907
train_iter_loss: 0.40942278504371643
train_iter_loss: 0.27342361211776733
train_iter_loss: 0.5076166391372681
train_iter_loss: 0.29016220569610596
train_iter_loss: 0.19430817663669586
train_iter_loss: 0.3926442861557007
train_iter_loss: 0.4075663387775421
train_iter_loss: 0.2833634614944458
train_iter_loss: 0.3001503050327301
train_iter_loss: 0.36667758226394653
train_iter_loss: 0.2731216251850128
train_iter_loss: 0.28475409746170044
train_iter_loss: 0.2611159384250641
train_iter_loss: 0.35404276847839355
train_iter_loss: 0.331245094537735
train_iter_loss: 0.271440714597702
train_iter_loss: 0.45132797956466675
train_iter_loss: 0.27809369564056396
train_iter_loss: 0.2102128565311432
train_iter_loss: 0.35036715865135193
train_iter_loss: 0.28338897228240967
train_iter_loss: 0.3694145679473877
train_iter_loss: 0.3019745349884033
train_iter_loss: 0.28091198205947876
train_iter_loss: 0.2690577805042267
train_iter_loss: 0.33976003527641296
train_iter_loss: 0.31897127628326416
train_iter_loss: 0.48228466510772705
train_iter_loss: 0.3472583591938019
train_iter_loss: 0.27780094742774963
train_iter_loss: 0.3189266324043274
train_iter_loss: 0.20099711418151855
train_iter_loss: 0.3373483717441559
train_iter_loss: 0.27095457911491394
train_iter_loss: 0.4614996314048767
train_iter_loss: 0.39234647154808044
train_iter_loss: 0.41567689180374146
train_iter_loss: 0.5623302459716797
train_iter_loss: 0.33079078793525696
train_iter_loss: 0.3628901243209839
train_iter_loss: 0.27911999821662903
train_iter_loss: 0.27924811840057373
train_iter_loss: 0.347562313079834
train_iter_loss: 0.24750187993049622
train_iter_loss: 0.31240135431289673
train_iter_loss: 0.2850908935070038
train_iter_loss: 0.44601911306381226
train_iter_loss: 0.15949204564094543
train_iter_loss: 0.2632140815258026
train_iter_loss: 0.3760189712047577
train_iter_loss: 0.4341434836387634
train_iter_loss: 0.3422417640686035
train_iter_loss: 0.4287501573562622
train_iter_loss: 0.2258375734090805
train_iter_loss: 0.3514590263366699
train loss :0.3271
---------------------
Validation seg loss: 0.41192876979849247 at epoch 45
epoch =     46/  1000, exp = train
train_iter_loss: 0.28935015201568604
train_iter_loss: 0.27043625712394714
train_iter_loss: 0.4789285659790039
train_iter_loss: 0.27903056144714355
train_iter_loss: 0.3126297891139984
train_iter_loss: 0.24403297901153564
train_iter_loss: 0.1817183494567871
train_iter_loss: 0.47394415736198425
train_iter_loss: 0.3600839078426361
train_iter_loss: 0.38606446981430054
train_iter_loss: 0.21273115277290344
train_iter_loss: 0.43123605847358704
train_iter_loss: 0.2777753472328186
train_iter_loss: 0.354975163936615
train_iter_loss: 0.42986568808555603
train_iter_loss: 0.3145996332168579
train_iter_loss: 0.4444689154624939
train_iter_loss: 0.40446674823760986
train_iter_loss: 0.18415077030658722
train_iter_loss: 0.34430086612701416
train_iter_loss: 0.2975275218486786
train_iter_loss: 0.16220088303089142
train_iter_loss: 0.21946309506893158
train_iter_loss: 0.47824546694755554
train_iter_loss: 0.6424424052238464
train_iter_loss: 0.27094829082489014
train_iter_loss: 0.3681085705757141
train_iter_loss: 0.23500293493270874
train_iter_loss: 0.31202346086502075
train_iter_loss: 0.29727092385292053
train_iter_loss: 0.27629411220550537
train_iter_loss: 0.20896074175834656
train_iter_loss: 0.32314231991767883
train_iter_loss: 0.32573986053466797
train_iter_loss: 0.1839459240436554
train_iter_loss: 0.3510020673274994
train_iter_loss: 0.3810995817184448
train_iter_loss: 0.3072164058685303
train_iter_loss: 0.5768334269523621
train_iter_loss: 0.3002541661262512
train_iter_loss: 0.26768699288368225
train_iter_loss: 0.2879762053489685
train_iter_loss: 0.3003602623939514
train_iter_loss: 0.40956947207450867
train_iter_loss: 0.27160030603408813
train_iter_loss: 0.3676852285861969
train_iter_loss: 0.4417463541030884
train_iter_loss: 0.340497761964798
train_iter_loss: 0.38825199007987976
train_iter_loss: 0.28767913579940796
train_iter_loss: 0.3001977503299713
train_iter_loss: 0.43615928292274475
train_iter_loss: 0.18163028359413147
train_iter_loss: 0.37306204438209534
train_iter_loss: 0.23477326333522797
train_iter_loss: 0.29717957973480225
train_iter_loss: 0.39112091064453125
train_iter_loss: 0.32731178402900696
train_iter_loss: 0.4693235754966736
train_iter_loss: 0.2865586578845978
train_iter_loss: 0.27864933013916016
train_iter_loss: 0.363680899143219
train_iter_loss: 0.2889877259731293
train_iter_loss: 0.42640942335128784
train_iter_loss: 0.21975190937519073
train_iter_loss: 0.42266738414764404
train_iter_loss: 0.31259685754776
train_iter_loss: 0.30751505494117737
train_iter_loss: 0.34620580077171326
train_iter_loss: 0.4070185124874115
train_iter_loss: 0.3066861033439636
train_iter_loss: 0.2712659537792206
train_iter_loss: 0.41625139117240906
train_iter_loss: 0.2971038818359375
train_iter_loss: 0.23952843248844147
train_iter_loss: 0.2876567244529724
train_iter_loss: 0.31179890036582947
train_iter_loss: 0.31656959652900696
train_iter_loss: 0.4142376780509949
train_iter_loss: 0.40926316380500793
train_iter_loss: 0.3574365973472595
train_iter_loss: 0.30559924244880676
train_iter_loss: 0.3661990761756897
train_iter_loss: 0.33301761746406555
train_iter_loss: 0.3431173264980316
train_iter_loss: 0.20319657027721405
train_iter_loss: 0.34045904874801636
train_iter_loss: 0.1686982363462448
train_iter_loss: 0.3143821656703949
train_iter_loss: 0.33221790194511414
train_iter_loss: 0.3739859461784363
train_iter_loss: 0.3133433163166046
train_iter_loss: 0.2998301684856415
train_iter_loss: 0.22013403475284576
train_iter_loss: 0.3046334981918335
train_iter_loss: 0.3975308835506439
train_iter_loss: 0.2940271198749542
train_iter_loss: 0.3336378037929535
train_iter_loss: 0.33999910950660706
train_iter_loss: 0.281315416097641
train loss :0.3305
---------------------
Validation seg loss: 0.3946572010458078 at epoch 46
epoch =     47/  1000, exp = train
train_iter_loss: 0.3333590030670166
train_iter_loss: 0.3728426992893219
train_iter_loss: 0.3231419324874878
train_iter_loss: 0.22082950174808502
train_iter_loss: 0.2294471114873886
train_iter_loss: 0.30047041177749634
train_iter_loss: 0.4261312484741211
train_iter_loss: 0.49989578127861023
train_iter_loss: 0.27902719378471375
train_iter_loss: 0.2961363196372986
train_iter_loss: 0.39445531368255615
train_iter_loss: 0.2880704402923584
train_iter_loss: 0.20832163095474243
train_iter_loss: 0.4211989939212799
train_iter_loss: 0.31563881039619446
train_iter_loss: 0.4241436719894409
train_iter_loss: 0.3321167826652527
train_iter_loss: 0.27740150690078735
train_iter_loss: 0.21472248435020447
train_iter_loss: 0.2956450581550598
train_iter_loss: 0.30838221311569214
train_iter_loss: 0.22510069608688354
train_iter_loss: 0.42258474230766296
train_iter_loss: 0.3436211049556732
train_iter_loss: 0.37329384684562683
train_iter_loss: 0.23193775117397308
train_iter_loss: 0.37402909994125366
train_iter_loss: 0.2434544861316681
train_iter_loss: 0.23723381757736206
train_iter_loss: 0.42105424404144287
train_iter_loss: 0.44420409202575684
train_iter_loss: 0.2887115180492401
train_iter_loss: 0.3979083299636841
train_iter_loss: 0.309162974357605
train_iter_loss: 0.4015687108039856
train_iter_loss: 0.3262505531311035
train_iter_loss: 0.3520853817462921
train_iter_loss: 0.4855205714702606
train_iter_loss: 0.4990502595901489
train_iter_loss: 0.2047358751296997
train_iter_loss: 0.3976113498210907
train_iter_loss: 0.39716988801956177
train_iter_loss: 0.15759509801864624
train_iter_loss: 0.36364680528640747
train_iter_loss: 0.2741146981716156
train_iter_loss: 0.2831485867500305
train_iter_loss: 0.26399320363998413
train_iter_loss: 0.21969527006149292
train_iter_loss: 0.30285122990608215
train_iter_loss: 0.3901588022708893
train_iter_loss: 0.5096496939659119
train_iter_loss: 0.39770057797431946
train_iter_loss: 0.33706000447273254
train_iter_loss: 0.3201731741428375
train_iter_loss: 0.3683570623397827
train_iter_loss: 0.5476747155189514
train_iter_loss: 0.5053398013114929
train_iter_loss: 0.3361762464046478
train_iter_loss: 0.4086073935031891
train_iter_loss: 0.38352611660957336
train_iter_loss: 0.5116628408432007
train_iter_loss: 0.25862744450569153
train_iter_loss: 0.25772467255592346
train_iter_loss: 0.17955103516578674
train_iter_loss: 0.4403686225414276
train_iter_loss: 0.43742087483406067
train_iter_loss: 0.30589208006858826
train_iter_loss: 0.28120094537734985
train_iter_loss: 0.2971152067184448
train_iter_loss: 0.35847318172454834
train_iter_loss: 0.29090043902397156
train_iter_loss: 0.3680357038974762
train_iter_loss: 0.27397510409355164
train_iter_loss: 0.16084161400794983
train_iter_loss: 0.25503721833229065
train_iter_loss: 0.28030717372894287
train_iter_loss: 0.31836462020874023
train_iter_loss: 0.6155101656913757
train_iter_loss: 0.4096706807613373
train_iter_loss: 0.36234232783317566
train_iter_loss: 0.23280897736549377
train_iter_loss: 0.2688351571559906
train_iter_loss: 0.25756630301475525
train_iter_loss: 0.3577211797237396
train_iter_loss: 0.3874303698539734
train_iter_loss: 0.25660440325737
train_iter_loss: 0.271258682012558
train_iter_loss: 0.32394081354141235
train_iter_loss: 0.3463534414768219
train_iter_loss: 0.3076012432575226
train_iter_loss: 0.32123225927352905
train_iter_loss: 0.35756242275238037
train_iter_loss: 0.2376479059457779
train_iter_loss: 0.4306066930294037
train_iter_loss: 0.375383198261261
train_iter_loss: 0.257224440574646
train_iter_loss: 0.2975435256958008
train_iter_loss: 0.254583477973938
train_iter_loss: 0.28690412640571594
train_iter_loss: 0.259439617395401
train loss :0.3359
---------------------
Validation seg loss: 0.39783129313925525 at epoch 47
epoch =     48/  1000, exp = train
train_iter_loss: 0.2723393738269806
train_iter_loss: 0.35733121633529663
train_iter_loss: 0.4305020570755005
train_iter_loss: 0.3711366653442383
train_iter_loss: 0.397002637386322
train_iter_loss: 0.39440643787384033
train_iter_loss: 0.24321915209293365
train_iter_loss: 0.34405940771102905
train_iter_loss: 0.3208046853542328
train_iter_loss: 0.359229177236557
train_iter_loss: 0.26868847012519836
train_iter_loss: 0.20558546483516693
train_iter_loss: 0.4197952151298523
train_iter_loss: 0.3084876239299774
train_iter_loss: 0.32803967595100403
train_iter_loss: 0.3776949346065521
train_iter_loss: 0.27917248010635376
train_iter_loss: 0.3032926321029663
train_iter_loss: 0.34471815824508667
train_iter_loss: 0.47877922654151917
train_iter_loss: 0.21738211810588837
train_iter_loss: 0.3956019878387451
train_iter_loss: 0.3515840768814087
train_iter_loss: 0.3180329501628876
train_iter_loss: 0.28256842494010925
train_iter_loss: 0.2847665250301361
train_iter_loss: 0.21093466877937317
train_iter_loss: 0.2868852913379669
train_iter_loss: 0.5791578888893127
train_iter_loss: 0.32959017157554626
train_iter_loss: 0.30810993909835815
train_iter_loss: 0.25167858600616455
train_iter_loss: 0.44159096479415894
train_iter_loss: 0.35294654965400696
train_iter_loss: 0.22615423798561096
train_iter_loss: 0.3612767457962036
train_iter_loss: 0.5813435316085815
train_iter_loss: 0.38014477491378784
train_iter_loss: 0.22079211473464966
train_iter_loss: 0.35829591751098633
train_iter_loss: 0.31936946511268616
train_iter_loss: 0.26359954476356506
train_iter_loss: 0.25149959325790405
train_iter_loss: 0.2489830106496811
train_iter_loss: 0.34564006328582764
train_iter_loss: 0.40627408027648926
train_iter_loss: 0.45788586139678955
train_iter_loss: 0.2760854661464691
train_iter_loss: 0.3150022327899933
train_iter_loss: 0.3512774109840393
train_iter_loss: 0.2714170217514038
train_iter_loss: 0.21379239857196808
train_iter_loss: 0.26031291484832764
train_iter_loss: 0.3174697458744049
train_iter_loss: 0.4631820619106293
train_iter_loss: 0.34380853176116943
train_iter_loss: 0.2644108831882477
train_iter_loss: 0.2395331859588623
train_iter_loss: 0.3637801706790924
train_iter_loss: 0.35114994645118713
train_iter_loss: 0.298115074634552
train_iter_loss: 0.31468328833580017
train_iter_loss: 0.3985224664211273
train_iter_loss: 0.3000181019306183
train_iter_loss: 0.33767858147621155
train_iter_loss: 0.25285282731056213
train_iter_loss: 0.2309715747833252
train_iter_loss: 0.3478979468345642
train_iter_loss: 0.3059813976287842
train_iter_loss: 0.400059312582016
train_iter_loss: 0.39234957098960876
train_iter_loss: 0.3062639534473419
train_iter_loss: 0.2862128019332886
train_iter_loss: 0.35276275873184204
train_iter_loss: 0.30829107761383057
train_iter_loss: 0.30302202701568604
train_iter_loss: 0.2016981989145279
train_iter_loss: 0.5200333595275879
train_iter_loss: 0.20751923322677612
train_iter_loss: 0.3924427330493927
train_iter_loss: 0.3316406309604645
train_iter_loss: 0.26718583703041077
train_iter_loss: 0.5000931620597839
train_iter_loss: 0.33004698157310486
train_iter_loss: 0.27473852038383484
train_iter_loss: 0.37422528862953186
train_iter_loss: 0.2001829892396927
train_iter_loss: 0.34724584221839905
train_iter_loss: 0.2798876166343689
train_iter_loss: 0.3111487329006195
train_iter_loss: 0.40893301367759705
train_iter_loss: 0.2914991080760956
train_iter_loss: 0.20337732136249542
train_iter_loss: 0.37167713046073914
train_iter_loss: 0.40011629462242126
train_iter_loss: 0.6526526212692261
train_iter_loss: 0.3630332052707672
train_iter_loss: 0.3191145360469818
train_iter_loss: 0.2630403935909271
train_iter_loss: 0.2183307558298111
train loss :0.3337
---------------------
Validation seg loss: 0.39007536262133213 at epoch 48
epoch =     49/  1000, exp = train
train_iter_loss: 0.4035499095916748
train_iter_loss: 0.3574620187282562
train_iter_loss: 0.3360758125782013
train_iter_loss: 0.33965566754341125
train_iter_loss: 0.39506450295448303
train_iter_loss: 0.4007888436317444
train_iter_loss: 0.3635367751121521
train_iter_loss: 0.3441680371761322
train_iter_loss: 0.337378591299057
train_iter_loss: 0.2972896099090576
train_iter_loss: 0.410126268863678
train_iter_loss: 0.17419932782649994
train_iter_loss: 0.2756423056125641
train_iter_loss: 0.343596488237381
train_iter_loss: 0.37977370619773865
train_iter_loss: 0.24007977545261383
train_iter_loss: 0.26115867495536804
train_iter_loss: 0.2703090310096741
train_iter_loss: 0.362494558095932
train_iter_loss: 0.21473091840744019
train_iter_loss: 0.3678446114063263
train_iter_loss: 0.278460294008255
train_iter_loss: 0.33010604977607727
train_iter_loss: 0.33296605944633484
train_iter_loss: 0.4139176607131958
train_iter_loss: 0.44121038913726807
train_iter_loss: 0.29448002576828003
train_iter_loss: 0.19391745328903198
train_iter_loss: 0.35793253779411316
train_iter_loss: 0.34719574451446533
train_iter_loss: 0.3865506052970886
train_iter_loss: 0.23311516642570496
train_iter_loss: 0.4903879761695862
train_iter_loss: 0.25570541620254517
train_iter_loss: 0.2638694941997528
train_iter_loss: 0.265738844871521
train_iter_loss: 0.41623228788375854
train_iter_loss: 0.4781993329524994
train_iter_loss: 0.38288915157318115
train_iter_loss: 0.22987298667430878
train_iter_loss: 0.24867112934589386
train_iter_loss: 0.2591951787471771
train_iter_loss: 0.3617909550666809
train_iter_loss: 0.38760456442832947
train_iter_loss: 0.4812071621417999
train_iter_loss: 0.36478888988494873
train_iter_loss: 0.3894196152687073
train_iter_loss: 0.23114727437496185
train_iter_loss: 0.28348201513290405
train_iter_loss: 0.25298047065734863
train_iter_loss: 0.3806714415550232
train_iter_loss: 0.3345010280609131
train_iter_loss: 0.35797014832496643
train_iter_loss: 0.5582036375999451
train_iter_loss: 0.2716759741306305
train_iter_loss: 0.25974059104919434
train_iter_loss: 0.3531150817871094
train_iter_loss: 0.30411872267723083
train_iter_loss: 0.26509401202201843
train_iter_loss: 0.3566899001598358
train_iter_loss: 0.23652470111846924
train_iter_loss: 0.48542913794517517
train_iter_loss: 0.23953591287136078
train_iter_loss: 0.38279011845588684
train_iter_loss: 0.40320491790771484
train_iter_loss: 0.4309193789958954
train_iter_loss: 0.2917577922344208
train_iter_loss: 0.4173034727573395
train_iter_loss: 0.3219292461872101
train_iter_loss: 0.27654770016670227
train_iter_loss: 0.26875877380371094
train_iter_loss: 0.3331160247325897
train_iter_loss: 0.33004844188690186
train_iter_loss: 0.3522481322288513
train_iter_loss: 0.21145617961883545
train_iter_loss: 0.33109965920448303
train_iter_loss: 0.23707105219364166
train_iter_loss: 0.3560974895954132
train_iter_loss: 0.20225322246551514
train_iter_loss: 0.3274427056312561
train_iter_loss: 0.19554084539413452
train_iter_loss: 0.3247931897640228
train_iter_loss: 0.3039565980434418
train_iter_loss: 0.3195701837539673
train_iter_loss: 0.23400312662124634
train_iter_loss: 0.2484355866909027
train_iter_loss: 0.4098757803440094
train_iter_loss: 0.25519758462905884
train_iter_loss: 0.3925807476043701
train_iter_loss: 0.3236866295337677
train_iter_loss: 0.15304513275623322
train_iter_loss: 0.26440396904945374
train_iter_loss: 0.4994766116142273
train_iter_loss: 0.3052256405353546
train_iter_loss: 0.39906221628189087
train_iter_loss: 0.262226939201355
train_iter_loss: 0.24326728284358978
train_iter_loss: 0.4374482035636902
train_iter_loss: 0.3456263840198517
train_iter_loss: 0.3311226963996887
train loss :0.3299
---------------------
Validation seg loss: 0.3918783809497671 at epoch 49
epoch =     50/  1000, exp = train
train_iter_loss: 0.5674060583114624
train_iter_loss: 0.23152337968349457
train_iter_loss: 0.3412504494190216
train_iter_loss: 0.30288219451904297
train_iter_loss: 0.3362472951412201
train_iter_loss: 0.359928160905838
train_iter_loss: 0.33507612347602844
train_iter_loss: 0.3147164285182953
train_iter_loss: 0.23245923221111298
train_iter_loss: 0.22658413648605347
train_iter_loss: 0.23305916786193848
train_iter_loss: 0.21338656544685364
train_iter_loss: 0.34451621770858765
train_iter_loss: 0.29841679334640503
train_iter_loss: 0.325144499540329
train_iter_loss: 0.37044405937194824
train_iter_loss: 0.29976245760917664
train_iter_loss: 0.3461209237575531
train_iter_loss: 0.3368530571460724
train_iter_loss: 0.316375195980072
train_iter_loss: 0.24430033564567566
train_iter_loss: 0.4015837609767914
train_iter_loss: 0.3038327693939209
train_iter_loss: 0.4064139127731323
train_iter_loss: 0.3215412497520447
train_iter_loss: 0.34218502044677734
train_iter_loss: 0.31142088770866394
train_iter_loss: 0.1956341713666916
train_iter_loss: 0.45748376846313477
train_iter_loss: 0.4352133572101593
train_iter_loss: 0.2533418536186218
train_iter_loss: 0.3905471861362457
train_iter_loss: 0.40848997235298157
train_iter_loss: 0.36011478304862976
train_iter_loss: 0.36240509152412415
train_iter_loss: 0.34288665652275085
train_iter_loss: 0.28526315093040466
train_iter_loss: 0.2208469808101654
train_iter_loss: 0.4341679811477661
train_iter_loss: 0.2622627913951874
train_iter_loss: 0.33180931210517883
train_iter_loss: 0.2621610462665558
train_iter_loss: 0.22431300580501556
train_iter_loss: 0.3402101695537567
train_iter_loss: 0.27324315905570984
train_iter_loss: 0.3314979374408722
train_iter_loss: 0.34161943197250366
train_iter_loss: 0.21113309264183044
train_iter_loss: 0.287569135427475
train_iter_loss: 0.3196351230144501
train_iter_loss: 0.38872987031936646
train_iter_loss: 0.418230265378952
train_iter_loss: 0.21422520279884338
train_iter_loss: 0.291457861661911
train_iter_loss: 0.26484304666519165
train_iter_loss: 0.2659226357936859
train_iter_loss: 0.3479398190975189
train_iter_loss: 0.30717697739601135
train_iter_loss: 0.33945420384407043
train_iter_loss: 0.30668506026268005
train_iter_loss: 0.40455466508865356
train_iter_loss: 0.32690441608428955
train_iter_loss: 0.336180180311203
train_iter_loss: 0.3140486180782318
train_iter_loss: 0.3321952819824219
train_iter_loss: 0.22788287699222565
train_iter_loss: 0.3339197337627411
train_iter_loss: 0.22656752169132233
train_iter_loss: 0.30218610167503357
train_iter_loss: 0.3555128574371338
train_iter_loss: 0.2740721106529236
train_iter_loss: 0.4229263365268707
train_iter_loss: 0.22447504103183746
train_iter_loss: 0.3062478005886078
train_iter_loss: 0.301204651594162
train_iter_loss: 0.4591572880744934
train_iter_loss: 0.23590417206287384
train_iter_loss: 0.18608221411705017
train_iter_loss: 0.2742471694946289
train_iter_loss: 0.2062922865152359
train_iter_loss: 0.32128894329071045
train_iter_loss: 0.4764171540737152
train_iter_loss: 0.2950519025325775
train_iter_loss: 0.35296425223350525
train_iter_loss: 0.4513554275035858
train_iter_loss: 0.4955948293209076
train_iter_loss: 0.3271113932132721
train_iter_loss: 0.3059391677379608
train_iter_loss: 0.3459395170211792
train_iter_loss: 0.36607739329338074
train_iter_loss: 0.4309578537940979
train_iter_loss: 0.22527244687080383
train_iter_loss: 0.36244669556617737
train_iter_loss: 0.26572319865226746
train_iter_loss: 0.2886176109313965
train_iter_loss: 0.3879106938838959
train_iter_loss: 0.40953025221824646
train_iter_loss: 0.27877533435821533
train_iter_loss: 0.2814766764640808
train_iter_loss: 0.6056365966796875
train loss :0.3279
---------------------
Validation seg loss: 0.40857543912276906 at epoch 50
epoch =     51/  1000, exp = train
train_iter_loss: 0.20953603088855743
train_iter_loss: 0.3417713940143585
train_iter_loss: 0.36527377367019653
train_iter_loss: 0.34501272439956665
train_iter_loss: 0.31582045555114746
train_iter_loss: 0.3696385324001312
train_iter_loss: 0.4147057831287384
train_iter_loss: 0.2689763605594635
train_iter_loss: 0.4610057473182678
train_iter_loss: 0.2859760522842407
train_iter_loss: 0.25624242424964905
train_iter_loss: 0.25730645656585693
train_iter_loss: 0.40349438786506653
train_iter_loss: 0.3522113561630249
train_iter_loss: 0.42074885964393616
train_iter_loss: 0.40104302763938904
train_iter_loss: 0.13240081071853638
train_iter_loss: 0.23009781539440155
train_iter_loss: 0.34441888332366943
train_iter_loss: 0.23638109862804413
train_iter_loss: 0.1953752636909485
train_iter_loss: 0.19735704362392426
train_iter_loss: 0.2683800160884857
train_iter_loss: 0.4253932237625122
train_iter_loss: 0.3003746569156647
train_iter_loss: 0.2730618715286255
train_iter_loss: 0.2756789028644562
train_iter_loss: 0.3755226731300354
train_iter_loss: 0.26058250665664673
train_iter_loss: 0.3500976860523224
train_iter_loss: 0.4680393636226654
train_iter_loss: 0.3031883239746094
train_iter_loss: 0.4868663549423218
train_iter_loss: 0.32352718710899353
train_iter_loss: 0.2623540461063385
train_iter_loss: 0.3130844831466675
train_iter_loss: 0.390881210565567
train_iter_loss: 0.37162622809410095
train_iter_loss: 0.3393550515174866
train_iter_loss: 0.33513814210891724
train_iter_loss: 0.29474660754203796
train_iter_loss: 0.22400549054145813
train_iter_loss: 0.3849960267543793
train_iter_loss: 0.1399255096912384
train_iter_loss: 0.3076237738132477
train_iter_loss: 0.29452744126319885
train_iter_loss: 0.3447977602481842
train_iter_loss: 0.46366235613822937
train_iter_loss: 0.2545511722564697
train_iter_loss: 0.2615359425544739
train_iter_loss: 0.22426246106624603
train_iter_loss: 0.39648351073265076
train_iter_loss: 0.2864106595516205
train_iter_loss: 0.2688003480434418
train_iter_loss: 0.5644968748092651
train_iter_loss: 0.25266072154045105
train_iter_loss: 0.31636863946914673
train_iter_loss: 0.2884271442890167
train_iter_loss: 0.268337607383728
train_iter_loss: 0.3663436770439148
train_iter_loss: 0.30085664987564087
train_iter_loss: 0.5937172174453735
train_iter_loss: 0.397038996219635
train_iter_loss: 0.28469258546829224
train_iter_loss: 0.46714597940444946
train_iter_loss: 0.3736068904399872
train_iter_loss: 0.4020272195339203
train_iter_loss: 0.2840687036514282
train_iter_loss: 0.3923198878765106
train_iter_loss: 0.3401283621788025
train_iter_loss: 0.48371344804763794
train_iter_loss: 0.26525893807411194
train_iter_loss: 0.4132744073867798
train_iter_loss: 0.3248130679130554
train_iter_loss: 0.3752705752849579
train_iter_loss: 0.24995659291744232
train_iter_loss: 0.28241828083992004
train_iter_loss: 0.3098149299621582
train_iter_loss: 0.3555713891983032
train_iter_loss: 0.3461783230304718
train_iter_loss: 0.28370851278305054
train_iter_loss: 0.20159967243671417
train_iter_loss: 0.37480342388153076
train_iter_loss: 0.25598520040512085
train_iter_loss: 0.47284266352653503
train_iter_loss: 0.3702342212200165
train_iter_loss: 0.3644622266292572
train_iter_loss: 0.20501708984375
train_iter_loss: 0.2874738276004791
train_iter_loss: 0.36738714575767517
train_iter_loss: 0.38559725880622864
train_iter_loss: 0.25749874114990234
train_iter_loss: 0.37164050340652466
train_iter_loss: 0.30050235986709595
train_iter_loss: 0.2703479826450348
train_iter_loss: 0.12794961035251617
train_iter_loss: 0.31229618191719055
train_iter_loss: 0.2733589708805084
train_iter_loss: 0.22324225306510925
train_iter_loss: 0.2671011686325073
train loss :0.3267
---------------------
Validation seg loss: 0.39086870181391825 at epoch 51
epoch =     52/  1000, exp = train
train_iter_loss: 0.315964013338089
train_iter_loss: 0.42469343543052673
train_iter_loss: 0.20429790019989014
train_iter_loss: 0.25354817509651184
train_iter_loss: 0.3045234680175781
train_iter_loss: 0.3170299530029297
train_iter_loss: 0.3620094656944275
train_iter_loss: 0.25655731558799744
train_iter_loss: 0.3220130205154419
train_iter_loss: 0.26620158553123474
train_iter_loss: 0.4696596562862396
train_iter_loss: 0.31273800134658813
train_iter_loss: 0.20671556890010834
train_iter_loss: 0.22677986323833466
train_iter_loss: 0.3185167908668518
train_iter_loss: 0.27713704109191895
train_iter_loss: 0.26954182982444763
train_iter_loss: 0.29134559631347656
train_iter_loss: 0.31210196018218994
train_iter_loss: 0.28021031618118286
train_iter_loss: 0.36246755719184875
train_iter_loss: 0.2341080605983734
train_iter_loss: 0.2824108898639679
train_iter_loss: 0.2951466143131256
train_iter_loss: 0.3383644223213196
train_iter_loss: 0.2825290858745575
train_iter_loss: 0.25980186462402344
train_iter_loss: 0.22683952748775482
train_iter_loss: 0.327106237411499
train_iter_loss: 0.28279274702072144
train_iter_loss: 0.31885820627212524
train_iter_loss: 0.20624202489852905
train_iter_loss: 0.3126106560230255
train_iter_loss: 0.18587543070316315
train_iter_loss: 0.3528732359409332
train_iter_loss: 0.34841087460517883
train_iter_loss: 0.2967715561389923
train_iter_loss: 0.39410197734832764
train_iter_loss: 0.2856559157371521
train_iter_loss: 0.33888179063796997
train_iter_loss: 0.45146042108535767
train_iter_loss: 0.21395644545555115
train_iter_loss: 0.4282616674900055
train_iter_loss: 0.3255598247051239
train_iter_loss: 0.2490125298500061
train_iter_loss: 0.41305413842201233
train_iter_loss: 0.3182268738746643
train_iter_loss: 0.33038219809532166
train_iter_loss: 0.36524149775505066
train_iter_loss: 0.31178656220436096
train_iter_loss: 0.40803009271621704
train_iter_loss: 0.29778340458869934
train_iter_loss: 0.19481852650642395
train_iter_loss: 0.36514973640441895
train_iter_loss: 0.35149654746055603
train_iter_loss: 0.4194968640804291
train_iter_loss: 0.4457252621650696
train_iter_loss: 0.40068507194519043
train_iter_loss: 0.2778044641017914
train_iter_loss: 0.2799135744571686
train_iter_loss: 0.21790280938148499
train_iter_loss: 0.2594589591026306
train_iter_loss: 0.23555167019367218
train_iter_loss: 0.3077253997325897
train_iter_loss: 0.38267284631729126
train_iter_loss: 0.437587708234787
train_iter_loss: 0.297412246465683
train_iter_loss: 0.33193808794021606
train_iter_loss: 0.3475612998008728
train_iter_loss: 0.24515533447265625
train_iter_loss: 0.2586539089679718
train_iter_loss: 0.2620048522949219
train_iter_loss: 0.41131991147994995
train_iter_loss: 0.3340013921260834
train_iter_loss: 0.29836589097976685
train_iter_loss: 0.3199799656867981
train_iter_loss: 0.21820124983787537
train_iter_loss: 0.3628445565700531
train_iter_loss: 0.3214697539806366
train_iter_loss: 0.28239569067955017
train_iter_loss: 0.3474104106426239
train_iter_loss: 0.3391495943069458
train_iter_loss: 0.4276793897151947
train_iter_loss: 0.31567686796188354
train_iter_loss: 0.4834475517272949
train_iter_loss: 0.37366703152656555
train_iter_loss: 0.3564535081386566
train_iter_loss: 0.38204845786094666
train_iter_loss: 0.3739653527736664
train_iter_loss: 0.35802775621414185
train_iter_loss: 0.26012763381004333
train_iter_loss: 0.3373463749885559
train_iter_loss: 0.2543768286705017
train_iter_loss: 0.39393287897109985
train_iter_loss: 0.30613210797309875
train_iter_loss: 0.32426348328590393
train_iter_loss: 0.26147520542144775
train_iter_loss: 0.3466484844684601
train_iter_loss: 0.31409916281700134
train_iter_loss: 0.5118038058280945
train loss :0.3230
---------------------
Validation seg loss: 0.4150264644468168 at epoch 52
epoch =     53/  1000, exp = train
train_iter_loss: 0.2879484295845032
train_iter_loss: 0.30981898307800293
train_iter_loss: 0.5192570686340332
train_iter_loss: 0.2666519284248352
train_iter_loss: 0.2322303205728531
train_iter_loss: 0.2952117323875427
train_iter_loss: 0.21361136436462402
train_iter_loss: 0.41428887844085693
train_iter_loss: 0.40145495533943176
train_iter_loss: 0.3346254229545593
train_iter_loss: 0.2187650501728058
train_iter_loss: 0.35678112506866455
train_iter_loss: 0.29743775725364685
train_iter_loss: 0.2589138150215149
train_iter_loss: 0.2657255232334137
train_iter_loss: 0.5583749413490295
train_iter_loss: 0.3151303827762604
train_iter_loss: 0.3180505335330963
train_iter_loss: 0.29869499802589417
train_iter_loss: 0.3819405138492584
train_iter_loss: 0.3473506271839142
train_iter_loss: 0.3949708640575409
train_iter_loss: 0.3426530063152313
train_iter_loss: 0.32522252202033997
train_iter_loss: 0.2581034004688263
train_iter_loss: 0.35704681277275085
train_iter_loss: 0.35168808698654175
train_iter_loss: 0.36059531569480896
train_iter_loss: 0.369205117225647
train_iter_loss: 0.28423967957496643
train_iter_loss: 0.23064887523651123
train_iter_loss: 0.3100948929786682
train_iter_loss: 0.27526894211769104
train_iter_loss: 0.2702508866786957
train_iter_loss: 0.32886457443237305
train_iter_loss: 0.4022928476333618
train_iter_loss: 0.25672394037246704
train_iter_loss: 0.4247252941131592
train_iter_loss: 0.2246702015399933
train_iter_loss: 0.43408259749412537
train_iter_loss: 0.407350093126297
train_iter_loss: 0.3465319871902466
train_iter_loss: 0.2866731584072113
train_iter_loss: 0.3215002417564392
train_iter_loss: 0.33712971210479736
train_iter_loss: 0.3313179016113281
train_iter_loss: 0.15854519605636597
train_iter_loss: 0.18673661351203918
train_iter_loss: 0.28473925590515137
train_iter_loss: 0.31338778138160706
train_iter_loss: 0.25992831587791443
train_iter_loss: 0.2787141501903534
train_iter_loss: 0.3777278959751129
train_iter_loss: 0.339253693819046
train_iter_loss: 0.3705589771270752
train_iter_loss: 0.4162560999393463
train_iter_loss: 0.3096814751625061
train_iter_loss: 0.3040672540664673
train_iter_loss: 0.3347533345222473
train_iter_loss: 0.30715277791023254
train_iter_loss: 0.2757372558116913
train_iter_loss: 0.32995179295539856
train_iter_loss: 0.28130027651786804
train_iter_loss: 0.3532842695713043
train_iter_loss: 0.22300460934638977
train_iter_loss: 0.3723095953464508
train_iter_loss: 0.365416944026947
train_iter_loss: 0.2771701216697693
train_iter_loss: 0.26123887300491333
train_iter_loss: 0.2937995195388794
train_iter_loss: 0.26607948541641235
train_iter_loss: 0.38444918394088745
train_iter_loss: 0.39023199677467346
train_iter_loss: 0.31675755977630615
train_iter_loss: 0.22132550179958344
train_iter_loss: 0.3224576711654663
train_iter_loss: 0.34050512313842773
train_iter_loss: 0.4029350280761719
train_iter_loss: 0.2600471079349518
train_iter_loss: 0.431354820728302
train_iter_loss: 0.33278316259384155
train_iter_loss: 0.29088667035102844
train_iter_loss: 0.31336548924446106
train_iter_loss: 0.44887709617614746
train_iter_loss: 0.2746484577655792
train_iter_loss: 0.3857983648777008
train_iter_loss: 0.3415299355983734
train_iter_loss: 0.3951817452907562
train_iter_loss: 0.42179232835769653
train_iter_loss: 0.2832954227924347
train_iter_loss: 0.3432108759880066
train_iter_loss: 0.3039301037788391
train_iter_loss: 0.37522509694099426
train_iter_loss: 0.30060040950775146
train_iter_loss: 0.41802600026130676
train_iter_loss: 0.2730204164981842
train_iter_loss: 0.2567479908466339
train_iter_loss: 0.19393329322338104
train_iter_loss: 0.29517292976379395
train_iter_loss: 0.33987802267074585
train loss :0.3265
---------------------
Validation seg loss: 0.3991212147762472 at epoch 53
epoch =     54/  1000, exp = train
train_iter_loss: 0.2461032122373581
train_iter_loss: 0.3996441662311554
train_iter_loss: 0.1972488909959793
train_iter_loss: 0.35472598671913147
train_iter_loss: 0.3895288109779358
train_iter_loss: 0.37240609526634216
train_iter_loss: 0.4321965277194977
train_iter_loss: 0.2556917071342468
train_iter_loss: 0.24292916059494019
train_iter_loss: 0.37147465348243713
train_iter_loss: 0.19179598987102509
train_iter_loss: 0.33217135071754456
train_iter_loss: 0.4887975752353668
train_iter_loss: 0.4484577178955078
train_iter_loss: 0.31826093792915344
train_iter_loss: 0.2180086225271225
train_iter_loss: 0.31205859780311584
train_iter_loss: 0.3536810576915741
train_iter_loss: 0.32089337706565857
train_iter_loss: 0.21247108280658722
train_iter_loss: 0.2699606120586395
train_iter_loss: 0.430019348859787
train_iter_loss: 0.28157588839530945
train_iter_loss: 0.4414207339286804
train_iter_loss: 0.3990238308906555
train_iter_loss: 0.39584848284721375
train_iter_loss: 0.34109413623809814
train_iter_loss: 0.25916367769241333
train_iter_loss: 0.26275837421417236
train_iter_loss: 0.2953963875770569
train_iter_loss: 0.3023732602596283
train_iter_loss: 0.09915369004011154
train_iter_loss: 0.2726537883281708
train_iter_loss: 0.5663396120071411
train_iter_loss: 0.43025264143943787
train_iter_loss: 0.23084548115730286
train_iter_loss: 0.3211095929145813
train_iter_loss: 0.18427693843841553
train_iter_loss: 0.39032453298568726
train_iter_loss: 0.4044394791126251
train_iter_loss: 0.4492622911930084
train_iter_loss: 0.4258624315261841
train_iter_loss: 0.2558590769767761
train_iter_loss: 0.4952338635921478
train_iter_loss: 0.42762628197669983
train_iter_loss: 0.38255825638771057
train_iter_loss: 0.3467520773410797
train_iter_loss: 0.40553462505340576
train_iter_loss: 0.4593822658061981
train_iter_loss: 0.27569130063056946
train_iter_loss: 0.26575830578804016
train_iter_loss: 0.46382802724838257
train_iter_loss: 0.5326504707336426
train_iter_loss: 0.21213595569133759
train_iter_loss: 0.5024092793464661
train_iter_loss: 0.16457009315490723
train_iter_loss: 0.1979004144668579
train_iter_loss: 0.39383572340011597
train_iter_loss: 0.3114851415157318
train_iter_loss: 0.48136746883392334
train_iter_loss: 0.38469046354293823
train_iter_loss: 0.43407025933265686
train_iter_loss: 0.4113030433654785
train_iter_loss: 0.295650452375412
train_iter_loss: 0.22288790345191956
train_iter_loss: 0.20910334587097168
train_iter_loss: 0.20390838384628296
train_iter_loss: 0.2825804054737091
train_iter_loss: 0.1736951619386673
train_iter_loss: 0.2394094616174698
train_iter_loss: 0.2695607841014862
train_iter_loss: 0.3093087375164032
train_iter_loss: 0.34883129596710205
train_iter_loss: 0.4069710969924927
train_iter_loss: 0.46898818016052246
train_iter_loss: 0.24665549397468567
train_iter_loss: 0.383249431848526
train_iter_loss: 0.23954421281814575
train_iter_loss: 0.31368523836135864
train_iter_loss: 0.2282785326242447
train_iter_loss: 0.4147163927555084
train_iter_loss: 0.25063514709472656
train_iter_loss: 0.20910276472568512
train_iter_loss: 0.23500441014766693
train_iter_loss: 0.29718446731567383
train_iter_loss: 0.3208272457122803
train_iter_loss: 0.3068223297595978
train_iter_loss: 0.3317066729068756
train_iter_loss: 0.2849237024784088
train_iter_loss: 0.2892973721027374
train_iter_loss: 0.29858317971229553
train_iter_loss: 0.4425775706768036
train_iter_loss: 0.27839043736457825
train_iter_loss: 0.441658616065979
train_iter_loss: 0.3812575936317444
train_iter_loss: 0.24962902069091797
train_iter_loss: 0.34480616450309753
train_iter_loss: 0.380703330039978
train_iter_loss: 0.3934233784675598
train_iter_loss: 0.21154625713825226
train loss :0.3325
---------------------
Validation seg loss: 0.39647036407775477 at epoch 54
epoch =     55/  1000, exp = train
train_iter_loss: 0.12609431147575378
train_iter_loss: 0.3487307131290436
train_iter_loss: 0.3130255937576294
train_iter_loss: 0.41375288367271423
train_iter_loss: 0.31321001052856445
train_iter_loss: 0.5144127011299133
train_iter_loss: 0.36905303597450256
train_iter_loss: 0.35669201612472534
train_iter_loss: 0.44068652391433716
train_iter_loss: 0.3639417588710785
train_iter_loss: 0.37325963377952576
train_iter_loss: 0.2967272400856018
train_iter_loss: 0.34600359201431274
train_iter_loss: 0.2831314206123352
train_iter_loss: 0.27011606097221375
train_iter_loss: 0.25899723172187805
train_iter_loss: 0.25618013739585876
train_iter_loss: 0.2419748306274414
train_iter_loss: 0.25294244289398193
train_iter_loss: 0.3524353802204132
train_iter_loss: 0.4430924355983734
train_iter_loss: 0.3465472459793091
train_iter_loss: 0.3242529332637787
train_iter_loss: 0.4009562134742737
train_iter_loss: 0.2701094448566437
train_iter_loss: 0.30529704689979553
train_iter_loss: 0.3400495648384094
train_iter_loss: 0.16258931159973145
train_iter_loss: 0.286898136138916
train_iter_loss: 0.23032483458518982
train_iter_loss: 0.3164260685443878
train_iter_loss: 0.4286195635795593
train_iter_loss: 0.2777293026447296
train_iter_loss: 0.30031538009643555
train_iter_loss: 0.25181829929351807
train_iter_loss: 0.4457278847694397
train_iter_loss: 0.303083598613739
train_iter_loss: 0.35375094413757324
train_iter_loss: 0.17796608805656433
train_iter_loss: 0.3453824520111084
train_iter_loss: 0.2782329320907593
train_iter_loss: 0.2689321041107178
train_iter_loss: 0.29770806431770325
train_iter_loss: 0.21873825788497925
train_iter_loss: 0.4675683081150055
train_iter_loss: 0.20836205780506134
train_iter_loss: 0.2843697965145111
train_iter_loss: 0.33818909525871277
train_iter_loss: 0.3284654915332794
train_iter_loss: 0.26380595564842224
train_iter_loss: 0.23208647966384888
train_iter_loss: 0.2870294749736786
train_iter_loss: 0.4940485656261444
train_iter_loss: 0.4004882872104645
train_iter_loss: 0.4026392102241516
train_iter_loss: 0.2477148175239563
train_iter_loss: 0.35507839918136597
train_iter_loss: 0.3772497773170471
train_iter_loss: 0.26091739535331726
train_iter_loss: 0.33177071809768677
train_iter_loss: 0.24443425238132477
train_iter_loss: 0.2732224464416504
train_iter_loss: 0.30456840991973877
train_iter_loss: 0.3350512683391571
train_iter_loss: 0.21141532063484192
train_iter_loss: 0.1783110499382019
train_iter_loss: 0.32374709844589233
train_iter_loss: 0.26276734471321106
train_iter_loss: 0.31658729910850525
train_iter_loss: 0.209851935505867
train_iter_loss: 0.42751216888427734
train_iter_loss: 0.2322813719511032
train_iter_loss: 0.29996755719184875
train_iter_loss: 0.413679838180542
train_iter_loss: 0.28296470642089844
train_iter_loss: 0.30198758840560913
train_iter_loss: 0.23095744848251343
train_iter_loss: 0.397232323884964
train_iter_loss: 0.32701268792152405
train_iter_loss: 0.19377420842647552
train_iter_loss: 0.3135901391506195
train_iter_loss: 0.4903484880924225
train_iter_loss: 0.28764766454696655
train_iter_loss: 0.5900408029556274
train_iter_loss: 0.47685518860816956
train_iter_loss: 0.3734218180179596
train_iter_loss: 0.4795033037662506
train_iter_loss: 0.3694829046726227
train_iter_loss: 0.42303597927093506
train_iter_loss: 0.23307856917381287
train_iter_loss: 0.3390887379646301
train_iter_loss: 0.2836686670780182
train_iter_loss: 0.4993678331375122
train_iter_loss: 0.3251431882381439
train_iter_loss: 0.3509922921657562
train_iter_loss: 0.6608186364173889
train_iter_loss: 0.36695003509521484
train_iter_loss: 0.33641788363456726
train_iter_loss: 0.3417690694332123
train_iter_loss: 0.3044624626636505
train loss :0.3309
---------------------
Validation seg loss: 0.3854854404399136 at epoch 55
epoch =     56/  1000, exp = train
train_iter_loss: 0.2809295952320099
train_iter_loss: 0.3960186243057251
train_iter_loss: 0.33307957649230957
train_iter_loss: 0.2745453417301178
train_iter_loss: 0.4108709692955017
train_iter_loss: 0.35790833830833435
train_iter_loss: 0.2063896656036377
train_iter_loss: 0.19951672852039337
train_iter_loss: 0.36471614241600037
train_iter_loss: 0.2335629165172577
train_iter_loss: 0.3942161798477173
train_iter_loss: 0.4014769494533539
train_iter_loss: 0.3851407468318939
train_iter_loss: 0.2225896567106247
train_iter_loss: 0.38426676392555237
train_iter_loss: 0.23430581390857697
train_iter_loss: 0.3737668991088867
train_iter_loss: 0.3235958218574524
train_iter_loss: 0.16415414214134216
train_iter_loss: 0.23416036367416382
train_iter_loss: 0.287707656621933
train_iter_loss: 0.34447646141052246
train_iter_loss: 0.23726966977119446
train_iter_loss: 0.23902761936187744
train_iter_loss: 0.5035950541496277
train_iter_loss: 0.32104530930519104
train_iter_loss: 0.23478230834007263
train_iter_loss: 0.15288957953453064
train_iter_loss: 0.25156402587890625
train_iter_loss: 0.24733059108257294
train_iter_loss: 0.4764283001422882
train_iter_loss: 0.37380605936050415
train_iter_loss: 0.3015124797821045
train_iter_loss: 0.4502054452896118
train_iter_loss: 0.2994243800640106
train_iter_loss: 0.2351899892091751
train_iter_loss: 0.33108681440353394
train_iter_loss: 0.34719231724739075
train_iter_loss: 0.4538871645927429
train_iter_loss: 0.3173828721046448
train_iter_loss: 0.3942147195339203
train_iter_loss: 0.28824582695961
train_iter_loss: 0.2801416516304016
train_iter_loss: 0.24590010941028595
train_iter_loss: 0.20625196397304535
train_iter_loss: 0.37301188707351685
train_iter_loss: 0.5347330570220947
train_iter_loss: 0.5115119218826294
train_iter_loss: 0.3071315288543701
train_iter_loss: 0.18566758930683136
train_iter_loss: 0.41192561388015747
train_iter_loss: 0.3182286024093628
train_iter_loss: 0.3738783895969391
train_iter_loss: 0.30163806676864624
train_iter_loss: 0.3122562766075134
train_iter_loss: 0.387506902217865
train_iter_loss: 0.4898737370967865
train_iter_loss: 0.25775983929634094
train_iter_loss: 0.2920319736003876
train_iter_loss: 0.46656811237335205
train_iter_loss: 0.35960012674331665
train_iter_loss: 0.27210038900375366
train_iter_loss: 0.27232128381729126
train_iter_loss: 0.2857978045940399
train_iter_loss: 0.5368351936340332
train_iter_loss: 0.3107500970363617
train_iter_loss: 0.2946716845035553
train_iter_loss: 0.25077542662620544
train_iter_loss: 0.32345518469810486
train_iter_loss: 0.33321261405944824
train_iter_loss: 0.3499026596546173
train_iter_loss: 0.41068485379219055
train_iter_loss: 0.4468759596347809
train_iter_loss: 0.3336828351020813
train_iter_loss: 0.5795121192932129
train_iter_loss: 0.32683539390563965
train_iter_loss: 0.2624443769454956
train_iter_loss: 0.20359942317008972
train_iter_loss: 0.3313754200935364
train_iter_loss: 0.34808284044265747
train_iter_loss: 0.20593814551830292
train_iter_loss: 0.29703912138938904
train_iter_loss: 0.5848274827003479
train_iter_loss: 0.48278331756591797
train_iter_loss: 0.4398898184299469
train_iter_loss: 0.33066296577453613
train_iter_loss: 0.17787428200244904
train_iter_loss: 0.397255003452301
train_iter_loss: 0.22565270960330963
train_iter_loss: 0.4002179503440857
train_iter_loss: 0.3004571199417114
train_iter_loss: 0.2866980731487274
train_iter_loss: 0.3221309781074524
train_iter_loss: 0.23415595293045044
train_iter_loss: 0.2344309389591217
train_iter_loss: 0.3098958432674408
train_iter_loss: 0.2366761565208435
train_iter_loss: 0.47462737560272217
train_iter_loss: 0.23536059260368347
train_iter_loss: 0.24528078734874725
train loss :0.3310
---------------------
Validation seg loss: 0.39887365227881466 at epoch 56
epoch =     57/  1000, exp = train
train_iter_loss: 0.2303139716386795
train_iter_loss: 0.40570148825645447
train_iter_loss: 0.2846929430961609
train_iter_loss: 0.30172228813171387
train_iter_loss: 0.41203179955482483
train_iter_loss: 0.32518091797828674
train_iter_loss: 0.35438159108161926
train_iter_loss: 0.3206384778022766
train_iter_loss: 0.17668622732162476
train_iter_loss: 0.2685326337814331
train_iter_loss: 0.36994144320487976
train_iter_loss: 0.3151024878025055
train_iter_loss: 0.42806389927864075
train_iter_loss: 0.23876698315143585
train_iter_loss: 0.26962774991989136
train_iter_loss: 0.35945576429367065
train_iter_loss: 0.36576762795448303
train_iter_loss: 0.5308062434196472
train_iter_loss: 0.32579100131988525
train_iter_loss: 0.4044244885444641
train_iter_loss: 0.40047821402549744
train_iter_loss: 0.33786383271217346
train_iter_loss: 0.28596776723861694
train_iter_loss: 0.32434678077697754
train_iter_loss: 0.3419351875782013
train_iter_loss: 0.24806475639343262
train_iter_loss: 0.4384375810623169
train_iter_loss: 0.25273916125297546
train_iter_loss: 0.313854843378067
train_iter_loss: 0.27224940061569214
train_iter_loss: 0.2521266043186188
train_iter_loss: 0.2960190773010254
train_iter_loss: 0.3073641359806061
train_iter_loss: 0.34441328048706055
train_iter_loss: 0.21687498688697815
train_iter_loss: 0.2812267243862152
train_iter_loss: 0.27519431710243225
train_iter_loss: 0.350614994764328
train_iter_loss: 0.2271510362625122
train_iter_loss: 0.2586705982685089
train_iter_loss: 0.2745414972305298
train_iter_loss: 0.3061316907405853
train_iter_loss: 0.27817025780677795
train_iter_loss: 0.27099233865737915
train_iter_loss: 0.3373836874961853
train_iter_loss: 0.3755215108394623
train_iter_loss: 0.3924870491027832
train_iter_loss: 0.2768397629261017
train_iter_loss: 0.28277817368507385
train_iter_loss: 0.21819739043712616
train_iter_loss: 0.38141462206840515
train_iter_loss: 0.24861350655555725
train_iter_loss: 0.31038904190063477
train_iter_loss: 0.5522688627243042
train_iter_loss: 0.5082524418830872
train_iter_loss: 0.23393069207668304
train_iter_loss: 0.47041642665863037
train_iter_loss: 0.3289012908935547
train_iter_loss: 0.23101530969142914
train_iter_loss: 0.29772767424583435
train_iter_loss: 0.33734235167503357
train_iter_loss: 0.32949814200401306
train_iter_loss: 0.2530735433101654
train_iter_loss: 0.331925630569458
train_iter_loss: 0.21020004153251648
train_iter_loss: 0.24861660599708557
train_iter_loss: 0.2708757221698761
train_iter_loss: 0.37129124999046326
train_iter_loss: 0.32392895221710205
train_iter_loss: 0.40869808197021484
train_iter_loss: 0.2514680325984955
train_iter_loss: 0.3607760965824127
train_iter_loss: 0.22985991835594177
train_iter_loss: 0.3116418421268463
train_iter_loss: 0.10847698152065277
train_iter_loss: 0.33899205923080444
train_iter_loss: 0.3074866831302643
train_iter_loss: 0.3088153600692749
train_iter_loss: 0.18334800004959106
train_iter_loss: 0.3221435248851776
train_iter_loss: 0.41563794016838074
train_iter_loss: 0.39932751655578613
train_iter_loss: 0.2662357985973358
train_iter_loss: 0.23196884989738464
train_iter_loss: 0.2590944170951843
train_iter_loss: 0.20467159152030945
train_iter_loss: 0.2927555739879608
train_iter_loss: 0.3678376078605652
train_iter_loss: 0.6342461705207825
train_iter_loss: 0.42353397607803345
train_iter_loss: 0.3100689649581909
train_iter_loss: 0.2695804834365845
train_iter_loss: 0.38947901129722595
train_iter_loss: 0.4275383949279785
train_iter_loss: 0.2556605637073517
train_iter_loss: 0.2944805920124054
train_iter_loss: 0.2577996253967285
train_iter_loss: 0.284816175699234
train_iter_loss: 0.440789133310318
train_iter_loss: 0.3004550635814667
train loss :0.3215
---------------------
Validation seg loss: 0.3989944103753792 at epoch 57
epoch =     58/  1000, exp = train
train_iter_loss: 0.19287322461605072
train_iter_loss: 0.4228162467479706
train_iter_loss: 0.30736520886421204
train_iter_loss: 0.3300820291042328
train_iter_loss: 0.2918391823768616
train_iter_loss: 0.33408188819885254
train_iter_loss: 0.38117173314094543
train_iter_loss: 0.24918892979621887
train_iter_loss: 0.25761786103248596
train_iter_loss: 0.22262929379940033
train_iter_loss: 0.289932519197464
train_iter_loss: 0.45131146907806396
train_iter_loss: 0.17637303471565247
train_iter_loss: 0.3134589195251465
train_iter_loss: 0.23817777633666992
train_iter_loss: 0.29840996861457825
train_iter_loss: 0.17109811305999756
train_iter_loss: 0.3282772898674011
train_iter_loss: 0.4018462002277374
train_iter_loss: 0.27401503920555115
train_iter_loss: 0.3208184242248535
train_iter_loss: 0.38206303119659424
train_iter_loss: 0.4356892704963684
train_iter_loss: 0.2978992760181427
train_iter_loss: 0.3153916299343109
train_iter_loss: 0.4038086533546448
train_iter_loss: 0.2652970552444458
train_iter_loss: 0.24698680639266968
train_iter_loss: 0.3474884033203125
train_iter_loss: 0.24421870708465576
train_iter_loss: 0.33474597334861755
train_iter_loss: 0.2856495976448059
train_iter_loss: 0.27249661087989807
train_iter_loss: 0.3254346251487732
train_iter_loss: 0.3403621315956116
train_iter_loss: 0.31412163376808167
train_iter_loss: 0.3711804449558258
train_iter_loss: 0.2762274444103241
train_iter_loss: 0.28083306550979614
train_iter_loss: 0.24416476488113403
train_iter_loss: 0.32357537746429443
train_iter_loss: 0.24214427173137665
train_iter_loss: 0.23813346028327942
train_iter_loss: 0.4021358788013458
train_iter_loss: 0.2170943170785904
train_iter_loss: 0.4817745089530945
train_iter_loss: 0.3283022940158844
train_iter_loss: 0.18810419738292694
train_iter_loss: 0.29814592003822327
train_iter_loss: 0.5231366157531738
train_iter_loss: 0.2791227698326111
train_iter_loss: 0.3150489330291748
train_iter_loss: 0.45183977484703064
train_iter_loss: 0.2684553265571594
train_iter_loss: 0.3949468433856964
train_iter_loss: 0.49491268396377563
train_iter_loss: 0.24393226206302643
train_iter_loss: 0.3598329424858093
train_iter_loss: 0.2079724371433258
train_iter_loss: 0.3135862350463867
train_iter_loss: 0.311870276927948
train_iter_loss: 0.2654898762702942
train_iter_loss: 0.2594926655292511
train_iter_loss: 0.31415534019470215
train_iter_loss: 0.2610434591770172
train_iter_loss: 0.4398094713687897
train_iter_loss: 0.5553114414215088
train_iter_loss: 0.17482838034629822
train_iter_loss: 0.3446885347366333
train_iter_loss: 0.2342967987060547
train_iter_loss: 0.2565496265888214
train_iter_loss: 0.19880326092243195
train_iter_loss: 0.2863512933254242
train_iter_loss: 0.3245728015899658
train_iter_loss: 0.405295729637146
train_iter_loss: 0.2491592913866043
train_iter_loss: 0.31979069113731384
train_iter_loss: 0.2902865409851074
train_iter_loss: 0.2875397205352783
train_iter_loss: 0.452642560005188
train_iter_loss: 0.2532251179218292
train_iter_loss: 0.36516979336738586
train_iter_loss: 0.2880710959434509
train_iter_loss: 0.2866618037223816
train_iter_loss: 0.23113159835338593
train_iter_loss: 0.556917667388916
train_iter_loss: 0.2650778293609619
train_iter_loss: 0.24383282661437988
train_iter_loss: 0.34225553274154663
train_iter_loss: 0.29137787222862244
train_iter_loss: 0.285461962223053
train_iter_loss: 0.29253461956977844
train_iter_loss: 0.3473028838634491
train_iter_loss: 0.20023943483829498
train_iter_loss: 0.28338128328323364
train_iter_loss: 0.26346758008003235
train_iter_loss: 0.3019167184829712
train_iter_loss: 0.3879010081291199
train_iter_loss: 0.4537844657897949
train_iter_loss: 0.27181926369667053
train loss :0.3157
---------------------
Validation seg loss: 0.3973911186891063 at epoch 58
epoch =     59/  1000, exp = train
train_iter_loss: 0.3635035455226898
train_iter_loss: 0.2985021471977234
train_iter_loss: 0.34057116508483887
train_iter_loss: 0.26487353444099426
train_iter_loss: 0.24855980277061462
train_iter_loss: 0.29301145672798157
train_iter_loss: 0.29899686574935913
train_iter_loss: 0.3145230710506439
train_iter_loss: 0.17254747450351715
train_iter_loss: 0.22037988901138306
train_iter_loss: 0.17915739119052887
train_iter_loss: 0.17622892558574677
train_iter_loss: 0.31623271107673645
train_iter_loss: 0.3283683955669403
train_iter_loss: 0.3408883810043335
train_iter_loss: 0.40314969420433044
train_iter_loss: 0.3132621943950653
train_iter_loss: 0.3970385789871216
train_iter_loss: 0.24677063524723053
train_iter_loss: 0.5082717537879944
train_iter_loss: 0.42531144618988037
train_iter_loss: 0.3119610548019409
train_iter_loss: 0.2560977637767792
train_iter_loss: 0.48106974363327026
train_iter_loss: 0.3787320554256439
train_iter_loss: 0.1453450471162796
train_iter_loss: 0.25606799125671387
train_iter_loss: 0.3371874690055847
train_iter_loss: 0.3435096740722656
train_iter_loss: 0.24468544125556946
train_iter_loss: 0.49038881063461304
train_iter_loss: 0.2603193521499634
train_iter_loss: 0.34245243668556213
train_iter_loss: 0.2044365406036377
train_iter_loss: 0.28108882904052734
train_iter_loss: 0.37938597798347473
train_iter_loss: 0.4082700312137604
train_iter_loss: 0.364250510931015
train_iter_loss: 0.2613523304462433
train_iter_loss: 0.34051668643951416
train_iter_loss: 0.24471302330493927
train_iter_loss: 0.274247407913208
train_iter_loss: 0.3367573618888855
train_iter_loss: 0.30412015318870544
train_iter_loss: 0.3546830415725708
train_iter_loss: 0.2820499539375305
train_iter_loss: 0.34469595551490784
train_iter_loss: 0.35006722807884216
train_iter_loss: 0.208818256855011
train_iter_loss: 0.36723288893699646
train_iter_loss: 0.3451823890209198
train_iter_loss: 0.38870078325271606
train_iter_loss: 0.3685763478279114
train_iter_loss: 0.32486870884895325
train_iter_loss: 0.20376665890216827
train_iter_loss: 0.40401995182037354
train_iter_loss: 0.2930128574371338
train_iter_loss: 0.17741353809833527
train_iter_loss: 0.3348657190799713
train_iter_loss: 0.23176439106464386
train_iter_loss: 0.3798745572566986
train_iter_loss: 0.3085198998451233
train_iter_loss: 0.2846325635910034
train_iter_loss: 0.267264723777771
train_iter_loss: 0.3645836114883423
train_iter_loss: 0.26987648010253906
train_iter_loss: 0.3587811291217804
train_iter_loss: 0.21879135072231293
train_iter_loss: 0.4448385238647461
train_iter_loss: 0.4197896122932434
train_iter_loss: 0.3024674654006958
train_iter_loss: 0.4265071451663971
train_iter_loss: 0.3694496750831604
train_iter_loss: 0.20947600901126862
train_iter_loss: 0.21466296911239624
train_iter_loss: 0.49660512804985046
train_iter_loss: 0.35839730501174927
train_iter_loss: 0.36763593554496765
train_iter_loss: 0.39733776450157166
train_iter_loss: 0.44276583194732666
train_iter_loss: 0.26573172211647034
train_iter_loss: 0.3671109974384308
train_iter_loss: 0.3579416871070862
train_iter_loss: 0.2578715980052948
train_iter_loss: 0.24575647711753845
train_iter_loss: 0.17391297221183777
train_iter_loss: 0.4101139307022095
train_iter_loss: 0.2658778131008148
train_iter_loss: 0.3554042875766754
train_iter_loss: 0.2470584362745285
train_iter_loss: 0.3251105546951294
train_iter_loss: 0.3156431019306183
train_iter_loss: 0.3634161055088043
train_iter_loss: 0.34318849444389343
train_iter_loss: 0.21115192770957947
train_iter_loss: 0.3490543067455292
train_iter_loss: 0.30279406905174255
train_iter_loss: 0.3576301038265228
train_iter_loss: 0.3088827431201935
train_iter_loss: 0.21448491513729095
train loss :0.3193
---------------------
Validation seg loss: 0.3952362107569879 at epoch 59
epoch =     60/  1000, exp = train
train_iter_loss: 0.27601945400238037
train_iter_loss: 0.3361265957355499
train_iter_loss: 0.379940003156662
train_iter_loss: 0.2795131206512451
train_iter_loss: 0.27432140707969666
train_iter_loss: 0.3742803633213043
train_iter_loss: 0.27050042152404785
train_iter_loss: 0.3445364534854889
train_iter_loss: 0.2905386686325073
train_iter_loss: 0.29208433628082275
train_iter_loss: 0.2966453433036804
train_iter_loss: 0.4772096872329712
train_iter_loss: 0.2797180712223053
train_iter_loss: 0.3971790075302124
train_iter_loss: 0.29433003067970276
train_iter_loss: 0.2831399440765381
train_iter_loss: 0.4140969216823578
train_iter_loss: 0.29406657814979553
train_iter_loss: 0.22560735046863556
train_iter_loss: 0.28446701169013977
train_iter_loss: 0.30297958850860596
train_iter_loss: 0.26407626271247864
train_iter_loss: 0.21087820827960968
train_iter_loss: 0.33285990357398987
train_iter_loss: 0.39021575450897217
train_iter_loss: 0.17286038398742676
train_iter_loss: 0.14000356197357178
train_iter_loss: 0.5165408253669739
train_iter_loss: 0.23142124712467194
train_iter_loss: 0.2120351642370224
train_iter_loss: 0.394137978553772
train_iter_loss: 0.33365747332572937
train_iter_loss: 0.22679360210895538
train_iter_loss: 0.2941758930683136
train_iter_loss: 0.1993028074502945
train_iter_loss: 0.36411863565444946
train_iter_loss: 0.21225939691066742
train_iter_loss: 0.23762375116348267
train_iter_loss: 0.5781400203704834
train_iter_loss: 0.46190688014030457
train_iter_loss: 0.3536490201950073
train_iter_loss: 0.3211662769317627
train_iter_loss: 0.2652114927768707
train_iter_loss: 0.3665470778942108
train_iter_loss: 0.18712012469768524
train_iter_loss: 0.38104501366615295
train_iter_loss: 0.2490757554769516
train_iter_loss: 0.196914404630661
train_iter_loss: 0.3939587473869324
train_iter_loss: 0.34051796793937683
train_iter_loss: 0.29762086272239685
train_iter_loss: 0.6536571979522705
train_iter_loss: 0.27535194158554077
train_iter_loss: 0.24435751140117645
train_iter_loss: 0.36360862851142883
train_iter_loss: 0.3940266966819763
train_iter_loss: 0.3643127977848053
train_iter_loss: 0.40220969915390015
train_iter_loss: 0.2914837896823883
train_iter_loss: 0.1754583716392517
train_iter_loss: 0.23644065856933594
train_iter_loss: 0.2752939760684967
train_iter_loss: 0.293266236782074
train_iter_loss: 0.4352402687072754
train_iter_loss: 0.3414502441883087
train_iter_loss: 0.2531489133834839
train_iter_loss: 0.42436861991882324
train_iter_loss: 0.24822968244552612
train_iter_loss: 0.43004679679870605
train_iter_loss: 0.373585045337677
train_iter_loss: 0.33677569031715393
train_iter_loss: 0.3046409785747528
train_iter_loss: 0.3058623969554901
train_iter_loss: 0.12219996005296707
train_iter_loss: 0.35367658734321594
train_iter_loss: 0.3300480246543884
train_iter_loss: 0.36831849813461304
train_iter_loss: 0.4867231249809265
train_iter_loss: 0.17777501046657562
train_iter_loss: 0.36794471740722656
train_iter_loss: 0.3106243312358856
train_iter_loss: 0.42295801639556885
train_iter_loss: 0.30529269576072693
train_iter_loss: 0.312460720539093
train_iter_loss: 0.44142982363700867
train_iter_loss: 0.25962239503860474
train_iter_loss: 0.22986693680286407
train_iter_loss: 0.3504902720451355
train_iter_loss: 0.2635360658168793
train_iter_loss: 0.2037057876586914
train_iter_loss: 0.3020934760570526
train_iter_loss: 0.29121819138526917
train_iter_loss: 0.30902358889579773
train_iter_loss: 0.39521703124046326
train_iter_loss: 0.2371625453233719
train_iter_loss: 0.36854323744773865
train_iter_loss: 0.2841850519180298
train_iter_loss: 0.46061965823173523
train_iter_loss: 0.3994203507900238
train_iter_loss: 0.34099721908569336
train loss :0.3223
---------------------
Validation seg loss: 0.3879917038182886 at epoch 60
epoch =     61/  1000, exp = train
train_iter_loss: 0.2776927947998047
train_iter_loss: 0.27935415506362915
train_iter_loss: 0.3415141701698303
train_iter_loss: 0.2782213091850281
train_iter_loss: 0.37979915738105774
train_iter_loss: 0.29478850960731506
train_iter_loss: 0.24787628650665283
train_iter_loss: 0.44943416118621826
train_iter_loss: 0.3059973120689392
train_iter_loss: 0.24078011512756348
train_iter_loss: 0.2882002890110016
train_iter_loss: 0.3162141740322113
train_iter_loss: 0.26167115569114685
train_iter_loss: 0.217131108045578
train_iter_loss: 0.30354875326156616
train_iter_loss: 0.29180869460105896
train_iter_loss: 0.26251503825187683
train_iter_loss: 0.30007752776145935
train_iter_loss: 0.3399205207824707
train_iter_loss: 0.37923142313957214
train_iter_loss: 0.3857796788215637
train_iter_loss: 0.5274136066436768
train_iter_loss: 0.2596907913684845
train_iter_loss: 0.18442213535308838
train_iter_loss: 0.38596612215042114
train_iter_loss: 0.39013954997062683
train_iter_loss: 0.3117300271987915
train_iter_loss: 0.39688098430633545
train_iter_loss: 0.41117486357688904
train_iter_loss: 0.38195541501045227
train_iter_loss: 0.3265077471733093
train_iter_loss: 0.2814030647277832
train_iter_loss: 0.3088587522506714
train_iter_loss: 0.5023406744003296
train_iter_loss: 0.25473132729530334
train_iter_loss: 0.39107298851013184
train_iter_loss: 0.20733477175235748
train_iter_loss: 0.4932296574115753
train_iter_loss: 0.3069376051425934
train_iter_loss: 0.31340134143829346
train_iter_loss: 0.24863885343074799
train_iter_loss: 0.38294535875320435
train_iter_loss: 0.23233263194561005
train_iter_loss: 0.4547811448574066
train_iter_loss: 0.39255547523498535
train_iter_loss: 0.2627246677875519
train_iter_loss: 0.26898330450057983
train_iter_loss: 0.23344366252422333
train_iter_loss: 0.3508133888244629
train_iter_loss: 0.3742493689060211
train_iter_loss: 0.2715851664543152
train_iter_loss: 0.33595022559165955
train_iter_loss: 0.28966549038887024
train_iter_loss: 0.46986037492752075
train_iter_loss: 0.2906133234500885
train_iter_loss: 0.43022486567497253
train_iter_loss: 0.18755179643630981
train_iter_loss: 0.24577362835407257
train_iter_loss: 0.3022586405277252
train_iter_loss: 0.21509987115859985
train_iter_loss: 0.3196791410446167
train_iter_loss: 0.3842841386795044
train_iter_loss: 0.27614638209342957
train_iter_loss: 0.34968844056129456
train_iter_loss: 0.35798171162605286
train_iter_loss: 0.4699581563472748
train_iter_loss: 0.32548952102661133
train_iter_loss: 0.334139883518219
train_iter_loss: 0.20520266890525818
train_iter_loss: 0.5289223194122314
train_iter_loss: 0.2682509124279022
train_iter_loss: 0.32586705684661865
train_iter_loss: 0.34971415996551514
train_iter_loss: 0.3359048366546631
train_iter_loss: 0.14231997728347778
train_iter_loss: 0.3348161280155182
train_iter_loss: 0.34307780861854553
train_iter_loss: 0.2318202555179596
train_iter_loss: 0.2757094204425812
train_iter_loss: 0.3128155469894409
train_iter_loss: 0.2110244333744049
train_iter_loss: 0.2598835825920105
train_iter_loss: 0.5068190097808838
train_iter_loss: 0.1917881816625595
train_iter_loss: 0.2858424484729767
train_iter_loss: 0.1656956821680069
train_iter_loss: 0.282200425863266
train_iter_loss: 0.3426090180873871
train_iter_loss: 0.40724992752075195
train_iter_loss: 0.2872362732887268
train_iter_loss: 0.2747699022293091
train_iter_loss: 0.34777477383613586
train_iter_loss: 0.3172233998775482
train_iter_loss: 0.23619773983955383
train_iter_loss: 0.3262169659137726
train_iter_loss: 0.21573761105537415
train_iter_loss: 0.27397269010543823
train_iter_loss: 0.2802329957485199
train_iter_loss: 0.30696702003479004
train_iter_loss: 0.3656024634838104
train loss :0.3200
---------------------
Validation seg loss: 0.3945080920140136 at epoch 61
epoch =     62/  1000, exp = train
train_iter_loss: 0.39424675703048706
train_iter_loss: 0.19825799763202667
train_iter_loss: 0.36270058155059814
train_iter_loss: 0.37357035279273987
train_iter_loss: 0.3811211884021759
train_iter_loss: 0.2399328351020813
train_iter_loss: 0.7598071098327637
train_iter_loss: 0.44070690870285034
train_iter_loss: 0.39669468998908997
train_iter_loss: 0.2466736137866974
train_iter_loss: 0.2540279030799866
train_iter_loss: 0.2868410348892212
train_iter_loss: 0.32227790355682373
train_iter_loss: 0.29179859161376953
train_iter_loss: 0.2952694892883301
train_iter_loss: 0.3848000764846802
train_iter_loss: 0.41205719113349915
train_iter_loss: 0.19346924126148224
train_iter_loss: 0.3643738031387329
train_iter_loss: 0.1595502346754074
train_iter_loss: 0.19416190683841705
train_iter_loss: 0.40683966875076294
train_iter_loss: 0.3777632415294647
train_iter_loss: 0.28807494044303894
train_iter_loss: 0.351711243391037
train_iter_loss: 0.2706335186958313
train_iter_loss: 0.2981617748737335
train_iter_loss: 0.3457343578338623
train_iter_loss: 0.23843015730381012
train_iter_loss: 0.2219427078962326
train_iter_loss: 0.4024355113506317
train_iter_loss: 0.3050973117351532
train_iter_loss: 0.29978200793266296
train_iter_loss: 0.4950424134731293
train_iter_loss: 0.3027638792991638
train_iter_loss: 0.31599441170692444
train_iter_loss: 0.2869594395160675
train_iter_loss: 0.24551516771316528
train_iter_loss: 0.3091089427471161
train_iter_loss: 0.3067510426044464
train_iter_loss: 0.39886391162872314
train_iter_loss: 0.3251962661743164
train_iter_loss: 0.32672953605651855
train_iter_loss: 0.27266955375671387
train_iter_loss: 0.41671544313430786
train_iter_loss: 0.17639456689357758
train_iter_loss: 0.23204408586025238
train_iter_loss: 0.29549598693847656
train_iter_loss: 0.38909077644348145
train_iter_loss: 0.16577211022377014
train_iter_loss: 0.38133326172828674
train_iter_loss: 0.19569653272628784
train_iter_loss: 0.2759602665901184
train_iter_loss: 0.36971592903137207
train_iter_loss: 0.49225732684135437
train_iter_loss: 0.22999544441699982
train_iter_loss: 0.32755839824676514
train_iter_loss: 0.2874405086040497
train_iter_loss: 0.30980947613716125
train_iter_loss: 0.38525155186653137
train_iter_loss: 0.42102038860321045
train_iter_loss: 0.40880778431892395
train_iter_loss: 0.5040268301963806
train_iter_loss: 0.31495940685272217
train_iter_loss: 0.1981237232685089
train_iter_loss: 0.3144673705101013
train_iter_loss: 0.2882497012615204
train_iter_loss: 0.5415942072868347
train_iter_loss: 0.3275381922721863
train_iter_loss: 0.30980926752090454
train_iter_loss: 0.4114898443222046
train_iter_loss: 0.740990936756134
train_iter_loss: 0.2478768527507782
train_iter_loss: 0.28713151812553406
train_iter_loss: 0.17920909821987152
train_iter_loss: 0.3242287039756775
train_iter_loss: 0.22562026977539062
train_iter_loss: 0.2778768837451935
train_iter_loss: 0.40139278769493103
train_iter_loss: 0.21271595358848572
train_iter_loss: 0.23766262829303741
train_iter_loss: 0.18990784883499146
train_iter_loss: 0.25449442863464355
train_iter_loss: 0.2483227252960205
train_iter_loss: 0.3576377034187317
train_iter_loss: 0.33090266585350037
train_iter_loss: 0.28323668241500854
train_iter_loss: 0.3885672390460968
train_iter_loss: 0.15490464866161346
train_iter_loss: 0.28199025988578796
train_iter_loss: 0.22803713381290436
train_iter_loss: 0.297763466835022
train_iter_loss: 0.22504228353500366
train_iter_loss: 0.3325396478176117
train_iter_loss: 0.41288304328918457
train_iter_loss: 0.3606371283531189
train_iter_loss: 0.20673391222953796
train_iter_loss: 0.22013211250305176
train_iter_loss: 0.22269460558891296
train_iter_loss: 0.30353033542633057
train loss :0.3207
---------------------
Validation seg loss: 0.4034269266006238 at epoch 62
epoch =     63/  1000, exp = train
train_iter_loss: 0.19655872881412506
train_iter_loss: 0.28447407484054565
train_iter_loss: 0.33674395084381104
train_iter_loss: 0.14044393599033356
train_iter_loss: 0.19062736630439758
train_iter_loss: 0.21626342833042145
train_iter_loss: 0.3777719736099243
train_iter_loss: 0.3014453649520874
train_iter_loss: 0.3535490930080414
train_iter_loss: 0.2915390133857727
train_iter_loss: 0.2790095806121826
train_iter_loss: 0.3990641236305237
train_iter_loss: 0.27447253465652466
train_iter_loss: 0.34030577540397644
train_iter_loss: 0.3473961055278778
train_iter_loss: 0.3261832892894745
train_iter_loss: 0.3674018383026123
train_iter_loss: 0.4264310300350189
train_iter_loss: 0.3089663088321686
train_iter_loss: 0.14818409085273743
train_iter_loss: 0.36438724398612976
train_iter_loss: 0.570844829082489
train_iter_loss: 0.28046149015426636
train_iter_loss: 0.3385940194129944
train_iter_loss: 0.3177259564399719
train_iter_loss: 0.38853687047958374
train_iter_loss: 0.25297483801841736
train_iter_loss: 0.291451096534729
train_iter_loss: 0.3268008232116699
train_iter_loss: 0.4561522603034973
train_iter_loss: 0.22280380129814148
train_iter_loss: 0.3278312087059021
train_iter_loss: 0.26717624068260193
train_iter_loss: 0.2562423348426819
train_iter_loss: 0.35181117057800293
train_iter_loss: 0.4573361277580261
train_iter_loss: 0.3639592230319977
train_iter_loss: 0.34481507539749146
train_iter_loss: 0.2540673017501831
train_iter_loss: 0.3940351903438568
train_iter_loss: 0.3096340596675873
train_iter_loss: 0.2420981079339981
train_iter_loss: 0.33998575806617737
train_iter_loss: 0.24466979503631592
train_iter_loss: 0.39402323961257935
train_iter_loss: 0.3681778013706207
train_iter_loss: 0.30469128489494324
train_iter_loss: 0.30544158816337585
train_iter_loss: 0.35943740606307983
train_iter_loss: 0.24689912796020508
train_iter_loss: 0.30118468403816223
train_iter_loss: 0.5009979605674744
train_iter_loss: 0.32547613978385925
train_iter_loss: 0.26652640104293823
train_iter_loss: 0.3538239598274231
train_iter_loss: 0.3861440420150757
train_iter_loss: 0.25570839643478394
train_iter_loss: 0.3294333517551422
train_iter_loss: 0.4086613655090332
train_iter_loss: 0.5021684169769287
train_iter_loss: 0.3680538535118103
train_iter_loss: 0.1933400183916092
train_iter_loss: 0.14923907816410065
train_iter_loss: 0.38018590211868286
train_iter_loss: 0.3403667211532593
train_iter_loss: 0.11702454090118408
train_iter_loss: 0.25719478726387024
train_iter_loss: 0.2678065299987793
train_iter_loss: 0.2951381802558899
train_iter_loss: 0.3798222243785858
train_iter_loss: 0.3089982271194458
train_iter_loss: 0.25191378593444824
train_iter_loss: 0.28094998002052307
train_iter_loss: 0.3098736107349396
train_iter_loss: 0.3058682680130005
train_iter_loss: 0.30823084712028503
train_iter_loss: 0.39806967973709106
train_iter_loss: 0.3608335256576538
train_iter_loss: 0.35770511627197266
train_iter_loss: 0.2623012363910675
train_iter_loss: 0.37639856338500977
train_iter_loss: 0.2808963656425476
train_iter_loss: 0.35862380266189575
train_iter_loss: 0.3013720214366913
train_iter_loss: 0.5115877389907837
train_iter_loss: 0.2724459767341614
train_iter_loss: 0.32307854294776917
train_iter_loss: 0.3691651225090027
train_iter_loss: 0.2229902446269989
train_iter_loss: 0.3248908817768097
train_iter_loss: 0.2370871752500534
train_iter_loss: 0.3838067948818207
train_iter_loss: 0.3340989053249359
train_iter_loss: 0.38742607831954956
train_iter_loss: 0.2873688340187073
train_iter_loss: 0.34100544452667236
train_iter_loss: 0.33719611167907715
train_iter_loss: 0.5244906544685364
train_iter_loss: 0.2997494041919708
train_iter_loss: 0.379584938287735
train loss :0.3255
---------------------
Validation seg loss: 0.4034373831917655 at epoch 63
epoch =     64/  1000, exp = train
train_iter_loss: 0.19936859607696533
train_iter_loss: 0.2126113474369049
train_iter_loss: 0.334270715713501
train_iter_loss: 0.3021039068698883
train_iter_loss: 0.29231953620910645
train_iter_loss: 0.25980809330940247
train_iter_loss: 0.41899678111076355
train_iter_loss: 0.25614333152770996
train_iter_loss: 0.36605432629585266
train_iter_loss: 0.2802394926548004
train_iter_loss: 0.31273356080055237
train_iter_loss: 0.33515894412994385
train_iter_loss: 0.3817635178565979
train_iter_loss: 0.45951664447784424
train_iter_loss: 0.28819605708122253
train_iter_loss: 0.1806928962469101
train_iter_loss: 0.2995285391807556
train_iter_loss: 0.38939762115478516
train_iter_loss: 0.35576820373535156
train_iter_loss: 0.2784084975719452
train_iter_loss: 0.2533709704875946
train_iter_loss: 0.37598472833633423
train_iter_loss: 0.21561026573181152
train_iter_loss: 0.2623351514339447
train_iter_loss: 0.31473150849342346
train_iter_loss: 0.21825765073299408
train_iter_loss: 0.3028634488582611
train_iter_loss: 0.24375812709331512
train_iter_loss: 0.2089148461818695
train_iter_loss: 0.5201864838600159
train_iter_loss: 0.3145117163658142
train_iter_loss: 0.2584454417228699
train_iter_loss: 0.2757313549518585
train_iter_loss: 0.162889301776886
train_iter_loss: 0.21732953190803528
train_iter_loss: 0.25666379928588867
train_iter_loss: 0.2941924035549164
train_iter_loss: 0.3224669396877289
train_iter_loss: 0.25723758339881897
train_iter_loss: 0.3349844813346863
train_iter_loss: 0.40400129556655884
train_iter_loss: 0.4497128427028656
train_iter_loss: 0.3847055435180664
train_iter_loss: 0.3594472110271454
train_iter_loss: 0.23157593607902527
train_iter_loss: 0.28190258145332336
train_iter_loss: 0.2783645987510681
train_iter_loss: 0.333778977394104
train_iter_loss: 0.2500547170639038
train_iter_loss: 0.24922512471675873
train_iter_loss: 0.23441234230995178
train_iter_loss: 0.31501057744026184
train_iter_loss: 0.31418535113334656
train_iter_loss: 0.3736322224140167
train_iter_loss: 0.330637663602829
train_iter_loss: 0.28326165676116943
train_iter_loss: 0.34057727456092834
train_iter_loss: 0.28425437211990356
train_iter_loss: 0.36996889114379883
train_iter_loss: 0.42771896719932556
train_iter_loss: 0.2850121557712555
train_iter_loss: 0.20594164729118347
train_iter_loss: 0.2518649697303772
train_iter_loss: 0.3752812445163727
train_iter_loss: 0.2906683385372162
train_iter_loss: 0.20222598314285278
train_iter_loss: 0.2938101589679718
train_iter_loss: 0.7370597124099731
train_iter_loss: 0.2975650131702423
train_iter_loss: 0.28355613350868225
train_iter_loss: 0.2842692732810974
train_iter_loss: 0.292188435792923
train_iter_loss: 0.36745157837867737
train_iter_loss: 0.344318687915802
train_iter_loss: 0.591971218585968
train_iter_loss: 0.4197509288787842
train_iter_loss: 0.4397529363632202
train_iter_loss: 0.28460463881492615
train_iter_loss: 0.3420069217681885
train_iter_loss: 0.38523367047309875
train_iter_loss: 0.30659618973731995
train_iter_loss: 0.3724072277545929
train_iter_loss: 0.38973942399024963
train_iter_loss: 0.35376685857772827
train_iter_loss: 0.2462070882320404
train_iter_loss: 0.2899862229824066
train_iter_loss: 0.31826165318489075
train_iter_loss: 0.20348982512950897
train_iter_loss: 0.3336925506591797
train_iter_loss: 0.24586701393127441
train_iter_loss: 0.4944680333137512
train_iter_loss: 0.24483996629714966
train_iter_loss: 0.2884882688522339
train_iter_loss: 0.23534199595451355
train_iter_loss: 0.252871572971344
train_iter_loss: 0.3468099534511566
train_iter_loss: 0.39478641748428345
train_iter_loss: 0.4424384534358978
train_iter_loss: 0.491278737783432
train_iter_loss: 0.3413686454296112
train loss :0.3220
---------------------
Validation seg loss: 0.410900731184432 at epoch 64
epoch =     65/  1000, exp = train
train_iter_loss: 0.30368080735206604
train_iter_loss: 0.3705625832080841
train_iter_loss: 0.1963784247636795
train_iter_loss: 0.349700003862381
train_iter_loss: 0.2604193389415741
train_iter_loss: 0.2722620368003845
train_iter_loss: 0.36744165420532227
train_iter_loss: 0.2575421631336212
train_iter_loss: 0.4291452467441559
train_iter_loss: 0.3218877911567688
train_iter_loss: 0.3359931409358978
train_iter_loss: 0.2019946128129959
train_iter_loss: 0.2309785783290863
train_iter_loss: 0.25278106331825256
train_iter_loss: 0.3265998959541321
train_iter_loss: 0.35109591484069824
train_iter_loss: 0.27387911081314087
train_iter_loss: 0.3742911219596863
train_iter_loss: 0.40688082575798035
train_iter_loss: 0.21035902202129364
train_iter_loss: 0.35888445377349854
train_iter_loss: 0.40261736512184143
train_iter_loss: 0.48685941100120544
train_iter_loss: 0.24693048000335693
train_iter_loss: 0.3097304403781891
train_iter_loss: 0.2457406371831894
train_iter_loss: 0.37662482261657715
train_iter_loss: 0.38423341512680054
train_iter_loss: 0.3782026469707489
train_iter_loss: 0.16548092663288116
train_iter_loss: 0.3329697549343109
train_iter_loss: 0.3072446286678314
train_iter_loss: 0.3137447237968445
train_iter_loss: 0.3038112223148346
train_iter_loss: 0.33176183700561523
train_iter_loss: 0.23016758263111115
train_iter_loss: 0.3500029146671295
train_iter_loss: 0.4774544835090637
train_iter_loss: 0.42200273275375366
train_iter_loss: 0.5893250703811646
train_iter_loss: 0.37686267495155334
train_iter_loss: 0.3888987600803375
train_iter_loss: 0.23409566283226013
train_iter_loss: 0.27103641629219055
train_iter_loss: 0.26900866627693176
train_iter_loss: 0.34346842765808105
train_iter_loss: 0.38911086320877075
train_iter_loss: 0.2957242429256439
train_iter_loss: 0.3505144417285919
train_iter_loss: 0.24412377178668976
train_iter_loss: 0.21341758966445923
train_iter_loss: 0.2988855540752411
train_iter_loss: 0.28160423040390015
train_iter_loss: 0.23274220526218414
train_iter_loss: 0.5437314510345459
train_iter_loss: 0.22708603739738464
train_iter_loss: 0.28703010082244873
train_iter_loss: 0.3143818974494934
train_iter_loss: 0.3442381024360657
train_iter_loss: 0.30326196551322937
train_iter_loss: 0.11692080646753311
train_iter_loss: 0.3010746240615845
train_iter_loss: 0.3304613530635834
train_iter_loss: 0.25440436601638794
train_iter_loss: 0.38325729966163635
train_iter_loss: 0.4792962372303009
train_iter_loss: 0.2706631124019623
train_iter_loss: 0.311611145734787
train_iter_loss: 0.2574135363101959
train_iter_loss: 0.2621816098690033
train_iter_loss: 0.2712089419364929
train_iter_loss: 0.3422945737838745
train_iter_loss: 0.24689118564128876
train_iter_loss: 0.438314825296402
train_iter_loss: 0.27914169430732727
train_iter_loss: 0.28352469205856323
train_iter_loss: 0.2520388960838318
train_iter_loss: 0.20869098603725433
train_iter_loss: 0.23795872926712036
train_iter_loss: 0.38084545731544495
train_iter_loss: 0.4177030622959137
train_iter_loss: 0.4023487865924835
train_iter_loss: 0.2445036917924881
train_iter_loss: 0.24793806672096252
train_iter_loss: 0.26137059926986694
train_iter_loss: 0.25624823570251465
train_iter_loss: 0.3407115042209625
train_iter_loss: 0.35868847370147705
train_iter_loss: 0.30918166041374207
train_iter_loss: 0.26211774349212646
train_iter_loss: 0.3202122151851654
train_iter_loss: 0.25515544414520264
train_iter_loss: 0.3232857882976532
train_iter_loss: 0.31070971488952637
train_iter_loss: 0.2664845883846283
train_iter_loss: 0.31019625067710876
train_iter_loss: 0.3137325644493103
train_iter_loss: 0.16887490451335907
train_iter_loss: 0.24289463460445404
train_iter_loss: 0.3347257077693939
train loss :0.3149
---------------------
Validation seg loss: 0.4027454560481996 at epoch 65
epoch =     66/  1000, exp = train
train_iter_loss: 0.3183433413505554
train_iter_loss: 0.26128944754600525
train_iter_loss: 0.22128503024578094
train_iter_loss: 0.4471868872642517
train_iter_loss: 0.24159345030784607
train_iter_loss: 0.31822896003723145
train_iter_loss: 0.1918133944272995
train_iter_loss: 0.2811996340751648
train_iter_loss: 0.30684491991996765
train_iter_loss: 0.18827810883522034
train_iter_loss: 0.44564199447631836
train_iter_loss: 0.3825966715812683
train_iter_loss: 0.30553963780403137
train_iter_loss: 0.26902684569358826
train_iter_loss: 0.37759870290756226
train_iter_loss: 0.3087761700153351
train_iter_loss: 0.3139931559562683
train_iter_loss: 0.34215807914733887
train_iter_loss: 0.28851667046546936
train_iter_loss: 0.31223127245903015
train_iter_loss: 0.22466270625591278
train_iter_loss: 0.3225463032722473
train_iter_loss: 0.3325863778591156
train_iter_loss: 0.24143780767917633
train_iter_loss: 0.37485817074775696
train_iter_loss: 0.2020607590675354
train_iter_loss: 0.31198006868362427
train_iter_loss: 0.35709598660469055
train_iter_loss: 0.3162533640861511
train_iter_loss: 0.3474995493888855
train_iter_loss: 0.3753771483898163
train_iter_loss: 0.3545958995819092
train_iter_loss: 0.4013577401638031
train_iter_loss: 0.23982423543930054
train_iter_loss: 0.27573734521865845
train_iter_loss: 0.5170848965644836
train_iter_loss: 0.2875497341156006
train_iter_loss: 0.30866849422454834
train_iter_loss: 0.27946797013282776
train_iter_loss: 0.3152978718280792
train_iter_loss: 0.36627885699272156
train_iter_loss: 0.2125205397605896
train_iter_loss: 0.19370996952056885
train_iter_loss: 0.31467166543006897
train_iter_loss: 0.44867661595344543
train_iter_loss: 0.2520354986190796
train_iter_loss: 0.25222980976104736
train_iter_loss: 0.355181485414505
train_iter_loss: 0.30638617277145386
train_iter_loss: 0.5205333828926086
train_iter_loss: 0.23086310923099518
train_iter_loss: 0.2771320343017578
train_iter_loss: 0.42002248764038086
train_iter_loss: 0.41817742586135864
train_iter_loss: 0.41909289360046387
train_iter_loss: 0.18134889006614685
train_iter_loss: 0.28406190872192383
train_iter_loss: 0.2892596125602722
train_iter_loss: 0.30885908007621765
train_iter_loss: 0.3254866600036621
train_iter_loss: 0.4091081917285919
train_iter_loss: 0.29980015754699707
train_iter_loss: 0.16681019961833954
train_iter_loss: 0.29553380608558655
train_iter_loss: 0.33313536643981934
train_iter_loss: 0.4874670207500458
train_iter_loss: 0.2892338037490845
train_iter_loss: 0.28376805782318115
train_iter_loss: 0.2293214499950409
train_iter_loss: 0.2846851646900177
train_iter_loss: 0.3544647693634033
train_iter_loss: 0.3651740849018097
train_iter_loss: 0.2828124165534973
train_iter_loss: 0.26058417558670044
train_iter_loss: 0.3224579095840454
train_iter_loss: 0.3945886492729187
train_iter_loss: 0.208173006772995
train_iter_loss: 0.4583260416984558
train_iter_loss: 0.3264068365097046
train_iter_loss: 0.3673352897167206
train_iter_loss: 0.27794384956359863
train_iter_loss: 0.28734320402145386
train_iter_loss: 0.5047292113304138
train_iter_loss: 0.38245412707328796
train_iter_loss: 0.14058101177215576
train_iter_loss: 0.17295193672180176
train_iter_loss: 0.28497177362442017
train_iter_loss: 0.4197072982788086
train_iter_loss: 0.2474668174982071
train_iter_loss: 0.32567843794822693
train_iter_loss: 0.2952907383441925
train_iter_loss: 0.30844035744667053
train_iter_loss: 0.2802388072013855
train_iter_loss: 0.33328765630722046
train_iter_loss: 0.41340091824531555
train_iter_loss: 0.3518638014793396
train_iter_loss: 0.2526020109653473
train_iter_loss: 0.47622179985046387
train_iter_loss: 0.15520836412906647
train_iter_loss: 0.43871521949768066
train loss :0.3197
---------------------
Validation seg loss: 0.4455159102689545 at epoch 66
epoch =     67/  1000, exp = train
train_iter_loss: 0.15162169933319092
train_iter_loss: 0.3674159646034241
train_iter_loss: 0.35838258266448975
train_iter_loss: 0.23054930567741394
train_iter_loss: 0.3152463734149933
train_iter_loss: 0.40248212218284607
train_iter_loss: 0.30194956064224243
train_iter_loss: 0.2642340064048767
train_iter_loss: 0.22506365180015564
train_iter_loss: 0.20637336373329163
train_iter_loss: 0.21694795787334442
train_iter_loss: 0.21741360425949097
train_iter_loss: 0.20643822848796844
train_iter_loss: 0.13753986358642578
train_iter_loss: 0.3472059369087219
train_iter_loss: 0.36044350266456604
train_iter_loss: 0.2546288073062897
train_iter_loss: 0.3566889464855194
train_iter_loss: 0.35279178619384766
train_iter_loss: 0.18397167325019836
train_iter_loss: 0.31091436743736267
train_iter_loss: 0.2853299081325531
train_iter_loss: 0.3412884771823883
train_iter_loss: 0.4738903045654297
train_iter_loss: 0.3662899136543274
train_iter_loss: 0.38265126943588257
train_iter_loss: 0.23106324672698975
train_iter_loss: 0.19882579147815704
train_iter_loss: 0.3144213855266571
train_iter_loss: 0.3020665645599365
train_iter_loss: 0.15377765893936157
train_iter_loss: 0.31886395812034607
train_iter_loss: 0.28818634152412415
train_iter_loss: 0.2257203608751297
train_iter_loss: 0.33184659481048584
train_iter_loss: 0.29981380701065063
train_iter_loss: 0.33148038387298584
train_iter_loss: 0.3361501097679138
train_iter_loss: 0.43626582622528076
train_iter_loss: 0.27268466353416443
train_iter_loss: 0.25029662251472473
train_iter_loss: 0.3179417848587036
train_iter_loss: 0.39592254161834717
train_iter_loss: 0.33164089918136597
train_iter_loss: 0.2860645353794098
train_iter_loss: 0.3674450218677521
train_iter_loss: 0.35150495171546936
train_iter_loss: 0.4727984070777893
train_iter_loss: 0.25712910294532776
train_iter_loss: 0.3152575194835663
train_iter_loss: 0.4007595181465149
train_iter_loss: 0.3807663321495056
train_iter_loss: 0.36061009764671326
train_iter_loss: 0.22993242740631104
train_iter_loss: 0.26237285137176514
train_iter_loss: 0.3417695164680481
train_iter_loss: 0.4284912049770355
train_iter_loss: 0.38755568861961365
train_iter_loss: 0.3016829788684845
train_iter_loss: 0.25438356399536133
train_iter_loss: 0.2396892011165619
train_iter_loss: 0.2729417383670807
train_iter_loss: 0.383070707321167
train_iter_loss: 0.44368383288383484
train_iter_loss: 0.3817293643951416
train_iter_loss: 0.31428277492523193
train_iter_loss: 0.4422000050544739
train_iter_loss: 0.3091779053211212
train_iter_loss: 0.1900818794965744
train_iter_loss: 0.5137007236480713
train_iter_loss: 0.3867620527744293
train_iter_loss: 0.27849993109703064
train_iter_loss: 0.22990381717681885
train_iter_loss: 0.2542153000831604
train_iter_loss: 0.33232536911964417
train_iter_loss: 0.3198992908000946
train_iter_loss: 0.175580233335495
train_iter_loss: 0.3949117660522461
train_iter_loss: 0.2793141007423401
train_iter_loss: 0.25481441617012024
train_iter_loss: 0.2567689120769501
train_iter_loss: 0.13702616095542908
train_iter_loss: 0.28621652722358704
train_iter_loss: 0.3037230670452118
train_iter_loss: 0.3304406404495239
train_iter_loss: 0.34773990511894226
train_iter_loss: 0.35218819975852966
train_iter_loss: 0.3806370496749878
train_iter_loss: 0.31271860003471375
train_iter_loss: 0.44628778100013733
train_iter_loss: 0.39340975880622864
train_iter_loss: 0.3548251688480377
train_iter_loss: 0.23931415379047394
train_iter_loss: 0.3432675302028656
train_iter_loss: 0.3753748834133148
train_iter_loss: 0.20885254442691803
train_iter_loss: 0.25141602754592896
train_iter_loss: 0.5613943338394165
train_iter_loss: 0.2983609735965729
train_iter_loss: 0.28727027773857117
train loss :0.3154
---------------------
Validation seg loss: 0.4051735093641394 at epoch 67
epoch =     68/  1000, exp = train
train_iter_loss: 0.2418702095746994
train_iter_loss: 0.2285102754831314
train_iter_loss: 0.30525222420692444
train_iter_loss: 0.2862599194049835
train_iter_loss: 0.2939984202384949
train_iter_loss: 0.29420068860054016
train_iter_loss: 0.2587733566761017
train_iter_loss: 0.3091396689414978
train_iter_loss: 0.23149625957012177
train_iter_loss: 0.24569456279277802
train_iter_loss: 0.2901042103767395
train_iter_loss: 0.46137645840644836
train_iter_loss: 0.5516159534454346
train_iter_loss: 0.3024716079235077
train_iter_loss: 0.2865236699581146
train_iter_loss: 0.5356356501579285
train_iter_loss: 0.40630608797073364
train_iter_loss: 0.37474921345710754
train_iter_loss: 0.2947450876235962
train_iter_loss: 0.2850731313228607
train_iter_loss: 0.32687315344810486
train_iter_loss: 0.3618471026420593
train_iter_loss: 0.2491212785243988
train_iter_loss: 0.15699677169322968
train_iter_loss: 0.6580171585083008
train_iter_loss: 0.3305796980857849
train_iter_loss: 0.17660509049892426
train_iter_loss: 0.41942667961120605
train_iter_loss: 0.5135337114334106
train_iter_loss: 0.2504778802394867
train_iter_loss: 0.2607007622718811
train_iter_loss: 0.28448617458343506
train_iter_loss: 0.3192747235298157
train_iter_loss: 0.28644391894340515
train_iter_loss: 0.2953515648841858
train_iter_loss: 0.336739182472229
train_iter_loss: 0.30182817578315735
train_iter_loss: 0.3032153844833374
train_iter_loss: 0.3907621502876282
train_iter_loss: 0.35737842321395874
train_iter_loss: 0.43609797954559326
train_iter_loss: 0.23768849670886993
train_iter_loss: 0.25627729296684265
train_iter_loss: 0.27958011627197266
train_iter_loss: 0.1307382434606552
train_iter_loss: 0.3467935621738434
train_iter_loss: 0.3186241090297699
train_iter_loss: 0.36474356055259705
train_iter_loss: 0.3420410752296448
train_iter_loss: 0.41261231899261475
train_iter_loss: 0.32472705841064453
train_iter_loss: 0.3049449026584625
train_iter_loss: 0.36688122153282166
train_iter_loss: 0.40122127532958984
train_iter_loss: 0.2870756983757019
train_iter_loss: 0.34689047932624817
train_iter_loss: 0.4706392288208008
train_iter_loss: 0.1400495320558548
train_iter_loss: 0.32879406213760376
train_iter_loss: 0.2670232057571411
train_iter_loss: 0.30518293380737305
train_iter_loss: 0.1944248378276825
train_iter_loss: 0.2570574879646301
train_iter_loss: 0.3243807852268219
train_iter_loss: 0.29918792843818665
train_iter_loss: 0.30707085132598877
train_iter_loss: 0.27240458130836487
train_iter_loss: 0.2589731812477112
train_iter_loss: 0.26723894476890564
train_iter_loss: 0.5405669808387756
train_iter_loss: 0.354361355304718
train_iter_loss: 0.30792853236198425
train_iter_loss: 0.4435398280620575
train_iter_loss: 0.3210638463497162
train_iter_loss: 0.19304972887039185
train_iter_loss: 0.2783532738685608
train_iter_loss: 0.21301725506782532
train_iter_loss: 0.28402528166770935
train_iter_loss: 0.2738202214241028
train_iter_loss: 0.35632434487342834
train_iter_loss: 0.4228062331676483
train_iter_loss: 0.2933107614517212
train_iter_loss: 0.2642553448677063
train_iter_loss: 0.22564063966274261
train_iter_loss: 0.2040713131427765
train_iter_loss: 0.2451423555612564
train_iter_loss: 0.44159990549087524
train_iter_loss: 0.39884233474731445
train_iter_loss: 0.19340382516384125
train_iter_loss: 0.2895262837409973
train_iter_loss: 0.3762207627296448
train_iter_loss: 0.33544567227363586
train_iter_loss: 0.300936758518219
train_iter_loss: 0.27546748518943787
train_iter_loss: 0.19034017622470856
train_iter_loss: 0.40464112162590027
train_iter_loss: 0.23170597851276398
train_iter_loss: 0.27372583746910095
train_iter_loss: 0.26309168338775635
train_iter_loss: 0.3927832841873169
train loss :0.3182
---------------------
Validation seg loss: 0.4087691269814968 at epoch 68
epoch =     69/  1000, exp = train
train_iter_loss: 0.4054839015007019
train_iter_loss: 0.2996824085712433
train_iter_loss: 0.2454800307750702
train_iter_loss: 0.3883512616157532
train_iter_loss: 0.2146412879228592
train_iter_loss: 0.3648063540458679
train_iter_loss: 0.3044411540031433
train_iter_loss: 0.2931131422519684
train_iter_loss: 0.3398754596710205
train_iter_loss: 0.39397087693214417
train_iter_loss: 0.3031075894832611
train_iter_loss: 0.142144575715065
train_iter_loss: 0.30491119623184204
train_iter_loss: 0.17865623533725739
train_iter_loss: 0.4255099594593048
train_iter_loss: 0.2857656180858612
train_iter_loss: 0.44312384724617004
train_iter_loss: 0.2505258321762085
train_iter_loss: 0.22622351348400116
train_iter_loss: 0.35172486305236816
train_iter_loss: 0.309340238571167
train_iter_loss: 0.4314829111099243
train_iter_loss: 0.3584737181663513
train_iter_loss: 0.3341713547706604
train_iter_loss: 0.17240189015865326
train_iter_loss: 0.5234769582748413
train_iter_loss: 0.31798020005226135
train_iter_loss: 0.3596844971179962
train_iter_loss: 0.3365159034729004
train_iter_loss: 0.39170214533805847
train_iter_loss: 0.345872163772583
train_iter_loss: 0.2455221265554428
train_iter_loss: 0.33750462532043457
train_iter_loss: 0.399467796087265
train_iter_loss: 0.24205699563026428
train_iter_loss: 0.1852211356163025
train_iter_loss: 0.48978203535079956
train_iter_loss: 0.3581617772579193
train_iter_loss: 0.3737777769565582
train_iter_loss: 0.37282800674438477
train_iter_loss: 0.17648163437843323
train_iter_loss: 0.25791093707084656
train_iter_loss: 0.49879470467567444
train_iter_loss: 0.26458558440208435
train_iter_loss: 0.3087044954299927
train_iter_loss: 0.40340903401374817
train_iter_loss: 0.19694899022579193
train_iter_loss: 0.23229505121707916
train_iter_loss: 0.42375436425209045
train_iter_loss: 0.1559218168258667
train_iter_loss: 0.32927918434143066
train_iter_loss: 0.4513131082057953
train_iter_loss: 0.2109106332063675
train_iter_loss: 0.44648462533950806
train_iter_loss: 0.2710193991661072
train_iter_loss: 0.3249066472053528
train_iter_loss: 0.26628267765045166
train_iter_loss: 0.14819058775901794
train_iter_loss: 0.2048386037349701
train_iter_loss: 0.357162743806839
train_iter_loss: 0.1547814905643463
train_iter_loss: 0.1668633669614792
train_iter_loss: 0.35830697417259216
train_iter_loss: 0.3697203993797302
train_iter_loss: 0.3344540596008301
train_iter_loss: 0.275310218334198
train_iter_loss: 0.31239053606987
train_iter_loss: 0.37332239747047424
train_iter_loss: 0.4958140552043915
train_iter_loss: 0.4440991282463074
train_iter_loss: 0.3356916010379791
train_iter_loss: 0.3445936143398285
train_iter_loss: 0.18043509125709534
train_iter_loss: 0.2472543567419052
train_iter_loss: 0.29059872031211853
train_iter_loss: 0.48041030764579773
train_iter_loss: 0.29221418499946594
train_iter_loss: 0.3481360375881195
train_iter_loss: 0.4115627110004425
train_iter_loss: 0.35938209295272827
train_iter_loss: 0.29170316457748413
train_iter_loss: 0.17257998883724213
train_iter_loss: 0.43494418263435364
train_iter_loss: 0.3339579403400421
train_iter_loss: 0.36278030276298523
train_iter_loss: 0.38505280017852783
train_iter_loss: 0.40004763007164
train_iter_loss: 0.3386465907096863
train_iter_loss: 0.3417205810546875
train_iter_loss: 0.3633155822753906
train_iter_loss: 0.21112726628780365
train_iter_loss: 0.28083333373069763
train_iter_loss: 0.3343812823295593
train_iter_loss: 0.27004557847976685
train_iter_loss: 0.47135129570961
train_iter_loss: 0.3662075996398926
train_iter_loss: 0.38834771513938904
train_iter_loss: 0.3303565979003906
train_iter_loss: 0.4083656668663025
train_iter_loss: 0.2589254677295685
train loss :0.3261
---------------------
Validation seg loss: 0.385408890503898 at epoch 69
epoch =     70/  1000, exp = train
train_iter_loss: 0.37291547656059265
train_iter_loss: 0.32066473364830017
train_iter_loss: 0.35003286600112915
train_iter_loss: 0.29564735293388367
train_iter_loss: 0.3446017801761627
train_iter_loss: 0.3198878765106201
train_iter_loss: 0.3770512640476227
train_iter_loss: 0.26229557394981384
train_iter_loss: 0.24610589444637299
train_iter_loss: 0.33946287631988525
train_iter_loss: 0.32877445220947266
train_iter_loss: 0.30971723794937134
train_iter_loss: 0.22980239987373352
train_iter_loss: 0.2808954119682312
train_iter_loss: 0.2577739357948303
train_iter_loss: 0.3282817304134369
train_iter_loss: 0.3835374116897583
train_iter_loss: 0.3377723693847656
train_iter_loss: 0.3156832754611969
train_iter_loss: 0.40274330973625183
train_iter_loss: 0.19643723964691162
train_iter_loss: 0.3528132140636444
train_iter_loss: 0.2697863280773163
train_iter_loss: 0.4089549779891968
train_iter_loss: 0.3385748863220215
train_iter_loss: 0.220428466796875
train_iter_loss: 0.5195024013519287
train_iter_loss: 0.29695582389831543
train_iter_loss: 0.2241450846195221
train_iter_loss: 0.3112894892692566
train_iter_loss: 0.43977323174476624
train_iter_loss: 0.3158242106437683
train_iter_loss: 0.24684308469295502
train_iter_loss: 0.22128425538539886
train_iter_loss: 0.34730908274650574
train_iter_loss: 0.3176553249359131
train_iter_loss: 0.25401565432548523
train_iter_loss: 0.3375307023525238
train_iter_loss: 0.23287124931812286
train_iter_loss: 0.3308365046977997
train_iter_loss: 0.46311503648757935
train_iter_loss: 0.35448533296585083
train_iter_loss: 0.22290711104869843
train_iter_loss: 0.29725489020347595
train_iter_loss: 0.35469919443130493
train_iter_loss: 0.3209371268749237
train_iter_loss: 0.22504155337810516
train_iter_loss: 0.19341440498828888
train_iter_loss: 0.3674696087837219
train_iter_loss: 0.2505349814891815
train_iter_loss: 0.39519748091697693
train_iter_loss: 0.2596716582775116
train_iter_loss: 0.24020977318286896
train_iter_loss: 0.44398272037506104
train_iter_loss: 0.25340187549591064
train_iter_loss: 0.32571226358413696
train_iter_loss: 0.35882923007011414
train_iter_loss: 0.352885365486145
train_iter_loss: 0.38190409541130066
train_iter_loss: 0.2433767318725586
train_iter_loss: 0.41209498047828674
train_iter_loss: 0.28185921907424927
train_iter_loss: 0.32485431432724
train_iter_loss: 0.20712721347808838
train_iter_loss: 0.24832916259765625
train_iter_loss: 0.2606129050254822
train_iter_loss: 0.2981143891811371
train_iter_loss: 0.22893674671649933
train_iter_loss: 0.37462127208709717
train_iter_loss: 0.30610644817352295
train_iter_loss: 0.3017491400241852
train_iter_loss: 0.3406352698802948
train_iter_loss: 0.41014882922172546
train_iter_loss: 0.18453393876552582
train_iter_loss: 0.3661785423755646
train_iter_loss: 0.3022560179233551
train_iter_loss: 0.2723042666912079
train_iter_loss: 0.28081071376800537
train_iter_loss: 0.2565211355686188
train_iter_loss: 0.2843989431858063
train_iter_loss: 0.1727544665336609
train_iter_loss: 0.34793752431869507
train_iter_loss: 0.3402319550514221
train_iter_loss: 0.1999003142118454
train_iter_loss: 0.21452760696411133
train_iter_loss: 0.5412158370018005
train_iter_loss: 0.28409329056739807
train_iter_loss: 0.3416740596294403
train_iter_loss: 0.3764774799346924
train_iter_loss: 0.4380130171775818
train_iter_loss: 0.3976270258426666
train_iter_loss: 0.2987588047981262
train_iter_loss: 0.260345458984375
train_iter_loss: 0.3132781386375427
train_iter_loss: 0.344982385635376
train_iter_loss: 0.3006654381752014
train_iter_loss: 0.34201109409332275
train_iter_loss: 0.27694061398506165
train_iter_loss: 0.17488792538642883
train_iter_loss: 0.27791479229927063
train loss :0.3140
---------------------
Validation seg loss: 0.4015974369919244 at epoch 70
epoch =     71/  1000, exp = train
train_iter_loss: 0.17988967895507812
train_iter_loss: 0.3838892877101898
train_iter_loss: 0.30559441447257996
train_iter_loss: 0.29570138454437256
train_iter_loss: 0.29461392760276794
train_iter_loss: 0.172105610370636
train_iter_loss: 0.3292134404182434
train_iter_loss: 0.20737160742282867
train_iter_loss: 0.2612926661968231
train_iter_loss: 0.27056002616882324
train_iter_loss: 0.41349688172340393
train_iter_loss: 0.23845252394676208
train_iter_loss: 0.30358776450157166
train_iter_loss: 0.38464802503585815
train_iter_loss: 0.23111894726753235
train_iter_loss: 0.34507471323013306
train_iter_loss: 0.32200831174850464
train_iter_loss: 0.5219061374664307
train_iter_loss: 0.29931795597076416
train_iter_loss: 0.2834544777870178
train_iter_loss: 0.23323409259319305
train_iter_loss: 0.4804646372795105
train_iter_loss: 0.24313707649707794
train_iter_loss: 0.44659310579299927
train_iter_loss: 0.22136476635932922
train_iter_loss: 0.28101471066474915
train_iter_loss: 0.23602575063705444
train_iter_loss: 0.3054080605506897
train_iter_loss: 0.3227704167366028
train_iter_loss: 0.17008386552333832
train_iter_loss: 0.30074670910835266
train_iter_loss: 0.3040724992752075
train_iter_loss: 0.4882470667362213
train_iter_loss: 0.366822212934494
train_iter_loss: 0.42277514934539795
train_iter_loss: 0.22506389021873474
train_iter_loss: 0.27677252888679504
train_iter_loss: 0.20771156251430511
train_iter_loss: 0.39858847856521606
train_iter_loss: 0.24425603449344635
train_iter_loss: 0.3605238199234009
train_iter_loss: 0.35598862171173096
train_iter_loss: 0.35888245701789856
train_iter_loss: 0.252556174993515
train_iter_loss: 0.1843428611755371
train_iter_loss: 0.33063891530036926
train_iter_loss: 0.2922185957431793
train_iter_loss: 0.29610615968704224
train_iter_loss: 0.4103401005268097
train_iter_loss: 0.32872411608695984
train_iter_loss: 0.40484511852264404
train_iter_loss: 0.4018257260322571
train_iter_loss: 0.3101169466972351
train_iter_loss: 0.265593558549881
train_iter_loss: 0.4585627019405365
train_iter_loss: 0.20177315175533295
train_iter_loss: 0.17157582938671112
train_iter_loss: 0.37297701835632324
train_iter_loss: 0.504622220993042
train_iter_loss: 0.1571136862039566
train_iter_loss: 0.321266770362854
train_iter_loss: 0.24689717590808868
train_iter_loss: 0.2182096540927887
train_iter_loss: 0.32421159744262695
train_iter_loss: 0.32483193278312683
train_iter_loss: 0.402873694896698
train_iter_loss: 0.328723669052124
train_iter_loss: 0.4808070957660675
train_iter_loss: 0.2984236478805542
train_iter_loss: 0.387600302696228
train_iter_loss: 0.3175511956214905
train_iter_loss: 0.28124406933784485
train_iter_loss: 0.4235891103744507
train_iter_loss: 0.2636373043060303
train_iter_loss: 0.2721171975135803
train_iter_loss: 0.40204793214797974
train_iter_loss: 0.2452501356601715
train_iter_loss: 0.13228733837604523
train_iter_loss: 0.327704519033432
train_iter_loss: 0.31440067291259766
train_iter_loss: 0.26922839879989624
train_iter_loss: 0.17423489689826965
train_iter_loss: 0.3013913631439209
train_iter_loss: 0.37165436148643494
train_iter_loss: 0.35684677958488464
train_iter_loss: 0.36276954412460327
train_iter_loss: 0.2785438895225525
train_iter_loss: 0.2866342067718506
train_iter_loss: 0.4150058329105377
train_iter_loss: 0.37364524602890015
train_iter_loss: 0.23212726414203644
train_iter_loss: 0.16322238743305206
train_iter_loss: 0.18898707628250122
train_iter_loss: 0.19201259315013885
train_iter_loss: 0.3650079369544983
train_iter_loss: 0.351661741733551
train_iter_loss: 0.35805004835128784
train_iter_loss: 0.2440766543149948
train_iter_loss: 0.3236026465892792
train_iter_loss: 0.19208702445030212
train loss :0.3107
---------------------
Validation seg loss: 0.4064355374155742 at epoch 71
epoch =     72/  1000, exp = train
train_iter_loss: 0.3144649267196655
train_iter_loss: 0.24009022116661072
train_iter_loss: 0.29848504066467285
train_iter_loss: 0.22055375576019287
train_iter_loss: 0.443043053150177
train_iter_loss: 0.3621184825897217
train_iter_loss: 0.4162989854812622
train_iter_loss: 0.2414780706167221
train_iter_loss: 0.38583800196647644
train_iter_loss: 0.19120769202709198
train_iter_loss: 0.24385465681552887
train_iter_loss: 0.18801948428153992
train_iter_loss: 0.3096163272857666
train_iter_loss: 0.23204676806926727
train_iter_loss: 0.21710072457790375
train_iter_loss: 0.27384909987449646
train_iter_loss: 0.3349171578884125
train_iter_loss: 0.3650078773498535
train_iter_loss: 0.21114400029182434
train_iter_loss: 0.29355576634407043
train_iter_loss: 0.39784738421440125
train_iter_loss: 0.33318760991096497
train_iter_loss: 0.2122015804052353
train_iter_loss: 0.3053046464920044
train_iter_loss: 0.40488874912261963
train_iter_loss: 0.2555268704891205
train_iter_loss: 0.2759667634963989
train_iter_loss: 0.34008896350860596
train_iter_loss: 0.2310204803943634
train_iter_loss: 0.2599021792411804
train_iter_loss: 0.46507832407951355
train_iter_loss: 0.27062007784843445
train_iter_loss: 0.221217542886734
train_iter_loss: 0.38766512274742126
train_iter_loss: 0.3744049668312073
train_iter_loss: 0.2092033177614212
train_iter_loss: 0.2969132661819458
train_iter_loss: 0.20367930829524994
train_iter_loss: 0.3779643476009369
train_iter_loss: 0.22437770664691925
train_iter_loss: 0.40327325463294983
train_iter_loss: 0.27594709396362305
train_iter_loss: 0.3163328170776367
train_iter_loss: 0.2620024085044861
train_iter_loss: 0.29371514916419983
train_iter_loss: 0.2313445806503296
train_iter_loss: 0.39803358912467957
train_iter_loss: 0.3835177421569824
train_iter_loss: 0.44896823167800903
train_iter_loss: 0.2972564697265625
train_iter_loss: 0.49984586238861084
train_iter_loss: 0.25889235734939575
train_iter_loss: 0.35657477378845215
train_iter_loss: 0.23110680282115936
train_iter_loss: 0.2647553086280823
train_iter_loss: 0.36982840299606323
train_iter_loss: 0.4542868733406067
train_iter_loss: 0.22625185549259186
train_iter_loss: 0.31409719586372375
train_iter_loss: 0.3871380686759949
train_iter_loss: 0.3496299684047699
train_iter_loss: 0.17706944048404694
train_iter_loss: 0.31903600692749023
train_iter_loss: 0.32634100317955017
train_iter_loss: 0.26313504576683044
train_iter_loss: 0.3129943013191223
train_iter_loss: 0.345670610666275
train_iter_loss: 0.2614251673221588
train_iter_loss: 0.46123597025871277
train_iter_loss: 0.22878164052963257
train_iter_loss: 0.21654851734638214
train_iter_loss: 0.3872510492801666
train_iter_loss: 0.3716757297515869
train_iter_loss: 0.2088511884212494
train_iter_loss: 0.29445934295654297
train_iter_loss: 0.2323637753725052
train_iter_loss: 0.4099244773387909
train_iter_loss: 0.334127813577652
train_iter_loss: 0.4338153004646301
train_iter_loss: 0.44928255677223206
train_iter_loss: 0.23097151517868042
train_iter_loss: 0.3128339648246765
train_iter_loss: 0.39138054847717285
train_iter_loss: 0.17915931344032288
train_iter_loss: 0.37058061361312866
train_iter_loss: 0.24155691266059875
train_iter_loss: 0.29950636625289917
train_iter_loss: 0.4166007936000824
train_iter_loss: 0.31914353370666504
train_iter_loss: 0.2510823607444763
train_iter_loss: 0.2939126193523407
train_iter_loss: 0.24927754700183868
train_iter_loss: 0.3968207836151123
train_iter_loss: 0.20681321620941162
train_iter_loss: 0.3843790292739868
train_iter_loss: 0.27307018637657166
train_iter_loss: 0.3178134858608246
train_iter_loss: 0.288561075925827
train_iter_loss: 0.3537985682487488
train_iter_loss: 0.27209416031837463
train loss :0.3125
---------------------
Validation seg loss: 0.4179166048624606 at epoch 72
epoch =     73/  1000, exp = train
train_iter_loss: 0.2558321952819824
train_iter_loss: 0.5303179621696472
train_iter_loss: 0.39624255895614624
train_iter_loss: 0.38038426637649536
train_iter_loss: 0.3075428605079651
train_iter_loss: 0.2611999809741974
train_iter_loss: 0.22578178346157074
train_iter_loss: 0.3781675696372986
train_iter_loss: 0.33089497685432434
train_iter_loss: 0.29969364404678345
train_iter_loss: 0.30312660336494446
train_iter_loss: 0.48316049575805664
train_iter_loss: 0.34634146094322205
train_iter_loss: 0.23181110620498657
train_iter_loss: 0.3724782466888428
train_iter_loss: 0.37128710746765137
train_iter_loss: 0.34804463386535645
train_iter_loss: 0.25895944237709045
train_iter_loss: 0.29216575622558594
train_iter_loss: 0.4894595146179199
train_iter_loss: 0.3953203856945038
train_iter_loss: 0.24638959765434265
train_iter_loss: 0.3415883779525757
train_iter_loss: 0.40315428376197815
train_iter_loss: 0.31225645542144775
train_iter_loss: 0.3107568919658661
train_iter_loss: 0.24347065389156342
train_iter_loss: 0.3274126946926117
train_iter_loss: 0.2036985456943512
train_iter_loss: 0.30004093050956726
train_iter_loss: 0.2798111140727997
train_iter_loss: 0.26028016209602356
train_iter_loss: 0.20871831476688385
train_iter_loss: 0.23969897627830505
train_iter_loss: 0.27788588404655457
train_iter_loss: 0.29430708289146423
train_iter_loss: 0.2750776410102844
train_iter_loss: 0.23790965974330902
train_iter_loss: 0.22201576828956604
train_iter_loss: 0.43149131536483765
train_iter_loss: 0.31306836009025574
train_iter_loss: 0.260432630777359
train_iter_loss: 0.3288823068141937
train_iter_loss: 0.3931139409542084
train_iter_loss: 0.29811981320381165
train_iter_loss: 0.5274548530578613
train_iter_loss: 0.10011594742536545
train_iter_loss: 0.3354026973247528
train_iter_loss: 0.35286206007003784
train_iter_loss: 0.3266371190547943
train_iter_loss: 0.35490259528160095
train_iter_loss: 0.3871256113052368
train_iter_loss: 0.3994326591491699
train_iter_loss: 0.42183712124824524
train_iter_loss: 0.36025068163871765
train_iter_loss: 0.41377249360084534
train_iter_loss: 0.3184545338153839
train_iter_loss: 0.29559949040412903
train_iter_loss: 0.2952834665775299
train_iter_loss: 0.22607317566871643
train_iter_loss: 0.3353881537914276
train_iter_loss: 0.26739266514778137
train_iter_loss: 0.30348774790763855
train_iter_loss: 0.24887363612651825
train_iter_loss: 0.23349840939044952
train_iter_loss: 0.3373577892780304
train_iter_loss: 0.28559401631355286
train_iter_loss: 0.20293977856636047
train_iter_loss: 0.3090636730194092
train_iter_loss: 0.2516464293003082
train_iter_loss: 0.3840957581996918
train_iter_loss: 0.3007042407989502
train_iter_loss: 0.2631397843360901
train_iter_loss: 0.22100545465946198
train_iter_loss: 0.15619558095932007
train_iter_loss: 0.46415236592292786
train_iter_loss: 0.2869417369365692
train_iter_loss: 0.3487374484539032
train_iter_loss: 0.27810391783714294
train_iter_loss: 0.24616216123104095
train_iter_loss: 0.2962837517261505
train_iter_loss: 0.28703632950782776
train_iter_loss: 0.19905389845371246
train_iter_loss: 0.15463782846927643
train_iter_loss: 0.26867368817329407
train_iter_loss: 0.14481772482395172
train_iter_loss: 0.34240958094596863
train_iter_loss: 0.3194652199745178
train_iter_loss: 0.2663078010082245
train_iter_loss: 0.2787359356880188
train_iter_loss: 0.23427367210388184
train_iter_loss: 0.3828406035900116
train_iter_loss: 0.44175928831100464
train_iter_loss: 0.3195927143096924
train_iter_loss: 0.29852259159088135
train_iter_loss: 0.40319836139678955
train_iter_loss: 0.3232344090938568
train_iter_loss: 0.32641616463661194
train_iter_loss: 0.41556671261787415
train_iter_loss: 0.42223918437957764
train loss :0.3155
---------------------
Validation seg loss: 0.3815722816430454 at epoch 73
********************
best_val_epoch_loss:  0.3815722816430454
MODEL UPDATED
epoch =     74/  1000, exp = train
train_iter_loss: 0.29718276858329773
train_iter_loss: 0.2336505800485611
train_iter_loss: 0.3350796699523926
train_iter_loss: 0.29990434646606445
train_iter_loss: 0.4477224349975586
train_iter_loss: 0.5243333578109741
train_iter_loss: 0.2862996459007263
train_iter_loss: 0.25181302428245544
train_iter_loss: 0.34246137738227844
train_iter_loss: 0.314341276884079
train_iter_loss: 0.19136297702789307
train_iter_loss: 0.25957149267196655
train_iter_loss: 0.2694675624370575
train_iter_loss: 0.2115316092967987
train_iter_loss: 0.47035837173461914
train_iter_loss: 0.43705013394355774
train_iter_loss: 0.34152114391326904
train_iter_loss: 0.37894630432128906
train_iter_loss: 0.26292961835861206
train_iter_loss: 0.22885963320732117
train_iter_loss: 0.3640911877155304
train_iter_loss: 0.15854087471961975
train_iter_loss: 0.3073562681674957
train_iter_loss: 0.1748475730419159
train_iter_loss: 0.17077936232089996
train_iter_loss: 0.41139698028564453
train_iter_loss: 0.245164155960083
train_iter_loss: 0.3541663885116577
train_iter_loss: 0.25658443570137024
train_iter_loss: 0.37284615635871887
train_iter_loss: 0.3199947476387024
train_iter_loss: 0.2717578411102295
train_iter_loss: 0.4268741011619568
train_iter_loss: 0.4851439595222473
train_iter_loss: 0.2581257224082947
train_iter_loss: 0.35033804178237915
train_iter_loss: 0.23802892863750458
train_iter_loss: 0.21993182599544525
train_iter_loss: 0.25077924132347107
train_iter_loss: 0.2829934060573578
train_iter_loss: 0.41874179244041443
train_iter_loss: 0.3026866316795349
train_iter_loss: 0.3080931007862091
train_iter_loss: 0.25610294938087463
train_iter_loss: 0.41597437858581543
train_iter_loss: 0.3681676685810089
train_iter_loss: 0.24656514823436737
train_iter_loss: 0.32178860902786255
train_iter_loss: 0.19461439549922943
train_iter_loss: 0.21209290623664856
train_iter_loss: 0.2266349345445633
train_iter_loss: 0.34579208493232727
train_iter_loss: 0.3335399031639099
train_iter_loss: 0.3808774948120117
train_iter_loss: 0.2606857419013977
train_iter_loss: 0.3022509217262268
train_iter_loss: 0.21438080072402954
train_iter_loss: 0.26012328267097473
train_iter_loss: 0.32903146743774414
train_iter_loss: 0.44965726137161255
train_iter_loss: 0.2654153108596802
train_iter_loss: 0.28078144788742065
train_iter_loss: 0.3037687838077545
train_iter_loss: 0.2330051213502884
train_iter_loss: 0.4292617738246918
train_iter_loss: 0.3244154155254364
train_iter_loss: 0.3690814971923828
train_iter_loss: 0.3504762351512909
train_iter_loss: 0.3028919994831085
train_iter_loss: 0.2751516103744507
train_iter_loss: 0.24651212990283966
train_iter_loss: 0.3014446198940277
train_iter_loss: 0.2804070711135864
train_iter_loss: 0.43537163734436035
train_iter_loss: 0.24587340652942657
train_iter_loss: 0.2967139482498169
train_iter_loss: 0.11257632076740265
train_iter_loss: 0.3313386142253876
train_iter_loss: 0.5347194075584412
train_iter_loss: 0.28662750124931335
train_iter_loss: 0.3150124251842499
train_iter_loss: 0.24948976933956146
train_iter_loss: 0.23842835426330566
train_iter_loss: 0.34291213750839233
train_iter_loss: 0.32685357332229614
train_iter_loss: 0.3393036723136902
train_iter_loss: 0.3178810477256775
train_iter_loss: 0.43472787737846375
train_iter_loss: 0.306450217962265
train_iter_loss: 0.27403607964515686
train_iter_loss: 0.5032823085784912
train_iter_loss: 0.2902095317840576
train_iter_loss: 0.23364396393299103
train_iter_loss: 0.43662092089653015
train_iter_loss: 0.3249857425689697
train_iter_loss: 0.31138792634010315
train_iter_loss: 0.2693226933479309
train_iter_loss: 0.29159224033355713
train_iter_loss: 0.2391183227300644
train_iter_loss: 0.31117624044418335
train loss :0.3133
---------------------
Validation seg loss: 0.44932704459313516 at epoch 74
epoch =     75/  1000, exp = train
train_iter_loss: 0.20719031989574432
train_iter_loss: 0.14019526541233063
train_iter_loss: 0.2166890799999237
train_iter_loss: 0.3224272131919861
train_iter_loss: 0.26365405321121216
train_iter_loss: 0.39918768405914307
train_iter_loss: 0.4600129723548889
train_iter_loss: 0.3567281663417816
train_iter_loss: 0.36069148778915405
train_iter_loss: 0.17579399049282074
train_iter_loss: 0.2491278499364853
train_iter_loss: 0.2049274444580078
train_iter_loss: 0.42051956057548523
train_iter_loss: 0.2551611363887787
train_iter_loss: 0.41061311960220337
train_iter_loss: 0.31670430302619934
train_iter_loss: 0.41589438915252686
train_iter_loss: 0.28315800428390503
train_iter_loss: 0.3067426383495331
train_iter_loss: 0.2917730212211609
train_iter_loss: 0.24797438085079193
train_iter_loss: 0.2876395881175995
train_iter_loss: 0.2504611313343048
train_iter_loss: 0.319344699382782
train_iter_loss: 0.44652050733566284
train_iter_loss: 0.22705930471420288
train_iter_loss: 0.3951420187950134
train_iter_loss: 0.368278831243515
train_iter_loss: 0.4433888792991638
train_iter_loss: 0.3598476052284241
train_iter_loss: 0.40285199880599976
train_iter_loss: 0.4157397747039795
train_iter_loss: 0.3912776708602905
train_iter_loss: 0.39042094349861145
train_iter_loss: 0.25083643198013306
train_iter_loss: 0.311330109834671
train_iter_loss: 0.29494068026542664
train_iter_loss: 0.251097172498703
train_iter_loss: 0.362537145614624
train_iter_loss: 0.33201566338539124
train_iter_loss: 0.3359840512275696
train_iter_loss: 0.23315833508968353
train_iter_loss: 0.3564568758010864
train_iter_loss: 0.22629722952842712
train_iter_loss: 0.16415755450725555
train_iter_loss: 0.31616324186325073
train_iter_loss: 0.452411949634552
train_iter_loss: 0.22288593649864197
train_iter_loss: 0.27044567465782166
train_iter_loss: 0.3962602913379669
train_iter_loss: 0.26332592964172363
train_iter_loss: 0.2474815994501114
train_iter_loss: 0.2154504954814911
train_iter_loss: 0.31877824664115906
train_iter_loss: 0.2704937160015106
train_iter_loss: 0.3111296594142914
train_iter_loss: 0.34046226739883423
train_iter_loss: 0.28321975469589233
train_iter_loss: 0.19605673849582672
train_iter_loss: 0.33277490735054016
train_iter_loss: 0.4896930754184723
train_iter_loss: 0.3132666051387787
train_iter_loss: 0.17994078993797302
train_iter_loss: 0.37833577394485474
train_iter_loss: 0.2588607668876648
train_iter_loss: 0.23551736772060394
train_iter_loss: 0.24191893637180328
train_iter_loss: 0.24884209036827087
train_iter_loss: 0.2558961510658264
train_iter_loss: 0.255168080329895
train_iter_loss: 0.4534569978713989
train_iter_loss: 0.4171593487262726
train_iter_loss: 0.32201942801475525
train_iter_loss: 0.21941356360912323
train_iter_loss: 0.33601248264312744
train_iter_loss: 0.30454182624816895
train_iter_loss: 0.3256700336933136
train_iter_loss: 0.3987977206707001
train_iter_loss: 0.26482465863227844
train_iter_loss: 0.358189195394516
train_iter_loss: 0.20240890979766846
train_iter_loss: 0.3258224427700043
train_iter_loss: 0.25224408507347107
train_iter_loss: 0.29829883575439453
train_iter_loss: 0.2155388742685318
train_iter_loss: 0.2995101809501648
train_iter_loss: 0.40965819358825684
train_iter_loss: 0.32727643847465515
train_iter_loss: 0.39852267503738403
train_iter_loss: 0.290728360414505
train_iter_loss: 0.251346617937088
train_iter_loss: 0.4765629768371582
train_iter_loss: 0.20638638734817505
train_iter_loss: 0.35657474398612976
train_iter_loss: 0.22004683315753937
train_iter_loss: 0.35764166712760925
train_iter_loss: 0.2328668087720871
train_iter_loss: 0.34738364815711975
train_iter_loss: 0.42926111817359924
train_iter_loss: 0.26807159185409546
train loss :0.3130
---------------------
Validation seg loss: 0.3889581683039103 at epoch 75
epoch =     76/  1000, exp = train
train_iter_loss: 0.281169593334198
train_iter_loss: 0.347536563873291
train_iter_loss: 0.30616047978401184
train_iter_loss: 0.2764662802219391
train_iter_loss: 0.4778214693069458
train_iter_loss: 0.2917230725288391
train_iter_loss: 0.19775539636611938
train_iter_loss: 0.4089208245277405
train_iter_loss: 0.29397910833358765
train_iter_loss: 0.2610521912574768
train_iter_loss: 0.3569748103618622
train_iter_loss: 0.3372993469238281
train_iter_loss: 0.29853248596191406
train_iter_loss: 0.18766890466213226
train_iter_loss: 0.4804135262966156
train_iter_loss: 0.5269175171852112
train_iter_loss: 0.42936018109321594
train_iter_loss: 0.37530943751335144
train_iter_loss: 0.3062758147716522
train_iter_loss: 0.3655647337436676
train_iter_loss: 0.25728553533554077
train_iter_loss: 0.36121487617492676
train_iter_loss: 0.250114381313324
train_iter_loss: 0.2014191746711731
train_iter_loss: 0.5223395228385925
train_iter_loss: 0.26015758514404297
train_iter_loss: 0.5644050240516663
train_iter_loss: 0.27466174960136414
train_iter_loss: 0.27147606015205383
train_iter_loss: 0.2709192931652069
train_iter_loss: 0.3465094566345215
train_iter_loss: 0.2465885579586029
train_iter_loss: 0.2864895462989807
train_iter_loss: 0.3058180809020996
train_iter_loss: 0.298164427280426
train_iter_loss: 0.4560188353061676
train_iter_loss: 0.2662688195705414
train_iter_loss: 0.3049625754356384
train_iter_loss: 0.27656689286231995
train_iter_loss: 0.2110041379928589
train_iter_loss: 0.34204596281051636
train_iter_loss: 0.22608740627765656
train_iter_loss: 0.30740705132484436
train_iter_loss: 0.3970402181148529
train_iter_loss: 0.1429700255393982
train_iter_loss: 0.15951691567897797
train_iter_loss: 0.2438567727804184
train_iter_loss: 0.2801332175731659
train_iter_loss: 0.24578405916690826
train_iter_loss: 0.3156496584415436
train_iter_loss: 0.3606416881084442
train_iter_loss: 0.4009935259819031
train_iter_loss: 0.37317928671836853
train_iter_loss: 0.33338409662246704
train_iter_loss: 0.29999789595603943
train_iter_loss: 0.31414541602134705
train_iter_loss: 0.31937965750694275
train_iter_loss: 0.2706117033958435
train_iter_loss: 0.1725282371044159
train_iter_loss: 0.21837380528450012
train_iter_loss: 0.2907707095146179
train_iter_loss: 0.27560892701148987
train_iter_loss: 0.2510607838630676
train_iter_loss: 0.3519544303417206
train_iter_loss: 0.3891359567642212
train_iter_loss: 0.2537823021411896
train_iter_loss: 0.26795023679733276
train_iter_loss: 0.27360254526138306
train_iter_loss: 0.33523592352867126
train_iter_loss: 0.295181542634964
train_iter_loss: 0.3035881519317627
train_iter_loss: 0.609556257724762
train_iter_loss: 0.25937536358833313
train_iter_loss: 0.23236633837223053
train_iter_loss: 0.289153516292572
train_iter_loss: 0.32846570014953613
train_iter_loss: 0.2551465928554535
train_iter_loss: 0.272942453622818
train_iter_loss: 0.5241342186927795
train_iter_loss: 0.2671043574810028
train_iter_loss: 0.5732109546661377
train_iter_loss: 0.30645552277565
train_iter_loss: 0.280020147562027
train_iter_loss: 0.19708555936813354
train_iter_loss: 0.2916731536388397
train_iter_loss: 0.40966320037841797
train_iter_loss: 0.24497641623020172
train_iter_loss: 0.29752928018569946
train_iter_loss: 0.3439221680164337
train_iter_loss: 0.24095363914966583
train_iter_loss: 0.427802175283432
train_iter_loss: 0.3397076725959778
train_iter_loss: 0.22598978877067566
train_iter_loss: 0.2747882306575775
train_iter_loss: 0.18307913839817047
train_iter_loss: 0.44475066661834717
train_iter_loss: 0.2832900285720825
train_iter_loss: 0.3804752230644226
train_iter_loss: 0.3321426510810852
train_iter_loss: 0.241777703166008
train loss :0.3172
---------------------
Validation seg loss: 0.3912158798742688 at epoch 76
epoch =     77/  1000, exp = train
train_iter_loss: 0.3454373776912689
train_iter_loss: 0.15100079774856567
train_iter_loss: 0.2496807873249054
train_iter_loss: 0.2579508423805237
train_iter_loss: 0.3429940342903137
train_iter_loss: 0.5465163588523865
train_iter_loss: 0.44637444615364075
train_iter_loss: 0.3269074559211731
train_iter_loss: 0.20203951001167297
train_iter_loss: 0.12777145206928253
train_iter_loss: 0.3122388422489166
train_iter_loss: 0.28756049275398254
train_iter_loss: 0.3224297761917114
train_iter_loss: 0.2419004887342453
train_iter_loss: 0.36428555846214294
train_iter_loss: 0.45595282316207886
train_iter_loss: 0.39440158009529114
train_iter_loss: 0.4678567945957184
train_iter_loss: 0.2850392162799835
train_iter_loss: 0.3975735306739807
train_iter_loss: 0.2383536398410797
train_iter_loss: 0.38077792525291443
train_iter_loss: 0.24062317609786987
train_iter_loss: 0.2893872559070587
train_iter_loss: 0.3176560699939728
train_iter_loss: 0.246577188372612
train_iter_loss: 0.4105468690395355
train_iter_loss: 0.3487255871295929
train_iter_loss: 0.32957059144973755
train_iter_loss: 0.4377909302711487
train_iter_loss: 0.3470389246940613
train_iter_loss: 0.32934850454330444
train_iter_loss: 0.1844606101512909
train_iter_loss: 0.32228928804397583
train_iter_loss: 0.3148276209831238
train_iter_loss: 0.49656209349632263
train_iter_loss: 0.3226219713687897
train_iter_loss: 0.2357024997472763
train_iter_loss: 0.12293574959039688
train_iter_loss: 0.3347553610801697
train_iter_loss: 0.5053861737251282
train_iter_loss: 0.2079377919435501
train_iter_loss: 0.21788153052330017
train_iter_loss: 0.40141043066978455
train_iter_loss: 0.3804003596305847
train_iter_loss: 0.21547122299671173
train_iter_loss: 0.27593469619750977
train_iter_loss: 0.4734955132007599
train_iter_loss: 0.265771746635437
train_iter_loss: 0.4201173186302185
train_iter_loss: 0.3475112318992615
train_iter_loss: 0.298556387424469
train_iter_loss: 0.2551868259906769
train_iter_loss: 0.18502016365528107
train_iter_loss: 0.29547804594039917
train_iter_loss: 0.24270978569984436
train_iter_loss: 0.3306181728839874
train_iter_loss: 0.4733209013938904
train_iter_loss: 0.22175993025302887
train_iter_loss: 0.30112960934638977
train_iter_loss: 0.24849636852741241
train_iter_loss: 0.2822374403476715
train_iter_loss: 0.3074977993965149
train_iter_loss: 0.32570940256118774
train_iter_loss: 0.23691658675670624
train_iter_loss: 0.5697449445724487
train_iter_loss: 0.2656678557395935
train_iter_loss: 0.39259734749794006
train_iter_loss: 0.3100510239601135
train_iter_loss: 0.29864177107810974
train_iter_loss: 0.4589090049266815
train_iter_loss: 0.3106949031352997
train_iter_loss: 0.28485187888145447
train_iter_loss: 0.2072484940290451
train_iter_loss: 0.3881034851074219
train_iter_loss: 0.32745876908302307
train_iter_loss: 0.23301847279071808
train_iter_loss: 0.19763153791427612
train_iter_loss: 0.35390397906303406
train_iter_loss: 0.2400941550731659
train_iter_loss: 0.3146000802516937
train_iter_loss: 0.28171125054359436
train_iter_loss: 0.3216695487499237
train_iter_loss: 0.2564960718154907
train_iter_loss: 0.4036841094493866
train_iter_loss: 0.3485887944698334
train_iter_loss: 0.29051968455314636
train_iter_loss: 0.3829078674316406
train_iter_loss: 0.3377349078655243
train_iter_loss: 0.40639621019363403
train_iter_loss: 0.3769585192203522
train_iter_loss: 0.3241080939769745
train_iter_loss: 0.24092550575733185
train_iter_loss: 0.2138233631849289
train_iter_loss: 0.32897937297821045
train_iter_loss: 0.17975705862045288
train_iter_loss: 0.31302687525749207
train_iter_loss: 0.2437548041343689
train_iter_loss: 0.2886061668395996
train_iter_loss: 0.34262171387672424
train loss :0.3185
---------------------
Validation seg loss: 0.3921049288561884 at epoch 77
epoch =     78/  1000, exp = train
train_iter_loss: 0.35726964473724365
train_iter_loss: 0.2621081471443176
train_iter_loss: 0.16569842398166656
train_iter_loss: 0.2684955298900604
train_iter_loss: 0.20589657127857208
train_iter_loss: 0.28560972213745117
train_iter_loss: 0.404355525970459
train_iter_loss: 0.2937503755092621
train_iter_loss: 0.40426215529441833
train_iter_loss: 0.24275147914886475
train_iter_loss: 0.11531046032905579
train_iter_loss: 0.384255975484848
train_iter_loss: 0.35339781641960144
train_iter_loss: 0.2812727093696594
train_iter_loss: 0.5539710521697998
train_iter_loss: 0.18115933239459991
train_iter_loss: 0.1997046023607254
train_iter_loss: 0.25389543175697327
train_iter_loss: 0.3430132567882538
train_iter_loss: 0.3391059637069702
train_iter_loss: 0.2594854235649109
train_iter_loss: 0.4003433287143707
train_iter_loss: 0.3621121048927307
train_iter_loss: 0.2363930642604828
train_iter_loss: 0.35246726870536804
train_iter_loss: 0.3077687919139862
train_iter_loss: 0.3371224105358124
train_iter_loss: 0.3665357232093811
train_iter_loss: 0.23638701438903809
train_iter_loss: 0.4835023581981659
train_iter_loss: 0.477120041847229
train_iter_loss: 0.23343423008918762
train_iter_loss: 0.3147057294845581
train_iter_loss: 0.43339604139328003
train_iter_loss: 0.25631600618362427
train_iter_loss: 0.26725080609321594
train_iter_loss: 0.23787328600883484
train_iter_loss: 0.2358008623123169
train_iter_loss: 0.3126384913921356
train_iter_loss: 0.32154130935668945
train_iter_loss: 0.43724578619003296
train_iter_loss: 0.38951218128204346
train_iter_loss: 0.2116961032152176
train_iter_loss: 0.2662529945373535
train_iter_loss: 0.24299997091293335
train_iter_loss: 0.4464748203754425
train_iter_loss: 0.4352284073829651
train_iter_loss: 0.2632702887058258
train_iter_loss: 0.3244892954826355
train_iter_loss: 0.2116595208644867
train_iter_loss: 0.37180089950561523
train_iter_loss: 0.25234752893447876
train_iter_loss: 0.3558722138404846
train_iter_loss: 0.22011445462703705
train_iter_loss: 0.28668826818466187
train_iter_loss: 0.25395169854164124
train_iter_loss: 0.22649812698364258
train_iter_loss: 0.3049618899822235
train_iter_loss: 0.34387585520744324
train_iter_loss: 0.20944960415363312
train_iter_loss: 0.3529850244522095
train_iter_loss: 0.28507211804389954
train_iter_loss: 0.33822357654571533
train_iter_loss: 0.38059136271476746
train_iter_loss: 0.36414068937301636
train_iter_loss: 0.21549998223781586
train_iter_loss: 0.13364684581756592
train_iter_loss: 0.354051798582077
train_iter_loss: 0.3328523337841034
train_iter_loss: 0.2555859684944153
train_iter_loss: 0.25762903690338135
train_iter_loss: 0.33752375841140747
train_iter_loss: 0.21605955064296722
train_iter_loss: 0.22454123198986053
train_iter_loss: 0.29773610830307007
train_iter_loss: 0.5500951409339905
train_iter_loss: 0.36915823817253113
train_iter_loss: 0.2842288315296173
train_iter_loss: 0.2619725167751312
train_iter_loss: 0.2553752362728119
train_iter_loss: 0.3447660207748413
train_iter_loss: 0.29347875714302063
train_iter_loss: 0.2842981517314911
train_iter_loss: 0.3807273209095001
train_iter_loss: 0.30346378684043884
train_iter_loss: 0.20191888511180878
train_iter_loss: 0.41334864497184753
train_iter_loss: 0.37365758419036865
train_iter_loss: 0.2760242223739624
train_iter_loss: 0.2966393232345581
train_iter_loss: 0.41284510493278503
train_iter_loss: 0.48365169763565063
train_iter_loss: 0.32462868094444275
train_iter_loss: 0.3001478612422943
train_iter_loss: 0.34485289454460144
train_iter_loss: 0.45024728775024414
train_iter_loss: 0.3236164450645447
train_iter_loss: 0.32170671224594116
train_iter_loss: 0.2413889318704605
train_iter_loss: 0.363491952419281
train loss :0.3150
---------------------
Validation seg loss: 0.37532439132062895 at epoch 78
********************
best_val_epoch_loss:  0.37532439132062895
MODEL UPDATED
epoch =     79/  1000, exp = train
train_iter_loss: 0.2765299379825592
train_iter_loss: 0.2950108051300049
train_iter_loss: 0.21017220616340637
train_iter_loss: 0.27479618787765503
train_iter_loss: 0.2922383248806
train_iter_loss: 0.2486129254102707
train_iter_loss: 0.43740829825401306
train_iter_loss: 0.26402372121810913
train_iter_loss: 0.27863597869873047
train_iter_loss: 0.4354168474674225
train_iter_loss: 0.3086562156677246
train_iter_loss: 0.31701385974884033
train_iter_loss: 0.318748414516449
train_iter_loss: 0.23207326233386993
train_iter_loss: 0.43027085065841675
train_iter_loss: 0.3212507367134094
train_iter_loss: 0.1356184184551239
train_iter_loss: 0.35759875178337097
train_iter_loss: 0.3417104184627533
train_iter_loss: 0.24670177698135376
train_iter_loss: 0.28148576617240906
train_iter_loss: 0.4537045955657959
train_iter_loss: 0.34400302171707153
train_iter_loss: 0.12622253596782684
train_iter_loss: 0.2764818072319031
train_iter_loss: 0.22235789895057678
train_iter_loss: 0.37818020582199097
train_iter_loss: 0.27816900610923767
train_iter_loss: 0.3668907880783081
train_iter_loss: 0.33828169107437134
train_iter_loss: 0.37235015630722046
train_iter_loss: 0.2826128602027893
train_iter_loss: 0.3125406801700592
train_iter_loss: 0.4239490330219269
train_iter_loss: 0.33412712812423706
train_iter_loss: 0.4140387773513794
train_iter_loss: 0.3165666460990906
train_iter_loss: 0.23340211808681488
train_iter_loss: 0.29526418447494507
train_iter_loss: 0.3406981825828552
train_iter_loss: 0.3023124933242798
train_iter_loss: 0.23149095475673676
train_iter_loss: 0.5059439539909363
train_iter_loss: 0.300366073846817
train_iter_loss: 0.2873923182487488
train_iter_loss: 0.6713787317276001
train_iter_loss: 0.22551403939723969
train_iter_loss: 0.40319356322288513
train_iter_loss: 0.2582131028175354
train_iter_loss: 0.31145578622817993
train_iter_loss: 0.33443939685821533
train_iter_loss: 0.5660725235939026
train_iter_loss: 0.36191728711128235
train_iter_loss: 0.31270429491996765
train_iter_loss: 0.3023286461830139
train_iter_loss: 0.2351962924003601
train_iter_loss: 0.3318709135055542
train_iter_loss: 0.46782657504081726
train_iter_loss: 0.42599111795425415
train_iter_loss: 0.3640895485877991
train_iter_loss: 0.27464771270751953
train_iter_loss: 0.33387207984924316
train_iter_loss: 0.24405613541603088
train_iter_loss: 0.20277509093284607
train_iter_loss: 0.28409239649772644
train_iter_loss: 0.2878683805465698
train_iter_loss: 0.26366370916366577
train_iter_loss: 0.3347089886665344
train_iter_loss: 0.22882789373397827
train_iter_loss: 0.24622486531734467
train_iter_loss: 0.4578408896923065
train_iter_loss: 0.436078280210495
train_iter_loss: 0.5409115552902222
train_iter_loss: 0.4900137186050415
train_iter_loss: 0.3138149678707123
train_iter_loss: 0.24874158203601837
train_iter_loss: 0.2053268551826477
train_iter_loss: 0.27943193912506104
train_iter_loss: 0.24408841133117676
train_iter_loss: 0.3000452518463135
train_iter_loss: 0.4851122200489044
train_iter_loss: 0.33179277181625366
train_iter_loss: 0.1991751492023468
train_iter_loss: 0.23220618069171906
train_iter_loss: 0.3708468973636627
train_iter_loss: 0.27697989344596863
train_iter_loss: 0.2122754156589508
train_iter_loss: 0.3596424460411072
train_iter_loss: 0.27195605635643005
train_iter_loss: 0.28790345788002014
train_iter_loss: 0.27934378385543823
train_iter_loss: 0.18814250826835632
train_iter_loss: 0.3720380365848541
train_iter_loss: 0.2610885202884674
train_iter_loss: 0.20512376725673676
train_iter_loss: 0.36818253993988037
train_iter_loss: 0.27633050084114075
train_iter_loss: 0.15488547086715698
train_iter_loss: 0.3077622950077057
train_iter_loss: 0.23866213858127594
train loss :0.3183
---------------------
Validation seg loss: 0.39346067558200853 at epoch 79
epoch =     80/  1000, exp = train
train_iter_loss: 0.3387194573879242
train_iter_loss: 0.4098871946334839
train_iter_loss: 0.2975483536720276
train_iter_loss: 0.37935563921928406
train_iter_loss: 0.2533518075942993
train_iter_loss: 0.3047468066215515
train_iter_loss: 0.2311048060655594
train_iter_loss: 0.2915557622909546
train_iter_loss: 0.2909697890281677
train_iter_loss: 0.16098549962043762
train_iter_loss: 0.25434184074401855
train_iter_loss: 0.21367359161376953
train_iter_loss: 0.25698357820510864
train_iter_loss: 0.22508862614631653
train_iter_loss: 0.4608581066131592
train_iter_loss: 0.4236466586589813
train_iter_loss: 0.3754265010356903
train_iter_loss: 0.29178130626678467
train_iter_loss: 0.4199937880039215
train_iter_loss: 0.20864133536815643
train_iter_loss: 0.38040539622306824
train_iter_loss: 0.27027642726898193
train_iter_loss: 0.39308682084083557
train_iter_loss: 0.326646625995636
train_iter_loss: 0.25571298599243164
train_iter_loss: 0.4451149106025696
train_iter_loss: 0.32263633608818054
train_iter_loss: 0.23825107514858246
train_iter_loss: 0.3195234537124634
train_iter_loss: 0.3255840241909027
train_iter_loss: 0.2910172641277313
train_iter_loss: 0.3584027886390686
train_iter_loss: 0.30915212631225586
train_iter_loss: 0.2346172034740448
train_iter_loss: 0.337470144033432
train_iter_loss: 0.3283848762512207
train_iter_loss: 0.18677671253681183
train_iter_loss: 0.34792816638946533
train_iter_loss: 0.310163289308548
train_iter_loss: 0.43918707966804504
train_iter_loss: 0.47278016805648804
train_iter_loss: 0.2470184564590454
train_iter_loss: 0.3185528516769409
train_iter_loss: 0.13997265696525574
train_iter_loss: 0.3340030312538147
train_iter_loss: 0.41344738006591797
train_iter_loss: 0.27216655015945435
train_iter_loss: 0.34545230865478516
train_iter_loss: 0.24213051795959473
train_iter_loss: 0.46322834491729736
train_iter_loss: 0.1319754421710968
train_iter_loss: 0.14487828314304352
train_iter_loss: 0.19474031031131744
train_iter_loss: 0.42547449469566345
train_iter_loss: 0.2853086590766907
train_iter_loss: 0.30311521887779236
train_iter_loss: 0.27092668414115906
train_iter_loss: 0.42528435587882996
train_iter_loss: 0.39769965410232544
train_iter_loss: 0.3016747534275055
train_iter_loss: 0.4126376509666443
train_iter_loss: 0.21566149592399597
train_iter_loss: 0.4273870289325714
train_iter_loss: 0.30690810084342957
train_iter_loss: 0.274899423122406
train_iter_loss: 0.3232288062572479
train_iter_loss: 0.373233824968338
train_iter_loss: 0.2867361307144165
train_iter_loss: 0.38245925307273865
train_iter_loss: 0.1934446543455124
train_iter_loss: 0.2639668583869934
train_iter_loss: 0.20608653128147125
train_iter_loss: 0.387040913105011
train_iter_loss: 0.41364553570747375
train_iter_loss: 0.28616324067115784
train_iter_loss: 0.36363184452056885
train_iter_loss: 0.30498626828193665
train_iter_loss: 0.32652047276496887
train_iter_loss: 0.34672412276268005
train_iter_loss: 0.31106725335121155
train_iter_loss: 0.18004421889781952
train_iter_loss: 0.269979864358902
train_iter_loss: 0.29369089007377625
train_iter_loss: 0.3659784495830536
train_iter_loss: 0.2887187600135803
train_iter_loss: 0.23611819744110107
train_iter_loss: 0.2820812165737152
train_iter_loss: 0.18054141104221344
train_iter_loss: 0.33858349919319153
train_iter_loss: 0.4758727550506592
train_iter_loss: 0.34339430928230286
train_iter_loss: 0.4879656136035919
train_iter_loss: 0.20578858256340027
train_iter_loss: 0.3464163839817047
train_iter_loss: 0.31782448291778564
train_iter_loss: 0.276754766702652
train_iter_loss: 0.37849119305610657
train_iter_loss: 0.2613314688205719
train_iter_loss: 0.3334231376647949
train_iter_loss: 0.4736538827419281
train loss :0.3170
---------------------
Validation seg loss: 0.3956418724836044 at epoch 80
epoch =     81/  1000, exp = train
train_iter_loss: 0.3263235092163086
train_iter_loss: 0.36069175601005554
train_iter_loss: 0.36943724751472473
train_iter_loss: 0.46346524357795715
train_iter_loss: 0.37739062309265137
train_iter_loss: 0.3367605209350586
train_iter_loss: 0.45628809928894043
train_iter_loss: 0.3542257845401764
train_iter_loss: 0.17292842268943787
train_iter_loss: 0.16735391318798065
train_iter_loss: 0.2935994267463684
train_iter_loss: 0.3807964622974396
train_iter_loss: 0.2363453507423401
train_iter_loss: 0.4231851398944855
train_iter_loss: 0.35763856768608093
train_iter_loss: 0.4907737076282501
train_iter_loss: 0.4016272723674774
train_iter_loss: 0.2216208577156067
train_iter_loss: 0.34405767917633057
train_iter_loss: 0.21052710711956024
train_iter_loss: 0.15480785071849823
train_iter_loss: 0.3053920269012451
train_iter_loss: 0.38545292615890503
train_iter_loss: 0.2779579758644104
train_iter_loss: 0.28551357984542847
train_iter_loss: 0.21130405366420746
train_iter_loss: 0.24595502018928528
train_iter_loss: 0.22700779139995575
train_iter_loss: 0.2941926419734955
train_iter_loss: 0.4651161730289459
train_iter_loss: 0.36914870142936707
train_iter_loss: 0.2765321135520935
train_iter_loss: 0.39270415902137756
train_iter_loss: 0.27247411012649536
train_iter_loss: 0.23363912105560303
train_iter_loss: 0.17027752101421356
train_iter_loss: 0.3120245337486267
train_iter_loss: 0.3408026397228241
train_iter_loss: 0.4140719473361969
train_iter_loss: 0.2845795750617981
train_iter_loss: 0.3057969808578491
train_iter_loss: 0.2838428020477295
train_iter_loss: 0.2766638398170471
train_iter_loss: 0.32500454783439636
train_iter_loss: 0.27733784914016724
train_iter_loss: 0.2705354392528534
train_iter_loss: 0.3129878342151642
train_iter_loss: 0.3621051013469696
train_iter_loss: 0.4103239178657532
train_iter_loss: 0.20300275087356567
train_iter_loss: 0.30637332797050476
train_iter_loss: 0.2796296179294586
train_iter_loss: 0.2581460177898407
train_iter_loss: 0.2772231698036194
train_iter_loss: 0.2921871542930603
train_iter_loss: 0.26817891001701355
train_iter_loss: 0.4118485748767853
train_iter_loss: 0.30642783641815186
train_iter_loss: 0.2354101538658142
train_iter_loss: 0.5337429046630859
train_iter_loss: 0.2625492811203003
train_iter_loss: 0.40033069252967834
train_iter_loss: 0.3280038833618164
train_iter_loss: 0.37800484895706177
train_iter_loss: 0.3474404215812683
train_iter_loss: 0.26750311255455017
train_iter_loss: 0.24085330963134766
train_iter_loss: 0.4750455915927887
train_iter_loss: 0.22120460867881775
train_iter_loss: 0.3525357246398926
train_iter_loss: 0.28649938106536865
train_iter_loss: 0.25812989473342896
train_iter_loss: 0.21727265417575836
train_iter_loss: 0.2519732713699341
train_iter_loss: 0.35747599601745605
train_iter_loss: 0.32786720991134644
train_iter_loss: 0.19885599613189697
train_iter_loss: 0.34400153160095215
train_iter_loss: 0.3011799156665802
train_iter_loss: 0.2508448660373688
train_iter_loss: 0.3274143934249878
train_iter_loss: 0.15850135684013367
train_iter_loss: 0.23876728117465973
train_iter_loss: 0.23629151284694672
train_iter_loss: 0.2884020209312439
train_iter_loss: 0.33552947640419006
train_iter_loss: 0.379708468914032
train_iter_loss: 0.3540312945842743
train_iter_loss: 0.3431530296802521
train_iter_loss: 0.3374180197715759
train_iter_loss: 0.4566349387168884
train_iter_loss: 0.3221270740032196
train_iter_loss: 0.32191818952560425
train_iter_loss: 0.2906351685523987
train_iter_loss: 0.3189595937728882
train_iter_loss: 0.3346308469772339
train_iter_loss: 0.2721913158893585
train_iter_loss: 0.1921745389699936
train_iter_loss: 0.36538538336753845
train_iter_loss: 0.34799280762672424
train loss :0.3147
---------------------
Validation seg loss: 0.3999250607811055 at epoch 81
epoch =     82/  1000, exp = train
train_iter_loss: 0.14302916824817657
train_iter_loss: 0.3227018415927887
train_iter_loss: 0.3626337945461273
train_iter_loss: 0.33345887064933777
train_iter_loss: 0.29355233907699585
train_iter_loss: 0.26545441150665283
train_iter_loss: 0.4095290005207062
train_iter_loss: 0.22664110362529755
train_iter_loss: 0.4653739929199219
train_iter_loss: 0.2955656051635742
train_iter_loss: 0.21618635952472687
train_iter_loss: 0.34213176369667053
train_iter_loss: 0.37279799580574036
train_iter_loss: 0.32500240206718445
train_iter_loss: 0.39563268423080444
train_iter_loss: 0.2649388313293457
train_iter_loss: 0.34823474287986755
train_iter_loss: 0.30658960342407227
train_iter_loss: 0.3684627413749695
train_iter_loss: 0.2964200973510742
train_iter_loss: 0.3578891158103943
train_iter_loss: 0.29538604617118835
train_iter_loss: 0.31352826952934265
train_iter_loss: 0.6252536773681641
train_iter_loss: 0.34668269753456116
train_iter_loss: 0.4008293151855469
train_iter_loss: 0.25229206681251526
train_iter_loss: 0.30293992161750793
train_iter_loss: 0.1711122840642929
train_iter_loss: 0.4498712122440338
train_iter_loss: 0.3097579777240753
train_iter_loss: 0.34734827280044556
train_iter_loss: 0.36452871561050415
train_iter_loss: 0.2573966383934021
train_iter_loss: 0.09387778490781784
train_iter_loss: 0.28273630142211914
train_iter_loss: 0.22799041867256165
train_iter_loss: 0.37514644861221313
train_iter_loss: 0.3065538704395294
train_iter_loss: 0.3546906113624573
train_iter_loss: 0.5147552490234375
train_iter_loss: 0.27840670943260193
train_iter_loss: 0.2959374189376831
train_iter_loss: 0.16295407712459564
train_iter_loss: 0.22383420169353485
train_iter_loss: 0.3603817820549011
train_iter_loss: 0.33170175552368164
train_iter_loss: 0.2364339679479599
train_iter_loss: 0.3012729585170746
train_iter_loss: 0.37856996059417725
train_iter_loss: 0.40850260853767395
train_iter_loss: 0.35238659381866455
train_iter_loss: 0.2653975188732147
train_iter_loss: 0.3961229920387268
train_iter_loss: 0.20525534451007843
train_iter_loss: 0.4051159918308258
train_iter_loss: 0.1577068716287613
train_iter_loss: 0.2355351448059082
train_iter_loss: 0.2854346036911011
train_iter_loss: 0.2737884521484375
train_iter_loss: 0.38009878993034363
train_iter_loss: 0.1931256800889969
train_iter_loss: 0.21013498306274414
train_iter_loss: 0.3540114760398865
train_iter_loss: 0.3354955017566681
train_iter_loss: 0.3697327673435211
train_iter_loss: 0.22050923109054565
train_iter_loss: 0.3800416588783264
train_iter_loss: 0.33670634031295776
train_iter_loss: 0.5317144989967346
train_iter_loss: 0.2571277320384979
train_iter_loss: 0.31676381826400757
train_iter_loss: 0.45065295696258545
train_iter_loss: 0.4688033163547516
train_iter_loss: 0.3592813014984131
train_iter_loss: 0.29956114292144775
train_iter_loss: 0.39539188146591187
train_iter_loss: 0.17856881022453308
train_iter_loss: 0.26972267031669617
train_iter_loss: 0.322134792804718
train_iter_loss: 0.17265909910202026
train_iter_loss: 0.14833049476146698
train_iter_loss: 0.15389230847358704
train_iter_loss: 0.46685513854026794
train_iter_loss: 0.31028154492378235
train_iter_loss: 0.4013153314590454
train_iter_loss: 0.35203275084495544
train_iter_loss: 0.4636210799217224
train_iter_loss: 0.3192945420742035
train_iter_loss: 0.16373085975646973
train_iter_loss: 0.3752734959125519
train_iter_loss: 0.3934890329837799
train_iter_loss: 0.3745018243789673
train_iter_loss: 0.32648614048957825
train_iter_loss: 0.1731017678976059
train_iter_loss: 0.3563626706600189
train_iter_loss: 0.17783311009407043
train_iter_loss: 0.286651074886322
train_iter_loss: 0.34930118918418884
train_iter_loss: 0.16889281570911407
train loss :0.3174
---------------------
Validation seg loss: 0.38697208349926854 at epoch 82
epoch =     83/  1000, exp = train
train_iter_loss: 0.3918248414993286
train_iter_loss: 0.2662486433982849
train_iter_loss: 0.29602521657943726
train_iter_loss: 0.2744137644767761
train_iter_loss: 0.1501719355583191
train_iter_loss: 0.37954092025756836
train_iter_loss: 0.27592355012893677
train_iter_loss: 0.3687970042228699
train_iter_loss: 0.31003275513648987
train_iter_loss: 0.1936502754688263
train_iter_loss: 0.2205839455127716
train_iter_loss: 0.28582876920700073
train_iter_loss: 0.39807406067848206
train_iter_loss: 0.32600224018096924
train_iter_loss: 0.4073907136917114
train_iter_loss: 0.3249790370464325
train_iter_loss: 0.3635108172893524
train_iter_loss: 0.2709125876426697
train_iter_loss: 0.250704288482666
train_iter_loss: 0.30627915263175964
train_iter_loss: 0.3585509955883026
train_iter_loss: 0.31465792655944824
train_iter_loss: 0.3603440821170807
train_iter_loss: 0.24047698080539703
train_iter_loss: 0.37986916303634644
train_iter_loss: 0.48740097880363464
train_iter_loss: 0.4101935029029846
train_iter_loss: 0.4502335786819458
train_iter_loss: 0.29489612579345703
train_iter_loss: 0.3023029565811157
train_iter_loss: 0.22562053799629211
train_iter_loss: 0.31381455063819885
train_iter_loss: 0.2296334058046341
train_iter_loss: 0.25266775488853455
train_iter_loss: 0.369874507188797
train_iter_loss: 0.38216495513916016
train_iter_loss: 0.23601780831813812
train_iter_loss: 0.3071528971195221
train_iter_loss: 0.33662426471710205
train_iter_loss: 0.2631123661994934
train_iter_loss: 0.28756582736968994
train_iter_loss: 0.2660026550292969
train_iter_loss: 0.4894405007362366
train_iter_loss: 0.358589231967926
train_iter_loss: 0.31523391604423523
train_iter_loss: 0.32478275895118713
train_iter_loss: 0.12097226083278656
train_iter_loss: 0.2593785226345062
train_iter_loss: 0.14937396347522736
train_iter_loss: 0.38659408688545227
train_iter_loss: 0.2858397960662842
train_iter_loss: 0.20310983061790466
train_iter_loss: 0.40180057287216187
train_iter_loss: 0.25885719060897827
train_iter_loss: 0.371548593044281
train_iter_loss: 0.2774134874343872
train_iter_loss: 0.3665500283241272
train_iter_loss: 0.3673202097415924
train_iter_loss: 0.3468323349952698
train_iter_loss: 0.2468293160200119
train_iter_loss: 0.26235431432724
train_iter_loss: 0.31129759550094604
train_iter_loss: 0.3456653952598572
train_iter_loss: 0.21543017029762268
train_iter_loss: 0.27703556418418884
train_iter_loss: 0.38765281438827515
train_iter_loss: 0.40515974164009094
train_iter_loss: 0.21228036284446716
train_iter_loss: 0.4228660464286804
train_iter_loss: 0.27364203333854675
train_iter_loss: 0.3135770857334137
train_iter_loss: 0.32776570320129395
train_iter_loss: 0.29188066720962524
train_iter_loss: 0.3415575921535492
train_iter_loss: 0.08610053360462189
train_iter_loss: 0.2340206503868103
train_iter_loss: 0.4472770690917969
train_iter_loss: 0.42466801404953003
train_iter_loss: 0.22127555310726166
train_iter_loss: 0.3308790326118469
train_iter_loss: 0.2995217442512512
train_iter_loss: 0.3460066020488739
train_iter_loss: 0.24496565759181976
train_iter_loss: 0.41503018140792847
train_iter_loss: 0.33154940605163574
train_iter_loss: 0.3712307810783386
train_iter_loss: 0.2662574350833893
train_iter_loss: 0.24088579416275024
train_iter_loss: 0.2869332730770111
train_iter_loss: 0.27685782313346863
train_iter_loss: 0.2763388752937317
train_iter_loss: 0.25527146458625793
train_iter_loss: 0.2776598334312439
train_iter_loss: 0.14830949902534485
train_iter_loss: 0.44987252354621887
train_iter_loss: 0.36170920729637146
train_iter_loss: 0.33774077892303467
train_iter_loss: 0.27779752016067505
train_iter_loss: 0.2721545100212097
train_iter_loss: 0.31559669971466064
train loss :0.3116
---------------------
Validation seg loss: 0.3852443257171028 at epoch 83
epoch =     84/  1000, exp = train
train_iter_loss: 0.34682920575141907
train_iter_loss: 0.1778828650712967
train_iter_loss: 0.27192381024360657
train_iter_loss: 0.2660149335861206
train_iter_loss: 0.32698944211006165
train_iter_loss: 0.3457130491733551
train_iter_loss: 0.29021313786506653
train_iter_loss: 0.39494332671165466
train_iter_loss: 0.37565767765045166
train_iter_loss: 0.24324247241020203
train_iter_loss: 0.3553014099597931
train_iter_loss: 0.15449316799640656
train_iter_loss: 0.4094962775707245
train_iter_loss: 0.20836833119392395
train_iter_loss: 0.2298734039068222
train_iter_loss: 0.34786152839660645
train_iter_loss: 0.24650108814239502
train_iter_loss: 0.4529308080673218
train_iter_loss: 0.26889732480049133
train_iter_loss: 0.4075319766998291
train_iter_loss: 0.4906235635280609
train_iter_loss: 0.2276908904314041
train_iter_loss: 0.26413148641586304
train_iter_loss: 0.23860424757003784
train_iter_loss: 0.4783112704753876
train_iter_loss: 0.3329678475856781
train_iter_loss: 0.2956206202507019
train_iter_loss: 0.3167911171913147
train_iter_loss: 0.28595733642578125
train_iter_loss: 0.30437523126602173
train_iter_loss: 0.30380749702453613
train_iter_loss: 0.31696876883506775
train_iter_loss: 0.2665722072124481
train_iter_loss: 0.22218279540538788
train_iter_loss: 0.2288140207529068
train_iter_loss: 0.3209303021430969
train_iter_loss: 0.33575451374053955
train_iter_loss: 0.2439493089914322
train_iter_loss: 0.2521081268787384
train_iter_loss: 0.23425480723381042
train_iter_loss: 0.251707524061203
train_iter_loss: 0.31442421674728394
train_iter_loss: 0.33882007002830505
train_iter_loss: 0.4853794574737549
train_iter_loss: 0.19262216985225677
train_iter_loss: 0.2577999234199524
train_iter_loss: 0.20106156170368195
train_iter_loss: 0.1725037693977356
train_iter_loss: 0.20612141489982605
train_iter_loss: 0.3748129904270172
train_iter_loss: 0.34837958216667175
train_iter_loss: 0.3924782872200012
train_iter_loss: 0.20109951496124268
train_iter_loss: 0.32970020174980164
train_iter_loss: 0.18746577203273773
train_iter_loss: 0.18909801542758942
train_iter_loss: 0.3770558536052704
train_iter_loss: 0.3753906190395355
train_iter_loss: 0.44700857996940613
train_iter_loss: 0.339657723903656
train_iter_loss: 0.4272749125957489
train_iter_loss: 0.28394919633865356
train_iter_loss: 0.33310967683792114
train_iter_loss: 0.3297653794288635
train_iter_loss: 0.29270341992378235
train_iter_loss: 0.4890517592430115
train_iter_loss: 0.17880932986736298
train_iter_loss: 0.2762192189693451
train_iter_loss: 0.24377915263175964
train_iter_loss: 0.23555846512317657
train_iter_loss: 0.40228691697120667
train_iter_loss: 0.41548410058021545
train_iter_loss: 0.31820088624954224
train_iter_loss: 0.39893272519111633
train_iter_loss: 0.2746867835521698
train_iter_loss: 0.3416462242603302
train_iter_loss: 0.2520120143890381
train_iter_loss: 0.2911079227924347
train_iter_loss: 0.3076951801776886
train_iter_loss: 0.2955932319164276
train_iter_loss: 0.4873640239238739
train_iter_loss: 0.22098910808563232
train_iter_loss: 0.2928253710269928
train_iter_loss: 0.3363732695579529
train_iter_loss: 0.30560243129730225
train_iter_loss: 0.2586731016635895
train_iter_loss: 0.21200959384441376
train_iter_loss: 0.3914937973022461
train_iter_loss: 0.25888052582740784
train_iter_loss: 0.30839160084724426
train_iter_loss: 0.4727976322174072
train_iter_loss: 0.29091283679008484
train_iter_loss: 0.27773919701576233
train_iter_loss: 0.2986737787723541
train_iter_loss: 0.4096011817455292
train_iter_loss: 0.21586713194847107
train_iter_loss: 0.2826385200023651
train_iter_loss: 0.2685891091823578
train_iter_loss: 0.373168021440506
train_iter_loss: 0.277635782957077
train loss :0.3100
---------------------
Validation seg loss: 0.4472346731971176 at epoch 84
epoch =     85/  1000, exp = train
train_iter_loss: 0.3451642394065857
train_iter_loss: 0.215164452791214
train_iter_loss: 0.25164592266082764
train_iter_loss: 0.40897366404533386
train_iter_loss: 0.2729591429233551
train_iter_loss: 0.34581249952316284
train_iter_loss: 0.23864386975765228
train_iter_loss: 0.40359553694725037
train_iter_loss: 0.3223702311515808
train_iter_loss: 0.4298650920391083
train_iter_loss: 0.36706945300102234
train_iter_loss: 0.3024405241012573
train_iter_loss: 0.14595605432987213
train_iter_loss: 0.2508472502231598
train_iter_loss: 0.2805846333503723
train_iter_loss: 0.31226998567581177
train_iter_loss: 0.3023993670940399
train_iter_loss: 0.407587468624115
train_iter_loss: 0.24200263619422913
train_iter_loss: 0.27575504779815674
train_iter_loss: 0.20094749331474304
train_iter_loss: 0.512637197971344
train_iter_loss: 0.37938618659973145
train_iter_loss: 0.28000280261039734
train_iter_loss: 0.29919666051864624
train_iter_loss: 0.2291698306798935
train_iter_loss: 0.29232025146484375
train_iter_loss: 0.33355242013931274
train_iter_loss: 0.5492475032806396
train_iter_loss: 0.2738718092441559
train_iter_loss: 0.2485445737838745
train_iter_loss: 0.3043143153190613
train_iter_loss: 0.2679307758808136
train_iter_loss: 0.29224464297294617
train_iter_loss: 0.3149137794971466
train_iter_loss: 0.23188640177249908
train_iter_loss: 0.28127196431159973
train_iter_loss: 0.22481390833854675
train_iter_loss: 0.34972167015075684
train_iter_loss: 0.26168107986450195
train_iter_loss: 0.3748326003551483
train_iter_loss: 0.36796462535858154
train_iter_loss: 0.3712133765220642
train_iter_loss: 0.3559163212776184
train_iter_loss: 0.32713499665260315
train_iter_loss: 0.38040831685066223
train_iter_loss: 0.21874068677425385
train_iter_loss: 0.26224178075790405
train_iter_loss: 0.2249681055545807
train_iter_loss: 0.2774468958377838
train_iter_loss: 0.26695266366004944
train_iter_loss: 0.29811179637908936
train_iter_loss: 0.27890321612358093
train_iter_loss: 0.24507926404476166
train_iter_loss: 0.30186793208122253
train_iter_loss: 0.2970302700996399
train_iter_loss: 0.6180428862571716
train_iter_loss: 0.2583260238170624
train_iter_loss: 0.28448665142059326
train_iter_loss: 0.3632846176624298
train_iter_loss: 0.27335524559020996
train_iter_loss: 0.3681265711784363
train_iter_loss: 0.2469525784254074
train_iter_loss: 0.3710237443447113
train_iter_loss: 0.3055942952632904
train_iter_loss: 0.17990007996559143
train_iter_loss: 0.3223351836204529
train_iter_loss: 0.2645122706890106
train_iter_loss: 0.3042358160018921
train_iter_loss: 0.41021019220352173
train_iter_loss: 0.3829142153263092
train_iter_loss: 0.21091343462467194
train_iter_loss: 0.2045852094888687
train_iter_loss: 0.3130655288696289
train_iter_loss: 0.3150641620159149
train_iter_loss: 0.20378339290618896
train_iter_loss: 0.2681816518306732
train_iter_loss: 0.2803540527820587
train_iter_loss: 0.24855469167232513
train_iter_loss: 0.5001853108406067
train_iter_loss: 0.24591860175132751
train_iter_loss: 0.3006000518798828
train_iter_loss: 0.1861724704504013
train_iter_loss: 0.19348418712615967
train_iter_loss: 0.3260822892189026
train_iter_loss: 0.4134771227836609
train_iter_loss: 0.2691915035247803
train_iter_loss: 0.3906271755695343
train_iter_loss: 0.33870190382003784
train_iter_loss: 0.1886311024427414
train_iter_loss: 0.20649223029613495
train_iter_loss: 0.2873642146587372
train_iter_loss: 0.2187964916229248
train_iter_loss: 0.35050123929977417
train_iter_loss: 0.23714931309223175
train_iter_loss: 0.3015200197696686
train_iter_loss: 0.6221011877059937
train_iter_loss: 0.4192526042461395
train_iter_loss: 0.2937711775302887
train_iter_loss: 0.2651267647743225
train loss :0.3096
---------------------
Validation seg loss: 0.394020864441288 at epoch 85
epoch =     86/  1000, exp = train
train_iter_loss: 0.39727723598480225
train_iter_loss: 0.3346802592277527
train_iter_loss: 0.22630737721920013
train_iter_loss: 0.17907965183258057
train_iter_loss: 0.3133757412433624
train_iter_loss: 0.24349187314510345
train_iter_loss: 0.39226704835891724
train_iter_loss: 0.3301512897014618
train_iter_loss: 0.20401686429977417
train_iter_loss: 0.3254512846469879
train_iter_loss: 0.2675867974758148
train_iter_loss: 0.45111167430877686
train_iter_loss: 0.32038915157318115
train_iter_loss: 0.3048977255821228
train_iter_loss: 0.38614749908447266
train_iter_loss: 0.3235305845737457
train_iter_loss: 0.17720675468444824
train_iter_loss: 0.2889910936355591
train_iter_loss: 0.46818220615386963
train_iter_loss: 0.24342146515846252
train_iter_loss: 0.18716870248317719
train_iter_loss: 0.4284506142139435
train_iter_loss: 0.3617798089981079
train_iter_loss: 0.34957948327064514
train_iter_loss: 0.24082298576831818
train_iter_loss: 0.2540059983730316
train_iter_loss: 0.31523397564888
train_iter_loss: 0.33239248394966125
train_iter_loss: 0.2530668079853058
train_iter_loss: 0.30995747447013855
train_iter_loss: 0.2579512298107147
train_iter_loss: 0.2699494957923889
train_iter_loss: 0.33262670040130615
train_iter_loss: 0.27756884694099426
train_iter_loss: 0.26931291818618774
train_iter_loss: 0.2888803780078888
train_iter_loss: 0.19260698556900024
train_iter_loss: 0.2893630564212799
train_iter_loss: 0.26591846346855164
train_iter_loss: 0.23944874107837677
train_iter_loss: 0.43222251534461975
train_iter_loss: 0.3557058274745941
train_iter_loss: 0.2243441641330719
train_iter_loss: 0.28212785720825195
train_iter_loss: 0.2998448312282562
train_iter_loss: 0.1617971807718277
train_iter_loss: 0.24027281999588013
train_iter_loss: 0.3549576699733734
train_iter_loss: 0.4173496663570404
train_iter_loss: 0.3420664072036743
train_iter_loss: 0.40584149956703186
train_iter_loss: 0.32582226395606995
train_iter_loss: 0.32258641719818115
train_iter_loss: 0.41309854388237
train_iter_loss: 0.2219010889530182
train_iter_loss: 0.19031046330928802
train_iter_loss: 0.2609286606311798
train_iter_loss: 0.16954372823238373
train_iter_loss: 0.21008077263832092
train_iter_loss: 0.24974043667316437
train_iter_loss: 0.36805635690689087
train_iter_loss: 0.2860499322414398
train_iter_loss: 0.37010714411735535
train_iter_loss: 0.16612373292446136
train_iter_loss: 0.2720928192138672
train_iter_loss: 0.36056220531463623
train_iter_loss: 0.3412744104862213
train_iter_loss: 0.3753180503845215
train_iter_loss: 0.28993937373161316
train_iter_loss: 0.23746773600578308
train_iter_loss: 0.18704846501350403
train_iter_loss: 0.24277663230895996
train_iter_loss: 0.28175026178359985
train_iter_loss: 0.3793153762817383
train_iter_loss: 0.26515984535217285
train_iter_loss: 0.4444144666194916
train_iter_loss: 0.26081836223602295
train_iter_loss: 0.4453047513961792
train_iter_loss: 0.36146146059036255
train_iter_loss: 0.296369731426239
train_iter_loss: 0.2161019891500473
train_iter_loss: 0.3652665615081787
train_iter_loss: 0.3312501907348633
train_iter_loss: 0.32973209023475647
train_iter_loss: 0.29029199481010437
train_iter_loss: 0.4686752259731293
train_iter_loss: 0.35821011662483215
train_iter_loss: 0.23506180942058563
train_iter_loss: 0.46575212478637695
train_iter_loss: 0.22694264352321625
train_iter_loss: 0.3099336624145508
train_iter_loss: 0.1817629486322403
train_iter_loss: 0.21631969511508942
train_iter_loss: 0.3818645477294922
train_iter_loss: 0.3498910069465637
train_iter_loss: 0.31272536516189575
train_iter_loss: 0.3451811373233795
train_iter_loss: 0.4201321005821228
train_iter_loss: 0.48029395937919617
train_iter_loss: 0.2702651023864746
train loss :0.3087
---------------------
Validation seg loss: 0.38539923029899037 at epoch 86
epoch =     87/  1000, exp = train
train_iter_loss: 0.18164688348770142
train_iter_loss: 0.5027220249176025
train_iter_loss: 0.282696008682251
train_iter_loss: 0.2510327398777008
train_iter_loss: 0.44996199011802673
train_iter_loss: 0.32426759600639343
train_iter_loss: 0.31985795497894287
train_iter_loss: 0.3422463834285736
train_iter_loss: 0.31162315607070923
train_iter_loss: 0.2273101955652237
train_iter_loss: 0.20684249699115753
train_iter_loss: 0.23473060131072998
train_iter_loss: 0.2859455645084381
train_iter_loss: 0.4440666735172272
train_iter_loss: 0.4126325845718384
train_iter_loss: 0.3670292794704437
train_iter_loss: 0.2675049602985382
train_iter_loss: 0.3126848042011261
train_iter_loss: 0.396248996257782
train_iter_loss: 0.36279842257499695
train_iter_loss: 0.2510243356227875
train_iter_loss: 0.4055643379688263
train_iter_loss: 0.27807560563087463
train_iter_loss: 0.19349190592765808
train_iter_loss: 0.20536315441131592
train_iter_loss: 0.284814715385437
train_iter_loss: 0.6882908940315247
train_iter_loss: 0.27767324447631836
train_iter_loss: 0.3403216302394867
train_iter_loss: 0.3802255392074585
train_iter_loss: 0.407245934009552
train_iter_loss: 0.4113177955150604
train_iter_loss: 0.3471023738384247
train_iter_loss: 0.4442717730998993
train_iter_loss: 0.5463977456092834
train_iter_loss: 0.23473700881004333
train_iter_loss: 0.45083722472190857
train_iter_loss: 0.28742215037345886
train_iter_loss: 0.20378100872039795
train_iter_loss: 0.36936214566230774
train_iter_loss: 0.2544451355934143
train_iter_loss: 0.3137087821960449
train_iter_loss: 0.32625141739845276
train_iter_loss: 0.361672580242157
train_iter_loss: 0.3668306767940521
train_iter_loss: 0.22424928843975067
train_iter_loss: 0.326447457075119
train_iter_loss: 0.40713366866111755
train_iter_loss: 0.2725888192653656
train_iter_loss: 0.27855440974235535
train_iter_loss: 0.23800969123840332
train_iter_loss: 0.22565795481204987
train_iter_loss: 0.22353872656822205
train_iter_loss: 0.20581698417663574
train_iter_loss: 0.27439361810684204
train_iter_loss: 0.3016044795513153
train_iter_loss: 0.332675039768219
train_iter_loss: 0.30358609557151794
train_iter_loss: 0.29439595341682434
train_iter_loss: 0.24470768868923187
train_iter_loss: 0.25809842348098755
train_iter_loss: 0.23232752084732056
train_iter_loss: 0.31629636883735657
train_iter_loss: 0.2632424831390381
train_iter_loss: 0.2805335223674774
train_iter_loss: 0.36803415417671204
train_iter_loss: 0.26338687539100647
train_iter_loss: 0.43866169452667236
train_iter_loss: 0.2919989824295044
train_iter_loss: 0.25883981585502625
train_iter_loss: 0.23468883335590363
train_iter_loss: 0.35469725728034973
train_iter_loss: 0.2673846185207367
train_iter_loss: 0.2705097496509552
train_iter_loss: 0.26094332337379456
train_iter_loss: 0.15201589465141296
train_iter_loss: 0.32520487904548645
train_iter_loss: 0.11719103902578354
train_iter_loss: 0.3635959029197693
train_iter_loss: 0.3265113830566406
train_iter_loss: 0.39415431022644043
train_iter_loss: 0.1693328469991684
train_iter_loss: 0.32908204197883606
train_iter_loss: 0.32180124521255493
train_iter_loss: 0.3007376790046692
train_iter_loss: 0.489603191614151
train_iter_loss: 0.3885640799999237
train_iter_loss: 0.23284228146076202
train_iter_loss: 0.29802215099334717
train_iter_loss: 0.389187753200531
train_iter_loss: 0.3334573209285736
train_iter_loss: 0.2839217185974121
train_iter_loss: 0.2936510443687439
train_iter_loss: 0.3147464692592621
train_iter_loss: 0.2895360291004181
train_iter_loss: 0.41253167390823364
train_iter_loss: 0.3424597680568695
train_iter_loss: 0.24352921545505524
train_iter_loss: 0.3385831117630005
train_iter_loss: 0.31471318006515503
train loss :0.3170
---------------------
Validation seg loss: 0.39859755683409154 at epoch 87
epoch =     88/  1000, exp = train
train_iter_loss: 0.19677631556987762
train_iter_loss: 0.3638978600502014
train_iter_loss: 0.35501790046691895
train_iter_loss: 0.2839905917644501
train_iter_loss: 0.2809050679206848
train_iter_loss: 0.3017633855342865
train_iter_loss: 0.24763169884681702
train_iter_loss: 0.3580053746700287
train_iter_loss: 0.486335426568985
train_iter_loss: 0.3234194815158844
train_iter_loss: 0.31833308935165405
train_iter_loss: 0.4185870289802551
train_iter_loss: 0.41396844387054443
train_iter_loss: 0.43799325823783875
train_iter_loss: 0.3074095547199249
train_iter_loss: 0.23198342323303223
train_iter_loss: 0.20600973069667816
train_iter_loss: 0.2770087718963623
train_iter_loss: 0.3438068628311157
train_iter_loss: 0.128086119890213
train_iter_loss: 0.23534849286079407
train_iter_loss: 0.2311190366744995
train_iter_loss: 0.37804099917411804
train_iter_loss: 0.2449101060628891
train_iter_loss: 0.4025169909000397
train_iter_loss: 0.32561859488487244
train_iter_loss: 0.4255487322807312
train_iter_loss: 0.11039066314697266
train_iter_loss: 0.36149972677230835
train_iter_loss: 0.3278864622116089
train_iter_loss: 0.35666412115097046
train_iter_loss: 0.3111032545566559
train_iter_loss: 0.19062449038028717
train_iter_loss: 0.23008133471012115
train_iter_loss: 0.22314222157001495
train_iter_loss: 0.2630593180656433
train_iter_loss: 0.30720797181129456
train_iter_loss: 0.35818544030189514
train_iter_loss: 0.34153711795806885
train_iter_loss: 0.32034897804260254
train_iter_loss: 0.5906305313110352
train_iter_loss: 0.30398455262184143
train_iter_loss: 0.27374687790870667
train_iter_loss: 0.19041787087917328
train_iter_loss: 0.34524255990982056
train_iter_loss: 0.31091853976249695
train_iter_loss: 0.4276377856731415
train_iter_loss: 0.2658389210700989
train_iter_loss: 0.24659183621406555
train_iter_loss: 0.40778061747550964
train_iter_loss: 0.3084186613559723
train_iter_loss: 0.25105905532836914
train_iter_loss: 0.23927076160907745
train_iter_loss: 0.26944175362586975
train_iter_loss: 0.25037312507629395
train_iter_loss: 0.26746833324432373
train_iter_loss: 0.326908141374588
train_iter_loss: 0.32534223794937134
train_iter_loss: 0.3784078061580658
train_iter_loss: 0.2403709590435028
train_iter_loss: 0.41147124767303467
train_iter_loss: 0.3478909730911255
train_iter_loss: 0.273092657327652
train_iter_loss: 0.3277074098587036
train_iter_loss: 0.2902478277683258
train_iter_loss: 0.21981662511825562
train_iter_loss: 0.1546044498682022
train_iter_loss: 0.30539244413375854
train_iter_loss: 0.17108851671218872
train_iter_loss: 0.32168489694595337
train_iter_loss: 0.24387717247009277
train_iter_loss: 0.6265544891357422
train_iter_loss: 0.30248814821243286
train_iter_loss: 0.3639857769012451
train_iter_loss: 0.22896161675453186
train_iter_loss: 0.32579517364501953
train_iter_loss: 0.32688117027282715
train_iter_loss: 0.3429110646247864
train_iter_loss: 0.31380319595336914
train_iter_loss: 0.32532334327697754
train_iter_loss: 0.35617595911026
train_iter_loss: 0.2809096574783325
train_iter_loss: 0.3476083278656006
train_iter_loss: 0.46011197566986084
train_iter_loss: 0.33572718501091003
train_iter_loss: 0.30607855319976807
train_iter_loss: 0.3843958377838135
train_iter_loss: 0.2795647978782654
train_iter_loss: 0.5212677121162415
train_iter_loss: 0.32905590534210205
train_iter_loss: 0.3202120363712311
train_iter_loss: 0.2943565249443054
train_iter_loss: 0.2581577003002167
train_iter_loss: 0.22650684416294098
train_iter_loss: 0.24408161640167236
train_iter_loss: 0.28377464413642883
train_iter_loss: 0.24906378984451294
train_iter_loss: 0.3363904356956482
train_iter_loss: 0.4520569443702698
train_iter_loss: 0.41087862849235535
train loss :0.3166
---------------------
Validation seg loss: 0.42721926902403246 at epoch 88
epoch =     89/  1000, exp = train
train_iter_loss: 0.29980406165122986
train_iter_loss: 0.2138095200061798
train_iter_loss: 0.4202456772327423
train_iter_loss: 0.13931861519813538
train_iter_loss: 0.1926610916852951
train_iter_loss: 0.2358173280954361
train_iter_loss: 0.3328303396701813
train_iter_loss: 0.3022564649581909
train_iter_loss: 0.20980235934257507
train_iter_loss: 0.23547367751598358
train_iter_loss: 0.32504960894584656
train_iter_loss: 0.39354050159454346
train_iter_loss: 0.5009714365005493
train_iter_loss: 0.3902672827243805
train_iter_loss: 0.26120027899742126
train_iter_loss: 0.2994277775287628
train_iter_loss: 0.39015984535217285
train_iter_loss: 0.2781372368335724
train_iter_loss: 0.2792421877384186
train_iter_loss: 0.3826025128364563
train_iter_loss: 0.29160743951797485
train_iter_loss: 0.3465438783168793
train_iter_loss: 0.35532498359680176
train_iter_loss: 0.299081951379776
train_iter_loss: 0.19923794269561768
train_iter_loss: 0.48192712664604187
train_iter_loss: 0.3917982578277588
train_iter_loss: 0.23599058389663696
train_iter_loss: 0.3266654312610626
train_iter_loss: 0.18375781178474426
train_iter_loss: 0.3097783625125885
train_iter_loss: 0.6843821406364441
train_iter_loss: 0.3860832154750824
train_iter_loss: 0.37541210651397705
train_iter_loss: 0.37835922837257385
train_iter_loss: 0.22666405141353607
train_iter_loss: 0.5488964319229126
train_iter_loss: 0.3560398519039154
train_iter_loss: 0.27103546261787415
train_iter_loss: 0.20114049315452576
train_iter_loss: 0.29039451479911804
train_iter_loss: 0.2346605360507965
train_iter_loss: 0.38031426072120667
train_iter_loss: 0.29774272441864014
train_iter_loss: 0.30568572878837585
train_iter_loss: 0.36225244402885437
train_iter_loss: 0.3757050037384033
train_iter_loss: 0.2209620624780655
train_iter_loss: 0.13889718055725098
train_iter_loss: 0.27582690119743347
train_iter_loss: 0.3204874098300934
train_iter_loss: 0.579004168510437
train_iter_loss: 0.34223878383636475
train_iter_loss: 0.23718374967575073
train_iter_loss: 0.2978671193122864
train_iter_loss: 0.2525002360343933
train_iter_loss: 0.21444235742092133
train_iter_loss: 0.29700446128845215
train_iter_loss: 0.3812999427318573
train_iter_loss: 0.3975967466831207
train_iter_loss: 0.4320851266384125
train_iter_loss: 0.30223792791366577
train_iter_loss: 0.38353225588798523
train_iter_loss: 0.3918289244174957
train_iter_loss: 0.23151952028274536
train_iter_loss: 0.15907733142375946
train_iter_loss: 0.317778617143631
train_iter_loss: 0.26128146052360535
train_iter_loss: 0.2146410346031189
train_iter_loss: 0.3230898082256317
train_iter_loss: 0.2686876654624939
train_iter_loss: 0.26665225625038147
train_iter_loss: 0.3059980869293213
train_iter_loss: 0.3114727735519409
train_iter_loss: 0.30890220403671265
train_iter_loss: 0.18542273342609406
train_iter_loss: 0.1897607147693634
train_iter_loss: 0.19369621574878693
train_iter_loss: 0.3046506345272064
train_iter_loss: 0.39539089798927307
train_iter_loss: 0.33646202087402344
train_iter_loss: 0.3258759677410126
train_iter_loss: 0.22252590954303741
train_iter_loss: 0.5474814772605896
train_iter_loss: 0.29061996936798096
train_iter_loss: 0.42855438590049744
train_iter_loss: 0.17538492381572723
train_iter_loss: 0.3011058270931244
train_iter_loss: 0.30139920115470886
train_iter_loss: 0.2676985263824463
train_iter_loss: 0.2689138352870941
train_iter_loss: 0.39505067467689514
train_iter_loss: 0.29607731103897095
train_iter_loss: 0.3097416162490845
train_iter_loss: 0.31852737069129944
train_iter_loss: 0.48562315106391907
train_iter_loss: 0.310652494430542
train_iter_loss: 0.2755048871040344
train_iter_loss: 0.2734473645687103
train_iter_loss: 0.1369486302137375
train loss :0.3147
---------------------
Validation seg loss: 0.39647210865580246 at epoch 89
epoch =     90/  1000, exp = train
train_iter_loss: 0.21241068840026855
train_iter_loss: 0.3716701567173004
train_iter_loss: 0.2957395613193512
train_iter_loss: 0.2932734787464142
train_iter_loss: 0.27240410447120667
train_iter_loss: 0.212381511926651
train_iter_loss: 0.2995762825012207
train_iter_loss: 0.42501357197761536
train_iter_loss: 0.23790094256401062
train_iter_loss: 0.23921160399913788
train_iter_loss: 0.2631181478500366
train_iter_loss: 0.2681241035461426
train_iter_loss: 0.34459248185157776
train_iter_loss: 0.34256500005722046
train_iter_loss: 0.27061590552330017
train_iter_loss: 0.2772058844566345
train_iter_loss: 0.3041365444660187
train_iter_loss: 0.310638964176178
train_iter_loss: 0.3203268051147461
train_iter_loss: 0.30938366055488586
train_iter_loss: 0.22568610310554504
train_iter_loss: 0.3425165116786957
train_iter_loss: 0.43191924691200256
train_iter_loss: 0.30731111764907837
train_iter_loss: 0.24135170876979828
train_iter_loss: 0.40124207735061646
train_iter_loss: 0.22632981836795807
train_iter_loss: 0.2745870351791382
train_iter_loss: 0.20808522403240204
train_iter_loss: 0.3057101368904114
train_iter_loss: 0.21861904859542847
train_iter_loss: 0.41395506262779236
train_iter_loss: 0.3239416182041168
train_iter_loss: 0.27772438526153564
train_iter_loss: 0.3041476011276245
train_iter_loss: 0.28457701206207275
train_iter_loss: 0.18662160634994507
train_iter_loss: 0.3252395987510681
train_iter_loss: 0.2932303547859192
train_iter_loss: 0.2729458510875702
train_iter_loss: 0.30694425106048584
train_iter_loss: 0.23804223537445068
train_iter_loss: 0.3427008092403412
train_iter_loss: 0.40431123971939087
train_iter_loss: 0.2804161608219147
train_iter_loss: 0.29494547843933105
train_iter_loss: 0.2714214324951172
train_iter_loss: 0.25352680683135986
train_iter_loss: 0.16557525098323822
train_iter_loss: 0.36529624462127686
train_iter_loss: 0.31264054775238037
train_iter_loss: 0.2195596843957901
train_iter_loss: 0.20578400790691376
train_iter_loss: 0.3122766315937042
train_iter_loss: 0.44627323746681213
train_iter_loss: 0.22021524608135223
train_iter_loss: 0.3092603385448456
train_iter_loss: 0.2863308787345886
train_iter_loss: 0.2927611470222473
train_iter_loss: 0.42624908685684204
train_iter_loss: 0.2677215039730072
train_iter_loss: 0.2634425163269043
train_iter_loss: 0.268915593624115
train_iter_loss: 0.47910401225090027
train_iter_loss: 0.2722390294075012
train_iter_loss: 0.3563697934150696
train_iter_loss: 0.25701671838760376
train_iter_loss: 0.34840673208236694
train_iter_loss: 0.3300374448299408
train_iter_loss: 0.33284106850624084
train_iter_loss: 0.2696610689163208
train_iter_loss: 0.18174336850643158
train_iter_loss: 0.26708847284317017
train_iter_loss: 0.2280106544494629
train_iter_loss: 0.5241217017173767
train_iter_loss: 0.36880072951316833
train_iter_loss: 0.5858758091926575
train_iter_loss: 0.2180113047361374
train_iter_loss: 0.48573487997055054
train_iter_loss: 0.33798015117645264
train_iter_loss: 0.3038947880268097
train_iter_loss: 0.2300732135772705
train_iter_loss: 0.18818721175193787
train_iter_loss: 0.1375475823879242
train_iter_loss: 0.23162569105625153
train_iter_loss: 0.5060985684394836
train_iter_loss: 0.24759821593761444
train_iter_loss: 0.2269953191280365
train_iter_loss: 0.35484611988067627
train_iter_loss: 0.292006254196167
train_iter_loss: 0.25703340768814087
train_iter_loss: 0.3152580261230469
train_iter_loss: 0.3440038561820984
train_iter_loss: 0.41285982728004456
train_iter_loss: 0.5534584522247314
train_iter_loss: 0.31853756308555603
train_iter_loss: 0.22925738990306854
train_iter_loss: 0.29960110783576965
train_iter_loss: 0.37642785906791687
train_iter_loss: 0.3271508812904358
train loss :0.3080
---------------------
Validation seg loss: 0.4072874216538555 at epoch 90
epoch =     91/  1000, exp = train
train_iter_loss: 0.40599414706230164
train_iter_loss: 0.2929898202419281
train_iter_loss: 0.2569728195667267
train_iter_loss: 0.39184921979904175
train_iter_loss: 0.2607490122318268
train_iter_loss: 0.19651570916175842
train_iter_loss: 0.3309570252895355
train_iter_loss: 0.41065269708633423
train_iter_loss: 0.3019067645072937
train_iter_loss: 0.21944652497768402
train_iter_loss: 0.2735432982444763
train_iter_loss: 0.3512687087059021
train_iter_loss: 0.27597445249557495
train_iter_loss: 0.24190856516361237
train_iter_loss: 0.3098874092102051
train_iter_loss: 0.4147169589996338
train_iter_loss: 0.3235727846622467
train_iter_loss: 0.24919947981834412
train_iter_loss: 0.6075426340103149
train_iter_loss: 0.21284273266792297
train_iter_loss: 0.23332402110099792
train_iter_loss: 0.48364418745040894
train_iter_loss: 0.3163232207298279
train_iter_loss: 0.2668103575706482
train_iter_loss: 0.25380074977874756
train_iter_loss: 0.22592544555664062
train_iter_loss: 0.29433169960975647
train_iter_loss: 0.3128988444805145
train_iter_loss: 0.3821960985660553
train_iter_loss: 0.32247665524482727
train_iter_loss: 0.15604718029499054
train_iter_loss: 0.34112027287483215
train_iter_loss: 0.5044320225715637
train_iter_loss: 0.43650585412979126
train_iter_loss: 0.20954236388206482
train_iter_loss: 0.2527104318141937
train_iter_loss: 0.40656226873397827
train_iter_loss: 0.29804709553718567
train_iter_loss: 0.2611074149608612
train_iter_loss: 0.3530426025390625
train_iter_loss: 0.40657973289489746
train_iter_loss: 0.34645798802375793
train_iter_loss: 0.39331820607185364
train_iter_loss: 0.18777956068515778
train_iter_loss: 0.24168822169303894
train_iter_loss: 0.22485414147377014
train_iter_loss: 0.12654919922351837
train_iter_loss: 0.476940393447876
train_iter_loss: 0.22472019493579865
train_iter_loss: 0.24526762962341309
train_iter_loss: 0.3798021674156189
train_iter_loss: 0.30173712968826294
train_iter_loss: 0.2380221039056778
train_iter_loss: 0.2152365893125534
train_iter_loss: 0.28559592366218567
train_iter_loss: 0.20911917090415955
train_iter_loss: 0.24093082547187805
train_iter_loss: 0.33186793327331543
train_iter_loss: 0.433952271938324
train_iter_loss: 0.2963072955608368
train_iter_loss: 0.319585919380188
train_iter_loss: 0.30051979422569275
train_iter_loss: 0.19679437577724457
train_iter_loss: 0.39065486192703247
train_iter_loss: 0.25757500529289246
train_iter_loss: 0.40307825803756714
train_iter_loss: 0.315807968378067
train_iter_loss: 0.23021429777145386
train_iter_loss: 0.266910195350647
train_iter_loss: 0.312375545501709
train_iter_loss: 0.35946154594421387
train_iter_loss: 0.20790262520313263
train_iter_loss: 0.375016987323761
train_iter_loss: 0.20561768114566803
train_iter_loss: 0.24218925833702087
train_iter_loss: 0.20806877315044403
train_iter_loss: 0.3379882574081421
train_iter_loss: 0.29966267943382263
train_iter_loss: 0.2932259440422058
train_iter_loss: 0.31414100527763367
train_iter_loss: 0.38761773705482483
train_iter_loss: 0.4511886537075043
train_iter_loss: 0.3820234537124634
train_iter_loss: 0.31505653262138367
train_iter_loss: 0.2528645396232605
train_iter_loss: 0.33478355407714844
train_iter_loss: 0.3019959330558777
train_iter_loss: 0.19423258304595947
train_iter_loss: 0.24647046625614166
train_iter_loss: 0.299137681722641
train_iter_loss: 0.2966555058956146
train_iter_loss: 0.42785266041755676
train_iter_loss: 0.29551035165786743
train_iter_loss: 0.19634570181369781
train_iter_loss: 0.4540136456489563
train_iter_loss: 0.2289515733718872
train_iter_loss: 0.31508731842041016
train_iter_loss: 0.38244909048080444
train_iter_loss: 0.32915207743644714
train_iter_loss: 0.2852950394153595
train loss :0.3097
---------------------
Validation seg loss: 0.37872367122052414 at epoch 91
epoch =     92/  1000, exp = train
train_iter_loss: 0.20647025108337402
train_iter_loss: 0.25900799036026
train_iter_loss: 0.28475984930992126
train_iter_loss: 0.28478318452835083
train_iter_loss: 0.22922489047050476
train_iter_loss: 0.24542784690856934
train_iter_loss: 0.387699156999588
train_iter_loss: 0.32244792580604553
train_iter_loss: 0.22184915840625763
train_iter_loss: 0.4414895176887512
train_iter_loss: 0.36270058155059814
train_iter_loss: 0.19916552305221558
train_iter_loss: 0.33536556363105774
train_iter_loss: 0.19151578843593597
train_iter_loss: 0.2151036560535431
train_iter_loss: 0.3156798779964447
train_iter_loss: 0.30528751015663147
train_iter_loss: 0.29459238052368164
train_iter_loss: 0.23448732495307922
train_iter_loss: 0.26370662450790405
train_iter_loss: 0.2393035590648651
train_iter_loss: 0.38813623785972595
train_iter_loss: 0.3575669527053833
train_iter_loss: 0.3827511668205261
train_iter_loss: 0.2704926133155823
train_iter_loss: 0.29272666573524475
train_iter_loss: 0.2958538234233856
train_iter_loss: 0.32874301075935364
train_iter_loss: 0.42584866285324097
train_iter_loss: 0.19328592717647552
train_iter_loss: 0.20650576055049896
train_iter_loss: 0.1442563682794571
train_iter_loss: 0.3086498975753784
train_iter_loss: 0.3889818489551544
train_iter_loss: 0.21234163641929626
train_iter_loss: 0.262350857257843
train_iter_loss: 0.3673584461212158
train_iter_loss: 0.30483198165893555
train_iter_loss: 0.24754497408866882
train_iter_loss: 0.2479836791753769
train_iter_loss: 0.5406784415245056
train_iter_loss: 0.3375057280063629
train_iter_loss: 0.5051783919334412
train_iter_loss: 0.22278833389282227
train_iter_loss: 0.3521699607372284
train_iter_loss: 0.43556565046310425
train_iter_loss: 0.21233448386192322
train_iter_loss: 0.3038795292377472
train_iter_loss: 0.3534867763519287
train_iter_loss: 0.3529094457626343
train_iter_loss: 0.4134916365146637
train_iter_loss: 0.5464944243431091
train_iter_loss: 0.20163072645664215
train_iter_loss: 0.18725672364234924
train_iter_loss: 0.20548443496227264
train_iter_loss: 0.2902606129646301
train_iter_loss: 0.32270947098731995
train_iter_loss: 0.2849220931529999
train_iter_loss: 0.4095127582550049
train_iter_loss: 0.41427290439605713
train_iter_loss: 0.33410030603408813
train_iter_loss: 0.3245961666107178
train_iter_loss: 0.4200483560562134
train_iter_loss: 0.270615816116333
train_iter_loss: 0.18215171992778778
train_iter_loss: 0.2768056392669678
train_iter_loss: 0.13072820007801056
train_iter_loss: 0.2524130344390869
train_iter_loss: 0.31177929043769836
train_iter_loss: 0.3208543658256531
train_iter_loss: 0.2679062485694885
train_iter_loss: 0.34122762084007263
train_iter_loss: 0.2940003573894501
train_iter_loss: 0.33696699142456055
train_iter_loss: 0.2533964216709137
train_iter_loss: 0.28026577830314636
train_iter_loss: 0.3940037786960602
train_iter_loss: 0.14068609476089478
train_iter_loss: 0.3433252274990082
train_iter_loss: 0.33878639340400696
train_iter_loss: 0.40641915798187256
train_iter_loss: 0.24597907066345215
train_iter_loss: 0.2852221727371216
train_iter_loss: 0.31618788838386536
train_iter_loss: 0.41095152497291565
train_iter_loss: 0.32819506525993347
train_iter_loss: 0.21952837705612183
train_iter_loss: 0.27918216586112976
train_iter_loss: 0.31002870202064514
train_iter_loss: 0.26428696513175964
train_iter_loss: 0.4355485439300537
train_iter_loss: 0.2382226288318634
train_iter_loss: 0.15379534661769867
train_iter_loss: 0.28659531474113464
train_iter_loss: 0.2467319369316101
train_iter_loss: 0.38025838136672974
train_iter_loss: 0.31042951345443726
train_iter_loss: 0.49983811378479004
train_iter_loss: 0.23722030222415924
train_iter_loss: 0.45103901624679565
train loss :0.3079
---------------------
Validation seg loss: 0.4155745696241282 at epoch 92
epoch =     93/  1000, exp = train
train_iter_loss: 0.4665301442146301
train_iter_loss: 0.2721671164035797
train_iter_loss: 0.33389654755592346
train_iter_loss: 0.19156402349472046
train_iter_loss: 0.26478204131126404
train_iter_loss: 0.19350692629814148
train_iter_loss: 0.205488920211792
train_iter_loss: 0.2700396180152893
train_iter_loss: 0.6371159553527832
train_iter_loss: 0.23845292627811432
train_iter_loss: 0.3322233259677887
train_iter_loss: 0.3114403486251831
train_iter_loss: 0.18323074281215668
train_iter_loss: 0.388530969619751
train_iter_loss: 0.26303499937057495
train_iter_loss: 0.1929907649755478
train_iter_loss: 0.3469870090484619
train_iter_loss: 0.277749240398407
train_iter_loss: 0.3730849325656891
train_iter_loss: 0.23226028680801392
train_iter_loss: 0.31959691643714905
train_iter_loss: 0.25788527727127075
train_iter_loss: 0.2884962558746338
train_iter_loss: 0.29945993423461914
train_iter_loss: 0.2502024471759796
train_iter_loss: 0.29373493790626526
train_iter_loss: 0.11406104266643524
train_iter_loss: 0.23281235992908478
train_iter_loss: 0.43551182746887207
train_iter_loss: 0.21647481620311737
train_iter_loss: 0.2806195914745331
train_iter_loss: 0.33166104555130005
train_iter_loss: 0.4222930669784546
train_iter_loss: 0.2519271671772003
train_iter_loss: 0.3466610908508301
train_iter_loss: 0.36668139696121216
train_iter_loss: 0.2748125195503235
train_iter_loss: 0.22217778861522675
train_iter_loss: 0.23680391907691956
train_iter_loss: 0.22568997740745544
train_iter_loss: 0.2420823574066162
train_iter_loss: 0.41684725880622864
train_iter_loss: 0.4801592528820038
train_iter_loss: 0.33998602628707886
train_iter_loss: 0.29808101058006287
train_iter_loss: 0.38435304164886475
train_iter_loss: 0.4461005926132202
train_iter_loss: 0.400149941444397
train_iter_loss: 0.31166326999664307
train_iter_loss: 0.4166904091835022
train_iter_loss: 0.3360069990158081
train_iter_loss: 0.20035845041275024
train_iter_loss: 0.34036916494369507
train_iter_loss: 0.22638002038002014
train_iter_loss: 0.3693898916244507
train_iter_loss: 0.2936781346797943
train_iter_loss: 0.14497080445289612
train_iter_loss: 0.14917440712451935
train_iter_loss: 0.37800005078315735
train_iter_loss: 0.36387139558792114
train_iter_loss: 0.27220436930656433
train_iter_loss: 0.3897749185562134
train_iter_loss: 0.33672916889190674
train_iter_loss: 0.25643330812454224
train_iter_loss: 0.2983624041080475
train_iter_loss: 0.31789305806159973
train_iter_loss: 0.2966477870941162
train_iter_loss: 0.24253647029399872
train_iter_loss: 0.3424919545650482
train_iter_loss: 0.3571608066558838
train_iter_loss: 0.3097662031650543
train_iter_loss: 0.32564324140548706
train_iter_loss: 0.34304094314575195
train_iter_loss: 0.34392842650413513
train_iter_loss: 0.4076547622680664
train_iter_loss: 0.39014732837677
train_iter_loss: 0.23355548083782196
train_iter_loss: 0.39775148034095764
train_iter_loss: 0.2101120799779892
train_iter_loss: 0.2368653565645218
train_iter_loss: 0.2707151472568512
train_iter_loss: 0.38382840156555176
train_iter_loss: 0.24406540393829346
train_iter_loss: 0.37933942675590515
train_iter_loss: 0.27661609649658203
train_iter_loss: 0.08388227224349976
train_iter_loss: 0.1962912678718567
train_iter_loss: 0.21728511154651642
train_iter_loss: 0.31480321288108826
train_iter_loss: 0.25583717226982117
train_iter_loss: 0.22280630469322205
train_iter_loss: 0.25180819630622864
train_iter_loss: 0.21377287805080414
train_iter_loss: 0.3960113525390625
train_iter_loss: 0.2557651996612549
train_iter_loss: 0.1944107711315155
train_iter_loss: 0.45702728629112244
train_iter_loss: 0.29064151644706726
train_iter_loss: 0.4070894718170166
train_iter_loss: 0.13132333755493164
train loss :0.3024
---------------------
Validation seg loss: 0.39026407496827953 at epoch 93
epoch =     94/  1000, exp = train
train_iter_loss: 0.2708454430103302
train_iter_loss: 0.24688728153705597
train_iter_loss: 0.16769473254680634
train_iter_loss: 0.27917540073394775
train_iter_loss: 0.46088650822639465
train_iter_loss: 0.31908121705055237
train_iter_loss: 0.37557119131088257
train_iter_loss: 0.23489181697368622
train_iter_loss: 0.28206345438957214
train_iter_loss: 0.44531548023223877
train_iter_loss: 0.2734972834587097
train_iter_loss: 0.31628701090812683
train_iter_loss: 0.2960367500782013
train_iter_loss: 0.2831137478351593
train_iter_loss: 0.3580683767795563
train_iter_loss: 0.526730477809906
train_iter_loss: 0.1903170794248581
train_iter_loss: 0.17111901938915253
train_iter_loss: 0.26235339045524597
train_iter_loss: 0.14551949501037598
train_iter_loss: 0.26356732845306396
train_iter_loss: 0.34896257519721985
train_iter_loss: 0.3224250078201294
train_iter_loss: 0.2688795030117035
train_iter_loss: 0.35395893454551697
train_iter_loss: 0.21447260677814484
train_iter_loss: 0.2718467712402344
train_iter_loss: 0.5913281440734863
train_iter_loss: 0.29107457399368286
train_iter_loss: 0.3403562903404236
train_iter_loss: 0.3966035842895508
train_iter_loss: 0.42687830328941345
train_iter_loss: 0.1837468147277832
train_iter_loss: 0.3631751835346222
train_iter_loss: 0.28179800510406494
train_iter_loss: 0.34727829694747925
train_iter_loss: 0.40290477871894836
train_iter_loss: 0.24036647379398346
train_iter_loss: 0.3088027238845825
train_iter_loss: 0.2977217435836792
train_iter_loss: 0.35843440890312195
train_iter_loss: 0.3116142749786377
train_iter_loss: 0.8226993083953857
train_iter_loss: 0.2844023108482361
train_iter_loss: 0.31311872601509094
train_iter_loss: 0.3395947813987732
train_iter_loss: 0.17151537537574768
train_iter_loss: 0.24655668437480927
train_iter_loss: 0.21056507527828217
train_iter_loss: 0.2578040659427643
train_iter_loss: 0.33901381492614746
train_iter_loss: 0.3472013473510742
train_iter_loss: 0.3438080847263336
train_iter_loss: 0.5600804090499878
train_iter_loss: 0.3455856740474701
train_iter_loss: 0.27564504742622375
train_iter_loss: 0.2966463565826416
train_iter_loss: 0.28319457173347473
train_iter_loss: 0.3233501613140106
train_iter_loss: 0.2680553197860718
train_iter_loss: 0.20327425003051758
train_iter_loss: 0.19491001963615417
train_iter_loss: 0.36462724208831787
train_iter_loss: 0.36685290932655334
train_iter_loss: 0.3147988021373749
train_iter_loss: 0.1848772168159485
train_iter_loss: 0.18670067191123962
train_iter_loss: 0.3851207196712494
train_iter_loss: 0.2684491276741028
train_iter_loss: 0.2702791392803192
train_iter_loss: 0.38009175658226013
train_iter_loss: 0.261792927980423
train_iter_loss: 0.18799668550491333
train_iter_loss: 0.28118446469306946
train_iter_loss: 0.324114590883255
train_iter_loss: 0.5193661451339722
train_iter_loss: 0.13884377479553223
train_iter_loss: 0.38955482840538025
train_iter_loss: 0.34806445240974426
train_iter_loss: 0.3578356206417084
train_iter_loss: 0.41699594259262085
train_iter_loss: 0.2935422956943512
train_iter_loss: 0.30589720606803894
train_iter_loss: 0.30501413345336914
train_iter_loss: 0.24072007834911346
train_iter_loss: 0.2810330390930176
train_iter_loss: 0.3436894118785858
train_iter_loss: 0.23317305743694305
train_iter_loss: 0.12901288270950317
train_iter_loss: 0.2148267775774002
train_iter_loss: 0.2004852294921875
train_iter_loss: 0.384669691324234
train_iter_loss: 0.26857665181159973
train_iter_loss: 0.281232625246048
train_iter_loss: 0.29779836535453796
train_iter_loss: 0.18934836983680725
train_iter_loss: 0.2322433590888977
train_iter_loss: 0.31061312556266785
train_iter_loss: 0.4433272182941437
train_iter_loss: 0.25417906045913696
train loss :0.3098
---------------------
Validation seg loss: 0.39534431124844077 at epoch 94
epoch =     95/  1000, exp = train
train_iter_loss: 0.23213733732700348
train_iter_loss: 0.21671271324157715
train_iter_loss: 0.44221988320350647
train_iter_loss: 0.3663959503173828
train_iter_loss: 0.19939589500427246
train_iter_loss: 0.28540000319480896
train_iter_loss: 0.5535933375358582
train_iter_loss: 0.30868926644325256
train_iter_loss: 0.2943040728569031
train_iter_loss: 0.19581055641174316
train_iter_loss: 0.2338777482509613
train_iter_loss: 0.2900792062282562
train_iter_loss: 0.23575817048549652
train_iter_loss: 0.2801516354084015
train_iter_loss: 0.2903698682785034
train_iter_loss: 0.27210375666618347
train_iter_loss: 0.2722916007041931
train_iter_loss: 0.3048303425312042
train_iter_loss: 0.2998805344104767
train_iter_loss: 0.46022406220436096
train_iter_loss: 0.3996194005012512
train_iter_loss: 0.32567867636680603
train_iter_loss: 0.36383941769599915
train_iter_loss: 0.26667749881744385
train_iter_loss: 0.3495071232318878
train_iter_loss: 0.24078333377838135
train_iter_loss: 0.2727603614330292
train_iter_loss: 0.3335135281085968
train_iter_loss: 0.16995450854301453
train_iter_loss: 0.4281342327594757
train_iter_loss: 0.23600530624389648
train_iter_loss: 0.33269351720809937
train_iter_loss: 0.3492857813835144
train_iter_loss: 0.6184311509132385
train_iter_loss: 0.27946192026138306
train_iter_loss: 0.2646593451499939
train_iter_loss: 0.2657381296157837
train_iter_loss: 0.22289401292800903
train_iter_loss: 0.15472838282585144
train_iter_loss: 0.27507513761520386
train_iter_loss: 0.36838045716285706
train_iter_loss: 0.352556973695755
train_iter_loss: 0.3414416015148163
train_iter_loss: 0.35019463300704956
train_iter_loss: 0.23361606895923615
train_iter_loss: 0.2800571918487549
train_iter_loss: 0.40040791034698486
train_iter_loss: 0.24206730723381042
train_iter_loss: 0.3610536754131317
train_iter_loss: 0.34043198823928833
train_iter_loss: 0.323579341173172
train_iter_loss: 0.2475041300058365
train_iter_loss: 0.2965201437473297
train_iter_loss: 0.17475396394729614
train_iter_loss: 0.2854366600513458
train_iter_loss: 0.39147287607192993
train_iter_loss: 0.15395855903625488
train_iter_loss: 0.24312353134155273
train_iter_loss: 0.2614857256412506
train_iter_loss: 0.29253441095352173
train_iter_loss: 0.18599803745746613
train_iter_loss: 0.21747028827667236
train_iter_loss: 0.45363327860832214
train_iter_loss: 0.35870611667633057
train_iter_loss: 0.14821010828018188
train_iter_loss: 0.42219480872154236
train_iter_loss: 0.2504333555698395
train_iter_loss: 0.42834803462028503
train_iter_loss: 0.29844069480895996
train_iter_loss: 0.43023011088371277
train_iter_loss: 0.3015606105327606
train_iter_loss: 0.23912638425827026
train_iter_loss: 0.28155291080474854
train_iter_loss: 0.30733442306518555
train_iter_loss: 0.24372494220733643
train_iter_loss: 0.16145135462284088
train_iter_loss: 0.3120979070663452
train_iter_loss: 0.25376003980636597
train_iter_loss: 0.25476041436195374
train_iter_loss: 0.283611536026001
train_iter_loss: 0.25543534755706787
train_iter_loss: 0.26712116599082947
train_iter_loss: 0.22927093505859375
train_iter_loss: 0.34146493673324585
train_iter_loss: 0.3831212818622589
train_iter_loss: 0.21627792716026306
train_iter_loss: 0.3259907364845276
train_iter_loss: 0.264055997133255
train_iter_loss: 0.3487294912338257
train_iter_loss: 0.5186828374862671
train_iter_loss: 0.26293542981147766
train_iter_loss: 0.4333915412425995
train_iter_loss: 0.2667725384235382
train_iter_loss: 0.23279957473278046
train_iter_loss: 0.22675305604934692
train_iter_loss: 0.2159736007452011
train_iter_loss: 0.22434495389461517
train_iter_loss: 0.31471338868141174
train_iter_loss: 0.3605484366416931
train_iter_loss: 0.34834760427474976
train loss :0.3030
---------------------
Validation seg loss: 0.38049587723359746 at epoch 95
epoch =     96/  1000, exp = train
train_iter_loss: 0.24351447820663452
train_iter_loss: 0.21831972897052765
train_iter_loss: 0.2921581566333771
train_iter_loss: 0.28837764263153076
train_iter_loss: 0.522824764251709
train_iter_loss: 0.2868559956550598
train_iter_loss: 0.308186411857605
train_iter_loss: 0.3169138431549072
train_iter_loss: 0.3163207173347473
train_iter_loss: 0.31390145421028137
train_iter_loss: 0.1815342754125595
train_iter_loss: 0.3075384497642517
train_iter_loss: 0.2710379362106323
train_iter_loss: 0.33016496896743774
train_iter_loss: 0.27485817670822144
train_iter_loss: 0.2904362082481384
train_iter_loss: 0.33044612407684326
train_iter_loss: 0.4817376136779785
train_iter_loss: 0.35084268450737
train_iter_loss: 0.33499643206596375
train_iter_loss: 0.24793140590190887
train_iter_loss: 0.35491618514060974
train_iter_loss: 0.29564881324768066
train_iter_loss: 0.435298353433609
train_iter_loss: 0.2417943924665451
train_iter_loss: 0.2752571403980255
train_iter_loss: 0.3294133245944977
train_iter_loss: 0.3846743404865265
train_iter_loss: 0.23704901337623596
train_iter_loss: 0.34259018301963806
train_iter_loss: 0.4247901439666748
train_iter_loss: 0.3199121654033661
train_iter_loss: 0.19865435361862183
train_iter_loss: 0.4624546766281128
train_iter_loss: 0.446817010641098
train_iter_loss: 0.2964267432689667
train_iter_loss: 0.3309975862503052
train_iter_loss: 0.35873648524284363
train_iter_loss: 0.33423468470573425
train_iter_loss: 0.29654431343078613
train_iter_loss: 0.30661582946777344
train_iter_loss: 0.2792244851589203
train_iter_loss: 0.19025683403015137
train_iter_loss: 0.2282460480928421
train_iter_loss: 0.359542578458786
train_iter_loss: 0.23306699097156525
train_iter_loss: 0.24176809191703796
train_iter_loss: 0.33742213249206543
train_iter_loss: 0.38277631998062134
train_iter_loss: 0.17285799980163574
train_iter_loss: 0.2794284224510193
train_iter_loss: 0.40923526883125305
train_iter_loss: 0.23146642744541168
train_iter_loss: 0.29710689187049866
train_iter_loss: 0.32244935631752014
train_iter_loss: 0.2660747766494751
train_iter_loss: 0.28419697284698486
train_iter_loss: 0.2972520887851715
train_iter_loss: 0.3346364200115204
train_iter_loss: 0.3332481384277344
train_iter_loss: 0.33973875641822815
train_iter_loss: 0.3572418987751007
train_iter_loss: 0.3621896803379059
train_iter_loss: 0.41224512457847595
train_iter_loss: 0.2757141590118408
train_iter_loss: 0.28680679202079773
train_iter_loss: 0.27044031023979187
train_iter_loss: 0.42018476128578186
train_iter_loss: 0.2618030309677124
train_iter_loss: 0.3483077585697174
train_iter_loss: 0.17609800398349762
train_iter_loss: 0.17170262336730957
train_iter_loss: 0.1434536874294281
train_iter_loss: 0.48311904072761536
train_iter_loss: 0.3624523878097534
train_iter_loss: 0.23193778097629547
train_iter_loss: 0.22995220124721527
train_iter_loss: 0.313127338886261
train_iter_loss: 0.08264651894569397
train_iter_loss: 0.3302188515663147
train_iter_loss: 0.36023616790771484
train_iter_loss: 0.5119858980178833
train_iter_loss: 0.4228617548942566
train_iter_loss: 0.20499779284000397
train_iter_loss: 0.29889604449272156
train_iter_loss: 0.38144707679748535
train_iter_loss: 0.30994823575019836
train_iter_loss: 0.42637747526168823
train_iter_loss: 0.236830934882164
train_iter_loss: 0.35593992471694946
train_iter_loss: 0.2326076775789261
train_iter_loss: 0.24331356585025787
train_iter_loss: 0.46978092193603516
train_iter_loss: 0.2657183110713959
train_iter_loss: 0.24562014639377594
train_iter_loss: 0.4627245366573334
train_iter_loss: 0.38677090406417847
train_iter_loss: 0.2741588056087494
train_iter_loss: 0.36990711092948914
train_iter_loss: 0.46840623021125793
train loss :0.3175
---------------------
Validation seg loss: 0.4052379894825929 at epoch 96
epoch =     97/  1000, exp = train
train_iter_loss: 0.3427184224128723
train_iter_loss: 0.2854953408241272
train_iter_loss: 0.3993435800075531
train_iter_loss: 0.2716626822948456
train_iter_loss: 0.29461807012557983
train_iter_loss: 0.3190547823905945
train_iter_loss: 0.277579128742218
train_iter_loss: 0.2115502804517746
train_iter_loss: 0.2945461869239807
train_iter_loss: 0.16371317207813263
train_iter_loss: 0.33078500628471375
train_iter_loss: 0.3398563265800476
train_iter_loss: 0.2728275954723358
train_iter_loss: 0.13149386644363403
train_iter_loss: 0.21674570441246033
train_iter_loss: 0.39180150628089905
train_iter_loss: 0.31292054057121277
train_iter_loss: 0.23950715363025665
train_iter_loss: 0.19186629354953766
train_iter_loss: 0.43315428495407104
train_iter_loss: 0.21055196225643158
train_iter_loss: 0.41540026664733887
train_iter_loss: 0.34086886048316956
train_iter_loss: 0.39207082986831665
train_iter_loss: 0.2042730450630188
train_iter_loss: 0.32807421684265137
train_iter_loss: 0.2319031059741974
train_iter_loss: 0.20882965624332428
train_iter_loss: 0.26390212774276733
train_iter_loss: 0.26864907145500183
train_iter_loss: 0.3178110122680664
train_iter_loss: 0.3101525902748108
train_iter_loss: 0.28419029712677
train_iter_loss: 0.31881701946258545
train_iter_loss: 0.23153336346149445
train_iter_loss: 0.3904629647731781
train_iter_loss: 0.3927483856678009
train_iter_loss: 0.29329368472099304
train_iter_loss: 0.21781449019908905
train_iter_loss: 0.24167589843273163
train_iter_loss: 0.3036191463470459
train_iter_loss: 0.19684761762619019
train_iter_loss: 0.3519992530345917
train_iter_loss: 0.24396754801273346
train_iter_loss: 0.2319059669971466
train_iter_loss: 0.551278829574585
train_iter_loss: 0.28599053621292114
train_iter_loss: 0.44058704376220703
train_iter_loss: 0.3948674201965332
train_iter_loss: 0.3271579444408417
train_iter_loss: 0.3857370615005493
train_iter_loss: 0.3457932770252228
train_iter_loss: 0.45790982246398926
train_iter_loss: 0.2207604944705963
train_iter_loss: 0.43612390756607056
train_iter_loss: 0.4666541516780853
train_iter_loss: 0.3679797649383545
train_iter_loss: 0.3152555525302887
train_iter_loss: 0.5371776223182678
train_iter_loss: 0.1912904530763626
train_iter_loss: 0.2819409668445587
train_iter_loss: 0.31827712059020996
train_iter_loss: 0.3252500593662262
train_iter_loss: 0.3183320164680481
train_iter_loss: 0.2983340620994568
train_iter_loss: 0.22459377348423004
train_iter_loss: 0.2486027032136917
train_iter_loss: 0.2512222230434418
train_iter_loss: 0.26000720262527466
train_iter_loss: 0.24060845375061035
train_iter_loss: 0.4582992494106293
train_iter_loss: 0.20050305128097534
train_iter_loss: 0.4806273579597473
train_iter_loss: 0.3228348195552826
train_iter_loss: 0.24173375964164734
train_iter_loss: 0.3003484308719635
train_iter_loss: 0.2058267444372177
train_iter_loss: 0.3236972391605377
train_iter_loss: 0.16446328163146973
train_iter_loss: 0.11180642247200012
train_iter_loss: 0.3217492401599884
train_iter_loss: 0.23833110928535461
train_iter_loss: 0.4398289918899536
train_iter_loss: 0.41396382451057434
train_iter_loss: 0.5029941201210022
train_iter_loss: 0.21521048247814178
train_iter_loss: 0.3076527714729309
train_iter_loss: 0.23721598088741302
train_iter_loss: 0.31286296248435974
train_iter_loss: 0.5352458357810974
train_iter_loss: 0.2839772403240204
train_iter_loss: 0.34630146622657776
train_iter_loss: 0.2821061909198761
train_iter_loss: 0.33871757984161377
train_iter_loss: 0.2386651337146759
train_iter_loss: 0.37013036012649536
train_iter_loss: 0.31942200660705566
train_iter_loss: 0.40753671526908875
train_iter_loss: 0.38068121671676636
train_iter_loss: 0.2472393959760666
train loss :0.3127
---------------------
Validation seg loss: 0.3954593698617141 at epoch 97
epoch =     98/  1000, exp = train
train_iter_loss: 0.2162952721118927
train_iter_loss: 0.29514360427856445
train_iter_loss: 0.3164190351963043
train_iter_loss: 0.17557431757450104
train_iter_loss: 0.36903899908065796
train_iter_loss: 0.30035823583602905
train_iter_loss: 0.24070799350738525
train_iter_loss: 0.18367327749729156
train_iter_loss: 0.24360081553459167
train_iter_loss: 0.3741770386695862
train_iter_loss: 0.3842669427394867
train_iter_loss: 0.5422751307487488
train_iter_loss: 0.38500043749809265
train_iter_loss: 0.37447547912597656
train_iter_loss: 0.3344675302505493
train_iter_loss: 0.29650571942329407
train_iter_loss: 0.3382984697818756
train_iter_loss: 0.27224934101104736
train_iter_loss: 0.45775559544563293
train_iter_loss: 0.27672192454338074
train_iter_loss: 0.33793923258781433
train_iter_loss: 0.23627781867980957
train_iter_loss: 0.32228609919548035
train_iter_loss: 0.36625435948371887
train_iter_loss: 0.3001019358634949
train_iter_loss: 0.17846553027629852
train_iter_loss: 0.31330931186676025
train_iter_loss: 0.44486144185066223
train_iter_loss: 0.20056696236133575
train_iter_loss: 0.3979020118713379
train_iter_loss: 0.2495865672826767
train_iter_loss: 0.428871750831604
train_iter_loss: 0.3538559675216675
train_iter_loss: 0.3474990427494049
train_iter_loss: 0.12386591732501984
train_iter_loss: 0.24384558200836182
train_iter_loss: 0.5271640419960022
train_iter_loss: 0.3900739550590515
train_iter_loss: 0.35781145095825195
train_iter_loss: 0.2534548044204712
train_iter_loss: 0.16793060302734375
train_iter_loss: 0.5526710152626038
train_iter_loss: 0.3986360430717468
train_iter_loss: 0.34558406472206116
train_iter_loss: 0.3266412317752838
train_iter_loss: 0.2773067355155945
train_iter_loss: 0.24966667592525482
train_iter_loss: 0.3457581400871277
train_iter_loss: 0.3720453679561615
train_iter_loss: 0.30516529083251953
train_iter_loss: 0.19969794154167175
train_iter_loss: 0.30231067538261414
train_iter_loss: 0.24978843331336975
train_iter_loss: 0.2745218575000763
train_iter_loss: 0.40463802218437195
train_iter_loss: 0.2783244252204895
train_iter_loss: 0.3022148609161377
train_iter_loss: 0.39313557744026184
train_iter_loss: 0.2909967601299286
train_iter_loss: 0.32180291414260864
train_iter_loss: 0.26809483766555786
train_iter_loss: 0.21851348876953125
train_iter_loss: 0.30352604389190674
train_iter_loss: 0.43412473797798157
train_iter_loss: 0.35262107849121094
train_iter_loss: 0.15892308950424194
train_iter_loss: 0.145346537232399
train_iter_loss: 0.20385698974132538
train_iter_loss: 0.22626256942749023
train_iter_loss: 0.15325602889060974
train_iter_loss: 0.3095928132534027
train_iter_loss: 0.30047884583473206
train_iter_loss: 0.23803669214248657
train_iter_loss: 0.32114607095718384
train_iter_loss: 0.25912535190582275
train_iter_loss: 0.3653913140296936
train_iter_loss: 0.23315420746803284
train_iter_loss: 0.1807723045349121
train_iter_loss: 0.3173834979534149
train_iter_loss: 0.3808460235595703
train_iter_loss: 0.514107882976532
train_iter_loss: 0.25762462615966797
train_iter_loss: 0.3072095811367035
train_iter_loss: 0.20358362793922424
train_iter_loss: 0.35675492882728577
train_iter_loss: 0.30597400665283203
train_iter_loss: 0.3032982051372528
train_iter_loss: 0.3820265233516693
train_iter_loss: 0.2944582402706146
train_iter_loss: 0.3792527914047241
train_iter_loss: 0.22537370026111603
train_iter_loss: 0.531728208065033
train_iter_loss: 0.24106080830097198
train_iter_loss: 0.25610989332199097
train_iter_loss: 0.2534092962741852
train_iter_loss: 0.21770915389060974
train_iter_loss: 0.3100229501724243
train_iter_loss: 0.2076040357351303
train_iter_loss: 0.28085845708847046
train_iter_loss: 0.2865222692489624
train loss :0.3091
---------------------
Validation seg loss: 0.3718546687994363 at epoch 98
********************
best_val_epoch_loss:  0.3718546687994363
MODEL UPDATED
epoch =     99/  1000, exp = train
train_iter_loss: 0.32663217186927795
train_iter_loss: 0.4479687809944153
train_iter_loss: 0.3436557352542877
train_iter_loss: 0.19730614125728607
train_iter_loss: 0.32626447081565857
train_iter_loss: 0.2615450322628021
train_iter_loss: 0.2457572966814041
train_iter_loss: 0.2445117086172104
train_iter_loss: 0.28442618250846863
train_iter_loss: 0.35715264081954956
train_iter_loss: 0.21710725128650665
train_iter_loss: 0.22102724015712738
train_iter_loss: 0.37375277280807495
train_iter_loss: 0.2645733654499054
train_iter_loss: 0.25968873500823975
train_iter_loss: 0.33545443415641785
train_iter_loss: 0.21097415685653687
train_iter_loss: 0.3195893466472626
train_iter_loss: 0.33457866311073303
train_iter_loss: 0.19002173840999603
train_iter_loss: 0.291140079498291
train_iter_loss: 0.22881080210208893
train_iter_loss: 0.17622524499893188
train_iter_loss: 0.1848573535680771
train_iter_loss: 0.41438066959381104
train_iter_loss: 0.34587305784225464
train_iter_loss: 0.32956716418266296
train_iter_loss: 0.1851121336221695
train_iter_loss: 0.24908216297626495
train_iter_loss: 0.2934185564517975
train_iter_loss: 0.3166452944278717
train_iter_loss: 0.30113476514816284
train_iter_loss: 0.3783864378929138
train_iter_loss: 0.2255273014307022
train_iter_loss: 0.400702565908432
train_iter_loss: 0.3884280025959015
train_iter_loss: 0.25516319274902344
train_iter_loss: 0.3124542236328125
train_iter_loss: 0.29692140221595764
train_iter_loss: 0.32242250442504883
train_iter_loss: 0.45319852232933044
train_iter_loss: 0.3362705707550049
train_iter_loss: 0.31654709577560425
train_iter_loss: 0.24684102833271027
train_iter_loss: 0.3459308445453644
train_iter_loss: 0.295187771320343
train_iter_loss: 0.2871902287006378
train_iter_loss: 0.27407634258270264
train_iter_loss: 0.32488563656806946
train_iter_loss: 0.3818812072277069
train_iter_loss: 0.31534913182258606
train_iter_loss: 0.42440226674079895
train_iter_loss: 0.26006829738616943
train_iter_loss: 0.3789418041706085
train_iter_loss: 0.5095567107200623
train_iter_loss: 0.16429878771305084
train_iter_loss: 0.26164889335632324
train_iter_loss: 0.3100438416004181
train_iter_loss: 0.23498089611530304
train_iter_loss: 0.19769135117530823
train_iter_loss: 0.2682167887687683
train_iter_loss: 0.19605238735675812
train_iter_loss: 0.3773539364337921
train_iter_loss: 0.2852450907230377
train_iter_loss: 0.24553033709526062
train_iter_loss: 0.26468542218208313
train_iter_loss: 0.47615769505500793
train_iter_loss: 0.19731943309307098
train_iter_loss: 0.31444472074508667
train_iter_loss: 0.3307914435863495
train_iter_loss: 0.28223758935928345
train_iter_loss: 0.29208675026893616
train_iter_loss: 0.33062583208084106
train_iter_loss: 0.2971322238445282
train_iter_loss: 0.3587292730808258
train_iter_loss: 0.24130989611148834
train_iter_loss: 0.24799731373786926
train_iter_loss: 0.35341429710388184
train_iter_loss: 0.3531242609024048
train_iter_loss: 0.21383266150951385
train_iter_loss: 0.23978573083877563
train_iter_loss: 0.39012280106544495
train_iter_loss: 0.2854504883289337
train_iter_loss: 0.354573518037796
train_iter_loss: 0.289396733045578
train_iter_loss: 0.20100006461143494
train_iter_loss: 0.3640183210372925
train_iter_loss: 0.36813488602638245
train_iter_loss: 0.3901321589946747
train_iter_loss: 0.31072232127189636
train_iter_loss: 0.3393555283546448
train_iter_loss: 0.30726200342178345
train_iter_loss: 0.26265984773635864
train_iter_loss: 0.3195347189903259
train_iter_loss: 0.26390910148620605
train_iter_loss: 0.24528881907463074
train_iter_loss: 0.36202535033226013
train_iter_loss: 0.22097499668598175
train_iter_loss: 0.3360022008419037
train_iter_loss: 0.2955462634563446
train loss :0.3036
---------------------
Validation seg loss: 0.4055566839030329 at epoch 99
epoch =    100/  1000, exp = train
train_iter_loss: 0.30280211567878723
train_iter_loss: 0.5597431063652039
train_iter_loss: 0.3809332251548767
train_iter_loss: 0.34102287888526917
train_iter_loss: 0.25003471970558167
train_iter_loss: 0.14569446444511414
train_iter_loss: 0.26554471254348755
train_iter_loss: 0.20033802092075348
train_iter_loss: 0.3315761089324951
train_iter_loss: 0.3644147217273712
train_iter_loss: 0.25639933347702026
train_iter_loss: 0.2695493698120117
train_iter_loss: 0.15854857861995697
train_iter_loss: 0.3746431767940521
train_iter_loss: 0.31314751505851746
train_iter_loss: 0.27556759119033813
train_iter_loss: 0.2756519317626953
train_iter_loss: 0.29298123717308044
train_iter_loss: 0.2677369713783264
train_iter_loss: 0.17951607704162598
train_iter_loss: 0.1812811642885208
train_iter_loss: 0.3193487524986267
train_iter_loss: 0.33979880809783936
train_iter_loss: 0.3193722069263458
train_iter_loss: 0.30803635716438293
train_iter_loss: 0.16692456603050232
train_iter_loss: 0.2357507050037384
train_iter_loss: 0.33235469460487366
train_iter_loss: 0.17348092794418335
train_iter_loss: 0.26467758417129517
train_iter_loss: 0.45758354663848877
train_iter_loss: 0.1903245896100998
train_iter_loss: 0.3126482367515564
train_iter_loss: 0.19713687896728516
train_iter_loss: 0.3764745593070984
train_iter_loss: 0.24063614010810852
train_iter_loss: 0.34929853677749634
train_iter_loss: 0.2729754149913788
train_iter_loss: 0.2749789357185364
train_iter_loss: 0.3105868697166443
train_iter_loss: 0.34901028871536255
train_iter_loss: 0.44406741857528687
train_iter_loss: 0.24734269082546234
train_iter_loss: 0.22929683327674866
train_iter_loss: 0.37510964274406433
train_iter_loss: 0.26506078243255615
train_iter_loss: 0.28380224108695984
train_iter_loss: 0.5204178094863892
train_iter_loss: 0.1914454698562622
train_iter_loss: 0.3106594681739807
train_iter_loss: 0.48234716057777405
train_iter_loss: 0.279761403799057
train_iter_loss: 0.23550570011138916
train_iter_loss: 0.3684864044189453
train_iter_loss: 0.27437642216682434
train_iter_loss: 0.24109524488449097
train_iter_loss: 0.30535462498664856
train_iter_loss: 0.5216836333274841
train_iter_loss: 0.45171022415161133
train_iter_loss: 0.2783123850822449
train_iter_loss: 0.30324801802635193
train_iter_loss: 0.21318088471889496
train_iter_loss: 0.3089422285556793
train_iter_loss: 0.3562979996204376
train_iter_loss: 0.38658735156059265
train_iter_loss: 0.3258427381515503
train_iter_loss: 0.2801761329174042
train_iter_loss: 0.2927011251449585
train_iter_loss: 0.3124915361404419
train_iter_loss: 0.3570932447910309
train_iter_loss: 0.34633323550224304
train_iter_loss: 0.15353579819202423
train_iter_loss: 0.3289867341518402
train_iter_loss: 0.28039753437042236
train_iter_loss: 0.30745944380760193
train_iter_loss: 0.5098373889923096
train_iter_loss: 0.3365832567214966
train_iter_loss: 0.2928016483783722
train_iter_loss: 0.2543488144874573
train_iter_loss: 0.20776784420013428
train_iter_loss: 0.2175212800502777
train_iter_loss: 0.23931264877319336
train_iter_loss: 0.3819802701473236
train_iter_loss: 0.24822582304477692
train_iter_loss: 0.439978688955307
train_iter_loss: 0.23199830949306488
train_iter_loss: 0.3557130694389343
train_iter_loss: 0.3567030429840088
train_iter_loss: 0.2357681691646576
train_iter_loss: 0.29336389899253845
train_iter_loss: 0.1842377781867981
train_iter_loss: 0.22689774632453918
train_iter_loss: 0.3659675419330597
train_iter_loss: 0.35551631450653076
train_iter_loss: 0.2113301306962967
train_iter_loss: 0.15596605837345123
train_iter_loss: 0.2714325785636902
train_iter_loss: 0.2912920415401459
train_iter_loss: 0.3905840218067169
train_iter_loss: 0.46373870968818665
train loss :0.3046
---------------------
Validation seg loss: 0.3953598252901772 at epoch 100
epoch =    101/  1000, exp = train
train_iter_loss: 0.34028327465057373
train_iter_loss: 0.6016125679016113
train_iter_loss: 0.24069412052631378
train_iter_loss: 0.36352473497390747
train_iter_loss: 0.4073489010334015
train_iter_loss: 0.2891463041305542
train_iter_loss: 0.2125510722398758
train_iter_loss: 0.2069263756275177
train_iter_loss: 0.3476906418800354
train_iter_loss: 0.48538655042648315
train_iter_loss: 0.4102359116077423
train_iter_loss: 0.36240145564079285
train_iter_loss: 0.2418355494737625
train_iter_loss: 0.29288896918296814
train_iter_loss: 0.23653863370418549
train_iter_loss: 0.14243240654468536
train_iter_loss: 0.1229969710111618
train_iter_loss: 0.32030659914016724
train_iter_loss: 0.31360283493995667
train_iter_loss: 0.35490450263023376
train_iter_loss: 0.3970172703266144
train_iter_loss: 0.27118873596191406
train_iter_loss: 0.4119161069393158
train_iter_loss: 0.38640791177749634
train_iter_loss: 0.24709543585777283
train_iter_loss: 0.13170838356018066
train_iter_loss: 0.4129619896411896
train_iter_loss: 0.30313587188720703
train_iter_loss: 0.3909619450569153
train_iter_loss: 0.2110053449869156
train_iter_loss: 0.20847411453723907
train_iter_loss: 0.3060702979564667
train_iter_loss: 0.19999758899211884
train_iter_loss: 0.4183284640312195
train_iter_loss: 0.243270605802536
train_iter_loss: 0.27096837759017944
train_iter_loss: 0.3496967852115631
train_iter_loss: 0.3483212888240814
train_iter_loss: 0.25559186935424805
train_iter_loss: 0.3902326226234436
train_iter_loss: 0.3331585228443146
train_iter_loss: 0.33031758666038513
train_iter_loss: 0.22331948578357697
train_iter_loss: 0.3089071810245514
train_iter_loss: 0.23851345479488373
train_iter_loss: 0.22954922914505005
train_iter_loss: 0.27156510949134827
train_iter_loss: 0.335550457239151
train_iter_loss: 0.27226337790489197
train_iter_loss: 0.22349053621292114
train_iter_loss: 0.2470441609621048
train_iter_loss: 0.298801988363266
train_iter_loss: 0.23471416532993317
train_iter_loss: 0.2695818543434143
train_iter_loss: 0.26394549012184143
train_iter_loss: 0.2967126965522766
train_iter_loss: 0.46634528040885925
train_iter_loss: 0.31604310870170593
train_iter_loss: 0.30328360199928284
train_iter_loss: 0.4173990786075592
train_iter_loss: 0.2609301209449768
train_iter_loss: 0.3468067944049835
train_iter_loss: 0.22479555010795593
train_iter_loss: 0.43745845556259155
train_iter_loss: 0.615074872970581
train_iter_loss: 0.3673931658267975
train_iter_loss: 0.39447537064552307
train_iter_loss: 0.21519747376441956
train_iter_loss: 0.14597469568252563
train_iter_loss: 0.34623846411705017
train_iter_loss: 0.13408318161964417
train_iter_loss: 0.29810312390327454
train_iter_loss: 0.11360953003168106
train_iter_loss: 0.28306400775909424
train_iter_loss: 0.37428799271583557
train_iter_loss: 0.24409788846969604
train_iter_loss: 0.35720980167388916
train_iter_loss: 0.40103012323379517
train_iter_loss: 0.2901109457015991
train_iter_loss: 0.25336670875549316
train_iter_loss: 0.14990180730819702
train_iter_loss: 0.37704163789749146
train_iter_loss: 0.47220081090927124
train_iter_loss: 0.2933056652545929
train_iter_loss: 0.34743383526802063
train_iter_loss: 0.18043766915798187
train_iter_loss: 0.26971834897994995
train_iter_loss: 0.3443509638309479
train_iter_loss: 0.26303786039352417
train_iter_loss: 0.3089945614337921
train_iter_loss: 0.44309908151626587
train_iter_loss: 0.19533835351467133
train_iter_loss: 0.2310207188129425
train_iter_loss: 0.3025682270526886
train_iter_loss: 0.44493964314460754
train_iter_loss: 0.486742228269577
train_iter_loss: 0.17815150320529938
train_iter_loss: 0.2182243913412094
train_iter_loss: 0.3985000252723694
train_iter_loss: 0.24606509506702423
train loss :0.3089
---------------------
Validation seg loss: 0.3907572112907216 at epoch 101
epoch =    102/  1000, exp = train
train_iter_loss: 0.15565671026706696
train_iter_loss: 0.2843581438064575
train_iter_loss: 0.20385557413101196
train_iter_loss: 0.26665198802948
train_iter_loss: 0.3476831614971161
train_iter_loss: 0.633152961730957
train_iter_loss: 0.39795926213264465
train_iter_loss: 0.24192209541797638
train_iter_loss: 0.20753957331180573
train_iter_loss: 0.5170377492904663
train_iter_loss: 0.5219184756278992
train_iter_loss: 0.2543394863605499
train_iter_loss: 0.3000657260417938
train_iter_loss: 0.11733629554510117
train_iter_loss: 0.15315650403499603
train_iter_loss: 0.17232422530651093
train_iter_loss: 0.3624108135700226
train_iter_loss: 0.4216437041759491
train_iter_loss: 0.23380418121814728
train_iter_loss: 0.4553789794445038
train_iter_loss: 0.37606722116470337
train_iter_loss: 0.3204936385154724
train_iter_loss: 0.32149428129196167
train_iter_loss: 0.28944265842437744
train_iter_loss: 0.4721008539199829
train_iter_loss: 0.3403874337673187
train_iter_loss: 0.31371766328811646
train_iter_loss: 0.23428204655647278
train_iter_loss: 0.231593057513237
train_iter_loss: 0.26260292530059814
train_iter_loss: 0.41291543841362
train_iter_loss: 0.5243542790412903
train_iter_loss: 0.14022833108901978
train_iter_loss: 0.32598045468330383
train_iter_loss: 0.3208972215652466
train_iter_loss: 0.4813746511936188
train_iter_loss: 0.35916632413864136
train_iter_loss: 0.34439778327941895
train_iter_loss: 0.2336488515138626
train_iter_loss: 0.317302405834198
train_iter_loss: 0.4864756762981415
train_iter_loss: 0.33064329624176025
train_iter_loss: 0.24087487161159515
train_iter_loss: 0.24362555146217346
train_iter_loss: 0.2320663034915924
train_iter_loss: 0.2737066447734833
train_iter_loss: 0.1679508239030838
train_iter_loss: 0.38881242275238037
train_iter_loss: 0.4087529480457306
train_iter_loss: 0.2500028908252716
train_iter_loss: 0.34073585271835327
train_iter_loss: 0.2975744307041168
train_iter_loss: 0.24243803322315216
train_iter_loss: 0.39558306336402893
train_iter_loss: 0.2539498805999756
train_iter_loss: 0.2762441039085388
train_iter_loss: 0.34313270449638367
train_iter_loss: 0.40877020359039307
train_iter_loss: 0.13707450032234192
train_iter_loss: 0.28581708669662476
train_iter_loss: 0.24005097150802612
train_iter_loss: 0.31202539801597595
train_iter_loss: 0.3581157624721527
train_iter_loss: 0.2475140541791916
train_iter_loss: 0.308002769947052
train_iter_loss: 0.32268431782722473
train_iter_loss: 0.22081699967384338
train_iter_loss: 0.2341334968805313
train_iter_loss: 0.3141127824783325
train_iter_loss: 0.25888592004776
train_iter_loss: 0.16221609711647034
train_iter_loss: 0.3249502182006836
train_iter_loss: 0.3563992977142334
train_iter_loss: 0.1660672128200531
train_iter_loss: 0.3086041212081909
train_iter_loss: 0.23758377134799957
train_iter_loss: 0.3164135217666626
train_iter_loss: 0.23869478702545166
train_iter_loss: 0.15374524891376495
train_iter_loss: 0.729475200176239
train_iter_loss: 0.3898565173149109
train_iter_loss: 0.5986545085906982
train_iter_loss: 0.2734995484352112
train_iter_loss: 0.44269347190856934
train_iter_loss: 0.2667023241519928
train_iter_loss: 0.2805740237236023
train_iter_loss: 0.38513317704200745
train_iter_loss: 0.32383993268013
train_iter_loss: 0.37161052227020264
train_iter_loss: 0.19763422012329102
train_iter_loss: 0.22450397908687592
train_iter_loss: 0.17251893877983093
train_iter_loss: 0.17762182652950287
train_iter_loss: 0.39805418252944946
train_iter_loss: 0.182221457362175
train_iter_loss: 0.2578965127468109
train_iter_loss: 0.3264165222644806
train_iter_loss: 0.24025490880012512
train_iter_loss: 0.40639838576316833
train_iter_loss: 0.25584590435028076
train loss :0.3117
---------------------
Validation seg loss: 0.37201879985349356 at epoch 102
epoch =    103/  1000, exp = train
train_iter_loss: 0.45079877972602844
train_iter_loss: 0.33112215995788574
train_iter_loss: 0.2529096007347107
train_iter_loss: 0.3576405942440033
train_iter_loss: 0.5353412628173828
train_iter_loss: 0.25752854347229004
train_iter_loss: 0.29477402567863464
train_iter_loss: 0.3872819244861603
train_iter_loss: 0.404398113489151
train_iter_loss: 0.3618723452091217
train_iter_loss: 0.5858550071716309
train_iter_loss: 0.17220686376094818
train_iter_loss: 0.2598750591278076
train_iter_loss: 0.3274987041950226
train_iter_loss: 0.21456551551818848
train_iter_loss: 0.36726757884025574
train_iter_loss: 0.40304309129714966
train_iter_loss: 0.285968154668808
train_iter_loss: 0.2544960379600525
train_iter_loss: 0.2389165759086609
train_iter_loss: 0.22744691371917725
train_iter_loss: 0.2988342046737671
train_iter_loss: 0.15432974696159363
train_iter_loss: 0.19364480674266815
train_iter_loss: 0.3063742220401764
train_iter_loss: 0.3884361982345581
train_iter_loss: 0.1901325285434723
train_iter_loss: 0.2588988244533539
train_iter_loss: 0.3691701889038086
train_iter_loss: 0.25614383816719055
train_iter_loss: 0.4870602786540985
train_iter_loss: 0.12474388629198074
train_iter_loss: 0.48760491609573364
train_iter_loss: 0.35535764694213867
train_iter_loss: 0.4657081961631775
train_iter_loss: 0.2820037007331848
train_iter_loss: 0.3043842017650604
train_iter_loss: 0.2403164654970169
train_iter_loss: 0.40293166041374207
train_iter_loss: 0.3322744071483612
train_iter_loss: 0.30974850058555603
train_iter_loss: 0.24224257469177246
train_iter_loss: 0.31188568472862244
train_iter_loss: 0.2899378836154938
train_iter_loss: 0.35497361421585083
train_iter_loss: 0.3573920428752899
train_iter_loss: 0.3236469626426697
train_iter_loss: 0.33612263202667236
train_iter_loss: 0.36352112889289856
train_iter_loss: 0.3355209529399872
train_iter_loss: 0.3436889350414276
train_iter_loss: 0.36951687932014465
train_iter_loss: 0.2532999515533447
train_iter_loss: 0.47810637950897217
train_iter_loss: 0.1967625916004181
train_iter_loss: 0.32698866724967957
train_iter_loss: 0.2653999626636505
train_iter_loss: 0.3245350420475006
train_iter_loss: 0.2607046067714691
train_iter_loss: 0.2850021719932556
train_iter_loss: 0.3122353255748749
train_iter_loss: 0.2699778974056244
train_iter_loss: 0.21097686886787415
train_iter_loss: 0.509647786617279
train_iter_loss: 0.21997641026973724
train_iter_loss: 0.260307252407074
train_iter_loss: 0.20886297523975372
train_iter_loss: 0.285384863615036
train_iter_loss: 0.2766229510307312
train_iter_loss: 0.26252642273902893
train_iter_loss: 0.4531809687614441
train_iter_loss: 0.3628050982952118
train_iter_loss: 0.3415201008319855
train_iter_loss: 0.33053281903266907
train_iter_loss: 0.34866800904273987
train_iter_loss: 0.29859721660614014
train_iter_loss: 0.31001582741737366
train_iter_loss: 0.3331674635410309
train_iter_loss: 0.26160314679145813
train_iter_loss: 0.14452184736728668
train_iter_loss: 0.225504532456398
train_iter_loss: 0.18917366862297058
train_iter_loss: 0.4689452350139618
train_iter_loss: 0.21754872798919678
train_iter_loss: 0.4756549596786499
train_iter_loss: 0.2780916392803192
train_iter_loss: 0.25343066453933716
train_iter_loss: 0.31155630946159363
train_iter_loss: 0.2792729437351227
train_iter_loss: 0.17566679418087006
train_iter_loss: 0.18626029789447784
train_iter_loss: 0.3843386173248291
train_iter_loss: 0.33390700817108154
train_iter_loss: 0.3185865581035614
train_iter_loss: 0.25930285453796387
train_iter_loss: 0.23362648487091064
train_iter_loss: 0.2864498496055603
train_iter_loss: 0.35120218992233276
train_iter_loss: 0.27327969670295715
train_iter_loss: 0.42535823583602905
train loss :0.3144
---------------------
Validation seg loss: 0.3832503453353947 at epoch 103
epoch =    104/  1000, exp = train
train_iter_loss: 0.2849220037460327
train_iter_loss: 0.29854482412338257
train_iter_loss: 0.16964071989059448
train_iter_loss: 0.32824254035949707
train_iter_loss: 0.21045735478401184
train_iter_loss: 0.29272201657295227
train_iter_loss: 0.547630250453949
train_iter_loss: 0.3714265525341034
train_iter_loss: 0.2923085391521454
train_iter_loss: 0.23671849071979523
train_iter_loss: 0.28289833664894104
train_iter_loss: 0.33795469999313354
train_iter_loss: 0.4241255819797516
train_iter_loss: 0.30684876441955566
train_iter_loss: 0.36128753423690796
train_iter_loss: 0.2742603123188019
train_iter_loss: 0.4141029715538025
train_iter_loss: 0.4154464304447174
train_iter_loss: 0.3371242582798004
train_iter_loss: 0.2652393877506256
train_iter_loss: 0.1907668262720108
train_iter_loss: 0.3539944589138031
train_iter_loss: 0.3807258605957031
train_iter_loss: 0.3712995946407318
train_iter_loss: 0.3106745779514313
train_iter_loss: 0.27949923276901245
train_iter_loss: 0.2593313455581665
train_iter_loss: 0.17794117331504822
train_iter_loss: 0.2586611807346344
train_iter_loss: 0.30862125754356384
train_iter_loss: 0.17523732781410217
train_iter_loss: 0.30145105719566345
train_iter_loss: 0.19771090149879456
train_iter_loss: 0.2666243314743042
train_iter_loss: 0.32209542393684387
train_iter_loss: 0.28064146637916565
train_iter_loss: 0.3932116627693176
train_iter_loss: 0.38871341943740845
train_iter_loss: 0.41646522283554077
train_iter_loss: 0.22560502588748932
train_iter_loss: 0.376667320728302
train_iter_loss: 0.3970170319080353
train_iter_loss: 0.27902793884277344
train_iter_loss: 0.2696605622768402
train_iter_loss: 0.17406800389289856
train_iter_loss: 0.29648858308792114
train_iter_loss: 0.23073826730251312
train_iter_loss: 0.3871907889842987
train_iter_loss: 0.29769137501716614
train_iter_loss: 0.42877545952796936
train_iter_loss: 0.21074888110160828
train_iter_loss: 0.24993647634983063
train_iter_loss: 0.40986064076423645
train_iter_loss: 0.297861248254776
train_iter_loss: 0.34411731362342834
train_iter_loss: 0.2846798896789551
train_iter_loss: 0.2546982765197754
train_iter_loss: 0.3083968162536621
train_iter_loss: 0.17743481695652008
train_iter_loss: 0.27750930190086365
train_iter_loss: 0.4696316421031952
train_iter_loss: 0.23614290356636047
train_iter_loss: 0.26291683316230774
train_iter_loss: 0.2470560073852539
train_iter_loss: 0.22754473984241486
train_iter_loss: 0.2525671124458313
train_iter_loss: 0.3138163983821869
train_iter_loss: 0.33258652687072754
train_iter_loss: 0.4544098973274231
train_iter_loss: 0.3504447340965271
train_iter_loss: 0.29931139945983887
train_iter_loss: 0.20070242881774902
train_iter_loss: 0.32420438528060913
train_iter_loss: 0.2886056900024414
train_iter_loss: 0.29071569442749023
train_iter_loss: 0.31011754274368286
train_iter_loss: 0.200708270072937
train_iter_loss: 0.24716328084468842
train_iter_loss: 0.36127689480781555
train_iter_loss: 0.3592522144317627
train_iter_loss: 0.24420452117919922
train_iter_loss: 0.38010308146476746
train_iter_loss: 0.23867076635360718
train_iter_loss: 0.3118084669113159
train_iter_loss: 0.3214026987552643
train_iter_loss: 0.13410726189613342
train_iter_loss: 0.24770782887935638
train_iter_loss: 0.27450278401374817
train_iter_loss: 0.2599760890007019
train_iter_loss: 0.3192248046398163
train_iter_loss: 0.2806152105331421
train_iter_loss: 0.2657065689563751
train_iter_loss: 0.3115612268447876
train_iter_loss: 0.3919605612754822
train_iter_loss: 0.2243674397468567
train_iter_loss: 0.34413257241249084
train_iter_loss: 0.2331586480140686
train_iter_loss: 0.41572198271751404
train_iter_loss: 0.21751746535301208
train_iter_loss: 0.31031709909439087
train loss :0.3033
---------------------
Validation seg loss: 0.40627846415242497 at epoch 104
epoch =    105/  1000, exp = train
train_iter_loss: 0.3388917148113251
train_iter_loss: 0.26554813981056213
train_iter_loss: 0.30468663573265076
train_iter_loss: 0.22020000219345093
train_iter_loss: 0.32375892996788025
train_iter_loss: 0.27086514234542847
train_iter_loss: 0.2164425253868103
train_iter_loss: 0.3633131980895996
train_iter_loss: 0.2605542540550232
train_iter_loss: 0.09741885960102081
train_iter_loss: 0.3375433087348938
train_iter_loss: 0.3154715299606323
train_iter_loss: 0.3931131660938263
train_iter_loss: 0.23916751146316528
train_iter_loss: 0.44204166531562805
train_iter_loss: 0.1742277443408966
train_iter_loss: 0.34548690915107727
train_iter_loss: 0.27551376819610596
train_iter_loss: 0.21800808608531952
train_iter_loss: 0.3690689206123352
train_iter_loss: 0.5173062682151794
train_iter_loss: 0.3901149332523346
train_iter_loss: 0.33705613017082214
train_iter_loss: 0.33814534544944763
train_iter_loss: 0.2294859141111374
train_iter_loss: 0.2975878417491913
train_iter_loss: 0.3383078873157501
train_iter_loss: 0.28011640906333923
train_iter_loss: 0.37268856167793274
train_iter_loss: 0.18727943301200867
train_iter_loss: 0.3143256902694702
train_iter_loss: 0.27737027406692505
train_iter_loss: 0.27518483996391296
train_iter_loss: 0.26465940475463867
train_iter_loss: 0.401358425617218
train_iter_loss: 0.26037678122520447
train_iter_loss: 0.3029572069644928
train_iter_loss: 0.29470422863960266
train_iter_loss: 0.3325425684452057
train_iter_loss: 0.47010332345962524
train_iter_loss: 0.33132490515708923
train_iter_loss: 0.2556275427341461
train_iter_loss: 0.1931103765964508
train_iter_loss: 0.16654644906520844
train_iter_loss: 0.30546343326568604
train_iter_loss: 0.20433399081230164
train_iter_loss: 0.1519942432641983
train_iter_loss: 0.1808643937110901
train_iter_loss: 0.266378253698349
train_iter_loss: 0.2670416533946991
train_iter_loss: 0.3967704772949219
train_iter_loss: 0.19240492582321167
train_iter_loss: 0.3620666563510895
train_iter_loss: 0.136528879404068
train_iter_loss: 0.3014242947101593
train_iter_loss: 0.10682704299688339
train_iter_loss: 0.23569947481155396
train_iter_loss: 0.5009506344795227
train_iter_loss: 0.5023472905158997
train_iter_loss: 0.07177110016345978
train_iter_loss: 0.2785089612007141
train_iter_loss: 0.26599985361099243
train_iter_loss: 0.5171439051628113
train_iter_loss: 0.3011409044265747
train_iter_loss: 0.33526700735092163
train_iter_loss: 0.4190131425857544
train_iter_loss: 0.24478012323379517
train_iter_loss: 0.20455314218997955
train_iter_loss: 0.2791067659854889
train_iter_loss: 0.2631377875804901
train_iter_loss: 0.39778372645378113
train_iter_loss: 0.3306773900985718
train_iter_loss: 0.40737104415893555
train_iter_loss: 0.2990170419216156
train_iter_loss: 0.33049553632736206
train_iter_loss: 0.4756445586681366
train_iter_loss: 0.2901018261909485
train_iter_loss: 0.1722845733165741
train_iter_loss: 0.258095920085907
train_iter_loss: 0.29217278957366943
train_iter_loss: 0.3657735586166382
train_iter_loss: 0.3619544804096222
train_iter_loss: 0.3347870707511902
train_iter_loss: 0.32736894488334656
train_iter_loss: 0.1838957667350769
train_iter_loss: 0.27034738659858704
train_iter_loss: 0.2596442699432373
train_iter_loss: 0.2630574107170105
train_iter_loss: 0.2534905672073364
train_iter_loss: 0.33245059847831726
train_iter_loss: 0.3437056541442871
train_iter_loss: 0.31580013036727905
train_iter_loss: 0.27894851565361023
train_iter_loss: 0.33246761560440063
train_iter_loss: 0.38348546624183655
train_iter_loss: 0.3220473825931549
train_iter_loss: 0.3959859311580658
train_iter_loss: 0.40085625648498535
train_iter_loss: 0.30851098895072937
train_iter_loss: 0.3491089344024658
train loss :0.3043
---------------------
Validation seg loss: 0.3961895710946816 at epoch 105
epoch =    106/  1000, exp = train
train_iter_loss: 0.34310775995254517
train_iter_loss: 0.27842289209365845
train_iter_loss: 0.3483451306819916
train_iter_loss: 0.25307679176330566
train_iter_loss: 0.33099502325057983
train_iter_loss: 0.3613802492618561
train_iter_loss: 0.3608231246471405
train_iter_loss: 0.2414291352033615
train_iter_loss: 0.5217069387435913
train_iter_loss: 0.3714718222618103
train_iter_loss: 0.21657387912273407
train_iter_loss: 0.26327627897262573
train_iter_loss: 0.4608219563961029
train_iter_loss: 0.31203359365463257
train_iter_loss: 0.2634606659412384
train_iter_loss: 0.22453297674655914
train_iter_loss: 0.16347897052764893
train_iter_loss: 0.32124757766723633
train_iter_loss: 0.40740081667900085
train_iter_loss: 0.3280511796474457
train_iter_loss: 0.23036959767341614
train_iter_loss: 0.29192206263542175
train_iter_loss: 0.3580608665943146
train_iter_loss: 0.3128826916217804
train_iter_loss: 0.3042784333229065
train_iter_loss: 0.16175015270709991
train_iter_loss: 0.19632938504219055
train_iter_loss: 0.2947644293308258
train_iter_loss: 0.3731897175312042
train_iter_loss: 0.2677682936191559
train_iter_loss: 0.19572585821151733
train_iter_loss: 0.2976534962654114
train_iter_loss: 0.35471266508102417
train_iter_loss: 0.44256436824798584
train_iter_loss: 0.36838704347610474
train_iter_loss: 0.34169724583625793
train_iter_loss: 0.2057427018880844
train_iter_loss: 0.2773783802986145
train_iter_loss: 0.2623453438282013
train_iter_loss: 0.22480808198451996
train_iter_loss: 0.4383127689361572
train_iter_loss: 0.304622083902359
train_iter_loss: 0.2437661737203598
train_iter_loss: 0.3863472640514374
train_iter_loss: 0.25796371698379517
train_iter_loss: 0.3343638479709625
train_iter_loss: 0.27915453910827637
train_iter_loss: 0.3070239722728729
train_iter_loss: 0.30178043246269226
train_iter_loss: 0.18713073432445526
train_iter_loss: 0.21050286293029785
train_iter_loss: 0.28877878189086914
train_iter_loss: 0.2433120459318161
train_iter_loss: 0.3197668790817261
train_iter_loss: 0.33008521795272827
train_iter_loss: 0.5059475898742676
train_iter_loss: 0.23814551532268524
train_iter_loss: 0.38163667917251587
train_iter_loss: 0.40588894486427307
train_iter_loss: 0.2737770080566406
train_iter_loss: 0.14772813022136688
train_iter_loss: 0.309626042842865
train_iter_loss: 0.28683072328567505
train_iter_loss: 0.2678182125091553
train_iter_loss: 0.25865334272384644
train_iter_loss: 0.3094569742679596
train_iter_loss: 0.24618612229824066
train_iter_loss: 0.2760111391544342
train_iter_loss: 0.2500345706939697
train_iter_loss: 0.3807271718978882
train_iter_loss: 0.32261261343955994
train_iter_loss: 0.2869335412979126
train_iter_loss: 0.3749271035194397
train_iter_loss: 0.3825777471065521
train_iter_loss: 0.27283358573913574
train_iter_loss: 0.26752325892448425
train_iter_loss: 0.30036064982414246
train_iter_loss: 0.28356871008872986
train_iter_loss: 0.30329790711402893
train_iter_loss: 0.2208387404680252
train_iter_loss: 0.2050604671239853
train_iter_loss: 0.3286493122577667
train_iter_loss: 0.3869302570819855
train_iter_loss: 0.32228201627731323
train_iter_loss: 0.30388137698173523
train_iter_loss: 0.28891444206237793
train_iter_loss: 0.27470138669013977
train_iter_loss: 0.31294289231300354
train_iter_loss: 0.46060994267463684
train_iter_loss: 0.38982680439949036
train_iter_loss: 0.3424742817878723
train_iter_loss: 0.2736630439758301
train_iter_loss: 0.37524399161338806
train_iter_loss: 0.31548693776130676
train_iter_loss: 0.26452112197875977
train_iter_loss: 0.3567047715187073
train_iter_loss: 0.33762702345848083
train_iter_loss: 0.1691073477268219
train_iter_loss: 0.16530992090702057
train_iter_loss: 0.2794637084007263
train loss :0.3058
---------------------
Validation seg loss: 0.37813889730792 at epoch 106
epoch =    107/  1000, exp = train
train_iter_loss: 0.2085677534341812
train_iter_loss: 0.29776349663734436
train_iter_loss: 0.46817442774772644
train_iter_loss: 0.3491641581058502
train_iter_loss: 0.34794023633003235
train_iter_loss: 0.28449589014053345
train_iter_loss: 0.26915037631988525
train_iter_loss: 0.34174349904060364
train_iter_loss: 0.2523704469203949
train_iter_loss: 0.4098132848739624
train_iter_loss: 0.3561360836029053
train_iter_loss: 0.3037894666194916
train_iter_loss: 0.34346047043800354
train_iter_loss: 0.4829094409942627
train_iter_loss: 0.34812435507774353
train_iter_loss: 0.43461889028549194
train_iter_loss: 0.308542400598526
train_iter_loss: 0.37873905897140503
train_iter_loss: 0.24322447180747986
train_iter_loss: 0.25164881348609924
train_iter_loss: 0.3579132556915283
train_iter_loss: 0.44028231501579285
train_iter_loss: 0.3531593382358551
train_iter_loss: 0.18562495708465576
train_iter_loss: 0.18553389608860016
train_iter_loss: 0.42459946870803833
train_iter_loss: 0.25986409187316895
train_iter_loss: 0.2986973822116852
train_iter_loss: 0.31205281615257263
train_iter_loss: 0.24550646543502808
train_iter_loss: 0.18455953896045685
train_iter_loss: 0.17633260786533356
train_iter_loss: 0.2480233758687973
train_iter_loss: 0.3116742670536041
train_iter_loss: 0.17954304814338684
train_iter_loss: 0.2811484932899475
train_iter_loss: 0.30360570549964905
train_iter_loss: 0.2593814432621002
train_iter_loss: 0.3255309462547302
train_iter_loss: 0.24715130031108856
train_iter_loss: 0.3405269384384155
train_iter_loss: 0.19300541281700134
train_iter_loss: 0.24850118160247803
train_iter_loss: 0.28053534030914307
train_iter_loss: 0.22387024760246277
train_iter_loss: 0.32637521624565125
train_iter_loss: 0.31761765480041504
train_iter_loss: 0.29643115401268005
train_iter_loss: 0.36334171891212463
train_iter_loss: 0.24741040170192719
train_iter_loss: 0.3401990234851837
train_iter_loss: 0.2932155132293701
train_iter_loss: 0.1747725009918213
train_iter_loss: 0.20567390322685242
train_iter_loss: 0.40228161215782166
train_iter_loss: 0.27691975235939026
train_iter_loss: 0.22630393505096436
train_iter_loss: 0.3018079996109009
train_iter_loss: 0.2524150609970093
train_iter_loss: 0.32186561822891235
train_iter_loss: 0.2739095687866211
train_iter_loss: 0.2417103350162506
train_iter_loss: 0.34117981791496277
train_iter_loss: 0.3349588215351105
train_iter_loss: 0.35831546783447266
train_iter_loss: 0.17971529066562653
train_iter_loss: 0.3673730194568634
train_iter_loss: 0.29889053106307983
train_iter_loss: 0.28508666157722473
train_iter_loss: 0.2922622859477997
train_iter_loss: 0.5430219769477844
train_iter_loss: 0.31936049461364746
train_iter_loss: 0.41315925121307373
train_iter_loss: 0.19981616735458374
train_iter_loss: 0.3575708270072937
train_iter_loss: 0.34556612372398376
train_iter_loss: 0.24988237023353577
train_iter_loss: 0.22507929801940918
train_iter_loss: 0.45804738998413086
train_iter_loss: 0.40936923027038574
train_iter_loss: 0.18806837499141693
train_iter_loss: 0.21183781325817108
train_iter_loss: 0.16661641001701355
train_iter_loss: 0.32202067971229553
train_iter_loss: 0.2814854085445404
train_iter_loss: 0.2514776587486267
train_iter_loss: 0.28877973556518555
train_iter_loss: 0.24535737931728363
train_iter_loss: 0.19409804046154022
train_iter_loss: 0.17809300124645233
train_iter_loss: 0.40995916724205017
train_iter_loss: 0.3889938294887543
train_iter_loss: 0.20610469579696655
train_iter_loss: 0.15632128715515137
train_iter_loss: 0.37147098779678345
train_iter_loss: 0.44277673959732056
train_iter_loss: 0.24598832428455353
train_iter_loss: 0.1551651805639267
train_iter_loss: 0.4154728651046753
train_iter_loss: 0.25842994451522827
train loss :0.3007
---------------------
Validation seg loss: 0.3723693565764236 at epoch 107
epoch =    108/  1000, exp = train
train_iter_loss: 0.2615695595741272
train_iter_loss: 0.24545606970787048
train_iter_loss: 0.286764919757843
train_iter_loss: 0.2427651733160019
train_iter_loss: 0.31481698155403137
train_iter_loss: 0.27073657512664795
train_iter_loss: 0.2270912081003189
train_iter_loss: 0.2596057057380676
train_iter_loss: 0.30459165573120117
train_iter_loss: 0.38496580719947815
train_iter_loss: 0.09963058680295944
train_iter_loss: 0.37868937849998474
train_iter_loss: 0.35876017808914185
train_iter_loss: 0.576025664806366
train_iter_loss: 0.27818217873573303
train_iter_loss: 0.3166976571083069
train_iter_loss: 0.20844151079654694
train_iter_loss: 0.17095239460468292
train_iter_loss: 0.2918406128883362
train_iter_loss: 0.34273621439933777
train_iter_loss: 0.17636899650096893
train_iter_loss: 0.22013001143932343
train_iter_loss: 0.1916174292564392
train_iter_loss: 0.3269403576850891
train_iter_loss: 0.3199266195297241
train_iter_loss: 0.3463287055492401
train_iter_loss: 0.2648240625858307
train_iter_loss: 0.24524888396263123
train_iter_loss: 0.35508692264556885
train_iter_loss: 0.3546917736530304
train_iter_loss: 0.41448143124580383
train_iter_loss: 0.2926439344882965
train_iter_loss: 0.27370136976242065
train_iter_loss: 0.22950728237628937
train_iter_loss: 0.3797978460788727
train_iter_loss: 0.37617093324661255
train_iter_loss: 0.26410576701164246
train_iter_loss: 0.2834261953830719
train_iter_loss: 0.6662626266479492
train_iter_loss: 0.20646046102046967
train_iter_loss: 0.07746265083551407
train_iter_loss: 0.1520908623933792
train_iter_loss: 0.2472769021987915
train_iter_loss: 0.44618508219718933
train_iter_loss: 0.39605510234832764
train_iter_loss: 0.2966867685317993
train_iter_loss: 0.3154347538948059
train_iter_loss: 0.24283021688461304
train_iter_loss: 0.23721244931221008
train_iter_loss: 0.17062164843082428
train_iter_loss: 0.20094075798988342
train_iter_loss: 0.2687881886959076
train_iter_loss: 0.35244518518447876
train_iter_loss: 0.5359811782836914
train_iter_loss: 0.33539503812789917
train_iter_loss: 0.3199015259742737
train_iter_loss: 0.36554181575775146
train_iter_loss: 0.21467258036136627
train_iter_loss: 0.3586571216583252
train_iter_loss: 0.32741665840148926
train_iter_loss: 0.4138979911804199
train_iter_loss: 0.38812968134880066
train_iter_loss: 0.2714043855667114
train_iter_loss: 0.22692041099071503
train_iter_loss: 0.35557955503463745
train_iter_loss: 0.3898119032382965
train_iter_loss: 0.3224896192550659
train_iter_loss: 0.28424522280693054
train_iter_loss: 0.43728190660476685
train_iter_loss: 0.25270843505859375
train_iter_loss: 0.2085670530796051
train_iter_loss: 0.3298597037792206
train_iter_loss: 0.1903739720582962
train_iter_loss: 0.4429643452167511
train_iter_loss: 0.37448105216026306
train_iter_loss: 0.36464348435401917
train_iter_loss: 0.11964855343103409
train_iter_loss: 0.263049840927124
train_iter_loss: 0.29154172539711
train_iter_loss: 0.2724759578704834
train_iter_loss: 0.1433795839548111
train_iter_loss: 0.286132276058197
train_iter_loss: 0.30328381061553955
train_iter_loss: 0.27218660712242126
train_iter_loss: 0.417385458946228
train_iter_loss: 0.29640883207321167
train_iter_loss: 0.2168000042438507
train_iter_loss: 0.25901997089385986
train_iter_loss: 0.22892452776432037
train_iter_loss: 0.350129634141922
train_iter_loss: 0.3573947548866272
train_iter_loss: 0.24846899509429932
train_iter_loss: 0.29763370752334595
train_iter_loss: 0.5325322151184082
train_iter_loss: 0.3262232542037964
train_iter_loss: 0.1272180676460266
train_iter_loss: 0.3225609362125397
train_iter_loss: 0.47309160232543945
train_iter_loss: 0.25960811972618103
train_iter_loss: 0.1316528618335724
train loss :0.3022
---------------------
Validation seg loss: 0.3915113568797989 at epoch 108
epoch =    109/  1000, exp = train
train_iter_loss: 0.4087388217449188
train_iter_loss: 0.2711857855319977
train_iter_loss: 0.2758982479572296
train_iter_loss: 0.3895120322704315
train_iter_loss: 0.3480847477912903
train_iter_loss: 0.38252565264701843
train_iter_loss: 0.3097230792045593
train_iter_loss: 0.239076629281044
train_iter_loss: 0.341691792011261
train_iter_loss: 0.3380679190158844
train_iter_loss: 0.3527577519416809
train_iter_loss: 0.39432600140571594
train_iter_loss: 0.1587710827589035
train_iter_loss: 0.25121936202049255
train_iter_loss: 0.2795482873916626
train_iter_loss: 0.23118551075458527
train_iter_loss: 0.21354717016220093
train_iter_loss: 0.3219618797302246
train_iter_loss: 0.4206259250640869
train_iter_loss: 0.338925302028656
train_iter_loss: 0.23718300461769104
train_iter_loss: 0.3644719123840332
train_iter_loss: 0.29508593678474426
train_iter_loss: 0.4083516299724579
train_iter_loss: 0.28050437569618225
train_iter_loss: 0.40369054675102234
train_iter_loss: 0.31868764758110046
train_iter_loss: 0.3620164096355438
train_iter_loss: 0.5273086428642273
train_iter_loss: 0.34501075744628906
train_iter_loss: 0.4599616527557373
train_iter_loss: 0.35709869861602783
train_iter_loss: 0.15473727881908417
train_iter_loss: 0.33551478385925293
train_iter_loss: 0.377119779586792
train_iter_loss: 0.18074612319469452
train_iter_loss: 0.4275909960269928
train_iter_loss: 0.15252968668937683
train_iter_loss: 0.512443482875824
train_iter_loss: 0.2467237412929535
train_iter_loss: 0.2652684152126312
train_iter_loss: 0.24808482825756073
train_iter_loss: 0.2285224199295044
train_iter_loss: 0.30511078238487244
train_iter_loss: 0.15443198382854462
train_iter_loss: 0.32706016302108765
train_iter_loss: 0.3221375644207001
train_iter_loss: 0.261301189661026
train_iter_loss: 0.19570042192935944
train_iter_loss: 0.3357997536659241
train_iter_loss: 0.4588261842727661
train_iter_loss: 0.32724013924598694
train_iter_loss: 0.3441559076309204
train_iter_loss: 0.274899959564209
train_iter_loss: 0.35088104009628296
train_iter_loss: 0.21546633541584015
train_iter_loss: 0.4105989336967468
train_iter_loss: 0.3356664478778839
train_iter_loss: 0.22761420905590057
train_iter_loss: 0.4079500138759613
train_iter_loss: 0.27041831612586975
train_iter_loss: 0.4681817293167114
train_iter_loss: 0.30598631501197815
train_iter_loss: 0.30120640993118286
train_iter_loss: 0.2488410770893097
train_iter_loss: 0.40307414531707764
train_iter_loss: 0.24611744284629822
train_iter_loss: 0.228724405169487
train_iter_loss: 0.2684844434261322
train_iter_loss: 0.33762747049331665
train_iter_loss: 0.25304538011550903
train_iter_loss: 0.16678421199321747
train_iter_loss: 0.41174766421318054
train_iter_loss: 0.1981908082962036
train_iter_loss: 0.19174934923648834
train_iter_loss: 0.30859553813934326
train_iter_loss: 0.23092471063137054
train_iter_loss: 0.30367311835289
train_iter_loss: 0.20631077885627747
train_iter_loss: 0.1793985664844513
train_iter_loss: 0.2930276393890381
train_iter_loss: 0.20682241022586823
train_iter_loss: 0.3648073971271515
train_iter_loss: 0.2598021626472473
train_iter_loss: 0.28188246488571167
train_iter_loss: 0.22745218873023987
train_iter_loss: 0.23422755300998688
train_iter_loss: 0.4465811848640442
train_iter_loss: 0.31256774067878723
train_iter_loss: 0.25251302123069763
train_iter_loss: 0.1826326698064804
train_iter_loss: 0.2913286089897156
train_iter_loss: 0.34923315048217773
train_iter_loss: 0.25620198249816895
train_iter_loss: 0.3911227583885193
train_iter_loss: 0.26285260915756226
train_iter_loss: 0.17440062761306763
train_iter_loss: 0.21008840203285217
train_iter_loss: 0.33542531728744507
train_iter_loss: 0.29680439829826355
train loss :0.3047
---------------------
Validation seg loss: 0.37129481316034524 at epoch 109
********************
best_val_epoch_loss:  0.37129481316034524
MODEL UPDATED
epoch =    110/  1000, exp = train
train_iter_loss: 0.2242845743894577
train_iter_loss: 0.31594598293304443
train_iter_loss: 0.4095543622970581
train_iter_loss: 0.5081395506858826
train_iter_loss: 0.3829936683177948
train_iter_loss: 0.18781691789627075
train_iter_loss: 0.4643026292324066
train_iter_loss: 0.4009585678577423
train_iter_loss: 0.18099597096443176
train_iter_loss: 0.28536954522132874
train_iter_loss: 0.29573729634284973
train_iter_loss: 0.45569923520088196
train_iter_loss: 0.3366823196411133
train_iter_loss: 0.2770227789878845
train_iter_loss: 0.3780566453933716
train_iter_loss: 0.437894344329834
train_iter_loss: 0.35469090938568115
train_iter_loss: 0.22041438519954681
train_iter_loss: 0.41181138157844543
train_iter_loss: 0.36576759815216064
train_iter_loss: 0.19418251514434814
train_iter_loss: 0.2817770838737488
train_iter_loss: 0.30941393971443176
train_iter_loss: 0.5182314515113831
train_iter_loss: 0.1679040640592575
train_iter_loss: 0.3921150863170624
train_iter_loss: 0.2548527419567108
train_iter_loss: 0.22774964570999146
train_iter_loss: 0.3262670636177063
train_iter_loss: 0.22899138927459717
train_iter_loss: 0.29345589876174927
train_iter_loss: 0.29187265038490295
train_iter_loss: 0.31680232286453247
train_iter_loss: 0.36595475673675537
train_iter_loss: 0.423644483089447
train_iter_loss: 0.19659638404846191
train_iter_loss: 0.48238101601600647
train_iter_loss: 0.23247869312763214
train_iter_loss: 0.24919147789478302
train_iter_loss: 0.3593065142631531
train_iter_loss: 0.35795557498931885
train_iter_loss: 0.28522446751594543
train_iter_loss: 0.3534891903400421
train_iter_loss: 0.18961869180202484
train_iter_loss: 0.2799868881702423
train_iter_loss: 0.3251511752605438
train_iter_loss: 0.27745959162712097
train_iter_loss: 0.20728932321071625
train_iter_loss: 0.2261921614408493
train_iter_loss: 0.27350345253944397
train_iter_loss: 0.32337701320648193
train_iter_loss: 0.4550069570541382
train_iter_loss: 0.17769011855125427
train_iter_loss: 0.3422407805919647
train_iter_loss: 0.17245151102542877
train_iter_loss: 0.3338454067707062
train_iter_loss: 0.30389678478240967
train_iter_loss: 0.21144987642765045
train_iter_loss: 0.38628682494163513
train_iter_loss: 0.26919567584991455
train_iter_loss: 0.18834234774112701
train_iter_loss: 0.2257462590932846
train_iter_loss: 0.33133217692375183
train_iter_loss: 0.22845125198364258
train_iter_loss: 0.3197149932384491
train_iter_loss: 0.23248574137687683
train_iter_loss: 0.2848742604255676
train_iter_loss: 0.19283047318458557
train_iter_loss: 0.3168889582157135
train_iter_loss: 0.26876574754714966
train_iter_loss: 0.26920148730278015
train_iter_loss: 0.20235411822795868
train_iter_loss: 0.31427156925201416
train_iter_loss: 0.3101733922958374
train_iter_loss: 0.2670173943042755
train_iter_loss: 0.37715643644332886
train_iter_loss: 0.47278741002082825
train_iter_loss: 0.28372398018836975
train_iter_loss: 0.38506796956062317
train_iter_loss: 0.31007546186447144
train_iter_loss: 0.21473048627376556
train_iter_loss: 0.299918532371521
train_iter_loss: 0.2994685173034668
train_iter_loss: 0.09545987844467163
train_iter_loss: 0.6278063058853149
train_iter_loss: 0.266069620847702
train_iter_loss: 0.16628330945968628
train_iter_loss: 0.24093784391880035
train_iter_loss: 0.24912817776203156
train_iter_loss: 0.28237444162368774
train_iter_loss: 0.41093969345092773
train_iter_loss: 0.3322417438030243
train_iter_loss: 0.22481150925159454
train_iter_loss: 0.28450635075569153
train_iter_loss: 0.40743687748908997
train_iter_loss: 0.36414551734924316
train_iter_loss: 0.3822464644908905
train_iter_loss: 0.3157424032688141
train_iter_loss: 0.4242689907550812
train_iter_loss: 0.17041529715061188
train loss :0.3090
---------------------
Validation seg loss: 0.37780457357260977 at epoch 110
epoch =    111/  1000, exp = train
train_iter_loss: 0.22539176046848297
train_iter_loss: 0.3932412266731262
train_iter_loss: 0.19845867156982422
train_iter_loss: 0.2545512020587921
train_iter_loss: 0.34906700253486633
train_iter_loss: 0.22320842742919922
train_iter_loss: 0.21338225901126862
train_iter_loss: 0.3095638155937195
train_iter_loss: 0.2829197645187378
train_iter_loss: 0.24130474030971527
train_iter_loss: 0.36855822801589966
train_iter_loss: 0.2819497883319855
train_iter_loss: 0.3728184700012207
train_iter_loss: 0.3483988046646118
train_iter_loss: 0.23507143557071686
train_iter_loss: 0.1926436722278595
train_iter_loss: 0.2825184464454651
train_iter_loss: 0.19685566425323486
train_iter_loss: 0.33316150307655334
train_iter_loss: 0.23416610062122345
train_iter_loss: 0.3106563091278076
train_iter_loss: 0.39492475986480713
train_iter_loss: 0.48527395725250244
train_iter_loss: 0.34270259737968445
train_iter_loss: 0.20661070942878723
train_iter_loss: 0.22028478980064392
train_iter_loss: 0.259663462638855
train_iter_loss: 0.3065544068813324
train_iter_loss: 0.3079584240913391
train_iter_loss: 0.21114133298397064
train_iter_loss: 0.5591469407081604
train_iter_loss: 0.36437752842903137
train_iter_loss: 0.3208761215209961
train_iter_loss: 0.3667266368865967
train_iter_loss: 0.28024038672447205
train_iter_loss: 0.3580292761325836
train_iter_loss: 0.29290100932121277
train_iter_loss: 0.31523361802101135
train_iter_loss: 0.21584181487560272
train_iter_loss: 0.4999406933784485
train_iter_loss: 0.20943564176559448
train_iter_loss: 0.2822893559932709
train_iter_loss: 0.30427491664886475
train_iter_loss: 0.3380649983882904
train_iter_loss: 0.4575592875480652
train_iter_loss: 0.25831618905067444
train_iter_loss: 0.2393077313899994
train_iter_loss: 0.3344179391860962
train_iter_loss: 0.26764732599258423
train_iter_loss: 0.3502334952354431
train_iter_loss: 0.4682358205318451
train_iter_loss: 0.2825973331928253
train_iter_loss: 0.537280797958374
train_iter_loss: 0.23612160980701447
train_iter_loss: 0.28767815232276917
train_iter_loss: 0.21776153147220612
train_iter_loss: 0.3204975724220276
train_iter_loss: 0.2506231963634491
train_iter_loss: 0.32862329483032227
train_iter_loss: 0.21071985363960266
train_iter_loss: 0.2918338179588318
train_iter_loss: 0.1259721964597702
train_iter_loss: 0.23095737397670746
train_iter_loss: 0.2539193630218506
train_iter_loss: 0.34917300939559937
train_iter_loss: 0.31740516424179077
train_iter_loss: 0.262818843126297
train_iter_loss: 0.30096200108528137
train_iter_loss: 0.13938350975513458
train_iter_loss: 0.20983117818832397
train_iter_loss: 0.36573806405067444
train_iter_loss: 0.22013980150222778
train_iter_loss: 0.24234539270401
train_iter_loss: 0.2214604616165161
train_iter_loss: 0.3151952028274536
train_iter_loss: 0.3522908687591553
train_iter_loss: 0.45083752274513245
train_iter_loss: 0.41911250352859497
train_iter_loss: 0.3017200231552124
train_iter_loss: 0.21021822094917297
train_iter_loss: 0.2875205874443054
train_iter_loss: 0.22436641156673431
train_iter_loss: 0.37523946166038513
train_iter_loss: 0.20255978405475616
train_iter_loss: 0.23377132415771484
train_iter_loss: 0.25306716561317444
train_iter_loss: 0.21831664443016052
train_iter_loss: 0.5028789043426514
train_iter_loss: 0.2996710538864136
train_iter_loss: 0.13924606144428253
train_iter_loss: 0.13278663158416748
train_iter_loss: 0.41349518299102783
train_iter_loss: 0.3411976099014282
train_iter_loss: 0.16412468254566193
train_iter_loss: 0.3447346091270447
train_iter_loss: 0.44300439953804016
train_iter_loss: 0.2509668171405792
train_iter_loss: 0.4113203287124634
train_iter_loss: 0.26881173253059387
train_iter_loss: 0.4269358217716217
train loss :0.3016
---------------------
Validation seg loss: 0.39174660908515163 at epoch 111
epoch =    112/  1000, exp = train
train_iter_loss: 0.4039223790168762
train_iter_loss: 0.3015861511230469
train_iter_loss: 0.2934735119342804
train_iter_loss: 0.3080931007862091
train_iter_loss: 0.3055165410041809
train_iter_loss: 0.4299320876598358
train_iter_loss: 0.3258717358112335
train_iter_loss: 0.3185233473777771
train_iter_loss: 0.2327444851398468
train_iter_loss: 0.27149271965026855
train_iter_loss: 0.2373374104499817
train_iter_loss: 0.38184958696365356
train_iter_loss: 0.32023900747299194
train_iter_loss: 0.24232791364192963
train_iter_loss: 0.26319998502731323
train_iter_loss: 0.3382348418235779
train_iter_loss: 0.6005625128746033
train_iter_loss: 0.33023345470428467
train_iter_loss: 0.22569766640663147
train_iter_loss: 0.1187080591917038
train_iter_loss: 0.19165246188640594
train_iter_loss: 0.2381521314382553
train_iter_loss: 0.235147625207901
train_iter_loss: 0.3281992971897125
train_iter_loss: 0.3059498071670532
train_iter_loss: 0.25154179334640503
train_iter_loss: 0.28003549575805664
train_iter_loss: 0.35221225023269653
train_iter_loss: 0.3294792175292969
train_iter_loss: 0.1871231347322464
train_iter_loss: 0.2847091555595398
train_iter_loss: 0.26197248697280884
train_iter_loss: 0.17575044929981232
train_iter_loss: 0.26035791635513306
train_iter_loss: 0.18987351655960083
train_iter_loss: 0.14769518375396729
train_iter_loss: 0.3010118305683136
train_iter_loss: 0.4179575741291046
train_iter_loss: 0.39575764536857605
train_iter_loss: 0.36556994915008545
train_iter_loss: 0.31207555532455444
train_iter_loss: 0.1856755167245865
train_iter_loss: 0.2724374830722809
train_iter_loss: 0.3122662603855133
train_iter_loss: 0.33558928966522217
train_iter_loss: 0.27945899963378906
train_iter_loss: 0.2756194472312927
train_iter_loss: 0.45195576548576355
train_iter_loss: 0.35470694303512573
train_iter_loss: 0.1975594013929367
train_iter_loss: 0.3483351469039917
train_iter_loss: 0.33115825057029724
train_iter_loss: 0.3712722957134247
train_iter_loss: 0.3092680871486664
train_iter_loss: 0.3956552743911743
train_iter_loss: 0.23941287398338318
train_iter_loss: 0.25763633847236633
train_iter_loss: 0.32982879877090454
train_iter_loss: 0.2335253208875656
train_iter_loss: 0.2699931561946869
train_iter_loss: 0.2979514002799988
train_iter_loss: 0.2871337831020355
train_iter_loss: 0.3270048499107361
train_iter_loss: 0.2666272222995758
train_iter_loss: 0.3641761541366577
train_iter_loss: 0.4553188383579254
train_iter_loss: 0.2714151442050934
train_iter_loss: 0.4453785717487335
train_iter_loss: 0.3356199264526367
train_iter_loss: 0.4320598840713501
train_iter_loss: 0.29035520553588867
train_iter_loss: 0.2564622163772583
train_iter_loss: 0.37764233350753784
train_iter_loss: 0.3610171675682068
train_iter_loss: 0.27677643299102783
train_iter_loss: 0.27465739846229553
train_iter_loss: 0.2324948012828827
train_iter_loss: 0.17508558928966522
train_iter_loss: 0.3431454598903656
train_iter_loss: 0.33705955743789673
train_iter_loss: 0.3037799596786499
train_iter_loss: 0.13870741426944733
train_iter_loss: 0.26401132345199585
train_iter_loss: 0.2699841558933258
train_iter_loss: 0.28235918283462524
train_iter_loss: 0.33676618337631226
train_iter_loss: 0.25178688764572144
train_iter_loss: 0.30490031838417053
train_iter_loss: 0.29686230421066284
train_iter_loss: 0.19882725179195404
train_iter_loss: 0.37142500281333923
train_iter_loss: 0.23100531101226807
train_iter_loss: 0.36143040657043457
train_iter_loss: 0.28070688247680664
train_iter_loss: 0.4657418727874756
train_iter_loss: 0.23025991022586823
train_iter_loss: 0.20865219831466675
train_iter_loss: 0.2864589989185333
train_iter_loss: 0.426078736782074
train_iter_loss: 0.1522366851568222
train loss :0.3018
---------------------
Validation seg loss: 0.38599806957228006 at epoch 112
epoch =    113/  1000, exp = train
train_iter_loss: 0.32168102264404297
train_iter_loss: 0.06970047205686569
train_iter_loss: 0.27164432406425476
train_iter_loss: 0.39497873187065125
train_iter_loss: 0.2849605977535248
train_iter_loss: 0.20160330832004547
train_iter_loss: 0.344314843416214
train_iter_loss: 0.43982353806495667
train_iter_loss: 0.20043696463108063
train_iter_loss: 0.2226567417383194
train_iter_loss: 0.36765146255493164
train_iter_loss: 0.23966363072395325
train_iter_loss: 0.2520451545715332
train_iter_loss: 0.4438190460205078
train_iter_loss: 0.21876728534698486
train_iter_loss: 0.34643587470054626
train_iter_loss: 0.4216267466545105
train_iter_loss: 0.3270299732685089
train_iter_loss: 0.3325018584728241
train_iter_loss: 0.3827357590198517
train_iter_loss: 0.26929160952568054
train_iter_loss: 0.41511431336402893
train_iter_loss: 0.4072953462600708
train_iter_loss: 0.28640803694725037
train_iter_loss: 0.24300383031368256
train_iter_loss: 0.3728509545326233
train_iter_loss: 0.1828281432390213
train_iter_loss: 0.21837380528450012
train_iter_loss: 0.39023450016975403
train_iter_loss: 0.10845129936933517
train_iter_loss: 0.3098025321960449
train_iter_loss: 0.2878133952617645
train_iter_loss: 0.34927138686180115
train_iter_loss: 0.17185768485069275
train_iter_loss: 0.28012537956237793
train_iter_loss: 0.32122284173965454
train_iter_loss: 0.4993542432785034
train_iter_loss: 0.20427703857421875
train_iter_loss: 0.14812728762626648
train_iter_loss: 0.24387986958026886
train_iter_loss: 0.25405558943748474
train_iter_loss: 0.31620776653289795
train_iter_loss: 0.3353235423564911
train_iter_loss: 0.2361941635608673
train_iter_loss: 0.558676540851593
train_iter_loss: 0.4115951657295227
train_iter_loss: 0.37763145565986633
train_iter_loss: 0.3492293953895569
train_iter_loss: 0.4195899963378906
train_iter_loss: 0.25673940777778625
train_iter_loss: 0.2366662472486496
train_iter_loss: 0.4559062421321869
train_iter_loss: 0.17821000516414642
train_iter_loss: 0.2812161147594452
train_iter_loss: 0.19837337732315063
train_iter_loss: 0.40069177746772766
train_iter_loss: 0.3273565173149109
train_iter_loss: 0.33003148436546326
train_iter_loss: 0.2945081889629364
train_iter_loss: 0.2963888645172119
train_iter_loss: 0.3527982532978058
train_iter_loss: 0.27281561493873596
train_iter_loss: 0.4549999237060547
train_iter_loss: 0.2701233923435211
train_iter_loss: 0.2642170488834381
train_iter_loss: 0.3909057378768921
train_iter_loss: 0.5054017305374146
train_iter_loss: 0.39323776960372925
train_iter_loss: 0.2881997227668762
train_iter_loss: 0.23471342027187347
train_iter_loss: 0.37877097725868225
train_iter_loss: 0.28170275688171387
train_iter_loss: 0.2846513092517853
train_iter_loss: 0.4038758873939514
train_iter_loss: 0.255083829164505
train_iter_loss: 0.24260732531547546
train_iter_loss: 0.20722368359565735
train_iter_loss: 0.32733353972435
train_iter_loss: 0.21951650083065033
train_iter_loss: 0.28339746594429016
train_iter_loss: 0.22659988701343536
train_iter_loss: 0.3683941066265106
train_iter_loss: 0.27677786350250244
train_iter_loss: 0.1443178355693817
train_iter_loss: 0.4070604145526886
train_iter_loss: 0.23268283903598785
train_iter_loss: 0.212315171957016
train_iter_loss: 0.4847800135612488
train_iter_loss: 0.4366014897823334
train_iter_loss: 0.11452142149209976
train_iter_loss: 0.20314748585224152
train_iter_loss: 0.23688553273677826
train_iter_loss: 0.419856995344162
train_iter_loss: 0.2210826426744461
train_iter_loss: 0.43853214383125305
train_iter_loss: 0.2833596467971802
train_iter_loss: 0.3748001754283905
train_iter_loss: 0.26610350608825684
train_iter_loss: 0.3748012185096741
train_iter_loss: 0.25011759996414185
train loss :0.3089
---------------------
Validation seg loss: 0.37000232929680144 at epoch 113
********************
best_val_epoch_loss:  0.37000232929680144
MODEL UPDATED
epoch =    114/  1000, exp = train
train_iter_loss: 0.2772393524646759
train_iter_loss: 0.2228085845708847
train_iter_loss: 0.4195409119129181
train_iter_loss: 0.19583198428153992
train_iter_loss: 0.3230731785297394
train_iter_loss: 0.4574345350265503
train_iter_loss: 0.1931808739900589
train_iter_loss: 0.15520930290222168
train_iter_loss: 0.4225079119205475
train_iter_loss: 0.3451465666294098
train_iter_loss: 0.4151545763015747
train_iter_loss: 0.31007102131843567
train_iter_loss: 0.3540726900100708
train_iter_loss: 0.31158918142318726
train_iter_loss: 0.14806175231933594
train_iter_loss: 0.229878768324852
train_iter_loss: 0.22944515943527222
train_iter_loss: 0.1602831780910492
train_iter_loss: 0.38414278626441956
train_iter_loss: 0.3852511942386627
train_iter_loss: 0.2626133859157562
train_iter_loss: 0.47493603825569153
train_iter_loss: 0.25040706992149353
train_iter_loss: 0.3724711537361145
train_iter_loss: 0.28257596492767334
train_iter_loss: 0.28071022033691406
train_iter_loss: 0.3267788290977478
train_iter_loss: 0.2496783435344696
train_iter_loss: 0.17841218411922455
train_iter_loss: 0.28568217158317566
train_iter_loss: 0.2516093850135803
train_iter_loss: 0.29444146156311035
train_iter_loss: 0.3560338616371155
train_iter_loss: 0.5497061610221863
train_iter_loss: 0.36417651176452637
train_iter_loss: 0.2993871569633484
train_iter_loss: 0.4050484001636505
train_iter_loss: 0.2237553894519806
train_iter_loss: 0.24490348994731903
train_iter_loss: 0.4876372218132019
train_iter_loss: 0.2161533087491989
train_iter_loss: 0.30365145206451416
train_iter_loss: 0.28584736585617065
train_iter_loss: 0.29207155108451843
train_iter_loss: 0.28087180852890015
train_iter_loss: 0.23856864869594574
train_iter_loss: 0.2705617845058441
train_iter_loss: 0.5143740773200989
train_iter_loss: 0.3247280716896057
train_iter_loss: 0.19381040334701538
train_iter_loss: 0.3593966066837311
train_iter_loss: 0.3880910277366638
train_iter_loss: 0.41705310344696045
train_iter_loss: 0.24879802763462067
train_iter_loss: 0.16384047269821167
train_iter_loss: 0.2984684705734253
train_iter_loss: 0.5661801695823669
train_iter_loss: 0.3507049083709717
train_iter_loss: 0.4439280033111572
train_iter_loss: 0.3187411427497864
train_iter_loss: 0.2658631503582001
train_iter_loss: 0.3558175563812256
train_iter_loss: 0.2583705484867096
train_iter_loss: 0.40859782695770264
train_iter_loss: 0.20277561247348785
train_iter_loss: 0.4022199213504791
train_iter_loss: 0.42463475465774536
train_iter_loss: 0.25662797689437866
train_iter_loss: 0.2458629012107849
train_iter_loss: 0.21258634328842163
train_iter_loss: 0.14048124849796295
train_iter_loss: 0.3333098292350769
train_iter_loss: 0.2961253821849823
train_iter_loss: 0.28198954463005066
train_iter_loss: 0.37186893820762634
train_iter_loss: 0.3105405569076538
train_iter_loss: 0.17358198761940002
train_iter_loss: 0.20629945397377014
train_iter_loss: 0.27511224150657654
train_iter_loss: 0.23392973840236664
train_iter_loss: 0.28051522374153137
train_iter_loss: 0.2014271318912506
train_iter_loss: 0.35187435150146484
train_iter_loss: 0.2277272343635559
train_iter_loss: 0.29009804129600525
train_iter_loss: 0.29725539684295654
train_iter_loss: 0.462047278881073
train_iter_loss: 0.2590177357196808
train_iter_loss: 0.20134197175502777
train_iter_loss: 0.17011402547359467
train_iter_loss: 0.505548894405365
train_iter_loss: 0.25950345396995544
train_iter_loss: 0.23303787410259247
train_iter_loss: 0.5841295719146729
train_iter_loss: 0.32640743255615234
train_iter_loss: 0.252380907535553
train_iter_loss: 0.3009381890296936
train_iter_loss: 0.20694966614246368
train_iter_loss: 0.3459906280040741
train_iter_loss: 0.30675020813941956
train loss :0.3089
---------------------
Validation seg loss: 0.3969849167602523 at epoch 114
epoch =    115/  1000, exp = train
train_iter_loss: 0.24396322667598724
train_iter_loss: 0.25640302896499634
train_iter_loss: 0.4211990237236023
train_iter_loss: 0.36073681712150574
train_iter_loss: 0.2942451536655426
train_iter_loss: 0.43000417947769165
train_iter_loss: 0.36246466636657715
train_iter_loss: 0.31529784202575684
train_iter_loss: 0.31689441204071045
train_iter_loss: 0.18062378466129303
train_iter_loss: 0.2892473638057709
train_iter_loss: 0.2242306023836136
train_iter_loss: 0.3077809512615204
train_iter_loss: 0.3762797713279724
train_iter_loss: 0.3115185797214508
train_iter_loss: 0.3426499664783478
train_iter_loss: 0.304073691368103
train_iter_loss: 0.23617984354496002
train_iter_loss: 0.48818105459213257
train_iter_loss: 0.2492409199476242
train_iter_loss: 0.37601974606513977
train_iter_loss: 0.18788936734199524
train_iter_loss: 0.2982567250728607
train_iter_loss: 0.3198401629924774
train_iter_loss: 0.298880010843277
train_iter_loss: 0.30178481340408325
train_iter_loss: 0.20432089269161224
train_iter_loss: 0.2224798947572708
train_iter_loss: 0.300104022026062
train_iter_loss: 0.43399524688720703
train_iter_loss: 0.36888381838798523
train_iter_loss: 0.24939343333244324
train_iter_loss: 0.2714199721813202
train_iter_loss: 0.30037587881088257
train_iter_loss: 0.2193099707365036
train_iter_loss: 0.3763180673122406
train_iter_loss: 0.37941300868988037
train_iter_loss: 0.37900349497795105
train_iter_loss: 0.19854356348514557
train_iter_loss: 0.2226964831352234
train_iter_loss: 0.18585848808288574
train_iter_loss: 0.3576270341873169
train_iter_loss: 0.24610202014446259
train_iter_loss: 0.1439964771270752
train_iter_loss: 0.3636336028575897
train_iter_loss: 0.49917271733283997
train_iter_loss: 0.36334067583084106
train_iter_loss: 0.25029051303863525
train_iter_loss: 0.26543161273002625
train_iter_loss: 0.3069058954715729
train_iter_loss: 0.26041874289512634
train_iter_loss: 0.37467122077941895
train_iter_loss: 0.3281434178352356
train_iter_loss: 0.2604925036430359
train_iter_loss: 0.3439226746559143
train_iter_loss: 0.13263902068138123
train_iter_loss: 0.2473200112581253
train_iter_loss: 0.3143942058086395
train_iter_loss: 0.31739112734794617
train_iter_loss: 0.4613020718097687
train_iter_loss: 0.24539875984191895
train_iter_loss: 0.2722957730293274
train_iter_loss: 0.5055122375488281
train_iter_loss: 0.32454919815063477
train_iter_loss: 0.37671518325805664
train_iter_loss: 0.3252767324447632
train_iter_loss: 0.3086920976638794
train_iter_loss: 0.21811164915561676
train_iter_loss: 0.2690032720565796
train_iter_loss: 0.23975305259227753
train_iter_loss: 0.42798495292663574
train_iter_loss: 0.32152941823005676
train_iter_loss: 0.2939130961894989
train_iter_loss: 0.3831310272216797
train_iter_loss: 0.2332710176706314
train_iter_loss: 0.23222145438194275
train_iter_loss: 0.3287977874279022
train_iter_loss: 0.3435342013835907
train_iter_loss: 0.29057279229164124
train_iter_loss: 0.24695642292499542
train_iter_loss: 0.3672812581062317
train_iter_loss: 0.170833021402359
train_iter_loss: 0.26438796520233154
train_iter_loss: 0.29061633348464966
train_iter_loss: 0.25867778062820435
train_iter_loss: 0.302778422832489
train_iter_loss: 0.2299329787492752
train_iter_loss: 0.22122006118297577
train_iter_loss: 0.2465846687555313
train_iter_loss: 0.2754041254520416
train_iter_loss: 0.26914265751838684
train_iter_loss: 0.32599756121635437
train_iter_loss: 0.15889282524585724
train_iter_loss: 0.2815335690975189
train_iter_loss: 0.2442191243171692
train_iter_loss: 0.4192989766597748
train_iter_loss: 0.1515539288520813
train_iter_loss: 0.23340432345867157
train_iter_loss: 0.4018189013004303
train_iter_loss: 0.3108508288860321
train loss :0.3016
---------------------
Validation seg loss: 0.3951731579518824 at epoch 115
epoch =    116/  1000, exp = train
train_iter_loss: 0.3771626055240631
train_iter_loss: 0.34630894660949707
train_iter_loss: 0.27837541699409485
train_iter_loss: 0.24712568521499634
train_iter_loss: 0.16475412249565125
train_iter_loss: 0.4469187259674072
train_iter_loss: 0.34371235966682434
train_iter_loss: 0.3012699484825134
train_iter_loss: 0.23503847420215607
train_iter_loss: 0.27695244550704956
train_iter_loss: 0.3515869379043579
train_iter_loss: 0.21542996168136597
train_iter_loss: 0.32537221908569336
train_iter_loss: 0.23186951875686646
train_iter_loss: 0.2771208584308624
train_iter_loss: 0.3141931891441345
train_iter_loss: 0.3056008517742157
train_iter_loss: 0.2843865752220154
train_iter_loss: 0.3416989743709564
train_iter_loss: 0.32029998302459717
train_iter_loss: 0.23865416646003723
train_iter_loss: 0.3515433371067047
train_iter_loss: 0.28147652745246887
train_iter_loss: 0.5260751843452454
train_iter_loss: 0.25815263390541077
train_iter_loss: 0.20769649744033813
train_iter_loss: 0.29130351543426514
train_iter_loss: 0.31722140312194824
train_iter_loss: 0.29974111914634705
train_iter_loss: 0.2920217216014862
train_iter_loss: 0.36672717332839966
train_iter_loss: 0.2721560597419739
train_iter_loss: 0.32265496253967285
train_iter_loss: 0.4579470753669739
train_iter_loss: 0.227376788854599
train_iter_loss: 0.361150324344635
train_iter_loss: 0.28181368112564087
train_iter_loss: 0.15205822885036469
train_iter_loss: 0.1967713087797165
train_iter_loss: 0.416629821062088
train_iter_loss: 0.2836867570877075
train_iter_loss: 0.3940872251987457
train_iter_loss: 0.28935471177101135
train_iter_loss: 0.2353449910879135
train_iter_loss: 0.4345386326313019
train_iter_loss: 0.2368597835302353
train_iter_loss: 0.303745836019516
train_iter_loss: 0.381117045879364
train_iter_loss: 0.34745660424232483
train_iter_loss: 0.38314998149871826
train_iter_loss: 0.28013160824775696
train_iter_loss: 0.30157145857810974
train_iter_loss: 0.2110738903284073
train_iter_loss: 0.2619718909263611
train_iter_loss: 0.3278599679470062
train_iter_loss: 0.29720261693000793
train_iter_loss: 0.15632642805576324
train_iter_loss: 0.3229968845844269
train_iter_loss: 0.34036365151405334
train_iter_loss: 0.4055972099304199
train_iter_loss: 0.40357136726379395
train_iter_loss: 0.33813124895095825
train_iter_loss: 0.3301210105419159
train_iter_loss: 0.22454732656478882
train_iter_loss: 0.18496209383010864
train_iter_loss: 0.4824695885181427
train_iter_loss: 0.27978062629699707
train_iter_loss: 0.16902880370616913
train_iter_loss: 0.11923093348741531
train_iter_loss: 0.43095946311950684
train_iter_loss: 0.3039701581001282
train_iter_loss: 0.27698326110839844
train_iter_loss: 0.34822991490364075
train_iter_loss: 0.3229532539844513
train_iter_loss: 0.47977253794670105
train_iter_loss: 0.316383957862854
train_iter_loss: 0.2785969376564026
train_iter_loss: 0.4103129804134369
train_iter_loss: 0.26602670550346375
train_iter_loss: 0.31471875309944153
train_iter_loss: 0.5064301490783691
train_iter_loss: 0.17067667841911316
train_iter_loss: 0.22261328995227814
train_iter_loss: 0.21083633601665497
train_iter_loss: 0.24697165191173553
train_iter_loss: 0.35957422852516174
train_iter_loss: 0.2622903883457184
train_iter_loss: 0.3011082112789154
train_iter_loss: 0.2710646688938141
train_iter_loss: 0.31739670038223267
train_iter_loss: 0.2390456199645996
train_iter_loss: 0.2644561529159546
train_iter_loss: 0.3444836139678955
train_iter_loss: 0.29357537627220154
train_iter_loss: 0.2816412150859833
train_iter_loss: 0.19266733527183533
train_iter_loss: 0.3908381462097168
train_iter_loss: 0.31253424286842346
train_iter_loss: 0.37149864435195923
train_iter_loss: 0.15609566867351532
train loss :0.3060
---------------------
Validation seg loss: 0.3987271798915177 at epoch 116
epoch =    117/  1000, exp = train
train_iter_loss: 0.22135110199451447
train_iter_loss: 0.22273121774196625
train_iter_loss: 0.2393789440393448
train_iter_loss: 0.22676457464694977
train_iter_loss: 0.3186032474040985
train_iter_loss: 0.4497382938861847
train_iter_loss: 0.27541711926460266
train_iter_loss: 0.14339828491210938
train_iter_loss: 0.2761721909046173
train_iter_loss: 0.22278857231140137
train_iter_loss: 0.32796722650527954
train_iter_loss: 0.26747944951057434
train_iter_loss: 0.1704598069190979
train_iter_loss: 0.42504188418388367
train_iter_loss: 0.2685113549232483
train_iter_loss: 0.17349238693714142
train_iter_loss: 0.26886603236198425
train_iter_loss: 0.2205708771944046
train_iter_loss: 0.3314986824989319
train_iter_loss: 0.22666217386722565
train_iter_loss: 0.3116852045059204
train_iter_loss: 0.3050755262374878
train_iter_loss: 0.3246684968471527
train_iter_loss: 0.23603923618793488
train_iter_loss: 0.3040981888771057
train_iter_loss: 0.36924290657043457
train_iter_loss: 0.41921892762184143
train_iter_loss: 0.19490210711956024
train_iter_loss: 0.3413010835647583
train_iter_loss: 0.184758260846138
train_iter_loss: 0.2691464126110077
train_iter_loss: 0.23288050293922424
train_iter_loss: 0.31196314096450806
train_iter_loss: 0.25144273042678833
train_iter_loss: 0.47004175186157227
train_iter_loss: 0.22980517148971558
train_iter_loss: 0.4135880470275879
train_iter_loss: 0.27630311250686646
train_iter_loss: 0.3019748628139496
train_iter_loss: 0.3374784588813782
train_iter_loss: 0.4138438403606415
train_iter_loss: 0.2089669108390808
train_iter_loss: 0.314126580953598
train_iter_loss: 0.3036227524280548
train_iter_loss: 0.43745145201683044
train_iter_loss: 0.2533794045448303
train_iter_loss: 0.3712030351161957
train_iter_loss: 0.3413979113101959
train_iter_loss: 0.33876314759254456
train_iter_loss: 0.29483726620674133
train_iter_loss: 0.34190699458122253
train_iter_loss: 0.21978922188282013
train_iter_loss: 0.21075737476348877
train_iter_loss: 0.23508091270923615
train_iter_loss: 0.36919161677360535
train_iter_loss: 0.4742119312286377
train_iter_loss: 0.23991017043590546
train_iter_loss: 0.3296334743499756
train_iter_loss: 0.30341941118240356
train_iter_loss: 0.39482739567756653
train_iter_loss: 0.4241196811199188
train_iter_loss: 0.275540292263031
train_iter_loss: 0.31951892375946045
train_iter_loss: 0.3223700523376465
train_iter_loss: 0.3145909309387207
train_iter_loss: 0.36706334352493286
train_iter_loss: 0.3867424726486206
train_iter_loss: 0.1721886247396469
train_iter_loss: 0.32929810881614685
train_iter_loss: 0.2683927118778229
train_iter_loss: 0.23649179935455322
train_iter_loss: 0.17397163808345795
train_iter_loss: 0.27479010820388794
train_iter_loss: 0.2741604745388031
train_iter_loss: 0.16612078249454498
train_iter_loss: 0.33409345149993896
train_iter_loss: 0.3192618787288666
train_iter_loss: 0.2063872516155243
train_iter_loss: 0.2541215419769287
train_iter_loss: 0.34196197986602783
train_iter_loss: 0.4209527373313904
train_iter_loss: 0.3414357900619507
train_iter_loss: 0.5007048845291138
train_iter_loss: 0.19269442558288574
train_iter_loss: 0.32443442940711975
train_iter_loss: 0.381351113319397
train_iter_loss: 0.2566840648651123
train_iter_loss: 0.2271680384874344
train_iter_loss: 0.3339327573776245
train_iter_loss: 0.34999990463256836
train_iter_loss: 0.24060076475143433
train_iter_loss: 0.3182343542575836
train_iter_loss: 0.37779828906059265
train_iter_loss: 0.34418725967407227
train_iter_loss: 0.2439287006855011
train_iter_loss: 0.3786585032939911
train_iter_loss: 0.2632903456687927
train_iter_loss: 0.283314973115921
train_iter_loss: 0.20760084688663483
train_iter_loss: 0.3235481381416321
train loss :0.3014
---------------------
Validation seg loss: 0.3761705721921797 at epoch 117
epoch =    118/  1000, exp = train
train_iter_loss: 0.30408748984336853
train_iter_loss: 0.2802375853061676
train_iter_loss: 0.22777096927165985
train_iter_loss: 0.2709817588329315
train_iter_loss: 0.2308843433856964
train_iter_loss: 0.22250765562057495
train_iter_loss: 0.3026581108570099
train_iter_loss: 0.4434332549571991
train_iter_loss: 0.17877212166786194
train_iter_loss: 0.5513510704040527
train_iter_loss: 0.322357714176178
train_iter_loss: 0.2600302994251251
train_iter_loss: 0.2922651767730713
train_iter_loss: 0.2621746063232422
train_iter_loss: 0.2736949324607849
train_iter_loss: 0.28186941146850586
train_iter_loss: 0.22767378389835358
train_iter_loss: 0.2885606288909912
train_iter_loss: 0.24191568791866302
train_iter_loss: 0.2997448146343231
train_iter_loss: 0.39469051361083984
train_iter_loss: 0.2802544832229614
train_iter_loss: 0.18691442906856537
train_iter_loss: 0.21779298782348633
train_iter_loss: 0.4101303815841675
train_iter_loss: 0.25516077876091003
train_iter_loss: 0.10676831752061844
train_iter_loss: 0.32417452335357666
train_iter_loss: 0.349710613489151
train_iter_loss: 0.19618037343025208
train_iter_loss: 0.2488829642534256
train_iter_loss: 0.24196122586727142
train_iter_loss: 0.21235080063343048
train_iter_loss: 0.3917054235935211
train_iter_loss: 0.23139959573745728
train_iter_loss: 0.5083593726158142
train_iter_loss: 0.31292447447776794
train_iter_loss: 0.26680049300193787
train_iter_loss: 0.3294210433959961
train_iter_loss: 0.21425248682498932
train_iter_loss: 0.2946222722530365
train_iter_loss: 0.2788803279399872
train_iter_loss: 0.41074690222740173
train_iter_loss: 0.30303704738616943
train_iter_loss: 0.35419586300849915
train_iter_loss: 0.23019829392433167
train_iter_loss: 0.40520086884498596
train_iter_loss: 0.34686756134033203
train_iter_loss: 0.3743633031845093
train_iter_loss: 0.37823808193206787
train_iter_loss: 0.24062615633010864
train_iter_loss: 0.3016102910041809
train_iter_loss: 0.2997419536113739
train_iter_loss: 0.22397811710834503
train_iter_loss: 0.18825043737888336
train_iter_loss: 0.17532820999622345
train_iter_loss: 0.21033509075641632
train_iter_loss: 0.1436825543642044
train_iter_loss: 0.1769438087940216
train_iter_loss: 0.42214086651802063
train_iter_loss: 0.34394052624702454
train_iter_loss: 0.31372565031051636
train_iter_loss: 0.31353145837783813
train_iter_loss: 0.2513444423675537
train_iter_loss: 0.47165048122406006
train_iter_loss: 0.21755878627300262
train_iter_loss: 0.38088175654411316
train_iter_loss: 0.223293274641037
train_iter_loss: 0.3308694362640381
train_iter_loss: 0.30006206035614014
train_iter_loss: 0.2905273735523224
train_iter_loss: 0.3039076030254364
train_iter_loss: 0.3118578791618347
train_iter_loss: 0.44623854756355286
train_iter_loss: 0.23857417702674866
train_iter_loss: 0.3665877878665924
train_iter_loss: 0.4012638032436371
train_iter_loss: 0.16322845220565796
train_iter_loss: 0.1889614462852478
train_iter_loss: 0.23286126554012299
train_iter_loss: 0.3918626010417938
train_iter_loss: 0.32792261242866516
train_iter_loss: 0.2787242829799652
train_iter_loss: 0.4618978202342987
train_iter_loss: 0.4176463782787323
train_iter_loss: 0.4183361530303955
train_iter_loss: 0.25822100043296814
train_iter_loss: 0.34425434470176697
train_iter_loss: 0.32254722714424133
train_iter_loss: 0.17473256587982178
train_iter_loss: 0.3149181604385376
train_iter_loss: 0.34065431356430054
train_iter_loss: 0.2556881904602051
train_iter_loss: 0.2584626078605652
train_iter_loss: 0.26108911633491516
train_iter_loss: 0.49009016156196594
train_iter_loss: 0.3079766631126404
train_iter_loss: 0.2874210476875305
train_iter_loss: 0.5654076337814331
train_iter_loss: 0.3373648524284363
train loss :0.3041
---------------------
Validation seg loss: 0.40057423963861644 at epoch 118
epoch =    119/  1000, exp = train
train_iter_loss: 0.15162041783332825
train_iter_loss: 0.2722279131412506
train_iter_loss: 0.33482155203819275
train_iter_loss: 0.2989361584186554
train_iter_loss: 0.19500517845153809
train_iter_loss: 0.3819040358066559
train_iter_loss: 0.22128932178020477
train_iter_loss: 0.4862268567085266
train_iter_loss: 0.4051932394504547
train_iter_loss: 0.32988056540489197
train_iter_loss: 0.31823208928108215
train_iter_loss: 0.3028669059276581
train_iter_loss: 0.37249937653541565
train_iter_loss: 0.22509093582630157
train_iter_loss: 0.37461555004119873
train_iter_loss: 0.22508414089679718
train_iter_loss: 0.5192841291427612
train_iter_loss: 0.2591972351074219
train_iter_loss: 0.2901580333709717
train_iter_loss: 0.327998548746109
train_iter_loss: 0.3540584444999695
train_iter_loss: 0.391746461391449
train_iter_loss: 0.37083688378334045
train_iter_loss: 0.282010018825531
train_iter_loss: 0.247171089053154
train_iter_loss: 0.3758154511451721
train_iter_loss: 0.49897500872612
train_iter_loss: 0.38369759917259216
train_iter_loss: 0.3686223030090332
train_iter_loss: 0.3039741516113281
train_iter_loss: 0.25437286496162415
train_iter_loss: 0.4188383221626282
train_iter_loss: 0.4323236644268036
train_iter_loss: 0.4653787910938263
train_iter_loss: 0.19059191644191742
train_iter_loss: 0.2653060257434845
train_iter_loss: 0.21700160205364227
train_iter_loss: 0.14972899854183197
train_iter_loss: 0.22869069874286652
train_iter_loss: 0.4145847260951996
train_iter_loss: 0.2596529424190521
train_iter_loss: 0.18214288353919983
train_iter_loss: 0.2362806499004364
train_iter_loss: 0.30994436144828796
train_iter_loss: 0.25033870339393616
train_iter_loss: 0.40137556195259094
train_iter_loss: 0.2836417853832245
train_iter_loss: 0.35087156295776367
train_iter_loss: 0.20393265783786774
train_iter_loss: 0.20774544775485992
train_iter_loss: 0.29559287428855896
train_iter_loss: 0.39430293440818787
train_iter_loss: 0.2590535283088684
train_iter_loss: 0.350077360868454
train_iter_loss: 0.3013954162597656
train_iter_loss: 0.2622927129268646
train_iter_loss: 0.33137503266334534
train_iter_loss: 0.35289865732192993
train_iter_loss: 0.2643280029296875
train_iter_loss: 0.17512944340705872
train_iter_loss: 0.34636324644088745
train_iter_loss: 0.2821044623851776
train_iter_loss: 0.39519405364990234
train_iter_loss: 0.3580130934715271
train_iter_loss: 0.2265850007534027
train_iter_loss: 0.2795092761516571
train_iter_loss: 0.3340185284614563
train_iter_loss: 0.3040953576564789
train_iter_loss: 0.20203785598278046
train_iter_loss: 0.25551605224609375
train_iter_loss: 0.3739278018474579
train_iter_loss: 0.22707024216651917
train_iter_loss: 0.36793988943099976
train_iter_loss: 0.32711178064346313
train_iter_loss: 0.28245818614959717
train_iter_loss: 0.3920711576938629
train_iter_loss: 0.28745973110198975
train_iter_loss: 0.34736451506614685
train_iter_loss: 0.1359187811613083
train_iter_loss: 0.2718430459499359
train_iter_loss: 0.22158361971378326
train_iter_loss: 0.18488062918186188
train_iter_loss: 0.16651590168476105
train_iter_loss: 0.28304851055145264
train_iter_loss: 0.33596113324165344
train_iter_loss: 0.2565966844558716
train_iter_loss: 0.3473237454891205
train_iter_loss: 0.26906490325927734
train_iter_loss: 0.10276582837104797
train_iter_loss: 0.3716156780719757
train_iter_loss: 0.3037666082382202
train_iter_loss: 0.2654806971549988
train_iter_loss: 0.3124541640281677
train_iter_loss: 0.3198695182800293
train_iter_loss: 0.2898508906364441
train_iter_loss: 0.2588171064853668
train_iter_loss: 0.41382139921188354
train_iter_loss: 0.23741327226161957
train_iter_loss: 0.26947492361068726
train_iter_loss: 0.2380494624376297
train loss :0.3033
---------------------
Validation seg loss: 0.3860253971597215 at epoch 119
epoch =    120/  1000, exp = train
train_iter_loss: 0.25307297706604004
train_iter_loss: 0.2382894605398178
train_iter_loss: 0.1721157729625702
train_iter_loss: 0.3357508182525635
train_iter_loss: 0.29620495438575745
train_iter_loss: 0.3724398612976074
train_iter_loss: 0.26152315735816956
train_iter_loss: 0.1375425159931183
train_iter_loss: 0.2848378121852875
train_iter_loss: 0.28124985098838806
train_iter_loss: 0.22406813502311707
train_iter_loss: 0.3014836609363556
train_iter_loss: 0.27715709805488586
train_iter_loss: 0.3549090027809143
train_iter_loss: 0.16057300567626953
train_iter_loss: 0.17885653674602509
train_iter_loss: 0.2340112179517746
train_iter_loss: 0.2388586550951004
train_iter_loss: 0.35449108481407166
train_iter_loss: 0.33815011382102966
train_iter_loss: 0.4016023278236389
train_iter_loss: 0.28662142157554626
train_iter_loss: 0.4297320544719696
train_iter_loss: 0.1614772230386734
train_iter_loss: 0.1806919425725937
train_iter_loss: 0.2848127782344818
train_iter_loss: 0.1750238835811615
train_iter_loss: 0.22729039192199707
train_iter_loss: 0.21363966166973114
train_iter_loss: 0.271005243062973
train_iter_loss: 0.44444283843040466
train_iter_loss: 0.3909907639026642
train_iter_loss: 0.1405874490737915
train_iter_loss: 0.23300310969352722
train_iter_loss: 0.4426904618740082
train_iter_loss: 0.24600893259048462
train_iter_loss: 0.23949219286441803
train_iter_loss: 0.407417356967926
train_iter_loss: 0.40928125381469727
train_iter_loss: 0.245441734790802
train_iter_loss: 0.29060861468315125
train_iter_loss: 0.28202080726623535
train_iter_loss: 0.4826924204826355
train_iter_loss: 0.28981122374534607
train_iter_loss: 0.41878390312194824
train_iter_loss: 0.3693007826805115
train_iter_loss: 0.30053380131721497
train_iter_loss: 0.11416344344615936
train_iter_loss: 0.5080674886703491
train_iter_loss: 0.27224814891815186
train_iter_loss: 0.44407761096954346
train_iter_loss: 0.36641937494277954
train_iter_loss: 0.3054666817188263
train_iter_loss: 0.40720459818840027
train_iter_loss: 0.2845861315727234
train_iter_loss: 0.3109932243824005
train_iter_loss: 0.13230116665363312
train_iter_loss: 0.32954147458076477
train_iter_loss: 0.3064650893211365
train_iter_loss: 0.19276228547096252
train_iter_loss: 0.17032895982265472
train_iter_loss: 0.4224875271320343
train_iter_loss: 0.39758777618408203
train_iter_loss: 0.3644096553325653
train_iter_loss: 0.17181351780891418
train_iter_loss: 0.38763031363487244
train_iter_loss: 0.22926019132137299
train_iter_loss: 0.19973742961883545
train_iter_loss: 0.22617453336715698
train_iter_loss: 0.30476143956184387
train_iter_loss: 0.2756451368331909
train_iter_loss: 0.20794984698295593
train_iter_loss: 0.21681252121925354
train_iter_loss: 0.37962859869003296
train_iter_loss: 0.2924939692020416
train_iter_loss: 0.2407945692539215
train_iter_loss: 0.20415621995925903
train_iter_loss: 0.28347882628440857
train_iter_loss: 0.2472953498363495
train_iter_loss: 0.35336780548095703
train_iter_loss: 0.26815149188041687
train_iter_loss: 0.26376867294311523
train_iter_loss: 0.4681270718574524
train_iter_loss: 0.36498603224754333
train_iter_loss: 0.3771665394306183
train_iter_loss: 0.29000136256217957
train_iter_loss: 0.22404508292675018
train_iter_loss: 0.30314624309539795
train_iter_loss: 0.37999406456947327
train_iter_loss: 0.31166496872901917
train_iter_loss: 0.3231942355632782
train_iter_loss: 0.2841056287288666
train_iter_loss: 0.38457345962524414
train_iter_loss: 0.41667965054512024
train_iter_loss: 0.29051661491394043
train_iter_loss: 0.37911078333854675
train_iter_loss: 0.24770283699035645
train_iter_loss: 0.1737198829650879
train_iter_loss: 0.47706228494644165
train_iter_loss: 0.3060323894023895
train loss :0.2995
---------------------
Validation seg loss: 0.3799994604591772 at epoch 120
epoch =    121/  1000, exp = train
train_iter_loss: 0.2176552712917328
train_iter_loss: 0.34205394983291626
train_iter_loss: 0.322220116853714
train_iter_loss: 0.28901171684265137
train_iter_loss: 0.3698092997074127
train_iter_loss: 0.17719432711601257
train_iter_loss: 0.27766862511634827
train_iter_loss: 0.24759359657764435
train_iter_loss: 0.3357018530368805
train_iter_loss: 0.40626659989356995
train_iter_loss: 0.3262263238430023
train_iter_loss: 0.3863256275653839
train_iter_loss: 0.294357568025589
train_iter_loss: 0.35140472650527954
train_iter_loss: 0.24626699090003967
train_iter_loss: 0.29102128744125366
train_iter_loss: 0.13014942407608032
train_iter_loss: 0.35034966468811035
train_iter_loss: 0.33882617950439453
train_iter_loss: 0.2983578145503998
train_iter_loss: 0.3879965841770172
train_iter_loss: 0.14892368018627167
train_iter_loss: 0.39117228984832764
train_iter_loss: 0.28303274512290955
train_iter_loss: 0.1972074955701828
train_iter_loss: 0.3574880063533783
train_iter_loss: 0.36242473125457764
train_iter_loss: 0.35103416442871094
train_iter_loss: 0.9326348304748535
train_iter_loss: 0.2869931757450104
train_iter_loss: 0.32768240571022034
train_iter_loss: 0.3163149058818817
train_iter_loss: 0.2017880082130432
train_iter_loss: 0.3350922167301178
train_iter_loss: 0.31492626667022705
train_iter_loss: 0.25371843576431274
train_iter_loss: 0.33488523960113525
train_iter_loss: 0.5950707793235779
train_iter_loss: 0.30164873600006104
train_iter_loss: 0.252517968416214
train_iter_loss: 0.32616156339645386
train_iter_loss: 0.3577893376350403
train_iter_loss: 0.4806394875049591
train_iter_loss: 0.27800458669662476
train_iter_loss: 0.42078956961631775
train_iter_loss: 0.39822256565093994
train_iter_loss: 0.27648022770881653
train_iter_loss: 0.17837446928024292
train_iter_loss: 0.2618858814239502
train_iter_loss: 0.16575205326080322
train_iter_loss: 0.34656378626823425
train_iter_loss: 0.2844795286655426
train_iter_loss: 0.3195491433143616
train_iter_loss: 0.3070722222328186
train_iter_loss: 0.34259602427482605
train_iter_loss: 0.38469424843788147
train_iter_loss: 0.17337492108345032
train_iter_loss: 0.26463034749031067
train_iter_loss: 0.3066912591457367
train_iter_loss: 0.3077855110168457
train_iter_loss: 0.1807851940393448
train_iter_loss: 0.2740894854068756
train_iter_loss: 0.35109809041023254
train_iter_loss: 0.31561198830604553
train_iter_loss: 0.13875342905521393
train_iter_loss: 0.3125026822090149
train_iter_loss: 0.25104570388793945
train_iter_loss: 0.1752859652042389
train_iter_loss: 0.32839471101760864
train_iter_loss: 0.37845608592033386
train_iter_loss: 0.3271613121032715
train_iter_loss: 0.25491413474082947
train_iter_loss: 0.3452633023262024
train_iter_loss: 0.24538277089595795
train_iter_loss: 0.10998441278934479
train_iter_loss: 0.3814437687397003
train_iter_loss: 0.2396414428949356
train_iter_loss: 0.36686933040618896
train_iter_loss: 0.37694770097732544
train_iter_loss: 0.2525319755077362
train_iter_loss: 0.15871450304985046
train_iter_loss: 0.41625016927719116
train_iter_loss: 0.23435550928115845
train_iter_loss: 0.20582999289035797
train_iter_loss: 0.5250732898712158
train_iter_loss: 0.2975590229034424
train_iter_loss: 0.13069970905780792
train_iter_loss: 0.2837418019771576
train_iter_loss: 0.4148125946521759
train_iter_loss: 0.3098076581954956
train_iter_loss: 0.38739344477653503
train_iter_loss: 0.2394856959581375
train_iter_loss: 0.28853848576545715
train_iter_loss: 0.32837262749671936
train_iter_loss: 0.35756057500839233
train_iter_loss: 0.34234949946403503
train_iter_loss: 0.36030468344688416
train_iter_loss: 0.17848585546016693
train_iter_loss: 0.3803234398365021
train_iter_loss: 0.30267199873924255
train loss :0.3116
---------------------
Validation seg loss: 0.3925583531886761 at epoch 121
epoch =    122/  1000, exp = train
train_iter_loss: 0.26637065410614014
train_iter_loss: 0.24016211926937103
train_iter_loss: 0.2908998727798462
train_iter_loss: 0.32308706641197205
train_iter_loss: 0.31554925441741943
train_iter_loss: 0.37760284543037415
train_iter_loss: 0.24775364995002747
train_iter_loss: 0.4624321162700653
train_iter_loss: 0.5321840643882751
train_iter_loss: 0.24402838945388794
train_iter_loss: 0.3804745376110077
train_iter_loss: 0.23562674224376678
train_iter_loss: 0.13368381559848785
train_iter_loss: 0.23900511860847473
train_iter_loss: 0.2692987024784088
train_iter_loss: 0.3688174784183502
train_iter_loss: 0.2729240953922272
train_iter_loss: 0.3079487979412079
train_iter_loss: 0.2598513960838318
train_iter_loss: 0.17433860898017883
train_iter_loss: 0.2671565115451813
train_iter_loss: 0.3074418902397156
train_iter_loss: 0.24551671743392944
train_iter_loss: 0.3139757215976715
train_iter_loss: 0.29284998774528503
train_iter_loss: 0.2161274254322052
train_iter_loss: 0.362409383058548
train_iter_loss: 0.2234201282262802
train_iter_loss: 0.3740961253643036
train_iter_loss: 0.2820616662502289
train_iter_loss: 0.3866513669490814
train_iter_loss: 0.3416768014431
train_iter_loss: 0.3654131591320038
train_iter_loss: 0.3178323209285736
train_iter_loss: 0.24419085681438446
train_iter_loss: 0.31027859449386597
train_iter_loss: 0.20158173143863678
train_iter_loss: 0.3644466698169708
train_iter_loss: 0.3986426293849945
train_iter_loss: 0.40780138969421387
train_iter_loss: 0.16163916885852814
train_iter_loss: 0.20861122012138367
train_iter_loss: 0.22320668399333954
train_iter_loss: 0.27821701765060425
train_iter_loss: 0.320789098739624
train_iter_loss: 0.19075541198253632
train_iter_loss: 0.276798278093338
train_iter_loss: 0.34169700741767883
train_iter_loss: 0.2917141318321228
train_iter_loss: 0.3522171080112457
train_iter_loss: 0.2586345970630646
train_iter_loss: 0.29254889488220215
train_iter_loss: 0.16537557542324066
train_iter_loss: 0.2180849313735962
train_iter_loss: 0.3968803286552429
train_iter_loss: 0.5666056871414185
train_iter_loss: 0.3823390305042267
train_iter_loss: 0.2979702949523926
train_iter_loss: 0.21249127388000488
train_iter_loss: 0.2568151652812958
train_iter_loss: 0.1970042586326599
train_iter_loss: 0.3425666391849518
train_iter_loss: 0.3286021649837494
train_iter_loss: 0.20673653483390808
train_iter_loss: 0.23292283713817596
train_iter_loss: 0.3788573741912842
train_iter_loss: 0.4061220586299896
train_iter_loss: 0.4136582612991333
train_iter_loss: 0.2959248125553131
train_iter_loss: 0.22601549327373505
train_iter_loss: 0.2688665986061096
train_iter_loss: 0.280972421169281
train_iter_loss: 0.25704023241996765
train_iter_loss: 0.23595120012760162
train_iter_loss: 0.19432565569877625
train_iter_loss: 0.23852787911891937
train_iter_loss: 0.31195372343063354
train_iter_loss: 0.3283456265926361
train_iter_loss: 0.35919398069381714
train_iter_loss: 0.32861873507499695
train_iter_loss: 0.27697286009788513
train_iter_loss: 0.16434285044670105
train_iter_loss: 0.34190961718559265
train_iter_loss: 0.40250155329704285
train_iter_loss: 0.39143234491348267
train_iter_loss: 0.3414691984653473
train_iter_loss: 0.18864260613918304
train_iter_loss: 0.3049546778202057
train_iter_loss: 0.40870991349220276
train_iter_loss: 0.2831287086009979
train_iter_loss: 0.2515186071395874
train_iter_loss: 0.2911390960216522
train_iter_loss: 0.23005416989326477
train_iter_loss: 0.4009960889816284
train_iter_loss: 0.20803441107273102
train_iter_loss: 0.23142829537391663
train_iter_loss: 0.3395542800426483
train_iter_loss: 0.2168910652399063
train_iter_loss: 0.22238211333751678
train_iter_loss: 0.2806223928928375
train loss :0.2978
---------------------
Validation seg loss: 0.37978545455086343 at epoch 122
epoch =    123/  1000, exp = train
train_iter_loss: 0.36746248602867126
train_iter_loss: 0.529037356376648
train_iter_loss: 0.19814564287662506
train_iter_loss: 0.2569270133972168
train_iter_loss: 0.4101373553276062
train_iter_loss: 0.3794482946395874
train_iter_loss: 0.27413588762283325
train_iter_loss: 0.2192188948392868
train_iter_loss: 0.20140917599201202
train_iter_loss: 0.26073572039604187
train_iter_loss: 0.3465507924556732
train_iter_loss: 0.26310521364212036
train_iter_loss: 0.29522615671157837
train_iter_loss: 0.29343098402023315
train_iter_loss: 0.33247295022010803
train_iter_loss: 0.2503391206264496
train_iter_loss: 0.26922154426574707
train_iter_loss: 0.23140954971313477
train_iter_loss: 0.20994365215301514
train_iter_loss: 0.2514267861843109
train_iter_loss: 0.13001787662506104
train_iter_loss: 0.2965332865715027
train_iter_loss: 0.2373766154050827
train_iter_loss: 0.3682234287261963
train_iter_loss: 0.16449220478534698
train_iter_loss: 0.31863853335380554
train_iter_loss: 0.22169287502765656
train_iter_loss: 0.27500826120376587
train_iter_loss: 0.3624403476715088
train_iter_loss: 0.3868280053138733
train_iter_loss: 0.33946022391319275
train_iter_loss: 0.3251417279243469
train_iter_loss: 0.36241671442985535
train_iter_loss: 0.29368311166763306
train_iter_loss: 0.3126518428325653
train_iter_loss: 0.15676027536392212
train_iter_loss: 0.08910444378852844
train_iter_loss: 0.29402685165405273
train_iter_loss: 0.23990938067436218
train_iter_loss: 0.25346478819847107
train_iter_loss: 0.3411296308040619
train_iter_loss: 0.37170255184173584
train_iter_loss: 0.3104594051837921
train_iter_loss: 0.17491842806339264
train_iter_loss: 0.33215948939323425
train_iter_loss: 0.4448091387748718
train_iter_loss: 0.3062730133533478
train_iter_loss: 0.29522597789764404
train_iter_loss: 0.16479577124118805
train_iter_loss: 0.41697198152542114
train_iter_loss: 0.2535504400730133
train_iter_loss: 0.42698875069618225
train_iter_loss: 0.32384923100471497
train_iter_loss: 0.2812587022781372
train_iter_loss: 0.41885316371917725
train_iter_loss: 0.4430405795574188
train_iter_loss: 0.3036106824874878
train_iter_loss: 0.1854870617389679
train_iter_loss: 0.48617982864379883
train_iter_loss: 0.24010580778121948
train_iter_loss: 0.29136213660240173
train_iter_loss: 0.3689832091331482
train_iter_loss: 0.22753474116325378
train_iter_loss: 0.2533334195613861
train_iter_loss: 0.27016156911849976
train_iter_loss: 0.3363378643989563
train_iter_loss: 0.22958171367645264
train_iter_loss: 0.3556115925312042
train_iter_loss: 0.34383684396743774
train_iter_loss: 0.23061563074588776
train_iter_loss: 0.38023969531059265
train_iter_loss: 0.3076649308204651
train_iter_loss: 0.18873611092567444
train_iter_loss: 0.2896678149700165
train_iter_loss: 0.18922320008277893
train_iter_loss: 0.27715611457824707
train_iter_loss: 0.2339913249015808
train_iter_loss: 0.33117103576660156
train_iter_loss: 0.2397340089082718
train_iter_loss: 0.18018999695777893
train_iter_loss: 0.25996047258377075
train_iter_loss: 0.2998355031013489
train_iter_loss: 0.3548062741756439
train_iter_loss: 0.1763148456811905
train_iter_loss: 0.3237822651863098
train_iter_loss: 0.32099124789237976
train_iter_loss: 0.5060467720031738
train_iter_loss: 0.2954707443714142
train_iter_loss: 0.28386515378952026
train_iter_loss: 0.2523972988128662
train_iter_loss: 0.19307900965213776
train_iter_loss: 0.28925013542175293
train_iter_loss: 0.3652951717376709
train_iter_loss: 0.4287961721420288
train_iter_loss: 0.16567422449588776
train_iter_loss: 0.33966147899627686
train_iter_loss: 0.4066525101661682
train_iter_loss: 0.3301704227924347
train_iter_loss: 0.33765360713005066
train_iter_loss: 0.3613331913948059
train loss :0.2990
---------------------
Validation seg loss: 0.3923081809935986 at epoch 123
epoch =    124/  1000, exp = train
train_iter_loss: 0.24760065972805023
train_iter_loss: 0.3873756527900696
train_iter_loss: 0.22168265283107758
train_iter_loss: 0.1300453543663025
train_iter_loss: 0.3415015637874603
train_iter_loss: 0.28276437520980835
train_iter_loss: 0.28688716888427734
train_iter_loss: 0.2764805257320404
train_iter_loss: 0.29808130860328674
train_iter_loss: 0.35682472586631775
train_iter_loss: 0.24187394976615906
train_iter_loss: 0.19390922784805298
train_iter_loss: 0.3117796778678894
train_iter_loss: 0.18272702395915985
train_iter_loss: 0.3633592426776886
train_iter_loss: 0.31983280181884766
train_iter_loss: 0.19013871252536774
train_iter_loss: 0.4090845286846161
train_iter_loss: 0.2538190484046936
train_iter_loss: 0.2300916165113449
train_iter_loss: 0.2699034512042999
train_iter_loss: 0.11475520581007004
train_iter_loss: 0.210526704788208
train_iter_loss: 0.4147759974002838
train_iter_loss: 0.2341829091310501
train_iter_loss: 0.126521497964859
train_iter_loss: 0.2008490413427353
train_iter_loss: 0.2587653398513794
train_iter_loss: 0.4632898271083832
train_iter_loss: 0.46625152230262756
train_iter_loss: 0.37845566868782043
train_iter_loss: 0.31150081753730774
train_iter_loss: 0.2660728096961975
train_iter_loss: 0.3338789939880371
train_iter_loss: 0.28600773215293884
train_iter_loss: 0.27044060826301575
train_iter_loss: 0.2946424186229706
train_iter_loss: 0.32056108117103577
train_iter_loss: 0.14149697124958038
train_iter_loss: 0.2550531029701233
train_iter_loss: 0.3865955173969269
train_iter_loss: 0.153045654296875
train_iter_loss: 0.39155885577201843
train_iter_loss: 0.23994016647338867
train_iter_loss: 0.21620270609855652
train_iter_loss: 0.43391165137290955
train_iter_loss: 0.2872270941734314
train_iter_loss: 0.3202916085720062
train_iter_loss: 0.40548598766326904
train_iter_loss: 0.11930759251117706
train_iter_loss: 0.32014065980911255
train_iter_loss: 0.4356743097305298
train_iter_loss: 0.3373377025127411
train_iter_loss: 0.3494773805141449
train_iter_loss: 0.250051349401474
train_iter_loss: 0.3421308994293213
train_iter_loss: 0.265886515378952
train_iter_loss: 0.3283724784851074
train_iter_loss: 0.14642897248268127
train_iter_loss: 0.4276992082595825
train_iter_loss: 0.27435657382011414
train_iter_loss: 0.34857186675071716
train_iter_loss: 0.2853470742702484
train_iter_loss: 0.27807486057281494
train_iter_loss: 0.3266294002532959
train_iter_loss: 0.19978420436382294
train_iter_loss: 0.3891509175300598
train_iter_loss: 0.3727395534515381
train_iter_loss: 0.19957788288593292
train_iter_loss: 0.2601430416107178
train_iter_loss: 0.23273292183876038
train_iter_loss: 0.3181525766849518
train_iter_loss: 0.30565568804740906
train_iter_loss: 0.374330997467041
train_iter_loss: 0.2609558701515198
train_iter_loss: 0.26635241508483887
train_iter_loss: 0.22450855374336243
train_iter_loss: 0.45331981778144836
train_iter_loss: 0.22525396943092346
train_iter_loss: 0.23050911724567413
train_iter_loss: 0.2676585018634796
train_iter_loss: 0.2417968213558197
train_iter_loss: 0.32578667998313904
train_iter_loss: 0.43010109663009644
train_iter_loss: 0.2767091989517212
train_iter_loss: 0.4607801139354706
train_iter_loss: 0.3894786536693573
train_iter_loss: 0.5443817973136902
train_iter_loss: 0.2928144037723541
train_iter_loss: 0.2666507065296173
train_iter_loss: 0.29432278871536255
train_iter_loss: 0.3158493638038635
train_iter_loss: 0.3726138472557068
train_iter_loss: 0.25036635994911194
train_iter_loss: 0.3368099331855774
train_iter_loss: 0.2868882417678833
train_iter_loss: 0.27671051025390625
train_iter_loss: 0.31066665053367615
train_iter_loss: 0.33234846591949463
train_iter_loss: 0.12238297611474991
train loss :0.2982
---------------------
Validation seg loss: 0.3664925853844802 at epoch 124
********************
best_val_epoch_loss:  0.3664925853844802
MODEL UPDATED
epoch =    125/  1000, exp = train
train_iter_loss: 0.27571651339530945
train_iter_loss: 0.2803804278373718
train_iter_loss: 0.4418033957481384
train_iter_loss: 0.3055034875869751
train_iter_loss: 0.18195740878582
train_iter_loss: 0.2960347831249237
train_iter_loss: 0.2319631725549698
train_iter_loss: 0.32851138710975647
train_iter_loss: 0.29336538910865784
train_iter_loss: 0.26736417412757874
train_iter_loss: 0.24914944171905518
train_iter_loss: 0.366349458694458
train_iter_loss: 0.21705971658229828
train_iter_loss: 0.1513296365737915
train_iter_loss: 0.21140915155410767
train_iter_loss: 0.383862167596817
train_iter_loss: 0.16714079678058624
train_iter_loss: 0.430574506521225
train_iter_loss: 0.5569823384284973
train_iter_loss: 0.30882158875465393
train_iter_loss: 0.17760711908340454
train_iter_loss: 0.2396342009305954
train_iter_loss: 0.2970309853553772
train_iter_loss: 0.4037648141384125
train_iter_loss: 0.16764231026172638
train_iter_loss: 0.4281030297279358
train_iter_loss: 0.310708224773407
train_iter_loss: 0.3585340678691864
train_iter_loss: 0.304159939289093
train_iter_loss: 0.2825562655925751
train_iter_loss: 0.29856598377227783
train_iter_loss: 0.271831750869751
train_iter_loss: 0.2480335682630539
train_iter_loss: 0.2833048105239868
train_iter_loss: 0.4029325842857361
train_iter_loss: 0.26356610655784607
train_iter_loss: 0.24015933275222778
train_iter_loss: 0.2607611119747162
train_iter_loss: 0.2465113252401352
train_iter_loss: 0.2837291657924652
train_iter_loss: 0.2673056721687317
train_iter_loss: 0.14794781804084778
train_iter_loss: 0.3068009316921234
train_iter_loss: 0.4833551347255707
train_iter_loss: 0.33746469020843506
train_iter_loss: 0.28721705079078674
train_iter_loss: 0.12574194371700287
train_iter_loss: 0.3628103733062744
train_iter_loss: 0.48060542345046997
train_iter_loss: 0.3404760956764221
train_iter_loss: 0.2761096954345703
train_iter_loss: 0.26646333932876587
train_iter_loss: 0.17885324358940125
train_iter_loss: 0.3952270746231079
train_iter_loss: 0.39240139722824097
train_iter_loss: 0.3194044530391693
train_iter_loss: 0.3907245099544525
train_iter_loss: 0.359855055809021
train_iter_loss: 0.3405039310455322
train_iter_loss: 0.3343171179294586
train_iter_loss: 0.2926386594772339
train_iter_loss: 0.19777649641036987
train_iter_loss: 0.12108249217271805
train_iter_loss: 0.40580856800079346
train_iter_loss: 0.2550111711025238
train_iter_loss: 0.16749413311481476
train_iter_loss: 0.3588283956050873
train_iter_loss: 0.17974594235420227
train_iter_loss: 0.3553411662578583
train_iter_loss: 0.2341754138469696
train_iter_loss: 0.29363539814949036
train_iter_loss: 0.327217161655426
train_iter_loss: 0.31203126907348633
train_iter_loss: 0.2617455720901489
train_iter_loss: 0.38555464148521423
train_iter_loss: 0.2186788022518158
train_iter_loss: 0.23469941318035126
train_iter_loss: 0.23001857101917267
train_iter_loss: 0.2559240162372589
train_iter_loss: 0.23868484795093536
train_iter_loss: 0.37448519468307495
train_iter_loss: 0.25119879841804504
train_iter_loss: 0.2947535216808319
train_iter_loss: 0.43963146209716797
train_iter_loss: 0.3770354390144348
train_iter_loss: 0.2122116982936859
train_iter_loss: 0.28840020298957825
train_iter_loss: 0.3389691114425659
train_iter_loss: 0.26906123757362366
train_iter_loss: 0.36971792578697205
train_iter_loss: 0.22417329251766205
train_iter_loss: 0.47589626908302307
train_iter_loss: 0.3678480386734009
train_iter_loss: 0.13919049501419067
train_iter_loss: 0.2307606190443039
train_iter_loss: 0.33769339323043823
train_iter_loss: 0.34977227449417114
train_iter_loss: 0.28566208481788635
train_iter_loss: 0.35454261302948
train_iter_loss: 0.10654324293136597
train loss :0.2982
---------------------
Validation seg loss: 0.38208237682240753 at epoch 125
epoch =    126/  1000, exp = train
train_iter_loss: 0.2831534445285797
train_iter_loss: 0.34109067916870117
train_iter_loss: 0.22274847328662872
train_iter_loss: 0.19662219285964966
train_iter_loss: 0.34091711044311523
train_iter_loss: 0.4450274109840393
train_iter_loss: 0.26162979006767273
train_iter_loss: 0.1837722659111023
train_iter_loss: 0.35245653986930847
train_iter_loss: 0.2794705331325531
train_iter_loss: 0.31847065687179565
train_iter_loss: 0.26263025403022766
train_iter_loss: 0.37473228573799133
train_iter_loss: 0.3311626613140106
train_iter_loss: 0.4271289110183716
train_iter_loss: 0.408820778131485
train_iter_loss: 0.365671843290329
train_iter_loss: 0.3240853548049927
train_iter_loss: 0.3139837980270386
train_iter_loss: 0.3757775127887726
train_iter_loss: 0.2849339246749878
train_iter_loss: 0.3356194496154785
train_iter_loss: 0.32098308205604553
train_iter_loss: 0.18487556278705597
train_iter_loss: 0.3067132830619812
train_iter_loss: 0.15053190290927887
train_iter_loss: 0.46593815088272095
train_iter_loss: 0.33084964752197266
train_iter_loss: 0.20654335618019104
train_iter_loss: 0.266451358795166
train_iter_loss: 0.18781156837940216
train_iter_loss: 0.273471474647522
train_iter_loss: 0.2528241276741028
train_iter_loss: 0.19083939492702484
train_iter_loss: 0.43595439195632935
train_iter_loss: 0.23454347252845764
train_iter_loss: 0.2676367461681366
train_iter_loss: 0.30839699506759644
train_iter_loss: 0.3245537579059601
train_iter_loss: 0.2542659640312195
train_iter_loss: 0.36181628704071045
train_iter_loss: 0.3252577483654022
train_iter_loss: 0.2603111267089844
train_iter_loss: 0.1805800199508667
train_iter_loss: 0.40045884251594543
train_iter_loss: 0.24904105067253113
train_iter_loss: 0.27051669359207153
train_iter_loss: 0.2487199902534485
train_iter_loss: 0.3379935920238495
train_iter_loss: 0.16832877695560455
train_iter_loss: 0.17515574395656586
train_iter_loss: 0.32733094692230225
train_iter_loss: 0.21955958008766174
train_iter_loss: 0.3017300069332123
train_iter_loss: 0.35657548904418945
train_iter_loss: 0.3165365159511566
train_iter_loss: 0.22084063291549683
train_iter_loss: 0.20322155952453613
train_iter_loss: 0.6320967674255371
train_iter_loss: 0.3674626350402832
train_iter_loss: 0.285139262676239
train_iter_loss: 0.1713213324546814
train_iter_loss: 0.23479698598384857
train_iter_loss: 0.3261123597621918
train_iter_loss: 0.21129748225212097
train_iter_loss: 0.23565208911895752
train_iter_loss: 0.3563562333583832
train_iter_loss: 0.29768529534339905
train_iter_loss: 0.257172167301178
train_iter_loss: 0.2733801305294037
train_iter_loss: 0.21761919558048248
train_iter_loss: 0.383841872215271
train_iter_loss: 0.24673448503017426
train_iter_loss: 0.25328993797302246
train_iter_loss: 0.27175018191337585
train_iter_loss: 0.2528787851333618
train_iter_loss: 0.24527513980865479
train_iter_loss: 0.3769626021385193
train_iter_loss: 0.2966609001159668
train_iter_loss: 0.271091490983963
train_iter_loss: 0.3187493085861206
train_iter_loss: 0.17067110538482666
train_iter_loss: 0.3263631761074066
train_iter_loss: 0.30605629086494446
train_iter_loss: 0.2374316304922104
train_iter_loss: 0.2579304873943329
train_iter_loss: 0.29993972182273865
train_iter_loss: 0.24716094136238098
train_iter_loss: 0.24711069464683533
train_iter_loss: 0.2831287086009979
train_iter_loss: 0.29535287618637085
train_iter_loss: 0.1908237338066101
train_iter_loss: 0.302489310503006
train_iter_loss: 0.2396911233663559
train_iter_loss: 0.2350110560655594
train_iter_loss: 0.22037039697170258
train_iter_loss: 0.4743382930755615
train_iter_loss: 0.38660097122192383
train_iter_loss: 0.2108444720506668
train_iter_loss: 0.5272724032402039
train loss :0.2946
---------------------
Validation seg loss: 0.38580615560190296 at epoch 126
epoch =    127/  1000, exp = train
train_iter_loss: 0.3411027193069458
train_iter_loss: 0.23642675578594208
train_iter_loss: 0.21096733212471008
train_iter_loss: 0.34319785237312317
train_iter_loss: 0.3192428946495056
train_iter_loss: 0.26520538330078125
train_iter_loss: 0.3479968011379242
train_iter_loss: 0.37466198205947876
train_iter_loss: 0.276208758354187
train_iter_loss: 0.14235439896583557
train_iter_loss: 0.3579398989677429
train_iter_loss: 0.3913702964782715
train_iter_loss: 0.4087125062942505
train_iter_loss: 0.24395816028118134
train_iter_loss: 0.3033114969730377
train_iter_loss: 0.21248938143253326
train_iter_loss: 0.36037710309028625
train_iter_loss: 0.20367126166820526
train_iter_loss: 0.20377862453460693
train_iter_loss: 0.5373181104660034
train_iter_loss: 0.2770703434944153
train_iter_loss: 0.2649388909339905
train_iter_loss: 0.214285209774971
train_iter_loss: 0.36219334602355957
train_iter_loss: 0.21400925517082214
train_iter_loss: 0.248806431889534
train_iter_loss: 0.23422281444072723
train_iter_loss: 0.3921325206756592
train_iter_loss: 0.39110854268074036
train_iter_loss: 0.4523686170578003
train_iter_loss: 0.26733437180519104
train_iter_loss: 0.3104510009288788
train_iter_loss: 0.3327074944972992
train_iter_loss: 0.30834445357322693
train_iter_loss: 0.29436519742012024
train_iter_loss: 0.28133952617645264
train_iter_loss: 0.22703005373477936
train_iter_loss: 0.4085843861103058
train_iter_loss: 0.21103693544864655
train_iter_loss: 0.3591707944869995
train_iter_loss: 0.2734062373638153
train_iter_loss: 0.2873103618621826
train_iter_loss: 0.16811299324035645
train_iter_loss: 0.2814125120639801
train_iter_loss: 0.3246132731437683
train_iter_loss: 0.13845093548297882
train_iter_loss: 0.2405216097831726
train_iter_loss: 0.2395218461751938
train_iter_loss: 0.3273361027240753
train_iter_loss: 0.222515270113945
train_iter_loss: 0.23408439755439758
train_iter_loss: 0.34170594811439514
train_iter_loss: 0.3268599510192871
train_iter_loss: 0.32680925726890564
train_iter_loss: 0.3129153549671173
train_iter_loss: 0.27735453844070435
train_iter_loss: 0.25426530838012695
train_iter_loss: 0.48990386724472046
train_iter_loss: 0.3216852843761444
train_iter_loss: 0.2443370670080185
train_iter_loss: 0.1887967586517334
train_iter_loss: 0.2067272812128067
train_iter_loss: 0.3416668176651001
train_iter_loss: 0.24511867761611938
train_iter_loss: 0.22601565718650818
train_iter_loss: 0.28234145045280457
train_iter_loss: 0.45660218596458435
train_iter_loss: 0.23938605189323425
train_iter_loss: 0.3839581608772278
train_iter_loss: 0.192006915807724
train_iter_loss: 0.23656660318374634
train_iter_loss: 0.39676493406295776
train_iter_loss: 0.3058982491493225
train_iter_loss: 0.2428659200668335
train_iter_loss: 0.3067352771759033
train_iter_loss: 0.14065420627593994
train_iter_loss: 0.43838363885879517
train_iter_loss: 0.2677226662635803
train_iter_loss: 0.19620667397975922
train_iter_loss: 0.31109556555747986
train_iter_loss: 0.2853682339191437
train_iter_loss: 0.2163260132074356
train_iter_loss: 0.528373122215271
train_iter_loss: 0.3965561091899872
train_iter_loss: 0.2530766427516937
train_iter_loss: 0.3825160264968872
train_iter_loss: 0.2598955035209656
train_iter_loss: 0.2901726961135864
train_iter_loss: 0.26304757595062256
train_iter_loss: 0.4294414222240448
train_iter_loss: 0.3368462920188904
train_iter_loss: 0.2549915909767151
train_iter_loss: 0.32240939140319824
train_iter_loss: 0.3750391900539398
train_iter_loss: 0.2483142763376236
train_iter_loss: 0.266944944858551
train_iter_loss: 0.2653294503688812
train_iter_loss: 0.4149898588657379
train_iter_loss: 0.20791135728359222
train_iter_loss: 0.21777163445949554
train loss :0.2988
---------------------
Validation seg loss: 0.4002095527841516 at epoch 127
epoch =    128/  1000, exp = train
train_iter_loss: 0.325499951839447
train_iter_loss: 0.2545701265335083
train_iter_loss: 0.41340896487236023
train_iter_loss: 0.3216124475002289
train_iter_loss: 0.24135316908359528
train_iter_loss: 0.29685649275779724
train_iter_loss: 0.3186045289039612
train_iter_loss: 0.3376619815826416
train_iter_loss: 0.2370460331439972
train_iter_loss: 0.2422509491443634
train_iter_loss: 0.25703516602516174
train_iter_loss: 0.26504504680633545
train_iter_loss: 0.29441332817077637
train_iter_loss: 0.7387746572494507
train_iter_loss: 0.3633745312690735
train_iter_loss: 0.3666919767856598
train_iter_loss: 0.22017915546894073
train_iter_loss: 0.39618566632270813
train_iter_loss: 0.33949771523475647
train_iter_loss: 0.36156341433525085
train_iter_loss: 0.1890496462583542
train_iter_loss: 0.22342802584171295
train_iter_loss: 0.1965959072113037
train_iter_loss: 0.46637994050979614
train_iter_loss: 0.29827457666397095
train_iter_loss: 0.27243906259536743
train_iter_loss: 0.35646653175354004
train_iter_loss: 0.16265732049942017
train_iter_loss: 0.32745102047920227
train_iter_loss: 0.3128315210342407
train_iter_loss: 0.2874338924884796
train_iter_loss: 0.27944156527519226
train_iter_loss: 0.30298322439193726
train_iter_loss: 0.21471859514713287
train_iter_loss: 0.3314313590526581
train_iter_loss: 0.3351106643676758
train_iter_loss: 0.28988975286483765
train_iter_loss: 0.36079806089401245
train_iter_loss: 0.423152357339859
train_iter_loss: 0.3868826627731323
train_iter_loss: 0.3753136992454529
train_iter_loss: 0.2784205973148346
train_iter_loss: 0.4969964921474457
train_iter_loss: 0.360302597284317
train_iter_loss: 0.31563758850097656
train_iter_loss: 0.3779037892818451
train_iter_loss: 0.24562792479991913
train_iter_loss: 0.33158162236213684
train_iter_loss: 0.2121855467557907
train_iter_loss: 0.34386005997657776
train_iter_loss: 0.2640749514102936
train_iter_loss: 0.22125279903411865
train_iter_loss: 0.17832477390766144
train_iter_loss: 0.2686851918697357
train_iter_loss: 0.2988230586051941
train_iter_loss: 0.3747081756591797
train_iter_loss: 0.3242439031600952
train_iter_loss: 0.18519249558448792
train_iter_loss: 0.3300511837005615
train_iter_loss: 0.2584041357040405
train_iter_loss: 0.25580090284347534
train_iter_loss: 0.2981531322002411
train_iter_loss: 0.24968379735946655
train_iter_loss: 0.22753824293613434
train_iter_loss: 0.07141903787851334
train_iter_loss: 0.24903401732444763
train_iter_loss: 0.2862277925014496
train_iter_loss: 0.27396321296691895
train_iter_loss: 0.24525092542171478
train_iter_loss: 0.3815748691558838
train_iter_loss: 0.27493786811828613
train_iter_loss: 0.32152628898620605
train_iter_loss: 0.3898761570453644
train_iter_loss: 0.17798085510730743
train_iter_loss: 0.3037163317203522
train_iter_loss: 0.4515742063522339
train_iter_loss: 0.31451624631881714
train_iter_loss: 0.3020133674144745
train_iter_loss: 0.2944765090942383
train_iter_loss: 0.33038148283958435
train_iter_loss: 0.260317862033844
train_iter_loss: 0.5302960872650146
train_iter_loss: 0.28861114382743835
train_iter_loss: 0.22600793838500977
train_iter_loss: 0.3268736004829407
train_iter_loss: 0.3013094365596771
train_iter_loss: 0.24798355996608734
train_iter_loss: 0.21891576051712036
train_iter_loss: 0.2770056426525116
train_iter_loss: 0.2465147227048874
train_iter_loss: 0.268597811460495
train_iter_loss: 0.22918052971363068
train_iter_loss: 0.36378201842308044
train_iter_loss: 0.2639424502849579
train_iter_loss: 0.315771222114563
train_iter_loss: 0.304919570684433
train_iter_loss: 0.36397114396095276
train_iter_loss: 0.27219483256340027
train_iter_loss: 0.24049171805381775
train_iter_loss: 0.27673542499542236
train loss :0.3047
---------------------
Validation seg loss: 0.3710137855366997 at epoch 128
epoch =    129/  1000, exp = train
train_iter_loss: 0.5020728707313538
train_iter_loss: 0.17747946083545685
train_iter_loss: 0.1896539032459259
train_iter_loss: 0.2620657980442047
train_iter_loss: 0.1888967603445053
train_iter_loss: 0.29528582096099854
train_iter_loss: 0.24928657710552216
train_iter_loss: 0.2216133028268814
train_iter_loss: 0.24814195930957794
train_iter_loss: 0.3467790484428406
train_iter_loss: 0.17965537309646606
train_iter_loss: 0.21566638350486755
train_iter_loss: 0.2639145255088806
train_iter_loss: 0.42284950613975525
train_iter_loss: 0.3767104148864746
train_iter_loss: 0.2598699927330017
train_iter_loss: 0.29651588201522827
train_iter_loss: 0.28919535875320435
train_iter_loss: 0.4229559302330017
train_iter_loss: 0.15467806160449982
train_iter_loss: 0.3677055239677429
train_iter_loss: 0.2502264678478241
train_iter_loss: 0.23856380581855774
train_iter_loss: 0.20979972183704376
train_iter_loss: 0.30563655495643616
train_iter_loss: 0.3205432891845703
train_iter_loss: 0.2370975911617279
train_iter_loss: 0.5731359124183655
train_iter_loss: 0.33491531014442444
train_iter_loss: 0.20907075703144073
train_iter_loss: 0.11924117058515549
train_iter_loss: 0.28356480598449707
train_iter_loss: 0.28388333320617676
train_iter_loss: 0.22841261327266693
train_iter_loss: 0.5023059844970703
train_iter_loss: 0.3587791621685028
train_iter_loss: 0.15962815284729004
train_iter_loss: 0.31034865975379944
train_iter_loss: 0.2744518220424652
train_iter_loss: 0.5276337265968323
train_iter_loss: 0.3023236095905304
train_iter_loss: 0.3537101745605469
train_iter_loss: 0.2784459590911865
train_iter_loss: 0.6439764499664307
train_iter_loss: 0.3207440972328186
train_iter_loss: 0.18192097544670105
train_iter_loss: 0.21555134654045105
train_iter_loss: 0.35139554738998413
train_iter_loss: 0.40189328789711
train_iter_loss: 0.3745098412036896
train_iter_loss: 0.3116178810596466
train_iter_loss: 0.2087748646736145
train_iter_loss: 0.1542249470949173
train_iter_loss: 0.3844710886478424
train_iter_loss: 0.27723750472068787
train_iter_loss: 0.29624977707862854
train_iter_loss: 0.21205787360668182
train_iter_loss: 0.22241072356700897
train_iter_loss: 0.3103870153427124
train_iter_loss: 0.17316068708896637
train_iter_loss: 0.28319409489631653
train_iter_loss: 0.2530672252178192
train_iter_loss: 0.32444021105766296
train_iter_loss: 0.16749858856201172
train_iter_loss: 0.33514514565467834
train_iter_loss: 0.2393212765455246
train_iter_loss: 0.35320672392845154
train_iter_loss: 0.38297319412231445
train_iter_loss: 0.30010607838630676
train_iter_loss: 0.21087582409381866
train_iter_loss: 0.23319678008556366
train_iter_loss: 0.12122108787298203
train_iter_loss: 0.34124282002449036
train_iter_loss: 0.20518599450588226
train_iter_loss: 0.30713677406311035
train_iter_loss: 0.31449830532073975
train_iter_loss: 0.30589500069618225
train_iter_loss: 0.18484459817409515
train_iter_loss: 0.36565884947776794
train_iter_loss: 0.32945898175239563
train_iter_loss: 0.17528481781482697
train_iter_loss: 0.2959696054458618
train_iter_loss: 0.28217601776123047
train_iter_loss: 0.17318540811538696
train_iter_loss: 0.35228580236434937
train_iter_loss: 0.24649760127067566
train_iter_loss: 0.5161179900169373
train_iter_loss: 0.33394476771354675
train_iter_loss: 0.3849431574344635
train_iter_loss: 0.22244893014431
train_iter_loss: 0.3208279609680176
train_iter_loss: 0.3748326897621155
train_iter_loss: 0.2933502793312073
train_iter_loss: 0.2659897208213806
train_iter_loss: 0.3488210439682007
train_iter_loss: 0.3628196120262146
train_iter_loss: 0.40384089946746826
train_iter_loss: 0.21543358266353607
train_iter_loss: 0.14769691228866577
train_iter_loss: 0.23085638880729675
train loss :0.2944
---------------------
Validation seg loss: 0.3719857824298571 at epoch 129
epoch =    130/  1000, exp = train
train_iter_loss: 0.18804647028446198
train_iter_loss: 0.3233889043331146
train_iter_loss: 0.43452516198158264
train_iter_loss: 0.22384706139564514
train_iter_loss: 0.41626331210136414
train_iter_loss: 0.18833093345165253
train_iter_loss: 0.25978341698646545
train_iter_loss: 0.28762900829315186
train_iter_loss: 0.14438040554523468
train_iter_loss: 0.33983033895492554
train_iter_loss: 0.19864089787006378
train_iter_loss: 0.37409910559654236
train_iter_loss: 0.31715598702430725
train_iter_loss: 0.33338984847068787
train_iter_loss: 0.4009908437728882
train_iter_loss: 0.24887964129447937
train_iter_loss: 0.2747425436973572
train_iter_loss: 0.22065164148807526
train_iter_loss: 0.18114492297172546
train_iter_loss: 0.36083561182022095
train_iter_loss: 0.3574252128601074
train_iter_loss: 0.23911194503307343
train_iter_loss: 0.4376020133495331
train_iter_loss: 0.33181968331336975
train_iter_loss: 0.32880303263664246
train_iter_loss: 0.29795926809310913
train_iter_loss: 0.45598500967025757
train_iter_loss: 0.21944573521614075
train_iter_loss: 0.41147491335868835
train_iter_loss: 0.22458496689796448
train_iter_loss: 0.3569716513156891
train_iter_loss: 0.4113791584968567
train_iter_loss: 0.3399732708930969
train_iter_loss: 0.27710020542144775
train_iter_loss: 0.2177530825138092
train_iter_loss: 0.11126107722520828
train_iter_loss: 0.3766400218009949
train_iter_loss: 0.39216357469558716
train_iter_loss: 0.4444509744644165
train_iter_loss: 0.2485351711511612
train_iter_loss: 0.2952430248260498
train_iter_loss: 0.2362411916255951
train_iter_loss: 0.478318989276886
train_iter_loss: 0.2736303508281708
train_iter_loss: 0.41658297181129456
train_iter_loss: 0.20351696014404297
train_iter_loss: 0.295644074678421
train_iter_loss: 0.2775660455226898
train_iter_loss: 0.23657196760177612
train_iter_loss: 0.2076398730278015
train_iter_loss: 0.3278215229511261
train_iter_loss: 0.24435290694236755
train_iter_loss: 0.3465838134288788
train_iter_loss: 0.33120256662368774
train_iter_loss: 0.22080440819263458
train_iter_loss: 0.07437586784362793
train_iter_loss: 0.3460165560245514
train_iter_loss: 0.15156741440296173
train_iter_loss: 0.3334449827671051
train_iter_loss: 0.3389703631401062
train_iter_loss: 0.6417356729507446
train_iter_loss: 0.38796791434288025
train_iter_loss: 0.26201438903808594
train_iter_loss: 0.20340712368488312
train_iter_loss: 0.233367919921875
train_iter_loss: 0.18258333206176758
train_iter_loss: 0.5439400672912598
train_iter_loss: 0.35474297404289246
train_iter_loss: 0.3225557804107666
train_iter_loss: 0.420320063829422
train_iter_loss: 0.39507460594177246
train_iter_loss: 0.3630763590335846
train_iter_loss: 0.3062640130519867
train_iter_loss: 0.2966381907463074
train_iter_loss: 0.45211008191108704
train_iter_loss: 0.24204879999160767
train_iter_loss: 0.26872333884239197
train_iter_loss: 0.3972649872303009
train_iter_loss: 0.36919206380844116
train_iter_loss: 0.40378686785697937
train_iter_loss: 0.35070398449897766
train_iter_loss: 0.4423603415489197
train_iter_loss: 0.19934140145778656
train_iter_loss: 0.2670954465866089
train_iter_loss: 0.30262669920921326
train_iter_loss: 0.2461959570646286
train_iter_loss: 0.3091929852962494
train_iter_loss: 0.33322975039482117
train_iter_loss: 0.33441591262817383
train_iter_loss: 0.18357889354228973
train_iter_loss: 0.20280274748802185
train_iter_loss: 0.3844461739063263
train_iter_loss: 0.22502845525741577
train_iter_loss: 0.20398017764091492
train_iter_loss: 0.2346661388874054
train_iter_loss: 0.2427247166633606
train_iter_loss: 0.144565612077713
train_iter_loss: 0.3411538898944855
train_iter_loss: 0.19615794718265533
train_iter_loss: 0.45495301485061646
train loss :0.3077
---------------------
Validation seg loss: 0.3702293323388077 at epoch 130
epoch =    131/  1000, exp = train
train_iter_loss: 0.22279854118824005
train_iter_loss: 0.2822183668613434
train_iter_loss: 0.20258203148841858
train_iter_loss: 0.20394986867904663
train_iter_loss: 0.1456305980682373
train_iter_loss: 0.4506043791770935
train_iter_loss: 0.25002631545066833
train_iter_loss: 0.3879392445087433
train_iter_loss: 0.28581634163856506
train_iter_loss: 0.3106408715248108
train_iter_loss: 0.284865140914917
train_iter_loss: 0.2478787750005722
train_iter_loss: 0.26688918471336365
train_iter_loss: 0.3137550354003906
train_iter_loss: 0.59185791015625
train_iter_loss: 0.2522979974746704
train_iter_loss: 0.40729761123657227
train_iter_loss: 0.17157161235809326
train_iter_loss: 0.1706574708223343
train_iter_loss: 0.2746986448764801
train_iter_loss: 0.23813945055007935
train_iter_loss: 0.24562250077724457
train_iter_loss: 0.18994945287704468
train_iter_loss: 0.33552780747413635
train_iter_loss: 0.3858361542224884
train_iter_loss: 0.2605389952659607
train_iter_loss: 0.2973018288612366
train_iter_loss: 0.3301733434200287
train_iter_loss: 0.13990700244903564
train_iter_loss: 0.2499237209558487
train_iter_loss: 0.29130327701568604
train_iter_loss: 0.6443681120872498
train_iter_loss: 0.2921830713748932
train_iter_loss: 0.25833800435066223
train_iter_loss: 0.384805828332901
train_iter_loss: 0.37773144245147705
train_iter_loss: 0.2758687436580658
train_iter_loss: 0.33251646161079407
train_iter_loss: 0.2973371148109436
train_iter_loss: 0.38083186745643616
train_iter_loss: 0.26357370615005493
train_iter_loss: 0.24071457982063293
train_iter_loss: 0.26297101378440857
train_iter_loss: 0.15855099260807037
train_iter_loss: 0.3395429849624634
train_iter_loss: 0.3270569145679474
train_iter_loss: 0.32015931606292725
train_iter_loss: 0.3166417181491852
train_iter_loss: 0.4903191924095154
train_iter_loss: 0.2011251300573349
train_iter_loss: 0.3229599893093109
train_iter_loss: 0.15009696781635284
train_iter_loss: 0.23278063535690308
train_iter_loss: 0.2798237204551697
train_iter_loss: 0.22546052932739258
train_iter_loss: 0.3290685713291168
train_iter_loss: 0.15139354765415192
train_iter_loss: 0.37181445956230164
train_iter_loss: 0.3162778615951538
train_iter_loss: 0.2679476737976074
train_iter_loss: 0.2592254877090454
train_iter_loss: 0.4585411846637726
train_iter_loss: 0.258150190114975
train_iter_loss: 0.3716600835323334
train_iter_loss: 0.2800230085849762
train_iter_loss: 0.4578949511051178
train_iter_loss: 0.23214216530323029
train_iter_loss: 0.2735614776611328
train_iter_loss: 0.16609124839305878
train_iter_loss: 0.3425144553184509
train_iter_loss: 0.30951425433158875
train_iter_loss: 0.3423897325992584
train_iter_loss: 0.3666035830974579
train_iter_loss: 0.2200087010860443
train_iter_loss: 0.28743454813957214
train_iter_loss: 0.45231184363365173
train_iter_loss: 0.24638214707374573
train_iter_loss: 0.23971782624721527
train_iter_loss: 0.4856950044631958
train_iter_loss: 0.25833866000175476
train_iter_loss: 0.2948755919933319
train_iter_loss: 0.3541640341281891
train_iter_loss: 0.29988789558410645
train_iter_loss: 0.3016464114189148
train_iter_loss: 0.32104209065437317
train_iter_loss: 0.24051454663276672
train_iter_loss: 0.3687038719654083
train_iter_loss: 0.3570736348628998
train_iter_loss: 0.4894495904445648
train_iter_loss: 0.3426469564437866
train_iter_loss: 0.296567440032959
train_iter_loss: 0.2154504954814911
train_iter_loss: 0.23067927360534668
train_iter_loss: 0.19666260480880737
train_iter_loss: 0.4123366177082062
train_iter_loss: 0.25903162360191345
train_iter_loss: 0.2081325799226761
train_iter_loss: 0.3942296802997589
train_iter_loss: 0.3242703378200531
train_iter_loss: 0.21336087584495544
train loss :0.3024
---------------------
Validation seg loss: 0.3875905772987402 at epoch 131
epoch =    132/  1000, exp = train
train_iter_loss: 0.36790555715560913
train_iter_loss: 0.3800109326839447
train_iter_loss: 0.2704629898071289
train_iter_loss: 0.2854163944721222
train_iter_loss: 0.23132255673408508
train_iter_loss: 0.20125673711299896
train_iter_loss: 0.3460272550582886
train_iter_loss: 0.23135308921337128
train_iter_loss: 0.2097783386707306
train_iter_loss: 0.34861359000205994
train_iter_loss: 0.21579742431640625
train_iter_loss: 0.42882469296455383
train_iter_loss: 0.22493983805179596
train_iter_loss: 0.21075372397899628
train_iter_loss: 0.22228336334228516
train_iter_loss: 0.29627642035484314
train_iter_loss: 0.29559046030044556
train_iter_loss: 0.34372466802597046
train_iter_loss: 0.12615397572517395
train_iter_loss: 0.3436618447303772
train_iter_loss: 0.3039339482784271
train_iter_loss: 0.269633948802948
train_iter_loss: 0.22840940952301025
train_iter_loss: 0.27073946595191956
train_iter_loss: 0.3642694056034088
train_iter_loss: 0.2999965250492096
train_iter_loss: 0.21375198662281036
train_iter_loss: 0.27452632784843445
train_iter_loss: 0.39069387316703796
train_iter_loss: 0.32871660590171814
train_iter_loss: 0.24383054673671722
train_iter_loss: 0.26068779826164246
train_iter_loss: 0.403717964887619
train_iter_loss: 0.43794235587120056
train_iter_loss: 0.13064391911029816
train_iter_loss: 0.23104599118232727
train_iter_loss: 0.2478310763835907
train_iter_loss: 0.3407570719718933
train_iter_loss: 0.22649966180324554
train_iter_loss: 0.2570514380931854
train_iter_loss: 0.2238021343946457
train_iter_loss: 0.3700185716152191
train_iter_loss: 0.15771494805812836
train_iter_loss: 0.4118068218231201
train_iter_loss: 0.26690673828125
train_iter_loss: 0.39023613929748535
train_iter_loss: 0.3596058785915375
train_iter_loss: 0.3295469582080841
train_iter_loss: 0.23556797206401825
train_iter_loss: 0.22426870465278625
train_iter_loss: 0.5652920603752136
train_iter_loss: 0.2458696961402893
train_iter_loss: 0.26392269134521484
train_iter_loss: 0.17475959658622742
train_iter_loss: 0.5057843327522278
train_iter_loss: 0.3201921284198761
train_iter_loss: 0.22968727350234985
train_iter_loss: 0.37494170665740967
train_iter_loss: 0.2982192635536194
train_iter_loss: 0.24336972832679749
train_iter_loss: 0.3494842052459717
train_iter_loss: 0.2780349850654602
train_iter_loss: 0.3904172480106354
train_iter_loss: 0.30357909202575684
train_iter_loss: 0.2649841904640198
train_iter_loss: 0.3809867799282074
train_iter_loss: 0.3295678496360779
train_iter_loss: 0.19360199570655823
train_iter_loss: 0.24935953319072723
train_iter_loss: 0.32829585671424866
train_iter_loss: 0.3646523356437683
train_iter_loss: 0.4215342104434967
train_iter_loss: 0.23895849287509918
train_iter_loss: 0.2454773485660553
train_iter_loss: 0.33153897523880005
train_iter_loss: 0.21903064846992493
train_iter_loss: 0.3303114175796509
train_iter_loss: 0.25107693672180176
train_iter_loss: 0.2516951262950897
train_iter_loss: 0.4512304365634918
train_iter_loss: 0.3609621822834015
train_iter_loss: 0.22393707931041718
train_iter_loss: 0.20565593242645264
train_iter_loss: 0.22953636944293976
train_iter_loss: 0.2682608962059021
train_iter_loss: 0.26001226902008057
train_iter_loss: 0.3181006908416748
train_iter_loss: 0.34407323598861694
train_iter_loss: 0.31552696228027344
train_iter_loss: 0.22449365258216858
train_iter_loss: 0.3200048506259918
train_iter_loss: 0.203923761844635
train_iter_loss: 0.35983923077583313
train_iter_loss: 0.4023902714252472
train_iter_loss: 0.3416443169116974
train_iter_loss: 0.30817198753356934
train_iter_loss: 0.23848779499530792
train_iter_loss: 0.21431417763233185
train_iter_loss: 0.4371752440929413
train_iter_loss: 0.23136098682880402
train loss :0.2978
---------------------
Validation seg loss: 0.36507984461649406 at epoch 132
********************
best_val_epoch_loss:  0.36507984461649406
MODEL UPDATED
epoch =    133/  1000, exp = train
train_iter_loss: 0.16551841795444489
train_iter_loss: 0.3587608337402344
train_iter_loss: 0.45302027463912964
train_iter_loss: 0.19973993301391602
train_iter_loss: 0.2775973379611969
train_iter_loss: 0.15445536375045776
train_iter_loss: 0.2670315206050873
train_iter_loss: 0.4221777319908142
train_iter_loss: 0.3317308723926544
train_iter_loss: 0.45963016152381897
train_iter_loss: 0.5266937613487244
train_iter_loss: 0.5406416058540344
train_iter_loss: 0.20975171029567719
train_iter_loss: 0.3002068102359772
train_iter_loss: 0.22649724781513214
train_iter_loss: 0.35437190532684326
train_iter_loss: 0.2104291021823883
train_iter_loss: 0.34174269437789917
train_iter_loss: 0.30450648069381714
train_iter_loss: 0.23694130778312683
train_iter_loss: 0.3166820704936981
train_iter_loss: 0.45801103115081787
train_iter_loss: 0.24820668995380402
train_iter_loss: 0.2474539428949356
train_iter_loss: 0.24103201925754547
train_iter_loss: 0.22703926265239716
train_iter_loss: 0.32448914647102356
train_iter_loss: 0.17399603128433228
train_iter_loss: 0.3714529871940613
train_iter_loss: 0.20292748510837555
train_iter_loss: 0.30199623107910156
train_iter_loss: 0.31287717819213867
train_iter_loss: 0.44852781295776367
train_iter_loss: 0.29487693309783936
train_iter_loss: 0.26317209005355835
train_iter_loss: 0.2385057508945465
train_iter_loss: 0.2525981068611145
train_iter_loss: 0.2292320281267166
train_iter_loss: 0.232709601521492
train_iter_loss: 0.2237202227115631
train_iter_loss: 0.2970237731933594
train_iter_loss: 0.2553638517856598
train_iter_loss: 0.3852672874927521
train_iter_loss: 0.2134140282869339
train_iter_loss: 0.5155202746391296
train_iter_loss: 0.24361710250377655
train_iter_loss: 0.35119280219078064
train_iter_loss: 0.27361297607421875
train_iter_loss: 0.4451889395713806
train_iter_loss: 0.36223721504211426
train_iter_loss: 0.31030240654945374
train_iter_loss: 0.19475769996643066
train_iter_loss: 0.31741783022880554
train_iter_loss: 0.34044498205184937
train_iter_loss: 0.3635414242744446
train_iter_loss: 0.16481798887252808
train_iter_loss: 0.18535006046295166
train_iter_loss: 0.41667020320892334
train_iter_loss: 0.20834557712078094
train_iter_loss: 0.3221745193004608
train_iter_loss: 0.1944774091243744
train_iter_loss: 0.25536811351776123
train_iter_loss: 0.30665650963783264
train_iter_loss: 0.33843061327934265
train_iter_loss: 0.2449725717306137
train_iter_loss: 0.41473743319511414
train_iter_loss: 0.37169149518013
train_iter_loss: 0.3235437870025635
train_iter_loss: 0.199393630027771
train_iter_loss: 0.29191190004348755
train_iter_loss: 0.24314133822917938
train_iter_loss: 0.4680835008621216
train_iter_loss: 0.2532808482646942
train_iter_loss: 0.22388088703155518
train_iter_loss: 0.28041693568229675
train_iter_loss: 0.3206726610660553
train_iter_loss: 0.2649116814136505
train_iter_loss: 0.3121899664402008
train_iter_loss: 0.379138320684433
train_iter_loss: 0.20899921655654907
train_iter_loss: 0.3200003206729889
train_iter_loss: 0.15467020869255066
train_iter_loss: 0.39794114232063293
train_iter_loss: 0.2969060242176056
train_iter_loss: 0.5419354438781738
train_iter_loss: 0.17397134006023407
train_iter_loss: 0.2730141878128052
train_iter_loss: 0.2590225338935852
train_iter_loss: 0.23254279792308807
train_iter_loss: 0.2900467813014984
train_iter_loss: 0.23170538246631622
train_iter_loss: 0.3790333569049835
train_iter_loss: 0.3201914429664612
train_iter_loss: 0.23905491828918457
train_iter_loss: 0.29784488677978516
train_iter_loss: 0.3634687066078186
train_iter_loss: 0.21433785557746887
train_iter_loss: 0.3132312297821045
train_iter_loss: 0.3656767010688782
train_iter_loss: 0.2913677394390106
train loss :0.3027
---------------------
Validation seg loss: 0.39007667590038114 at epoch 133
epoch =    134/  1000, exp = train
train_iter_loss: 0.3916468322277069
train_iter_loss: 0.3961803615093231
train_iter_loss: 0.20620408654212952
train_iter_loss: 0.26253971457481384
train_iter_loss: 0.3624299168586731
train_iter_loss: 0.21739983558654785
train_iter_loss: 0.31200286746025085
train_iter_loss: 0.33157461881637573
train_iter_loss: 0.27739107608795166
train_iter_loss: 0.14443594217300415
train_iter_loss: 0.11820164322853088
train_iter_loss: 0.2875789403915405
train_iter_loss: 0.26434534788131714
train_iter_loss: 0.27109843492507935
train_iter_loss: 0.37134256958961487
train_iter_loss: 0.35258084535598755
train_iter_loss: 0.32206130027770996
train_iter_loss: 0.2925148904323578
train_iter_loss: 0.2893027663230896
train_iter_loss: 0.2811282277107239
train_iter_loss: 0.45679450035095215
train_iter_loss: 0.5292391180992126
train_iter_loss: 0.25675341486930847
train_iter_loss: 0.2883913516998291
train_iter_loss: 0.27985647320747375
train_iter_loss: 0.3950789272785187
train_iter_loss: 0.6305608153343201
train_iter_loss: 0.2815662920475006
train_iter_loss: 0.357707679271698
train_iter_loss: 0.34270599484443665
train_iter_loss: 0.2722051739692688
train_iter_loss: 0.21397458016872406
train_iter_loss: 0.2333468645811081
train_iter_loss: 0.35349738597869873
train_iter_loss: 0.2144482582807541
train_iter_loss: 0.32209905982017517
train_iter_loss: 0.36317428946495056
train_iter_loss: 0.46227240562438965
train_iter_loss: 0.30763736367225647
train_iter_loss: 0.2618580162525177
train_iter_loss: 0.1062578409910202
train_iter_loss: 0.44401228427886963
train_iter_loss: 0.39311426877975464
train_iter_loss: 0.29162299633026123
train_iter_loss: 0.33086737990379333
train_iter_loss: 0.3122999966144562
train_iter_loss: 0.4673473834991455
train_iter_loss: 0.3496245741844177
train_iter_loss: 0.1912633329629898
train_iter_loss: 0.24663887917995453
train_iter_loss: 0.31296414136886597
train_iter_loss: 0.17614930868148804
train_iter_loss: 0.20650996267795563
train_iter_loss: 0.21294642984867096
train_iter_loss: 0.27024394273757935
train_iter_loss: 0.3333436846733093
train_iter_loss: 0.29597949981689453
train_iter_loss: 0.42349499464035034
train_iter_loss: 0.29845762252807617
train_iter_loss: 0.22805026173591614
train_iter_loss: 0.4246782064437866
train_iter_loss: 0.24978236854076385
train_iter_loss: 0.22158490121364594
train_iter_loss: 0.2765003740787506
train_iter_loss: 0.29768824577331543
train_iter_loss: 0.2156909704208374
train_iter_loss: 0.2755907475948334
train_iter_loss: 0.4651361107826233
train_iter_loss: 0.3357601761817932
train_iter_loss: 0.3285396993160248
train_iter_loss: 0.3726416826248169
train_iter_loss: 0.3748505711555481
train_iter_loss: 0.26850372552871704
train_iter_loss: 0.22778460383415222
train_iter_loss: 0.26707687973976135
train_iter_loss: 0.43929338455200195
train_iter_loss: 0.3782779276371002
train_iter_loss: 0.17949141561985016
train_iter_loss: 0.33110085129737854
train_iter_loss: 0.2464691698551178
train_iter_loss: 0.2877194881439209
train_iter_loss: 0.29347342252731323
train_iter_loss: 0.25177958607673645
train_iter_loss: 0.3172638416290283
train_iter_loss: 0.1859959214925766
train_iter_loss: 0.2648586928844452
train_iter_loss: 0.15856939554214478
train_iter_loss: 0.25721096992492676
train_iter_loss: 0.248693585395813
train_iter_loss: 0.3510727286338806
train_iter_loss: 0.309653639793396
train_iter_loss: 0.5145279169082642
train_iter_loss: 0.22006741166114807
train_iter_loss: 0.35393306612968445
train_iter_loss: 0.23157382011413574
train_iter_loss: 0.2580571472644806
train_iter_loss: 0.1562366634607315
train_iter_loss: 0.2611140310764313
train_iter_loss: 0.28004083037376404
train_iter_loss: 0.3610759973526001
train loss :0.3051
---------------------
Validation seg loss: 0.4077670806843155 at epoch 134
epoch =    135/  1000, exp = train
train_iter_loss: 0.38827580213546753
train_iter_loss: 0.22668446600437164
train_iter_loss: 0.36158356070518494
train_iter_loss: 0.2739201784133911
train_iter_loss: 0.3127674460411072
train_iter_loss: 0.1784374862909317
train_iter_loss: 0.2742135226726532
train_iter_loss: 0.3877343535423279
train_iter_loss: 0.22376655042171478
train_iter_loss: 0.3345978558063507
train_iter_loss: 0.22963212430477142
train_iter_loss: 0.37294507026672363
train_iter_loss: 0.46350154280662537
train_iter_loss: 0.20370495319366455
train_iter_loss: 0.21877577900886536
train_iter_loss: 0.31379595398902893
train_iter_loss: 0.2395453155040741
train_iter_loss: 0.2399359494447708
train_iter_loss: 0.2076745629310608
train_iter_loss: 0.2363666146993637
train_iter_loss: 0.21802213788032532
train_iter_loss: 0.25385385751724243
train_iter_loss: 0.34406766295433044
train_iter_loss: 0.3242203891277313
train_iter_loss: 0.20762868225574493
train_iter_loss: 0.3334887623786926
train_iter_loss: 0.30300647020339966
train_iter_loss: 0.33168265223503113
train_iter_loss: 0.4276765286922455
train_iter_loss: 0.3340047299861908
train_iter_loss: 0.1308211237192154
train_iter_loss: 0.274069607257843
train_iter_loss: 0.31560084223747253
train_iter_loss: 0.15991449356079102
train_iter_loss: 0.28580209612846375
train_iter_loss: 0.17803403735160828
train_iter_loss: 0.30684295296669006
train_iter_loss: 0.2171771079301834
train_iter_loss: 0.35838010907173157
train_iter_loss: 0.28824087977409363
train_iter_loss: 0.37666821479797363
train_iter_loss: 0.30561068654060364
train_iter_loss: 0.22929057478904724
train_iter_loss: 0.20704753696918488
train_iter_loss: 0.10347459465265274
train_iter_loss: 0.18263891339302063
train_iter_loss: 0.284525066614151
train_iter_loss: 0.3647845387458801
train_iter_loss: 0.42421090602874756
train_iter_loss: 0.1453738808631897
train_iter_loss: 0.3083072006702423
train_iter_loss: 0.4541752338409424
train_iter_loss: 0.1799108237028122
train_iter_loss: 0.24928271770477295
train_iter_loss: 0.2405269742012024
train_iter_loss: 0.1080547571182251
train_iter_loss: 0.2906011939048767
train_iter_loss: 0.21200987696647644
train_iter_loss: 0.2968393564224243
train_iter_loss: 0.4123004972934723
train_iter_loss: 0.2647880017757416
train_iter_loss: 0.4416763186454773
train_iter_loss: 0.5062513947486877
train_iter_loss: 0.45354071259498596
train_iter_loss: 0.3146730363368988
train_iter_loss: 0.2605617344379425
train_iter_loss: 0.3165772557258606
train_iter_loss: 0.26044711470603943
train_iter_loss: 0.20754031836986542
train_iter_loss: 0.26188549399375916
train_iter_loss: 0.46130436658859253
train_iter_loss: 0.25580155849456787
train_iter_loss: 0.26645490527153015
train_iter_loss: 0.3210751712322235
train_iter_loss: 0.36518150568008423
train_iter_loss: 0.2973465323448181
train_iter_loss: 0.363255113363266
train_iter_loss: 0.3934895694255829
train_iter_loss: 0.32407146692276
train_iter_loss: 0.3345201015472412
train_iter_loss: 0.4211438000202179
train_iter_loss: 0.30504530668258667
train_iter_loss: 0.243305966258049
train_iter_loss: 0.23478995263576508
train_iter_loss: 0.2018929123878479
train_iter_loss: 0.27398744225502014
train_iter_loss: 0.5010862946510315
train_iter_loss: 0.28867819905281067
train_iter_loss: 0.2865917384624481
train_iter_loss: 0.40498441457748413
train_iter_loss: 0.34280484914779663
train_iter_loss: 0.19944587349891663
train_iter_loss: 0.30743205547332764
train_iter_loss: 0.4205477833747864
train_iter_loss: 0.273333340883255
train_iter_loss: 0.23592564463615417
train_iter_loss: 0.15384569764137268
train_iter_loss: 0.176640123128891
train_iter_loss: 0.34789684414863586
train_iter_loss: 0.3542393445968628
train loss :0.2961
---------------------
Validation seg loss: 0.3697895906022135 at epoch 135
epoch =    136/  1000, exp = train
train_iter_loss: 0.19670014083385468
train_iter_loss: 0.19968180358409882
train_iter_loss: 0.23113945126533508
train_iter_loss: 0.15199948847293854
train_iter_loss: 0.28572526574134827
train_iter_loss: 0.4405106008052826
train_iter_loss: 0.19431579113006592
train_iter_loss: 0.27428725361824036
train_iter_loss: 0.4421713948249817
train_iter_loss: 0.2042209506034851
train_iter_loss: 0.23000818490982056
train_iter_loss: 0.28898942470550537
train_iter_loss: 0.25013503432273865
train_iter_loss: 0.17057596147060394
train_iter_loss: 0.4635976552963257
train_iter_loss: 0.3465033769607544
train_iter_loss: 0.2385581135749817
train_iter_loss: 0.2015598863363266
train_iter_loss: 0.3858768939971924
train_iter_loss: 0.3437539041042328
train_iter_loss: 0.20037591457366943
train_iter_loss: 0.24361014366149902
train_iter_loss: 0.38719072937965393
train_iter_loss: 0.18072102963924408
train_iter_loss: 0.15551547706127167
train_iter_loss: 0.24493712186813354
train_iter_loss: 0.2178599238395691
train_iter_loss: 0.21814347803592682
train_iter_loss: 0.32595276832580566
train_iter_loss: 0.2640531659126282
train_iter_loss: 0.31942686438560486
train_iter_loss: 0.21876870095729828
train_iter_loss: 0.3673943281173706
train_iter_loss: 0.21219271421432495
train_iter_loss: 0.32281607389450073
train_iter_loss: 0.25928768515586853
train_iter_loss: 0.20587147772312164
train_iter_loss: 0.42594975233078003
train_iter_loss: 0.1699582040309906
train_iter_loss: 0.3589467406272888
train_iter_loss: 0.20987631380558014
train_iter_loss: 0.39458128809928894
train_iter_loss: 0.31646499037742615
train_iter_loss: 0.4431878328323364
train_iter_loss: 0.4032379984855652
train_iter_loss: 0.358510822057724
train_iter_loss: 0.2900698781013489
train_iter_loss: 0.3777369260787964
train_iter_loss: 0.2613171935081482
train_iter_loss: 0.20655260980129242
train_iter_loss: 0.3237185478210449
train_iter_loss: 0.23994022607803345
train_iter_loss: 0.2895686626434326
train_iter_loss: 0.29690539836883545
train_iter_loss: 0.10655444860458374
train_iter_loss: 0.3546362817287445
train_iter_loss: 0.34580501914024353
train_iter_loss: 0.2963198125362396
train_iter_loss: 0.4268314838409424
train_iter_loss: 0.3273254632949829
train_iter_loss: 0.26935654878616333
train_iter_loss: 0.37857869267463684
train_iter_loss: 0.21611472964286804
train_iter_loss: 0.4628725051879883
train_iter_loss: 0.393826425075531
train_iter_loss: 0.2501029968261719
train_iter_loss: 0.26069557666778564
train_iter_loss: 0.22147588431835175
train_iter_loss: 0.42615166306495667
train_iter_loss: 0.43274036049842834
train_iter_loss: 0.3472748398780823
train_iter_loss: 0.22088086605072021
train_iter_loss: 0.2991633117198944
train_iter_loss: 0.2687908113002777
train_iter_loss: 0.36348986625671387
train_iter_loss: 0.38855043053627014
train_iter_loss: 0.39145201444625854
train_iter_loss: 0.17678414285182953
train_iter_loss: 0.3602750897407532
train_iter_loss: 0.2978624999523163
train_iter_loss: 0.23545712232589722
train_iter_loss: 0.285223126411438
train_iter_loss: 0.24004299938678741
train_iter_loss: 0.24591948091983795
train_iter_loss: 0.21442201733589172
train_iter_loss: 0.27942144870758057
train_iter_loss: 0.32922127842903137
train_iter_loss: 0.31694546341896057
train_iter_loss: 0.38263028860092163
train_iter_loss: 0.376942902803421
train_iter_loss: 0.5277755260467529
train_iter_loss: 0.15053878724575043
train_iter_loss: 0.3523329496383667
train_iter_loss: 0.2217758744955063
train_iter_loss: 0.4425880014896393
train_iter_loss: 0.26093587279319763
train_iter_loss: 0.3539382517337799
train_iter_loss: 0.4031582474708557
train_iter_loss: 0.20742428302764893
train_iter_loss: 0.2512222230434418
train loss :0.2991
---------------------
Validation seg loss: 0.41456040318282145 at epoch 136
epoch =    137/  1000, exp = train
train_iter_loss: 0.36685317754745483
train_iter_loss: 0.24511994421482086
train_iter_loss: 0.23289136588573456
train_iter_loss: 0.33089765906333923
train_iter_loss: 0.44593048095703125
train_iter_loss: 0.26611146330833435
train_iter_loss: 0.29640743136405945
train_iter_loss: 0.2668388783931732
train_iter_loss: 0.2238711267709732
train_iter_loss: 0.32816097140312195
train_iter_loss: 0.3645625114440918
train_iter_loss: 0.2756298780441284
train_iter_loss: 0.34286847710609436
train_iter_loss: 0.1782844364643097
train_iter_loss: 0.34607747197151184
train_iter_loss: 0.3178844749927521
train_iter_loss: 0.27521365880966187
train_iter_loss: 0.47056785225868225
train_iter_loss: 0.2599833309650421
train_iter_loss: 0.25451526045799255
train_iter_loss: 0.29841089248657227
train_iter_loss: 0.198791965842247
train_iter_loss: 0.31867772340774536
train_iter_loss: 0.2759862244129181
train_iter_loss: 0.3420380651950836
train_iter_loss: 0.3033679723739624
train_iter_loss: 0.36912086606025696
train_iter_loss: 0.25855553150177
train_iter_loss: 0.2517400085926056
train_iter_loss: 0.19748903810977936
train_iter_loss: 0.3306024670600891
train_iter_loss: 0.31766360998153687
train_iter_loss: 0.38744038343429565
train_iter_loss: 0.3876592814922333
train_iter_loss: 0.3222034275531769
train_iter_loss: 0.20141562819480896
train_iter_loss: 0.1992053985595703
train_iter_loss: 0.1283043920993805
train_iter_loss: 0.26848235726356506
train_iter_loss: 0.3955475687980652
train_iter_loss: 0.25666964054107666
train_iter_loss: 0.3975014090538025
train_iter_loss: 0.2525816559791565
train_iter_loss: 0.19305826723575592
train_iter_loss: 0.18761399388313293
train_iter_loss: 0.23946061730384827
train_iter_loss: 0.20095601677894592
train_iter_loss: 0.3028049170970917
train_iter_loss: 0.3185994029045105
train_iter_loss: 0.3959055244922638
train_iter_loss: 0.260356068611145
train_iter_loss: 0.245036318898201
train_iter_loss: 0.3627752363681793
train_iter_loss: 0.23472946882247925
train_iter_loss: 0.5432444214820862
train_iter_loss: 0.3678230345249176
train_iter_loss: 0.17515994608402252
train_iter_loss: 0.26740410923957825
train_iter_loss: 0.20844456553459167
train_iter_loss: 0.21456028521060944
train_iter_loss: 0.2389828860759735
train_iter_loss: 0.2907841205596924
train_iter_loss: 0.16870449483394623
train_iter_loss: 0.2517911493778229
train_iter_loss: 0.32056882977485657
train_iter_loss: 0.37825655937194824
train_iter_loss: 0.2893339693546295
train_iter_loss: 0.3620520830154419
train_iter_loss: 0.37066909670829773
train_iter_loss: 0.17975765466690063
train_iter_loss: 0.26501309871673584
train_iter_loss: 0.3046420216560364
train_iter_loss: 0.3020904064178467
train_iter_loss: 0.32294729351997375
train_iter_loss: 0.23555529117584229
train_iter_loss: 0.26746872067451477
train_iter_loss: 0.3020484745502472
train_iter_loss: 0.2739262580871582
train_iter_loss: 0.19365143775939941
train_iter_loss: 0.34431785345077515
train_iter_loss: 0.2658417820930481
train_iter_loss: 0.43760621547698975
train_iter_loss: 0.2371995896100998
train_iter_loss: 0.3914507031440735
train_iter_loss: 0.23803049325942993
train_iter_loss: 0.3222089409828186
train_iter_loss: 0.2182919681072235
train_iter_loss: 0.37409019470214844
train_iter_loss: 0.31989234685897827
train_iter_loss: 0.21297867596149445
train_iter_loss: 0.29173165559768677
train_iter_loss: 0.14807267487049103
train_iter_loss: 0.31455281376838684
train_iter_loss: 0.3446531593799591
train_iter_loss: 0.2941167950630188
train_iter_loss: 0.39536166191101074
train_iter_loss: 0.32801008224487305
train_iter_loss: 0.36257320642471313
train_iter_loss: 0.20328640937805176
train_iter_loss: 0.3439079225063324
train loss :0.2947
---------------------
Validation seg loss: 0.3916141433764319 at epoch 137
epoch =    138/  1000, exp = train
train_iter_loss: 0.3691787123680115
train_iter_loss: 0.23520708084106445
train_iter_loss: 0.30918365716934204
train_iter_loss: 0.3627786338329315
train_iter_loss: 0.3691507577896118
train_iter_loss: 0.27191808819770813
train_iter_loss: 0.20328520238399506
train_iter_loss: 0.252188116312027
train_iter_loss: 0.3957328200340271
train_iter_loss: 0.22087915241718292
train_iter_loss: 0.1822178214788437
train_iter_loss: 0.22417481243610382
train_iter_loss: 0.31076279282569885
train_iter_loss: 0.21531428396701813
train_iter_loss: 0.3296533524990082
train_iter_loss: 0.25000718235969543
train_iter_loss: 0.19098593294620514
train_iter_loss: 0.12281884253025055
train_iter_loss: 0.16049768030643463
train_iter_loss: 0.34739530086517334
train_iter_loss: 0.3529535233974457
train_iter_loss: 0.32350629568099976
train_iter_loss: 0.23025888204574585
train_iter_loss: 0.3597829043865204
train_iter_loss: 0.23225316405296326
train_iter_loss: 0.38428330421447754
train_iter_loss: 0.21130962669849396
train_iter_loss: 0.3952242136001587
train_iter_loss: 0.25631263852119446
train_iter_loss: 0.3074098825454712
train_iter_loss: 0.25687363743782043
train_iter_loss: 0.46782636642456055
train_iter_loss: 0.14452090859413147
train_iter_loss: 0.19049426913261414
train_iter_loss: 0.29871803522109985
train_iter_loss: 0.34010598063468933
train_iter_loss: 0.484605610370636
train_iter_loss: 0.28886184096336365
train_iter_loss: 0.3025077283382416
train_iter_loss: 0.3156081736087799
train_iter_loss: 0.23534910380840302
train_iter_loss: 0.2787514328956604
train_iter_loss: 0.32698655128479004
train_iter_loss: 0.3480497896671295
train_iter_loss: 0.17791630327701569
train_iter_loss: 0.22601798176765442
train_iter_loss: 0.2084764838218689
train_iter_loss: 0.21561643481254578
train_iter_loss: 0.19522875547409058
train_iter_loss: 0.46922802925109863
train_iter_loss: 0.37735897302627563
train_iter_loss: 0.31441229581832886
train_iter_loss: 0.26792386174201965
train_iter_loss: 0.37982431054115295
train_iter_loss: 0.3841615617275238
train_iter_loss: 0.19318930804729462
train_iter_loss: 0.42697009444236755
train_iter_loss: 0.4457836449146271
train_iter_loss: 0.198661670088768
train_iter_loss: 0.3554465174674988
train_iter_loss: 0.40055423974990845
train_iter_loss: 0.3561777174472809
train_iter_loss: 0.40423789620399475
train_iter_loss: 0.29827114939689636
train_iter_loss: 0.19620351493358612
train_iter_loss: 0.3241564929485321
train_iter_loss: 0.14654092490673065
train_iter_loss: 0.47850021719932556
train_iter_loss: 0.2189778983592987
train_iter_loss: 0.221351757645607
train_iter_loss: 0.21517470479011536
train_iter_loss: 0.26598823070526123
train_iter_loss: 0.2314341813325882
train_iter_loss: 0.2745260000228882
train_iter_loss: 0.40766966342926025
train_iter_loss: 0.12527674436569214
train_iter_loss: 0.24876633286476135
train_iter_loss: 0.287093847990036
train_iter_loss: 0.2966565191745758
train_iter_loss: 0.2903549373149872
train_iter_loss: 0.25317898392677307
train_iter_loss: 0.2862427830696106
train_iter_loss: 0.3621790111064911
train_iter_loss: 0.452040433883667
train_iter_loss: 0.23137234151363373
train_iter_loss: 0.37130653858184814
train_iter_loss: 0.17225214838981628
train_iter_loss: 0.2982216775417328
train_iter_loss: 0.5695264935493469
train_iter_loss: 0.35708120465278625
train_iter_loss: 0.3148900866508484
train_iter_loss: 0.40276843309402466
train_iter_loss: 0.311567097902298
train_iter_loss: 0.2039327323436737
train_iter_loss: 0.37070146203041077
train_iter_loss: 0.35707250237464905
train_iter_loss: 0.19288091361522675
train_iter_loss: 0.3824378252029419
train_iter_loss: 0.14430774748325348
train_iter_loss: 0.2811550796031952
train loss :0.2979
---------------------
Validation seg loss: 0.3832804781894358 at epoch 138
epoch =    139/  1000, exp = train
train_iter_loss: 0.29784685373306274
train_iter_loss: 0.4137808084487915
train_iter_loss: 0.307671457529068
train_iter_loss: 0.2922488749027252
train_iter_loss: 0.2282838076353073
train_iter_loss: 0.3025686740875244
train_iter_loss: 0.19246603548526764
train_iter_loss: 0.26093220710754395
train_iter_loss: 0.41699090600013733
train_iter_loss: 0.34638339281082153
train_iter_loss: 0.2745419144630432
train_iter_loss: 0.3134216368198395
train_iter_loss: 0.3571639060974121
train_iter_loss: 0.17251494526863098
train_iter_loss: 0.3748334050178528
train_iter_loss: 0.08059705048799515
train_iter_loss: 0.3460840582847595
train_iter_loss: 0.4457170069217682
train_iter_loss: 0.42592573165893555
train_iter_loss: 0.22760775685310364
train_iter_loss: 0.24232368171215057
train_iter_loss: 0.35755306482315063
train_iter_loss: 0.14264705777168274
train_iter_loss: 0.3821887671947479
train_iter_loss: 0.31172171235084534
train_iter_loss: 0.38391584157943726
train_iter_loss: 0.2185511440038681
train_iter_loss: 0.36439836025238037
train_iter_loss: 0.2588775157928467
train_iter_loss: 0.31233030557632446
train_iter_loss: 0.21074408292770386
train_iter_loss: 0.19446322321891785
train_iter_loss: 0.29686179757118225
train_iter_loss: 0.40288954973220825
train_iter_loss: 0.21882565319538116
train_iter_loss: 0.33732661604881287
train_iter_loss: 0.18859447538852692
train_iter_loss: 0.18366172909736633
train_iter_loss: 0.3572007119655609
train_iter_loss: 0.34407031536102295
train_iter_loss: 0.20587462186813354
train_iter_loss: 0.3292025923728943
train_iter_loss: 0.27740511298179626
train_iter_loss: 0.37357133626937866
train_iter_loss: 0.33439305424690247
train_iter_loss: 0.46134594082832336
train_iter_loss: 0.29222410917282104
train_iter_loss: 0.19545714557170868
train_iter_loss: 0.3892851769924164
train_iter_loss: 0.1722845882177353
train_iter_loss: 0.31656521558761597
train_iter_loss: 0.09223111718893051
train_iter_loss: 0.21105650067329407
train_iter_loss: 0.28555944561958313
train_iter_loss: 0.3687649369239807
train_iter_loss: 0.44879549741744995
train_iter_loss: 0.30085211992263794
train_iter_loss: 0.258733332157135
train_iter_loss: 0.25305935740470886
train_iter_loss: 0.34191587567329407
train_iter_loss: 0.3457823097705841
train_iter_loss: 0.2979842722415924
train_iter_loss: 0.3193170726299286
train_iter_loss: 0.3730674684047699
train_iter_loss: 0.27965015172958374
train_iter_loss: 0.33520472049713135
train_iter_loss: 0.35486161708831787
train_iter_loss: 0.38464614748954773
train_iter_loss: 0.605596661567688
train_iter_loss: 0.42379269003868103
train_iter_loss: 0.23039497435092926
train_iter_loss: 0.385869562625885
train_iter_loss: 0.31284111738204956
train_iter_loss: 0.2893070876598358
train_iter_loss: 0.33433640003204346
train_iter_loss: 0.5448892116546631
train_iter_loss: 0.2748057246208191
train_iter_loss: 0.2644054889678955
train_iter_loss: 0.12609854340553284
train_iter_loss: 0.33220985531806946
train_iter_loss: 0.26134756207466125
train_iter_loss: 0.15947790443897247
train_iter_loss: 0.2806125283241272
train_iter_loss: 0.39935141801834106
train_iter_loss: 0.23573149740695953
train_iter_loss: 0.3394562005996704
train_iter_loss: 0.3367941081523895
train_iter_loss: 0.4624828100204468
train_iter_loss: 0.30546510219573975
train_iter_loss: 0.26970580220222473
train_iter_loss: 0.1367693692445755
train_iter_loss: 0.3305445909500122
train_iter_loss: 0.06969480216503143
train_iter_loss: 0.3528752326965332
train_iter_loss: 0.14898642897605896
train_iter_loss: 0.3178272843360901
train_iter_loss: 0.18370647728443146
train_iter_loss: 0.30022069811820984
train_iter_loss: 0.16139854490756989
train_iter_loss: 0.4239063858985901
train loss :0.3028
---------------------
Validation seg loss: 0.380484051158968 at epoch 139
epoch =    140/  1000, exp = train
train_iter_loss: 0.23778147995471954
train_iter_loss: 0.4033113121986389
train_iter_loss: 0.34223732352256775
train_iter_loss: 0.2584213614463806
train_iter_loss: 0.3497098386287689
train_iter_loss: 0.3867815136909485
train_iter_loss: 0.3175215721130371
train_iter_loss: 0.1522544026374817
train_iter_loss: 0.21974244713783264
train_iter_loss: 0.17771105468273163
train_iter_loss: 0.40320509672164917
train_iter_loss: 0.18686671555042267
train_iter_loss: 0.3502086400985718
train_iter_loss: 0.2488468438386917
train_iter_loss: 0.3046988844871521
train_iter_loss: 0.29317405819892883
train_iter_loss: 0.20511548221111298
train_iter_loss: 0.3211084008216858
train_iter_loss: 0.32318881154060364
train_iter_loss: 0.32685184478759766
train_iter_loss: 0.3170422911643982
train_iter_loss: 0.20322436094284058
train_iter_loss: 0.4486939311027527
train_iter_loss: 0.5651508569717407
train_iter_loss: 0.2540138363838196
train_iter_loss: 0.1904941201210022
train_iter_loss: 0.28562840819358826
train_iter_loss: 0.14256399869918823
train_iter_loss: 0.22657041251659393
train_iter_loss: 0.31231510639190674
train_iter_loss: 0.1588352620601654
train_iter_loss: 0.1935681700706482
train_iter_loss: 0.3308306336402893
train_iter_loss: 0.2788490056991577
train_iter_loss: 0.2306298166513443
train_iter_loss: 0.413271427154541
train_iter_loss: 0.5038668513298035
train_iter_loss: 0.18434521555900574
train_iter_loss: 0.2192639261484146
train_iter_loss: 0.28158023953437805
train_iter_loss: 0.2369440495967865
train_iter_loss: 0.4907318651676178
train_iter_loss: 0.41866591572761536
train_iter_loss: 0.25783678889274597
train_iter_loss: 0.16601993143558502
train_iter_loss: 0.28900855779647827
train_iter_loss: 0.47387799620628357
train_iter_loss: 0.4347643554210663
train_iter_loss: 0.2520226240158081
train_iter_loss: 0.3322327435016632
train_iter_loss: 0.27948111295700073
train_iter_loss: 0.39488130807876587
train_iter_loss: 0.32602909207344055
train_iter_loss: 0.32951629161834717
train_iter_loss: 0.27192237973213196
train_iter_loss: 0.3755083680152893
train_iter_loss: 0.32505398988723755
train_iter_loss: 0.17339669167995453
train_iter_loss: 0.16549943387508392
train_iter_loss: 0.3980558216571808
train_iter_loss: 0.3608688414096832
train_iter_loss: 0.17396198213100433
train_iter_loss: 0.30870211124420166
train_iter_loss: 0.3010917007923126
train_iter_loss: 0.20999646186828613
train_iter_loss: 0.15562555193901062
train_iter_loss: 0.31784504652023315
train_iter_loss: 0.31272268295288086
train_iter_loss: 0.37596240639686584
train_iter_loss: 0.3294287621974945
train_iter_loss: 0.3584430515766144
train_iter_loss: 0.32257428765296936
train_iter_loss: 0.22711814939975739
train_iter_loss: 0.32467901706695557
train_iter_loss: 0.28806623816490173
train_iter_loss: 0.5006418228149414
train_iter_loss: 0.18823078274726868
train_iter_loss: 0.1742386668920517
train_iter_loss: 0.2809953987598419
train_iter_loss: 0.3666616976261139
train_iter_loss: 0.4155026376247406
train_iter_loss: 0.2579071521759033
train_iter_loss: 0.3237650692462921
train_iter_loss: 0.2563498914241791
train_iter_loss: 0.35826411843299866
train_iter_loss: 0.43169790506362915
train_iter_loss: 0.3387816250324249
train_iter_loss: 0.3322291374206543
train_iter_loss: 0.2654264271259308
train_iter_loss: 0.4060335159301758
train_iter_loss: 0.262515127658844
train_iter_loss: 0.36872968077659607
train_iter_loss: 0.24728594720363617
train_iter_loss: 0.2566535472869873
train_iter_loss: 0.28964245319366455
train_iter_loss: 0.28890708088874817
train_iter_loss: 0.3110923171043396
train_iter_loss: 0.23594067990779877
train_iter_loss: 0.13057559728622437
train_iter_loss: 0.5350260138511658
train loss :0.3043
---------------------
Validation seg loss: 0.39084277089404046 at epoch 140
epoch =    141/  1000, exp = train
train_iter_loss: 0.27822446823120117
train_iter_loss: 0.25841909646987915
train_iter_loss: 0.07569070160388947
train_iter_loss: 0.31144800782203674
train_iter_loss: 0.15184131264686584
train_iter_loss: 0.3471788167953491
train_iter_loss: 0.21643957495689392
train_iter_loss: 0.20086300373077393
train_iter_loss: 0.3751778304576874
train_iter_loss: 0.18627096712589264
train_iter_loss: 0.37888962030410767
train_iter_loss: 0.2696530222892761
train_iter_loss: 0.22049787640571594
train_iter_loss: 0.23304001986980438
train_iter_loss: 0.24344488978385925
train_iter_loss: 0.2385835200548172
train_iter_loss: 0.3190138041973114
train_iter_loss: 0.2501734495162964
train_iter_loss: 0.37799859046936035
train_iter_loss: 0.31481149792671204
train_iter_loss: 0.1479339897632599
train_iter_loss: 0.2566031813621521
train_iter_loss: 0.36876052618026733
train_iter_loss: 0.3261679708957672
train_iter_loss: 0.2659328579902649
train_iter_loss: 0.4601408839225769
train_iter_loss: 0.3159565329551697
train_iter_loss: 0.5092653632164001
train_iter_loss: 0.4073761999607086
train_iter_loss: 0.299023300409317
train_iter_loss: 0.43051329255104065
train_iter_loss: 0.200572207570076
train_iter_loss: 0.18810969591140747
train_iter_loss: 0.2763047516345978
train_iter_loss: 0.3121432662010193
train_iter_loss: 0.28983014822006226
train_iter_loss: 0.2280016988515854
train_iter_loss: 0.23534999787807465
train_iter_loss: 0.4004896879196167
train_iter_loss: 0.282612681388855
train_iter_loss: 0.34852465987205505
train_iter_loss: 0.34229397773742676
train_iter_loss: 0.4274234175682068
train_iter_loss: 0.3085099458694458
train_iter_loss: 0.2627025544643402
train_iter_loss: 0.28971558809280396
train_iter_loss: 0.3713917136192322
train_iter_loss: 0.37787383794784546
train_iter_loss: 0.2878694534301758
train_iter_loss: 0.2384985387325287
train_iter_loss: 0.23394346237182617
train_iter_loss: 0.370986670255661
train_iter_loss: 0.24728034436702728
train_iter_loss: 0.20508651435375214
train_iter_loss: 0.4775591790676117
train_iter_loss: 0.3530665636062622
train_iter_loss: 0.3660430908203125
train_iter_loss: 0.3699779510498047
train_iter_loss: 0.26014602184295654
train_iter_loss: 0.490745484828949
train_iter_loss: 0.27468380331993103
train_iter_loss: 0.2470502406358719
train_iter_loss: 0.24642501771450043
train_iter_loss: 0.286291241645813
train_iter_loss: 0.3720220923423767
train_iter_loss: 0.3653659224510193
train_iter_loss: 0.23466609418392181
train_iter_loss: 0.30867743492126465
train_iter_loss: 0.3295276165008545
train_iter_loss: 0.4372256100177765
train_iter_loss: 0.3082576096057892
train_iter_loss: 0.1716897040605545
train_iter_loss: 0.38489535450935364
train_iter_loss: 0.12571567296981812
train_iter_loss: 0.2119961529970169
train_iter_loss: 0.33052578568458557
train_iter_loss: 0.26296764612197876
train_iter_loss: 0.28829425573349
train_iter_loss: 0.31376272439956665
train_iter_loss: 0.34942564368247986
train_iter_loss: 0.07994125783443451
train_iter_loss: 0.22874750196933746
train_iter_loss: 0.39734840393066406
train_iter_loss: 0.23530712723731995
train_iter_loss: 0.4234457314014435
train_iter_loss: 0.27260226011276245
train_iter_loss: 0.4079168140888214
train_iter_loss: 0.2072180211544037
train_iter_loss: 0.3939323425292969
train_iter_loss: 0.5015548467636108
train_iter_loss: 0.24283725023269653
train_iter_loss: 0.3782077133655548
train_iter_loss: 0.24727170169353485
train_iter_loss: 0.20114092528820038
train_iter_loss: 0.18667516112327576
train_iter_loss: 0.23373383283615112
train_iter_loss: 0.27979394793510437
train_iter_loss: 0.2554033696651459
train_iter_loss: 0.35237956047058105
train_iter_loss: 0.20543092489242554
train loss :0.2996
---------------------
Validation seg loss: 0.39764849416349296 at epoch 141
epoch =    142/  1000, exp = train
train_iter_loss: 0.3655849099159241
train_iter_loss: 0.3426454961299896
train_iter_loss: 0.2819401025772095
train_iter_loss: 0.24512389302253723
train_iter_loss: 0.24944742023944855
train_iter_loss: 0.3447819948196411
train_iter_loss: 0.2600541114807129
train_iter_loss: 0.18217502534389496
train_iter_loss: 0.3835349977016449
train_iter_loss: 0.1876445859670639
train_iter_loss: 0.3592188358306885
train_iter_loss: 0.38650527596473694
train_iter_loss: 0.3655913770198822
train_iter_loss: 0.38539814949035645
train_iter_loss: 0.47496747970581055
train_iter_loss: 0.28921863436698914
train_iter_loss: 0.15357212722301483
train_iter_loss: 0.33473581075668335
train_iter_loss: 0.21391074359416962
train_iter_loss: 0.20167836546897888
train_iter_loss: 0.24820277094841003
train_iter_loss: 0.29203179478645325
train_iter_loss: 0.161514550447464
train_iter_loss: 0.30014893412590027
train_iter_loss: 0.32905569672584534
train_iter_loss: 0.3779089152812958
train_iter_loss: 0.4184778928756714
train_iter_loss: 0.1742296665906906
train_iter_loss: 0.3523463010787964
train_iter_loss: 0.1286393105983734
train_iter_loss: 0.3483028709888458
train_iter_loss: 0.21878954768180847
train_iter_loss: 0.4302051365375519
train_iter_loss: 0.1952451765537262
train_iter_loss: 0.2905197739601135
train_iter_loss: 0.3015986979007721
train_iter_loss: 0.3161091208457947
train_iter_loss: 0.24446073174476624
train_iter_loss: 0.3318740427494049
train_iter_loss: 0.30908748507499695
train_iter_loss: 0.23416180908679962
train_iter_loss: 0.2958654761314392
train_iter_loss: 0.23964782059192657
train_iter_loss: 0.3120805621147156
train_iter_loss: 0.2189929336309433
train_iter_loss: 0.39086487889289856
train_iter_loss: 0.24436013400554657
train_iter_loss: 0.31294554471969604
train_iter_loss: 0.35125330090522766
train_iter_loss: 0.16101954877376556
train_iter_loss: 0.3523414134979248
train_iter_loss: 0.07583380490541458
train_iter_loss: 0.3373270034790039
train_iter_loss: 0.3679189383983612
train_iter_loss: 0.2953108847141266
train_iter_loss: 0.2430199235677719
train_iter_loss: 0.2689594328403473
train_iter_loss: 0.3630276024341583
train_iter_loss: 0.2854347825050354
train_iter_loss: 0.5136203765869141
train_iter_loss: 0.22204862534999847
train_iter_loss: 0.31096580624580383
train_iter_loss: 0.21174407005310059
train_iter_loss: 0.23203399777412415
train_iter_loss: 0.41565245389938354
train_iter_loss: 0.40003713965415955
train_iter_loss: 0.26850730180740356
train_iter_loss: 0.21913747489452362
train_iter_loss: 0.2555062472820282
train_iter_loss: 0.3625335097312927
train_iter_loss: 0.38403668999671936
train_iter_loss: 0.3566652536392212
train_iter_loss: 0.3751155138015747
train_iter_loss: 0.20632879436016083
train_iter_loss: 0.22850851714611053
train_iter_loss: 0.3230614960193634
train_iter_loss: 0.33672675490379333
train_iter_loss: 0.08039414137601852
train_iter_loss: 0.554013729095459
train_iter_loss: 0.38401514291763306
train_iter_loss: 0.1448078155517578
train_iter_loss: 0.1747293323278427
train_iter_loss: 0.5350447297096252
train_iter_loss: 0.28931164741516113
train_iter_loss: 0.34533125162124634
train_iter_loss: 0.226356640458107
train_iter_loss: 0.2633427381515503
train_iter_loss: 0.197372704744339
train_iter_loss: 0.3037015497684479
train_iter_loss: 0.2777426540851593
train_iter_loss: 0.17880800366401672
train_iter_loss: 0.33637478947639465
train_iter_loss: 0.22001482546329498
train_iter_loss: 0.1919127106666565
train_iter_loss: 0.31976646184921265
train_iter_loss: 0.264043390750885
train_iter_loss: 0.4924590289592743
train_iter_loss: 0.42046475410461426
train_iter_loss: 0.3292439579963684
train_iter_loss: 0.2465079128742218
train loss :0.2982
---------------------
Validation seg loss: 0.391764720497688 at epoch 142
epoch =    143/  1000, exp = train
train_iter_loss: 0.37565430998802185
train_iter_loss: 0.26879191398620605
train_iter_loss: 0.3406788110733032
train_iter_loss: 0.1357494443655014
train_iter_loss: 0.4780046343803406
train_iter_loss: 0.33624154329299927
train_iter_loss: 0.31224945187568665
train_iter_loss: 0.1479962170124054
train_iter_loss: 0.3121848404407501
train_iter_loss: 0.20374897122383118
train_iter_loss: 0.302531361579895
train_iter_loss: 0.3914943039417267
train_iter_loss: 0.18596166372299194
train_iter_loss: 0.2720147669315338
train_iter_loss: 0.21679465472698212
train_iter_loss: 0.3882408142089844
train_iter_loss: 0.34475353360176086
train_iter_loss: 0.254618376493454
train_iter_loss: 0.2638002038002014
train_iter_loss: 0.3367469906806946
train_iter_loss: 0.2549780011177063
train_iter_loss: 0.2390541136264801
train_iter_loss: 0.305877149105072
train_iter_loss: 0.13086602091789246
train_iter_loss: 0.2883043885231018
train_iter_loss: 0.4046693444252014
train_iter_loss: 0.1899016797542572
train_iter_loss: 0.2796078026294708
train_iter_loss: 0.36145851016044617
train_iter_loss: 0.24689674377441406
train_iter_loss: 0.19614997506141663
train_iter_loss: 0.3563278615474701
train_iter_loss: 0.3354630768299103
train_iter_loss: 0.11202739179134369
train_iter_loss: 0.3772059381008148
train_iter_loss: 0.3138871192932129
train_iter_loss: 0.3815854489803314
train_iter_loss: 0.4521646797657013
train_iter_loss: 0.2843845784664154
train_iter_loss: 0.2833325266838074
train_iter_loss: 0.40674707293510437
train_iter_loss: 0.18536975979804993
train_iter_loss: 0.3436949551105499
train_iter_loss: 0.30008384585380554
train_iter_loss: 0.2612627446651459
train_iter_loss: 0.385222852230072
train_iter_loss: 0.32723376154899597
train_iter_loss: 0.18066978454589844
train_iter_loss: 0.23930494487285614
train_iter_loss: 0.4023629128932953
train_iter_loss: 0.26645633578300476
train_iter_loss: 0.23688264191150665
train_iter_loss: 0.19916459918022156
train_iter_loss: 0.2896353602409363
train_iter_loss: 0.3340880274772644
train_iter_loss: 0.29685178399086
train_iter_loss: 0.2544836103916168
train_iter_loss: 0.3320492208003998
train_iter_loss: 0.34192150831222534
train_iter_loss: 0.326228529214859
train_iter_loss: 0.2655833959579468
train_iter_loss: 0.22853298485279083
train_iter_loss: 0.24961471557617188
train_iter_loss: 0.18906201422214508
train_iter_loss: 0.3008156716823578
train_iter_loss: 0.11319322884082794
train_iter_loss: 0.43562015891075134
train_iter_loss: 0.21071287989616394
train_iter_loss: 0.33086416125297546
train_iter_loss: 0.23409268260002136
train_iter_loss: 0.25675278902053833
train_iter_loss: 0.46573007106781006
train_iter_loss: 0.3428651988506317
train_iter_loss: 0.11934808641672134
train_iter_loss: 0.2656334638595581
train_iter_loss: 0.1834936887025833
train_iter_loss: 0.2857758104801178
train_iter_loss: 0.2506893277168274
train_iter_loss: 0.3015022575855255
train_iter_loss: 0.31948909163475037
train_iter_loss: 0.29101577401161194
train_iter_loss: 0.1470872312784195
train_iter_loss: 0.12043626606464386
train_iter_loss: 0.3164047300815582
train_iter_loss: 0.37747421860694885
train_iter_loss: 0.3921853005886078
train_iter_loss: 0.42651239037513733
train_iter_loss: 0.26646387577056885
train_iter_loss: 0.17049463093280792
train_iter_loss: 0.37884843349456787
train_iter_loss: 0.4433766305446625
train_iter_loss: 0.336577445268631
train_iter_loss: 0.2557130455970764
train_iter_loss: 0.21759779751300812
train_iter_loss: 0.4604763984680176
train_iter_loss: 0.22434626519680023
train_iter_loss: 0.2645738124847412
train_iter_loss: 0.21484771370887756
train_iter_loss: 0.3486505150794983
train_iter_loss: 0.33689451217651367
train loss :0.2921
---------------------
Validation seg loss: 0.3842447417138039 at epoch 143
epoch =    144/  1000, exp = train
train_iter_loss: 0.23941664397716522
train_iter_loss: 0.2594623267650604
train_iter_loss: 0.3172200918197632
train_iter_loss: 0.17549636960029602
train_iter_loss: 0.18080812692642212
train_iter_loss: 0.2377602458000183
train_iter_loss: 0.2063601166009903
train_iter_loss: 0.27719563245773315
train_iter_loss: 0.34912988543510437
train_iter_loss: 0.19223257899284363
train_iter_loss: 0.4763626158237457
train_iter_loss: 0.23773102462291718
train_iter_loss: 0.3326459228992462
train_iter_loss: 0.18840985000133514
train_iter_loss: 0.2689323425292969
train_iter_loss: 0.23047283291816711
train_iter_loss: 0.25292184948921204
train_iter_loss: 0.22004559636116028
train_iter_loss: 0.2553834915161133
train_iter_loss: 0.32804057002067566
train_iter_loss: 0.2594514489173889
train_iter_loss: 0.3465864062309265
train_iter_loss: 0.1705578863620758
train_iter_loss: 0.3463582992553711
train_iter_loss: 0.3094848096370697
train_iter_loss: 0.30148887634277344
train_iter_loss: 0.35218989849090576
train_iter_loss: 0.7395044565200806
train_iter_loss: 0.28263038396835327
train_iter_loss: 0.2593139410018921
train_iter_loss: 0.21445493400096893
train_iter_loss: 0.3109157383441925
train_iter_loss: 0.36139532923698425
train_iter_loss: 0.17794573307037354
train_iter_loss: 0.17967985570430756
train_iter_loss: 0.24145495891571045
train_iter_loss: 0.14512348175048828
train_iter_loss: 0.31377485394477844
train_iter_loss: 0.3541580140590668
train_iter_loss: 0.16920042037963867
train_iter_loss: 0.2694389224052429
train_iter_loss: 0.25029829144477844
train_iter_loss: 0.3079628348350525
train_iter_loss: 0.24239030480384827
train_iter_loss: 0.3832351267337799
train_iter_loss: 0.23108981549739838
train_iter_loss: 0.34754928946495056
train_iter_loss: 0.26247894763946533
train_iter_loss: 0.20812059938907623
train_iter_loss: 0.28370440006256104
train_iter_loss: 0.3008909821510315
train_iter_loss: 0.3638670742511749
train_iter_loss: 0.2934959828853607
train_iter_loss: 0.4489721953868866
train_iter_loss: 0.20269320905208588
train_iter_loss: 0.21167150139808655
train_iter_loss: 0.5606547594070435
train_iter_loss: 0.3611314296722412
train_iter_loss: 0.19199466705322266
train_iter_loss: 0.3349217176437378
train_iter_loss: 0.399082750082016
train_iter_loss: 0.27905821800231934
train_iter_loss: 0.3562326729297638
train_iter_loss: 0.2901665270328522
train_iter_loss: 0.17389610409736633
train_iter_loss: 0.18011793494224548
train_iter_loss: 0.20938605070114136
train_iter_loss: 0.4723905026912689
train_iter_loss: 0.2547682821750641
train_iter_loss: 0.3192789852619171
train_iter_loss: 0.35143551230430603
train_iter_loss: 0.2377893328666687
train_iter_loss: 0.15667900443077087
train_iter_loss: 0.56947922706604
train_iter_loss: 0.24395696818828583
train_iter_loss: 0.40845543146133423
train_iter_loss: 0.16512860357761383
train_iter_loss: 0.31200456619262695
train_iter_loss: 0.25335758924484253
train_iter_loss: 0.2710030972957611
train_iter_loss: 0.3801431655883789
train_iter_loss: 0.15586626529693604
train_iter_loss: 0.20555107295513153
train_iter_loss: 0.5466806292533875
train_iter_loss: 0.256057471036911
train_iter_loss: 0.2984510064125061
train_iter_loss: 0.3604576885700226
train_iter_loss: 0.34356939792633057
train_iter_loss: 0.44815847277641296
train_iter_loss: 0.28431782126426697
train_iter_loss: 0.1873376965522766
train_iter_loss: 0.33371201157569885
train_iter_loss: 0.1427951455116272
train_iter_loss: 0.28139010071754456
train_iter_loss: 0.33157601952552795
train_iter_loss: 0.3840368092060089
train_iter_loss: 0.5416761636734009
train_iter_loss: 0.3247815668582916
train_iter_loss: 0.3228864073753357
train_iter_loss: 0.3552088737487793
train loss :0.2980
---------------------
Validation seg loss: 0.3994950861406495 at epoch 144
epoch =    145/  1000, exp = train
train_iter_loss: 0.4196503460407257
train_iter_loss: 0.2484128773212433
train_iter_loss: 0.31170645356178284
train_iter_loss: 0.27736929059028625
train_iter_loss: 0.19371184706687927
train_iter_loss: 0.3334583342075348
train_iter_loss: 0.4471677243709564
train_iter_loss: 0.25147745013237
train_iter_loss: 0.26576468348503113
train_iter_loss: 0.33607423305511475
train_iter_loss: 0.3261558413505554
train_iter_loss: 0.3151927590370178
train_iter_loss: 0.31357908248901367
train_iter_loss: 0.34127405285835266
train_iter_loss: 0.24043968319892883
train_iter_loss: 0.43835750222206116
train_iter_loss: 0.3908204436302185
train_iter_loss: 0.3859297037124634
train_iter_loss: 0.13996249437332153
train_iter_loss: 0.2200894057750702
train_iter_loss: 0.2831771671772003
train_iter_loss: 0.3136371076107025
train_iter_loss: 0.3243756890296936
train_iter_loss: 0.18525974452495575
train_iter_loss: 0.2281024008989334
train_iter_loss: 0.4617563784122467
train_iter_loss: 0.3697091341018677
train_iter_loss: 0.3735882639884949
train_iter_loss: 0.19549712538719177
train_iter_loss: 0.3061334788799286
train_iter_loss: 0.30711817741394043
train_iter_loss: 0.3708266615867615
train_iter_loss: 0.2699810862541199
train_iter_loss: 0.2981131076812744
train_iter_loss: 0.3708569407463074
train_iter_loss: 0.25368738174438477
train_iter_loss: 0.47069278359413147
train_iter_loss: 0.2337908148765564
train_iter_loss: 0.1775227040052414
train_iter_loss: 0.3143012821674347
train_iter_loss: 0.405205637216568
train_iter_loss: 0.47076907753944397
train_iter_loss: 0.3355010449886322
train_iter_loss: 0.24806766211986542
train_iter_loss: 0.37251511216163635
train_iter_loss: 0.23696628212928772
train_iter_loss: 0.45242708921432495
train_iter_loss: 0.33363327383995056
train_iter_loss: 0.25862812995910645
train_iter_loss: 0.45533302426338196
train_iter_loss: 0.3147226870059967
train_iter_loss: 0.2774288058280945
train_iter_loss: 0.31716063618659973
train_iter_loss: 0.25123047828674316
train_iter_loss: 0.2729659378528595
train_iter_loss: 0.43718427419662476
train_iter_loss: 0.2884446382522583
train_iter_loss: 0.46459347009658813
train_iter_loss: 0.20829015970230103
train_iter_loss: 0.31695717573165894
train_iter_loss: 0.15055012702941895
train_iter_loss: 0.4465580880641937
train_iter_loss: 0.3170812427997589
train_iter_loss: 0.35679394006729126
train_iter_loss: 0.28961366415023804
train_iter_loss: 0.2572169005870819
train_iter_loss: 0.2561766505241394
train_iter_loss: 0.26828962564468384
train_iter_loss: 0.12748287618160248
train_iter_loss: 0.2022702395915985
train_iter_loss: 0.31420233845710754
train_iter_loss: 0.2533782720565796
train_iter_loss: 0.1965356469154358
train_iter_loss: 0.3198526203632355
train_iter_loss: 0.1808580905199051
train_iter_loss: 0.23947077989578247
train_iter_loss: 0.31217527389526367
train_iter_loss: 0.2354564666748047
train_iter_loss: 0.390303373336792
train_iter_loss: 0.2683720588684082
train_iter_loss: 0.22671999037265778
train_iter_loss: 0.20578700304031372
train_iter_loss: 0.23054450750350952
train_iter_loss: 0.35136714577674866
train_iter_loss: 0.26679423451423645
train_iter_loss: 0.21988491714000702
train_iter_loss: 0.24011015892028809
train_iter_loss: 0.2632698118686676
train_iter_loss: 0.34298810362815857
train_iter_loss: 0.19329766929149628
train_iter_loss: 0.41679349541664124
train_iter_loss: 0.26147007942199707
train_iter_loss: 0.2665494978427887
train_iter_loss: 0.22551767528057098
train_iter_loss: 0.3312741816043854
train_iter_loss: 0.22373446822166443
train_iter_loss: 0.2506334185600281
train_iter_loss: 0.3047844469547272
train_iter_loss: 0.2605423331260681
train_iter_loss: 0.1723983734846115
train loss :0.2996
---------------------
Validation seg loss: 0.382507485425416 at epoch 145
epoch =    146/  1000, exp = train
train_iter_loss: 0.18604271113872528
train_iter_loss: 0.27569305896759033
train_iter_loss: 0.25476163625717163
train_iter_loss: 0.24372273683547974
train_iter_loss: 0.22138145565986633
train_iter_loss: 0.25573477149009705
train_iter_loss: 0.3589797019958496
train_iter_loss: 0.23935195803642273
train_iter_loss: 0.33101311326026917
train_iter_loss: 0.26617786288261414
train_iter_loss: 0.2005443572998047
train_iter_loss: 0.488475501537323
train_iter_loss: 0.4256056845188141
train_iter_loss: 0.1493169665336609
train_iter_loss: 0.22403188049793243
train_iter_loss: 0.35107606649398804
train_iter_loss: 0.31563079357147217
train_iter_loss: 0.22712431848049164
train_iter_loss: 0.2227759063243866
train_iter_loss: 0.39391282200813293
train_iter_loss: 0.18534348905086517
train_iter_loss: 0.37627705931663513
train_iter_loss: 0.26300692558288574
train_iter_loss: 0.3064613938331604
train_iter_loss: 0.23823995888233185
train_iter_loss: 0.26476263999938965
train_iter_loss: 0.25926047563552856
train_iter_loss: 0.3641726076602936
train_iter_loss: 0.3300762474536896
train_iter_loss: 0.3160831034183502
train_iter_loss: 0.20715226233005524
train_iter_loss: 0.5510110855102539
train_iter_loss: 0.16762392222881317
train_iter_loss: 0.24525928497314453
train_iter_loss: 0.36277881264686584
train_iter_loss: 0.3231044411659241
train_iter_loss: 0.286519855260849
train_iter_loss: 0.34589123725891113
train_iter_loss: 0.2423349767923355
train_iter_loss: 0.21051427721977234
train_iter_loss: 0.26131296157836914
train_iter_loss: 0.10087540000677109
train_iter_loss: 0.26274049282073975
train_iter_loss: 0.2105872631072998
train_iter_loss: 0.1696779876947403
train_iter_loss: 0.4208011329174042
train_iter_loss: 0.2534594237804413
train_iter_loss: 0.08594293147325516
train_iter_loss: 0.20055916905403137
train_iter_loss: 0.14999762177467346
train_iter_loss: 0.22835545241832733
train_iter_loss: 0.36855652928352356
train_iter_loss: 0.5770506858825684
train_iter_loss: 0.2576342523097992
train_iter_loss: 0.23946666717529297
train_iter_loss: 0.13774313032627106
train_iter_loss: 0.23758387565612793
train_iter_loss: 0.25361448526382446
train_iter_loss: 0.6124724745750427
train_iter_loss: 0.32604336738586426
train_iter_loss: 0.5327211022377014
train_iter_loss: 0.20675219595432281
train_iter_loss: 0.4933387339115143
train_iter_loss: 0.28586530685424805
train_iter_loss: 0.43730223178863525
train_iter_loss: 0.28460702300071716
train_iter_loss: 0.2960668206214905
train_iter_loss: 0.19692015647888184
train_iter_loss: 0.2536316215991974
train_iter_loss: 0.3427010476589203
train_iter_loss: 0.19044460356235504
train_iter_loss: 0.28007274866104126
train_iter_loss: 0.28614142537117004
train_iter_loss: 0.2626853287220001
train_iter_loss: 0.378328800201416
train_iter_loss: 0.2777217626571655
train_iter_loss: 0.23472869396209717
train_iter_loss: 0.37307918071746826
train_iter_loss: 0.4073171317577362
train_iter_loss: 0.21103309094905853
train_iter_loss: 0.35146066546440125
train_iter_loss: 0.3079512417316437
train_iter_loss: 0.2548169195652008
train_iter_loss: 0.285733699798584
train_iter_loss: 0.3126247823238373
train_iter_loss: 0.41102296113967896
train_iter_loss: 0.2715451121330261
train_iter_loss: 0.2608555853366852
train_iter_loss: 0.28734615445137024
train_iter_loss: 0.2354518175125122
train_iter_loss: 0.33836713433265686
train_iter_loss: 0.30057331919670105
train_iter_loss: 0.3527925908565521
train_iter_loss: 0.3362284004688263
train_iter_loss: 0.2547973394393921
train_iter_loss: 0.4603685438632965
train_iter_loss: 0.2652694582939148
train_iter_loss: 0.28212133049964905
train_iter_loss: 0.32199177145957947
train_iter_loss: 0.22320392727851868
train loss :0.2948
---------------------
Validation seg loss: 0.38214073082397004 at epoch 146
epoch =    147/  1000, exp = train
train_iter_loss: 0.3403618037700653
train_iter_loss: 0.2690928876399994
train_iter_loss: 0.34467577934265137
train_iter_loss: 0.14785346388816833
train_iter_loss: 0.3029698431491852
train_iter_loss: 0.27550357580184937
train_iter_loss: 0.37923556566238403
train_iter_loss: 0.2794678807258606
train_iter_loss: 0.2516339123249054
train_iter_loss: 0.29699474573135376
train_iter_loss: 0.2449951171875
train_iter_loss: 0.33333849906921387
train_iter_loss: 0.3964438736438751
train_iter_loss: 0.3657677173614502
train_iter_loss: 0.2296837419271469
train_iter_loss: 0.2351747751235962
train_iter_loss: 0.41138583421707153
train_iter_loss: 0.1215829849243164
train_iter_loss: 0.24144156277179718
train_iter_loss: 0.3835311233997345
train_iter_loss: 0.24462738633155823
train_iter_loss: 0.41277211904525757
train_iter_loss: 0.3464673161506653
train_iter_loss: 0.40391838550567627
train_iter_loss: 0.31202611327171326
train_iter_loss: 0.2727152109146118
train_iter_loss: 0.36328554153442383
train_iter_loss: 0.38799601793289185
train_iter_loss: 0.1948775351047516
train_iter_loss: 0.40599870681762695
train_iter_loss: 0.23095275461673737
train_iter_loss: 0.3910023272037506
train_iter_loss: 0.3465762436389923
train_iter_loss: 0.32742470502853394
train_iter_loss: 0.16255633533000946
train_iter_loss: 0.2621714770793915
train_iter_loss: 0.39067938923835754
train_iter_loss: 0.3157443404197693
train_iter_loss: 0.24159139394760132
train_iter_loss: 0.2459052950143814
train_iter_loss: 0.4118492603302002
train_iter_loss: 0.17072859406471252
train_iter_loss: 0.2976190447807312
train_iter_loss: 0.2831920087337494
train_iter_loss: 0.3296035826206207
train_iter_loss: 0.23534421622753143
train_iter_loss: 0.15952442586421967
train_iter_loss: 0.32397687435150146
train_iter_loss: 0.4002012312412262
train_iter_loss: 0.26952415704727173
train_iter_loss: 0.48213788866996765
train_iter_loss: 0.18530400097370148
train_iter_loss: 0.45963287353515625
train_iter_loss: 0.29760730266571045
train_iter_loss: 0.22582367062568665
train_iter_loss: 0.3352793753147125
train_iter_loss: 0.19810041785240173
train_iter_loss: 0.17856717109680176
train_iter_loss: 0.23929260671138763
train_iter_loss: 0.2809504270553589
train_iter_loss: 0.379266619682312
train_iter_loss: 0.35153403878211975
train_iter_loss: 0.3917931020259857
train_iter_loss: 0.36418697237968445
train_iter_loss: 0.2681424617767334
train_iter_loss: 0.2657916247844696
train_iter_loss: 0.30656081438064575
train_iter_loss: 0.22945775091648102
train_iter_loss: 0.33258408308029175
train_iter_loss: 0.17218607664108276
train_iter_loss: 0.2695344090461731
train_iter_loss: 0.2256655991077423
train_iter_loss: 0.2408011257648468
train_iter_loss: 0.23090790212154388
train_iter_loss: 0.25089797377586365
train_iter_loss: 0.2249225378036499
train_iter_loss: 0.3377763032913208
train_iter_loss: 0.39933887124061584
train_iter_loss: 0.22825537621974945
train_iter_loss: 0.2694012522697449
train_iter_loss: 0.35173937678337097
train_iter_loss: 0.27885890007019043
train_iter_loss: 0.2800305485725403
train_iter_loss: 0.2778293192386627
train_iter_loss: 0.4406859278678894
train_iter_loss: 0.25478342175483704
train_iter_loss: 0.30500346422195435
train_iter_loss: 0.3797207474708557
train_iter_loss: 0.24809634685516357
train_iter_loss: 0.2603614926338196
train_iter_loss: 0.24568843841552734
train_iter_loss: 0.18943066895008087
train_iter_loss: 0.33248770236968994
train_iter_loss: 0.22686776518821716
train_iter_loss: 0.3373872637748718
train_iter_loss: 0.35537856817245483
train_iter_loss: 0.21371114253997803
train_iter_loss: 0.24503043293952942
train_iter_loss: 0.36523178219795227
train_iter_loss: 0.39354339241981506
train loss :0.2989
---------------------
Validation seg loss: 0.37612514292715854 at epoch 147
epoch =    148/  1000, exp = train
train_iter_loss: 0.25456005334854126
train_iter_loss: 0.3212283253669739
train_iter_loss: 0.32072463631629944
train_iter_loss: 0.1716567575931549
train_iter_loss: 0.2750316262245178
train_iter_loss: 0.26841408014297485
train_iter_loss: 0.24778538942337036
train_iter_loss: 0.30926045775413513
train_iter_loss: 0.21013769507408142
train_iter_loss: 0.37746578454971313
train_iter_loss: 0.37873896956443787
train_iter_loss: 0.3930871784687042
train_iter_loss: 0.37715020775794983
train_iter_loss: 0.24055233597755432
train_iter_loss: 0.21602168679237366
train_iter_loss: 0.3636696934700012
train_iter_loss: 0.2761983275413513
train_iter_loss: 0.4713044762611389
train_iter_loss: 0.2906135022640228
train_iter_loss: 0.23070453107357025
train_iter_loss: 0.3554997742176056
train_iter_loss: 0.3782450556755066
train_iter_loss: 0.31514790654182434
train_iter_loss: 0.2945498526096344
train_iter_loss: 0.22985833883285522
train_iter_loss: 0.23638799786567688
train_iter_loss: 0.3458159863948822
train_iter_loss: 0.22953473031520844
train_iter_loss: 0.43850481510162354
train_iter_loss: 0.19918924570083618
train_iter_loss: 0.3728402554988861
train_iter_loss: 0.2220313400030136
train_iter_loss: 0.28380653262138367
train_iter_loss: 0.2869453430175781
train_iter_loss: 0.331163227558136
train_iter_loss: 0.2900110185146332
train_iter_loss: 0.3262059688568115
train_iter_loss: 0.26524806022644043
train_iter_loss: 0.22146107256412506
train_iter_loss: 0.3672497272491455
train_iter_loss: 0.22916091978549957
train_iter_loss: 0.24946348369121552
train_iter_loss: 0.28408172726631165
train_iter_loss: 0.27457335591316223
train_iter_loss: 0.30383849143981934
train_iter_loss: 0.22893023490905762
train_iter_loss: 0.22052644193172455
train_iter_loss: 0.2835331857204437
train_iter_loss: 0.32007718086242676
train_iter_loss: 0.24590276181697845
train_iter_loss: 0.3950246274471283
train_iter_loss: 0.3814462721347809
train_iter_loss: 0.3581677973270416
train_iter_loss: 0.2298598736524582
train_iter_loss: 0.31017181277275085
train_iter_loss: 0.2270543873310089
train_iter_loss: 0.1666296422481537
train_iter_loss: 0.3414003551006317
train_iter_loss: 0.1983664482831955
train_iter_loss: 0.22638900578022003
train_iter_loss: 0.25846704840660095
train_iter_loss: 0.30031922459602356
train_iter_loss: 0.4251602590084076
train_iter_loss: 0.37942641973495483
train_iter_loss: 0.14802393317222595
train_iter_loss: 0.1991538107395172
train_iter_loss: 0.4063659608364105
train_iter_loss: 0.35735389590263367
train_iter_loss: 0.3416483700275421
train_iter_loss: 0.24801532924175262
train_iter_loss: 0.3145900070667267
train_iter_loss: 0.3965432643890381
train_iter_loss: 0.34051981568336487
train_iter_loss: 0.1452328860759735
train_iter_loss: 0.27279233932495117
train_iter_loss: 0.27831438183784485
train_iter_loss: 0.30264490842819214
train_iter_loss: 0.2947627007961273
train_iter_loss: 0.2889055013656616
train_iter_loss: 0.27574706077575684
train_iter_loss: 0.5233467817306519
train_iter_loss: 0.21899770200252533
train_iter_loss: 0.23174400627613068
train_iter_loss: 0.2170674055814743
train_iter_loss: 0.2289394736289978
train_iter_loss: 0.3441077172756195
train_iter_loss: 0.4259888529777527
train_iter_loss: 0.19582968950271606
train_iter_loss: 0.3904759883880615
train_iter_loss: 0.4878822863101959
train_iter_loss: 0.26293250918388367
train_iter_loss: 0.3286973834037781
train_iter_loss: 0.26458093523979187
train_iter_loss: 0.4056120812892914
train_iter_loss: 0.2523779571056366
train_iter_loss: 0.3431480824947357
train_iter_loss: 0.1753358542919159
train_iter_loss: 0.26626595854759216
train_iter_loss: 0.24919526278972626
train_iter_loss: 0.2157890349626541
train loss :0.2976
---------------------
Validation seg loss: 0.3793853022738026 at epoch 148
epoch =    149/  1000, exp = train
train_iter_loss: 0.2690427601337433
train_iter_loss: 0.1991783231496811
train_iter_loss: 0.1664821207523346
train_iter_loss: 0.33612409234046936
train_iter_loss: 0.24433742463588715
train_iter_loss: 0.3004511892795563
train_iter_loss: 0.25065937638282776
train_iter_loss: 0.37813058495521545
train_iter_loss: 0.37424007058143616
train_iter_loss: 0.33506864309310913
train_iter_loss: 0.15192048251628876
train_iter_loss: 0.3933238387107849
train_iter_loss: 0.3744094967842102
train_iter_loss: 0.17798133194446564
train_iter_loss: 0.23009561002254486
train_iter_loss: 0.1900656819343567
train_iter_loss: 0.3241867423057556
train_iter_loss: 0.21885813772678375
train_iter_loss: 0.298798143863678
train_iter_loss: 0.2407330423593521
train_iter_loss: 0.16593962907791138
train_iter_loss: 0.2398379147052765
train_iter_loss: 0.2753678262233734
train_iter_loss: 0.3654918372631073
train_iter_loss: 0.3025340139865875
train_iter_loss: 0.28722265362739563
train_iter_loss: 0.4061713218688965
train_iter_loss: 0.2193928211927414
train_iter_loss: 0.23555199801921844
train_iter_loss: 0.1067655086517334
train_iter_loss: 0.215885192155838
train_iter_loss: 0.1974000632762909
train_iter_loss: 0.3344019651412964
train_iter_loss: 0.41932734847068787
train_iter_loss: 0.3074583113193512
train_iter_loss: 0.19029748439788818
train_iter_loss: 0.32946982979774475
train_iter_loss: 0.36153483390808105
train_iter_loss: 0.3546324372291565
train_iter_loss: 0.2529754638671875
train_iter_loss: 0.2203044891357422
train_iter_loss: 0.5205011963844299
train_iter_loss: 0.3775503635406494
train_iter_loss: 0.3570115864276886
train_iter_loss: 0.5494061708450317
train_iter_loss: 0.2754479944705963
train_iter_loss: 0.22219666838645935
train_iter_loss: 0.3403703272342682
train_iter_loss: 0.2615396976470947
train_iter_loss: 0.27708619832992554
train_iter_loss: 0.25181812047958374
train_iter_loss: 0.23391032218933105
train_iter_loss: 0.35005250573158264
train_iter_loss: 0.30593645572662354
train_iter_loss: 0.40187564492225647
train_iter_loss: 0.4086812436580658
train_iter_loss: 0.2565658390522003
train_iter_loss: 0.17148451507091522
train_iter_loss: 0.2918927073478699
train_iter_loss: 0.33299151062965393
train_iter_loss: 0.34686821699142456
train_iter_loss: 0.19432194530963898
train_iter_loss: 0.3448745012283325
train_iter_loss: 0.2800259292125702
train_iter_loss: 0.3267132043838501
train_iter_loss: 0.2601180374622345
train_iter_loss: 0.2806076109409332
train_iter_loss: 0.340070903301239
train_iter_loss: 0.4017511308193207
train_iter_loss: 0.3090941905975342
train_iter_loss: 0.48650702834129333
train_iter_loss: 0.27863574028015137
train_iter_loss: 0.38299664855003357
train_iter_loss: 0.30165016651153564
train_iter_loss: 0.21507994830608368
train_iter_loss: 0.24570496380329132
train_iter_loss: 0.30754438042640686
train_iter_loss: 0.3195151686668396
train_iter_loss: 0.32761502265930176
train_iter_loss: 0.2677055299282074
train_iter_loss: 0.3486728370189667
train_iter_loss: 0.3377262055873871
train_iter_loss: 0.31664803624153137
train_iter_loss: 0.274470716714859
train_iter_loss: 0.22320586442947388
train_iter_loss: 0.3562922775745392
train_iter_loss: 0.05945814773440361
train_iter_loss: 0.2984285354614258
train_iter_loss: 0.35924121737480164
train_iter_loss: 0.4230503737926483
train_iter_loss: 0.21651042997837067
train_iter_loss: 0.30885282158851624
train_iter_loss: 0.29140976071357727
train_iter_loss: 0.36204618215560913
train_iter_loss: 0.2149515450000763
train_iter_loss: 0.22752195596694946
train_iter_loss: 0.1936340034008026
train_iter_loss: 0.3926810324192047
train_iter_loss: 0.22642982006072998
train_iter_loss: 0.21087485551834106
train loss :0.2959
---------------------
Validation seg loss: 0.39050510359646856 at epoch 149
epoch =    150/  1000, exp = train
train_iter_loss: 0.30838215351104736
train_iter_loss: 0.37825101613998413
train_iter_loss: 0.21274156868457794
train_iter_loss: 0.22200019657611847
train_iter_loss: 0.3781535029411316
train_iter_loss: 0.28820255398750305
train_iter_loss: 0.25860297679901123
train_iter_loss: 0.3550166189670563
train_iter_loss: 0.323590487241745
train_iter_loss: 0.2973768413066864
train_iter_loss: 0.4073978364467621
train_iter_loss: 0.29085662961006165
train_iter_loss: 0.2690608501434326
train_iter_loss: 0.3105814456939697
train_iter_loss: 0.281821608543396
train_iter_loss: 0.5137481689453125
train_iter_loss: 0.25741299986839294
train_iter_loss: 0.31144800782203674
train_iter_loss: 0.26135140657424927
train_iter_loss: 0.41150742769241333
train_iter_loss: 0.2543417513370514
train_iter_loss: 0.3377644717693329
train_iter_loss: 0.2335086315870285
train_iter_loss: 0.42132583260536194
train_iter_loss: 0.25144317746162415
train_iter_loss: 0.20464424788951874
train_iter_loss: 0.3256323039531708
train_iter_loss: 0.29699867963790894
train_iter_loss: 0.2741120457649231
train_iter_loss: 0.3278409540653229
train_iter_loss: 0.36421164870262146
train_iter_loss: 0.2535828948020935
train_iter_loss: 0.4246472418308258
train_iter_loss: 0.24025827646255493
train_iter_loss: 0.3021692931652069
train_iter_loss: 0.2929409146308899
train_iter_loss: 0.15848524868488312
train_iter_loss: 0.2813439965248108
train_iter_loss: 0.5941217541694641
train_iter_loss: 0.2791344225406647
train_iter_loss: 0.20933248102664948
train_iter_loss: 0.3770001530647278
train_iter_loss: 0.26943421363830566
train_iter_loss: 0.32063332200050354
train_iter_loss: 0.22268013656139374
train_iter_loss: 0.26405856013298035
train_iter_loss: 0.36563360691070557
train_iter_loss: 0.14989688992500305
train_iter_loss: 0.42243891954421997
train_iter_loss: 0.22770605981349945
train_iter_loss: 0.21943429112434387
train_iter_loss: 0.34962770342826843
train_iter_loss: 0.1847633272409439
train_iter_loss: 0.3365793228149414
train_iter_loss: 0.25614896416664124
train_iter_loss: 0.3255389332771301
train_iter_loss: 0.4091204106807709
train_iter_loss: 0.2362585961818695
train_iter_loss: 0.21340247988700867
train_iter_loss: 0.21552005410194397
train_iter_loss: 0.371926486492157
train_iter_loss: 0.27135130763053894
train_iter_loss: 0.19739478826522827
train_iter_loss: 0.22758223116397858
train_iter_loss: 0.2743021249771118
train_iter_loss: 0.21427443623542786
train_iter_loss: 0.3953568637371063
train_iter_loss: 0.12021559476852417
train_iter_loss: 0.46201664209365845
train_iter_loss: 0.44170302152633667
train_iter_loss: 0.18956351280212402
train_iter_loss: 0.21258266270160675
train_iter_loss: 0.30080246925354004
train_iter_loss: 0.6893528699874878
train_iter_loss: 0.23536017537117004
train_iter_loss: 0.23853951692581177
train_iter_loss: 0.2718723714351654
train_iter_loss: 0.26932138204574585
train_iter_loss: 0.28485777974128723
train_iter_loss: 0.2857307493686676
train_iter_loss: 0.3376805782318115
train_iter_loss: 0.20787090063095093
train_iter_loss: 0.37657201290130615
train_iter_loss: 0.20366270840168
train_iter_loss: 0.21667668223381042
train_iter_loss: 0.210881769657135
train_iter_loss: 0.30017566680908203
train_iter_loss: 0.29725080728530884
train_iter_loss: 0.28434261679649353
train_iter_loss: 0.4694846570491791
train_iter_loss: 0.3152477443218231
train_iter_loss: 0.2536325752735138
train_iter_loss: 0.28009286522865295
train_iter_loss: 0.2500278353691101
train_iter_loss: 0.38512876629829407
train_iter_loss: 0.4344448149204254
train_iter_loss: 0.27150556445121765
train_iter_loss: 0.1959228664636612
train_iter_loss: 0.2601196765899658
train_iter_loss: 0.2808810770511627
train loss :0.3011
---------------------
Validation seg loss: 0.3697925083128349 at epoch 150
epoch =    151/  1000, exp = train
train_iter_loss: 0.22368204593658447
train_iter_loss: 0.48593395948410034
train_iter_loss: 0.31543853878974915
train_iter_loss: 0.17647680640220642
train_iter_loss: 0.3572066128253937
train_iter_loss: 0.1924719214439392
train_iter_loss: 0.45766890048980713
train_iter_loss: 0.1611364483833313
train_iter_loss: 0.36824068427085876
train_iter_loss: 0.3648177683353424
train_iter_loss: 0.2924417555332184
train_iter_loss: 0.26013168692588806
train_iter_loss: 0.32952672243118286
train_iter_loss: 0.5538130402565002
train_iter_loss: 0.26060423254966736
train_iter_loss: 0.4284217059612274
train_iter_loss: 0.20745906233787537
train_iter_loss: 0.31665387749671936
train_iter_loss: 0.26633426547050476
train_iter_loss: 0.2869054675102234
train_iter_loss: 0.3316163718700409
train_iter_loss: 0.2924395203590393
train_iter_loss: 0.2064002901315689
train_iter_loss: 0.21827077865600586
train_iter_loss: 0.2607593536376953
train_iter_loss: 0.1111321970820427
train_iter_loss: 0.39583832025527954
train_iter_loss: 0.29629233479499817
train_iter_loss: 0.2634994387626648
train_iter_loss: 0.3894161581993103
train_iter_loss: 0.1441168189048767
train_iter_loss: 0.1817357838153839
train_iter_loss: 0.34733930230140686
train_iter_loss: 0.2349775731563568
train_iter_loss: 0.428316205739975
train_iter_loss: 0.2765311300754547
train_iter_loss: 0.4979422688484192
train_iter_loss: 0.2768237888813019
train_iter_loss: 0.36730536818504333
train_iter_loss: 0.24978920817375183
train_iter_loss: 0.41873615980148315
train_iter_loss: 0.24125103652477264
train_iter_loss: 0.2488558441400528
train_iter_loss: 0.2996312081813812
train_iter_loss: 0.22148370742797852
train_iter_loss: 0.30164363980293274
train_iter_loss: 0.17381742596626282
train_iter_loss: 0.3457293212413788
train_iter_loss: 0.2514920234680176
train_iter_loss: 0.3230173885822296
train_iter_loss: 0.18937146663665771
train_iter_loss: 0.25868043303489685
train_iter_loss: 0.40060457587242126
train_iter_loss: 0.18859145045280457
train_iter_loss: 0.23317332565784454
train_iter_loss: 0.2206372767686844
train_iter_loss: 0.2740200459957123
train_iter_loss: 0.407336562871933
train_iter_loss: 0.3759949803352356
train_iter_loss: 0.3523673713207245
train_iter_loss: 0.2643052637577057
train_iter_loss: 0.27685704827308655
train_iter_loss: 0.29318591952323914
train_iter_loss: 0.13292533159255981
train_iter_loss: 0.32413822412490845
train_iter_loss: 0.15097564458847046
train_iter_loss: 0.17507697641849518
train_iter_loss: 0.3588292598724365
train_iter_loss: 0.3460195064544678
train_iter_loss: 0.1788061261177063
train_iter_loss: 0.37764716148376465
train_iter_loss: 0.35254570841789246
train_iter_loss: 0.434437096118927
train_iter_loss: 0.33448395133018494
train_iter_loss: 0.37938106060028076
train_iter_loss: 0.3039495050907135
train_iter_loss: 0.39109566807746887
train_iter_loss: 0.3183079957962036
train_iter_loss: 0.28084883093833923
train_iter_loss: 0.3381042182445526
train_iter_loss: 0.30821526050567627
train_iter_loss: 0.39079198241233826
train_iter_loss: 0.301313191652298
train_iter_loss: 0.28818172216415405
train_iter_loss: 0.26270490884780884
train_iter_loss: 0.31209951639175415
train_iter_loss: 0.2744485139846802
train_iter_loss: 0.2919784486293793
train_iter_loss: 0.31782132387161255
train_iter_loss: 0.19575002789497375
train_iter_loss: 0.2510206401348114
train_iter_loss: 0.3125978410243988
train_iter_loss: 0.1761060655117035
train_iter_loss: 0.3327065110206604
train_iter_loss: 0.31573939323425293
train_iter_loss: 0.36506786942481995
train_iter_loss: 0.23094560205936432
train_iter_loss: 0.24357764422893524
train_iter_loss: 0.28604209423065186
train_iter_loss: 0.21392008662223816
train loss :0.2979
---------------------
Validation seg loss: 0.37346822752634873 at epoch 151
epoch =    152/  1000, exp = train
train_iter_loss: 0.23935218155384064
train_iter_loss: 0.24303896725177765
train_iter_loss: 0.40925177931785583
train_iter_loss: 0.21703177690505981
train_iter_loss: 0.20957225561141968
train_iter_loss: 0.4128086268901825
train_iter_loss: 0.3938128352165222
train_iter_loss: 0.25478920340538025
train_iter_loss: 0.16416054964065552
train_iter_loss: 0.23035751283168793
train_iter_loss: 0.2935091555118561
train_iter_loss: 0.22885146737098694
train_iter_loss: 0.2719108462333679
train_iter_loss: 0.25815722346305847
train_iter_loss: 0.43174663186073303
train_iter_loss: 0.30253511667251587
train_iter_loss: 0.31162646412849426
train_iter_loss: 0.21468646824359894
train_iter_loss: 0.31945157051086426
train_iter_loss: 0.3422161936759949
train_iter_loss: 0.29991281032562256
train_iter_loss: 0.3068859875202179
train_iter_loss: 0.3709128499031067
train_iter_loss: 0.24403207004070282
train_iter_loss: 0.25498655438423157
train_iter_loss: 0.32810214161872864
train_iter_loss: 0.304315984249115
train_iter_loss: 0.25613299012184143
train_iter_loss: 0.30182090401649475
train_iter_loss: 0.28299078345298767
train_iter_loss: 0.14079445600509644
train_iter_loss: 0.170413538813591
train_iter_loss: 0.2672346830368042
train_iter_loss: 0.13068804144859314
train_iter_loss: 0.1245928406715393
train_iter_loss: 0.20093435049057007
train_iter_loss: 0.28535348176956177
train_iter_loss: 0.22392374277114868
train_iter_loss: 0.0428638830780983
train_iter_loss: 0.25526025891304016
train_iter_loss: 0.34020140767097473
train_iter_loss: 0.36356121301651
train_iter_loss: 0.32792675495147705
train_iter_loss: 0.2576799690723419
train_iter_loss: 0.3722366988658905
train_iter_loss: 0.34624379873275757
train_iter_loss: 0.2449321746826172
train_iter_loss: 0.2170199453830719
train_iter_loss: 0.3860921859741211
train_iter_loss: 0.28605255484580994
train_iter_loss: 0.3920219838619232
train_iter_loss: 0.2550972104072571
train_iter_loss: 0.3562494218349457
train_iter_loss: 0.18782609701156616
train_iter_loss: 0.19959011673927307
train_iter_loss: 0.28655603528022766
train_iter_loss: 0.4155118763446808
train_iter_loss: 0.3547876477241516
train_iter_loss: 0.3583003282546997
train_iter_loss: 0.20548436045646667
train_iter_loss: 0.15368235111236572
train_iter_loss: 0.360134482383728
train_iter_loss: 0.43018725514411926
train_iter_loss: 0.18410004675388336
train_iter_loss: 0.44913291931152344
train_iter_loss: 0.2769394516944885
train_iter_loss: 0.18793578445911407
train_iter_loss: 0.5224618315696716
train_iter_loss: 0.3058528006076813
train_iter_loss: 0.39318200945854187
train_iter_loss: 0.3970584273338318
train_iter_loss: 0.2399899959564209
train_iter_loss: 0.2644483149051666
train_iter_loss: 0.24990199506282806
train_iter_loss: 0.20051772892475128
train_iter_loss: 0.2628518342971802
train_iter_loss: 0.18375946581363678
train_iter_loss: 0.35294532775878906
train_iter_loss: 0.5748034715652466
train_iter_loss: 0.19045139849185944
train_iter_loss: 0.2928146719932556
train_iter_loss: 0.28165316581726074
train_iter_loss: 0.250867635011673
train_iter_loss: 0.4658351540565491
train_iter_loss: 0.46442940831184387
train_iter_loss: 0.31868675351142883
train_iter_loss: 0.18144498765468597
train_iter_loss: 0.32048654556274414
train_iter_loss: 0.2801681160926819
train_iter_loss: 0.3319545388221741
train_iter_loss: 0.5230787992477417
train_iter_loss: 0.3285531997680664
train_iter_loss: 0.34510400891304016
train_iter_loss: 0.4039156436920166
train_iter_loss: 0.15972867608070374
train_iter_loss: 0.25602689385414124
train_iter_loss: 0.24927063286304474
train_iter_loss: 0.2855059802532196
train_iter_loss: 0.24591580033302307
train_iter_loss: 0.2723481059074402
train loss :0.2943
---------------------
Validation seg loss: 0.3781202586789457 at epoch 152
epoch =    153/  1000, exp = train
train_iter_loss: 0.35069555044174194
train_iter_loss: 0.2905217111110687
train_iter_loss: 0.2799804210662842
train_iter_loss: 0.5958773493766785
train_iter_loss: 0.3823927938938141
train_iter_loss: 0.20268724858760834
train_iter_loss: 0.41576987504959106
train_iter_loss: 0.37619584798812866
train_iter_loss: 0.39303627610206604
train_iter_loss: 0.20828819274902344
train_iter_loss: 0.31789323687553406
train_iter_loss: 0.3857918977737427
train_iter_loss: 0.21579785645008087
train_iter_loss: 0.3312565088272095
train_iter_loss: 0.402197927236557
train_iter_loss: 0.23580597341060638
train_iter_loss: 0.3633977472782135
train_iter_loss: 0.17604073882102966
train_iter_loss: 0.28488990664482117
train_iter_loss: 0.14721612632274628
train_iter_loss: 0.26187169551849365
train_iter_loss: 0.26877549290657043
train_iter_loss: 0.21925827860832214
train_iter_loss: 0.21603886783123016
train_iter_loss: 0.31517162919044495
train_iter_loss: 0.23882299661636353
train_iter_loss: 0.3588881194591522
train_iter_loss: 0.19772745668888092
train_iter_loss: 0.18596921861171722
train_iter_loss: 0.2760259807109833
train_iter_loss: 0.2866603434085846
train_iter_loss: 0.2484828680753708
train_iter_loss: 0.4034036695957184
train_iter_loss: 0.18869534134864807
train_iter_loss: 0.30275532603263855
train_iter_loss: 0.21606549620628357
train_iter_loss: 0.3592855632305145
train_iter_loss: 0.3338451683521271
train_iter_loss: 0.2694186866283417
train_iter_loss: 0.28350168466567993
train_iter_loss: 0.38997983932495117
train_iter_loss: 0.3842757046222687
train_iter_loss: 0.252754807472229
train_iter_loss: 0.3422216773033142
train_iter_loss: 0.3183002769947052
train_iter_loss: 0.14781275391578674
train_iter_loss: 0.3087505102157593
train_iter_loss: 0.27829065918922424
train_iter_loss: 0.14423425495624542
train_iter_loss: 0.1536795198917389
train_iter_loss: 0.2931646704673767
train_iter_loss: 0.3418434262275696
train_iter_loss: 0.32627809047698975
train_iter_loss: 0.3035319149494171
train_iter_loss: 0.2319544106721878
train_iter_loss: 0.11466580629348755
train_iter_loss: 0.2329920381307602
train_iter_loss: 0.3303866386413574
train_iter_loss: 0.48380711674690247
train_iter_loss: 0.22299258410930634
train_iter_loss: 0.2958005666732788
train_iter_loss: 0.2256220430135727
train_iter_loss: 0.33654090762138367
train_iter_loss: 0.23474247753620148
train_iter_loss: 0.24795304238796234
train_iter_loss: 0.31441211700439453
train_iter_loss: 0.2874956429004669
train_iter_loss: 0.21166673302650452
train_iter_loss: 0.382429301738739
train_iter_loss: 0.22113671898841858
train_iter_loss: 0.3834677040576935
train_iter_loss: 0.19327816367149353
train_iter_loss: 0.34891051054000854
train_iter_loss: 0.36013227701187134
train_iter_loss: 0.3074188530445099
train_iter_loss: 0.2838686406612396
train_iter_loss: 0.2802521288394928
train_iter_loss: 0.35287684202194214
train_iter_loss: 0.300857275724411
train_iter_loss: 0.33706435561180115
train_iter_loss: 0.36417722702026367
train_iter_loss: 0.15161143243312836
train_iter_loss: 0.16890676319599152
train_iter_loss: 0.33346545696258545
train_iter_loss: 0.2723328471183777
train_iter_loss: 0.3046649098396301
train_iter_loss: 0.37200450897216797
train_iter_loss: 0.25008854269981384
train_iter_loss: 0.43683603405952454
train_iter_loss: 0.28722327947616577
train_iter_loss: 0.38526153564453125
train_iter_loss: 0.24729323387145996
train_iter_loss: 0.19064292311668396
train_iter_loss: 0.16786327958106995
train_iter_loss: 0.3749787211418152
train_iter_loss: 0.26963770389556885
train_iter_loss: 0.23211942613124847
train_iter_loss: 0.29245057702064514
train_iter_loss: 0.3757825195789337
train_iter_loss: 0.3839147984981537
train loss :0.2945
---------------------
Validation seg loss: 0.3763408453098305 at epoch 153
epoch =    154/  1000, exp = train
train_iter_loss: 0.3249821662902832
train_iter_loss: 0.29721900820732117
train_iter_loss: 0.3587642014026642
train_iter_loss: 0.3197934925556183
train_iter_loss: 0.2267797887325287
train_iter_loss: 0.7586043477058411
train_iter_loss: 0.2294272482395172
train_iter_loss: 0.2429187297821045
train_iter_loss: 0.3768123686313629
train_iter_loss: 0.24754376709461212
train_iter_loss: 0.17459657788276672
train_iter_loss: 0.28480657935142517
train_iter_loss: 0.24851050972938538
train_iter_loss: 0.1975974589586258
train_iter_loss: 0.23683446645736694
train_iter_loss: 0.3492334187030792
train_iter_loss: 0.4251408874988556
train_iter_loss: 0.40137383341789246
train_iter_loss: 0.2253800481557846
train_iter_loss: 0.3485969007015228
train_iter_loss: 0.267719030380249
train_iter_loss: 0.3209921717643738
train_iter_loss: 0.40813830494880676
train_iter_loss: 0.23131413757801056
train_iter_loss: 0.18969285488128662
train_iter_loss: 0.3110710084438324
train_iter_loss: 0.2873528003692627
train_iter_loss: 0.2475113719701767
train_iter_loss: 0.3000323474407196
train_iter_loss: 0.24612540006637573
train_iter_loss: 0.33230626583099365
train_iter_loss: 0.25282981991767883
train_iter_loss: 0.1854754239320755
train_iter_loss: 0.2047346830368042
train_iter_loss: 0.24123625457286835
train_iter_loss: 0.29972749948501587
train_iter_loss: 0.26607444882392883
train_iter_loss: 0.4196324646472931
train_iter_loss: 0.3163195550441742
train_iter_loss: 0.2776985168457031
train_iter_loss: 0.3032127916812897
train_iter_loss: 0.3245161473751068
train_iter_loss: 0.22490300238132477
train_iter_loss: 0.3136613965034485
train_iter_loss: 0.24215564131736755
train_iter_loss: 0.2760488986968994
train_iter_loss: 0.32767707109451294
train_iter_loss: 0.23044364154338837
train_iter_loss: 0.32403817772865295
train_iter_loss: 0.27677249908447266
train_iter_loss: 0.2345893532037735
train_iter_loss: 0.22880129516124725
train_iter_loss: 0.43775489926338196
train_iter_loss: 0.34935396909713745
train_iter_loss: 0.3450193405151367
train_iter_loss: 0.3032922148704529
train_iter_loss: 0.3722509741783142
train_iter_loss: 0.22811876237392426
train_iter_loss: 0.06424923986196518
train_iter_loss: 0.33069196343421936
train_iter_loss: 0.26263681054115295
train_iter_loss: 0.2724290192127228
train_iter_loss: 0.2841830551624298
train_iter_loss: 0.2988176941871643
train_iter_loss: 0.3575875163078308
train_iter_loss: 0.23540139198303223
train_iter_loss: 0.46927064657211304
train_iter_loss: 0.31805944442749023
train_iter_loss: 0.2952534854412079
train_iter_loss: 0.2901369035243988
train_iter_loss: 0.28681880235671997
train_iter_loss: 0.24429859220981598
train_iter_loss: 0.30738165974617004
train_iter_loss: 0.2999425232410431
train_iter_loss: 0.6203709840774536
train_iter_loss: 0.149152934551239
train_iter_loss: 0.31441885232925415
train_iter_loss: 0.26999741792678833
train_iter_loss: 0.23655223846435547
train_iter_loss: 0.3617335855960846
train_iter_loss: 0.2971416413784027
train_iter_loss: 0.21945162117481232
train_iter_loss: 0.2494068443775177
train_iter_loss: 0.27701061964035034
train_iter_loss: 0.19559802114963531
train_iter_loss: 0.2540150582790375
train_iter_loss: 0.3058399558067322
train_iter_loss: 0.34137797355651855
train_iter_loss: 0.3396029770374298
train_iter_loss: 0.21033483743667603
train_iter_loss: 0.3497147262096405
train_iter_loss: 0.32911771535873413
train_iter_loss: 0.3105396628379822
train_iter_loss: 0.290749192237854
train_iter_loss: 0.1446261703968048
train_iter_loss: 0.3014829754829407
train_iter_loss: 0.19844506680965424
train_iter_loss: 0.3296378552913666
train_iter_loss: 0.22893160581588745
train_iter_loss: 0.28673872351646423
train loss :0.2955
---------------------
Validation seg loss: 0.373369529368883 at epoch 154
epoch =    155/  1000, exp = train
train_iter_loss: 0.382533997297287
train_iter_loss: 0.22651872038841248
train_iter_loss: 0.24452458322048187
train_iter_loss: 0.21151287853717804
train_iter_loss: 0.3153850734233856
train_iter_loss: 0.25838837027549744
train_iter_loss: 0.24097265303134918
train_iter_loss: 0.23910124599933624
train_iter_loss: 0.464575856924057
train_iter_loss: 0.30456942319869995
train_iter_loss: 0.17638176679611206
train_iter_loss: 0.22130440175533295
train_iter_loss: 0.38583701848983765
train_iter_loss: 0.4529210925102234
train_iter_loss: 0.42911532521247864
train_iter_loss: 0.2510001063346863
train_iter_loss: 0.5444492101669312
train_iter_loss: 0.2507481575012207
train_iter_loss: 0.3522501587867737
train_iter_loss: 0.1860145777463913
train_iter_loss: 0.30330690741539
train_iter_loss: 0.2925552725791931
train_iter_loss: 0.36107486486434937
train_iter_loss: 0.31531262397766113
train_iter_loss: 0.32471540570259094
train_iter_loss: 0.19001035392284393
train_iter_loss: 0.4631955623626709
train_iter_loss: 0.3006749451160431
train_iter_loss: 0.26140034198760986
train_iter_loss: 0.2844987213611603
train_iter_loss: 0.34302374720573425
train_iter_loss: 0.18483534455299377
train_iter_loss: 0.24279676377773285
train_iter_loss: 0.30418750643730164
train_iter_loss: 0.37886667251586914
train_iter_loss: 0.2651808261871338
train_iter_loss: 0.14468111097812653
train_iter_loss: 0.21824763715267181
train_iter_loss: 0.1577555388212204
train_iter_loss: 0.1705613136291504
train_iter_loss: 0.32097992300987244
train_iter_loss: 0.11359286308288574
train_iter_loss: 0.4138169586658478
train_iter_loss: 0.3692752718925476
train_iter_loss: 0.3971792757511139
train_iter_loss: 0.2837705612182617
train_iter_loss: 0.3745315670967102
train_iter_loss: 0.4059159457683563
train_iter_loss: 0.2299904227256775
train_iter_loss: 0.302121639251709
train_iter_loss: 0.21641087532043457
train_iter_loss: 0.23241978883743286
train_iter_loss: 0.2750821113586426
train_iter_loss: 0.2968447208404541
train_iter_loss: 0.34071388840675354
train_iter_loss: 0.29343295097351074
train_iter_loss: 0.2841600179672241
train_iter_loss: 0.20346000790596008
train_iter_loss: 0.2203187495470047
train_iter_loss: 0.2601149082183838
train_iter_loss: 0.2535925805568695
train_iter_loss: 0.281162291765213
train_iter_loss: 0.3903404176235199
train_iter_loss: 0.26131671667099
train_iter_loss: 0.27721795439720154
train_iter_loss: 0.3188031315803528
train_iter_loss: 0.31599941849708557
train_iter_loss: 0.3825097680091858
train_iter_loss: 0.2258048951625824
train_iter_loss: 0.24518123269081116
train_iter_loss: 0.3711342215538025
train_iter_loss: 0.3483186662197113
train_iter_loss: 0.2502303123474121
train_iter_loss: 0.24266429245471954
train_iter_loss: 0.42821288108825684
train_iter_loss: 0.21581657230854034
train_iter_loss: 0.3065372705459595
train_iter_loss: 0.28922519087791443
train_iter_loss: 0.23543712496757507
train_iter_loss: 0.2924834191799164
train_iter_loss: 0.22188681364059448
train_iter_loss: 0.23290053009986877
train_iter_loss: 0.29662957787513733
train_iter_loss: 0.2055673897266388
train_iter_loss: 0.3143405318260193
train_iter_loss: 0.3845697045326233
train_iter_loss: 0.21269136667251587
train_iter_loss: 0.40388035774230957
train_iter_loss: 0.39419594407081604
train_iter_loss: 0.23238156735897064
train_iter_loss: 0.3000997304916382
train_iter_loss: 0.2669769525527954
train_iter_loss: 0.21270732581615448
train_iter_loss: 0.1910134255886078
train_iter_loss: 0.42612820863723755
train_iter_loss: 0.3051397502422333
train_iter_loss: 0.3057713508605957
train_iter_loss: 0.2936299443244934
train_iter_loss: 0.33112263679504395
train_iter_loss: 0.3024461567401886
train loss :0.2954
---------------------
Validation seg loss: 0.3720539617264327 at epoch 155
epoch =    156/  1000, exp = train
train_iter_loss: 0.5437737703323364
train_iter_loss: 0.2922074496746063
train_iter_loss: 0.256081223487854
train_iter_loss: 0.26210078597068787
train_iter_loss: 0.24602575600147247
train_iter_loss: 0.3509928286075592
train_iter_loss: 0.35200101137161255
train_iter_loss: 0.19870130717754364
train_iter_loss: 0.3289985954761505
train_iter_loss: 0.48737016320228577
train_iter_loss: 0.2155737578868866
train_iter_loss: 0.35777539014816284
train_iter_loss: 0.38153940439224243
train_iter_loss: 0.3455509841442108
train_iter_loss: 0.4644439220428467
train_iter_loss: 0.42156416177749634
train_iter_loss: 0.28546568751335144
train_iter_loss: 0.3644416630268097
train_iter_loss: 0.3446151316165924
train_iter_loss: 0.33066073060035706
train_iter_loss: 0.3966446816921234
train_iter_loss: 0.24637818336486816
train_iter_loss: 0.23931142687797546
train_iter_loss: 0.2581672966480255
train_iter_loss: 0.16868117451667786
train_iter_loss: 0.21297577023506165
train_iter_loss: 0.4186110198497772
train_iter_loss: 0.2963300347328186
train_iter_loss: 0.24541939795017242
train_iter_loss: 0.08218757808208466
train_iter_loss: 0.26327240467071533
train_iter_loss: 0.2905891239643097
train_iter_loss: 0.32264888286590576
train_iter_loss: 0.30707603693008423
train_iter_loss: 0.39526429772377014
train_iter_loss: 0.2534124553203583
train_iter_loss: 0.2830946743488312
train_iter_loss: 0.31661492586135864
train_iter_loss: 0.27683183550834656
train_iter_loss: 0.27383387088775635
train_iter_loss: 0.19029377400875092
train_iter_loss: 0.2129737138748169
train_iter_loss: 0.22646352648735046
train_iter_loss: 0.35943880677223206
train_iter_loss: 0.32722392678260803
train_iter_loss: 0.3264780640602112
train_iter_loss: 0.3277024030685425
train_iter_loss: 0.2969249188899994
train_iter_loss: 0.38513439893722534
train_iter_loss: 0.2597053647041321
train_iter_loss: 0.23733511567115784
train_iter_loss: 0.3706219792366028
train_iter_loss: 0.26727402210235596
train_iter_loss: 0.24263112246990204
train_iter_loss: 0.48132938146591187
train_iter_loss: 0.5166139602661133
train_iter_loss: 0.20177146792411804
train_iter_loss: 0.3454412519931793
train_iter_loss: 0.22637838125228882
train_iter_loss: 0.22039517760276794
train_iter_loss: 0.465210497379303
train_iter_loss: 0.2734949290752411
train_iter_loss: 0.26442283391952515
train_iter_loss: 0.1954939067363739
train_iter_loss: 0.2600167393684387
train_iter_loss: 0.22670134902000427
train_iter_loss: 0.11308504641056061
train_iter_loss: 0.2668248414993286
train_iter_loss: 0.49083465337753296
train_iter_loss: 0.2821120321750641
train_iter_loss: 0.38349419832229614
train_iter_loss: 0.24261298775672913
train_iter_loss: 0.13343197107315063
train_iter_loss: 0.15268896520137787
train_iter_loss: 0.3536704182624817
train_iter_loss: 0.44058671593666077
train_iter_loss: 0.2612575590610504
train_iter_loss: 0.1756434291601181
train_iter_loss: 0.2311582714319229
train_iter_loss: 0.2545889616012573
train_iter_loss: 0.34023812413215637
train_iter_loss: 0.12123841792345047
train_iter_loss: 0.24503567814826965
train_iter_loss: 0.34327325224876404
train_iter_loss: 0.30320465564727783
train_iter_loss: 0.40215373039245605
train_iter_loss: 0.3545362651348114
train_iter_loss: 0.24843333661556244
train_iter_loss: 0.2494526356458664
train_iter_loss: 0.2443069964647293
train_iter_loss: 0.3010428547859192
train_iter_loss: 0.1727537214756012
train_iter_loss: 0.26043573021888733
train_iter_loss: 0.3794064521789551
train_iter_loss: 0.2715824544429779
train_iter_loss: 0.36455097794532776
train_iter_loss: 0.2718661427497864
train_iter_loss: 0.16794182360172272
train_iter_loss: 0.3370749354362488
train_iter_loss: 0.17634379863739014
train loss :0.2972
---------------------
Validation seg loss: 0.39002567936473015 at epoch 156
epoch =    157/  1000, exp = train
train_iter_loss: 0.25778934359550476
train_iter_loss: 0.2995881140232086
train_iter_loss: 0.3349945843219757
train_iter_loss: 0.33860787749290466
train_iter_loss: 0.4432002604007721
train_iter_loss: 0.195212259888649
train_iter_loss: 0.2909645736217499
train_iter_loss: 0.23121728003025055
train_iter_loss: 0.2646075487136841
train_iter_loss: 0.29651758074760437
train_iter_loss: 0.296152263879776
train_iter_loss: 0.3865153193473816
train_iter_loss: 0.1877458691596985
train_iter_loss: 0.32954320311546326
train_iter_loss: 0.26160916686058044
train_iter_loss: 0.12608052790164948
train_iter_loss: 0.14079901576042175
train_iter_loss: 0.28993135690689087
train_iter_loss: 0.3070235848426819
train_iter_loss: 0.29950299859046936
train_iter_loss: 0.16554658114910126
train_iter_loss: 0.34735044836997986
train_iter_loss: 0.2916512191295624
train_iter_loss: 0.16188549995422363
train_iter_loss: 0.36584845185279846
train_iter_loss: 0.3432358205318451
train_iter_loss: 0.3079302906990051
train_iter_loss: 0.20154687762260437
train_iter_loss: 0.3038538992404938
train_iter_loss: 0.33293768763542175
train_iter_loss: 0.11210490763187408
train_iter_loss: 0.4112480878829956
train_iter_loss: 0.24925096333026886
train_iter_loss: 0.2906317114830017
train_iter_loss: 0.34503796696662903
train_iter_loss: 0.2214454561471939
train_iter_loss: 0.2729942202568054
train_iter_loss: 0.25681236386299133
train_iter_loss: 0.2633515000343323
train_iter_loss: 0.32965660095214844
train_iter_loss: 0.4402618408203125
train_iter_loss: 0.22603023052215576
train_iter_loss: 0.32799625396728516
train_iter_loss: 0.6233978271484375
train_iter_loss: 0.21931838989257812
train_iter_loss: 0.46396487951278687
train_iter_loss: 0.14225943386554718
train_iter_loss: 0.37621617317199707
train_iter_loss: 0.4296988844871521
train_iter_loss: 0.4503527581691742
train_iter_loss: 0.19495323300361633
train_iter_loss: 0.4358198940753937
train_iter_loss: 0.2872469127178192
train_iter_loss: 0.349960058927536
train_iter_loss: 0.5679231286048889
train_iter_loss: 0.3354380130767822
train_iter_loss: 0.27421924471855164
train_iter_loss: 0.34986162185668945
train_iter_loss: 0.2878393232822418
train_iter_loss: 0.08548519015312195
train_iter_loss: 0.2766371965408325
train_iter_loss: 0.37198662757873535
train_iter_loss: 0.23604153096675873
train_iter_loss: 0.3570232391357422
train_iter_loss: 0.2088882476091385
train_iter_loss: 0.13597460091114044
train_iter_loss: 0.2156210094690323
train_iter_loss: 0.28156304359436035
train_iter_loss: 0.2950532138347626
train_iter_loss: 0.33302152156829834
train_iter_loss: 0.21494607627391815
train_iter_loss: 0.41351908445358276
train_iter_loss: 0.40595102310180664
train_iter_loss: 0.3793210983276367
train_iter_loss: 0.11825865507125854
train_iter_loss: 0.24507425725460052
train_iter_loss: 0.21758221089839935
train_iter_loss: 0.15924732387065887
train_iter_loss: 0.49513882398605347
train_iter_loss: 0.19660061597824097
train_iter_loss: 0.30818232893943787
train_iter_loss: 0.27857086062431335
train_iter_loss: 0.4371722638607025
train_iter_loss: 0.2488481104373932
train_iter_loss: 0.2311948835849762
train_iter_loss: 0.22036471962928772
train_iter_loss: 0.09100115299224854
train_iter_loss: 0.15254424512386322
train_iter_loss: 0.16409754753112793
train_iter_loss: 0.2597075402736664
train_iter_loss: 0.5790232419967651
train_iter_loss: 0.20279422402381897
train_iter_loss: 0.2965572476387024
train_iter_loss: 0.33964434266090393
train_iter_loss: 0.2169773131608963
train_iter_loss: 0.28592967987060547
train_iter_loss: 0.3464149534702301
train_iter_loss: 0.19606603682041168
train_iter_loss: 0.36736583709716797
train_iter_loss: 0.27278947830200195
train loss :0.2937
---------------------
Validation seg loss: 0.3989978005828441 at epoch 157
epoch =    158/  1000, exp = train
train_iter_loss: 0.319121778011322
train_iter_loss: 0.30092838406562805
train_iter_loss: 0.29096361994743347
train_iter_loss: 0.06327849626541138
train_iter_loss: 0.32758545875549316
train_iter_loss: 0.3021208643913269
train_iter_loss: 0.2937163710594177
train_iter_loss: 0.37248364090919495
train_iter_loss: 0.3699863851070404
train_iter_loss: 0.4796992540359497
train_iter_loss: 0.42790672183036804
train_iter_loss: 0.29931482672691345
train_iter_loss: 0.2596825063228607
train_iter_loss: 0.28387919068336487
train_iter_loss: 0.2631864547729492
train_iter_loss: 0.2657766342163086
train_iter_loss: 0.16858816146850586
train_iter_loss: 0.32583892345428467
train_iter_loss: 0.4332789182662964
train_iter_loss: 0.3209867477416992
train_iter_loss: 0.2081383466720581
train_iter_loss: 0.26795250177383423
train_iter_loss: 0.3489038348197937
train_iter_loss: 0.406419575214386
train_iter_loss: 0.2964226305484772
train_iter_loss: 0.17199701070785522
train_iter_loss: 0.28346726298332214
train_iter_loss: 0.23876875638961792
train_iter_loss: 0.31011611223220825
train_iter_loss: 0.24893324077129364
train_iter_loss: 0.20062106847763062
train_iter_loss: 0.10444996505975723
train_iter_loss: 0.2990368902683258
train_iter_loss: 0.24970868229866028
train_iter_loss: 0.32524368166923523
train_iter_loss: 0.2118212878704071
train_iter_loss: 0.24932901561260223
train_iter_loss: 0.3240351378917694
train_iter_loss: 0.20550668239593506
train_iter_loss: 0.23320704698562622
train_iter_loss: 0.30018970370292664
train_iter_loss: 0.24819323420524597
train_iter_loss: 0.28255337476730347
train_iter_loss: 0.05052058771252632
train_iter_loss: 0.3189855217933655
train_iter_loss: 0.3506201505661011
train_iter_loss: 0.5060468912124634
train_iter_loss: 0.3102732002735138
train_iter_loss: 0.35511088371276855
train_iter_loss: 0.20083995163440704
train_iter_loss: 0.242105171084404
train_iter_loss: 0.26306718587875366
train_iter_loss: 0.21334558725357056
train_iter_loss: 0.6022434234619141
train_iter_loss: 0.35407543182373047
train_iter_loss: 0.4825117290019989
train_iter_loss: 0.4720061123371124
train_iter_loss: 0.2707337737083435
train_iter_loss: 0.2623923420906067
train_iter_loss: 0.43917784094810486
train_iter_loss: 0.24058419466018677
train_iter_loss: 0.2643466591835022
train_iter_loss: 0.3011627793312073
train_iter_loss: 0.2210618108510971
train_iter_loss: 0.33367919921875
train_iter_loss: 0.269367516040802
train_iter_loss: 0.3631680905818939
train_iter_loss: 0.2205895632505417
train_iter_loss: 0.35723942518234253
train_iter_loss: 0.20315982401371002
train_iter_loss: 0.2468869984149933
train_iter_loss: 0.3973000645637512
train_iter_loss: 0.15500661730766296
train_iter_loss: 0.24280045926570892
train_iter_loss: 0.2888283133506775
train_iter_loss: 0.43898868560791016
train_iter_loss: 0.2709406614303589
train_iter_loss: 0.27364543080329895
train_iter_loss: 0.34723326563835144
train_iter_loss: 0.3028419017791748
train_iter_loss: 0.27740922570228577
train_iter_loss: 0.3482457101345062
train_iter_loss: 0.2590806186199188
train_iter_loss: 0.11431345343589783
train_iter_loss: 0.2763345241546631
train_iter_loss: 0.18342982232570648
train_iter_loss: 0.2991923987865448
train_iter_loss: 0.2820637822151184
train_iter_loss: 0.2060389220714569
train_iter_loss: 0.2690271735191345
train_iter_loss: 0.2770877778530121
train_iter_loss: 0.29118266701698303
train_iter_loss: 0.2415379136800766
train_iter_loss: 0.1909039467573166
train_iter_loss: 0.25262632966041565
train_iter_loss: 0.3310595154762268
train_iter_loss: 0.296543687582016
train_iter_loss: 0.41314059495925903
train_iter_loss: 0.29561761021614075
train_iter_loss: 0.2659474015235901
train loss :0.2928
---------------------
Validation seg loss: 0.38271275275158434 at epoch 158
epoch =    159/  1000, exp = train
train_iter_loss: 0.3026484549045563
train_iter_loss: 0.34041258692741394
train_iter_loss: 0.5101684331893921
train_iter_loss: 0.3244387209415436
train_iter_loss: 0.4406236708164215
train_iter_loss: 0.2103726863861084
train_iter_loss: 0.2762702703475952
train_iter_loss: 0.30407771468162537
train_iter_loss: 0.5194178819656372
train_iter_loss: 0.32218822836875916
train_iter_loss: 0.3870496153831482
train_iter_loss: 0.35747507214546204
train_iter_loss: 0.2994258403778076
train_iter_loss: 0.2649387717247009
train_iter_loss: 0.3771006166934967
train_iter_loss: 0.27222776412963867
train_iter_loss: 0.26489683985710144
train_iter_loss: 0.3684532046318054
train_iter_loss: 0.17265670001506805
train_iter_loss: 0.3689703047275543
train_iter_loss: 0.29422032833099365
train_iter_loss: 0.3270905017852783
train_iter_loss: 0.24070236086845398
train_iter_loss: 0.19417177140712738
train_iter_loss: 0.35268065333366394
train_iter_loss: 0.21422332525253296
train_iter_loss: 0.257406622171402
train_iter_loss: 0.22133591771125793
train_iter_loss: 0.21795499324798584
train_iter_loss: 0.2870558798313141
train_iter_loss: 0.3320183753967285
train_iter_loss: 0.27538907527923584
train_iter_loss: 0.35486167669296265
train_iter_loss: 0.31990867853164673
train_iter_loss: 0.28214630484580994
train_iter_loss: 0.3609229028224945
train_iter_loss: 0.33969399333000183
train_iter_loss: 0.2898091673851013
train_iter_loss: 0.2017400711774826
train_iter_loss: 0.2945527136325836
train_iter_loss: 0.16721519827842712
train_iter_loss: 0.523850679397583
train_iter_loss: 0.26473844051361084
train_iter_loss: 0.3880839943885803
train_iter_loss: 0.36172640323638916
train_iter_loss: 0.20619235932826996
train_iter_loss: 0.19967477023601532
train_iter_loss: 0.3491600751876831
train_iter_loss: 0.32148346304893494
train_iter_loss: 0.29568055272102356
train_iter_loss: 0.37379512190818787
train_iter_loss: 0.5242626667022705
train_iter_loss: 0.4198237657546997
train_iter_loss: 0.3241259455680847
train_iter_loss: 0.24017025530338287
train_iter_loss: 0.10566277801990509
train_iter_loss: 0.45035460591316223
train_iter_loss: 0.2675289511680603
train_iter_loss: 0.23831158876419067
train_iter_loss: 0.3605063855648041
train_iter_loss: 0.22363977134227753
train_iter_loss: 0.34039145708084106
train_iter_loss: 0.14623191952705383
train_iter_loss: 0.1821475774049759
train_iter_loss: 0.42156118154525757
train_iter_loss: 0.28722116351127625
train_iter_loss: 0.12011062353849411
train_iter_loss: 0.2682228088378906
train_iter_loss: 0.3120027184486389
train_iter_loss: 0.3609575629234314
train_iter_loss: 0.22161608934402466
train_iter_loss: 0.262911856174469
train_iter_loss: 0.2954472303390503
train_iter_loss: 0.2619931697845459
train_iter_loss: 0.32759803533554077
train_iter_loss: 0.1261378675699234
train_iter_loss: 0.3668239414691925
train_iter_loss: 0.26261699199676514
train_iter_loss: 0.37645286321640015
train_iter_loss: 0.3764494061470032
train_iter_loss: 0.17340593039989471
train_iter_loss: 0.44630521535873413
train_iter_loss: 0.3320121467113495
train_iter_loss: 0.1635553240776062
train_iter_loss: 0.3171832263469696
train_iter_loss: 0.2792949378490448
train_iter_loss: 0.3370082378387451
train_iter_loss: 0.3286849558353424
train_iter_loss: 0.19518618285655975
train_iter_loss: 0.15227310359477997
train_iter_loss: 0.16706642508506775
train_iter_loss: 0.15392963588237762
train_iter_loss: 0.6255908012390137
train_iter_loss: 0.2553524076938629
train_iter_loss: 0.1840733140707016
train_iter_loss: 0.2585478723049164
train_iter_loss: 0.16815300285816193
train_iter_loss: 0.3959546685218811
train_iter_loss: 0.24887396395206451
train_iter_loss: 0.40453997254371643
train loss :0.3014
---------------------
Validation seg loss: 0.3866978608915266 at epoch 159
epoch =    160/  1000, exp = train
train_iter_loss: 0.4680817723274231
train_iter_loss: 0.3655532896518707
train_iter_loss: 0.21718855202198029
train_iter_loss: 0.48137789964675903
train_iter_loss: 0.42035502195358276
train_iter_loss: 0.24445077776908875
train_iter_loss: 0.295709490776062
train_iter_loss: 0.27900978922843933
train_iter_loss: 0.26213595271110535
train_iter_loss: 0.3003695607185364
train_iter_loss: 0.4549719989299774
train_iter_loss: 0.2884441018104553
train_iter_loss: 0.2370847761631012
train_iter_loss: 0.3138893246650696
train_iter_loss: 0.31464341282844543
train_iter_loss: 0.2835644781589508
train_iter_loss: 0.23633438348770142
train_iter_loss: 0.3368263244628906
train_iter_loss: 0.24345876276493073
train_iter_loss: 0.27341538667678833
train_iter_loss: 0.282327264547348
train_iter_loss: 0.2840941250324249
train_iter_loss: 0.15722057223320007
train_iter_loss: 0.3724361062049866
train_iter_loss: 0.3398681581020355
train_iter_loss: 0.2453194260597229
train_iter_loss: 0.11924440413713455
train_iter_loss: 0.27467793226242065
train_iter_loss: 0.3322449326515198
train_iter_loss: 0.420505553483963
train_iter_loss: 0.3354896008968353
train_iter_loss: 0.30591699481010437
train_iter_loss: 0.44424521923065186
train_iter_loss: 0.24745479226112366
train_iter_loss: 0.31375977396965027
train_iter_loss: 0.41179028153419495
train_iter_loss: 0.2953691780567169
train_iter_loss: 0.3016055226325989
train_iter_loss: 0.30026185512542725
train_iter_loss: 0.24698811769485474
train_iter_loss: 0.2765721380710602
train_iter_loss: 0.21728022396564484
train_iter_loss: 0.28280341625213623
train_iter_loss: 0.27632850408554077
train_iter_loss: 0.1879560649394989
train_iter_loss: 0.20011872053146362
train_iter_loss: 0.2531730830669403
train_iter_loss: 0.39336636662483215
train_iter_loss: 0.24826200306415558
train_iter_loss: 0.2993437945842743
train_iter_loss: 0.28670015931129456
train_iter_loss: 0.30270975828170776
train_iter_loss: 0.1493493914604187
train_iter_loss: 0.3696269690990448
train_iter_loss: 0.16242532432079315
train_iter_loss: 0.2771160304546356
train_iter_loss: 0.23938605189323425
train_iter_loss: 0.2662394642829895
train_iter_loss: 0.24528586864471436
train_iter_loss: 0.30142948031425476
train_iter_loss: 0.404011607170105
train_iter_loss: 0.24678415060043335
train_iter_loss: 0.17538461089134216
train_iter_loss: 0.2854461967945099
train_iter_loss: 0.3044789731502533
train_iter_loss: 0.2440151870250702
train_iter_loss: 0.26113274693489075
train_iter_loss: 0.3219496011734009
train_iter_loss: 0.24544544517993927
train_iter_loss: 0.1782180368900299
train_iter_loss: 0.38636693358421326
train_iter_loss: 0.2776859998703003
train_iter_loss: 0.38502663373947144
train_iter_loss: 0.22397300601005554
train_iter_loss: 0.4805615246295929
train_iter_loss: 0.3508177399635315
train_iter_loss: 0.24478352069854736
train_iter_loss: 0.29826101660728455
train_iter_loss: 0.31974515318870544
train_iter_loss: 0.46909835934638977
train_iter_loss: 0.28100988268852234
train_iter_loss: 0.2848992943763733
train_iter_loss: 0.24179594218730927
train_iter_loss: 0.12421455979347229
train_iter_loss: 0.15468630194664001
train_iter_loss: 0.3638724684715271
train_iter_loss: 0.1409977376461029
train_iter_loss: 0.29612764716148376
train_iter_loss: 0.3325585126876831
train_iter_loss: 0.22640551626682281
train_iter_loss: 0.2657919228076935
train_iter_loss: 0.3604116439819336
train_iter_loss: 0.33347564935684204
train_iter_loss: 0.2848678529262543
train_iter_loss: 0.34509530663490295
train_iter_loss: 0.11814823746681213
train_iter_loss: 0.28582948446273804
train_iter_loss: 0.33246591687202454
train_iter_loss: 0.2662453353404999
train_iter_loss: 0.29989415407180786
train loss :0.2933
---------------------
Validation seg loss: 0.3678189137915395 at epoch 160
epoch =    161/  1000, exp = train
train_iter_loss: 0.18269413709640503
train_iter_loss: 0.32043859362602234
train_iter_loss: 0.2598361372947693
train_iter_loss: 0.17504312098026276
train_iter_loss: 0.21604660153388977
train_iter_loss: 0.3536036014556885
train_iter_loss: 0.2070770263671875
train_iter_loss: 0.25611332058906555
train_iter_loss: 0.38260629773139954
train_iter_loss: 0.20798394083976746
train_iter_loss: 0.18643280863761902
train_iter_loss: 0.4958578944206238
train_iter_loss: 0.23864176869392395
train_iter_loss: 0.30549880862236023
train_iter_loss: 0.22150634229183197
train_iter_loss: 0.2295634001493454
train_iter_loss: 0.1958388090133667
train_iter_loss: 0.1726827770471573
train_iter_loss: 0.3338482677936554
train_iter_loss: 0.3221901059150696
train_iter_loss: 0.2218310832977295
train_iter_loss: 0.2651853561401367
train_iter_loss: 0.3920667767524719
train_iter_loss: 0.30900856852531433
train_iter_loss: 0.3160719573497772
train_iter_loss: 0.2683633267879486
train_iter_loss: 0.3523533046245575
train_iter_loss: 0.21748022735118866
train_iter_loss: 0.29020315408706665
train_iter_loss: 0.2578894793987274
train_iter_loss: 0.3338870704174042
train_iter_loss: 0.4428130090236664
train_iter_loss: 0.22401688992977142
train_iter_loss: 0.29681095480918884
train_iter_loss: 0.2578507363796234
train_iter_loss: 0.43670427799224854
train_iter_loss: 0.3207247257232666
train_iter_loss: 0.2676713764667511
train_iter_loss: 0.33861809968948364
train_iter_loss: 0.37055522203445435
train_iter_loss: 0.266328901052475
train_iter_loss: 0.3456578850746155
train_iter_loss: 0.30057957768440247
train_iter_loss: 0.3031015992164612
train_iter_loss: 0.2569097876548767
train_iter_loss: 0.28461289405822754
train_iter_loss: 0.16615764796733856
train_iter_loss: 0.5866783857345581
train_iter_loss: 0.3031359612941742
train_iter_loss: 0.26633378863334656
train_iter_loss: 0.36576464772224426
train_iter_loss: 0.23742234706878662
train_iter_loss: 0.30225229263305664
train_iter_loss: 0.31487879157066345
train_iter_loss: 0.40495333075523376
train_iter_loss: 0.25684183835983276
train_iter_loss: 0.4078461229801178
train_iter_loss: 0.1884973794221878
train_iter_loss: 0.2811858355998993
train_iter_loss: 0.39384913444519043
train_iter_loss: 0.327810138463974
train_iter_loss: 0.19382552802562714
train_iter_loss: 0.379867821931839
train_iter_loss: 0.31089508533477783
train_iter_loss: 0.1879037767648697
train_iter_loss: 0.282448410987854
train_iter_loss: 0.2777981162071228
train_iter_loss: 0.3579687774181366
train_iter_loss: 0.20810838043689728
train_iter_loss: 0.296721488237381
train_iter_loss: 0.22289741039276123
train_iter_loss: 0.31018251180648804
train_iter_loss: 0.34653741121292114
train_iter_loss: 0.2889134883880615
train_iter_loss: 0.26953375339508057
train_iter_loss: 0.3852987289428711
train_iter_loss: 0.17304669320583344
train_iter_loss: 0.3595573604106903
train_iter_loss: 0.2648211419582367
train_iter_loss: 0.07848674803972244
train_iter_loss: 0.40271371603012085
train_iter_loss: 0.18759118020534515
train_iter_loss: 0.3293820917606354
train_iter_loss: 0.3443925678730011
train_iter_loss: 0.16867661476135254
train_iter_loss: 0.18494927883148193
train_iter_loss: 0.15999753773212433
train_iter_loss: 0.28061991930007935
train_iter_loss: 0.3386344313621521
train_iter_loss: 0.31595346331596375
train_iter_loss: 0.3044418394565582
train_iter_loss: 0.5250892639160156
train_iter_loss: 0.3085189759731293
train_iter_loss: 0.14296604692935944
train_iter_loss: 0.2637093663215637
train_iter_loss: 0.3545380234718323
train_iter_loss: 0.31367066502571106
train_iter_loss: 0.31882041692733765
train_iter_loss: 0.2720361351966858
train_iter_loss: 0.11737970262765884
train loss :0.2913
---------------------
Validation seg loss: 0.3621887736338771 at epoch 161
********************
best_val_epoch_loss:  0.3621887736338771
MODEL UPDATED
epoch =    162/  1000, exp = train
train_iter_loss: 0.20155219733715057
train_iter_loss: 0.18976660072803497
train_iter_loss: 0.21516923606395721
train_iter_loss: 0.2572023868560791
train_iter_loss: 0.19632042944431305
train_iter_loss: 0.2814810276031494
train_iter_loss: 0.40972137451171875
train_iter_loss: 0.2653402090072632
train_iter_loss: 0.10684888809919357
train_iter_loss: 0.31577813625335693
train_iter_loss: 0.24146926403045654
train_iter_loss: 0.2156468629837036
train_iter_loss: 0.1563522070646286
train_iter_loss: 0.2814696431159973
train_iter_loss: 0.321007639169693
train_iter_loss: 0.323040634393692
train_iter_loss: 0.28438833355903625
train_iter_loss: 0.25170427560806274
train_iter_loss: 0.1569427251815796
train_iter_loss: 0.3381165564060211
train_iter_loss: 0.37310996651649475
train_iter_loss: 0.2980090379714966
train_iter_loss: 0.3330412805080414
train_iter_loss: 0.4591808617115021
train_iter_loss: 0.20306658744812012
train_iter_loss: 0.2701757848262787
train_iter_loss: 0.26215097308158875
train_iter_loss: 0.2279457300901413
train_iter_loss: 0.47428077459335327
train_iter_loss: 0.18101294338703156
train_iter_loss: 0.41956228017807007
train_iter_loss: 0.48832401633262634
train_iter_loss: 0.22294031083583832
train_iter_loss: 0.21915386617183685
train_iter_loss: 0.2308627963066101
train_iter_loss: 0.4062594771385193
train_iter_loss: 0.38891205191612244
train_iter_loss: 0.25639575719833374
train_iter_loss: 0.3865180015563965
train_iter_loss: 0.24718959629535675
train_iter_loss: 0.2829822897911072
train_iter_loss: 0.2548854351043701
train_iter_loss: 0.1466919481754303
train_iter_loss: 0.4343312382698059
train_iter_loss: 0.2261204570531845
train_iter_loss: 0.24647101759910583
train_iter_loss: 0.17998269200325012
train_iter_loss: 0.4333046078681946
train_iter_loss: 0.3642922341823578
train_iter_loss: 0.15845002233982086
train_iter_loss: 0.2652069628238678
train_iter_loss: 0.26899296045303345
train_iter_loss: 0.3283815085887909
train_iter_loss: 0.31591519713401794
train_iter_loss: 0.3116942048072815
train_iter_loss: 0.41432279348373413
train_iter_loss: 0.32134029269218445
train_iter_loss: 0.36665016412734985
train_iter_loss: 0.5465470552444458
train_iter_loss: 0.17206338047981262
train_iter_loss: 0.28449270129203796
train_iter_loss: 0.19869808852672577
train_iter_loss: 0.2102305293083191
train_iter_loss: 0.19717617332935333
train_iter_loss: 0.24902090430259705
train_iter_loss: 0.3296937644481659
train_iter_loss: 0.34802675247192383
train_iter_loss: 0.3708125054836273
train_iter_loss: 0.13337573409080505
train_iter_loss: 0.3064684271812439
train_iter_loss: 0.17320293188095093
train_iter_loss: 0.19329427182674408
train_iter_loss: 0.3360215425491333
train_iter_loss: 0.5440382361412048
train_iter_loss: 0.24830886721611023
train_iter_loss: 0.30762559175491333
train_iter_loss: 0.33286651968955994
train_iter_loss: 0.24407656490802765
train_iter_loss: 0.6464560031890869
train_iter_loss: 0.203433096408844
train_iter_loss: 0.5079052448272705
train_iter_loss: 0.250779926776886
train_iter_loss: 0.24148766696453094
train_iter_loss: 0.1581445038318634
train_iter_loss: 0.26210060715675354
train_iter_loss: 0.2687235176563263
train_iter_loss: 0.2888525724411011
train_iter_loss: 0.2888433039188385
train_iter_loss: 0.27625319361686707
train_iter_loss: 0.3935585916042328
train_iter_loss: 0.46337905526161194
train_iter_loss: 0.301910400390625
train_iter_loss: 0.2758597433567047
train_iter_loss: 0.3707994222640991
train_iter_loss: 0.27402356266975403
train_iter_loss: 0.16478797793388367
train_iter_loss: 0.2664923369884491
train_iter_loss: 0.20736244320869446
train_iter_loss: 0.407958060503006
train_iter_loss: 0.3744879961013794
train loss :0.2959
---------------------
Validation seg loss: 0.3981270155394977 at epoch 162
epoch =    163/  1000, exp = train
train_iter_loss: 0.25019270181655884
train_iter_loss: 0.30514922738075256
train_iter_loss: 0.23802457749843597
train_iter_loss: 0.15399935841560364
train_iter_loss: 0.45425650477409363
train_iter_loss: 0.20064282417297363
train_iter_loss: 0.2093871831893921
train_iter_loss: 0.33949917554855347
train_iter_loss: 0.35032230615615845
train_iter_loss: 0.2869196832180023
train_iter_loss: 0.11096347123384476
train_iter_loss: 0.2964816987514496
train_iter_loss: 0.36354267597198486
train_iter_loss: 0.2256532609462738
train_iter_loss: 0.4045005738735199
train_iter_loss: 0.21238671243190765
train_iter_loss: 0.1545332670211792
train_iter_loss: 0.2705199122428894
train_iter_loss: 0.3178941607475281
train_iter_loss: 0.4234015941619873
train_iter_loss: 0.3591395318508148
train_iter_loss: 0.25023332238197327
train_iter_loss: 0.4339196979999542
train_iter_loss: 0.38562726974487305
train_iter_loss: 0.1646665334701538
train_iter_loss: 0.23345477879047394
train_iter_loss: 0.21723003685474396
train_iter_loss: 0.287450909614563
train_iter_loss: 0.1428147405385971
train_iter_loss: 0.29930374026298523
train_iter_loss: 0.360899418592453
train_iter_loss: 0.3917398452758789
train_iter_loss: 0.26279401779174805
train_iter_loss: 0.2690615653991699
train_iter_loss: 0.25654011964797974
train_iter_loss: 0.43046990036964417
train_iter_loss: 0.19641141593456268
train_iter_loss: 0.4419252276420593
train_iter_loss: 0.364717036485672
train_iter_loss: 0.360249787569046
train_iter_loss: 0.23967275023460388
train_iter_loss: 0.24307145178318024
train_iter_loss: 0.32303348183631897
train_iter_loss: 0.21436388790607452
train_iter_loss: 0.4311034381389618
train_iter_loss: 0.2496107667684555
train_iter_loss: 0.14752094447612762
train_iter_loss: 0.33772262930870056
train_iter_loss: 0.36279991269111633
train_iter_loss: 0.2982548475265503
train_iter_loss: 0.07999148219823837
train_iter_loss: 0.2142617404460907
train_iter_loss: 0.25346919894218445
train_iter_loss: 0.29560762643814087
train_iter_loss: 0.28213217854499817
train_iter_loss: 0.1503484547138214
train_iter_loss: 0.24366076290607452
train_iter_loss: 0.2011243999004364
train_iter_loss: 0.3718664348125458
train_iter_loss: 0.346919983625412
train_iter_loss: 0.2891683280467987
train_iter_loss: 0.2421712875366211
train_iter_loss: 0.5078309774398804
train_iter_loss: 0.14851737022399902
train_iter_loss: 0.23892943561077118
train_iter_loss: 0.20182031393051147
train_iter_loss: 0.38619086146354675
train_iter_loss: 0.3024080693721771
train_iter_loss: 0.2482125163078308
train_iter_loss: 0.37105175852775574
train_iter_loss: 0.3235081136226654
train_iter_loss: 0.2638401687145233
train_iter_loss: 0.2541748881340027
train_iter_loss: 0.34976476430892944
train_iter_loss: 0.43934503197669983
train_iter_loss: 0.31451719999313354
train_iter_loss: 0.2678747773170471
train_iter_loss: 0.2890600860118866
train_iter_loss: 0.40119779109954834
train_iter_loss: 0.23533101379871368
train_iter_loss: 0.18991903960704803
train_iter_loss: 0.33243149518966675
train_iter_loss: 0.34382757544517517
train_iter_loss: 0.345872163772583
train_iter_loss: 0.3363396227359772
train_iter_loss: 0.19331663846969604
train_iter_loss: 0.2332712560892105
train_iter_loss: 0.3175385594367981
train_iter_loss: 0.24366767704486847
train_iter_loss: 0.3414798974990845
train_iter_loss: 0.1745426207780838
train_iter_loss: 0.37198135256767273
train_iter_loss: 0.24210096895694733
train_iter_loss: 0.4371281862258911
train_iter_loss: 0.18520322442054749
train_iter_loss: 0.2711702883243561
train_iter_loss: 0.31109684705734253
train_iter_loss: 0.3048308789730072
train_iter_loss: 0.31778523325920105
train_iter_loss: 0.37455815076828003
train loss :0.2920
---------------------
Validation seg loss: 0.40398558902220344 at epoch 163
epoch =    164/  1000, exp = train
train_iter_loss: 0.32840049266815186
train_iter_loss: 0.24301594495773315
train_iter_loss: 0.250551700592041
train_iter_loss: 0.43473681807518005
train_iter_loss: 0.3087635040283203
train_iter_loss: 0.3976426124572754
train_iter_loss: 0.25365984439849854
train_iter_loss: 0.297029972076416
train_iter_loss: 0.20893442630767822
train_iter_loss: 0.15617766976356506
train_iter_loss: 0.28091153502464294
train_iter_loss: 0.2781901955604553
train_iter_loss: 0.16167327761650085
train_iter_loss: 0.3629820942878723
train_iter_loss: 0.24630166590213776
train_iter_loss: 0.42229411005973816
train_iter_loss: 0.22865521907806396
train_iter_loss: 0.5092211365699768
train_iter_loss: 0.1866237074136734
train_iter_loss: 0.35174986720085144
train_iter_loss: 0.26855915784835815
train_iter_loss: 0.26236492395401
train_iter_loss: 0.28655704855918884
train_iter_loss: 0.30259644985198975
train_iter_loss: 0.15473875403404236
train_iter_loss: 0.5418587327003479
train_iter_loss: 0.353128045797348
train_iter_loss: 0.4736701548099518
train_iter_loss: 0.3708149790763855
train_iter_loss: 0.31872233748435974
train_iter_loss: 0.2585378885269165
train_iter_loss: 0.2787918746471405
train_iter_loss: 0.2830181419849396
train_iter_loss: 0.3031403720378876
train_iter_loss: 0.27723079919815063
train_iter_loss: 0.19907543063163757
train_iter_loss: 0.24630513787269592
train_iter_loss: 0.288587749004364
train_iter_loss: 0.26403799653053284
train_iter_loss: 0.4176616072654724
train_iter_loss: 0.28013303875923157
train_iter_loss: 0.28711774945259094
train_iter_loss: 0.15137642621994019
train_iter_loss: 0.2236449271440506
train_iter_loss: 0.23290184140205383
train_iter_loss: 0.3785545825958252
train_iter_loss: 0.407096266746521
train_iter_loss: 0.229238361120224
train_iter_loss: 0.22328881919384003
train_iter_loss: 0.3121439814567566
train_iter_loss: 0.3520936071872711
train_iter_loss: 0.19874738156795502
train_iter_loss: 0.3413347601890564
train_iter_loss: 0.21393470466136932
train_iter_loss: 0.28836265206336975
train_iter_loss: 0.3444857895374298
train_iter_loss: 0.4540405571460724
train_iter_loss: 0.21264801919460297
train_iter_loss: 0.23410719633102417
train_iter_loss: 0.28642380237579346
train_iter_loss: 0.3846849799156189
train_iter_loss: 0.32703813910484314
train_iter_loss: 0.37155070900917053
train_iter_loss: 0.38163208961486816
train_iter_loss: 0.1919315904378891
train_iter_loss: 0.2176455706357956
train_iter_loss: 0.2735436260700226
train_iter_loss: 0.5078408122062683
train_iter_loss: 0.3133965730667114
train_iter_loss: 0.20409372448921204
train_iter_loss: 0.27016526460647583
train_iter_loss: 0.30055782198905945
train_iter_loss: 0.25934886932373047
train_iter_loss: 0.22415894269943237
train_iter_loss: 0.24019460380077362
train_iter_loss: 0.29252272844314575
train_iter_loss: 0.22387370467185974
train_iter_loss: 0.2213238626718521
train_iter_loss: 0.4106977581977844
train_iter_loss: 0.3550938069820404
train_iter_loss: 0.31784579157829285
train_iter_loss: 0.3228367269039154
train_iter_loss: 0.22351732850074768
train_iter_loss: 0.19262830913066864
train_iter_loss: 0.2562271058559418
train_iter_loss: 0.37788835167884827
train_iter_loss: 0.19634246826171875
train_iter_loss: 0.1703363060951233
train_iter_loss: 0.22172632813453674
train_iter_loss: 0.5056443214416504
train_iter_loss: 0.24646565318107605
train_iter_loss: 0.2249077409505844
train_iter_loss: 0.3299226462841034
train_iter_loss: 0.2541097402572632
train_iter_loss: 0.3747827708721161
train_iter_loss: 0.20828154683113098
train_iter_loss: 0.2730662226676941
train_iter_loss: 0.43556392192840576
train_iter_loss: 0.323265939950943
train_iter_loss: 0.19599489867687225
train loss :0.2962
---------------------
Validation seg loss: 0.40398526602899126 at epoch 164
epoch =    165/  1000, exp = train
train_iter_loss: 0.3529687821865082
train_iter_loss: 0.29240480065345764
train_iter_loss: 0.23885096609592438
train_iter_loss: 0.26651430130004883
train_iter_loss: 0.30045807361602783
train_iter_loss: 0.34801235795021057
train_iter_loss: 0.3222353458404541
train_iter_loss: 0.22053635120391846
train_iter_loss: 0.3594346046447754
train_iter_loss: 0.22369185090065002
train_iter_loss: 0.2948009669780731
train_iter_loss: 0.2639813721179962
train_iter_loss: 0.3355353772640228
train_iter_loss: 0.4388965368270874
train_iter_loss: 0.30365869402885437
train_iter_loss: 0.24484358727931976
train_iter_loss: 0.2007463574409485
train_iter_loss: 0.42106151580810547
train_iter_loss: 0.28085237741470337
train_iter_loss: 0.2521119713783264
train_iter_loss: 0.3064413368701935
train_iter_loss: 0.21151824295520782
train_iter_loss: 0.27553343772888184
train_iter_loss: 0.28943732380867004
train_iter_loss: 0.1747729778289795
train_iter_loss: 0.5237423181533813
train_iter_loss: 0.3059473931789398
train_iter_loss: 0.34818190336227417
train_iter_loss: 0.24359232187271118
train_iter_loss: 0.2810981571674347
train_iter_loss: 0.3509909510612488
train_iter_loss: 0.21856847405433655
train_iter_loss: 0.27195465564727783
train_iter_loss: 0.274898886680603
train_iter_loss: 0.09495396167039871
train_iter_loss: 0.2525153160095215
train_iter_loss: 0.2436629682779312
train_iter_loss: 0.43636494874954224
train_iter_loss: 0.2742181718349457
train_iter_loss: 0.2887517213821411
train_iter_loss: 0.37524810433387756
train_iter_loss: 0.2653820216655731
train_iter_loss: 0.29608267545700073
train_iter_loss: 0.32179099321365356
train_iter_loss: 0.25575417280197144
train_iter_loss: 0.2036396861076355
train_iter_loss: 0.15350103378295898
train_iter_loss: 0.20742839574813843
train_iter_loss: 0.32440635561943054
train_iter_loss: 0.29085758328437805
train_iter_loss: 0.18563789129257202
train_iter_loss: 0.19626207649707794
train_iter_loss: 0.17315487563610077
train_iter_loss: 0.31653091311454773
train_iter_loss: 0.20194275677204132
train_iter_loss: 0.2949828803539276
train_iter_loss: 0.2277858853340149
train_iter_loss: 0.1569610983133316
train_iter_loss: 0.29471251368522644
train_iter_loss: 0.4437844455242157
train_iter_loss: 0.39643073081970215
train_iter_loss: 0.19963763654232025
train_iter_loss: 0.4741722047328949
train_iter_loss: 0.3492415249347687
train_iter_loss: 0.210592582821846
train_iter_loss: 0.5231114029884338
train_iter_loss: 0.25755637884140015
train_iter_loss: 0.19545403122901917
train_iter_loss: 0.42410314083099365
train_iter_loss: 0.2959187924861908
train_iter_loss: 0.25219476222991943
train_iter_loss: 0.252688467502594
train_iter_loss: 0.26392602920532227
train_iter_loss: 0.29240304231643677
train_iter_loss: 0.21505510807037354
train_iter_loss: 0.31699398159980774
train_iter_loss: 0.2779734134674072
train_iter_loss: 0.27276867628097534
train_iter_loss: 0.2428230196237564
train_iter_loss: 0.49991515278816223
train_iter_loss: 0.32556161284446716
train_iter_loss: 0.21845169365406036
train_iter_loss: 0.2251156121492386
train_iter_loss: 0.4457384943962097
train_iter_loss: 0.41822052001953125
train_iter_loss: 0.3149770498275757
train_iter_loss: 0.20290623605251312
train_iter_loss: 0.33802783489227295
train_iter_loss: 0.2696206271648407
train_iter_loss: 0.3634285628795624
train_iter_loss: 0.37715384364128113
train_iter_loss: 0.14232847094535828
train_iter_loss: 0.19930899143218994
train_iter_loss: 0.137168288230896
train_iter_loss: 0.41007569432258606
train_iter_loss: 0.36640554666519165
train_iter_loss: 0.41315218806266785
train_iter_loss: 0.29430872201919556
train_iter_loss: 0.23149904608726501
train_iter_loss: 0.3400024473667145
train loss :0.2936
---------------------
Validation seg loss: 0.4142444314927144 at epoch 165
epoch =    166/  1000, exp = train
train_iter_loss: 0.27244657278060913
train_iter_loss: 0.22205212712287903
train_iter_loss: 0.18193091452121735
train_iter_loss: 0.615277886390686
train_iter_loss: 0.32572588324546814
train_iter_loss: 0.21865324676036835
train_iter_loss: 0.2017875760793686
train_iter_loss: 0.5070998668670654
train_iter_loss: 0.29977381229400635
train_iter_loss: 0.2175341248512268
train_iter_loss: 0.3312602639198303
train_iter_loss: 0.2009757161140442
train_iter_loss: 0.2767252027988434
train_iter_loss: 0.2596823275089264
train_iter_loss: 0.36909452080726624
train_iter_loss: 0.34836602210998535
train_iter_loss: 0.26718178391456604
train_iter_loss: 0.26332926750183105
train_iter_loss: 0.18809814751148224
train_iter_loss: 0.4064357280731201
train_iter_loss: 0.24499474465847015
train_iter_loss: 0.22454850375652313
train_iter_loss: 0.2489129900932312
train_iter_loss: 0.21953709423542023
train_iter_loss: 0.31066542863845825
train_iter_loss: 0.3608928322792053
train_iter_loss: 0.20566733181476593
train_iter_loss: 0.29291677474975586
train_iter_loss: 0.32750365138053894
train_iter_loss: 0.22356992959976196
train_iter_loss: 0.4040464758872986
train_iter_loss: 0.25735315680503845
train_iter_loss: 0.6366333365440369
train_iter_loss: 0.31197598576545715
train_iter_loss: 0.2988484799861908
train_iter_loss: 0.3140661418437958
train_iter_loss: 0.336182177066803
train_iter_loss: 0.2815495431423187
train_iter_loss: 0.19777941703796387
train_iter_loss: 0.2617131173610687
train_iter_loss: 0.2285524159669876
train_iter_loss: 0.3495497405529022
train_iter_loss: 0.3763599693775177
train_iter_loss: 0.31275689601898193
train_iter_loss: 0.13797281682491302
train_iter_loss: 0.1961388885974884
train_iter_loss: 0.17005500197410583
train_iter_loss: 0.3488031327724457
train_iter_loss: 0.2739204466342926
train_iter_loss: 0.2852359414100647
train_iter_loss: 0.18357552587985992
train_iter_loss: 0.2911074459552765
train_iter_loss: 0.4359332025051117
train_iter_loss: 0.29770419001579285
train_iter_loss: 0.5262187123298645
train_iter_loss: 0.2941341698169708
train_iter_loss: 0.36568352580070496
train_iter_loss: 0.2261691838502884
train_iter_loss: 0.15783414244651794
train_iter_loss: 0.313422828912735
train_iter_loss: 0.25502556562423706
train_iter_loss: 0.22798825800418854
train_iter_loss: 0.18858179450035095
train_iter_loss: 0.18839454650878906
train_iter_loss: 0.2522600591182709
train_iter_loss: 0.3646635413169861
train_iter_loss: 0.5672826766967773
train_iter_loss: 0.349761962890625
train_iter_loss: 0.24548302590847015
train_iter_loss: 0.2942422330379486
train_iter_loss: 0.215504452586174
train_iter_loss: 0.1671798974275589
train_iter_loss: 0.3590184152126312
train_iter_loss: 0.3067520260810852
train_iter_loss: 0.5318894386291504
train_iter_loss: 0.44061192870140076
train_iter_loss: 0.29619139432907104
train_iter_loss: 0.3439367115497589
train_iter_loss: 0.2951700687408447
train_iter_loss: 0.19311058521270752
train_iter_loss: 0.244384303689003
train_iter_loss: 0.44143736362457275
train_iter_loss: 0.2490854114294052
train_iter_loss: 0.2522282898426056
train_iter_loss: 0.408488005399704
train_iter_loss: 0.29480308294296265
train_iter_loss: 0.21031630039215088
train_iter_loss: 0.2492327094078064
train_iter_loss: 0.2539283037185669
train_iter_loss: 0.31707054376602173
train_iter_loss: 0.3359631299972534
train_iter_loss: 0.18167367577552795
train_iter_loss: 0.1670159101486206
train_iter_loss: 0.3374793529510498
train_iter_loss: 0.2484583854675293
train_iter_loss: 0.3173806369304657
train_iter_loss: 0.4225083291530609
train_iter_loss: 0.23100988566875458
train_iter_loss: 0.2668885886669159
train_iter_loss: 0.3583986461162567
train loss :0.2987
---------------------
Validation seg loss: 0.3758999889470496 at epoch 166
epoch =    167/  1000, exp = train
train_iter_loss: 0.21995127201080322
train_iter_loss: 0.15917031466960907
train_iter_loss: 0.30726513266563416
train_iter_loss: 0.3077976107597351
train_iter_loss: 0.32476603984832764
train_iter_loss: 0.20220480859279633
train_iter_loss: 0.3168070912361145
train_iter_loss: 0.2996976971626282
train_iter_loss: 0.27726292610168457
train_iter_loss: 0.28388941287994385
train_iter_loss: 0.3111320436000824
train_iter_loss: 0.41301655769348145
train_iter_loss: 0.2633707523345947
train_iter_loss: 0.2657918334007263
train_iter_loss: 0.2813936769962311
train_iter_loss: 0.3104569911956787
train_iter_loss: 0.25706571340560913
train_iter_loss: 0.09427637606859207
train_iter_loss: 0.2553200423717499
train_iter_loss: 0.29645848274230957
train_iter_loss: 0.4134662449359894
train_iter_loss: 0.18473580479621887
train_iter_loss: 0.25358280539512634
train_iter_loss: 0.2326769381761551
train_iter_loss: 0.3351454734802246
train_iter_loss: 0.3751800060272217
train_iter_loss: 0.43978726863861084
train_iter_loss: 0.4161445200443268
train_iter_loss: 0.11199881881475449
train_iter_loss: 0.13778890669345856
train_iter_loss: 0.32423558831214905
train_iter_loss: 0.4558050036430359
train_iter_loss: 0.27337366342544556
train_iter_loss: 0.37875086069107056
train_iter_loss: 0.22852754592895508
train_iter_loss: 0.3135209381580353
train_iter_loss: 0.0968698039650917
train_iter_loss: 0.3486940860748291
train_iter_loss: 0.36044013500213623
train_iter_loss: 0.21179834008216858
train_iter_loss: 0.2625874876976013
train_iter_loss: 0.16772237420082092
train_iter_loss: 0.269912987947464
train_iter_loss: 0.1963130682706833
train_iter_loss: 0.3071690499782562
train_iter_loss: 0.1155768632888794
train_iter_loss: 0.26782459020614624
train_iter_loss: 0.27040836215019226
train_iter_loss: 0.21101492643356323
train_iter_loss: 0.3467652201652527
train_iter_loss: 0.200067400932312
train_iter_loss: 0.34104225039482117
train_iter_loss: 0.3298738896846771
train_iter_loss: 0.45350104570388794
train_iter_loss: 0.28765901923179626
train_iter_loss: 0.4215579926967621
train_iter_loss: 0.38655340671539307
train_iter_loss: 0.3556387424468994
train_iter_loss: 0.3692147135734558
train_iter_loss: 0.3439885079860687
train_iter_loss: 0.4336572587490082
train_iter_loss: 0.23395715653896332
train_iter_loss: 0.31090110540390015
train_iter_loss: 0.3741215467453003
train_iter_loss: 0.35730209946632385
train_iter_loss: 0.3105088770389557
train_iter_loss: 0.26344412565231323
train_iter_loss: 0.30637460947036743
train_iter_loss: 0.2688256502151489
train_iter_loss: 0.34140002727508545
train_iter_loss: 0.1944865584373474
train_iter_loss: 0.22382088005542755
train_iter_loss: 0.376249223947525
train_iter_loss: 0.29392075538635254
train_iter_loss: 0.25630876421928406
train_iter_loss: 0.23923076689243317
train_iter_loss: 0.3066484332084656
train_iter_loss: 0.2842779755592346
train_iter_loss: 0.18570387363433838
train_iter_loss: 0.17149120569229126
train_iter_loss: 0.26881152391433716
train_iter_loss: 0.19073724746704102
train_iter_loss: 0.4352255165576935
train_iter_loss: 0.4485492408275604
train_iter_loss: 0.31400036811828613
train_iter_loss: 0.1787485033273697
train_iter_loss: 0.1259082704782486
train_iter_loss: 0.3657878041267395
train_iter_loss: 0.3333563804626465
train_iter_loss: 0.23543532192707062
train_iter_loss: 0.35174626111984253
train_iter_loss: 0.31225332617759705
train_iter_loss: 0.22876395285129547
train_iter_loss: 0.4282202422618866
train_iter_loss: 0.264138400554657
train_iter_loss: 0.4731294512748718
train_iter_loss: 0.21819652616977692
train_iter_loss: 0.3515152335166931
train_iter_loss: 0.228620707988739
train_iter_loss: 0.26617586612701416
train loss :0.2929
---------------------
Validation seg loss: 0.4127232995376272 at epoch 167
epoch =    168/  1000, exp = train
train_iter_loss: 0.35761725902557373
train_iter_loss: 0.3195728063583374
train_iter_loss: 0.32426387071609497
train_iter_loss: 0.31481578946113586
train_iter_loss: 0.3048588037490845
train_iter_loss: 0.2113499939441681
train_iter_loss: 0.18656766414642334
train_iter_loss: 0.283993124961853
train_iter_loss: 0.3734569549560547
train_iter_loss: 0.22841686010360718
train_iter_loss: 0.47550830245018005
train_iter_loss: 0.24278390407562256
train_iter_loss: 0.3960173726081848
train_iter_loss: 0.20487980544567108
train_iter_loss: 0.3754103183746338
train_iter_loss: 0.3417486548423767
train_iter_loss: 0.34911879897117615
train_iter_loss: 0.23205380141735077
train_iter_loss: 0.24022802710533142
train_iter_loss: 0.1897130012512207
train_iter_loss: 0.39101073145866394
train_iter_loss: 0.38308408856391907
train_iter_loss: 0.2303043156862259
train_iter_loss: 0.3085334599018097
train_iter_loss: 0.3668459355831146
train_iter_loss: 0.2728666663169861
train_iter_loss: 0.3192993104457855
train_iter_loss: 0.25683295726776123
train_iter_loss: 0.3279315233230591
train_iter_loss: 0.30128535628318787
train_iter_loss: 0.1584705263376236
train_iter_loss: 0.2792176902294159
train_iter_loss: 0.15609042346477509
train_iter_loss: 0.09521784633398056
train_iter_loss: 0.20242273807525635
train_iter_loss: 0.16530410945415497
train_iter_loss: 0.16579864919185638
train_iter_loss: 0.29691246151924133
train_iter_loss: 0.19503548741340637
train_iter_loss: 0.07711471617221832
train_iter_loss: 0.2892976999282837
train_iter_loss: 0.44907140731811523
train_iter_loss: 0.2919614017009735
train_iter_loss: 0.30695345997810364
train_iter_loss: 0.4342484474182129
train_iter_loss: 0.29562562704086304
train_iter_loss: 0.3968948423862457
train_iter_loss: 0.24234861135482788
train_iter_loss: 0.20549067854881287
train_iter_loss: 0.16220535337924957
train_iter_loss: 0.3251582682132721
train_iter_loss: 0.28132566809654236
train_iter_loss: 0.2568145990371704
train_iter_loss: 0.3444046378135681
train_iter_loss: 0.3170321583747864
train_iter_loss: 0.2922278940677643
train_iter_loss: 0.34673336148262024
train_iter_loss: 0.27422255277633667
train_iter_loss: 0.22854501008987427
train_iter_loss: 0.3452319800853729
train_iter_loss: 0.342609167098999
train_iter_loss: 0.34104645252227783
train_iter_loss: 0.2704099416732788
train_iter_loss: 0.30719220638275146
train_iter_loss: 0.1376953274011612
train_iter_loss: 0.31671154499053955
train_iter_loss: 0.18914416432380676
train_iter_loss: 0.22118215262889862
train_iter_loss: 0.2590522766113281
train_iter_loss: 0.2203405648469925
train_iter_loss: 0.2516121566295624
train_iter_loss: 0.1645275503396988
train_iter_loss: 0.3842335045337677
train_iter_loss: 0.1897127777338028
train_iter_loss: 0.336194783449173
train_iter_loss: 0.2158423811197281
train_iter_loss: 0.3224392235279083
train_iter_loss: 0.3243664801120758
train_iter_loss: 0.14688169956207275
train_iter_loss: 0.17263317108154297
train_iter_loss: 0.3176509439945221
train_iter_loss: 0.2236376702785492
train_iter_loss: 0.33474215865135193
train_iter_loss: 0.24555587768554688
train_iter_loss: 0.2771921455860138
train_iter_loss: 0.2538839280605316
train_iter_loss: 0.3046509921550751
train_iter_loss: 0.2634817957878113
train_iter_loss: 0.21643322706222534
train_iter_loss: 0.2954464852809906
train_iter_loss: 0.2938198447227478
train_iter_loss: 0.2999497652053833
train_iter_loss: 0.38209018111228943
train_iter_loss: 0.3829532563686371
train_iter_loss: 0.4214557707309723
train_iter_loss: 0.3633367419242859
train_iter_loss: 0.37230491638183594
train_iter_loss: 0.3764644265174866
train_iter_loss: 0.18573947250843048
train_iter_loss: 0.35349199175834656
train loss :0.2856
---------------------
Validation seg loss: 0.40177562851283066 at epoch 168
epoch =    169/  1000, exp = train
train_iter_loss: 0.37184637784957886
train_iter_loss: 0.345595121383667
train_iter_loss: 0.3179599642753601
train_iter_loss: 0.17839179933071136
train_iter_loss: 0.34285831451416016
train_iter_loss: 0.23165372014045715
train_iter_loss: 0.33040887117385864
train_iter_loss: 0.3540034890174866
train_iter_loss: 0.3641810417175293
train_iter_loss: 0.24248448014259338
train_iter_loss: 0.2684461772441864
train_iter_loss: 0.4696306586265564
train_iter_loss: 0.345862478017807
train_iter_loss: 0.3305136561393738
train_iter_loss: 0.22975021600723267
train_iter_loss: 0.33403658866882324
train_iter_loss: 0.19449302554130554
train_iter_loss: 0.27883246541023254
train_iter_loss: 0.3134906589984894
train_iter_loss: 0.364780455827713
train_iter_loss: 0.2545657753944397
train_iter_loss: 0.3953533172607422
train_iter_loss: 0.33598166704177856
train_iter_loss: 0.22926324605941772
train_iter_loss: 0.20836125314235687
train_iter_loss: 0.21592454612255096
train_iter_loss: 0.37292778491973877
train_iter_loss: 0.244793102145195
train_iter_loss: 0.24636754393577576
train_iter_loss: 0.32777464389801025
train_iter_loss: 0.24793735146522522
train_iter_loss: 0.15899209678173065
train_iter_loss: 0.20075353980064392
train_iter_loss: 0.22489282488822937
train_iter_loss: 0.24615558981895447
train_iter_loss: 0.3524770140647888
train_iter_loss: 0.31382107734680176
train_iter_loss: 0.3214786648750305
train_iter_loss: 0.40091466903686523
train_iter_loss: 0.22726590931415558
train_iter_loss: 0.11417796462774277
train_iter_loss: 0.16722774505615234
train_iter_loss: 0.38579970598220825
train_iter_loss: 0.3064480423927307
train_iter_loss: 0.3150661587715149
train_iter_loss: 0.265931636095047
train_iter_loss: 0.264613538980484
train_iter_loss: 0.5074283480644226
train_iter_loss: 0.26475387811660767
train_iter_loss: 0.20044207572937012
train_iter_loss: 0.3202061951160431
train_iter_loss: 0.27452507615089417
train_iter_loss: 0.14885012805461884
train_iter_loss: 0.3281933069229126
train_iter_loss: 0.16656090319156647
train_iter_loss: 0.3372443914413452
train_iter_loss: 0.28894907236099243
train_iter_loss: 0.10357644408941269
train_iter_loss: 0.4567578136920929
train_iter_loss: 0.2475176453590393
train_iter_loss: 0.20426936447620392
train_iter_loss: 0.32538923621177673
train_iter_loss: 0.2262256145477295
train_iter_loss: 0.2848210036754608
train_iter_loss: 0.3224394619464874
train_iter_loss: 0.1992671638727188
train_iter_loss: 0.31493836641311646
train_iter_loss: 0.19904880225658417
train_iter_loss: 0.2661750614643097
train_iter_loss: 0.24226130545139313
train_iter_loss: 0.27576208114624023
train_iter_loss: 0.29819878935813904
train_iter_loss: 0.2789704501628876
train_iter_loss: 0.31515830755233765
train_iter_loss: 0.3611525595188141
train_iter_loss: 0.20479564368724823
train_iter_loss: 0.3091945946216583
train_iter_loss: 0.3034366965293884
train_iter_loss: 0.3551880419254303
train_iter_loss: 0.26360884308815
train_iter_loss: 0.31602150201797485
train_iter_loss: 0.052732910960912704
train_iter_loss: 0.5711109638214111
train_iter_loss: 0.4225187599658966
train_iter_loss: 0.4144355058670044
train_iter_loss: 0.338146448135376
train_iter_loss: 0.1737370491027832
train_iter_loss: 0.28062310814857483
train_iter_loss: 0.3442836105823517
train_iter_loss: 0.3574925363063812
train_iter_loss: 0.2319979965686798
train_iter_loss: 0.34199991822242737
train_iter_loss: 0.3286738395690918
train_iter_loss: 0.2504299581050873
train_iter_loss: 0.31839707493782043
train_iter_loss: 0.25107160210609436
train_iter_loss: 0.34718596935272217
train_iter_loss: 0.349040687084198
train_iter_loss: 0.18984834849834442
train_iter_loss: 0.46463796496391296
train loss :0.2938
---------------------
Validation seg loss: 0.39722980574687133 at epoch 169
epoch =    170/  1000, exp = train
train_iter_loss: 0.2318163812160492
train_iter_loss: 0.296835720539093
train_iter_loss: 0.20019221305847168
train_iter_loss: 0.32794493436813354
train_iter_loss: 0.39189788699150085
train_iter_loss: 0.31134143471717834
train_iter_loss: 0.3629804849624634
train_iter_loss: 0.29046371579170227
train_iter_loss: 0.26823145151138306
train_iter_loss: 0.5100542306900024
train_iter_loss: 0.3259906470775604
train_iter_loss: 0.3508375287055969
train_iter_loss: 0.25849658250808716
train_iter_loss: 0.3150448501110077
train_iter_loss: 0.23717756569385529
train_iter_loss: 0.25432685017585754
train_iter_loss: 0.24246574938297272
train_iter_loss: 0.27684417366981506
train_iter_loss: 0.32058632373809814
train_iter_loss: 0.27336204051971436
train_iter_loss: 0.3278920650482178
train_iter_loss: 0.2327776402235031
train_iter_loss: 0.22758403420448303
train_iter_loss: 0.27799496054649353
train_iter_loss: 0.2546572685241699
train_iter_loss: 0.2622356712818146
train_iter_loss: 0.18449506163597107
train_iter_loss: 0.3392793834209442
train_iter_loss: 0.5186974406242371
train_iter_loss: 0.1633206456899643
train_iter_loss: 0.23813119530677795
train_iter_loss: 0.2739889919757843
train_iter_loss: 0.1833062320947647
train_iter_loss: 0.31196144223213196
train_iter_loss: 0.31907689571380615
train_iter_loss: 0.10496987402439117
train_iter_loss: 0.28058552742004395
train_iter_loss: 0.3456100821495056
train_iter_loss: 0.31833401322364807
train_iter_loss: 0.2744162678718567
train_iter_loss: 0.31862539052963257
train_iter_loss: 0.2673776149749756
train_iter_loss: 0.3668338656425476
train_iter_loss: 0.27778974175453186
train_iter_loss: 0.3871827721595764
train_iter_loss: 0.40717586874961853
train_iter_loss: 0.2635571360588074
train_iter_loss: 0.26834189891815186
train_iter_loss: 0.5101537704467773
train_iter_loss: 0.27811717987060547
train_iter_loss: 0.24884554743766785
train_iter_loss: 0.28322717547416687
train_iter_loss: 0.359061062335968
train_iter_loss: 0.3077237904071808
train_iter_loss: 0.3308931589126587
train_iter_loss: 0.28217044472694397
train_iter_loss: 0.2135547399520874
train_iter_loss: 0.24523498117923737
train_iter_loss: 0.30217745900154114
train_iter_loss: 0.2807513475418091
train_iter_loss: 0.3055676221847534
train_iter_loss: 0.2919062077999115
train_iter_loss: 0.1805001199245453
train_iter_loss: 0.38243526220321655
train_iter_loss: 0.2784176170825958
train_iter_loss: 0.2971555292606354
train_iter_loss: 0.2299550175666809
train_iter_loss: 0.3305447995662689
train_iter_loss: 0.3006148934364319
train_iter_loss: 0.27389422059059143
train_iter_loss: 0.2981383204460144
train_iter_loss: 0.16479827463626862
train_iter_loss: 0.18568767607212067
train_iter_loss: 0.1798187643289566
train_iter_loss: 0.30518674850463867
train_iter_loss: 0.22413824498653412
train_iter_loss: 0.13006305694580078
train_iter_loss: 0.4143855571746826
train_iter_loss: 0.43737342953681946
train_iter_loss: 0.36657780408859253
train_iter_loss: 0.29337140917778015
train_iter_loss: 0.12675979733467102
train_iter_loss: 0.4574773907661438
train_iter_loss: 0.2995242476463318
train_iter_loss: 0.17829830944538116
train_iter_loss: 0.3323056101799011
train_iter_loss: 0.2535507380962372
train_iter_loss: 0.44552457332611084
train_iter_loss: 0.2165875881910324
train_iter_loss: 0.2211395651102066
train_iter_loss: 0.19595710933208466
train_iter_loss: 0.19604124128818512
train_iter_loss: 0.26764392852783203
train_iter_loss: 0.2668542265892029
train_iter_loss: 0.42443081736564636
train_iter_loss: 0.3292824923992157
train_iter_loss: 0.3479979336261749
train_iter_loss: 0.28530266880989075
train_iter_loss: 0.2175290882587433
train_iter_loss: 0.3458572030067444
train loss :0.2926
---------------------
Validation seg loss: 0.39033318785423377 at epoch 170
epoch =    171/  1000, exp = train
train_iter_loss: 0.30697575211524963
train_iter_loss: 0.16302427649497986
train_iter_loss: 0.2741226851940155
train_iter_loss: 0.23169739544391632
train_iter_loss: 0.1498902142047882
train_iter_loss: 0.3588680028915405
train_iter_loss: 0.3303065598011017
train_iter_loss: 0.1712631732225418
train_iter_loss: 0.4006613790988922
train_iter_loss: 0.28391677141189575
train_iter_loss: 0.4076508581638336
train_iter_loss: 0.30936506390571594
train_iter_loss: 0.4049024283885956
train_iter_loss: 0.35337260365486145
train_iter_loss: 0.44172751903533936
train_iter_loss: 0.3370847702026367
train_iter_loss: 0.2580949664115906
train_iter_loss: 0.29612332582473755
train_iter_loss: 0.3837592303752899
train_iter_loss: 0.32859063148498535
train_iter_loss: 0.35360175371170044
train_iter_loss: 0.17510613799095154
train_iter_loss: 0.4460296332836151
train_iter_loss: 0.3604528605937958
train_iter_loss: 0.313097208738327
train_iter_loss: 0.31859588623046875
train_iter_loss: 0.295864462852478
train_iter_loss: 0.3213965892791748
train_iter_loss: 0.16466310620307922
train_iter_loss: 0.3837461471557617
train_iter_loss: 0.3535917401313782
train_iter_loss: 0.37886539101600647
train_iter_loss: 0.12077433615922928
train_iter_loss: 0.3179609179496765
train_iter_loss: 0.24316011369228363
train_iter_loss: 0.19809898734092712
train_iter_loss: 0.4747944474220276
train_iter_loss: 0.31469717621803284
train_iter_loss: 0.19414591789245605
train_iter_loss: 0.3732774257659912
train_iter_loss: 0.19339266419410706
train_iter_loss: 0.28947827219963074
train_iter_loss: 0.12973178923130035
train_iter_loss: 0.15466204285621643
train_iter_loss: 0.1301029771566391
train_iter_loss: 0.09379864484071732
train_iter_loss: 0.35648098587989807
train_iter_loss: 0.26701757311820984
train_iter_loss: 0.26011067628860474
train_iter_loss: 0.24159575998783112
train_iter_loss: 0.2985050678253174
train_iter_loss: 0.21119645237922668
train_iter_loss: 0.31151705980300903
train_iter_loss: 0.3441888689994812
train_iter_loss: 0.3445262312889099
train_iter_loss: 0.38606929779052734
train_iter_loss: 0.2860308587551117
train_iter_loss: 0.2550000548362732
train_iter_loss: 0.2530122399330139
train_iter_loss: 0.27533891797065735
train_iter_loss: 0.1142774224281311
train_iter_loss: 0.23363986611366272
train_iter_loss: 0.41956692934036255
train_iter_loss: 0.2815941870212555
train_iter_loss: 0.33162355422973633
train_iter_loss: 0.24097508192062378
train_iter_loss: 0.26719075441360474
train_iter_loss: 0.29630613327026367
train_iter_loss: 0.24117730557918549
train_iter_loss: 0.3345423638820648
train_iter_loss: 0.33014005422592163
train_iter_loss: 0.28291910886764526
train_iter_loss: 0.2810867726802826
train_iter_loss: 0.22686171531677246
train_iter_loss: 0.30507418513298035
train_iter_loss: 0.20588995516300201
train_iter_loss: 0.3559412360191345
train_iter_loss: 0.2299777865409851
train_iter_loss: 0.3161814212799072
train_iter_loss: 0.155179962515831
train_iter_loss: 0.3266005218029022
train_iter_loss: 0.33294954895973206
train_iter_loss: 0.45885780453681946
train_iter_loss: 0.3386843204498291
train_iter_loss: 0.1715245246887207
train_iter_loss: 0.47890064120292664
train_iter_loss: 0.23889301717281342
train_iter_loss: 0.29089105129241943
train_iter_loss: 0.32696032524108887
train_iter_loss: 0.30916672945022583
train_iter_loss: 0.24993732571601868
train_iter_loss: 0.15704937279224396
train_iter_loss: 0.2497013360261917
train_iter_loss: 0.37610772252082825
train_iter_loss: 0.1938764452934265
train_iter_loss: 0.18143746256828308
train_iter_loss: 0.19025428593158722
train_iter_loss: 0.3095262050628662
train_iter_loss: 0.2742408812046051
train_iter_loss: 0.3542105257511139
train loss :0.2893
---------------------
Validation seg loss: 0.38988913858379676 at epoch 171
epoch =    172/  1000, exp = train
train_iter_loss: 0.26139482855796814
train_iter_loss: 0.4102696180343628
train_iter_loss: 0.3659411072731018
train_iter_loss: 0.25088056921958923
train_iter_loss: 0.4351578652858734
train_iter_loss: 0.3877217769622803
train_iter_loss: 0.27492257952690125
train_iter_loss: 0.3386109173297882
train_iter_loss: 0.2675541043281555
train_iter_loss: 0.28615620732307434
train_iter_loss: 0.18758524954319
train_iter_loss: 0.270760715007782
train_iter_loss: 0.2104470133781433
train_iter_loss: 0.31242799758911133
train_iter_loss: 0.30678698420524597
train_iter_loss: 0.26829347014427185
train_iter_loss: 0.3785162568092346
train_iter_loss: 0.3576807379722595
train_iter_loss: 0.2675324082374573
train_iter_loss: 0.24793432652950287
train_iter_loss: 0.16151708364486694
train_iter_loss: 0.3508807122707367
train_iter_loss: 0.27810996770858765
train_iter_loss: 0.3136592507362366
train_iter_loss: 0.20276600122451782
train_iter_loss: 0.2482793927192688
train_iter_loss: 0.37034183740615845
train_iter_loss: 0.19567395746707916
train_iter_loss: 0.2649458050727844
train_iter_loss: 0.2945760190486908
train_iter_loss: 0.2822500467300415
train_iter_loss: 0.32317861914634705
train_iter_loss: 0.23074805736541748
train_iter_loss: 0.2132609337568283
train_iter_loss: 0.38784927129745483
train_iter_loss: 0.25414687395095825
train_iter_loss: 0.2870059609413147
train_iter_loss: 0.20096537470817566
train_iter_loss: 0.2817496061325073
train_iter_loss: 0.2645878195762634
train_iter_loss: 0.32859596610069275
train_iter_loss: 0.20194144546985626
train_iter_loss: 0.4541608691215515
train_iter_loss: 0.22628286480903625
train_iter_loss: 0.24388760328292847
train_iter_loss: 0.4698525667190552
train_iter_loss: 0.24612464010715485
train_iter_loss: 0.3938150405883789
train_iter_loss: 0.1819078028202057
train_iter_loss: 0.17588913440704346
train_iter_loss: 0.2872330844402313
train_iter_loss: 0.19514283537864685
train_iter_loss: 0.26430660486221313
train_iter_loss: 0.24005550146102905
train_iter_loss: 0.3441673815250397
train_iter_loss: 0.19960978627204895
train_iter_loss: 0.23217856884002686
train_iter_loss: 0.22613100707530975
train_iter_loss: 0.518423855304718
train_iter_loss: 0.38094231486320496
train_iter_loss: 0.3821384310722351
train_iter_loss: 0.3162690997123718
train_iter_loss: 0.3125903904438019
train_iter_loss: 0.27059462666511536
train_iter_loss: 0.2909696698188782
train_iter_loss: 0.17289400100708008
train_iter_loss: 0.2711664140224457
train_iter_loss: 0.35932496190071106
train_iter_loss: 0.23004165291786194
train_iter_loss: 0.26029691100120544
train_iter_loss: 0.0427410788834095
train_iter_loss: 0.22044044733047485
train_iter_loss: 0.3155604600906372
train_iter_loss: 0.30893710255622864
train_iter_loss: 0.2613970935344696
train_iter_loss: 0.2724413573741913
train_iter_loss: 0.2628214359283447
train_iter_loss: 0.2148694396018982
train_iter_loss: 0.23720625042915344
train_iter_loss: 0.3487686514854431
train_iter_loss: 0.1893986314535141
train_iter_loss: 0.3565747141838074
train_iter_loss: 0.28923314809799194
train_iter_loss: 0.28339043259620667
train_iter_loss: 0.30287304520606995
train_iter_loss: 0.2603873610496521
train_iter_loss: 0.37505218386650085
train_iter_loss: 0.2536548674106598
train_iter_loss: 0.379648357629776
train_iter_loss: 0.3277297019958496
train_iter_loss: 0.32679668068885803
train_iter_loss: 0.1539938747882843
train_iter_loss: 0.25283199548721313
train_iter_loss: 0.23479612171649933
train_iter_loss: 0.3152602016925812
train_iter_loss: 0.3262064456939697
train_iter_loss: 0.2830297648906708
train_iter_loss: 0.3053984045982361
train_iter_loss: 0.2060181200504303
train_iter_loss: 0.20001061260700226
train loss :0.2857
---------------------
Validation seg loss: 0.3821218170331053 at epoch 172
epoch =    173/  1000, exp = train
train_iter_loss: 0.3118654191493988
train_iter_loss: 0.23450365662574768
train_iter_loss: 0.3029729723930359
train_iter_loss: 0.41192561388015747
train_iter_loss: 0.3531854450702667
train_iter_loss: 0.2289511114358902
train_iter_loss: 0.24878917634487152
train_iter_loss: 0.2906009554862976
train_iter_loss: 0.30940550565719604
train_iter_loss: 0.3277668058872223
train_iter_loss: 0.16418755054473877
train_iter_loss: 0.3499753475189209
train_iter_loss: 0.2646818161010742
train_iter_loss: 0.36808228492736816
train_iter_loss: 0.30620038509368896
train_iter_loss: 0.33781349658966064
train_iter_loss: 0.20674461126327515
train_iter_loss: 0.3267788887023926
train_iter_loss: 0.2505759298801422
train_iter_loss: 0.3182806372642517
train_iter_loss: 0.3565715551376343
train_iter_loss: 0.2563970983028412
train_iter_loss: 0.3044094443321228
train_iter_loss: 0.29863616824150085
train_iter_loss: 0.2656082510948181
train_iter_loss: 0.2674224376678467
train_iter_loss: 0.23059503734111786
train_iter_loss: 0.12014961987733841
train_iter_loss: 0.31991952657699585
train_iter_loss: 0.24716463685035706
train_iter_loss: 0.22707800567150116
train_iter_loss: 0.1892125904560089
train_iter_loss: 0.450647234916687
train_iter_loss: 0.5535731315612793
train_iter_loss: 0.2955435812473297
train_iter_loss: 0.30329179763793945
train_iter_loss: 0.41818323731422424
train_iter_loss: 0.12235162407159805
train_iter_loss: 0.3522419333457947
train_iter_loss: 0.34233978390693665
train_iter_loss: 0.3319547772407532
train_iter_loss: 0.1382005661725998
train_iter_loss: 0.3231745660305023
train_iter_loss: 0.1873229295015335
train_iter_loss: 0.34324759244918823
train_iter_loss: 0.3133842349052429
train_iter_loss: 0.1323176920413971
train_iter_loss: 0.33747777342796326
train_iter_loss: 0.35534682869911194
train_iter_loss: 0.3218664526939392
train_iter_loss: 0.2986975908279419
train_iter_loss: 0.34861478209495544
train_iter_loss: 0.2431022673845291
train_iter_loss: 0.38497233390808105
train_iter_loss: 0.5443245768547058
train_iter_loss: 0.2399132400751114
train_iter_loss: 0.3546069860458374
train_iter_loss: 0.21354201436042786
train_iter_loss: 0.6096476316452026
train_iter_loss: 0.3546282947063446
train_iter_loss: 0.38656944036483765
train_iter_loss: 0.19915005564689636
train_iter_loss: 0.25902092456817627
train_iter_loss: 0.2122364491224289
train_iter_loss: 0.20512741804122925
train_iter_loss: 0.11566852778196335
train_iter_loss: 0.3368782103061676
train_iter_loss: 0.3161175847053528
train_iter_loss: 0.14627282321453094
train_iter_loss: 0.1933080554008484
train_iter_loss: 0.2078068107366562
train_iter_loss: 0.2581672966480255
train_iter_loss: 0.27614888548851013
train_iter_loss: 0.20563772320747375
train_iter_loss: 0.20138782262802124
train_iter_loss: 0.18291957676410675
train_iter_loss: 0.1401306837797165
train_iter_loss: 0.22713997960090637
train_iter_loss: 0.2685137689113617
train_iter_loss: 0.317853718996048
train_iter_loss: 0.2864964008331299
train_iter_loss: 0.26799139380455017
train_iter_loss: 0.4067022502422333
train_iter_loss: 0.2653596103191376
train_iter_loss: 0.31354954838752747
train_iter_loss: 0.2039153277873993
train_iter_loss: 0.3986571431159973
train_iter_loss: 0.4109644591808319
train_iter_loss: 0.39847174286842346
train_iter_loss: 0.31981316208839417
train_iter_loss: 0.3429079055786133
train_iter_loss: 0.2335088700056076
train_iter_loss: 0.39162319898605347
train_iter_loss: 0.2181672751903534
train_iter_loss: 0.2441837340593338
train_iter_loss: 0.23664799332618713
train_iter_loss: 0.19214726984500885
train_iter_loss: 0.25784337520599365
train_iter_loss: 0.19690494239330292
train_iter_loss: 0.09680534899234772
train loss :0.2884
---------------------
Validation seg loss: 0.36925693798177645 at epoch 173
epoch =    174/  1000, exp = train
train_iter_loss: 0.27619320154190063
train_iter_loss: 0.3411233723163605
train_iter_loss: 0.3427349030971527
train_iter_loss: 0.21698321402072906
train_iter_loss: 0.27819615602493286
train_iter_loss: 0.27361586689949036
train_iter_loss: 0.3003523647785187
train_iter_loss: 0.21568931639194489
train_iter_loss: 0.2696417570114136
train_iter_loss: 0.3305700421333313
train_iter_loss: 0.24161581695079803
train_iter_loss: 0.19239790737628937
train_iter_loss: 0.29482924938201904
train_iter_loss: 0.2526620924472809
train_iter_loss: 0.2392626255750656
train_iter_loss: 0.34009861946105957
train_iter_loss: 0.4357057213783264
train_iter_loss: 0.3131646513938904
train_iter_loss: 0.32039740681648254
train_iter_loss: 0.3552020192146301
train_iter_loss: 0.1974075436592102
train_iter_loss: 0.3682345449924469
train_iter_loss: 0.2633815407752991
train_iter_loss: 0.2501506507396698
train_iter_loss: 0.3956645131111145
train_iter_loss: 0.265032559633255
train_iter_loss: 0.22529451549053192
train_iter_loss: 0.28062179684638977
train_iter_loss: 0.2600228488445282
train_iter_loss: 0.3164888620376587
train_iter_loss: 0.34919849038124084
train_iter_loss: 0.10674092173576355
train_iter_loss: 0.3378244936466217
train_iter_loss: 0.2658829987049103
train_iter_loss: 0.40472620725631714
train_iter_loss: 0.28499075770378113
train_iter_loss: 0.2489975094795227
train_iter_loss: 0.18804442882537842
train_iter_loss: 0.1555536985397339
train_iter_loss: 0.3185630142688751
train_iter_loss: 0.18969568610191345
train_iter_loss: 0.3042304217815399
train_iter_loss: 0.3120321035385132
train_iter_loss: 0.2836076319217682
train_iter_loss: 0.25836512446403503
train_iter_loss: 0.24792969226837158
train_iter_loss: 0.4957917630672455
train_iter_loss: 0.25949493050575256
train_iter_loss: 0.24232007563114166
train_iter_loss: 0.31422165036201477
train_iter_loss: 0.33211106061935425
train_iter_loss: 0.20651905238628387
train_iter_loss: 0.28237226605415344
train_iter_loss: 0.22385017573833466
train_iter_loss: 0.2421441674232483
train_iter_loss: 0.48382624983787537
train_iter_loss: 0.24077671766281128
train_iter_loss: 0.19085629284381866
train_iter_loss: 0.4114241600036621
train_iter_loss: 0.36298754811286926
train_iter_loss: 0.23295335471630096
train_iter_loss: 0.4864277243614197
train_iter_loss: 0.3392733037471771
train_iter_loss: 0.33622533082962036
train_iter_loss: 0.3312937021255493
train_iter_loss: 0.30493295192718506
train_iter_loss: 0.2831471860408783
train_iter_loss: 0.30282604694366455
train_iter_loss: 0.2092476785182953
train_iter_loss: 0.22073400020599365
train_iter_loss: 0.23186706006526947
train_iter_loss: 0.2498594969511032
train_iter_loss: 0.2257750928401947
train_iter_loss: 0.3321574032306671
train_iter_loss: 0.3319269120693207
train_iter_loss: 0.3052988350391388
train_iter_loss: 0.2885906398296356
train_iter_loss: 0.33718618750572205
train_iter_loss: 0.4415307939052582
train_iter_loss: 0.18604768812656403
train_iter_loss: 0.4472944438457489
train_iter_loss: 0.23658841848373413
train_iter_loss: 0.27810052037239075
train_iter_loss: 0.3489157557487488
train_iter_loss: 0.26664766669273376
train_iter_loss: 0.358101487159729
train_iter_loss: 0.16536852717399597
train_iter_loss: 0.42102423310279846
train_iter_loss: 0.19470952451229095
train_iter_loss: 0.4531077444553375
train_iter_loss: 0.28464508056640625
train_iter_loss: 0.35777658224105835
train_iter_loss: 0.3920944333076477
train_iter_loss: 0.17069973051548004
train_iter_loss: 0.34656041860580444
train_iter_loss: 0.3790299892425537
train_iter_loss: 0.32697993516921997
train_iter_loss: 0.2936895787715912
train_iter_loss: 0.3339649438858032
train_iter_loss: 0.3444487750530243
train loss :0.2986
---------------------
Validation seg loss: 0.3718433313177161 at epoch 174
epoch =    175/  1000, exp = train
train_iter_loss: 0.4138570725917816
train_iter_loss: 0.22931009531021118
train_iter_loss: 0.19198842346668243
train_iter_loss: 0.28906604647636414
train_iter_loss: 0.46955621242523193
train_iter_loss: 0.19971507787704468
train_iter_loss: 0.5176151394844055
train_iter_loss: 0.20046867430210114
train_iter_loss: 0.44172403216362
train_iter_loss: 0.14895029366016388
train_iter_loss: 0.30623430013656616
train_iter_loss: 0.3387935757637024
train_iter_loss: 0.42882975935935974
train_iter_loss: 0.4410672187805176
train_iter_loss: 0.2843003571033478
train_iter_loss: 0.2789328098297119
train_iter_loss: 0.23345555365085602
train_iter_loss: 0.22190281748771667
train_iter_loss: 0.3758593499660492
train_iter_loss: 0.2948302924633026
train_iter_loss: 0.33320432901382446
train_iter_loss: 0.19981613755226135
train_iter_loss: 0.23089206218719482
train_iter_loss: 0.3145747482776642
train_iter_loss: 0.2502487599849701
train_iter_loss: 0.2724002003669739
train_iter_loss: 0.335812509059906
train_iter_loss: 0.3001744747161865
train_iter_loss: 0.2917065918445587
train_iter_loss: 0.2610440254211426
train_iter_loss: 0.3886089026927948
train_iter_loss: 0.26891028881073
train_iter_loss: 0.3693200647830963
train_iter_loss: 0.27104347944259644
train_iter_loss: 0.24652931094169617
train_iter_loss: 0.24178576469421387
train_iter_loss: 0.35639888048171997
train_iter_loss: 0.31756678223609924
train_iter_loss: 0.30042165517807007
train_iter_loss: 0.22010573744773865
train_iter_loss: 0.36778953671455383
train_iter_loss: 0.2798140048980713
train_iter_loss: 0.2039148360490799
train_iter_loss: 0.35761940479278564
train_iter_loss: 0.26663294434547424
train_iter_loss: 0.45307043194770813
train_iter_loss: 0.3085234463214874
train_iter_loss: 0.37721526622772217
train_iter_loss: 0.22080041468143463
train_iter_loss: 0.23220254480838776
train_iter_loss: 0.12205297499895096
train_iter_loss: 0.30583396553993225
train_iter_loss: 0.2765316665172577
train_iter_loss: 0.35141128301620483
train_iter_loss: 0.2517831027507782
train_iter_loss: 0.26406311988830566
train_iter_loss: 0.3000327944755554
train_iter_loss: 0.28316617012023926
train_iter_loss: 0.19535835087299347
train_iter_loss: 0.396228164434433
train_iter_loss: 0.3976060748100281
train_iter_loss: 0.24357081949710846
train_iter_loss: 0.35612717270851135
train_iter_loss: 0.28922826051712036
train_iter_loss: 0.2663852274417877
train_iter_loss: 0.1442520022392273
train_iter_loss: 0.3137843906879425
train_iter_loss: 0.27090513706207275
train_iter_loss: 0.18762505054473877
train_iter_loss: 0.31779924035072327
train_iter_loss: 0.41702237725257874
train_iter_loss: 0.2151651233434677
train_iter_loss: 0.3178566098213196
train_iter_loss: 0.28911009430885315
train_iter_loss: 0.259610652923584
train_iter_loss: 0.08363772183656693
train_iter_loss: 0.21691910922527313
train_iter_loss: 0.12619458138942719
train_iter_loss: 0.1465184986591339
train_iter_loss: 0.2956995368003845
train_iter_loss: 0.22836031019687653
train_iter_loss: 0.3263493478298187
train_iter_loss: 0.12572982907295227
train_iter_loss: 0.22217051684856415
train_iter_loss: 0.331813782453537
train_iter_loss: 0.2799851894378662
train_iter_loss: 0.2542625665664673
train_iter_loss: 0.2728457450866699
train_iter_loss: 0.1779058277606964
train_iter_loss: 0.43895331025123596
train_iter_loss: 0.34878429770469666
train_iter_loss: 0.23965167999267578
train_iter_loss: 0.45075780153274536
train_iter_loss: 0.36601734161376953
train_iter_loss: 0.274183988571167
train_iter_loss: 0.3195152282714844
train_iter_loss: 0.43760788440704346
train_iter_loss: 0.34117934107780457
train_iter_loss: 0.30222824215888977
train_iter_loss: 0.2639203369617462
train loss :0.2935
---------------------
Validation seg loss: 0.38890050329654563 at epoch 175
epoch =    176/  1000, exp = train
train_iter_loss: 0.3154401183128357
train_iter_loss: 0.2875305712223053
train_iter_loss: 0.15107734501361847
train_iter_loss: 0.1594831794500351
train_iter_loss: 0.45704326033592224
train_iter_loss: 0.4019618332386017
train_iter_loss: 0.2706245183944702
train_iter_loss: 0.19187219440937042
train_iter_loss: 0.19593095779418945
train_iter_loss: 0.2983260452747345
train_iter_loss: 0.2959441542625427
train_iter_loss: 0.2242688089609146
train_iter_loss: 0.30828917026519775
train_iter_loss: 0.385621577501297
train_iter_loss: 0.34612002968788147
train_iter_loss: 0.15418973565101624
train_iter_loss: 0.25324657559394836
train_iter_loss: 0.28328177332878113
train_iter_loss: 0.31291142106056213
train_iter_loss: 0.2576439082622528
train_iter_loss: 0.28336620330810547
train_iter_loss: 0.24520571529865265
train_iter_loss: 0.2417193204164505
train_iter_loss: 0.28659385442733765
train_iter_loss: 0.3653128147125244
train_iter_loss: 0.2410995066165924
train_iter_loss: 0.2736833989620209
train_iter_loss: 0.253449946641922
train_iter_loss: 0.28777337074279785
train_iter_loss: 0.3130996823310852
train_iter_loss: 0.3973332643508911
train_iter_loss: 0.3803354501724243
train_iter_loss: 0.19531385600566864
train_iter_loss: 0.3244861662387848
train_iter_loss: 0.3566979765892029
train_iter_loss: 0.29730984568595886
train_iter_loss: 0.30368274450302124
train_iter_loss: 0.33206960558891296
train_iter_loss: 0.16423915326595306
train_iter_loss: 0.11650142818689346
train_iter_loss: 0.3558753430843353
train_iter_loss: 0.11548762023448944
train_iter_loss: 0.2910676896572113
train_iter_loss: 0.2748725712299347
train_iter_loss: 0.43749579787254333
train_iter_loss: 0.2968057096004486
train_iter_loss: 0.200506791472435
train_iter_loss: 0.3124183416366577
train_iter_loss: 0.2664754092693329
train_iter_loss: 0.240327849984169
train_iter_loss: 0.29359912872314453
train_iter_loss: 0.3565842807292938
train_iter_loss: 0.3946692943572998
train_iter_loss: 0.259848952293396
train_iter_loss: 0.3262719511985779
train_iter_loss: 0.47930440306663513
train_iter_loss: 0.24842190742492676
train_iter_loss: 0.15207332372665405
train_iter_loss: 0.3165487051010132
train_iter_loss: 0.39210429787635803
train_iter_loss: 0.2131955772638321
train_iter_loss: 0.21832768619060516
train_iter_loss: 0.4029511511325836
train_iter_loss: 0.37588682770729065
train_iter_loss: 0.29694610834121704
train_iter_loss: 0.21344639360904694
train_iter_loss: 0.13731402158737183
train_iter_loss: 0.3592585623264313
train_iter_loss: 0.2594020962715149
train_iter_loss: 0.20921939611434937
train_iter_loss: 0.19313177466392517
train_iter_loss: 0.3368745744228363
train_iter_loss: 0.3328394889831543
train_iter_loss: 0.1561896950006485
train_iter_loss: 0.10452743619680405
train_iter_loss: 0.3426206707954407
train_iter_loss: 0.1828426867723465
train_iter_loss: 0.28978070616722107
train_iter_loss: 0.1784934252500534
train_iter_loss: 0.2685922682285309
train_iter_loss: 0.2115614414215088
train_iter_loss: 0.39507782459259033
train_iter_loss: 0.2629995346069336
train_iter_loss: 0.24275273084640503
train_iter_loss: 0.29243630170822144
train_iter_loss: 0.30003270506858826
train_iter_loss: 0.14031469821929932
train_iter_loss: 0.4314202666282654
train_iter_loss: 0.29365336894989014
train_iter_loss: 0.5060656070709229
train_iter_loss: 0.3380519449710846
train_iter_loss: 0.23278659582138062
train_iter_loss: 0.44406741857528687
train_iter_loss: 0.3656999170780182
train_iter_loss: 0.3328119218349457
train_iter_loss: 0.36656245589256287
train_iter_loss: 0.2646705210208893
train_iter_loss: 0.35298267006874084
train_iter_loss: 0.520422637462616
train_iter_loss: 0.3090977668762207
train loss :0.2919
---------------------
Validation seg loss: 0.4416496087269822 at epoch 176
epoch =    177/  1000, exp = train
train_iter_loss: 0.39931243658065796
train_iter_loss: 0.3156299889087677
train_iter_loss: 0.28485947847366333
train_iter_loss: 0.22172686457633972
train_iter_loss: 0.2061077058315277
train_iter_loss: 0.4130800664424896
train_iter_loss: 0.3833473026752472
train_iter_loss: 0.38190552592277527
train_iter_loss: 0.22359177470207214
train_iter_loss: 0.19097574055194855
train_iter_loss: 0.3809036314487457
train_iter_loss: 0.19307231903076172
train_iter_loss: 0.27389711141586304
train_iter_loss: 0.2990800738334656
train_iter_loss: 0.17456404864788055
train_iter_loss: 0.2617552876472473
train_iter_loss: 0.4950818419456482
train_iter_loss: 0.19726614654064178
train_iter_loss: 0.3739084005355835
train_iter_loss: 0.14419390261173248
train_iter_loss: 0.3369314670562744
train_iter_loss: 0.1280212700366974
train_iter_loss: 0.3236817717552185
train_iter_loss: 0.28696054220199585
train_iter_loss: 0.17863838374614716
train_iter_loss: 0.17931799590587616
train_iter_loss: 0.4223335087299347
train_iter_loss: 0.40722891688346863
train_iter_loss: 0.2129361480474472
train_iter_loss: 0.2778923213481903
train_iter_loss: 0.16582617163658142
train_iter_loss: 0.2031625360250473
train_iter_loss: 0.2052878588438034
train_iter_loss: 0.39081674814224243
train_iter_loss: 0.23863370716571808
train_iter_loss: 0.3015793561935425
train_iter_loss: 0.3110925555229187
train_iter_loss: 0.2383536696434021
train_iter_loss: 0.3305083215236664
train_iter_loss: 0.24829719960689545
train_iter_loss: 0.277801513671875
train_iter_loss: 0.23256558179855347
train_iter_loss: 0.18208354711532593
train_iter_loss: 0.3564598858356476
train_iter_loss: 0.23964239656925201
train_iter_loss: 0.3588321805000305
train_iter_loss: 0.13628794252872467
train_iter_loss: 0.3290865123271942
train_iter_loss: 0.3986213207244873
train_iter_loss: 0.2703551650047302
train_iter_loss: 0.23250533640384674
train_iter_loss: 0.3235529065132141
train_iter_loss: 0.3572646677494049
train_iter_loss: 0.4831602871417999
train_iter_loss: 0.21077658236026764
train_iter_loss: 0.4378229081630707
train_iter_loss: 0.38579607009887695
train_iter_loss: 0.39308202266693115
train_iter_loss: 0.3614238500595093
train_iter_loss: 0.2534469664096832
train_iter_loss: 0.31238967180252075
train_iter_loss: 0.3986651599407196
train_iter_loss: 0.29886382818222046
train_iter_loss: 0.25505882501602173
train_iter_loss: 0.4056417644023895
train_iter_loss: 0.2809397578239441
train_iter_loss: 0.14074555039405823
train_iter_loss: 0.19992467761039734
train_iter_loss: 0.28409141302108765
train_iter_loss: 0.27703335881233215
train_iter_loss: 0.34729325771331787
train_iter_loss: 0.26570186018943787
train_iter_loss: 0.27833476662635803
train_iter_loss: 0.22165359556674957
train_iter_loss: 0.28678762912750244
train_iter_loss: 0.19086691737174988
train_iter_loss: 0.2771749198436737
train_iter_loss: 0.2822449505329132
train_iter_loss: 0.31391388177871704
train_iter_loss: 0.2577351927757263
train_iter_loss: 0.4598321318626404
train_iter_loss: 0.4357447624206543
train_iter_loss: 0.34095457196235657
train_iter_loss: 0.48225080966949463
train_iter_loss: 0.2630922496318817
train_iter_loss: 0.2470255047082901
train_iter_loss: 0.2535311281681061
train_iter_loss: 0.22847767174243927
train_iter_loss: 0.3753475546836853
train_iter_loss: 0.28500282764434814
train_iter_loss: 0.3065253794193268
train_iter_loss: 0.2871544063091278
train_iter_loss: 0.1883319467306137
train_iter_loss: 0.30600351095199585
train_iter_loss: 0.30663731694221497
train_iter_loss: 0.19112402200698853
train_iter_loss: 0.2593103349208832
train_iter_loss: 0.3222258388996124
train_iter_loss: 0.32394397258758545
train_iter_loss: 0.39811453223228455
train loss :0.2955
---------------------
Validation seg loss: 0.3854794530900863 at epoch 177
epoch =    178/  1000, exp = train
train_iter_loss: 0.36608636379241943
train_iter_loss: 0.286109983921051
train_iter_loss: 0.35526949167251587
train_iter_loss: 0.27034425735473633
train_iter_loss: 0.16198846697807312
train_iter_loss: 0.23588721454143524
train_iter_loss: 0.30977678298950195
train_iter_loss: 0.38409170508384705
train_iter_loss: 0.319442480802536
train_iter_loss: 0.3032347857952118
train_iter_loss: 0.15796759724617004
train_iter_loss: 0.3583701550960541
train_iter_loss: 0.37737351655960083
train_iter_loss: 0.35694077610969543
train_iter_loss: 0.2939269244670868
train_iter_loss: 0.2384078949689865
train_iter_loss: 0.37461090087890625
train_iter_loss: 0.26087331771850586
train_iter_loss: 0.18830272555351257
train_iter_loss: 0.214127317070961
train_iter_loss: 0.2451457381248474
train_iter_loss: 0.326340913772583
train_iter_loss: 0.24084290862083435
train_iter_loss: 0.21378111839294434
train_iter_loss: 0.18384110927581787
train_iter_loss: 0.4183078110218048
train_iter_loss: 0.3326292932033539
train_iter_loss: 0.17482459545135498
train_iter_loss: 0.23332074284553528
train_iter_loss: 0.423694372177124
train_iter_loss: 0.27564340829849243
train_iter_loss: 0.2445441335439682
train_iter_loss: 0.2935776114463806
train_iter_loss: 0.29601168632507324
train_iter_loss: 0.3040114939212799
train_iter_loss: 0.36010295152664185
train_iter_loss: 0.2574702203273773
train_iter_loss: 0.30524617433547974
train_iter_loss: 0.43189263343811035
train_iter_loss: 0.32241976261138916
train_iter_loss: 0.25749242305755615
train_iter_loss: 0.2482793778181076
train_iter_loss: 0.16330495476722717
train_iter_loss: 0.29040443897247314
train_iter_loss: 0.34058237075805664
train_iter_loss: 0.22925600409507751
train_iter_loss: 0.27016308903694153
train_iter_loss: 0.23998647928237915
train_iter_loss: 0.3208407163619995
train_iter_loss: 0.3422197699546814
train_iter_loss: 0.3201310634613037
train_iter_loss: 0.21413211524486542
train_iter_loss: 0.24006956815719604
train_iter_loss: 0.30246269702911377
train_iter_loss: 0.3537272810935974
train_iter_loss: 0.7253139615058899
train_iter_loss: 0.4208105206489563
train_iter_loss: 0.31638091802597046
train_iter_loss: 0.20219974219799042
train_iter_loss: 0.21080157160758972
train_iter_loss: 0.4122079610824585
train_iter_loss: 0.3573273718357086
train_iter_loss: 0.31740373373031616
train_iter_loss: 0.3724915683269501
train_iter_loss: 0.36862504482269287
train_iter_loss: 0.36846432089805603
train_iter_loss: 0.18644709885120392
train_iter_loss: 0.07572361826896667
train_iter_loss: 0.261465460062027
train_iter_loss: 0.3335815370082855
train_iter_loss: 0.22634653747081757
train_iter_loss: 0.37184685468673706
train_iter_loss: 0.45846259593963623
train_iter_loss: 0.2948630750179291
train_iter_loss: 0.21022112667560577
train_iter_loss: 0.34607741236686707
train_iter_loss: 0.3529396951198578
train_iter_loss: 0.23570838570594788
train_iter_loss: 0.25436151027679443
train_iter_loss: 0.26934024691581726
train_iter_loss: 0.31266310811042786
train_iter_loss: 0.3200753629207611
train_iter_loss: 0.236662358045578
train_iter_loss: 0.19449391961097717
train_iter_loss: 0.3722388744354248
train_iter_loss: 0.32766637206077576
train_iter_loss: 0.18505661189556122
train_iter_loss: 0.30769219994544983
train_iter_loss: 0.3561969995498657
train_iter_loss: 0.2755672335624695
train_iter_loss: 0.23413096368312836
train_iter_loss: 0.29656997323036194
train_iter_loss: 0.23570406436920166
train_iter_loss: 0.20656739175319672
train_iter_loss: 0.21023517847061157
train_iter_loss: 0.29917463660240173
train_iter_loss: 0.29637107253074646
train_iter_loss: 0.2420671284198761
train_iter_loss: 0.2917283773422241
train_iter_loss: 0.24996481835842133
train loss :0.2953
---------------------
Validation seg loss: 0.37023742827323247 at epoch 178
epoch =    179/  1000, exp = train
train_iter_loss: 0.17892438173294067
train_iter_loss: 0.3311764895915985
train_iter_loss: 0.3057928681373596
train_iter_loss: 0.4535890519618988
train_iter_loss: 0.2760327160358429
train_iter_loss: 0.4136129319667816
train_iter_loss: 0.14016246795654297
train_iter_loss: 0.32799309492111206
train_iter_loss: 0.3195938467979431
train_iter_loss: 0.28102973103523254
train_iter_loss: 0.2907082438468933
train_iter_loss: 0.25539878010749817
train_iter_loss: 0.43711555004119873
train_iter_loss: 0.30463743209838867
train_iter_loss: 0.35520079731941223
train_iter_loss: 0.2719113528728485
train_iter_loss: 0.19755502045154572
train_iter_loss: 0.2584601640701294
train_iter_loss: 0.347846120595932
train_iter_loss: 0.2115950733423233
train_iter_loss: 0.37396439909935
train_iter_loss: 0.2948567569255829
train_iter_loss: 0.11805523931980133
train_iter_loss: 0.30237361788749695
train_iter_loss: 0.49717989563941956
train_iter_loss: 0.2014380693435669
train_iter_loss: 0.31381845474243164
train_iter_loss: 0.29299718141555786
train_iter_loss: 0.24877510964870453
train_iter_loss: 0.12121174484491348
train_iter_loss: 0.32015717029571533
train_iter_loss: 0.46182921528816223
train_iter_loss: 0.24372342228889465
train_iter_loss: 0.33737367391586304
train_iter_loss: 0.2269473522901535
train_iter_loss: 0.4176790714263916
train_iter_loss: 0.25273454189300537
train_iter_loss: 0.20791682600975037
train_iter_loss: 0.245865136384964
train_iter_loss: 0.44859883189201355
train_iter_loss: 0.18419191241264343
train_iter_loss: 0.250569224357605
train_iter_loss: 0.3015104830265045
train_iter_loss: 0.27465957403182983
train_iter_loss: 0.4003220498561859
train_iter_loss: 0.24795827269554138
train_iter_loss: 0.30117547512054443
train_iter_loss: 0.29807358980178833
train_iter_loss: 0.4059341251850128
train_iter_loss: 0.19754698872566223
train_iter_loss: 0.21317847073078156
train_iter_loss: 0.31064119935035706
train_iter_loss: 0.25057733058929443
train_iter_loss: 0.412261426448822
train_iter_loss: 0.3493651747703552
train_iter_loss: 0.2119280993938446
train_iter_loss: 0.3291514813899994
train_iter_loss: 0.27513089776039124
train_iter_loss: 0.3268319368362427
train_iter_loss: 0.20892058312892914
train_iter_loss: 0.10916920006275177
train_iter_loss: 0.3931580185890198
train_iter_loss: 0.1910744160413742
train_iter_loss: 0.3306521475315094
train_iter_loss: 0.2832472026348114
train_iter_loss: 0.33309516310691833
train_iter_loss: 0.4221278727054596
train_iter_loss: 0.23457002639770508
train_iter_loss: 0.36088237166404724
train_iter_loss: 0.17981155216693878
train_iter_loss: 0.2615663707256317
train_iter_loss: 0.3383367359638214
train_iter_loss: 0.26080650091171265
train_iter_loss: 0.2830915153026581
train_iter_loss: 0.30247896909713745
train_iter_loss: 0.3930278420448303
train_iter_loss: 0.4149632155895233
train_iter_loss: 0.17127420008182526
train_iter_loss: 0.25408923625946045
train_iter_loss: 0.34609657526016235
train_iter_loss: 0.21927814185619354
train_iter_loss: 0.25078055262565613
train_iter_loss: 0.28025585412979126
train_iter_loss: 0.4279833137989044
train_iter_loss: 0.27808877825737
train_iter_loss: 0.2512647807598114
train_iter_loss: 0.24681855738162994
train_iter_loss: 0.1805102527141571
train_iter_loss: 0.2336501032114029
train_iter_loss: 0.30229392647743225
train_iter_loss: 0.26005327701568604
train_iter_loss: 0.22248120605945587
train_iter_loss: 0.2620410621166229
train_iter_loss: 0.21061348915100098
train_iter_loss: 0.16093428432941437
train_iter_loss: 0.3807453513145447
train_iter_loss: 0.32274821400642395
train_iter_loss: 0.3464870750904083
train_iter_loss: 0.23354534804821014
train_iter_loss: 0.20344510674476624
train loss :0.2907
---------------------
Validation seg loss: 0.36922653014156614 at epoch 179
epoch =    180/  1000, exp = train
train_iter_loss: 0.2855428457260132
train_iter_loss: 0.4415651261806488
train_iter_loss: 0.2974678874015808
train_iter_loss: 0.32905396819114685
train_iter_loss: 0.5287080407142639
train_iter_loss: 0.2723586857318878
train_iter_loss: 0.19218263030052185
train_iter_loss: 0.2771100103855133
train_iter_loss: 0.45251673460006714
train_iter_loss: 0.2374534010887146
train_iter_loss: 0.13687129318714142
train_iter_loss: 0.36378535628318787
train_iter_loss: 0.17753984034061432
train_iter_loss: 0.22559155523777008
train_iter_loss: 0.21534566581249237
train_iter_loss: 0.35976794362068176
train_iter_loss: 0.2613232135772705
train_iter_loss: 0.30927056074142456
train_iter_loss: 0.3344408869743347
train_iter_loss: 0.3052169382572174
train_iter_loss: 0.4860369563102722
train_iter_loss: 0.18740585446357727
train_iter_loss: 0.223146453499794
train_iter_loss: 0.3565673530101776
train_iter_loss: 0.19525150954723358
train_iter_loss: 0.2990783751010895
train_iter_loss: 0.17741750180721283
train_iter_loss: 0.21933208405971527
train_iter_loss: 0.2530078887939453
train_iter_loss: 0.3273674249649048
train_iter_loss: 0.3277551531791687
train_iter_loss: 0.34074193239212036
train_iter_loss: 0.21428315341472626
train_iter_loss: 0.21500475704669952
train_iter_loss: 0.24469102919101715
train_iter_loss: 0.3101727366447449
train_iter_loss: 0.11941630393266678
train_iter_loss: 0.4840710163116455
train_iter_loss: 0.3342457413673401
train_iter_loss: 0.5033184289932251
train_iter_loss: 0.17912782728672028
train_iter_loss: 0.25537532567977905
train_iter_loss: 0.25480887293815613
train_iter_loss: 0.3545624911785126
train_iter_loss: 0.432365745306015
train_iter_loss: 0.321229487657547
train_iter_loss: 0.21818146109580994
train_iter_loss: 0.33286839723587036
train_iter_loss: 0.20015962421894073
train_iter_loss: 0.24926386773586273
train_iter_loss: 0.3312751352787018
train_iter_loss: 0.3995770812034607
train_iter_loss: 0.2509147524833679
train_iter_loss: 0.24464231729507446
train_iter_loss: 0.2402486503124237
train_iter_loss: 0.22624090313911438
train_iter_loss: 0.41248321533203125
train_iter_loss: 0.10347694903612137
train_iter_loss: 0.30220890045166016
train_iter_loss: 0.18013614416122437
train_iter_loss: 0.288399338722229
train_iter_loss: 0.3066489100456238
train_iter_loss: 0.16210457682609558
train_iter_loss: 0.3715403974056244
train_iter_loss: 0.2571113705635071
train_iter_loss: 0.34350502490997314
train_iter_loss: 0.3265707194805145
train_iter_loss: 0.3777677118778229
train_iter_loss: 0.16759319603443146
train_iter_loss: 0.20781855285167694
train_iter_loss: 0.208901509642601
train_iter_loss: 0.3870382010936737
train_iter_loss: 0.19574876129627228
train_iter_loss: 0.4203404188156128
train_iter_loss: 0.24136140942573547
train_iter_loss: 0.20295250415802002
train_iter_loss: 0.24704501032829285
train_iter_loss: 0.15155300498008728
train_iter_loss: 0.23798277974128723
train_iter_loss: 0.3919799327850342
train_iter_loss: 0.4003836214542389
train_iter_loss: 0.3683116137981415
train_iter_loss: 0.2652938663959503
train_iter_loss: 0.24775224924087524
train_iter_loss: 0.4741595983505249
train_iter_loss: 0.3141694664955139
train_iter_loss: 0.19421246647834778
train_iter_loss: 0.28289473056793213
train_iter_loss: 0.47818225622177124
train_iter_loss: 0.20152126252651215
train_iter_loss: 0.19220270216464996
train_iter_loss: 0.2755839228630066
train_iter_loss: 0.5175429582595825
train_iter_loss: 0.4961478114128113
train_iter_loss: 0.26067113876342773
train_iter_loss: 0.39598557353019714
train_iter_loss: 0.5331642031669617
train_iter_loss: 0.3312760591506958
train_iter_loss: 0.24367766082286835
train_iter_loss: 0.3298943340778351
train loss :0.2990
---------------------
Validation seg loss: 0.3638455136591252 at epoch 180
epoch =    181/  1000, exp = train
train_iter_loss: 0.3123891353607178
train_iter_loss: 0.31738731265068054
train_iter_loss: 0.309076189994812
train_iter_loss: 0.2803420424461365
train_iter_loss: 0.3517605662345886
train_iter_loss: 0.18603378534317017
train_iter_loss: 0.13290411233901978
train_iter_loss: 0.3338005542755127
train_iter_loss: 0.28922465443611145
train_iter_loss: 0.34019362926483154
train_iter_loss: 0.09127630293369293
train_iter_loss: 0.3562733232975006
train_iter_loss: 0.30586233735084534
train_iter_loss: 0.26448798179626465
train_iter_loss: 0.2843545973300934
train_iter_loss: 0.29943904280662537
train_iter_loss: 0.423216313123703
train_iter_loss: 0.2951851785182953
train_iter_loss: 0.28323090076446533
train_iter_loss: 0.3975616693496704
train_iter_loss: 0.1957489550113678
train_iter_loss: 0.3430934250354767
train_iter_loss: 0.36372339725494385
train_iter_loss: 0.31454557180404663
train_iter_loss: 0.23814180493354797
train_iter_loss: 0.3679268956184387
train_iter_loss: 0.2674904465675354
train_iter_loss: 0.32074445486068726
train_iter_loss: 0.2275109589099884
train_iter_loss: 0.3282524049282074
train_iter_loss: 0.18756049871444702
train_iter_loss: 0.1527174711227417
train_iter_loss: 0.24569185078144073
train_iter_loss: 0.17066334187984467
train_iter_loss: 0.35133180022239685
train_iter_loss: 0.30566293001174927
train_iter_loss: 0.43327954411506653
train_iter_loss: 0.25812768936157227
train_iter_loss: 0.4643661379814148
train_iter_loss: 0.3780718147754669
train_iter_loss: 0.31938600540161133
train_iter_loss: 0.42920979857444763
train_iter_loss: 0.15911825001239777
train_iter_loss: 0.2930200397968292
train_iter_loss: 0.2904598116874695
train_iter_loss: 0.37224823236465454
train_iter_loss: 0.25793635845184326
train_iter_loss: 0.21794381737709045
train_iter_loss: 0.22103659808635712
train_iter_loss: 0.195614293217659
train_iter_loss: 0.1377086043357849
train_iter_loss: 0.28987130522727966
train_iter_loss: 0.3328803777694702
train_iter_loss: 0.34888407588005066
train_iter_loss: 0.3079872131347656
train_iter_loss: 0.26820430159568787
train_iter_loss: 0.234136700630188
train_iter_loss: 0.33187541365623474
train_iter_loss: 0.26102009415626526
train_iter_loss: 0.30814042687416077
train_iter_loss: 0.21996502578258514
train_iter_loss: 0.2440478354692459
train_iter_loss: 0.35028553009033203
train_iter_loss: 0.42960426211357117
train_iter_loss: 0.2042136937379837
train_iter_loss: 0.33180296421051025
train_iter_loss: 0.31236961483955383
train_iter_loss: 0.3099454343318939
train_iter_loss: 0.49316704273223877
train_iter_loss: 0.25029730796813965
train_iter_loss: 0.3478725552558899
train_iter_loss: 0.4186144471168518
train_iter_loss: 0.20088542997837067
train_iter_loss: 0.2335910201072693
train_iter_loss: 0.3401027321815491
train_iter_loss: 0.3844224214553833
train_iter_loss: 0.2449486404657364
train_iter_loss: 0.2994391918182373
train_iter_loss: 0.16446031630039215
train_iter_loss: 0.2787102162837982
train_iter_loss: 0.30827954411506653
train_iter_loss: 0.2646254301071167
train_iter_loss: 0.2336689680814743
train_iter_loss: 0.36274057626724243
train_iter_loss: 0.2715698778629303
train_iter_loss: 0.3354716897010803
train_iter_loss: 0.3371216058731079
train_iter_loss: 0.1536789983510971
train_iter_loss: 0.3651700019836426
train_iter_loss: 0.21314792335033417
train_iter_loss: 0.20379815995693207
train_iter_loss: 0.31007522344589233
train_iter_loss: 0.19463451206684113
train_iter_loss: 0.40551814436912537
train_iter_loss: 0.4563518464565277
train_iter_loss: 0.50881028175354
train_iter_loss: 0.32660672068595886
train_iter_loss: 0.2698934078216553
train_iter_loss: 0.1311497539281845
train_iter_loss: 0.17998044192790985
train loss :0.2954
---------------------
Validation seg loss: 0.36874492557824784 at epoch 181
epoch =    182/  1000, exp = train
train_iter_loss: 0.26552245020866394
train_iter_loss: 0.2460414171218872
train_iter_loss: 0.26631951332092285
train_iter_loss: 0.2963229715824127
train_iter_loss: 0.23732565343379974
train_iter_loss: 0.4470609128475189
train_iter_loss: 0.38394105434417725
train_iter_loss: 0.2848067581653595
train_iter_loss: 0.31900081038475037
train_iter_loss: 0.3420270085334778
train_iter_loss: 0.17744800448417664
train_iter_loss: 0.1957101970911026
train_iter_loss: 0.37367433309555054
train_iter_loss: 0.2920031249523163
train_iter_loss: 0.3420359492301941
train_iter_loss: 0.37599995732307434
train_iter_loss: 0.2659749388694763
train_iter_loss: 0.36486509442329407
train_iter_loss: 0.3081468939781189
train_iter_loss: 0.2948531210422516
train_iter_loss: 0.2555035352706909
train_iter_loss: 0.25272172689437866
train_iter_loss: 0.2541356086730957
train_iter_loss: 0.3301220238208771
train_iter_loss: 0.17231109738349915
train_iter_loss: 0.3730948269367218
train_iter_loss: 0.1609695702791214
train_iter_loss: 0.2363417148590088
train_iter_loss: 0.255340039730072
train_iter_loss: 0.2213776409626007
train_iter_loss: 0.18214748799800873
train_iter_loss: 0.3782016634941101
train_iter_loss: 0.32390275597572327
train_iter_loss: 0.2554508149623871
train_iter_loss: 0.1560700535774231
train_iter_loss: 0.2061150223016739
train_iter_loss: 0.2872544825077057
train_iter_loss: 0.30513718724250793
train_iter_loss: 0.21921564638614655
train_iter_loss: 0.421347975730896
train_iter_loss: 0.4134122133255005
train_iter_loss: 0.19066743552684784
train_iter_loss: 0.24801655113697052
train_iter_loss: 0.13758039474487305
train_iter_loss: 0.4155808985233307
train_iter_loss: 0.2816898822784424
train_iter_loss: 0.5662720203399658
train_iter_loss: 0.44324806332588196
train_iter_loss: 0.2615501284599304
train_iter_loss: 0.3439611494541168
train_iter_loss: 0.2872467637062073
train_iter_loss: 0.19557997584342957
train_iter_loss: 0.2588382959365845
train_iter_loss: 0.3124859929084778
train_iter_loss: 0.49525684118270874
train_iter_loss: 0.2727588713169098
train_iter_loss: 0.25257065892219543
train_iter_loss: 0.3595309555530548
train_iter_loss: 0.4353274405002594
train_iter_loss: 0.49570968747138977
train_iter_loss: 0.2935965061187744
train_iter_loss: 0.22690926492214203
train_iter_loss: 0.16358304023742676
train_iter_loss: 0.21854105591773987
train_iter_loss: 0.3157634437084198
train_iter_loss: 0.20430268347263336
train_iter_loss: 0.2677769660949707
train_iter_loss: 0.4117322266101837
train_iter_loss: 0.31502240896224976
train_iter_loss: 0.17611809074878693
train_iter_loss: 0.2778715193271637
train_iter_loss: 0.2759009599685669
train_iter_loss: 0.29869091510772705
train_iter_loss: 0.27717360854148865
train_iter_loss: 0.25337785482406616
train_iter_loss: 0.33446502685546875
train_iter_loss: 0.3056490123271942
train_iter_loss: 0.27905142307281494
train_iter_loss: 0.28623247146606445
train_iter_loss: 0.1680395007133484
train_iter_loss: 0.2687535881996155
train_iter_loss: 0.17912372946739197
train_iter_loss: 0.20035909116268158
train_iter_loss: 0.3684356212615967
train_iter_loss: 0.15892739593982697
train_iter_loss: 0.2808292508125305
train_iter_loss: 0.18997669219970703
train_iter_loss: 0.3720007538795471
train_iter_loss: 0.3765713572502136
train_iter_loss: 0.2903893291950226
train_iter_loss: 0.34315821528434753
train_iter_loss: 0.4160597324371338
train_iter_loss: 0.31403669714927673
train_iter_loss: 0.21650288999080658
train_iter_loss: 0.4018278419971466
train_iter_loss: 0.13023525476455688
train_iter_loss: 0.35745659470558167
train_iter_loss: 0.2543366551399231
train_iter_loss: 0.34533149003982544
train_iter_loss: 0.2000722587108612
train loss :0.2930
---------------------
Validation seg loss: 0.3686315032987381 at epoch 182
epoch =    183/  1000, exp = train
train_iter_loss: 0.24235132336616516
train_iter_loss: 0.3027229607105255
train_iter_loss: 0.1889214664697647
train_iter_loss: 0.31469646096229553
train_iter_loss: 0.5737053155899048
train_iter_loss: 0.35071030259132385
train_iter_loss: 0.32363349199295044
train_iter_loss: 0.294156014919281
train_iter_loss: 0.30903497338294983
train_iter_loss: 0.3048206865787506
train_iter_loss: 0.4044960141181946
train_iter_loss: 0.17687606811523438
train_iter_loss: 0.26989153027534485
train_iter_loss: 0.22696040570735931
train_iter_loss: 0.2410479038953781
train_iter_loss: 0.415833443403244
train_iter_loss: 0.26656273007392883
train_iter_loss: 0.14313817024230957
train_iter_loss: 0.17207737267017365
train_iter_loss: 0.2602948546409607
train_iter_loss: 0.2290552258491516
train_iter_loss: 0.28802093863487244
train_iter_loss: 0.23596297204494476
train_iter_loss: 0.18571142852306366
train_iter_loss: 0.28940582275390625
train_iter_loss: 0.13912895321846008
train_iter_loss: 0.44842270016670227
train_iter_loss: 0.19397445023059845
train_iter_loss: 0.2821684777736664
train_iter_loss: 0.2140139937400818
train_iter_loss: 0.25917544960975647
train_iter_loss: 0.3234085142612457
train_iter_loss: 0.4233807623386383
train_iter_loss: 0.31944185495376587
train_iter_loss: 0.26163214445114136
train_iter_loss: 0.32727017998695374
train_iter_loss: 0.16486231982707977
train_iter_loss: 0.22699713706970215
train_iter_loss: 0.3274056911468506
train_iter_loss: 0.26601505279541016
train_iter_loss: 0.18258647620677948
train_iter_loss: 0.2582169473171234
train_iter_loss: 0.3054378032684326
train_iter_loss: 0.2115096002817154
train_iter_loss: 0.283251017332077
train_iter_loss: 0.240122988820076
train_iter_loss: 0.25499165058135986
train_iter_loss: 0.4145696759223938
train_iter_loss: 0.2616419196128845
train_iter_loss: 0.4039190113544464
train_iter_loss: 0.24428391456604004
train_iter_loss: 0.29894590377807617
train_iter_loss: 0.34795916080474854
train_iter_loss: 0.3590008318424225
train_iter_loss: 0.31480497121810913
train_iter_loss: 0.16093318164348602
train_iter_loss: 0.4228431284427643
train_iter_loss: 0.34383007884025574
train_iter_loss: 0.36146917939186096
train_iter_loss: 0.23066486418247223
train_iter_loss: 0.32621482014656067
train_iter_loss: 0.2231520712375641
train_iter_loss: 0.28335684537887573
train_iter_loss: 0.5955493450164795
train_iter_loss: 0.33746692538261414
train_iter_loss: 0.2501862347126007
train_iter_loss: 0.23731371760368347
train_iter_loss: 0.18479275703430176
train_iter_loss: 0.30655959248542786
train_iter_loss: 0.3139917850494385
train_iter_loss: 0.24810440838336945
train_iter_loss: 0.26028358936309814
train_iter_loss: 0.29610252380371094
train_iter_loss: 0.28683310747146606
train_iter_loss: 0.3051832914352417
train_iter_loss: 0.1930224746465683
train_iter_loss: 0.15726859867572784
train_iter_loss: 0.30397531390190125
train_iter_loss: 0.25106582045555115
train_iter_loss: 0.2807866632938385
train_iter_loss: 0.4292483925819397
train_iter_loss: 0.30828139185905457
train_iter_loss: 0.22237235307693481
train_iter_loss: 0.3424689471721649
train_iter_loss: 0.3954487144947052
train_iter_loss: 0.22966323792934418
train_iter_loss: 0.32012414932250977
train_iter_loss: 0.16736504435539246
train_iter_loss: 0.2631644308567047
train_iter_loss: 0.2986726760864258
train_iter_loss: 0.3014439046382904
train_iter_loss: 0.2946600019931793
train_iter_loss: 0.3820791244506836
train_iter_loss: 0.3617539703845978
train_iter_loss: 0.2539201080799103
train_iter_loss: 0.15431106090545654
train_iter_loss: 0.33917900919914246
train_iter_loss: 0.49734044075012207
train_iter_loss: 0.3638198673725128
train_iter_loss: 0.30215537548065186
train loss :0.2925
---------------------
Validation seg loss: 0.37614357798307574 at epoch 183
epoch =    184/  1000, exp = train
train_iter_loss: 0.20073001086711884
train_iter_loss: 0.214303657412529
train_iter_loss: 0.3227003812789917
train_iter_loss: 0.29334160685539246
train_iter_loss: 0.33961665630340576
train_iter_loss: 0.2846391201019287
train_iter_loss: 0.3103146255016327
train_iter_loss: 0.3314402401447296
train_iter_loss: 0.2628277838230133
train_iter_loss: 0.3387824594974518
train_iter_loss: 0.23843839764595032
train_iter_loss: 0.2129039168357849
train_iter_loss: 0.410102903842926
train_iter_loss: 0.20201332867145538
train_iter_loss: 0.26382139325141907
train_iter_loss: 0.4519493877887726
train_iter_loss: 0.20187708735466003
train_iter_loss: 0.2820573151111603
train_iter_loss: 0.2502095699310303
train_iter_loss: 0.34368830919265747
train_iter_loss: 0.3099459111690521
train_iter_loss: 0.30710920691490173
train_iter_loss: 0.42282235622406006
train_iter_loss: 0.18187613785266876
train_iter_loss: 0.21275536715984344
train_iter_loss: 0.30406904220581055
train_iter_loss: 0.3228326737880707
train_iter_loss: 0.32810983061790466
train_iter_loss: 0.20810648798942566
train_iter_loss: 0.1708420366048813
train_iter_loss: 0.1617412120103836
train_iter_loss: 0.39282703399658203
train_iter_loss: 0.18271513283252716
train_iter_loss: 0.3714199662208557
train_iter_loss: 0.2715831398963928
train_iter_loss: 0.16678203642368317
train_iter_loss: 0.23206739127635956
train_iter_loss: 0.2989024817943573
train_iter_loss: 0.30099552869796753
train_iter_loss: 0.22430571913719177
train_iter_loss: 0.19138851761817932
train_iter_loss: 0.3722829222679138
train_iter_loss: 0.4180920124053955
train_iter_loss: 0.344176709651947
train_iter_loss: 0.22202502191066742
train_iter_loss: 0.298410028219223
train_iter_loss: 0.1489468216896057
train_iter_loss: 0.2397695779800415
train_iter_loss: 0.24677269160747528
train_iter_loss: 0.2705916464328766
train_iter_loss: 0.403117299079895
train_iter_loss: 0.34588417410850525
train_iter_loss: 0.33698591589927673
train_iter_loss: 0.26955097913742065
train_iter_loss: 0.10463155061006546
train_iter_loss: 0.41105416417121887
train_iter_loss: 0.21251867711544037
train_iter_loss: 0.34520450234413147
train_iter_loss: 0.33998894691467285
train_iter_loss: 0.29452797770500183
train_iter_loss: 0.3359401524066925
train_iter_loss: 0.27499109506607056
train_iter_loss: 0.2239166647195816
train_iter_loss: 0.34657159447669983
train_iter_loss: 0.21525557339191437
train_iter_loss: 0.18168237805366516
train_iter_loss: 0.3458585739135742
train_iter_loss: 0.3030626177787781
train_iter_loss: 0.22792358696460724
train_iter_loss: 0.3208337426185608
train_iter_loss: 0.38929522037506104
train_iter_loss: 0.46230611205101013
train_iter_loss: 0.38080015778541565
train_iter_loss: 0.22279895842075348
train_iter_loss: 0.2848757803440094
train_iter_loss: 0.3495054543018341
train_iter_loss: 0.28476837277412415
train_iter_loss: 0.3270397484302521
train_iter_loss: 0.3263428211212158
train_iter_loss: 0.23302538692951202
train_iter_loss: 0.3414927124977112
train_iter_loss: 0.37162140011787415
train_iter_loss: 0.31721439957618713
train_iter_loss: 0.22973059117794037
train_iter_loss: 0.2535160779953003
train_iter_loss: 0.3968622386455536
train_iter_loss: 0.2839289903640747
train_iter_loss: 0.2329433113336563
train_iter_loss: 0.3179323077201843
train_iter_loss: 0.1477971225976944
train_iter_loss: 0.15331049263477325
train_iter_loss: 0.4122002422809601
train_iter_loss: 0.2535340487957001
train_iter_loss: 0.16547928750514984
train_iter_loss: 0.2836921513080597
train_iter_loss: 0.3048191964626312
train_iter_loss: 0.3351329565048218
train_iter_loss: 0.21008695662021637
train_iter_loss: 0.24674032628536224
train_iter_loss: 0.2760379910469055
train loss :0.2881
---------------------
Validation seg loss: 0.3759962465122061 at epoch 184
epoch =    185/  1000, exp = train
train_iter_loss: 0.3555808663368225
train_iter_loss: 0.3278130888938904
train_iter_loss: 0.36438900232315063
train_iter_loss: 0.4170548617839813
train_iter_loss: 0.3121984302997589
train_iter_loss: 0.2918151319026947
train_iter_loss: 0.296347051858902
train_iter_loss: 0.2523813247680664
train_iter_loss: 0.27768462896347046
train_iter_loss: 0.2822164297103882
train_iter_loss: 0.32679057121276855
train_iter_loss: 0.17585943639278412
train_iter_loss: 0.2630656659603119
train_iter_loss: 0.2798646092414856
train_iter_loss: 0.3179796040058136
train_iter_loss: 0.23778383433818817
train_iter_loss: 0.3557775020599365
train_iter_loss: 0.2304692268371582
train_iter_loss: 0.1615028828382492
train_iter_loss: 0.3421742618083954
train_iter_loss: 0.3058103322982788
train_iter_loss: 0.13285255432128906
train_iter_loss: 0.23352524638175964
train_iter_loss: 0.23630468547344208
train_iter_loss: 0.23482635617256165
train_iter_loss: 0.19515152275562286
train_iter_loss: 0.2290857881307602
train_iter_loss: 0.23888330161571503
train_iter_loss: 0.2668856978416443
train_iter_loss: 0.2868253290653229
train_iter_loss: 0.2896507680416107
train_iter_loss: 0.17185251414775848
train_iter_loss: 0.3019462525844574
train_iter_loss: 0.2687121629714966
train_iter_loss: 0.1665624976158142
train_iter_loss: 0.2784370183944702
train_iter_loss: 0.23324128985404968
train_iter_loss: 0.5902313590049744
train_iter_loss: 0.2566942274570465
train_iter_loss: 0.3582574725151062
train_iter_loss: 0.23757699131965637
train_iter_loss: 0.293607622385025
train_iter_loss: 0.372053861618042
train_iter_loss: 0.40517082810401917
train_iter_loss: 0.18706828355789185
train_iter_loss: 0.27551519870758057
train_iter_loss: 0.4229351580142975
train_iter_loss: 0.31025832891464233
train_iter_loss: 0.40026700496673584
train_iter_loss: 0.445995032787323
train_iter_loss: 0.3138850927352905
train_iter_loss: 0.26173532009124756
train_iter_loss: 0.22825947403907776
train_iter_loss: 0.21826571226119995
train_iter_loss: 0.27611061930656433
train_iter_loss: 0.1400548368692398
train_iter_loss: 0.2994935214519501
train_iter_loss: 0.32882043719291687
train_iter_loss: 0.3107297122478485
train_iter_loss: 0.3063892126083374
train_iter_loss: 0.15406301617622375
train_iter_loss: 0.3304826021194458
train_iter_loss: 0.36540713906288147
train_iter_loss: 0.30946239829063416
train_iter_loss: 0.48662281036376953
train_iter_loss: 0.3142610490322113
train_iter_loss: 0.24951709806919098
train_iter_loss: 0.34358325600624084
train_iter_loss: 0.31846824288368225
train_iter_loss: 0.30410459637641907
train_iter_loss: 0.13987228274345398
train_iter_loss: 0.23441535234451294
train_iter_loss: 0.1767912060022354
train_iter_loss: 0.2600860297679901
train_iter_loss: 0.19923050701618195
train_iter_loss: 0.33657094836235046
train_iter_loss: 0.2752588391304016
train_iter_loss: 0.21192456781864166
train_iter_loss: 0.27212095260620117
train_iter_loss: 0.4101884663105011
train_iter_loss: 0.21519142389297485
train_iter_loss: 0.30818164348602295
train_iter_loss: 0.25650233030319214
train_iter_loss: 0.33825182914733887
train_iter_loss: 0.32040083408355713
train_iter_loss: 0.1845981627702713
train_iter_loss: 0.16240040957927704
train_iter_loss: 0.24217703938484192
train_iter_loss: 0.20635303854942322
train_iter_loss: 0.2800944149494171
train_iter_loss: 0.6256006956100464
train_iter_loss: 0.48660916090011597
train_iter_loss: 0.37673747539520264
train_iter_loss: 0.3530528247356415
train_iter_loss: 0.2696976661682129
train_iter_loss: 0.16278743743896484
train_iter_loss: 0.22219030559062958
train_iter_loss: 0.21269655227661133
train_iter_loss: 0.3605610132217407
train_iter_loss: 0.309209942817688
train loss :0.2905
---------------------
Validation seg loss: 0.37046090269215265 at epoch 185
epoch =    186/  1000, exp = train
train_iter_loss: 0.29295966029167175
train_iter_loss: 0.278583288192749
train_iter_loss: 0.33850011229515076
train_iter_loss: 0.29008591175079346
train_iter_loss: 0.48110508918762207
train_iter_loss: 0.2308932989835739
train_iter_loss: 0.26088622212409973
train_iter_loss: 0.23771093785762787
train_iter_loss: 0.23774167895317078
train_iter_loss: 0.3114800751209259
train_iter_loss: 0.33698052167892456
train_iter_loss: 0.29716020822525024
train_iter_loss: 0.21012194454669952
train_iter_loss: 0.5120940208435059
train_iter_loss: 0.271095871925354
train_iter_loss: 0.4142731726169586
train_iter_loss: 0.20956334471702576
train_iter_loss: 0.218715637922287
train_iter_loss: 0.40040549635887146
train_iter_loss: 0.23498134315013885
train_iter_loss: 0.19149605929851532
train_iter_loss: 0.38128846883773804
train_iter_loss: 0.2921343147754669
train_iter_loss: 0.3082588016986847
train_iter_loss: 0.46359995007514954
train_iter_loss: 0.3069375157356262
train_iter_loss: 0.2674274146556854
train_iter_loss: 0.3535086512565613
train_iter_loss: 0.3454399108886719
train_iter_loss: 0.3368149697780609
train_iter_loss: 0.20660002529621124
train_iter_loss: 0.12219245731830597
train_iter_loss: 0.22061671316623688
train_iter_loss: 0.49317270517349243
train_iter_loss: 0.20127315819263458
train_iter_loss: 0.33033281564712524
train_iter_loss: 0.18728986382484436
train_iter_loss: 0.22942869365215302
train_iter_loss: 0.15584631264209747
train_iter_loss: 0.19571086764335632
train_iter_loss: 0.2294027954339981
train_iter_loss: 0.3367733657360077
train_iter_loss: 0.19887106120586395
train_iter_loss: 0.27353399991989136
train_iter_loss: 0.4534148573875427
train_iter_loss: 0.3671018183231354
train_iter_loss: 0.1791163682937622
train_iter_loss: 0.324215829372406
train_iter_loss: 0.39652299880981445
train_iter_loss: 0.21581925451755524
train_iter_loss: 0.4449424147605896
train_iter_loss: 0.35176005959510803
train_iter_loss: 0.2320791333913803
train_iter_loss: 0.20503798127174377
train_iter_loss: 0.27616381645202637
train_iter_loss: 0.2869425117969513
train_iter_loss: 0.39739713072776794
train_iter_loss: 0.3960830271244049
train_iter_loss: 0.2903927266597748
train_iter_loss: 0.3074287474155426
train_iter_loss: 0.2186492532491684
train_iter_loss: 0.4158993661403656
train_iter_loss: 0.30094924569129944
train_iter_loss: 0.2463829517364502
train_iter_loss: 0.2799733877182007
train_iter_loss: 0.5950071811676025
train_iter_loss: 0.28345808386802673
train_iter_loss: 0.20648407936096191
train_iter_loss: 0.3630524277687073
train_iter_loss: 0.331455260515213
train_iter_loss: 0.2787202000617981
train_iter_loss: 0.24826721847057343
train_iter_loss: 0.2176034152507782
train_iter_loss: 0.22500529885292053
train_iter_loss: 0.24180932343006134
train_iter_loss: 0.13893790543079376
train_iter_loss: 0.23913034796714783
train_iter_loss: 0.31488943099975586
train_iter_loss: 0.19996775686740875
train_iter_loss: 0.4641014337539673
train_iter_loss: 0.26549604535102844
train_iter_loss: 0.32961899042129517
train_iter_loss: 0.16917404532432556
train_iter_loss: 0.2513270676136017
train_iter_loss: 0.38249701261520386
train_iter_loss: 0.16456569731235504
train_iter_loss: 0.542009174823761
train_iter_loss: 0.21970808506011963
train_iter_loss: 0.2086302489042282
train_iter_loss: 0.3559974133968353
train_iter_loss: 0.25373902916908264
train_iter_loss: 0.1404913365840912
train_iter_loss: 0.24964800477027893
train_iter_loss: 0.4061693847179413
train_iter_loss: 0.3237835168838501
train_iter_loss: 0.3807375431060791
train_iter_loss: 0.3186485767364502
train_iter_loss: 0.3521691858768463
train_iter_loss: 0.3517186641693115
train_iter_loss: 0.27888861298561096
train loss :0.2986
---------------------
Validation seg loss: 0.3792509218185859 at epoch 186
epoch =    187/  1000, exp = train
train_iter_loss: 0.20964491367340088
train_iter_loss: 0.20515625178813934
train_iter_loss: 0.29152950644493103
train_iter_loss: 0.5822679996490479
train_iter_loss: 0.2589617669582367
train_iter_loss: 0.2177359014749527
train_iter_loss: 0.4648129343986511
train_iter_loss: 0.30592358112335205
train_iter_loss: 0.18243035674095154
train_iter_loss: 0.25177672505378723
train_iter_loss: 0.45369797945022583
train_iter_loss: 0.2031262218952179
train_iter_loss: 0.2616802453994751
train_iter_loss: 0.3151729106903076
train_iter_loss: 0.2281143218278885
train_iter_loss: 0.1713750958442688
train_iter_loss: 0.18992751836776733
train_iter_loss: 0.21372418105602264
train_iter_loss: 0.19508622586727142
train_iter_loss: 0.30398836731910706
train_iter_loss: 0.3118179142475128
train_iter_loss: 0.22268392145633698
train_iter_loss: 0.24410468339920044
train_iter_loss: 0.15961652994155884
train_iter_loss: 0.2983946204185486
train_iter_loss: 0.17769017815589905
train_iter_loss: 0.402544230222702
train_iter_loss: 0.22351188957691193
train_iter_loss: 0.2522161602973938
train_iter_loss: 0.35886529088020325
train_iter_loss: 0.47790127992630005
train_iter_loss: 0.3811664879322052
train_iter_loss: 0.29293134808540344
train_iter_loss: 0.30793485045433044
train_iter_loss: 0.30447086691856384
train_iter_loss: 0.2862952947616577
train_iter_loss: 0.2535754144191742
train_iter_loss: 0.43575674295425415
train_iter_loss: 0.3105051815509796
train_iter_loss: 0.3629121780395508
train_iter_loss: 0.25437840819358826
train_iter_loss: 0.25201719999313354
train_iter_loss: 0.287171334028244
train_iter_loss: 0.2513813078403473
train_iter_loss: 0.29580098390579224
train_iter_loss: 0.21218441426753998
train_iter_loss: 0.28091251850128174
train_iter_loss: 0.2127656638622284
train_iter_loss: 0.3919202983379364
train_iter_loss: 0.20502901077270508
train_iter_loss: 0.27382057905197144
train_iter_loss: 0.22158493101596832
train_iter_loss: 0.43159157037734985
train_iter_loss: 0.7021593451499939
train_iter_loss: 0.37995755672454834
train_iter_loss: 0.5343307256698608
train_iter_loss: 0.2531100809574127
train_iter_loss: 0.24487081170082092
train_iter_loss: 0.17692628502845764
train_iter_loss: 0.28503847122192383
train_iter_loss: 0.1797608584165573
train_iter_loss: 0.2728661596775055
train_iter_loss: 0.24532894790172577
train_iter_loss: 0.07102037966251373
train_iter_loss: 0.3555658161640167
train_iter_loss: 0.3229772448539734
train_iter_loss: 0.1773419976234436
train_iter_loss: 0.22054129838943481
train_iter_loss: 0.3969542384147644
train_iter_loss: 0.22645772993564606
train_iter_loss: 0.34373724460601807
train_iter_loss: 0.3211559057235718
train_iter_loss: 0.37260207533836365
train_iter_loss: 0.2087414711713791
train_iter_loss: 0.27807626128196716
train_iter_loss: 0.24568817019462585
train_iter_loss: 0.14274632930755615
train_iter_loss: 0.5209140181541443
train_iter_loss: 0.2754569947719574
train_iter_loss: 0.38028502464294434
train_iter_loss: 0.4739743769168854
train_iter_loss: 0.19849303364753723
train_iter_loss: 0.3495098948478699
train_iter_loss: 0.29426294565200806
train_iter_loss: 0.342744380235672
train_iter_loss: 0.2806060016155243
train_iter_loss: 0.4265975058078766
train_iter_loss: 0.2438012957572937
train_iter_loss: 0.38895514607429504
train_iter_loss: 0.3856388330459595
train_iter_loss: 0.26811620593070984
train_iter_loss: 0.31754907965660095
train_iter_loss: 0.3133071959018707
train_iter_loss: 0.3418750762939453
train_iter_loss: 0.3574512004852295
train_iter_loss: 0.4413672983646393
train_iter_loss: 0.3392249345779419
train_iter_loss: 0.19810031354427338
train_iter_loss: 0.3193954825401306
train_iter_loss: 0.21965336799621582
train loss :0.3008
---------------------
Validation seg loss: 0.3761284045945361 at epoch 187
epoch =    188/  1000, exp = train
train_iter_loss: 0.26665496826171875
train_iter_loss: 0.13718795776367188
train_iter_loss: 0.2054724544286728
train_iter_loss: 0.2832988500595093
train_iter_loss: 0.403751015663147
train_iter_loss: 0.3288152515888214
train_iter_loss: 0.3809683620929718
train_iter_loss: 0.30102038383483887
train_iter_loss: 0.2983899414539337
train_iter_loss: 0.30135631561279297
train_iter_loss: 0.27977341413497925
train_iter_loss: 0.31092965602874756
train_iter_loss: 0.4361063241958618
train_iter_loss: 0.2776341140270233
train_iter_loss: 0.43247297406196594
train_iter_loss: 0.26474690437316895
train_iter_loss: 0.2825762927532196
train_iter_loss: 0.2913142442703247
train_iter_loss: 0.2446141242980957
train_iter_loss: 0.34968575835227966
train_iter_loss: 0.14586485922336578
train_iter_loss: 0.3299404978752136
train_iter_loss: 0.2067587971687317
train_iter_loss: 0.32106903195381165
train_iter_loss: 0.25799959897994995
train_iter_loss: 0.34508904814720154
train_iter_loss: 0.19687515497207642
train_iter_loss: 0.2900819480419159
train_iter_loss: 0.2675412893295288
train_iter_loss: 0.2316002994775772
train_iter_loss: 0.22572466731071472
train_iter_loss: 0.16513465344905853
train_iter_loss: 0.28278517723083496
train_iter_loss: 0.27620360255241394
train_iter_loss: 0.24988749623298645
train_iter_loss: 0.36165329813957214
train_iter_loss: 0.272518128156662
train_iter_loss: 0.21407350897789001
train_iter_loss: 0.22913888096809387
train_iter_loss: 0.3467300236225128
train_iter_loss: 0.22583356499671936
train_iter_loss: 0.181021586060524
train_iter_loss: 0.5053236484527588
train_iter_loss: 0.2789757549762726
train_iter_loss: 0.3550894856452942
train_iter_loss: 0.25364959239959717
train_iter_loss: 0.22490499913692474
train_iter_loss: 0.22549119591712952
train_iter_loss: 0.14708518981933594
train_iter_loss: 0.30568927526474
train_iter_loss: 0.3278685212135315
train_iter_loss: 0.18660683929920197
train_iter_loss: 0.4113725423812866
train_iter_loss: 0.20891547203063965
train_iter_loss: 0.3211079239845276
train_iter_loss: 0.24604114890098572
train_iter_loss: 0.25764474272727966
train_iter_loss: 0.40005064010620117
train_iter_loss: 0.5177350640296936
train_iter_loss: 0.25683510303497314
train_iter_loss: 0.38031429052352905
train_iter_loss: 0.3034207224845886
train_iter_loss: 0.10530202835798264
train_iter_loss: 0.31420767307281494
train_iter_loss: 0.16753749549388885
train_iter_loss: 0.21537604928016663
train_iter_loss: 0.23635753989219666
train_iter_loss: 0.42706984281539917
train_iter_loss: 0.17701302468776703
train_iter_loss: 0.35487908124923706
train_iter_loss: 0.267790287733078
train_iter_loss: 0.30699968338012695
train_iter_loss: 0.2483890801668167
train_iter_loss: 0.27472418546676636
train_iter_loss: 0.368876576423645
train_iter_loss: 0.33882248401641846
train_iter_loss: 0.26348164677619934
train_iter_loss: 0.2118474245071411
train_iter_loss: 0.22993655502796173
train_iter_loss: 0.33026963472366333
train_iter_loss: 0.32963699102401733
train_iter_loss: 0.3944055736064911
train_iter_loss: 0.21137791872024536
train_iter_loss: 0.31173762679100037
train_iter_loss: 0.3600609600543976
train_iter_loss: 0.2577141225337982
train_iter_loss: 0.1828622967004776
train_iter_loss: 0.32710686326026917
train_iter_loss: 0.3093494474887848
train_iter_loss: 0.31904304027557373
train_iter_loss: 0.36798107624053955
train_iter_loss: 0.2373470962047577
train_iter_loss: 0.31344369053840637
train_iter_loss: 0.22963383793830872
train_iter_loss: 0.367501437664032
train_iter_loss: 0.3724629878997803
train_iter_loss: 0.36663153767585754
train_iter_loss: 0.17234507203102112
train_iter_loss: 0.29977330565452576
train_iter_loss: 0.39715489745140076
train loss :0.2908
---------------------
Validation seg loss: 0.3652277384480497 at epoch 188
epoch =    189/  1000, exp = train
train_iter_loss: 0.2587086856365204
train_iter_loss: 0.20807167887687683
train_iter_loss: 0.2831783592700958
train_iter_loss: 0.24218802154064178
train_iter_loss: 0.27050113677978516
train_iter_loss: 0.3057629466056824
train_iter_loss: 0.08619564026594162
train_iter_loss: 0.345560222864151
train_iter_loss: 0.3556501865386963
train_iter_loss: 0.30832988023757935
train_iter_loss: 0.37644174695014954
train_iter_loss: 0.3806094229221344
train_iter_loss: 0.3612949550151825
train_iter_loss: 0.20491696894168854
train_iter_loss: 0.13675431907176971
train_iter_loss: 0.2463100105524063
train_iter_loss: 0.175676628947258
train_iter_loss: 0.17563286423683167
train_iter_loss: 0.22179090976715088
train_iter_loss: 0.28251996636390686
train_iter_loss: 0.24037671089172363
train_iter_loss: 0.1103614866733551
train_iter_loss: 0.20334608852863312
train_iter_loss: 0.3594750463962555
train_iter_loss: 0.35621386766433716
train_iter_loss: 0.36058181524276733
train_iter_loss: 0.25375428795814514
train_iter_loss: 0.3400135040283203
train_iter_loss: 0.30968379974365234
train_iter_loss: 0.43826255202293396
train_iter_loss: 0.20049139857292175
train_iter_loss: 0.23422318696975708
train_iter_loss: 0.2706308960914612
train_iter_loss: 0.22757166624069214
train_iter_loss: 0.2796502411365509
train_iter_loss: 0.16918052732944489
train_iter_loss: 0.16657812893390656
train_iter_loss: 0.22049251198768616
train_iter_loss: 0.6134347319602966
train_iter_loss: 0.2150195837020874
train_iter_loss: 0.32121968269348145
train_iter_loss: 0.42727914452552795
train_iter_loss: 0.27757924795150757
train_iter_loss: 0.23003938794136047
train_iter_loss: 0.2778070271015167
train_iter_loss: 0.3131427466869354
train_iter_loss: 0.4474317133426666
train_iter_loss: 0.33610081672668457
train_iter_loss: 0.2658146023750305
train_iter_loss: 0.37321820855140686
train_iter_loss: 0.243102565407753
train_iter_loss: 0.3394581377506256
train_iter_loss: 0.25297975540161133
train_iter_loss: 0.2848009765148163
train_iter_loss: 0.3650938868522644
train_iter_loss: 0.4236985743045807
train_iter_loss: 0.23788359761238098
train_iter_loss: 0.27232131361961365
train_iter_loss: 0.2684195637702942
train_iter_loss: 0.304862916469574
train_iter_loss: 0.37330275774002075
train_iter_loss: 0.3521888852119446
train_iter_loss: 0.2686580717563629
train_iter_loss: 0.36304202675819397
train_iter_loss: 0.3016222417354584
train_iter_loss: 0.3538731336593628
train_iter_loss: 0.30189940333366394
train_iter_loss: 0.37132760882377625
train_iter_loss: 0.22252339124679565
train_iter_loss: 0.2461833506822586
train_iter_loss: 0.21454480290412903
train_iter_loss: 0.1904243677854538
train_iter_loss: 0.3297936022281647
train_iter_loss: 0.27945274114608765
train_iter_loss: 0.19082841277122498
train_iter_loss: 0.289542555809021
train_iter_loss: 0.37830570340156555
train_iter_loss: 0.2003614604473114
train_iter_loss: 0.2875988483428955
train_iter_loss: 0.4210023581981659
train_iter_loss: 0.3142207860946655
train_iter_loss: 0.2671166658401489
train_iter_loss: 0.28640392422676086
train_iter_loss: 0.2300662249326706
train_iter_loss: 0.26888853311538696
train_iter_loss: 0.3012325167655945
train_iter_loss: 0.2184527963399887
train_iter_loss: 0.3792686462402344
train_iter_loss: 0.27014294266700745
train_iter_loss: 0.2633959650993347
train_iter_loss: 0.3289586901664734
train_iter_loss: 0.5236257910728455
train_iter_loss: 0.21281050145626068
train_iter_loss: 0.3312954008579254
train_iter_loss: 0.23489052057266235
train_iter_loss: 0.3469601273536682
train_iter_loss: 0.3446546792984009
train_iter_loss: 0.349861204624176
train_iter_loss: 0.16961832344532013
train_iter_loss: 0.11950420588254929
train loss :0.2905
---------------------
Validation seg loss: 0.38143771686503347 at epoch 189
epoch =    190/  1000, exp = train
train_iter_loss: 0.45802825689315796
train_iter_loss: 0.2944583296775818
train_iter_loss: 0.2542731463909149
train_iter_loss: 0.1438024491071701
train_iter_loss: 0.45597076416015625
train_iter_loss: 0.4430542588233948
train_iter_loss: 0.2473582923412323
train_iter_loss: 0.28786614537239075
train_iter_loss: 0.34705740213394165
train_iter_loss: 0.26258552074432373
train_iter_loss: 0.36935776472091675
train_iter_loss: 0.2569781541824341
train_iter_loss: 0.31066903471946716
train_iter_loss: 0.6475182771682739
train_iter_loss: 0.2116457223892212
train_iter_loss: 0.29288896918296814
train_iter_loss: 0.15709389746189117
train_iter_loss: 0.3605516254901886
train_iter_loss: 0.18868762254714966
train_iter_loss: 0.3253469169139862
train_iter_loss: 0.20021717250347137
train_iter_loss: 0.18047276139259338
train_iter_loss: 0.2584346830844879
train_iter_loss: 0.2780415415763855
train_iter_loss: 0.42116519808769226
train_iter_loss: 0.3390297591686249
train_iter_loss: 0.26224568486213684
train_iter_loss: 0.2576293647289276
train_iter_loss: 0.31230178475379944
train_iter_loss: 0.36081135272979736
train_iter_loss: 0.2248431295156479
train_iter_loss: 0.21001823246479034
train_iter_loss: 0.2726060748100281
train_iter_loss: 0.2605489194393158
train_iter_loss: 0.2954116761684418
train_iter_loss: 0.25306951999664307
train_iter_loss: 0.3303584158420563
train_iter_loss: 0.25949209928512573
train_iter_loss: 0.26303791999816895
train_iter_loss: 0.2466484010219574
train_iter_loss: 0.24300961196422577
train_iter_loss: 0.28521445393562317
train_iter_loss: 0.4016897976398468
train_iter_loss: 0.35539236664772034
train_iter_loss: 0.15047097206115723
train_iter_loss: 0.25939035415649414
train_iter_loss: 0.3286702036857605
train_iter_loss: 0.3934584856033325
train_iter_loss: 0.2982552647590637
train_iter_loss: 0.48804745078086853
train_iter_loss: 0.22941619157791138
train_iter_loss: 0.3160788416862488
train_iter_loss: 0.2797020673751831
train_iter_loss: 0.3254092335700989
train_iter_loss: 0.22580869495868683
train_iter_loss: 0.3964909315109253
train_iter_loss: 0.2160104662179947
train_iter_loss: 0.2190728485584259
train_iter_loss: 0.42373162508010864
train_iter_loss: 0.14324554800987244
train_iter_loss: 0.20897680521011353
train_iter_loss: 0.37894967198371887
train_iter_loss: 0.2740744650363922
train_iter_loss: 0.19963759183883667
train_iter_loss: 0.2776356041431427
train_iter_loss: 0.29462891817092896
train_iter_loss: 0.29029539227485657
train_iter_loss: 0.38679593801498413
train_iter_loss: 0.22486400604248047
train_iter_loss: 0.2806472182273865
train_iter_loss: 0.29291796684265137
train_iter_loss: 0.12052389979362488
train_iter_loss: 0.27176085114479065
train_iter_loss: 0.45630401372909546
train_iter_loss: 0.35508105158805847
train_iter_loss: 0.30894291400909424
train_iter_loss: 0.360299676656723
train_iter_loss: 0.13489778339862823
train_iter_loss: 0.22883114218711853
train_iter_loss: 0.10234281420707703
train_iter_loss: 0.24210339784622192
train_iter_loss: 0.2219509482383728
train_iter_loss: 0.37627193331718445
train_iter_loss: 0.13009928166866302
train_iter_loss: 0.4604587256908417
train_iter_loss: 0.21037602424621582
train_iter_loss: 0.3410092294216156
train_iter_loss: 0.23292161524295807
train_iter_loss: 0.2125503569841385
train_iter_loss: 0.18066957592964172
train_iter_loss: 0.4282996952533722
train_iter_loss: 0.3196333050727844
train_iter_loss: 0.17066064476966858
train_iter_loss: 0.4760141372680664
train_iter_loss: 0.31583747267723083
train_iter_loss: 0.4224691390991211
train_iter_loss: 0.38928183913230896
train_iter_loss: 0.30374422669410706
train_iter_loss: 0.3350023627281189
train_iter_loss: 0.36837121844291687
train loss :0.2966
---------------------
Validation seg loss: 0.3770761503047257 at epoch 190
epoch =    191/  1000, exp = train
train_iter_loss: 0.32260793447494507
train_iter_loss: 0.3067214787006378
train_iter_loss: 0.210963174700737
train_iter_loss: 0.33606669306755066
train_iter_loss: 0.19266170263290405
train_iter_loss: 0.37234073877334595
train_iter_loss: 0.33615297079086304
train_iter_loss: 0.13546405732631683
train_iter_loss: 0.25597628951072693
train_iter_loss: 0.18191659450531006
train_iter_loss: 0.2588656544685364
train_iter_loss: 0.2414548397064209
train_iter_loss: 0.12662135064601898
train_iter_loss: 0.37382230162620544
train_iter_loss: 0.39454594254493713
train_iter_loss: 0.22675275802612305
train_iter_loss: 0.21037372946739197
train_iter_loss: 0.22723440825939178
train_iter_loss: 0.22036349773406982
train_iter_loss: 0.4338679015636444
train_iter_loss: 0.20312370359897614
train_iter_loss: 0.389807790517807
train_iter_loss: 0.2783506214618683
train_iter_loss: 0.3361507058143616
train_iter_loss: 0.19476914405822754
train_iter_loss: 0.21068687736988068
train_iter_loss: 0.3978111445903778
train_iter_loss: 0.2697192132472992
train_iter_loss: 0.4211658835411072
train_iter_loss: 0.07181540131568909
train_iter_loss: 0.41305580735206604
train_iter_loss: 0.24071833491325378
train_iter_loss: 0.37740084528923035
train_iter_loss: 0.4053219258785248
train_iter_loss: 0.29202547669410706
train_iter_loss: 0.275865763425827
train_iter_loss: 0.3204187750816345
train_iter_loss: 0.21587620675563812
train_iter_loss: 0.29875701665878296
train_iter_loss: 0.3095080256462097
train_iter_loss: 0.2571483850479126
train_iter_loss: 0.4968204200267792
train_iter_loss: 0.3316331207752228
train_iter_loss: 0.3037409782409668
train_iter_loss: 0.17761392891407013
train_iter_loss: 0.40958932042121887
train_iter_loss: 0.21914392709732056
train_iter_loss: 0.30957624316215515
train_iter_loss: 0.19672344624996185
train_iter_loss: 0.4145532548427582
train_iter_loss: 0.377416729927063
train_iter_loss: 0.3813911974430084
train_iter_loss: 0.38547495007514954
train_iter_loss: 0.2823982834815979
train_iter_loss: 0.15606003999710083
train_iter_loss: 0.3209267258644104
train_iter_loss: 0.17800384759902954
train_iter_loss: 0.4306429326534271
train_iter_loss: 0.24731354415416718
train_iter_loss: 0.3149404525756836
train_iter_loss: 0.3122512996196747
train_iter_loss: 0.3672005832195282
train_iter_loss: 0.3707023561000824
train_iter_loss: 0.4458652436733246
train_iter_loss: 0.25746938586235046
train_iter_loss: 0.32228678464889526
train_iter_loss: 0.2614906132221222
train_iter_loss: 0.17406286299228668
train_iter_loss: 0.33513540029525757
train_iter_loss: 0.27811399102211
train_iter_loss: 0.24650274217128754
train_iter_loss: 0.19558213651180267
train_iter_loss: 0.2615601718425751
train_iter_loss: 0.2650567293167114
train_iter_loss: 0.24653515219688416
train_iter_loss: 0.17955002188682556
train_iter_loss: 0.29037725925445557
train_iter_loss: 0.2866237759590149
train_iter_loss: 0.33523234724998474
train_iter_loss: 0.2985009253025055
train_iter_loss: 0.17565016448497772
train_iter_loss: 0.23890714347362518
train_iter_loss: 0.2473812997341156
train_iter_loss: 0.2602587044239044
train_iter_loss: 0.3986300230026245
train_iter_loss: 0.3443377614021301
train_iter_loss: 0.2871003746986389
train_iter_loss: 0.2959257960319519
train_iter_loss: 0.327859491109848
train_iter_loss: 0.2770596742630005
train_iter_loss: 0.29358962178230286
train_iter_loss: 0.23710551857948303
train_iter_loss: 0.25802120566368103
train_iter_loss: 0.5518690943717957
train_iter_loss: 0.32478684186935425
train_iter_loss: 0.28323298692703247
train_iter_loss: 0.20847661793231964
train_iter_loss: 0.30125051736831665
train_iter_loss: 0.16274018585681915
train_iter_loss: 0.30290910601615906
train loss :0.2928
---------------------
Validation seg loss: 0.3998742616682682 at epoch 191
epoch =    192/  1000, exp = train
train_iter_loss: 0.34188348054885864
train_iter_loss: 0.4591858386993408
train_iter_loss: 0.35587868094444275
train_iter_loss: 0.4204779863357544
train_iter_loss: 0.3107830882072449
train_iter_loss: 0.1763528734445572
train_iter_loss: 0.25580042600631714
train_iter_loss: 0.31896352767944336
train_iter_loss: 0.1734105348587036
train_iter_loss: 0.2056587189435959
train_iter_loss: 0.11702308058738708
train_iter_loss: 0.17054034769535065
train_iter_loss: 0.34791868925094604
train_iter_loss: 0.25312522053718567
train_iter_loss: 0.22909726202487946
train_iter_loss: 0.23698480427265167
train_iter_loss: 0.3234666883945465
train_iter_loss: 0.3694608509540558
train_iter_loss: 0.2424481362104416
train_iter_loss: 0.1840515434741974
train_iter_loss: 0.2582559585571289
train_iter_loss: 0.4195846617221832
train_iter_loss: 0.2011818289756775
train_iter_loss: 0.2597723603248596
train_iter_loss: 0.3034800589084625
train_iter_loss: 0.33280107378959656
train_iter_loss: 0.3204851746559143
train_iter_loss: 0.3075183629989624
train_iter_loss: 0.19294847548007965
train_iter_loss: 0.38769295811653137
train_iter_loss: 0.3141106367111206
train_iter_loss: 0.22672761976718903
train_iter_loss: 0.213215634226799
train_iter_loss: 0.18504928052425385
train_iter_loss: 0.2555525004863739
train_iter_loss: 0.21661067008972168
train_iter_loss: 0.3766983151435852
train_iter_loss: 0.2914247214794159
train_iter_loss: 0.20898117125034332
train_iter_loss: 0.24642325937747955
train_iter_loss: 0.3386918008327484
train_iter_loss: 0.43183884024620056
train_iter_loss: 0.28844788670539856
train_iter_loss: 0.34599971771240234
train_iter_loss: 0.2548440396785736
train_iter_loss: 0.1329679787158966
train_iter_loss: 0.2502593994140625
train_iter_loss: 0.2084885686635971
train_iter_loss: 0.2195860594511032
train_iter_loss: 0.1918741762638092
train_iter_loss: 0.3991299271583557
train_iter_loss: 0.4198198616504669
train_iter_loss: 0.3562828600406647
train_iter_loss: 0.266539990901947
train_iter_loss: 0.295474112033844
train_iter_loss: 0.2181970775127411
train_iter_loss: 0.19413837790489197
train_iter_loss: 0.17514963448047638
train_iter_loss: 0.36925405263900757
train_iter_loss: 0.30141761898994446
train_iter_loss: 0.16118937730789185
train_iter_loss: 0.4383845627307892
train_iter_loss: 0.2761077284812927
train_iter_loss: 0.24453389644622803
train_iter_loss: 0.25511860847473145
train_iter_loss: 0.27573928236961365
train_iter_loss: 0.30813977122306824
train_iter_loss: 0.32377147674560547
train_iter_loss: 0.2388242930173874
train_iter_loss: 0.26451951265335083
train_iter_loss: 0.09485455602407455
train_iter_loss: 0.20147426426410675
train_iter_loss: 0.19567787647247314
train_iter_loss: 0.33972328901290894
train_iter_loss: 0.30817505717277527
train_iter_loss: 0.3735845685005188
train_iter_loss: 0.5483366847038269
train_iter_loss: 0.26759451627731323
train_iter_loss: 0.47694745659828186
train_iter_loss: 0.2922177016735077
train_iter_loss: 0.3007427155971527
train_iter_loss: 0.29062381386756897
train_iter_loss: 0.26640117168426514
train_iter_loss: 0.14072677493095398
train_iter_loss: 0.43162837624549866
train_iter_loss: 0.3820439875125885
train_iter_loss: 0.3444913923740387
train_iter_loss: 0.3463701009750366
train_iter_loss: 0.2404843121767044
train_iter_loss: 0.25874796509742737
train_iter_loss: 0.13717228174209595
train_iter_loss: 0.3062115013599396
train_iter_loss: 0.33385786414146423
train_iter_loss: 0.30554935336112976
train_iter_loss: 0.353679895401001
train_iter_loss: 0.26937106251716614
train_iter_loss: 0.27286121249198914
train_iter_loss: 0.42023733258247375
train_iter_loss: 0.3274416923522949
train_iter_loss: 0.2827335596084595
train loss :0.2886
---------------------
Validation seg loss: 0.3713235531775457 at epoch 192
epoch =    193/  1000, exp = train
train_iter_loss: 0.29236796498298645
train_iter_loss: 0.4723550081253052
train_iter_loss: 0.29516395926475525
train_iter_loss: 0.28672075271606445
train_iter_loss: 0.2730812430381775
train_iter_loss: 0.15303263068199158
train_iter_loss: 0.2752607464790344
train_iter_loss: 0.31975436210632324
train_iter_loss: 0.5675246119499207
train_iter_loss: 0.2376682460308075
train_iter_loss: 0.29539725184440613
train_iter_loss: 0.2834911346435547
train_iter_loss: 0.3662917912006378
train_iter_loss: 0.2045564353466034
train_iter_loss: 0.27283570170402527
train_iter_loss: 0.2693161964416504
train_iter_loss: 0.1802634596824646
train_iter_loss: 0.21447017788887024
train_iter_loss: 0.4089775085449219
train_iter_loss: 0.4011407196521759
train_iter_loss: 0.38048747181892395
train_iter_loss: 0.30300530791282654
train_iter_loss: 0.2961563766002655
train_iter_loss: 0.25596320629119873
train_iter_loss: 0.21699197590351105
train_iter_loss: 0.19586534798145294
train_iter_loss: 0.45116809010505676
train_iter_loss: 0.32228052616119385
train_iter_loss: 0.27554142475128174
train_iter_loss: 0.3617818355560303
train_iter_loss: 0.19822104275226593
train_iter_loss: 0.21618658304214478
train_iter_loss: 0.23429074883460999
train_iter_loss: 0.2625878155231476
train_iter_loss: 0.3591751456260681
train_iter_loss: 0.38680171966552734
train_iter_loss: 0.25036460161209106
train_iter_loss: 0.13059496879577637
train_iter_loss: 0.40397360920906067
train_iter_loss: 0.34192678332328796
train_iter_loss: 0.2610852122306824
train_iter_loss: 0.3519396185874939
train_iter_loss: 0.2701684832572937
train_iter_loss: 0.19162915647029877
train_iter_loss: 0.33925923705101013
train_iter_loss: 0.31713607907295227
train_iter_loss: 0.33073052763938904
train_iter_loss: 0.08567658066749573
train_iter_loss: 0.21013319492340088
train_iter_loss: 0.22342877089977264
train_iter_loss: 0.440877765417099
train_iter_loss: 0.25013312697410583
train_iter_loss: 0.25339046120643616
train_iter_loss: 0.19946815073490143
train_iter_loss: 0.28446394205093384
train_iter_loss: 0.18887954950332642
train_iter_loss: 0.3612353801727295
train_iter_loss: 0.3119007349014282
train_iter_loss: 0.08611369132995605
train_iter_loss: 0.1645222157239914
train_iter_loss: 0.3287903070449829
train_iter_loss: 0.30399027466773987
train_iter_loss: 0.4098878502845764
train_iter_loss: 0.20736674964427948
train_iter_loss: 0.38034093379974365
train_iter_loss: 0.249238982796669
train_iter_loss: 0.18747639656066895
train_iter_loss: 0.33292144536972046
train_iter_loss: 0.40138375759124756
train_iter_loss: 0.28532877564430237
train_iter_loss: 0.23650749027729034
train_iter_loss: 0.3409419357776642
train_iter_loss: 0.3451804518699646
train_iter_loss: 0.2278691977262497
train_iter_loss: 0.318027526140213
train_iter_loss: 0.22185590863227844
train_iter_loss: 0.263614684343338
train_iter_loss: 0.39868131279945374
train_iter_loss: 0.27304720878601074
train_iter_loss: 0.2790497839450836
train_iter_loss: 0.2214246392250061
train_iter_loss: 0.28402525186538696
train_iter_loss: 0.23617567121982574
train_iter_loss: 0.2576066851615906
train_iter_loss: 0.1699642688035965
train_iter_loss: 0.3118517994880676
train_iter_loss: 0.2726133465766907
train_iter_loss: 0.34359753131866455
train_iter_loss: 0.45516082644462585
train_iter_loss: 0.26430177688598633
train_iter_loss: 0.4304178059101105
train_iter_loss: 0.40504714846611023
train_iter_loss: 0.27877509593963623
train_iter_loss: 0.31986314058303833
train_iter_loss: 0.2215956449508667
train_iter_loss: 0.246041938662529
train_iter_loss: 0.32540252804756165
train_iter_loss: 0.24927330017089844
train_iter_loss: 0.3701554536819458
train_iter_loss: 0.21394647657871246
train loss :0.2919
---------------------
Validation seg loss: 0.3681552023890446 at epoch 193
epoch =    194/  1000, exp = train
train_iter_loss: 0.22050030529499054
train_iter_loss: 0.25571972131729126
train_iter_loss: 0.3381890654563904
train_iter_loss: 0.3793742060661316
train_iter_loss: 0.35827720165252686
train_iter_loss: 0.38596111536026
train_iter_loss: 0.3215607702732086
train_iter_loss: 0.0668485090136528
train_iter_loss: 0.3210388123989105
train_iter_loss: 0.3689926862716675
train_iter_loss: 0.20136721432209015
train_iter_loss: 0.3585115373134613
train_iter_loss: 0.3330610990524292
train_iter_loss: 0.3555467426776886
train_iter_loss: 0.2505406439304352
train_iter_loss: 0.22205032408237457
train_iter_loss: 0.09804809838533401
train_iter_loss: 0.359655499458313
train_iter_loss: 0.365687757730484
train_iter_loss: 0.32937973737716675
train_iter_loss: 0.40004897117614746
train_iter_loss: 0.3194725513458252
train_iter_loss: 0.3116050064563751
train_iter_loss: 0.3383568525314331
train_iter_loss: 0.15598948299884796
train_iter_loss: 0.39264383912086487
train_iter_loss: 0.15144270658493042
train_iter_loss: 0.28263747692108154
train_iter_loss: 0.2177499383687973
train_iter_loss: 0.20192016661167145
train_iter_loss: 0.212257519364357
train_iter_loss: 0.23395632207393646
train_iter_loss: 0.44261008501052856
train_iter_loss: 0.15255746245384216
train_iter_loss: 0.30903172492980957
train_iter_loss: 0.2748340666294098
train_iter_loss: 0.3671143651008606
train_iter_loss: 0.22165119647979736
train_iter_loss: 0.3203107714653015
train_iter_loss: 0.3451703190803528
train_iter_loss: 0.2519359588623047
train_iter_loss: 0.22927013039588928
train_iter_loss: 0.13061119616031647
train_iter_loss: 0.2188902050256729
train_iter_loss: 0.26482266187667847
train_iter_loss: 0.3194262981414795
train_iter_loss: 0.3316226005554199
train_iter_loss: 0.2738461196422577
train_iter_loss: 0.33602458238601685
train_iter_loss: 0.3238840103149414
train_iter_loss: 0.0909522995352745
train_iter_loss: 0.23739883303642273
train_iter_loss: 0.256244421005249
train_iter_loss: 0.30651864409446716
train_iter_loss: 0.14105457067489624
train_iter_loss: 0.3615984618663788
train_iter_loss: 0.21426242589950562
train_iter_loss: 0.4315992295742035
train_iter_loss: 0.359038770198822
train_iter_loss: 0.5822368860244751
train_iter_loss: 0.21378710865974426
train_iter_loss: 0.4771779179573059
train_iter_loss: 0.3369576930999756
train_iter_loss: 0.23966450989246368
train_iter_loss: 0.28797248005867004
train_iter_loss: 0.3583395779132843
train_iter_loss: 0.27243223786354065
train_iter_loss: 0.3133394122123718
train_iter_loss: 0.24695061147212982
train_iter_loss: 0.19989702105522156
train_iter_loss: 0.13876937329769135
train_iter_loss: 0.317735493183136
train_iter_loss: 0.370910108089447
train_iter_loss: 0.25778815150260925
train_iter_loss: 0.2205738127231598
train_iter_loss: 0.2784990668296814
train_iter_loss: 0.2890018820762634
train_iter_loss: 0.40329262614250183
train_iter_loss: 0.37555360794067383
train_iter_loss: 0.23315192759037018
train_iter_loss: 0.2332477569580078
train_iter_loss: 0.3577703535556793
train_iter_loss: 0.23271237313747406
train_iter_loss: 0.2744573652744293
train_iter_loss: 0.35468626022338867
train_iter_loss: 0.44096890091896057
train_iter_loss: 0.24240121245384216
train_iter_loss: 0.17947278916835785
train_iter_loss: 0.35321176052093506
train_iter_loss: 0.16272832453250885
train_iter_loss: 0.3535170257091522
train_iter_loss: 0.3231406807899475
train_iter_loss: 0.34053751826286316
train_iter_loss: 0.21955057978630066
train_iter_loss: 0.3270925283432007
train_iter_loss: 0.09217928349971771
train_iter_loss: 0.19967105984687805
train_iter_loss: 0.25025758147239685
train_iter_loss: 0.19256289303302765
train_iter_loss: 0.41010770201683044
train loss :0.2887
---------------------
Validation seg loss: 0.36887266750465025 at epoch 194
epoch =    195/  1000, exp = train
train_iter_loss: 0.4261954426765442
train_iter_loss: 0.22269220650196075
train_iter_loss: 0.2486570179462433
train_iter_loss: 0.43121856451034546
train_iter_loss: 0.2503332793712616
train_iter_loss: 0.2834479808807373
train_iter_loss: 0.30068498849868774
train_iter_loss: 0.23127135634422302
train_iter_loss: 0.2987900376319885
train_iter_loss: 0.20441430807113647
train_iter_loss: 0.25687897205352783
train_iter_loss: 0.35321110486984253
train_iter_loss: 0.2713412046432495
train_iter_loss: 0.23835718631744385
train_iter_loss: 0.1964924931526184
train_iter_loss: 0.3694992661476135
train_iter_loss: 0.21977292001247406
train_iter_loss: 0.25882190465927124
train_iter_loss: 0.2974427342414856
train_iter_loss: 0.37978559732437134
train_iter_loss: 0.3037473261356354
train_iter_loss: 0.23151175677776337
train_iter_loss: 0.22112232446670532
train_iter_loss: 0.30569231510162354
train_iter_loss: 0.2734202742576599
train_iter_loss: 0.20793114602565765
train_iter_loss: 0.31433552503585815
train_iter_loss: 0.3665824234485626
train_iter_loss: 0.21515320241451263
train_iter_loss: 0.2564924359321594
train_iter_loss: 0.3511769771575928
train_iter_loss: 0.26396191120147705
train_iter_loss: 0.3244157135486603
train_iter_loss: 0.3654784858226776
train_iter_loss: 0.2448423057794571
train_iter_loss: 0.18101565539836884
train_iter_loss: 0.2611406743526459
train_iter_loss: 0.296299010515213
train_iter_loss: 0.25656431913375854
train_iter_loss: 0.33646702766418457
train_iter_loss: 0.2360495626926422
train_iter_loss: 0.39499831199645996
train_iter_loss: 0.3206823766231537
train_iter_loss: 0.3506155014038086
train_iter_loss: 0.28589728474617004
train_iter_loss: 0.2606044113636017
train_iter_loss: 0.3701646327972412
train_iter_loss: 0.155784472823143
train_iter_loss: 0.36098551750183105
train_iter_loss: 0.1569518893957138
train_iter_loss: 0.39530742168426514
train_iter_loss: 0.2685486674308777
train_iter_loss: 0.40469470620155334
train_iter_loss: 0.25864943861961365
train_iter_loss: 0.36594873666763306
train_iter_loss: 0.31471744179725647
train_iter_loss: 0.18592631816864014
train_iter_loss: 0.2134331315755844
train_iter_loss: 0.3861641585826874
train_iter_loss: 0.1764007806777954
train_iter_loss: 0.31275901198387146
train_iter_loss: 0.279367059469223
train_iter_loss: 0.19907355308532715
train_iter_loss: 0.12702514231204987
train_iter_loss: 0.2760735750198364
train_iter_loss: 0.3641626834869385
train_iter_loss: 0.31533822417259216
train_iter_loss: 0.27045321464538574
train_iter_loss: 0.17287009954452515
train_iter_loss: 0.33276307582855225
train_iter_loss: 0.25978344678878784
train_iter_loss: 0.25972557067871094
train_iter_loss: 0.19264395534992218
train_iter_loss: 0.23365184664726257
train_iter_loss: 0.29376935958862305
train_iter_loss: 0.20710332691669464
train_iter_loss: 0.25235283374786377
train_iter_loss: 0.3437308371067047
train_iter_loss: 0.40467649698257446
train_iter_loss: 0.4179649353027344
train_iter_loss: 0.4397711753845215
train_iter_loss: 0.26432228088378906
train_iter_loss: 0.2655123472213745
train_iter_loss: 0.33721432089805603
train_iter_loss: 0.541846752166748
train_iter_loss: 0.2489403784275055
train_iter_loss: 0.2777603268623352
train_iter_loss: 0.20606490969657898
train_iter_loss: 0.23888516426086426
train_iter_loss: 0.2205415666103363
train_iter_loss: 0.36419278383255005
train_iter_loss: 0.2412126362323761
train_iter_loss: 0.316514790058136
train_iter_loss: 0.2886514365673065
train_iter_loss: 0.35404473543167114
train_iter_loss: 0.21528856456279755
train_iter_loss: 0.41145285964012146
train_iter_loss: 0.2790033221244812
train_iter_loss: 0.31332656741142273
train_iter_loss: 0.3235507905483246
train loss :0.2913
---------------------
Validation seg loss: 0.36462602133528804 at epoch 195
epoch =    196/  1000, exp = train
train_iter_loss: 0.3598059415817261
train_iter_loss: 0.290437251329422
train_iter_loss: 0.4322756230831146
train_iter_loss: 0.2305385321378708
train_iter_loss: 0.254986971616745
train_iter_loss: 0.21425963938236237
train_iter_loss: 0.18856699764728546
train_iter_loss: 0.26255327463150024
train_iter_loss: 0.20819474756717682
train_iter_loss: 0.2276003211736679
train_iter_loss: 0.28812718391418457
train_iter_loss: 0.22876980900764465
train_iter_loss: 0.28544744849205017
train_iter_loss: 0.2542852461338043
train_iter_loss: 0.18725940585136414
train_iter_loss: 0.24228499829769135
train_iter_loss: 0.3559131622314453
train_iter_loss: 0.3120805025100708
train_iter_loss: 0.1856306493282318
train_iter_loss: 0.27566152811050415
train_iter_loss: 0.2596276104450226
train_iter_loss: 0.3414019048213959
train_iter_loss: 0.16817902028560638
train_iter_loss: 0.2558401823043823
train_iter_loss: 0.20149433612823486
train_iter_loss: 0.29127082228660583
train_iter_loss: 0.2826405167579651
train_iter_loss: 0.3604690730571747
train_iter_loss: 0.19360795617103577
train_iter_loss: 0.27351412177085876
train_iter_loss: 0.34525972604751587
train_iter_loss: 0.12740489840507507
train_iter_loss: 0.3481575846672058
train_iter_loss: 0.20234625041484833
train_iter_loss: 0.2923124134540558
train_iter_loss: 0.1280636489391327
train_iter_loss: 0.2514004111289978
train_iter_loss: 0.28179073333740234
train_iter_loss: 0.14939112961292267
train_iter_loss: 0.2773706912994385
train_iter_loss: 0.3970310688018799
train_iter_loss: 0.2536623477935791
train_iter_loss: 0.2854013442993164
train_iter_loss: 0.2236778736114502
train_iter_loss: 0.2654898464679718
train_iter_loss: 0.26112058758735657
train_iter_loss: 0.4847640097141266
train_iter_loss: 0.43357476592063904
train_iter_loss: 0.1708173155784607
train_iter_loss: 0.3201732933521271
train_iter_loss: 0.09134916961193085
train_iter_loss: 0.4179936945438385
train_iter_loss: 0.31163331866264343
train_iter_loss: 0.3346896171569824
train_iter_loss: 0.29145342111587524
train_iter_loss: 0.3298918604850769
train_iter_loss: 0.33040711283683777
train_iter_loss: 0.4769194722175598
train_iter_loss: 0.3351634740829468
train_iter_loss: 0.2167416661977768
train_iter_loss: 0.35790592432022095
train_iter_loss: 0.21837134659290314
train_iter_loss: 0.3538823127746582
train_iter_loss: 0.36088231205940247
train_iter_loss: 0.23679746687412262
train_iter_loss: 0.16967596113681793
train_iter_loss: 0.2657707929611206
train_iter_loss: 0.4548599421977997
train_iter_loss: 0.31765469908714294
train_iter_loss: 0.1671667993068695
train_iter_loss: 0.304186075925827
train_iter_loss: 0.2188616544008255
train_iter_loss: 0.25376540422439575
train_iter_loss: 0.16069041192531586
train_iter_loss: 0.3711034953594208
train_iter_loss: 0.3393099904060364
train_iter_loss: 0.315514475107193
train_iter_loss: 0.19774064421653748
train_iter_loss: 0.380045086145401
train_iter_loss: 0.15691938996315002
train_iter_loss: 0.21304692327976227
train_iter_loss: 0.27717941999435425
train_iter_loss: 0.39276766777038574
train_iter_loss: 0.3273334503173828
train_iter_loss: 0.5015711784362793
train_iter_loss: 0.251203715801239
train_iter_loss: 0.22016064822673798
train_iter_loss: 0.3714138865470886
train_iter_loss: 0.31133976578712463
train_iter_loss: 0.22795765101909637
train_iter_loss: 0.6637475490570068
train_iter_loss: 0.3823249042034149
train_iter_loss: 0.2793506681919098
train_iter_loss: 0.4229060709476471
train_iter_loss: 0.4697311818599701
train_iter_loss: 0.3220331072807312
train_iter_loss: 0.22762846946716309
train_iter_loss: 0.09380088746547699
train_iter_loss: 0.28497302532196045
train_iter_loss: 0.17252737283706665
train loss :0.2885
---------------------
Validation seg loss: 0.36903053912211137 at epoch 196
epoch =    197/  1000, exp = train
train_iter_loss: 0.37748199701309204
train_iter_loss: 0.3211975693702698
train_iter_loss: 0.17973175644874573
train_iter_loss: 0.22589129209518433
train_iter_loss: 0.49640265107154846
train_iter_loss: 0.3307707905769348
train_iter_loss: 0.24002942442893982
train_iter_loss: 0.24846972525119781
train_iter_loss: 0.2371116578578949
train_iter_loss: 0.42222660779953003
train_iter_loss: 0.16922618448734283
train_iter_loss: 0.16860787570476532
train_iter_loss: 0.3333078622817993
train_iter_loss: 0.3104107677936554
train_iter_loss: 0.4592931568622589
train_iter_loss: 0.21291105449199677
train_iter_loss: 0.36672377586364746
train_iter_loss: 0.4224972128868103
train_iter_loss: 0.3778666853904724
train_iter_loss: 0.2265697568655014
train_iter_loss: 0.1510487049818039
train_iter_loss: 0.393571674823761
train_iter_loss: 0.33170124888420105
train_iter_loss: 0.5565305948257446
train_iter_loss: 0.3557966649532318
train_iter_loss: 0.2151353359222412
train_iter_loss: 0.14917303621768951
train_iter_loss: 0.29183289408683777
train_iter_loss: 0.17333915829658508
train_iter_loss: 0.22528314590454102
train_iter_loss: 0.2880380153656006
train_iter_loss: 0.26877716183662415
train_iter_loss: 0.2484855353832245
train_iter_loss: 0.2936233580112457
train_iter_loss: 0.3017279803752899
train_iter_loss: 0.16889452934265137
train_iter_loss: 0.40908899903297424
train_iter_loss: 0.2977254390716553
train_iter_loss: 0.4059503376483917
train_iter_loss: 0.3352205157279968
train_iter_loss: 0.33995044231414795
train_iter_loss: 0.3662625551223755
train_iter_loss: 0.29725706577301025
train_iter_loss: 0.20979905128479004
train_iter_loss: 0.21922804415225983
train_iter_loss: 0.2551262676715851
train_iter_loss: 0.35515767335891724
train_iter_loss: 0.30765295028686523
train_iter_loss: 0.19770120084285736
train_iter_loss: 0.1788237988948822
train_iter_loss: 0.262424498796463
train_iter_loss: 0.1710340529680252
train_iter_loss: 0.3418395221233368
train_iter_loss: 0.3091323971748352
train_iter_loss: 0.1907910853624344
train_iter_loss: 0.42478233575820923
train_iter_loss: 0.2550406754016876
train_iter_loss: 0.5451580286026001
train_iter_loss: 0.259213387966156
train_iter_loss: 0.3110789954662323
train_iter_loss: 0.19017280638217926
train_iter_loss: 0.25767409801483154
train_iter_loss: 0.2712532877922058
train_iter_loss: 0.37387996912002563
train_iter_loss: 0.29151225090026855
train_iter_loss: 0.4224063754081726
train_iter_loss: 0.3081751763820648
train_iter_loss: 0.18492230772972107
train_iter_loss: 0.19087034463882446
train_iter_loss: 0.38854119181632996
train_iter_loss: 0.2665638327598572
train_iter_loss: 0.24253375828266144
train_iter_loss: 0.31262701749801636
train_iter_loss: 0.3099222481250763
train_iter_loss: 0.2475995570421219
train_iter_loss: 0.16919881105422974
train_iter_loss: 0.3139687478542328
train_iter_loss: 0.22766022384166718
train_iter_loss: 0.28208160400390625
train_iter_loss: 0.13440313935279846
train_iter_loss: 0.26985278725624084
train_iter_loss: 0.2601032257080078
train_iter_loss: 0.4445578157901764
train_iter_loss: 0.40703442692756653
train_iter_loss: 0.1784764975309372
train_iter_loss: 0.3159503638744354
train_iter_loss: 0.3341905176639557
train_iter_loss: 0.2881892919540405
train_iter_loss: 0.2756337821483612
train_iter_loss: 0.2886671721935272
train_iter_loss: 0.20059345662593842
train_iter_loss: 0.2727532386779785
train_iter_loss: 0.18446344137191772
train_iter_loss: 0.3181334435939789
train_iter_loss: 0.22421661019325256
train_iter_loss: 0.21490336954593658
train_iter_loss: 0.3803633153438568
train_iter_loss: 0.25575774908065796
train_iter_loss: 0.40188148617744446
train_iter_loss: 0.42540308833122253
train loss :0.2940
---------------------
Validation seg loss: 0.3812678086237525 at epoch 197
epoch =    198/  1000, exp = train
train_iter_loss: 0.09735020250082016
train_iter_loss: 0.25230634212493896
train_iter_loss: 0.27422961592674255
train_iter_loss: 0.242350235581398
train_iter_loss: 0.2952237129211426
train_iter_loss: 0.29244470596313477
train_iter_loss: 0.3030487596988678
train_iter_loss: 0.2826876938343048
train_iter_loss: 0.3624172806739807
train_iter_loss: 0.2931731343269348
train_iter_loss: 0.2053276002407074
train_iter_loss: 0.2904887795448303
train_iter_loss: 0.22455553710460663
train_iter_loss: 0.25480780005455017
train_iter_loss: 0.27747175097465515
train_iter_loss: 0.3121473491191864
train_iter_loss: 0.21133488416671753
train_iter_loss: 0.29190731048583984
train_iter_loss: 0.38045817613601685
train_iter_loss: 0.3201325833797455
train_iter_loss: 0.23337289690971375
train_iter_loss: 0.3560523986816406
train_iter_loss: 0.2652890682220459
train_iter_loss: 0.13105732202529907
train_iter_loss: 0.243186816573143
train_iter_loss: 0.19112300872802734
train_iter_loss: 0.42873188853263855
train_iter_loss: 0.35469600558280945
train_iter_loss: 0.4283933937549591
train_iter_loss: 0.24634875357151031
train_iter_loss: 0.33220624923706055
train_iter_loss: 0.32596835494041443
train_iter_loss: 0.27280834317207336
train_iter_loss: 0.4367988705635071
train_iter_loss: 0.19237981736660004
train_iter_loss: 0.22445036470890045
train_iter_loss: 0.15641959011554718
train_iter_loss: 0.18944509327411652
train_iter_loss: 0.2949129343032837
train_iter_loss: 0.2883625328540802
train_iter_loss: 0.18513981997966766
train_iter_loss: 0.1605384349822998
train_iter_loss: 0.29937341809272766
train_iter_loss: 0.2544950246810913
train_iter_loss: 0.26535457372665405
train_iter_loss: 0.5259720087051392
train_iter_loss: 0.19233325123786926
train_iter_loss: 0.21403297781944275
train_iter_loss: 0.3587944209575653
train_iter_loss: 0.15191274881362915
train_iter_loss: 0.30625131726264954
train_iter_loss: 0.24301789700984955
train_iter_loss: 0.2194916158914566
train_iter_loss: 0.22013241052627563
train_iter_loss: 0.38804784417152405
train_iter_loss: 0.19091933965682983
train_iter_loss: 0.32506799697875977
train_iter_loss: 0.21921829879283905
train_iter_loss: 0.18108391761779785
train_iter_loss: 0.36820167303085327
train_iter_loss: 0.149206280708313
train_iter_loss: 0.25571396946907043
train_iter_loss: 0.40171483159065247
train_iter_loss: 0.3700692057609558
train_iter_loss: 0.3817780315876007
train_iter_loss: 0.32378166913986206
train_iter_loss: 0.3176044821739197
train_iter_loss: 0.2814168334007263
train_iter_loss: 0.19675660133361816
train_iter_loss: 0.3599551320075989
train_iter_loss: 0.21166658401489258
train_iter_loss: 0.3125898838043213
train_iter_loss: 0.3263164758682251
train_iter_loss: 0.2956821918487549
train_iter_loss: 0.3068810701370239
train_iter_loss: 0.41650402545928955
train_iter_loss: 0.3488490879535675
train_iter_loss: 0.2803294062614441
train_iter_loss: 0.2656731605529785
train_iter_loss: 0.19517074525356293
train_iter_loss: 0.27261245250701904
train_iter_loss: 0.18600013852119446
train_iter_loss: 0.25039049983024597
train_iter_loss: 0.39151260256767273
train_iter_loss: 0.334880530834198
train_iter_loss: 0.28082361817359924
train_iter_loss: 0.43578672409057617
train_iter_loss: 0.2548467218875885
train_iter_loss: 0.149272158741951
train_iter_loss: 0.7734917402267456
train_iter_loss: 0.3027344048023224
train_iter_loss: 0.3717556595802307
train_iter_loss: 0.1522434651851654
train_iter_loss: 0.2397671490907669
train_iter_loss: 0.16913774609565735
train_iter_loss: 0.35406219959259033
train_iter_loss: 0.29438209533691406
train_iter_loss: 0.30240634083747864
train_iter_loss: 0.31217604875564575
train_iter_loss: 0.2995903491973877
train loss :0.2874
---------------------
Validation seg loss: 0.36535976421228555 at epoch 198
epoch =    199/  1000, exp = train
train_iter_loss: 0.3598324656486511
train_iter_loss: 0.09007430076599121
train_iter_loss: 0.32118481397628784
train_iter_loss: 0.31210383772850037
train_iter_loss: 0.2865179479122162
train_iter_loss: 0.1621944159269333
train_iter_loss: 0.2353050261735916
train_iter_loss: 0.4027997851371765
train_iter_loss: 0.23300546407699585
train_iter_loss: 0.3728080987930298
train_iter_loss: 0.2634870111942291
train_iter_loss: 0.17904718220233917
train_iter_loss: 0.23610036075115204
train_iter_loss: 0.3404916226863861
train_iter_loss: 0.3067953586578369
train_iter_loss: 0.27151262760162354
train_iter_loss: 0.39859938621520996
train_iter_loss: 0.2339448481798172
train_iter_loss: 0.3715963065624237
train_iter_loss: 0.2548445463180542
train_iter_loss: 0.2745749056339264
train_iter_loss: 0.18344838917255402
train_iter_loss: 0.502546489238739
train_iter_loss: 0.2985583543777466
train_iter_loss: 0.2309051752090454
train_iter_loss: 0.2759123146533966
train_iter_loss: 0.40305057168006897
train_iter_loss: 0.3108726739883423
train_iter_loss: 0.24549748003482819
train_iter_loss: 0.2178347408771515
train_iter_loss: 0.19321951270103455
train_iter_loss: 0.4292886257171631
train_iter_loss: 0.2505981922149658
train_iter_loss: 0.3168984651565552
train_iter_loss: 0.4173148572444916
train_iter_loss: 0.2205919772386551
train_iter_loss: 0.1345410943031311
train_iter_loss: 0.30936798453330994
train_iter_loss: 0.2205461859703064
train_iter_loss: 0.35604262351989746
train_iter_loss: 0.22083856165409088
train_iter_loss: 0.4042847156524658
train_iter_loss: 0.304781973361969
train_iter_loss: 0.3603205978870392
train_iter_loss: 0.2899126708507538
train_iter_loss: 0.1980723738670349
train_iter_loss: 0.2282848060131073
train_iter_loss: 0.31709301471710205
train_iter_loss: 0.19371697306632996
train_iter_loss: 0.6575734615325928
train_iter_loss: 0.3621504604816437
train_iter_loss: 0.17408007383346558
train_iter_loss: 0.10249240696430206
train_iter_loss: 0.18448816239833832
train_iter_loss: 0.3113638460636139
train_iter_loss: 0.3215351402759552
train_iter_loss: 0.29549160599708557
train_iter_loss: 0.27913668751716614
train_iter_loss: 0.2491697072982788
train_iter_loss: 0.2637716829776764
train_iter_loss: 0.27244865894317627
train_iter_loss: 0.3851831555366516
train_iter_loss: 0.38706234097480774
train_iter_loss: 0.24711838364601135
train_iter_loss: 0.32211965322494507
train_iter_loss: 0.09603852033615112
train_iter_loss: 0.3045704960823059
train_iter_loss: 0.29086601734161377
train_iter_loss: 0.3245525062084198
train_iter_loss: 0.2705073356628418
train_iter_loss: 0.2579247057437897
train_iter_loss: 0.29104703664779663
train_iter_loss: 0.3474951684474945
train_iter_loss: 0.2581462860107422
train_iter_loss: 0.3781887888908386
train_iter_loss: 0.2717742919921875
train_iter_loss: 0.29527226090431213
train_iter_loss: 0.25455281138420105
train_iter_loss: 0.26430079340934753
train_iter_loss: 0.3527451455593109
train_iter_loss: 0.3222280442714691
train_iter_loss: 0.239806666970253
train_iter_loss: 0.2804924249649048
train_iter_loss: 0.29959946870803833
train_iter_loss: 0.3430241346359253
train_iter_loss: 0.2977665364742279
train_iter_loss: 0.2238648384809494
train_iter_loss: 0.28954315185546875
train_iter_loss: 0.271901935338974
train_iter_loss: 0.23498712480068207
train_iter_loss: 0.2184310406446457
train_iter_loss: 0.3192097544670105
train_iter_loss: 0.1545751690864563
train_iter_loss: 0.24050910770893097
train_iter_loss: 0.18080183863639832
train_iter_loss: 0.1766611486673355
train_iter_loss: 0.27107805013656616
train_iter_loss: 0.2835216522216797
train_iter_loss: 0.16791939735412598
train_iter_loss: 0.39098626375198364
train loss :0.2851
---------------------
Validation seg loss: 0.3787553033457612 at epoch 199
epoch =    200/  1000, exp = train
train_iter_loss: 0.43290993571281433
train_iter_loss: 0.27430257201194763
train_iter_loss: 0.3357834815979004
train_iter_loss: 0.2597673833370209
train_iter_loss: 0.24604357779026031
train_iter_loss: 0.4540073871612549
train_iter_loss: 0.17769606411457062
train_iter_loss: 0.2655602693557739
train_iter_loss: 0.23163634538650513
train_iter_loss: 0.37245920300483704
train_iter_loss: 0.2697180211544037
train_iter_loss: 0.2316259741783142
train_iter_loss: 0.33084166049957275
train_iter_loss: 0.4119899570941925
train_iter_loss: 0.1512509137392044
train_iter_loss: 0.27865681052207947
train_iter_loss: 0.2968558073043823
train_iter_loss: 0.15049085021018982
train_iter_loss: 0.33805543184280396
train_iter_loss: 0.3103554844856262
train_iter_loss: 0.44727030396461487
train_iter_loss: 0.3100549578666687
train_iter_loss: 0.24771273136138916
train_iter_loss: 0.42677387595176697
train_iter_loss: 0.29583045840263367
train_iter_loss: 0.377998024225235
train_iter_loss: 0.3889184594154358
train_iter_loss: 0.3176610469818115
train_iter_loss: 0.366329550743103
train_iter_loss: 0.41429492831230164
train_iter_loss: 0.13876761496067047
train_iter_loss: 0.26850375533103943
train_iter_loss: 0.2059101015329361
train_iter_loss: 0.3334871530532837
train_iter_loss: 0.22495876252651215
train_iter_loss: 0.44179579615592957
train_iter_loss: 0.2962696850299835
train_iter_loss: 0.35758787393569946
train_iter_loss: 0.26141199469566345
train_iter_loss: 0.18725663423538208
train_iter_loss: 0.3005967438220978
train_iter_loss: 0.2651457190513611
train_iter_loss: 0.20209330320358276
train_iter_loss: 0.17847813665866852
train_iter_loss: 0.261799156665802
train_iter_loss: 0.21385568380355835
train_iter_loss: 0.23110263049602509
train_iter_loss: 0.3494744598865509
train_iter_loss: 0.26083338260650635
train_iter_loss: 0.19629521667957306
train_iter_loss: 0.20195166766643524
train_iter_loss: 0.2288135588169098
train_iter_loss: 0.21441979706287384
train_iter_loss: 0.16846728324890137
train_iter_loss: 0.38735270500183105
train_iter_loss: 0.32583945989608765
train_iter_loss: 0.20365948975086212
train_iter_loss: 0.34306055307388306
train_iter_loss: 0.3305026590824127
train_iter_loss: 0.277787983417511
train_iter_loss: 0.31228533387184143
train_iter_loss: 0.4549718201160431
train_iter_loss: 0.2637665569782257
train_iter_loss: 0.22272926568984985
train_iter_loss: 0.2443278282880783
train_iter_loss: 0.16817347705364227
train_iter_loss: 0.24083276093006134
train_iter_loss: 0.2735430896282196
train_iter_loss: 0.3293677270412445
train_iter_loss: 0.2577480673789978
train_iter_loss: 0.16568471491336823
train_iter_loss: 0.27890545129776
train_iter_loss: 0.3914094865322113
train_iter_loss: 0.2721998691558838
train_iter_loss: 0.17781569063663483
train_iter_loss: 0.3094842731952667
train_iter_loss: 0.33491694927215576
train_iter_loss: 0.30770331621170044
train_iter_loss: 0.31289929151535034
train_iter_loss: 0.17754480242729187
train_iter_loss: 0.44940847158432007
train_iter_loss: 0.3103092908859253
train_iter_loss: 0.31632131338119507
train_iter_loss: 0.2748848795890808
train_iter_loss: 0.3980185091495514
train_iter_loss: 0.30152618885040283
train_iter_loss: 0.18654079735279083
train_iter_loss: 0.3606298267841339
train_iter_loss: 0.16857366263866425
train_iter_loss: 0.2758764624595642
train_iter_loss: 0.3375650942325592
train_iter_loss: 0.3679603338241577
train_iter_loss: 0.3618223965167999
train_iter_loss: 0.3256339728832245
train_iter_loss: 0.41529691219329834
train_iter_loss: 0.4411862790584564
train_iter_loss: 0.25356242060661316
train_iter_loss: 0.34481385350227356
train_iter_loss: 0.23739226162433624
train_iter_loss: 0.26513582468032837
train loss :0.2944
---------------------
Validation seg loss: 0.37260391318045977 at epoch 200
epoch =    201/  1000, exp = train
train_iter_loss: 0.17721955478191376
train_iter_loss: 0.3855843245983124
train_iter_loss: 0.28181391954421997
train_iter_loss: 0.2219793051481247
train_iter_loss: 0.378898024559021
train_iter_loss: 0.2871212959289551
train_iter_loss: 0.3162339925765991
train_iter_loss: 0.36519551277160645
train_iter_loss: 0.29883670806884766
train_iter_loss: 0.3400029242038727
train_iter_loss: 0.3335891366004944
train_iter_loss: 0.47829923033714294
train_iter_loss: 0.2873435914516449
train_iter_loss: 0.2423858940601349
train_iter_loss: 0.3087354600429535
train_iter_loss: 0.3110901713371277
train_iter_loss: 0.11445696651935577
train_iter_loss: 0.28035515546798706
train_iter_loss: 0.24349068105220795
train_iter_loss: 0.19283351302146912
train_iter_loss: 0.25552549958229065
train_iter_loss: 0.364653617143631
train_iter_loss: 0.09588390588760376
train_iter_loss: 0.41619670391082764
train_iter_loss: 0.28446242213249207
train_iter_loss: 0.4100443720817566
train_iter_loss: 0.2540612816810608
train_iter_loss: 0.11531506478786469
train_iter_loss: 0.16630084812641144
train_iter_loss: 0.1814773976802826
train_iter_loss: 0.10113981366157532
train_iter_loss: 0.2688332796096802
train_iter_loss: 0.3139769434928894
train_iter_loss: 0.30412212014198303
train_iter_loss: 0.20237404108047485
train_iter_loss: 0.19681426882743835
train_iter_loss: 0.3843366801738739
train_iter_loss: 0.26992475986480713
train_iter_loss: 0.3536730706691742
train_iter_loss: 0.44280463457107544
train_iter_loss: 0.2852831780910492
train_iter_loss: 0.3928670287132263
train_iter_loss: 0.2971033453941345
train_iter_loss: 0.3253132104873657
train_iter_loss: 0.23656007647514343
train_iter_loss: 0.434359610080719
train_iter_loss: 0.35644441843032837
train_iter_loss: 0.24123640358448029
train_iter_loss: 0.38760578632354736
train_iter_loss: 0.5508555173873901
train_iter_loss: 0.20889343321323395
train_iter_loss: 0.48248061537742615
train_iter_loss: 0.3495883345603943
train_iter_loss: 0.25343021750450134
train_iter_loss: 0.3905426561832428
train_iter_loss: 0.270751953125
train_iter_loss: 0.2520122826099396
train_iter_loss: 0.36826229095458984
train_iter_loss: 0.24786213040351868
train_iter_loss: 0.30978235602378845
train_iter_loss: 0.41820046305656433
train_iter_loss: 0.11506873369216919
train_iter_loss: 0.3079165518283844
train_iter_loss: 0.2314687967300415
train_iter_loss: 0.38921797275543213
train_iter_loss: 0.31498488783836365
train_iter_loss: 0.40278902649879456
train_iter_loss: 0.2607986330986023
train_iter_loss: 0.2559002637863159
train_iter_loss: 0.25338393449783325
train_iter_loss: 0.33267199993133545
train_iter_loss: 0.2311367690563202
train_iter_loss: 0.09501082450151443
train_iter_loss: 0.2552184760570526
train_iter_loss: 0.283416211605072
train_iter_loss: 0.1914900690317154
train_iter_loss: 0.3591654300689697
train_iter_loss: 0.39974355697631836
train_iter_loss: 0.23657308518886566
train_iter_loss: 0.255368173122406
train_iter_loss: 0.1427592784166336
train_iter_loss: 0.24675941467285156
train_iter_loss: 0.27341076731681824
train_iter_loss: 0.2936577796936035
train_iter_loss: 0.3043724000453949
train_iter_loss: 0.28694748878479004
train_iter_loss: 0.18175320327281952
train_iter_loss: 0.21821773052215576
train_iter_loss: 0.26151859760284424
train_iter_loss: 0.2037932574748993
train_iter_loss: 0.37405675649642944
train_iter_loss: 0.24860535562038422
train_iter_loss: 0.2944936752319336
train_iter_loss: 0.3193317949771881
train_iter_loss: 0.1941375434398651
train_iter_loss: 0.3019734025001526
train_iter_loss: 0.30617421865463257
train_iter_loss: 0.309379905462265
train_iter_loss: 0.2526482939720154
train_iter_loss: 0.2683153450489044
train loss :0.2903
---------------------
Validation seg loss: 0.37848487259152364 at epoch 201
epoch =    202/  1000, exp = train
train_iter_loss: 0.383696973323822
train_iter_loss: 0.22458425164222717
train_iter_loss: 0.22620829939842224
train_iter_loss: 0.4501954913139343
train_iter_loss: 0.16977281868457794
train_iter_loss: 0.17634300887584686
train_iter_loss: 0.27144840359687805
train_iter_loss: 0.25863274931907654
train_iter_loss: 0.15041814744472504
train_iter_loss: 0.26626649498939514
train_iter_loss: 0.2683996856212616
train_iter_loss: 0.3242132365703583
train_iter_loss: 0.31875959038734436
train_iter_loss: 0.32653430104255676
train_iter_loss: 0.19000358879566193
train_iter_loss: 0.22402764856815338
train_iter_loss: 0.24377846717834473
train_iter_loss: 0.22611850500106812
train_iter_loss: 0.30531251430511475
train_iter_loss: 0.28385648131370544
train_iter_loss: 0.2216663807630539
train_iter_loss: 0.22240297496318817
train_iter_loss: 0.18795405328273773
train_iter_loss: 0.403047114610672
train_iter_loss: 0.2891906499862671
train_iter_loss: 0.27324816584587097
train_iter_loss: 0.33941566944122314
train_iter_loss: 0.3561539649963379
train_iter_loss: 0.23926514387130737
train_iter_loss: 0.319850891828537
train_iter_loss: 0.38089191913604736
train_iter_loss: 0.44221439957618713
train_iter_loss: 0.29676055908203125
train_iter_loss: 0.24341805279254913
train_iter_loss: 0.43172743916511536
train_iter_loss: 0.17565448582172394
train_iter_loss: 0.06389026343822479
train_iter_loss: 0.39308711886405945
train_iter_loss: 0.27323150634765625
train_iter_loss: 0.3897384703159332
train_iter_loss: 0.2545112073421478
train_iter_loss: 0.28360873460769653
train_iter_loss: 0.3543415367603302
train_iter_loss: 0.18917466700077057
train_iter_loss: 0.3227015435695648
train_iter_loss: 0.3250889182090759
train_iter_loss: 0.38002660870552063
train_iter_loss: 0.2948428988456726
train_iter_loss: 0.2046680748462677
train_iter_loss: 0.19468466937541962
train_iter_loss: 0.3746234178543091
train_iter_loss: 0.32129552960395813
train_iter_loss: 0.399659663438797
train_iter_loss: 0.34825193881988525
train_iter_loss: 0.31067419052124023
train_iter_loss: 0.3417149484157562
train_iter_loss: 0.255315363407135
train_iter_loss: 0.1715155988931656
train_iter_loss: 0.21549662947654724
train_iter_loss: 0.28342822194099426
train_iter_loss: 0.238515242934227
train_iter_loss: 0.3187679946422577
train_iter_loss: 0.3892265260219574
train_iter_loss: 0.35665613412857056
train_iter_loss: 0.24510458111763
train_iter_loss: 0.27940455079078674
train_iter_loss: 0.3092978000640869
train_iter_loss: 0.1933012753725052
train_iter_loss: 0.26517966389656067
train_iter_loss: 0.19798891246318817
train_iter_loss: 0.3419480323791504
train_iter_loss: 0.3804374039173126
train_iter_loss: 0.23736654222011566
train_iter_loss: 0.33979177474975586
train_iter_loss: 0.29486656188964844
train_iter_loss: 0.25492197275161743
train_iter_loss: 0.19394372403621674
train_iter_loss: 0.28882068395614624
train_iter_loss: 0.2713169753551483
train_iter_loss: 0.31509602069854736
train_iter_loss: 0.24238139390945435
train_iter_loss: 0.5382095575332642
train_iter_loss: 0.4998294711112976
train_iter_loss: 0.19023214280605316
train_iter_loss: 0.15847742557525635
train_iter_loss: 0.34297680854797363
train_iter_loss: 0.4983484745025635
train_iter_loss: 0.35837292671203613
train_iter_loss: 0.23431573808193207
train_iter_loss: 0.19699524343013763
train_iter_loss: 0.24982526898384094
train_iter_loss: 0.23008993268013
train_iter_loss: 0.347453773021698
train_iter_loss: 0.21574589610099792
train_iter_loss: 0.36160922050476074
train_iter_loss: 0.2253129631280899
train_iter_loss: 0.18089677393436432
train_iter_loss: 0.15491075813770294
train_iter_loss: 0.21450293064117432
train_iter_loss: 0.3532821238040924
train loss :0.2885
---------------------
Validation seg loss: 0.3857561580239321 at epoch 202
epoch =    203/  1000, exp = train
train_iter_loss: 0.38941794633865356
train_iter_loss: 0.42411908507347107
train_iter_loss: 0.2889631986618042
train_iter_loss: 0.3728291988372803
train_iter_loss: 0.283186674118042
train_iter_loss: 0.25264203548431396
train_iter_loss: 0.23536312580108643
train_iter_loss: 0.2919606566429138
train_iter_loss: 0.3773648142814636
train_iter_loss: 0.30653971433639526
train_iter_loss: 0.31973257660865784
train_iter_loss: 0.2932109534740448
train_iter_loss: 0.2825878858566284
train_iter_loss: 0.2569771409034729
train_iter_loss: 0.33250001072883606
train_iter_loss: 0.2487119734287262
train_iter_loss: 0.16951152682304382
train_iter_loss: 0.19159327447414398
train_iter_loss: 0.4110509753227234
train_iter_loss: 0.3826718330383301
train_iter_loss: 0.3430011570453644
train_iter_loss: 0.24429747462272644
train_iter_loss: 0.5681901574134827
train_iter_loss: 0.2515816390514374
train_iter_loss: 0.2656840980052948
train_iter_loss: 0.04756048321723938
train_iter_loss: 0.24941442906856537
train_iter_loss: 0.3055129051208496
train_iter_loss: 0.3052152991294861
train_iter_loss: 0.23420099914073944
train_iter_loss: 0.3061937689781189
train_iter_loss: 0.22816617786884308
train_iter_loss: 0.3702549636363983
train_iter_loss: 0.26053541898727417
train_iter_loss: 0.2052975445985794
train_iter_loss: 0.15240682661533356
train_iter_loss: 0.1937275379896164
train_iter_loss: 0.230575293302536
train_iter_loss: 0.34601616859436035
train_iter_loss: 0.4210948348045349
train_iter_loss: 0.30263829231262207
train_iter_loss: 0.21759088337421417
train_iter_loss: 0.31334179639816284
train_iter_loss: 0.46514296531677246
train_iter_loss: 0.3803686499595642
train_iter_loss: 0.24033603072166443
train_iter_loss: 0.29125723242759705
train_iter_loss: 0.28468582034111023
train_iter_loss: 0.3575693368911743
train_iter_loss: 0.21198096871376038
train_iter_loss: 0.25820764899253845
train_iter_loss: 0.3476710319519043
train_iter_loss: 0.24268503487110138
train_iter_loss: 0.26646894216537476
train_iter_loss: 0.25218889117240906
train_iter_loss: 0.2630380094051361
train_iter_loss: 0.23729883134365082
train_iter_loss: 0.3009334206581116
train_iter_loss: 0.2758331596851349
train_iter_loss: 0.27529728412628174
train_iter_loss: 0.32204994559288025
train_iter_loss: 0.35675397515296936
train_iter_loss: 0.2663370370864868
train_iter_loss: 0.23450444638729095
train_iter_loss: 0.1541425734758377
train_iter_loss: 0.2141929417848587
train_iter_loss: 0.2990703284740448
train_iter_loss: 0.369966059923172
train_iter_loss: 0.2897227704524994
train_iter_loss: 0.15858221054077148
train_iter_loss: 0.23463021218776703
train_iter_loss: 0.47052356600761414
train_iter_loss: 0.5066782832145691
train_iter_loss: 0.3061997592449188
train_iter_loss: 0.33681827783584595
train_iter_loss: 0.3062598407268524
train_iter_loss: 0.2650194466114044
train_iter_loss: 0.35864347219467163
train_iter_loss: 0.3664216995239258
train_iter_loss: 0.31511127948760986
train_iter_loss: 0.16880208253860474
train_iter_loss: 0.36246904730796814
train_iter_loss: 0.15388640761375427
train_iter_loss: 0.4611281752586365
train_iter_loss: 0.23768770694732666
train_iter_loss: 0.22040210664272308
train_iter_loss: 0.4144802987575531
train_iter_loss: 0.261533260345459
train_iter_loss: 0.3360329270362854
train_iter_loss: 0.29966703057289124
train_iter_loss: 0.14415355026721954
train_iter_loss: 0.17268182337284088
train_iter_loss: 0.15997670590877533
train_iter_loss: 0.23342002928256989
train_iter_loss: 0.3040497899055481
train_iter_loss: 0.19935747981071472
train_iter_loss: 0.39711251854896545
train_iter_loss: 0.3216443657875061
train_iter_loss: 0.2547459900379181
train_iter_loss: 0.21600890159606934
train loss :0.2916
---------------------
Validation seg loss: 0.39582610634630017 at epoch 203
epoch =    204/  1000, exp = train
train_iter_loss: 0.20158927142620087
train_iter_loss: 0.2704969644546509
train_iter_loss: 0.21050557494163513
train_iter_loss: 0.3915344774723053
train_iter_loss: 0.3254046142101288
train_iter_loss: 0.3478618264198303
train_iter_loss: 0.34701061248779297
train_iter_loss: 0.3606497347354889
train_iter_loss: 0.31890198588371277
train_iter_loss: 0.3427889943122864
train_iter_loss: 0.26390591263771057
train_iter_loss: 0.13762524724006653
train_iter_loss: 0.40578198432922363
train_iter_loss: 0.19941695034503937
train_iter_loss: 0.24787943065166473
train_iter_loss: 0.1781962364912033
train_iter_loss: 0.4493647813796997
train_iter_loss: 0.6017819046974182
train_iter_loss: 0.29464301466941833
train_iter_loss: 0.26139378547668457
train_iter_loss: 0.3568766713142395
train_iter_loss: 0.32559311389923096
train_iter_loss: 0.25806984305381775
train_iter_loss: 0.401718407869339
train_iter_loss: 0.46639373898506165
train_iter_loss: 0.2050127387046814
train_iter_loss: 0.20257656276226044
train_iter_loss: 0.36215564608573914
train_iter_loss: 0.40650296211242676
train_iter_loss: 0.30855774879455566
train_iter_loss: 0.2720147669315338
train_iter_loss: 0.3619944751262665
train_iter_loss: 0.3246136009693146
train_iter_loss: 0.29683560132980347
train_iter_loss: 0.1869291514158249
train_iter_loss: 0.28728482127189636
train_iter_loss: 0.19279450178146362
train_iter_loss: 0.4045841693878174
train_iter_loss: 0.24852998554706573
train_iter_loss: 0.41629624366760254
train_iter_loss: 0.15864744782447815
train_iter_loss: 0.2231670320034027
train_iter_loss: 0.24583929777145386
train_iter_loss: 0.20332734286785126
train_iter_loss: 0.336037278175354
train_iter_loss: 0.2836909592151642
train_iter_loss: 0.3268490433692932
train_iter_loss: 0.19954819977283478
train_iter_loss: 0.39499855041503906
train_iter_loss: 0.4118742048740387
train_iter_loss: 0.23918449878692627
train_iter_loss: 0.31814849376678467
train_iter_loss: 0.33777305483818054
train_iter_loss: 0.3030378222465515
train_iter_loss: 0.47625938057899475
train_iter_loss: 0.12621928751468658
train_iter_loss: 0.27814745903015137
train_iter_loss: 0.39363789558410645
train_iter_loss: 0.3093739449977875
train_iter_loss: 0.20854119956493378
train_iter_loss: 0.2052597850561142
train_iter_loss: 0.4183591604232788
train_iter_loss: 0.3013201355934143
train_iter_loss: 0.27802014350891113
train_iter_loss: 0.23272940516471863
train_iter_loss: 0.25754573941230774
train_iter_loss: 0.12748149037361145
train_iter_loss: 0.33456069231033325
train_iter_loss: 0.1988655924797058
train_iter_loss: 0.22068490087985992
train_iter_loss: 0.3664287328720093
train_iter_loss: 0.22404110431671143
train_iter_loss: 0.2145940512418747
train_iter_loss: 0.2933783233165741
train_iter_loss: 0.14133334159851074
train_iter_loss: 0.21476082503795624
train_iter_loss: 0.25863316655158997
train_iter_loss: 0.2737865447998047
train_iter_loss: 0.3133279085159302
train_iter_loss: 0.1967637538909912
train_iter_loss: 0.3143901228904724
train_iter_loss: 0.3469691574573517
train_iter_loss: 0.28434932231903076
train_iter_loss: 0.2728245258331299
train_iter_loss: 0.23956985771656036
train_iter_loss: 0.3870493173599243
train_iter_loss: 0.2855762541294098
train_iter_loss: 0.46957215666770935
train_iter_loss: 0.2470736801624298
train_iter_loss: 0.13360807299613953
train_iter_loss: 0.19440896809101105
train_iter_loss: 0.3411097228527069
train_iter_loss: 0.2633965015411377
train_iter_loss: 0.19930128753185272
train_iter_loss: 0.2255866676568985
train_iter_loss: 0.36874377727508545
train_iter_loss: 0.225758358836174
train_iter_loss: 0.2452571541070938
train_iter_loss: 0.19309763610363007
train_iter_loss: 0.3812783360481262
train loss :0.2920
---------------------
Validation seg loss: 0.36851946000923524 at epoch 204
epoch =    205/  1000, exp = train
train_iter_loss: 0.1814182549715042
train_iter_loss: 0.23590721189975739
train_iter_loss: 0.20173053443431854
train_iter_loss: 0.24423101544380188
train_iter_loss: 0.24351048469543457
train_iter_loss: 0.2915469706058502
train_iter_loss: 0.35523852705955505
train_iter_loss: 0.178518146276474
train_iter_loss: 0.5874527096748352
train_iter_loss: 0.3363095223903656
train_iter_loss: 0.23812001943588257
train_iter_loss: 0.2505417466163635
train_iter_loss: 0.2790367305278778
train_iter_loss: 0.18299764394760132
train_iter_loss: 0.13381950557231903
train_iter_loss: 0.3621746301651001
train_iter_loss: 0.4077434241771698
train_iter_loss: 0.36101818084716797
train_iter_loss: 0.2988380789756775
train_iter_loss: 0.477439284324646
train_iter_loss: 0.3244371712207794
train_iter_loss: 0.3344903588294983
train_iter_loss: 0.5020768642425537
train_iter_loss: 0.25670838356018066
train_iter_loss: 0.32822343707084656
train_iter_loss: 0.2941266894340515
train_iter_loss: 0.230101078748703
train_iter_loss: 0.1787502020597458
train_iter_loss: 0.3760184943675995
train_iter_loss: 0.13278314471244812
train_iter_loss: 0.245254784822464
train_iter_loss: 0.17377246916294098
train_iter_loss: 0.3567913770675659
train_iter_loss: 0.3924652636051178
train_iter_loss: 0.1757553517818451
train_iter_loss: 0.35000181198120117
train_iter_loss: 0.16488558053970337
train_iter_loss: 0.22100728750228882
train_iter_loss: 0.3543908894062042
train_iter_loss: 0.28184983134269714
train_iter_loss: 0.40789568424224854
train_iter_loss: 0.26841968297958374
train_iter_loss: 0.2794003486633301
train_iter_loss: 0.31575706601142883
train_iter_loss: 0.30735957622528076
train_iter_loss: 0.18985876441001892
train_iter_loss: 0.35010048747062683
train_iter_loss: 0.21752582490444183
train_iter_loss: 0.34318163990974426
train_iter_loss: 0.3020491898059845
train_iter_loss: 0.34424328804016113
train_iter_loss: 0.2122945934534073
train_iter_loss: 0.18584983050823212
train_iter_loss: 0.377739280462265
train_iter_loss: 0.37235715985298157
train_iter_loss: 0.2504497766494751
train_iter_loss: 0.08881327509880066
train_iter_loss: 0.19857682287693024
train_iter_loss: 0.251268208026886
train_iter_loss: 0.32033592462539673
train_iter_loss: 0.29254525899887085
train_iter_loss: 0.2357778698205948
train_iter_loss: 0.25132179260253906
train_iter_loss: 0.33070501685142517
train_iter_loss: 0.18760022521018982
train_iter_loss: 0.3767051100730896
train_iter_loss: 0.4087483882904053
train_iter_loss: 0.2864235043525696
train_iter_loss: 0.26571157574653625
train_iter_loss: 0.183207169175148
train_iter_loss: 0.3628222644329071
train_iter_loss: 0.21079932153224945
train_iter_loss: 0.2224973440170288
train_iter_loss: 0.29901060461997986
train_iter_loss: 0.24768176674842834
train_iter_loss: 0.30365777015686035
train_iter_loss: 0.3833203911781311
train_iter_loss: 0.2823282480239868
train_iter_loss: 0.3146328926086426
train_iter_loss: 0.3192286789417267
train_iter_loss: 0.20709733664989471
train_iter_loss: 0.32323479652404785
train_iter_loss: 0.26523497700691223
train_iter_loss: 0.32251957058906555
train_iter_loss: 0.27811360359191895
train_iter_loss: 0.284364253282547
train_iter_loss: 0.3612491488456726
train_iter_loss: 0.16349855065345764
train_iter_loss: 0.2449926882982254
train_iter_loss: 0.3991740643978119
train_iter_loss: 0.3406316041946411
train_iter_loss: 0.2809584438800812
train_iter_loss: 0.29877614974975586
train_iter_loss: 0.1390659511089325
train_iter_loss: 0.20498406887054443
train_iter_loss: 0.3571529984474182
train_iter_loss: 0.1267237514257431
train_iter_loss: 0.2337871938943863
train_iter_loss: 0.31160715222358704
train_iter_loss: 0.30540016293525696
train loss :0.2861
---------------------
Validation seg loss: 0.37898788009858075 at epoch 205
epoch =    206/  1000, exp = train
train_iter_loss: 0.3596675395965576
train_iter_loss: 0.17809882760047913
train_iter_loss: 0.30123376846313477
train_iter_loss: 0.21399013698101044
train_iter_loss: 0.20139648020267487
train_iter_loss: 0.37086156010627747
train_iter_loss: 0.13402095437049866
train_iter_loss: 0.5690621137619019
train_iter_loss: 0.2868604362010956
train_iter_loss: 0.28344473242759705
train_iter_loss: 0.27593180537223816
train_iter_loss: 0.2334400713443756
train_iter_loss: 0.5542704463005066
train_iter_loss: 0.23900361359119415
train_iter_loss: 0.15840816497802734
train_iter_loss: 0.23819737136363983
train_iter_loss: 0.18078428506851196
train_iter_loss: 0.5809493660926819
train_iter_loss: 0.195250004529953
train_iter_loss: 0.24548450112342834
train_iter_loss: 0.29545095562934875
train_iter_loss: 0.3897916376590729
train_iter_loss: 0.3955729305744171
train_iter_loss: 0.11455640196800232
train_iter_loss: 0.20754079520702362
train_iter_loss: 0.1921817809343338
train_iter_loss: 0.42523542046546936
train_iter_loss: 0.29987841844558716
train_iter_loss: 0.29902634024620056
train_iter_loss: 0.15390901267528534
train_iter_loss: 0.13629047572612762
train_iter_loss: 0.1550743281841278
train_iter_loss: 0.32697057723999023
train_iter_loss: 0.4259607493877411
train_iter_loss: 0.34232431650161743
train_iter_loss: 0.22892701625823975
train_iter_loss: 0.3748534917831421
train_iter_loss: 0.34512269496917725
train_iter_loss: 0.4679250717163086
train_iter_loss: 0.22961124777793884
train_iter_loss: 0.3223957121372223
train_iter_loss: 0.24117769300937653
train_iter_loss: 0.2718442678451538
train_iter_loss: 0.14569807052612305
train_iter_loss: 0.2476048767566681
train_iter_loss: 0.17400053143501282
train_iter_loss: 0.28238359093666077
train_iter_loss: 0.25550273060798645
train_iter_loss: 0.1826924979686737
train_iter_loss: 0.46239838004112244
train_iter_loss: 0.2665083706378937
train_iter_loss: 0.29850664734840393
train_iter_loss: 0.40846577286720276
train_iter_loss: 0.2696109712123871
train_iter_loss: 0.25830116868019104
train_iter_loss: 0.3312164545059204
train_iter_loss: 0.3346286714076996
train_iter_loss: 0.2461000382900238
train_iter_loss: 0.23634935915470123
train_iter_loss: 0.25897929072380066
train_iter_loss: 0.3371407985687256
train_iter_loss: 0.25514739751815796
train_iter_loss: 0.33413824439048767
train_iter_loss: 0.2903435528278351
train_iter_loss: 0.1955796331167221
train_iter_loss: 0.2772517204284668
train_iter_loss: 0.17218992114067078
train_iter_loss: 0.2811129093170166
train_iter_loss: 0.27507150173187256
train_iter_loss: 0.35995545983314514
train_iter_loss: 0.2837284803390503
train_iter_loss: 0.2784520089626312
train_iter_loss: 0.1708194762468338
train_iter_loss: 0.23802490532398224
train_iter_loss: 0.12952972948551178
train_iter_loss: 0.2993714213371277
train_iter_loss: 0.30990710854530334
train_iter_loss: 0.23411305248737335
train_iter_loss: 0.185815691947937
train_iter_loss: 0.2447395771741867
train_iter_loss: 0.20266282558441162
train_iter_loss: 0.28561827540397644
train_iter_loss: 0.2521589398384094
train_iter_loss: 0.2831583619117737
train_iter_loss: 0.16982336342334747
train_iter_loss: 0.3024290204048157
train_iter_loss: 0.5998590588569641
train_iter_loss: 0.22083233296871185
train_iter_loss: 0.2230922281742096
train_iter_loss: 0.37172621488571167
train_iter_loss: 0.3655121922492981
train_iter_loss: 0.2620267868041992
train_iter_loss: 0.3688521087169647
train_iter_loss: 0.21486307680606842
train_iter_loss: 0.48793017864227295
train_iter_loss: 0.30972135066986084
train_iter_loss: 0.25414204597473145
train_iter_loss: 0.4491516649723053
train_iter_loss: 0.23690272867679596
train_iter_loss: 0.3370150029659271
train loss :0.2874
---------------------
Validation seg loss: 0.40503285090737745 at epoch 206
epoch =    207/  1000, exp = train
train_iter_loss: 0.41665124893188477
train_iter_loss: 0.16559246182441711
train_iter_loss: 0.2215585708618164
train_iter_loss: 0.36730337142944336
train_iter_loss: 0.4792912006378174
train_iter_loss: 0.32876116037368774
train_iter_loss: 0.45088139176368713
train_iter_loss: 0.2671043872833252
train_iter_loss: 0.17294107377529144
train_iter_loss: 0.383909672498703
train_iter_loss: 0.3057588040828705
train_iter_loss: 0.325484037399292
train_iter_loss: 0.31632253527641296
train_iter_loss: 0.286445677280426
train_iter_loss: 0.37795042991638184
train_iter_loss: 0.25163134932518005
train_iter_loss: 0.2882002294063568
train_iter_loss: 0.211399644613266
train_iter_loss: 0.25935813784599304
train_iter_loss: 0.32061201333999634
train_iter_loss: 0.19309917092323303
train_iter_loss: 0.3219042420387268
train_iter_loss: 0.4117763936519623
train_iter_loss: 0.2561623156070709
train_iter_loss: 0.2639968693256378
train_iter_loss: 0.3989174962043762
train_iter_loss: 0.25168082118034363
train_iter_loss: 0.27608323097229004
train_iter_loss: 0.3399535119533539
train_iter_loss: 0.3625589609146118
train_iter_loss: 0.2621440291404724
train_iter_loss: 0.23328736424446106
train_iter_loss: 0.2933730185031891
train_iter_loss: 0.27986806631088257
train_iter_loss: 0.38213780522346497
train_iter_loss: 0.3112086355686188
train_iter_loss: 0.31967198848724365
train_iter_loss: 0.24193963408470154
train_iter_loss: 0.33723121881484985
train_iter_loss: 0.30742737650871277
train_iter_loss: 0.3211340606212616
train_iter_loss: 0.2624718248844147
train_iter_loss: 0.24566112458705902
train_iter_loss: 0.2749662697315216
train_iter_loss: 0.4591527283191681
train_iter_loss: 0.27227842807769775
train_iter_loss: 0.25014203786849976
train_iter_loss: 0.2919740676879883
train_iter_loss: 0.4375481903553009
train_iter_loss: 0.34829869866371155
train_iter_loss: 0.21003791689872742
train_iter_loss: 0.5028618574142456
train_iter_loss: 0.3841179609298706
train_iter_loss: 0.35298627614974976
train_iter_loss: 0.38881656527519226
train_iter_loss: 0.4167255759239197
train_iter_loss: 0.2870875895023346
train_iter_loss: 0.2861371338367462
train_iter_loss: 0.2946193814277649
train_iter_loss: 0.2670552432537079
train_iter_loss: 0.4100973308086395
train_iter_loss: 0.21037080883979797
train_iter_loss: 0.3100986182689667
train_iter_loss: 0.27585723996162415
train_iter_loss: 0.32272666692733765
train_iter_loss: 0.3058153986930847
train_iter_loss: 0.2994217276573181
train_iter_loss: 0.2346671223640442
train_iter_loss: 0.1688072383403778
train_iter_loss: 0.2764708697795868
train_iter_loss: 0.366121381521225
train_iter_loss: 0.32727304100990295
train_iter_loss: 0.3144897520542145
train_iter_loss: 0.24671612679958344
train_iter_loss: 0.296490341424942
train_iter_loss: 0.2341010570526123
train_iter_loss: 0.369888573884964
train_iter_loss: 0.2610238194465637
train_iter_loss: 0.09190673381090164
train_iter_loss: 0.23960429430007935
train_iter_loss: 0.25414425134658813
train_iter_loss: 0.2921563386917114
train_iter_loss: 0.20974242687225342
train_iter_loss: 0.2802956998348236
train_iter_loss: 0.14538735151290894
train_iter_loss: 0.3601575195789337
train_iter_loss: 0.16477824747562408
train_iter_loss: 0.1701410859823227
train_iter_loss: 0.26890960335731506
train_iter_loss: 0.3068549633026123
train_iter_loss: 0.26388078927993774
train_iter_loss: 0.27723634243011475
train_iter_loss: 0.3127918541431427
train_iter_loss: 0.24967873096466064
train_iter_loss: 0.2538936138153076
train_iter_loss: 0.23307989537715912
train_iter_loss: 0.30154913663864136
train_iter_loss: 0.1400536745786667
train_iter_loss: 0.23075896501541138
train_iter_loss: 0.3138097822666168
train loss :0.2967
---------------------
Validation seg loss: 0.37515002131497244 at epoch 207
epoch =    208/  1000, exp = train
train_iter_loss: 0.25418975949287415
train_iter_loss: 0.374891459941864
train_iter_loss: 0.4430886507034302
train_iter_loss: 0.435788631439209
train_iter_loss: 0.3083350956439972
train_iter_loss: 0.24563691020011902
train_iter_loss: 0.2956121861934662
train_iter_loss: 0.16223956644535065
train_iter_loss: 0.3132142424583435
train_iter_loss: 0.4310583472251892
train_iter_loss: 0.23670899868011475
train_iter_loss: 0.18789418041706085
train_iter_loss: 0.3346593379974365
train_iter_loss: 0.3736967146396637
train_iter_loss: 0.3570597767829895
train_iter_loss: 0.12058240920305252
train_iter_loss: 0.18540538847446442
train_iter_loss: 0.25027263164520264
train_iter_loss: 0.36311259865760803
train_iter_loss: 0.20500409603118896
train_iter_loss: 0.30323901772499084
train_iter_loss: 0.08346710354089737
train_iter_loss: 0.21597035229206085
train_iter_loss: 0.37227314710617065
train_iter_loss: 0.3117550313472748
train_iter_loss: 0.371040403842926
train_iter_loss: 0.23347796499729156
train_iter_loss: 0.33811286091804504
train_iter_loss: 0.3896917402744293
train_iter_loss: 0.2654635012149811
train_iter_loss: 0.22677840292453766
train_iter_loss: 0.2701707184314728
train_iter_loss: 0.5589771270751953
train_iter_loss: 0.13830068707466125
train_iter_loss: 0.28940239548683167
train_iter_loss: 0.32732242345809937
train_iter_loss: 0.25906801223754883
train_iter_loss: 0.3067376911640167
train_iter_loss: 0.299489825963974
train_iter_loss: 0.12105194479227066
train_iter_loss: 0.29083865880966187
train_iter_loss: 0.12425924092531204
train_iter_loss: 0.34468281269073486
train_iter_loss: 0.2811848223209381
train_iter_loss: 0.10563545674085617
train_iter_loss: 0.23328574001789093
train_iter_loss: 0.1532081812620163
train_iter_loss: 0.21473972499370575
train_iter_loss: 0.3595196604728699
train_iter_loss: 0.37850236892700195
train_iter_loss: 0.3026804029941559
train_iter_loss: 0.39351749420166016
train_iter_loss: 0.30826297402381897
train_iter_loss: 0.15746337175369263
train_iter_loss: 0.23649939894676208
train_iter_loss: 0.20400866866111755
train_iter_loss: 0.3509046137332916
train_iter_loss: 0.31646421551704407
train_iter_loss: 0.35986992716789246
train_iter_loss: 0.17447951436042786
train_iter_loss: 0.26896482706069946
train_iter_loss: 0.16349206864833832
train_iter_loss: 0.282137006521225
train_iter_loss: 0.34628209471702576
train_iter_loss: 0.23295480012893677
train_iter_loss: 0.368948757648468
train_iter_loss: 0.28482478857040405
train_iter_loss: 0.15813417732715607
train_iter_loss: 0.20124636590480804
train_iter_loss: 0.28998929262161255
train_iter_loss: 0.2533624470233917
train_iter_loss: 0.30160295963287354
train_iter_loss: 0.375761479139328
train_iter_loss: 0.21941743791103363
train_iter_loss: 0.3173561990261078
train_iter_loss: 0.40994301438331604
train_iter_loss: 0.34783700108528137
train_iter_loss: 0.44773849844932556
train_iter_loss: 0.38091984391212463
train_iter_loss: 0.24233704805374146
train_iter_loss: 0.17334428429603577
train_iter_loss: 0.1947348266839981
train_iter_loss: 0.3017631471157074
train_iter_loss: 0.29361313581466675
train_iter_loss: 0.2719639837741852
train_iter_loss: 0.34122344851493835
train_iter_loss: 0.21669431030750275
train_iter_loss: 0.28968480229377747
train_iter_loss: 0.2705724835395813
train_iter_loss: 0.2779967188835144
train_iter_loss: 0.34576770663261414
train_iter_loss: 0.3145720064640045
train_iter_loss: 0.2723483443260193
train_iter_loss: 0.36091816425323486
train_iter_loss: 0.15709958970546722
train_iter_loss: 0.3361327350139618
train_iter_loss: 0.38499611616134644
train_iter_loss: 0.3315666615962982
train_iter_loss: 0.3559676706790924
train_iter_loss: 0.34607112407684326
train loss :0.2888
---------------------
Validation seg loss: 0.36391222223920644 at epoch 208
epoch =    209/  1000, exp = train
train_iter_loss: 0.20580075681209564
train_iter_loss: 0.406292587518692
train_iter_loss: 0.26242536306381226
train_iter_loss: 0.22840750217437744
train_iter_loss: 0.15005727112293243
train_iter_loss: 0.3655035197734833
train_iter_loss: 0.11552897840738297
train_iter_loss: 0.2583410143852234
train_iter_loss: 0.31642305850982666
train_iter_loss: 0.31290721893310547
train_iter_loss: 0.24351173639297485
train_iter_loss: 0.26082557439804077
train_iter_loss: 0.44075557589530945
train_iter_loss: 0.10925518721342087
train_iter_loss: 0.5951906442642212
train_iter_loss: 0.3272770047187805
train_iter_loss: 0.2424759417772293
train_iter_loss: 0.3939271569252014
train_iter_loss: 0.2555120587348938
train_iter_loss: 0.34607601165771484
train_iter_loss: 0.4337422847747803
train_iter_loss: 0.3603348731994629
train_iter_loss: 0.21184413135051727
train_iter_loss: 0.16620084643363953
train_iter_loss: 0.2448219656944275
train_iter_loss: 0.18475686013698578
train_iter_loss: 0.3969712555408478
train_iter_loss: 0.25797852873802185
train_iter_loss: 0.39785975217819214
train_iter_loss: 0.4026547372341156
train_iter_loss: 0.4234527349472046
train_iter_loss: 0.15234071016311646
train_iter_loss: 0.22258290648460388
train_iter_loss: 0.2893102765083313
train_iter_loss: 0.44956865906715393
train_iter_loss: 0.2597963213920593
train_iter_loss: 0.39084944128990173
train_iter_loss: 0.23153507709503174
train_iter_loss: 0.3499656021595001
train_iter_loss: 0.15586765110492706
train_iter_loss: 0.221986323595047
train_iter_loss: 0.24307319521903992
train_iter_loss: 0.2562515139579773
train_iter_loss: 0.2939775884151459
train_iter_loss: 0.19977973401546478
train_iter_loss: 0.3530068099498749
train_iter_loss: 0.270124226808548
train_iter_loss: 0.3457238972187042
train_iter_loss: 0.2513057589530945
train_iter_loss: 0.35586434602737427
train_iter_loss: 0.3085406720638275
train_iter_loss: 0.4045322835445404
train_iter_loss: 0.24811454117298126
train_iter_loss: 0.35199445486068726
train_iter_loss: 0.27885523438453674
train_iter_loss: 0.33526092767715454
train_iter_loss: 0.18451343476772308
train_iter_loss: 0.15414120256900787
train_iter_loss: 0.2683049738407135
train_iter_loss: 0.43340781331062317
train_iter_loss: 0.4129115641117096
train_iter_loss: 0.275966614484787
train_iter_loss: 0.1610923558473587
train_iter_loss: 0.2716105282306671
train_iter_loss: 0.4434315860271454
train_iter_loss: 0.3687797784805298
train_iter_loss: 0.14834946393966675
train_iter_loss: 0.40083038806915283
train_iter_loss: 0.401601105928421
train_iter_loss: 0.2627391815185547
train_iter_loss: 0.29932069778442383
train_iter_loss: 0.2909540832042694
train_iter_loss: 0.1327393651008606
train_iter_loss: 0.34799209237098694
train_iter_loss: 0.1827634871006012
train_iter_loss: 0.3867056667804718
train_iter_loss: 0.2726788818836212
train_iter_loss: 0.3410908579826355
train_iter_loss: 0.23040473461151123
train_iter_loss: 0.15785184502601624
train_iter_loss: 0.3318815529346466
train_iter_loss: 0.212468683719635
train_iter_loss: 0.11078717559576035
train_iter_loss: 0.30610227584838867
train_iter_loss: 0.3469405174255371
train_iter_loss: 0.14639586210250854
train_iter_loss: 0.33181577920913696
train_iter_loss: 0.36232858896255493
train_iter_loss: 0.29410216212272644
train_iter_loss: 0.21625934541225433
train_iter_loss: 0.3342442214488983
train_iter_loss: 0.26913702487945557
train_iter_loss: 0.26601579785346985
train_iter_loss: 0.22009973227977753
train_iter_loss: 0.3875463306903839
train_iter_loss: 0.2640227973461151
train_iter_loss: 0.17397740483283997
train_iter_loss: 0.23899054527282715
train_iter_loss: 0.24831533432006836
train_iter_loss: 0.2967558801174164
train loss :0.2901
---------------------
Validation seg loss: 0.38312434378730237 at epoch 209
epoch =    210/  1000, exp = train
train_iter_loss: 0.24263522028923035
train_iter_loss: 0.17008233070373535
train_iter_loss: 0.203854501247406
train_iter_loss: 0.43358829617500305
train_iter_loss: 0.34623512625694275
train_iter_loss: 0.1935918778181076
train_iter_loss: 0.2705635726451874
train_iter_loss: 0.2413165122270584
train_iter_loss: 0.18642352521419525
train_iter_loss: 0.45889508724212646
train_iter_loss: 0.3414674997329712
train_iter_loss: 0.28743234276771545
train_iter_loss: 0.43767979741096497
train_iter_loss: 0.6418251991271973
train_iter_loss: 0.20038865506649017
train_iter_loss: 0.30578356981277466
train_iter_loss: 0.18686699867248535
train_iter_loss: 0.43339699506759644
train_iter_loss: 0.21595607697963715
train_iter_loss: 0.48826155066490173
train_iter_loss: 0.27443671226501465
train_iter_loss: 0.10077837854623795
train_iter_loss: 0.4197957515716553
train_iter_loss: 0.19238001108169556
train_iter_loss: 0.3501741290092468
train_iter_loss: 0.18601791560649872
train_iter_loss: 0.3545229136943817
train_iter_loss: 0.3649761378765106
train_iter_loss: 0.2375909984111786
train_iter_loss: 0.3174561858177185
train_iter_loss: 0.31744301319122314
train_iter_loss: 0.16572237014770508
train_iter_loss: 0.33443257212638855
train_iter_loss: 0.26354679465293884
train_iter_loss: 0.2783201336860657
train_iter_loss: 0.3180561065673828
train_iter_loss: 0.253768652677536
train_iter_loss: 0.17812322080135345
train_iter_loss: 0.3008725345134735
train_iter_loss: 0.3038656413555145
train_iter_loss: 0.35370421409606934
train_iter_loss: 0.37422308325767517
train_iter_loss: 0.3442544639110565
train_iter_loss: 0.37501466274261475
train_iter_loss: 0.14812317490577698
train_iter_loss: 0.24369169771671295
train_iter_loss: 0.49021196365356445
train_iter_loss: 0.2378988265991211
train_iter_loss: 0.2645467519760132
train_iter_loss: 0.4349590241909027
train_iter_loss: 0.2382144033908844
train_iter_loss: 0.23307494819164276
train_iter_loss: 0.2129095494747162
train_iter_loss: 0.4023383557796478
train_iter_loss: 0.5221571326255798
train_iter_loss: 0.24109189212322235
train_iter_loss: 0.2872941792011261
train_iter_loss: 0.14668092131614685
train_iter_loss: 0.24422454833984375
train_iter_loss: 0.1709175705909729
train_iter_loss: 0.334127277135849
train_iter_loss: 0.12225140631198883
train_iter_loss: 0.21990597248077393
train_iter_loss: 0.5764076113700867
train_iter_loss: 0.4228035807609558
train_iter_loss: 0.4072006940841675
train_iter_loss: 0.1689624935388565
train_iter_loss: 0.2884446680545807
train_iter_loss: 0.29306310415267944
train_iter_loss: 0.26084062457084656
train_iter_loss: 0.17970752716064453
train_iter_loss: 0.2328496277332306
train_iter_loss: 0.290628582239151
train_iter_loss: 0.2725989818572998
train_iter_loss: 0.3647611141204834
train_iter_loss: 0.2628589868545532
train_iter_loss: 0.16683505475521088
train_iter_loss: 0.4167279005050659
train_iter_loss: 0.24824389815330505
train_iter_loss: 0.25145429372787476
train_iter_loss: 0.35765042901039124
train_iter_loss: 0.24885441362857819
train_iter_loss: 0.3168342709541321
train_iter_loss: 0.2655496299266815
train_iter_loss: 0.09632457792758942
train_iter_loss: 0.24275310337543488
train_iter_loss: 0.28714022040367126
train_iter_loss: 0.2466522455215454
train_iter_loss: 0.3285689353942871
train_iter_loss: 0.21876148879528046
train_iter_loss: 0.16204844415187836
train_iter_loss: 0.3260789215564728
train_iter_loss: 0.371955931186676
train_iter_loss: 0.2054210603237152
train_iter_loss: 0.3699619472026825
train_iter_loss: 0.3757910132408142
train_iter_loss: 0.3045307993888855
train_iter_loss: 0.5122641324996948
train_iter_loss: 0.20837460458278656
train_iter_loss: 0.26444873213768005
train loss :0.2954
---------------------
Validation seg loss: 0.3768082859943498 at epoch 210
epoch =    211/  1000, exp = train
train_iter_loss: 0.22296126186847687
train_iter_loss: 0.25003957748413086
train_iter_loss: 0.21070551872253418
train_iter_loss: 0.23097707331180573
train_iter_loss: 0.4499191343784332
train_iter_loss: 0.3235319256782532
train_iter_loss: 0.2814755141735077
train_iter_loss: 0.4115198850631714
train_iter_loss: 0.2645134925842285
train_iter_loss: 0.32008007168769836
train_iter_loss: 0.3163550794124603
train_iter_loss: 0.2646994888782501
train_iter_loss: 0.26686373353004456
train_iter_loss: 0.3484843671321869
train_iter_loss: 0.24767759442329407
train_iter_loss: 0.2369471788406372
train_iter_loss: 0.35760417580604553
train_iter_loss: 0.21905891597270966
train_iter_loss: 0.4095151722431183
train_iter_loss: 0.2100660800933838
train_iter_loss: 0.30079910159111023
train_iter_loss: 0.14517830312252045
train_iter_loss: 0.21199437975883484
train_iter_loss: 0.3226761817932129
train_iter_loss: 0.23954005539417267
train_iter_loss: 0.31771519780158997
train_iter_loss: 0.21028181910514832
train_iter_loss: 0.5039747357368469
train_iter_loss: 0.3409230411052704
train_iter_loss: 0.29451629519462585
train_iter_loss: 0.19641602039337158
train_iter_loss: 0.2204582244157791
train_iter_loss: 0.6080514192581177
train_iter_loss: 0.2481614202260971
train_iter_loss: 0.16500678658485413
train_iter_loss: 0.44733864068984985
train_iter_loss: 0.21887239813804626
train_iter_loss: 0.12847790122032166
train_iter_loss: 0.28525492548942566
train_iter_loss: 0.12415530532598495
train_iter_loss: 0.4032982587814331
train_iter_loss: 0.21547524631023407
train_iter_loss: 0.2204541563987732
train_iter_loss: 0.36706212162971497
train_iter_loss: 0.28313499689102173
train_iter_loss: 0.37403231859207153
train_iter_loss: 0.22364269196987152
train_iter_loss: 0.17225652933120728
train_iter_loss: 0.309530109167099
train_iter_loss: 0.29698503017425537
train_iter_loss: 0.20642369985580444
train_iter_loss: 0.1859576553106308
train_iter_loss: 0.19263985753059387
train_iter_loss: 0.26966747641563416
train_iter_loss: 0.18075379729270935
train_iter_loss: 0.23009327054023743
train_iter_loss: 0.21452270448207855
train_iter_loss: 0.4501575231552124
train_iter_loss: 0.23367613554000854
train_iter_loss: 0.22822752594947815
train_iter_loss: 0.3813793659210205
train_iter_loss: 0.22190603613853455
train_iter_loss: 0.2736232578754425
train_iter_loss: 0.49495065212249756
train_iter_loss: 0.34249112010002136
train_iter_loss: 0.2556282579898834
train_iter_loss: 0.3697635233402252
train_iter_loss: 0.28194713592529297
train_iter_loss: 0.2715259790420532
train_iter_loss: 0.2566770613193512
train_iter_loss: 0.2922751307487488
train_iter_loss: 0.24624003469944
train_iter_loss: 0.3573050796985626
train_iter_loss: 0.15728794038295746
train_iter_loss: 0.2334514856338501
train_iter_loss: 0.15872181951999664
train_iter_loss: 0.31018391251564026
train_iter_loss: 0.2386198788881302
train_iter_loss: 0.3107353746891022
train_iter_loss: 0.40141040086746216
train_iter_loss: 0.2906615734100342
train_iter_loss: 0.3307037651538849
train_iter_loss: 0.3120289444923401
train_iter_loss: 0.12055851519107819
train_iter_loss: 0.27712705731391907
train_iter_loss: 0.37843069434165955
train_iter_loss: 0.1662604808807373
train_iter_loss: 0.26254168152809143
train_iter_loss: 0.25003349781036377
train_iter_loss: 0.22116422653198242
train_iter_loss: 0.35726627707481384
train_iter_loss: 0.503569483757019
train_iter_loss: 0.22224700450897217
train_iter_loss: 0.2586108446121216
train_iter_loss: 0.37967705726623535
train_iter_loss: 0.40613487362861633
train_iter_loss: 0.31215518712997437
train_iter_loss: 0.33616751432418823
train_iter_loss: 0.49583321809768677
train_iter_loss: 0.3004833459854126
train loss :0.2896
---------------------
Validation seg loss: 0.3713098066981952 at epoch 211
epoch =    212/  1000, exp = train
train_iter_loss: 0.3376583456993103
train_iter_loss: 0.3237895667552948
train_iter_loss: 0.24513313174247742
train_iter_loss: 0.4363795816898346
train_iter_loss: 0.17096742987632751
train_iter_loss: 0.3318154811859131
train_iter_loss: 0.12321887165307999
train_iter_loss: 0.3534303605556488
train_iter_loss: 0.2793497145175934
train_iter_loss: 0.2815139591693878
train_iter_loss: 0.3533424735069275
train_iter_loss: 0.4307897090911865
train_iter_loss: 0.3864036500453949
train_iter_loss: 0.34048977494239807
train_iter_loss: 0.2597424387931824
train_iter_loss: 0.2575196921825409
train_iter_loss: 0.3634570240974426
train_iter_loss: 0.3084082007408142
train_iter_loss: 0.28715240955352783
train_iter_loss: 0.18579760193824768
train_iter_loss: 0.3505743145942688
train_iter_loss: 0.4511077404022217
train_iter_loss: 0.2666913568973541
train_iter_loss: 0.4633045792579651
train_iter_loss: 0.44926583766937256
train_iter_loss: 0.47917288541793823
train_iter_loss: 0.30761438608169556
train_iter_loss: 0.19309982657432556
train_iter_loss: 0.4376099109649658
train_iter_loss: 0.25475233793258667
train_iter_loss: 0.2386651486158371
train_iter_loss: 0.24920588731765747
train_iter_loss: 0.19065676629543304
train_iter_loss: 0.34767675399780273
train_iter_loss: 0.362980455160141
train_iter_loss: 0.0651770830154419
train_iter_loss: 0.15132255852222443
train_iter_loss: 0.22830258309841156
train_iter_loss: 0.21834269165992737
train_iter_loss: 0.32573509216308594
train_iter_loss: 0.284485399723053
train_iter_loss: 0.16847391426563263
train_iter_loss: 0.2895084619522095
train_iter_loss: 0.4655333459377289
train_iter_loss: 0.1797569990158081
train_iter_loss: 0.2000935822725296
train_iter_loss: 0.3566930592060089
train_iter_loss: 0.3424378037452698
train_iter_loss: 0.14290758967399597
train_iter_loss: 0.16510210931301117
train_iter_loss: 0.24492147564888
train_iter_loss: 0.24721336364746094
train_iter_loss: 0.29542553424835205
train_iter_loss: 0.2701641023159027
train_iter_loss: 0.12389795482158661
train_iter_loss: 0.16466517746448517
train_iter_loss: 0.25153714418411255
train_iter_loss: 0.4123859703540802
train_iter_loss: 0.31927579641342163
train_iter_loss: 0.2781936228275299
train_iter_loss: 0.2949196994304657
train_iter_loss: 0.5136626958847046
train_iter_loss: 0.27230316400527954
train_iter_loss: 0.26709234714508057
train_iter_loss: 0.3437710404396057
train_iter_loss: 0.2043749988079071
train_iter_loss: 0.1910085678100586
train_iter_loss: 0.35008683800697327
train_iter_loss: 0.13454553484916687
train_iter_loss: 0.22384494543075562
train_iter_loss: 0.2537171542644501
train_iter_loss: 0.23733581602573395
train_iter_loss: 0.3347714841365814
train_iter_loss: 0.26479610800743103
train_iter_loss: 0.5222592353820801
train_iter_loss: 0.23232324421405792
train_iter_loss: 0.35812443494796753
train_iter_loss: 0.27238139510154724
train_iter_loss: 0.6725797057151794
train_iter_loss: 0.3556325137615204
train_iter_loss: 0.1271871030330658
train_iter_loss: 0.3879271447658539
train_iter_loss: 0.2381492555141449
train_iter_loss: 0.29364529252052307
train_iter_loss: 0.31621742248535156
train_iter_loss: 0.16122545301914215
train_iter_loss: 0.3571741580963135
train_iter_loss: 0.3405173420906067
train_iter_loss: 0.21071478724479675
train_iter_loss: 0.4475780129432678
train_iter_loss: 0.1557902842760086
train_iter_loss: 0.16843844950199127
train_iter_loss: 0.2910473346710205
train_iter_loss: 0.2086963653564453
train_iter_loss: 0.38373398780822754
train_iter_loss: 0.36198902130126953
train_iter_loss: 0.08502115309238434
train_iter_loss: 0.28982704877853394
train_iter_loss: 0.3060232400894165
train_iter_loss: 0.2535713016986847
train loss :0.2926
---------------------
Validation seg loss: 0.3666060015488908 at epoch 212
epoch =    213/  1000, exp = train
train_iter_loss: 0.4491655230522156
train_iter_loss: 0.225785493850708
train_iter_loss: 0.2857479751110077
train_iter_loss: 0.2088034600019455
train_iter_loss: 0.2981208860874176
train_iter_loss: 0.24823299050331116
train_iter_loss: 0.4256483316421509
train_iter_loss: 0.13462837040424347
train_iter_loss: 0.4163907766342163
train_iter_loss: 0.2716934382915497
train_iter_loss: 0.1680159568786621
train_iter_loss: 0.36189478635787964
train_iter_loss: 0.5165179967880249
train_iter_loss: 0.3779996931552887
train_iter_loss: 0.3153173625469208
train_iter_loss: 0.4314509630203247
train_iter_loss: 0.5886101722717285
train_iter_loss: 0.33577704429626465
train_iter_loss: 0.1823597103357315
train_iter_loss: 0.3435037434101105
train_iter_loss: 0.11680515110492706
train_iter_loss: 0.261306494474411
train_iter_loss: 0.39463648200035095
train_iter_loss: 0.12346267700195312
train_iter_loss: 0.36342406272888184
train_iter_loss: 0.24039122462272644
train_iter_loss: 0.205230250954628
train_iter_loss: 0.19672057032585144
train_iter_loss: 0.24610871076583862
train_iter_loss: 0.14147232472896576
train_iter_loss: 0.2718421518802643
train_iter_loss: 0.2469954788684845
train_iter_loss: 0.5432285666465759
train_iter_loss: 0.294352650642395
train_iter_loss: 0.27803879976272583
train_iter_loss: 0.15106689929962158
train_iter_loss: 0.24260807037353516
train_iter_loss: 0.14638429880142212
train_iter_loss: 0.2388051301240921
train_iter_loss: 0.3209196925163269
train_iter_loss: 0.32914823293685913
train_iter_loss: 0.26946938037872314
train_iter_loss: 0.30247819423675537
train_iter_loss: 0.16061030328273773
train_iter_loss: 0.13166913390159607
train_iter_loss: 0.24946263432502747
train_iter_loss: 0.3110226094722748
train_iter_loss: 0.2802044451236725
train_iter_loss: 0.5289337635040283
train_iter_loss: 0.34081241488456726
train_iter_loss: 0.324367493391037
train_iter_loss: 0.2586124539375305
train_iter_loss: 0.33809855580329895
train_iter_loss: 0.19869352877140045
train_iter_loss: 0.19063520431518555
train_iter_loss: 0.5005674362182617
train_iter_loss: 0.29233506321907043
train_iter_loss: 0.289276123046875
train_iter_loss: 0.22821789979934692
train_iter_loss: 0.2888696491718292
train_iter_loss: 0.2804292142391205
train_iter_loss: 0.27545166015625
train_iter_loss: 0.49192744493484497
train_iter_loss: 0.2560356855392456
train_iter_loss: 0.3962515592575073
train_iter_loss: 0.3066868484020233
train_iter_loss: 0.1790514886379242
train_iter_loss: 0.3042936325073242
train_iter_loss: 0.21232187747955322
train_iter_loss: 0.48784929513931274
train_iter_loss: 0.3496571481227875
train_iter_loss: 0.22225868701934814
train_iter_loss: 0.19779902696609497
train_iter_loss: 0.22285960614681244
train_iter_loss: 0.5127824544906616
train_iter_loss: 0.2799731492996216
train_iter_loss: 0.3427361249923706
train_iter_loss: 0.19996117055416107
train_iter_loss: 0.19144773483276367
train_iter_loss: 0.36850395798683167
train_iter_loss: 0.37101006507873535
train_iter_loss: 0.30157068371772766
train_iter_loss: 0.20487478375434875
train_iter_loss: 0.20828686654567719
train_iter_loss: 0.31765300035476685
train_iter_loss: 0.3190475106239319
train_iter_loss: 0.33845627307891846
train_iter_loss: 0.301082968711853
train_iter_loss: 0.18926531076431274
train_iter_loss: 0.2555341124534607
train_iter_loss: 0.25098949670791626
train_iter_loss: 0.22200150787830353
train_iter_loss: 0.45163533091545105
train_iter_loss: 0.41267910599708557
train_iter_loss: 0.3927113115787506
train_iter_loss: 0.40623676776885986
train_iter_loss: 0.22154319286346436
train_iter_loss: 0.27167809009552
train_iter_loss: 0.2386886477470398
train_iter_loss: 0.2714891731739044
train loss :0.2974
---------------------
Validation seg loss: 0.3885063719584273 at epoch 213
epoch =    214/  1000, exp = train
train_iter_loss: 0.335575133562088
train_iter_loss: 0.34990692138671875
train_iter_loss: 0.2599537670612335
train_iter_loss: 0.1544039249420166
train_iter_loss: 0.34647300839424133
train_iter_loss: 0.289949506521225
train_iter_loss: 0.3521069586277008
train_iter_loss: 0.13194595277309418
train_iter_loss: 0.2165035754442215
train_iter_loss: 0.23275932669639587
train_iter_loss: 0.35450753569602966
train_iter_loss: 0.0945463702082634
train_iter_loss: 0.3319406807422638
train_iter_loss: 0.39208078384399414
train_iter_loss: 0.24781709909439087
train_iter_loss: 0.25138887763023376
train_iter_loss: 0.19240659475326538
train_iter_loss: 0.3284706175327301
train_iter_loss: 0.18966425955295563
train_iter_loss: 0.2408936321735382
train_iter_loss: 0.22463326156139374
train_iter_loss: 0.46258991956710815
train_iter_loss: 0.25939905643463135
train_iter_loss: 0.32509174942970276
train_iter_loss: 0.20306001603603363
train_iter_loss: 0.13911129534244537
train_iter_loss: 0.18041996657848358
train_iter_loss: 0.3397652506828308
train_iter_loss: 0.31954750418663025
train_iter_loss: 0.41164934635162354
train_iter_loss: 0.27125057578086853
train_iter_loss: 0.31155461072921753
train_iter_loss: 0.23805157840251923
train_iter_loss: 0.3980036973953247
train_iter_loss: 0.2783811688423157
train_iter_loss: 0.2548297047615051
train_iter_loss: 0.3410801291465759
train_iter_loss: 0.23139892518520355
train_iter_loss: 0.39821282029151917
train_iter_loss: 0.44018253684043884
train_iter_loss: 0.39923688769340515
train_iter_loss: 0.1984855681657791
train_iter_loss: 0.46197518706321716
train_iter_loss: 0.35881170630455017
train_iter_loss: 0.26332801580429077
train_iter_loss: 0.2605331540107727
train_iter_loss: 0.29262492060661316
train_iter_loss: 0.2814849615097046
train_iter_loss: 0.36847785115242004
train_iter_loss: 0.2849739193916321
train_iter_loss: 0.19939054548740387
train_iter_loss: 0.20334188640117645
train_iter_loss: 0.34418654441833496
train_iter_loss: 0.48191213607788086
train_iter_loss: 0.45464619994163513
train_iter_loss: 0.45916837453842163
train_iter_loss: 0.2567593455314636
train_iter_loss: 0.3370250165462494
train_iter_loss: 0.34711194038391113
train_iter_loss: 0.29115957021713257
train_iter_loss: 0.25521141290664673
train_iter_loss: 0.371855229139328
train_iter_loss: 0.3643282949924469
train_iter_loss: 0.15646815299987793
train_iter_loss: 0.3457697033882141
train_iter_loss: 0.3291056454181671
train_iter_loss: 0.23134328424930573
train_iter_loss: 0.30889275670051575
train_iter_loss: 0.15749812126159668
train_iter_loss: 0.21001258492469788
train_iter_loss: 0.17438215017318726
train_iter_loss: 0.25719502568244934
train_iter_loss: 0.29958778619766235
train_iter_loss: 0.2929110825061798
train_iter_loss: 0.12760236859321594
train_iter_loss: 0.1925116926431656
train_iter_loss: 0.2975938022136688
train_iter_loss: 0.24680571258068085
train_iter_loss: 0.1653527319431305
train_iter_loss: 0.18702924251556396
train_iter_loss: 0.3161866068840027
train_iter_loss: 0.22876299917697906
train_iter_loss: 0.32386907935142517
train_iter_loss: 0.28786319494247437
train_iter_loss: 0.32131388783454895
train_iter_loss: 0.35880380868911743
train_iter_loss: 0.2527587115764618
train_iter_loss: 0.2971005439758301
train_iter_loss: 0.1794193983078003
train_iter_loss: 0.24241286516189575
train_iter_loss: 0.3447805941104889
train_iter_loss: 0.21803182363510132
train_iter_loss: 0.5258669853210449
train_iter_loss: 0.33831390738487244
train_iter_loss: 0.20360210537910461
train_iter_loss: 0.3235520124435425
train_iter_loss: 0.24322165548801422
train_iter_loss: 0.30209875106811523
train_iter_loss: 0.3406755030155182
train_iter_loss: 0.3278202414512634
train loss :0.2911
---------------------
Validation seg loss: 0.37572675109577347 at epoch 214
epoch =    215/  1000, exp = train
train_iter_loss: 0.27812817692756653
train_iter_loss: 0.22158873081207275
train_iter_loss: 0.31870946288108826
train_iter_loss: 0.15418492257595062
train_iter_loss: 0.22446998953819275
train_iter_loss: 0.26769590377807617
train_iter_loss: 0.2366943061351776
train_iter_loss: 0.2645668685436249
train_iter_loss: 0.32215380668640137
train_iter_loss: 0.2237890064716339
train_iter_loss: 0.24285517632961273
train_iter_loss: 0.2754516899585724
train_iter_loss: 0.2627848982810974
train_iter_loss: 0.5627636313438416
train_iter_loss: 0.36277827620506287
train_iter_loss: 0.19919906556606293
train_iter_loss: 0.394323468208313
train_iter_loss: 0.3861098289489746
train_iter_loss: 0.3986457288265228
train_iter_loss: 0.35316023230552673
train_iter_loss: 0.13992364704608917
train_iter_loss: 0.30495908856391907
train_iter_loss: 0.244900181889534
train_iter_loss: 0.22095824778079987
train_iter_loss: 0.25381410121917725
train_iter_loss: 0.31172335147857666
train_iter_loss: 0.14394456148147583
train_iter_loss: 0.21852245926856995
train_iter_loss: 0.34184810519218445
train_iter_loss: 0.3223620653152466
train_iter_loss: 0.2587575912475586
train_iter_loss: 0.267589807510376
train_iter_loss: 0.26231375336647034
train_iter_loss: 0.3841385543346405
train_iter_loss: 0.22074443101882935
train_iter_loss: 0.2057710886001587
train_iter_loss: 0.1845308542251587
train_iter_loss: 0.30676600337028503
train_iter_loss: 0.3841968774795532
train_iter_loss: 0.3541202247142792
train_iter_loss: 0.2975953221321106
train_iter_loss: 0.2539462149143219
train_iter_loss: 0.17436043918132782
train_iter_loss: 0.3037475347518921
train_iter_loss: 0.2824016809463501
train_iter_loss: 0.2660822868347168
train_iter_loss: 0.3481808304786682
train_iter_loss: 0.2638739347457886
train_iter_loss: 0.2726210355758667
train_iter_loss: 0.2897118926048279
train_iter_loss: 0.31624922156333923
train_iter_loss: 0.3369053304195404
train_iter_loss: 0.3378884792327881
train_iter_loss: 0.2833891808986664
train_iter_loss: 0.31348323822021484
train_iter_loss: 0.2810812294483185
train_iter_loss: 0.2637726366519928
train_iter_loss: 0.2911735475063324
train_iter_loss: 0.4076929986476898
train_iter_loss: 0.3157652020454407
train_iter_loss: 0.3183126449584961
train_iter_loss: 0.24778586626052856
train_iter_loss: 0.32645776867866516
train_iter_loss: 0.23330873250961304
train_iter_loss: 0.19253697991371155
train_iter_loss: 0.2663879096508026
train_iter_loss: 0.3060397207736969
train_iter_loss: 0.28644025325775146
train_iter_loss: 0.24446795880794525
train_iter_loss: 0.4898366928100586
train_iter_loss: 0.3443887233734131
train_iter_loss: 0.26797759532928467
train_iter_loss: 0.2586246728897095
train_iter_loss: 0.26591747999191284
train_iter_loss: 0.35907045006752014
train_iter_loss: 0.2644196152687073
train_iter_loss: 0.3923569917678833
train_iter_loss: 0.445159912109375
train_iter_loss: 0.5547615885734558
train_iter_loss: 0.2690919041633606
train_iter_loss: 0.2716955542564392
train_iter_loss: 0.33317896723747253
train_iter_loss: 0.2298564314842224
train_iter_loss: 0.22107617557048798
train_iter_loss: 0.22921259701251984
train_iter_loss: 0.4963793456554413
train_iter_loss: 0.13574665784835815
train_iter_loss: 0.3830752372741699
train_iter_loss: 0.27170032262802124
train_iter_loss: 0.21089762449264526
train_iter_loss: 0.19823375344276428
train_iter_loss: 0.32485538721084595
train_iter_loss: 0.2725406587123871
train_iter_loss: 0.3313412368297577
train_iter_loss: 0.2849995493888855
train_iter_loss: 0.18556886911392212
train_iter_loss: 0.4588654935359955
train_iter_loss: 0.22753794491291046
train_iter_loss: 0.2714087963104248
train_iter_loss: 0.18784978985786438
train loss :0.2933
---------------------
Validation seg loss: 0.36560179806261695 at epoch 215
epoch =    216/  1000, exp = train
train_iter_loss: 0.3740946650505066
train_iter_loss: 0.28303828835487366
train_iter_loss: 0.2007329910993576
train_iter_loss: 0.20685173571109772
train_iter_loss: 0.1102980300784111
train_iter_loss: 0.35038501024246216
train_iter_loss: 0.18557392060756683
train_iter_loss: 0.28733524680137634
train_iter_loss: 0.30113673210144043
train_iter_loss: 0.27533528208732605
train_iter_loss: 0.23380987346172333
train_iter_loss: 0.21256878972053528
train_iter_loss: 0.22813333570957184
train_iter_loss: 0.3571654260158539
train_iter_loss: 0.4439128041267395
train_iter_loss: 0.2361483871936798
train_iter_loss: 0.3570331037044525
train_iter_loss: 0.17648951709270477
train_iter_loss: 0.3221113085746765
train_iter_loss: 0.36840224266052246
train_iter_loss: 0.2630425989627838
train_iter_loss: 0.2107476145029068
train_iter_loss: 0.237235888838768
train_iter_loss: 0.2460923045873642
train_iter_loss: 0.24028605222702026
train_iter_loss: 0.2875156104564667
train_iter_loss: 0.1886713057756424
train_iter_loss: 0.2864922285079956
train_iter_loss: 0.27154669165611267
train_iter_loss: 0.3333827555179596
train_iter_loss: 0.22785481810569763
train_iter_loss: 0.30562278628349304
train_iter_loss: 0.27332964539527893
train_iter_loss: 0.15706628561019897
train_iter_loss: 0.21726712584495544
train_iter_loss: 0.251051127910614
train_iter_loss: 0.367972195148468
train_iter_loss: 0.34330183267593384
train_iter_loss: 0.2942963242530823
train_iter_loss: 0.24338878691196442
train_iter_loss: 0.3084782660007477
train_iter_loss: 0.19272327423095703
train_iter_loss: 0.3550759255886078
train_iter_loss: 0.3446846306324005
train_iter_loss: 0.35791918635368347
train_iter_loss: 0.2571258544921875
train_iter_loss: 0.3812740445137024
train_iter_loss: 0.24842265248298645
train_iter_loss: 0.3311901390552521
train_iter_loss: 0.2724408507347107
train_iter_loss: 0.2070518583059311
train_iter_loss: 0.22255206108093262
train_iter_loss: 0.29663166403770447
train_iter_loss: 0.37361347675323486
train_iter_loss: 0.20344868302345276
train_iter_loss: 0.32842451333999634
train_iter_loss: 0.17231881618499756
train_iter_loss: 0.2858985662460327
train_iter_loss: 0.4128481149673462
train_iter_loss: 0.29876619577407837
train_iter_loss: 0.24917802214622498
train_iter_loss: 0.19548554718494415
train_iter_loss: 0.22776228189468384
train_iter_loss: 0.27694669365882874
train_iter_loss: 0.22814063727855682
train_iter_loss: 0.5411493182182312
train_iter_loss: 0.3304584324359894
train_iter_loss: 0.3065136969089508
train_iter_loss: 0.2967819273471832
train_iter_loss: 0.23361901938915253
train_iter_loss: 0.36573725938796997
train_iter_loss: 0.34544098377227783
train_iter_loss: 0.41696596145629883
train_iter_loss: 0.3185535967350006
train_iter_loss: 0.2081526219844818
train_iter_loss: 0.4483184516429901
train_iter_loss: 0.40435245633125305
train_iter_loss: 0.2796461582183838
train_iter_loss: 0.25485455989837646
train_iter_loss: 0.27461057901382446
train_iter_loss: 0.33090507984161377
train_iter_loss: 0.2794576585292816
train_iter_loss: 0.3100452721118927
train_iter_loss: 0.2592608630657196
train_iter_loss: 0.14958392083644867
train_iter_loss: 0.1453758180141449
train_iter_loss: 0.2611137628555298
train_iter_loss: 0.1621542125940323
train_iter_loss: 0.2699531316757202
train_iter_loss: 0.3279118835926056
train_iter_loss: 0.25875023007392883
train_iter_loss: 0.1858324408531189
train_iter_loss: 0.23126620054244995
train_iter_loss: 0.2547406256198883
train_iter_loss: 0.36866405606269836
train_iter_loss: 0.23021486401557922
train_iter_loss: 0.2708275616168976
train_iter_loss: 0.2644585072994232
train_iter_loss: 0.21331608295440674
train_iter_loss: 0.31466251611709595
train loss :0.2819
---------------------
Validation seg loss: 0.37527785863165025 at epoch 216
epoch =    217/  1000, exp = train
train_iter_loss: 0.1849174052476883
train_iter_loss: 0.11616089195013046
train_iter_loss: 0.25423699617385864
train_iter_loss: 0.23341090977191925
train_iter_loss: 0.17206765711307526
train_iter_loss: 0.19396302103996277
train_iter_loss: 0.48899438977241516
train_iter_loss: 0.36097535490989685
train_iter_loss: 0.1463366448879242
train_iter_loss: 0.1168968677520752
train_iter_loss: 0.3285316824913025
train_iter_loss: 0.22469785809516907
train_iter_loss: 0.4835047423839569
train_iter_loss: 0.24172954261302948
train_iter_loss: 0.2212599217891693
train_iter_loss: 0.18921950459480286
train_iter_loss: 0.2520042955875397
train_iter_loss: 0.13807889819145203
train_iter_loss: 0.2702105939388275
train_iter_loss: 0.28545135259628296
train_iter_loss: 0.38788560032844543
train_iter_loss: 0.3999783396720886
train_iter_loss: 0.35764724016189575
train_iter_loss: 0.27783042192459106
train_iter_loss: 0.19815583527088165
train_iter_loss: 0.4060538411140442
train_iter_loss: 0.3335331082344055
train_iter_loss: 0.4268493950366974
train_iter_loss: 0.27741608023643494
train_iter_loss: 0.165292888879776
train_iter_loss: 0.28211554884910583
train_iter_loss: 0.17779646813869476
train_iter_loss: 0.18904876708984375
train_iter_loss: 0.40623581409454346
train_iter_loss: 0.29849007725715637
train_iter_loss: 0.24270150065422058
train_iter_loss: 0.2592461407184601
train_iter_loss: 0.206892728805542
train_iter_loss: 0.2593974769115448
train_iter_loss: 0.2516537010669708
train_iter_loss: 0.2288791984319687
train_iter_loss: 0.19864508509635925
train_iter_loss: 0.3073897361755371
train_iter_loss: 0.31837302446365356
train_iter_loss: 0.22886447608470917
train_iter_loss: 0.40743935108184814
train_iter_loss: 0.23850059509277344
train_iter_loss: 0.3602355718612671
train_iter_loss: 0.3002622425556183
train_iter_loss: 0.4623027741909027
train_iter_loss: 0.24522319436073303
train_iter_loss: 0.2718254625797272
train_iter_loss: 0.35157451033592224
train_iter_loss: 0.34981459379196167
train_iter_loss: 0.40213802456855774
train_iter_loss: 0.26125237345695496
train_iter_loss: 0.30370718240737915
train_iter_loss: 0.3391595780849457
train_iter_loss: 0.2498399019241333
train_iter_loss: 0.38304784893989563
train_iter_loss: 0.24380438029766083
train_iter_loss: 0.22873465716838837
train_iter_loss: 0.3758222460746765
train_iter_loss: 0.40132853388786316
train_iter_loss: 0.26940464973449707
train_iter_loss: 0.4787294566631317
train_iter_loss: 0.1285741627216339
train_iter_loss: 0.22512416541576385
train_iter_loss: 0.26519113779067993
train_iter_loss: 0.3279575705528259
train_iter_loss: 0.3947407901287079
train_iter_loss: 0.36342403292655945
train_iter_loss: 0.21469533443450928
train_iter_loss: 0.24492716789245605
train_iter_loss: 0.2808331549167633
train_iter_loss: 0.3836735188961029
train_iter_loss: 0.20837630331516266
train_iter_loss: 0.4946441054344177
train_iter_loss: 0.3391503691673279
train_iter_loss: 0.3478498160839081
train_iter_loss: 0.21917739510536194
train_iter_loss: 0.43263688683509827
train_iter_loss: 0.39791756868362427
train_iter_loss: 0.3622245192527771
train_iter_loss: 0.23720252513885498
train_iter_loss: 0.2637823820114136
train_iter_loss: 0.4548603892326355
train_iter_loss: 0.2961326241493225
train_iter_loss: 0.2429000735282898
train_iter_loss: 0.11856336146593094
train_iter_loss: 0.2764911949634552
train_iter_loss: 0.3486221432685852
train_iter_loss: 0.4137718677520752
train_iter_loss: 0.31331589818000793
train_iter_loss: 0.3685397207736969
train_iter_loss: 0.3412877023220062
train_iter_loss: 0.1297299712896347
train_iter_loss: 0.23079241812229156
train_iter_loss: 0.3960918188095093
train_iter_loss: 0.23028793931007385
train loss :0.2949
---------------------
Validation seg loss: 0.3916848515740262 at epoch 217
epoch =    218/  1000, exp = train
train_iter_loss: 0.18244773149490356
train_iter_loss: 0.3445114493370056
train_iter_loss: 0.3471287190914154
train_iter_loss: 0.10850638896226883
train_iter_loss: 0.33719536662101746
train_iter_loss: 0.2702362537384033
train_iter_loss: 0.3301694691181183
train_iter_loss: 0.10911107063293457
train_iter_loss: 0.36753445863723755
train_iter_loss: 0.33481958508491516
train_iter_loss: 0.27742668986320496
train_iter_loss: 0.23213180899620056
train_iter_loss: 0.32174941897392273
train_iter_loss: 0.3248172402381897
train_iter_loss: 0.10660403966903687
train_iter_loss: 0.24673667550086975
train_iter_loss: 0.23736852407455444
train_iter_loss: 0.214553102850914
train_iter_loss: 0.3513137102127075
train_iter_loss: 0.26672521233558655
train_iter_loss: 0.21081258356571198
train_iter_loss: 0.29238781332969666
train_iter_loss: 0.39672914147377014
train_iter_loss: 0.15464623272418976
train_iter_loss: 0.3185940682888031
train_iter_loss: 0.26581522822380066
train_iter_loss: 0.3968760073184967
train_iter_loss: 0.18166343867778778
train_iter_loss: 0.2506416141986847
train_iter_loss: 0.14100460708141327
train_iter_loss: 0.26485392451286316
train_iter_loss: 0.20274287462234497
train_iter_loss: 0.38466253876686096
train_iter_loss: 0.2519432008266449
train_iter_loss: 0.3028092384338379
train_iter_loss: 0.33915939927101135
train_iter_loss: 0.2967289984226227
train_iter_loss: 0.3033788502216339
train_iter_loss: 0.2861018180847168
train_iter_loss: 0.34833642840385437
train_iter_loss: 0.30135083198547363
train_iter_loss: 0.2634706497192383
train_iter_loss: 0.2259489744901657
train_iter_loss: 0.33759963512420654
train_iter_loss: 0.39958450198173523
train_iter_loss: 0.2868121564388275
train_iter_loss: 0.5344994068145752
train_iter_loss: 0.38045534491539
train_iter_loss: 0.14313870668411255
train_iter_loss: 0.2907971441745758
train_iter_loss: 0.24227182567119598
train_iter_loss: 0.20735040307044983
train_iter_loss: 0.26101380586624146
train_iter_loss: 0.24968896806240082
train_iter_loss: 0.354337215423584
train_iter_loss: 0.44917649030685425
train_iter_loss: 0.3153702914714813
train_iter_loss: 0.37569183111190796
train_iter_loss: 0.1695941984653473
train_iter_loss: 0.2751442492008209
train_iter_loss: 0.3315337598323822
train_iter_loss: 0.3176060914993286
train_iter_loss: 0.38701218366622925
train_iter_loss: 0.3950105309486389
train_iter_loss: 0.43143123388290405
train_iter_loss: 0.38019198179244995
train_iter_loss: 0.26511043310165405
train_iter_loss: 0.33155009150505066
train_iter_loss: 0.2754184901714325
train_iter_loss: 0.32112330198287964
train_iter_loss: 0.19086526334285736
train_iter_loss: 0.2721126079559326
train_iter_loss: 0.28782153129577637
train_iter_loss: 0.33820515871047974
train_iter_loss: 0.3886999487876892
train_iter_loss: 0.2175566703081131
train_iter_loss: 0.20075708627700806
train_iter_loss: 0.22831478714942932
train_iter_loss: 0.15583190321922302
train_iter_loss: 0.22662445902824402
train_iter_loss: 0.3369603753089905
train_iter_loss: 0.273473858833313
train_iter_loss: 0.2963748872280121
train_iter_loss: 0.41175901889801025
train_iter_loss: 0.21806636452674866
train_iter_loss: 0.3367437422275543
train_iter_loss: 0.4260845482349396
train_iter_loss: 0.15667104721069336
train_iter_loss: 0.32309389114379883
train_iter_loss: 0.2827828824520111
train_iter_loss: 0.2578698992729187
train_iter_loss: 0.14342354238033295
train_iter_loss: 0.1533280313014984
train_iter_loss: 0.25938504934310913
train_iter_loss: 0.32855653762817383
train_iter_loss: 0.20664125680923462
train_iter_loss: 0.3588266968727112
train_iter_loss: 0.35915327072143555
train_iter_loss: 0.2811397314071655
train_iter_loss: 0.30465278029441833
train loss :0.2892
---------------------
Validation seg loss: 0.3651995164570662 at epoch 218
epoch =    219/  1000, exp = train
train_iter_loss: 0.19010154902935028
train_iter_loss: 0.28245216608047485
train_iter_loss: 0.19150829315185547
train_iter_loss: 0.12124256044626236
train_iter_loss: 0.3245640695095062
train_iter_loss: 0.1989222764968872
train_iter_loss: 0.2903910279273987
train_iter_loss: 0.2919921278953552
train_iter_loss: 0.24339716136455536
train_iter_loss: 0.16825096309185028
train_iter_loss: 0.16918423771858215
train_iter_loss: 0.30952322483062744
train_iter_loss: 0.3805583715438843
train_iter_loss: 0.12769734859466553
train_iter_loss: 0.31680428981781006
train_iter_loss: 0.2676840126514435
train_iter_loss: 0.5319800972938538
train_iter_loss: 0.17351903021335602
train_iter_loss: 0.2558593153953552
train_iter_loss: 0.36213722825050354
train_iter_loss: 0.31376272439956665
train_iter_loss: 0.21940840780735016
train_iter_loss: 0.29752862453460693
train_iter_loss: 0.2622821033000946
train_iter_loss: 0.20150664448738098
train_iter_loss: 0.29035258293151855
train_iter_loss: 0.3147567808628082
train_iter_loss: 0.48081761598587036
train_iter_loss: 0.2959793210029602
train_iter_loss: 0.3570622503757477
train_iter_loss: 0.25770294666290283
train_iter_loss: 0.33194491267204285
train_iter_loss: 0.38754260540008545
train_iter_loss: 0.302056223154068
train_iter_loss: 0.49108701944351196
train_iter_loss: 0.3461947739124298
train_iter_loss: 0.227483332157135
train_iter_loss: 0.1957506686449051
train_iter_loss: 0.2488163858652115
train_iter_loss: 0.2379770576953888
train_iter_loss: 0.30899518728256226
train_iter_loss: 0.18457423150539398
train_iter_loss: 0.1781146079301834
train_iter_loss: 0.3477032780647278
train_iter_loss: 0.3134167790412903
train_iter_loss: 0.25932279229164124
train_iter_loss: 0.2839781641960144
train_iter_loss: 0.44643643498420715
train_iter_loss: 0.45212721824645996
train_iter_loss: 0.41548943519592285
train_iter_loss: 0.3032989501953125
train_iter_loss: 0.3238241970539093
train_iter_loss: 0.19886352121829987
train_iter_loss: 0.26214438676834106
train_iter_loss: 0.2341490536928177
train_iter_loss: 0.24889536201953888
train_iter_loss: 0.12441691756248474
train_iter_loss: 0.3726276159286499
train_iter_loss: 0.3301917612552643
train_iter_loss: 0.2939404249191284
train_iter_loss: 0.14748649299144745
train_iter_loss: 0.13318417966365814
train_iter_loss: 0.2588775157928467
train_iter_loss: 0.15373575687408447
train_iter_loss: 0.15565745532512665
train_iter_loss: 0.37081053853034973
train_iter_loss: 0.2956160604953766
train_iter_loss: 0.22285254299640656
train_iter_loss: 0.4036574065685272
train_iter_loss: 0.3325992226600647
train_iter_loss: 0.4696122109889984
train_iter_loss: 0.28058481216430664
train_iter_loss: 0.2054043859243393
train_iter_loss: 0.22513075172901154
train_iter_loss: 0.26324743032455444
train_iter_loss: 0.33313241600990295
train_iter_loss: 0.33645105361938477
train_iter_loss: 0.250209778547287
train_iter_loss: 0.3532388210296631
train_iter_loss: 0.35113173723220825
train_iter_loss: 0.39928004145622253
train_iter_loss: 0.43020710349082947
train_iter_loss: 0.3186895251274109
train_iter_loss: 0.24406315386295319
train_iter_loss: 0.19036434590816498
train_iter_loss: 0.13981418311595917
train_iter_loss: 0.1673218160867691
train_iter_loss: 0.24384437501430511
train_iter_loss: 0.28425726294517517
train_iter_loss: 0.33872681856155396
train_iter_loss: 0.27001485228538513
train_iter_loss: 0.4074552059173584
train_iter_loss: 0.1462133675813675
train_iter_loss: 0.36683401465415955
train_iter_loss: 0.2982003390789032
train_iter_loss: 0.27399441599845886
train_iter_loss: 0.29799169301986694
train_iter_loss: 0.29208144545555115
train_iter_loss: 0.1545194387435913
train_iter_loss: 0.3639459013938904
train loss :0.2860
---------------------
Validation seg loss: 0.3910185335168861 at epoch 219
epoch =    220/  1000, exp = train
train_iter_loss: 0.4396081268787384
train_iter_loss: 0.18880045413970947
train_iter_loss: 0.1859370321035385
train_iter_loss: 0.40625447034835815
train_iter_loss: 0.38185936212539673
train_iter_loss: 0.30300766229629517
train_iter_loss: 0.2933659553527832
train_iter_loss: 0.2955383062362671
train_iter_loss: 0.3511199951171875
train_iter_loss: 0.19947092235088348
train_iter_loss: 0.1256202906370163
train_iter_loss: 0.3304029107093811
train_iter_loss: 0.36523860692977905
train_iter_loss: 0.33541443943977356
train_iter_loss: 0.3458389341831207
train_iter_loss: 0.2826384902000427
train_iter_loss: 0.38749709725379944
train_iter_loss: 0.24064067006111145
train_iter_loss: 0.25439560413360596
train_iter_loss: 0.2505747973918915
train_iter_loss: 0.4061562120914459
train_iter_loss: 0.23800316452980042
train_iter_loss: 0.278136670589447
train_iter_loss: 0.14420142769813538
train_iter_loss: 0.3686773180961609
train_iter_loss: 0.38870444893836975
train_iter_loss: 0.15002129971981049
train_iter_loss: 0.27052488923072815
train_iter_loss: 0.366751104593277
train_iter_loss: 0.16553114354610443
train_iter_loss: 0.25542759895324707
train_iter_loss: 0.2555468678474426
train_iter_loss: 0.2148813009262085
train_iter_loss: 0.2824014723300934
train_iter_loss: 0.23264864087104797
train_iter_loss: 0.1396239995956421
train_iter_loss: 0.2491854876279831
train_iter_loss: 0.24869495630264282
train_iter_loss: 0.34265872836112976
train_iter_loss: 0.3594578802585602
train_iter_loss: 0.3671407699584961
train_iter_loss: 0.41017967462539673
train_iter_loss: 0.2772426903247833
train_iter_loss: 0.26405712962150574
train_iter_loss: 0.5296075940132141
train_iter_loss: 0.20003044605255127
train_iter_loss: 0.27966588735580444
train_iter_loss: 0.1448240876197815
train_iter_loss: 0.21505509316921234
train_iter_loss: 0.33664199709892273
train_iter_loss: 0.34661945700645447
train_iter_loss: 0.2476290464401245
train_iter_loss: 0.1881616860628128
train_iter_loss: 0.3100927770137787
train_iter_loss: 0.3223753869533539
train_iter_loss: 0.29660412669181824
train_iter_loss: 0.36051562428474426
train_iter_loss: 0.2168457955121994
train_iter_loss: 0.27316033840179443
train_iter_loss: 0.3601680397987366
train_iter_loss: 0.22458504140377045
train_iter_loss: 0.1602279096841812
train_iter_loss: 0.2666401267051697
train_iter_loss: 0.2704009711742401
train_iter_loss: 0.2806065082550049
train_iter_loss: 0.14435933530330658
train_iter_loss: 0.22173972427845
train_iter_loss: 0.30735674500465393
train_iter_loss: 0.2547103762626648
train_iter_loss: 0.28349292278289795
train_iter_loss: 0.3313545882701874
train_iter_loss: 0.24817396700382233
train_iter_loss: 0.4270607531070709
train_iter_loss: 0.41876667737960815
train_iter_loss: 0.42097803950309753
train_iter_loss: 0.3186441659927368
train_iter_loss: 0.3041474521160126
train_iter_loss: 0.2809039354324341
train_iter_loss: 0.24904367327690125
train_iter_loss: 0.34061580896377563
train_iter_loss: 0.346645712852478
train_iter_loss: 0.3454463481903076
train_iter_loss: 0.2857939302921295
train_iter_loss: 0.2745163142681122
train_iter_loss: 0.2565479874610901
train_iter_loss: 0.25605839490890503
train_iter_loss: 0.2884218096733093
train_iter_loss: 0.19368189573287964
train_iter_loss: 0.3304286301136017
train_iter_loss: 0.3616306781768799
train_iter_loss: 0.24808178842067719
train_iter_loss: 0.3393012583255768
train_iter_loss: 0.3444770574569702
train_iter_loss: 0.3070506155490875
train_iter_loss: 0.2569330632686615
train_iter_loss: 0.2138759344816208
train_iter_loss: 0.22158277034759521
train_iter_loss: 0.2815040946006775
train_iter_loss: 0.26901552081108093
train_iter_loss: 0.2837064564228058
train loss :0.2901
---------------------
Validation seg loss: 0.37781164027258474 at epoch 220
epoch =    221/  1000, exp = train
train_iter_loss: 0.2887590527534485
train_iter_loss: 0.2224884033203125
train_iter_loss: 0.28152748942375183
train_iter_loss: 0.36379116773605347
train_iter_loss: 0.3972766399383545
train_iter_loss: 0.49815183877944946
train_iter_loss: 0.21870756149291992
train_iter_loss: 0.34838107228279114
train_iter_loss: 0.20451290905475616
train_iter_loss: 0.19944529235363007
train_iter_loss: 0.29670798778533936
train_iter_loss: 0.3922071158885956
train_iter_loss: 0.23521919548511505
train_iter_loss: 0.260888010263443
train_iter_loss: 0.36452093720436096
train_iter_loss: 0.30931907892227173
train_iter_loss: 0.1701483577489853
train_iter_loss: 0.3990549147129059
train_iter_loss: 0.22820240259170532
train_iter_loss: 0.3661268949508667
train_iter_loss: 0.333271861076355
train_iter_loss: 0.3374517560005188
train_iter_loss: 0.2467612624168396
train_iter_loss: 0.24232622981071472
train_iter_loss: 0.45783403515815735
train_iter_loss: 0.22514130175113678
train_iter_loss: 0.3058788776397705
train_iter_loss: 0.2247006595134735
train_iter_loss: 0.17611205577850342
train_iter_loss: 0.2695622444152832
train_iter_loss: 0.22775809466838837
train_iter_loss: 0.3591010868549347
train_iter_loss: 0.2872810363769531
train_iter_loss: 0.3658260107040405
train_iter_loss: 0.3993699550628662
train_iter_loss: 0.3256561756134033
train_iter_loss: 0.2739792764186859
train_iter_loss: 0.28513777256011963
train_iter_loss: 0.31198859214782715
train_iter_loss: 0.27309390902519226
train_iter_loss: 0.2365420162677765
train_iter_loss: 0.14956021308898926
train_iter_loss: 0.31316158175468445
train_iter_loss: 0.3507990539073944
train_iter_loss: 0.3160374164581299
train_iter_loss: 0.2192131131887436
train_iter_loss: 0.3392687439918518
train_iter_loss: 0.38295939564704895
train_iter_loss: 0.37508031725883484
train_iter_loss: 0.20207951962947845
train_iter_loss: 0.23863433301448822
train_iter_loss: 0.4224222004413605
train_iter_loss: 0.28070399165153503
train_iter_loss: 0.1882086992263794
train_iter_loss: 0.37640032172203064
train_iter_loss: 0.6354299783706665
train_iter_loss: 0.2734549343585968
train_iter_loss: 0.436190128326416
train_iter_loss: 0.2761988341808319
train_iter_loss: 0.26303377747535706
train_iter_loss: 0.3455789089202881
train_iter_loss: 0.1939481496810913
train_iter_loss: 0.48196524381637573
train_iter_loss: 0.2979466915130615
train_iter_loss: 0.2430589348077774
train_iter_loss: 0.3201281428337097
train_iter_loss: 0.2162303477525711
train_iter_loss: 0.12743107974529266
train_iter_loss: 0.1971563845872879
train_iter_loss: 0.16104818880558014
train_iter_loss: 0.14434653520584106
train_iter_loss: 0.2455236315727234
train_iter_loss: 0.3764023184776306
train_iter_loss: 0.31248217821121216
train_iter_loss: 0.15363533794879913
train_iter_loss: 0.2509972155094147
train_iter_loss: 0.24479500949382782
train_iter_loss: 0.281324177980423
train_iter_loss: 0.1476881355047226
train_iter_loss: 0.2993507385253906
train_iter_loss: 0.30196613073349
train_iter_loss: 0.2363339364528656
train_iter_loss: 0.39389052987098694
train_iter_loss: 0.08986156433820724
train_iter_loss: 0.3560793697834015
train_iter_loss: 0.19764381647109985
train_iter_loss: 0.31299763917922974
train_iter_loss: 0.2162400484085083
train_iter_loss: 0.24142883718013763
train_iter_loss: 0.29127615690231323
train_iter_loss: 0.3123575448989868
train_iter_loss: 0.15560145676136017
train_iter_loss: 0.3669432997703552
train_iter_loss: 0.23302631080150604
train_iter_loss: 0.30965402722358704
train_iter_loss: 0.29187923669815063
train_iter_loss: 0.32009047269821167
train_iter_loss: 0.04021216556429863
train_iter_loss: 0.31902626156806946
train_iter_loss: 0.164699986577034
train loss :0.2873
---------------------
Validation seg loss: 0.40559216104623563 at epoch 221
epoch =    222/  1000, exp = train
train_iter_loss: 0.2956892251968384
train_iter_loss: 0.2769877016544342
train_iter_loss: 0.1308496743440628
train_iter_loss: 0.2426975667476654
train_iter_loss: 0.18574830889701843
train_iter_loss: 0.2680480182170868
train_iter_loss: 0.3399326503276825
train_iter_loss: 0.28127506375312805
train_iter_loss: 0.3656277358531952
train_iter_loss: 0.41802939772605896
train_iter_loss: 0.3045172393321991
train_iter_loss: 0.31535804271698
train_iter_loss: 0.2147027552127838
train_iter_loss: 0.18178094923496246
train_iter_loss: 0.3047727942466736
train_iter_loss: 0.24394527077674866
train_iter_loss: 0.3938540816307068
train_iter_loss: 0.39665669202804565
train_iter_loss: 0.26416638493537903
train_iter_loss: 0.23913343250751495
train_iter_loss: 0.4122292995452881
train_iter_loss: 0.2179027646780014
train_iter_loss: 0.3649405241012573
train_iter_loss: 0.15146875381469727
train_iter_loss: 0.31072524189949036
train_iter_loss: 0.30108481645584106
train_iter_loss: 0.3058585822582245
train_iter_loss: 0.20948639512062073
train_iter_loss: 0.2985377311706543
train_iter_loss: 0.19271481037139893
train_iter_loss: 0.2918476164340973
train_iter_loss: 0.3998582661151886
train_iter_loss: 0.24491119384765625
train_iter_loss: 0.16739346086978912
train_iter_loss: 0.2408590167760849
train_iter_loss: 0.1617923080921173
train_iter_loss: 0.2485283613204956
train_iter_loss: 0.35408100485801697
train_iter_loss: 0.22923880815505981
train_iter_loss: 0.33422696590423584
train_iter_loss: 0.26020151376724243
train_iter_loss: 0.34793826937675476
train_iter_loss: 0.24001707136631012
train_iter_loss: 0.29711055755615234
train_iter_loss: 0.1317800134420395
train_iter_loss: 0.32661673426628113
train_iter_loss: 0.4199315309524536
train_iter_loss: 0.22367285192012787
train_iter_loss: 0.22276552021503448
train_iter_loss: 0.380653977394104
train_iter_loss: 0.21627432107925415
train_iter_loss: 0.19882330298423767
train_iter_loss: 0.43403926491737366
train_iter_loss: 0.2410033792257309
train_iter_loss: 0.4223119616508484
train_iter_loss: 0.2819441258907318
train_iter_loss: 0.2219664454460144
train_iter_loss: 0.262317031621933
train_iter_loss: 0.29030606150627136
train_iter_loss: 0.28147798776626587
train_iter_loss: 0.19992537796497345
train_iter_loss: 0.17434944212436676
train_iter_loss: 0.21607084572315216
train_iter_loss: 0.33055180311203003
train_iter_loss: 0.23643600940704346
train_iter_loss: 0.16810059547424316
train_iter_loss: 0.1716611683368683
train_iter_loss: 0.5183451771736145
train_iter_loss: 0.23082970082759857
train_iter_loss: 0.27104657888412476
train_iter_loss: 0.30246293544769287
train_iter_loss: 0.34045717120170593
train_iter_loss: 0.3038393557071686
train_iter_loss: 0.1556003838777542
train_iter_loss: 0.35943952202796936
train_iter_loss: 0.18442417681217194
train_iter_loss: 0.23783205449581146
train_iter_loss: 0.19831009209156036
train_iter_loss: 0.3196789026260376
train_iter_loss: 0.12687571346759796
train_iter_loss: 0.25532394647598267
train_iter_loss: 0.31639575958251953
train_iter_loss: 0.3140176236629486
train_iter_loss: 0.3492829501628876
train_iter_loss: 0.2216893881559372
train_iter_loss: 0.3996230661869049
train_iter_loss: 0.2966774106025696
train_iter_loss: 0.18930713832378387
train_iter_loss: 0.2457161396741867
train_iter_loss: 0.2829294800758362
train_iter_loss: 0.28959307074546814
train_iter_loss: 0.32163769006729126
train_iter_loss: 0.3644249141216278
train_iter_loss: 0.31573763489723206
train_iter_loss: 0.2725543975830078
train_iter_loss: 0.30123254656791687
train_iter_loss: 0.4537523090839386
train_iter_loss: 0.20248733460903168
train_iter_loss: 0.2441449910402298
train_iter_loss: 0.23415040969848633
train loss :0.2801
---------------------
Validation seg loss: 0.3675873499574526 at epoch 222
epoch =    223/  1000, exp = train
train_iter_loss: 0.15083707869052887
train_iter_loss: 0.33029717206954956
train_iter_loss: 0.316772997379303
train_iter_loss: 0.2228296846151352
train_iter_loss: 0.4534125030040741
train_iter_loss: 0.2832431197166443
train_iter_loss: 0.2296951413154602
train_iter_loss: 0.05926299840211868
train_iter_loss: 0.11201664805412292
train_iter_loss: 0.3147885799407959
train_iter_loss: 0.2597537040710449
train_iter_loss: 0.2975195646286011
train_iter_loss: 0.32561132311820984
train_iter_loss: 0.21265511214733124
train_iter_loss: 0.27619603276252747
train_iter_loss: 0.275002121925354
train_iter_loss: 0.37639474868774414
train_iter_loss: 0.2858949899673462
train_iter_loss: 0.3419693112373352
train_iter_loss: 0.17858928442001343
train_iter_loss: 0.3948397636413574
train_iter_loss: 0.18308116495609283
train_iter_loss: 0.2920701503753662
train_iter_loss: 0.17107562720775604
train_iter_loss: 0.3218477666378021
train_iter_loss: 0.19303442537784576
train_iter_loss: 0.3064255714416504
train_iter_loss: 0.3186527490615845
train_iter_loss: 0.12773029506206512
train_iter_loss: 0.2652381360530853
train_iter_loss: 0.32533013820648193
train_iter_loss: 0.31539735198020935
train_iter_loss: 0.1812964826822281
train_iter_loss: 0.5227683186531067
train_iter_loss: 0.39506059885025024
train_iter_loss: 0.2900370955467224
train_iter_loss: 0.2871943414211273
train_iter_loss: 0.22761180996894836
train_iter_loss: 0.3798271119594574
train_iter_loss: 0.2863140404224396
train_iter_loss: 0.205644890666008
train_iter_loss: 0.24523863196372986
train_iter_loss: 0.2837895154953003
train_iter_loss: 0.10673122107982635
train_iter_loss: 0.4103876054286957
train_iter_loss: 0.35464611649513245
train_iter_loss: 0.21106591820716858
train_iter_loss: 0.32707124948501587
train_iter_loss: 0.2719763219356537
train_iter_loss: 0.2600090205669403
train_iter_loss: 0.38286954164505005
train_iter_loss: 0.27153480052948
train_iter_loss: 0.34250664710998535
train_iter_loss: 0.406597375869751
train_iter_loss: 0.2635592818260193
train_iter_loss: 0.3782237470149994
train_iter_loss: 0.27609485387802124
train_iter_loss: 0.30959945917129517
train_iter_loss: 0.3428190052509308
train_iter_loss: 0.3965905010700226
train_iter_loss: 0.1838417947292328
train_iter_loss: 0.31326478719711304
train_iter_loss: 0.23750309646129608
train_iter_loss: 0.40829166769981384
train_iter_loss: 0.2383490949869156
train_iter_loss: 0.19680273532867432
train_iter_loss: 0.23845358192920685
train_iter_loss: 0.28937533497810364
train_iter_loss: 0.3651902675628662
train_iter_loss: 0.23380711674690247
train_iter_loss: 0.3446604013442993
train_iter_loss: 0.27287009358406067
train_iter_loss: 0.4626117944717407
train_iter_loss: 0.3061365783214569
train_iter_loss: 0.3630916476249695
train_iter_loss: 0.2985880970954895
train_iter_loss: 0.23073159158229828
train_iter_loss: 0.21356016397476196
train_iter_loss: 0.18045365810394287
train_iter_loss: 0.21077249944210052
train_iter_loss: 0.3297036290168762
train_iter_loss: 0.2646491229534149
train_iter_loss: 0.21183009445667267
train_iter_loss: 0.30515456199645996
train_iter_loss: 0.21759329736232758
train_iter_loss: 0.2627490162849426
train_iter_loss: 0.27639779448509216
train_iter_loss: 0.24820347130298615
train_iter_loss: 0.2615433633327484
train_iter_loss: 0.2603197991847992
train_iter_loss: 0.33330413699150085
train_iter_loss: 0.15675194561481476
train_iter_loss: 0.33921727538108826
train_iter_loss: 0.23565515875816345
train_iter_loss: 0.3048187494277954
train_iter_loss: 0.3624798655509949
train_iter_loss: 0.38438379764556885
train_iter_loss: 0.23674389719963074
train_iter_loss: 0.41539695858955383
train_iter_loss: 0.4460650384426117
train loss :0.2886
---------------------
Validation seg loss: 0.45471592194769744 at epoch 223
epoch =    224/  1000, exp = train
train_iter_loss: 0.3022899329662323
train_iter_loss: 0.27710017561912537
train_iter_loss: 0.46208837628364563
train_iter_loss: 0.20264621078968048
train_iter_loss: 0.3285808861255646
train_iter_loss: 0.31051507592201233
train_iter_loss: 0.46612927317619324
train_iter_loss: 0.3613523840904236
train_iter_loss: 0.15557564795017242
train_iter_loss: 0.18817245960235596
train_iter_loss: 0.26556330919265747
train_iter_loss: 0.25369688868522644
train_iter_loss: 0.3284396529197693
train_iter_loss: 0.14725005626678467
train_iter_loss: 0.24575939774513245
train_iter_loss: 0.31934666633605957
train_iter_loss: 0.22427020967006683
train_iter_loss: 0.4145835041999817
train_iter_loss: 0.27219435572624207
train_iter_loss: 0.4743770658969879
train_iter_loss: 0.25648143887519836
train_iter_loss: 0.4658442437648773
train_iter_loss: 0.24219252169132233
train_iter_loss: 0.20604363083839417
train_iter_loss: 0.12874992191791534
train_iter_loss: 0.35655921697616577
train_iter_loss: 0.3047393560409546
train_iter_loss: 0.4670362174510956
train_iter_loss: 0.4494212567806244
train_iter_loss: 0.348530650138855
train_iter_loss: 0.20302684605121613
train_iter_loss: 0.20885677635669708
train_iter_loss: 0.2455725073814392
train_iter_loss: 0.15510322153568268
train_iter_loss: 0.30561429262161255
train_iter_loss: 0.4185839593410492
train_iter_loss: 0.3089718818664551
train_iter_loss: 0.18774181604385376
train_iter_loss: 0.30243396759033203
train_iter_loss: 0.34178516268730164
train_iter_loss: 0.3203730583190918
train_iter_loss: 0.10551340132951736
train_iter_loss: 0.19985555112361908
train_iter_loss: 0.46978169679641724
train_iter_loss: 0.4838842451572418
train_iter_loss: 0.5136659145355225
train_iter_loss: 0.28757670521736145
train_iter_loss: 0.6190266013145447
train_iter_loss: 0.14442071318626404
train_iter_loss: 0.3011572062969208
train_iter_loss: 0.30966389179229736
train_iter_loss: 0.23789344727993011
train_iter_loss: 0.24248583614826202
train_iter_loss: 0.3430458605289459
train_iter_loss: 0.3426138758659363
train_iter_loss: 0.1526840329170227
train_iter_loss: 0.18176762759685516
train_iter_loss: 0.2766638696193695
train_iter_loss: 0.25907662510871887
train_iter_loss: 0.24688242375850677
train_iter_loss: 0.28856027126312256
train_iter_loss: 0.24158504605293274
train_iter_loss: 0.17848068475723267
train_iter_loss: 0.1790934056043625
train_iter_loss: 0.14172716438770294
train_iter_loss: 0.24661560356616974
train_iter_loss: 0.29197579622268677
train_iter_loss: 0.27278491854667664
train_iter_loss: 0.20662346482276917
train_iter_loss: 0.30732083320617676
train_iter_loss: 0.25519904494285583
train_iter_loss: 0.10615437477827072
train_iter_loss: 0.23019221425056458
train_iter_loss: 0.4346810579299927
train_iter_loss: 0.23183248937129974
train_iter_loss: 0.311492383480072
train_iter_loss: 0.3672123849391937
train_iter_loss: 0.39492422342300415
train_iter_loss: 0.15302075445652008
train_iter_loss: 0.19135110080242157
train_iter_loss: 0.1288926601409912
train_iter_loss: 0.2723737359046936
train_iter_loss: 0.23103949427604675
train_iter_loss: 0.35242393612861633
train_iter_loss: 0.2624547779560089
train_iter_loss: 0.34788280725479126
train_iter_loss: 0.35532039403915405
train_iter_loss: 0.25249549746513367
train_iter_loss: 0.2561315596103668
train_iter_loss: 0.3958943784236908
train_iter_loss: 0.3004792034626007
train_iter_loss: 0.22868497669696808
train_iter_loss: 0.3289966881275177
train_iter_loss: 0.4318769574165344
train_iter_loss: 0.17539238929748535
train_iter_loss: 0.21701131761074066
train_iter_loss: 0.3447641134262085
train_iter_loss: 0.325969934463501
train_iter_loss: 0.36619240045547485
train_iter_loss: 0.3614155948162079
train loss :0.2920
---------------------
Validation seg loss: 0.39874942473329184 at epoch 224
epoch =    225/  1000, exp = train
train_iter_loss: 0.21916717290878296
train_iter_loss: 0.17942069470882416
train_iter_loss: 0.3637685179710388
train_iter_loss: 0.1343451738357544
train_iter_loss: 0.2610699236392975
train_iter_loss: 0.3201535642147064
train_iter_loss: 0.3700697124004364
train_iter_loss: 0.1644984781742096
train_iter_loss: 0.23835240304470062
train_iter_loss: 0.39265012741088867
train_iter_loss: 0.20864039659500122
train_iter_loss: 0.2514544129371643
train_iter_loss: 0.26740750670433044
train_iter_loss: 0.5453730821609497
train_iter_loss: 0.489582896232605
train_iter_loss: 0.3223639726638794
train_iter_loss: 0.2713111639022827
train_iter_loss: 0.2121725082397461
train_iter_loss: 0.14513333141803741
train_iter_loss: 0.25714582204818726
train_iter_loss: 0.3030559718608856
train_iter_loss: 0.2954641282558441
train_iter_loss: 0.2710913121700287
train_iter_loss: 0.3886629045009613
train_iter_loss: 0.3647187650203705
train_iter_loss: 0.29152241349220276
train_iter_loss: 0.2698234021663666
train_iter_loss: 0.2718709707260132
train_iter_loss: 0.28460264205932617
train_iter_loss: 0.3961234986782074
train_iter_loss: 0.20581349730491638
train_iter_loss: 0.394402414560318
train_iter_loss: 0.343940794467926
train_iter_loss: 0.3574533760547638
train_iter_loss: 0.2835550308227539
train_iter_loss: 0.18232499063014984
train_iter_loss: 0.3765410780906677
train_iter_loss: 0.2587212324142456
train_iter_loss: 0.16008466482162476
train_iter_loss: 0.1367834061384201
train_iter_loss: 0.43028977513313293
train_iter_loss: 0.20994919538497925
train_iter_loss: 0.34264200925827026
train_iter_loss: 0.25390321016311646
train_iter_loss: 0.3389740586280823
train_iter_loss: 0.2533814013004303
train_iter_loss: 0.39432013034820557
train_iter_loss: 0.31263887882232666
train_iter_loss: 0.3718383014202118
train_iter_loss: 0.15054042637348175
train_iter_loss: 0.33210253715515137
train_iter_loss: 0.23299600183963776
train_iter_loss: 0.3115876019001007
train_iter_loss: 0.361424058675766
train_iter_loss: 0.3840242922306061
train_iter_loss: 0.34159836173057556
train_iter_loss: 0.32000625133514404
train_iter_loss: 0.2801869809627533
train_iter_loss: 0.18725261092185974
train_iter_loss: 0.1917552798986435
train_iter_loss: 0.3186076283454895
train_iter_loss: 0.31540167331695557
train_iter_loss: 0.24037359654903412
train_iter_loss: 0.29395872354507446
train_iter_loss: 0.3331112861633301
train_iter_loss: 0.18914537131786346
train_iter_loss: 0.32170915603637695
train_iter_loss: 0.26931291818618774
train_iter_loss: 0.1376214176416397
train_iter_loss: 0.3514518737792969
train_iter_loss: 0.36309605836868286
train_iter_loss: 0.32147014141082764
train_iter_loss: 0.24867671728134155
train_iter_loss: 0.317625492811203
train_iter_loss: 0.30055347084999084
train_iter_loss: 0.19954925775527954
train_iter_loss: 0.10654081404209137
train_iter_loss: 0.19175885617733002
train_iter_loss: 0.1233542338013649
train_iter_loss: 0.29257163405418396
train_iter_loss: 0.20501942932605743
train_iter_loss: 0.40779805183410645
train_iter_loss: 0.173289492726326
train_iter_loss: 0.2728728950023651
train_iter_loss: 0.24269112944602966
train_iter_loss: 0.23205405473709106
train_iter_loss: 0.5564095377922058
train_iter_loss: 0.39425528049468994
train_iter_loss: 0.2416495531797409
train_iter_loss: 0.2215132713317871
train_iter_loss: 0.28289398550987244
train_iter_loss: 0.37730249762535095
train_iter_loss: 0.22389093041419983
train_iter_loss: 0.26759466528892517
train_iter_loss: 0.31230074167251587
train_iter_loss: 0.09220395237207413
train_iter_loss: 0.3319094777107239
train_iter_loss: 0.27960366010665894
train_iter_loss: 0.28292417526245117
train_iter_loss: 0.20694780349731445
train loss :0.2859
---------------------
Validation seg loss: 0.37500774675874776 at epoch 225
epoch =    226/  1000, exp = train
train_iter_loss: 0.33159130811691284
train_iter_loss: 0.34350085258483887
train_iter_loss: 0.35185706615448
train_iter_loss: 0.33182796835899353
train_iter_loss: 0.2630031704902649
train_iter_loss: 0.2675853967666626
train_iter_loss: 0.3009983003139496
train_iter_loss: 0.340149462223053
train_iter_loss: 0.26864123344421387
train_iter_loss: 0.35066598653793335
train_iter_loss: 0.26452910900115967
train_iter_loss: 0.22850534319877625
train_iter_loss: 0.23511147499084473
train_iter_loss: 0.35360270738601685
train_iter_loss: 0.3332950472831726
train_iter_loss: 0.3136771619319916
train_iter_loss: 0.1882142424583435
train_iter_loss: 0.22001615166664124
train_iter_loss: 0.19459675252437592
train_iter_loss: 0.16049008071422577
train_iter_loss: 0.39734670519828796
train_iter_loss: 0.2227683663368225
train_iter_loss: 0.26854172348976135
train_iter_loss: 0.33927401900291443
train_iter_loss: 0.1529005765914917
train_iter_loss: 0.17213070392608643
train_iter_loss: 0.19443412125110626
train_iter_loss: 0.3170742094516754
train_iter_loss: 0.3506566882133484
train_iter_loss: 0.35872387886047363
train_iter_loss: 0.3322058916091919
train_iter_loss: 0.3055481016635895
train_iter_loss: 0.4038926959037781
train_iter_loss: 0.1653810739517212
train_iter_loss: 0.2327173948287964
train_iter_loss: 0.3055204451084137
train_iter_loss: 0.17379431426525116
train_iter_loss: 0.28692692518234253
train_iter_loss: 0.31168854236602783
train_iter_loss: 0.2572566866874695
train_iter_loss: 0.39732590317726135
train_iter_loss: 0.2794566750526428
train_iter_loss: 0.16503456234931946
train_iter_loss: 0.35260531306266785
train_iter_loss: 0.34371083974838257
train_iter_loss: 0.2674720287322998
train_iter_loss: 0.23411142826080322
train_iter_loss: 0.2232498973608017
train_iter_loss: 0.4077226519584656
train_iter_loss: 0.36269015073776245
train_iter_loss: 0.589707612991333
train_iter_loss: 0.36782145500183105
train_iter_loss: 0.304958701133728
train_iter_loss: 0.23906801640987396
train_iter_loss: 0.2647804319858551
train_iter_loss: 0.3360708951950073
train_iter_loss: 0.1659029722213745
train_iter_loss: 0.3595958948135376
train_iter_loss: 0.2180340588092804
train_iter_loss: 0.30410611629486084
train_iter_loss: 0.2199733853340149
train_iter_loss: 0.2342454344034195
train_iter_loss: 0.15768420696258545
train_iter_loss: 0.12981374561786652
train_iter_loss: 0.3087749481201172
train_iter_loss: 0.1897711604833603
train_iter_loss: 0.23671592772006989
train_iter_loss: 0.35012197494506836
train_iter_loss: 0.2537195682525635
train_iter_loss: 0.36063650250434875
train_iter_loss: 0.430170476436615
train_iter_loss: 0.4368954598903656
train_iter_loss: 0.3644910752773285
train_iter_loss: 0.35539427399635315
train_iter_loss: 0.307873010635376
train_iter_loss: 0.2623431980609894
train_iter_loss: 0.3111409544944763
train_iter_loss: 0.18299148976802826
train_iter_loss: 0.2959481477737427
train_iter_loss: 0.3710709810256958
train_iter_loss: 0.13316401839256287
train_iter_loss: 0.3216617703437805
train_iter_loss: 0.2601645290851593
train_iter_loss: 0.32650765776634216
train_iter_loss: 0.34171736240386963
train_iter_loss: 0.40431463718414307
train_iter_loss: 0.23512345552444458
train_iter_loss: 0.46699583530426025
train_iter_loss: 0.3302983343601227
train_iter_loss: 0.2322300374507904
train_iter_loss: 0.16052284836769104
train_iter_loss: 0.2539723515510559
train_iter_loss: 0.22977317869663239
train_iter_loss: 0.2987985908985138
train_iter_loss: 0.3025979995727539
train_iter_loss: 0.19834694266319275
train_iter_loss: 0.25635063648223877
train_iter_loss: 0.2235994189977646
train_iter_loss: 0.37319016456604004
train_iter_loss: 0.2573406398296356
train loss :0.2903
---------------------
Validation seg loss: 0.40558895685727586 at epoch 226
epoch =    227/  1000, exp = train
train_iter_loss: 0.2577781081199646
train_iter_loss: 0.33287638425827026
train_iter_loss: 0.24830491840839386
train_iter_loss: 0.35913026332855225
train_iter_loss: 0.27180206775665283
train_iter_loss: 0.24954456090927124
train_iter_loss: 0.21866126358509064
train_iter_loss: 0.3192421793937683
train_iter_loss: 0.372981458902359
train_iter_loss: 0.5806032419204712
train_iter_loss: 0.1783958524465561
train_iter_loss: 0.2866314649581909
train_iter_loss: 0.3512944281101227
train_iter_loss: 0.18202796578407288
train_iter_loss: 0.41693466901779175
train_iter_loss: 0.34997957944869995
train_iter_loss: 0.3756546378135681
train_iter_loss: 0.2580595314502716
train_iter_loss: 0.3565318286418915
train_iter_loss: 0.28567394614219666
train_iter_loss: 0.3763481676578522
train_iter_loss: 0.3124431073665619
train_iter_loss: 0.28778859972953796
train_iter_loss: 0.2839929759502411
train_iter_loss: 0.36285194754600525
train_iter_loss: 0.24531789124011993
train_iter_loss: 0.30306482315063477
train_iter_loss: 0.3384420573711395
train_iter_loss: 0.3083493113517761
train_iter_loss: 0.4546143710613251
train_iter_loss: 0.25140321254730225
train_iter_loss: 0.2488458901643753
train_iter_loss: 0.15288834273815155
train_iter_loss: 0.39047080278396606
train_iter_loss: 0.1925336867570877
train_iter_loss: 0.1254459023475647
train_iter_loss: 0.3938538134098053
train_iter_loss: 0.2148246467113495
train_iter_loss: 0.3190619647502899
train_iter_loss: 0.14966613054275513
train_iter_loss: 0.29972973465919495
train_iter_loss: 0.20286338031291962
train_iter_loss: 0.20599962770938873
train_iter_loss: 0.30318716168403625
train_iter_loss: 0.4542529284954071
train_iter_loss: 0.22894376516342163
train_iter_loss: 0.2824619710445404
train_iter_loss: 0.2928256094455719
train_iter_loss: 0.1643231213092804
train_iter_loss: 0.3700401484966278
train_iter_loss: 0.2868960499763489
train_iter_loss: 0.2069312334060669
train_iter_loss: 0.3275185525417328
train_iter_loss: 0.2815499007701874
train_iter_loss: 0.4489956200122833
train_iter_loss: 0.4732304811477661
train_iter_loss: 0.32713088393211365
train_iter_loss: 0.20586708188056946
train_iter_loss: 0.16844363510608673
train_iter_loss: 0.278622031211853
train_iter_loss: 0.22984178364276886
train_iter_loss: 0.30757516622543335
train_iter_loss: 0.2916935086250305
train_iter_loss: 0.37794825434684753
train_iter_loss: 0.4316578209400177
train_iter_loss: 0.37313517928123474
train_iter_loss: 0.2800372540950775
train_iter_loss: 0.2145310491323471
train_iter_loss: 0.22959592938423157
train_iter_loss: 0.17103487253189087
train_iter_loss: 0.3446117341518402
train_iter_loss: 0.22581370174884796
train_iter_loss: 0.18746323883533478
train_iter_loss: 0.16286037862300873
train_iter_loss: 0.3163509964942932
train_iter_loss: 0.19290180504322052
train_iter_loss: 0.2302120476961136
train_iter_loss: 0.2883282005786896
train_iter_loss: 0.229500412940979
train_iter_loss: 0.36081409454345703
train_iter_loss: 0.28510069847106934
train_iter_loss: 0.30222761631011963
train_iter_loss: 0.17447638511657715
train_iter_loss: 0.5008441805839539
train_iter_loss: 0.3194785416126251
train_iter_loss: 0.2527801990509033
train_iter_loss: 0.2581239938735962
train_iter_loss: 0.3151540160179138
train_iter_loss: 0.33455726504325867
train_iter_loss: 0.2913985550403595
train_iter_loss: 0.2200794368982315
train_iter_loss: 0.3742944896221161
train_iter_loss: 0.2657628059387207
train_iter_loss: 0.21305452287197113
train_iter_loss: 0.376645028591156
train_iter_loss: 0.2925723195075989
train_iter_loss: 0.3021552860736847
train_iter_loss: 0.17299342155456543
train_iter_loss: 0.33449840545654297
train_iter_loss: 0.2191569209098816
train loss :0.2932
---------------------
Validation seg loss: 0.36791008297037686 at epoch 227
epoch =    228/  1000, exp = train
train_iter_loss: 0.2893790900707245
train_iter_loss: 0.2782975435256958
train_iter_loss: 0.3112817406654358
train_iter_loss: 0.36847078800201416
train_iter_loss: 0.19932037591934204
train_iter_loss: 0.5311911702156067
train_iter_loss: 0.3756619691848755
train_iter_loss: 0.24457240104675293
train_iter_loss: 0.2315903902053833
train_iter_loss: 0.27189138531684875
train_iter_loss: 0.31163671612739563
train_iter_loss: 0.20341724157333374
train_iter_loss: 0.3308956027030945
train_iter_loss: 0.20261149108409882
train_iter_loss: 0.22117416560649872
train_iter_loss: 0.3879690170288086
train_iter_loss: 0.2945892810821533
train_iter_loss: 0.45088255405426025
train_iter_loss: 0.3284592926502228
train_iter_loss: 0.23056931793689728
train_iter_loss: 0.31986236572265625
train_iter_loss: 0.17324863374233246
train_iter_loss: 0.33249977231025696
train_iter_loss: 0.5256840586662292
train_iter_loss: 0.2853855788707733
train_iter_loss: 0.3816724419593811
train_iter_loss: 0.36228907108306885
train_iter_loss: 0.3216824531555176
train_iter_loss: 0.23786135017871857
train_iter_loss: 0.2509704530239105
train_iter_loss: 0.33107608556747437
train_iter_loss: 0.29552707076072693
train_iter_loss: 0.35962456464767456
train_iter_loss: 0.20701813697814941
train_iter_loss: 0.21706557273864746
train_iter_loss: 0.3645741045475006
train_iter_loss: 0.18050695955753326
train_iter_loss: 0.27646738290786743
train_iter_loss: 0.2767837643623352
train_iter_loss: 0.30016276240348816
train_iter_loss: 0.19980518519878387
train_iter_loss: 0.31446897983551025
train_iter_loss: 0.224333718419075
train_iter_loss: 0.39154666662216187
train_iter_loss: 0.35791897773742676
train_iter_loss: 0.2593545615673065
train_iter_loss: 0.2166108936071396
train_iter_loss: 0.23466238379478455
train_iter_loss: 0.24528713524341583
train_iter_loss: 0.16569733619689941
train_iter_loss: 0.21827004849910736
train_iter_loss: 0.26797157526016235
train_iter_loss: 0.24867598712444305
train_iter_loss: 0.32705870270729065
train_iter_loss: 0.2236173301935196
train_iter_loss: 0.1578046977519989
train_iter_loss: 0.15479525923728943
train_iter_loss: 0.15067583322525024
train_iter_loss: 0.16704241931438446
train_iter_loss: 0.49435707926750183
train_iter_loss: 0.32984790205955505
train_iter_loss: 0.45595091581344604
train_iter_loss: 0.18234272301197052
train_iter_loss: 0.18105576932430267
train_iter_loss: 0.3589075207710266
train_iter_loss: 0.385152667760849
train_iter_loss: 0.22337569296360016
train_iter_loss: 0.3395342528820038
train_iter_loss: 0.28061002492904663
train_iter_loss: 0.3343450129032135
train_iter_loss: 0.2718801498413086
train_iter_loss: 0.4594061076641083
train_iter_loss: 0.306527316570282
train_iter_loss: 0.30026981234550476
train_iter_loss: 0.2956940829753876
train_iter_loss: 0.27442097663879395
train_iter_loss: 0.16759264469146729
train_iter_loss: 0.2632795572280884
train_iter_loss: 0.3778304159641266
train_iter_loss: 0.273555725812912
train_iter_loss: 0.2643766701221466
train_iter_loss: 0.22132477164268494
train_iter_loss: 0.35086333751678467
train_iter_loss: 0.4160720407962799
train_iter_loss: 0.3318246006965637
train_iter_loss: 0.29843056201934814
train_iter_loss: 0.19283200800418854
train_iter_loss: 0.22831667959690094
train_iter_loss: 0.23472176492214203
train_iter_loss: 0.2757021486759186
train_iter_loss: 0.18308624625205994
train_iter_loss: 0.36807122826576233
train_iter_loss: 0.32222169637680054
train_iter_loss: 0.44492900371551514
train_iter_loss: 0.3008781671524048
train_iter_loss: 0.2941336929798126
train_iter_loss: 0.24981184303760529
train_iter_loss: 0.37430694699287415
train_iter_loss: 0.24830108880996704
train_iter_loss: 0.23719266057014465
train loss :0.2928
---------------------
Validation seg loss: 0.3629309113383434 at epoch 228
epoch =    229/  1000, exp = train
train_iter_loss: 0.3863445818424225
train_iter_loss: 0.3353271782398224
train_iter_loss: 0.34221792221069336
train_iter_loss: 0.4709540605545044
train_iter_loss: 0.17386338114738464
train_iter_loss: 0.27074944972991943
train_iter_loss: 0.39409253001213074
train_iter_loss: 0.22718843817710876
train_iter_loss: 0.2086457908153534
train_iter_loss: 0.30935370922088623
train_iter_loss: 0.27073153853416443
train_iter_loss: 0.31063851714134216
train_iter_loss: 0.33434414863586426
train_iter_loss: 0.4266836941242218
train_iter_loss: 0.4592853784561157
train_iter_loss: 0.264059454202652
train_iter_loss: 0.23981529474258423
train_iter_loss: 0.1330292820930481
train_iter_loss: 0.22630362212657928
train_iter_loss: 0.2517510950565338
train_iter_loss: 0.2890806794166565
train_iter_loss: 0.19303128123283386
train_iter_loss: 0.27251505851745605
train_iter_loss: 0.32898256182670593
train_iter_loss: 0.33453232049942017
train_iter_loss: 0.31682178378105164
train_iter_loss: 0.3293789327144623
train_iter_loss: 0.20942768454551697
train_iter_loss: 0.4004897177219391
train_iter_loss: 0.27703604102134705
train_iter_loss: 0.3841012716293335
train_iter_loss: 0.42681270837783813
train_iter_loss: 0.27561718225479126
train_iter_loss: 0.32225945591926575
train_iter_loss: 0.238091379404068
train_iter_loss: 0.24039046466350555
train_iter_loss: 0.2850058078765869
train_iter_loss: 0.2114255130290985
train_iter_loss: 0.18744546175003052
train_iter_loss: 0.2801372706890106
train_iter_loss: 0.17535626888275146
train_iter_loss: 0.31446677446365356
train_iter_loss: 0.39712226390838623
train_iter_loss: 0.22212865948677063
train_iter_loss: 0.2557145953178406
train_iter_loss: 0.17194704711437225
train_iter_loss: 0.32987168431282043
train_iter_loss: 0.22519701719284058
train_iter_loss: 0.16669832170009613
train_iter_loss: 0.3082635998725891
train_iter_loss: 0.4262373149394989
train_iter_loss: 0.20290988683700562
train_iter_loss: 0.14709512889385223
train_iter_loss: 0.2513725459575653
train_iter_loss: 0.1470349133014679
train_iter_loss: 0.32376572489738464
train_iter_loss: 0.30875957012176514
train_iter_loss: 0.20365042984485626
train_iter_loss: 0.2611592710018158
train_iter_loss: 0.2738058567047119
train_iter_loss: 0.37144774198532104
train_iter_loss: 0.08683031052350998
train_iter_loss: 0.2525598108768463
train_iter_loss: 0.1528409719467163
train_iter_loss: 0.2992759048938751
train_iter_loss: 0.16348552703857422
train_iter_loss: 0.2797437012195587
train_iter_loss: 0.3804458975791931
train_iter_loss: 0.3694981038570404
train_iter_loss: 0.22037363052368164
train_iter_loss: 0.08994172513484955
train_iter_loss: 0.1602822244167328
train_iter_loss: 0.22864782810211182
train_iter_loss: 0.44813695549964905
train_iter_loss: 0.17194877564907074
train_iter_loss: 0.37103113532066345
train_iter_loss: 0.307072252035141
train_iter_loss: 0.5279064178466797
train_iter_loss: 0.472252756357193
train_iter_loss: 0.30572640895843506
train_iter_loss: 0.35519319772720337
train_iter_loss: 0.20375001430511475
train_iter_loss: 0.26886096596717834
train_iter_loss: 0.4164430797100067
train_iter_loss: 0.23790496587753296
train_iter_loss: 0.2708512246608734
train_iter_loss: 0.34130579233169556
train_iter_loss: 0.2381211519241333
train_iter_loss: 0.2671339511871338
train_iter_loss: 0.31207188963890076
train_iter_loss: 0.44536903500556946
train_iter_loss: 0.3553611934185028
train_iter_loss: 0.3642582297325134
train_iter_loss: 0.31544986367225647
train_iter_loss: 0.500594437122345
train_iter_loss: 0.26485496759414673
train_iter_loss: 0.2601129710674286
train_iter_loss: 0.2650047838687897
train_iter_loss: 0.30419838428497314
train_iter_loss: 0.26251208782196045
train loss :0.2915
---------------------
Validation seg loss: 0.4012751205701311 at epoch 229
epoch =    230/  1000, exp = train
train_iter_loss: 0.3829832375049591
train_iter_loss: 0.15794001519680023
train_iter_loss: 0.30728879570961
train_iter_loss: 0.2645885944366455
train_iter_loss: 0.19309085607528687
train_iter_loss: 0.3405529856681824
train_iter_loss: 0.41836026310920715
train_iter_loss: 0.0648358166217804
train_iter_loss: 0.23884256184101105
train_iter_loss: 0.3190421462059021
train_iter_loss: 0.4559655487537384
train_iter_loss: 0.20603136718273163
train_iter_loss: 0.287474125623703
train_iter_loss: 0.4807405173778534
train_iter_loss: 0.2622860074043274
train_iter_loss: 0.1711176335811615
train_iter_loss: 0.1576281189918518
train_iter_loss: 0.222089946269989
train_iter_loss: 0.3735230267047882
train_iter_loss: 0.43995949625968933
train_iter_loss: 0.25067150592803955
train_iter_loss: 0.21671082079410553
train_iter_loss: 0.2696720063686371
train_iter_loss: 0.31446295976638794
train_iter_loss: 0.2840256989002228
train_iter_loss: 0.2683425545692444
train_iter_loss: 0.2976686358451843
train_iter_loss: 0.23182719945907593
train_iter_loss: 0.24489597976207733
train_iter_loss: 0.3136415481567383
train_iter_loss: 0.41052526235580444
train_iter_loss: 0.3100013732910156
train_iter_loss: 0.25367242097854614
train_iter_loss: 0.13893623650074005
train_iter_loss: 0.3509291708469391
train_iter_loss: 0.3252849876880646
train_iter_loss: 0.230635866522789
train_iter_loss: 0.18362928926944733
train_iter_loss: 0.13308465480804443
train_iter_loss: 0.3165886104106903
train_iter_loss: 0.2119234949350357
train_iter_loss: 0.15709534287452698
train_iter_loss: 0.2236206978559494
train_iter_loss: 0.2522665560245514
train_iter_loss: 0.28790074586868286
train_iter_loss: 0.21660564839839935
train_iter_loss: 0.35272687673568726
train_iter_loss: 0.3467060923576355
train_iter_loss: 0.18234552443027496
train_iter_loss: 0.41179659962654114
train_iter_loss: 0.29449713230133057
train_iter_loss: 0.2815149128437042
train_iter_loss: 0.2418137788772583
train_iter_loss: 0.34493452310562134
train_iter_loss: 0.6190302968025208
train_iter_loss: 0.2602963447570801
train_iter_loss: 0.2217244952917099
train_iter_loss: 0.2127779871225357
train_iter_loss: 0.5206215381622314
train_iter_loss: 0.22236952185630798
train_iter_loss: 0.2799692153930664
train_iter_loss: 0.19241218268871307
train_iter_loss: 0.21631377935409546
train_iter_loss: 0.22778908908367157
train_iter_loss: 0.27425116300582886
train_iter_loss: 0.2937225103378296
train_iter_loss: 0.3596639633178711
train_iter_loss: 0.37933826446533203
train_iter_loss: 0.27040165662765503
train_iter_loss: 0.35009026527404785
train_iter_loss: 0.36600980162620544
train_iter_loss: 0.11670753359794617
train_iter_loss: 0.3006030023097992
train_iter_loss: 0.3196514844894409
train_iter_loss: 0.3816472887992859
train_iter_loss: 0.2333844155073166
train_iter_loss: 0.20230230689048767
train_iter_loss: 0.3771997094154358
train_iter_loss: 0.16986306011676788
train_iter_loss: 0.2871907651424408
train_iter_loss: 0.3192064166069031
train_iter_loss: 0.3255521357059479
train_iter_loss: 0.2488875389099121
train_iter_loss: 0.41934141516685486
train_iter_loss: 0.21710297465324402
train_iter_loss: 0.42036986351013184
train_iter_loss: 0.28151702880859375
train_iter_loss: 0.2485816329717636
train_iter_loss: 0.302697092294693
train_iter_loss: 0.16853387653827667
train_iter_loss: 0.1397714912891388
train_iter_loss: 0.37784045934677124
train_iter_loss: 0.4167333245277405
train_iter_loss: 0.35105374455451965
train_iter_loss: 0.3071978688240051
train_iter_loss: 0.3928966224193573
train_iter_loss: 0.24278928339481354
train_iter_loss: 0.2607073187828064
train_iter_loss: 0.17363080382347107
train_iter_loss: 0.31584301590919495
train loss :0.2877
---------------------
Validation seg loss: 0.40162660713079124 at epoch 230
epoch =    231/  1000, exp = train
train_iter_loss: 0.23612253367900848
train_iter_loss: 0.3996540307998657
train_iter_loss: 0.31876257061958313
train_iter_loss: 0.36388638615608215
train_iter_loss: 0.1682860255241394
train_iter_loss: 0.3964715003967285
train_iter_loss: 0.430232971906662
train_iter_loss: 0.2872675955295563
train_iter_loss: 0.027715954929590225
train_iter_loss: 0.15431848168373108
train_iter_loss: 0.3755214512348175
train_iter_loss: 0.5254477858543396
train_iter_loss: 0.31888899207115173
train_iter_loss: 0.4023134410381317
train_iter_loss: 0.3204095661640167
train_iter_loss: 0.23883654177188873
train_iter_loss: 0.4271722435951233
train_iter_loss: 0.3917621076107025
train_iter_loss: 0.33562201261520386
train_iter_loss: 0.2223624885082245
train_iter_loss: 0.26726651191711426
train_iter_loss: 0.2679242193698883
train_iter_loss: 0.3540055453777313
train_iter_loss: 0.11970414221286774
train_iter_loss: 0.26296332478523254
train_iter_loss: 0.3827427327632904
train_iter_loss: 0.37224677205085754
train_iter_loss: 0.2946058213710785
train_iter_loss: 0.19055216014385223
train_iter_loss: 0.4480046033859253
train_iter_loss: 0.30427086353302
train_iter_loss: 0.27934524416923523
train_iter_loss: 0.20188850164413452
train_iter_loss: 0.24361176788806915
train_iter_loss: 0.24409577250480652
train_iter_loss: 0.33570295572280884
train_iter_loss: 0.29267412424087524
train_iter_loss: 0.39705008268356323
train_iter_loss: 0.3961148262023926
train_iter_loss: 0.3389540910720825
train_iter_loss: 0.48249080777168274
train_iter_loss: 0.21101261675357819
train_iter_loss: 0.31962940096855164
train_iter_loss: 0.31063172221183777
train_iter_loss: 0.26068031787872314
train_iter_loss: 0.2639613449573517
train_iter_loss: 0.32116520404815674
train_iter_loss: 0.22075487673282623
train_iter_loss: 0.3842640221118927
train_iter_loss: 0.24925296008586884
train_iter_loss: 0.21475854516029358
train_iter_loss: 0.3022743761539459
train_iter_loss: 0.34529224038124084
train_iter_loss: 0.2599736750125885
train_iter_loss: 0.30455583333969116
train_iter_loss: 0.2321150153875351
train_iter_loss: 0.21684010326862335
train_iter_loss: 0.16251106560230255
train_iter_loss: 0.42399150133132935
train_iter_loss: 0.2862408459186554
train_iter_loss: 0.316500723361969
train_iter_loss: 0.365612268447876
train_iter_loss: 0.16945308446884155
train_iter_loss: 0.17065100371837616
train_iter_loss: 0.20159439742565155
train_iter_loss: 0.36107882857322693
train_iter_loss: 0.19290289282798767
train_iter_loss: 0.39654701948165894
train_iter_loss: 0.23522548377513885
train_iter_loss: 0.45340055227279663
train_iter_loss: 0.19334350526332855
train_iter_loss: 0.3698936998844147
train_iter_loss: 0.3600476384162903
train_iter_loss: 0.23217548429965973
train_iter_loss: 0.1515185832977295
train_iter_loss: 0.2846945524215698
train_iter_loss: 0.25327032804489136
train_iter_loss: 0.3357832729816437
train_iter_loss: 0.332658976316452
train_iter_loss: 0.3133968114852905
train_iter_loss: 0.41135719418525696
train_iter_loss: 0.19390767812728882
train_iter_loss: 0.30348625779151917
train_iter_loss: 0.2975451350212097
train_iter_loss: 0.39795684814453125
train_iter_loss: 0.22018824517726898
train_iter_loss: 0.3455788195133209
train_iter_loss: 0.21779868006706238
train_iter_loss: 0.1679695099592209
train_iter_loss: 0.18491609394550323
train_iter_loss: 0.22887420654296875
train_iter_loss: 0.17189493775367737
train_iter_loss: 0.19682075083255768
train_iter_loss: 0.4353381097316742
train_iter_loss: 0.20513470470905304
train_iter_loss: 0.2711681127548218
train_iter_loss: 0.2802938222885132
train_iter_loss: 0.25407424569129944
train_iter_loss: 0.15722641348838806
train_iter_loss: 0.13759630918502808
train loss :0.2916
---------------------
Validation seg loss: 0.3916834105544214 at epoch 231
epoch =    232/  1000, exp = train
train_iter_loss: 0.20621153712272644
train_iter_loss: 0.2683442533016205
train_iter_loss: 0.33798789978027344
train_iter_loss: 0.2946811616420746
train_iter_loss: 0.37843891978263855
train_iter_loss: 0.1708151251077652
train_iter_loss: 0.1717022955417633
train_iter_loss: 0.3150116801261902
train_iter_loss: 0.5253221392631531
train_iter_loss: 0.2382114827632904
train_iter_loss: 0.3458257019519806
train_iter_loss: 0.3956036865711212
train_iter_loss: 0.2598969042301178
train_iter_loss: 0.23397871851921082
train_iter_loss: 0.2893911600112915
train_iter_loss: 0.38100650906562805
train_iter_loss: 0.23523582518100739
train_iter_loss: 0.3592180013656616
train_iter_loss: 0.3042128086090088
train_iter_loss: 0.2483496516942978
train_iter_loss: 0.18017375469207764
train_iter_loss: 0.4167521297931671
train_iter_loss: 0.16351613402366638
train_iter_loss: 0.3370372951030731
train_iter_loss: 0.18387646973133087
train_iter_loss: 0.19649431109428406
train_iter_loss: 0.22672051191329956
train_iter_loss: 0.44315287470817566
train_iter_loss: 0.1092110425233841
train_iter_loss: 0.3499946594238281
train_iter_loss: 0.1991155743598938
train_iter_loss: 0.2623409032821655
train_iter_loss: 0.20688524842262268
train_iter_loss: 0.30519789457321167
train_iter_loss: 0.23066717386245728
train_iter_loss: 0.22557136416435242
train_iter_loss: 0.32820045948028564
train_iter_loss: 0.31987690925598145
train_iter_loss: 0.22511537373065948
train_iter_loss: 0.15609686076641083
train_iter_loss: 0.26163169741630554
train_iter_loss: 0.3945532441139221
train_iter_loss: 0.2365545779466629
train_iter_loss: 0.30498114228248596
train_iter_loss: 0.34095141291618347
train_iter_loss: 0.3048156797885895
train_iter_loss: 0.18735796213150024
train_iter_loss: 0.225067600607872
train_iter_loss: 0.3692789673805237
train_iter_loss: 0.37392744421958923
train_iter_loss: 0.291132390499115
train_iter_loss: 0.28394854068756104
train_iter_loss: 0.42952868342399597
train_iter_loss: 0.27520307898521423
train_iter_loss: 0.34185174107551575
train_iter_loss: 0.5307440757751465
train_iter_loss: 0.39194509387016296
train_iter_loss: 0.2909529507160187
train_iter_loss: 0.30776867270469666
train_iter_loss: 0.3416827619075775
train_iter_loss: 0.3234584331512451
train_iter_loss: 0.34474796056747437
train_iter_loss: 0.49656784534454346
train_iter_loss: 0.33470919728279114
train_iter_loss: 0.1525694727897644
train_iter_loss: 0.2211734503507614
train_iter_loss: 0.1525086611509323
train_iter_loss: 0.17124563455581665
train_iter_loss: 0.27425140142440796
train_iter_loss: 0.3120906949043274
train_iter_loss: 0.3548508286476135
train_iter_loss: 0.2536039650440216
train_iter_loss: 0.3142404556274414
train_iter_loss: 0.1239115297794342
train_iter_loss: 0.19376401603221893
train_iter_loss: 0.2478051483631134
train_iter_loss: 0.3539390563964844
train_iter_loss: 0.31058230996131897
train_iter_loss: 0.25657156109809875
train_iter_loss: 0.2045721709728241
train_iter_loss: 0.27703967690467834
train_iter_loss: 0.2832569181919098
train_iter_loss: 0.29829803109169006
train_iter_loss: 0.3915846645832062
train_iter_loss: 0.20122897624969482
train_iter_loss: 0.32315734028816223
train_iter_loss: 0.19410037994384766
train_iter_loss: 0.19573314487934113
train_iter_loss: 0.3943318724632263
train_iter_loss: 0.24147988855838776
train_iter_loss: 0.1457996666431427
train_iter_loss: 0.3806430697441101
train_iter_loss: 0.2120005488395691
train_iter_loss: 0.30243179202079773
train_iter_loss: 0.5330075621604919
train_iter_loss: 0.27584928274154663
train_iter_loss: 0.4192424416542053
train_iter_loss: 0.15708529949188232
train_iter_loss: 0.32540005445480347
train_iter_loss: 0.2994612455368042
train loss :0.2905
---------------------
Validation seg loss: 0.35686150017493173 at epoch 232
********************
best_val_epoch_loss:  0.35686150017493173
MODEL UPDATED
epoch =    233/  1000, exp = train
train_iter_loss: 0.36878159642219543
train_iter_loss: 0.3193606734275818
train_iter_loss: 0.38272544741630554
train_iter_loss: 0.22214290499687195
train_iter_loss: 0.3275764286518097
train_iter_loss: 0.14716936647891998
train_iter_loss: 0.22800247371196747
train_iter_loss: 0.32148468494415283
train_iter_loss: 0.39338546991348267
train_iter_loss: 0.17412228882312775
train_iter_loss: 0.22788827121257782
train_iter_loss: 0.21510154008865356
train_iter_loss: 0.27848803997039795
train_iter_loss: 0.3135490119457245
train_iter_loss: 0.2654894292354584
train_iter_loss: 0.28301721811294556
train_iter_loss: 0.2778150737285614
train_iter_loss: 0.3901195228099823
train_iter_loss: 0.5078956484794617
train_iter_loss: 0.26100704073905945
train_iter_loss: 0.20582741498947144
train_iter_loss: 0.3819848299026489
train_iter_loss: 0.2769719660282135
train_iter_loss: 0.41703954339027405
train_iter_loss: 0.13300499320030212
train_iter_loss: 0.2825084328651428
train_iter_loss: 0.27548080682754517
train_iter_loss: 0.1801140457391739
train_iter_loss: 0.2200377881526947
train_iter_loss: 0.22825631499290466
train_iter_loss: 0.262253999710083
train_iter_loss: 0.24546152353286743
train_iter_loss: 0.10651247203350067
train_iter_loss: 0.3115707039833069
train_iter_loss: 0.3995077610015869
train_iter_loss: 0.37068647146224976
train_iter_loss: 0.38201162219047546
train_iter_loss: 0.21328844130039215
train_iter_loss: 0.24443595111370087
train_iter_loss: 0.16915681958198547
train_iter_loss: 0.2923664152622223
train_iter_loss: 0.07100507616996765
train_iter_loss: 0.2853579819202423
train_iter_loss: 0.4326047897338867
train_iter_loss: 0.28256234526634216
train_iter_loss: 0.21558989584445953
train_iter_loss: 0.20189550518989563
train_iter_loss: 0.32754430174827576
train_iter_loss: 0.26979517936706543
train_iter_loss: 0.34715506434440613
train_iter_loss: 0.2820110619068146
train_iter_loss: 0.3403237462043762
train_iter_loss: 0.3333031237125397
train_iter_loss: 0.23771758377552032
train_iter_loss: 0.39222198724746704
train_iter_loss: 0.31949934363365173
train_iter_loss: 0.28482112288475037
train_iter_loss: 0.4346182942390442
train_iter_loss: 0.4039388597011566
train_iter_loss: 0.3231464922428131
train_iter_loss: 0.4055330157279968
train_iter_loss: 0.21465656161308289
train_iter_loss: 0.2571422755718231
train_iter_loss: 0.2219492793083191
train_iter_loss: 0.27739453315734863
train_iter_loss: 0.19565944373607635
train_iter_loss: 0.3023627698421478
train_iter_loss: 0.4698908030986786
train_iter_loss: 0.37666523456573486
train_iter_loss: 0.27322643995285034
train_iter_loss: 0.43251779675483704
train_iter_loss: 0.39755094051361084
train_iter_loss: 0.25101956725120544
train_iter_loss: 0.3911703824996948
train_iter_loss: 0.3426207900047302
train_iter_loss: 0.35068896412849426
train_iter_loss: 0.09126195311546326
train_iter_loss: 0.21114389598369598
train_iter_loss: 0.2684234082698822
train_iter_loss: 0.2821466028690338
train_iter_loss: 0.32271459698677063
train_iter_loss: 0.18469376862049103
train_iter_loss: 0.36207160353660583
train_iter_loss: 0.28705894947052
train_iter_loss: 0.34853437542915344
train_iter_loss: 0.2349923998117447
train_iter_loss: 0.1901501566171646
train_iter_loss: 0.2622973322868347
train_iter_loss: 0.26174601912498474
train_iter_loss: 0.19576670229434967
train_iter_loss: 0.30597513914108276
train_iter_loss: 0.1675184667110443
train_iter_loss: 0.25495362281799316
train_iter_loss: 0.30044135451316833
train_iter_loss: 0.15221218764781952
train_iter_loss: 0.4100130796432495
train_iter_loss: 0.16518667340278625
train_iter_loss: 0.19731229543685913
train_iter_loss: 0.11855898797512054
train_iter_loss: 0.36840733885765076
train loss :0.2868
---------------------
Validation seg loss: 0.37422851799055934 at epoch 233
epoch =    234/  1000, exp = train
train_iter_loss: 0.3071121275424957
train_iter_loss: 0.18698497116565704
train_iter_loss: 0.2362075001001358
train_iter_loss: 0.32017895579338074
train_iter_loss: 0.17798544466495514
train_iter_loss: 0.19374889135360718
train_iter_loss: 0.2532511353492737
train_iter_loss: 0.21126645803451538
train_iter_loss: 0.2350439429283142
train_iter_loss: 0.3993396461009979
train_iter_loss: 0.35254141688346863
train_iter_loss: 0.3965665400028229
train_iter_loss: 0.1567080169916153
train_iter_loss: 0.2565675377845764
train_iter_loss: 0.10048630088567734
train_iter_loss: 0.22518743574619293
train_iter_loss: 0.3545541763305664
train_iter_loss: 0.354862779378891
train_iter_loss: 0.4114736318588257
train_iter_loss: 0.24260322749614716
train_iter_loss: 0.4084782004356384
train_iter_loss: 0.14491088688373566
train_iter_loss: 0.3108465075492859
train_iter_loss: 0.23084521293640137
train_iter_loss: 0.27725112438201904
train_iter_loss: 0.1660296767950058
train_iter_loss: 0.2550220489501953
train_iter_loss: 0.6035951375961304
train_iter_loss: 0.1472390592098236
train_iter_loss: 0.29692021012306213
train_iter_loss: 0.09233736246824265
train_iter_loss: 0.2892119586467743
train_iter_loss: 0.24756832420825958
train_iter_loss: 0.1756771355867386
train_iter_loss: 0.25065839290618896
train_iter_loss: 0.33066555857658386
train_iter_loss: 0.16546232998371124
train_iter_loss: 0.4103231728076935
train_iter_loss: 0.35277122259140015
train_iter_loss: 0.4515008330345154
train_iter_loss: 0.7132011651992798
train_iter_loss: 0.2279004454612732
train_iter_loss: 0.2543514370918274
train_iter_loss: 0.4769621789455414
train_iter_loss: 0.28318166732788086
train_iter_loss: 0.2927631139755249
train_iter_loss: 0.3386586606502533
train_iter_loss: 0.3286583721637726
train_iter_loss: 0.32436224818229675
train_iter_loss: 0.392048716545105
train_iter_loss: 0.25596874952316284
train_iter_loss: 0.24464385211467743
train_iter_loss: 0.2932732105255127
train_iter_loss: 0.2927006185054779
train_iter_loss: 0.34708157181739807
train_iter_loss: 0.2600997984409332
train_iter_loss: 0.4513506591320038
train_iter_loss: 0.31496864557266235
train_iter_loss: 0.35444000363349915
train_iter_loss: 0.3129742741584778
train_iter_loss: 0.31824472546577454
train_iter_loss: 0.16680020093917847
train_iter_loss: 0.19052883982658386
train_iter_loss: 0.3400932252407074
train_iter_loss: 0.19682978093624115
train_iter_loss: 0.22894783318042755
train_iter_loss: 0.21377801895141602
train_iter_loss: 0.14990867674350739
train_iter_loss: 0.22601738572120667
train_iter_loss: 0.2580472230911255
train_iter_loss: 0.37668275833129883
train_iter_loss: 0.5605047941207886
train_iter_loss: 0.3756643533706665
train_iter_loss: 0.30574536323547363
train_iter_loss: 0.2864307165145874
train_iter_loss: 0.22764138877391815
train_iter_loss: 0.2704176902770996
train_iter_loss: 0.2684124708175659
train_iter_loss: 0.38133615255355835
train_iter_loss: 0.41561880707740784
train_iter_loss: 0.44907334446907043
train_iter_loss: 0.2262563854455948
train_iter_loss: 0.2031281292438507
train_iter_loss: 0.28807133436203003
train_iter_loss: 0.18261972069740295
train_iter_loss: 0.18362005054950714
train_iter_loss: 0.23196905851364136
train_iter_loss: 0.22869810461997986
train_iter_loss: 0.2519717216491699
train_iter_loss: 0.2079647183418274
train_iter_loss: 0.33606648445129395
train_iter_loss: 0.44502097368240356
train_iter_loss: 0.18803900480270386
train_iter_loss: 0.38845735788345337
train_iter_loss: 0.3253871500492096
train_iter_loss: 0.37034615874290466
train_iter_loss: 0.23371411859989166
train_iter_loss: 0.39916983246803284
train_iter_loss: 0.23847566545009613
train_iter_loss: 0.29809239506721497
train loss :0.2946
---------------------
Validation seg loss: 0.35943557102373747 at epoch 234
epoch =    235/  1000, exp = train
train_iter_loss: 0.40811726450920105
train_iter_loss: 0.35877525806427
train_iter_loss: 0.29142895340919495
train_iter_loss: 0.31432461738586426
train_iter_loss: 0.16537994146347046
train_iter_loss: 0.19259041547775269
train_iter_loss: 0.4109879434108734
train_iter_loss: 0.3464398980140686
train_iter_loss: 0.23834362626075745
train_iter_loss: 0.3075115382671356
train_iter_loss: 0.3930646479129791
train_iter_loss: 0.43287426233291626
train_iter_loss: 0.2836857736110687
train_iter_loss: 0.29158085584640503
train_iter_loss: 0.33774012327194214
train_iter_loss: 0.10289530456066132
train_iter_loss: 0.16935232281684875
train_iter_loss: 0.2646735608577728
train_iter_loss: 0.27418169379234314
train_iter_loss: 0.31045663356781006
train_iter_loss: 0.3876628875732422
train_iter_loss: 0.22805990278720856
train_iter_loss: 0.19816188514232635
train_iter_loss: 0.1920897364616394
train_iter_loss: 0.42141738533973694
train_iter_loss: 0.37443679571151733
train_iter_loss: 0.36944958567619324
train_iter_loss: 0.22468321025371552
train_iter_loss: 0.4019642174243927
train_iter_loss: 0.21384982764720917
train_iter_loss: 0.46152088046073914
train_iter_loss: 0.3657289147377014
train_iter_loss: 0.2428206503391266
train_iter_loss: 0.17498569190502167
train_iter_loss: 0.420479416847229
train_iter_loss: 0.5364797711372375
train_iter_loss: 0.2814183533191681
train_iter_loss: 0.35581162571907043
train_iter_loss: 0.29491138458251953
train_iter_loss: 0.3361666798591614
train_iter_loss: 0.30549314618110657
train_iter_loss: 0.37483423948287964
train_iter_loss: 0.12654662132263184
train_iter_loss: 0.3272157609462738
train_iter_loss: 0.31829050183296204
train_iter_loss: 0.21832919120788574
train_iter_loss: 0.18929697573184967
train_iter_loss: 0.36542558670043945
train_iter_loss: 0.16626082360744476
train_iter_loss: 0.15158087015151978
train_iter_loss: 0.4017815887928009
train_iter_loss: 0.19207212328910828
train_iter_loss: 0.2791355848312378
train_iter_loss: 0.3909989893436432
train_iter_loss: 0.17658959329128265
train_iter_loss: 0.2879476845264435
train_iter_loss: 0.18097633123397827
train_iter_loss: 0.26085805892944336
train_iter_loss: 0.3077886700630188
train_iter_loss: 0.3199251592159271
train_iter_loss: 0.14706826210021973
train_iter_loss: 0.19821235537528992
train_iter_loss: 0.26239195466041565
train_iter_loss: 0.3196447789669037
train_iter_loss: 0.29857540130615234
train_iter_loss: 0.39811620116233826
train_iter_loss: 0.219493106007576
train_iter_loss: 0.2662522792816162
train_iter_loss: 0.15637925267219543
train_iter_loss: 0.20490515232086182
train_iter_loss: 0.23587381839752197
train_iter_loss: 0.2789306640625
train_iter_loss: 0.24102847278118134
train_iter_loss: 0.20658883452415466
train_iter_loss: 0.2043820023536682
train_iter_loss: 0.3944069445133209
train_iter_loss: 0.4488278031349182
train_iter_loss: 0.22530922293663025
train_iter_loss: 0.21722760796546936
train_iter_loss: 0.2982136309146881
train_iter_loss: 0.28499913215637207
train_iter_loss: 0.40317973494529724
train_iter_loss: 0.27645283937454224
train_iter_loss: 0.3247377872467041
train_iter_loss: 0.3451797664165497
train_iter_loss: 0.18076100945472717
train_iter_loss: 0.2776440382003784
train_iter_loss: 0.2884310483932495
train_iter_loss: 0.33424344658851624
train_iter_loss: 0.16733212769031525
train_iter_loss: 0.45261141657829285
train_iter_loss: 0.33219000697135925
train_iter_loss: 0.30984190106391907
train_iter_loss: 0.30212464928627014
train_iter_loss: 0.24054229259490967
train_iter_loss: 0.35020557045936584
train_iter_loss: 0.21851223707199097
train_iter_loss: 0.11734561622142792
train_iter_loss: 0.23775522410869598
train_iter_loss: 0.16312308609485626
train loss :0.2884
---------------------
Validation seg loss: 0.36953144703271257 at epoch 235
epoch =    236/  1000, exp = train
train_iter_loss: 0.23951834440231323
train_iter_loss: 0.1984412670135498
train_iter_loss: 0.3419143557548523
train_iter_loss: 0.3873547613620758
train_iter_loss: 0.2673269212245941
train_iter_loss: 0.6448489427566528
train_iter_loss: 0.21298010647296906
train_iter_loss: 0.3150806427001953
train_iter_loss: 0.4320204257965088
train_iter_loss: 0.26173505187034607
train_iter_loss: 0.22719435393810272
train_iter_loss: 0.6089977025985718
train_iter_loss: 0.4032438397407532
train_iter_loss: 0.2978816330432892
train_iter_loss: 0.22803030908107758
train_iter_loss: 0.13082683086395264
train_iter_loss: 0.20929712057113647
train_iter_loss: 0.1334138959646225
train_iter_loss: 0.13626570999622345
train_iter_loss: 0.16396115720272064
train_iter_loss: 0.3868850767612457
train_iter_loss: 0.3790307641029358
train_iter_loss: 0.38815924525260925
train_iter_loss: 0.3487771451473236
train_iter_loss: 0.2558092176914215
train_iter_loss: 0.22881047427654266
train_iter_loss: 0.1276879608631134
train_iter_loss: 0.15236712992191315
train_iter_loss: 0.2588416039943695
train_iter_loss: 0.2640005350112915
train_iter_loss: 0.21284982562065125
train_iter_loss: 0.321039617061615
train_iter_loss: 0.10037481784820557
train_iter_loss: 0.20054912567138672
train_iter_loss: 0.28357425332069397
train_iter_loss: 0.3386291563510895
train_iter_loss: 0.362518846988678
train_iter_loss: 0.27556705474853516
train_iter_loss: 0.34979790449142456
train_iter_loss: 0.4309730529785156
train_iter_loss: 0.28545668721199036
train_iter_loss: 0.3088044822216034
train_iter_loss: 0.36795076727867126
train_iter_loss: 0.457904577255249
train_iter_loss: 0.23096928000450134
train_iter_loss: 0.19357991218566895
train_iter_loss: 0.35325887799263
train_iter_loss: 0.22409820556640625
train_iter_loss: 0.32708388566970825
train_iter_loss: 0.1926419734954834
train_iter_loss: 0.3966810405254364
train_iter_loss: 0.2690086364746094
train_iter_loss: 0.21616819500923157
train_iter_loss: 0.4301810562610626
train_iter_loss: 0.3803086578845978
train_iter_loss: 0.3623999059200287
train_iter_loss: 0.15229903161525726
train_iter_loss: 0.3542499840259552
train_iter_loss: 0.27489495277404785
train_iter_loss: 0.33907896280288696
train_iter_loss: 0.3308864235877991
train_iter_loss: 0.34744998812675476
train_iter_loss: 0.1721690595149994
train_iter_loss: 0.4436566233634949
train_iter_loss: 0.21584996581077576
train_iter_loss: 0.3699646592140198
train_iter_loss: 0.4418013095855713
train_iter_loss: 0.14527641236782074
train_iter_loss: 0.36269164085388184
train_iter_loss: 0.2091294676065445
train_iter_loss: 0.5435898303985596
train_iter_loss: 0.2290300726890564
train_iter_loss: 0.23552480340003967
train_iter_loss: 0.20710133016109467
train_iter_loss: 0.3207715153694153
train_iter_loss: 0.15074779093265533
train_iter_loss: 0.21790176630020142
train_iter_loss: 0.2792441248893738
train_iter_loss: 0.2538016438484192
train_iter_loss: 0.2886636555194855
train_iter_loss: 0.26279982924461365
train_iter_loss: 0.31645268201828003
train_iter_loss: 0.1758861392736435
train_iter_loss: 0.15730011463165283
train_iter_loss: 0.30259597301483154
train_iter_loss: 0.19852733612060547
train_iter_loss: 0.3075072169303894
train_iter_loss: 0.16268640756607056
train_iter_loss: 0.19451890885829926
train_iter_loss: 0.27977219223976135
train_iter_loss: 0.25819531083106995
train_iter_loss: 0.31030476093292236
train_iter_loss: 0.46114763617515564
train_iter_loss: 0.10808677226305008
train_iter_loss: 0.21504129469394684
train_iter_loss: 0.20245684683322906
train_iter_loss: 0.28781750798225403
train_iter_loss: 0.36983686685562134
train_iter_loss: 0.319751501083374
train_iter_loss: 0.4248175621032715
train loss :0.2899
---------------------
Validation seg loss: 0.36931354360092644 at epoch 236
epoch =    237/  1000, exp = train
train_iter_loss: 0.16141247749328613
train_iter_loss: 0.35464122891426086
train_iter_loss: 0.18250466883182526
train_iter_loss: 0.3057573139667511
train_iter_loss: 0.2608397305011749
train_iter_loss: 0.4585095942020416
train_iter_loss: 0.3075598478317261
train_iter_loss: 0.22401896119117737
train_iter_loss: 0.3434484601020813
train_iter_loss: 0.33304551243782043
train_iter_loss: 0.17001257836818695
train_iter_loss: 0.22989563643932343
train_iter_loss: 0.24280375242233276
train_iter_loss: 0.41422733664512634
train_iter_loss: 0.25025829672813416
train_iter_loss: 0.22000788152217865
train_iter_loss: 0.3578431308269501
train_iter_loss: 0.196223184466362
train_iter_loss: 0.1691133677959442
train_iter_loss: 0.4450565576553345
train_iter_loss: 0.19637727737426758
train_iter_loss: 0.3685550391674042
train_iter_loss: 0.36638879776000977
train_iter_loss: 0.2536569833755493
train_iter_loss: 0.33474257588386536
train_iter_loss: 0.3671879470348358
train_iter_loss: 0.21116326749324799
train_iter_loss: 0.29877370595932007
train_iter_loss: 0.32519087195396423
train_iter_loss: 0.19272162020206451
train_iter_loss: 0.47990843653678894
train_iter_loss: 0.30913296341896057
train_iter_loss: 0.40314796566963196
train_iter_loss: 0.3184968829154968
train_iter_loss: 0.3798975944519043
train_iter_loss: 0.33771026134490967
train_iter_loss: 0.27752041816711426
train_iter_loss: 0.2528722286224365
train_iter_loss: 0.2084820419549942
train_iter_loss: 0.19294601678848267
train_iter_loss: 0.24683848023414612
train_iter_loss: 0.20202402770519257
train_iter_loss: 0.24133162200450897
train_iter_loss: 0.2170669138431549
train_iter_loss: 0.34358280897140503
train_iter_loss: 0.40169963240623474
train_iter_loss: 0.4462905526161194
train_iter_loss: 0.10107897222042084
train_iter_loss: 0.3311598002910614
train_iter_loss: 0.3242855966091156
train_iter_loss: 0.31282341480255127
train_iter_loss: 0.3143148422241211
train_iter_loss: 0.2616307735443115
train_iter_loss: 0.15657919645309448
train_iter_loss: 0.3795216381549835
train_iter_loss: 0.22330056130886078
train_iter_loss: 0.22281430661678314
train_iter_loss: 0.3917974829673767
train_iter_loss: 0.4269298017024994
train_iter_loss: 0.22002865374088287
train_iter_loss: 0.3122575581073761
train_iter_loss: 0.28815603256225586
train_iter_loss: 0.32223016023635864
train_iter_loss: 0.20040158927440643
train_iter_loss: 0.257306843996048
train_iter_loss: 0.2510058879852295
train_iter_loss: 0.3379865884780884
train_iter_loss: 0.11403848975896835
train_iter_loss: 0.38600364327430725
train_iter_loss: 0.36683571338653564
train_iter_loss: 0.14900247752666473
train_iter_loss: 0.05620604008436203
train_iter_loss: 0.14402370154857635
train_iter_loss: 0.2335144579410553
train_iter_loss: 0.41267338395118713
train_iter_loss: 0.07917424291372299
train_iter_loss: 0.2522536516189575
train_iter_loss: 0.3117373287677765
train_iter_loss: 0.21307101845741272
train_iter_loss: 0.2222963571548462
train_iter_loss: 0.28980153799057007
train_iter_loss: 0.26524272561073303
train_iter_loss: 0.2610195279121399
train_iter_loss: 0.4207683801651001
train_iter_loss: 0.2841375172138214
train_iter_loss: 0.13462691009044647
train_iter_loss: 0.20751817524433136
train_iter_loss: 0.21830520033836365
train_iter_loss: 0.23230920732021332
train_iter_loss: 0.46648427844047546
train_iter_loss: 0.3374812602996826
train_iter_loss: 0.22481992840766907
train_iter_loss: 0.2833764851093292
train_iter_loss: 0.1506527215242386
train_iter_loss: 0.3032415211200714
train_iter_loss: 0.390623539686203
train_iter_loss: 0.17798562347888947
train_iter_loss: 0.41240233182907104
train_iter_loss: 0.26081010699272156
train_iter_loss: 0.3910406231880188
train loss :0.2838
---------------------
Validation seg loss: 0.37982405424293764 at epoch 237
epoch =    238/  1000, exp = train
train_iter_loss: 0.312337726354599
train_iter_loss: 0.41008394956588745
train_iter_loss: 0.19743160903453827
train_iter_loss: 0.18722976744174957
train_iter_loss: 0.2446635514497757
train_iter_loss: 0.2652253210544586
train_iter_loss: 0.13784396648406982
train_iter_loss: 0.1232486292719841
train_iter_loss: 0.22041137516498566
train_iter_loss: 0.14909039437770844
train_iter_loss: 0.28331562876701355
train_iter_loss: 0.1397273689508438
train_iter_loss: 0.2156219333410263
train_iter_loss: 0.28264448046684265
train_iter_loss: 0.5924915075302124
train_iter_loss: 0.33526942133903503
train_iter_loss: 0.2924362123012543
train_iter_loss: 0.4054329991340637
train_iter_loss: 0.2775479257106781
train_iter_loss: 0.3388868272304535
train_iter_loss: 0.18849973380565643
train_iter_loss: 0.3322911262512207
train_iter_loss: 0.36050471663475037
train_iter_loss: 0.26343604922294617
train_iter_loss: 0.30466046929359436
train_iter_loss: 0.21475781500339508
train_iter_loss: 0.28184378147125244
train_iter_loss: 0.2389746457338333
train_iter_loss: 0.3526464104652405
train_iter_loss: 0.6780040860176086
train_iter_loss: 0.34289610385894775
train_iter_loss: 0.11388199776411057
train_iter_loss: 0.4068734645843506
train_iter_loss: 0.3203767240047455
train_iter_loss: 0.4275166690349579
train_iter_loss: 0.21305391192436218
train_iter_loss: 0.23685045540332794
train_iter_loss: 0.1380370706319809
train_iter_loss: 0.21581551432609558
train_iter_loss: 0.37406259775161743
train_iter_loss: 0.1836542934179306
train_iter_loss: 0.23641535639762878
train_iter_loss: 0.30542662739753723
train_iter_loss: 0.35418811440467834
train_iter_loss: 0.2576107680797577
train_iter_loss: 0.45822998881340027
train_iter_loss: 0.1913507878780365
train_iter_loss: 0.11635791510343552
train_iter_loss: 0.25176286697387695
train_iter_loss: 0.2653751075267792
train_iter_loss: 0.3508554995059967
train_iter_loss: 0.33507680892944336
train_iter_loss: 0.21070291101932526
train_iter_loss: 0.3240821361541748
train_iter_loss: 0.238811194896698
train_iter_loss: 0.3107415735721588
train_iter_loss: 0.24454903602600098
train_iter_loss: 0.3024124801158905
train_iter_loss: 0.19498054683208466
train_iter_loss: 0.3735566735267639
train_iter_loss: 0.37479639053344727
train_iter_loss: 0.2631414234638214
train_iter_loss: 0.29988956451416016
train_iter_loss: 0.29570406675338745
train_iter_loss: 0.18846693634986877
train_iter_loss: 0.36845555901527405
train_iter_loss: 0.232183039188385
train_iter_loss: 0.2479761838912964
train_iter_loss: 0.2733883857727051
train_iter_loss: 0.27006515860557556
train_iter_loss: 0.24634744226932526
train_iter_loss: 0.2740899622440338
train_iter_loss: 0.4220500886440277
train_iter_loss: 0.36541125178337097
train_iter_loss: 0.305031955242157
train_iter_loss: 0.1885332465171814
train_iter_loss: 0.3320915102958679
train_iter_loss: 0.2654639184474945
train_iter_loss: 0.30259281396865845
train_iter_loss: 0.44920995831489563
train_iter_loss: 0.3172221779823303
train_iter_loss: 0.46306416392326355
train_iter_loss: 0.35480552911758423
train_iter_loss: 0.1974436640739441
train_iter_loss: 0.19035688042640686
train_iter_loss: 0.14766424894332886
train_iter_loss: 0.3446744680404663
train_iter_loss: 0.3092796802520752
train_iter_loss: 0.19606253504753113
train_iter_loss: 0.3414362370967865
train_iter_loss: 0.22911857068538666
train_iter_loss: 0.3455570936203003
train_iter_loss: 0.2981540560722351
train_iter_loss: 0.35909777879714966
train_iter_loss: 0.2809295058250427
train_iter_loss: 0.3703356683254242
train_iter_loss: 0.26284879446029663
train_iter_loss: 0.33023756742477417
train_iter_loss: 0.31865859031677246
train_iter_loss: 0.23909971117973328
train loss :0.2907
---------------------
Validation seg loss: 0.3674360558056747 at epoch 238
epoch =    239/  1000, exp = train
train_iter_loss: 0.208902046084404
train_iter_loss: 0.15980228781700134
train_iter_loss: 0.3199891448020935
train_iter_loss: 0.19528613984584808
train_iter_loss: 0.3269382417201996
train_iter_loss: 0.28474313020706177
train_iter_loss: 0.18060576915740967
train_iter_loss: 0.2661263644695282
train_iter_loss: 0.20143304765224457
train_iter_loss: 0.3061327338218689
train_iter_loss: 0.44945070147514343
train_iter_loss: 0.33763599395751953
train_iter_loss: 0.3322603106498718
train_iter_loss: 0.3576276898384094
train_iter_loss: 0.22121675312519073
train_iter_loss: 0.26285284757614136
train_iter_loss: 0.20712769031524658
train_iter_loss: 0.41339215636253357
train_iter_loss: 0.18235617876052856
train_iter_loss: 0.2422708421945572
train_iter_loss: 0.2956736385822296
train_iter_loss: 0.3730865716934204
train_iter_loss: 0.32618898153305054
train_iter_loss: 0.33636587858200073
train_iter_loss: 0.30236783623695374
train_iter_loss: 0.21936944127082825
train_iter_loss: 0.3133634924888611
train_iter_loss: 0.1376393437385559
train_iter_loss: 0.34295353293418884
train_iter_loss: 0.30910950899124146
train_iter_loss: 0.3372874855995178
train_iter_loss: 0.21435344219207764
train_iter_loss: 0.25129249691963196
train_iter_loss: 0.3165391981601715
train_iter_loss: 0.22045540809631348
train_iter_loss: 0.38272178173065186
train_iter_loss: 0.1988796591758728
train_iter_loss: 0.2142104059457779
train_iter_loss: 0.28639698028564453
train_iter_loss: 0.20440176129341125
train_iter_loss: 0.24128739535808563
train_iter_loss: 0.33975687623023987
train_iter_loss: 0.37484094500541687
train_iter_loss: 0.17665164172649384
train_iter_loss: 0.2848733961582184
train_iter_loss: 0.19302521646022797
train_iter_loss: 0.22128555178642273
train_iter_loss: 0.26620715856552124
train_iter_loss: 0.22518399357795715
train_iter_loss: 0.22964602708816528
train_iter_loss: 0.4747350513935089
train_iter_loss: 0.1766631007194519
train_iter_loss: 0.28833287954330444
train_iter_loss: 0.3097683787345886
train_iter_loss: 0.36089569330215454
train_iter_loss: 0.23762542009353638
train_iter_loss: 0.1964111030101776
train_iter_loss: 0.13837413489818573
train_iter_loss: 0.26759281754493713
train_iter_loss: 0.3690139055252075
train_iter_loss: 0.3002931773662567
train_iter_loss: 0.22152036428451538
train_iter_loss: 0.14622169733047485
train_iter_loss: 0.3948250114917755
train_iter_loss: 0.4854154586791992
train_iter_loss: 0.37743720412254333
train_iter_loss: 0.2175101339817047
train_iter_loss: 0.2691991627216339
train_iter_loss: 0.3117128908634186
train_iter_loss: 0.19683930277824402
train_iter_loss: 0.3379596769809723
train_iter_loss: 0.40303757786750793
train_iter_loss: 0.3783618211746216
train_iter_loss: 0.18403224647045135
train_iter_loss: 0.3740999698638916
train_iter_loss: 0.2313564568758011
train_iter_loss: 0.2406025379896164
train_iter_loss: 0.26866018772125244
train_iter_loss: 0.3409825563430786
train_iter_loss: 0.36605075001716614
train_iter_loss: 0.1308329552412033
train_iter_loss: 0.24974006414413452
train_iter_loss: 0.2886590361595154
train_iter_loss: 0.24030981957912445
train_iter_loss: 0.2800801992416382
train_iter_loss: 0.32700490951538086
train_iter_loss: 0.3778947591781616
train_iter_loss: 0.28460291028022766
train_iter_loss: 0.24701805412769318
train_iter_loss: 0.21908676624298096
train_iter_loss: 0.27065882086753845
train_iter_loss: 0.42718538641929626
train_iter_loss: 0.4507286548614502
train_iter_loss: 0.4092654287815094
train_iter_loss: 0.2474474310874939
train_iter_loss: 0.19011513888835907
train_iter_loss: 0.3601919710636139
train_iter_loss: 0.5091464519500732
train_iter_loss: 0.32314372062683105
train_iter_loss: 0.3217051029205322
train loss :0.2890
---------------------
Validation seg loss: 0.37902644735251395 at epoch 239
epoch =    240/  1000, exp = train
train_iter_loss: 0.376956969499588
train_iter_loss: 0.21076011657714844
train_iter_loss: 0.24756494164466858
train_iter_loss: 0.2555862069129944
train_iter_loss: 0.2663247883319855
train_iter_loss: 0.2320907711982727
train_iter_loss: 0.29934126138687134
train_iter_loss: 0.293903648853302
train_iter_loss: 0.3419821262359619
train_iter_loss: 0.3279796540737152
train_iter_loss: 0.2973315119743347
train_iter_loss: 0.513538122177124
train_iter_loss: 0.262876033782959
train_iter_loss: 0.28099310398101807
train_iter_loss: 0.33518028259277344
train_iter_loss: 0.31772181391716003
train_iter_loss: 0.2864288091659546
train_iter_loss: 0.233224019408226
train_iter_loss: 0.2624410390853882
train_iter_loss: 0.33413976430892944
train_iter_loss: 0.2096661776304245
train_iter_loss: 0.37350234389305115
train_iter_loss: 0.33650654554367065
train_iter_loss: 0.10044759511947632
train_iter_loss: 0.29283663630485535
train_iter_loss: 0.26072263717651367
train_iter_loss: 0.14233092963695526
train_iter_loss: 0.1986997127532959
train_iter_loss: 0.3498854339122772
train_iter_loss: 0.11315269023180008
train_iter_loss: 0.30361688137054443
train_iter_loss: 0.3994584381580353
train_iter_loss: 0.3424474895000458
train_iter_loss: 0.4534560441970825
train_iter_loss: 0.2648193836212158
train_iter_loss: 0.35734230279922485
train_iter_loss: 0.17679087817668915
train_iter_loss: 0.3411596715450287
train_iter_loss: 0.2562701106071472
train_iter_loss: 0.1194574385881424
train_iter_loss: 0.31705915927886963
train_iter_loss: 0.3232788145542145
train_iter_loss: 0.24473810195922852
train_iter_loss: 0.3014245629310608
train_iter_loss: 0.48032426834106445
train_iter_loss: 0.24632228910923004
train_iter_loss: 0.3614833354949951
train_iter_loss: 0.24494633078575134
train_iter_loss: 0.3468483090400696
train_iter_loss: 0.18072105944156647
train_iter_loss: 0.17889761924743652
train_iter_loss: 0.22339947521686554
train_iter_loss: 0.28827300667762756
train_iter_loss: 0.2126559168100357
train_iter_loss: 0.17266510426998138
train_iter_loss: 0.2531265914440155
train_iter_loss: 0.17957301437854767
train_iter_loss: 0.335899293422699
train_iter_loss: 0.23088200390338898
train_iter_loss: 0.3060767948627472
train_iter_loss: 0.11875982582569122
train_iter_loss: 0.41970095038414
train_iter_loss: 0.42165595293045044
train_iter_loss: 0.25065526366233826
train_iter_loss: 0.28433066606521606
train_iter_loss: 0.18734689056873322
train_iter_loss: 0.17580021917819977
train_iter_loss: 0.29209867119789124
train_iter_loss: 0.25713419914245605
train_iter_loss: 0.23011918365955353
train_iter_loss: 0.1924426108598709
train_iter_loss: 0.34255364537239075
train_iter_loss: 0.3196106553077698
train_iter_loss: 0.30690866708755493
train_iter_loss: 0.17188182473182678
train_iter_loss: 0.2586384415626526
train_iter_loss: 0.4067753851413727
train_iter_loss: 0.22232641279697418
train_iter_loss: 0.31961479783058167
train_iter_loss: 0.32549646496772766
train_iter_loss: 0.29964974522590637
train_iter_loss: 0.2516556680202484
train_iter_loss: 0.2247736006975174
train_iter_loss: 0.289105087518692
train_iter_loss: 0.20274563133716583
train_iter_loss: 0.2259269654750824
train_iter_loss: 0.2890263497829437
train_iter_loss: 0.38377293944358826
train_iter_loss: 0.3037511706352234
train_iter_loss: 0.31027787923812866
train_iter_loss: 0.05551379919052124
train_iter_loss: 0.2428945004940033
train_iter_loss: 0.24940915405750275
train_iter_loss: 0.40685734152793884
train_iter_loss: 0.2685421407222748
train_iter_loss: 0.326742947101593
train_iter_loss: 0.39721354842185974
train_iter_loss: 0.43833187222480774
train_iter_loss: 0.34262534976005554
train_iter_loss: 0.2590135931968689
train loss :0.2836
---------------------
Validation seg loss: 0.3612601226810717 at epoch 240
epoch =    241/  1000, exp = train
train_iter_loss: 0.27697357535362244
train_iter_loss: 0.22812554240226746
train_iter_loss: 0.33086180686950684
train_iter_loss: 0.43518686294555664
train_iter_loss: 0.2428218424320221
train_iter_loss: 0.21574297547340393
train_iter_loss: 0.326589971780777
train_iter_loss: 0.25955912470817566
train_iter_loss: 0.2668965458869934
train_iter_loss: 0.3549622893333435
train_iter_loss: 0.39516428112983704
train_iter_loss: 0.13732990622520447
train_iter_loss: 0.19661512970924377
train_iter_loss: 0.40910935401916504
train_iter_loss: 0.3747730851173401
train_iter_loss: 0.26900896430015564
train_iter_loss: 0.28621959686279297
train_iter_loss: 0.4165336489677429
train_iter_loss: 0.24456216394901276
train_iter_loss: 0.13953685760498047
train_iter_loss: 0.22359995543956757
train_iter_loss: 0.2275354266166687
train_iter_loss: 0.37239423394203186
train_iter_loss: 0.3739631772041321
train_iter_loss: 0.2916821539402008
train_iter_loss: 0.276313453912735
train_iter_loss: 0.26027560234069824
train_iter_loss: 0.16954395174980164
train_iter_loss: 0.2782871425151825
train_iter_loss: 0.19764965772628784
train_iter_loss: 0.1630374938249588
train_iter_loss: 0.13038082420825958
train_iter_loss: 0.2850451171398163
train_iter_loss: 0.2502616047859192
train_iter_loss: 0.28663739562034607
train_iter_loss: 0.2496839314699173
train_iter_loss: 0.24540922045707703
train_iter_loss: 0.47862833738327026
train_iter_loss: 0.2950091063976288
train_iter_loss: 0.3154306411743164
train_iter_loss: 0.2878779172897339
train_iter_loss: 0.2869816720485687
train_iter_loss: 0.3341718316078186
train_iter_loss: 0.26927849650382996
train_iter_loss: 0.18061186373233795
train_iter_loss: 0.31647390127182007
train_iter_loss: 0.30405542254447937
train_iter_loss: 0.4183098375797272
train_iter_loss: 0.4610661268234253
train_iter_loss: 0.3150576949119568
train_iter_loss: 0.3475188612937927
train_iter_loss: 0.31264856457710266
train_iter_loss: 0.3564012050628662
train_iter_loss: 0.35779884457588196
train_iter_loss: 0.3061637282371521
train_iter_loss: 0.2981509268283844
train_iter_loss: 0.2661686837673187
train_iter_loss: 0.28120964765548706
train_iter_loss: 0.24872837960720062
train_iter_loss: 0.2825067639350891
train_iter_loss: 0.4040890336036682
train_iter_loss: 0.2992763817310333
train_iter_loss: 0.2201157808303833
train_iter_loss: 0.08937028050422668
train_iter_loss: 0.2757251560688019
train_iter_loss: 0.23490473628044128
train_iter_loss: 0.2306123822927475
train_iter_loss: 0.22882267832756042
train_iter_loss: 0.1686890572309494
train_iter_loss: 0.14532029628753662
train_iter_loss: 0.2554004490375519
train_iter_loss: 0.5260228514671326
train_iter_loss: 0.554955244064331
train_iter_loss: 0.2380802035331726
train_iter_loss: 0.4253455102443695
train_iter_loss: 0.1626288890838623
train_iter_loss: 0.2860543429851532
train_iter_loss: 0.24160932004451752
train_iter_loss: 0.20750053226947784
train_iter_loss: 0.46020737290382385
train_iter_loss: 0.16609157621860504
train_iter_loss: 0.2856524586677551
train_iter_loss: 0.19220580160617828
train_iter_loss: 0.2067127525806427
train_iter_loss: 0.2169545590877533
train_iter_loss: 0.1570911705493927
train_iter_loss: 0.12800651788711548
train_iter_loss: 0.22221362590789795
train_iter_loss: 0.23567897081375122
train_iter_loss: 0.3103911280632019
train_iter_loss: 0.395203560590744
train_iter_loss: 0.27930429577827454
train_iter_loss: 0.3276492655277252
train_iter_loss: 0.23835276067256927
train_iter_loss: 0.41637998819351196
train_iter_loss: 0.3645388185977936
train_iter_loss: 0.21716469526290894
train_iter_loss: 0.3456256091594696
train_iter_loss: 0.162759467959404
train_iter_loss: 0.36301177740097046
train loss :0.2858
---------------------
Validation seg loss: 0.365103517166989 at epoch 241
epoch =    242/  1000, exp = train
train_iter_loss: 0.16112180054187775
train_iter_loss: 0.3022960126399994
train_iter_loss: 0.3024033308029175
train_iter_loss: 0.2626895010471344
train_iter_loss: 0.20084984600543976
train_iter_loss: 0.1711214780807495
train_iter_loss: 0.18454736471176147
train_iter_loss: 0.30938905477523804
train_iter_loss: 0.23934794962406158
train_iter_loss: 0.16277951002120972
train_iter_loss: 0.2609751522541046
train_iter_loss: 0.2699006199836731
train_iter_loss: 0.4142376780509949
train_iter_loss: 0.1965574026107788
train_iter_loss: 0.20454953610897064
train_iter_loss: 0.3392447531223297
train_iter_loss: 0.3383345305919647
train_iter_loss: 0.3249945640563965
train_iter_loss: 0.33262065052986145
train_iter_loss: 0.4217168688774109
train_iter_loss: 0.10767204314470291
train_iter_loss: 0.3837003707885742
train_iter_loss: 0.3697799742221832
train_iter_loss: 0.4011991620063782
train_iter_loss: 0.22723662853240967
train_iter_loss: 0.4194088280200958
train_iter_loss: 0.3570954501628876
train_iter_loss: 0.20274756848812103
train_iter_loss: 0.3095191717147827
train_iter_loss: 0.339765727519989
train_iter_loss: 0.33044150471687317
train_iter_loss: 0.28779280185699463
train_iter_loss: 0.3340097665786743
train_iter_loss: 0.3231029510498047
train_iter_loss: 0.30408135056495667
train_iter_loss: 0.3135148882865906
train_iter_loss: 0.18075397610664368
train_iter_loss: 0.45128199458122253
train_iter_loss: 0.28639695048332214
train_iter_loss: 0.22074203193187714
train_iter_loss: 0.22650861740112305
train_iter_loss: 0.2874816060066223
train_iter_loss: 0.2569326162338257
train_iter_loss: 0.34554022550582886
train_iter_loss: 0.23440779745578766
train_iter_loss: 0.31841692328453064
train_iter_loss: 0.301031231880188
train_iter_loss: 0.1946386694908142
train_iter_loss: 0.4114164710044861
train_iter_loss: 0.31917867064476013
train_iter_loss: 0.23919044435024261
train_iter_loss: 0.17474478483200073
train_iter_loss: 0.35954010486602783
train_iter_loss: 0.3079149127006531
train_iter_loss: 0.2765958309173584
train_iter_loss: 0.2865292727947235
train_iter_loss: 0.2671706974506378
train_iter_loss: 0.19066627323627472
train_iter_loss: 0.07921929657459259
train_iter_loss: 0.1921505182981491
train_iter_loss: 0.07108442485332489
train_iter_loss: 0.2064605951309204
train_iter_loss: 0.5143041014671326
train_iter_loss: 0.23914501070976257
train_iter_loss: 0.24068036675453186
train_iter_loss: 0.23352664709091187
train_iter_loss: 0.29940882325172424
train_iter_loss: 0.26342716813087463
train_iter_loss: 0.3320430815219879
train_iter_loss: 0.25128450989723206
train_iter_loss: 0.29042142629623413
train_iter_loss: 0.5326413512229919
train_iter_loss: 0.11226153373718262
train_iter_loss: 0.3642839789390564
train_iter_loss: 0.1278393268585205
train_iter_loss: 0.25389960408210754
train_iter_loss: 0.2232927680015564
train_iter_loss: 0.3788038194179535
train_iter_loss: 0.235955148935318
train_iter_loss: 0.29387423396110535
train_iter_loss: 0.24286745488643646
train_iter_loss: 0.32344967126846313
train_iter_loss: 0.39909473061561584
train_iter_loss: 0.2628665566444397
train_iter_loss: 0.2768363654613495
train_iter_loss: 0.6324478387832642
train_iter_loss: 0.3023693561553955
train_iter_loss: 0.36979398131370544
train_iter_loss: 0.22482222318649292
train_iter_loss: 0.21458236873149872
train_iter_loss: 0.18155331909656525
train_iter_loss: 0.3141823709011078
train_iter_loss: 0.247634619474411
train_iter_loss: 0.3355919420719147
train_iter_loss: 0.27646583318710327
train_iter_loss: 0.32102420926094055
train_iter_loss: 0.33041757345199585
train_iter_loss: 0.24544869363307953
train_iter_loss: 0.22943218052387238
train_iter_loss: 0.29501575231552124
train loss :0.2860
---------------------
Validation seg loss: 0.36913193672684563 at epoch 242
epoch =    243/  1000, exp = train
train_iter_loss: 0.2683817744255066
train_iter_loss: 0.3529352843761444
train_iter_loss: 0.343368262052536
train_iter_loss: 0.2687954604625702
train_iter_loss: 0.2391449362039566
train_iter_loss: 0.37810012698173523
train_iter_loss: 0.22421963512897491
train_iter_loss: 0.19982211291790009
train_iter_loss: 0.2155756950378418
train_iter_loss: 0.20675180852413177
train_iter_loss: 0.29477202892303467
train_iter_loss: 0.1779133677482605
train_iter_loss: 0.31427720189094543
train_iter_loss: 0.293833464384079
train_iter_loss: 0.35445529222488403
train_iter_loss: 0.19073231518268585
train_iter_loss: 0.32073208689689636
train_iter_loss: 0.19347529113292694
train_iter_loss: 0.10453971475362778
train_iter_loss: 0.24425680935382843
train_iter_loss: 0.2738952338695526
train_iter_loss: 0.30642709136009216
train_iter_loss: 0.2759733200073242
train_iter_loss: 0.21580280363559723
train_iter_loss: 0.16031116247177124
train_iter_loss: 0.34636402130126953
train_iter_loss: 0.3089517652988434
train_iter_loss: 0.29666784405708313
train_iter_loss: 0.30858054757118225
train_iter_loss: 0.23511290550231934
train_iter_loss: 0.26113903522491455
train_iter_loss: 0.18199069797992706
train_iter_loss: 0.30480608344078064
train_iter_loss: 0.21084439754486084
train_iter_loss: 0.33597466349601746
train_iter_loss: 0.14868159592151642
train_iter_loss: 0.3531839847564697
train_iter_loss: 0.3708989918231964
train_iter_loss: 0.31363123655319214
train_iter_loss: 0.20484767854213715
train_iter_loss: 0.29154589772224426
train_iter_loss: 0.35699477791786194
train_iter_loss: 0.19403478503227234
train_iter_loss: 0.32007190585136414
train_iter_loss: 0.31053897738456726
train_iter_loss: 0.2087906301021576
train_iter_loss: 0.35390543937683105
train_iter_loss: 0.3303056061267853
train_iter_loss: 0.40575236082077026
train_iter_loss: 0.26825323700904846
train_iter_loss: 0.31047138571739197
train_iter_loss: 0.3653831481933594
train_iter_loss: 0.32634812593460083
train_iter_loss: 0.3489503562450409
train_iter_loss: 0.18536481261253357
train_iter_loss: 0.2423778474330902
train_iter_loss: 0.34740549325942993
train_iter_loss: 0.22345508635044098
train_iter_loss: 0.4234618842601776
train_iter_loss: 0.322471022605896
train_iter_loss: 0.35587242245674133
train_iter_loss: 0.4006422758102417
train_iter_loss: 0.27047544717788696
train_iter_loss: 0.34328168630599976
train_iter_loss: 0.32872432470321655
train_iter_loss: 0.15935936570167542
train_iter_loss: 0.2728441655635834
train_iter_loss: 0.1661430299282074
train_iter_loss: 0.31203702092170715
train_iter_loss: 0.3732198476791382
train_iter_loss: 0.3187197744846344
train_iter_loss: 0.17665879428386688
train_iter_loss: 0.18749412894248962
train_iter_loss: 0.3535807132720947
train_iter_loss: 0.21883566677570343
train_iter_loss: 0.24216389656066895
train_iter_loss: 0.22687983512878418
train_iter_loss: 0.28310802578926086
train_iter_loss: 0.2746542692184448
train_iter_loss: 0.25380387902259827
train_iter_loss: 0.30547910928726196
train_iter_loss: 0.33923575282096863
train_iter_loss: 0.2034953236579895
train_iter_loss: 0.215328648686409
train_iter_loss: 0.38342544436454773
train_iter_loss: 0.396368145942688
train_iter_loss: 0.3542160987854004
train_iter_loss: 0.2661333978176117
train_iter_loss: 0.2169753760099411
train_iter_loss: 0.2167281061410904
train_iter_loss: 0.20460957288742065
train_iter_loss: 0.28263556957244873
train_iter_loss: 0.21354760229587555
train_iter_loss: 0.3524177670478821
train_iter_loss: 0.45892590284347534
train_iter_loss: 0.1372004747390747
train_iter_loss: 0.3960648775100708
train_iter_loss: 0.38934123516082764
train_iter_loss: 0.3222911059856415
train_iter_loss: 0.24306219816207886
train loss :0.2844
---------------------
Validation seg loss: 0.3669403009424162 at epoch 243
epoch =    244/  1000, exp = train
train_iter_loss: 0.10369286686182022
train_iter_loss: 0.22548627853393555
train_iter_loss: 0.3754110038280487
train_iter_loss: 0.2752532660961151
train_iter_loss: 0.41824430227279663
train_iter_loss: 0.11736319214105606
train_iter_loss: 0.2922351360321045
train_iter_loss: 0.46983522176742554
train_iter_loss: 0.2404986470937729
train_iter_loss: 0.27246546745300293
train_iter_loss: 0.2648327946662903
train_iter_loss: 0.36040160059928894
train_iter_loss: 0.29666802287101746
train_iter_loss: 0.24348993599414825
train_iter_loss: 0.4026879072189331
train_iter_loss: 0.20078615844249725
train_iter_loss: 0.2499813735485077
train_iter_loss: 0.21980783343315125
train_iter_loss: 0.24876120686531067
train_iter_loss: 0.3889220058917999
train_iter_loss: 0.3052468001842499
train_iter_loss: 0.46947115659713745
train_iter_loss: 0.3030891418457031
train_iter_loss: 0.09655758738517761
train_iter_loss: 0.22918729484081268
train_iter_loss: 0.3279886245727539
train_iter_loss: 0.2788560688495636
train_iter_loss: 0.2412198930978775
train_iter_loss: 0.2828822433948517
train_iter_loss: 0.22658959031105042
train_iter_loss: 0.22463056445121765
train_iter_loss: 0.4224303662776947
train_iter_loss: 0.198709174990654
train_iter_loss: 0.2676411271095276
train_iter_loss: 0.3406743109226227
train_iter_loss: 0.20756520330905914
train_iter_loss: 0.20899410545825958
train_iter_loss: 0.1725090891122818
train_iter_loss: 0.25511541962623596
train_iter_loss: 0.2517814338207245
train_iter_loss: 0.22519509494304657
train_iter_loss: 0.17082424461841583
train_iter_loss: 0.23687267303466797
train_iter_loss: 0.12849180400371552
train_iter_loss: 0.26189470291137695
train_iter_loss: 0.247270867228508
train_iter_loss: 0.2644265294075012
train_iter_loss: 0.39116764068603516
train_iter_loss: 0.3893623948097229
train_iter_loss: 0.2707540690898895
train_iter_loss: 0.18877029418945312
train_iter_loss: 0.3147469162940979
train_iter_loss: 0.24368049204349518
train_iter_loss: 0.1891736090183258
train_iter_loss: 0.2669542729854584
train_iter_loss: 0.2918585538864136
train_iter_loss: 0.26920709013938904
train_iter_loss: 0.25187617540359497
train_iter_loss: 0.377970427274704
train_iter_loss: 0.5004390478134155
train_iter_loss: 0.5060861706733704
train_iter_loss: 0.22587169706821442
train_iter_loss: 0.287491112947464
train_iter_loss: 0.23287320137023926
train_iter_loss: 0.2857649028301239
train_iter_loss: 0.4465422034263611
train_iter_loss: 0.3530716300010681
train_iter_loss: 0.39032259583473206
train_iter_loss: 0.42805981636047363
train_iter_loss: 0.2479216307401657
train_iter_loss: 0.2226109653711319
train_iter_loss: 0.3696475625038147
train_iter_loss: 0.20229628682136536
train_iter_loss: 0.28492099046707153
train_iter_loss: 0.35111984610557556
train_iter_loss: 0.4211338460445404
train_iter_loss: 0.3593370020389557
train_iter_loss: 0.3139804005622864
train_iter_loss: 0.253910094499588
train_iter_loss: 0.23974564671516418
train_iter_loss: 0.28530892729759216
train_iter_loss: 0.17977386713027954
train_iter_loss: 0.3033454418182373
train_iter_loss: 0.1547059714794159
train_iter_loss: 0.14885500073432922
train_iter_loss: 0.3181644082069397
train_iter_loss: 0.45126235485076904
train_iter_loss: 0.45959752798080444
train_iter_loss: 0.5210195183753967
train_iter_loss: 0.29052597284317017
train_iter_loss: 0.18037419021129608
train_iter_loss: 0.23298944532871246
train_iter_loss: 0.22780479490756989
train_iter_loss: 0.2613077461719513
train_iter_loss: 0.3644219934940338
train_iter_loss: 0.21858355402946472
train_iter_loss: 0.41111546754837036
train_iter_loss: 0.2084384262561798
train_iter_loss: 0.3107224404811859
train_iter_loss: 0.2757399380207062
train loss :0.2897
---------------------
Validation seg loss: 0.37906784245322617 at epoch 244
epoch =    245/  1000, exp = train
train_iter_loss: 0.3495609760284424
train_iter_loss: 0.14201432466506958
train_iter_loss: 0.3045211434364319
train_iter_loss: 0.28735247254371643
train_iter_loss: 0.25262191891670227
train_iter_loss: 0.17527852952480316
train_iter_loss: 0.07436305284500122
train_iter_loss: 0.23321349918842316
train_iter_loss: 0.13612736761569977
train_iter_loss: 0.4039961099624634
train_iter_loss: 0.21300585567951202
train_iter_loss: 0.32818540930747986
train_iter_loss: 0.2735200822353363
train_iter_loss: 0.22462788224220276
train_iter_loss: 0.1911277323961258
train_iter_loss: 0.17401596903800964
train_iter_loss: 0.2644784450531006
train_iter_loss: 0.2729605734348297
train_iter_loss: 0.251221626996994
train_iter_loss: 0.3396945595741272
train_iter_loss: 0.4351159930229187
train_iter_loss: 0.2752801179885864
train_iter_loss: 0.2807644307613373
train_iter_loss: 0.23166990280151367
train_iter_loss: 0.39050528407096863
train_iter_loss: 0.24214360117912292
train_iter_loss: 0.18610996007919312
train_iter_loss: 0.17248757183551788
train_iter_loss: 0.3399004340171814
train_iter_loss: 0.37330445647239685
train_iter_loss: 0.253351628780365
train_iter_loss: 0.29523739218711853
train_iter_loss: 0.18521690368652344
train_iter_loss: 0.2282077521085739
train_iter_loss: 0.4236532747745514
train_iter_loss: 0.44918501377105713
train_iter_loss: 0.29043325781822205
train_iter_loss: 0.41240501403808594
train_iter_loss: 0.30670905113220215
train_iter_loss: 0.30350205302238464
train_iter_loss: 0.28504958748817444
train_iter_loss: 0.2619601786136627
train_iter_loss: 0.2978160083293915
train_iter_loss: 0.20161770284175873
train_iter_loss: 0.42311960458755493
train_iter_loss: 0.26495474576950073
train_iter_loss: 0.3420766294002533
train_iter_loss: 0.18412289023399353
train_iter_loss: 0.31410911679267883
train_iter_loss: 0.17484137415885925
train_iter_loss: 0.27649667859077454
train_iter_loss: 0.33166974782943726
train_iter_loss: 0.36658015847206116
train_iter_loss: 0.34029093384742737
train_iter_loss: 0.18517176806926727
train_iter_loss: 0.20928792655467987
train_iter_loss: 0.24676142632961273
train_iter_loss: 0.1513986438512802
train_iter_loss: 0.3015204071998596
train_iter_loss: 0.40940558910369873
train_iter_loss: 0.33502820134162903
train_iter_loss: 0.27151504158973694
train_iter_loss: 0.37285465002059937
train_iter_loss: 0.2818397283554077
train_iter_loss: 0.25975966453552246
train_iter_loss: 0.16944938898086548
train_iter_loss: 0.16638128459453583
train_iter_loss: 0.3015897274017334
train_iter_loss: 0.27566373348236084
train_iter_loss: 0.40149810910224915
train_iter_loss: 0.3150424063205719
train_iter_loss: 0.3298255205154419
train_iter_loss: 0.14227373898029327
train_iter_loss: 0.4378715753555298
train_iter_loss: 0.1685805767774582
train_iter_loss: 0.3134768605232239
train_iter_loss: 0.553087055683136
train_iter_loss: 0.42755386233329773
train_iter_loss: 0.13675878942012787
train_iter_loss: 0.28753215074539185
train_iter_loss: 0.2554863393306732
train_iter_loss: 0.295523077249527
train_iter_loss: 0.2536623775959015
train_iter_loss: 0.20753858983516693
train_iter_loss: 0.3455517888069153
train_iter_loss: 0.5216264724731445
train_iter_loss: 0.20827141404151917
train_iter_loss: 0.2769148349761963
train_iter_loss: 0.34440138936042786
train_iter_loss: 0.4089597761631012
train_iter_loss: 0.11990834772586823
train_iter_loss: 0.22710317373275757
train_iter_loss: 0.4471546411514282
train_iter_loss: 0.19190220534801483
train_iter_loss: 0.23460784554481506
train_iter_loss: 0.3399561941623688
train_iter_loss: 0.3168262839317322
train_iter_loss: 0.37967079877853394
train_iter_loss: 0.16937994956970215
train_iter_loss: 0.1381109058856964
train loss :0.2846
---------------------
Validation seg loss: 0.3659745083137784 at epoch 245
epoch =    246/  1000, exp = train
train_iter_loss: 0.16670101881027222
train_iter_loss: 0.27917397022247314
train_iter_loss: 0.3584587574005127
train_iter_loss: 0.16682301461696625
train_iter_loss: 0.1974448561668396
train_iter_loss: 0.14200089871883392
train_iter_loss: 0.2879801392555237
train_iter_loss: 0.21509408950805664
train_iter_loss: 0.403764545917511
train_iter_loss: 0.24134041368961334
train_iter_loss: 0.31319504976272583
train_iter_loss: 0.32916319370269775
train_iter_loss: 0.34390580654144287
train_iter_loss: 0.19412370026111603
train_iter_loss: 0.2714233696460724
train_iter_loss: 0.3529708683490753
train_iter_loss: 0.2788552939891815
train_iter_loss: 0.19825398921966553
train_iter_loss: 0.21974727511405945
train_iter_loss: 0.40104591846466064
train_iter_loss: 0.3176119029521942
train_iter_loss: 0.30548739433288574
train_iter_loss: 0.30526217818260193
train_iter_loss: 0.21479476988315582
train_iter_loss: 0.3510476350784302
train_iter_loss: 0.11653981357812881
train_iter_loss: 0.32405272126197815
train_iter_loss: 0.3060450553894043
train_iter_loss: 0.22657349705696106
train_iter_loss: 0.29731032252311707
train_iter_loss: 0.2035326510667801
train_iter_loss: 0.37540000677108765
train_iter_loss: 0.24335269629955292
train_iter_loss: 0.4128558933734894
train_iter_loss: 0.3370909094810486
train_iter_loss: 0.39287567138671875
train_iter_loss: 0.31124168634414673
train_iter_loss: 0.36046603322029114
train_iter_loss: 0.28844398260116577
train_iter_loss: 0.30570510029792786
train_iter_loss: 0.32410627603530884
train_iter_loss: 0.20756930112838745
train_iter_loss: 0.29467707872390747
train_iter_loss: 0.15750350058078766
train_iter_loss: 0.2700881063938141
train_iter_loss: 0.37151214480400085
train_iter_loss: 0.26894888281822205
train_iter_loss: 0.30685311555862427
train_iter_loss: 0.2219395637512207
train_iter_loss: 0.38238048553466797
train_iter_loss: 0.32639822363853455
train_iter_loss: 0.3110635280609131
train_iter_loss: 0.2646520435810089
train_iter_loss: 0.24166147410869598
train_iter_loss: 0.3563082814216614
train_iter_loss: 0.2977987825870514
train_iter_loss: 0.26482975482940674
train_iter_loss: 0.43883228302001953
train_iter_loss: 0.1150432825088501
train_iter_loss: 0.34140846133232117
train_iter_loss: 0.2587132453918457
train_iter_loss: 0.25639787316322327
train_iter_loss: 0.5781869292259216
train_iter_loss: 0.24297277629375458
train_iter_loss: 0.19449393451213837
train_iter_loss: 0.33196738362312317
train_iter_loss: 0.3239762485027313
train_iter_loss: 0.34576812386512756
train_iter_loss: 0.2129114419221878
train_iter_loss: 0.3580755591392517
train_iter_loss: 0.22932855784893036
train_iter_loss: 0.3404865264892578
train_iter_loss: 0.33511173725128174
train_iter_loss: 0.16366027295589447
train_iter_loss: 0.3244169056415558
train_iter_loss: 0.14061281085014343
train_iter_loss: 0.2874446213245392
train_iter_loss: 0.4505697786808014
train_iter_loss: 0.29778558015823364
train_iter_loss: 0.1622658371925354
train_iter_loss: 0.2658393383026123
train_iter_loss: 0.3259431719779968
train_iter_loss: 0.29523757100105286
train_iter_loss: 0.37338072061538696
train_iter_loss: 0.21974249184131622
train_iter_loss: 0.2609337568283081
train_iter_loss: 0.326212614774704
train_iter_loss: 0.16354693472385406
train_iter_loss: 0.26470863819122314
train_iter_loss: 0.179802805185318
train_iter_loss: 0.2971682846546173
train_iter_loss: 0.2998916506767273
train_iter_loss: 0.2849328815937042
train_iter_loss: 0.22228944301605225
train_iter_loss: 0.33936795592308044
train_iter_loss: 0.2904520630836487
train_iter_loss: 0.2416045218706131
train_iter_loss: 0.2801031768321991
train_iter_loss: 0.08981696516275406
train_iter_loss: 0.28864818811416626
train loss :0.2855
---------------------
Validation seg loss: 0.38223215457016846 at epoch 246
epoch =    247/  1000, exp = train
train_iter_loss: 0.28767576813697815
train_iter_loss: 0.2674562633037567
train_iter_loss: 0.3827022910118103
train_iter_loss: 0.30381330847740173
train_iter_loss: 0.31126439571380615
train_iter_loss: 0.2901051938533783
train_iter_loss: 0.3480373024940491
train_iter_loss: 0.27912437915802
train_iter_loss: 0.3858959376811981
train_iter_loss: 0.3110455274581909
train_iter_loss: 0.25128689408302307
train_iter_loss: 0.3885839283466339
train_iter_loss: 0.36280786991119385
train_iter_loss: 0.2480662316083908
train_iter_loss: 0.1541929692029953
train_iter_loss: 0.12413757294416428
train_iter_loss: 0.15877313911914825
train_iter_loss: 0.20096302032470703
train_iter_loss: 0.3331686854362488
train_iter_loss: 0.15568196773529053
train_iter_loss: 0.6234841346740723
train_iter_loss: 0.2379395067691803
train_iter_loss: 0.19817766547203064
train_iter_loss: 0.16920681297779083
train_iter_loss: 0.40145114064216614
train_iter_loss: 0.3497890532016754
train_iter_loss: 0.28799986839294434
train_iter_loss: 0.13865429162979126
train_iter_loss: 0.36096900701522827
train_iter_loss: 0.14047838747501373
train_iter_loss: 0.18434156477451324
train_iter_loss: 0.31828367710113525
train_iter_loss: 0.4124799072742462
train_iter_loss: 0.36114269495010376
train_iter_loss: 0.3775458335876465
train_iter_loss: 0.47145596146583557
train_iter_loss: 0.22175654768943787
train_iter_loss: 0.400433748960495
train_iter_loss: 0.3402678370475769
train_iter_loss: 0.38511666655540466
train_iter_loss: 0.20068983733654022
train_iter_loss: 0.2547856271266937
train_iter_loss: 0.41618600487709045
train_iter_loss: 0.1545039266347885
train_iter_loss: 0.38545289635658264
train_iter_loss: 0.21421952545642853
train_iter_loss: 0.23653946816921234
train_iter_loss: 0.23477287590503693
train_iter_loss: 0.3075461685657501
train_iter_loss: 0.29429182410240173
train_iter_loss: 0.22346621751785278
train_iter_loss: 0.23552857339382172
train_iter_loss: 0.24270924925804138
train_iter_loss: 0.16683357954025269
train_iter_loss: 0.16779237985610962
train_iter_loss: 0.141594797372818
train_iter_loss: 0.28485164046287537
train_iter_loss: 0.32626113295555115
train_iter_loss: 0.1799613982439041
train_iter_loss: 0.21794556081295013
train_iter_loss: 0.13271553814411163
train_iter_loss: 0.140680730342865
train_iter_loss: 0.3501834571361542
train_iter_loss: 0.2648300230503082
train_iter_loss: 0.4087672233581543
train_iter_loss: 0.3066147267818451
train_iter_loss: 0.3869093060493469
train_iter_loss: 0.2518336772918701
train_iter_loss: 0.14494073390960693
train_iter_loss: 0.17616961896419525
train_iter_loss: 0.2730156183242798
train_iter_loss: 0.3695693612098694
train_iter_loss: 0.17321744561195374
train_iter_loss: 0.3426719009876251
train_iter_loss: 0.27794456481933594
train_iter_loss: 0.3134799897670746
train_iter_loss: 0.3037945032119751
train_iter_loss: 0.2043873816728592
train_iter_loss: 0.2857263386249542
train_iter_loss: 0.27007073163986206
train_iter_loss: 0.339869886636734
train_iter_loss: 0.3301539123058319
train_iter_loss: 0.3816034197807312
train_iter_loss: 0.29401877522468567
train_iter_loss: 0.3700580596923828
train_iter_loss: 0.2191902995109558
train_iter_loss: 0.3152596950531006
train_iter_loss: 0.4078851342201233
train_iter_loss: 0.10751641541719437
train_iter_loss: 0.3934478163719177
train_iter_loss: 0.38750627636909485
train_iter_loss: 0.36787161231040955
train_iter_loss: 0.26575490832328796
train_iter_loss: 0.3145464360713959
train_iter_loss: 0.26737067103385925
train_iter_loss: 0.19940364360809326
train_iter_loss: 0.21796821057796478
train_iter_loss: 0.20921185612678528
train_iter_loss: 0.4279671311378479
train_iter_loss: 0.14104445278644562
train loss :0.2843
---------------------
Validation seg loss: 0.35819186683181886 at epoch 247
epoch =    248/  1000, exp = train
train_iter_loss: 0.3109699487686157
train_iter_loss: 0.4787600338459015
train_iter_loss: 0.2809576094150543
train_iter_loss: 0.13106060028076172
train_iter_loss: 0.15990859270095825
train_iter_loss: 0.26878464221954346
train_iter_loss: 0.3327215313911438
train_iter_loss: 0.3353438675403595
train_iter_loss: 0.1620027869939804
train_iter_loss: 0.27321717143058777
train_iter_loss: 0.3253847658634186
train_iter_loss: 0.20187954604625702
train_iter_loss: 0.3381139636039734
train_iter_loss: 0.254506379365921
train_iter_loss: 0.2252902090549469
train_iter_loss: 0.27426379919052124
train_iter_loss: 0.19312196969985962
train_iter_loss: 0.1710120439529419
train_iter_loss: 0.15058688819408417
train_iter_loss: 0.35939744114875793
train_iter_loss: 0.20019185543060303
train_iter_loss: 0.27205267548561096
train_iter_loss: 0.41698089241981506
train_iter_loss: 0.38083669543266296
train_iter_loss: 0.24058006703853607
train_iter_loss: 0.3728461265563965
train_iter_loss: 0.3601195812225342
train_iter_loss: 0.2417241781949997
train_iter_loss: 0.3259633779525757
train_iter_loss: 0.3122272789478302
train_iter_loss: 0.24845567345619202
train_iter_loss: 0.3092404007911682
train_iter_loss: 0.31855687499046326
train_iter_loss: 0.18438175320625305
train_iter_loss: 0.33579909801483154
train_iter_loss: 0.25599974393844604
train_iter_loss: 0.13608857989311218
train_iter_loss: 0.25255438685417175
train_iter_loss: 0.49137699604034424
train_iter_loss: 0.27408266067504883
train_iter_loss: 0.2608160078525543
train_iter_loss: 0.22516335546970367
train_iter_loss: 0.25943318009376526
train_iter_loss: 0.2887879014015198
train_iter_loss: 0.3402676284313202
train_iter_loss: 0.3489009439945221
train_iter_loss: 0.19669309258460999
train_iter_loss: 0.27579763531684875
train_iter_loss: 0.2936425507068634
train_iter_loss: 0.2642785608768463
train_iter_loss: 0.29064950346946716
train_iter_loss: 0.2088676393032074
train_iter_loss: 0.43639203906059265
train_iter_loss: 0.33262479305267334
train_iter_loss: 0.12729622423648834
train_iter_loss: 0.29652169346809387
train_iter_loss: 0.21312643587589264
train_iter_loss: 0.20162968337535858
train_iter_loss: 0.332690954208374
train_iter_loss: 0.22240248322486877
train_iter_loss: 0.13146275281906128
train_iter_loss: 0.35040345788002014
train_iter_loss: 0.33744359016418457
train_iter_loss: 0.2902894914150238
train_iter_loss: 0.3550682067871094
train_iter_loss: 0.20271848142147064
train_iter_loss: 0.19849424064159393
train_iter_loss: 0.20318962633609772
train_iter_loss: 0.30748122930526733
train_iter_loss: 0.3640173077583313
train_iter_loss: 0.1525808721780777
train_iter_loss: 0.3163524568080902
train_iter_loss: 0.36513596773147583
train_iter_loss: 0.4136925935745239
train_iter_loss: 0.24489519000053406
train_iter_loss: 0.10217387974262238
train_iter_loss: 0.29339128732681274
train_iter_loss: 0.11128468811511993
train_iter_loss: 0.2512787878513336
train_iter_loss: 0.39044198393821716
train_iter_loss: 0.47052618861198425
train_iter_loss: 0.3196188509464264
train_iter_loss: 0.23616500198841095
train_iter_loss: 0.3034273684024811
train_iter_loss: 0.2619715929031372
train_iter_loss: 0.3376103639602661
train_iter_loss: 0.24036315083503723
train_iter_loss: 0.36875882744789124
train_iter_loss: 0.48541948199272156
train_iter_loss: 0.439370334148407
train_iter_loss: 0.3963685929775238
train_iter_loss: 0.2678448259830475
train_iter_loss: 0.3030088245868683
train_iter_loss: 0.38594454526901245
train_iter_loss: 0.24229931831359863
train_iter_loss: 0.49540266394615173
train_iter_loss: 0.19338928163051605
train_iter_loss: 0.26617422699928284
train_iter_loss: 0.26499107480049133
train_iter_loss: 0.23349007964134216
train loss :0.2875
---------------------
Validation seg loss: 0.37135415599804444 at epoch 248
epoch =    249/  1000, exp = train
train_iter_loss: 0.21825219690799713
train_iter_loss: 0.36358579993247986
train_iter_loss: 0.37817350029945374
train_iter_loss: 0.15575481951236725
train_iter_loss: 0.33173006772994995
train_iter_loss: 0.24033570289611816
train_iter_loss: 0.2245265543460846
train_iter_loss: 0.18656021356582642
train_iter_loss: 0.28004714846611023
train_iter_loss: 0.20140321552753448
train_iter_loss: 0.34555771946907043
train_iter_loss: 0.25969555974006653
train_iter_loss: 0.20262618362903595
train_iter_loss: 0.31716689467430115
train_iter_loss: 0.30756497383117676
train_iter_loss: 0.26608434319496155
train_iter_loss: 0.38618457317352295
train_iter_loss: 0.2751927077770233
train_iter_loss: 0.10627569258213043
train_iter_loss: 0.2628890573978424
train_iter_loss: 0.3794417381286621
train_iter_loss: 0.14660033583641052
train_iter_loss: 0.322356641292572
train_iter_loss: 0.36973533034324646
train_iter_loss: 0.3482949137687683
train_iter_loss: 0.2378758192062378
train_iter_loss: 0.33651232719421387
train_iter_loss: 0.23844139277935028
train_iter_loss: 0.41552734375
train_iter_loss: 0.4264681041240692
train_iter_loss: 0.2676171362400055
train_iter_loss: 0.18410786986351013
train_iter_loss: 0.23657993972301483
train_iter_loss: 0.2381969541311264
train_iter_loss: 0.14462627470493317
train_iter_loss: 0.28693053126335144
train_iter_loss: 0.2967964708805084
train_iter_loss: 0.24380721151828766
train_iter_loss: 0.36342400312423706
train_iter_loss: 0.4147879183292389
train_iter_loss: 0.3096753656864166
train_iter_loss: 0.43919065594673157
train_iter_loss: 0.2784384787082672
train_iter_loss: 0.22789977490901947
train_iter_loss: 0.2606508135795593
train_iter_loss: 0.26674970984458923
train_iter_loss: 0.18466044962406158
train_iter_loss: 0.22407269477844238
train_iter_loss: 0.19643573462963104
train_iter_loss: 0.26354336738586426
train_iter_loss: 0.3078167140483856
train_iter_loss: 0.25466543436050415
train_iter_loss: 0.27679720520973206
train_iter_loss: 0.3617737889289856
train_iter_loss: 0.4881349205970764
train_iter_loss: 0.30766817927360535
train_iter_loss: 0.5450560450553894
train_iter_loss: 0.3777850270271301
train_iter_loss: 0.36114639043807983
train_iter_loss: 0.21699711680412292
train_iter_loss: 0.1132977232336998
train_iter_loss: 0.262116938829422
train_iter_loss: 0.2658504247665405
train_iter_loss: 0.1944100260734558
train_iter_loss: 0.2934773564338684
train_iter_loss: 0.5044628977775574
train_iter_loss: 0.33211377263069153
train_iter_loss: 0.33085840940475464
train_iter_loss: 0.22963213920593262
train_iter_loss: 0.27393388748168945
train_iter_loss: 0.13435502350330353
train_iter_loss: 0.19452530145645142
train_iter_loss: 0.4195009469985962
train_iter_loss: 0.20719262957572937
train_iter_loss: 0.3312794268131256
train_iter_loss: 0.3012048900127411
train_iter_loss: 0.18227796256542206
train_iter_loss: 0.3456435203552246
train_iter_loss: 0.30227959156036377
train_iter_loss: 0.30158886313438416
train_iter_loss: 0.26103833317756653
train_iter_loss: 0.2175716906785965
train_iter_loss: 0.2070576697587967
train_iter_loss: 0.2519361674785614
train_iter_loss: 0.4122202694416046
train_iter_loss: 0.17412029206752777
train_iter_loss: 0.2241288721561432
train_iter_loss: 0.41124770045280457
train_iter_loss: 0.21314173936843872
train_iter_loss: 0.2772805988788605
train_iter_loss: 0.27027928829193115
train_iter_loss: 0.37398603558540344
train_iter_loss: 0.13712061941623688
train_iter_loss: 0.19594314694404602
train_iter_loss: 0.33144018054008484
train_iter_loss: 0.3251444101333618
train_iter_loss: 0.4451923072338104
train_iter_loss: 0.32585638761520386
train_iter_loss: 0.31075987219810486
train_iter_loss: 0.19475102424621582
train loss :0.2872
---------------------
Validation seg loss: 0.38817720440269077 at epoch 249
epoch =    250/  1000, exp = train
train_iter_loss: 0.2541051208972931
train_iter_loss: 0.44140076637268066
train_iter_loss: 0.3164895474910736
train_iter_loss: 0.18952742218971252
train_iter_loss: 0.20346398651599884
train_iter_loss: 0.3452486991882324
train_iter_loss: 0.2089449018239975
train_iter_loss: 0.3024635314941406
train_iter_loss: 0.3237554132938385
train_iter_loss: 0.22560325264930725
train_iter_loss: 0.2513095736503601
train_iter_loss: 0.17039412260055542
train_iter_loss: 0.32868412137031555
train_iter_loss: 0.2824629247188568
train_iter_loss: 0.2730283737182617
train_iter_loss: 0.1543872356414795
train_iter_loss: 0.3405117690563202
train_iter_loss: 0.23307350277900696
train_iter_loss: 0.2980034053325653
train_iter_loss: 0.28401562571525574
train_iter_loss: 0.35123106837272644
train_iter_loss: 0.26245495676994324
train_iter_loss: 0.28102144598960876
train_iter_loss: 0.20569926500320435
train_iter_loss: 0.2225506752729416
train_iter_loss: 0.19646161794662476
train_iter_loss: 0.24184812605381012
train_iter_loss: 0.26355811953544617
train_iter_loss: 0.20466555655002594
train_iter_loss: 0.43345075845718384
train_iter_loss: 0.49454808235168457
train_iter_loss: 0.19665400683879852
train_iter_loss: 0.3316904306411743
train_iter_loss: 0.14402705430984497
train_iter_loss: 0.20389916002750397
train_iter_loss: 0.25367072224617004
train_iter_loss: 0.3736213445663452
train_iter_loss: 0.252035528421402
train_iter_loss: 0.2382236123085022
train_iter_loss: 0.26112720370292664
train_iter_loss: 0.17446736991405487
train_iter_loss: 0.18281535804271698
train_iter_loss: 0.4072863459587097
train_iter_loss: 0.2682263255119324
train_iter_loss: 0.20229339599609375
train_iter_loss: 0.19259235262870789
train_iter_loss: 0.4772612154483795
train_iter_loss: 0.5985831618309021
train_iter_loss: 0.4863797426223755
train_iter_loss: 0.18393173813819885
train_iter_loss: 0.21438045799732208
train_iter_loss: 0.4335838556289673
train_iter_loss: 0.2034095674753189
train_iter_loss: 0.33415836095809937
train_iter_loss: 0.36549806594848633
train_iter_loss: 0.3422553539276123
train_iter_loss: 0.18124665319919586
train_iter_loss: 0.24430671334266663
train_iter_loss: 0.30434074997901917
train_iter_loss: 0.26763397455215454
train_iter_loss: 0.34828001260757446
train_iter_loss: 0.22146137058734894
train_iter_loss: 0.25177374482154846
train_iter_loss: 0.47265055775642395
train_iter_loss: 0.3952355682849884
train_iter_loss: 0.29380279779434204
train_iter_loss: 0.27735573053359985
train_iter_loss: 0.27873656153678894
train_iter_loss: 0.3840096890926361
train_iter_loss: 0.12068793177604675
train_iter_loss: 0.27490368485450745
train_iter_loss: 0.23993869125843048
train_iter_loss: 0.331764280796051
train_iter_loss: 0.29927268624305725
train_iter_loss: 0.298525333404541
train_iter_loss: 0.3634670078754425
train_iter_loss: 0.48257219791412354
train_iter_loss: 0.22232063114643097
train_iter_loss: 0.2857002913951874
train_iter_loss: 0.17105604708194733
train_iter_loss: 0.13800373673439026
train_iter_loss: 0.24116910994052887
train_iter_loss: 0.2619369626045227
train_iter_loss: 0.28013747930526733
train_iter_loss: 0.09519267082214355
train_iter_loss: 0.3323431611061096
train_iter_loss: 0.16127832233905792
train_iter_loss: 0.37432482838630676
train_iter_loss: 0.3169822096824646
train_iter_loss: 0.3535628914833069
train_iter_loss: 0.2964816689491272
train_iter_loss: 0.32493123412132263
train_iter_loss: 0.3975263833999634
train_iter_loss: 0.28792092204093933
train_iter_loss: 0.35622209310531616
train_iter_loss: 0.21187002956867218
train_iter_loss: 0.3508972227573395
train_iter_loss: 0.2731401026248932
train_iter_loss: 0.27552101016044617
train_iter_loss: 0.24657563865184784
train loss :0.2878
---------------------
Validation seg loss: 0.364143528079368 at epoch 250
epoch =    251/  1000, exp = train
train_iter_loss: 0.38330188393592834
train_iter_loss: 0.2616291046142578
train_iter_loss: 0.3010428845882416
train_iter_loss: 0.27839189767837524
train_iter_loss: 0.3863890767097473
train_iter_loss: 0.4093000292778015
train_iter_loss: 0.234817773103714
train_iter_loss: 0.41977906227111816
train_iter_loss: 0.299988329410553
train_iter_loss: 0.24366246163845062
train_iter_loss: 0.14395847916603088
train_iter_loss: 0.26895028352737427
train_iter_loss: 0.29604649543762207
train_iter_loss: 0.09014403074979782
train_iter_loss: 0.16559268534183502
train_iter_loss: 0.36536675691604614
train_iter_loss: 0.26330995559692383
train_iter_loss: 0.2185024470090866
train_iter_loss: 0.3551201820373535
train_iter_loss: 0.32268834114074707
train_iter_loss: 0.3743639588356018
train_iter_loss: 0.29583725333213806
train_iter_loss: 0.32704925537109375
train_iter_loss: 0.30731916427612305
train_iter_loss: 0.25552672147750854
train_iter_loss: 0.30325114727020264
train_iter_loss: 0.37405911087989807
train_iter_loss: 0.33418604731559753
train_iter_loss: 0.3771470785140991
train_iter_loss: 0.267654687166214
train_iter_loss: 0.31335169076919556
train_iter_loss: 0.2134016901254654
train_iter_loss: 0.3054601550102234
train_iter_loss: 0.21855750679969788
train_iter_loss: 0.36515915393829346
train_iter_loss: 0.19232837855815887
train_iter_loss: 0.2916196584701538
train_iter_loss: 0.2135968804359436
train_iter_loss: 0.45225265622138977
train_iter_loss: 0.3478453457355499
train_iter_loss: 0.305358350276947
train_iter_loss: 0.3138369917869568
train_iter_loss: 0.2773101031780243
train_iter_loss: 0.32763487100601196
train_iter_loss: 0.4213947653770447
train_iter_loss: 0.10406916588544846
train_iter_loss: 0.3224337697029114
train_iter_loss: 0.26753416657447815
train_iter_loss: 0.27900373935699463
train_iter_loss: 0.20271144807338715
train_iter_loss: 0.3830009996891022
train_iter_loss: 0.19433270394802094
train_iter_loss: 0.1446462869644165
train_iter_loss: 0.28148403763771057
train_iter_loss: 0.2086474448442459
train_iter_loss: 0.30823081731796265
train_iter_loss: 0.17024624347686768
train_iter_loss: 0.3438011705875397
train_iter_loss: 0.338279664516449
train_iter_loss: 0.11793678998947144
train_iter_loss: 0.29541733860969543
train_iter_loss: 0.26897069811820984
train_iter_loss: 0.12149367481470108
train_iter_loss: 0.40774378180503845
train_iter_loss: 0.2210710644721985
train_iter_loss: 0.41277796030044556
train_iter_loss: 0.2542118430137634
train_iter_loss: 0.34677526354789734
train_iter_loss: 0.2906768023967743
train_iter_loss: 0.4559764266014099
train_iter_loss: 0.3021039664745331
train_iter_loss: 0.16090647876262665
train_iter_loss: 0.5281519293785095
train_iter_loss: 0.21416935324668884
train_iter_loss: 0.4573597013950348
train_iter_loss: 0.37707677483558655
train_iter_loss: 0.3231445550918579
train_iter_loss: 0.3352351784706116
train_iter_loss: 0.17282001674175262
train_iter_loss: 0.2437756359577179
train_iter_loss: 0.37372830510139465
train_iter_loss: 0.21718730032444
train_iter_loss: 0.17087234556674957
train_iter_loss: 0.29319173097610474
train_iter_loss: 0.21710294485092163
train_iter_loss: 0.271363765001297
train_iter_loss: 0.40706712007522583
train_iter_loss: 0.12252043932676315
train_iter_loss: 0.3320423364639282
train_iter_loss: 0.2883431911468506
train_iter_loss: 0.18584521114826202
train_iter_loss: 0.3320158123970032
train_iter_loss: 0.2704094648361206
train_iter_loss: 0.11425179243087769
train_iter_loss: 0.4340458810329437
train_iter_loss: 0.1839723438024521
train_iter_loss: 0.46668028831481934
train_iter_loss: 0.24421504139900208
train_iter_loss: 0.1638321876525879
train_iter_loss: 0.1867925375699997
train loss :0.2890
---------------------
Validation seg loss: 0.3860040550682483 at epoch 251
epoch =    252/  1000, exp = train
train_iter_loss: 0.1716526299715042
train_iter_loss: 0.2648090720176697
train_iter_loss: 0.3894047737121582
train_iter_loss: 0.34993189573287964
train_iter_loss: 0.3401479721069336
train_iter_loss: 0.26722094416618347
train_iter_loss: 0.25090235471725464
train_iter_loss: 0.2404400259256363
train_iter_loss: 0.1516357809305191
train_iter_loss: 0.3285805583000183
train_iter_loss: 0.1485440880060196
train_iter_loss: 0.23774045705795288
train_iter_loss: 0.2526581287384033
train_iter_loss: 0.32794418931007385
train_iter_loss: 0.38426968455314636
train_iter_loss: 0.19876663386821747
train_iter_loss: 0.2558298707008362
train_iter_loss: 0.2041088193655014
train_iter_loss: 0.19521038234233856
train_iter_loss: 0.2753555476665497
train_iter_loss: 0.2711324393749237
train_iter_loss: 0.2921183109283447
train_iter_loss: 0.08786586672067642
train_iter_loss: 0.4741975963115692
train_iter_loss: 0.17646239697933197
train_iter_loss: 0.3815859854221344
train_iter_loss: 0.33225882053375244
train_iter_loss: 0.2771882116794586
train_iter_loss: 0.4279745817184448
train_iter_loss: 0.30963361263275146
train_iter_loss: 0.35946565866470337
train_iter_loss: 0.2988009452819824
train_iter_loss: 0.1484977900981903
train_iter_loss: 0.30237293243408203
train_iter_loss: 0.3366079330444336
train_iter_loss: 0.3465139865875244
train_iter_loss: 0.22536735236644745
train_iter_loss: 0.22277317941188812
train_iter_loss: 0.2708999514579773
train_iter_loss: 0.3574487864971161
train_iter_loss: 0.3540413975715637
train_iter_loss: 0.24061985313892365
train_iter_loss: 0.358567476272583
train_iter_loss: 0.2977277636528015
train_iter_loss: 0.1863890290260315
train_iter_loss: 0.19444824755191803
train_iter_loss: 0.22331035137176514
train_iter_loss: 0.24182426929473877
train_iter_loss: 0.46729469299316406
train_iter_loss: 0.2706584632396698
train_iter_loss: 0.3537241518497467
train_iter_loss: 0.2926008403301239
train_iter_loss: 0.3517591953277588
train_iter_loss: 0.30204513669013977
train_iter_loss: 0.3369484543800354
train_iter_loss: 0.2578413784503937
train_iter_loss: 0.3032957911491394
train_iter_loss: 0.1490277349948883
train_iter_loss: 0.3039444088935852
train_iter_loss: 0.1573605090379715
train_iter_loss: 0.32983633875846863
train_iter_loss: 0.2513129711151123
train_iter_loss: 0.28615114092826843
train_iter_loss: 0.3628212809562683
train_iter_loss: 0.3198617398738861
train_iter_loss: 0.5025175213813782
train_iter_loss: 0.19616961479187012
train_iter_loss: 0.2032376527786255
train_iter_loss: 0.34005990624427795
train_iter_loss: 0.1842067539691925
train_iter_loss: 0.03909710422158241
train_iter_loss: 0.2522507309913635
train_iter_loss: 0.2162657380104065
train_iter_loss: 0.38031402230262756
train_iter_loss: 0.1579333394765854
train_iter_loss: 0.20797115564346313
train_iter_loss: 0.3409273028373718
train_iter_loss: 0.2997438907623291
train_iter_loss: 0.25427675247192383
train_iter_loss: 0.17945915460586548
train_iter_loss: 0.3523370325565338
train_iter_loss: 0.279079407453537
train_iter_loss: 0.2992432713508606
train_iter_loss: 0.2396668791770935
train_iter_loss: 0.23278652131557465
train_iter_loss: 0.3086964190006256
train_iter_loss: 0.3844638466835022
train_iter_loss: 0.6263046860694885
train_iter_loss: 0.2417360544204712
train_iter_loss: 0.3366451561450958
train_iter_loss: 0.27161186933517456
train_iter_loss: 0.29719749093055725
train_iter_loss: 0.3789355754852295
train_iter_loss: 0.32552099227905273
train_iter_loss: 0.36531758308410645
train_iter_loss: 0.15346960723400116
train_iter_loss: 0.28890347480773926
train_iter_loss: 0.11556760966777802
train_iter_loss: 0.22592100501060486
train_iter_loss: 0.18345093727111816
train loss :0.2828
---------------------
Validation seg loss: 0.35830489817549876 at epoch 252
epoch =    253/  1000, exp = train
train_iter_loss: 0.1765454262495041
train_iter_loss: 0.0779537782073021
train_iter_loss: 0.1797710806131363
train_iter_loss: 0.1713729202747345
train_iter_loss: 0.20142818987369537
train_iter_loss: 0.5601778626441956
train_iter_loss: 0.31471356749534607
train_iter_loss: 0.2895969748497009
train_iter_loss: 0.12359242141246796
train_iter_loss: 0.21072617173194885
train_iter_loss: 0.1933818757534027
train_iter_loss: 0.29919710755348206
train_iter_loss: 0.2660984992980957
train_iter_loss: 0.4218969941139221
train_iter_loss: 0.4574262499809265
train_iter_loss: 0.20844149589538574
train_iter_loss: 0.2584216594696045
train_iter_loss: 0.2103361040353775
train_iter_loss: 0.28243014216423035
train_iter_loss: 0.2668732702732086
train_iter_loss: 0.17550025880336761
train_iter_loss: 0.22100110352039337
train_iter_loss: 0.28992003202438354
train_iter_loss: 0.398601770401001
train_iter_loss: 0.3780054748058319
train_iter_loss: 0.4385751485824585
train_iter_loss: 0.24732087552547455
train_iter_loss: 0.29652726650238037
train_iter_loss: 0.38522589206695557
train_iter_loss: 0.2525942027568817
train_iter_loss: 0.17903877794742584
train_iter_loss: 0.2726319134235382
train_iter_loss: 0.3157420754432678
train_iter_loss: 0.1958896517753601
train_iter_loss: 0.15216189622879028
train_iter_loss: 0.259285032749176
train_iter_loss: 0.18974895775318146
train_iter_loss: 0.4523182213306427
train_iter_loss: 0.342557817697525
train_iter_loss: 0.2938431203365326
train_iter_loss: 0.29282572865486145
train_iter_loss: 0.25943225622177124
train_iter_loss: 0.23258446156978607
train_iter_loss: 0.25312691926956177
train_iter_loss: 0.2369483858346939
train_iter_loss: 0.41922616958618164
train_iter_loss: 0.2801445424556732
train_iter_loss: 0.44961652159690857
train_iter_loss: 0.2909605801105499
train_iter_loss: 0.2878769338130951
train_iter_loss: 0.28640156984329224
train_iter_loss: 0.1607978641986847
train_iter_loss: 0.30741143226623535
train_iter_loss: 0.3879241347312927
train_iter_loss: 0.19802433252334595
train_iter_loss: 0.24924853444099426
train_iter_loss: 0.17589730024337769
train_iter_loss: 0.21643538773059845
train_iter_loss: 0.1745913326740265
train_iter_loss: 0.24197125434875488
train_iter_loss: 0.3594317138195038
train_iter_loss: 0.5401771068572998
train_iter_loss: 0.3353845477104187
train_iter_loss: 0.2716505229473114
train_iter_loss: 0.36581388115882874
train_iter_loss: 0.407861590385437
train_iter_loss: 0.20600847899913788
train_iter_loss: 0.3614976406097412
train_iter_loss: 0.2153693288564682
train_iter_loss: 0.35088038444519043
train_iter_loss: 0.3844769299030304
train_iter_loss: 0.17988668382167816
train_iter_loss: 0.3557592034339905
train_iter_loss: 0.3456786870956421
train_iter_loss: 0.2171938270330429
train_iter_loss: 0.27774184942245483
train_iter_loss: 0.16073283553123474
train_iter_loss: 0.2934221625328064
train_iter_loss: 0.2790391445159912
train_iter_loss: 0.33839744329452515
train_iter_loss: 0.3485247492790222
train_iter_loss: 0.33754923939704895
train_iter_loss: 0.27451464533805847
train_iter_loss: 0.3423399329185486
train_iter_loss: 0.25852006673812866
train_iter_loss: 0.23698417842388153
train_iter_loss: 0.31663230061531067
train_iter_loss: 0.2706402540206909
train_iter_loss: 0.3692034184932709
train_iter_loss: 0.17034243047237396
train_iter_loss: 0.36453670263290405
train_iter_loss: 0.2310500293970108
train_iter_loss: 0.20619632303714752
train_iter_loss: 0.5989726781845093
train_iter_loss: 0.31910189986228943
train_iter_loss: 0.2225884199142456
train_iter_loss: 0.3015798330307007
train_iter_loss: 0.13321693241596222
train_iter_loss: 0.2207586020231247
train_iter_loss: 0.2519330680370331
train loss :0.2861
---------------------
Validation seg loss: 0.37994958789808286 at epoch 253
epoch =    254/  1000, exp = train
train_iter_loss: 0.3191542625427246
train_iter_loss: 0.289725661277771
train_iter_loss: 0.24653245508670807
train_iter_loss: 0.19907142221927643
train_iter_loss: 0.430606871843338
train_iter_loss: 0.4016381502151489
train_iter_loss: 0.39557018876075745
train_iter_loss: 0.24795638024806976
train_iter_loss: 0.3057713210582733
train_iter_loss: 0.22576996684074402
train_iter_loss: 0.28907281160354614
train_iter_loss: 0.26268744468688965
train_iter_loss: 0.3787114918231964
train_iter_loss: 0.33332157135009766
train_iter_loss: 0.24365730583667755
train_iter_loss: 0.32922783493995667
train_iter_loss: 0.31386831402778625
train_iter_loss: 0.2315397709608078
train_iter_loss: 0.36303219199180603
train_iter_loss: 0.30018350481987
train_iter_loss: 0.2609265148639679
train_iter_loss: 0.3450683057308197
train_iter_loss: 0.08152921497821808
train_iter_loss: 0.3307727873325348
train_iter_loss: 0.12729375064373016
train_iter_loss: 0.2453845739364624
train_iter_loss: 0.3305826485157013
train_iter_loss: 0.2520870268344879
train_iter_loss: 0.3068867325782776
train_iter_loss: 0.19335734844207764
train_iter_loss: 0.26883620023727417
train_iter_loss: 0.2210010290145874
train_iter_loss: 0.37519559264183044
train_iter_loss: 0.23707735538482666
train_iter_loss: 0.17749051749706268
train_iter_loss: 0.49415624141693115
train_iter_loss: 0.17401812970638275
train_iter_loss: 0.2769217789173126
train_iter_loss: 0.34244826436042786
train_iter_loss: 0.363821417093277
train_iter_loss: 0.10081329196691513
train_iter_loss: 0.33087363839149475
train_iter_loss: 0.26235708594322205
train_iter_loss: 0.2856972813606262
train_iter_loss: 0.2385496199131012
train_iter_loss: 0.28922027349472046
train_iter_loss: 0.16747821867465973
train_iter_loss: 0.34365102648735046
train_iter_loss: 0.24347448348999023
train_iter_loss: 0.17742475867271423
train_iter_loss: 0.3586152493953705
train_iter_loss: 0.2667843997478485
train_iter_loss: 0.37574392557144165
train_iter_loss: 0.11760581284761429
train_iter_loss: 0.2517728805541992
train_iter_loss: 0.3420071005821228
train_iter_loss: 0.28191766142845154
train_iter_loss: 0.27454134821891785
train_iter_loss: 0.42310842871665955
train_iter_loss: 0.34221723675727844
train_iter_loss: 0.14858180284500122
train_iter_loss: 0.1873134970664978
train_iter_loss: 0.1744793802499771
train_iter_loss: 0.2565756142139435
train_iter_loss: 0.23152711987495422
train_iter_loss: 0.3749326467514038
train_iter_loss: 0.23061266541481018
train_iter_loss: 0.3740774393081665
train_iter_loss: 0.3217760920524597
train_iter_loss: 0.3142591118812561
train_iter_loss: 0.1967940479516983
train_iter_loss: 0.23091402649879456
train_iter_loss: 0.33836808800697327
train_iter_loss: 0.34834015369415283
train_iter_loss: 0.3245653212070465
train_iter_loss: 0.4545542001724243
train_iter_loss: 0.21504808962345123
train_iter_loss: 0.2906138300895691
train_iter_loss: 0.29023972153663635
train_iter_loss: 0.2794611155986786
train_iter_loss: 0.30207279324531555
train_iter_loss: 0.26260727643966675
train_iter_loss: 0.2331172376871109
train_iter_loss: 0.10957147181034088
train_iter_loss: 0.34696221351623535
train_iter_loss: 0.4807279706001282
train_iter_loss: 0.28325769305229187
train_iter_loss: 0.304973840713501
train_iter_loss: 0.321734756231308
train_iter_loss: 0.25724244117736816
train_iter_loss: 0.4234544336795807
train_iter_loss: 0.4641049802303314
train_iter_loss: 0.2579902708530426
train_iter_loss: 0.27059406042099
train_iter_loss: 0.21949847042560577
train_iter_loss: 0.20832209289073944
train_iter_loss: 0.14912910759449005
train_iter_loss: 0.18299153447151184
train_iter_loss: 0.41916266083717346
train_iter_loss: 0.23304705321788788
train loss :0.2859
---------------------
Validation seg loss: 0.37798385952933217 at epoch 254
epoch =    255/  1000, exp = train
train_iter_loss: 0.17308172583580017
train_iter_loss: 0.37261462211608887
train_iter_loss: 0.31984326243400574
train_iter_loss: 0.21238663792610168
train_iter_loss: 0.228092223405838
train_iter_loss: 0.4222537875175476
train_iter_loss: 0.253214567899704
train_iter_loss: 0.2951263189315796
train_iter_loss: 0.280703604221344
train_iter_loss: 0.44685596227645874
train_iter_loss: 0.30407965183258057
train_iter_loss: 0.25325337052345276
train_iter_loss: 0.21032634377479553
train_iter_loss: 0.22219766676425934
train_iter_loss: 0.19925935566425323
train_iter_loss: 0.4365023970603943
train_iter_loss: 0.18259435892105103
train_iter_loss: 0.187557190656662
train_iter_loss: 0.2547484040260315
train_iter_loss: 0.29433518648147583
train_iter_loss: 0.21294498443603516
train_iter_loss: 0.23628650605678558
train_iter_loss: 0.17413651943206787
train_iter_loss: 0.23494240641593933
train_iter_loss: 0.24048686027526855
train_iter_loss: 0.4396430552005768
train_iter_loss: 0.39582589268684387
train_iter_loss: 0.1454189270734787
train_iter_loss: 0.3122124671936035
train_iter_loss: 0.3680863678455353
train_iter_loss: 0.36260658502578735
train_iter_loss: 0.3243570029735565
train_iter_loss: 0.2733541429042816
train_iter_loss: 0.2864649295806885
train_iter_loss: 0.3516925871372223
train_iter_loss: 0.31320837140083313
train_iter_loss: 0.11804123967885971
train_iter_loss: 0.36508336663246155
train_iter_loss: 0.15093670785427094
train_iter_loss: 0.1920405924320221
train_iter_loss: 0.22037602961063385
train_iter_loss: 0.3150853216648102
train_iter_loss: 0.2562963366508484
train_iter_loss: 0.4040747582912445
train_iter_loss: 0.3716801106929779
train_iter_loss: 0.24984638392925262
train_iter_loss: 0.24518299102783203
train_iter_loss: 0.24654269218444824
train_iter_loss: 0.400369793176651
train_iter_loss: 0.18587571382522583
train_iter_loss: 0.3062041997909546
train_iter_loss: 0.17565253376960754
train_iter_loss: 0.3554285764694214
train_iter_loss: 0.40209323167800903
train_iter_loss: 0.25905686616897583
train_iter_loss: 0.22185255587100983
train_iter_loss: 0.34167206287384033
train_iter_loss: 0.27991047501564026
train_iter_loss: 0.13634797930717468
train_iter_loss: 0.1798558384180069
train_iter_loss: 0.26940667629241943
train_iter_loss: 0.2979792654514313
train_iter_loss: 0.3370582163333893
train_iter_loss: 0.2198319286108017
train_iter_loss: 0.2811242341995239
train_iter_loss: 0.2809733748435974
train_iter_loss: 0.4782702624797821
train_iter_loss: 0.24140486121177673
train_iter_loss: 0.321346640586853
train_iter_loss: 0.2622825801372528
train_iter_loss: 0.4411386549472809
train_iter_loss: 0.4258567988872528
train_iter_loss: 0.16318079829216003
train_iter_loss: 0.21452507376670837
train_iter_loss: 0.4420957863330841
train_iter_loss: 0.313444048166275
train_iter_loss: 0.19351954758167267
train_iter_loss: 0.2743634581565857
train_iter_loss: 0.3798290193080902
train_iter_loss: 0.21515847742557526
train_iter_loss: 0.1448618769645691
train_iter_loss: 0.17724432051181793
train_iter_loss: 0.26053565740585327
train_iter_loss: 0.31324005126953125
train_iter_loss: 0.12191598117351532
train_iter_loss: 0.26626354455947876
train_iter_loss: 0.2995941638946533
train_iter_loss: 0.4505923092365265
train_iter_loss: 0.23732559382915497
train_iter_loss: 0.336000919342041
train_iter_loss: 0.3139740228652954
train_iter_loss: 0.42137494683265686
train_iter_loss: 0.11047409474849701
train_iter_loss: 0.34700873494148254
train_iter_loss: 0.2607036530971527
train_iter_loss: 0.3657194972038269
train_iter_loss: 0.5189698934555054
train_iter_loss: 0.17108824849128723
train_iter_loss: 0.21968644857406616
train_iter_loss: 0.2777256965637207
train loss :0.2855
---------------------
Validation seg loss: 0.37454466318781926 at epoch 255
epoch =    256/  1000, exp = train
train_iter_loss: 0.48445719480514526
train_iter_loss: 0.19624973833560944
train_iter_loss: 0.24403303861618042
train_iter_loss: 0.15081366896629333
train_iter_loss: 0.29868850111961365
train_iter_loss: 0.4408930540084839
train_iter_loss: 0.2555805444717407
train_iter_loss: 0.2972056269645691
train_iter_loss: 0.32210293412208557
train_iter_loss: 0.2343187779188156
train_iter_loss: 0.23775196075439453
train_iter_loss: 0.17142705619335175
train_iter_loss: 0.25901755690574646
train_iter_loss: 0.17844155430793762
train_iter_loss: 0.2679097354412079
train_iter_loss: 0.2258298099040985
train_iter_loss: 0.3309555649757385
train_iter_loss: 0.19778454303741455
train_iter_loss: 0.21693018078804016
train_iter_loss: 0.21610888838768005
train_iter_loss: 0.2896130383014679
train_iter_loss: 0.370765745639801
train_iter_loss: 0.30172666907310486
train_iter_loss: 0.27144956588745117
train_iter_loss: 0.25059759616851807
train_iter_loss: 0.19622257351875305
train_iter_loss: 0.20381194353103638
train_iter_loss: 0.25430887937545776
train_iter_loss: 0.2453097403049469
train_iter_loss: 0.31789475679397583
train_iter_loss: 0.2626759707927704
train_iter_loss: 0.243296816945076
train_iter_loss: 0.3055066764354706
train_iter_loss: 0.30916523933410645
train_iter_loss: 0.2608432173728943
train_iter_loss: 0.0872199684381485
train_iter_loss: 0.27235084772109985
train_iter_loss: 0.18430744111537933
train_iter_loss: 0.28832292556762695
train_iter_loss: 0.47491830587387085
train_iter_loss: 0.20562180876731873
train_iter_loss: 0.21639126539230347
train_iter_loss: 0.25054481625556946
train_iter_loss: 0.2821528911590576
train_iter_loss: 0.3289240896701813
train_iter_loss: 0.25913533568382263
train_iter_loss: 0.3827383518218994
train_iter_loss: 0.5154457688331604
train_iter_loss: 0.19464336335659027
train_iter_loss: 0.16473060846328735
train_iter_loss: 0.3685992658138275
train_iter_loss: 0.3156307339668274
train_iter_loss: 0.2506604492664337
train_iter_loss: 0.33357709646224976
train_iter_loss: 0.28813090920448303
train_iter_loss: 0.18573863804340363
train_iter_loss: 0.26959744095802307
train_iter_loss: 0.4457910656929016
train_iter_loss: 0.25172486901283264
train_iter_loss: 0.33104994893074036
train_iter_loss: 0.46997421979904175
train_iter_loss: 0.13252604007720947
train_iter_loss: 0.28684425354003906
train_iter_loss: 0.24688206613063812
train_iter_loss: 0.44304051995277405
train_iter_loss: 0.298168420791626
train_iter_loss: 0.27424725890159607
train_iter_loss: 0.3354314863681793
train_iter_loss: 0.3214000165462494
train_iter_loss: 0.30750545859336853
train_iter_loss: 0.4918742775917053
train_iter_loss: 0.3866172432899475
train_iter_loss: 0.2563668191432953
train_iter_loss: 0.20757637917995453
train_iter_loss: 0.3032956123352051
train_iter_loss: 0.3018694519996643
train_iter_loss: 0.25001034140586853
train_iter_loss: 0.24460655450820923
train_iter_loss: 0.19014950096607208
train_iter_loss: 0.28004124760627747
train_iter_loss: 0.15612876415252686
train_iter_loss: 0.2853389382362366
train_iter_loss: 0.22211140394210815
train_iter_loss: 0.2567879855632782
train_iter_loss: 0.25102540850639343
train_iter_loss: 0.2661997079849243
train_iter_loss: 0.24407769739627838
train_iter_loss: 0.5067007541656494
train_iter_loss: 0.15815924108028412
train_iter_loss: 0.3151084780693054
train_iter_loss: 0.29297974705696106
train_iter_loss: 0.22888948023319244
train_iter_loss: 0.2745000422000885
train_iter_loss: 0.30604612827301025
train_iter_loss: 0.1786980777978897
train_iter_loss: 0.20572103559970856
train_iter_loss: 0.062081824988126755
train_iter_loss: 0.39002326130867004
train_iter_loss: 0.2518036663532257
train_iter_loss: 0.18874643743038177
train loss :0.2781
---------------------
Validation seg loss: 0.35866837205259866 at epoch 256
epoch =    257/  1000, exp = train
train_iter_loss: 0.18808411061763763
train_iter_loss: 0.050567470490932465
train_iter_loss: 0.15876361727714539
train_iter_loss: 0.2779354751110077
train_iter_loss: 0.2875385284423828
train_iter_loss: 0.19901038706302643
train_iter_loss: 0.11528172343969345
train_iter_loss: 0.2951774001121521
train_iter_loss: 0.266412615776062
train_iter_loss: 0.10241784900426865
train_iter_loss: 0.30362504720687866
train_iter_loss: 0.22668899595737457
train_iter_loss: 0.2617606222629547
train_iter_loss: 0.44277170300483704
train_iter_loss: 0.20450253784656525
train_iter_loss: 0.16796022653579712
train_iter_loss: 0.7070854902267456
train_iter_loss: 0.5674742460250854
train_iter_loss: 0.14078009128570557
train_iter_loss: 0.13326337933540344
train_iter_loss: 0.2896352708339691
train_iter_loss: 0.25699061155319214
train_iter_loss: 0.24179959297180176
train_iter_loss: 0.1506693959236145
train_iter_loss: 0.3792681396007538
train_iter_loss: 0.23433077335357666
train_iter_loss: 0.15885800123214722
train_iter_loss: 0.09916424006223679
train_iter_loss: 0.2647022604942322
train_iter_loss: 0.27350321412086487
train_iter_loss: 0.47214847803115845
train_iter_loss: 0.21338045597076416
train_iter_loss: 0.316083163022995
train_iter_loss: 0.5251902341842651
train_iter_loss: 0.2559124827384949
train_iter_loss: 0.3974791467189789
train_iter_loss: 0.3012493848800659
train_iter_loss: 0.2526888847351074
train_iter_loss: 0.25046005845069885
train_iter_loss: 0.26136359572410583
train_iter_loss: 0.5891932249069214
train_iter_loss: 0.25096458196640015
train_iter_loss: 0.39587923884391785
train_iter_loss: 0.3064425587654114
train_iter_loss: 0.28375014662742615
train_iter_loss: 0.36614158749580383
train_iter_loss: 0.1544223427772522
train_iter_loss: 0.3486058712005615
train_iter_loss: 0.26758474111557007
train_iter_loss: 0.3206620514392853
train_iter_loss: 0.21842771768569946
train_iter_loss: 0.2028624266386032
train_iter_loss: 0.264866441488266
train_iter_loss: 0.27572235465049744
train_iter_loss: 0.3522860109806061
train_iter_loss: 0.33122706413269043
train_iter_loss: 0.35464537143707275
train_iter_loss: 0.3555222749710083
train_iter_loss: 0.10400982946157455
train_iter_loss: 0.22744525969028473
train_iter_loss: 0.16046471893787384
train_iter_loss: 0.295022577047348
train_iter_loss: 0.2480878382921219
train_iter_loss: 0.2906409800052643
train_iter_loss: 0.1999746412038803
train_iter_loss: 0.2424936592578888
train_iter_loss: 0.3128039836883545
train_iter_loss: 0.16408589482307434
train_iter_loss: 0.285336971282959
train_iter_loss: 0.3174913823604584
train_iter_loss: 0.252963125705719
train_iter_loss: 0.38649389147758484
train_iter_loss: 0.2690480053424835
train_iter_loss: 0.2734226882457733
train_iter_loss: 0.21916967630386353
train_iter_loss: 0.33887216448783875
train_iter_loss: 0.22415480017662048
train_iter_loss: 0.31669342517852783
train_iter_loss: 0.3083193302154541
train_iter_loss: 0.3453701436519623
train_iter_loss: 0.29900458455085754
train_iter_loss: 0.24142271280288696
train_iter_loss: 0.19575469195842743
train_iter_loss: 0.2423604130744934
train_iter_loss: 0.2984829545021057
train_iter_loss: 0.21303443610668182
train_iter_loss: 0.22393064200878143
train_iter_loss: 0.16161945462226868
train_iter_loss: 0.3397811949253082
train_iter_loss: 0.23990637063980103
train_iter_loss: 0.18287934362888336
train_iter_loss: 0.28096890449523926
train_iter_loss: 0.45045986771583557
train_iter_loss: 0.3295649290084839
train_iter_loss: 0.3465038239955902
train_iter_loss: 0.3092000484466553
train_iter_loss: 0.20557035505771637
train_iter_loss: 0.39955073595046997
train_iter_loss: 0.1964314579963684
train_iter_loss: 0.38422542810440063
train loss :0.2793
---------------------
Validation seg loss: 0.37576631756218254 at epoch 257
epoch =    258/  1000, exp = train
train_iter_loss: 0.27173730731010437
train_iter_loss: 0.35441678762435913
train_iter_loss: 0.21867437660694122
train_iter_loss: 0.41884154081344604
train_iter_loss: 0.3191092312335968
train_iter_loss: 0.20020776987075806
train_iter_loss: 0.2686326205730438
train_iter_loss: 0.23907095193862915
train_iter_loss: 0.43574050068855286
train_iter_loss: 0.26665979623794556
train_iter_loss: 0.19007250666618347
train_iter_loss: 0.31486305594444275
train_iter_loss: 0.22413991391658783
train_iter_loss: 0.3016241490840912
train_iter_loss: 0.20299901068210602
train_iter_loss: 0.48603954911231995
train_iter_loss: 0.2522667944431305
train_iter_loss: 0.2287902534008026
train_iter_loss: 0.27998989820480347
train_iter_loss: 0.33922672271728516
train_iter_loss: 0.39882925152778625
train_iter_loss: 0.2756277620792389
train_iter_loss: 0.5747631192207336
train_iter_loss: 0.3210400342941284
train_iter_loss: 0.31196361780166626
train_iter_loss: 0.2794749140739441
train_iter_loss: 0.2669733166694641
train_iter_loss: 0.2874082922935486
train_iter_loss: 0.25026610493659973
train_iter_loss: 0.47807374596595764
train_iter_loss: 0.1959429830312729
train_iter_loss: 0.29739418625831604
train_iter_loss: 0.23513372242450714
train_iter_loss: 0.10946834832429886
train_iter_loss: 0.2169378101825714
train_iter_loss: 0.18107053637504578
train_iter_loss: 0.1800670474767685
train_iter_loss: 0.3078584671020508
train_iter_loss: 0.23286178708076477
train_iter_loss: 0.33522823452949524
train_iter_loss: 0.40614452958106995
train_iter_loss: 0.2596876919269562
train_iter_loss: 0.24553391337394714
train_iter_loss: 0.4142288863658905
train_iter_loss: 0.4524695873260498
train_iter_loss: 0.2741759121417999
train_iter_loss: 0.1584247499704361
train_iter_loss: 0.2423529475927353
train_iter_loss: 0.2957082688808441
train_iter_loss: 0.22203172743320465
train_iter_loss: 0.1291048675775528
train_iter_loss: 0.26856809854507446
train_iter_loss: 0.25703567266464233
train_iter_loss: 0.39055436849594116
train_iter_loss: 0.4127633273601532
train_iter_loss: 0.33985814452171326
train_iter_loss: 0.3434881865978241
train_iter_loss: 0.4857712984085083
train_iter_loss: 0.3981931209564209
train_iter_loss: 0.3914169371128082
train_iter_loss: 0.3343997299671173
train_iter_loss: 0.389241486787796
train_iter_loss: 0.2769545912742615
train_iter_loss: 0.3835056722164154
train_iter_loss: 0.29177209734916687
train_iter_loss: 0.17528647184371948
train_iter_loss: 0.3690677881240845
train_iter_loss: 0.15232953429222107
train_iter_loss: 0.2128896862268448
train_iter_loss: 0.1653279960155487
train_iter_loss: 0.2703890800476074
train_iter_loss: 0.2598549723625183
train_iter_loss: 0.19543614983558655
train_iter_loss: 0.2247081845998764
train_iter_loss: 0.29934069514274597
train_iter_loss: 0.16299276053905487
train_iter_loss: 0.16566899418830872
train_iter_loss: 0.42274442315101624
train_iter_loss: 0.20303168892860413
train_iter_loss: 0.30167511105537415
train_iter_loss: 0.16535043716430664
train_iter_loss: 0.2161114662885666
train_iter_loss: 0.24107204377651215
train_iter_loss: 0.3040134012699127
train_iter_loss: 0.29660770297050476
train_iter_loss: 0.09412023425102234
train_iter_loss: 0.32789346575737
train_iter_loss: 0.251496821641922
train_iter_loss: 0.3137638568878174
train_iter_loss: 0.34278786182403564
train_iter_loss: 0.191358745098114
train_iter_loss: 0.3280501663684845
train_iter_loss: 0.33781254291534424
train_iter_loss: 0.4088814854621887
train_iter_loss: 0.3225129544734955
train_iter_loss: 0.44553297758102417
train_iter_loss: 0.3561449646949768
train_iter_loss: 0.21908491849899292
train_iter_loss: 0.3909391462802887
train_iter_loss: 0.22208251059055328
train loss :0.2925
---------------------
Validation seg loss: 0.377637931620175 at epoch 258
epoch =    259/  1000, exp = train
train_iter_loss: 0.391044944524765
train_iter_loss: 0.3637581467628479
train_iter_loss: 0.2711922526359558
train_iter_loss: 0.2879769802093506
train_iter_loss: 0.30157017707824707
train_iter_loss: 0.36817416548728943
train_iter_loss: 0.2844003140926361
train_iter_loss: 0.39316898584365845
train_iter_loss: 0.4236908555030823
train_iter_loss: 0.25475382804870605
train_iter_loss: 0.3377210199832916
train_iter_loss: 0.16670802235603333
train_iter_loss: 0.25256839394569397
train_iter_loss: 0.33338984847068787
train_iter_loss: 0.49410489201545715
train_iter_loss: 0.20723868906497955
train_iter_loss: 0.2865326702594757
train_iter_loss: 0.1925048977136612
train_iter_loss: 0.17591968178749084
train_iter_loss: 0.1519048511981964
train_iter_loss: 0.21605736017227173
train_iter_loss: 0.217635378241539
train_iter_loss: 0.35059288144111633
train_iter_loss: 0.30699247121810913
train_iter_loss: 0.26621487736701965
train_iter_loss: 0.31477996706962585
train_iter_loss: 0.21831943094730377
train_iter_loss: 0.39075443148612976
train_iter_loss: 0.39118659496307373
train_iter_loss: 0.25098347663879395
train_iter_loss: 0.20919132232666016
train_iter_loss: 0.2113705426454544
train_iter_loss: 0.3609449863433838
train_iter_loss: 0.2135361284017563
train_iter_loss: 0.19931620359420776
train_iter_loss: 0.1870836764574051
train_iter_loss: 0.3308286964893341
train_iter_loss: 0.22296547889709473
train_iter_loss: 0.2531602084636688
train_iter_loss: 0.2611187696456909
train_iter_loss: 0.20692840218544006
train_iter_loss: 0.3891091048717499
train_iter_loss: 0.3006148040294647
train_iter_loss: 0.22661270201206207
train_iter_loss: 0.20147988200187683
train_iter_loss: 0.18382298946380615
train_iter_loss: 0.3187980353832245
train_iter_loss: 0.09647694230079651
train_iter_loss: 0.3248210549354553
train_iter_loss: 0.1676316112279892
train_iter_loss: 0.48235848546028137
train_iter_loss: 0.44225403666496277
train_iter_loss: 0.32135799527168274
train_iter_loss: 0.4570777118206024
train_iter_loss: 0.24784919619560242
train_iter_loss: 0.18693114817142487
train_iter_loss: 0.22800613939762115
train_iter_loss: 0.2415207326412201
train_iter_loss: 0.2783741056919098
train_iter_loss: 0.12977471947669983
train_iter_loss: 0.3353109657764435
train_iter_loss: 0.3635307848453522
train_iter_loss: 0.37315160036087036
train_iter_loss: 0.2575293481349945
train_iter_loss: 0.27897316217422485
train_iter_loss: 0.36789318919181824
train_iter_loss: 0.21118144690990448
train_iter_loss: 0.2324811816215515
train_iter_loss: 0.3257914185523987
train_iter_loss: 0.3132856786251068
train_iter_loss: 0.20614361763000488
train_iter_loss: 0.1347046196460724
train_iter_loss: 0.25368040800094604
train_iter_loss: 0.31460219621658325
train_iter_loss: 0.057035546749830246
train_iter_loss: 0.24725212156772614
train_iter_loss: 0.14374054968357086
train_iter_loss: 0.2607631981372833
train_iter_loss: 0.27136510610580444
train_iter_loss: 0.5793188214302063
train_iter_loss: 0.41343238949775696
train_iter_loss: 0.3051237165927887
train_iter_loss: 0.3067016005516052
train_iter_loss: 0.3260306715965271
train_iter_loss: 0.09698652476072311
train_iter_loss: 0.46632394194602966
train_iter_loss: 0.2736635208129883
train_iter_loss: 0.34508344531059265
train_iter_loss: 0.2799909710884094
train_iter_loss: 0.229697123169899
train_iter_loss: 0.3402705788612366
train_iter_loss: 0.2860429883003235
train_iter_loss: 0.21452151238918304
train_iter_loss: 0.39502662420272827
train_iter_loss: 0.042326610535383224
train_iter_loss: 0.2916679382324219
train_iter_loss: 0.21975421905517578
train_iter_loss: 0.13920436799526215
train_iter_loss: 0.38680917024612427
train_iter_loss: 0.4401676654815674
train loss :0.2836
---------------------
Validation seg loss: 0.3857734435466382 at epoch 259
epoch =    260/  1000, exp = train
train_iter_loss: 0.22809836268424988
train_iter_loss: 0.20793363451957703
train_iter_loss: 0.2948666512966156
train_iter_loss: 0.24863988161087036
train_iter_loss: 0.32584139704704285
train_iter_loss: 0.37030401825904846
train_iter_loss: 0.3633756637573242
train_iter_loss: 0.2132512480020523
train_iter_loss: 0.15777282416820526
train_iter_loss: 0.30445972084999084
train_iter_loss: 0.18651336431503296
train_iter_loss: 0.10997705161571503
train_iter_loss: 0.2178160399198532
train_iter_loss: 0.36992529034614563
train_iter_loss: 0.315615713596344
train_iter_loss: 0.4115547239780426
train_iter_loss: 0.4562245309352875
train_iter_loss: 0.20894663035869598
train_iter_loss: 0.25404030084609985
train_iter_loss: 0.3096562623977661
train_iter_loss: 0.26366040110588074
train_iter_loss: 0.4762459993362427
train_iter_loss: 0.27035632729530334
train_iter_loss: 0.25654152035713196
train_iter_loss: 0.2796669602394104
train_iter_loss: 0.3053191304206848
train_iter_loss: 0.33862000703811646
train_iter_loss: 0.349710077047348
train_iter_loss: 0.3643963932991028
train_iter_loss: 0.31266704201698303
train_iter_loss: 0.22389979660511017
train_iter_loss: 0.24809496104717255
train_iter_loss: 0.32096582651138306
train_iter_loss: 0.2887150049209595
train_iter_loss: 0.36880919337272644
train_iter_loss: 0.25285476446151733
train_iter_loss: 0.23895299434661865
train_iter_loss: 0.21340300142765045
train_iter_loss: 0.26116397976875305
train_iter_loss: 0.35260531306266785
train_iter_loss: 0.06783443689346313
train_iter_loss: 0.19582611322402954
train_iter_loss: 0.4271779954433441
train_iter_loss: 0.19299034774303436
train_iter_loss: 0.197905033826828
train_iter_loss: 0.3988529443740845
train_iter_loss: 0.23345953226089478
train_iter_loss: 0.2500814199447632
train_iter_loss: 0.2744247019290924
train_iter_loss: 0.3276696503162384
train_iter_loss: 0.10903018712997437
train_iter_loss: 0.1350192278623581
train_iter_loss: 0.2108694463968277
train_iter_loss: 0.3916926980018616
train_iter_loss: 0.15519189834594727
train_iter_loss: 0.21033966541290283
train_iter_loss: 0.1408219337463379
train_iter_loss: 0.37173593044281006
train_iter_loss: 0.26431989669799805
train_iter_loss: 0.2902580499649048
train_iter_loss: 0.4442662298679352
train_iter_loss: 0.2290620654821396
train_iter_loss: 0.30798518657684326
train_iter_loss: 0.08526631444692612
train_iter_loss: 0.2740284502506256
train_iter_loss: 0.16605651378631592
train_iter_loss: 0.275729775428772
train_iter_loss: 0.29091981053352356
train_iter_loss: 0.27257832884788513
train_iter_loss: 0.4317066967487335
train_iter_loss: 0.3019672930240631
train_iter_loss: 0.22066999971866608
train_iter_loss: 0.16159707307815552
train_iter_loss: 0.1425134539604187
train_iter_loss: 0.22640715539455414
train_iter_loss: 0.31175464391708374
train_iter_loss: 0.21910622715950012
train_iter_loss: 0.21350842714309692
train_iter_loss: 0.27004146575927734
train_iter_loss: 0.15186230838298798
train_iter_loss: 0.2485562264919281
train_iter_loss: 0.20217207074165344
train_iter_loss: 0.22692397236824036
train_iter_loss: 0.22731313109397888
train_iter_loss: 0.3118581175804138
train_iter_loss: 0.2582014501094818
train_iter_loss: 0.36146342754364014
train_iter_loss: 0.47497233748435974
train_iter_loss: 0.31141313910484314
train_iter_loss: 0.301674485206604
train_iter_loss: 0.29811227321624756
train_iter_loss: 0.39399242401123047
train_iter_loss: 0.2457374632358551
train_iter_loss: 0.47673654556274414
train_iter_loss: 0.18713988363742828
train_iter_loss: 0.26544705033302307
train_iter_loss: 0.4047166407108307
train_iter_loss: 0.31056326627731323
train_iter_loss: 0.2148578017950058
train_iter_loss: 0.25412964820861816
train loss :0.2764
---------------------
Validation seg loss: 0.37321368613684514 at epoch 260
epoch =    261/  1000, exp = train
train_iter_loss: 0.27585718035697937
train_iter_loss: 0.330541729927063
train_iter_loss: 0.40394046902656555
train_iter_loss: 0.13462834060192108
train_iter_loss: 0.2675219774246216
train_iter_loss: 0.25078344345092773
train_iter_loss: 0.24337226152420044
train_iter_loss: 0.32025328278541565
train_iter_loss: 0.2373388111591339
train_iter_loss: 0.16930843889713287
train_iter_loss: 0.14337705075740814
train_iter_loss: 0.3280019462108612
train_iter_loss: 0.40046408772468567
train_iter_loss: 0.20040489733219147
train_iter_loss: 0.4499076008796692
train_iter_loss: 0.3289449214935303
train_iter_loss: 0.2523112893104553
train_iter_loss: 0.2311304658651352
train_iter_loss: 0.28151845932006836
train_iter_loss: 0.09223224967718124
train_iter_loss: 0.27880004048347473
train_iter_loss: 0.2155340611934662
train_iter_loss: 0.18259045481681824
train_iter_loss: 0.268359512090683
train_iter_loss: 0.23597125709056854
train_iter_loss: 0.29275065660476685
train_iter_loss: 0.3284896910190582
train_iter_loss: 0.29310500621795654
train_iter_loss: 0.3844328820705414
train_iter_loss: 0.10917788743972778
train_iter_loss: 0.36980611085891724
train_iter_loss: 0.40141043066978455
train_iter_loss: 0.3280075788497925
train_iter_loss: 0.3080252408981323
train_iter_loss: 0.3004513084888458
train_iter_loss: 0.2994076907634735
train_iter_loss: 0.3238440454006195
train_iter_loss: 0.2356267273426056
train_iter_loss: 0.4961046576499939
train_iter_loss: 0.23073670268058777
train_iter_loss: 0.4015968143939972
train_iter_loss: 0.31441402435302734
train_iter_loss: 0.18737256526947021
train_iter_loss: 0.29544150829315186
train_iter_loss: 0.2763739824295044
train_iter_loss: 0.3847607970237732
train_iter_loss: 0.25898879766464233
train_iter_loss: 0.25241416692733765
train_iter_loss: 0.11651183664798737
train_iter_loss: 0.2643435597419739
train_iter_loss: 0.1985616236925125
train_iter_loss: 0.26726990938186646
train_iter_loss: 0.17417356371879578
train_iter_loss: 0.45165160298347473
train_iter_loss: 0.26890626549720764
train_iter_loss: 0.2571980357170105
train_iter_loss: 0.18218058347702026
train_iter_loss: 0.3881552219390869
train_iter_loss: 0.2582143545150757
train_iter_loss: 0.4448898732662201
train_iter_loss: 0.19033177196979523
train_iter_loss: 0.30607637763023376
train_iter_loss: 0.25094074010849
train_iter_loss: 0.23219610750675201
train_iter_loss: 0.3979753851890564
train_iter_loss: 0.3997245728969574
train_iter_loss: 0.26509493589401245
train_iter_loss: 0.16702300310134888
train_iter_loss: 0.21556030213832855
train_iter_loss: 0.36339980363845825
train_iter_loss: 0.31901004910469055
train_iter_loss: 0.21174661815166473
train_iter_loss: 0.2258909046649933
train_iter_loss: 0.2215384691953659
train_iter_loss: 0.29236096143722534
train_iter_loss: 0.4317266047000885
train_iter_loss: 0.2436627447605133
train_iter_loss: 0.2793615758419037
train_iter_loss: 0.49296337366104126
train_iter_loss: 0.18929746747016907
train_iter_loss: 0.3759182095527649
train_iter_loss: 0.2107047587633133
train_iter_loss: 0.34687960147857666
train_iter_loss: 0.19224916398525238
train_iter_loss: 0.3283187448978424
train_iter_loss: 0.35174494981765747
train_iter_loss: 0.29753395915031433
train_iter_loss: 0.22719357907772064
train_iter_loss: 0.43828216195106506
train_iter_loss: 0.26564159989356995
train_iter_loss: 0.26529785990715027
train_iter_loss: 0.22735930979251862
train_iter_loss: 0.31750667095184326
train_iter_loss: 0.27706044912338257
train_iter_loss: 0.2690708041191101
train_iter_loss: 0.32893142104148865
train_iter_loss: 0.23141296207904816
train_iter_loss: 0.2827523350715637
train_iter_loss: 0.2593531012535095
train_iter_loss: 0.39360448718070984
train loss :0.2873
---------------------
Validation seg loss: 0.38280447010161744 at epoch 261
epoch =    262/  1000, exp = train
train_iter_loss: 0.17241235077381134
train_iter_loss: 0.2941858172416687
train_iter_loss: 0.3672698736190796
train_iter_loss: 0.33184096217155457
train_iter_loss: 0.32103633880615234
train_iter_loss: 0.17003518342971802
train_iter_loss: 0.3176271915435791
train_iter_loss: 0.32067033648490906
train_iter_loss: 0.318454772233963
train_iter_loss: 0.28300437331199646
train_iter_loss: 0.2644079625606537
train_iter_loss: 0.16859111189842224
train_iter_loss: 0.21272912621498108
train_iter_loss: 0.2615954875946045
train_iter_loss: 0.43710091710090637
train_iter_loss: 0.1111067458987236
train_iter_loss: 0.18266788125038147
train_iter_loss: 0.27745291590690613
train_iter_loss: 0.33946892619132996
train_iter_loss: 0.30503520369529724
train_iter_loss: 0.41485899686813354
train_iter_loss: 0.3635256290435791
train_iter_loss: 0.30261754989624023
train_iter_loss: 0.36163148283958435
train_iter_loss: 0.20134863257408142
train_iter_loss: 0.18814405798912048
train_iter_loss: 0.18791460990905762
train_iter_loss: 0.4350503087043762
train_iter_loss: 0.2528188228607178
train_iter_loss: 0.18960702419281006
train_iter_loss: 0.10484595596790314
train_iter_loss: 0.2965078353881836
train_iter_loss: 0.1189088225364685
train_iter_loss: 0.3267606198787689
train_iter_loss: 0.30961573123931885
train_iter_loss: 0.3550894558429718
train_iter_loss: 0.15797822177410126
train_iter_loss: 0.1753387302160263
train_iter_loss: 0.3576059937477112
train_iter_loss: 0.19496141374111176
train_iter_loss: 0.23715776205062866
train_iter_loss: 0.2033577710390091
train_iter_loss: 0.3021230697631836
train_iter_loss: 0.20892497897148132
train_iter_loss: 0.17139674723148346
train_iter_loss: 0.19779005646705627
train_iter_loss: 0.22791887819766998
train_iter_loss: 0.25410208106040955
train_iter_loss: 0.39036354422569275
train_iter_loss: 0.3644099831581116
train_iter_loss: 0.32292306423187256
train_iter_loss: 0.230255126953125
train_iter_loss: 0.3692837655544281
train_iter_loss: 0.25320589542388916
train_iter_loss: 0.3029012084007263
train_iter_loss: 0.45263564586639404
train_iter_loss: 0.26374417543411255
train_iter_loss: 0.3528352677822113
train_iter_loss: 0.2917836010456085
train_iter_loss: 0.1578119993209839
train_iter_loss: 0.31633564829826355
train_iter_loss: 0.27593666315078735
train_iter_loss: 0.35311993956565857
train_iter_loss: 0.432037889957428
train_iter_loss: 0.21293552219867706
train_iter_loss: 0.29559803009033203
train_iter_loss: 0.3191060721874237
train_iter_loss: 0.3707192838191986
train_iter_loss: 0.18505103886127472
train_iter_loss: 0.3700264096260071
train_iter_loss: 0.34216320514678955
train_iter_loss: 0.3813711404800415
train_iter_loss: 0.27788016200065613
train_iter_loss: 0.39521506428718567
train_iter_loss: 0.17702922224998474
train_iter_loss: 0.2611602246761322
train_iter_loss: 0.16521961987018585
train_iter_loss: 0.36940741539001465
train_iter_loss: 0.2829791009426117
train_iter_loss: 0.3012317419052124
train_iter_loss: 0.3684820234775543
train_iter_loss: 0.407156378030777
train_iter_loss: 0.33170652389526367
train_iter_loss: 0.37583211064338684
train_iter_loss: 0.27502280473709106
train_iter_loss: 0.17302750051021576
train_iter_loss: 0.32582858204841614
train_iter_loss: 0.25150513648986816
train_iter_loss: 0.12323755025863647
train_iter_loss: 0.12790453433990479
train_iter_loss: 0.35256460309028625
train_iter_loss: 0.34650200605392456
train_iter_loss: 0.1961672157049179
train_iter_loss: 0.3047730326652527
train_iter_loss: 0.25908929109573364
train_iter_loss: 0.4352249205112457
train_iter_loss: 0.29155945777893066
train_iter_loss: 0.1717544049024582
train_iter_loss: 0.2628645896911621
train_iter_loss: 0.18660993874073029
train loss :0.2825
---------------------
Validation seg loss: 0.3785751305722614 at epoch 262
epoch =    263/  1000, exp = train
train_iter_loss: 0.45209550857543945
train_iter_loss: 0.20568959414958954
train_iter_loss: 0.27283865213394165
train_iter_loss: 0.35103103518486023
train_iter_loss: 0.18299274146556854
train_iter_loss: 0.3369278013706207
train_iter_loss: 0.24347476661205292
train_iter_loss: 0.2660380005836487
train_iter_loss: 0.43491020798683167
train_iter_loss: 0.22155043482780457
train_iter_loss: 0.2169175148010254
train_iter_loss: 0.1632346361875534
train_iter_loss: 0.6464129686355591
train_iter_loss: 0.32589173316955566
train_iter_loss: 0.23777130246162415
train_iter_loss: 0.2885238528251648
train_iter_loss: 0.10238053649663925
train_iter_loss: 0.39044567942619324
train_iter_loss: 0.33415186405181885
train_iter_loss: 0.3228403329849243
train_iter_loss: 0.7642799615859985
train_iter_loss: 0.28305110335350037
train_iter_loss: 0.32615572214126587
train_iter_loss: 0.2600070834159851
train_iter_loss: 0.31827881932258606
train_iter_loss: 0.26459288597106934
train_iter_loss: 0.3621807396411896
train_iter_loss: 0.338055819272995
train_iter_loss: 0.3229561448097229
train_iter_loss: 0.21830730140209198
train_iter_loss: 0.29561206698417664
train_iter_loss: 0.3192412257194519
train_iter_loss: 0.254630446434021
train_iter_loss: 0.24851948022842407
train_iter_loss: 0.38152292370796204
train_iter_loss: 0.23708491027355194
train_iter_loss: 0.3693915903568268
train_iter_loss: 0.30207690596580505
train_iter_loss: 0.3063909709453583
train_iter_loss: 0.2549881935119629
train_iter_loss: 0.28295180201530457
train_iter_loss: 0.3866329789161682
train_iter_loss: 0.22027097642421722
train_iter_loss: 0.38353946805000305
train_iter_loss: 0.29511553049087524
train_iter_loss: 0.36509305238723755
train_iter_loss: 0.10275420546531677
train_iter_loss: 0.26110732555389404
train_iter_loss: 0.25206097960472107
train_iter_loss: 0.23631751537322998
train_iter_loss: 0.2336544543504715
train_iter_loss: 0.1538258194923401
train_iter_loss: 0.2425927072763443
train_iter_loss: 0.28168147802352905
train_iter_loss: 0.21493420004844666
train_iter_loss: 0.32873937487602234
train_iter_loss: 0.25086522102355957
train_iter_loss: 0.17744764685630798
train_iter_loss: 0.428602933883667
train_iter_loss: 0.27712032198905945
train_iter_loss: 0.20719610154628754
train_iter_loss: 0.1776503026485443
train_iter_loss: 0.3090015649795532
train_iter_loss: 0.3285093903541565
train_iter_loss: 0.21245357394218445
train_iter_loss: 0.3543262183666229
train_iter_loss: 0.18015429377555847
train_iter_loss: 0.28122323751449585
train_iter_loss: 0.34019720554351807
train_iter_loss: 0.5910806059837341
train_iter_loss: 0.14670734107494354
train_iter_loss: 0.2559277415275574
train_iter_loss: 0.19522792100906372
train_iter_loss: 0.22445257008075714
train_iter_loss: 0.14948861300945282
train_iter_loss: 0.1685463935136795
train_iter_loss: 0.25575488805770874
train_iter_loss: 0.1324312537908554
train_iter_loss: 0.38103199005126953
train_iter_loss: 0.3431858718395233
train_iter_loss: 0.20042164623737335
train_iter_loss: 0.40146058797836304
train_iter_loss: 0.32330384850502014
train_iter_loss: 0.25729501247406006
train_iter_loss: 0.26989248394966125
train_iter_loss: 0.18778261542320251
train_iter_loss: 0.2657451033592224
train_iter_loss: 0.45255568623542786
train_iter_loss: 0.281789630651474
train_iter_loss: 0.2993920147418976
train_iter_loss: 0.37051549553871155
train_iter_loss: 0.209634467959404
train_iter_loss: 0.298162043094635
train_iter_loss: 0.26416948437690735
train_iter_loss: 0.2939390242099762
train_iter_loss: 0.3215152621269226
train_iter_loss: 0.27964887022972107
train_iter_loss: 0.2706836462020874
train_iter_loss: 0.2224930077791214
train_iter_loss: 0.3216249346733093
train loss :0.2911
---------------------
Validation seg loss: 0.37583932440446793 at epoch 263
epoch =    264/  1000, exp = train
train_iter_loss: 0.1031254231929779
train_iter_loss: 0.2673680782318115
train_iter_loss: 0.3148752450942993
train_iter_loss: 0.3487255573272705
train_iter_loss: 0.2098631113767624
train_iter_loss: 0.23494862020015717
train_iter_loss: 0.16369594633579254
train_iter_loss: 0.23161062598228455
train_iter_loss: 0.23072490096092224
train_iter_loss: 0.28325316309928894
train_iter_loss: 0.23793238401412964
train_iter_loss: 0.39704713225364685
train_iter_loss: 0.14141876995563507
train_iter_loss: 0.40006840229034424
train_iter_loss: 0.15603871643543243
train_iter_loss: 0.30580049753189087
train_iter_loss: 0.33918482065200806
train_iter_loss: 0.18731780350208282
train_iter_loss: 0.17578308284282684
train_iter_loss: 0.4181893765926361
train_iter_loss: 0.2646564543247223
train_iter_loss: 0.24369463324546814
train_iter_loss: 0.37023594975471497
train_iter_loss: 0.2795717716217041
train_iter_loss: 0.17527933418750763
train_iter_loss: 0.35663992166519165
train_iter_loss: 0.32447630167007446
train_iter_loss: 0.24036860466003418
train_iter_loss: 0.4220036268234253
train_iter_loss: 0.29015499353408813
train_iter_loss: 0.24632267653942108
train_iter_loss: 0.3252163827419281
train_iter_loss: 0.2211814820766449
train_iter_loss: 0.10258007049560547
train_iter_loss: 0.2942422330379486
train_iter_loss: 0.19341936707496643
train_iter_loss: 0.32396048307418823
train_iter_loss: 0.18892841041088104
train_iter_loss: 0.2966172397136688
train_iter_loss: 0.2667483389377594
train_iter_loss: 0.3823848068714142
train_iter_loss: 0.25441795587539673
train_iter_loss: 0.3198857605457306
train_iter_loss: 0.2237255573272705
train_iter_loss: 0.36950263381004333
train_iter_loss: 0.2648627460002899
train_iter_loss: 0.1551918387413025
train_iter_loss: 0.19872400164604187
train_iter_loss: 0.24130502343177795
train_iter_loss: 0.3583395779132843
train_iter_loss: 0.3385836184024811
train_iter_loss: 0.3111167252063751
train_iter_loss: 0.13714760541915894
train_iter_loss: 0.17943325638771057
train_iter_loss: 0.2629237174987793
train_iter_loss: 0.3120279610157013
train_iter_loss: 0.28408515453338623
train_iter_loss: 0.2185884416103363
train_iter_loss: 0.4242015779018402
train_iter_loss: 0.20736439526081085
train_iter_loss: 0.2917259931564331
train_iter_loss: 0.3640594482421875
train_iter_loss: 0.23751434683799744
train_iter_loss: 0.2416231483221054
train_iter_loss: 0.35379016399383545
train_iter_loss: 0.24793288111686707
train_iter_loss: 0.20377780497074127
train_iter_loss: 0.25328701734542847
train_iter_loss: 0.4475742280483246
train_iter_loss: 0.27583348751068115
train_iter_loss: 0.23290328681468964
train_iter_loss: 0.22902123630046844
train_iter_loss: 0.30616480112075806
train_iter_loss: 0.19467365741729736
train_iter_loss: 0.3639720678329468
train_iter_loss: 0.40015432238578796
train_iter_loss: 0.2662235498428345
train_iter_loss: 0.3733759820461273
train_iter_loss: 0.29623737931251526
train_iter_loss: 0.1991221159696579
train_iter_loss: 0.2626962661743164
train_iter_loss: 0.287909597158432
train_iter_loss: 0.2631799876689911
train_iter_loss: 0.538636326789856
train_iter_loss: 0.34340232610702515
train_iter_loss: 0.5355852246284485
train_iter_loss: 0.29638656973838806
train_iter_loss: 0.42523127794265747
train_iter_loss: 0.40110060572624207
train_iter_loss: 0.3005610704421997
train_iter_loss: 0.16638238728046417
train_iter_loss: 0.20484140515327454
train_iter_loss: 0.25038379430770874
train_iter_loss: 0.40993261337280273
train_iter_loss: 0.21297642588615417
train_iter_loss: 0.29385706782341003
train_iter_loss: 0.19152158498764038
train_iter_loss: 0.26671159267425537
train_iter_loss: 0.16038382053375244
train_iter_loss: 0.49423861503601074
train loss :0.2839
---------------------
Validation seg loss: 0.3870092459585307 at epoch 264
epoch =    265/  1000, exp = train
train_iter_loss: 0.3639678657054901
train_iter_loss: 0.4210706651210785
train_iter_loss: 0.34278151392936707
train_iter_loss: 0.18000943958759308
train_iter_loss: 0.25446000695228577
train_iter_loss: 0.24729494750499725
train_iter_loss: 0.34597864747047424
train_iter_loss: 0.22884584963321686
train_iter_loss: 0.47024962306022644
train_iter_loss: 0.35394272208213806
train_iter_loss: 0.3241276443004608
train_iter_loss: 0.38368919491767883
train_iter_loss: 0.28829044103622437
train_iter_loss: 0.20732003450393677
train_iter_loss: 0.30627503991127014
train_iter_loss: 0.2958889603614807
train_iter_loss: 0.31187009811401367
train_iter_loss: 0.20378020405769348
train_iter_loss: 0.35664644837379456
train_iter_loss: 0.2680874764919281
train_iter_loss: 0.3561435639858246
train_iter_loss: 0.31055158376693726
train_iter_loss: 0.24591174721717834
train_iter_loss: 0.3234342038631439
train_iter_loss: 0.35608917474746704
train_iter_loss: 0.2883521020412445
train_iter_loss: 0.3485269546508789
train_iter_loss: 0.34255144000053406
train_iter_loss: 0.22661852836608887
train_iter_loss: 0.20343106985092163
train_iter_loss: 0.19761314988136292
train_iter_loss: 0.30982640385627747
train_iter_loss: 0.36415669322013855
train_iter_loss: 0.21833735704421997
train_iter_loss: 0.2830949127674103
train_iter_loss: 0.24378055334091187
train_iter_loss: 0.3086583614349365
train_iter_loss: 0.2721697986125946
train_iter_loss: 0.4816778898239136
train_iter_loss: 0.3176533281803131
train_iter_loss: 0.25498896837234497
train_iter_loss: 0.2445274442434311
train_iter_loss: 0.4089260995388031
train_iter_loss: 0.23859335482120514
train_iter_loss: 0.374192476272583
train_iter_loss: 0.3931880295276642
train_iter_loss: 0.3668782711029053
train_iter_loss: 0.2924175262451172
train_iter_loss: 0.2694072723388672
train_iter_loss: 0.49378934502601624
train_iter_loss: 0.1480981409549713
train_iter_loss: 0.23205938935279846
train_iter_loss: 0.19066880643367767
train_iter_loss: 0.3309330940246582
train_iter_loss: 0.20160727202892303
train_iter_loss: 0.2814144790172577
train_iter_loss: 0.2623775601387024
train_iter_loss: 0.26264306902885437
train_iter_loss: 0.290455162525177
train_iter_loss: 0.3022177815437317
train_iter_loss: 0.17720353603363037
train_iter_loss: 0.30164262652397156
train_iter_loss: 0.28005409240722656
train_iter_loss: 0.4330281615257263
train_iter_loss: 0.29266616702079773
train_iter_loss: 0.11313892900943756
train_iter_loss: 0.32447686791419983
train_iter_loss: 0.3417617976665497
train_iter_loss: 0.24593941867351532
train_iter_loss: 0.24720178544521332
train_iter_loss: 0.22528766095638275
train_iter_loss: 0.21011647582054138
train_iter_loss: 0.21888679265975952
train_iter_loss: 0.1812572181224823
train_iter_loss: 0.14164160192012787
train_iter_loss: 0.24427101016044617
train_iter_loss: 0.15717273950576782
train_iter_loss: 0.2939649224281311
train_iter_loss: 0.11928489804267883
train_iter_loss: 0.15676210820674896
train_iter_loss: 0.2816121280193329
train_iter_loss: 0.21157804131507874
train_iter_loss: 0.3373803496360779
train_iter_loss: 0.24189461767673492
train_iter_loss: 0.4836951792240143
train_iter_loss: 0.3493592441082001
train_iter_loss: 0.3515584468841553
train_iter_loss: 0.34856361150741577
train_iter_loss: 0.3000223934650421
train_iter_loss: 0.3065353035926819
train_iter_loss: 0.30170661211013794
train_iter_loss: 0.34416282176971436
train_iter_loss: 0.2685975730419159
train_iter_loss: 0.2453262060880661
train_iter_loss: 0.38513433933258057
train_iter_loss: 0.21489249169826508
train_iter_loss: 0.19787068665027618
train_iter_loss: 0.21772366762161255
train_iter_loss: 0.35106682777404785
train_iter_loss: 0.2540322244167328
train loss :0.2897
---------------------
Validation seg loss: 0.3773403751997734 at epoch 265
epoch =    266/  1000, exp = train
train_iter_loss: 0.3162737190723419
train_iter_loss: 0.16771158576011658
train_iter_loss: 0.27427518367767334
train_iter_loss: 0.3109321892261505
train_iter_loss: 0.20891956984996796
train_iter_loss: 0.22003203630447388
train_iter_loss: 0.3123708665370941
train_iter_loss: 0.33240917325019836
train_iter_loss: 0.24615217745304108
train_iter_loss: 0.2008201628923416
train_iter_loss: 0.25262269377708435
train_iter_loss: 0.27862343192100525
train_iter_loss: 0.27910056710243225
train_iter_loss: 0.1807607263326645
train_iter_loss: 0.31001126766204834
train_iter_loss: 0.3474467694759369
train_iter_loss: 0.3366898000240326
train_iter_loss: 0.4318196773529053
train_iter_loss: 0.25772160291671753
train_iter_loss: 0.19775676727294922
train_iter_loss: 0.21412093937397003
train_iter_loss: 0.24372759461402893
train_iter_loss: 0.1223626658320427
train_iter_loss: 0.31754690408706665
train_iter_loss: 0.10894259065389633
train_iter_loss: 0.3136245608329773
train_iter_loss: 0.21242308616638184
train_iter_loss: 0.2538096010684967
train_iter_loss: 0.4381764531135559
train_iter_loss: 0.37719205021858215
train_iter_loss: 0.3166179358959198
train_iter_loss: 0.2631824314594269
train_iter_loss: 0.4009571373462677
train_iter_loss: 0.1844639927148819
train_iter_loss: 0.15446750819683075
train_iter_loss: 0.16674692928791046
train_iter_loss: 0.24265673756599426
train_iter_loss: 0.2883286476135254
train_iter_loss: 0.10251857340335846
train_iter_loss: 0.3113783597946167
train_iter_loss: 0.47796785831451416
train_iter_loss: 0.16009895503520966
train_iter_loss: 0.2561737596988678
train_iter_loss: 0.32863137125968933
train_iter_loss: 0.24848341941833496
train_iter_loss: 0.36452051997184753
train_iter_loss: 0.23956459760665894
train_iter_loss: 0.23090432584285736
train_iter_loss: 0.2735012173652649
train_iter_loss: 0.19519531726837158
train_iter_loss: 0.2249213010072708
train_iter_loss: 0.36330193281173706
train_iter_loss: 0.27980324625968933
train_iter_loss: 0.21615703403949738
train_iter_loss: 0.26466265320777893
train_iter_loss: 0.21986714005470276
train_iter_loss: 0.3816574811935425
train_iter_loss: 0.1821700781583786
train_iter_loss: 0.21904848515987396
train_iter_loss: 0.23714038729667664
train_iter_loss: 0.20255552232265472
train_iter_loss: 0.3077715039253235
train_iter_loss: 0.32820916175842285
train_iter_loss: 0.2621622085571289
train_iter_loss: 0.34162598848342896
train_iter_loss: 0.08765624463558197
train_iter_loss: 0.33499467372894287
train_iter_loss: 0.33109521865844727
train_iter_loss: 0.38903021812438965
train_iter_loss: 0.2145317643880844
train_iter_loss: 0.301496684551239
train_iter_loss: 0.27625757455825806
train_iter_loss: 0.27580612897872925
train_iter_loss: 0.1700710952281952
train_iter_loss: 0.40205997228622437
train_iter_loss: 0.2578851580619812
train_iter_loss: 0.27572736144065857
train_iter_loss: 0.14419597387313843
train_iter_loss: 0.29809001088142395
train_iter_loss: 0.24281522631645203
train_iter_loss: 0.10989169031381607
train_iter_loss: 0.1991221159696579
train_iter_loss: 0.34649133682250977
train_iter_loss: 0.20875704288482666
train_iter_loss: 0.2830725312232971
train_iter_loss: 0.42978814244270325
train_iter_loss: 0.3966965675354004
train_iter_loss: 0.24106724560260773
train_iter_loss: 0.302216500043869
train_iter_loss: 0.23241470754146576
train_iter_loss: 0.25575676560401917
train_iter_loss: 0.39317217469215393
train_iter_loss: 0.2544876039028168
train_iter_loss: 0.3593628704547882
train_iter_loss: 0.26894181966781616
train_iter_loss: 0.2347192019224167
train_iter_loss: 0.5239460468292236
train_iter_loss: 0.15482544898986816
train_iter_loss: 0.579372763633728
train_iter_loss: 0.4130137264728546
train loss :0.2778
---------------------
Validation seg loss: 0.3874748615583159 at epoch 266
epoch =    267/  1000, exp = train
train_iter_loss: 0.32237544655799866
train_iter_loss: 0.29898104071617126
train_iter_loss: 0.15828993916511536
train_iter_loss: 0.1934060901403427
train_iter_loss: 0.1719491183757782
train_iter_loss: 0.20000667870044708
train_iter_loss: 0.389538049697876
train_iter_loss: 0.3015281856060028
train_iter_loss: 0.35026735067367554
train_iter_loss: 0.27911376953125
train_iter_loss: 0.3300893306732178
train_iter_loss: 0.17393462359905243
train_iter_loss: 0.4491958022117615
train_iter_loss: 0.23258857429027557
train_iter_loss: 0.26574575901031494
train_iter_loss: 0.3511398434638977
train_iter_loss: 0.2505033016204834
train_iter_loss: 0.37286490201950073
train_iter_loss: 0.25141820311546326
train_iter_loss: 0.29727572202682495
train_iter_loss: 0.0716107189655304
train_iter_loss: 0.3453018367290497
train_iter_loss: 0.318164199590683
train_iter_loss: 0.26187413930892944
train_iter_loss: 0.3089141547679901
train_iter_loss: 0.1876460462808609
train_iter_loss: 0.19253408908843994
train_iter_loss: 0.3051852881908417
train_iter_loss: 0.10894612222909927
train_iter_loss: 0.2856471538543701
train_iter_loss: 0.4524409770965576
train_iter_loss: 0.13381536304950714
train_iter_loss: 0.3391048014163971
train_iter_loss: 0.37525951862335205
train_iter_loss: 0.06395799666643143
train_iter_loss: 0.21773427724838257
train_iter_loss: 0.3626865744590759
train_iter_loss: 0.3084889352321625
train_iter_loss: 0.4510233998298645
train_iter_loss: 0.2838889956474304
train_iter_loss: 0.08563313633203506
train_iter_loss: 0.3948194682598114
train_iter_loss: 0.44152140617370605
train_iter_loss: 0.18938922882080078
train_iter_loss: 0.3638532757759094
train_iter_loss: 0.33210667967796326
train_iter_loss: 0.27656134963035583
train_iter_loss: 0.3308297395706177
train_iter_loss: 0.1782783567905426
train_iter_loss: 0.3255266845226288
train_iter_loss: 0.39394015073776245
train_iter_loss: 0.3282938003540039
train_iter_loss: 0.3552917540073395
train_iter_loss: 0.29462265968322754
train_iter_loss: 0.20750242471694946
train_iter_loss: 0.3796131908893585
train_iter_loss: 0.32831627130508423
train_iter_loss: 0.1053033396601677
train_iter_loss: 0.34956836700439453
train_iter_loss: 0.21633704006671906
train_iter_loss: 0.2765249013900757
train_iter_loss: 0.3146900534629822
train_iter_loss: 0.4035075008869171
train_iter_loss: 0.24617306888103485
train_iter_loss: 0.20557112991809845
train_iter_loss: 0.15025396645069122
train_iter_loss: 0.25936150550842285
train_iter_loss: 0.314802885055542
train_iter_loss: 0.20127244293689728
train_iter_loss: 0.22696109116077423
train_iter_loss: 0.17320969700813293
train_iter_loss: 0.3089924156665802
train_iter_loss: 0.3767768144607544
train_iter_loss: 0.30770009756088257
train_iter_loss: 0.19828391075134277
train_iter_loss: 0.3244282007217407
train_iter_loss: 0.10513558983802795
train_iter_loss: 0.20507316291332245
train_iter_loss: 0.40842634439468384
train_iter_loss: 0.375715434551239
train_iter_loss: 0.3247998356819153
train_iter_loss: 0.3812759220600128
train_iter_loss: 0.401886522769928
train_iter_loss: 0.18082161247730255
train_iter_loss: 0.33878880739212036
train_iter_loss: 0.34195274114608765
train_iter_loss: 0.3886669874191284
train_iter_loss: 0.2844102680683136
train_iter_loss: 0.25187814235687256
train_iter_loss: 0.5055854916572571
train_iter_loss: 0.2644372880458832
train_iter_loss: 0.28704050183296204
train_iter_loss: 0.27733349800109863
train_iter_loss: 0.3199060261249542
train_iter_loss: 0.17107798159122467
train_iter_loss: 0.39214226603507996
train_iter_loss: 0.305843710899353
train_iter_loss: 0.31688186526298523
train_iter_loss: 0.2067076861858368
train_iter_loss: 0.13772588968276978
train loss :0.2863
---------------------
Validation seg loss: 0.4190534178940755 at epoch 267
epoch =    268/  1000, exp = train
train_iter_loss: 0.21000854671001434
train_iter_loss: 0.26307350397109985
train_iter_loss: 0.33232662081718445
train_iter_loss: 0.19320455193519592
train_iter_loss: 0.24961718916893005
train_iter_loss: 0.2918374240398407
train_iter_loss: 0.1515447199344635
train_iter_loss: 0.2389480173587799
train_iter_loss: 0.19062815606594086
train_iter_loss: 0.3524179458618164
train_iter_loss: 0.26013821363449097
train_iter_loss: 0.2674666941165924
train_iter_loss: 0.5395399928092957
train_iter_loss: 0.3164548873901367
train_iter_loss: 0.3363175392150879
train_iter_loss: 0.3553457260131836
train_iter_loss: 0.3321778476238251
train_iter_loss: 0.3807999789714813
train_iter_loss: 0.23320826888084412
train_iter_loss: 0.28795385360717773
train_iter_loss: 0.09025665372610092
train_iter_loss: 0.13637405633926392
train_iter_loss: 0.32392337918281555
train_iter_loss: 0.3465934991836548
train_iter_loss: 0.21631261706352234
train_iter_loss: 0.20744217932224274
train_iter_loss: 0.2577558159828186
train_iter_loss: 0.25576645135879517
train_iter_loss: 0.17617516219615936
train_iter_loss: 0.2727103531360626
train_iter_loss: 0.24627390503883362
train_iter_loss: 0.3961153030395508
train_iter_loss: 0.2741905152797699
train_iter_loss: 0.2625232934951782
train_iter_loss: 0.20480255782604218
train_iter_loss: 0.43551304936408997
train_iter_loss: 0.34243300557136536
train_iter_loss: 0.3113381862640381
train_iter_loss: 0.28623199462890625
train_iter_loss: 0.3288571536540985
train_iter_loss: 0.33695337176322937
train_iter_loss: 0.29313182830810547
train_iter_loss: 0.3743475377559662
train_iter_loss: 0.3395247161388397
train_iter_loss: 0.3346883952617645
train_iter_loss: 0.3531409204006195
train_iter_loss: 0.33860403299331665
train_iter_loss: 0.23228789865970612
train_iter_loss: 0.2835750877857208
train_iter_loss: 0.4136963188648224
train_iter_loss: 0.33136075735092163
train_iter_loss: 0.13358312845230103
train_iter_loss: 0.4607122242450714
train_iter_loss: 0.24200405180454254
train_iter_loss: 0.27529099583625793
train_iter_loss: 0.3630833327770233
train_iter_loss: 0.2117578536272049
train_iter_loss: 0.23007264733314514
train_iter_loss: 0.3772662580013275
train_iter_loss: 0.29180511832237244
train_iter_loss: 0.3638642132282257
train_iter_loss: 0.35013315081596375
train_iter_loss: 0.23317959904670715
train_iter_loss: 0.26329317688941956
train_iter_loss: 0.39740410447120667
train_iter_loss: 0.37690263986587524
train_iter_loss: 0.39617544412612915
train_iter_loss: 0.15918196737766266
train_iter_loss: 0.1515161544084549
train_iter_loss: 0.2532140612602234
train_iter_loss: 0.31266722083091736
train_iter_loss: 0.22724956274032593
train_iter_loss: 0.20279182493686676
train_iter_loss: 0.3119858503341675
train_iter_loss: 0.42241808772087097
train_iter_loss: 0.2657160460948944
train_iter_loss: 0.2991882860660553
train_iter_loss: 0.2217070758342743
train_iter_loss: 0.22078251838684082
train_iter_loss: 0.2675655484199524
train_iter_loss: 0.23122195899486542
train_iter_loss: 0.32045778632164
train_iter_loss: 0.23611772060394287
train_iter_loss: 0.3761085867881775
train_iter_loss: 0.38283494114875793
train_iter_loss: 0.30597078800201416
train_iter_loss: 0.13257600367069244
train_iter_loss: 0.22553230822086334
train_iter_loss: 0.24155400693416595
train_iter_loss: 0.2718826234340668
train_iter_loss: 0.17577138543128967
train_iter_loss: 0.3631087839603424
train_iter_loss: 0.2520388066768646
train_iter_loss: 0.15206192433834076
train_iter_loss: 0.18530429899692535
train_iter_loss: 0.20815908908843994
train_iter_loss: 0.22729136049747467
train_iter_loss: 0.36656373739242554
train_iter_loss: 0.24769951403141022
train_iter_loss: 0.2543931007385254
train loss :0.2851
---------------------
Validation seg loss: 0.3746124878877176 at epoch 268
epoch =    269/  1000, exp = train
train_iter_loss: 0.2047376036643982
train_iter_loss: 0.2955799400806427
train_iter_loss: 0.3318980038166046
train_iter_loss: 0.29010552167892456
train_iter_loss: 0.31828057765960693
train_iter_loss: 0.29226845502853394
train_iter_loss: 0.27524036169052124
train_iter_loss: 0.3942183554172516
train_iter_loss: 0.2803928852081299
train_iter_loss: 0.20591284334659576
train_iter_loss: 0.29919353127479553
train_iter_loss: 0.297904908657074
train_iter_loss: 0.19282494485378265
train_iter_loss: 0.3306283950805664
train_iter_loss: 0.26699113845825195
train_iter_loss: 0.22726568579673767
train_iter_loss: 0.2713377773761749
train_iter_loss: 0.2520683705806732
train_iter_loss: 0.2693805396556854
train_iter_loss: 0.43269526958465576
train_iter_loss: 0.075352743268013
train_iter_loss: 0.42236876487731934
train_iter_loss: 0.22175352275371552
train_iter_loss: 0.19864550232887268
train_iter_loss: 0.2601475417613983
train_iter_loss: 0.2026088833808899
train_iter_loss: 0.23592813313007355
train_iter_loss: 0.29030340909957886
train_iter_loss: 0.2498309463262558
train_iter_loss: 0.3051094710826874
train_iter_loss: 0.4657512605190277
train_iter_loss: 0.3085363507270813
train_iter_loss: 0.20540866255760193
train_iter_loss: 0.3398430645465851
train_iter_loss: 0.17245365679264069
train_iter_loss: 0.1905832141637802
train_iter_loss: 0.09590139985084534
train_iter_loss: 0.3914460837841034
train_iter_loss: 0.4068417251110077
train_iter_loss: 0.3341982364654541
train_iter_loss: 0.4343271553516388
train_iter_loss: 0.44449883699417114
train_iter_loss: 0.2454248070716858
train_iter_loss: 0.3198390007019043
train_iter_loss: 0.251620888710022
train_iter_loss: 0.3915219008922577
train_iter_loss: 0.3039040267467499
train_iter_loss: 0.29317334294319153
train_iter_loss: 0.30948930978775024
train_iter_loss: 0.35214102268218994
train_iter_loss: 0.3940872848033905
train_iter_loss: 0.33866581320762634
train_iter_loss: 0.1929088830947876
train_iter_loss: 0.27289995551109314
train_iter_loss: 0.14266034960746765
train_iter_loss: 0.1875217705965042
train_iter_loss: 0.3180539309978485
train_iter_loss: 0.32448670268058777
train_iter_loss: 0.5107686519622803
train_iter_loss: 0.2893684208393097
train_iter_loss: 0.21058209240436554
train_iter_loss: 0.1436271369457245
train_iter_loss: 0.22005833685398102
train_iter_loss: 0.2490096241235733
train_iter_loss: 0.24207720160484314
train_iter_loss: 0.23508068919181824
train_iter_loss: 0.3399449288845062
train_iter_loss: 0.24808992445468903
train_iter_loss: 0.28110581636428833
train_iter_loss: 0.4164457619190216
train_iter_loss: 0.3624860346317291
train_iter_loss: 0.41553911566734314
train_iter_loss: 0.33141258358955383
train_iter_loss: 0.17138239741325378
train_iter_loss: 0.08808532357215881
train_iter_loss: 0.24079935252666473
train_iter_loss: 0.05322650820016861
train_iter_loss: 0.3783067762851715
train_iter_loss: 0.296425461769104
train_iter_loss: 0.2441856563091278
train_iter_loss: 0.22022587060928345
train_iter_loss: 0.38327816128730774
train_iter_loss: 0.4394630491733551
train_iter_loss: 0.3833026885986328
train_iter_loss: 0.36492884159088135
train_iter_loss: 0.3900790810585022
train_iter_loss: 0.3532473146915436
train_iter_loss: 0.28684210777282715
train_iter_loss: 0.3754338026046753
train_iter_loss: 0.27259042859077454
train_iter_loss: 0.3433467149734497
train_iter_loss: 0.24177353084087372
train_iter_loss: 0.2015906274318695
train_iter_loss: 0.13662520051002502
train_iter_loss: 0.26607051491737366
train_iter_loss: 0.32910144329071045
train_iter_loss: 0.32371848821640015
train_iter_loss: 0.3526237905025482
train_iter_loss: 0.3302552103996277
train_iter_loss: 0.19834764301776886
train loss :0.2907
---------------------
Validation seg loss: 0.3684324589470085 at epoch 269
epoch =    270/  1000, exp = train
train_iter_loss: 0.27318546175956726
train_iter_loss: 0.2697051465511322
train_iter_loss: 0.32208147644996643
train_iter_loss: 0.18531496822834015
train_iter_loss: 0.2893945276737213
train_iter_loss: 0.3494645059108734
train_iter_loss: 0.21593549847602844
train_iter_loss: 0.3669356405735016
train_iter_loss: 0.4061923027038574
train_iter_loss: 0.07083428651094437
train_iter_loss: 0.3490045666694641
train_iter_loss: 0.34186798334121704
train_iter_loss: 0.2721097469329834
train_iter_loss: 0.1798238456249237
train_iter_loss: 0.30665066838264465
train_iter_loss: 0.3471660614013672
train_iter_loss: 0.3447636663913727
train_iter_loss: 0.2952144742012024
train_iter_loss: 0.24762344360351562
train_iter_loss: 0.16262313723564148
train_iter_loss: 0.17580805718898773
train_iter_loss: 0.3000386953353882
train_iter_loss: 0.24007342755794525
train_iter_loss: 0.19891886413097382
train_iter_loss: 0.2360268384218216
train_iter_loss: 0.18379920721054077
train_iter_loss: 0.31477439403533936
train_iter_loss: 0.28060558438301086
train_iter_loss: 0.2729313373565674
train_iter_loss: 0.36210495233535767
train_iter_loss: 0.23814961314201355
train_iter_loss: 0.41848987340927124
train_iter_loss: 0.13118483126163483
train_iter_loss: 0.33940792083740234
train_iter_loss: 0.32639092206954956
train_iter_loss: 0.26647961139678955
train_iter_loss: 0.26556530594825745
train_iter_loss: 0.14554829895496368
train_iter_loss: 0.3715551495552063
train_iter_loss: 0.20163419842720032
train_iter_loss: 0.19330854713916779
train_iter_loss: 0.2545540928840637
train_iter_loss: 0.3639516532421112
train_iter_loss: 0.3972170650959015
train_iter_loss: 0.2331719994544983
train_iter_loss: 0.1758280098438263
train_iter_loss: 0.24639272689819336
train_iter_loss: 0.2073228657245636
train_iter_loss: 0.21385744214057922
train_iter_loss: 0.28632208704948425
train_iter_loss: 0.11791453510522842
train_iter_loss: 0.21266020834445953
train_iter_loss: 0.35639384388923645
train_iter_loss: 0.1575823575258255
train_iter_loss: 0.18566620349884033
train_iter_loss: 0.20857703685760498
train_iter_loss: 0.1961609125137329
train_iter_loss: 0.39351320266723633
train_iter_loss: 0.4671899974346161
train_iter_loss: 0.22030681371688843
train_iter_loss: 0.3110247254371643
train_iter_loss: 0.3259872496128082
train_iter_loss: 0.30091214179992676
train_iter_loss: 0.18743792176246643
train_iter_loss: 0.18328921496868134
train_iter_loss: 0.3820917308330536
train_iter_loss: 0.26538771390914917
train_iter_loss: 0.3105720579624176
train_iter_loss: 0.32073092460632324
train_iter_loss: 0.4424263536930084
train_iter_loss: 0.2292451411485672
train_iter_loss: 0.4478438198566437
train_iter_loss: 0.2937869131565094
train_iter_loss: 0.3812405467033386
train_iter_loss: 0.1790095865726471
train_iter_loss: 0.25917524099349976
train_iter_loss: 0.34139886498451233
train_iter_loss: 0.40162283182144165
train_iter_loss: 0.6866623759269714
train_iter_loss: 0.24621888995170593
train_iter_loss: 0.3830365538597107
train_iter_loss: 0.2484922707080841
train_iter_loss: 0.11903597414493561
train_iter_loss: 0.1900501251220703
train_iter_loss: 0.3245994448661804
train_iter_loss: 0.22661451995372772
train_iter_loss: 0.316554993391037
train_iter_loss: 0.3756583631038666
train_iter_loss: 0.3961498439311981
train_iter_loss: 0.30378755927085876
train_iter_loss: 0.28240710496902466
train_iter_loss: 0.2885134220123291
train_iter_loss: 0.23418830335140228
train_iter_loss: 0.2959005534648895
train_iter_loss: 0.20410306751728058
train_iter_loss: 0.20485270023345947
train_iter_loss: 0.1820620894432068
train_iter_loss: 0.32493361830711365
train_iter_loss: 0.39008477330207825
train_iter_loss: 0.32802248001098633
train loss :0.2836
---------------------
Validation seg loss: 0.35927936472525857 at epoch 270
epoch =    271/  1000, exp = train
train_iter_loss: 0.5478866696357727
train_iter_loss: 0.17754770815372467
train_iter_loss: 0.2897774875164032
train_iter_loss: 0.2889659106731415
train_iter_loss: 0.30962902307510376
train_iter_loss: 0.18850667774677277
train_iter_loss: 0.641405463218689
train_iter_loss: 0.20989346504211426
train_iter_loss: 0.26244691014289856
train_iter_loss: 0.21585798263549805
train_iter_loss: 0.1705571860074997
train_iter_loss: 0.2411588430404663
train_iter_loss: 0.2378150224685669
train_iter_loss: 0.281116247177124
train_iter_loss: 0.18965020775794983
train_iter_loss: 0.4002474844455719
train_iter_loss: 0.34424522519111633
train_iter_loss: 0.267798513174057
train_iter_loss: 0.3736515939235687
train_iter_loss: 0.16297908127307892
train_iter_loss: 0.3070082664489746
train_iter_loss: 0.23635375499725342
train_iter_loss: 0.26074185967445374
train_iter_loss: 0.3135111331939697
train_iter_loss: 0.15547873079776764
train_iter_loss: 0.2510612905025482
train_iter_loss: 0.21367301046848297
train_iter_loss: 0.31241148710250854
train_iter_loss: 0.3339492082595825
train_iter_loss: 0.27705928683280945
train_iter_loss: 0.22568923234939575
train_iter_loss: 0.39504188299179077
train_iter_loss: 0.2509006857872009
train_iter_loss: 0.34459999203681946
train_iter_loss: 0.2616884410381317
train_iter_loss: 0.26555874943733215
train_iter_loss: 0.2686213254928589
train_iter_loss: 0.20077061653137207
train_iter_loss: 0.3029235005378723
train_iter_loss: 0.26894599199295044
train_iter_loss: 0.2546406090259552
train_iter_loss: 0.26580536365509033
train_iter_loss: 0.3388393819332123
train_iter_loss: 0.2745594084262848
train_iter_loss: 0.3152046203613281
train_iter_loss: 0.28581711649894714
train_iter_loss: 0.40386369824409485
train_iter_loss: 0.18384243547916412
train_iter_loss: 0.3208918571472168
train_iter_loss: 0.19831953942775726
train_iter_loss: 0.34218186140060425
train_iter_loss: 0.4280693233013153
train_iter_loss: 0.3142372667789459
train_iter_loss: 0.32454341650009155
train_iter_loss: 0.46330204606056213
train_iter_loss: 0.30652403831481934
train_iter_loss: 0.32772281765937805
train_iter_loss: 0.37822073698043823
train_iter_loss: 0.23607470095157623
train_iter_loss: 0.2381058931350708
train_iter_loss: 0.2102370709180832
train_iter_loss: 0.2505679428577423
train_iter_loss: 0.2645009160041809
train_iter_loss: 0.19934481382369995
train_iter_loss: 0.29687103629112244
train_iter_loss: 0.2841583788394928
train_iter_loss: 0.17434099316596985
train_iter_loss: 0.3838425874710083
train_iter_loss: 0.30284154415130615
train_iter_loss: 0.24855414032936096
train_iter_loss: 0.20173165202140808
train_iter_loss: 0.28110384941101074
train_iter_loss: 0.28843390941619873
train_iter_loss: 0.3408460319042206
train_iter_loss: 0.2706412374973297
train_iter_loss: 0.11516109853982925
train_iter_loss: 0.13429206609725952
train_iter_loss: 0.30408620834350586
train_iter_loss: 0.19541621208190918
train_iter_loss: 0.3053494989871979
train_iter_loss: 0.2885269820690155
train_iter_loss: 0.36920660734176636
train_iter_loss: 0.2853008508682251
train_iter_loss: 0.28509870171546936
train_iter_loss: 0.273967981338501
train_iter_loss: 0.32708731293678284
train_iter_loss: 0.2746316194534302
train_iter_loss: 0.4468620717525482
train_iter_loss: 0.3026665449142456
train_iter_loss: 0.23305749893188477
train_iter_loss: 0.20629791915416718
train_iter_loss: 0.46579334139823914
train_iter_loss: 0.2873086929321289
train_iter_loss: 0.14337222278118134
train_iter_loss: 0.3283742666244507
train_iter_loss: 0.2735232412815094
train_iter_loss: 0.3751886487007141
train_iter_loss: 0.21748435497283936
train_iter_loss: 0.405629962682724
train_iter_loss: 0.2834749221801758
train loss :0.2886
---------------------
Validation seg loss: 0.3667692166208377 at epoch 271
epoch =    272/  1000, exp = train
train_iter_loss: 0.35163429379463196
train_iter_loss: 0.25050589442253113
train_iter_loss: 0.08372442424297333
train_iter_loss: 0.29138830304145813
train_iter_loss: 0.2338503897190094
train_iter_loss: 0.18664248287677765
train_iter_loss: 0.32131102681159973
train_iter_loss: 0.2432394027709961
train_iter_loss: 0.36372849345207214
train_iter_loss: 0.2850666642189026
train_iter_loss: 0.27827396988868713
train_iter_loss: 0.2714264392852783
train_iter_loss: 0.36371275782585144
train_iter_loss: 0.2669762074947357
train_iter_loss: 0.28048381209373474
train_iter_loss: 0.3235643208026886
train_iter_loss: 0.20358163118362427
train_iter_loss: 0.26086246967315674
train_iter_loss: 0.23187480866909027
train_iter_loss: 0.08873722702264786
train_iter_loss: 0.37592795491218567
train_iter_loss: 0.3581903278827667
train_iter_loss: 0.3373311161994934
train_iter_loss: 0.3209323287010193
train_iter_loss: 0.11974455416202545
train_iter_loss: 0.2724030911922455
train_iter_loss: 0.26668408513069153
train_iter_loss: 0.39278528094291687
train_iter_loss: 0.21961449086666107
train_iter_loss: 0.24651511013507843
train_iter_loss: 0.23576174676418304
train_iter_loss: 0.3094519376754761
train_iter_loss: 0.2309696227312088
train_iter_loss: 0.276009202003479
train_iter_loss: 0.42222175002098083
train_iter_loss: 0.3623238205909729
train_iter_loss: 0.2541268765926361
train_iter_loss: 0.15219348669052124
train_iter_loss: 0.21125105023384094
train_iter_loss: 0.33732450008392334
train_iter_loss: 0.3441046476364136
train_iter_loss: 0.11162468791007996
train_iter_loss: 0.211367666721344
train_iter_loss: 0.22604361176490784
train_iter_loss: 0.1996224969625473
train_iter_loss: 0.47183382511138916
train_iter_loss: 0.38176313042640686
train_iter_loss: 0.20471499860286713
train_iter_loss: 0.3213909864425659
train_iter_loss: 0.30768632888793945
train_iter_loss: 0.31045278906822205
train_iter_loss: 0.26665475964546204
train_iter_loss: 0.22243191301822662
train_iter_loss: 0.32030341029167175
train_iter_loss: 0.11692644655704498
train_iter_loss: 0.33891984820365906
train_iter_loss: 0.23476997017860413
train_iter_loss: 0.2535497546195984
train_iter_loss: 0.32962295413017273
train_iter_loss: 0.22281721234321594
train_iter_loss: 0.2356700301170349
train_iter_loss: 0.4330432116985321
train_iter_loss: 0.35632455348968506
train_iter_loss: 0.21092292666435242
train_iter_loss: 0.28034335374832153
train_iter_loss: 0.3207763731479645
train_iter_loss: 0.3833635151386261
train_iter_loss: 0.27682727575302124
train_iter_loss: 0.3438187539577484
train_iter_loss: 0.2966029644012451
train_iter_loss: 0.24859201908111572
train_iter_loss: 0.218516007065773
train_iter_loss: 0.4381416440010071
train_iter_loss: 0.18596702814102173
train_iter_loss: 0.2636844217777252
train_iter_loss: 0.23482944071292877
train_iter_loss: 0.28031066060066223
train_iter_loss: 0.2328350991010666
train_iter_loss: 0.25302499532699585
train_iter_loss: 0.3258095383644104
train_iter_loss: 0.40884238481521606
train_iter_loss: 0.2517135739326477
train_iter_loss: 0.24638226628303528
train_iter_loss: 0.22293145954608917
train_iter_loss: 0.2234184741973877
train_iter_loss: 0.3907822370529175
train_iter_loss: 0.3623387813568115
train_iter_loss: 0.22729095816612244
train_iter_loss: 0.21844467520713806
train_iter_loss: 0.11685433983802795
train_iter_loss: 0.35917776823043823
train_iter_loss: 0.28725704550743103
train_iter_loss: 0.2727384865283966
train_iter_loss: 0.279363751411438
train_iter_loss: 0.28689512610435486
train_iter_loss: 0.19945865869522095
train_iter_loss: 0.36981937289237976
train_iter_loss: 0.24143269658088684
train_iter_loss: 0.4279817044734955
train_iter_loss: 0.385990172624588
train loss :0.2817
---------------------
Validation seg loss: 0.3807115271722652 at epoch 272
epoch =    273/  1000, exp = train
train_iter_loss: 0.20594653487205505
train_iter_loss: 0.3317980468273163
train_iter_loss: 0.18862861394882202
train_iter_loss: 0.42749735713005066
train_iter_loss: 0.39515140652656555
train_iter_loss: 0.291517436504364
train_iter_loss: 0.3070020377635956
train_iter_loss: 0.3243825435638428
train_iter_loss: 0.32325467467308044
train_iter_loss: 0.17091649770736694
train_iter_loss: 0.3669132888317108
train_iter_loss: 0.2486511617898941
train_iter_loss: 0.262042760848999
train_iter_loss: 0.28351736068725586
train_iter_loss: 0.4509463906288147
train_iter_loss: 0.1980343759059906
train_iter_loss: 0.2645159661769867
train_iter_loss: 0.29135018587112427
train_iter_loss: 0.16839399933815002
train_iter_loss: 0.36205676198005676
train_iter_loss: 0.16363967955112457
train_iter_loss: 0.3337654173374176
train_iter_loss: 0.21537484228610992
train_iter_loss: 0.33942151069641113
train_iter_loss: 0.22027863562107086
train_iter_loss: 0.29218652844429016
train_iter_loss: 0.18343748152256012
train_iter_loss: 0.2406502366065979
train_iter_loss: 0.10094564408063889
train_iter_loss: 0.3321831226348877
train_iter_loss: 0.40558701753616333
train_iter_loss: 0.3189672529697418
train_iter_loss: 0.27709415555000305
train_iter_loss: 0.2540491223335266
train_iter_loss: 0.17616473138332367
train_iter_loss: 0.25775647163391113
train_iter_loss: 0.21515707671642303
train_iter_loss: 0.1621890813112259
train_iter_loss: 0.24475307762622833
train_iter_loss: 0.3134274482727051
train_iter_loss: 0.25911593437194824
train_iter_loss: 0.39327213168144226
train_iter_loss: 0.2618597447872162
train_iter_loss: 0.29430434107780457
train_iter_loss: 0.3490355908870697
train_iter_loss: 0.3924265205860138
train_iter_loss: 0.15590742230415344
train_iter_loss: 0.24229003489017487
train_iter_loss: 0.26465651392936707
train_iter_loss: 0.17379452288150787
train_iter_loss: 0.3813874423503876
train_iter_loss: 0.32106250524520874
train_iter_loss: 0.2520390748977661
train_iter_loss: 0.34241268038749695
train_iter_loss: 0.22066056728363037
train_iter_loss: 0.25111180543899536
train_iter_loss: 0.20958636701107025
train_iter_loss: 0.4286750257015228
train_iter_loss: 0.20254476368427277
train_iter_loss: 0.3297209143638611
train_iter_loss: 0.15495890378952026
train_iter_loss: 0.35599395632743835
train_iter_loss: 0.3144321143627167
train_iter_loss: 0.27048876881599426
train_iter_loss: 0.4538936913013458
train_iter_loss: 0.3054180145263672
train_iter_loss: 0.1893121749162674
train_iter_loss: 0.2907572090625763
train_iter_loss: 0.2913638651371002
train_iter_loss: 0.3325429856777191
train_iter_loss: 0.31316280364990234
train_iter_loss: 0.21509583294391632
train_iter_loss: 0.4518544375896454
train_iter_loss: 0.18122324347496033
train_iter_loss: 0.2658727765083313
train_iter_loss: 0.31429755687713623
train_iter_loss: 0.30359119176864624
train_iter_loss: 0.44837796688079834
train_iter_loss: 0.365348219871521
train_iter_loss: 0.17287737131118774
train_iter_loss: 0.37639930844306946
train_iter_loss: 0.23480641841888428
train_iter_loss: 0.23866094648838043
train_iter_loss: 0.18165695667266846
train_iter_loss: 0.2175227701663971
train_iter_loss: 0.49043983221054077
train_iter_loss: 0.3358020782470703
train_iter_loss: 0.27063390612602234
train_iter_loss: 0.26902133226394653
train_iter_loss: 0.22920313477516174
train_iter_loss: 0.2472783774137497
train_iter_loss: 0.2639528512954712
train_iter_loss: 0.17005375027656555
train_iter_loss: 0.3500891625881195
train_iter_loss: 0.20379771292209625
train_iter_loss: 0.32560959458351135
train_iter_loss: 0.27451035380363464
train_iter_loss: 0.4793722331523895
train_iter_loss: 0.4236605763435364
train_iter_loss: 0.26768168807029724
train loss :0.2876
---------------------
Validation seg loss: 0.3661403340674093 at epoch 273
epoch =    274/  1000, exp = train
train_iter_loss: 0.31008246541023254
train_iter_loss: 0.3577817678451538
train_iter_loss: 0.3486805558204651
train_iter_loss: 0.3944292664527893
train_iter_loss: 0.3384871780872345
train_iter_loss: 0.16450031101703644
train_iter_loss: 0.2892860770225525
train_iter_loss: 0.22867411375045776
train_iter_loss: 0.23370122909545898
train_iter_loss: 0.2558145225048065
train_iter_loss: 0.26587164402008057
train_iter_loss: 0.33748725056648254
train_iter_loss: 0.44906744360923767
train_iter_loss: 0.22927308082580566
train_iter_loss: 0.17343707382678986
train_iter_loss: 0.3065628707408905
train_iter_loss: 0.255418062210083
train_iter_loss: 0.3679770827293396
train_iter_loss: 0.26431024074554443
train_iter_loss: 0.22337058186531067
train_iter_loss: 0.3241196572780609
train_iter_loss: 0.34677407145500183
train_iter_loss: 0.31859052181243896
train_iter_loss: 0.12036631256341934
train_iter_loss: 0.40534406900405884
train_iter_loss: 0.24409647285938263
train_iter_loss: 0.22304667532444
train_iter_loss: 0.43787163496017456
train_iter_loss: 0.3651292026042938
train_iter_loss: 0.36129510402679443
train_iter_loss: 0.3372853994369507
train_iter_loss: 0.33498069643974304
train_iter_loss: 0.2269926220178604
train_iter_loss: 0.3465248644351959
train_iter_loss: 0.3536611795425415
train_iter_loss: 0.3069227933883667
train_iter_loss: 0.3652213215827942
train_iter_loss: 0.26386821269989014
train_iter_loss: 0.2008044272661209
train_iter_loss: 0.14015506207942963
train_iter_loss: 0.29773175716400146
train_iter_loss: 0.32258516550064087
train_iter_loss: 0.273179292678833
train_iter_loss: 0.2584870755672455
train_iter_loss: 0.4227427840232849
train_iter_loss: 0.31954696774482727
train_iter_loss: 0.27474939823150635
train_iter_loss: 0.27610883116722107
train_iter_loss: 0.22753161191940308
train_iter_loss: 0.2217622548341751
train_iter_loss: 0.27394187450408936
train_iter_loss: 0.3551255166530609
train_iter_loss: 0.2972092926502228
train_iter_loss: 0.3255027234554291
train_iter_loss: 0.2334759682416916
train_iter_loss: 0.2683960497379303
train_iter_loss: 0.16906458139419556
train_iter_loss: 0.16755332052707672
train_iter_loss: 0.16483332216739655
train_iter_loss: 0.18208417296409607
train_iter_loss: 0.23403076827526093
train_iter_loss: 0.2472078949213028
train_iter_loss: 0.2716621458530426
train_iter_loss: 0.3000160753726959
train_iter_loss: 0.24589072167873383
train_iter_loss: 0.33573076128959656
train_iter_loss: 0.3517164885997772
train_iter_loss: 0.28304725885391235
train_iter_loss: 0.18495677411556244
train_iter_loss: 0.3095841705799103
train_iter_loss: 0.28987550735473633
train_iter_loss: 0.19184254109859467
train_iter_loss: 0.29058924317359924
train_iter_loss: 0.4714060425758362
train_iter_loss: 0.25857773423194885
train_iter_loss: 0.23823870718479156
train_iter_loss: 0.2215452790260315
train_iter_loss: 0.4139256477355957
train_iter_loss: 0.23215267062187195
train_iter_loss: 0.37071719765663147
train_iter_loss: 0.24239937961101532
train_iter_loss: 0.2758323848247528
train_iter_loss: 0.1003936380147934
train_iter_loss: 0.13677722215652466
train_iter_loss: 0.21542976796627045
train_iter_loss: 0.28320276737213135
train_iter_loss: 0.4183364510536194
train_iter_loss: 0.21809278428554535
train_iter_loss: 0.33381667733192444
train_iter_loss: 0.5072306394577026
train_iter_loss: 0.32890287041664124
train_iter_loss: 0.20699359476566315
train_iter_loss: 0.28707703948020935
train_iter_loss: 0.4081915318965912
train_iter_loss: 0.3285865783691406
train_iter_loss: 0.26511576771736145
train_iter_loss: 0.25041279196739197
train_iter_loss: 0.17785213887691498
train_iter_loss: 0.3489810526371002
train_iter_loss: 0.3032328188419342
train loss :0.2882
---------------------
Validation seg loss: 0.3759034946736581 at epoch 274
epoch =    275/  1000, exp = train
train_iter_loss: 0.1326454132795334
train_iter_loss: 0.14136996865272522
train_iter_loss: 0.23967023193836212
train_iter_loss: 0.30909156799316406
train_iter_loss: 0.21366672217845917
train_iter_loss: 0.33312273025512695
train_iter_loss: 0.21878871321678162
train_iter_loss: 0.2712218463420868
train_iter_loss: 0.2666413486003876
train_iter_loss: 0.22676528990268707
train_iter_loss: 0.44147002696990967
train_iter_loss: 0.33956244587898254
train_iter_loss: 0.2936510145664215
train_iter_loss: 0.3321805000305176
train_iter_loss: 0.2845320999622345
train_iter_loss: 0.273356169462204
train_iter_loss: 0.2253337800502777
train_iter_loss: 0.3203612267971039
train_iter_loss: 0.3417159914970398
train_iter_loss: 0.4522184729576111
train_iter_loss: 0.2217034548521042
train_iter_loss: 0.3737495541572571
train_iter_loss: 0.2655505836009979
train_iter_loss: 0.23103731870651245
train_iter_loss: 0.3377704620361328
train_iter_loss: 0.27459821105003357
train_iter_loss: 0.3780249357223511
train_iter_loss: 0.4309349060058594
train_iter_loss: 0.29099881649017334
train_iter_loss: 0.13818930089473724
train_iter_loss: 0.27622711658477783
train_iter_loss: 0.3323298990726471
train_iter_loss: 0.139193594455719
train_iter_loss: 0.47159019112586975
train_iter_loss: 0.2462030053138733
train_iter_loss: 0.39802026748657227
train_iter_loss: 0.19771412014961243
train_iter_loss: 0.4138849675655365
train_iter_loss: 0.22027596831321716
train_iter_loss: 0.23322130739688873
train_iter_loss: 0.2678939402103424
train_iter_loss: 0.35065972805023193
train_iter_loss: 0.1498221606016159
train_iter_loss: 0.25121188163757324
train_iter_loss: 0.2708844840526581
train_iter_loss: 0.34863781929016113
train_iter_loss: 0.1587384194135666
train_iter_loss: 0.2277480810880661
train_iter_loss: 0.23112636804580688
train_iter_loss: 0.42559564113616943
train_iter_loss: 0.2988399565219879
train_iter_loss: 0.27013224363327026
train_iter_loss: 0.37007808685302734
train_iter_loss: 0.2654636800289154
train_iter_loss: 0.19067899882793427
train_iter_loss: 0.2195281982421875
train_iter_loss: 0.40474846959114075
train_iter_loss: 0.2724972069263458
train_iter_loss: 0.2699630856513977
train_iter_loss: 0.27287477254867554
train_iter_loss: 0.30371472239494324
train_iter_loss: 0.4215417802333832
train_iter_loss: 0.21244610846042633
train_iter_loss: 0.12098980695009232
train_iter_loss: 0.17314651608467102
train_iter_loss: 0.22214068472385406
train_iter_loss: 0.3887932300567627
train_iter_loss: 0.2672787010669708
train_iter_loss: 0.33942678570747375
train_iter_loss: 0.17104992270469666
train_iter_loss: 0.37026989459991455
train_iter_loss: 0.2183590829372406
train_iter_loss: 0.23908746242523193
train_iter_loss: 0.34611162543296814
train_iter_loss: 0.4082345962524414
train_iter_loss: 0.3767632842063904
train_iter_loss: 0.2869017422199249
train_iter_loss: 0.3353966772556305
train_iter_loss: 0.29894280433654785
train_iter_loss: 0.2801309823989868
train_iter_loss: 0.2566479444503784
train_iter_loss: 0.25830399990081787
train_iter_loss: 0.3220059275627136
train_iter_loss: 0.24583221971988678
train_iter_loss: 0.2322457879781723
train_iter_loss: 0.17416304349899292
train_iter_loss: 0.2532957196235657
train_iter_loss: 0.47780126333236694
train_iter_loss: 0.2984711825847626
train_iter_loss: 0.3125916123390198
train_iter_loss: 0.21127761900424957
train_iter_loss: 0.21175247430801392
train_iter_loss: 0.2540740966796875
train_iter_loss: 0.16120599210262299
train_iter_loss: 0.28742513060569763
train_iter_loss: 0.24146541953086853
train_iter_loss: 0.29618364572525024
train_iter_loss: 0.29805874824523926
train_iter_loss: 0.20103175938129425
train_iter_loss: 0.19139786064624786
train loss :0.2830
---------------------
Validation seg loss: 0.3612267734430168 at epoch 275
epoch =    276/  1000, exp = train
train_iter_loss: 0.31221824884414673
train_iter_loss: 0.46966665983200073
train_iter_loss: 0.17198097705841064
train_iter_loss: 0.2956352233886719
train_iter_loss: 0.33498305082321167
train_iter_loss: 0.22043296694755554
train_iter_loss: 0.24854467809200287
train_iter_loss: 0.29044198989868164
train_iter_loss: 0.3447149395942688
train_iter_loss: 0.46652349829673767
train_iter_loss: 0.2574582099914551
train_iter_loss: 0.2542281746864319
train_iter_loss: 0.28730884194374084
train_iter_loss: 0.4165230095386505
train_iter_loss: 0.22172246873378754
train_iter_loss: 0.2982591390609741
train_iter_loss: 0.2788850665092468
train_iter_loss: 0.20514605939388275
train_iter_loss: 0.6677773594856262
train_iter_loss: 0.4099419116973877
train_iter_loss: 0.12089701741933823
train_iter_loss: 0.3747912645339966
train_iter_loss: 0.2756390869617462
train_iter_loss: 0.22167006134986877
train_iter_loss: 0.2567310929298401
train_iter_loss: 0.26860475540161133
train_iter_loss: 0.19213701784610748
train_iter_loss: 0.36514294147491455
train_iter_loss: 0.2555018961429596
train_iter_loss: 0.30428019165992737
train_iter_loss: 0.2272261530160904
train_iter_loss: 0.26364320516586304
train_iter_loss: 0.13663090765476227
train_iter_loss: 0.25036951899528503
train_iter_loss: 0.21617385745048523
train_iter_loss: 0.3483608663082123
train_iter_loss: 0.40972429513931274
train_iter_loss: 0.2904588580131531
train_iter_loss: 0.34922096133232117
train_iter_loss: 0.257669061422348
train_iter_loss: 0.1419045329093933
train_iter_loss: 0.3173027038574219
train_iter_loss: 0.23008227348327637
train_iter_loss: 0.38772013783454895
train_iter_loss: 0.3314245939254761
train_iter_loss: 0.24043719470500946
train_iter_loss: 0.2167942076921463
train_iter_loss: 0.36469677090644836
train_iter_loss: 0.1722015142440796
train_iter_loss: 0.402990460395813
train_iter_loss: 0.25925812125205994
train_iter_loss: 0.32424095273017883
train_iter_loss: 0.3266635835170746
train_iter_loss: 0.2875496447086334
train_iter_loss: 0.45522767305374146
train_iter_loss: 0.26562175154685974
train_iter_loss: 0.2239736169576645
train_iter_loss: 0.22131215035915375
train_iter_loss: 0.18246258795261383
train_iter_loss: 0.23615694046020508
train_iter_loss: 0.14052027463912964
train_iter_loss: 0.2701016068458557
train_iter_loss: 0.30773216485977173
train_iter_loss: 0.20360153913497925
train_iter_loss: 0.26308971643447876
train_iter_loss: 0.3282497525215149
train_iter_loss: 0.2092244029045105
train_iter_loss: 0.19326387345790863
train_iter_loss: 0.16041713953018188
train_iter_loss: 0.21211890876293182
train_iter_loss: 0.152193084359169
train_iter_loss: 0.3488500118255615
train_iter_loss: 0.3700636327266693
train_iter_loss: 0.27813395857810974
train_iter_loss: 0.2574344575405121
train_iter_loss: 0.3985234797000885
train_iter_loss: 0.40775421261787415
train_iter_loss: 0.31817880272865295
train_iter_loss: 0.2979624569416046
train_iter_loss: 0.22586527466773987
train_iter_loss: 0.27114081382751465
train_iter_loss: 0.2044873833656311
train_iter_loss: 0.24197879433631897
train_iter_loss: 0.24994544684886932
train_iter_loss: 0.21888722479343414
train_iter_loss: 0.24431882798671722
train_iter_loss: 0.19359531998634338
train_iter_loss: 0.41970184445381165
train_iter_loss: 0.2544093430042267
train_iter_loss: 0.3165709376335144
train_iter_loss: 0.21521684527397156
train_iter_loss: 0.15305690467357635
train_iter_loss: 0.21435199677944183
train_iter_loss: 0.2102435976266861
train_iter_loss: 0.39408227801322937
train_iter_loss: 0.2611590623855591
train_iter_loss: 0.31539952754974365
train_iter_loss: 0.29627591371536255
train_iter_loss: 0.2724999785423279
train_iter_loss: 0.3512430191040039
train loss :0.2835
---------------------
Validation seg loss: 0.41142669320985115 at epoch 276
epoch =    277/  1000, exp = train
train_iter_loss: 0.18791982531547546
train_iter_loss: 0.25751468539237976
train_iter_loss: 0.2418483942747116
train_iter_loss: 0.32222461700439453
train_iter_loss: 0.3538214862346649
train_iter_loss: 0.2220858633518219
train_iter_loss: 0.3060404062271118
train_iter_loss: 0.34734129905700684
train_iter_loss: 0.3761370778083801
train_iter_loss: 0.29240795969963074
train_iter_loss: 0.373268723487854
train_iter_loss: 0.45808058977127075
train_iter_loss: 0.19477345049381256
train_iter_loss: 0.08442847430706024
train_iter_loss: 0.22971558570861816
train_iter_loss: 0.2806384265422821
train_iter_loss: 0.11034147441387177
train_iter_loss: 0.24439595639705658
train_iter_loss: 0.4992665946483612
train_iter_loss: 0.298355370759964
train_iter_loss: 0.23423883318901062
train_iter_loss: 0.378627747297287
train_iter_loss: 0.4282817542552948
train_iter_loss: 0.38772082328796387
train_iter_loss: 0.17889085412025452
train_iter_loss: 0.3442787230014801
train_iter_loss: 0.37077778577804565
train_iter_loss: 0.37757608294487
train_iter_loss: 0.2869754135608673
train_iter_loss: 0.17996583878993988
train_iter_loss: 0.2033417969942093
train_iter_loss: 0.2778320610523224
train_iter_loss: 0.25972720980644226
train_iter_loss: 0.25433334708213806
train_iter_loss: 0.28383302688598633
train_iter_loss: 0.1549999713897705
train_iter_loss: 0.37753376364707947
train_iter_loss: 0.20016932487487793
train_iter_loss: 0.3343476355075836
train_iter_loss: 0.12220728397369385
train_iter_loss: 0.3518645167350769
train_iter_loss: 0.20656763017177582
train_iter_loss: 0.31760308146476746
train_iter_loss: 0.29295527935028076
train_iter_loss: 0.19385972619056702
train_iter_loss: 0.29123082756996155
train_iter_loss: 0.36185774207115173
train_iter_loss: 0.40018099546432495
train_iter_loss: 0.3800522983074188
train_iter_loss: 0.2808493375778198
train_iter_loss: 0.24388468265533447
train_iter_loss: 0.31303125619888306
train_iter_loss: 0.5802887082099915
train_iter_loss: 0.30668777227401733
train_iter_loss: 0.19398045539855957
train_iter_loss: 0.41601136326789856
train_iter_loss: 0.25460222363471985
train_iter_loss: 0.3667323589324951
train_iter_loss: 0.28188034892082214
train_iter_loss: 0.3216562271118164
train_iter_loss: 0.22583498060703278
train_iter_loss: 0.3546275198459625
train_iter_loss: 0.4057193398475647
train_iter_loss: 0.3024481236934662
train_iter_loss: 0.12009558081626892
train_iter_loss: 0.32087796926498413
train_iter_loss: 0.1522957682609558
train_iter_loss: 0.2280353456735611
train_iter_loss: 0.33828088641166687
train_iter_loss: 0.26349911093711853
train_iter_loss: 0.24104179441928864
train_iter_loss: 0.331134557723999
train_iter_loss: 0.1821001172065735
train_iter_loss: 0.2645096480846405
train_iter_loss: 0.4923739731311798
train_iter_loss: 0.2670605182647705
train_iter_loss: 0.14043480157852173
train_iter_loss: 0.19591456651687622
train_iter_loss: 0.4665151834487915
train_iter_loss: 0.3482464849948883
train_iter_loss: 0.3226618766784668
train_iter_loss: 0.1895170658826828
train_iter_loss: 0.32849377393722534
train_iter_loss: 0.3032173812389374
train_iter_loss: 0.20407021045684814
train_iter_loss: 0.20954933762550354
train_iter_loss: 0.2703736424446106
train_iter_loss: 0.2664210796356201
train_iter_loss: 0.19101166725158691
train_iter_loss: 0.2778247892856598
train_iter_loss: 0.23363502323627472
train_iter_loss: 0.28294050693511963
train_iter_loss: 0.2631484270095825
train_iter_loss: 0.26253628730773926
train_iter_loss: 0.2108720988035202
train_iter_loss: 0.3285071849822998
train_iter_loss: 0.2571866810321808
train_iter_loss: 0.3000492453575134
train_iter_loss: 0.20470482110977173
train_iter_loss: 0.20496076345443726
train loss :0.2871
---------------------
Validation seg loss: 0.37409476236492956 at epoch 277
epoch =    278/  1000, exp = train
train_iter_loss: 0.4238821566104889
train_iter_loss: 0.3019033670425415
train_iter_loss: 0.2717471122741699
train_iter_loss: 0.3865502178668976
train_iter_loss: 0.44353845715522766
train_iter_loss: 0.28097593784332275
train_iter_loss: 0.15143661201000214
train_iter_loss: 0.26260870695114136
train_iter_loss: 0.251334011554718
train_iter_loss: 0.3277339041233063
train_iter_loss: 0.4357180595397949
train_iter_loss: 0.29904529452323914
train_iter_loss: 0.3351360857486725
train_iter_loss: 0.21100355684757233
train_iter_loss: 0.3270253539085388
train_iter_loss: 0.2079024612903595
train_iter_loss: 0.22643381357192993
train_iter_loss: 0.3238140046596527
train_iter_loss: 0.24451124668121338
train_iter_loss: 0.27244558930397034
train_iter_loss: 0.24512599408626556
train_iter_loss: 0.19455936551094055
train_iter_loss: 0.1256490796804428
train_iter_loss: 0.2986675202846527
train_iter_loss: 0.3650347888469696
train_iter_loss: 0.3794609606266022
train_iter_loss: 0.2989363968372345
train_iter_loss: 0.18494494259357452
train_iter_loss: 0.29789987206459045
train_iter_loss: 0.34719428420066833
train_iter_loss: 0.3475932478904724
train_iter_loss: 0.13347147405147552
train_iter_loss: 0.4657711088657379
train_iter_loss: 0.21369948983192444
train_iter_loss: 0.28294506669044495
train_iter_loss: 0.1951514631509781
train_iter_loss: 0.331666499376297
train_iter_loss: 0.21492014825344086
train_iter_loss: 0.2469148337841034
train_iter_loss: 0.4047715663909912
train_iter_loss: 0.25401321053504944
train_iter_loss: 0.1551479995250702
train_iter_loss: 0.2654736042022705
train_iter_loss: 0.40220457315444946
train_iter_loss: 0.2902185916900635
train_iter_loss: 0.22372809052467346
train_iter_loss: 0.30641528964042664
train_iter_loss: 0.22755450010299683
train_iter_loss: 0.1857670098543167
train_iter_loss: 0.2867884635925293
train_iter_loss: 0.280048668384552
train_iter_loss: 0.25602537393569946
train_iter_loss: 0.3937970697879791
train_iter_loss: 0.34063011407852173
train_iter_loss: 0.209769606590271
train_iter_loss: 0.33165979385375977
train_iter_loss: 0.37259912490844727
train_iter_loss: 0.23975680768489838
train_iter_loss: 0.20357169210910797
train_iter_loss: 0.40628883242607117
train_iter_loss: 0.41568997502326965
train_iter_loss: 0.30318528413772583
train_iter_loss: 0.27184703946113586
train_iter_loss: 0.24866759777069092
train_iter_loss: 0.2834947407245636
train_iter_loss: 0.19871409237384796
train_iter_loss: 0.2784503996372223
train_iter_loss: 0.2906571626663208
train_iter_loss: 0.3466928005218506
train_iter_loss: 0.19886645674705505
train_iter_loss: 0.24512819945812225
train_iter_loss: 0.26482850313186646
train_iter_loss: 0.394778847694397
train_iter_loss: 0.3236179053783417
train_iter_loss: 0.22771523892879486
train_iter_loss: 0.29342877864837646
train_iter_loss: 0.25572070479393005
train_iter_loss: 0.32751235365867615
train_iter_loss: 0.2684885859489441
train_iter_loss: 0.21151848137378693
train_iter_loss: 0.16982479393482208
train_iter_loss: 0.2786708176136017
train_iter_loss: 0.20405073463916779
train_iter_loss: 0.22649890184402466
train_iter_loss: 0.2936843931674957
train_iter_loss: 0.13139575719833374
train_iter_loss: 0.3185926675796509
train_iter_loss: 0.2133999764919281
train_iter_loss: 0.3866391181945801
train_iter_loss: 0.24232424795627594
train_iter_loss: 0.35468578338623047
train_iter_loss: 0.4202331304550171
train_iter_loss: 0.20585857331752777
train_iter_loss: 0.3114871680736542
train_iter_loss: 0.27907252311706543
train_iter_loss: 0.33928704261779785
train_iter_loss: 0.36804044246673584
train_iter_loss: 0.32496216893196106
train_iter_loss: 0.2681284546852112
train_iter_loss: 0.22583064436912537
train loss :0.2869
---------------------
Validation seg loss: 0.3663710950662927 at epoch 278
epoch =    279/  1000, exp = train
train_iter_loss: 0.17801831662654877
train_iter_loss: 0.47963741421699524
train_iter_loss: 0.35786277055740356
train_iter_loss: 0.3834685683250427
train_iter_loss: 0.215121328830719
train_iter_loss: 0.17351527512073517
train_iter_loss: 0.16722188889980316
train_iter_loss: 0.3487969934940338
train_iter_loss: 0.29430216550827026
train_iter_loss: 0.33508753776550293
train_iter_loss: 0.2783038318157196
train_iter_loss: 0.2658644914627075
train_iter_loss: 0.1703472137451172
train_iter_loss: 0.32138657569885254
train_iter_loss: 0.3071473240852356
train_iter_loss: 0.22738021612167358
train_iter_loss: 0.25073471665382385
train_iter_loss: 0.23397333920001984
train_iter_loss: 0.2927081286907196
train_iter_loss: 0.36945825815200806
train_iter_loss: 0.26493167877197266
train_iter_loss: 0.23473460972309113
train_iter_loss: 0.19661331176757812
train_iter_loss: 0.25836512446403503
train_iter_loss: 0.4197806715965271
train_iter_loss: 0.17216521501541138
train_iter_loss: 0.16724227368831635
train_iter_loss: 0.2515560984611511
train_iter_loss: 0.3388376533985138
train_iter_loss: 0.04211820289492607
train_iter_loss: 0.3352750241756439
train_iter_loss: 0.186248779296875
train_iter_loss: 0.15275469422340393
train_iter_loss: 0.23833848536014557
train_iter_loss: 0.23815204203128815
train_iter_loss: 0.13209176063537598
train_iter_loss: 0.24962922930717468
train_iter_loss: 0.2751729190349579
train_iter_loss: 0.1723456084728241
train_iter_loss: 0.31556764245033264
train_iter_loss: 0.4762275815010071
train_iter_loss: 0.2945135533809662
train_iter_loss: 0.27227506041526794
train_iter_loss: 0.31809863448143005
train_iter_loss: 0.21924032270908356
train_iter_loss: 0.19789333641529083
train_iter_loss: 0.5060809850692749
train_iter_loss: 0.3400462567806244
train_iter_loss: 0.23167075216770172
train_iter_loss: 0.3178654909133911
train_iter_loss: 0.2041056603193283
train_iter_loss: 0.2003205567598343
train_iter_loss: 0.3077274262905121
train_iter_loss: 0.19278757274150848
train_iter_loss: 0.29238125681877136
train_iter_loss: 0.357092022895813
train_iter_loss: 0.2624772787094116
train_iter_loss: 0.2934025228023529
train_iter_loss: 0.17176319658756256
train_iter_loss: 0.22066816687583923
train_iter_loss: 0.37681081891059875
train_iter_loss: 0.28810688853263855
train_iter_loss: 0.28641951084136963
train_iter_loss: 0.357865571975708
train_iter_loss: 0.3063850402832031
train_iter_loss: 0.3211851716041565
train_iter_loss: 0.476095974445343
train_iter_loss: 0.39330360293388367
train_iter_loss: 0.3118687570095062
train_iter_loss: 0.2833136320114136
train_iter_loss: 0.26885414123535156
train_iter_loss: 0.3311881721019745
train_iter_loss: 0.3278294801712036
train_iter_loss: 0.38011008501052856
train_iter_loss: 0.19687683880329132
train_iter_loss: 0.25404784083366394
train_iter_loss: 0.2287900447845459
train_iter_loss: 0.1945715844631195
train_iter_loss: 0.46987825632095337
train_iter_loss: 0.14794504642486572
train_iter_loss: 0.24741177260875702
train_iter_loss: 0.33723586797714233
train_iter_loss: 0.2698054313659668
train_iter_loss: 0.2661905586719513
train_iter_loss: 0.247877299785614
train_iter_loss: 0.34942111372947693
train_iter_loss: 0.38608095049858093
train_iter_loss: 0.325779527425766
train_iter_loss: 0.40725207328796387
train_iter_loss: 0.2403125762939453
train_iter_loss: 0.3475513458251953
train_iter_loss: 0.44396328926086426
train_iter_loss: 0.18046443164348602
train_iter_loss: 0.17478109896183014
train_iter_loss: 0.2912285625934601
train_iter_loss: 0.266482412815094
train_iter_loss: 0.3073474168777466
train_iter_loss: 0.25466829538345337
train_iter_loss: 0.2605857849121094
train_iter_loss: 0.259529173374176
train loss :0.2832
---------------------
Validation seg loss: 0.3709119129750245 at epoch 279
epoch =    280/  1000, exp = train
train_iter_loss: 0.36256012320518494
train_iter_loss: 0.3323349356651306
train_iter_loss: 0.12838581204414368
train_iter_loss: 0.20089460909366608
train_iter_loss: 0.1348453164100647
train_iter_loss: 0.31720948219299316
train_iter_loss: 0.32329437136650085
train_iter_loss: 0.31879428029060364
train_iter_loss: 0.3345237374305725
train_iter_loss: 0.37375131249427795
train_iter_loss: 0.17291384935379028
train_iter_loss: 0.3088909089565277
train_iter_loss: 0.29093700647354126
train_iter_loss: 0.1673624962568283
train_iter_loss: 0.6456881165504456
train_iter_loss: 0.25893670320510864
train_iter_loss: 0.33604902029037476
train_iter_loss: 0.2064719796180725
train_iter_loss: 0.32615578174591064
train_iter_loss: 0.41309279203414917
train_iter_loss: 0.17855952680110931
train_iter_loss: 0.28719931840896606
train_iter_loss: 0.15789388120174408
train_iter_loss: 0.27561450004577637
train_iter_loss: 0.2247592955827713
train_iter_loss: 0.2189563512802124
train_iter_loss: 0.3117000162601471
train_iter_loss: 0.20010456442832947
train_iter_loss: 0.3306938707828522
train_iter_loss: 0.3919026851654053
train_iter_loss: 0.19765137135982513
train_iter_loss: 0.27285513281822205
train_iter_loss: 0.282166063785553
train_iter_loss: 0.23959647119045258
train_iter_loss: 0.3332088887691498
train_iter_loss: 0.38423043489456177
train_iter_loss: 0.23081432282924652
train_iter_loss: 0.30692222714424133
train_iter_loss: 0.2947410047054291
train_iter_loss: 0.37839066982269287
train_iter_loss: 0.32700568437576294
train_iter_loss: 0.3466269075870514
train_iter_loss: 0.31183844804763794
train_iter_loss: 0.3066953718662262
train_iter_loss: 0.17486165463924408
train_iter_loss: 0.30139273405075073
train_iter_loss: 0.28310853242874146
train_iter_loss: 0.2502747178077698
train_iter_loss: 0.30987516045570374
train_iter_loss: 0.26501891016960144
train_iter_loss: 0.2715573310852051
train_iter_loss: 0.2846048176288605
train_iter_loss: 0.19322152435779572
train_iter_loss: 0.19607391953468323
train_iter_loss: 0.19489067792892456
train_iter_loss: 0.2595019042491913
train_iter_loss: 0.3088875710964203
train_iter_loss: 0.28544119000434875
train_iter_loss: 0.2725122570991516
train_iter_loss: 0.3754017949104309
train_iter_loss: 0.4173797070980072
train_iter_loss: 0.24286331236362457
train_iter_loss: 0.3400820195674896
train_iter_loss: 0.25156208872795105
train_iter_loss: 0.24698206782341003
train_iter_loss: 0.40882250666618347
train_iter_loss: 0.2808321416378021
train_iter_loss: 0.2972014248371124
train_iter_loss: 0.21090807020664215
train_iter_loss: 0.29870548844337463
train_iter_loss: 0.30605217814445496
train_iter_loss: 0.2966184914112091
train_iter_loss: 0.4249741733074188
train_iter_loss: 0.22295476496219635
train_iter_loss: 0.24372565746307373
train_iter_loss: 0.2229718118906021
train_iter_loss: 0.23712807893753052
train_iter_loss: 0.24819333851337433
train_iter_loss: 0.2286766618490219
train_iter_loss: 0.3223758041858673
train_iter_loss: 0.18225668370723724
train_iter_loss: 0.12279728055000305
train_iter_loss: 0.3538622558116913
train_iter_loss: 0.23142510652542114
train_iter_loss: 0.3460237383842468
train_iter_loss: 0.31822842359542847
train_iter_loss: 0.34543412923812866
train_iter_loss: 0.4865727722644806
train_iter_loss: 0.2575452923774719
train_iter_loss: 0.25999656319618225
train_iter_loss: 0.20370574295520782
train_iter_loss: 0.1911444365978241
train_iter_loss: 0.4549160897731781
train_iter_loss: 0.2722257673740387
train_iter_loss: 0.3223218321800232
train_iter_loss: 0.3455142378807068
train_iter_loss: 0.21182453632354736
train_iter_loss: 0.22341333329677582
train_iter_loss: 0.30361902713775635
train_iter_loss: 0.3630020022392273
train loss :0.2880
---------------------
Validation seg loss: 0.3855234325318685 at epoch 280
epoch =    281/  1000, exp = train
train_iter_loss: 0.22385014593601227
train_iter_loss: 0.2882464528083801
train_iter_loss: 0.23693665862083435
train_iter_loss: 0.3088105022907257
train_iter_loss: 0.1492888629436493
train_iter_loss: 0.3530188202857971
train_iter_loss: 0.47281476855278015
train_iter_loss: 0.25830450654029846
train_iter_loss: 0.19937099516391754
train_iter_loss: 0.3177083730697632
train_iter_loss: 0.19573865830898285
train_iter_loss: 0.407382994890213
train_iter_loss: 0.19500192999839783
train_iter_loss: 0.16020481288433075
train_iter_loss: 0.2362365871667862
train_iter_loss: 0.19414649903774261
train_iter_loss: 0.2893165946006775
train_iter_loss: 0.08911546319723129
train_iter_loss: 0.13869397342205048
train_iter_loss: 0.12273605912923813
train_iter_loss: 0.33608561754226685
train_iter_loss: 0.1594809591770172
train_iter_loss: 0.20517003536224365
train_iter_loss: 0.4812900424003601
train_iter_loss: 0.4324038624763489
train_iter_loss: 0.36216482520103455
train_iter_loss: 0.38964661955833435
train_iter_loss: 0.20897503197193146
train_iter_loss: 0.27721932530403137
train_iter_loss: 0.3725840747356415
train_iter_loss: 0.2906981408596039
train_iter_loss: 0.3738224506378174
train_iter_loss: 0.28719857335090637
train_iter_loss: 0.09521535784006119
train_iter_loss: 0.29241374135017395
train_iter_loss: 0.31933268904685974
train_iter_loss: 0.06269562989473343
train_iter_loss: 0.29901036620140076
train_iter_loss: 0.2873685657978058
train_iter_loss: 0.40219902992248535
train_iter_loss: 0.361465722322464
train_iter_loss: 0.13157381117343903
train_iter_loss: 0.21834462881088257
train_iter_loss: 0.3471848964691162
train_iter_loss: 0.17826798558235168
train_iter_loss: 0.26227980852127075
train_iter_loss: 0.2583296298980713
train_iter_loss: 0.19940809905529022
train_iter_loss: 0.31863072514533997
train_iter_loss: 0.30538204312324524
train_iter_loss: 0.1952846348285675
train_iter_loss: 0.23264437913894653
train_iter_loss: 0.17592273652553558
train_iter_loss: 0.30736449360847473
train_iter_loss: 0.3273475468158722
train_iter_loss: 0.30137282609939575
train_iter_loss: 0.22041437029838562
train_iter_loss: 0.42696768045425415
train_iter_loss: 0.3613283932209015
train_iter_loss: 0.3421952724456787
train_iter_loss: 0.1819925755262375
train_iter_loss: 0.3060993552207947
train_iter_loss: 0.3311617970466614
train_iter_loss: 0.469942569732666
train_iter_loss: 0.17616255581378937
train_iter_loss: 0.37226876616477966
train_iter_loss: 0.2833665609359741
train_iter_loss: 0.3486941456794739
train_iter_loss: 0.2082376629114151
train_iter_loss: 0.26989981532096863
train_iter_loss: 0.34920528531074524
train_iter_loss: 0.3967219293117523
train_iter_loss: 0.2374693602323532
train_iter_loss: 0.21807263791561127
train_iter_loss: 0.3300588130950928
train_iter_loss: 0.6317381262779236
train_iter_loss: 0.1395140439271927
train_iter_loss: 0.3755655586719513
train_iter_loss: 0.3804073929786682
train_iter_loss: 0.33380553126335144
train_iter_loss: 0.3085721731185913
train_iter_loss: 0.2455875724554062
train_iter_loss: 0.2983696460723877
train_iter_loss: 0.1948443204164505
train_iter_loss: 0.2990511655807495
train_iter_loss: 0.29303762316703796
train_iter_loss: 0.23924559354782104
train_iter_loss: 0.31266382336616516
train_iter_loss: 0.29915302991867065
train_iter_loss: 0.19514964520931244
train_iter_loss: 0.18161623179912567
train_iter_loss: 0.1185150295495987
train_iter_loss: 0.20489592850208282
train_iter_loss: 0.2572594881057739
train_iter_loss: 0.26983335614204407
train_iter_loss: 0.4751891791820526
train_iter_loss: 0.3323264420032501
train_iter_loss: 0.15471674501895905
train_iter_loss: 0.21938437223434448
train_iter_loss: 0.3738161027431488
train loss :0.2815
---------------------
Validation seg loss: 0.36003217876906385 at epoch 281
epoch =    282/  1000, exp = train
train_iter_loss: 0.2881892919540405
train_iter_loss: 0.331725150346756
train_iter_loss: 0.5872699022293091
train_iter_loss: 0.4096125364303589
train_iter_loss: 0.35289549827575684
train_iter_loss: 0.24838784337043762
train_iter_loss: 0.40173834562301636
train_iter_loss: 0.3115808367729187
train_iter_loss: 0.23257195949554443
train_iter_loss: 0.25650131702423096
train_iter_loss: 0.23877716064453125
train_iter_loss: 0.13985681533813477
train_iter_loss: 0.15575900673866272
train_iter_loss: 0.25285395979881287
train_iter_loss: 0.3623065948486328
train_iter_loss: 0.18325763940811157
train_iter_loss: 0.4473443329334259
train_iter_loss: 0.3224363327026367
train_iter_loss: 0.4054880738258362
train_iter_loss: 0.11071506887674332
train_iter_loss: 0.2022888958454132
train_iter_loss: 0.25537577271461487
train_iter_loss: 0.24112842977046967
train_iter_loss: 0.3705318868160248
train_iter_loss: 0.24383698403835297
train_iter_loss: 0.24876625835895538
train_iter_loss: 0.2561357617378235
train_iter_loss: 0.26691991090774536
train_iter_loss: 0.2567434012889862
train_iter_loss: 0.2397952675819397
train_iter_loss: 0.30317702889442444
train_iter_loss: 0.22592690587043762
train_iter_loss: 0.3162466585636139
train_iter_loss: 0.2493903487920761
train_iter_loss: 0.2308443933725357
train_iter_loss: 0.3830197751522064
train_iter_loss: 0.3691541850566864
train_iter_loss: 0.25588858127593994
train_iter_loss: 0.3491794466972351
train_iter_loss: 0.29723310470581055
train_iter_loss: 0.2038675993680954
train_iter_loss: 0.16151784360408783
train_iter_loss: 0.2160327285528183
train_iter_loss: 0.33598577976226807
train_iter_loss: 0.3206230103969574
train_iter_loss: 0.3739268183708191
train_iter_loss: 0.32707667350769043
train_iter_loss: 0.31358692049980164
train_iter_loss: 0.35092583298683167
train_iter_loss: 0.3426312208175659
train_iter_loss: 0.3371567130088806
train_iter_loss: 0.16012339293956757
train_iter_loss: 0.3098832368850708
train_iter_loss: 0.2194635421037674
train_iter_loss: 0.24716222286224365
train_iter_loss: 0.18222947418689728
train_iter_loss: 0.2796787917613983
train_iter_loss: 0.3218395411968231
train_iter_loss: 0.27009499073028564
train_iter_loss: 0.33346182107925415
train_iter_loss: 0.3811267912387848
train_iter_loss: 0.24336445331573486
train_iter_loss: 0.1834816336631775
train_iter_loss: 0.280834436416626
train_iter_loss: 0.3077695667743683
train_iter_loss: 0.33430054783821106
train_iter_loss: 0.2547512650489807
train_iter_loss: 0.499909907579422
train_iter_loss: 0.25801679491996765
train_iter_loss: 0.14473822712898254
train_iter_loss: 0.3355945944786072
train_iter_loss: 0.18250952661037445
train_iter_loss: 0.30912479758262634
train_iter_loss: 0.38741520047187805
train_iter_loss: 0.2727925479412079
train_iter_loss: 0.08897294849157333
train_iter_loss: 0.314566433429718
train_iter_loss: 0.1532307267189026
train_iter_loss: 0.30483707785606384
train_iter_loss: 0.22083599865436554
train_iter_loss: 0.28219130635261536
train_iter_loss: 0.24519449472427368
train_iter_loss: 0.22980467975139618
train_iter_loss: 0.23853608965873718
train_iter_loss: 0.204104945063591
train_iter_loss: 0.20125237107276917
train_iter_loss: 0.3387640416622162
train_iter_loss: 0.27147576212882996
train_iter_loss: 0.28388410806655884
train_iter_loss: 0.2166702151298523
train_iter_loss: 0.38551533222198486
train_iter_loss: 0.2130746692419052
train_iter_loss: 0.37961718440055847
train_iter_loss: 0.33595043420791626
train_iter_loss: 0.38441595435142517
train_iter_loss: 0.2334679365158081
train_iter_loss: 0.2680756151676178
train_iter_loss: 0.40844520926475525
train_iter_loss: 0.3547944724559784
train_iter_loss: 0.1583460569381714
train loss :0.2858
---------------------
Validation seg loss: 0.3647333486532828 at epoch 282
epoch =    283/  1000, exp = train
train_iter_loss: 0.27890923619270325
train_iter_loss: 0.27653512358665466
train_iter_loss: 0.2281147837638855
train_iter_loss: 0.3092534840106964
train_iter_loss: 0.36839979887008667
train_iter_loss: 0.3297741115093231
train_iter_loss: 0.20120511949062347
train_iter_loss: 0.19609785079956055
train_iter_loss: 0.21887265145778656
train_iter_loss: 0.24527661502361298
train_iter_loss: 0.20430099964141846
train_iter_loss: 0.3360487222671509
train_iter_loss: 0.4042254388332367
train_iter_loss: 0.22508184611797333
train_iter_loss: 0.2744598984718323
train_iter_loss: 0.39643144607543945
train_iter_loss: 0.19638484716415405
train_iter_loss: 0.1493747979402542
train_iter_loss: 0.390584260225296
train_iter_loss: 0.24460192024707794
train_iter_loss: 0.495391845703125
train_iter_loss: 0.22063326835632324
train_iter_loss: 0.4606676399707794
train_iter_loss: 0.3568814992904663
train_iter_loss: 0.10919900238513947
train_iter_loss: 0.28478050231933594
train_iter_loss: 0.18167583644390106
train_iter_loss: 0.2768430709838867
train_iter_loss: 0.4213680922985077
train_iter_loss: 0.270148903131485
train_iter_loss: 0.2566075921058655
train_iter_loss: 0.2775876522064209
train_iter_loss: 0.33188313245773315
train_iter_loss: 0.22057755291461945
train_iter_loss: 0.356064110994339
train_iter_loss: 0.18443070352077484
train_iter_loss: 0.27676713466644287
train_iter_loss: 0.19661974906921387
train_iter_loss: 0.28284379839897156
train_iter_loss: 0.2907237708568573
train_iter_loss: 0.39160415530204773
train_iter_loss: 0.17244328558444977
train_iter_loss: 0.3840067684650421
train_iter_loss: 0.2668384313583374
train_iter_loss: 0.3262348771095276
train_iter_loss: 0.33381035923957825
train_iter_loss: 0.28403371572494507
train_iter_loss: 0.3006388247013092
train_iter_loss: 0.3743225634098053
train_iter_loss: 0.355362206697464
train_iter_loss: 0.3000606894493103
train_iter_loss: 0.36915814876556396
train_iter_loss: 0.15318885445594788
train_iter_loss: 0.31926819682121277
train_iter_loss: 0.5121319890022278
train_iter_loss: 0.21632573008537292
train_iter_loss: 0.20110267400741577
train_iter_loss: 0.18804562091827393
train_iter_loss: 0.24710437655448914
train_iter_loss: 0.24694199860095978
train_iter_loss: 0.127633199095726
train_iter_loss: 0.2906692326068878
train_iter_loss: 0.29823535680770874
train_iter_loss: 0.3130476474761963
train_iter_loss: 0.3112407624721527
train_iter_loss: 0.32838624715805054
train_iter_loss: 0.2757299244403839
train_iter_loss: 0.2582313120365143
train_iter_loss: 0.15195617079734802
train_iter_loss: 0.13711413741111755
train_iter_loss: 0.23716704547405243
train_iter_loss: 0.24643893539905548
train_iter_loss: 0.3646702766418457
train_iter_loss: 0.1609594076871872
train_iter_loss: 0.33282506465911865
train_iter_loss: 0.529511570930481
train_iter_loss: 0.30083999037742615
train_iter_loss: 0.2826175391674042
train_iter_loss: 0.19125694036483765
train_iter_loss: 0.2811341881752014
train_iter_loss: 0.1986677050590515
train_iter_loss: 0.3003067672252655
train_iter_loss: 0.37331652641296387
train_iter_loss: 0.3314131200313568
train_iter_loss: 0.18050651252269745
train_iter_loss: 0.22286981344223022
train_iter_loss: 0.2944348156452179
train_iter_loss: 0.30489736795425415
train_iter_loss: 0.6380376219749451
train_iter_loss: 0.24456144869327545
train_iter_loss: 0.30869999527931213
train_iter_loss: 0.06463730335235596
train_iter_loss: 0.28666824102401733
train_iter_loss: 0.2947259843349457
train_iter_loss: 0.4160594344139099
train_iter_loss: 0.2674492597579956
train_iter_loss: 0.1391768455505371
train_iter_loss: 0.39520639181137085
train_iter_loss: 0.28920039534568787
train_iter_loss: 0.27652546763420105
train loss :0.2870
---------------------
Validation seg loss: 0.3772771989417104 at epoch 283
epoch =    284/  1000, exp = train
train_iter_loss: 0.15545803308486938
train_iter_loss: 0.15455429255962372
train_iter_loss: 0.07876580953598022
train_iter_loss: 0.4691753387451172
train_iter_loss: 0.3340781629085541
train_iter_loss: 0.29657596349716187
train_iter_loss: 0.28403592109680176
train_iter_loss: 0.2825653553009033
train_iter_loss: 0.2747577130794525
train_iter_loss: 0.2667042016983032
train_iter_loss: 0.1922544538974762
train_iter_loss: 0.2132352739572525
train_iter_loss: 0.17299845814704895
train_iter_loss: 0.17941893637180328
train_iter_loss: 0.44519299268722534
train_iter_loss: 0.3359602093696594
train_iter_loss: 0.1999215930700302
train_iter_loss: 0.1792316734790802
train_iter_loss: 0.24142003059387207
train_iter_loss: 0.30937179923057556
train_iter_loss: 0.46106407046318054
train_iter_loss: 0.2837018072605133
train_iter_loss: 0.15904104709625244
train_iter_loss: 0.3017292618751526
train_iter_loss: 0.326673299074173
train_iter_loss: 0.33294788002967834
train_iter_loss: 0.1793607771396637
train_iter_loss: 0.3976306617259979
train_iter_loss: 0.24931009113788605
train_iter_loss: 0.20693974196910858
train_iter_loss: 0.3068525195121765
train_iter_loss: 0.1827320158481598
train_iter_loss: 0.35446545481681824
train_iter_loss: 0.3341119885444641
train_iter_loss: 0.3690166473388672
train_iter_loss: 0.252543568611145
train_iter_loss: 0.28021249175071716
train_iter_loss: 0.31437525153160095
train_iter_loss: 0.19348426163196564
train_iter_loss: 0.21888549625873566
train_iter_loss: 0.4776116609573364
train_iter_loss: 0.2383456826210022
train_iter_loss: 0.1404239684343338
train_iter_loss: 0.40424004197120667
train_iter_loss: 0.2745985686779022
train_iter_loss: 0.2299812287092209
train_iter_loss: 0.21866938471794128
train_iter_loss: 0.19867996871471405
train_iter_loss: 0.384528249502182
train_iter_loss: 0.34661030769348145
train_iter_loss: 0.17876236140727997
train_iter_loss: 0.2050623595714569
train_iter_loss: 0.1502106785774231
train_iter_loss: 0.19780832529067993
train_iter_loss: 0.24014312028884888
train_iter_loss: 0.2681002616882324
train_iter_loss: 0.15122629702091217
train_iter_loss: 0.362118124961853
train_iter_loss: 0.15785297751426697
train_iter_loss: 0.3839015066623688
train_iter_loss: 0.31049665808677673
train_iter_loss: 0.22475473582744598
train_iter_loss: 0.42689159512519836
train_iter_loss: 0.35548293590545654
train_iter_loss: 0.4212319850921631
train_iter_loss: 0.2602502405643463
train_iter_loss: 0.4173091948032379
train_iter_loss: 0.2985503673553467
train_iter_loss: 0.20664054155349731
train_iter_loss: 0.422690749168396
train_iter_loss: 0.29734665155410767
train_iter_loss: 0.25643694400787354
train_iter_loss: 0.30827605724334717
train_iter_loss: 0.2975289821624756
train_iter_loss: 0.4684787690639496
train_iter_loss: 0.28132733702659607
train_iter_loss: 0.21009305119514465
train_iter_loss: 0.06579640507698059
train_iter_loss: 0.3133390545845032
train_iter_loss: 0.2939468324184418
train_iter_loss: 0.3014552593231201
train_iter_loss: 0.43806612491607666
train_iter_loss: 0.25703680515289307
train_iter_loss: 0.1736409068107605
train_iter_loss: 0.36690980195999146
train_iter_loss: 0.26901423931121826
train_iter_loss: 0.12451934069395065
train_iter_loss: 0.22644966840744019
train_iter_loss: 0.19148816168308258
train_iter_loss: 0.2513539791107178
train_iter_loss: 0.3471575379371643
train_iter_loss: 0.2905105650424957
train_iter_loss: 0.532034695148468
train_iter_loss: 0.4726683795452118
train_iter_loss: 0.18865551054477692
train_iter_loss: 0.36713042855262756
train_iter_loss: 0.2255571335554123
train_iter_loss: 0.2980421483516693
train_iter_loss: 0.14349527657032013
train_iter_loss: 0.2577202022075653
train loss :0.2813
---------------------
Validation seg loss: 0.36340251510505966 at epoch 284
epoch =    285/  1000, exp = train
train_iter_loss: 0.21465805172920227
train_iter_loss: 0.2024202197790146
train_iter_loss: 0.2935924530029297
train_iter_loss: 0.2940690815448761
train_iter_loss: 0.2954844534397125
train_iter_loss: 0.1839660108089447
train_iter_loss: 0.33551955223083496
train_iter_loss: 0.2189878672361374
train_iter_loss: 0.17091426253318787
train_iter_loss: 0.48864758014678955
train_iter_loss: 0.2646270990371704
train_iter_loss: 0.24427680671215057
train_iter_loss: 0.21292757987976074
train_iter_loss: 0.2065257430076599
train_iter_loss: 0.24055030941963196
train_iter_loss: 0.28682810068130493
train_iter_loss: 0.25375649333000183
train_iter_loss: 0.37831610441207886
train_iter_loss: 0.34228238463401794
train_iter_loss: 0.2909198999404907
train_iter_loss: 0.31107380986213684
train_iter_loss: 0.32505670189857483
train_iter_loss: 0.3225381076335907
train_iter_loss: 0.36447903513908386
train_iter_loss: 0.23118381202220917
train_iter_loss: 0.3292771279811859
train_iter_loss: 0.18861711025238037
train_iter_loss: 0.16619759798049927
train_iter_loss: 0.4547765552997589
train_iter_loss: 0.22328317165374756
train_iter_loss: 0.2955320179462433
train_iter_loss: 0.19067074358463287
train_iter_loss: 0.21715471148490906
train_iter_loss: 0.3904164731502533
train_iter_loss: 0.31430384516716003
train_iter_loss: 0.3314029276371002
train_iter_loss: 0.31272032856941223
train_iter_loss: 0.2863277792930603
train_iter_loss: 0.2954503893852234
train_iter_loss: 0.11840356886386871
train_iter_loss: 0.4238719344139099
train_iter_loss: 0.25182175636291504
train_iter_loss: 0.25923600792884827
train_iter_loss: 0.16819553077220917
train_iter_loss: 0.2257957011461258
train_iter_loss: 0.39713677763938904
train_iter_loss: 0.3299085199832916
train_iter_loss: 0.1674617975950241
train_iter_loss: 0.26570937037467957
train_iter_loss: 0.34803009033203125
train_iter_loss: 0.359165757894516
train_iter_loss: 0.22621290385723114
train_iter_loss: 0.24184650182724
train_iter_loss: 0.24646952748298645
train_iter_loss: 0.13152843713760376
train_iter_loss: 0.27325570583343506
train_iter_loss: 0.4773310124874115
train_iter_loss: 0.19366231560707092
train_iter_loss: 0.1987171769142151
train_iter_loss: 0.3315064013004303
train_iter_loss: 0.40593594312667847
train_iter_loss: 0.19115670025348663
train_iter_loss: 0.3851087689399719
train_iter_loss: 0.3074985444545746
train_iter_loss: 0.22106561064720154
train_iter_loss: 0.20725645124912262
train_iter_loss: 0.2749979794025421
train_iter_loss: 0.12218695133924484
train_iter_loss: 0.3029877841472626
train_iter_loss: 0.14424361288547516
train_iter_loss: 0.31728747487068176
train_iter_loss: 0.23829853534698486
train_iter_loss: 0.305375874042511
train_iter_loss: 0.5054499506950378
train_iter_loss: 0.45294058322906494
train_iter_loss: 0.2039705514907837
train_iter_loss: 0.2831067144870758
train_iter_loss: 0.1793852150440216
train_iter_loss: 0.20153765380382538
train_iter_loss: 0.33940601348876953
train_iter_loss: 0.23581166565418243
train_iter_loss: 0.15809056162834167
train_iter_loss: 0.35634681582450867
train_iter_loss: 0.24193604290485382
train_iter_loss: 0.4266345798969269
train_iter_loss: 0.33541080355644226
train_iter_loss: 0.29056933522224426
train_iter_loss: 0.24710124731063843
train_iter_loss: 0.3292362689971924
train_iter_loss: 0.14459624886512756
train_iter_loss: 0.3792489767074585
train_iter_loss: 0.5121974349021912
train_iter_loss: 0.3786146938800812
train_iter_loss: 0.5202591419219971
train_iter_loss: 0.30704402923583984
train_iter_loss: 0.2950916886329651
train_iter_loss: 0.18867236375808716
train_iter_loss: 0.22365178167819977
train_iter_loss: 0.08587165176868439
train_iter_loss: 0.26389485597610474
train loss :0.2840
---------------------
Validation seg loss: 0.3589538104199576 at epoch 285
epoch =    286/  1000, exp = train
train_iter_loss: 0.31374940276145935
train_iter_loss: 0.3506218194961548
train_iter_loss: 0.33349961042404175
train_iter_loss: 0.22270925343036652
train_iter_loss: 0.35001349449157715
train_iter_loss: 0.2972625494003296
train_iter_loss: 0.2611066997051239
train_iter_loss: 0.29921942949295044
train_iter_loss: 0.16040368378162384
train_iter_loss: 0.3381476402282715
train_iter_loss: 0.48471611738204956
train_iter_loss: 0.2743937373161316
train_iter_loss: 0.2847348153591156
train_iter_loss: 0.2586742043495178
train_iter_loss: 0.280219703912735
train_iter_loss: 0.4713006019592285
train_iter_loss: 0.20982177555561066
train_iter_loss: 0.28552913665771484
train_iter_loss: 0.22414663434028625
train_iter_loss: 0.09878285974264145
train_iter_loss: 0.22361838817596436
train_iter_loss: 0.12353899329900742
train_iter_loss: 0.17298392951488495
train_iter_loss: 0.11854507774114609
train_iter_loss: 0.2576654553413391
train_iter_loss: 0.2721143662929535
train_iter_loss: 0.3444124162197113
train_iter_loss: 0.26974552869796753
train_iter_loss: 0.31076210737228394
train_iter_loss: 0.1998901218175888
train_iter_loss: 0.3020222783088684
train_iter_loss: 0.454971045255661
train_iter_loss: 0.5302141308784485
train_iter_loss: 0.2075193077325821
train_iter_loss: 0.27200421690940857
train_iter_loss: 0.24819844961166382
train_iter_loss: 0.45830726623535156
train_iter_loss: 0.2294650673866272
train_iter_loss: 0.3131136894226074
train_iter_loss: 0.17581193149089813
train_iter_loss: 0.37187016010284424
train_iter_loss: 0.3938387930393219
train_iter_loss: 0.2710997760295868
train_iter_loss: 0.10931064933538437
train_iter_loss: 0.2624354362487793
train_iter_loss: 0.20878048241138458
train_iter_loss: 0.1254073679447174
train_iter_loss: 0.14277823269367218
train_iter_loss: 0.2835935652256012
train_iter_loss: 0.24376623332500458
train_iter_loss: 0.40590253472328186
train_iter_loss: 0.29432475566864014
train_iter_loss: 0.32220736145973206
train_iter_loss: 0.20858681201934814
train_iter_loss: 0.33057862520217896
train_iter_loss: 0.2769608199596405
train_iter_loss: 0.23294062912464142
train_iter_loss: 0.24920165538787842
train_iter_loss: 0.25143560767173767
train_iter_loss: 0.34783807396888733
train_iter_loss: 0.09178498387336731
train_iter_loss: 0.39691823720932007
train_iter_loss: 0.24659524857997894
train_iter_loss: 0.46300461888313293
train_iter_loss: 0.2818271219730377
train_iter_loss: 0.25807642936706543
train_iter_loss: 0.21680289506912231
train_iter_loss: 0.3195395767688751
train_iter_loss: 0.3559318482875824
train_iter_loss: 0.3590441942214966
train_iter_loss: 0.10170073062181473
train_iter_loss: 0.42413732409477234
train_iter_loss: 0.2797394096851349
train_iter_loss: 0.2734558582305908
train_iter_loss: 0.23627258837223053
train_iter_loss: 0.3009980618953705
train_iter_loss: 0.30630382895469666
train_iter_loss: 0.3158459961414337
train_iter_loss: 0.193101167678833
train_iter_loss: 0.1575613170862198
train_iter_loss: 0.44016435742378235
train_iter_loss: 0.3119666278362274
train_iter_loss: 0.314421147108078
train_iter_loss: 0.2222042977809906
train_iter_loss: 0.3259505033493042
train_iter_loss: 0.323203444480896
train_iter_loss: 0.3397793471813202
train_iter_loss: 0.2727024555206299
train_iter_loss: 0.2472078502178192
train_iter_loss: 0.29650625586509705
train_iter_loss: 0.23697516322135925
train_iter_loss: 0.26945626735687256
train_iter_loss: 0.2872013747692108
train_iter_loss: 0.298843652009964
train_iter_loss: 0.46876898407936096
train_iter_loss: 0.2467334270477295
train_iter_loss: 0.26804473996162415
train_iter_loss: 0.2217607945203781
train_iter_loss: 0.29148074984550476
train_iter_loss: 0.12949983775615692
train loss :0.2829
---------------------
Validation seg loss: 0.4304185786266934 at epoch 286
epoch =    287/  1000, exp = train
train_iter_loss: 0.29658931493759155
train_iter_loss: 0.3740452527999878
train_iter_loss: 0.3599856197834015
train_iter_loss: 0.4018886387348175
train_iter_loss: 0.25927600264549255
train_iter_loss: 0.20000728964805603
train_iter_loss: 0.234693706035614
train_iter_loss: 0.20936071872711182
train_iter_loss: 0.09525688737630844
train_iter_loss: 0.18498270213603973
train_iter_loss: 0.2870718836784363
train_iter_loss: 0.2960388958454132
train_iter_loss: 0.24953681230545044
train_iter_loss: 0.23392584919929504
train_iter_loss: 0.41706252098083496
train_iter_loss: 0.19144827127456665
train_iter_loss: 0.327603816986084
train_iter_loss: 0.29960891604423523
train_iter_loss: 0.2447158247232437
train_iter_loss: 0.3988448977470398
train_iter_loss: 0.19512878358364105
train_iter_loss: 0.3819120526313782
train_iter_loss: 0.4134598970413208
train_iter_loss: 0.14479757845401764
train_iter_loss: 0.1578034907579422
train_iter_loss: 0.2901787757873535
train_iter_loss: 0.3944299817085266
train_iter_loss: 0.11357968300580978
train_iter_loss: 0.14793363213539124
train_iter_loss: 0.32488903403282166
train_iter_loss: 0.43673017621040344
train_iter_loss: 0.23865684866905212
train_iter_loss: 0.33425065875053406
train_iter_loss: 0.3020663857460022
train_iter_loss: 0.2298639416694641
train_iter_loss: 0.24413923919200897
train_iter_loss: 0.25539013743400574
train_iter_loss: 0.34825417399406433
train_iter_loss: 0.19143792986869812
train_iter_loss: 0.19946862757205963
train_iter_loss: 0.31249192357063293
train_iter_loss: 0.32897621393203735
train_iter_loss: 0.1539636254310608
train_iter_loss: 0.1966661661863327
train_iter_loss: 0.18335257470607758
train_iter_loss: 0.33765554428100586
train_iter_loss: 0.4170801639556885
train_iter_loss: 0.3226991295814514
train_iter_loss: 0.3279111087322235
train_iter_loss: 0.3653898239135742
train_iter_loss: 0.45770663022994995
train_iter_loss: 0.2452908456325531
train_iter_loss: 0.2362440675497055
train_iter_loss: 0.35210973024368286
train_iter_loss: 0.43380919098854065
train_iter_loss: 0.14464232325553894
train_iter_loss: 0.34216734766960144
train_iter_loss: 0.2474827915430069
train_iter_loss: 0.4120865762233734
train_iter_loss: 0.20955225825309753
train_iter_loss: 0.15162307024002075
train_iter_loss: 0.3263046443462372
train_iter_loss: 0.14508217573165894
train_iter_loss: 0.21992604434490204
train_iter_loss: 0.267479807138443
train_iter_loss: 0.27606073021888733
train_iter_loss: 0.3576529622077942
train_iter_loss: 0.33182328939437866
train_iter_loss: 0.2804652452468872
train_iter_loss: 0.27923405170440674
train_iter_loss: 0.1703590750694275
train_iter_loss: 0.1414494812488556
train_iter_loss: 0.1973925530910492
train_iter_loss: 0.3992907702922821
train_iter_loss: 0.14937958121299744
train_iter_loss: 0.27888554334640503
train_iter_loss: 0.38738399744033813
train_iter_loss: 0.21311382949352264
train_iter_loss: 0.18264195322990417
train_iter_loss: 0.330512672662735
train_iter_loss: 0.3076794445514679
train_iter_loss: 0.2777922749519348
train_iter_loss: 0.2635315954685211
train_iter_loss: 0.2220665067434311
train_iter_loss: 0.20245994627475739
train_iter_loss: 0.27546578645706177
train_iter_loss: 0.30101993680000305
train_iter_loss: 0.5153220891952515
train_iter_loss: 0.29647141695022583
train_iter_loss: 0.20583432912826538
train_iter_loss: 0.42461729049682617
train_iter_loss: 0.1379506140947342
train_iter_loss: 0.24830107390880585
train_iter_loss: 0.27895861864089966
train_iter_loss: 0.32919979095458984
train_iter_loss: 0.29963165521621704
train_iter_loss: 0.35455626249313354
train_iter_loss: 0.2727581858634949
train_iter_loss: 0.343599796295166
train_iter_loss: 0.2562445402145386
train loss :0.2812
---------------------
Validation seg loss: 0.4018280693804318 at epoch 287
epoch =    288/  1000, exp = train
train_iter_loss: 0.13716690242290497
train_iter_loss: 0.3124418258666992
train_iter_loss: 0.19643434882164001
train_iter_loss: 0.3552429974079132
train_iter_loss: 0.3713327646255493
train_iter_loss: 0.38450631499290466
train_iter_loss: 0.22092783451080322
train_iter_loss: 0.23812970519065857
train_iter_loss: 0.48167654871940613
train_iter_loss: 0.34972307085990906
train_iter_loss: 0.07779563963413239
train_iter_loss: 0.23576620221138
train_iter_loss: 0.1644067019224167
train_iter_loss: 0.26934120059013367
train_iter_loss: 0.23078560829162598
train_iter_loss: 0.05577889829874039
train_iter_loss: 0.48219242691993713
train_iter_loss: 0.19474908709526062
train_iter_loss: 0.24608978629112244
train_iter_loss: 0.288756787776947
train_iter_loss: 0.33293384313583374
train_iter_loss: 0.1914016306400299
train_iter_loss: 0.21058137714862823
train_iter_loss: 0.38123226165771484
train_iter_loss: 0.2998340129852295
train_iter_loss: 0.2729964852333069
train_iter_loss: 0.34485313296318054
train_iter_loss: 0.4624091684818268
train_iter_loss: 0.30205538868904114
train_iter_loss: 0.2001323401927948
train_iter_loss: 0.41869738698005676
train_iter_loss: 0.3473329246044159
train_iter_loss: 0.31353726983070374
train_iter_loss: 0.37477418780326843
train_iter_loss: 0.30391308665275574
train_iter_loss: 0.27287960052490234
train_iter_loss: 0.2885579466819763
train_iter_loss: 0.16484558582305908
train_iter_loss: 0.2322540134191513
train_iter_loss: 0.23770084977149963
train_iter_loss: 0.2913205325603485
train_iter_loss: 0.1046581044793129
train_iter_loss: 0.18475329875946045
train_iter_loss: 0.3183191120624542
train_iter_loss: 0.3163294196128845
train_iter_loss: 0.25695914030075073
train_iter_loss: 0.5049689412117004
train_iter_loss: 0.3225357234477997
train_iter_loss: 0.3753507137298584
train_iter_loss: 0.2637403607368469
train_iter_loss: 0.38590872287750244
train_iter_loss: 0.3183375895023346
train_iter_loss: 0.2783072292804718
train_iter_loss: 0.2155309021472931
train_iter_loss: 0.21653413772583008
train_iter_loss: 0.24278829991817474
train_iter_loss: 0.34398794174194336
train_iter_loss: 0.17907938361167908
train_iter_loss: 0.3425358235836029
train_iter_loss: 0.24329951405525208
train_iter_loss: 0.2868514060974121
train_iter_loss: 0.14537784457206726
train_iter_loss: 0.4046964645385742
train_iter_loss: 0.12662960588932037
train_iter_loss: 0.30612677335739136
train_iter_loss: 0.31995484232902527
train_iter_loss: 0.5479487180709839
train_iter_loss: 0.23914363980293274
train_iter_loss: 0.24283340573310852
train_iter_loss: 0.25635290145874023
train_iter_loss: 0.14920639991760254
train_iter_loss: 0.28695055842399597
train_iter_loss: 0.19131790101528168
train_iter_loss: 0.41119107604026794
train_iter_loss: 0.2695419490337372
train_iter_loss: 0.19506162405014038
train_iter_loss: 0.23250356316566467
train_iter_loss: 0.21133653819561005
train_iter_loss: 0.4145612418651581
train_iter_loss: 0.24448218941688538
train_iter_loss: 0.49425575137138367
train_iter_loss: 0.20193113386631012
train_iter_loss: 0.28290462493896484
train_iter_loss: 0.35848841071128845
train_iter_loss: 0.31079497933387756
train_iter_loss: 0.25055453181266785
train_iter_loss: 0.33162790536880493
train_iter_loss: 0.24664457142353058
train_iter_loss: 0.26008695363998413
train_iter_loss: 0.3196825087070465
train_iter_loss: 0.39518067240715027
train_iter_loss: 0.3426799178123474
train_iter_loss: 0.39802953600883484
train_iter_loss: 0.33962759375572205
train_iter_loss: 0.15622766315937042
train_iter_loss: 0.30906370282173157
train_iter_loss: 0.18099814653396606
train_iter_loss: 0.16411913931369781
train_iter_loss: 0.26113954186439514
train_iter_loss: 0.18307967483997345
train loss :0.2850
---------------------
Validation seg loss: 0.3862569940381877 at epoch 288
epoch =    289/  1000, exp = train
train_iter_loss: 0.2504938542842865
train_iter_loss: 0.23171797394752502
train_iter_loss: 0.1775810569524765
train_iter_loss: 0.2870779037475586
train_iter_loss: 0.3139830231666565
train_iter_loss: 0.29578208923339844
train_iter_loss: 0.1051778644323349
train_iter_loss: 0.35860639810562134
train_iter_loss: 0.2939188480377197
train_iter_loss: 0.38059860467910767
train_iter_loss: 0.2607482969760895
train_iter_loss: 0.49437415599823
train_iter_loss: 0.15217053890228271
train_iter_loss: 0.30465564131736755
train_iter_loss: 0.5415667295455933
train_iter_loss: 0.5996997952461243
train_iter_loss: 0.267394483089447
train_iter_loss: 0.23734338581562042
train_iter_loss: 0.34631332755088806
train_iter_loss: 0.22127050161361694
train_iter_loss: 0.1491323560476303
train_iter_loss: 0.3160794973373413
train_iter_loss: 0.2982758581638336
train_iter_loss: 0.18778687715530396
train_iter_loss: 0.1523011475801468
train_iter_loss: 0.40566301345825195
train_iter_loss: 0.33148694038391113
train_iter_loss: 0.10463783144950867
train_iter_loss: 0.2878897190093994
train_iter_loss: 0.20034553110599518
train_iter_loss: 0.37164607644081116
train_iter_loss: 0.2929626405239105
train_iter_loss: 0.14960642158985138
train_iter_loss: 0.582838237285614
train_iter_loss: 0.3143158555030823
train_iter_loss: 0.2433527410030365
train_iter_loss: 0.2913365364074707
train_iter_loss: 0.13683369755744934
train_iter_loss: 0.3847576379776001
train_iter_loss: 0.20026320219039917
train_iter_loss: 0.2211754471063614
train_iter_loss: 0.23600761592388153
train_iter_loss: 0.41327062249183655
train_iter_loss: 0.19566114246845245
train_iter_loss: 0.3522052764892578
train_iter_loss: 0.35138392448425293
train_iter_loss: 0.407765656709671
train_iter_loss: 0.14260396361351013
train_iter_loss: 0.34964126348495483
train_iter_loss: 0.26782506704330444
train_iter_loss: 0.19924603402614594
train_iter_loss: 0.12816482782363892
train_iter_loss: 0.17502084374427795
train_iter_loss: 0.2385130673646927
train_iter_loss: 0.22402741014957428
train_iter_loss: 0.21202680468559265
train_iter_loss: 0.35492828488349915
train_iter_loss: 0.41729292273521423
train_iter_loss: 0.6405596733093262
train_iter_loss: 0.24299083650112152
train_iter_loss: 0.32985976338386536
train_iter_loss: 0.2192675918340683
train_iter_loss: 0.2448406219482422
train_iter_loss: 0.26985788345336914
train_iter_loss: 0.5397613048553467
train_iter_loss: 0.37209340929985046
train_iter_loss: 0.35149717330932617
train_iter_loss: 0.24345707893371582
train_iter_loss: 0.27251818776130676
train_iter_loss: 0.2383352667093277
train_iter_loss: 0.3829452693462372
train_iter_loss: 0.29176321625709534
train_iter_loss: 0.1998613327741623
train_iter_loss: 0.13895060122013092
train_iter_loss: 0.22209560871124268
train_iter_loss: 0.39999639987945557
train_iter_loss: 0.21445824205875397
train_iter_loss: 0.10500381141901016
train_iter_loss: 0.35024839639663696
train_iter_loss: 0.23759540915489197
train_iter_loss: 0.21403682231903076
train_iter_loss: 0.25989529490470886
train_iter_loss: 0.28589189052581787
train_iter_loss: 0.27691587805747986
train_iter_loss: 0.14067132771015167
train_iter_loss: 0.21346613764762878
train_iter_loss: 0.10570399463176727
train_iter_loss: 0.29191040992736816
train_iter_loss: 0.2155076414346695
train_iter_loss: 0.17501585185527802
train_iter_loss: 0.20160537958145142
train_iter_loss: 0.40406307578086853
train_iter_loss: 0.1740923970937729
train_iter_loss: 0.48516857624053955
train_iter_loss: 0.24805958569049835
train_iter_loss: 0.25957396626472473
train_iter_loss: 0.2988317012786865
train_iter_loss: 0.3073558807373047
train_iter_loss: 0.32146143913269043
train_iter_loss: 0.35590311884880066
train loss :0.2844
---------------------
Validation seg loss: 0.38173435265549793 at epoch 289
epoch =    290/  1000, exp = train
train_iter_loss: 0.38604360818862915
train_iter_loss: 0.23511341214179993
train_iter_loss: 0.28611209988594055
train_iter_loss: 0.39436811208724976
train_iter_loss: 0.28427061438560486
train_iter_loss: 0.3311648666858673
train_iter_loss: 0.41141682863235474
train_iter_loss: 0.35612910985946655
train_iter_loss: 0.22803136706352234
train_iter_loss: 0.22357161343097687
train_iter_loss: 0.24194183945655823
train_iter_loss: 0.19875548779964447
train_iter_loss: 0.23881633579730988
train_iter_loss: 0.49355849623680115
train_iter_loss: 0.2513729929924011
train_iter_loss: 0.3498823344707489
train_iter_loss: 0.3050845265388489
train_iter_loss: 0.3307464122772217
train_iter_loss: 0.3762369155883789
train_iter_loss: 0.18234676122665405
train_iter_loss: 0.40251097083091736
train_iter_loss: 0.37753593921661377
train_iter_loss: 0.29099753499031067
train_iter_loss: 0.4146650433540344
train_iter_loss: 0.3543039560317993
train_iter_loss: 0.38693946599960327
train_iter_loss: 0.369125634431839
train_iter_loss: 0.3344912827014923
train_iter_loss: 0.3498058319091797
train_iter_loss: 0.36015772819519043
train_iter_loss: 0.1728813499212265
train_iter_loss: 0.22918085753917694
train_iter_loss: 0.3105294108390808
train_iter_loss: 0.16396254301071167
train_iter_loss: 0.1476418524980545
train_iter_loss: 0.3422541916370392
train_iter_loss: 0.11992952972650528
train_iter_loss: 0.12895214557647705
train_iter_loss: 0.30198904871940613
train_iter_loss: 0.3285842537879944
train_iter_loss: 0.35374870896339417
train_iter_loss: 0.2909330427646637
train_iter_loss: 0.25618627667427063
train_iter_loss: 0.35699141025543213
train_iter_loss: 0.21494553983211517
train_iter_loss: 0.25580528378486633
train_iter_loss: 0.11925198137760162
train_iter_loss: 0.2434447705745697
train_iter_loss: 0.211983785033226
train_iter_loss: 0.11151431500911713
train_iter_loss: 0.1918514221906662
train_iter_loss: 0.2977517247200012
train_iter_loss: 0.2774305045604706
train_iter_loss: 0.28322848677635193
train_iter_loss: 0.3035455346107483
train_iter_loss: 0.3684001863002777
train_iter_loss: 0.514490008354187
train_iter_loss: 0.26238295435905457
train_iter_loss: 0.32223349809646606
train_iter_loss: 0.21502625942230225
train_iter_loss: 0.31843119859695435
train_iter_loss: 0.35011839866638184
train_iter_loss: 0.2419607788324356
train_iter_loss: 0.29996734857559204
train_iter_loss: 0.3410544991493225
train_iter_loss: 0.49131542444229126
train_iter_loss: 0.3116885721683502
train_iter_loss: 0.3777691125869751
train_iter_loss: 0.32051461935043335
train_iter_loss: 0.24107244610786438
train_iter_loss: 0.25211215019226074
train_iter_loss: 0.2616278827190399
train_iter_loss: 0.3255712389945984
train_iter_loss: 0.21051381528377533
train_iter_loss: 0.3906519412994385
train_iter_loss: 0.4413682818412781
train_iter_loss: 0.22811923921108246
train_iter_loss: 0.3208405673503876
train_iter_loss: 0.24424785375595093
train_iter_loss: 0.2567085027694702
train_iter_loss: 0.24861691892147064
train_iter_loss: 0.23218806087970734
train_iter_loss: 0.27004775404930115
train_iter_loss: 0.3934772312641144
train_iter_loss: 0.1477249264717102
train_iter_loss: 0.21730810403823853
train_iter_loss: 0.4045211672782898
train_iter_loss: 0.31941282749176025
train_iter_loss: 0.236728236079216
train_iter_loss: 0.06911232322454453
train_iter_loss: 0.31578686833381653
train_iter_loss: 0.2364128977060318
train_iter_loss: 0.3244919776916504
train_iter_loss: 0.35766398906707764
train_iter_loss: 0.4598489999771118
train_iter_loss: 0.21499671041965485
train_iter_loss: 0.2775505483150482
train_iter_loss: 0.17222805321216583
train_iter_loss: 0.23677965998649597
train_iter_loss: 0.23882192373275757
train loss :0.2933
---------------------
Validation seg loss: 0.4186138499106439 at epoch 290
epoch =    291/  1000, exp = train
train_iter_loss: 0.3038097620010376
train_iter_loss: 0.2921856641769409
train_iter_loss: 0.19233013689517975
train_iter_loss: 0.24864721298217773
train_iter_loss: 0.300025075674057
train_iter_loss: 0.2505576014518738
train_iter_loss: 0.309259295463562
train_iter_loss: 0.25628140568733215
train_iter_loss: 0.31842562556266785
train_iter_loss: 0.3541474938392639
train_iter_loss: 0.2767002582550049
train_iter_loss: 0.2901897728443146
train_iter_loss: 0.11837030202150345
train_iter_loss: 0.18685324490070343
train_iter_loss: 0.14399950206279755
train_iter_loss: 0.2953449785709381
train_iter_loss: 0.3245556354522705
train_iter_loss: 0.32867079973220825
train_iter_loss: 0.1796559989452362
train_iter_loss: 0.126596599817276
train_iter_loss: 0.3191079795360565
train_iter_loss: 0.21800531446933746
train_iter_loss: 0.1456819325685501
train_iter_loss: 0.45161327719688416
train_iter_loss: 0.2950858771800995
train_iter_loss: 0.45421072840690613
train_iter_loss: 0.3892805874347687
train_iter_loss: 0.2388668954372406
train_iter_loss: 0.2265268713235855
train_iter_loss: 0.2126978635787964
train_iter_loss: 0.2525767683982849
train_iter_loss: 0.3156875669956207
train_iter_loss: 0.2906506657600403
train_iter_loss: 0.28092652559280396
train_iter_loss: 0.24669262766838074
train_iter_loss: 0.17058110237121582
train_iter_loss: 0.13545888662338257
train_iter_loss: 0.2479371577501297
train_iter_loss: 0.349872350692749
train_iter_loss: 0.2098151594400406
train_iter_loss: 0.1694425493478775
train_iter_loss: 0.28581130504608154
train_iter_loss: 0.08585698157548904
train_iter_loss: 0.2192254215478897
train_iter_loss: 0.16418886184692383
train_iter_loss: 0.3635618984699249
train_iter_loss: 0.3312057852745056
train_iter_loss: 0.4517791271209717
train_iter_loss: 0.21453669667243958
train_iter_loss: 0.5486621260643005
train_iter_loss: 0.1784863919019699
train_iter_loss: 0.253706693649292
train_iter_loss: 0.3411903977394104
train_iter_loss: 0.29994237422943115
train_iter_loss: 0.2446252703666687
train_iter_loss: 0.32137495279312134
train_iter_loss: 0.2221488356590271
train_iter_loss: 0.32359427213668823
train_iter_loss: 0.35930532217025757
train_iter_loss: 0.345240980386734
train_iter_loss: 0.25857722759246826
train_iter_loss: 0.22337035834789276
train_iter_loss: 0.3634660243988037
train_iter_loss: 0.12936361134052277
train_iter_loss: 0.18178267776966095
train_iter_loss: 0.5087105631828308
train_iter_loss: 0.2888501286506653
train_iter_loss: 0.28478071093559265
train_iter_loss: 0.23842158913612366
train_iter_loss: 0.2453640103340149
train_iter_loss: 0.3663023114204407
train_iter_loss: 0.22152221202850342
train_iter_loss: 0.27499184012413025
train_iter_loss: 0.3249390721321106
train_iter_loss: 0.37170732021331787
train_iter_loss: 0.23979264497756958
train_iter_loss: 0.26642316579818726
train_iter_loss: 0.22304172813892365
train_iter_loss: 0.21556037664413452
train_iter_loss: 0.3267017900943756
train_iter_loss: 0.4038269817829132
train_iter_loss: 0.23804828524589539
train_iter_loss: 0.2613176703453064
train_iter_loss: 0.3445298969745636
train_iter_loss: 0.4596776068210602
train_iter_loss: 0.3536418080329895
train_iter_loss: 0.2915930151939392
train_iter_loss: 0.3132237493991852
train_iter_loss: 0.3223683834075928
train_iter_loss: 0.22905515134334564
train_iter_loss: 0.29354435205459595
train_iter_loss: 0.22021183371543884
train_iter_loss: 0.2021699994802475
train_iter_loss: 0.3687003552913666
train_iter_loss: 0.2948535978794098
train_iter_loss: 0.2771715521812439
train_iter_loss: 0.18096250295639038
train_iter_loss: 0.20872189104557037
train_iter_loss: 0.25404319167137146
train_iter_loss: 0.18804709613323212
train loss :0.2782
---------------------
Validation seg loss: 0.4054715965505479 at epoch 291
epoch =    292/  1000, exp = train
train_iter_loss: 0.3274138867855072
train_iter_loss: 0.24911168217658997
train_iter_loss: 0.4412490725517273
train_iter_loss: 0.2830812335014343
train_iter_loss: 0.3353538513183594
train_iter_loss: 0.2601366937160492
train_iter_loss: 0.4401405155658722
train_iter_loss: 0.2907888889312744
train_iter_loss: 0.23522351682186127
train_iter_loss: 0.2504405975341797
train_iter_loss: 0.3483148217201233
train_iter_loss: 0.19662366807460785
train_iter_loss: 0.3954836428165436
train_iter_loss: 0.3438017666339874
train_iter_loss: 0.31486913561820984
train_iter_loss: 0.24849265813827515
train_iter_loss: 0.2827073931694031
train_iter_loss: 0.23763981461524963
train_iter_loss: 0.23385705053806305
train_iter_loss: 0.28596243262290955
train_iter_loss: 0.32322555780410767
train_iter_loss: 0.16150043904781342
train_iter_loss: 0.1820017397403717
train_iter_loss: 0.3698744773864746
train_iter_loss: 0.2622661888599396
train_iter_loss: 0.35280200839042664
train_iter_loss: 0.09072753041982651
train_iter_loss: 0.26041078567504883
train_iter_loss: 0.3410649597644806
train_iter_loss: 0.25212618708610535
train_iter_loss: 0.23326480388641357
train_iter_loss: 0.319789856672287
train_iter_loss: 0.14034199714660645
train_iter_loss: 0.3094440698623657
train_iter_loss: 0.31065985560417175
train_iter_loss: 0.44338110089302063
train_iter_loss: 0.3008083701133728
train_iter_loss: 0.38213440775871277
train_iter_loss: 0.29284653067588806
train_iter_loss: 0.1967879831790924
train_iter_loss: 0.22086459398269653
train_iter_loss: 0.2301478236913681
train_iter_loss: 0.40933850407600403
train_iter_loss: 0.2742629945278168
train_iter_loss: 0.3633083999156952
train_iter_loss: 0.2605372965335846
train_iter_loss: 0.36109787225723267
train_iter_loss: 0.2505132555961609
train_iter_loss: 0.48082372546195984
train_iter_loss: 0.20050126314163208
train_iter_loss: 0.3494859039783478
train_iter_loss: 0.4475645124912262
train_iter_loss: 0.2569168508052826
train_iter_loss: 0.23074544966220856
train_iter_loss: 0.25712934136390686
train_iter_loss: 0.15245123207569122
train_iter_loss: 0.20138752460479736
train_iter_loss: 0.21163195371627808
train_iter_loss: 0.3131367862224579
train_iter_loss: 0.29731231927871704
train_iter_loss: 0.21570253372192383
train_iter_loss: 0.18844997882843018
train_iter_loss: 0.2153363972902298
train_iter_loss: 0.2626745402812958
train_iter_loss: 0.20912079513072968
train_iter_loss: 0.22939662635326385
train_iter_loss: 0.4204588532447815
train_iter_loss: 0.2740614116191864
train_iter_loss: 0.3622311055660248
train_iter_loss: 0.3888089060783386
train_iter_loss: 0.2928191125392914
train_iter_loss: 0.269290953874588
train_iter_loss: 0.2856557071208954
train_iter_loss: 0.17428654432296753
train_iter_loss: 0.3612203896045685
train_iter_loss: 0.1601017713546753
train_iter_loss: 0.2839634418487549
train_iter_loss: 0.4111773669719696
train_iter_loss: 0.3041304647922516
train_iter_loss: 0.2989036440849304
train_iter_loss: 0.24514396488666534
train_iter_loss: 0.3264658451080322
train_iter_loss: 0.6157909035682678
train_iter_loss: 0.3397921323776245
train_iter_loss: 0.46615302562713623
train_iter_loss: 0.2551787197589874
train_iter_loss: 0.22152745723724365
train_iter_loss: 0.26327890157699585
train_iter_loss: 0.28081899881362915
train_iter_loss: 0.3677358329296112
train_iter_loss: 0.2942492365837097
train_iter_loss: 0.18769672513008118
train_iter_loss: 0.3990878462791443
train_iter_loss: 0.1745273917913437
train_iter_loss: 0.12315242737531662
train_iter_loss: 0.15541672706604004
train_iter_loss: 0.34048017859458923
train_iter_loss: 0.24320483207702637
train_iter_loss: 0.17649753391742706
train_iter_loss: 0.25281140208244324
train loss :0.2891
---------------------
Validation seg loss: 0.402330341002078 at epoch 292
epoch =    293/  1000, exp = train
train_iter_loss: 0.3470321297645569
train_iter_loss: 0.2387004941701889
train_iter_loss: 0.1267179399728775
train_iter_loss: 0.07416138052940369
train_iter_loss: 0.3362675607204437
train_iter_loss: 0.35457926988601685
train_iter_loss: 0.3252358138561249
train_iter_loss: 0.29794424772262573
train_iter_loss: 0.281107634305954
train_iter_loss: 0.26039767265319824
train_iter_loss: 0.4343777298927307
train_iter_loss: 0.3380594551563263
train_iter_loss: 0.23126859962940216
train_iter_loss: 0.30261680483818054
train_iter_loss: 0.4257966876029968
train_iter_loss: 0.27952495217323303
train_iter_loss: 0.1540651023387909
train_iter_loss: 0.30217841267585754
train_iter_loss: 0.12117104232311249
train_iter_loss: 0.5794022083282471
train_iter_loss: 0.23945683240890503
train_iter_loss: 0.32352641224861145
train_iter_loss: 0.36058875918388367
train_iter_loss: 0.19179518520832062
train_iter_loss: 0.26309263706207275
train_iter_loss: 0.30820366740226746
train_iter_loss: 0.25139519572257996
train_iter_loss: 0.12904256582260132
train_iter_loss: 0.19205224514007568
train_iter_loss: 0.32678309082984924
train_iter_loss: 0.21355681121349335
train_iter_loss: 0.22769930958747864
train_iter_loss: 0.3012508749961853
train_iter_loss: 0.3621853291988373
train_iter_loss: 0.2495521903038025
train_iter_loss: 0.2541857659816742
train_iter_loss: 0.36472102999687195
train_iter_loss: 0.2687501907348633
train_iter_loss: 0.2810959815979004
train_iter_loss: 0.28185805678367615
train_iter_loss: 0.1506211906671524
train_iter_loss: 0.19949474930763245
train_iter_loss: 0.2778986394405365
train_iter_loss: 0.2625488042831421
train_iter_loss: 0.368432879447937
train_iter_loss: 0.181704580783844
train_iter_loss: 0.15679241716861725
train_iter_loss: 0.17209292948246002
train_iter_loss: 0.3769803047180176
train_iter_loss: 0.2490948885679245
train_iter_loss: 0.35056787729263306
train_iter_loss: 0.32146450877189636
train_iter_loss: 0.3016820549964905
train_iter_loss: 0.33653461933135986
train_iter_loss: 0.2906739413738251
train_iter_loss: 0.3491346538066864
train_iter_loss: 0.24417561292648315
train_iter_loss: 0.2555321156978607
train_iter_loss: 0.29413509368896484
train_iter_loss: 0.19381922483444214
train_iter_loss: 0.24187304079532623
train_iter_loss: 0.2463337928056717
train_iter_loss: 0.2641521990299225
train_iter_loss: 0.2463853657245636
train_iter_loss: 0.20541056990623474
train_iter_loss: 0.31679490208625793
train_iter_loss: 0.4310302436351776
train_iter_loss: 0.20763994753360748
train_iter_loss: 0.2154291421175003
train_iter_loss: 0.24805454909801483
train_iter_loss: 0.19621263444423676
train_iter_loss: 0.35019898414611816
train_iter_loss: 0.26292842626571655
train_iter_loss: 0.16132959723472595
train_iter_loss: 0.24535846710205078
train_iter_loss: 0.2607418894767761
train_iter_loss: 0.19850276410579681
train_iter_loss: 0.34289440512657166
train_iter_loss: 0.19565388560295105
train_iter_loss: 0.20525173842906952
train_iter_loss: 0.32140839099884033
train_iter_loss: 0.29474806785583496
train_iter_loss: 0.32202795147895813
train_iter_loss: 0.29527315497398376
train_iter_loss: 0.21746383607387543
train_iter_loss: 0.31679949164390564
train_iter_loss: 0.28077951073646545
train_iter_loss: 0.21662209928035736
train_iter_loss: 0.35297638177871704
train_iter_loss: 0.3735066056251526
train_iter_loss: 0.3246694505214691
train_iter_loss: 0.2849764823913574
train_iter_loss: 0.22849705815315247
train_iter_loss: 0.16427277028560638
train_iter_loss: 0.34035390615463257
train_iter_loss: 0.2787051796913147
train_iter_loss: 0.3182019591331482
train_iter_loss: 0.3135072886943817
train_iter_loss: 0.19463351368904114
train_iter_loss: 0.35379868745803833
train loss :0.2766
---------------------
Validation seg loss: 0.3760583713685848 at epoch 293
epoch =    294/  1000, exp = train
train_iter_loss: 0.21142949163913727
train_iter_loss: 0.2409876585006714
train_iter_loss: 0.3338961601257324
train_iter_loss: 0.327604740858078
train_iter_loss: 0.40406250953674316
train_iter_loss: 0.24452832341194153
train_iter_loss: 0.20981326699256897
train_iter_loss: 0.39001694321632385
train_iter_loss: 0.1351252943277359
train_iter_loss: 0.29829293489456177
train_iter_loss: 0.35880425572395325
train_iter_loss: 0.2675987780094147
train_iter_loss: 0.330009788274765
train_iter_loss: 0.3278971314430237
train_iter_loss: 0.33474934101104736
train_iter_loss: 0.25982245802879333
train_iter_loss: 0.22900250554084778
train_iter_loss: 0.25947296619415283
train_iter_loss: 0.2989468276500702
train_iter_loss: 0.22564734518527985
train_iter_loss: 0.2849053740501404
train_iter_loss: 0.32000112533569336
train_iter_loss: 0.27655863761901855
train_iter_loss: 0.19064611196517944
train_iter_loss: 0.22852304577827454
train_iter_loss: 0.12032171338796616
train_iter_loss: 0.3733469545841217
train_iter_loss: 0.3604954779148102
train_iter_loss: 0.1679914891719818
train_iter_loss: 0.3798762261867523
train_iter_loss: 0.26268714666366577
train_iter_loss: 0.3063980042934418
train_iter_loss: 0.3292773962020874
train_iter_loss: 0.29447057843208313
train_iter_loss: 0.5224089622497559
train_iter_loss: 0.2781662344932556
train_iter_loss: 0.3673333525657654
train_iter_loss: 0.2911774516105652
train_iter_loss: 0.25244081020355225
train_iter_loss: 0.3706642687320709
train_iter_loss: 0.3117934763431549
train_iter_loss: 0.32509008049964905
train_iter_loss: 0.23382346332073212
train_iter_loss: 0.24413418769836426
train_iter_loss: 0.15593194961547852
train_iter_loss: 0.45406854152679443
train_iter_loss: 0.16918718814849854
train_iter_loss: 0.34920719265937805
train_iter_loss: 0.2580532133579254
train_iter_loss: 0.4100606143474579
train_iter_loss: 0.19805417954921722
train_iter_loss: 0.3427945375442505
train_iter_loss: 0.13539566099643707
train_iter_loss: 0.36789992451667786
train_iter_loss: 0.15348465740680695
train_iter_loss: 0.20434345304965973
train_iter_loss: 0.15085794031620026
train_iter_loss: 0.23666246235370636
train_iter_loss: 0.1925947666168213
train_iter_loss: 0.23588374257087708
train_iter_loss: 0.2749723792076111
train_iter_loss: 0.30910754203796387
train_iter_loss: 0.12892766296863556
train_iter_loss: 0.2625640034675598
train_iter_loss: 0.24920131266117096
train_iter_loss: 0.2030210942029953
train_iter_loss: 0.29803141951560974
train_iter_loss: 0.2760566473007202
train_iter_loss: 0.3357042670249939
train_iter_loss: 0.3588297665119171
train_iter_loss: 0.44193699955940247
train_iter_loss: 0.16623438894748688
train_iter_loss: 0.23969373106956482
train_iter_loss: 0.24670249223709106
train_iter_loss: 0.215158149600029
train_iter_loss: 0.30848369002342224
train_iter_loss: 0.33117538690567017
train_iter_loss: 0.048367779701948166
train_iter_loss: 0.1901659071445465
train_iter_loss: 0.3138268291950226
train_iter_loss: 0.3192671239376068
train_iter_loss: 0.30918678641319275
train_iter_loss: 0.25526323914527893
train_iter_loss: 0.19879023730754852
train_iter_loss: 0.20363134145736694
train_iter_loss: 0.27585479617118835
train_iter_loss: 0.48298707604408264
train_iter_loss: 0.19563867151737213
train_iter_loss: 0.24465909600257874
train_iter_loss: 0.0970136746764183
train_iter_loss: 0.2599942088127136
train_iter_loss: 0.21110603213310242
train_iter_loss: 0.3833186626434326
train_iter_loss: 0.23698803782463074
train_iter_loss: 0.20399534702301025
train_iter_loss: 0.39655232429504395
train_iter_loss: 0.394246906042099
train_iter_loss: 0.30055373907089233
train_iter_loss: 0.37359705567359924
train_iter_loss: 0.25527504086494446
train loss :0.2787
---------------------
Validation seg loss: 0.4014274421991464 at epoch 294
epoch =    295/  1000, exp = train
train_iter_loss: 0.2899075746536255
train_iter_loss: 0.2815575897693634
train_iter_loss: 0.21849624812602997
train_iter_loss: 0.18428589403629303
train_iter_loss: 0.2884657382965088
train_iter_loss: 0.3415879011154175
train_iter_loss: 0.3906242251396179
train_iter_loss: 0.41324830055236816
train_iter_loss: 0.30851688981056213
train_iter_loss: 0.2263067364692688
train_iter_loss: 0.2594727873802185
train_iter_loss: 0.16148683428764343
train_iter_loss: 0.23209206759929657
train_iter_loss: 0.2390032857656479
train_iter_loss: 0.37098953127861023
train_iter_loss: 0.4287242293357849
train_iter_loss: 0.3452053666114807
train_iter_loss: 0.16096869111061096
train_iter_loss: 0.1343492865562439
train_iter_loss: 0.27239474654197693
train_iter_loss: 0.12057323753833771
train_iter_loss: 0.2402380108833313
train_iter_loss: 0.22414731979370117
train_iter_loss: 0.18075451254844666
train_iter_loss: 0.338603675365448
train_iter_loss: 0.31802403926849365
train_iter_loss: 0.3311580717563629
train_iter_loss: 0.1850591003894806
train_iter_loss: 0.3151448965072632
train_iter_loss: 0.36563199758529663
train_iter_loss: 0.2917502522468567
train_iter_loss: 0.28024277091026306
train_iter_loss: 0.22429734468460083
train_iter_loss: 0.19391009211540222
train_iter_loss: 0.20348429679870605
train_iter_loss: 0.09107871353626251
train_iter_loss: 0.21266120672225952
train_iter_loss: 0.2457529902458191
train_iter_loss: 0.38473254442214966
train_iter_loss: 0.2809714376926422
train_iter_loss: 0.31734657287597656
train_iter_loss: 0.3161083161830902
train_iter_loss: 0.36437129974365234
train_iter_loss: 0.32889461517333984
train_iter_loss: 0.21391113102436066
train_iter_loss: 0.4556528925895691
train_iter_loss: 0.14206033945083618
train_iter_loss: 0.25926005840301514
train_iter_loss: 0.30569857358932495
train_iter_loss: 0.22113727033138275
train_iter_loss: 0.24862273037433624
train_iter_loss: 0.3757711350917816
train_iter_loss: 0.29073402285575867
train_iter_loss: 0.4526478350162506
train_iter_loss: 0.416853129863739
train_iter_loss: 0.3577049672603607
train_iter_loss: 0.43193358182907104
train_iter_loss: 0.3252928853034973
train_iter_loss: 0.2278927117586136
train_iter_loss: 0.255001425743103
train_iter_loss: 0.2765737771987915
train_iter_loss: 0.3566093146800995
train_iter_loss: 0.166807621717453
train_iter_loss: 0.07857099175453186
train_iter_loss: 0.33640727400779724
train_iter_loss: 0.27237236499786377
train_iter_loss: 0.26448875665664673
train_iter_loss: 0.1757095456123352
train_iter_loss: 0.38748764991760254
train_iter_loss: 0.38512271642684937
train_iter_loss: 0.3222404420375824
train_iter_loss: 0.20223718881607056
train_iter_loss: 0.44164684414863586
train_iter_loss: 0.297608345746994
train_iter_loss: 0.34402114152908325
train_iter_loss: 0.20539678633213043
train_iter_loss: 0.33756738901138306
train_iter_loss: 0.17104527354240417
train_iter_loss: 0.34986981749534607
train_iter_loss: 0.22204694151878357
train_iter_loss: 0.2592771351337433
train_iter_loss: 0.26892992854118347
train_iter_loss: 0.16469746828079224
train_iter_loss: 0.2191212922334671
train_iter_loss: 0.44209668040275574
train_iter_loss: 0.36565831303596497
train_iter_loss: 0.40558379888534546
train_iter_loss: 0.2883590757846832
train_iter_loss: 0.16200101375579834
train_iter_loss: 0.18017703294754028
train_iter_loss: 0.17415820062160492
train_iter_loss: 0.28921353816986084
train_iter_loss: 0.1972118467092514
train_iter_loss: 0.11632569134235382
train_iter_loss: 0.23708966374397278
train_iter_loss: 0.37085554003715515
train_iter_loss: 0.3322279155254364
train_iter_loss: 0.3037766218185425
train_iter_loss: 0.2272566854953766
train_iter_loss: 0.2775121033191681
train loss :0.2804
---------------------
Validation seg loss: 0.3932476696241998 at epoch 295
epoch =    296/  1000, exp = train
train_iter_loss: 0.31150469183921814
train_iter_loss: 0.3175680637359619
train_iter_loss: 0.19287234544754028
train_iter_loss: 0.3710470199584961
train_iter_loss: 0.09652531147003174
train_iter_loss: 0.14731979370117188
train_iter_loss: 0.3507443964481354
train_iter_loss: 0.2455226182937622
train_iter_loss: 0.34681740403175354
train_iter_loss: 0.3628862500190735
train_iter_loss: 0.20994645357131958
train_iter_loss: 0.3598727285861969
train_iter_loss: 0.1763560026884079
train_iter_loss: 0.2575564980506897
train_iter_loss: 0.32490336894989014
train_iter_loss: 0.24169430136680603
train_iter_loss: 0.33987870812416077
train_iter_loss: 0.2970123589038849
train_iter_loss: 0.2960593104362488
train_iter_loss: 0.3787844479084015
train_iter_loss: 0.28821608424186707
train_iter_loss: 0.27024680376052856
train_iter_loss: 0.27679941058158875
train_iter_loss: 0.23252655565738678
train_iter_loss: 0.27481383085250854
train_iter_loss: 0.2076452374458313
train_iter_loss: 0.31686490774154663
train_iter_loss: 0.14941665530204773
train_iter_loss: 0.3379337191581726
train_iter_loss: 0.21043133735656738
train_iter_loss: 0.3876488506793976
train_iter_loss: 0.25805380940437317
train_iter_loss: 0.369692325592041
train_iter_loss: 0.1929410994052887
train_iter_loss: 0.3635926842689514
train_iter_loss: 0.2248663306236267
train_iter_loss: 0.3668389320373535
train_iter_loss: 0.42600223422050476
train_iter_loss: 0.2137380689382553
train_iter_loss: 0.2735579013824463
train_iter_loss: 0.06606480479240417
train_iter_loss: 0.1977548599243164
train_iter_loss: 0.3464885354042053
train_iter_loss: 0.3078143298625946
train_iter_loss: 0.35923412442207336
train_iter_loss: 0.2699160873889923
train_iter_loss: 0.21516714990139008
train_iter_loss: 0.21720731258392334
train_iter_loss: 0.18929032981395721
train_iter_loss: 0.22949083149433136
train_iter_loss: 0.2698199450969696
train_iter_loss: 0.27159029245376587
train_iter_loss: 0.2286616712808609
train_iter_loss: 0.22137151658535004
train_iter_loss: 0.3360224664211273
train_iter_loss: 0.4213514029979706
train_iter_loss: 0.2210804969072342
train_iter_loss: 0.32028624415397644
train_iter_loss: 0.24143078923225403
train_iter_loss: 0.3581567704677582
train_iter_loss: 0.11759565770626068
train_iter_loss: 0.21608617901802063
train_iter_loss: 0.173553928732872
train_iter_loss: 0.20052193105220795
train_iter_loss: 0.3801335394382477
train_iter_loss: 0.26237452030181885
train_iter_loss: 0.3093406856060028
train_iter_loss: 0.3629990220069885
train_iter_loss: 0.26673468947410583
train_iter_loss: 0.32062363624572754
train_iter_loss: 0.3416617214679718
train_iter_loss: 0.19510620832443237
train_iter_loss: 0.187972292304039
train_iter_loss: 0.3143633306026459
train_iter_loss: 0.35455524921417236
train_iter_loss: 0.3551253378391266
train_iter_loss: 0.3623022139072418
train_iter_loss: 0.28890836238861084
train_iter_loss: 0.26089924573898315
train_iter_loss: 0.4408341646194458
train_iter_loss: 0.3495604693889618
train_iter_loss: 0.2840225100517273
train_iter_loss: 0.23269936442375183
train_iter_loss: 0.36876654624938965
train_iter_loss: 0.2959746718406677
train_iter_loss: 0.28699421882629395
train_iter_loss: 0.2606286406517029
train_iter_loss: 0.18058045208454132
train_iter_loss: 0.5293201804161072
train_iter_loss: 0.2718394696712494
train_iter_loss: 0.22100991010665894
train_iter_loss: 0.21862445771694183
train_iter_loss: 0.29921460151672363
train_iter_loss: 0.3622116148471832
train_iter_loss: 0.28235718607902527
train_iter_loss: 0.2518274188041687
train_iter_loss: 0.40195563435554504
train_iter_loss: 0.3124643564224243
train_iter_loss: 0.41129449009895325
train_iter_loss: 0.19040264189243317
train loss :0.2856
---------------------
Validation seg loss: 0.38029756508591883 at epoch 296
epoch =    297/  1000, exp = train
train_iter_loss: 0.36991673707962036
train_iter_loss: 0.40138038992881775
train_iter_loss: 0.36428558826446533
train_iter_loss: 0.28114384412765503
train_iter_loss: 0.23472347855567932
train_iter_loss: 0.2699294686317444
train_iter_loss: 0.17787012457847595
train_iter_loss: 0.30752646923065186
train_iter_loss: 0.1819567233324051
train_iter_loss: 0.2121724635362625
train_iter_loss: 0.2741779685020447
train_iter_loss: 0.33015087246894836
train_iter_loss: 0.36714810132980347
train_iter_loss: 0.07486669719219208
train_iter_loss: 0.26164814829826355
train_iter_loss: 0.1262652724981308
train_iter_loss: 0.20634335279464722
train_iter_loss: 0.1722482442855835
train_iter_loss: 0.19228966534137726
train_iter_loss: 0.21162530779838562
train_iter_loss: 0.30763450264930725
train_iter_loss: 0.20926418900489807
train_iter_loss: 0.15663132071495056
train_iter_loss: 0.2870464026927948
train_iter_loss: 0.4052410125732422
train_iter_loss: 0.2666807174682617
train_iter_loss: 0.3394511938095093
train_iter_loss: 0.15669211745262146
train_iter_loss: 0.3303021788597107
train_iter_loss: 0.2002316564321518
train_iter_loss: 0.2391154170036316
train_iter_loss: 0.3241022527217865
train_iter_loss: 0.2867279350757599
train_iter_loss: 0.15884356200695038
train_iter_loss: 0.16548782587051392
train_iter_loss: 0.12640149891376495
train_iter_loss: 0.41379961371421814
train_iter_loss: 0.2217499017715454
train_iter_loss: 0.17687833309173584
train_iter_loss: 0.27685898542404175
train_iter_loss: 0.3038351535797119
train_iter_loss: 0.213588684797287
train_iter_loss: 0.37428754568099976
train_iter_loss: 0.2834326922893524
train_iter_loss: 0.2986777424812317
train_iter_loss: 0.4561074674129486
train_iter_loss: 0.31470564007759094
train_iter_loss: 0.26358941197395325
train_iter_loss: 0.4691423177719116
train_iter_loss: 0.4693368673324585
train_iter_loss: 0.14235332608222961
train_iter_loss: 0.33340391516685486
train_iter_loss: 0.2242182493209839
train_iter_loss: 0.28234854340553284
train_iter_loss: 0.26259952783584595
train_iter_loss: 0.246247336268425
train_iter_loss: 0.2594859302043915
train_iter_loss: 0.28848305344581604
train_iter_loss: 0.09755348414182663
train_iter_loss: 0.25967878103256226
train_iter_loss: 0.30794551968574524
train_iter_loss: 0.25492990016937256
train_iter_loss: 0.3276519477367401
train_iter_loss: 0.3045901954174042
train_iter_loss: 0.2760414779186249
train_iter_loss: 0.25838154554367065
train_iter_loss: 0.2977677881717682
train_iter_loss: 0.32322457432746887
train_iter_loss: 0.27399396896362305
train_iter_loss: 0.3197442293167114
train_iter_loss: 0.3324569761753082
train_iter_loss: 0.37866294384002686
train_iter_loss: 0.26439791917800903
train_iter_loss: 0.37124088406562805
train_iter_loss: 0.3730209469795227
train_iter_loss: 0.2482089251279831
train_iter_loss: 0.3817875385284424
train_iter_loss: 0.37153398990631104
train_iter_loss: 0.3318168520927429
train_iter_loss: 0.42203739285469055
train_iter_loss: 0.2533110976219177
train_iter_loss: 0.4184850752353668
train_iter_loss: 0.29968076944351196
train_iter_loss: 0.10805675387382507
train_iter_loss: 0.2650895118713379
train_iter_loss: 0.2548827826976776
train_iter_loss: 0.31971582770347595
train_iter_loss: 0.24376077950000763
train_iter_loss: 0.24049586057662964
train_iter_loss: 0.3277028501033783
train_iter_loss: 0.19229084253311157
train_iter_loss: 0.5042542219161987
train_iter_loss: 0.3354450762271881
train_iter_loss: 0.445013165473938
train_iter_loss: 0.20525693893432617
train_iter_loss: 0.2797911465167999
train_iter_loss: 0.33649230003356934
train_iter_loss: 0.36729469895362854
train_iter_loss: 0.19027599692344666
train_iter_loss: 0.31248360872268677
train loss :0.2851
---------------------
Validation seg loss: 0.41886727025134946 at epoch 297
epoch =    298/  1000, exp = train
train_iter_loss: 0.4573056101799011
train_iter_loss: 0.35058844089508057
train_iter_loss: 0.3237984776496887
train_iter_loss: 0.2025737762451172
train_iter_loss: 0.2963177263736725
train_iter_loss: 0.41655224561691284
train_iter_loss: 0.17878848314285278
train_iter_loss: 0.29288601875305176
train_iter_loss: 0.17535467445850372
train_iter_loss: 0.21146202087402344
train_iter_loss: 0.2739270329475403
train_iter_loss: 0.1811603456735611
train_iter_loss: 0.28030845522880554
train_iter_loss: 0.13722218573093414
train_iter_loss: 0.2969420254230499
train_iter_loss: 0.3170619606971741
train_iter_loss: 0.0902201235294342
train_iter_loss: 0.18875186145305634
train_iter_loss: 0.29378220438957214
train_iter_loss: 0.13875243067741394
train_iter_loss: 0.23376213014125824
train_iter_loss: 0.35723283886909485
train_iter_loss: 0.34577086567878723
train_iter_loss: 0.469499796628952
train_iter_loss: 0.13708868622779846
train_iter_loss: 0.22348685562610626
train_iter_loss: 0.2821004390716553
train_iter_loss: 0.11887168139219284
train_iter_loss: 0.28751322627067566
train_iter_loss: 0.2150813341140747
train_iter_loss: 0.3448605537414551
train_iter_loss: 0.3100590705871582
train_iter_loss: 0.4996611773967743
train_iter_loss: 0.16554397344589233
train_iter_loss: 0.35834935307502747
train_iter_loss: 0.22782613337039948
train_iter_loss: 0.23898760974407196
train_iter_loss: 0.4453877806663513
train_iter_loss: 0.17887623608112335
train_iter_loss: 0.22157786786556244
train_iter_loss: 0.14581260085105896
train_iter_loss: 0.21316465735435486
train_iter_loss: 0.1284356713294983
train_iter_loss: 0.4441584646701813
train_iter_loss: 0.18561086058616638
train_iter_loss: 0.15580494701862335
train_iter_loss: 0.29689356684684753
train_iter_loss: 0.2947084903717041
train_iter_loss: 0.2441045492887497
train_iter_loss: 0.29605633020401
train_iter_loss: 0.3112163245677948
train_iter_loss: 0.23718543350696564
train_iter_loss: 0.25378936529159546
train_iter_loss: 0.2647145390510559
train_iter_loss: 0.39091014862060547
train_iter_loss: 0.4370619058609009
train_iter_loss: 0.2793704867362976
train_iter_loss: 0.27618643641471863
train_iter_loss: 0.09986810386180878
train_iter_loss: 0.27635911107063293
train_iter_loss: 0.25864413380622864
train_iter_loss: 0.31470587849617004
train_iter_loss: 0.2480081021785736
train_iter_loss: 0.36494535207748413
train_iter_loss: 0.23344428837299347
train_iter_loss: 0.30893903970718384
train_iter_loss: 0.21414552628993988
train_iter_loss: 0.298105925321579
train_iter_loss: 0.3425920605659485
train_iter_loss: 0.19364267587661743
train_iter_loss: 0.47584977746009827
train_iter_loss: 0.24893639981746674
train_iter_loss: 0.18599560856819153
train_iter_loss: 0.31765660643577576
train_iter_loss: 0.18421770632266998
train_iter_loss: 0.10600695759057999
train_iter_loss: 0.18998952209949493
train_iter_loss: 0.18262046575546265
train_iter_loss: 0.25509750843048096
train_iter_loss: 0.2643466293811798
train_iter_loss: 0.4453341066837311
train_iter_loss: 0.3702247738838196
train_iter_loss: 0.43763548135757446
train_iter_loss: 0.25794774293899536
train_iter_loss: 0.3010164797306061
train_iter_loss: 0.2704414427280426
train_iter_loss: 0.26637566089630127
train_iter_loss: 0.5396674871444702
train_iter_loss: 0.15873131155967712
train_iter_loss: 0.24032309651374817
train_iter_loss: 0.4968397915363312
train_iter_loss: 0.8262007832527161
train_iter_loss: 0.21459174156188965
train_iter_loss: 0.24174974858760834
train_iter_loss: 0.45628905296325684
train_iter_loss: 0.280568927526474
train_iter_loss: 0.34831225872039795
train_iter_loss: 0.14406836032867432
train_iter_loss: 0.16481633484363556
train_iter_loss: 0.30347996950149536
train loss :0.2823
---------------------
Validation seg loss: 0.36495989761403147 at epoch 298
epoch =    299/  1000, exp = train
train_iter_loss: 0.2621515989303589
train_iter_loss: 0.06793181598186493
train_iter_loss: 0.3626459538936615
train_iter_loss: 0.3650270998477936
train_iter_loss: 0.31031572818756104
train_iter_loss: 0.26913633942604065
train_iter_loss: 0.2574349343776703
train_iter_loss: 0.16433699429035187
train_iter_loss: 0.3583536744117737
train_iter_loss: 0.206039696931839
train_iter_loss: 0.2976863980293274
train_iter_loss: 0.26589295268058777
train_iter_loss: 0.266023188829422
train_iter_loss: 0.18503440916538239
train_iter_loss: 0.3294031322002411
train_iter_loss: 0.173368439078331
train_iter_loss: 0.36669623851776123
train_iter_loss: 0.13990908861160278
train_iter_loss: 0.4033614695072174
train_iter_loss: 0.31967660784721375
train_iter_loss: 0.3825404644012451
train_iter_loss: 0.14099964499473572
train_iter_loss: 0.3603123724460602
train_iter_loss: 0.3484111428260803
train_iter_loss: 0.5572845935821533
train_iter_loss: 0.43047773838043213
train_iter_loss: 0.2184709906578064
train_iter_loss: 0.32352370023727417
train_iter_loss: 0.16307029128074646
train_iter_loss: 0.3125544488430023
train_iter_loss: 0.3416866064071655
train_iter_loss: 0.18235771358013153
train_iter_loss: 0.40196025371551514
train_iter_loss: 0.2759568989276886
train_iter_loss: 0.2843009829521179
train_iter_loss: 0.276876300573349
train_iter_loss: 0.26729267835617065
train_iter_loss: 0.28147852420806885
train_iter_loss: 0.16358914971351624
train_iter_loss: 0.2298448532819748
train_iter_loss: 0.4682273864746094
train_iter_loss: 0.2849581241607666
train_iter_loss: 0.21856476366519928
train_iter_loss: 0.16272856295108795
train_iter_loss: 0.3681388199329376
train_iter_loss: 0.28823530673980713
train_iter_loss: 0.23477831482887268
train_iter_loss: 0.13047346472740173
train_iter_loss: 0.3346188962459564
train_iter_loss: 0.519637405872345
train_iter_loss: 0.31633681058883667
train_iter_loss: 0.2223433554172516
train_iter_loss: 0.21480487287044525
train_iter_loss: 0.3825388252735138
train_iter_loss: 0.26945608854293823
train_iter_loss: 0.30093616247177124
train_iter_loss: 0.2047293484210968
train_iter_loss: 0.24932846426963806
train_iter_loss: 0.21640536189079285
train_iter_loss: 0.2550787031650543
train_iter_loss: 0.26206332445144653
train_iter_loss: 0.44222989678382874
train_iter_loss: 0.368185818195343
train_iter_loss: 0.23961113393306732
train_iter_loss: 0.2938801050186157
train_iter_loss: 0.17104662954807281
train_iter_loss: 0.18915775418281555
train_iter_loss: 0.24980761110782623
train_iter_loss: 0.2613668441772461
train_iter_loss: 0.28826144337654114
train_iter_loss: 0.2333213835954666
train_iter_loss: 0.32124316692352295
train_iter_loss: 0.1873313933610916
train_iter_loss: 0.24474549293518066
train_iter_loss: 0.24297495186328888
train_iter_loss: 0.2357500195503235
train_iter_loss: 0.32098478078842163
train_iter_loss: 0.17888715863227844
train_iter_loss: 0.21063029766082764
train_iter_loss: 0.307681143283844
train_iter_loss: 0.17027419805526733
train_iter_loss: 0.22875595092773438
train_iter_loss: 0.3925074338912964
train_iter_loss: 0.3024587035179138
train_iter_loss: 0.37377241253852844
train_iter_loss: 0.1908441185951233
train_iter_loss: 0.2550951838493347
train_iter_loss: 0.26260054111480713
train_iter_loss: 0.3247935175895691
train_iter_loss: 0.3331107795238495
train_iter_loss: 0.1992865800857544
train_iter_loss: 0.23622927069664001
train_iter_loss: 0.6346821188926697
train_iter_loss: 0.18937699496746063
train_iter_loss: 0.2072535902261734
train_iter_loss: 0.19862470030784607
train_iter_loss: 0.24128688871860504
train_iter_loss: 0.22255884110927582
train_iter_loss: 0.2507021427154541
train_iter_loss: 0.3021289110183716
train loss :0.2800
---------------------
Validation seg loss: 0.4169741591333218 at epoch 299
epoch =    300/  1000, exp = train
train_iter_loss: 0.11182931065559387
train_iter_loss: 0.2744922637939453
train_iter_loss: 0.3245611786842346
train_iter_loss: 0.15726448595523834
train_iter_loss: 0.22747474908828735
train_iter_loss: 0.26327311992645264
train_iter_loss: 0.24438591301441193
train_iter_loss: 0.2015513777732849
train_iter_loss: 0.2346765100955963
train_iter_loss: 0.3205200433731079
train_iter_loss: 0.23665595054626465
train_iter_loss: 0.15686270594596863
train_iter_loss: 0.16231176257133484
train_iter_loss: 0.22309857606887817
train_iter_loss: 0.21314623951911926
train_iter_loss: 0.4043951630592346
train_iter_loss: 0.18926674127578735
train_iter_loss: 0.2817932367324829
train_iter_loss: 0.40387728810310364
train_iter_loss: 0.34508848190307617
train_iter_loss: 0.29506799578666687
train_iter_loss: 0.30618607997894287
train_iter_loss: 0.29514220356941223
train_iter_loss: 0.48644647002220154
train_iter_loss: 0.2052534967660904
train_iter_loss: 0.32626983523368835
train_iter_loss: 0.25658783316612244
train_iter_loss: 0.23612603545188904
train_iter_loss: 0.3073927164077759
train_iter_loss: 0.3793465495109558
train_iter_loss: 0.2385566383600235
train_iter_loss: 0.24706725776195526
train_iter_loss: 0.40743857622146606
train_iter_loss: 0.256286084651947
train_iter_loss: 0.38729456067085266
train_iter_loss: 0.23731912672519684
train_iter_loss: 0.3047018051147461
train_iter_loss: 0.3846417963504791
train_iter_loss: 0.2520618438720703
train_iter_loss: 0.2138870507478714
train_iter_loss: 0.3098343312740326
train_iter_loss: 0.24390876293182373
train_iter_loss: 0.30713823437690735
train_iter_loss: 0.26940229535102844
train_iter_loss: 0.3032905161380768
train_iter_loss: 0.41970938444137573
train_iter_loss: 0.34229692816734314
train_iter_loss: 0.21123503148555756
train_iter_loss: 0.4082876443862915
train_iter_loss: 0.2566426694393158
train_iter_loss: 0.36045041680336
train_iter_loss: 0.27570006251335144
train_iter_loss: 0.35354095697402954
train_iter_loss: 0.33576634526252747
train_iter_loss: 0.23019684851169586
train_iter_loss: 0.3318868577480316
train_iter_loss: 0.31792905926704407
train_iter_loss: 0.460446298122406
train_iter_loss: 0.26524776220321655
train_iter_loss: 0.3303692042827606
train_iter_loss: 0.16543570160865784
train_iter_loss: 0.10459654033184052
train_iter_loss: 0.3222847282886505
train_iter_loss: 0.39922386407852173
train_iter_loss: 0.3327481746673584
train_iter_loss: 0.16887342929840088
train_iter_loss: 0.2903963625431061
train_iter_loss: 0.2802515923976898
train_iter_loss: 0.14361737668514252
train_iter_loss: 0.23243197798728943
train_iter_loss: 0.15088246762752533
train_iter_loss: 0.21056874096393585
train_iter_loss: 0.19439110159873962
train_iter_loss: 0.30168458819389343
train_iter_loss: 0.29116830229759216
train_iter_loss: 0.253783643245697
train_iter_loss: 0.10440239310264587
train_iter_loss: 0.2025585025548935
train_iter_loss: 0.3019108772277832
train_iter_loss: 0.3983755111694336
train_iter_loss: 0.23252174258232117
train_iter_loss: 0.26357361674308777
train_iter_loss: 0.25999951362609863
train_iter_loss: 0.2278842031955719
train_iter_loss: 0.3047962486743927
train_iter_loss: 0.2580101490020752
train_iter_loss: 0.23884029686450958
train_iter_loss: 0.11963697522878647
train_iter_loss: 0.28618860244750977
train_iter_loss: 0.2599574327468872
train_iter_loss: 0.31100690364837646
train_iter_loss: 0.30140388011932373
train_iter_loss: 0.20466624200344086
train_iter_loss: 0.21329593658447266
train_iter_loss: 0.3188491761684418
train_iter_loss: 0.26593977212905884
train_iter_loss: 0.34217026829719543
train_iter_loss: 0.33728575706481934
train_iter_loss: 0.43508729338645935
train_iter_loss: 0.3573722839355469
train loss :0.2800
---------------------
Validation seg loss: 0.3837265522781549 at epoch 300
epoch =    301/  1000, exp = train
train_iter_loss: 0.36955803632736206
train_iter_loss: 0.43869295716285706
train_iter_loss: 0.21693705022335052
train_iter_loss: 0.51180499792099
train_iter_loss: 0.3631172180175781
train_iter_loss: 0.25281593203544617
train_iter_loss: 0.36036404967308044
train_iter_loss: 0.23333178460597992
train_iter_loss: 0.40894415974617004
train_iter_loss: 0.17637433111667633
train_iter_loss: 0.35151174664497375
train_iter_loss: 0.23765261471271515
train_iter_loss: 0.1616470068693161
train_iter_loss: 0.24758774042129517
train_iter_loss: 0.3715958595275879
train_iter_loss: 0.19452719390392303
train_iter_loss: 0.32193756103515625
train_iter_loss: 0.2235192507505417
train_iter_loss: 0.41468584537506104
train_iter_loss: 0.26752904057502747
train_iter_loss: 0.1604037880897522
train_iter_loss: 0.34139180183410645
train_iter_loss: 0.3025033175945282
train_iter_loss: 0.2925470471382141
train_iter_loss: 0.40814512968063354
train_iter_loss: 0.24626046419143677
train_iter_loss: 0.3727665841579437
train_iter_loss: 0.37473633885383606
train_iter_loss: 0.32447099685668945
train_iter_loss: 0.32407787442207336
train_iter_loss: 0.08454344421625137
train_iter_loss: 0.2807510793209076
train_iter_loss: 0.15217232704162598
train_iter_loss: 0.22339603304862976
train_iter_loss: 0.31599560379981995
train_iter_loss: 0.28555649518966675
train_iter_loss: 0.30046242475509644
train_iter_loss: 0.3963829278945923
train_iter_loss: 0.4502544701099396
train_iter_loss: 0.35426467657089233
train_iter_loss: 0.2769871950149536
train_iter_loss: 0.3358731269836426
train_iter_loss: 0.11780712753534317
train_iter_loss: 0.2959071695804596
train_iter_loss: 0.1761483997106552
train_iter_loss: 0.2786579430103302
train_iter_loss: 0.5288281440734863
train_iter_loss: 0.19049441814422607
train_iter_loss: 0.35362547636032104
train_iter_loss: 0.3609810471534729
train_iter_loss: 0.16511206328868866
train_iter_loss: 0.32520368695259094
train_iter_loss: 0.2683168649673462
train_iter_loss: 0.37790584564208984
train_iter_loss: 0.5073326230049133
train_iter_loss: 0.2246323525905609
train_iter_loss: 0.32282158732414246
train_iter_loss: 0.24189868569374084
train_iter_loss: 0.2967342138290405
train_iter_loss: 0.25405046343803406
train_iter_loss: 0.33482232689857483
train_iter_loss: 0.34128230810165405
train_iter_loss: 0.2457551658153534
train_iter_loss: 0.329156756401062
train_iter_loss: 0.2740924060344696
train_iter_loss: 0.27398109436035156
train_iter_loss: 0.16146136820316315
train_iter_loss: 0.3418678343296051
train_iter_loss: 0.19675973057746887
train_iter_loss: 0.23331089317798615
train_iter_loss: 0.16233159601688385
train_iter_loss: 0.1430152803659439
train_iter_loss: 0.33144545555114746
train_iter_loss: 0.2693772315979004
train_iter_loss: 0.3394955098628998
train_iter_loss: 0.2835056483745575
train_iter_loss: 0.21636836230754852
train_iter_loss: 0.3709883391857147
train_iter_loss: 0.2514687180519104
train_iter_loss: 0.27475816011428833
train_iter_loss: 0.3153446614742279
train_iter_loss: 0.24762548506259918
train_iter_loss: 0.3187633752822876
train_iter_loss: 0.2991015613079071
train_iter_loss: 0.2714839577674866
train_iter_loss: 0.2977200448513031
train_iter_loss: 0.322191447019577
train_iter_loss: 0.34590938687324524
train_iter_loss: 0.27734607458114624
train_iter_loss: 0.1510830670595169
train_iter_loss: 0.2024867981672287
train_iter_loss: 0.16785277426242828
train_iter_loss: 0.1551799476146698
train_iter_loss: 0.18783710896968842
train_iter_loss: 0.25543200969696045
train_iter_loss: 0.34975892305374146
train_iter_loss: 0.2131398767232895
train_iter_loss: 0.23443269729614258
train_iter_loss: 0.25336185097694397
train_iter_loss: 0.3292768597602844
train loss :0.2887
---------------------
Validation seg loss: 0.3768452999867358 at epoch 301
epoch =    302/  1000, exp = train
train_iter_loss: 0.19731643795967102
train_iter_loss: 0.27724501490592957
train_iter_loss: 0.19056501984596252
train_iter_loss: 0.19783994555473328
train_iter_loss: 0.35377663373947144
train_iter_loss: 0.4671500325202942
train_iter_loss: 0.3702522814273834
train_iter_loss: 0.19835317134857178
train_iter_loss: 0.33513036370277405
train_iter_loss: 0.366942435503006
train_iter_loss: 0.23415282368659973
train_iter_loss: 0.19684940576553345
train_iter_loss: 0.39088335633277893
train_iter_loss: 0.36060774326324463
train_iter_loss: 0.4498814642429352
train_iter_loss: 0.22401411831378937
train_iter_loss: 0.4499158561229706
train_iter_loss: 0.1938701570034027
train_iter_loss: 0.30964311957359314
train_iter_loss: 0.2448529154062271
train_iter_loss: 0.3632740378379822
train_iter_loss: 0.19319996237754822
train_iter_loss: 0.27992287278175354
train_iter_loss: 0.23688867688179016
train_iter_loss: 0.2754441499710083
train_iter_loss: 0.280717134475708
train_iter_loss: 0.1943928450345993
train_iter_loss: 0.368153840303421
train_iter_loss: 0.1995851695537567
train_iter_loss: 0.25353357195854187
train_iter_loss: 0.2844700813293457
train_iter_loss: 0.3392764627933502
train_iter_loss: 0.3360368311405182
train_iter_loss: 0.22697322070598602
train_iter_loss: 0.27430909872055054
train_iter_loss: 0.3857828974723816
train_iter_loss: 0.3345522880554199
train_iter_loss: 0.27778029441833496
train_iter_loss: 0.2834059000015259
train_iter_loss: 0.15915532410144806
train_iter_loss: 0.353867769241333
train_iter_loss: 0.32201194763183594
train_iter_loss: 0.19061173498630524
train_iter_loss: 0.23319734632968903
train_iter_loss: 0.15129339694976807
train_iter_loss: 0.2814997434616089
train_iter_loss: 0.22794993221759796
train_iter_loss: 0.39535459876060486
train_iter_loss: 0.15440218150615692
train_iter_loss: 0.3427755534648895
train_iter_loss: 0.2857496738433838
train_iter_loss: 0.24466080963611603
train_iter_loss: 0.3280244469642639
train_iter_loss: 0.3344036638736725
train_iter_loss: 0.1331835389137268
train_iter_loss: 0.3365953266620636
train_iter_loss: 0.3352496027946472
train_iter_loss: 0.18506567180156708
train_iter_loss: 0.3523052930831909
train_iter_loss: 0.3564550280570984
train_iter_loss: 0.21671278774738312
train_iter_loss: 0.36136314272880554
train_iter_loss: 0.32227781414985657
train_iter_loss: 0.21472357213497162
train_iter_loss: 0.26751917600631714
train_iter_loss: 0.3145650029182434
train_iter_loss: 0.3016085922718048
train_iter_loss: 0.2964620292186737
train_iter_loss: 0.2494543343782425
train_iter_loss: 0.3146151304244995
train_iter_loss: 0.23437806963920593
train_iter_loss: 0.3758302628993988
train_iter_loss: 0.09718599170446396
train_iter_loss: 0.38985300064086914
train_iter_loss: 0.2648872137069702
train_iter_loss: 0.14500313997268677
train_iter_loss: 0.46839267015457153
train_iter_loss: 0.31260210275650024
train_iter_loss: 0.39932799339294434
train_iter_loss: 0.27573561668395996
train_iter_loss: 0.20376309752464294
train_iter_loss: 0.20263078808784485
train_iter_loss: 0.2434169203042984
train_iter_loss: 0.2632884085178375
train_iter_loss: 0.209390327334404
train_iter_loss: 0.3047838807106018
train_iter_loss: 0.11944104731082916
train_iter_loss: 0.2916269898414612
train_iter_loss: 0.222879558801651
train_iter_loss: 0.30843785405158997
train_iter_loss: 0.1571498066186905
train_iter_loss: 0.16447198390960693
train_iter_loss: 0.16200846433639526
train_iter_loss: 0.18129804730415344
train_iter_loss: 0.25537654757499695
train_iter_loss: 0.38088276982307434
train_iter_loss: 0.22188487648963928
train_iter_loss: 0.21098648011684418
train_iter_loss: 0.2124205380678177
train_iter_loss: 0.33919695019721985
train loss :0.2787
---------------------
Validation seg loss: 0.39497077030147304 at epoch 302
epoch =    303/  1000, exp = train
train_iter_loss: 0.22772973775863647
train_iter_loss: 0.39886000752449036
train_iter_loss: 0.2144073247909546
train_iter_loss: 0.3232420086860657
train_iter_loss: 0.18373516201972961
train_iter_loss: 0.26639649271965027
train_iter_loss: 0.5028327703475952
train_iter_loss: 0.29720234870910645
train_iter_loss: 0.21944358944892883
train_iter_loss: 0.22435785830020905
train_iter_loss: 0.3039596378803253
train_iter_loss: 0.31357237696647644
train_iter_loss: 0.08502642810344696
train_iter_loss: 0.1453348994255066
train_iter_loss: 0.23529022932052612
train_iter_loss: 0.23222742974758148
train_iter_loss: 0.38146260380744934
train_iter_loss: 0.18709668517112732
train_iter_loss: 0.3449722230434418
train_iter_loss: 0.31285253167152405
train_iter_loss: 0.2973231077194214
train_iter_loss: 0.20753465592861176
train_iter_loss: 0.3127266466617584
train_iter_loss: 0.4866637885570526
train_iter_loss: 0.2574988603591919
train_iter_loss: 0.18471744656562805
train_iter_loss: 0.3610142767429352
train_iter_loss: 0.28376415371894836
train_iter_loss: 0.28413015604019165
train_iter_loss: 0.2739899456501007
train_iter_loss: 0.2385939061641693
train_iter_loss: 0.43415939807891846
train_iter_loss: 0.23757606744766235
train_iter_loss: 0.36621227860450745
train_iter_loss: 0.19318047165870667
train_iter_loss: 0.38662081956863403
train_iter_loss: 0.13646617531776428
train_iter_loss: 0.3148232400417328
train_iter_loss: 0.18368925154209137
train_iter_loss: 0.23929531872272491
train_iter_loss: 0.25010210275650024
train_iter_loss: 0.18932503461837769
train_iter_loss: 0.3062897026538849
train_iter_loss: 0.12967996299266815
train_iter_loss: 0.35072246193885803
train_iter_loss: 0.2079046070575714
train_iter_loss: 0.11110850423574448
train_iter_loss: 0.2444894164800644
train_iter_loss: 0.3084845244884491
train_iter_loss: 0.4639085531234741
train_iter_loss: 0.18788529932498932
train_iter_loss: 0.32948794960975647
train_iter_loss: 0.3039032220840454
train_iter_loss: 0.22541444003582
train_iter_loss: 0.12865647673606873
train_iter_loss: 0.09682058542966843
train_iter_loss: 0.20720620453357697
train_iter_loss: 0.17208074033260345
train_iter_loss: 0.4793902337551117
train_iter_loss: 0.2636148929595947
train_iter_loss: 0.2724393606185913
train_iter_loss: 0.5016530752182007
train_iter_loss: 0.3189663589000702
train_iter_loss: 0.21163366734981537
train_iter_loss: 0.13308075070381165
train_iter_loss: 0.36731526255607605
train_iter_loss: 0.3406682014465332
train_iter_loss: 0.3083646893501282
train_iter_loss: 0.25744354724884033
train_iter_loss: 0.2559724748134613
train_iter_loss: 0.2565670609474182
train_iter_loss: 0.13091854751110077
train_iter_loss: 0.2772190570831299
train_iter_loss: 0.2888023555278778
train_iter_loss: 0.2539384067058563
train_iter_loss: 0.2793848514556885
train_iter_loss: 0.26394474506378174
train_iter_loss: 0.36747196316719055
train_iter_loss: 0.2913396656513214
train_iter_loss: 0.3268831968307495
train_iter_loss: 0.30075639486312866
train_iter_loss: 0.41397783160209656
train_iter_loss: 0.2803136110305786
train_iter_loss: 0.25049811601638794
train_iter_loss: 0.4123601019382477
train_iter_loss: 0.4879489541053772
train_iter_loss: 0.33328619599342346
train_iter_loss: 0.33329978585243225
train_iter_loss: 0.19746173918247223
train_iter_loss: 0.2593500018119812
train_iter_loss: 0.2512255609035492
train_iter_loss: 0.24257321655750275
train_iter_loss: 0.3889675438404083
train_iter_loss: 0.20296518504619598
train_iter_loss: 0.2741844058036804
train_iter_loss: 0.2555123567581177
train_iter_loss: 0.26234909892082214
train_iter_loss: 0.30321621894836426
train_iter_loss: 0.27600350975990295
train_iter_loss: 0.31064704060554504
train loss :0.2806
---------------------
Validation seg loss: 0.3825946838138098 at epoch 303
epoch =    304/  1000, exp = train
train_iter_loss: 0.3435598611831665
train_iter_loss: 0.37858378887176514
train_iter_loss: 0.10881944745779037
train_iter_loss: 0.32281380891799927
train_iter_loss: 0.35452431440353394
train_iter_loss: 0.3132692575454712
train_iter_loss: 0.3635077178478241
train_iter_loss: 0.24645370244979858
train_iter_loss: 0.43842989206314087
train_iter_loss: 0.3094809651374817
train_iter_loss: 0.3083372414112091
train_iter_loss: 0.4980703592300415
train_iter_loss: 0.2058190256357193
train_iter_loss: 0.2991848587989807
train_iter_loss: 0.20187772810459137
train_iter_loss: 0.20583003759384155
train_iter_loss: 0.13181231915950775
train_iter_loss: 0.21485482156276703
train_iter_loss: 0.23327334225177765
train_iter_loss: 0.3571995496749878
train_iter_loss: 0.35553091764450073
train_iter_loss: 0.2131650596857071
train_iter_loss: 0.2084803283214569
train_iter_loss: 0.37376201152801514
train_iter_loss: 0.26298877596855164
train_iter_loss: 0.2905811667442322
train_iter_loss: 0.38859936594963074
train_iter_loss: 0.2329452633857727
train_iter_loss: 0.3507845997810364
train_iter_loss: 0.3028631806373596
train_iter_loss: 0.14766407012939453
train_iter_loss: 0.27461621165275574
train_iter_loss: 0.30385318398475647
train_iter_loss: 0.30698084831237793
train_iter_loss: 0.23068378865718842
train_iter_loss: 0.06243693828582764
train_iter_loss: 0.12778538465499878
train_iter_loss: 0.35475319623947144
train_iter_loss: 0.39363667368888855
train_iter_loss: 0.23310261964797974
train_iter_loss: 0.47224968671798706
train_iter_loss: 0.29848504066467285
train_iter_loss: 0.2270873636007309
train_iter_loss: 0.26058053970336914
train_iter_loss: 0.3435288071632385
train_iter_loss: 0.2767464518547058
train_iter_loss: 0.28051820397377014
train_iter_loss: 0.39218437671661377
train_iter_loss: 0.22057579457759857
train_iter_loss: 0.3656250238418579
train_iter_loss: 0.2066500335931778
train_iter_loss: 0.31760990619659424
train_iter_loss: 0.249162957072258
train_iter_loss: 0.3197055757045746
train_iter_loss: 0.22461344301700592
train_iter_loss: 0.2522605359554291
train_iter_loss: 0.22436147928237915
train_iter_loss: 0.33960267901420593
train_iter_loss: 0.2090856432914734
train_iter_loss: 0.25619104504585266
train_iter_loss: 0.18800659477710724
train_iter_loss: 0.3701881170272827
train_iter_loss: 0.22941358387470245
train_iter_loss: 0.366312712430954
train_iter_loss: 0.1630086749792099
train_iter_loss: 0.33913522958755493
train_iter_loss: 0.3001251518726349
train_iter_loss: 0.18226106464862823
train_iter_loss: 0.3424094021320343
train_iter_loss: 0.3102261424064636
train_iter_loss: 0.1312100887298584
train_iter_loss: 0.3008996844291687
train_iter_loss: 0.24676324427127838
train_iter_loss: 0.2764546573162079
train_iter_loss: 0.2103377878665924
train_iter_loss: 0.24758552014827728
train_iter_loss: 0.2121308594942093
train_iter_loss: 0.2183520495891571
train_iter_loss: 0.19084613025188446
train_iter_loss: 0.3181561231613159
train_iter_loss: 0.2581254839897156
train_iter_loss: 0.33275261521339417
train_iter_loss: 0.17179375886917114
train_iter_loss: 0.29855620861053467
train_iter_loss: 0.14682257175445557
train_iter_loss: 0.3072931468486786
train_iter_loss: 0.20875918865203857
train_iter_loss: 0.23703958094120026
train_iter_loss: 0.3276787996292114
train_iter_loss: 0.2715579569339752
train_iter_loss: 0.2127104252576828
train_iter_loss: 0.21400004625320435
train_iter_loss: 0.4069638252258301
train_iter_loss: 0.3213675320148468
train_iter_loss: 0.11059319972991943
train_iter_loss: 0.3423769772052765
train_iter_loss: 0.19200803339481354
train_iter_loss: 0.24122166633605957
train_iter_loss: 0.16884367167949677
train_iter_loss: 0.23584552109241486
train loss :0.2742
---------------------
Validation seg loss: 0.3891102240139724 at epoch 304
epoch =    305/  1000, exp = train
train_iter_loss: 0.32654231786727905
train_iter_loss: 0.2174575924873352
train_iter_loss: 0.22545738518238068
train_iter_loss: 0.3704071044921875
train_iter_loss: 0.16670596599578857
train_iter_loss: 0.25439369678497314
train_iter_loss: 0.30689987540245056
train_iter_loss: 0.22258174419403076
train_iter_loss: 0.31945666670799255
train_iter_loss: 0.20118439197540283
train_iter_loss: 0.19982358813285828
train_iter_loss: 0.2019919604063034
train_iter_loss: 0.28093820810317993
train_iter_loss: 0.19211867451667786
train_iter_loss: 0.33213064074516296
train_iter_loss: 0.2984405755996704
train_iter_loss: 0.4270355701446533
train_iter_loss: 0.2946382164955139
train_iter_loss: 0.31898143887519836
train_iter_loss: 0.29714733362197876
train_iter_loss: 0.18503673374652863
train_iter_loss: 0.38889071345329285
train_iter_loss: 0.3639543056488037
train_iter_loss: 0.26418429613113403
train_iter_loss: 0.31896835565567017
train_iter_loss: 0.29599690437316895
train_iter_loss: 0.29806235432624817
train_iter_loss: 0.3890348970890045
train_iter_loss: 0.4638145864009857
train_iter_loss: 0.392869234085083
train_iter_loss: 0.19853176176548004
train_iter_loss: 0.33555763959884644
train_iter_loss: 0.27930372953414917
train_iter_loss: 0.42057329416275024
train_iter_loss: 0.29929643869400024
train_iter_loss: 0.3032451868057251
train_iter_loss: 0.39553704857826233
train_iter_loss: 0.22226335108280182
train_iter_loss: 0.11420553922653198
train_iter_loss: 0.3476041257381439
train_iter_loss: 0.1407930701971054
train_iter_loss: 0.07152263820171356
train_iter_loss: 0.2551479935646057
train_iter_loss: 0.3014810383319855
train_iter_loss: 0.3073117733001709
train_iter_loss: 0.24486678838729858
train_iter_loss: 0.1914021223783493
train_iter_loss: 0.13096697628498077
train_iter_loss: 0.1951792985200882
train_iter_loss: 0.13093017041683197
train_iter_loss: 0.3954295217990875
train_iter_loss: 0.2870141267776489
train_iter_loss: 0.20233672857284546
train_iter_loss: 0.15762844681739807
train_iter_loss: 0.34192588925361633
train_iter_loss: 0.32022231817245483
train_iter_loss: 0.1459362804889679
train_iter_loss: 0.31863197684288025
train_iter_loss: 0.2796080410480499
train_iter_loss: 0.21788550913333893
train_iter_loss: 0.3656679689884186
train_iter_loss: 0.22717909514904022
train_iter_loss: 0.36534711718559265
train_iter_loss: 0.3588511049747467
train_iter_loss: 0.309971421957016
train_iter_loss: 0.17371311783790588
train_iter_loss: 0.3002822995185852
train_iter_loss: 0.23931623995304108
train_iter_loss: 0.3599221408367157
train_iter_loss: 0.3200896680355072
train_iter_loss: 0.1723317950963974
train_iter_loss: 0.16505713760852814
train_iter_loss: 0.2915841042995453
train_iter_loss: 0.37740403413772583
train_iter_loss: 0.30919915437698364
train_iter_loss: 0.23836790025234222
train_iter_loss: 0.3262374699115753
train_iter_loss: 0.29646140336990356
train_iter_loss: 0.24156351387500763
train_iter_loss: 0.14414022862911224
train_iter_loss: 0.24060475826263428
train_iter_loss: 0.3893561065196991
train_iter_loss: 0.22042690217494965
train_iter_loss: 0.35948869585990906
train_iter_loss: 0.20745079219341278
train_iter_loss: 0.3149634301662445
train_iter_loss: 0.2828565835952759
train_iter_loss: 0.273906409740448
train_iter_loss: 0.273997962474823
train_iter_loss: 0.15831580758094788
train_iter_loss: 0.2660248875617981
train_iter_loss: 0.15276610851287842
train_iter_loss: 0.3281906545162201
train_iter_loss: 0.4476845860481262
train_iter_loss: 0.3700367212295532
train_iter_loss: 0.3969999849796295
train_iter_loss: 0.2005428969860077
train_iter_loss: 0.14907951653003693
train_iter_loss: 0.5079259276390076
train_iter_loss: 0.24009908735752106
train loss :0.2801
---------------------
Validation seg loss: 0.3781913578826583 at epoch 305
epoch =    306/  1000, exp = train
train_iter_loss: 0.33964595198631287
train_iter_loss: 0.33140313625335693
train_iter_loss: 0.2543696165084839
train_iter_loss: 0.3153396546840668
train_iter_loss: 0.38479727506637573
train_iter_loss: 0.23955029249191284
train_iter_loss: 0.3300304114818573
train_iter_loss: 0.21244564652442932
train_iter_loss: 0.3233199119567871
train_iter_loss: 0.25065934658050537
train_iter_loss: 0.19407859444618225
train_iter_loss: 0.3910539150238037
train_iter_loss: 0.24504871666431427
train_iter_loss: 0.15435285866260529
train_iter_loss: 0.43195340037345886
train_iter_loss: 0.25937849283218384
train_iter_loss: 0.3739124536514282
train_iter_loss: 0.16607558727264404
train_iter_loss: 0.09350917488336563
train_iter_loss: 0.14085879921913147
train_iter_loss: 0.1117953211069107
train_iter_loss: 0.3472616970539093
train_iter_loss: 0.31856438517570496
train_iter_loss: 0.3153296709060669
train_iter_loss: 0.1998765766620636
train_iter_loss: 0.402942419052124
train_iter_loss: 0.2717694640159607
train_iter_loss: 0.3747449517250061
train_iter_loss: 0.2582666575908661
train_iter_loss: 0.19125086069107056
train_iter_loss: 0.26145926117897034
train_iter_loss: 0.2580243647098541
train_iter_loss: 0.25938403606414795
train_iter_loss: 0.3878208100795746
train_iter_loss: 0.42326006293296814
train_iter_loss: 0.2698827087879181
train_iter_loss: 0.35079190135002136
train_iter_loss: 0.23815417289733887
train_iter_loss: 0.3430623412132263
train_iter_loss: 0.2368575781583786
train_iter_loss: 0.2980646789073944
train_iter_loss: 0.2945703864097595
train_iter_loss: 0.24162837862968445
train_iter_loss: 0.239312082529068
train_iter_loss: 0.33083921670913696
train_iter_loss: 0.2380463033914566
train_iter_loss: 0.5516064167022705
train_iter_loss: 0.38639822602272034
train_iter_loss: 0.1577812135219574
train_iter_loss: 0.2528247833251953
train_iter_loss: 0.3132081925868988
train_iter_loss: 0.2643125057220459
train_iter_loss: 0.29556018114089966
train_iter_loss: 0.3353079855442047
train_iter_loss: 0.2610701620578766
train_iter_loss: 0.39197400212287903
train_iter_loss: 0.18043756484985352
train_iter_loss: 0.3044775724411011
train_iter_loss: 0.45961740612983704
train_iter_loss: 0.24780422449111938
train_iter_loss: 0.17988260090351105
train_iter_loss: 0.24346253275871277
train_iter_loss: 0.30195894837379456
train_iter_loss: 0.33272433280944824
train_iter_loss: 0.20506471395492554
train_iter_loss: 0.3278561532497406
train_iter_loss: 0.29843151569366455
train_iter_loss: 0.33536285161972046
train_iter_loss: 0.3196338415145874
train_iter_loss: 0.2193431258201599
train_iter_loss: 0.3232838213443756
train_iter_loss: 0.3122706711292267
train_iter_loss: 0.14350348711013794
train_iter_loss: 0.24360257387161255
train_iter_loss: 0.27976617217063904
train_iter_loss: 0.27569761872291565
train_iter_loss: 0.3118111789226532
train_iter_loss: 0.27932873368263245
train_iter_loss: 0.5364747047424316
train_iter_loss: 0.4538540840148926
train_iter_loss: 0.16633784770965576
train_iter_loss: 0.3329330086708069
train_iter_loss: 0.21773386001586914
train_iter_loss: 0.27688342332839966
train_iter_loss: 0.34147924184799194
train_iter_loss: 0.25200968980789185
train_iter_loss: 0.11679390072822571
train_iter_loss: 0.3747890889644623
train_iter_loss: 0.2580186128616333
train_iter_loss: 0.3909079134464264
train_iter_loss: 0.237618550658226
train_iter_loss: 0.2953399121761322
train_iter_loss: 0.4067416787147522
train_iter_loss: 0.49443694949150085
train_iter_loss: 0.31535419821739197
train_iter_loss: 0.23614834249019623
train_iter_loss: 0.2766958475112915
train_iter_loss: 0.20398008823394775
train_iter_loss: 0.21324147284030914
train_iter_loss: 0.11156338453292847
train loss :0.2902
---------------------
Validation seg loss: 0.38712570892717196 at epoch 306
epoch =    307/  1000, exp = train
train_iter_loss: 0.37475621700286865
train_iter_loss: 0.28740519285202026
train_iter_loss: 0.3931208550930023
train_iter_loss: 0.12185639142990112
train_iter_loss: 0.288393497467041
train_iter_loss: 0.18555204570293427
train_iter_loss: 0.23992452025413513
train_iter_loss: 0.2157379537820816
train_iter_loss: 0.33352458477020264
train_iter_loss: 0.22537603974342346
train_iter_loss: 0.41463980078697205
train_iter_loss: 0.25360941886901855
train_iter_loss: 0.19229385256767273
train_iter_loss: 0.24363508820533752
train_iter_loss: 0.2556661069393158
train_iter_loss: 0.32573872804641724
train_iter_loss: 0.2614622116088867
train_iter_loss: 0.38819947838783264
train_iter_loss: 0.25968390703201294
train_iter_loss: 0.4119417667388916
train_iter_loss: 0.28057172894477844
train_iter_loss: 0.3679584264755249
train_iter_loss: 0.24711696803569794
train_iter_loss: 0.2819461524486542
train_iter_loss: 0.257484495639801
train_iter_loss: 0.3038833439350128
train_iter_loss: 0.36263608932495117
train_iter_loss: 0.29978707432746887
train_iter_loss: 0.2649720311164856
train_iter_loss: 0.288194477558136
train_iter_loss: 0.25978145003318787
train_iter_loss: 0.34441304206848145
train_iter_loss: 0.23759125173091888
train_iter_loss: 0.2712732255458832
train_iter_loss: 0.2913742661476135
train_iter_loss: 0.4069145619869232
train_iter_loss: 0.14794498682022095
train_iter_loss: 0.1828925460577011
train_iter_loss: 0.273272305727005
train_iter_loss: 0.19446532428264618
train_iter_loss: 0.19401684403419495
train_iter_loss: 0.34419623017311096
train_iter_loss: 0.24977654218673706
train_iter_loss: 0.2236344814300537
train_iter_loss: 0.1579124480485916
train_iter_loss: 0.31509149074554443
train_iter_loss: 0.25673162937164307
train_iter_loss: 0.25294581055641174
train_iter_loss: 0.19703684747219086
train_iter_loss: 0.2500263750553131
train_iter_loss: 0.18050001561641693
train_iter_loss: 0.38010790944099426
train_iter_loss: 0.163114994764328
train_iter_loss: 0.31481799483299255
train_iter_loss: 0.2183874100446701
train_iter_loss: 0.2432713657617569
train_iter_loss: 0.23026923835277557
train_iter_loss: 0.4153968393802643
train_iter_loss: 0.19525155425071716
train_iter_loss: 0.17948117852210999
train_iter_loss: 0.20984306931495667
train_iter_loss: 0.20471198856830597
train_iter_loss: 0.4265614449977875
train_iter_loss: 0.3294905126094818
train_iter_loss: 0.28338757157325745
train_iter_loss: 0.23983608186244965
train_iter_loss: 0.6345067620277405
train_iter_loss: 0.1944170743227005
train_iter_loss: 0.1494748592376709
train_iter_loss: 0.2201312929391861
train_iter_loss: 0.3572431206703186
train_iter_loss: 0.22365188598632812
train_iter_loss: 0.28383979201316833
train_iter_loss: 0.20900483429431915
train_iter_loss: 0.49512994289398193
train_iter_loss: 0.27012374997138977
train_iter_loss: 0.32647421956062317
train_iter_loss: 0.31332167983055115
train_iter_loss: 0.2021467536687851
train_iter_loss: 0.23385018110275269
train_iter_loss: 0.2917967736721039
train_iter_loss: 0.26185858249664307
train_iter_loss: 0.2491736114025116
train_iter_loss: 0.12233544141054153
train_iter_loss: 0.4832996726036072
train_iter_loss: 0.48368585109710693
train_iter_loss: 0.37373754382133484
train_iter_loss: 0.2120189666748047
train_iter_loss: 0.29747873544692993
train_iter_loss: 0.36029911041259766
train_iter_loss: 0.29633453488349915
train_iter_loss: 0.4346309304237366
train_iter_loss: 0.3377334475517273
train_iter_loss: 0.17630413174629211
train_iter_loss: 0.3370431065559387
train_iter_loss: 0.16170763969421387
train_iter_loss: 0.49495574831962585
train_iter_loss: 0.2477126568555832
train_iter_loss: 0.285045325756073
train_iter_loss: 0.2866326570510864
train loss :0.2849
---------------------
Validation seg loss: 0.3757363077217959 at epoch 307
epoch =    308/  1000, exp = train
train_iter_loss: 0.31611377000808716
train_iter_loss: 0.3025248050689697
train_iter_loss: 0.2815593183040619
train_iter_loss: 0.2325589954853058
train_iter_loss: 0.27431720495224
train_iter_loss: 0.21581444144248962
train_iter_loss: 0.30349138379096985
train_iter_loss: 0.2943371832370758
train_iter_loss: 0.3186948597431183
train_iter_loss: 0.580916702747345
train_iter_loss: 0.31700313091278076
train_iter_loss: 0.39282193779945374
train_iter_loss: 0.3219367563724518
train_iter_loss: 0.38502031564712524
train_iter_loss: 0.13756926357746124
train_iter_loss: 0.17480526864528656
train_iter_loss: 0.23628215491771698
train_iter_loss: 0.33498695492744446
train_iter_loss: 0.18914712965488434
train_iter_loss: 0.2765878438949585
train_iter_loss: 0.24672050774097443
train_iter_loss: 0.2361644208431244
train_iter_loss: 0.4258671998977661
train_iter_loss: 0.30662333965301514
train_iter_loss: 0.399062842130661
train_iter_loss: 0.4032149016857147
train_iter_loss: 0.34369325637817383
train_iter_loss: 0.3658740818500519
train_iter_loss: 0.36105552315711975
train_iter_loss: 0.2508181631565094
train_iter_loss: 0.17617075145244598
train_iter_loss: 0.3396197259426117
train_iter_loss: 0.26238998770713806
train_iter_loss: 0.3783136010169983
train_iter_loss: 0.19886761903762817
train_iter_loss: 0.2957359552383423
train_iter_loss: 0.33850330114364624
train_iter_loss: 0.22257296741008759
train_iter_loss: 0.39468029141426086
train_iter_loss: 0.19647659361362457
train_iter_loss: 0.36177170276641846
train_iter_loss: 0.2842869460582733
train_iter_loss: 0.1560155600309372
train_iter_loss: 0.29230812191963196
train_iter_loss: 0.3160954415798187
train_iter_loss: 0.17224307358264923
train_iter_loss: 0.30544036626815796
train_iter_loss: 0.12298443913459778
train_iter_loss: 0.1949237585067749
train_iter_loss: 0.2902633845806122
train_iter_loss: 0.22492195665836334
train_iter_loss: 0.23692737519741058
train_iter_loss: 0.18262091279029846
train_iter_loss: 0.259280264377594
train_iter_loss: 0.27847790718078613
train_iter_loss: 0.4192523956298828
train_iter_loss: 0.31821566820144653
train_iter_loss: 0.27294009923934937
train_iter_loss: 0.24170692265033722
train_iter_loss: 0.15708492696285248
train_iter_loss: 0.14503297209739685
train_iter_loss: 0.28719842433929443
train_iter_loss: 0.3063220679759979
train_iter_loss: 0.3737095892429352
train_iter_loss: 0.3386892080307007
train_iter_loss: 0.28261691331863403
train_iter_loss: 0.15584519505500793
train_iter_loss: 0.23694774508476257
train_iter_loss: 0.4129723310470581
train_iter_loss: 0.30803510546684265
train_iter_loss: 0.14527946710586548
train_iter_loss: 0.21184520423412323
train_iter_loss: 0.1950337141752243
train_iter_loss: 0.22106041014194489
train_iter_loss: 0.22736385464668274
train_iter_loss: 0.3675532341003418
train_iter_loss: 0.2530752718448639
train_iter_loss: 0.3077782690525055
train_iter_loss: 0.28941085934638977
train_iter_loss: 0.18263573944568634
train_iter_loss: 0.20100820064544678
train_iter_loss: 0.3732930123806
train_iter_loss: 0.3210786283016205
train_iter_loss: 0.19216759502887726
train_iter_loss: 0.15086734294891357
train_iter_loss: 0.1017780601978302
train_iter_loss: 0.2738184332847595
train_iter_loss: 0.2099877893924713
train_iter_loss: 0.14190219342708588
train_iter_loss: 0.1191074550151825
train_iter_loss: 0.39534565806388855
train_iter_loss: 0.27656227350234985
train_iter_loss: 0.06025572121143341
train_iter_loss: 0.3076987862586975
train_iter_loss: 0.3549264371395111
train_iter_loss: 0.45821133255958557
train_iter_loss: 0.33336806297302246
train_iter_loss: 0.3833543658256531
train_iter_loss: 0.30782806873321533
train_iter_loss: 0.15428920090198517
train loss :0.2777
---------------------
Validation seg loss: 0.35953027816524485 at epoch 308
epoch =    309/  1000, exp = train
train_iter_loss: 0.4277941584587097
train_iter_loss: 0.33976104855537415
train_iter_loss: 0.2421071082353592
train_iter_loss: 0.2250625491142273
train_iter_loss: 0.09562759101390839
train_iter_loss: 0.34395289421081543
train_iter_loss: 0.46759262681007385
train_iter_loss: 0.23737242817878723
train_iter_loss: 0.3616184890270233
train_iter_loss: 0.1813327968120575
train_iter_loss: 0.19814254343509674
train_iter_loss: 0.3334689438343048
train_iter_loss: 0.2733360230922699
train_iter_loss: 0.2475394606590271
train_iter_loss: 0.3173168897628784
train_iter_loss: 0.28903844952583313
train_iter_loss: 0.2556656002998352
train_iter_loss: 0.4474409520626068
train_iter_loss: 0.23199795186519623
train_iter_loss: 0.25809136033058167
train_iter_loss: 0.19220393896102905
train_iter_loss: 0.1341225504875183
train_iter_loss: 0.21985672414302826
train_iter_loss: 0.33273786306381226
train_iter_loss: 0.2350746989250183
train_iter_loss: 0.40059366822242737
train_iter_loss: 0.18537187576293945
train_iter_loss: 0.44150760769844055
train_iter_loss: 0.20178955793380737
train_iter_loss: 0.23309367895126343
train_iter_loss: 0.2672717869281769
train_iter_loss: 0.32328590750694275
train_iter_loss: 0.3394666612148285
train_iter_loss: 0.12954692542552948
train_iter_loss: 0.34991976618766785
train_iter_loss: 0.5702466368675232
train_iter_loss: 0.360561341047287
train_iter_loss: 0.13307562470436096
train_iter_loss: 0.1614677906036377
train_iter_loss: 0.41536223888397217
train_iter_loss: 0.27401453256607056
train_iter_loss: 0.2668764889240265
train_iter_loss: 0.3572453558444977
train_iter_loss: 0.36291855573654175
train_iter_loss: 0.213810995221138
train_iter_loss: 0.2825378179550171
train_iter_loss: 0.35166293382644653
train_iter_loss: 0.3084057867527008
train_iter_loss: 0.3373045325279236
train_iter_loss: 0.1898828148841858
train_iter_loss: 0.3530733287334442
train_iter_loss: 0.16803419589996338
train_iter_loss: 0.31680506467819214
train_iter_loss: 0.314827024936676
train_iter_loss: 0.10763511061668396
train_iter_loss: 0.23060911893844604
train_iter_loss: 0.29307621717453003
train_iter_loss: 0.2785908877849579
train_iter_loss: 0.3412441909313202
train_iter_loss: 0.38625088334083557
train_iter_loss: 0.2713260352611542
train_iter_loss: 0.24843370914459229
train_iter_loss: 0.3028765916824341
train_iter_loss: 0.26108378171920776
train_iter_loss: 0.2964598834514618
train_iter_loss: 0.3619227707386017
train_iter_loss: 0.2928750216960907
train_iter_loss: 0.1722887009382248
train_iter_loss: 0.12159165740013123
train_iter_loss: 0.1833672672510147
train_iter_loss: 0.2405225932598114
train_iter_loss: 0.29984983801841736
train_iter_loss: 0.23271052539348602
train_iter_loss: 0.15532039105892181
train_iter_loss: 0.4059007167816162
train_iter_loss: 0.27075663208961487
train_iter_loss: 0.17038428783416748
train_iter_loss: 0.27546200156211853
train_iter_loss: 0.38155603408813477
train_iter_loss: 0.19601212441921234
train_iter_loss: 0.3229878544807434
train_iter_loss: 0.43509751558303833
train_iter_loss: 0.28388500213623047
train_iter_loss: 0.3555225133895874
train_iter_loss: 0.21948358416557312
train_iter_loss: 0.2959998846054077
train_iter_loss: 0.287396103143692
train_iter_loss: 0.3127250671386719
train_iter_loss: 0.20519524812698364
train_iter_loss: 0.3430992662906647
train_iter_loss: 0.25076591968536377
train_iter_loss: 0.3265121281147003
train_iter_loss: 0.19136203825473785
train_iter_loss: 0.14502103626728058
train_iter_loss: 0.32294610142707825
train_iter_loss: 0.3831658959388733
train_iter_loss: 0.3513937294483185
train_iter_loss: 0.2774501144886017
train_iter_loss: 0.28552889823913574
train_iter_loss: 0.1436012089252472
train loss :0.2829
---------------------
Validation seg loss: 0.3683232653112906 at epoch 309
epoch =    310/  1000, exp = train
train_iter_loss: 0.3632330000400543
train_iter_loss: 0.3837945759296417
train_iter_loss: 0.3861643373966217
train_iter_loss: 0.2505847215652466
train_iter_loss: 0.2892344295978546
train_iter_loss: 0.2529006898403168
train_iter_loss: 0.39359813928604126
train_iter_loss: 0.1584496796131134
train_iter_loss: 0.29416319727897644
train_iter_loss: 0.23554565012454987
train_iter_loss: 0.3492397964000702
train_iter_loss: 0.28895530104637146
train_iter_loss: 0.26082998514175415
train_iter_loss: 0.2614980638027191
train_iter_loss: 0.30521050095558167
train_iter_loss: 0.3284747302532196
train_iter_loss: 0.31279700994491577
train_iter_loss: 0.2134503424167633
train_iter_loss: 0.1640765219926834
train_iter_loss: 0.29439401626586914
train_iter_loss: 0.4875280261039734
train_iter_loss: 0.3871351182460785
train_iter_loss: 0.35173046588897705
train_iter_loss: 0.297037810087204
train_iter_loss: 0.1208919882774353
train_iter_loss: 0.22913140058517456
train_iter_loss: 0.2968204915523529
train_iter_loss: 0.2939649820327759
train_iter_loss: 0.3342002034187317
train_iter_loss: 0.3150007426738739
train_iter_loss: 0.34582024812698364
train_iter_loss: 0.3417302370071411
train_iter_loss: 0.24666056036949158
train_iter_loss: 0.3295959234237671
train_iter_loss: 0.5262121558189392
train_iter_loss: 0.25565168261528015
train_iter_loss: 0.1722746342420578
train_iter_loss: 0.45624637603759766
train_iter_loss: 0.1006249487400055
train_iter_loss: 0.5332492589950562
train_iter_loss: 0.29766005277633667
train_iter_loss: 0.3288900554180145
train_iter_loss: 0.20823414623737335
train_iter_loss: 0.17606572806835175
train_iter_loss: 0.29340651631355286
train_iter_loss: 0.26282763481140137
train_iter_loss: 0.34246009588241577
train_iter_loss: 0.20331329107284546
train_iter_loss: 0.21647289395332336
train_iter_loss: 0.367622047662735
train_iter_loss: 0.20955738425254822
train_iter_loss: 0.19664880633354187
train_iter_loss: 0.1808222532272339
train_iter_loss: 0.25288230180740356
train_iter_loss: 0.28615155816078186
train_iter_loss: 0.19606605172157288
train_iter_loss: 0.2399730235338211
train_iter_loss: 0.30613988637924194
train_iter_loss: 0.35857507586479187
train_iter_loss: 0.29048192501068115
train_iter_loss: 0.3579580783843994
train_iter_loss: 0.19478048384189606
train_iter_loss: 0.35792863368988037
train_iter_loss: 0.28230178356170654
train_iter_loss: 0.37727177143096924
train_iter_loss: 0.15717115998268127
train_iter_loss: 0.2681051194667816
train_iter_loss: 0.24719540774822235
train_iter_loss: 0.3236415982246399
train_iter_loss: 0.32312554121017456
train_iter_loss: 0.182892844080925
train_iter_loss: 0.3783934712409973
train_iter_loss: 0.3758178949356079
train_iter_loss: 0.18187382817268372
train_iter_loss: 0.507986307144165
train_iter_loss: 0.20797346532344818
train_iter_loss: 0.1181325912475586
train_iter_loss: 0.19367194175720215
train_iter_loss: 0.2977067828178406
train_iter_loss: 0.3557700216770172
train_iter_loss: 0.34923645853996277
train_iter_loss: 0.26822996139526367
train_iter_loss: 0.3363357484340668
train_iter_loss: 0.16041773557662964
train_iter_loss: 0.21551012992858887
train_iter_loss: 0.18803662061691284
train_iter_loss: 0.35035762190818787
train_iter_loss: 0.21960562467575073
train_iter_loss: 0.2509480118751526
train_iter_loss: 0.36788055300712585
train_iter_loss: 0.36433133482933044
train_iter_loss: 0.20487873256206512
train_iter_loss: 0.33333900570869446
train_iter_loss: 0.17166291177272797
train_iter_loss: 0.3155772387981415
train_iter_loss: 0.2872219979763031
train_iter_loss: 0.2230331301689148
train_iter_loss: 0.39479124546051025
train_iter_loss: 0.16826193034648895
train_iter_loss: 0.14911241829395294
train loss :0.2871
---------------------
Validation seg loss: 0.3615209645644393 at epoch 310
epoch =    311/  1000, exp = train
train_iter_loss: 0.26527589559555054
train_iter_loss: 0.17669686675071716
train_iter_loss: 0.31274867057800293
train_iter_loss: 0.4376208484172821
train_iter_loss: 0.15221354365348816
train_iter_loss: 0.3404630422592163
train_iter_loss: 0.34929677844047546
train_iter_loss: 0.24963681399822235
train_iter_loss: 0.3337053954601288
train_iter_loss: 0.32362234592437744
train_iter_loss: 0.18846756219863892
train_iter_loss: 0.11708837747573853
train_iter_loss: 0.3243623673915863
train_iter_loss: 0.2885761559009552
train_iter_loss: 0.2565963566303253
train_iter_loss: 0.2843385636806488
train_iter_loss: 0.14737527072429657
train_iter_loss: 0.20630984008312225
train_iter_loss: 0.34966588020324707
train_iter_loss: 0.49020788073539734
train_iter_loss: 0.19275762140750885
train_iter_loss: 0.20708496868610382
train_iter_loss: 0.32705157995224
train_iter_loss: 0.24124379456043243
train_iter_loss: 0.22235767543315887
train_iter_loss: 0.3900366425514221
train_iter_loss: 0.4000951647758484
train_iter_loss: 0.3175880014896393
train_iter_loss: 0.21117809414863586
train_iter_loss: 0.3117809593677521
train_iter_loss: 0.12634561955928802
train_iter_loss: 0.3843958079814911
train_iter_loss: 0.254845529794693
train_iter_loss: 0.28916728496551514
train_iter_loss: 0.43729284405708313
train_iter_loss: 0.28156960010528564
train_iter_loss: 0.2686288356781006
train_iter_loss: 0.19922234117984772
train_iter_loss: 0.24505296349525452
train_iter_loss: 0.26739946007728577
train_iter_loss: 0.2774446904659271
train_iter_loss: 0.061798274517059326
train_iter_loss: 0.2559834122657776
train_iter_loss: 0.3062281012535095
train_iter_loss: 0.30145713686943054
train_iter_loss: 0.33806708455085754
train_iter_loss: 0.3811071813106537
train_iter_loss: 0.15962909162044525
train_iter_loss: 0.33029282093048096
train_iter_loss: 0.2205737978219986
train_iter_loss: 0.29693803191185
train_iter_loss: 0.19351264834403992
train_iter_loss: 0.17659519612789154
train_iter_loss: 0.28083908557891846
train_iter_loss: 0.08907093852758408
train_iter_loss: 0.25974559783935547
train_iter_loss: 0.2386999875307083
train_iter_loss: 0.33821552991867065
train_iter_loss: 0.32678693532943726
train_iter_loss: 0.30682140588760376
train_iter_loss: 0.24579432606697083
train_iter_loss: 0.23552767932415009
train_iter_loss: 0.24543142318725586
train_iter_loss: 0.3384818136692047
train_iter_loss: 0.2519359290599823
train_iter_loss: 0.327651709318161
train_iter_loss: 0.36949291825294495
train_iter_loss: 0.19090764224529266
train_iter_loss: 0.31267455220222473
train_iter_loss: 0.3564515709877014
train_iter_loss: 0.1445140689611435
train_iter_loss: 0.30772024393081665
train_iter_loss: 0.370952844619751
train_iter_loss: 0.15574179589748383
train_iter_loss: 0.40502578020095825
train_iter_loss: 0.25552910566329956
train_iter_loss: 0.3555358648300171
train_iter_loss: 0.5307316780090332
train_iter_loss: 0.37962910532951355
train_iter_loss: 0.3129693269729614
train_iter_loss: 0.33559879660606384
train_iter_loss: 0.3034619688987732
train_iter_loss: 0.1761738508939743
train_iter_loss: 0.19451633095741272
train_iter_loss: 0.3355187177658081
train_iter_loss: 0.2756340205669403
train_iter_loss: 0.3679034113883972
train_iter_loss: 0.37268874049186707
train_iter_loss: 0.258952260017395
train_iter_loss: 0.2356269508600235
train_iter_loss: 0.36954307556152344
train_iter_loss: 0.3181209862232208
train_iter_loss: 0.3138020634651184
train_iter_loss: 0.39810338616371155
train_iter_loss: 0.26032111048698425
train_iter_loss: 0.03816310688853264
train_iter_loss: 0.35014861822128296
train_iter_loss: 0.1595292091369629
train_iter_loss: 0.3503091633319855
train_iter_loss: 0.14907047152519226
train loss :0.2825
---------------------
Validation seg loss: 0.3714901056527248 at epoch 311
epoch =    312/  1000, exp = train
train_iter_loss: 0.3637956380844116
train_iter_loss: 0.14404457807540894
train_iter_loss: 0.3632552921772003
train_iter_loss: 0.2617436349391937
train_iter_loss: 0.28046542406082153
train_iter_loss: 0.2065548151731491
train_iter_loss: 0.37547844648361206
train_iter_loss: 0.3286513388156891
train_iter_loss: 0.21963432431221008
train_iter_loss: 0.3707021474838257
train_iter_loss: 0.20384372770786285
train_iter_loss: 0.28277426958084106
train_iter_loss: 0.30857497453689575
train_iter_loss: 0.3361586332321167
train_iter_loss: 0.28720584511756897
train_iter_loss: 0.40593522787094116
train_iter_loss: 0.3364849090576172
train_iter_loss: 0.3674982488155365
train_iter_loss: 0.2583591341972351
train_iter_loss: 0.2543714642524719
train_iter_loss: 0.21800976991653442
train_iter_loss: 0.294522762298584
train_iter_loss: 0.35871875286102295
train_iter_loss: 0.07865729182958603
train_iter_loss: 0.3343757390975952
train_iter_loss: 0.31826743483543396
train_iter_loss: 0.2659371495246887
train_iter_loss: 0.2503347396850586
train_iter_loss: 0.180647611618042
train_iter_loss: 0.30742132663726807
train_iter_loss: 0.12905843555927277
train_iter_loss: 0.3170471787452698
train_iter_loss: 0.3434927761554718
train_iter_loss: 0.28467318415641785
train_iter_loss: 0.13347505033016205
train_iter_loss: 0.27794045209884644
train_iter_loss: 0.3047499656677246
train_iter_loss: 0.30850017070770264
train_iter_loss: 0.2338699996471405
train_iter_loss: 0.11100989580154419
train_iter_loss: 0.29512453079223633
train_iter_loss: 0.3657950758934021
train_iter_loss: 0.19497504830360413
train_iter_loss: 0.1514207422733307
train_iter_loss: 0.22478002309799194
train_iter_loss: 0.3164288103580475
train_iter_loss: 0.32186076045036316
train_iter_loss: 0.3126388490200043
train_iter_loss: 0.3679906725883484
train_iter_loss: 0.23914778232574463
train_iter_loss: 0.23503006994724274
train_iter_loss: 0.12260850518941879
train_iter_loss: 0.2631760835647583
train_iter_loss: 0.22259217500686646
train_iter_loss: 0.19739370048046112
train_iter_loss: 0.3912781774997711
train_iter_loss: 0.48395588994026184
train_iter_loss: 0.37605321407318115
train_iter_loss: 0.3517057001590729
train_iter_loss: 0.3554480969905853
train_iter_loss: 0.3653167188167572
train_iter_loss: 0.20170879364013672
train_iter_loss: 0.3626377284526825
train_iter_loss: 0.24698346853256226
train_iter_loss: 0.3506038188934326
train_iter_loss: 0.40420857071876526
train_iter_loss: 0.30239951610565186
train_iter_loss: 0.1969577968120575
train_iter_loss: 0.34374505281448364
train_iter_loss: 0.3923836052417755
train_iter_loss: 0.1694830060005188
train_iter_loss: 0.2674155533313751
train_iter_loss: 0.17652621865272522
train_iter_loss: 0.3703397512435913
train_iter_loss: 0.18729731440544128
train_iter_loss: 0.1629079133272171
train_iter_loss: 0.48708391189575195
train_iter_loss: 0.23134402930736542
train_iter_loss: 0.26079824566841125
train_iter_loss: 0.29054683446884155
train_iter_loss: 0.19998286664485931
train_iter_loss: 0.2692651152610779
train_iter_loss: 0.3206446170806885
train_iter_loss: 0.2687671482563019
train_iter_loss: 0.2776259481906891
train_iter_loss: 0.32704901695251465
train_iter_loss: 0.3807487487792969
train_iter_loss: 0.20236057043075562
train_iter_loss: 0.31272321939468384
train_iter_loss: 0.3165948688983917
train_iter_loss: 0.09355323761701584
train_iter_loss: 0.2991395890712738
train_iter_loss: 0.2699717879295349
train_iter_loss: 0.20361442863941193
train_iter_loss: 0.29273632168769836
train_iter_loss: 0.34299740195274353
train_iter_loss: 0.3511933386325836
train_iter_loss: 0.14214617013931274
train_iter_loss: 0.3479638397693634
train_iter_loss: 0.22499729692935944
train loss :0.2829
---------------------
Validation seg loss: 0.3798896074119323 at epoch 312
epoch =    313/  1000, exp = train
train_iter_loss: 0.2969505786895752
train_iter_loss: 0.20521993935108185
train_iter_loss: 0.33672401309013367
train_iter_loss: 0.21364589035511017
train_iter_loss: 0.1634708046913147
train_iter_loss: 0.18320800364017487
train_iter_loss: 0.2219487428665161
train_iter_loss: 0.3409366309642792
train_iter_loss: 0.18912436068058014
train_iter_loss: 0.3095392882823944
train_iter_loss: 0.23664423823356628
train_iter_loss: 0.27876463532447815
train_iter_loss: 0.29775071144104004
train_iter_loss: 0.26333823800086975
train_iter_loss: 0.22159625589847565
train_iter_loss: 0.25734448432922363
train_iter_loss: 0.2485215663909912
train_iter_loss: 0.3697153627872467
train_iter_loss: 0.28505417704582214
train_iter_loss: 0.3330913782119751
train_iter_loss: 0.23403440415859222
train_iter_loss: 0.2507396936416626
train_iter_loss: 0.21441981196403503
train_iter_loss: 0.24394530057907104
train_iter_loss: 0.2812846302986145
train_iter_loss: 0.4683744013309479
train_iter_loss: 0.26426851749420166
train_iter_loss: 0.24836647510528564
train_iter_loss: 0.149196058511734
train_iter_loss: 0.5487412810325623
train_iter_loss: 0.2088925540447235
train_iter_loss: 0.20253883302211761
train_iter_loss: 0.09268444031476974
train_iter_loss: 0.26794955134391785
train_iter_loss: 0.12334639579057693
train_iter_loss: 0.3296198844909668
train_iter_loss: 0.26091820001602173
train_iter_loss: 0.31067416071891785
train_iter_loss: 0.3174518346786499
train_iter_loss: 0.2866508662700653
train_iter_loss: 0.2699602544307709
train_iter_loss: 0.2161218672990799
train_iter_loss: 0.23880311846733093
train_iter_loss: 0.35187476873397827
train_iter_loss: 0.2602228820323944
train_iter_loss: 0.16316667199134827
train_iter_loss: 0.27525466680526733
train_iter_loss: 0.26438939571380615
train_iter_loss: 0.24388369917869568
train_iter_loss: 0.2445441633462906
train_iter_loss: 0.30318498611450195
train_iter_loss: 0.2388133555650711
train_iter_loss: 0.24220013618469238
train_iter_loss: 0.17742660641670227
train_iter_loss: 0.2548739016056061
train_iter_loss: 0.32892918586730957
train_iter_loss: 0.18448506295681
train_iter_loss: 0.20774385333061218
train_iter_loss: 0.3911585509777069
train_iter_loss: 0.2298242151737213
train_iter_loss: 0.33590805530548096
train_iter_loss: 0.4141528606414795
train_iter_loss: 0.28710153698921204
train_iter_loss: 0.3670739233493805
train_iter_loss: 0.38649940490722656
train_iter_loss: 0.4526066482067108
train_iter_loss: 0.1827169954776764
train_iter_loss: 0.1761465221643448
train_iter_loss: 0.17382530868053436
train_iter_loss: 0.2618323266506195
train_iter_loss: 0.23457561433315277
train_iter_loss: 0.3815513849258423
train_iter_loss: 0.28543907403945923
train_iter_loss: 0.22387872636318207
train_iter_loss: 0.32738369703292847
train_iter_loss: 0.29037806391716003
train_iter_loss: 0.2823527455329895
train_iter_loss: 0.25436338782310486
train_iter_loss: 0.24072231352329254
train_iter_loss: 0.19485914707183838
train_iter_loss: 0.3880150318145752
train_iter_loss: 0.2935698330402374
train_iter_loss: 0.5102621912956238
train_iter_loss: 0.16456378996372223
train_iter_loss: 0.0536842904984951
train_iter_loss: 0.4109571874141693
train_iter_loss: 0.201448455452919
train_iter_loss: 0.14422470331192017
train_iter_loss: 0.29066306352615356
train_iter_loss: 0.25014743208885193
train_iter_loss: 0.35784628987312317
train_iter_loss: 0.2300601750612259
train_iter_loss: 0.2654664218425751
train_iter_loss: 0.29838597774505615
train_iter_loss: 0.36139899492263794
train_iter_loss: 0.2779679000377655
train_iter_loss: 0.3187011778354645
train_iter_loss: 0.3819824159145355
train_iter_loss: 0.2168412208557129
train_iter_loss: 0.29512518644332886
train loss :0.2739
---------------------
Validation seg loss: 0.43863368698589084 at epoch 313
epoch =    314/  1000, exp = train
train_iter_loss: 0.09329573065042496
train_iter_loss: 0.36464783549308777
train_iter_loss: 0.11919962614774704
train_iter_loss: 0.38178569078445435
train_iter_loss: 0.30216723680496216
train_iter_loss: 0.19857870042324066
train_iter_loss: 0.23621243238449097
train_iter_loss: 0.23106302320957184
train_iter_loss: 0.2543901205062866
train_iter_loss: 0.2813981771469116
train_iter_loss: 0.23386836051940918
train_iter_loss: 0.4375203549861908
train_iter_loss: 0.20907628536224365
train_iter_loss: 0.18698234856128693
train_iter_loss: 0.10892333090305328
train_iter_loss: 0.41614991426467896
train_iter_loss: 0.2874847650527954
train_iter_loss: 0.228645920753479
train_iter_loss: 0.415931761264801
train_iter_loss: 0.23349392414093018
train_iter_loss: 0.19575613737106323
train_iter_loss: 0.20093248784542084
train_iter_loss: 0.3156176209449768
train_iter_loss: 0.35218650102615356
train_iter_loss: 0.31442439556121826
train_iter_loss: 0.25560054183006287
train_iter_loss: 0.15960268676280975
train_iter_loss: 0.29537123441696167
train_iter_loss: 0.303303599357605
train_iter_loss: 0.475859671831131
train_iter_loss: 0.35077908635139465
train_iter_loss: 0.4424075186252594
train_iter_loss: 0.41127586364746094
train_iter_loss: 0.1947486400604248
train_iter_loss: 0.23569494485855103
train_iter_loss: 0.33564847707748413
train_iter_loss: 0.21945393085479736
train_iter_loss: 0.30721160769462585
train_iter_loss: 0.27593687176704407
train_iter_loss: 0.09132161736488342
train_iter_loss: 0.22361621260643005
train_iter_loss: 0.28241899609565735
train_iter_loss: 0.20570959150791168
train_iter_loss: 0.31122955679893494
train_iter_loss: 0.2546370029449463
train_iter_loss: 0.1840345561504364
train_iter_loss: 0.31303250789642334
train_iter_loss: 0.22302868962287903
train_iter_loss: 0.2700255215167999
train_iter_loss: 0.22522592544555664
train_iter_loss: 0.2628577649593353
train_iter_loss: 0.3634278178215027
train_iter_loss: 0.17748938500881195
train_iter_loss: 0.21387162804603577
train_iter_loss: 0.2104732245206833
train_iter_loss: 0.16224965453147888
train_iter_loss: 0.40781310200691223
train_iter_loss: 0.2602393627166748
train_iter_loss: 0.30193179845809937
train_iter_loss: 0.2488984912633896
train_iter_loss: 0.2659209668636322
train_iter_loss: 0.37229031324386597
train_iter_loss: 0.22416852414608002
train_iter_loss: 0.27640631794929504
train_iter_loss: 0.26629889011383057
train_iter_loss: 0.31803208589553833
train_iter_loss: 0.1937403678894043
train_iter_loss: 0.20443543791770935
train_iter_loss: 0.24824781715869904
train_iter_loss: 0.3694113790988922
train_iter_loss: 0.11729047447443008
train_iter_loss: 0.19724063575267792
train_iter_loss: 0.2753567397594452
train_iter_loss: 0.28888314962387085
train_iter_loss: 0.39494621753692627
train_iter_loss: 0.14953581988811493
train_iter_loss: 0.265084832906723
train_iter_loss: 0.4006699323654175
train_iter_loss: 0.19135501980781555
train_iter_loss: 0.17842769622802734
train_iter_loss: 0.21124279499053955
train_iter_loss: 0.45851385593414307
train_iter_loss: 0.2825588285923004
train_iter_loss: 0.38082215189933777
train_iter_loss: 0.5507219433784485
train_iter_loss: 0.19965146481990814
train_iter_loss: 0.22547996044158936
train_iter_loss: 0.16142210364341736
train_iter_loss: 0.29733824729919434
train_iter_loss: 0.4252586364746094
train_iter_loss: 0.2874840497970581
train_iter_loss: 0.28740394115448
train_iter_loss: 0.2776655852794647
train_iter_loss: 0.16850434243679047
train_iter_loss: 0.26311638951301575
train_iter_loss: 0.25235798954963684
train_iter_loss: 0.3685961961746216
train_iter_loss: 0.21479810774326324
train_iter_loss: 0.16628186404705048
train_iter_loss: 0.5397335886955261
train loss :0.2755
---------------------
Validation seg loss: 0.3571412253000264 at epoch 314
epoch =    315/  1000, exp = train
train_iter_loss: 0.2723581790924072
train_iter_loss: 0.36663177609443665
train_iter_loss: 0.3191884458065033
train_iter_loss: 0.2500775158405304
train_iter_loss: 0.21556973457336426
train_iter_loss: 0.23495756089687347
train_iter_loss: 0.20718610286712646
train_iter_loss: 0.2913121283054352
train_iter_loss: 0.3742668628692627
train_iter_loss: 0.36604881286621094
train_iter_loss: 0.2943277955055237
train_iter_loss: 0.5314304232597351
train_iter_loss: 0.36177343130111694
train_iter_loss: 0.37786704301834106
train_iter_loss: 0.24734491109848022
train_iter_loss: 0.2587452828884125
train_iter_loss: 0.34660083055496216
train_iter_loss: 0.23289012908935547
train_iter_loss: 0.32008877396583557
train_iter_loss: 0.23880352079868317
train_iter_loss: 0.20445767045021057
train_iter_loss: 0.2202833741903305
train_iter_loss: 0.33794528245925903
train_iter_loss: 0.24540835618972778
train_iter_loss: 0.25928160548210144
train_iter_loss: 0.32676833868026733
train_iter_loss: 0.24893894791603088
train_iter_loss: 0.2762901186943054
train_iter_loss: 0.16179890930652618
train_iter_loss: 0.4355306625366211
train_iter_loss: 0.23579064011573792
train_iter_loss: 0.20571696758270264
train_iter_loss: 0.1706799864768982
train_iter_loss: 0.3819030821323395
train_iter_loss: 0.2529991567134857
train_iter_loss: 0.20702435076236725
train_iter_loss: 0.18857216835021973
train_iter_loss: 0.3702614903450012
train_iter_loss: 0.30839845538139343
train_iter_loss: 0.3110768496990204
train_iter_loss: 0.2419998049736023
train_iter_loss: 0.3926343321800232
train_iter_loss: 0.32572102546691895
train_iter_loss: 0.2660967707633972
train_iter_loss: 0.2118159681558609
train_iter_loss: 0.15394119918346405
train_iter_loss: 0.2526116371154785
train_iter_loss: 0.2698810398578644
train_iter_loss: 0.126708984375
train_iter_loss: 0.23012955486774445
train_iter_loss: 0.28069084882736206
train_iter_loss: 0.43333566188812256
train_iter_loss: 0.28897079825401306
train_iter_loss: 0.17645807564258575
train_iter_loss: 0.32042983174324036
train_iter_loss: 0.3068477511405945
train_iter_loss: 0.2977907955646515
train_iter_loss: 0.3853648602962494
train_iter_loss: 0.23961114883422852
train_iter_loss: 0.28026992082595825
train_iter_loss: 0.2115698903799057
train_iter_loss: 0.33651188015937805
train_iter_loss: 0.23696041107177734
train_iter_loss: 0.2802960276603699
train_iter_loss: 0.31863054633140564
train_iter_loss: 0.30732956528663635
train_iter_loss: 0.2444889396429062
train_iter_loss: 0.20660839974880219
train_iter_loss: 0.30709996819496155
train_iter_loss: 0.2673111855983734
train_iter_loss: 0.38453733921051025
train_iter_loss: 0.38749781250953674
train_iter_loss: 0.1591438204050064
train_iter_loss: 0.2833911180496216
train_iter_loss: 0.405537873506546
train_iter_loss: 0.20770296454429626
train_iter_loss: 0.22213785350322723
train_iter_loss: 0.1835021823644638
train_iter_loss: 0.287445068359375
train_iter_loss: 0.21534639596939087
train_iter_loss: 0.32031452655792236
train_iter_loss: 0.3912620544433594
train_iter_loss: 0.20551425218582153
train_iter_loss: 0.2836448848247528
train_iter_loss: 0.2829599380493164
train_iter_loss: 0.1707690805196762
train_iter_loss: 0.29774174094200134
train_iter_loss: 0.2574562430381775
train_iter_loss: 0.4889675974845886
train_iter_loss: 0.2809858024120331
train_iter_loss: 0.22191238403320312
train_iter_loss: 0.23730894923210144
train_iter_loss: 0.3181115388870239
train_iter_loss: 0.18788711726665497
train_iter_loss: 0.1937319040298462
train_iter_loss: 0.1448514461517334
train_iter_loss: 0.36595603823661804
train_iter_loss: 0.28053125739097595
train_iter_loss: 0.5345871448516846
train_iter_loss: 0.14309342205524445
train loss :0.2827
---------------------
Validation seg loss: 0.3592078920172633 at epoch 315
epoch =    316/  1000, exp = train
train_iter_loss: 0.37726303935050964
train_iter_loss: 0.28197014331817627
train_iter_loss: 0.3440079689025879
train_iter_loss: 0.35091784596443176
train_iter_loss: 0.37417569756507874
train_iter_loss: 0.14131949841976166
train_iter_loss: 0.2467086911201477
train_iter_loss: 0.27304065227508545
train_iter_loss: 0.2852739989757538
train_iter_loss: 0.12399522215127945
train_iter_loss: 0.2656964361667633
train_iter_loss: 0.2304885983467102
train_iter_loss: 0.2648564875125885
train_iter_loss: 0.35283055901527405
train_iter_loss: 0.3974587917327881
train_iter_loss: 0.24400694668293
train_iter_loss: 0.15740835666656494
train_iter_loss: 0.21188826858997345
train_iter_loss: 0.2375585436820984
train_iter_loss: 0.20212896168231964
train_iter_loss: 0.28010693192481995
train_iter_loss: 0.4342370331287384
train_iter_loss: 0.2852713465690613
train_iter_loss: 0.11097970604896545
train_iter_loss: 0.40180152654647827
train_iter_loss: 0.4954485595226288
train_iter_loss: 0.4407196342945099
train_iter_loss: 0.2237369269132614
train_iter_loss: 0.274284303188324
train_iter_loss: 0.2804463803768158
train_iter_loss: 0.23239082098007202
train_iter_loss: 0.28475603461265564
train_iter_loss: 0.27994871139526367
train_iter_loss: 0.20707501471042633
train_iter_loss: 0.11094319075345993
train_iter_loss: 0.3736817538738251
train_iter_loss: 0.20091886818408966
train_iter_loss: 0.25104033946990967
train_iter_loss: 0.3079786002635956
train_iter_loss: 0.20898833870887756
train_iter_loss: 0.10798374563455582
train_iter_loss: 0.28700175881385803
train_iter_loss: 0.38408517837524414
train_iter_loss: 0.20616194605827332
train_iter_loss: 0.28640270233154297
train_iter_loss: 0.2827535569667816
train_iter_loss: 0.4385802447795868
train_iter_loss: 0.24873630702495575
train_iter_loss: 0.19937318563461304
train_iter_loss: 0.3101251423358917
train_iter_loss: 0.19154734909534454
train_iter_loss: 0.32191702723503113
train_iter_loss: 0.1106778159737587
train_iter_loss: 0.22058367729187012
train_iter_loss: 0.2903376817703247
train_iter_loss: 0.3296062648296356
train_iter_loss: 0.1710718274116516
train_iter_loss: 0.21614111959934235
train_iter_loss: 0.2764488756656647
train_iter_loss: 0.3503339886665344
train_iter_loss: 0.2742951512336731
train_iter_loss: 0.36067214608192444
train_iter_loss: 0.1495828777551651
train_iter_loss: 0.3149777352809906
train_iter_loss: 0.34922608733177185
train_iter_loss: 0.22039921581745148
train_iter_loss: 0.2704601287841797
train_iter_loss: 0.34121984243392944
train_iter_loss: 0.4980180859565735
train_iter_loss: 0.3595040440559387
train_iter_loss: 0.26629841327667236
train_iter_loss: 0.25900721549987793
train_iter_loss: 0.22148671746253967
train_iter_loss: 0.2776840925216675
train_iter_loss: 0.44186341762542725
train_iter_loss: 0.29603850841522217
train_iter_loss: 0.1262570023536682
train_iter_loss: 0.446207731962204
train_iter_loss: 0.23579123616218567
train_iter_loss: 0.25218936800956726
train_iter_loss: 0.3235507607460022
train_iter_loss: 0.15765869617462158
train_iter_loss: 0.31520453095436096
train_iter_loss: 0.6542201042175293
train_iter_loss: 0.33828458189964294
train_iter_loss: 0.42534056305885315
train_iter_loss: 0.2290606051683426
train_iter_loss: 0.19571195542812347
train_iter_loss: 0.18972788751125336
train_iter_loss: 0.1760033816099167
train_iter_loss: 0.31819769740104675
train_iter_loss: 0.36418628692626953
train_iter_loss: 0.21316759288311005
train_iter_loss: 0.24115018546581268
train_iter_loss: 0.1951504796743393
train_iter_loss: 0.19853554666042328
train_iter_loss: 0.36300575733184814
train_iter_loss: 0.2970849871635437
train_iter_loss: 0.33368349075317383
train_iter_loss: 0.3563592731952667
train loss :0.2841
---------------------
Validation seg loss: 0.3876949637196958 at epoch 316
epoch =    317/  1000, exp = train
train_iter_loss: 0.2935406267642975
train_iter_loss: 0.262964129447937
train_iter_loss: 0.26321470737457275
train_iter_loss: 0.22302836179733276
train_iter_loss: 0.3147253394126892
train_iter_loss: 0.42505311965942383
train_iter_loss: 0.2169938087463379
train_iter_loss: 0.2512318789958954
train_iter_loss: 0.21582981944084167
train_iter_loss: 0.2937735617160797
train_iter_loss: 0.27593380212783813
train_iter_loss: 0.37436455488204956
train_iter_loss: 0.30164116621017456
train_iter_loss: 0.22070905566215515
train_iter_loss: 0.29024261236190796
train_iter_loss: 0.24851025640964508
train_iter_loss: 0.21344824135303497
train_iter_loss: 0.21450982987880707
train_iter_loss: 0.32823842763900757
train_iter_loss: 0.15884801745414734
train_iter_loss: 0.27269431948661804
train_iter_loss: 0.4567711651325226
train_iter_loss: 0.2893429100513458
train_iter_loss: 0.344129741191864
train_iter_loss: 0.12686797976493835
train_iter_loss: 0.1521938443183899
train_iter_loss: 0.4372974634170532
train_iter_loss: 0.16125763952732086
train_iter_loss: 0.35932624340057373
train_iter_loss: 0.5161214470863342
train_iter_loss: 0.2232186198234558
train_iter_loss: 0.41203925013542175
train_iter_loss: 0.19726242125034332
train_iter_loss: 0.3499690592288971
train_iter_loss: 0.4238497316837311
train_iter_loss: 0.24097511172294617
train_iter_loss: 0.291401207447052
train_iter_loss: 0.21079672873020172
train_iter_loss: 0.29304200410842896
train_iter_loss: 0.2633608281612396
train_iter_loss: 0.18453407287597656
train_iter_loss: 0.4562154710292816
train_iter_loss: 0.29135483503341675
train_iter_loss: 0.33111387491226196
train_iter_loss: 0.31772443652153015
train_iter_loss: 0.18452408909797668
train_iter_loss: 0.3405297100543976
train_iter_loss: 0.28965166211128235
train_iter_loss: 0.2701403498649597
train_iter_loss: 0.42586183547973633
train_iter_loss: 0.34470823407173157
train_iter_loss: 0.251106321811676
train_iter_loss: 0.27626562118530273
train_iter_loss: 0.20019501447677612
train_iter_loss: 0.30622464418411255
train_iter_loss: 0.40460440516471863
train_iter_loss: 0.22635279595851898
train_iter_loss: 0.1868477314710617
train_iter_loss: 0.3010852038860321
train_iter_loss: 0.2999807596206665
train_iter_loss: 0.5266952514648438
train_iter_loss: 0.46581947803497314
train_iter_loss: 0.08367051929235458
train_iter_loss: 0.26638850569725037
train_iter_loss: 0.6464992165565491
train_iter_loss: 0.2976880669593811
train_iter_loss: 0.3453378677368164
train_iter_loss: 0.41426053643226624
train_iter_loss: 0.28650224208831787
train_iter_loss: 0.25606220960617065
train_iter_loss: 0.3229183256626129
train_iter_loss: 0.22462014853954315
train_iter_loss: 0.183768168091774
train_iter_loss: 0.29934459924697876
train_iter_loss: 0.23855984210968018
train_iter_loss: 0.30376172065734863
train_iter_loss: 0.4093506634235382
train_iter_loss: 0.05859840288758278
train_iter_loss: 0.2638319730758667
train_iter_loss: 0.24722333252429962
train_iter_loss: 0.12802115082740784
train_iter_loss: 0.33466967940330505
train_iter_loss: 0.2623579800128937
train_iter_loss: 0.25621336698532104
train_iter_loss: 0.25783613324165344
train_iter_loss: 0.3151017427444458
train_iter_loss: 0.11325839906930923
train_iter_loss: 0.2549193501472473
train_iter_loss: 0.40833574533462524
train_iter_loss: 0.4137251675128937
train_iter_loss: 0.36361163854599
train_iter_loss: 0.2198965698480606
train_iter_loss: 0.14717096090316772
train_iter_loss: 0.24611309170722961
train_iter_loss: 0.3325634300708771
train_iter_loss: 0.201134592294693
train_iter_loss: 0.17269760370254517
train_iter_loss: 0.14323298633098602
train_iter_loss: 0.25545650720596313
train_iter_loss: 0.2074122279882431
train loss :0.2876
---------------------
Validation seg loss: 0.35462978369784803 at epoch 317
********************
best_val_epoch_loss:  0.35462978369784803
MODEL UPDATED
epoch =    318/  1000, exp = train
train_iter_loss: 0.4166239798069
train_iter_loss: 0.2671436369419098
train_iter_loss: 0.20238032937049866
train_iter_loss: 0.3004744052886963
train_iter_loss: 0.2791232168674469
train_iter_loss: 0.44746139645576477
train_iter_loss: 0.2998616695404053
train_iter_loss: 0.3835937976837158
train_iter_loss: 0.3022249937057495
train_iter_loss: 0.19804756343364716
train_iter_loss: 0.2215082198381424
train_iter_loss: 0.19247916340827942
train_iter_loss: 0.3018077313899994
train_iter_loss: 0.23436762392520905
train_iter_loss: 0.5221298336982727
train_iter_loss: 0.19129903614521027
train_iter_loss: 0.17463919520378113
train_iter_loss: 0.22760270535945892
train_iter_loss: 0.2107899934053421
train_iter_loss: 0.6274237632751465
train_iter_loss: 0.2846512496471405
train_iter_loss: 0.3070082366466522
train_iter_loss: 0.38796234130859375
train_iter_loss: 0.18584474921226501
train_iter_loss: 0.3154042661190033
train_iter_loss: 0.22060593962669373
train_iter_loss: 0.22815757989883423
train_iter_loss: 0.261476993560791
train_iter_loss: 0.3860228657722473
train_iter_loss: 0.21484282612800598
train_iter_loss: 0.26056429743766785
train_iter_loss: 0.10413545370101929
train_iter_loss: 0.20395199954509735
train_iter_loss: 0.2873378396034241
train_iter_loss: 0.32436543703079224
train_iter_loss: 0.3920373320579529
train_iter_loss: 0.12308469414710999
train_iter_loss: 0.1982656866312027
train_iter_loss: 0.15959550440311432
train_iter_loss: 0.40221214294433594
train_iter_loss: 0.10910827666521072
train_iter_loss: 0.29143381118774414
train_iter_loss: 0.3810817301273346
train_iter_loss: 0.26017796993255615
train_iter_loss: 0.19301290810108185
train_iter_loss: 0.2663152515888214
train_iter_loss: 0.1999691277742386
train_iter_loss: 0.36885392665863037
train_iter_loss: 0.3070557415485382
train_iter_loss: 0.2442363202571869
train_iter_loss: 0.3102168142795563
train_iter_loss: 0.20417287945747375
train_iter_loss: 0.28850826621055603
train_iter_loss: 0.3376094400882721
train_iter_loss: 0.2233155220746994
train_iter_loss: 0.23166392743587494
train_iter_loss: 0.33595338463783264
train_iter_loss: 0.11484195291996002
train_iter_loss: 0.215041846036911
train_iter_loss: 0.18742738664150238
train_iter_loss: 0.29715049266815186
train_iter_loss: 0.10877980291843414
train_iter_loss: 0.3408726751804352
train_iter_loss: 0.33984801173210144
train_iter_loss: 0.34919291734695435
train_iter_loss: 0.27881497144699097
train_iter_loss: 0.4571484625339508
train_iter_loss: 0.5274897217750549
train_iter_loss: 0.19620893895626068
train_iter_loss: 0.3062327802181244
train_iter_loss: 0.4035169780254364
train_iter_loss: 0.2519558370113373
train_iter_loss: 0.27949100732803345
train_iter_loss: 0.33392781019210815
train_iter_loss: 0.33908507227897644
train_iter_loss: 0.3200642466545105
train_iter_loss: 0.27508994936943054
train_iter_loss: 0.27760636806488037
train_iter_loss: 0.2803659439086914
train_iter_loss: 0.35736939311027527
train_iter_loss: 0.13631559908390045
train_iter_loss: 0.16538362205028534
train_iter_loss: 0.24225212633609772
train_iter_loss: 0.2561168968677521
train_iter_loss: 0.1835658699274063
train_iter_loss: 0.23199839890003204
train_iter_loss: 0.2194063812494278
train_iter_loss: 0.45542559027671814
train_iter_loss: 0.3630602955818176
train_iter_loss: 0.3276000916957855
train_iter_loss: 0.3029126822948456
train_iter_loss: 0.38022929430007935
train_iter_loss: 0.3584321439266205
train_iter_loss: 0.2652380168437958
train_iter_loss: 0.3564091622829437
train_iter_loss: 0.26765376329421997
train_iter_loss: 0.25190114974975586
train_iter_loss: 0.32257598638534546
train_iter_loss: 0.24207527935504913
train_iter_loss: 0.24704331159591675
train loss :0.2850
---------------------
Validation seg loss: 0.4048281883697887 at epoch 318
epoch =    319/  1000, exp = train
train_iter_loss: 0.33940067887306213
train_iter_loss: 0.20625077188014984
train_iter_loss: 0.2797432243824005
train_iter_loss: 0.25425174832344055
train_iter_loss: 0.1867440640926361
train_iter_loss: 0.2187366932630539
train_iter_loss: 0.17048633098602295
train_iter_loss: 0.22903765738010406
train_iter_loss: 0.1379326581954956
train_iter_loss: 0.3502333164215088
train_iter_loss: 0.32795482873916626
train_iter_loss: 0.2774165868759155
train_iter_loss: 0.22210751473903656
train_iter_loss: 0.3797561228275299
train_iter_loss: 0.19631808996200562
train_iter_loss: 0.26883256435394287
train_iter_loss: 0.5187681913375854
train_iter_loss: 0.2391796112060547
train_iter_loss: 0.315927654504776
train_iter_loss: 0.1767420917749405
train_iter_loss: 0.13477866351604462
train_iter_loss: 0.29784220457077026
train_iter_loss: 0.19459645450115204
train_iter_loss: 0.41143208742141724
train_iter_loss: 0.2343900203704834
train_iter_loss: 0.4645911157131195
train_iter_loss: 0.2753415107727051
train_iter_loss: 0.2645786702632904
train_iter_loss: 0.37088412046432495
train_iter_loss: 0.17310969531536102
train_iter_loss: 0.35513797402381897
train_iter_loss: 0.32133913040161133
train_iter_loss: 0.0773775577545166
train_iter_loss: 0.4213787615299225
train_iter_loss: 0.4117199182510376
train_iter_loss: 0.26296529173851013
train_iter_loss: 0.4310186207294464
train_iter_loss: 0.20755811035633087
train_iter_loss: 0.19879789650440216
train_iter_loss: 0.3483704924583435
train_iter_loss: 0.31085005402565
train_iter_loss: 0.2396489977836609
train_iter_loss: 0.2595194876194
train_iter_loss: 0.2456280142068863
train_iter_loss: 0.2739647924900055
train_iter_loss: 0.36267951130867004
train_iter_loss: 0.2243904322385788
train_iter_loss: 0.2340603470802307
train_iter_loss: 0.09437461942434311
train_iter_loss: 0.373975932598114
train_iter_loss: 0.28846997022628784
train_iter_loss: 0.30720385909080505
train_iter_loss: 0.2922379970550537
train_iter_loss: 0.26469314098358154
train_iter_loss: 0.33906954526901245
train_iter_loss: 0.27533257007598877
train_iter_loss: 0.18490757048130035
train_iter_loss: 0.22381773591041565
train_iter_loss: 0.26614969968795776
train_iter_loss: 0.13661351799964905
train_iter_loss: 0.5013893246650696
train_iter_loss: 0.35790514945983887
train_iter_loss: 0.42479270696640015
train_iter_loss: 0.25471416115760803
train_iter_loss: 0.40680992603302
train_iter_loss: 0.26276594400405884
train_iter_loss: 0.3304230272769928
train_iter_loss: 0.340333491563797
train_iter_loss: 0.5093405246734619
train_iter_loss: 0.2954053580760956
train_iter_loss: 0.30260705947875977
train_iter_loss: 0.18769729137420654
train_iter_loss: 0.25080785155296326
train_iter_loss: 0.2052268534898758
train_iter_loss: 0.2058057337999344
train_iter_loss: 0.1834706962108612
train_iter_loss: 0.3348202705383301
train_iter_loss: 0.341597318649292
train_iter_loss: 0.25270673632621765
train_iter_loss: 0.35725271701812744
train_iter_loss: 0.20419269800186157
train_iter_loss: 0.2639114260673523
train_iter_loss: 0.2590680718421936
train_iter_loss: 0.2922019064426422
train_iter_loss: 0.29215753078460693
train_iter_loss: 0.353258341550827
train_iter_loss: 0.24367903172969818
train_iter_loss: 0.2594761550426483
train_iter_loss: 0.3931189477443695
train_iter_loss: 0.17251254618167877
train_iter_loss: 0.24786417186260223
train_iter_loss: 0.28063350915908813
train_iter_loss: 0.32608598470687866
train_iter_loss: 0.22906655073165894
train_iter_loss: 0.3259795308113098
train_iter_loss: 0.34346726536750793
train_iter_loss: 0.19481217861175537
train_iter_loss: 0.3203848600387573
train_iter_loss: 0.35392332077026367
train_iter_loss: 0.11968148499727249
train loss :0.2842
---------------------
Validation seg loss: 0.37670410323431186 at epoch 319
epoch =    320/  1000, exp = train
train_iter_loss: 0.22390948235988617
train_iter_loss: 0.19970305263996124
train_iter_loss: 0.6342096924781799
train_iter_loss: 0.36902397871017456
train_iter_loss: 0.08977718651294708
train_iter_loss: 0.3312785029411316
train_iter_loss: 0.32683128118515015
train_iter_loss: 0.10180807113647461
train_iter_loss: 0.26952630281448364
train_iter_loss: 0.42600879073143005
train_iter_loss: 0.23630714416503906
train_iter_loss: 0.24553778767585754
train_iter_loss: 0.19530324637889862
train_iter_loss: 0.18418288230895996
train_iter_loss: 0.4190739393234253
train_iter_loss: 0.2569657564163208
train_iter_loss: 0.5117471814155579
train_iter_loss: 0.3175601661205292
train_iter_loss: 0.2366396188735962
train_iter_loss: 0.2845459580421448
train_iter_loss: 0.22388873994350433
train_iter_loss: 0.2581767439842224
train_iter_loss: 0.31555992364883423
train_iter_loss: 0.2519494295120239
train_iter_loss: 0.3467922806739807
train_iter_loss: 0.2833951413631439
train_iter_loss: 0.34731006622314453
train_iter_loss: 0.23673120141029358
train_iter_loss: 0.2876221537590027
train_iter_loss: 0.1460822969675064
train_iter_loss: 0.30302518606185913
train_iter_loss: 0.40370041131973267
train_iter_loss: 0.3248711824417114
train_iter_loss: 0.29018792510032654
train_iter_loss: 0.1724148541688919
train_iter_loss: 0.37018871307373047
train_iter_loss: 0.2710105776786804
train_iter_loss: 0.2559064030647278
train_iter_loss: 0.21882709860801697
train_iter_loss: 0.234193816781044
train_iter_loss: 0.3078427016735077
train_iter_loss: 0.3299958407878876
train_iter_loss: 0.38435301184654236
train_iter_loss: 0.19466619193553925
train_iter_loss: 0.2372089922428131
train_iter_loss: 0.09497612714767456
train_iter_loss: 0.22912099957466125
train_iter_loss: 0.21774432063102722
train_iter_loss: 0.1816372126340866
train_iter_loss: 0.36732959747314453
train_iter_loss: 0.2646808624267578
train_iter_loss: 0.2658144235610962
train_iter_loss: 0.41382578015327454
train_iter_loss: 0.3074430823326111
train_iter_loss: 0.3334915041923523
train_iter_loss: 0.23150382936000824
train_iter_loss: 0.3007984459400177
train_iter_loss: 0.2925191819667816
train_iter_loss: 0.3632484972476959
train_iter_loss: 0.3052677810192108
train_iter_loss: 0.6602880954742432
train_iter_loss: 0.31265437602996826
train_iter_loss: 0.32650473713874817
train_iter_loss: 0.38907453417778015
train_iter_loss: 0.27069905400276184
train_iter_loss: 0.38348159193992615
train_iter_loss: 0.2791663706302643
train_iter_loss: 0.49409669637680054
train_iter_loss: 0.25775229930877686
train_iter_loss: 0.21574580669403076
train_iter_loss: 0.2606200575828552
train_iter_loss: 0.24637500941753387
train_iter_loss: 0.29592758417129517
train_iter_loss: 0.29846835136413574
train_iter_loss: 0.16691026091575623
train_iter_loss: 0.2865616977214813
train_iter_loss: 0.3179813325405121
train_iter_loss: 0.22794683277606964
train_iter_loss: 0.2930049002170563
train_iter_loss: 0.3274149000644684
train_iter_loss: 0.30702313780784607
train_iter_loss: 0.36464664340019226
train_iter_loss: 0.3326434791088104
train_iter_loss: 0.19524580240249634
train_iter_loss: 0.3603052794933319
train_iter_loss: 0.14083720743656158
train_iter_loss: 0.297933965921402
train_iter_loss: 0.300112783908844
train_iter_loss: 0.22866596281528473
train_iter_loss: 0.36394768953323364
train_iter_loss: 0.13136744499206543
train_iter_loss: 0.1093667522072792
train_iter_loss: 0.26092153787612915
train_iter_loss: 0.2929343283176422
train_iter_loss: 0.35414108633995056
train_iter_loss: 0.15395507216453552
train_iter_loss: 0.3415687382221222
train_iter_loss: 0.30821093916893005
train_iter_loss: 0.1845942735671997
train_iter_loss: 0.17190003395080566
train loss :0.2882
---------------------
Validation seg loss: 0.36782075786295365 at epoch 320
epoch =    321/  1000, exp = train
train_iter_loss: 0.22079934179782867
train_iter_loss: 0.31340309977531433
train_iter_loss: 0.3102185130119324
train_iter_loss: 0.34266549348831177
train_iter_loss: 0.36253631114959717
train_iter_loss: 0.23013964295387268
train_iter_loss: 0.24893249571323395
train_iter_loss: 0.2613680958747864
train_iter_loss: 0.1649564653635025
train_iter_loss: 0.2996312975883484
train_iter_loss: 0.14586316049098969
train_iter_loss: 0.3495241403579712
train_iter_loss: 0.2311965972185135
train_iter_loss: 0.18075376749038696
train_iter_loss: 0.1350553184747696
train_iter_loss: 0.407966285943985
train_iter_loss: 0.16203023493289948
train_iter_loss: 0.530450701713562
train_iter_loss: 0.22617053985595703
train_iter_loss: 0.22751256823539734
train_iter_loss: 0.1763121634721756
train_iter_loss: 0.3023644983768463
train_iter_loss: 0.1378273069858551
train_iter_loss: 0.31736627221107483
train_iter_loss: 0.14755308628082275
train_iter_loss: 0.18599435687065125
train_iter_loss: 0.38289162516593933
train_iter_loss: 0.26524272561073303
train_iter_loss: 0.267595499753952
train_iter_loss: 0.2635403573513031
train_iter_loss: 0.27619168162345886
train_iter_loss: 0.3680717945098877
train_iter_loss: 0.28856611251831055
train_iter_loss: 0.35657379031181335
train_iter_loss: 0.4318861961364746
train_iter_loss: 0.336124062538147
train_iter_loss: 0.21821728348731995
train_iter_loss: 0.6362652778625488
train_iter_loss: 0.2667367160320282
train_iter_loss: 0.3425123691558838
train_iter_loss: 0.18694491684436798
train_iter_loss: 0.2143203616142273
train_iter_loss: 0.2980603575706482
train_iter_loss: 0.21988219022750854
train_iter_loss: 0.2052869200706482
train_iter_loss: 0.351262629032135
train_iter_loss: 0.23243607580661774
train_iter_loss: 0.10012267529964447
train_iter_loss: 0.2151447832584381
train_iter_loss: 0.4241674542427063
train_iter_loss: 0.1410510540008545
train_iter_loss: 0.23155294358730316
train_iter_loss: 0.34721067547798157
train_iter_loss: 0.24044129252433777
train_iter_loss: 0.19745689630508423
train_iter_loss: 0.34545138478279114
train_iter_loss: 0.21019680798053741
train_iter_loss: 0.34951022267341614
train_iter_loss: 0.2888142168521881
train_iter_loss: 0.3381313681602478
train_iter_loss: 0.285063773393631
train_iter_loss: 0.20619164407253265
train_iter_loss: 0.3316400647163391
train_iter_loss: 0.2918316721916199
train_iter_loss: 0.2455201894044876
train_iter_loss: 0.2423551380634308
train_iter_loss: 0.1821756213903427
train_iter_loss: 0.19486483931541443
train_iter_loss: 0.28505775332450867
train_iter_loss: 0.26534518599510193
train_iter_loss: 0.2717188000679016
train_iter_loss: 0.42633378505706787
train_iter_loss: 0.2887342870235443
train_iter_loss: 0.27724358439445496
train_iter_loss: 0.3258596658706665
train_iter_loss: 0.268799364566803
train_iter_loss: 0.4127925634384155
train_iter_loss: 0.44422370195388794
train_iter_loss: 0.3199933171272278
train_iter_loss: 0.3475310802459717
train_iter_loss: 0.18715070188045502
train_iter_loss: 0.3830099403858185
train_iter_loss: 0.27190709114074707
train_iter_loss: 0.35679712891578674
train_iter_loss: 0.26551637053489685
train_iter_loss: 0.2844083309173584
train_iter_loss: 0.38900843262672424
train_iter_loss: 0.2745712399482727
train_iter_loss: 0.15703654289245605
train_iter_loss: 0.18082280457019806
train_iter_loss: 0.16532285511493683
train_iter_loss: 0.31158915162086487
train_iter_loss: 0.18975481390953064
train_iter_loss: 0.2352638840675354
train_iter_loss: 0.18276609480381012
train_iter_loss: 0.23748160898685455
train_iter_loss: 0.32340431213378906
train_iter_loss: 0.296794593334198
train_iter_loss: 0.18703000247478485
train_iter_loss: 0.38299962878227234
train loss :0.2792
---------------------
Validation seg loss: 0.3618531655695922 at epoch 321
epoch =    322/  1000, exp = train
train_iter_loss: 0.1061120331287384
train_iter_loss: 0.3060150444507599
train_iter_loss: 0.4429515302181244
train_iter_loss: 0.24802081286907196
train_iter_loss: 0.33407801389694214
train_iter_loss: 0.24262498319149017
train_iter_loss: 0.2339211255311966
train_iter_loss: 0.2925648093223572
train_iter_loss: 0.18443019688129425
train_iter_loss: 0.19753560423851013
train_iter_loss: 0.1686060130596161
train_iter_loss: 0.26489388942718506
train_iter_loss: 0.2759911119937897
train_iter_loss: 0.29145196080207825
train_iter_loss: 0.31408166885375977
train_iter_loss: 0.23842471837997437
train_iter_loss: 0.2342326045036316
train_iter_loss: 0.45762723684310913
train_iter_loss: 0.376617968082428
train_iter_loss: 0.2747676372528076
train_iter_loss: 0.3618893027305603
train_iter_loss: 0.23741187155246735
train_iter_loss: 0.2512505352497101
train_iter_loss: 0.09420959651470184
train_iter_loss: 0.369855135679245
train_iter_loss: 0.40945228934288025
train_iter_loss: 0.20206035673618317
train_iter_loss: 0.2593773305416107
train_iter_loss: 0.3309647738933563
train_iter_loss: 0.3424213230609894
train_iter_loss: 0.2826964557170868
train_iter_loss: 0.2513293921947479
train_iter_loss: 0.3188464939594269
train_iter_loss: 0.26031607389450073
train_iter_loss: 0.3320547640323639
train_iter_loss: 0.2823660373687744
train_iter_loss: 0.37383508682250977
train_iter_loss: 0.1130153089761734
train_iter_loss: 0.36642736196517944
train_iter_loss: 0.32222679257392883
train_iter_loss: 0.1468590497970581
train_iter_loss: 0.3424214720726013
train_iter_loss: 0.21993210911750793
train_iter_loss: 0.4798506498336792
train_iter_loss: 0.09621303528547287
train_iter_loss: 0.3621737062931061
train_iter_loss: 0.30396825075149536
train_iter_loss: 0.4018532633781433
train_iter_loss: 0.12259995192289352
train_iter_loss: 0.30248886346817017
train_iter_loss: 0.22953088581562042
train_iter_loss: 0.21293431520462036
train_iter_loss: 0.22401461005210876
train_iter_loss: 0.2315092831850052
train_iter_loss: 0.33793559670448303
train_iter_loss: 0.20584894716739655
train_iter_loss: 0.04040351137518883
train_iter_loss: 0.33543866872787476
train_iter_loss: 0.3492695093154907
train_iter_loss: 0.4670303463935852
train_iter_loss: 0.3082285225391388
train_iter_loss: 0.3588651716709137
train_iter_loss: 0.18486425280570984
train_iter_loss: 0.252763569355011
train_iter_loss: 0.31960606575012207
train_iter_loss: 0.3703259229660034
train_iter_loss: 0.3285953998565674
train_iter_loss: 0.3112412989139557
train_iter_loss: 0.28588971495628357
train_iter_loss: 0.5686895251274109
train_iter_loss: 0.3254927098751068
train_iter_loss: 0.21385550498962402
train_iter_loss: 0.23332437872886658
train_iter_loss: 0.2311437875032425
train_iter_loss: 0.37686264514923096
train_iter_loss: 0.3806883692741394
train_iter_loss: 0.28072047233581543
train_iter_loss: 0.2692193388938904
train_iter_loss: 0.2326515167951584
train_iter_loss: 0.46317318081855774
train_iter_loss: 0.30256661772727966
train_iter_loss: 0.22644197940826416
train_iter_loss: 0.23509030044078827
train_iter_loss: 0.36546823382377625
train_iter_loss: 0.16539785265922546
train_iter_loss: 0.26267167925834656
train_iter_loss: 0.10371959954500198
train_iter_loss: 0.18304339051246643
train_iter_loss: 0.24303783476352692
train_iter_loss: 0.29684093594551086
train_iter_loss: 0.3446170389652252
train_iter_loss: 0.5624844431877136
train_iter_loss: 0.12991583347320557
train_iter_loss: 0.272490531206131
train_iter_loss: 0.3427198529243469
train_iter_loss: 0.18386416137218475
train_iter_loss: 0.28006550669670105
train_iter_loss: 0.23055444657802582
train_iter_loss: 0.1478218138217926
train_iter_loss: 0.13738422095775604
train loss :0.2825
---------------------
Validation seg loss: 0.3621346238859982 at epoch 322
epoch =    323/  1000, exp = train
train_iter_loss: 0.24207648634910583
train_iter_loss: 0.25958672165870667
train_iter_loss: 0.13838762044906616
train_iter_loss: 0.337005615234375
train_iter_loss: 0.2293950766324997
train_iter_loss: 0.4060527980327606
train_iter_loss: 0.2966825067996979
train_iter_loss: 0.32544443011283875
train_iter_loss: 0.2959229350090027
train_iter_loss: 0.336608350276947
train_iter_loss: 0.15149812400341034
train_iter_loss: 0.39472702145576477
train_iter_loss: 0.329883873462677
train_iter_loss: 0.2530786395072937
train_iter_loss: 0.26295727491378784
train_iter_loss: 0.2576189637184143
train_iter_loss: 0.21238861978054047
train_iter_loss: 0.35974809527397156
train_iter_loss: 0.16856667399406433
train_iter_loss: 0.3509047031402588
train_iter_loss: 0.28568795323371887
train_iter_loss: 0.24323201179504395
train_iter_loss: 0.30797162652015686
train_iter_loss: 0.366349458694458
train_iter_loss: 0.17687979340553284
train_iter_loss: 0.1762242615222931
train_iter_loss: 0.2637326717376709
train_iter_loss: 0.45953643321990967
train_iter_loss: 0.3185277283191681
train_iter_loss: 0.2390478551387787
train_iter_loss: 0.25767630338668823
train_iter_loss: 0.3192204236984253
train_iter_loss: 0.21117247641086578
train_iter_loss: 0.3702549934387207
train_iter_loss: 0.25175511837005615
train_iter_loss: 0.2036074697971344
train_iter_loss: 0.43627670407295227
train_iter_loss: 0.2236260026693344
train_iter_loss: 0.4372052550315857
train_iter_loss: 0.2092934250831604
train_iter_loss: 0.2785584628582001
train_iter_loss: 0.10652896761894226
train_iter_loss: 0.18732509016990662
train_iter_loss: 0.34357139468193054
train_iter_loss: 0.4958217442035675
train_iter_loss: 0.24096928536891937
train_iter_loss: 0.22441473603248596
train_iter_loss: 0.3796822428703308
train_iter_loss: 0.3737291097640991
train_iter_loss: 0.14032460749149323
train_iter_loss: 0.14837245643138885
train_iter_loss: 0.22797179222106934
train_iter_loss: 0.3372012972831726
train_iter_loss: 0.28043925762176514
train_iter_loss: 0.33750876784324646
train_iter_loss: 0.23835961520671844
train_iter_loss: 0.1941688507795334
train_iter_loss: 0.5197807550430298
train_iter_loss: 0.38617831468582153
train_iter_loss: 0.13769091665744781
train_iter_loss: 0.23736464977264404
train_iter_loss: 0.4476558566093445
train_iter_loss: 0.31789177656173706
train_iter_loss: 0.2118772715330124
train_iter_loss: 0.038914307951927185
train_iter_loss: 0.1953418105840683
train_iter_loss: 0.20537957549095154
train_iter_loss: 0.29125434160232544
train_iter_loss: 0.20588234066963196
train_iter_loss: 0.19376373291015625
train_iter_loss: 0.3274737000465393
train_iter_loss: 0.18381661176681519
train_iter_loss: 0.3665167987346649
train_iter_loss: 0.32513099908828735
train_iter_loss: 0.2413322478532791
train_iter_loss: 0.23854213953018188
train_iter_loss: 0.26123273372650146
train_iter_loss: 0.2635396718978882
train_iter_loss: 0.374857097864151
train_iter_loss: 0.28567400574684143
train_iter_loss: 0.27330049872398376
train_iter_loss: 0.2786433696746826
train_iter_loss: 0.16575731337070465
train_iter_loss: 0.4946180582046509
train_iter_loss: 0.1474124789237976
train_iter_loss: 0.2172774225473404
train_iter_loss: 0.3272591233253479
train_iter_loss: 0.16586527228355408
train_iter_loss: 0.3084995150566101
train_iter_loss: 0.21540848910808563
train_iter_loss: 0.19944259524345398
train_iter_loss: 0.23729437589645386
train_iter_loss: 0.451518714427948
train_iter_loss: 0.3355500102043152
train_iter_loss: 0.29726195335388184
train_iter_loss: 0.26258787512779236
train_iter_loss: 0.16356097161769867
train_iter_loss: 0.26610520482063293
train_iter_loss: 0.20486189424991608
train_iter_loss: 0.374239057302475
train loss :0.2783
---------------------
Validation seg loss: 0.36818555510550177 at epoch 323
epoch =    324/  1000, exp = train
train_iter_loss: 0.362598717212677
train_iter_loss: 0.29546359181404114
train_iter_loss: 0.27549663186073303
train_iter_loss: 0.24528151750564575
train_iter_loss: 0.4127812385559082
train_iter_loss: 0.27826958894729614
train_iter_loss: 0.46020326018333435
train_iter_loss: 0.3424209654331207
train_iter_loss: 0.22817228734493256
train_iter_loss: 0.22076091170310974
train_iter_loss: 0.36667630076408386
train_iter_loss: 0.13324213027954102
train_iter_loss: 0.09820970147848129
train_iter_loss: 0.30467933416366577
train_iter_loss: 0.054513949900865555
train_iter_loss: 0.4022097885608673
train_iter_loss: 0.42472097277641296
train_iter_loss: 0.2610321640968323
train_iter_loss: 0.2864168584346771
train_iter_loss: 0.15046511590480804
train_iter_loss: 0.2932075560092926
train_iter_loss: 0.32745033502578735
train_iter_loss: 0.2536561191082001
train_iter_loss: 0.25505566596984863
train_iter_loss: 0.14865270256996155
train_iter_loss: 0.24633322656154633
train_iter_loss: 0.13450483977794647
train_iter_loss: 0.1706012785434723
train_iter_loss: 0.290522962808609
train_iter_loss: 0.5999628901481628
train_iter_loss: 0.2115575671195984
train_iter_loss: 0.4467802345752716
train_iter_loss: 0.34688398241996765
train_iter_loss: 0.24075855314731598
train_iter_loss: 0.277427613735199
train_iter_loss: 0.2389432191848755
train_iter_loss: 0.40523555874824524
train_iter_loss: 0.462571918964386
train_iter_loss: 0.28777578473091125
train_iter_loss: 0.2938256859779358
train_iter_loss: 0.33359915018081665
train_iter_loss: 0.22216051816940308
train_iter_loss: 0.32830026745796204
train_iter_loss: 0.36325573921203613
train_iter_loss: 0.24469386041164398
train_iter_loss: 0.2856766879558563
train_iter_loss: 0.24040459096431732
train_iter_loss: 0.2607809901237488
train_iter_loss: 0.3913854956626892
train_iter_loss: 0.05910908803343773
train_iter_loss: 0.3299316167831421
train_iter_loss: 0.46380579471588135
train_iter_loss: 0.2656695246696472
train_iter_loss: 0.31117525696754456
train_iter_loss: 0.41641542315483093
train_iter_loss: 0.2976967692375183
train_iter_loss: 0.25851598381996155
train_iter_loss: 0.315886527299881
train_iter_loss: 0.25415468215942383
train_iter_loss: 0.31202957034111023
train_iter_loss: 0.22951167821884155
train_iter_loss: 0.2718982696533203
train_iter_loss: 0.2271285355091095
train_iter_loss: 0.2002449631690979
train_iter_loss: 0.3007628917694092
train_iter_loss: 0.30712389945983887
train_iter_loss: 0.19886444509029388
train_iter_loss: 0.3134711980819702
train_iter_loss: 0.2850601375102997
train_iter_loss: 0.3264525532722473
train_iter_loss: 0.1821383833885193
train_iter_loss: 0.4152772128582001
train_iter_loss: 0.1533365249633789
train_iter_loss: 0.31414079666137695
train_iter_loss: 0.1619030386209488
train_iter_loss: 0.3199208378791809
train_iter_loss: 0.28584274649620056
train_iter_loss: 0.18495652079582214
train_iter_loss: 0.1930890679359436
train_iter_loss: 0.10759277641773224
train_iter_loss: 0.3219945430755615
train_iter_loss: 0.6801662445068359
train_iter_loss: 0.315335214138031
train_iter_loss: 0.2412068247795105
train_iter_loss: 0.1479252427816391
train_iter_loss: 0.36753788590431213
train_iter_loss: 0.4314844310283661
train_iter_loss: 0.16892699897289276
train_iter_loss: 0.317811518907547
train_iter_loss: 0.16457316279411316
train_iter_loss: 0.34666967391967773
train_iter_loss: 0.28863129019737244
train_iter_loss: 0.31809383630752563
train_iter_loss: 0.29511943459510803
train_iter_loss: 0.2845962941646576
train_iter_loss: 0.2004060447216034
train_iter_loss: 0.33797067403793335
train_iter_loss: 0.2778704762458801
train_iter_loss: 0.2527126669883728
train_iter_loss: 0.26123616099357605
train loss :0.2877
---------------------
Validation seg loss: 0.3812504020818281 at epoch 324
epoch =    325/  1000, exp = train
train_iter_loss: 0.2856473922729492
train_iter_loss: 0.336793452501297
train_iter_loss: 0.13390299677848816
train_iter_loss: 0.30446115136146545
train_iter_loss: 0.2378656268119812
train_iter_loss: 0.2055296003818512
train_iter_loss: 0.18298228085041046
train_iter_loss: 0.1697734296321869
train_iter_loss: 0.2814735770225525
train_iter_loss: 0.22539328038692474
train_iter_loss: 0.32200726866722107
train_iter_loss: 0.2532281279563904
train_iter_loss: 0.299276202917099
train_iter_loss: 0.3561413586139679
train_iter_loss: 0.34281232953071594
train_iter_loss: 0.3744632303714752
train_iter_loss: 0.32628756761550903
train_iter_loss: 0.37954962253570557
train_iter_loss: 0.1522790938615799
train_iter_loss: 0.3252318501472473
train_iter_loss: 0.16062705218791962
train_iter_loss: 0.09702636301517487
train_iter_loss: 0.21808215975761414
train_iter_loss: 0.20360900461673737
train_iter_loss: 0.23990516364574432
train_iter_loss: 0.34948551654815674
train_iter_loss: 0.31190189719200134
train_iter_loss: 0.1639116108417511
train_iter_loss: 0.24526812136173248
train_iter_loss: 0.33491870760917664
train_iter_loss: 0.2758362591266632
train_iter_loss: 0.4968619644641876
train_iter_loss: 0.4124475419521332
train_iter_loss: 0.21881450712680817
train_iter_loss: 0.10208283364772797
train_iter_loss: 0.1661420613527298
train_iter_loss: 0.06936267018318176
train_iter_loss: 0.13577406108379364
train_iter_loss: 0.3305598795413971
train_iter_loss: 0.3759945333003998
train_iter_loss: 0.3233417570590973
train_iter_loss: 0.2599078118801117
train_iter_loss: 0.35919585824012756
train_iter_loss: 0.22632141411304474
train_iter_loss: 0.2585861384868622
train_iter_loss: 0.19924721121788025
train_iter_loss: 0.18112201988697052
train_iter_loss: 0.27786606550216675
train_iter_loss: 0.13309302926063538
train_iter_loss: 0.25962933897972107
train_iter_loss: 0.4002722501754761
train_iter_loss: 0.3074321448802948
train_iter_loss: 0.2595621347427368
train_iter_loss: 0.2906353771686554
train_iter_loss: 0.3846854865550995
train_iter_loss: 0.4676382541656494
train_iter_loss: 0.3277469277381897
train_iter_loss: 0.3404518961906433
train_iter_loss: 0.1916918009519577
train_iter_loss: 0.4564880132675171
train_iter_loss: 0.137019082903862
train_iter_loss: 0.288869708776474
train_iter_loss: 0.30063769221305847
train_iter_loss: 0.21007969975471497
train_iter_loss: 0.4662306010723114
train_iter_loss: 0.26908227801322937
train_iter_loss: 0.24918071925640106
train_iter_loss: 0.28253602981567383
train_iter_loss: 0.30363646149635315
train_iter_loss: 0.31049713492393494
train_iter_loss: 0.22220560908317566
train_iter_loss: 0.14156383275985718
train_iter_loss: 0.19222426414489746
train_iter_loss: 0.3167622983455658
train_iter_loss: 0.3391842544078827
train_iter_loss: 0.26727187633514404
train_iter_loss: 0.3052692413330078
train_iter_loss: 0.218000128865242
train_iter_loss: 0.24911953508853912
train_iter_loss: 0.31336724758148193
train_iter_loss: 0.5161319375038147
train_iter_loss: 0.3567923307418823
train_iter_loss: 0.2314860224723816
train_iter_loss: 0.14060468971729279
train_iter_loss: 0.3323136866092682
train_iter_loss: 0.30817562341690063
train_iter_loss: 0.23305128514766693
train_iter_loss: 0.2986730635166168
train_iter_loss: 0.1037013903260231
train_iter_loss: 0.24375633895397186
train_iter_loss: 0.2836114168167114
train_iter_loss: 0.3193950951099396
train_iter_loss: 0.060949813574552536
train_iter_loss: 0.29480046033859253
train_iter_loss: 0.4009365439414978
train_iter_loss: 0.27504387497901917
train_iter_loss: 0.09642914682626724
train_iter_loss: 0.3498716950416565
train_iter_loss: 0.36538800597190857
train_iter_loss: 0.44956356287002563
train loss :0.2764
---------------------
Validation seg loss: 0.3721316128152089 at epoch 325
epoch =    326/  1000, exp = train
train_iter_loss: 0.2839023768901825
train_iter_loss: 0.38602566719055176
train_iter_loss: 0.23027470707893372
train_iter_loss: 0.282629132270813
train_iter_loss: 0.27700525522232056
train_iter_loss: 0.20198819041252136
train_iter_loss: 0.1928974688053131
train_iter_loss: 0.43195343017578125
train_iter_loss: 0.3235330879688263
train_iter_loss: 0.37628304958343506
train_iter_loss: 0.2826744318008423
train_iter_loss: 0.2845119833946228
train_iter_loss: 0.15426528453826904
train_iter_loss: 0.4491000473499298
train_iter_loss: 0.22744864225387573
train_iter_loss: 0.25608715415000916
train_iter_loss: 0.163114532828331
train_iter_loss: 0.2986607253551483
train_iter_loss: 0.34015128016471863
train_iter_loss: 0.18287120759487152
train_iter_loss: 0.2342265397310257
train_iter_loss: 0.19227471947669983
train_iter_loss: 0.28078359365463257
train_iter_loss: 0.19384323060512543
train_iter_loss: 0.34964343905448914
train_iter_loss: 0.25400444865226746
train_iter_loss: 0.357170969247818
train_iter_loss: 0.19663675129413605
train_iter_loss: 0.33548635244369507
train_iter_loss: 0.2333947718143463
train_iter_loss: 0.3841831088066101
train_iter_loss: 0.24413099884986877
train_iter_loss: 0.18949617445468903
train_iter_loss: 0.21880920231342316
train_iter_loss: 0.3105722963809967
train_iter_loss: 0.4595045745372772
train_iter_loss: 0.2916598618030548
train_iter_loss: 0.3129977881908417
train_iter_loss: 0.26897069811820984
train_iter_loss: 0.1851721704006195
train_iter_loss: 0.19918477535247803
train_iter_loss: 0.20733001828193665
train_iter_loss: 0.3826252520084381
train_iter_loss: 0.10950922966003418
train_iter_loss: 0.3055236339569092
train_iter_loss: 0.5016679763793945
train_iter_loss: 0.2092682123184204
train_iter_loss: 0.3363514840602875
train_iter_loss: 0.16070453822612762
train_iter_loss: 0.3318977952003479
train_iter_loss: 0.14693209528923035
train_iter_loss: 0.05025682598352432
train_iter_loss: 0.34136390686035156
train_iter_loss: 0.05531059950590134
train_iter_loss: 0.3235453963279724
train_iter_loss: 0.31794655323028564
train_iter_loss: 0.3251104950904846
train_iter_loss: 0.24836465716362
train_iter_loss: 0.34541407227516174
train_iter_loss: 0.39565473794937134
train_iter_loss: 0.284481942653656
train_iter_loss: 0.06345298141241074
train_iter_loss: 0.23105496168136597
train_iter_loss: 0.2783481478691101
train_iter_loss: 0.30576270818710327
train_iter_loss: 0.338651180267334
train_iter_loss: 0.28537824749946594
train_iter_loss: 0.21889826655387878
train_iter_loss: 0.2597990334033966
train_iter_loss: 0.334654837846756
train_iter_loss: 0.30079251527786255
train_iter_loss: 0.5707587003707886
train_iter_loss: 0.20133920013904572
train_iter_loss: 0.10892609506845474
train_iter_loss: 0.3316236138343811
train_iter_loss: 0.24015139043331146
train_iter_loss: 0.2887587249279022
train_iter_loss: 0.27385109663009644
train_iter_loss: 0.1752372533082962
train_iter_loss: 0.3719334304332733
train_iter_loss: 0.34076592326164246
train_iter_loss: 0.3069422245025635
train_iter_loss: 0.44070112705230713
train_iter_loss: 0.3484172821044922
train_iter_loss: 0.26032763719558716
train_iter_loss: 0.3383302688598633
train_iter_loss: 0.20104269683361053
train_iter_loss: 0.16569600999355316
train_iter_loss: 0.2788420617580414
train_iter_loss: 0.38884130120277405
train_iter_loss: 0.24050068855285645
train_iter_loss: 0.3705236315727234
train_iter_loss: 0.33347806334495544
train_iter_loss: 0.18157342076301575
train_iter_loss: 0.43968525528907776
train_iter_loss: 0.3286508023738861
train_iter_loss: 0.2433248907327652
train_iter_loss: 0.30192261934280396
train_iter_loss: 0.1675253063440323
train_iter_loss: 0.28413569927215576
train loss :0.2811
---------------------
Validation seg loss: 0.35747499010121486 at epoch 326
epoch =    327/  1000, exp = train
train_iter_loss: 0.2763293385505676
train_iter_loss: 0.18046630918979645
train_iter_loss: 0.3051488697528839
train_iter_loss: 0.32688024640083313
train_iter_loss: 0.2272905856370926
train_iter_loss: 0.4174298644065857
train_iter_loss: 0.17385172843933105
train_iter_loss: 0.14499573409557343
train_iter_loss: 0.3103415369987488
train_iter_loss: 0.24397140741348267
train_iter_loss: 0.2832809090614319
train_iter_loss: 0.42671945691108704
train_iter_loss: 0.39518481492996216
train_iter_loss: 0.2239445000886917
train_iter_loss: 0.375194251537323
train_iter_loss: 0.5489180684089661
train_iter_loss: 0.30642473697662354
train_iter_loss: 0.21818408370018005
train_iter_loss: 0.394642174243927
train_iter_loss: 0.41895440220832825
train_iter_loss: 0.2345796376466751
train_iter_loss: 0.2668642997741699
train_iter_loss: 0.3295064866542816
train_iter_loss: 0.3836175203323364
train_iter_loss: 0.08963190019130707
train_iter_loss: 0.4131610095500946
train_iter_loss: 0.3329383134841919
train_iter_loss: 0.27417030930519104
train_iter_loss: 0.25772714614868164
train_iter_loss: 0.07471733540296555
train_iter_loss: 0.17320461571216583
train_iter_loss: 0.27361708879470825
train_iter_loss: 0.32209864258766174
train_iter_loss: 0.2489698976278305
train_iter_loss: 0.1642482876777649
train_iter_loss: 0.39895766973495483
train_iter_loss: 0.32387495040893555
train_iter_loss: 0.363531231880188
train_iter_loss: 0.08237646520137787
train_iter_loss: 0.41941213607788086
train_iter_loss: 0.2490653693675995
train_iter_loss: 0.1470641791820526
train_iter_loss: 0.2951517403125763
train_iter_loss: 0.1884152591228485
train_iter_loss: 0.15695708990097046
train_iter_loss: 0.25091007351875305
train_iter_loss: 0.30799129605293274
train_iter_loss: 0.08983700722455978
train_iter_loss: 0.1891322284936905
train_iter_loss: 0.2214195281267166
train_iter_loss: 0.32166001200675964
train_iter_loss: 0.3144721984863281
train_iter_loss: 0.337086021900177
train_iter_loss: 0.27589279413223267
train_iter_loss: 0.33199480175971985
train_iter_loss: 0.30686643719673157
train_iter_loss: 0.35428479313850403
train_iter_loss: 0.38626861572265625
train_iter_loss: 0.28916922211647034
train_iter_loss: 0.28979384899139404
train_iter_loss: 0.38468533754348755
train_iter_loss: 0.3697053790092468
train_iter_loss: 0.2419857531785965
train_iter_loss: 0.25990310311317444
train_iter_loss: 0.20314405858516693
train_iter_loss: 0.21550825238227844
train_iter_loss: 0.3401985466480255
train_iter_loss: 0.16617465019226074
train_iter_loss: 0.19250836968421936
train_iter_loss: 0.31674817204475403
train_iter_loss: 0.26611676812171936
train_iter_loss: 0.3931048512458801
train_iter_loss: 0.2689400315284729
train_iter_loss: 0.3188553750514984
train_iter_loss: 0.4228825569152832
train_iter_loss: 0.39806023240089417
train_iter_loss: 0.21928046643733978
train_iter_loss: 0.2334107607603073
train_iter_loss: 0.35937896370887756
train_iter_loss: 0.24433283507823944
train_iter_loss: 0.30323535203933716
train_iter_loss: 0.18165358901023865
train_iter_loss: 0.2644977569580078
train_iter_loss: 0.31665870547294617
train_iter_loss: 0.340660035610199
train_iter_loss: 0.3517811894416809
train_iter_loss: 0.15499109029769897
train_iter_loss: 0.286599338054657
train_iter_loss: 0.3065028190612793
train_iter_loss: 0.24083296954631805
train_iter_loss: 0.19719694554805756
train_iter_loss: 0.16972608864307404
train_iter_loss: 0.3199431598186493
train_iter_loss: 0.1639247089624405
train_iter_loss: 0.13979366421699524
train_iter_loss: 0.12475220113992691
train_iter_loss: 0.3013085722923279
train_iter_loss: 0.09887711703777313
train_iter_loss: 0.3794253170490265
train_iter_loss: 0.09729158878326416
train loss :0.2776
---------------------
Validation seg loss: 0.3569015724373595 at epoch 327
epoch =    328/  1000, exp = train
train_iter_loss: 0.27115190029144287
train_iter_loss: 0.08045253902673721
train_iter_loss: 0.26928889751434326
train_iter_loss: 0.22266727685928345
train_iter_loss: 0.3099287152290344
train_iter_loss: 0.5749409198760986
train_iter_loss: 0.2597979009151459
train_iter_loss: 0.08477205783128738
train_iter_loss: 0.16506265103816986
train_iter_loss: 0.18766020238399506
train_iter_loss: 0.1941303163766861
train_iter_loss: 0.1504264622926712
train_iter_loss: 0.28625133633613586
train_iter_loss: 0.21062250435352325
train_iter_loss: 0.1738698035478592
train_iter_loss: 0.1974535882472992
train_iter_loss: 0.31738197803497314
train_iter_loss: 0.21425595879554749
train_iter_loss: 0.3236004114151001
train_iter_loss: 0.14923062920570374
train_iter_loss: 0.35497793555259705
train_iter_loss: 0.22684122622013092
train_iter_loss: 0.3507806360721588
train_iter_loss: 0.24573343992233276
train_iter_loss: 0.3470422625541687
train_iter_loss: 0.20022760331630707
train_iter_loss: 0.3128829002380371
train_iter_loss: 0.34742996096611023
train_iter_loss: 0.3408343195915222
train_iter_loss: 0.35414913296699524
train_iter_loss: 0.3318175673484802
train_iter_loss: 0.09188143908977509
train_iter_loss: 0.3350450396537781
train_iter_loss: 0.1587410420179367
train_iter_loss: 0.2033109962940216
train_iter_loss: 0.20566806197166443
train_iter_loss: 0.23530857264995575
train_iter_loss: 0.2862643599510193
train_iter_loss: 0.38321036100387573
train_iter_loss: 0.296239972114563
train_iter_loss: 0.21782973408699036
train_iter_loss: 0.32642456889152527
train_iter_loss: 0.35117480158805847
train_iter_loss: 0.1990855187177658
train_iter_loss: 0.23911145329475403
train_iter_loss: 0.2959718704223633
train_iter_loss: 0.3316602110862732
train_iter_loss: 0.2066134661436081
train_iter_loss: 0.1750553548336029
train_iter_loss: 0.2966969311237335
train_iter_loss: 0.33338019251823425
train_iter_loss: 0.19304952025413513
train_iter_loss: 0.34735020995140076
train_iter_loss: 0.3626883924007416
train_iter_loss: 0.28310054540634155
train_iter_loss: 0.40351101756095886
train_iter_loss: 0.413469135761261
train_iter_loss: 0.11864551156759262
train_iter_loss: 0.3132109045982361
train_iter_loss: 0.31008172035217285
train_iter_loss: 0.40643617510795593
train_iter_loss: 0.48237812519073486
train_iter_loss: 0.2067243903875351
train_iter_loss: 0.31019407510757446
train_iter_loss: 0.16746573150157928
train_iter_loss: 0.21790452301502228
train_iter_loss: 0.25729790329933167
train_iter_loss: 0.30745840072631836
train_iter_loss: 0.24177753925323486
train_iter_loss: 0.25140172243118286
train_iter_loss: 0.38330987095832825
train_iter_loss: 0.49880897998809814
train_iter_loss: 0.21269673109054565
train_iter_loss: 0.3549724221229553
train_iter_loss: 0.1890983283519745
train_iter_loss: 0.31451672315597534
train_iter_loss: 0.3629193902015686
train_iter_loss: 0.22146698832511902
train_iter_loss: 0.36811700463294983
train_iter_loss: 0.27811455726623535
train_iter_loss: 0.305229514837265
train_iter_loss: 0.3457922041416168
train_iter_loss: 0.27592629194259644
train_iter_loss: 0.15214797854423523
train_iter_loss: 0.18219290673732758
train_iter_loss: 0.47975772619247437
train_iter_loss: 0.2782749831676483
train_iter_loss: 0.3661341965198517
train_iter_loss: 0.3469204306602478
train_iter_loss: 0.30635741353034973
train_iter_loss: 0.28127938508987427
train_iter_loss: 0.29411056637763977
train_iter_loss: 0.10482622683048248
train_iter_loss: 0.3287712633609772
train_iter_loss: 0.16576118767261505
train_iter_loss: 0.3763183355331421
train_iter_loss: 0.22672435641288757
train_iter_loss: 0.2241169810295105
train_iter_loss: 0.3764908015727997
train_iter_loss: 0.24485503137111664
train loss :0.2794
---------------------
Validation seg loss: 0.38504458206512454 at epoch 328
epoch =    329/  1000, exp = train
train_iter_loss: 0.38920798897743225
train_iter_loss: 0.12462394684553146
train_iter_loss: 0.19348567724227905
train_iter_loss: 0.21415482461452484
train_iter_loss: 0.3591151237487793
train_iter_loss: 0.4697071611881256
train_iter_loss: 0.11544520407915115
train_iter_loss: 0.1481126844882965
train_iter_loss: 0.21161840856075287
train_iter_loss: 0.372587114572525
train_iter_loss: 0.22693684697151184
train_iter_loss: 0.30431774258613586
train_iter_loss: 0.48088014125823975
train_iter_loss: 0.324409157037735
train_iter_loss: 0.2822956442832947
train_iter_loss: 0.22065195441246033
train_iter_loss: 0.2595100402832031
train_iter_loss: 0.23539739847183228
train_iter_loss: 0.29101699590682983
train_iter_loss: 0.5346899628639221
train_iter_loss: 0.29058897495269775
train_iter_loss: 0.12822739779949188
train_iter_loss: 0.4753148853778839
train_iter_loss: 0.24798071384429932
train_iter_loss: 0.14681346714496613
train_iter_loss: 0.12219748646020889
train_iter_loss: 0.184779092669487
train_iter_loss: 0.3289194703102112
train_iter_loss: 0.2687445282936096
train_iter_loss: 0.17424310743808746
train_iter_loss: 0.30400028824806213
train_iter_loss: 0.15132088959217072
train_iter_loss: 0.2654735743999481
train_iter_loss: 0.44880059361457825
train_iter_loss: 0.1767217069864273
train_iter_loss: 0.323687344789505
train_iter_loss: 0.21701273322105408
train_iter_loss: 0.24749399721622467
train_iter_loss: 0.1820688545703888
train_iter_loss: 0.3739207088947296
train_iter_loss: 0.24485240876674652
train_iter_loss: 0.2362801730632782
train_iter_loss: 0.24493883550167084
train_iter_loss: 0.23255230486392975
train_iter_loss: 0.2576325833797455
train_iter_loss: 0.18652518093585968
train_iter_loss: 0.2397928684949875
train_iter_loss: 0.26441529393196106
train_iter_loss: 0.2378172129392624
train_iter_loss: 0.2473304569721222
train_iter_loss: 0.17967577278614044
train_iter_loss: 0.34971165657043457
train_iter_loss: 0.3035898208618164
train_iter_loss: 0.17393438518047333
train_iter_loss: 0.3069618344306946
train_iter_loss: 0.4460624158382416
train_iter_loss: 0.24842408299446106
train_iter_loss: 0.258742094039917
train_iter_loss: 0.28545811772346497
train_iter_loss: 0.31160977482795715
train_iter_loss: 0.2300456464290619
train_iter_loss: 0.4259119927883148
train_iter_loss: 0.26197347044944763
train_iter_loss: 0.12991932034492493
train_iter_loss: 0.4857410192489624
train_iter_loss: 0.2602093517780304
train_iter_loss: 0.3273161053657532
train_iter_loss: 0.3039519488811493
train_iter_loss: 0.38697004318237305
train_iter_loss: 0.24369551241397858
train_iter_loss: 0.2951267659664154
train_iter_loss: 0.3292079269886017
train_iter_loss: 0.20689035952091217
train_iter_loss: 0.3282727301120758
train_iter_loss: 0.21578367054462433
train_iter_loss: 0.1715134233236313
train_iter_loss: 0.28536465764045715
train_iter_loss: 0.31350675225257874
train_iter_loss: 0.24000662565231323
train_iter_loss: 0.3448977768421173
train_iter_loss: 0.3072771728038788
train_iter_loss: 0.16851551830768585
train_iter_loss: 0.26695364713668823
train_iter_loss: 0.23477843403816223
train_iter_loss: 0.1307315230369568
train_iter_loss: 0.20853659510612488
train_iter_loss: 0.4744187295436859
train_iter_loss: 0.22171594202518463
train_iter_loss: 0.2747780680656433
train_iter_loss: 0.1296093463897705
train_iter_loss: 0.49271708726882935
train_iter_loss: 0.28252431750297546
train_iter_loss: 0.3872663080692291
train_iter_loss: 0.19694626331329346
train_iter_loss: 0.1611495018005371
train_iter_loss: 0.29960039258003235
train_iter_loss: 0.26578062772750854
train_iter_loss: 0.2915898561477661
train_iter_loss: 0.20461514592170715
train_iter_loss: 0.31690770387649536
train loss :0.2745
---------------------
Validation seg loss: 0.36842569787898716 at epoch 329
epoch =    330/  1000, exp = train
train_iter_loss: 0.2857315242290497
train_iter_loss: 0.14158815145492554
train_iter_loss: 0.1496078372001648
train_iter_loss: 0.33469995856285095
train_iter_loss: 0.28198689222335815
train_iter_loss: 0.2520534098148346
train_iter_loss: 0.37650632858276367
train_iter_loss: 0.3693848252296448
train_iter_loss: 0.29747217893600464
train_iter_loss: 0.27904364466667175
train_iter_loss: 0.13179489970207214
train_iter_loss: 0.4857569634914398
train_iter_loss: 0.2568742632865906
train_iter_loss: 0.29517489671707153
train_iter_loss: 0.1870747208595276
train_iter_loss: 0.38464775681495667
train_iter_loss: 0.2237803339958191
train_iter_loss: 0.30456581711769104
train_iter_loss: 0.3190460503101349
train_iter_loss: 0.2420177310705185
train_iter_loss: 0.35533803701400757
train_iter_loss: 0.10739278048276901
train_iter_loss: 0.33861610293388367
train_iter_loss: 0.5047191381454468
train_iter_loss: 0.4027884304523468
train_iter_loss: 0.3956861197948456
train_iter_loss: 0.4949495494365692
train_iter_loss: 0.4519871175289154
train_iter_loss: 0.42358019948005676
train_iter_loss: 0.26551422476768494
train_iter_loss: 0.440792441368103
train_iter_loss: 0.3998374342918396
train_iter_loss: 0.22803573310375214
train_iter_loss: 0.2656049132347107
train_iter_loss: 0.2805577218532562
train_iter_loss: 0.26575684547424316
train_iter_loss: 0.24293600022792816
train_iter_loss: 0.09761570394039154
train_iter_loss: 0.23540262877941132
train_iter_loss: 0.3032241761684418
train_iter_loss: 0.33996060490608215
train_iter_loss: 0.1995585411787033
train_iter_loss: 0.27294492721557617
train_iter_loss: 0.2202029824256897
train_iter_loss: 0.30919763445854187
train_iter_loss: 0.2044142633676529
train_iter_loss: 0.3491992950439453
train_iter_loss: 0.34332194924354553
train_iter_loss: 0.2973160445690155
train_iter_loss: 0.3137199580669403
train_iter_loss: 0.2549927532672882
train_iter_loss: 0.21671399474143982
train_iter_loss: 0.3704429268836975
train_iter_loss: 0.2674092650413513
train_iter_loss: 0.261940598487854
train_iter_loss: 0.2645592987537384
train_iter_loss: 0.3537852466106415
train_iter_loss: 0.35266223549842834
train_iter_loss: 0.3013286590576172
train_iter_loss: 0.13025091588497162
train_iter_loss: 0.19521214067935944
train_iter_loss: 0.228494331240654
train_iter_loss: 0.17986594140529633
train_iter_loss: 0.2823030650615692
train_iter_loss: 0.2814483940601349
train_iter_loss: 0.07168596982955933
train_iter_loss: 0.19972385466098785
train_iter_loss: 0.35851991176605225
train_iter_loss: 0.22459591925144196
train_iter_loss: 0.2701531648635864
train_iter_loss: 0.28721219301223755
train_iter_loss: 0.33496594429016113
train_iter_loss: 0.3075653910636902
train_iter_loss: 0.2647278606891632
train_iter_loss: 0.35193172097206116
train_iter_loss: 0.38391444087028503
train_iter_loss: 0.4000198245048523
train_iter_loss: 0.32270345091819763
train_iter_loss: 0.38340291380882263
train_iter_loss: 0.19969163835048676
train_iter_loss: 0.21411199867725372
train_iter_loss: 0.19779850542545319
train_iter_loss: 0.29587534070014954
train_iter_loss: 0.11761820316314697
train_iter_loss: 0.21759538352489471
train_iter_loss: 0.29261258244514465
train_iter_loss: 0.39549991488456726
train_iter_loss: 0.3497975468635559
train_iter_loss: 0.21254415810108185
train_iter_loss: 0.3464635908603668
train_iter_loss: 0.3035486042499542
train_iter_loss: 0.21877950429916382
train_iter_loss: 0.2471495121717453
train_iter_loss: 0.27257347106933594
train_iter_loss: 0.10140904784202576
train_iter_loss: 0.3108063042163849
train_iter_loss: 0.19504514336585999
train_iter_loss: 0.2700783312320709
train_iter_loss: 0.07225252687931061
train_iter_loss: 0.3333164155483246
train loss :0.2849
---------------------
Validation seg loss: 0.4447850903478574 at epoch 330
epoch =    331/  1000, exp = train
train_iter_loss: 0.21766765415668488
train_iter_loss: 0.22764967381954193
train_iter_loss: 0.24804122745990753
train_iter_loss: 0.18894290924072266
train_iter_loss: 0.2835281193256378
train_iter_loss: 0.17991526424884796
train_iter_loss: 0.23851007223129272
train_iter_loss: 0.3832949101924896
train_iter_loss: 0.19364795088768005
train_iter_loss: 0.023069707676768303
train_iter_loss: 0.6227811574935913
train_iter_loss: 0.19799186289310455
train_iter_loss: 0.3367651104927063
train_iter_loss: 0.3142489790916443
train_iter_loss: 0.29862362146377563
train_iter_loss: 0.22491733729839325
train_iter_loss: 0.33186981081962585
train_iter_loss: 0.37008821964263916
train_iter_loss: 0.2999689280986786
train_iter_loss: 0.193818598985672
train_iter_loss: 0.14472083747386932
train_iter_loss: 0.2510598599910736
train_iter_loss: 0.26122787594795227
train_iter_loss: 0.23593011498451233
train_iter_loss: 0.2548971474170685
train_iter_loss: 0.2392008900642395
train_iter_loss: 0.035946764051914215
train_iter_loss: 0.2867853045463562
train_iter_loss: 0.3321032226085663
train_iter_loss: 0.29578909277915955
train_iter_loss: 0.237060546875
train_iter_loss: 0.346118301153183
train_iter_loss: 0.27842068672180176
train_iter_loss: 0.35110732913017273
train_iter_loss: 0.18987736105918884
train_iter_loss: 0.315415620803833
train_iter_loss: 0.3528038561344147
train_iter_loss: 0.3160978853702545
train_iter_loss: 0.42608675360679626
train_iter_loss: 0.33138155937194824
train_iter_loss: 0.2650473415851593
train_iter_loss: 0.39041680097579956
train_iter_loss: 0.3613077700138092
train_iter_loss: 0.24649791419506073
train_iter_loss: 0.22754277288913727
train_iter_loss: 0.3392570912837982
train_iter_loss: 0.3208247721195221
train_iter_loss: 0.29838576912879944
train_iter_loss: 0.2623806595802307
train_iter_loss: 0.2766838073730469
train_iter_loss: 0.27005675435066223
train_iter_loss: 0.3306262791156769
train_iter_loss: 0.3387986719608307
train_iter_loss: 0.3096502721309662
train_iter_loss: 0.293311208486557
train_iter_loss: 0.26288554072380066
train_iter_loss: 0.22102922201156616
train_iter_loss: 0.3343161344528198
train_iter_loss: 0.41644880175590515
train_iter_loss: 0.21105878055095673
train_iter_loss: 0.24296019971370697
train_iter_loss: 0.19286201894283295
train_iter_loss: 0.11988908797502518
train_iter_loss: 0.2890837490558624
train_iter_loss: 0.3512122929096222
train_iter_loss: 0.23246155679225922
train_iter_loss: 0.2170819342136383
train_iter_loss: 0.31547337770462036
train_iter_loss: 0.33061131834983826
train_iter_loss: 0.3935772776603699
train_iter_loss: 0.09408677369356155
train_iter_loss: 0.22679778933525085
train_iter_loss: 0.21911321580410004
train_iter_loss: 0.3299408555030823
train_iter_loss: 0.29419147968292236
train_iter_loss: 0.30891287326812744
train_iter_loss: 0.30728986859321594
train_iter_loss: 0.2996082603931427
train_iter_loss: 0.3414624333381653
train_iter_loss: 0.11514319479465485
train_iter_loss: 0.2372690588235855
train_iter_loss: 0.35565680265426636
train_iter_loss: 0.3076193928718567
train_iter_loss: 0.2978663444519043
train_iter_loss: 0.19294311106204987
train_iter_loss: 0.14215494692325592
train_iter_loss: 0.17550355195999146
train_iter_loss: 0.3455027639865875
train_iter_loss: 0.39204391837120056
train_iter_loss: 0.20460595190525055
train_iter_loss: 0.4279640018939972
train_iter_loss: 0.2375088483095169
train_iter_loss: 0.36732345819473267
train_iter_loss: 0.19289343059062958
train_iter_loss: 0.1325579136610031
train_iter_loss: 0.1797506958246231
train_iter_loss: 0.3049418032169342
train_iter_loss: 0.33023494482040405
train_iter_loss: 0.31986290216445923
train_iter_loss: 0.2292080968618393
train loss :0.2771
---------------------
Validation seg loss: 0.39074076154216564 at epoch 331
epoch =    332/  1000, exp = train
train_iter_loss: 0.26706379652023315
train_iter_loss: 0.36101633310317993
train_iter_loss: 0.26619207859039307
train_iter_loss: 0.4628661870956421
train_iter_loss: 0.4462796449661255
train_iter_loss: 0.26489055156707764
train_iter_loss: 0.29399219155311584
train_iter_loss: 0.27834516763687134
train_iter_loss: 0.33473849296569824
train_iter_loss: 0.2344953864812851
train_iter_loss: 0.1576353907585144
train_iter_loss: 0.25618380308151245
train_iter_loss: 0.261886328458786
train_iter_loss: 0.24984535574913025
train_iter_loss: 0.38495543599128723
train_iter_loss: 0.30364152789115906
train_iter_loss: 0.23049889504909515
train_iter_loss: 0.40582290291786194
train_iter_loss: 0.2789735794067383
train_iter_loss: 0.19551843404769897
train_iter_loss: 0.3432840406894684
train_iter_loss: 0.3117232918739319
train_iter_loss: 0.1990669071674347
train_iter_loss: 0.23549985885620117
train_iter_loss: 0.29544684290885925
train_iter_loss: 0.3019067347049713
train_iter_loss: 0.2488349974155426
train_iter_loss: 0.3729807436466217
train_iter_loss: 0.20357093214988708
train_iter_loss: 0.07386347651481628
train_iter_loss: 0.3397388160228729
train_iter_loss: 0.26488178968429565
train_iter_loss: 0.35791438817977905
train_iter_loss: 0.24161562323570251
train_iter_loss: 0.22416909039020538
train_iter_loss: 0.13958828151226044
train_iter_loss: 0.17794963717460632
train_iter_loss: 0.15980961918830872
train_iter_loss: 0.2021893411874771
train_iter_loss: 0.22747424244880676
train_iter_loss: 0.28204232454299927
train_iter_loss: 0.3266838788986206
train_iter_loss: 0.37225931882858276
train_iter_loss: 0.251148521900177
train_iter_loss: 0.24721980094909668
train_iter_loss: 0.3053542375564575
train_iter_loss: 0.3368457853794098
train_iter_loss: 0.3911800682544708
train_iter_loss: 0.20402464270591736
train_iter_loss: 0.27210724353790283
train_iter_loss: 0.4269847571849823
train_iter_loss: 0.2781997323036194
train_iter_loss: 0.21094825863838196
train_iter_loss: 0.20630541443824768
train_iter_loss: 0.2756340205669403
train_iter_loss: 0.237080916762352
train_iter_loss: 0.3267315626144409
train_iter_loss: 0.10194486379623413
train_iter_loss: 0.1920798271894455
train_iter_loss: 0.19530805945396423
train_iter_loss: 0.39442455768585205
train_iter_loss: 0.4757971167564392
train_iter_loss: 0.1980651319026947
train_iter_loss: 0.4568704068660736
train_iter_loss: 0.40482935309410095
train_iter_loss: 0.2827571928501129
train_iter_loss: 0.16975897550582886
train_iter_loss: 0.41198766231536865
train_iter_loss: 0.33416426181793213
train_iter_loss: 0.21213015913963318
train_iter_loss: 0.4614869952201843
train_iter_loss: 0.17854361236095428
train_iter_loss: 0.10383041203022003
train_iter_loss: 0.20749863982200623
train_iter_loss: 0.07456329464912415
train_iter_loss: 0.3625429570674896
train_iter_loss: 0.3599355220794678
train_iter_loss: 0.306133508682251
train_iter_loss: 0.2942180037498474
train_iter_loss: 0.158490851521492
train_iter_loss: 0.4183267652988434
train_iter_loss: 0.18223734200000763
train_iter_loss: 0.36889901757240295
train_iter_loss: 0.20652933418750763
train_iter_loss: 0.30810365080833435
train_iter_loss: 0.27517834305763245
train_iter_loss: 0.2697129547595978
train_iter_loss: 0.36790722608566284
train_iter_loss: 0.3048577904701233
train_iter_loss: 0.08040785044431686
train_iter_loss: 0.13074477016925812
train_iter_loss: 0.3884710967540741
train_iter_loss: 0.24439890682697296
train_iter_loss: 0.2967221736907959
train_iter_loss: 0.22228090465068817
train_iter_loss: 0.25578251481056213
train_iter_loss: 0.14130187034606934
train_iter_loss: 0.27516278624534607
train_iter_loss: 0.3115638494491577
train_iter_loss: 0.5183577537536621
train loss :0.2799
---------------------
Validation seg loss: 0.3605387490594162 at epoch 332
epoch =    333/  1000, exp = train
train_iter_loss: 0.19287504255771637
train_iter_loss: 0.19579805433750153
train_iter_loss: 0.15862515568733215
train_iter_loss: 0.22451592981815338
train_iter_loss: 0.3749087452888489
train_iter_loss: 0.19876250624656677
train_iter_loss: 0.39925628900527954
train_iter_loss: 0.1192665547132492
train_iter_loss: 0.25562232732772827
train_iter_loss: 0.18118739128112793
train_iter_loss: 0.4029180407524109
train_iter_loss: 0.40829452872276306
train_iter_loss: 0.3825128972530365
train_iter_loss: 0.17513936758041382
train_iter_loss: 0.370958149433136
train_iter_loss: 0.31617629528045654
train_iter_loss: 0.27824851870536804
train_iter_loss: 0.5173083543777466
train_iter_loss: 0.14285124838352203
train_iter_loss: 0.35223984718322754
train_iter_loss: 0.24743004143238068
train_iter_loss: 0.35366716980934143
train_iter_loss: 0.38072267174720764
train_iter_loss: 0.3983404040336609
train_iter_loss: 0.3013349175453186
train_iter_loss: 0.28261780738830566
train_iter_loss: 0.23630286753177643
train_iter_loss: 0.33907490968704224
train_iter_loss: 0.33379173278808594
train_iter_loss: 0.18083348870277405
train_iter_loss: 0.25592464208602905
train_iter_loss: 0.3916374444961548
train_iter_loss: 0.22247609496116638
train_iter_loss: 0.1697714775800705
train_iter_loss: 0.08692503720521927
train_iter_loss: 0.14316613972187042
train_iter_loss: 0.14854958653450012
train_iter_loss: 0.23432378470897675
train_iter_loss: 0.2462143748998642
train_iter_loss: 0.2632059156894684
train_iter_loss: 0.31442269682884216
train_iter_loss: 0.2165696620941162
train_iter_loss: 0.29633885622024536
train_iter_loss: 0.30952179431915283
train_iter_loss: 0.2760585844516754
train_iter_loss: 0.16933739185333252
train_iter_loss: 0.29254665970802307
train_iter_loss: 0.26114562153816223
train_iter_loss: 0.28994253277778625
train_iter_loss: 0.25821104645729065
train_iter_loss: 0.22448380291461945
train_iter_loss: 0.24269701540470123
train_iter_loss: 0.35965603590011597
train_iter_loss: 0.30809256434440613
train_iter_loss: 0.3038189113140106
train_iter_loss: 0.27263036370277405
train_iter_loss: 0.253997802734375
train_iter_loss: 0.2538462281227112
train_iter_loss: 0.209525927901268
train_iter_loss: 0.2583968937397003
train_iter_loss: 0.22426040470600128
train_iter_loss: 0.2463075965642929
train_iter_loss: 0.19466552138328552
train_iter_loss: 0.29555174708366394
train_iter_loss: 0.3865753412246704
train_iter_loss: 0.3762304484844208
train_iter_loss: 0.3177556097507477
train_iter_loss: 0.2777233421802521
train_iter_loss: 0.26808488368988037
train_iter_loss: 0.3235112428665161
train_iter_loss: 0.2987688183784485
train_iter_loss: 0.36746135354042053
train_iter_loss: 0.28326499462127686
train_iter_loss: 0.18336446583271027
train_iter_loss: 0.20921775698661804
train_iter_loss: 0.25302937626838684
train_iter_loss: 0.2847732901573181
train_iter_loss: 0.30005139112472534
train_iter_loss: 0.29958656430244446
train_iter_loss: 0.18337590992450714
train_iter_loss: 0.22307582199573517
train_iter_loss: 0.1968124657869339
train_iter_loss: 0.291280597448349
train_iter_loss: 0.10373564809560776
train_iter_loss: 0.30288124084472656
train_iter_loss: 0.28991782665252686
train_iter_loss: 0.2533203363418579
train_iter_loss: 0.3236890733242035
train_iter_loss: 0.23521342873573303
train_iter_loss: 0.14447368681430817
train_iter_loss: 0.3973231613636017
train_iter_loss: 0.19025197625160217
train_iter_loss: 0.28547903895378113
train_iter_loss: 0.31630486249923706
train_iter_loss: 0.22226294875144958
train_iter_loss: 0.35546091198921204
train_iter_loss: 0.34080928564071655
train_iter_loss: 0.3708011507987976
train_iter_loss: 0.18108537793159485
train_iter_loss: 0.24380072951316833
train loss :0.2736
---------------------
Validation seg loss: 0.385752108123786 at epoch 333
epoch =    334/  1000, exp = train
train_iter_loss: 0.23229244351387024
train_iter_loss: 0.41194912791252136
train_iter_loss: 0.1268741488456726
train_iter_loss: 0.4047297239303589
train_iter_loss: 0.37004172801971436
train_iter_loss: 0.42310330271720886
train_iter_loss: 0.28965580463409424
train_iter_loss: 0.3207540810108185
train_iter_loss: 0.31797152757644653
train_iter_loss: 0.2741831839084625
train_iter_loss: 0.288575679063797
train_iter_loss: 0.39383795857429504
train_iter_loss: 0.13921654224395752
train_iter_loss: 0.31205350160598755
train_iter_loss: 0.24328814446926117
train_iter_loss: 0.2635813355445862
train_iter_loss: 0.3486276865005493
train_iter_loss: 0.33775147795677185
train_iter_loss: 0.2343374788761139
train_iter_loss: 0.23730887472629547
train_iter_loss: 0.2542189955711365
train_iter_loss: 0.3058081269264221
train_iter_loss: 0.178025484085083
train_iter_loss: 0.25544649362564087
train_iter_loss: 0.2228696495294571
train_iter_loss: 0.17543312907218933
train_iter_loss: 0.2232518494129181
train_iter_loss: 0.21863490343093872
train_iter_loss: 0.32707658410072327
train_iter_loss: 0.2789101302623749
train_iter_loss: 0.22609151899814606
train_iter_loss: 0.11527951806783676
train_iter_loss: 0.4478302001953125
train_iter_loss: 0.23678642511367798
train_iter_loss: 0.33101266622543335
train_iter_loss: 0.2907698452472687
train_iter_loss: 0.183799147605896
train_iter_loss: 0.24925684928894043
train_iter_loss: 0.3465256094932556
train_iter_loss: 0.3563900887966156
train_iter_loss: 0.23510843515396118
train_iter_loss: 0.32915258407592773
train_iter_loss: 0.315123587846756
train_iter_loss: 0.3258105218410492
train_iter_loss: 0.22523233294487
train_iter_loss: 0.28341466188430786
train_iter_loss: 0.4035322070121765
train_iter_loss: 0.34041938185691833
train_iter_loss: 0.3209834098815918
train_iter_loss: 0.262751042842865
train_iter_loss: 0.26585182547569275
train_iter_loss: 0.20614410936832428
train_iter_loss: 0.3448503911495209
train_iter_loss: 0.33845609426498413
train_iter_loss: 0.12356149405241013
train_iter_loss: 0.2990645170211792
train_iter_loss: 0.24567082524299622
train_iter_loss: 0.40130534768104553
train_iter_loss: 0.22669486701488495
train_iter_loss: 0.2515762746334076
train_iter_loss: 0.33180639147758484
train_iter_loss: 0.4473770260810852
train_iter_loss: 0.20166443288326263
train_iter_loss: 0.2559814453125
train_iter_loss: 0.36594435572624207
train_iter_loss: 0.23691301047801971
train_iter_loss: 0.3711322844028473
train_iter_loss: 0.4400392770767212
train_iter_loss: 0.3314310610294342
train_iter_loss: 0.1647406965494156
train_iter_loss: 0.15667688846588135
train_iter_loss: 0.3645562529563904
train_iter_loss: 0.21232277154922485
train_iter_loss: 0.29470616579055786
train_iter_loss: 0.22946488857269287
train_iter_loss: 0.24666781723499298
train_iter_loss: 0.17007741332054138
train_iter_loss: 0.2706547677516937
train_iter_loss: 0.12497977167367935
train_iter_loss: 0.27712878584861755
train_iter_loss: 0.30264437198638916
train_iter_loss: 0.32276618480682373
train_iter_loss: 0.2867969870567322
train_iter_loss: 0.3110648989677429
train_iter_loss: 0.3064524531364441
train_iter_loss: 0.18191292881965637
train_iter_loss: 0.26670122146606445
train_iter_loss: 0.26452207565307617
train_iter_loss: 0.2475224882364273
train_iter_loss: 0.2748081386089325
train_iter_loss: 0.2357121855020523
train_iter_loss: 0.17835451662540436
train_iter_loss: 0.2969997823238373
train_iter_loss: 0.18010172247886658
train_iter_loss: 0.21070009469985962
train_iter_loss: 0.3289315104484558
train_iter_loss: 0.2480616271495819
train_iter_loss: 0.28254398703575134
train_iter_loss: 0.36613011360168457
train_iter_loss: 0.37600186467170715
train loss :0.2817
---------------------
Validation seg loss: 0.37957544758353595 at epoch 334
epoch =    335/  1000, exp = train
train_iter_loss: 0.2435968965291977
train_iter_loss: 0.22872918844223022
train_iter_loss: 0.3017898201942444
train_iter_loss: 0.34481075406074524
train_iter_loss: 0.3785228133201599
train_iter_loss: 0.1550372838973999
train_iter_loss: 0.16997498273849487
train_iter_loss: 0.2670591175556183
train_iter_loss: 0.21765364706516266
train_iter_loss: 0.3656754493713379
train_iter_loss: 0.26309388875961304
train_iter_loss: 0.3630557060241699
train_iter_loss: 0.21473126113414764
train_iter_loss: 0.19368727505207062
train_iter_loss: 0.2744221091270447
train_iter_loss: 0.22638864815235138
train_iter_loss: 0.2192835807800293
train_iter_loss: 0.305914044380188
train_iter_loss: 0.15158341825008392
train_iter_loss: 0.4078376293182373
train_iter_loss: 0.19030405580997467
train_iter_loss: 0.20694079995155334
train_iter_loss: 0.2112189680337906
train_iter_loss: 0.23001419007778168
train_iter_loss: 0.3446078300476074
train_iter_loss: 0.16845442354679108
train_iter_loss: 0.2328648716211319
train_iter_loss: 0.26901209354400635
train_iter_loss: 0.24649429321289062
train_iter_loss: 0.3082694411277771
train_iter_loss: 0.32805925607681274
train_iter_loss: 0.2036227583885193
train_iter_loss: 0.4182291030883789
train_iter_loss: 0.25584253668785095
train_iter_loss: 0.4820030629634857
train_iter_loss: 0.3118610680103302
train_iter_loss: 0.26367589831352234
train_iter_loss: 0.22438955307006836
train_iter_loss: 0.22364023327827454
train_iter_loss: 0.18393629789352417
train_iter_loss: 0.3573322296142578
train_iter_loss: 0.2961428165435791
train_iter_loss: 0.2801268994808197
train_iter_loss: 0.33036139607429504
train_iter_loss: 0.1472609043121338
train_iter_loss: 0.12770235538482666
train_iter_loss: 0.2963050603866577
train_iter_loss: 0.28950193524360657
train_iter_loss: 0.3749159574508667
train_iter_loss: 0.33050552010536194
train_iter_loss: 0.5342035889625549
train_iter_loss: 0.18969909846782684
train_iter_loss: 0.27948856353759766
train_iter_loss: 0.24871188402175903
train_iter_loss: 0.2831549644470215
train_iter_loss: 0.24309852719306946
train_iter_loss: 0.20552875101566315
train_iter_loss: 0.25284650921821594
train_iter_loss: 0.25356972217559814
train_iter_loss: 0.26076066493988037
train_iter_loss: 0.3014618754386902
train_iter_loss: 0.26163381338119507
train_iter_loss: 0.21297967433929443
train_iter_loss: 0.4701119661331177
train_iter_loss: 0.308866411447525
train_iter_loss: 0.2518443167209625
train_iter_loss: 0.374590665102005
train_iter_loss: 0.32906174659729004
train_iter_loss: 0.3403126895427704
train_iter_loss: 0.23481285572052002
train_iter_loss: 0.11064622551202774
train_iter_loss: 0.27202197909355164
train_iter_loss: 0.22788286209106445
train_iter_loss: 0.23011556267738342
train_iter_loss: 0.24183650314807892
train_iter_loss: 0.2613523602485657
train_iter_loss: 0.25761640071868896
train_iter_loss: 0.3062179684638977
train_iter_loss: 0.18174830079078674
train_iter_loss: 0.3557136058807373
train_iter_loss: 0.340642511844635
train_iter_loss: 0.43101966381073
train_iter_loss: 0.24898619949817657
train_iter_loss: 0.21025478839874268
train_iter_loss: 0.3031495213508606
train_iter_loss: 0.3021639585494995
train_iter_loss: 0.2914199233055115
train_iter_loss: 0.28505948185920715
train_iter_loss: 0.20939777791500092
train_iter_loss: 0.28761065006256104
train_iter_loss: 0.2767343521118164
train_iter_loss: 0.2690415680408478
train_iter_loss: 0.31707650423049927
train_iter_loss: 0.31781864166259766
train_iter_loss: 0.29389411211013794
train_iter_loss: 0.40165889263153076
train_iter_loss: 0.4178319573402405
train_iter_loss: 0.16486680507659912
train_iter_loss: 0.15529723465442657
train_iter_loss: 0.1376219391822815
train loss :0.2765
---------------------
Validation seg loss: 0.3734018597751856 at epoch 335
epoch =    336/  1000, exp = train
train_iter_loss: 0.3807540237903595
train_iter_loss: 0.37470027804374695
train_iter_loss: 0.3442656695842743
train_iter_loss: 0.1970313936471939
train_iter_loss: 0.2878737151622772
train_iter_loss: 0.22579899430274963
train_iter_loss: 0.3906741142272949
train_iter_loss: 0.2117888331413269
train_iter_loss: 0.3580985367298126
train_iter_loss: 0.22128990292549133
train_iter_loss: 0.3095152676105499
train_iter_loss: 0.24994005262851715
train_iter_loss: 0.26393428444862366
train_iter_loss: 0.08355499804019928
train_iter_loss: 0.26361748576164246
train_iter_loss: 0.3494548201560974
train_iter_loss: 0.21289202570915222
train_iter_loss: 0.3709959387779236
train_iter_loss: 0.19460228085517883
train_iter_loss: 0.11794814467430115
train_iter_loss: 0.15857155621051788
train_iter_loss: 0.3056334853172302
train_iter_loss: 0.2890762388706207
train_iter_loss: 0.259279727935791
train_iter_loss: 0.3976849317550659
train_iter_loss: 0.25609055161476135
train_iter_loss: 0.15383026003837585
train_iter_loss: 0.7834175825119019
train_iter_loss: 0.19059818983078003
train_iter_loss: 0.2522982954978943
train_iter_loss: 0.3023061454296112
train_iter_loss: 0.40446937084198
train_iter_loss: 0.21443015336990356
train_iter_loss: 0.35780221223831177
train_iter_loss: 0.2982614040374756
train_iter_loss: 0.18068787455558777
train_iter_loss: 0.17204166948795319
train_iter_loss: 0.3116830587387085
train_iter_loss: 0.24554799497127533
train_iter_loss: 0.3021458685398102
train_iter_loss: 0.2923629581928253
train_iter_loss: 0.23313556611537933
train_iter_loss: 0.47965195775032043
train_iter_loss: 0.2580943703651428
train_iter_loss: 0.2265205979347229
train_iter_loss: 0.12905295193195343
train_iter_loss: 0.2447100281715393
train_iter_loss: 0.35338908433914185
train_iter_loss: 0.2203211933374405
train_iter_loss: 0.404426246881485
train_iter_loss: 0.2797587811946869
train_iter_loss: 0.30030903220176697
train_iter_loss: 0.1651918888092041
train_iter_loss: 0.19177237153053284
train_iter_loss: 0.3282914161682129
train_iter_loss: 0.14760787785053253
train_iter_loss: 0.288815438747406
train_iter_loss: 0.18247468769550323
train_iter_loss: 0.26228147745132446
train_iter_loss: 0.3227981925010681
train_iter_loss: 0.23393632471561432
train_iter_loss: 0.344272255897522
train_iter_loss: 0.11936141550540924
train_iter_loss: 0.255450963973999
train_iter_loss: 0.28260040283203125
train_iter_loss: 0.4626670181751251
train_iter_loss: 0.3191496431827545
train_iter_loss: 0.22243060171604156
train_iter_loss: 0.16035261750221252
train_iter_loss: 0.17875218391418457
train_iter_loss: 0.22512461245059967
train_iter_loss: 0.3699797987937927
train_iter_loss: 0.26982077956199646
train_iter_loss: 0.4170861542224884
train_iter_loss: 0.23488974571228027
train_iter_loss: 0.36244475841522217
train_iter_loss: 0.17551954090595245
train_iter_loss: 0.27514970302581787
train_iter_loss: 0.15948224067687988
train_iter_loss: 0.3843524754047394
train_iter_loss: 0.38301125168800354
train_iter_loss: 0.208539217710495
train_iter_loss: 0.3397170901298523
train_iter_loss: 0.26528939604759216
train_iter_loss: 0.20330384373664856
train_iter_loss: 0.3524523377418518
train_iter_loss: 0.4045576751232147
train_iter_loss: 0.42779770493507385
train_iter_loss: 0.3942122757434845
train_iter_loss: 0.22928352653980255
train_iter_loss: 0.276268869638443
train_iter_loss: 0.34504440426826477
train_iter_loss: 0.23366975784301758
train_iter_loss: 0.2856638729572296
train_iter_loss: 0.25330445170402527
train_iter_loss: 0.2982422113418579
train_iter_loss: 0.308930903673172
train_iter_loss: 0.4351450502872467
train_iter_loss: 0.03064025193452835
train_iter_loss: 0.2823601961135864
train loss :0.2824
---------------------
Validation seg loss: 0.387946445327955 at epoch 336
epoch =    337/  1000, exp = train
train_iter_loss: 0.08184286206960678
train_iter_loss: 0.27723032236099243
train_iter_loss: 0.28934991359710693
train_iter_loss: 0.33821043372154236
train_iter_loss: 0.24309216439723969
train_iter_loss: 0.37737375497817993
train_iter_loss: 0.376120924949646
train_iter_loss: 0.1617196649312973
train_iter_loss: 0.17642588913440704
train_iter_loss: 0.31227028369903564
train_iter_loss: 0.1425856351852417
train_iter_loss: 0.24839623272418976
train_iter_loss: 0.3694051206111908
train_iter_loss: 0.2844424545764923
train_iter_loss: 0.23949119448661804
train_iter_loss: 0.1844547837972641
train_iter_loss: 0.2652263045310974
train_iter_loss: 0.10716531425714493
train_iter_loss: 0.11626678705215454
train_iter_loss: 0.3290903568267822
train_iter_loss: 0.28522685170173645
train_iter_loss: 0.25803735852241516
train_iter_loss: 0.3334914445877075
train_iter_loss: 0.16793493926525116
train_iter_loss: 0.3553697466850281
train_iter_loss: 0.11786479502916336
train_iter_loss: 0.26196473836898804
train_iter_loss: 0.29353928565979004
train_iter_loss: 0.17823711037635803
train_iter_loss: 0.36821460723876953
train_iter_loss: 0.23175987601280212
train_iter_loss: 0.389384925365448
train_iter_loss: 0.22612428665161133
train_iter_loss: 0.4647231101989746
train_iter_loss: 0.2734734117984772
train_iter_loss: 0.264431357383728
train_iter_loss: 0.30993959307670593
train_iter_loss: 0.3000471591949463
train_iter_loss: 0.1486205905675888
train_iter_loss: 0.2778829038143158
train_iter_loss: 0.23394742608070374
train_iter_loss: 0.2114550769329071
train_iter_loss: 0.25881698727607727
train_iter_loss: 0.2750551998615265
train_iter_loss: 0.29876017570495605
train_iter_loss: 0.26963624358177185
train_iter_loss: 0.38811609148979187
train_iter_loss: 0.24115458130836487
train_iter_loss: 0.3411903381347656
train_iter_loss: 0.4056895971298218
train_iter_loss: 0.3387630879878998
train_iter_loss: 0.19204866886138916
train_iter_loss: 0.21836601197719574
train_iter_loss: 0.24739080667495728
train_iter_loss: 0.14755474030971527
train_iter_loss: 0.3770003616809845
train_iter_loss: 0.247641459107399
train_iter_loss: 0.22928662598133087
train_iter_loss: 0.2744689881801605
train_iter_loss: 0.3286965787410736
train_iter_loss: 0.18543793261051178
train_iter_loss: 0.2394954115152359
train_iter_loss: 0.2367323935031891
train_iter_loss: 0.2958088219165802
train_iter_loss: 0.2973472476005554
train_iter_loss: 0.3476231098175049
train_iter_loss: 0.253166526556015
train_iter_loss: 0.2586787939071655
train_iter_loss: 0.4690166711807251
train_iter_loss: 0.2668585777282715
train_iter_loss: 0.2584601938724518
train_iter_loss: 0.33306124806404114
train_iter_loss: 0.34168654680252075
train_iter_loss: 0.25426357984542847
train_iter_loss: 0.248036190867424
train_iter_loss: 0.3796536326408386
train_iter_loss: 0.2982759177684784
train_iter_loss: 0.33073651790618896
train_iter_loss: 0.3039714992046356
train_iter_loss: 0.2998333275318146
train_iter_loss: 0.19161559641361237
train_iter_loss: 0.2897539436817169
train_iter_loss: 0.22878798842430115
train_iter_loss: 0.2785559296607971
train_iter_loss: 0.19202034175395966
train_iter_loss: 0.3779517114162445
train_iter_loss: 0.2556019723415375
train_iter_loss: 0.3840348720550537
train_iter_loss: 0.5417125821113586
train_iter_loss: 0.2187972217798233
train_iter_loss: 0.26934289932250977
train_iter_loss: 0.1164935976266861
train_iter_loss: 0.34099072217941284
train_iter_loss: 0.23133131861686707
train_iter_loss: 0.2009640336036682
train_iter_loss: 0.13979686796665192
train_iter_loss: 0.2640397250652313
train_iter_loss: 0.22017039358615875
train_iter_loss: 0.18905360996723175
train_iter_loss: 0.27516138553619385
train loss :0.2734
---------------------
Validation seg loss: 0.3837519894125608 at epoch 337
epoch =    338/  1000, exp = train
train_iter_loss: 0.24170644581317902
train_iter_loss: 0.2203703671693802
train_iter_loss: 0.21710920333862305
train_iter_loss: 0.24166837334632874
train_iter_loss: 0.2165008783340454
train_iter_loss: 0.21079862117767334
train_iter_loss: 0.27505698800086975
train_iter_loss: 0.254219114780426
train_iter_loss: 0.227462038397789
train_iter_loss: 0.17359060049057007
train_iter_loss: 0.5289105772972107
train_iter_loss: 0.24761436879634857
train_iter_loss: 0.20770767331123352
train_iter_loss: 0.1853003203868866
train_iter_loss: 0.2188413292169571
train_iter_loss: 0.4860427677631378
train_iter_loss: 0.2999374568462372
train_iter_loss: 0.29333817958831787
train_iter_loss: 0.3440277874469757
train_iter_loss: 0.3341565430164337
train_iter_loss: 0.31306374073028564
train_iter_loss: 0.27281761169433594
train_iter_loss: 0.2592240273952484
train_iter_loss: 0.27248117327690125
train_iter_loss: 0.2428705394268036
train_iter_loss: 0.379184752702713
train_iter_loss: 0.30767011642456055
train_iter_loss: 0.3078134059906006
train_iter_loss: 0.17969343066215515
train_iter_loss: 0.25992050766944885
train_iter_loss: 0.41846925020217896
train_iter_loss: 0.225361168384552
train_iter_loss: 0.4216003715991974
train_iter_loss: 0.3099041283130646
train_iter_loss: 0.23911911249160767
train_iter_loss: 0.19636030495166779
train_iter_loss: 0.2954532206058502
train_iter_loss: 0.4628787338733673
train_iter_loss: 0.2269168347120285
train_iter_loss: 0.24928027391433716
train_iter_loss: 0.19822338223457336
train_iter_loss: 0.1562749296426773
train_iter_loss: 0.2838532328605652
train_iter_loss: 0.3401537537574768
train_iter_loss: 0.18191415071487427
train_iter_loss: 0.2528876066207886
train_iter_loss: 0.3236945569515228
train_iter_loss: 0.3632027804851532
train_iter_loss: 0.24665401875972748
train_iter_loss: 0.24130594730377197
train_iter_loss: 0.30634093284606934
train_iter_loss: 0.20431923866271973
train_iter_loss: 0.25488194823265076
train_iter_loss: 0.2387331873178482
train_iter_loss: 0.22718490660190582
train_iter_loss: 0.31873732805252075
train_iter_loss: 0.5758019685745239
train_iter_loss: 0.2822587192058563
train_iter_loss: 0.4288019835948944
train_iter_loss: 0.4205329120159149
train_iter_loss: 0.34151145815849304
train_iter_loss: 0.3232897222042084
train_iter_loss: 0.3301200568675995
train_iter_loss: 0.10982204973697662
train_iter_loss: 0.27071043848991394
train_iter_loss: 0.24099795520305634
train_iter_loss: 0.3596853017807007
train_iter_loss: 0.25915050506591797
train_iter_loss: 0.3326408565044403
train_iter_loss: 0.3822532594203949
train_iter_loss: 0.17208021879196167
train_iter_loss: 0.1755184680223465
train_iter_loss: 0.29408493638038635
train_iter_loss: 0.26263561844825745
train_iter_loss: 0.28603845834732056
train_iter_loss: 0.28051528334617615
train_iter_loss: 0.352683424949646
train_iter_loss: 0.19902101159095764
train_iter_loss: 0.2555346190929413
train_iter_loss: 0.3457412123680115
train_iter_loss: 0.1894700825214386
train_iter_loss: 0.16550299525260925
train_iter_loss: 0.2727903723716736
train_iter_loss: 0.2627474069595337
train_iter_loss: 0.2132062315940857
train_iter_loss: 0.20614635944366455
train_iter_loss: 0.1971915066242218
train_iter_loss: 0.16448494791984558
train_iter_loss: 0.18452826142311096
train_iter_loss: 0.29885271191596985
train_iter_loss: 0.09894220530986786
train_iter_loss: 0.24875091016292572
train_iter_loss: 0.2877148985862732
train_iter_loss: 0.16772432625293732
train_iter_loss: 0.24410949647426605
train_iter_loss: 0.2727864980697632
train_iter_loss: 0.250619113445282
train_iter_loss: 0.4713699519634247
train_iter_loss: 0.20709064602851868
train_iter_loss: 0.23029470443725586
train loss :0.2759
---------------------
Validation seg loss: 0.3664981552382123 at epoch 338
epoch =    339/  1000, exp = train
train_iter_loss: 0.2981470823287964
train_iter_loss: 0.3219643831253052
train_iter_loss: 0.34016987681388855
train_iter_loss: 0.24343663454055786
train_iter_loss: 0.35012224316596985
train_iter_loss: 0.2640080749988556
train_iter_loss: 0.30091115832328796
train_iter_loss: 0.30000820755958557
train_iter_loss: 0.12223208695650101
train_iter_loss: 0.26280996203422546
train_iter_loss: 0.14864462614059448
train_iter_loss: 0.3220555782318115
train_iter_loss: 0.3166934847831726
train_iter_loss: 0.4129832684993744
train_iter_loss: 0.25402501225471497
train_iter_loss: 0.29815438389778137
train_iter_loss: 0.3813963830471039
train_iter_loss: 0.2178870588541031
train_iter_loss: 0.212347149848938
train_iter_loss: 0.385986864566803
train_iter_loss: 0.29502391815185547
train_iter_loss: 0.2592097520828247
train_iter_loss: 0.23190350830554962
train_iter_loss: 0.35834696888923645
train_iter_loss: 0.15545734763145447
train_iter_loss: 0.2594437897205353
train_iter_loss: 0.27187269926071167
train_iter_loss: 0.26404106616973877
train_iter_loss: 0.35855036973953247
train_iter_loss: 0.42998066544532776
train_iter_loss: 0.26209795475006104
train_iter_loss: 0.397040992975235
train_iter_loss: 0.21285401284694672
train_iter_loss: 0.3058716356754303
train_iter_loss: 0.20751585066318512
train_iter_loss: 0.1898161917924881
train_iter_loss: 0.3804442286491394
train_iter_loss: 0.3434469997882843
train_iter_loss: 0.22272269427776337
train_iter_loss: 0.3584293723106384
train_iter_loss: 0.16550663113594055
train_iter_loss: 0.27781417965888977
train_iter_loss: 0.22228138148784637
train_iter_loss: 0.15412284433841705
train_iter_loss: 0.30847862362861633
train_iter_loss: 0.38117608428001404
train_iter_loss: 0.39252743124961853
train_iter_loss: 0.3141012191772461
train_iter_loss: 0.3743916451931
train_iter_loss: 0.3281971514225006
train_iter_loss: 0.23880788683891296
train_iter_loss: 0.25487956404685974
train_iter_loss: 0.17432579398155212
train_iter_loss: 0.27151742577552795
train_iter_loss: 0.2950688600540161
train_iter_loss: 0.3127877414226532
train_iter_loss: 0.184535413980484
train_iter_loss: 0.242347851395607
train_iter_loss: 0.17317235469818115
train_iter_loss: 0.10188312828540802
train_iter_loss: 0.3767695724964142
train_iter_loss: 0.19689682126045227
train_iter_loss: 0.20523279905319214
train_iter_loss: 0.20487545430660248
train_iter_loss: 0.33243274688720703
train_iter_loss: 0.3777647912502289
train_iter_loss: 0.1472385972738266
train_iter_loss: 0.3186182975769043
train_iter_loss: 0.23035044968128204
train_iter_loss: 0.1414233297109604
train_iter_loss: 0.10345291346311569
train_iter_loss: 0.37943342328071594
train_iter_loss: 0.24617058038711548
train_iter_loss: 0.3650877773761749
train_iter_loss: 0.2871718406677246
train_iter_loss: 0.32334110140800476
train_iter_loss: 0.2562542259693146
train_iter_loss: 0.253013551235199
train_iter_loss: 0.22072188556194305
train_iter_loss: 0.21619336307048798
train_iter_loss: 0.27331599593162537
train_iter_loss: 0.3440414071083069
train_iter_loss: 0.19847288727760315
train_iter_loss: 0.2977513074874878
train_iter_loss: 0.5114908814430237
train_iter_loss: 0.28959575295448303
train_iter_loss: 0.18407869338989258
train_iter_loss: 0.37392038106918335
train_iter_loss: 0.28571078181266785
train_iter_loss: 0.16280673444271088
train_iter_loss: 0.2325926572084427
train_iter_loss: 0.3185107111930847
train_iter_loss: 0.2527551054954529
train_iter_loss: 0.34921953082084656
train_iter_loss: 0.2152891606092453
train_iter_loss: 0.4316573441028595
train_iter_loss: 0.34253761172294617
train_iter_loss: 0.1448403298854828
train_iter_loss: 0.19842305779457092
train_iter_loss: 0.23409222066402435
train loss :0.2779
---------------------
Validation seg loss: 0.36469740167541326 at epoch 339
epoch =    340/  1000, exp = train
train_iter_loss: 0.2644719183444977
train_iter_loss: 0.2151394933462143
train_iter_loss: 0.25429269671440125
train_iter_loss: 0.30440089106559753
train_iter_loss: 0.3294842541217804
train_iter_loss: 0.16393885016441345
train_iter_loss: 0.2093648910522461
train_iter_loss: 0.4151829183101654
train_iter_loss: 0.10643162578344345
train_iter_loss: 0.3040180504322052
train_iter_loss: 0.23356294631958008
train_iter_loss: 0.4376380443572998
train_iter_loss: 0.2522253692150116
train_iter_loss: 0.3143497705459595
train_iter_loss: 0.36757534742355347
train_iter_loss: 0.1602589637041092
train_iter_loss: 0.2198515683412552
train_iter_loss: 0.17903389036655426
train_iter_loss: 0.2613292336463928
train_iter_loss: 0.14326444268226624
train_iter_loss: 0.3270661234855652
train_iter_loss: 0.10210612416267395
train_iter_loss: 0.23097290098667145
train_iter_loss: 0.16806228458881378
train_iter_loss: 0.2678811848163605
train_iter_loss: 0.20734803378582
train_iter_loss: 0.227381631731987
train_iter_loss: 0.2261255383491516
train_iter_loss: 0.3401729464530945
train_iter_loss: 0.3752361536026001
train_iter_loss: 0.286630243062973
train_iter_loss: 0.33916476368904114
train_iter_loss: 0.33439525961875916
train_iter_loss: 0.3336145579814911
train_iter_loss: 0.3764338493347168
train_iter_loss: 0.43011561036109924
train_iter_loss: 0.08373679220676422
train_iter_loss: 0.282383531332016
train_iter_loss: 0.18289798498153687
train_iter_loss: 0.29632091522216797
train_iter_loss: 0.2388865202665329
train_iter_loss: 0.26959797739982605
train_iter_loss: 0.29160329699516296
train_iter_loss: 0.3669417202472687
train_iter_loss: 0.3393588960170746
train_iter_loss: 0.20802639424800873
train_iter_loss: 0.2453542947769165
train_iter_loss: 0.324204683303833
train_iter_loss: 0.3051460087299347
train_iter_loss: 0.22950316965579987
train_iter_loss: 0.3425314724445343
train_iter_loss: 0.1284753531217575
train_iter_loss: 0.3539256155490875
train_iter_loss: 0.1903398334980011
train_iter_loss: 0.3989414870738983
train_iter_loss: 0.239153191447258
train_iter_loss: 0.3596254587173462
train_iter_loss: 0.42266178131103516
train_iter_loss: 0.35212185978889465
train_iter_loss: 0.3818119764328003
train_iter_loss: 0.24415946006774902
train_iter_loss: 0.3289927840232849
train_iter_loss: 0.26838284730911255
train_iter_loss: 0.3109712302684784
train_iter_loss: 0.12838435173034668
train_iter_loss: 0.2351546585559845
train_iter_loss: 0.16129708290100098
train_iter_loss: 0.22289393842220306
train_iter_loss: 0.25791385769844055
train_iter_loss: 0.25323739647865295
train_iter_loss: 0.28946200013160706
train_iter_loss: 0.30564165115356445
train_iter_loss: 0.36936283111572266
train_iter_loss: 0.4348238408565521
train_iter_loss: 0.2925948202610016
train_iter_loss: 0.22285470366477966
train_iter_loss: 0.2927907705307007
train_iter_loss: 0.34977850317955017
train_iter_loss: 0.518478512763977
train_iter_loss: 0.3048405051231384
train_iter_loss: 0.3263443112373352
train_iter_loss: 0.40775808691978455
train_iter_loss: 0.28444260358810425
train_iter_loss: 0.24107445776462555
train_iter_loss: 0.23646794259548187
train_iter_loss: 0.21956129372119904
train_iter_loss: 0.14502020180225372
train_iter_loss: 0.36749160289764404
train_iter_loss: 0.2189255803823471
train_iter_loss: 0.2362966537475586
train_iter_loss: 0.25017979741096497
train_iter_loss: 0.3342810571193695
train_iter_loss: 0.31538042426109314
train_iter_loss: 0.4320303201675415
train_iter_loss: 0.2167080044746399
train_iter_loss: 0.35400286316871643
train_iter_loss: 0.29956498742103577
train_iter_loss: 0.24554800987243652
train_iter_loss: 0.3100980222225189
train_iter_loss: 0.1563657522201538
train loss :0.2821
---------------------
Validation seg loss: 0.3675646214253919 at epoch 340
epoch =    341/  1000, exp = train
train_iter_loss: 0.20225654542446136
train_iter_loss: 0.24603483080863953
train_iter_loss: 0.3755526542663574
train_iter_loss: 0.16660554707050323
train_iter_loss: 0.268314391374588
train_iter_loss: 0.25354188680648804
train_iter_loss: 0.10193505883216858
train_iter_loss: 0.29419082403182983
train_iter_loss: 0.21165719628334045
train_iter_loss: 0.5376946330070496
train_iter_loss: 0.032796625047922134
train_iter_loss: 0.3123033344745636
train_iter_loss: 0.20917202532291412
train_iter_loss: 0.2840993106365204
train_iter_loss: 0.3035883605480194
train_iter_loss: 0.16011805832386017
train_iter_loss: 0.2496812492609024
train_iter_loss: 0.2938518524169922
train_iter_loss: 0.26677650213241577
train_iter_loss: 0.1845979392528534
train_iter_loss: 0.515366792678833
train_iter_loss: 0.3274853527545929
train_iter_loss: 0.2538447082042694
train_iter_loss: 0.30174314975738525
train_iter_loss: 0.36092111468315125
train_iter_loss: 0.262542724609375
train_iter_loss: 0.29657500982284546
train_iter_loss: 0.24377258121967316
train_iter_loss: 0.29984521865844727
train_iter_loss: 0.4290879964828491
train_iter_loss: 0.2881409823894501
train_iter_loss: 0.20668774843215942
train_iter_loss: 0.35664692521095276
train_iter_loss: 0.304381400346756
train_iter_loss: 0.3601580858230591
train_iter_loss: 0.26711881160736084
train_iter_loss: 0.3605750501155853
train_iter_loss: 0.3281105160713196
train_iter_loss: 0.33358457684516907
train_iter_loss: 0.2675466239452362
train_iter_loss: 0.20816831290721893
train_iter_loss: 0.13863016664981842
train_iter_loss: 0.14244118332862854
train_iter_loss: 0.3801029324531555
train_iter_loss: 0.16997800767421722
train_iter_loss: 0.25620800256729126
train_iter_loss: 0.20214447379112244
train_iter_loss: 0.12108644098043442
train_iter_loss: 0.2271030843257904
train_iter_loss: 0.5399472713470459
train_iter_loss: 0.44561293721199036
train_iter_loss: 0.31421202421188354
train_iter_loss: 0.34620124101638794
train_iter_loss: 0.29925328493118286
train_iter_loss: 0.2023351788520813
train_iter_loss: 0.21506085991859436
train_iter_loss: 0.2644670903682709
train_iter_loss: 0.27262547612190247
train_iter_loss: 0.18750739097595215
train_iter_loss: 0.28595760464668274
train_iter_loss: 0.25787389278411865
train_iter_loss: 0.3823930025100708
train_iter_loss: 0.16767238080501556
train_iter_loss: 0.31253188848495483
train_iter_loss: 0.2691487967967987
train_iter_loss: 0.2684999406337738
train_iter_loss: 0.2783105969429016
train_iter_loss: 0.26609885692596436
train_iter_loss: 0.24612224102020264
train_iter_loss: 0.10458783805370331
train_iter_loss: 0.3261922597885132
train_iter_loss: 0.37646031379699707
train_iter_loss: 0.2736000418663025
train_iter_loss: 0.6049025058746338
train_iter_loss: 0.2590131461620331
train_iter_loss: 0.22225771844387054
train_iter_loss: 0.30118879675865173
train_iter_loss: 0.3005279302597046
train_iter_loss: 0.2551974058151245
train_iter_loss: 0.1762474924325943
train_iter_loss: 0.1796378195285797
train_iter_loss: 0.356796532869339
train_iter_loss: 0.3464122712612152
train_iter_loss: 0.08825407922267914
train_iter_loss: 0.1740305870771408
train_iter_loss: 0.2741214632987976
train_iter_loss: 0.1299506276845932
train_iter_loss: 0.22952494025230408
train_iter_loss: 0.3619076907634735
train_iter_loss: 0.2505250871181488
train_iter_loss: 0.3888707160949707
train_iter_loss: 0.40106260776519775
train_iter_loss: 0.2810684144496918
train_iter_loss: 0.34924933314323425
train_iter_loss: 0.3621950149536133
train_iter_loss: 0.46759575605392456
train_iter_loss: 0.4399169981479645
train_iter_loss: 0.26848679780960083
train_iter_loss: 0.25008535385131836
train_iter_loss: 0.32290592789649963
train loss :0.2839
---------------------
Validation seg loss: 0.38124749455425255 at epoch 341
epoch =    342/  1000, exp = train
train_iter_loss: 0.2511195242404938
train_iter_loss: 0.3851560652256012
train_iter_loss: 0.16445575654506683
train_iter_loss: 0.32998552918434143
train_iter_loss: 0.2896408438682556
train_iter_loss: 0.18607471883296967
train_iter_loss: 0.3977303206920624
train_iter_loss: 0.20447498559951782
train_iter_loss: 0.19158463180065155
train_iter_loss: 0.4069836139678955
train_iter_loss: 0.24702416360378265
train_iter_loss: 0.19201843440532684
train_iter_loss: 0.34116318821907043
train_iter_loss: 0.24389103055000305
train_iter_loss: 0.2039644867181778
train_iter_loss: 0.3589177131652832
train_iter_loss: 0.21325178444385529
train_iter_loss: 0.23194532096385956
train_iter_loss: 0.2661413252353668
train_iter_loss: 0.3078312575817108
train_iter_loss: 0.32328832149505615
train_iter_loss: 0.21267402172088623
train_iter_loss: 0.16456182301044464
train_iter_loss: 0.27583757042884827
train_iter_loss: 0.3057103157043457
train_iter_loss: 0.3405015170574188
train_iter_loss: 0.37002313137054443
train_iter_loss: 0.3624500036239624
train_iter_loss: 0.3307528495788574
train_iter_loss: 0.13261039555072784
train_iter_loss: 0.33177483081817627
train_iter_loss: 0.21888597309589386
train_iter_loss: 0.20469897985458374
train_iter_loss: 0.39403852820396423
train_iter_loss: 0.25847113132476807
train_iter_loss: 0.08153259009122849
train_iter_loss: 0.34152987599372864
train_iter_loss: 0.2435223013162613
train_iter_loss: 0.2539811134338379
train_iter_loss: 0.3042398691177368
train_iter_loss: 0.3174287676811218
train_iter_loss: 0.15566758811473846
train_iter_loss: 0.2857842743396759
train_iter_loss: 0.32763826847076416
train_iter_loss: 0.43716391921043396
train_iter_loss: 0.300966739654541
train_iter_loss: 0.2655850946903229
train_iter_loss: 0.20653878152370453
train_iter_loss: 0.2121475636959076
train_iter_loss: 0.2626517713069916
train_iter_loss: 0.23019050061702728
train_iter_loss: 0.31660181283950806
train_iter_loss: 0.13543324172496796
train_iter_loss: 0.23781543970108032
train_iter_loss: 0.21836397051811218
train_iter_loss: 0.34278208017349243
train_iter_loss: 0.25982391834259033
train_iter_loss: 0.36352115869522095
train_iter_loss: 0.0764070600271225
train_iter_loss: 0.29497602581977844
train_iter_loss: 0.38147789239883423
train_iter_loss: 0.20652323961257935
train_iter_loss: 0.3762601315975189
train_iter_loss: 0.15072426199913025
train_iter_loss: 0.4225804805755615
train_iter_loss: 0.3329784870147705
train_iter_loss: 0.29645299911499023
train_iter_loss: 0.2954331338405609
train_iter_loss: 0.1800931990146637
train_iter_loss: 0.221495121717453
train_iter_loss: 0.18093308806419373
train_iter_loss: 0.3645462095737457
train_iter_loss: 0.2109007090330124
train_iter_loss: 0.2519443929195404
train_iter_loss: 0.1952693611383438
train_iter_loss: 0.2338123321533203
train_iter_loss: 0.13765281438827515
train_iter_loss: 0.32342925667762756
train_iter_loss: 0.22403131425380707
train_iter_loss: 0.3154151439666748
train_iter_loss: 0.4238757789134979
train_iter_loss: 0.13115544617176056
train_iter_loss: 0.19261184334754944
train_iter_loss: 0.3752865791320801
train_iter_loss: 0.3074466586112976
train_iter_loss: 0.3410797715187073
train_iter_loss: 0.34349703788757324
train_iter_loss: 0.24506893754005432
train_iter_loss: 0.2603723108768463
train_iter_loss: 0.4105473458766937
train_iter_loss: 0.2351592779159546
train_iter_loss: 0.33540499210357666
train_iter_loss: 0.25579598546028137
train_iter_loss: 0.19101907312870026
train_iter_loss: 0.3622397780418396
train_iter_loss: 0.1518777310848236
train_iter_loss: 0.3331372141838074
train_iter_loss: 0.24579845368862152
train_iter_loss: 0.33047720789909363
train_iter_loss: 0.3605891764163971
train loss :0.2753
---------------------
Validation seg loss: 0.3844221171237669 at epoch 342
epoch =    343/  1000, exp = train
train_iter_loss: 0.37180987000465393
train_iter_loss: 0.2116689532995224
train_iter_loss: 0.36811232566833496
train_iter_loss: 0.14024946093559265
train_iter_loss: 0.37183472514152527
train_iter_loss: 0.21371282637119293
train_iter_loss: 0.33176690340042114
train_iter_loss: 0.38149890303611755
train_iter_loss: 0.3361470103263855
train_iter_loss: 0.2436332106590271
train_iter_loss: 0.44404780864715576
train_iter_loss: 0.2471923679113388
train_iter_loss: 0.24993810057640076
train_iter_loss: 0.25954532623291016
train_iter_loss: 0.38621047139167786
train_iter_loss: 0.3234315514564514
train_iter_loss: 0.2621285021305084
train_iter_loss: 0.16661816835403442
train_iter_loss: 0.23674704134464264
train_iter_loss: 0.36359497904777527
train_iter_loss: 0.22004848718643188
train_iter_loss: 0.2567984461784363
train_iter_loss: 0.2254360020160675
train_iter_loss: 0.27083566784858704
train_iter_loss: 0.16179727017879486
train_iter_loss: 0.1811779886484146
train_iter_loss: 0.2412167191505432
train_iter_loss: 0.3025411069393158
train_iter_loss: 0.23263850808143616
train_iter_loss: 0.2177242934703827
train_iter_loss: 0.43295514583587646
train_iter_loss: 0.2870037853717804
train_iter_loss: 0.1582324057817459
train_iter_loss: 0.34996262192726135
train_iter_loss: 0.2329980731010437
train_iter_loss: 0.20268851518630981
train_iter_loss: 0.28791144490242004
train_iter_loss: 0.23434817790985107
train_iter_loss: 0.2788766324520111
train_iter_loss: 0.1284179538488388
train_iter_loss: 0.23063908517360687
train_iter_loss: 0.12904790043830872
train_iter_loss: 0.3898477852344513
train_iter_loss: 0.2954365611076355
train_iter_loss: 0.4645612835884094
train_iter_loss: 0.2311844378709793
train_iter_loss: 0.25918149948120117
train_iter_loss: 0.3491995632648468
train_iter_loss: 0.37738037109375
train_iter_loss: 0.24053707718849182
train_iter_loss: 0.1697760671377182
train_iter_loss: 0.27321162819862366
train_iter_loss: 0.10393807291984558
train_iter_loss: 0.4904921352863312
train_iter_loss: 0.13471955060958862
train_iter_loss: 0.2535785436630249
train_iter_loss: 0.3151561915874481
train_iter_loss: 0.5195790529251099
train_iter_loss: 0.30088627338409424
train_iter_loss: 0.3773464560508728
train_iter_loss: 0.49122121930122375
train_iter_loss: 0.16106043756008148
train_iter_loss: 0.2739061713218689
train_iter_loss: 0.33059144020080566
train_iter_loss: 0.10946080088615417
train_iter_loss: 0.24904654920101166
train_iter_loss: 0.4107702672481537
train_iter_loss: 0.4036239981651306
train_iter_loss: 0.11471424996852875
train_iter_loss: 0.2691578269004822
train_iter_loss: 0.2778586149215698
train_iter_loss: 0.21404850482940674
train_iter_loss: 0.25015950202941895
train_iter_loss: 0.24079445004463196
train_iter_loss: 0.24896983802318573
train_iter_loss: 0.3712489604949951
train_iter_loss: 0.3193050026893616
train_iter_loss: 0.26621225476264954
train_iter_loss: 0.26394614577293396
train_iter_loss: 0.2033664733171463
train_iter_loss: 0.2717697024345398
train_iter_loss: 0.2161969393491745
train_iter_loss: 0.3461383879184723
train_iter_loss: 0.2509916424751282
train_iter_loss: 0.2883633077144623
train_iter_loss: 0.23247911036014557
train_iter_loss: 0.4373909533023834
train_iter_loss: 0.35761553049087524
train_iter_loss: 0.24905449151992798
train_iter_loss: 0.23866857588291168
train_iter_loss: 0.16958992183208466
train_iter_loss: 0.28223511576652527
train_iter_loss: 0.26220542192459106
train_iter_loss: 0.23313438892364502
train_iter_loss: 0.24665050208568573
train_iter_loss: 0.3458341956138611
train_iter_loss: 0.3604971170425415
train_iter_loss: 0.18901365995407104
train_iter_loss: 0.19984109699726105
train_iter_loss: 0.2677755057811737
train loss :0.2791
---------------------
Validation seg loss: 0.35344757215242906 at epoch 343
********************
best_val_epoch_loss:  0.35344757215242906
MODEL UPDATED
epoch =    344/  1000, exp = train
train_iter_loss: 0.2845574617385864
train_iter_loss: 0.20645514130592346
train_iter_loss: 0.37334007024765015
train_iter_loss: 0.252599835395813
train_iter_loss: 0.1535187065601349
train_iter_loss: 0.19105535745620728
train_iter_loss: 0.2552832365036011
train_iter_loss: 0.35065290331840515
train_iter_loss: 0.4249708950519562
train_iter_loss: 0.30674096941947937
train_iter_loss: 0.14460468292236328
train_iter_loss: 0.22711679339408875
train_iter_loss: 0.21912622451782227
train_iter_loss: 0.2765588164329529
train_iter_loss: 0.2660217881202698
train_iter_loss: 0.4025702178478241
train_iter_loss: 0.22896814346313477
train_iter_loss: 0.3687077760696411
train_iter_loss: 0.31745725870132446
train_iter_loss: 0.37916597723960876
train_iter_loss: 0.2403607815504074
train_iter_loss: 0.44811466336250305
train_iter_loss: 0.3825126886367798
train_iter_loss: 0.25074493885040283
train_iter_loss: 0.18558934330940247
train_iter_loss: 0.41465073823928833
train_iter_loss: 0.146936297416687
train_iter_loss: 0.25920718908309937
train_iter_loss: 0.3121499717235565
train_iter_loss: 0.31006351113319397
train_iter_loss: 0.40503552556037903
train_iter_loss: 0.21337053179740906
train_iter_loss: 0.1988106518983841
train_iter_loss: 0.3424335718154907
train_iter_loss: 0.32674726843833923
train_iter_loss: 0.21342617273330688
train_iter_loss: 0.3470360040664673
train_iter_loss: 0.3833455443382263
train_iter_loss: 0.32636260986328125
train_iter_loss: 0.2546892762184143
train_iter_loss: 0.1873515546321869
train_iter_loss: 0.22299933433532715
train_iter_loss: 0.24040628969669342
train_iter_loss: 0.2402655929327011
train_iter_loss: 0.3784715533256531
train_iter_loss: 0.2657897174358368
train_iter_loss: 0.20090526342391968
train_iter_loss: 0.11800344288349152
train_iter_loss: 0.29307249188423157
train_iter_loss: 0.27442666888237
train_iter_loss: 0.21345309913158417
train_iter_loss: 0.15775594115257263
train_iter_loss: 0.23631234467029572
train_iter_loss: 0.2942418158054352
train_iter_loss: 0.3795916736125946
train_iter_loss: 0.2941954433917999
train_iter_loss: 0.21522870659828186
train_iter_loss: 0.22629483044147491
train_iter_loss: 0.22614778578281403
train_iter_loss: 0.2936379313468933
train_iter_loss: 0.16432107985019684
train_iter_loss: 0.3055625259876251
train_iter_loss: 0.23928876221179962
train_iter_loss: 0.23661962151527405
train_iter_loss: 0.3076334297657013
train_iter_loss: 0.28388217091560364
train_iter_loss: 0.3500475585460663
train_iter_loss: 0.1864679455757141
train_iter_loss: 0.17883306741714478
train_iter_loss: 0.3410181701183319
train_iter_loss: 0.12534913420677185
train_iter_loss: 0.22239163517951965
train_iter_loss: 0.4675983190536499
train_iter_loss: 0.294911652803421
train_iter_loss: 0.16661584377288818
train_iter_loss: 0.21105323731899261
train_iter_loss: 0.495204359292984
train_iter_loss: 0.36368703842163086
train_iter_loss: 0.24292832612991333
train_iter_loss: 0.1675652116537094
train_iter_loss: 0.37119728326797485
train_iter_loss: 0.16365469992160797
train_iter_loss: 0.2296929657459259
train_iter_loss: 0.17839348316192627
train_iter_loss: 0.23148462176322937
train_iter_loss: 0.28479447960853577
train_iter_loss: 0.14303824305534363
train_iter_loss: 0.17244863510131836
train_iter_loss: 0.30971279740333557
train_iter_loss: 0.36534520983695984
train_iter_loss: 0.26130133867263794
train_iter_loss: 0.19274486601352692
train_iter_loss: 0.4433286190032959
train_iter_loss: 0.2801535725593567
train_iter_loss: 0.503959059715271
train_iter_loss: 0.30640074610710144
train_iter_loss: 0.25484079122543335
train_iter_loss: 0.37942758202552795
train_iter_loss: 0.296747624874115
train_iter_loss: 0.16693753004074097
train loss :0.2772
---------------------
Validation seg loss: 0.383287379764161 at epoch 344
epoch =    345/  1000, exp = train
train_iter_loss: 0.30880478024482727
train_iter_loss: 0.2498168796300888
train_iter_loss: 0.44777947664260864
train_iter_loss: 0.28266438841819763
train_iter_loss: 0.2813038229942322
train_iter_loss: 0.20646987855434418
train_iter_loss: 0.2664426565170288
train_iter_loss: 0.21568451821804047
train_iter_loss: 0.15727536380290985
train_iter_loss: 0.14294813573360443
train_iter_loss: 0.2348513901233673
train_iter_loss: 0.28995221853256226
train_iter_loss: 0.22300536930561066
train_iter_loss: 0.21189740300178528
train_iter_loss: 0.21329066157341003
train_iter_loss: 0.22303496301174164
train_iter_loss: 0.20520485937595367
train_iter_loss: 0.3470115065574646
train_iter_loss: 0.3184623122215271
train_iter_loss: 0.23866888880729675
train_iter_loss: 0.18232499063014984
train_iter_loss: 0.17799757421016693
train_iter_loss: 0.19960574805736542
train_iter_loss: 0.04605703428387642
train_iter_loss: 0.3333972096443176
train_iter_loss: 0.23074156045913696
train_iter_loss: 0.27793046832084656
train_iter_loss: 0.28868913650512695
train_iter_loss: 0.2320924997329712
train_iter_loss: 0.2296019345521927
train_iter_loss: 0.27964308857917786
train_iter_loss: 0.23117032647132874
train_iter_loss: 0.4075396955013275
train_iter_loss: 0.30770179629325867
train_iter_loss: 0.24457776546478271
train_iter_loss: 0.36359015107154846
train_iter_loss: 0.419131875038147
train_iter_loss: 0.2628785967826843
train_iter_loss: 0.23999282717704773
train_iter_loss: 0.26964741945266724
train_iter_loss: 0.4259921610355377
train_iter_loss: 0.18284256756305695
train_iter_loss: 0.18842832744121552
train_iter_loss: 0.25968241691589355
train_iter_loss: 0.2508772611618042
train_iter_loss: 0.26432928442955017
train_iter_loss: 0.2477063238620758
train_iter_loss: 0.4686378240585327
train_iter_loss: 0.2610301375389099
train_iter_loss: 0.09938332438468933
train_iter_loss: 0.4236600995063782
train_iter_loss: 0.42050260305404663
train_iter_loss: 0.2120772898197174
train_iter_loss: 0.41794490814208984
train_iter_loss: 0.18621252477169037
train_iter_loss: 0.3217307925224304
train_iter_loss: 0.3847607374191284
train_iter_loss: 0.3149583041667938
train_iter_loss: 0.3657786250114441
train_iter_loss: 0.26535743474960327
train_iter_loss: 0.22915375232696533
train_iter_loss: 0.2351534515619278
train_iter_loss: 0.26398923993110657
train_iter_loss: 0.3891570568084717
train_iter_loss: 0.3059062957763672
train_iter_loss: 0.23592251539230347
train_iter_loss: 0.19634538888931274
train_iter_loss: 0.2669747471809387
train_iter_loss: 0.16256843507289886
train_iter_loss: 0.2072019875049591
train_iter_loss: 0.247881680727005
train_iter_loss: 0.3065955936908722
train_iter_loss: 0.14465750753879547
train_iter_loss: 0.27005472779273987
train_iter_loss: 0.18097490072250366
train_iter_loss: 0.30035486817359924
train_iter_loss: 0.19899818301200867
train_iter_loss: 0.48722466826438904
train_iter_loss: 0.25731658935546875
train_iter_loss: 0.35188862681388855
train_iter_loss: 0.15635636448860168
train_iter_loss: 0.3520185053348541
train_iter_loss: 0.29397666454315186
train_iter_loss: 0.2007073312997818
train_iter_loss: 0.394801527261734
train_iter_loss: 0.2785049080848694
train_iter_loss: 0.3569604158401489
train_iter_loss: 0.3286008834838867
train_iter_loss: 0.2143583446741104
train_iter_loss: 0.2536769211292267
train_iter_loss: 0.32492563128471375
train_iter_loss: 0.39914312958717346
train_iter_loss: 0.3387604355812073
train_iter_loss: 0.4491395652294159
train_iter_loss: 0.26582369208335876
train_iter_loss: 0.26029831171035767
train_iter_loss: 0.18630120158195496
train_iter_loss: 0.2281375229358673
train_iter_loss: 0.28144729137420654
train_iter_loss: 0.22994482517242432
train loss :0.2756
---------------------
Validation seg loss: 0.370767072510888 at epoch 345
epoch =    346/  1000, exp = train
train_iter_loss: 0.19858776032924652
train_iter_loss: 0.1772579550743103
train_iter_loss: 0.43024614453315735
train_iter_loss: 0.2218909114599228
train_iter_loss: 0.22012482583522797
train_iter_loss: 0.2644617557525635
train_iter_loss: 0.3613711893558502
train_iter_loss: 0.16645434498786926
train_iter_loss: 0.32608455419540405
train_iter_loss: 0.31128212809562683
train_iter_loss: 0.23036977648735046
train_iter_loss: 0.09615352004766464
train_iter_loss: 0.26157107949256897
train_iter_loss: 0.2781433165073395
train_iter_loss: 0.3651052713394165
train_iter_loss: 0.37386301159858704
train_iter_loss: 0.20991189777851105
train_iter_loss: 0.3901383578777313
train_iter_loss: 0.12801747024059296
train_iter_loss: 0.2600749135017395
train_iter_loss: 0.19217783212661743
train_iter_loss: 0.17535103857517242
train_iter_loss: 0.35324132442474365
train_iter_loss: 0.2325817346572876
train_iter_loss: 0.31244155764579773
train_iter_loss: 0.1967952400445938
train_iter_loss: 0.23074492812156677
train_iter_loss: 0.20933635532855988
train_iter_loss: 0.2222311943769455
train_iter_loss: 0.2936027944087982
train_iter_loss: 0.09490513801574707
train_iter_loss: 0.34868118166923523
train_iter_loss: 0.40253764390945435
train_iter_loss: 0.262482613325119
train_iter_loss: 0.21696911752223969
train_iter_loss: 0.3291224539279938
train_iter_loss: 0.2737654447555542
train_iter_loss: 0.3572467863559723
train_iter_loss: 0.3037542402744293
train_iter_loss: 0.2940746545791626
train_iter_loss: 0.2842957377433777
train_iter_loss: 0.37406715750694275
train_iter_loss: 0.3694695234298706
train_iter_loss: 0.22100624442100525
train_iter_loss: 0.2776605486869812
train_iter_loss: 0.20208820700645447
train_iter_loss: 0.1899321973323822
train_iter_loss: 0.19900557398796082
train_iter_loss: 0.32270023226737976
train_iter_loss: 0.33860519528388977
train_iter_loss: 0.3944738507270813
train_iter_loss: 0.2116292268037796
train_iter_loss: 0.3444724380970001
train_iter_loss: 0.3922770321369171
train_iter_loss: 0.3005581200122833
train_iter_loss: 0.22125868499279022
train_iter_loss: 0.29812484979629517
train_iter_loss: 0.28490540385246277
train_iter_loss: 0.3124485909938812
train_iter_loss: 0.112710140645504
train_iter_loss: 0.4187390208244324
train_iter_loss: 0.23321153223514557
train_iter_loss: 0.3257269859313965
train_iter_loss: 0.2706035077571869
train_iter_loss: 0.1994820237159729
train_iter_loss: 0.17458705604076385
train_iter_loss: 0.31507810950279236
train_iter_loss: 0.4200182557106018
train_iter_loss: 0.31559568643569946
train_iter_loss: 0.41998380422592163
train_iter_loss: 0.21265831589698792
train_iter_loss: 0.22883488237857819
train_iter_loss: 0.29311496019363403
train_iter_loss: 0.21347224712371826
train_iter_loss: 0.38684043288230896
train_iter_loss: 0.3036912679672241
train_iter_loss: 0.17096994817256927
train_iter_loss: 0.3464639186859131
train_iter_loss: 0.25773078203201294
train_iter_loss: 0.19929909706115723
train_iter_loss: 0.2278331071138382
train_iter_loss: 0.10857947915792465
train_iter_loss: 0.26580050587654114
train_iter_loss: 0.30440643429756165
train_iter_loss: 0.16049796342849731
train_iter_loss: 0.07901588082313538
train_iter_loss: 0.0846337080001831
train_iter_loss: 0.2579968273639679
train_iter_loss: 0.18087776005268097
train_iter_loss: 0.3580843210220337
train_iter_loss: 0.3389057219028473
train_iter_loss: 0.41148415207862854
train_iter_loss: 0.6874743103981018
train_iter_loss: 0.40659260749816895
train_iter_loss: 0.2990804612636566
train_iter_loss: 0.30878040194511414
train_iter_loss: 0.28627270460128784
train_iter_loss: 0.1009332612156868
train_iter_loss: 0.2432313859462738
train_iter_loss: 0.3954226076602936
train loss :0.2771
---------------------
Validation seg loss: 0.385190320886531 at epoch 346
epoch =    347/  1000, exp = train
train_iter_loss: 0.19567230343818665
train_iter_loss: 0.20861296355724335
train_iter_loss: 0.27254197001457214
train_iter_loss: 0.4147348701953888
train_iter_loss: 0.4164403975009918
train_iter_loss: 0.2686198353767395
train_iter_loss: 0.31447741389274597
train_iter_loss: 0.41084402799606323
train_iter_loss: 0.24767695367336273
train_iter_loss: 0.4908179044723511
train_iter_loss: 0.194403737783432
train_iter_loss: 0.352918803691864
train_iter_loss: 0.22703124582767487
train_iter_loss: 0.31057652831077576
train_iter_loss: 0.3172081410884857
train_iter_loss: 0.3317578136920929
train_iter_loss: 0.20810355246067047
train_iter_loss: 0.38902807235717773
train_iter_loss: 0.2442634403705597
train_iter_loss: 0.13116554915905
train_iter_loss: 0.40495848655700684
train_iter_loss: 0.19210900366306305
train_iter_loss: 0.37384718656539917
train_iter_loss: 0.3467424511909485
train_iter_loss: 0.3693101406097412
train_iter_loss: 0.2951889634132385
train_iter_loss: 0.21742495894432068
train_iter_loss: 0.369016170501709
train_iter_loss: 0.18762145936489105
train_iter_loss: 0.22848720848560333
train_iter_loss: 0.2999187409877777
train_iter_loss: 0.232268825173378
train_iter_loss: 0.27011480927467346
train_iter_loss: 0.15183520317077637
train_iter_loss: 0.4369890093803406
train_iter_loss: 0.17498666048049927
train_iter_loss: 0.27195680141448975
train_iter_loss: 0.20490200817584991
train_iter_loss: 0.17220602929592133
train_iter_loss: 0.09112276136875153
train_iter_loss: 0.2372327744960785
train_iter_loss: 0.15370294451713562
train_iter_loss: 0.1669219583272934
train_iter_loss: 0.22982709109783173
train_iter_loss: 0.5330012440681458
train_iter_loss: 0.20751513540744781
train_iter_loss: 0.3208710551261902
train_iter_loss: 0.31206488609313965
train_iter_loss: 0.12366411834955215
train_iter_loss: 0.18759708106517792
train_iter_loss: 0.2624146342277527
train_iter_loss: 0.17596498131752014
train_iter_loss: 0.24516122043132782
train_iter_loss: 0.24775494635105133
train_iter_loss: 0.2767004370689392
train_iter_loss: 0.2545563578605652
train_iter_loss: 0.2691861093044281
train_iter_loss: 0.2107381373643875
train_iter_loss: 0.2860294282436371
train_iter_loss: 0.2758874297142029
train_iter_loss: 0.45786046981811523
train_iter_loss: 0.1869278997182846
train_iter_loss: 0.0836414098739624
train_iter_loss: 0.31655535101890564
train_iter_loss: 0.1857803761959076
train_iter_loss: 0.24761317670345306
train_iter_loss: 0.17843472957611084
train_iter_loss: 0.35261452198028564
train_iter_loss: 0.33074283599853516
train_iter_loss: 0.19455818831920624
train_iter_loss: 0.1984717696905136
train_iter_loss: 0.24966955184936523
train_iter_loss: 0.2069718837738037
train_iter_loss: 0.33512088656425476
train_iter_loss: 0.2614244222640991
train_iter_loss: 0.5960750579833984
train_iter_loss: 0.2397034764289856
train_iter_loss: 0.2418033927679062
train_iter_loss: 0.08879964053630829
train_iter_loss: 0.12998747825622559
train_iter_loss: 0.2198919951915741
train_iter_loss: 0.2175273299217224
train_iter_loss: 0.4650072455406189
train_iter_loss: 0.3789675831794739
train_iter_loss: 0.35401082038879395
train_iter_loss: 0.3409233093261719
train_iter_loss: 0.30351340770721436
train_iter_loss: 0.31858590245246887
train_iter_loss: 0.42932841181755066
train_iter_loss: 0.19169795513153076
train_iter_loss: 0.302355021238327
train_iter_loss: 0.28704777359962463
train_iter_loss: 0.2732744514942169
train_iter_loss: 0.2632012963294983
train_iter_loss: 0.15878967940807343
train_iter_loss: 0.20037510991096497
train_iter_loss: 0.34382688999176025
train_iter_loss: 0.36573436856269836
train_iter_loss: 0.4049548804759979
train_iter_loss: 0.2540498971939087
train loss :0.2765
---------------------
Validation seg loss: 0.37270653231139733 at epoch 347
epoch =    348/  1000, exp = train
train_iter_loss: 0.28416553139686584
train_iter_loss: 0.33855724334716797
train_iter_loss: 0.19599339365959167
train_iter_loss: 0.303822785615921
train_iter_loss: 0.21233855187892914
train_iter_loss: 0.24395686388015747
train_iter_loss: 0.22615815699100494
train_iter_loss: 0.2804551422595978
train_iter_loss: 0.37980443239212036
train_iter_loss: 0.3241731524467468
train_iter_loss: 0.2821812033653259
train_iter_loss: 0.2568892538547516
train_iter_loss: 0.2263052761554718
train_iter_loss: 0.2874971032142639
train_iter_loss: 0.3649123013019562
train_iter_loss: 0.26405763626098633
train_iter_loss: 0.4826333224773407
train_iter_loss: 0.3141087293624878
train_iter_loss: 0.22424061596393585
train_iter_loss: 0.37478721141815186
train_iter_loss: 0.17122820019721985
train_iter_loss: 0.2017003893852234
train_iter_loss: 0.31572699546813965
train_iter_loss: 0.19685275852680206
train_iter_loss: 0.3426641523838043
train_iter_loss: 0.29545217752456665
train_iter_loss: 0.09105513244867325
train_iter_loss: 0.2939748466014862
train_iter_loss: 0.18335357308387756
train_iter_loss: 0.3852463364601135
train_iter_loss: 0.32779383659362793
train_iter_loss: 0.2995259761810303
train_iter_loss: 0.1464659422636032
train_iter_loss: 0.40117985010147095
train_iter_loss: 0.22371554374694824
train_iter_loss: 0.3962833285331726
train_iter_loss: 0.37361571192741394
train_iter_loss: 0.3481019139289856
train_iter_loss: 0.2329610288143158
train_iter_loss: 0.301178514957428
train_iter_loss: 0.20181605219841003
train_iter_loss: 0.24033436179161072
train_iter_loss: 0.3651124835014343
train_iter_loss: 0.17379754781723022
train_iter_loss: 0.2233315110206604
train_iter_loss: 0.314656138420105
train_iter_loss: 0.2805483043193817
train_iter_loss: 0.20490853488445282
train_iter_loss: 0.27745556831359863
train_iter_loss: 0.23347550630569458
train_iter_loss: 0.21657313406467438
train_iter_loss: 0.32709839940071106
train_iter_loss: 0.26301655173301697
train_iter_loss: 0.2518114745616913
train_iter_loss: 0.2834147810935974
train_iter_loss: 0.19110608100891113
train_iter_loss: 0.31816020607948303
train_iter_loss: 0.31057214736938477
train_iter_loss: 0.1741221696138382
train_iter_loss: 0.2841689884662628
train_iter_loss: 0.2061035931110382
train_iter_loss: 0.1310594379901886
train_iter_loss: 0.2334176003932953
train_iter_loss: 0.08295643329620361
train_iter_loss: 0.23392100632190704
train_iter_loss: 0.31427201628685
train_iter_loss: 0.2395324409008026
train_iter_loss: 0.2482142150402069
train_iter_loss: 0.3548717796802521
train_iter_loss: 0.2824000418186188
train_iter_loss: 0.2549179494380951
train_iter_loss: 0.3086419403553009
train_iter_loss: 0.24046730995178223
train_iter_loss: 0.33377254009246826
train_iter_loss: 0.1242770180106163
train_iter_loss: 0.1885279417037964
train_iter_loss: 0.311309814453125
train_iter_loss: 0.14140713214874268
train_iter_loss: 0.24028505384922028
train_iter_loss: 0.41304007172584534
train_iter_loss: 0.14581823348999023
train_iter_loss: 0.3523849546909332
train_iter_loss: 0.2662244737148285
train_iter_loss: 0.2799413204193115
train_iter_loss: 0.3338415324687958
train_iter_loss: 0.21880348026752472
train_iter_loss: 0.23985368013381958
train_iter_loss: 0.20129039883613586
train_iter_loss: 0.2799178659915924
train_iter_loss: 0.22145451605319977
train_iter_loss: 0.23061899840831757
train_iter_loss: 0.5083149671554565
train_iter_loss: 0.09839769452810287
train_iter_loss: 0.20647136867046356
train_iter_loss: 0.3419645130634308
train_iter_loss: 0.18906013667583466
train_iter_loss: 0.24782417714595795
train_iter_loss: 0.18805928528308868
train_iter_loss: 0.23783403635025024
train_iter_loss: 0.26066911220550537
train loss :0.2674
---------------------
Validation seg loss: 0.37244973944956966 at epoch 348
epoch =    349/  1000, exp = train
train_iter_loss: 0.2637285888195038
train_iter_loss: 0.12203409522771835
train_iter_loss: 0.12240821123123169
train_iter_loss: 0.15272168815135956
train_iter_loss: 0.20628607273101807
train_iter_loss: 0.24800460040569305
train_iter_loss: 0.3103163242340088
train_iter_loss: 0.2706662118434906
train_iter_loss: 0.32905906438827515
train_iter_loss: 0.22550824284553528
train_iter_loss: 0.2873181402683258
train_iter_loss: 0.42806172370910645
train_iter_loss: 0.20718695223331451
train_iter_loss: 0.23826880753040314
train_iter_loss: 0.3745863735675812
train_iter_loss: 0.06377106159925461
train_iter_loss: 0.1650773286819458
train_iter_loss: 0.31850698590278625
train_iter_loss: 0.2921927869319916
train_iter_loss: 0.4547506868839264
train_iter_loss: 0.3143147826194763
train_iter_loss: 0.23853521049022675
train_iter_loss: 0.24199306964874268
train_iter_loss: 0.15456616878509521
train_iter_loss: 0.21552807092666626
train_iter_loss: 0.2422749251127243
train_iter_loss: 0.23286771774291992
train_iter_loss: 0.14926272630691528
train_iter_loss: 0.2979850172996521
train_iter_loss: 0.4042762517929077
train_iter_loss: 0.312467098236084
train_iter_loss: 0.3058262765407562
train_iter_loss: 0.2826254665851593
train_iter_loss: 0.20263294875621796
train_iter_loss: 0.1168440505862236
train_iter_loss: 0.24083533883094788
train_iter_loss: 0.30673620104789734
train_iter_loss: 0.2669106721878052
train_iter_loss: 0.25911927223205566
train_iter_loss: 0.13351574540138245
train_iter_loss: 0.2478228360414505
train_iter_loss: 0.312533438205719
train_iter_loss: 0.3358582556247711
train_iter_loss: 0.4454710781574249
train_iter_loss: 0.2779208719730377
train_iter_loss: 0.14946846663951874
train_iter_loss: 0.4634036421775818
train_iter_loss: 0.18431293964385986
train_iter_loss: 0.25733083486557007
train_iter_loss: 0.19163063168525696
train_iter_loss: 0.23570819199085236
train_iter_loss: 0.3576516807079315
train_iter_loss: 0.28461745381355286
train_iter_loss: 0.3022700250148773
train_iter_loss: 0.5154427289962769
train_iter_loss: 0.21443137526512146
train_iter_loss: 0.1412268579006195
train_iter_loss: 0.1515423208475113
train_iter_loss: 0.34889549016952515
train_iter_loss: 0.45482388138771057
train_iter_loss: 0.29300934076309204
train_iter_loss: 0.3814236521720886
train_iter_loss: 0.22921139001846313
train_iter_loss: 0.3409412205219269
train_iter_loss: 0.24118266999721527
train_iter_loss: 0.1625356376171112
train_iter_loss: 0.29218509793281555
train_iter_loss: 0.3574593961238861
train_iter_loss: 0.2874249219894409
train_iter_loss: 0.3414517343044281
train_iter_loss: 0.1843164563179016
train_iter_loss: 0.24012909829616547
train_iter_loss: 0.24107235670089722
train_iter_loss: 0.1551368236541748
train_iter_loss: 0.3660018742084503
train_iter_loss: 0.3503311574459076
train_iter_loss: 0.3042658865451813
train_iter_loss: 0.2696097195148468
train_iter_loss: 0.14318151772022247
train_iter_loss: 0.33938372135162354
train_iter_loss: 0.4376283288002014
train_iter_loss: 0.23700876533985138
train_iter_loss: 0.4030114710330963
train_iter_loss: 0.34299391508102417
train_iter_loss: 0.3034460246562958
train_iter_loss: 0.23962822556495667
train_iter_loss: 0.13620078563690186
train_iter_loss: 0.33581748604774475
train_iter_loss: 0.22876453399658203
train_iter_loss: 0.22785040736198425
train_iter_loss: 0.4845668375492096
train_iter_loss: 0.3139476180076599
train_iter_loss: 0.3505338132381439
train_iter_loss: 0.16623197495937347
train_iter_loss: 0.37883108854293823
train_iter_loss: 0.19901084899902344
train_iter_loss: 0.14368632435798645
train_iter_loss: 0.18281450867652893
train_iter_loss: 0.15495093166828156
train_iter_loss: 0.3354981541633606
train loss :0.2726
---------------------
Validation seg loss: 0.36191126909809856 at epoch 349
epoch =    350/  1000, exp = train
train_iter_loss: 0.17789851129055023
train_iter_loss: 0.23048880696296692
train_iter_loss: 0.43999937176704407
train_iter_loss: 0.30595889687538147
train_iter_loss: 0.17899075150489807
train_iter_loss: 0.3824000358581543
train_iter_loss: 0.22914740443229675
train_iter_loss: 0.257750540971756
train_iter_loss: 0.2573881149291992
train_iter_loss: 0.18857097625732422
train_iter_loss: 0.2996308505535126
train_iter_loss: 0.17896954715251923
train_iter_loss: 0.22996513545513153
train_iter_loss: 0.26150086522102356
train_iter_loss: 0.36950135231018066
train_iter_loss: 0.27873972058296204
train_iter_loss: 0.12107297033071518
train_iter_loss: 0.35717642307281494
train_iter_loss: 0.2864542603492737
train_iter_loss: 0.1704726219177246
train_iter_loss: 0.4447462260723114
train_iter_loss: 0.2617725729942322
train_iter_loss: 0.26806578040122986
train_iter_loss: 0.1979837715625763
train_iter_loss: 0.21841740608215332
train_iter_loss: 0.268138587474823
train_iter_loss: 0.21774595975875854
train_iter_loss: 0.2488226592540741
train_iter_loss: 0.27154648303985596
train_iter_loss: 0.2881338596343994
train_iter_loss: 0.15398013591766357
train_iter_loss: 0.3008071184158325
train_iter_loss: 0.4080372750759125
train_iter_loss: 0.23123744130134583
train_iter_loss: 0.4957487881183624
train_iter_loss: 0.15900205075740814
train_iter_loss: 0.17847104370594025
train_iter_loss: 0.17714592814445496
train_iter_loss: 0.32307809591293335
train_iter_loss: 0.25244396924972534
train_iter_loss: 0.2551393210887909
train_iter_loss: 0.3482499420642853
train_iter_loss: 0.24070799350738525
train_iter_loss: 0.16506066918373108
train_iter_loss: 0.25441110134124756
train_iter_loss: 0.37970298528671265
train_iter_loss: 0.4016261398792267
train_iter_loss: 0.18068747222423553
train_iter_loss: 0.2610463798046112
train_iter_loss: 0.5004586577415466
train_iter_loss: 0.17913930118083954
train_iter_loss: 0.21402326226234436
train_iter_loss: 0.3557167649269104
train_iter_loss: 0.36356452107429504
train_iter_loss: 0.244432732462883
train_iter_loss: 0.35518041253089905
train_iter_loss: 0.17351815104484558
train_iter_loss: 0.3412514328956604
train_iter_loss: 0.3471183776855469
train_iter_loss: 0.39514145255088806
train_iter_loss: 0.45266175270080566
train_iter_loss: 0.4019264876842499
train_iter_loss: 0.1805853694677353
train_iter_loss: 0.36758148670196533
train_iter_loss: 0.2237250804901123
train_iter_loss: 0.2861999273300171
train_iter_loss: 0.128814697265625
train_iter_loss: 0.3109127879142761
train_iter_loss: 0.3369307518005371
train_iter_loss: 0.4279153048992157
train_iter_loss: 0.2128329873085022
train_iter_loss: 0.1081962138414383
train_iter_loss: 0.32264015078544617
train_iter_loss: 0.23669567704200745
train_iter_loss: 0.19741104543209076
train_iter_loss: 0.44812506437301636
train_iter_loss: 0.21311311423778534
train_iter_loss: 0.3841065764427185
train_iter_loss: 0.22544297575950623
train_iter_loss: 0.15464156866073608
train_iter_loss: 0.32034027576446533
train_iter_loss: 0.5056999921798706
train_iter_loss: 0.3654788136482239
train_iter_loss: 0.3633081912994385
train_iter_loss: 0.09644126892089844
train_iter_loss: 0.25062671303749084
train_iter_loss: 0.2729269564151764
train_iter_loss: 0.26620733737945557
train_iter_loss: 0.20588847994804382
train_iter_loss: 0.19571387767791748
train_iter_loss: 0.20164087414741516
train_iter_loss: 0.26174166798591614
train_iter_loss: 0.14963828027248383
train_iter_loss: 0.31254205107688904
train_iter_loss: 0.27868548035621643
train_iter_loss: 0.2755456864833832
train_iter_loss: 0.32351788878440857
train_iter_loss: 0.3283669054508209
train_iter_loss: 0.3336111307144165
train_iter_loss: 0.22679707407951355
train loss :0.2795
---------------------
Validation seg loss: 0.3831144790428708 at epoch 350
epoch =    351/  1000, exp = train
train_iter_loss: 0.33201396465301514
train_iter_loss: 0.16539062559604645
train_iter_loss: 0.29999449849128723
train_iter_loss: 0.3614899516105652
train_iter_loss: 0.19283288717269897
train_iter_loss: 0.2500689625740051
train_iter_loss: 0.27705302834510803
train_iter_loss: 0.2589189112186432
train_iter_loss: 0.22918733954429626
train_iter_loss: 0.12482143938541412
train_iter_loss: 0.2956227958202362
train_iter_loss: 0.2752794325351715
train_iter_loss: 0.36710840463638306
train_iter_loss: 0.32073351740837097
train_iter_loss: 0.44140568375587463
train_iter_loss: 0.2751348316669464
train_iter_loss: 0.26538535952568054
train_iter_loss: 0.38328784704208374
train_iter_loss: 0.288335382938385
train_iter_loss: 0.20240552723407745
train_iter_loss: 0.24877090752124786
train_iter_loss: 0.31883612275123596
train_iter_loss: 0.3501216769218445
train_iter_loss: 0.1582564115524292
train_iter_loss: 0.24299012124538422
train_iter_loss: 0.147288978099823
train_iter_loss: 0.3525528311729431
train_iter_loss: 0.24487094581127167
train_iter_loss: 0.36506494879722595
train_iter_loss: 0.2160976082086563
train_iter_loss: 0.1944272667169571
train_iter_loss: 0.15747886896133423
train_iter_loss: 0.32807043194770813
train_iter_loss: 0.13449320197105408
train_iter_loss: 0.317655086517334
train_iter_loss: 0.1642487496137619
train_iter_loss: 0.2434033453464508
train_iter_loss: 0.3436596095561981
train_iter_loss: 0.20593439042568207
train_iter_loss: 0.21605008840560913
train_iter_loss: 0.19938677549362183
train_iter_loss: 0.43240419030189514
train_iter_loss: 0.18151360750198364
train_iter_loss: 0.2251405119895935
train_iter_loss: 0.2383667677640915
train_iter_loss: 0.2618999779224396
train_iter_loss: 0.2824506461620331
train_iter_loss: 0.2127133160829544
train_iter_loss: 0.17147627472877502
train_iter_loss: 0.3384399712085724
train_iter_loss: 0.19096820056438446
train_iter_loss: 0.2801256477832794
train_iter_loss: 0.2533930540084839
train_iter_loss: 0.22364357113838196
train_iter_loss: 0.27096304297447205
train_iter_loss: 0.37189745903015137
train_iter_loss: 0.2692425549030304
train_iter_loss: 0.21876205503940582
train_iter_loss: 0.35696765780448914
train_iter_loss: 0.29824769496917725
train_iter_loss: 0.2836185395717621
train_iter_loss: 0.1382405161857605
train_iter_loss: 0.42062538862228394
train_iter_loss: 0.27162835001945496
train_iter_loss: 0.38083651661872864
train_iter_loss: 0.2149711698293686
train_iter_loss: 0.35139113664627075
train_iter_loss: 0.22103069722652435
train_iter_loss: 0.373285174369812
train_iter_loss: 0.5334084630012512
train_iter_loss: 0.3732403516769409
train_iter_loss: 0.1557690054178238
train_iter_loss: 0.41464000940322876
train_iter_loss: 0.24104923009872437
train_iter_loss: 0.19088543951511383
train_iter_loss: 0.2264454960823059
train_iter_loss: 0.05072540044784546
train_iter_loss: 0.3721814453601837
train_iter_loss: 0.32417166233062744
train_iter_loss: 0.2868918776512146
train_iter_loss: 0.24391667544841766
train_iter_loss: 0.32385462522506714
train_iter_loss: 0.22286902368068695
train_iter_loss: 0.5568938255310059
train_iter_loss: 0.34318843483924866
train_iter_loss: 0.25965920090675354
train_iter_loss: 0.29325172305107117
train_iter_loss: 0.29639536142349243
train_iter_loss: 0.32033002376556396
train_iter_loss: 0.2903234660625458
train_iter_loss: 0.3980925679206848
train_iter_loss: 0.2227296382188797
train_iter_loss: 0.28206557035446167
train_iter_loss: 0.23578386008739471
train_iter_loss: 0.24065043032169342
train_iter_loss: 0.2024921476840973
train_iter_loss: 0.2821327745914459
train_iter_loss: 0.2522427439689636
train_iter_loss: 0.19598883390426636
train_iter_loss: 0.26489633321762085
train loss :0.2767
---------------------
Validation seg loss: 0.3621303992486506 at epoch 351
epoch =    352/  1000, exp = train
train_iter_loss: 0.29647284746170044
train_iter_loss: 0.3292944133281708
train_iter_loss: 0.32864221930503845
train_iter_loss: 0.329862505197525
train_iter_loss: 0.3109506070613861
train_iter_loss: 0.214029923081398
train_iter_loss: 0.21086442470550537
train_iter_loss: 0.29167798161506653
train_iter_loss: 0.23541617393493652
train_iter_loss: 0.30645477771759033
train_iter_loss: 0.20470388233661652
train_iter_loss: 0.4254755973815918
train_iter_loss: 0.3347524106502533
train_iter_loss: 0.24847106635570526
train_iter_loss: 0.46947959065437317
train_iter_loss: 0.1428610235452652
train_iter_loss: 0.2963999807834625
train_iter_loss: 0.35857975482940674
train_iter_loss: 0.3175055980682373
train_iter_loss: 0.4472743570804596
train_iter_loss: 0.3394487798213959
train_iter_loss: 0.3336636424064636
train_iter_loss: 0.30059146881103516
train_iter_loss: 0.15823794901371002
train_iter_loss: 0.2625311315059662
train_iter_loss: 0.2061605453491211
train_iter_loss: 0.08269751816987991
train_iter_loss: 0.27072539925575256
train_iter_loss: 0.16338396072387695
train_iter_loss: 0.22674767673015594
train_iter_loss: 0.15181712806224823
train_iter_loss: 0.22189132869243622
train_iter_loss: 0.3586382567882538
train_iter_loss: 0.14928115904331207
train_iter_loss: 0.357709676027298
train_iter_loss: 0.2581254839897156
train_iter_loss: 0.20799550414085388
train_iter_loss: 0.3223874568939209
train_iter_loss: 0.21607747673988342
train_iter_loss: 0.18425992131233215
train_iter_loss: 0.38835060596466064
train_iter_loss: 0.4193934202194214
train_iter_loss: 0.311937153339386
train_iter_loss: 0.23188818991184235
train_iter_loss: 0.2251453399658203
train_iter_loss: 0.588415265083313
train_iter_loss: 0.28148338198661804
train_iter_loss: 0.19949467480182648
train_iter_loss: 0.19929316639900208
train_iter_loss: 0.1185838133096695
train_iter_loss: 0.1865846961736679
train_iter_loss: 0.15203529596328735
train_iter_loss: 0.2840108275413513
train_iter_loss: 0.33074676990509033
train_iter_loss: 0.17120181024074554
train_iter_loss: 0.356186181306839
train_iter_loss: 0.2092135101556778
train_iter_loss: 0.21115636825561523
train_iter_loss: 0.4222712516784668
train_iter_loss: 0.3080470860004425
train_iter_loss: 0.21347571909427643
train_iter_loss: 0.1476481854915619
train_iter_loss: 0.439668744802475
train_iter_loss: 0.21322016417980194
train_iter_loss: 0.3619166314601898
train_iter_loss: 0.24874264001846313
train_iter_loss: 0.21094222366809845
train_iter_loss: 0.37107399106025696
train_iter_loss: 0.2182074934244156
train_iter_loss: 0.0818297415971756
train_iter_loss: 0.19655869901180267
train_iter_loss: 0.24693819880485535
train_iter_loss: 0.3396039307117462
train_iter_loss: 0.37695011496543884
train_iter_loss: 0.2544543445110321
train_iter_loss: 0.38823139667510986
train_iter_loss: 0.24157150089740753
train_iter_loss: 0.17428365349769592
train_iter_loss: 0.3226321339607239
train_iter_loss: 0.42351940274238586
train_iter_loss: 0.15102653205394745
train_iter_loss: 0.23651763796806335
train_iter_loss: 0.26218584179878235
train_iter_loss: 0.2566848695278168
train_iter_loss: 0.1551489382982254
train_iter_loss: 0.3548428416252136
train_iter_loss: 0.41539114713668823
train_iter_loss: 0.3187359869480133
train_iter_loss: 0.5552740097045898
train_iter_loss: 0.3426860272884369
train_iter_loss: 0.1821509599685669
train_iter_loss: 0.3583984673023224
train_iter_loss: 0.10365565866231918
train_iter_loss: 0.19050011038780212
train_iter_loss: 0.16553503274917603
train_iter_loss: 0.3199920058250427
train_iter_loss: 0.3967845141887665
train_iter_loss: 0.36709287762641907
train_iter_loss: 0.35636666417121887
train_iter_loss: 0.2640484571456909
train loss :0.2797
---------------------
Validation seg loss: 0.3818638244608663 at epoch 352
epoch =    353/  1000, exp = train
train_iter_loss: 0.31400594115257263
train_iter_loss: 0.28344935178756714
train_iter_loss: 0.2923615574836731
train_iter_loss: 0.17010276019573212
train_iter_loss: 0.1531645953655243
train_iter_loss: 0.2622286379337311
train_iter_loss: 0.2946986258029938
train_iter_loss: 0.2747783660888672
train_iter_loss: 0.3840888738632202
train_iter_loss: 0.24367636442184448
train_iter_loss: 0.2092752456665039
train_iter_loss: 0.328228235244751
train_iter_loss: 0.1570756733417511
train_iter_loss: 0.2789560556411743
train_iter_loss: 0.2654086649417877
train_iter_loss: 0.240362286567688
train_iter_loss: 0.3247672915458679
train_iter_loss: 0.2718002498149872
train_iter_loss: 0.15285950899124146
train_iter_loss: 0.29403430223464966
train_iter_loss: 0.25110119581222534
train_iter_loss: 0.24984490871429443
train_iter_loss: 0.27097511291503906
train_iter_loss: 0.25551271438598633
train_iter_loss: 0.19453327357769012
train_iter_loss: 0.20067167282104492
train_iter_loss: 0.29827138781547546
train_iter_loss: 0.2071847766637802
train_iter_loss: 0.25904566049575806
train_iter_loss: 0.3128170073032379
train_iter_loss: 0.32404327392578125
train_iter_loss: 0.3184310793876648
train_iter_loss: 0.2326018363237381
train_iter_loss: 0.216074600815773
train_iter_loss: 0.1784304529428482
train_iter_loss: 0.2860979735851288
train_iter_loss: 0.2788737416267395
train_iter_loss: 0.290902704000473
train_iter_loss: 0.2426995038986206
train_iter_loss: 0.2629408836364746
train_iter_loss: 0.2315567135810852
train_iter_loss: 0.3304332196712494
train_iter_loss: 0.2422701120376587
train_iter_loss: 0.09361260384321213
train_iter_loss: 0.17033348977565765
train_iter_loss: 0.3641990125179291
train_iter_loss: 0.22827009856700897
train_iter_loss: 0.30596208572387695
train_iter_loss: 0.3173840641975403
train_iter_loss: 0.24391266703605652
train_iter_loss: 0.3781552314758301
train_iter_loss: 0.38488221168518066
train_iter_loss: 0.15277963876724243
train_iter_loss: 0.22852514684200287
train_iter_loss: 0.31727293133735657
train_iter_loss: 0.30868467688560486
train_iter_loss: 0.2867099940776825
train_iter_loss: 0.28337082266807556
train_iter_loss: 0.3196732699871063
train_iter_loss: 0.38802921772003174
train_iter_loss: 0.19491524994373322
train_iter_loss: 0.25775474309921265
train_iter_loss: 0.269436240196228
train_iter_loss: 0.43356940150260925
train_iter_loss: 0.3347199857234955
train_iter_loss: 0.3582657277584076
train_iter_loss: 0.34393858909606934
train_iter_loss: 0.2950093448162079
train_iter_loss: 0.44982168078422546
train_iter_loss: 0.4614385664463043
train_iter_loss: 0.30089378356933594
train_iter_loss: 0.2100229412317276
train_iter_loss: 0.1839820295572281
train_iter_loss: 0.25848376750946045
train_iter_loss: 0.32552334666252136
train_iter_loss: 0.2624856233596802
train_iter_loss: 0.25287583470344543
train_iter_loss: 0.34375184774398804
train_iter_loss: 0.37789851427078247
train_iter_loss: 0.16326819360256195
train_iter_loss: 0.14502359926700592
train_iter_loss: 0.37052100896835327
train_iter_loss: 0.3258635699748993
train_iter_loss: 0.27794837951660156
train_iter_loss: 0.29512619972229004
train_iter_loss: 0.3373728394508362
train_iter_loss: 0.15358099341392517
train_iter_loss: 0.4989822208881378
train_iter_loss: 0.17691898345947266
train_iter_loss: 0.06401949375867844
train_iter_loss: 0.3330528140068054
train_iter_loss: 0.2748454511165619
train_iter_loss: 0.2348363995552063
train_iter_loss: 0.17797741293907166
train_iter_loss: 0.34848684072494507
train_iter_loss: 0.1827867329120636
train_iter_loss: 0.2808226943016052
train_iter_loss: 0.28799229860305786
train_iter_loss: 0.3901411294937134
train_iter_loss: 0.2230856716632843
train loss :0.2764
---------------------
Validation seg loss: 0.39794793256716626 at epoch 353
epoch =    354/  1000, exp = train
train_iter_loss: 0.25940054655075073
train_iter_loss: 0.20546314120292664
train_iter_loss: 0.12629972398281097
train_iter_loss: 0.2922214865684509
train_iter_loss: 0.16328682005405426
train_iter_loss: 0.2394581288099289
train_iter_loss: 0.22983330488204956
train_iter_loss: 0.23415276408195496
train_iter_loss: 0.3572024703025818
train_iter_loss: 0.33451682329177856
train_iter_loss: 0.13526643812656403
train_iter_loss: 0.1770867109298706
train_iter_loss: 0.3277832567691803
train_iter_loss: 0.25346100330352783
train_iter_loss: 0.3077998161315918
train_iter_loss: 0.1604221761226654
train_iter_loss: 0.2538517415523529
train_iter_loss: 0.22246627509593964
train_iter_loss: 0.2347131371498108
train_iter_loss: 0.3567286431789398
train_iter_loss: 0.2965857684612274
train_iter_loss: 0.41415461897850037
train_iter_loss: 0.3206741213798523
train_iter_loss: 0.3791614770889282
train_iter_loss: 0.3236594796180725
train_iter_loss: 0.15614300966262817
train_iter_loss: 0.30730515718460083
train_iter_loss: 0.27753081917762756
train_iter_loss: 0.11058986186981201
train_iter_loss: 0.36176279187202454
train_iter_loss: 0.2758610248565674
train_iter_loss: 0.3022776246070862
train_iter_loss: 0.18146051466464996
train_iter_loss: 0.20035269856452942
train_iter_loss: 0.3206113576889038
train_iter_loss: 0.3880814015865326
train_iter_loss: 0.25167426466941833
train_iter_loss: 0.31458866596221924
train_iter_loss: 0.381197988986969
train_iter_loss: 0.33618804812431335
train_iter_loss: 0.29551365971565247
train_iter_loss: 0.24847067892551422
train_iter_loss: 0.08078502118587494
train_iter_loss: 0.15155549347400665
train_iter_loss: 0.27056682109832764
train_iter_loss: 0.20107650756835938
train_iter_loss: 0.29825252294540405
train_iter_loss: 0.27905404567718506
train_iter_loss: 0.2688516676425934
train_iter_loss: 0.14603985846042633
train_iter_loss: 0.3062393069267273
train_iter_loss: 0.22641368210315704
train_iter_loss: 0.22698336839675903
train_iter_loss: 0.4489011764526367
train_iter_loss: 0.4595109820365906
train_iter_loss: 0.22835378348827362
train_iter_loss: 0.20449526607990265
train_iter_loss: 0.31043991446495056
train_iter_loss: 0.23370389640331268
train_iter_loss: 0.3131437599658966
train_iter_loss: 0.4404488205909729
train_iter_loss: 0.28828656673431396
train_iter_loss: 0.16561609506607056
train_iter_loss: 0.11136606335639954
train_iter_loss: 0.22055800259113312
train_iter_loss: 0.341218501329422
train_iter_loss: 0.32907259464263916
train_iter_loss: 0.28520748019218445
train_iter_loss: 0.29317784309387207
train_iter_loss: 0.32717597484588623
train_iter_loss: 0.22211958467960358
train_iter_loss: 0.3411857783794403
train_iter_loss: 0.323534220457077
train_iter_loss: 0.21973185241222382
train_iter_loss: 0.2942216396331787
train_iter_loss: 0.31761807203292847
train_iter_loss: 0.3397999405860901
train_iter_loss: 0.2631744146347046
train_iter_loss: 0.2806497812271118
train_iter_loss: 0.2923447787761688
train_iter_loss: 0.1429213285446167
train_iter_loss: 0.354369580745697
train_iter_loss: 0.2949658930301666
train_iter_loss: 0.3389800786972046
train_iter_loss: 0.2494940310716629
train_iter_loss: 0.2152615189552307
train_iter_loss: 0.2009301483631134
train_iter_loss: 0.3506283760070801
train_iter_loss: 0.2892710268497467
train_iter_loss: 0.23355650901794434
train_iter_loss: 0.27817094326019287
train_iter_loss: 0.2578701674938202
train_iter_loss: 0.34332019090652466
train_iter_loss: 0.5367768406867981
train_iter_loss: 0.38039761781692505
train_iter_loss: 0.06770723313093185
train_iter_loss: 0.09291316568851471
train_iter_loss: 0.3320760130882263
train_iter_loss: 0.19932465255260468
train_iter_loss: 0.43745288252830505
train loss :0.2754
---------------------
Validation seg loss: 0.3586234374516556 at epoch 354
epoch =    355/  1000, exp = train
train_iter_loss: 0.24904155731201172
train_iter_loss: 0.1957981139421463
train_iter_loss: 0.3977549076080322
train_iter_loss: 0.3396296203136444
train_iter_loss: 0.2946810722351074
train_iter_loss: 0.37489429116249084
train_iter_loss: 0.3335101306438446
train_iter_loss: 0.3206246495246887
train_iter_loss: 0.3221666216850281
train_iter_loss: 0.18762189149856567
train_iter_loss: 0.1981373429298401
train_iter_loss: 0.2538301646709442
train_iter_loss: 0.15759442746639252
train_iter_loss: 0.21591976284980774
train_iter_loss: 0.07216569036245346
train_iter_loss: 0.2661387026309967
train_iter_loss: 0.2358691394329071
train_iter_loss: 0.33283478021621704
train_iter_loss: 0.2116091549396515
train_iter_loss: 0.3077462613582611
train_iter_loss: 0.1890362948179245
train_iter_loss: 0.20128551125526428
train_iter_loss: 0.24628150463104248
train_iter_loss: 0.3762791156768799
train_iter_loss: 0.3270845115184784
train_iter_loss: 0.15146426856517792
train_iter_loss: 0.23432427644729614
train_iter_loss: 0.28234878182411194
train_iter_loss: 0.13360802829265594
train_iter_loss: 0.25061967968940735
train_iter_loss: 0.2668565511703491
train_iter_loss: 0.32954245805740356
train_iter_loss: 0.2911054790019989
train_iter_loss: 0.2819889187812805
train_iter_loss: 0.28153806924819946
train_iter_loss: 0.37013277411460876
train_iter_loss: 0.1770174354314804
train_iter_loss: 0.1894775778055191
train_iter_loss: 0.28558990359306335
train_iter_loss: 0.30656224489212036
train_iter_loss: 0.17227104306221008
train_iter_loss: 0.37658795714378357
train_iter_loss: 0.2544976472854614
train_iter_loss: 0.17159533500671387
train_iter_loss: 0.26742368936538696
train_iter_loss: 0.4668154716491699
train_iter_loss: 0.30189767479896545
train_iter_loss: 0.2940573990345001
train_iter_loss: 0.17054420709609985
train_iter_loss: 0.38055649399757385
train_iter_loss: 0.3166750371456146
train_iter_loss: 0.21600665152072906
train_iter_loss: 0.2671760320663452
train_iter_loss: 0.28469061851501465
train_iter_loss: 0.14758053421974182
train_iter_loss: 0.2163248062133789
train_iter_loss: 0.5944229960441589
train_iter_loss: 0.34423622488975525
train_iter_loss: 0.1370025873184204
train_iter_loss: 0.34608596563339233
train_iter_loss: 0.24495692551136017
train_iter_loss: 0.1897902637720108
train_iter_loss: 0.31348201632499695
train_iter_loss: 0.44885000586509705
train_iter_loss: 0.27693161368370056
train_iter_loss: 0.495521605014801
train_iter_loss: 0.3103477954864502
train_iter_loss: 0.22232146561145782
train_iter_loss: 0.22334915399551392
train_iter_loss: 0.2356262058019638
train_iter_loss: 0.3515971899032593
train_iter_loss: 0.6428162455558777
train_iter_loss: 0.31043392419815063
train_iter_loss: 0.2908324599266052
train_iter_loss: 0.313155859708786
train_iter_loss: 0.3534172475337982
train_iter_loss: 0.32608065009117126
train_iter_loss: 0.44827356934547424
train_iter_loss: 0.27622827887535095
train_iter_loss: 0.17503990232944489
train_iter_loss: 0.23974822461605072
train_iter_loss: 0.3066840171813965
train_iter_loss: 0.19599252939224243
train_iter_loss: 0.1613141894340515
train_iter_loss: 0.24721406400203705
train_iter_loss: 0.26489874720573425
train_iter_loss: 0.16624687612056732
train_iter_loss: 0.3189716041088104
train_iter_loss: 0.1766006052494049
train_iter_loss: 0.13112477958202362
train_iter_loss: 0.21564078330993652
train_iter_loss: 0.37065771222114563
train_iter_loss: 0.2723653018474579
train_iter_loss: 0.22134031355381012
train_iter_loss: 0.09115228801965714
train_iter_loss: 0.327618271112442
train_iter_loss: 0.3863641917705536
train_iter_loss: 0.2807135283946991
train_iter_loss: 0.3817782998085022
train_iter_loss: 0.21157938241958618
train loss :0.2786
---------------------
Validation seg loss: 0.38356174494333145 at epoch 355
epoch =    356/  1000, exp = train
train_iter_loss: 0.15432214736938477
train_iter_loss: 0.2206021547317505
train_iter_loss: 0.19932366907596588
train_iter_loss: 0.25155550241470337
train_iter_loss: 0.16052792966365814
train_iter_loss: 0.2662605345249176
train_iter_loss: 0.26082515716552734
train_iter_loss: 0.2628619074821472
train_iter_loss: 0.18037816882133484
train_iter_loss: 0.2385924607515335
train_iter_loss: 0.3378167152404785
train_iter_loss: 0.21255561709403992
train_iter_loss: 0.3108801245689392
train_iter_loss: 0.2680610418319702
train_iter_loss: 0.21588532626628876
train_iter_loss: 0.3691376745700836
train_iter_loss: 0.3962191939353943
train_iter_loss: 0.2870122194290161
train_iter_loss: 0.21301884949207306
train_iter_loss: 0.27856579422950745
train_iter_loss: 0.22356963157653809
train_iter_loss: 0.3258024752140045
train_iter_loss: 0.3278333842754364
train_iter_loss: 0.29602348804473877
train_iter_loss: 0.22805917263031006
train_iter_loss: 0.1676926165819168
train_iter_loss: 0.18977390229701996
train_iter_loss: 0.3556648790836334
train_iter_loss: 0.30309972167015076
train_iter_loss: 0.14060525596141815
train_iter_loss: 0.26438653469085693
train_iter_loss: 0.22374840080738068
train_iter_loss: 0.437844455242157
train_iter_loss: 0.37216970324516296
train_iter_loss: 0.37787681818008423
train_iter_loss: 0.1390622854232788
train_iter_loss: 0.14641809463500977
train_iter_loss: 0.2975248098373413
train_iter_loss: 0.3508007526397705
train_iter_loss: 0.2753889560699463
train_iter_loss: 0.35628363490104675
train_iter_loss: 0.30232712626457214
train_iter_loss: 0.23352225124835968
train_iter_loss: 0.6985522508621216
train_iter_loss: 0.23470045626163483
train_iter_loss: 0.15594644844532013
train_iter_loss: 0.24822555482387543
train_iter_loss: 0.28785941004753113
train_iter_loss: 0.45200100541114807
train_iter_loss: 0.2727682590484619
train_iter_loss: 0.2583954334259033
train_iter_loss: 0.18825095891952515
train_iter_loss: 0.18287484347820282
train_iter_loss: 0.2421460896730423
train_iter_loss: 0.6219638586044312
train_iter_loss: 0.31665921211242676
train_iter_loss: 0.39905256032943726
train_iter_loss: 0.21692359447479248
train_iter_loss: 0.42852598428726196
train_iter_loss: 0.2505277395248413
train_iter_loss: 0.18149326741695404
train_iter_loss: 0.432487428188324
train_iter_loss: 0.22502705454826355
train_iter_loss: 0.30289193987846375
train_iter_loss: 0.34085795283317566
train_iter_loss: 0.20136964321136475
train_iter_loss: 0.1578865498304367
train_iter_loss: 0.3238518536090851
train_iter_loss: 0.2594084143638611
train_iter_loss: 0.281454861164093
train_iter_loss: 0.5659200549125671
train_iter_loss: 0.22126340866088867
train_iter_loss: 0.36730918288230896
train_iter_loss: 0.2500115633010864
train_iter_loss: 0.2287169098854065
train_iter_loss: 0.29770031571388245
train_iter_loss: 0.3204125165939331
train_iter_loss: 0.252562552690506
train_iter_loss: 0.2470063418149948
train_iter_loss: 0.12825120985507965
train_iter_loss: 0.2928936183452606
train_iter_loss: 0.3160443902015686
train_iter_loss: 0.517296552658081
train_iter_loss: 0.20025938749313354
train_iter_loss: 0.1989320069551468
train_iter_loss: 0.25177156925201416
train_iter_loss: 0.21646670997142792
train_iter_loss: 0.2738015353679657
train_iter_loss: 0.15399610996246338
train_iter_loss: 0.25778746604919434
train_iter_loss: 0.09913793206214905
train_iter_loss: 0.25018611550331116
train_iter_loss: 0.1916428953409195
train_iter_loss: 0.32381847500801086
train_iter_loss: 0.23788028955459595
train_iter_loss: 0.3138144314289093
train_iter_loss: 0.28232380747795105
train_iter_loss: 0.16884082555770874
train_iter_loss: 0.3357282876968384
train_iter_loss: 0.21016326546669006
train loss :0.2783
---------------------
Validation seg loss: 0.3612723292273311 at epoch 356
epoch =    357/  1000, exp = train
train_iter_loss: 0.18504710495471954
train_iter_loss: 0.24610716104507446
train_iter_loss: 0.34750306606292725
train_iter_loss: 0.665995180606842
train_iter_loss: 0.3195514678955078
train_iter_loss: 0.33908677101135254
train_iter_loss: 0.11823874711990356
train_iter_loss: 0.12187105417251587
train_iter_loss: 0.22950300574302673
train_iter_loss: 0.40340176224708557
train_iter_loss: 0.2687722146511078
train_iter_loss: 0.2359793335199356
train_iter_loss: 0.3581782579421997
train_iter_loss: 0.2591286897659302
train_iter_loss: 0.24103328585624695
train_iter_loss: 0.47433722019195557
train_iter_loss: 0.281093031167984
train_iter_loss: 0.2134915292263031
train_iter_loss: 0.25282350182533264
train_iter_loss: 0.3176650106906891
train_iter_loss: 0.28250202536582947
train_iter_loss: 0.29506757855415344
train_iter_loss: 0.3402635455131531
train_iter_loss: 0.34825780987739563
train_iter_loss: 0.43191301822662354
train_iter_loss: 0.20268464088439941
train_iter_loss: 0.2996162474155426
train_iter_loss: 0.29103124141693115
train_iter_loss: 0.22365932166576385
train_iter_loss: 0.2917289733886719
train_iter_loss: 0.24797816574573517
train_iter_loss: 0.15214644372463226
train_iter_loss: 0.3069740831851959
train_iter_loss: 0.308721125125885
train_iter_loss: 0.14204257726669312
train_iter_loss: 0.1977294683456421
train_iter_loss: 0.22202825546264648
train_iter_loss: 0.2069000005722046
train_iter_loss: 0.3756946325302124
train_iter_loss: 0.24567462503910065
train_iter_loss: 0.3467399775981903
train_iter_loss: 0.3277851343154907
train_iter_loss: 0.3021235764026642
train_iter_loss: 0.10211503505706787
train_iter_loss: 0.29734161496162415
train_iter_loss: 0.2063574194908142
train_iter_loss: 0.34592875838279724
train_iter_loss: 0.2778523862361908
train_iter_loss: 0.30119752883911133
train_iter_loss: 0.34792616963386536
train_iter_loss: 0.5204229950904846
train_iter_loss: 0.215683713555336
train_iter_loss: 0.18640077114105225
train_iter_loss: 0.2822025716304779
train_iter_loss: 0.25431743264198303
train_iter_loss: 0.31068864464759827
train_iter_loss: 0.24747446179389954
train_iter_loss: 0.4742809236049652
train_iter_loss: 0.5488253831863403
train_iter_loss: 0.29104480147361755
train_iter_loss: 0.2191861867904663
train_iter_loss: 0.24075724184513092
train_iter_loss: 0.3780541718006134
train_iter_loss: 0.17987357079982758
train_iter_loss: 0.2567766606807709
train_iter_loss: 0.16362129151821136
train_iter_loss: 0.2523898184299469
train_iter_loss: 0.4826308488845825
train_iter_loss: 0.2920650839805603
train_iter_loss: 0.1761859655380249
train_iter_loss: 0.1690339297056198
train_iter_loss: 0.15053386986255646
train_iter_loss: 0.12926197052001953
train_iter_loss: 0.4244750142097473
train_iter_loss: 0.2712424397468567
train_iter_loss: 0.30709460377693176
train_iter_loss: 0.309002161026001
train_iter_loss: 0.32194724678993225
train_iter_loss: 0.4905870258808136
train_iter_loss: 0.3757370114326477
train_iter_loss: 0.19369825720787048
train_iter_loss: 0.30078354477882385
train_iter_loss: 0.3565833270549774
train_iter_loss: 0.11949201673269272
train_iter_loss: 0.15488430857658386
train_iter_loss: 0.24225842952728271
train_iter_loss: 0.3856375217437744
train_iter_loss: 0.2769494950771332
train_iter_loss: 0.21835923194885254
train_iter_loss: 0.15356723964214325
train_iter_loss: 0.3489512503147125
train_iter_loss: 0.25875067710876465
train_iter_loss: 0.19598142802715302
train_iter_loss: 0.28151416778564453
train_iter_loss: 0.4301568567752838
train_iter_loss: 0.29050612449645996
train_iter_loss: 0.10864134132862091
train_iter_loss: 0.17259478569030762
train_iter_loss: 0.3924786150455475
train_iter_loss: 0.24084942042827606
train loss :0.2847
---------------------
Validation seg loss: 0.3603172065286001 at epoch 357
epoch =    358/  1000, exp = train
train_iter_loss: 0.2778012752532959
train_iter_loss: 0.3652496933937073
train_iter_loss: 0.4033030569553375
train_iter_loss: 0.4060429036617279
train_iter_loss: 0.21625693142414093
train_iter_loss: 0.206019327044487
train_iter_loss: 0.34322860836982727
train_iter_loss: 0.3463863730430603
train_iter_loss: 0.25877445936203003
train_iter_loss: 0.2743453085422516
train_iter_loss: 0.19165681302547455
train_iter_loss: 0.2996503710746765
train_iter_loss: 0.3254273235797882
train_iter_loss: 0.35833555459976196
train_iter_loss: 0.3155694305896759
train_iter_loss: 0.23996096849441528
train_iter_loss: 0.2641780972480774
train_iter_loss: 0.20932075381278992
train_iter_loss: 0.22922146320343018
train_iter_loss: 0.24519042670726776
train_iter_loss: 0.25973162055015564
train_iter_loss: 0.37080255150794983
train_iter_loss: 0.2542155981063843
train_iter_loss: 0.20789362490177155
train_iter_loss: 0.3539983928203583
train_iter_loss: 0.19769689440727234
train_iter_loss: 0.1090025082230568
train_iter_loss: 0.26284360885620117
train_iter_loss: 0.24250870943069458
train_iter_loss: 0.18750032782554626
train_iter_loss: 0.14812347292900085
train_iter_loss: 0.1526956409215927
train_iter_loss: 0.321369469165802
train_iter_loss: 0.2767742872238159
train_iter_loss: 0.4497673809528351
train_iter_loss: 0.20533260703086853
train_iter_loss: 0.31974145770072937
train_iter_loss: 0.3040606379508972
train_iter_loss: 0.3459215462207794
train_iter_loss: 0.17595309019088745
train_iter_loss: 0.17242582142353058
train_iter_loss: 0.3027731478214264
train_iter_loss: 0.27628979086875916
train_iter_loss: 0.21675457060337067
train_iter_loss: 0.3464878499507904
train_iter_loss: 0.17641250789165497
train_iter_loss: 0.11676519364118576
train_iter_loss: 0.30950668454170227
train_iter_loss: 0.270107239484787
train_iter_loss: 0.16182224452495575
train_iter_loss: 0.279553085565567
train_iter_loss: 0.2591579556465149
train_iter_loss: 0.22854472696781158
train_iter_loss: 0.2441122829914093
train_iter_loss: 0.3409825265407562
train_iter_loss: 0.2795087993144989
train_iter_loss: 0.3816233277320862
train_iter_loss: 0.21658125519752502
train_iter_loss: 0.3011215627193451
train_iter_loss: 0.22344131767749786
train_iter_loss: 0.2212134450674057
train_iter_loss: 0.23151761293411255
train_iter_loss: 0.3328346312046051
train_iter_loss: 0.3249722421169281
train_iter_loss: 0.2640869915485382
train_iter_loss: 0.31282660365104675
train_iter_loss: 0.3061071038246155
train_iter_loss: 0.32457879185676575
train_iter_loss: 0.3670685887336731
train_iter_loss: 0.2653752565383911
train_iter_loss: 0.30131611227989197
train_iter_loss: 0.31564608216285706
train_iter_loss: 0.37018465995788574
train_iter_loss: 0.1666296422481537
train_iter_loss: 0.21523824334144592
train_iter_loss: 0.3986319303512573
train_iter_loss: 0.2733880281448364
train_iter_loss: 0.46695005893707275
train_iter_loss: 0.34140732884407043
train_iter_loss: 0.2417060136795044
train_iter_loss: 0.3859609067440033
train_iter_loss: 0.30075183510780334
train_iter_loss: 0.32606980204582214
train_iter_loss: 0.237653911113739
train_iter_loss: 0.42234373092651367
train_iter_loss: 0.26644378900527954
train_iter_loss: 0.40558576583862305
train_iter_loss: 0.32939258217811584
train_iter_loss: 0.2546137869358063
train_iter_loss: 0.27525830268859863
train_iter_loss: 0.3197013735771179
train_iter_loss: 0.1456792801618576
train_iter_loss: 0.2694183886051178
train_iter_loss: 0.1963917464017868
train_iter_loss: 0.30912384390830994
train_iter_loss: 0.23255378007888794
train_iter_loss: 0.14747273921966553
train_iter_loss: 0.29457512497901917
train_iter_loss: 0.2380944788455963
train_iter_loss: 0.28080788254737854
train loss :0.2799
---------------------
Validation seg loss: 0.3676985428440121 at epoch 358
epoch =    359/  1000, exp = train
train_iter_loss: 0.17148032784461975
train_iter_loss: 0.22040820121765137
train_iter_loss: 0.2250528782606125
train_iter_loss: 0.18211762607097626
train_iter_loss: 0.30083978176116943
train_iter_loss: 0.28145983815193176
train_iter_loss: 0.2846638262271881
train_iter_loss: 0.2739240527153015
train_iter_loss: 0.22271758317947388
train_iter_loss: 0.20399180054664612
train_iter_loss: 0.3914045989513397
train_iter_loss: 0.3035978674888611
train_iter_loss: 0.31949207186698914
train_iter_loss: 0.3239409327507019
train_iter_loss: 0.14560763537883759
train_iter_loss: 0.2783418595790863
train_iter_loss: 0.2019338756799698
train_iter_loss: 0.2932767868041992
train_iter_loss: 0.17114676535129547
train_iter_loss: 0.29563772678375244
train_iter_loss: 0.35230234265327454
train_iter_loss: 0.24442777037620544
train_iter_loss: 0.2073095291852951
train_iter_loss: 0.4656866192817688
train_iter_loss: 0.34775716066360474
train_iter_loss: 0.14625906944274902
train_iter_loss: 0.32247981429100037
train_iter_loss: 0.320238322019577
train_iter_loss: 0.21232262253761292
train_iter_loss: 0.1940203756093979
train_iter_loss: 0.0925658792257309
train_iter_loss: 0.263550728559494
train_iter_loss: 0.2420426607131958
train_iter_loss: 0.4838504493236542
train_iter_loss: 0.21619483828544617
train_iter_loss: 0.506653368473053
train_iter_loss: 0.2185293436050415
train_iter_loss: 0.2851150631904602
train_iter_loss: 0.2204292267560959
train_iter_loss: 0.2831707298755646
train_iter_loss: 0.23510615527629852
train_iter_loss: 0.14865493774414062
train_iter_loss: 0.3650563061237335
train_iter_loss: 0.27243688702583313
train_iter_loss: 0.1533893644809723
train_iter_loss: 0.31219175457954407
train_iter_loss: 0.16405066847801208
train_iter_loss: 0.2594309449195862
train_iter_loss: 0.1983153074979782
train_iter_loss: 0.23735462129116058
train_iter_loss: 0.4375250041484833
train_iter_loss: 0.3364090025424957
train_iter_loss: 0.2807849943637848
train_iter_loss: 0.2222672700881958
train_iter_loss: 0.2942112684249878
train_iter_loss: 0.22659462690353394
train_iter_loss: 0.12088919430971146
train_iter_loss: 0.13277913630008698
train_iter_loss: 0.23679465055465698
train_iter_loss: 0.44702041149139404
train_iter_loss: 0.32741808891296387
train_iter_loss: 0.2809333801269531
train_iter_loss: 0.2743498980998993
train_iter_loss: 0.384606271982193
train_iter_loss: 0.1961410641670227
train_iter_loss: 0.17047013342380524
train_iter_loss: 0.3429470658302307
train_iter_loss: 0.16194196045398712
train_iter_loss: 0.2676968276500702
train_iter_loss: 0.26783737540245056
train_iter_loss: 0.34654849767684937
train_iter_loss: 0.2262551486492157
train_iter_loss: 0.2746155261993408
train_iter_loss: 0.5547065734863281
train_iter_loss: 0.34201371669769287
train_iter_loss: 0.14695923030376434
train_iter_loss: 0.4291748106479645
train_iter_loss: 0.4564502537250519
train_iter_loss: 0.2584976255893707
train_iter_loss: 0.19947056472301483
train_iter_loss: 0.25111323595046997
train_iter_loss: 0.2827616035938263
train_iter_loss: 0.3251582384109497
train_iter_loss: 0.22318187355995178
train_iter_loss: 0.2134523242712021
train_iter_loss: 0.2562769651412964
train_iter_loss: 0.3448938727378845
train_iter_loss: 0.36817654967308044
train_iter_loss: 0.24393586814403534
train_iter_loss: 0.4254905581474304
train_iter_loss: 0.18849168717861176
train_iter_loss: 0.22957752645015717
train_iter_loss: 0.4068358838558197
train_iter_loss: 0.33535894751548767
train_iter_loss: 0.34014973044395447
train_iter_loss: 0.4212418794631958
train_iter_loss: 0.33797982335090637
train_iter_loss: 0.32834166288375854
train_iter_loss: 0.4614751636981964
train_iter_loss: 0.1781170815229416
train loss :0.2815
---------------------
Validation seg loss: 0.3509335693253099 at epoch 359
********************
best_val_epoch_loss:  0.3509335693253099
MODEL UPDATED
epoch =    360/  1000, exp = train
train_iter_loss: 0.309216171503067
train_iter_loss: 0.1959456503391266
train_iter_loss: 0.3860890567302704
train_iter_loss: 0.24013063311576843
train_iter_loss: 0.1672510802745819
train_iter_loss: 0.2379738986492157
train_iter_loss: 0.1893945038318634
train_iter_loss: 0.32066136598587036
train_iter_loss: 0.2766980230808258
train_iter_loss: 0.31077051162719727
train_iter_loss: 0.4208490550518036
train_iter_loss: 0.2530938684940338
train_iter_loss: 0.27423280477523804
train_iter_loss: 0.21917259693145752
train_iter_loss: 0.33622658252716064
train_iter_loss: 0.347832053899765
train_iter_loss: 0.37715208530426025
train_iter_loss: 0.25950974225997925
train_iter_loss: 0.252763956785202
train_iter_loss: 0.3362363874912262
train_iter_loss: 0.3621922731399536
train_iter_loss: 0.18551874160766602
train_iter_loss: 0.28469762206077576
train_iter_loss: 0.2801954448223114
train_iter_loss: 0.26121196150779724
train_iter_loss: 0.26259616017341614
train_iter_loss: 0.32738256454467773
train_iter_loss: 0.3750128448009491
train_iter_loss: 0.35604575276374817
train_iter_loss: 0.31928810477256775
train_iter_loss: 0.41734716296195984
train_iter_loss: 0.39148515462875366
train_iter_loss: 0.15263840556144714
train_iter_loss: 0.26380395889282227
train_iter_loss: 0.2571493983268738
train_iter_loss: 0.20729157328605652
train_iter_loss: 0.18220946192741394
train_iter_loss: 0.2650603652000427
train_iter_loss: 0.30726492404937744
train_iter_loss: 0.11328770965337753
train_iter_loss: 0.25456660985946655
train_iter_loss: 0.2544773817062378
train_iter_loss: 0.28030556440353394
train_iter_loss: 0.309495747089386
train_iter_loss: 0.30797797441482544
train_iter_loss: 0.3562726378440857
train_iter_loss: 0.45425906777381897
train_iter_loss: 0.18593557178974152
train_iter_loss: 0.17493782937526703
train_iter_loss: 0.25718143582344055
train_iter_loss: 0.22305814921855927
train_iter_loss: 0.2433987259864807
train_iter_loss: 0.3740941286087036
train_iter_loss: 0.3507535755634308
train_iter_loss: 0.22506625950336456
train_iter_loss: 0.23550178110599518
train_iter_loss: 0.4075770974159241
train_iter_loss: 0.24710391461849213
train_iter_loss: 0.2792995572090149
train_iter_loss: 0.22301694750785828
train_iter_loss: 0.4195733368396759
train_iter_loss: 0.21393191814422607
train_iter_loss: 0.28455695509910583
train_iter_loss: 0.25228458642959595
train_iter_loss: 0.24239176511764526
train_iter_loss: 0.19214758276939392
train_iter_loss: 0.28812769055366516
train_iter_loss: 0.11283890157938004
train_iter_loss: 0.2703646421432495
train_iter_loss: 0.21772870421409607
train_iter_loss: 0.2762475311756134
train_iter_loss: 0.378166526556015
train_iter_loss: 0.18386617302894592
train_iter_loss: 0.23312528431415558
train_iter_loss: 0.29869532585144043
train_iter_loss: 0.273070752620697
train_iter_loss: 0.23711338639259338
train_iter_loss: 0.26510828733444214
train_iter_loss: 0.1666979342699051
train_iter_loss: 0.38242343068122864
train_iter_loss: 0.30882948637008667
train_iter_loss: 0.27807289361953735
train_iter_loss: 0.2804166376590729
train_iter_loss: 0.2927451431751251
train_iter_loss: 0.2226644605398178
train_iter_loss: 0.20827758312225342
train_iter_loss: 0.2773135304450989
train_iter_loss: 0.21547043323516846
train_iter_loss: 0.2823077142238617
train_iter_loss: 0.46771836280822754
train_iter_loss: 0.3506412208080292
train_iter_loss: 0.33224090933799744
train_iter_loss: 0.3021671772003174
train_iter_loss: 0.35659465193748474
train_iter_loss: 0.21984019875526428
train_iter_loss: 0.2436271607875824
train_iter_loss: 0.2300986796617508
train_iter_loss: 0.2799268662929535
train_iter_loss: 0.23232759535312653
train_iter_loss: 0.41966307163238525
train loss :0.2820
---------------------
Validation seg loss: 0.36633355862830047 at epoch 360
epoch =    361/  1000, exp = train
train_iter_loss: 0.2917839586734772
train_iter_loss: 0.2775944471359253
train_iter_loss: 0.40958356857299805
train_iter_loss: 0.1993502825498581
train_iter_loss: 0.37413254380226135
train_iter_loss: 0.3301433324813843
train_iter_loss: 0.28261107206344604
train_iter_loss: 0.44785192608833313
train_iter_loss: 0.250285267829895
train_iter_loss: 0.37504294514656067
train_iter_loss: 0.25628912448883057
train_iter_loss: 0.35817062854766846
train_iter_loss: 0.282997727394104
train_iter_loss: 0.23313970863819122
train_iter_loss: 0.3438866138458252
train_iter_loss: 0.2835155427455902
train_iter_loss: 0.27627813816070557
train_iter_loss: 0.2458038032054901
train_iter_loss: 0.16391830146312714
train_iter_loss: 0.20639245212078094
train_iter_loss: 0.19774749875068665
train_iter_loss: 0.29922306537628174
train_iter_loss: 0.13917827606201172
train_iter_loss: 0.21493203938007355
train_iter_loss: 0.41670870780944824
train_iter_loss: 0.07790624350309372
train_iter_loss: 0.1587585061788559
train_iter_loss: 0.3957957327365875
train_iter_loss: 0.3289397656917572
train_iter_loss: 0.415990948677063
train_iter_loss: 0.25588536262512207
train_iter_loss: 0.37282153964042664
train_iter_loss: 0.27953219413757324
train_iter_loss: 0.2569381594657898
train_iter_loss: 0.2661226689815521
train_iter_loss: 0.3369458317756653
train_iter_loss: 0.3660174310207367
train_iter_loss: 0.2905808985233307
train_iter_loss: 0.44650521874427795
train_iter_loss: 0.2731115520000458
train_iter_loss: 0.29304057359695435
train_iter_loss: 0.3151148855686188
train_iter_loss: 0.20297783613204956
train_iter_loss: 0.16944436728954315
train_iter_loss: 0.2560550272464752
train_iter_loss: 0.2513827979564667
train_iter_loss: 0.2445516288280487
train_iter_loss: 0.20463921129703522
train_iter_loss: 0.312404602766037
train_iter_loss: 0.31013044714927673
train_iter_loss: 0.2370041161775589
train_iter_loss: 0.3243821859359741
train_iter_loss: 0.25370457768440247
train_iter_loss: 0.3460860252380371
train_iter_loss: 0.34860506653785706
train_iter_loss: 0.2517825961112976
train_iter_loss: 0.18246783316135406
train_iter_loss: 0.3372894525527954
train_iter_loss: 0.33871734142303467
train_iter_loss: 0.37722334265708923
train_iter_loss: 0.27408161759376526
train_iter_loss: 0.22074298560619354
train_iter_loss: 0.27394962310791016
train_iter_loss: 0.20337378978729248
train_iter_loss: 0.35816317796707153
train_iter_loss: 0.22620439529418945
train_iter_loss: 0.5190226435661316
train_iter_loss: 0.2114666849374771
train_iter_loss: 0.3456135094165802
train_iter_loss: 0.3404091000556946
train_iter_loss: 0.1543300300836563
train_iter_loss: 0.14521196484565735
train_iter_loss: 0.14834417402744293
train_iter_loss: 0.12850023806095123
train_iter_loss: 0.1572423130273819
train_iter_loss: 0.29440203309059143
train_iter_loss: 0.5237722992897034
train_iter_loss: 0.1642037183046341
train_iter_loss: 0.26447951793670654
train_iter_loss: 0.332981675863266
train_iter_loss: 0.22235433757305145
train_iter_loss: 0.21541641652584076
train_iter_loss: 0.3973464369773865
train_iter_loss: 0.2391425371170044
train_iter_loss: 0.34891191124916077
train_iter_loss: 0.19483087956905365
train_iter_loss: 0.14878001809120178
train_iter_loss: 0.1781059354543686
train_iter_loss: 0.19497954845428467
train_iter_loss: 0.19201812148094177
train_iter_loss: 0.310744971036911
train_iter_loss: 0.2298867553472519
train_iter_loss: 0.3572322428226471
train_iter_loss: 0.3003075122833252
train_iter_loss: 0.2316104769706726
train_iter_loss: 0.28401872515678406
train_iter_loss: 0.3825129270553589
train_iter_loss: 0.11839186400175095
train_iter_loss: 0.23708805441856384
train_iter_loss: 0.26842617988586426
train loss :0.2786
---------------------
Validation seg loss: 0.3728669384529568 at epoch 361
epoch =    362/  1000, exp = train
train_iter_loss: 0.18186092376708984
train_iter_loss: 0.36981385946273804
train_iter_loss: 0.14466124773025513
train_iter_loss: 0.2244883030653
train_iter_loss: 0.4343549311161041
train_iter_loss: 0.2152353674173355
train_iter_loss: 0.30334100127220154
train_iter_loss: 0.17690961062908173
train_iter_loss: 0.1957554966211319
train_iter_loss: 0.40187498927116394
train_iter_loss: 0.15287244319915771
train_iter_loss: 0.28321346640586853
train_iter_loss: 0.15896205604076385
train_iter_loss: 0.22261551022529602
train_iter_loss: 0.2601712942123413
train_iter_loss: 0.23462139070034027
train_iter_loss: 0.3565199673175812
train_iter_loss: 0.26078975200653076
train_iter_loss: 0.28585323691368103
train_iter_loss: 0.3355608880519867
train_iter_loss: 0.2433304339647293
train_iter_loss: 0.39493292570114136
train_iter_loss: 0.29975757002830505
train_iter_loss: 0.2963784337043762
train_iter_loss: 0.18819867074489594
train_iter_loss: 0.34525641798973083
train_iter_loss: 0.29394975304603577
train_iter_loss: 0.2948317229747772
train_iter_loss: 0.3327791094779968
train_iter_loss: 0.30053409934043884
train_iter_loss: 0.4085753262042999
train_iter_loss: 0.16750243306159973
train_iter_loss: 0.5722494125366211
train_iter_loss: 0.2673228979110718
train_iter_loss: 0.12623625993728638
train_iter_loss: 0.34449368715286255
train_iter_loss: 0.25425487756729126
train_iter_loss: 0.2557690143585205
train_iter_loss: 0.2625304162502289
train_iter_loss: 0.1980135291814804
train_iter_loss: 0.20227926969528198
train_iter_loss: 0.09414701908826828
train_iter_loss: 0.24133595824241638
train_iter_loss: 0.2909323573112488
train_iter_loss: 0.2945597171783447
train_iter_loss: 0.28391915559768677
train_iter_loss: 0.27930131554603577
train_iter_loss: 0.16837625205516815
train_iter_loss: 0.12137238681316376
train_iter_loss: 0.22913986444473267
train_iter_loss: 0.2265981286764145
train_iter_loss: 0.2301207035779953
train_iter_loss: 0.29988163709640503
train_iter_loss: 0.3159390985965729
train_iter_loss: 0.32019275426864624
train_iter_loss: 0.3683847188949585
train_iter_loss: 0.3651094436645508
train_iter_loss: 0.3837435245513916
train_iter_loss: 0.3221248388290405
train_iter_loss: 0.34588178992271423
train_iter_loss: 0.16655874252319336
train_iter_loss: 0.19255810976028442
train_iter_loss: 0.6820675730705261
train_iter_loss: 0.3092339336872101
train_iter_loss: 0.17919206619262695
train_iter_loss: 0.230977401137352
train_iter_loss: 0.20742426812648773
train_iter_loss: 0.30020588636398315
train_iter_loss: 0.1783103346824646
train_iter_loss: 0.2620735466480255
train_iter_loss: 0.17318026721477509
train_iter_loss: 0.19549617171287537
train_iter_loss: 0.2626335024833679
train_iter_loss: 0.3796338438987732
train_iter_loss: 0.13015557825565338
train_iter_loss: 0.1995169073343277
train_iter_loss: 0.3960375189781189
train_iter_loss: 0.2651616036891937
train_iter_loss: 0.35670116543769836
train_iter_loss: 0.2351669818162918
train_iter_loss: 0.17842648923397064
train_iter_loss: 0.21038304269313812
train_iter_loss: 0.22251740097999573
train_iter_loss: 0.2692992389202118
train_iter_loss: 0.3889852464199066
train_iter_loss: 0.24419941008090973
train_iter_loss: 0.26845258474349976
train_iter_loss: 0.2869129776954651
train_iter_loss: 0.230044886469841
train_iter_loss: 0.33576422929763794
train_iter_loss: 0.5203373432159424
train_iter_loss: 0.2607893943786621
train_iter_loss: 0.17030778527259827
train_iter_loss: 0.24660827219486237
train_iter_loss: 0.23425185680389404
train_iter_loss: 0.31158193945884705
train_iter_loss: 0.182718425989151
train_iter_loss: 0.30697736144065857
train_iter_loss: 0.3464645445346832
train_iter_loss: 0.2724049687385559
train loss :0.2750
---------------------
Validation seg loss: 0.36268132308252016 at epoch 362
epoch =    363/  1000, exp = train
train_iter_loss: 0.226568341255188
train_iter_loss: 0.552010178565979
train_iter_loss: 0.19311867654323578
train_iter_loss: 0.27719515562057495
train_iter_loss: 0.23583287000656128
train_iter_loss: 0.418006032705307
train_iter_loss: 0.2772279381752014
train_iter_loss: 0.33731114864349365
train_iter_loss: 0.2749614417552948
train_iter_loss: 0.47799941897392273
train_iter_loss: 0.33849141001701355
train_iter_loss: 0.2013816386461258
train_iter_loss: 0.28984519839286804
train_iter_loss: 0.2927435338497162
train_iter_loss: 0.4089244306087494
train_iter_loss: 0.28420862555503845
train_iter_loss: 0.21945737302303314
train_iter_loss: 0.29635921120643616
train_iter_loss: 0.23951049149036407
train_iter_loss: 0.2536275386810303
train_iter_loss: 0.08254945278167725
train_iter_loss: 0.2386786937713623
train_iter_loss: 0.24381141364574432
train_iter_loss: 0.2089790552854538
train_iter_loss: 0.23336902260780334
train_iter_loss: 0.40057143568992615
train_iter_loss: 0.2978746294975281
train_iter_loss: 0.28249892592430115
train_iter_loss: 0.22480498254299164
train_iter_loss: 0.0785408765077591
train_iter_loss: 0.2502320408821106
train_iter_loss: 0.3557724952697754
train_iter_loss: 0.4445657432079315
train_iter_loss: 0.28190529346466064
train_iter_loss: 0.17910334467887878
train_iter_loss: 0.17969724535942078
train_iter_loss: 0.35945382714271545
train_iter_loss: 0.2704624831676483
train_iter_loss: 0.26692232489585876
train_iter_loss: 0.39523693919181824
train_iter_loss: 0.32033154368400574
train_iter_loss: 0.27628445625305176
train_iter_loss: 0.23532719910144806
train_iter_loss: 0.26160094141960144
train_iter_loss: 0.2145785540342331
train_iter_loss: 0.2561803162097931
train_iter_loss: 0.26663827896118164
train_iter_loss: 0.271714448928833
train_iter_loss: 0.24750269949436188
train_iter_loss: 0.26959508657455444
train_iter_loss: 0.33945608139038086
train_iter_loss: 0.4462563097476959
train_iter_loss: 0.4060637950897217
train_iter_loss: 0.33934399485588074
train_iter_loss: 0.321498304605484
train_iter_loss: 0.30737924575805664
train_iter_loss: 0.3963809907436371
train_iter_loss: 0.3034370541572571
train_iter_loss: 0.3367694616317749
train_iter_loss: 0.38800927996635437
train_iter_loss: 0.20565243065357208
train_iter_loss: 0.3560829162597656
train_iter_loss: 0.15130364894866943
train_iter_loss: 0.22270622849464417
train_iter_loss: 0.2518635094165802
train_iter_loss: 0.19111408293247223
train_iter_loss: 0.3393462002277374
train_iter_loss: 0.27181559801101685
train_iter_loss: 0.20606443285942078
train_iter_loss: 0.34052804112434387
train_iter_loss: 0.18510976433753967
train_iter_loss: 0.3313397467136383
train_iter_loss: 0.2817540764808655
train_iter_loss: 0.2066856026649475
train_iter_loss: 0.3131013810634613
train_iter_loss: 0.23889277875423431
train_iter_loss: 0.37779295444488525
train_iter_loss: 0.203353613615036
train_iter_loss: 0.06873564422130585
train_iter_loss: 0.20888444781303406
train_iter_loss: 0.18306389451026917
train_iter_loss: 0.3768502175807953
train_iter_loss: 0.14784343540668488
train_iter_loss: 0.24484199285507202
train_iter_loss: 0.3914127051830292
train_iter_loss: 0.21511174738407135
train_iter_loss: 0.2637483477592468
train_iter_loss: 0.16403204202651978
train_iter_loss: 0.26203659176826477
train_iter_loss: 0.33635732531547546
train_iter_loss: 0.2280411422252655
train_iter_loss: 0.16537344455718994
train_iter_loss: 0.22531072795391083
train_iter_loss: 0.16119985282421112
train_iter_loss: 0.3686891794204712
train_iter_loss: 0.2844896912574768
train_iter_loss: 0.26973745226860046
train_iter_loss: 0.29077669978141785
train_iter_loss: 0.3834320306777954
train_iter_loss: 0.22279410064220428
train loss :0.2799
---------------------
Validation seg loss: 0.37874914132023474 at epoch 363
epoch =    364/  1000, exp = train
train_iter_loss: 0.2438381314277649
train_iter_loss: 0.2588355541229248
train_iter_loss: 0.20952072739601135
train_iter_loss: 0.20235766470432281
train_iter_loss: 0.34346720576286316
train_iter_loss: 0.32736459374427795
train_iter_loss: 0.2113574892282486
train_iter_loss: 0.3998693525791168
train_iter_loss: 0.13841065764427185
train_iter_loss: 0.25970014929771423
train_iter_loss: 0.1763647347688675
train_iter_loss: 0.2778959572315216
train_iter_loss: 0.2896859049797058
train_iter_loss: 0.34144142270088196
train_iter_loss: 0.19727492332458496
train_iter_loss: 0.4621133804321289
train_iter_loss: 0.3401023745536804
train_iter_loss: 0.3884139955043793
train_iter_loss: 0.41205716133117676
train_iter_loss: 0.18700838088989258
train_iter_loss: 0.22913964092731476
train_iter_loss: 0.1376730352640152
train_iter_loss: 0.3146551847457886
train_iter_loss: 0.20100805163383484
train_iter_loss: 0.3046827018260956
train_iter_loss: 0.8099337220191956
train_iter_loss: 0.33878883719444275
train_iter_loss: 0.15443292260169983
train_iter_loss: 0.17776452004909515
train_iter_loss: 0.3678742051124573
train_iter_loss: 0.42439815402030945
train_iter_loss: 0.4561963379383087
train_iter_loss: 0.25369131565093994
train_iter_loss: 0.12442925572395325
train_iter_loss: 0.24859881401062012
train_iter_loss: 0.3006284832954407
train_iter_loss: 0.20071083307266235
train_iter_loss: 0.29918748140335083
train_iter_loss: 0.3286919593811035
train_iter_loss: 0.4362524747848511
train_iter_loss: 0.26750680804252625
train_iter_loss: 0.28365883231163025
train_iter_loss: 0.358530193567276
train_iter_loss: 0.3207158148288727
train_iter_loss: 0.38989022374153137
train_iter_loss: 0.09620966017246246
train_iter_loss: 0.26667842268943787
train_iter_loss: 0.277179092168808
train_iter_loss: 0.17838183045387268
train_iter_loss: 0.1962328851222992
train_iter_loss: 0.3701147437095642
train_iter_loss: 0.2217021882534027
train_iter_loss: 0.14904792606830597
train_iter_loss: 0.17944160103797913
train_iter_loss: 0.27070167660713196
train_iter_loss: 0.26166611909866333
train_iter_loss: 0.33349016308784485
train_iter_loss: 0.389343798160553
train_iter_loss: 0.4889127314090729
train_iter_loss: 0.3073122203350067
train_iter_loss: 0.2833396792411804
train_iter_loss: 0.2743496894836426
train_iter_loss: 0.402531236410141
train_iter_loss: 0.2389727234840393
train_iter_loss: 0.18332251906394958
train_iter_loss: 0.13415047526359558
train_iter_loss: 0.21919818222522736
train_iter_loss: 0.32902634143829346
train_iter_loss: 0.12512552738189697
train_iter_loss: 0.23649035394191742
train_iter_loss: 0.1518186777830124
train_iter_loss: 0.20246359705924988
train_iter_loss: 0.1865655481815338
train_iter_loss: 0.2548089623451233
train_iter_loss: 0.10824845731258392
train_iter_loss: 0.3167918026447296
train_iter_loss: 0.246547669172287
train_iter_loss: 0.40206900238990784
train_iter_loss: 0.3460817337036133
train_iter_loss: 0.1747797131538391
train_iter_loss: 0.4665113389492035
train_iter_loss: 0.3770562708377838
train_iter_loss: 0.3639446496963501
train_iter_loss: 0.3082704544067383
train_iter_loss: 0.28882506489753723
train_iter_loss: 0.41495218873023987
train_iter_loss: 0.17257022857666016
train_iter_loss: 0.2800058424472809
train_iter_loss: 0.44006964564323425
train_iter_loss: 0.17980331182479858
train_iter_loss: 0.1174614205956459
train_iter_loss: 0.17334304749965668
train_iter_loss: 0.21912313997745514
train_iter_loss: 0.2185712605714798
train_iter_loss: 0.21386104822158813
train_iter_loss: 0.3268181085586548
train_iter_loss: 0.2898474931716919
train_iter_loss: 0.2377496212720871
train_iter_loss: 0.21569181978702545
train_iter_loss: 0.10938836634159088
train loss :0.2789
---------------------
Validation seg loss: 0.36313847945300193 at epoch 364
epoch =    365/  1000, exp = train
train_iter_loss: 0.37230756878852844
train_iter_loss: 0.18180043995380402
train_iter_loss: 0.1604301482439041
train_iter_loss: 0.19465996325016022
train_iter_loss: 0.4378517270088196
train_iter_loss: 0.4165368974208832
train_iter_loss: 0.32275640964508057
train_iter_loss: 0.1982642114162445
train_iter_loss: 0.34047478437423706
train_iter_loss: 0.09633776545524597
train_iter_loss: 0.18345898389816284
train_iter_loss: 0.42196983098983765
train_iter_loss: 0.3252846896648407
train_iter_loss: 0.3960537910461426
train_iter_loss: 0.3828241527080536
train_iter_loss: 0.2780090570449829
train_iter_loss: 0.18978051841259003
train_iter_loss: 0.23287902772426605
train_iter_loss: 0.46028798818588257
train_iter_loss: 0.25595417618751526
train_iter_loss: 0.19123178720474243
train_iter_loss: 0.2743723392486572
train_iter_loss: 0.1138596162199974
train_iter_loss: 0.27375727891921997
train_iter_loss: 0.2446250170469284
train_iter_loss: 0.29016944766044617
train_iter_loss: 0.20002485811710358
train_iter_loss: 0.23027373850345612
train_iter_loss: 0.2007148712873459
train_iter_loss: 0.30932560563087463
train_iter_loss: 0.2872655689716339
train_iter_loss: 0.3198469579219818
train_iter_loss: 0.3641665279865265
train_iter_loss: 0.31779152154922485
train_iter_loss: 0.10552488267421722
train_iter_loss: 0.347565621137619
train_iter_loss: 0.23527191579341888
train_iter_loss: 0.25721362233161926
train_iter_loss: 0.46161696314811707
train_iter_loss: 0.3056843876838684
train_iter_loss: 0.2517489194869995
train_iter_loss: 0.263621062040329
train_iter_loss: 0.1032361090183258
train_iter_loss: 0.24108505249023438
train_iter_loss: 0.2269616723060608
train_iter_loss: 0.33830857276916504
train_iter_loss: 0.2939542531967163
train_iter_loss: 0.1626548171043396
train_iter_loss: 0.1636517196893692
train_iter_loss: 0.3315304219722748
train_iter_loss: 0.2609134614467621
train_iter_loss: 0.21636077761650085
train_iter_loss: 0.16789135336875916
train_iter_loss: 0.28101468086242676
train_iter_loss: 0.18901784718036652
train_iter_loss: 0.2516886591911316
train_iter_loss: 0.1791897416114807
train_iter_loss: 0.4510965943336487
train_iter_loss: 0.20324364304542542
train_iter_loss: 0.2962578237056732
train_iter_loss: 0.2408885955810547
train_iter_loss: 0.3025503158569336
train_iter_loss: 0.27870652079582214
train_iter_loss: 0.1920579969882965
train_iter_loss: 0.21490025520324707
train_iter_loss: 0.3303682506084442
train_iter_loss: 0.23122821748256683
train_iter_loss: 0.28753241896629333
train_iter_loss: 0.461921364068985
train_iter_loss: 0.2491290420293808
train_iter_loss: 0.3255220353603363
train_iter_loss: 0.3209068179130554
train_iter_loss: 0.3691430985927582
train_iter_loss: 0.2584249675273895
train_iter_loss: 0.29440513253211975
train_iter_loss: 0.23619964718818665
train_iter_loss: 0.27897417545318604
train_iter_loss: 0.26451802253723145
train_iter_loss: 0.14233361184597015
train_iter_loss: 0.3560559153556824
train_iter_loss: 0.24765963852405548
train_iter_loss: 0.16093710064888
train_iter_loss: 0.24776099622249603
train_iter_loss: 0.27944111824035645
train_iter_loss: 0.14781302213668823
train_iter_loss: 0.40615421533584595
train_iter_loss: 0.26109984517097473
train_iter_loss: 0.3391945958137512
train_iter_loss: 0.1648710072040558
train_iter_loss: 0.34857407212257385
train_iter_loss: 0.31925272941589355
train_iter_loss: 0.28750091791152954
train_iter_loss: 0.5270892977714539
train_iter_loss: 0.22936828434467316
train_iter_loss: 0.24792172014713287
train_iter_loss: 0.34846970438957214
train_iter_loss: 0.1267096847295761
train_iter_loss: 0.19576020538806915
train_iter_loss: 0.23559537529945374
train_iter_loss: 0.3016902208328247
train loss :0.2739
---------------------
Validation seg loss: 0.35448910874964773 at epoch 365
epoch =    366/  1000, exp = train
train_iter_loss: 0.20018313825130463
train_iter_loss: 0.24218028783798218
train_iter_loss: 0.30510413646698
train_iter_loss: 0.3282266855239868
train_iter_loss: 0.34441930055618286
train_iter_loss: 0.3320981562137604
train_iter_loss: 0.3953706920146942
train_iter_loss: 0.16149106621742249
train_iter_loss: 0.17558832466602325
train_iter_loss: 0.6164897680282593
train_iter_loss: 0.24665017426013947
train_iter_loss: 0.11384321004152298
train_iter_loss: 0.3332795798778534
train_iter_loss: 0.3009461462497711
train_iter_loss: 0.30817773938179016
train_iter_loss: 0.4625409245491028
train_iter_loss: 0.23549146950244904
train_iter_loss: 0.333233505487442
train_iter_loss: 0.09646061062812805
train_iter_loss: 0.3957638144493103
train_iter_loss: 0.29431894421577454
train_iter_loss: 0.2672402262687683
train_iter_loss: 0.23420386016368866
train_iter_loss: 0.4695661962032318
train_iter_loss: 0.3925461769104004
train_iter_loss: 0.27489516139030457
train_iter_loss: 0.21160593628883362
train_iter_loss: 0.25946202874183655
train_iter_loss: 0.3286975622177124
train_iter_loss: 0.29673460125923157
train_iter_loss: 0.4040486216545105
train_iter_loss: 0.15575045347213745
train_iter_loss: 0.2938041687011719
train_iter_loss: 0.3157724440097809
train_iter_loss: 0.22103726863861084
train_iter_loss: 0.2791270315647125
train_iter_loss: 0.2515915334224701
train_iter_loss: 0.3129872977733612
train_iter_loss: 0.34694841504096985
train_iter_loss: 0.0982525572180748
train_iter_loss: 0.2864092290401459
train_iter_loss: 0.43252912163734436
train_iter_loss: 0.3135336637496948
train_iter_loss: 0.42568880319595337
train_iter_loss: 0.16676737368106842
train_iter_loss: 0.36114752292633057
train_iter_loss: 0.20299746096134186
train_iter_loss: 0.3014806807041168
train_iter_loss: 0.31098678708076477
train_iter_loss: 0.2495899647474289
train_iter_loss: 0.2041844129562378
train_iter_loss: 0.13061212003231049
train_iter_loss: 0.350674569606781
train_iter_loss: 0.16061970591545105
train_iter_loss: 0.2660015821456909
train_iter_loss: 0.1808452606201172
train_iter_loss: 0.24295349419116974
train_iter_loss: 0.15336206555366516
train_iter_loss: 0.24846147000789642
train_iter_loss: 0.23589804768562317
train_iter_loss: 0.1218848004937172
train_iter_loss: 0.45426851511001587
train_iter_loss: 0.23391932249069214
train_iter_loss: 0.33430615067481995
train_iter_loss: 0.3581114709377289
train_iter_loss: 0.3196100890636444
train_iter_loss: 0.28947505354881287
train_iter_loss: 0.25867852568626404
train_iter_loss: 0.21990175545215607
train_iter_loss: 0.21086029708385468
train_iter_loss: 0.36337655782699585
train_iter_loss: 0.19354182481765747
train_iter_loss: 0.25399282574653625
train_iter_loss: 0.39625176787376404
train_iter_loss: 0.20097284018993378
train_iter_loss: 0.25036147236824036
train_iter_loss: 0.300778865814209
train_iter_loss: 0.30569690465927124
train_iter_loss: 0.401388943195343
train_iter_loss: 0.2492939531803131
train_iter_loss: 0.26021909713745117
train_iter_loss: 0.06493961066007614
train_iter_loss: 0.331881046295166
train_iter_loss: 0.18982993066310883
train_iter_loss: 0.27849000692367554
train_iter_loss: 0.0767122134566307
train_iter_loss: 0.14285610616207123
train_iter_loss: 0.27483639121055603
train_iter_loss: 0.1831444352865219
train_iter_loss: 0.1558908075094223
train_iter_loss: 0.2745378315448761
train_iter_loss: 0.2872297763824463
train_iter_loss: 0.3361854553222656
train_iter_loss: 0.35625961422920227
train_iter_loss: 0.12424758821725845
train_iter_loss: 0.14178575575351715
train_iter_loss: 0.31526458263397217
train_iter_loss: 0.2235298752784729
train_iter_loss: 0.14199252426624298
train_iter_loss: 0.29687803983688354
train loss :0.2731
---------------------
Validation seg loss: 0.3787985797268602 at epoch 366
epoch =    367/  1000, exp = train
train_iter_loss: 0.3000274896621704
train_iter_loss: 0.29336416721343994
train_iter_loss: 0.24175003170967102
train_iter_loss: 0.19994275271892548
train_iter_loss: 0.34994930028915405
train_iter_loss: 0.346150666475296
train_iter_loss: 0.25053367018699646
train_iter_loss: 0.2567136585712433
train_iter_loss: 0.23944365978240967
train_iter_loss: 0.2906093895435333
train_iter_loss: 0.29509711265563965
train_iter_loss: 0.2838176488876343
train_iter_loss: 0.28670668601989746
train_iter_loss: 0.4060633182525635
train_iter_loss: 0.39432239532470703
train_iter_loss: 0.2918970286846161
train_iter_loss: 0.3141988217830658
train_iter_loss: 0.17470337450504303
train_iter_loss: 0.30675798654556274
train_iter_loss: 0.18294212222099304
train_iter_loss: 0.40309327840805054
train_iter_loss: 0.2860610783100128
train_iter_loss: 0.250450998544693
train_iter_loss: 0.15371860563755035
train_iter_loss: 0.3319692611694336
train_iter_loss: 0.28305742144584656
train_iter_loss: 0.13005511462688446
train_iter_loss: 0.22531288862228394
train_iter_loss: 0.30542272329330444
train_iter_loss: 0.28823134303092957
train_iter_loss: 0.40636563301086426
train_iter_loss: 0.2655705213546753
train_iter_loss: 0.29155537486076355
train_iter_loss: 0.3012979328632355
train_iter_loss: 0.28521251678466797
train_iter_loss: 0.35104402899742126
train_iter_loss: 0.2800724506378174
train_iter_loss: 0.14771690964698792
train_iter_loss: 0.3195360600948334
train_iter_loss: 0.15214155614376068
train_iter_loss: 0.24982763826847076
train_iter_loss: 0.19834038615226746
train_iter_loss: 0.3700903654098511
train_iter_loss: 0.27497240900993347
train_iter_loss: 0.36089658737182617
train_iter_loss: 0.3288755714893341
train_iter_loss: 0.26130008697509766
train_iter_loss: 0.19090794026851654
train_iter_loss: 0.1532909870147705
train_iter_loss: 0.21841643750667572
train_iter_loss: 0.280994713306427
train_iter_loss: 0.36153125762939453
train_iter_loss: 0.2922165095806122
train_iter_loss: 0.33696436882019043
train_iter_loss: 0.36870723962783813
train_iter_loss: 0.30358651280403137
train_iter_loss: 0.2546909749507904
train_iter_loss: 0.06498470902442932
train_iter_loss: 0.28060033917427063
train_iter_loss: 0.18995380401611328
train_iter_loss: 0.22386404871940613
train_iter_loss: 0.42486926913261414
train_iter_loss: 0.34937289357185364
train_iter_loss: 0.25642961263656616
train_iter_loss: 0.1997828632593155
train_iter_loss: 0.3489120304584503
train_iter_loss: 0.19889092445373535
train_iter_loss: 0.27438125014305115
train_iter_loss: 0.3840273916721344
train_iter_loss: 0.31833669543266296
train_iter_loss: 0.21938566863536835
train_iter_loss: 0.2785509526729584
train_iter_loss: 0.3547702431678772
train_iter_loss: 0.23648497462272644
train_iter_loss: 0.3275735080242157
train_iter_loss: 0.3357206881046295
train_iter_loss: 0.25038403272628784
train_iter_loss: 0.41807201504707336
train_iter_loss: 0.22702470421791077
train_iter_loss: 0.21837088465690613
train_iter_loss: 0.16694137454032898
train_iter_loss: 0.2769554853439331
train_iter_loss: 0.2713422179222107
train_iter_loss: 0.27860119938850403
train_iter_loss: 0.10599924623966217
train_iter_loss: 0.15762048959732056
train_iter_loss: 0.12457433342933655
train_iter_loss: 0.2349943220615387
train_iter_loss: 0.24155057966709137
train_iter_loss: 0.6543616056442261
train_iter_loss: 0.207585409283638
train_iter_loss: 0.4328421950340271
train_iter_loss: 0.27917757630348206
train_iter_loss: 0.1719774305820465
train_iter_loss: 0.33205127716064453
train_iter_loss: 0.23337231576442719
train_iter_loss: 0.32478341460227966
train_iter_loss: 0.18172794580459595
train_iter_loss: 0.29396697878837585
train_iter_loss: 0.1602257639169693
train loss :0.2775
---------------------
Validation seg loss: 0.3588958764712344 at epoch 367
epoch =    368/  1000, exp = train
train_iter_loss: 0.19801902770996094
train_iter_loss: 0.22207625210285187
train_iter_loss: 0.33027470111846924
train_iter_loss: 0.14735914766788483
train_iter_loss: 0.2196183055639267
train_iter_loss: 0.15858517587184906
train_iter_loss: 0.19160188734531403
train_iter_loss: 0.2807164192199707
train_iter_loss: 0.16446435451507568
train_iter_loss: 0.2325722575187683
train_iter_loss: 0.10098576545715332
train_iter_loss: 0.300417959690094
train_iter_loss: 0.247186541557312
train_iter_loss: 0.4098597764968872
train_iter_loss: 0.24897772073745728
train_iter_loss: 0.41995033621788025
train_iter_loss: 0.4738926589488983
train_iter_loss: 0.37307026982307434
train_iter_loss: 0.2609499990940094
train_iter_loss: 0.1932007223367691
train_iter_loss: 0.21513842046260834
train_iter_loss: 0.515509307384491
train_iter_loss: 0.26354482769966125
train_iter_loss: 0.3195565342903137
train_iter_loss: 0.16798053681850433
train_iter_loss: 0.23597805202007294
train_iter_loss: 0.25659438967704773
train_iter_loss: 0.22533009946346283
train_iter_loss: 0.18586525321006775
train_iter_loss: 0.21070660650730133
train_iter_loss: 0.21540285646915436
train_iter_loss: 0.3841482102870941
train_iter_loss: 0.12989668548107147
train_iter_loss: 0.1250554323196411
train_iter_loss: 0.3479984402656555
train_iter_loss: 0.24926121532917023
train_iter_loss: 0.39586031436920166
train_iter_loss: 0.34811437129974365
train_iter_loss: 0.19658105075359344
train_iter_loss: 0.26469480991363525
train_iter_loss: 0.26079559326171875
train_iter_loss: 0.10655137896537781
train_iter_loss: 0.20264258980751038
train_iter_loss: 0.3202943503856659
train_iter_loss: 0.2728238105773926
train_iter_loss: 0.3545483648777008
train_iter_loss: 0.4226763844490051
train_iter_loss: 0.14054670929908752
train_iter_loss: 0.21609747409820557
train_iter_loss: 0.4814593493938446
train_iter_loss: 0.3137516975402832
train_iter_loss: 0.32674604654312134
train_iter_loss: 0.2841988503932953
train_iter_loss: 0.30547481775283813
train_iter_loss: 0.15521392226219177
train_iter_loss: 0.25746622681617737
train_iter_loss: 0.3898623287677765
train_iter_loss: 0.09900347143411636
train_iter_loss: 0.35348644852638245
train_iter_loss: 0.6691163182258606
train_iter_loss: 0.36551806330680847
train_iter_loss: 0.28590643405914307
train_iter_loss: 0.24095745384693146
train_iter_loss: 0.46787020564079285
train_iter_loss: 0.48349836468696594
train_iter_loss: 0.33152684569358826
train_iter_loss: 0.23747855424880981
train_iter_loss: 0.2818419337272644
train_iter_loss: 0.45656049251556396
train_iter_loss: 0.2609553933143616
train_iter_loss: 0.3321297764778137
train_iter_loss: 0.23731619119644165
train_iter_loss: 0.3049784004688263
train_iter_loss: 0.31054165959358215
train_iter_loss: 0.2766861915588379
train_iter_loss: 0.21706528961658478
train_iter_loss: 0.17968107759952545
train_iter_loss: 0.2773887813091278
train_iter_loss: 0.1382145881652832
train_iter_loss: 0.317004531621933
train_iter_loss: 0.37235167622566223
train_iter_loss: 0.14118541777133942
train_iter_loss: 0.1918249875307083
train_iter_loss: 0.0919552817940712
train_iter_loss: 0.1850999891757965
train_iter_loss: 0.21883201599121094
train_iter_loss: 0.3148164451122284
train_iter_loss: 0.20317009091377258
train_iter_loss: 0.3419908881187439
train_iter_loss: 0.3234720528125763
train_iter_loss: 0.3010476529598236
train_iter_loss: 0.34447070956230164
train_iter_loss: 0.4080442488193512
train_iter_loss: 0.23778316378593445
train_iter_loss: 0.32839909195899963
train_iter_loss: 0.21537849307060242
train_iter_loss: 0.3070971965789795
train_iter_loss: 0.15140220522880554
train_iter_loss: 0.34897381067276
train_iter_loss: 0.36792296171188354
train loss :0.2804
---------------------
Validation seg loss: 0.37892227687539076 at epoch 368
epoch =    369/  1000, exp = train
train_iter_loss: 0.3117997944355011
train_iter_loss: 0.2578815817832947
train_iter_loss: 0.28468412160873413
train_iter_loss: 0.5296956896781921
train_iter_loss: 0.29002293944358826
train_iter_loss: 0.25190258026123047
train_iter_loss: 0.18947984278202057
train_iter_loss: 0.11982817202806473
train_iter_loss: 0.365352064371109
train_iter_loss: 0.29225391149520874
train_iter_loss: 0.35612165927886963
train_iter_loss: 0.21872349083423615
train_iter_loss: 0.341179758310318
train_iter_loss: 0.12187258899211884
train_iter_loss: 0.2749404311180115
train_iter_loss: 0.2952969968318939
train_iter_loss: 0.4293491542339325
train_iter_loss: 0.11583790183067322
train_iter_loss: 0.1570407748222351
train_iter_loss: 0.4320216476917267
train_iter_loss: 0.2928246259689331
train_iter_loss: 0.22952280938625336
train_iter_loss: 0.10001315921545029
train_iter_loss: 0.42017626762390137
train_iter_loss: 0.4021582305431366
train_iter_loss: 0.29778847098350525
train_iter_loss: 0.24119704961776733
train_iter_loss: 0.27649158239364624
train_iter_loss: 0.21778367459774017
train_iter_loss: 0.22603094577789307
train_iter_loss: 0.2555275857448578
train_iter_loss: 0.23452137410640717
train_iter_loss: 0.20051264762878418
train_iter_loss: 0.4220527112483978
train_iter_loss: 0.25675052404403687
train_iter_loss: 0.3352245092391968
train_iter_loss: 0.24582308530807495
train_iter_loss: 0.2735937535762787
train_iter_loss: 0.2833080291748047
train_iter_loss: 0.5064153075218201
train_iter_loss: 0.3484884202480316
train_iter_loss: 0.40547919273376465
train_iter_loss: 0.451157808303833
train_iter_loss: 0.220951646566391
train_iter_loss: 0.2907540500164032
train_iter_loss: 0.2417202591896057
train_iter_loss: 0.16248103976249695
train_iter_loss: 0.06817933917045593
train_iter_loss: 0.37718141078948975
train_iter_loss: 0.2104923576116562
train_iter_loss: 0.24270328879356384
train_iter_loss: 0.19422702491283417
train_iter_loss: 0.24787747859954834
train_iter_loss: 0.15822330117225647
train_iter_loss: 0.09007216989994049
train_iter_loss: 0.5188319683074951
train_iter_loss: 0.21453098952770233
train_iter_loss: 0.16484911739826202
train_iter_loss: 0.19247247278690338
train_iter_loss: 0.21797128021717072
train_iter_loss: 0.12369558215141296
train_iter_loss: 0.3080346882343292
train_iter_loss: 0.1546078473329544
train_iter_loss: 0.26257240772247314
train_iter_loss: 0.21163532137870789
train_iter_loss: 0.35837000608444214
train_iter_loss: 0.22508183121681213
train_iter_loss: 0.3431340157985687
train_iter_loss: 0.23539485037326813
train_iter_loss: 0.34010186791419983
train_iter_loss: 0.385195255279541
train_iter_loss: 0.36995911598205566
train_iter_loss: 0.28138211369514465
train_iter_loss: 0.32579776644706726
train_iter_loss: 0.12232189625501633
train_iter_loss: 0.2716211676597595
train_iter_loss: 0.32805201411247253
train_iter_loss: 0.349195271730423
train_iter_loss: 0.1960504651069641
train_iter_loss: 0.21609127521514893
train_iter_loss: 0.20265702903270721
train_iter_loss: 0.22736407816410065
train_iter_loss: 0.29538553953170776
train_iter_loss: 0.2347763329744339
train_iter_loss: 0.3637176752090454
train_iter_loss: 0.14737661182880402
train_iter_loss: 0.19833339750766754
train_iter_loss: 0.2897586524486542
train_iter_loss: 0.32567620277404785
train_iter_loss: 0.17999692261219025
train_iter_loss: 0.3325350284576416
train_iter_loss: 0.3383444845676422
train_iter_loss: 0.26833346486091614
train_iter_loss: 0.2711126208305359
train_iter_loss: 0.2357901930809021
train_iter_loss: 0.4097200334072113
train_iter_loss: 0.2793303430080414
train_iter_loss: 0.20592224597930908
train_iter_loss: 0.5169406533241272
train_iter_loss: 0.30444616079330444
train loss :0.2769
---------------------
Validation seg loss: 0.38776840160618414 at epoch 369
epoch =    370/  1000, exp = train
train_iter_loss: 0.2856638431549072
train_iter_loss: 0.14033541083335876
train_iter_loss: 0.2683004140853882
train_iter_loss: 0.2121487259864807
train_iter_loss: 0.3516845703125
train_iter_loss: 0.2372383028268814
train_iter_loss: 0.2228866070508957
train_iter_loss: 0.10515768080949783
train_iter_loss: 0.21473807096481323
train_iter_loss: 0.11873484402894974
train_iter_loss: 0.3496096432209015
train_iter_loss: 0.2788518965244293
train_iter_loss: 0.1856987625360489
train_iter_loss: 0.2530509829521179
train_iter_loss: 0.31525665521621704
train_iter_loss: 0.30731356143951416
train_iter_loss: 0.29108792543411255
train_iter_loss: 0.32062795758247375
train_iter_loss: 0.2975199818611145
train_iter_loss: 0.18982967734336853
train_iter_loss: 0.2572399377822876
train_iter_loss: 0.28621938824653625
train_iter_loss: 0.3332938849925995
train_iter_loss: 0.43070000410079956
train_iter_loss: 0.35407769680023193
train_iter_loss: 0.2881467938423157
train_iter_loss: 0.24139171838760376
train_iter_loss: 0.1847873032093048
train_iter_loss: 0.3011549413204193
train_iter_loss: 0.19728423655033112
train_iter_loss: 0.6107527613639832
train_iter_loss: 0.2067183405160904
train_iter_loss: 0.39363986253738403
train_iter_loss: 0.4189773499965668
train_iter_loss: 0.19876161217689514
train_iter_loss: 0.1488620489835739
train_iter_loss: 0.2592441737651825
train_iter_loss: 0.28309011459350586
train_iter_loss: 0.30779343843460083
train_iter_loss: 0.23915128409862518
train_iter_loss: 0.15743862092494965
train_iter_loss: 0.3043663203716278
train_iter_loss: 0.19959203898906708
train_iter_loss: 0.16009481251239777
train_iter_loss: 0.2864373028278351
train_iter_loss: 0.3247169852256775
train_iter_loss: 0.39410436153411865
train_iter_loss: 0.14968837797641754
train_iter_loss: 0.14079409837722778
train_iter_loss: 0.4054882824420929
train_iter_loss: 0.4745631515979767
train_iter_loss: 0.3712233304977417
train_iter_loss: 0.2147790938615799
train_iter_loss: 0.24457919597625732
train_iter_loss: 0.2616339921951294
train_iter_loss: 0.3546118438243866
train_iter_loss: 0.35672077536582947
train_iter_loss: 0.2912710905075073
train_iter_loss: 0.3237336575984955
train_iter_loss: 0.4126862585544586
train_iter_loss: 0.3621255159378052
train_iter_loss: 0.2997162938117981
train_iter_loss: 0.19658038020133972
train_iter_loss: 0.2867366075515747
train_iter_loss: 0.3714063763618469
train_iter_loss: 0.3527326285839081
train_iter_loss: 0.366425096988678
train_iter_loss: 0.13730637729167938
train_iter_loss: 0.21385401487350464
train_iter_loss: 0.4082275629043579
train_iter_loss: 0.16113901138305664
train_iter_loss: 0.31955164670944214
train_iter_loss: 0.2840988039970398
train_iter_loss: 0.19322986900806427
train_iter_loss: 0.31296399235725403
train_iter_loss: 0.47134318947792053
train_iter_loss: 0.3135179877281189
train_iter_loss: 0.29587996006011963
train_iter_loss: 0.2251129299402237
train_iter_loss: 0.1965031623840332
train_iter_loss: 0.3042077124118805
train_iter_loss: 0.32074227929115295
train_iter_loss: 0.3553752601146698
train_iter_loss: 0.3597336411476135
train_iter_loss: 0.351764053106308
train_iter_loss: 0.2574290931224823
train_iter_loss: 0.12047473341226578
train_iter_loss: 0.25496548414230347
train_iter_loss: 0.2436600625514984
train_iter_loss: 0.26091545820236206
train_iter_loss: 0.3831947147846222
train_iter_loss: 0.17944839596748352
train_iter_loss: 0.3472055196762085
train_iter_loss: 0.15939204394817352
train_iter_loss: 0.1997217983007431
train_iter_loss: 0.2909049689769745
train_iter_loss: 0.4810803234577179
train_iter_loss: 0.23290680348873138
train_iter_loss: 0.2656696140766144
train_iter_loss: 0.24817204475402832
train loss :0.2838
---------------------
Validation seg loss: 0.3706550579430219 at epoch 370
epoch =    371/  1000, exp = train
train_iter_loss: 0.4895261526107788
train_iter_loss: 0.2764882743358612
train_iter_loss: 0.2004329264163971
train_iter_loss: 0.21742886304855347
train_iter_loss: 0.5276238918304443
train_iter_loss: 0.35363155603408813
train_iter_loss: 0.22386108338832855
train_iter_loss: 0.3178142011165619
train_iter_loss: 0.3020869493484497
train_iter_loss: 0.29234883189201355
train_iter_loss: 0.3099667727947235
train_iter_loss: 0.2670832872390747
train_iter_loss: 0.16897901892662048
train_iter_loss: 0.23301111161708832
train_iter_loss: 0.22063864767551422
train_iter_loss: 0.12135560810565948
train_iter_loss: 0.2683185040950775
train_iter_loss: 0.18942518532276154
train_iter_loss: 0.2414819747209549
train_iter_loss: 0.2849409282207489
train_iter_loss: 0.2466014325618744
train_iter_loss: 0.3142823278903961
train_iter_loss: 0.2177785187959671
train_iter_loss: 0.21037252247333527
train_iter_loss: 0.34524157643318176
train_iter_loss: 0.30015528202056885
train_iter_loss: 0.304137259721756
train_iter_loss: 0.29893267154693604
train_iter_loss: 0.2531374394893646
train_iter_loss: 0.3113393783569336
train_iter_loss: 0.285622239112854
train_iter_loss: 0.1771588921546936
train_iter_loss: 0.18859103322029114
train_iter_loss: 0.24010345339775085
train_iter_loss: 0.20563700795173645
train_iter_loss: 0.21987691521644592
train_iter_loss: 0.2912362813949585
train_iter_loss: 0.19426026940345764
train_iter_loss: 0.1576170027256012
train_iter_loss: 0.18432734906673431
train_iter_loss: 0.45967891812324524
train_iter_loss: 0.2153627723455429
train_iter_loss: 0.2553597092628479
train_iter_loss: 0.23292170464992523
train_iter_loss: 0.24042826890945435
train_iter_loss: 0.2064768522977829
train_iter_loss: 0.3166901469230652
train_iter_loss: 0.36777955293655396
train_iter_loss: 0.3131040036678314
train_iter_loss: 0.2881929278373718
train_iter_loss: 0.2304193675518036
train_iter_loss: 0.25877153873443604
train_iter_loss: 0.21262124180793762
train_iter_loss: 0.4956055283546448
train_iter_loss: 0.40741926431655884
train_iter_loss: 0.32494470477104187
train_iter_loss: 0.23773273825645447
train_iter_loss: 0.24313494563102722
train_iter_loss: 0.39750128984451294
train_iter_loss: 0.4441167712211609
train_iter_loss: 0.2309500128030777
train_iter_loss: 0.20903396606445312
train_iter_loss: 0.29443979263305664
train_iter_loss: 0.25748929381370544
train_iter_loss: 0.2823711633682251
train_iter_loss: 0.33922478556632996
train_iter_loss: 0.27444756031036377
train_iter_loss: 0.20952697098255157
train_iter_loss: 0.2989242374897003
train_iter_loss: 0.26352524757385254
train_iter_loss: 0.1993488371372223
train_iter_loss: 0.22035813331604004
train_iter_loss: 0.2842312455177307
train_iter_loss: 0.19800254702568054
train_iter_loss: 0.2537570595741272
train_iter_loss: 0.3058355450630188
train_iter_loss: 0.20115768909454346
train_iter_loss: 0.27731823921203613
train_iter_loss: 0.2442231923341751
train_iter_loss: 0.17462721467018127
train_iter_loss: 0.21131497621536255
train_iter_loss: 0.6223194599151611
train_iter_loss: 0.32463783025741577
train_iter_loss: 0.18251940608024597
train_iter_loss: 0.11453656852245331
train_iter_loss: 0.26332756876945496
train_iter_loss: 0.31954237818717957
train_iter_loss: 0.28706079721450806
train_iter_loss: 0.4950854182243347
train_iter_loss: 0.31432050466537476
train_iter_loss: 0.34184351563453674
train_iter_loss: 0.3911591172218323
train_iter_loss: 0.2478511929512024
train_iter_loss: 0.142290398478508
train_iter_loss: 0.28857481479644775
train_iter_loss: 0.30912715196609497
train_iter_loss: 0.32663172483444214
train_iter_loss: 0.1508997231721878
train_iter_loss: 0.3058125078678131
train_iter_loss: 0.24770081043243408
train loss :0.2779
---------------------
Validation seg loss: 0.3809510485954442 at epoch 371
epoch =    372/  1000, exp = train
train_iter_loss: 0.15151889622211456
train_iter_loss: 0.30000776052474976
train_iter_loss: 0.16640913486480713
train_iter_loss: 0.37020185589790344
train_iter_loss: 0.21061824262142181
train_iter_loss: 0.18372777104377747
train_iter_loss: 0.10591774433851242
train_iter_loss: 0.27143847942352295
train_iter_loss: 0.15475468337535858
train_iter_loss: 0.20668338239192963
train_iter_loss: 0.22354066371917725
train_iter_loss: 0.19246211647987366
train_iter_loss: 0.21073728799819946
train_iter_loss: 0.14687390625476837
train_iter_loss: 0.22344568371772766
train_iter_loss: 0.3108976185321808
train_iter_loss: 0.41013795137405396
train_iter_loss: 0.48602068424224854
train_iter_loss: 0.3785454332828522
train_iter_loss: 0.4025736153125763
train_iter_loss: 0.2451017051935196
train_iter_loss: 0.1893056333065033
train_iter_loss: 0.5103496313095093
train_iter_loss: 0.23729801177978516
train_iter_loss: 0.2834469676017761
train_iter_loss: 0.24450716376304626
train_iter_loss: 0.23814520239830017
train_iter_loss: 0.25987663865089417
train_iter_loss: 0.3182274401187897
train_iter_loss: 0.20614531636238098
train_iter_loss: 0.3986954391002655
train_iter_loss: 0.1862349808216095
train_iter_loss: 0.2188674807548523
train_iter_loss: 0.12286604195833206
train_iter_loss: 0.2972867488861084
train_iter_loss: 0.2498050034046173
train_iter_loss: 0.12148670107126236
train_iter_loss: 0.29913902282714844
train_iter_loss: 0.2121875137090683
train_iter_loss: 0.29458752274513245
train_iter_loss: 0.3768274486064911
train_iter_loss: 0.07771064341068268
train_iter_loss: 0.4517030417919159
train_iter_loss: 0.3209916949272156
train_iter_loss: 0.2898350656032562
train_iter_loss: 0.3856048285961151
train_iter_loss: 0.2689012885093689
train_iter_loss: 0.39914146065711975
train_iter_loss: 0.3938596546649933
train_iter_loss: 0.17640559375286102
train_iter_loss: 0.4500126838684082
train_iter_loss: 0.15064281225204468
train_iter_loss: 0.27738991379737854
train_iter_loss: 0.19706162810325623
train_iter_loss: 0.2519279420375824
train_iter_loss: 0.27741795778274536
train_iter_loss: 0.26399797201156616
train_iter_loss: 0.3199387192726135
train_iter_loss: 0.2556172311306
train_iter_loss: 0.289539635181427
train_iter_loss: 0.2529580593109131
train_iter_loss: 0.5536145567893982
train_iter_loss: 0.16214388608932495
train_iter_loss: 0.20127417147159576
train_iter_loss: 0.15244978666305542
train_iter_loss: 0.35548871755599976
train_iter_loss: 0.27692198753356934
train_iter_loss: 0.25353261828422546
train_iter_loss: 0.3896844983100891
train_iter_loss: 0.21742482483386993
train_iter_loss: 0.1757955402135849
train_iter_loss: 0.3236859142780304
train_iter_loss: 0.20138800144195557
train_iter_loss: 0.3250185251235962
train_iter_loss: 0.22891946136951447
train_iter_loss: 0.31725072860717773
train_iter_loss: 0.22522777318954468
train_iter_loss: 0.22339177131652832
train_iter_loss: 0.1493992805480957
train_iter_loss: 0.3320821225643158
train_iter_loss: 0.39094966650009155
train_iter_loss: 0.23324395716190338
train_iter_loss: 0.3198273479938507
train_iter_loss: 0.2389787882566452
train_iter_loss: 0.27521032094955444
train_iter_loss: 0.2941874861717224
train_iter_loss: 0.21631595492362976
train_iter_loss: 0.33886557817459106
train_iter_loss: 0.38618209958076477
train_iter_loss: 0.26977041363716125
train_iter_loss: 0.3307500183582306
train_iter_loss: 0.29886215925216675
train_iter_loss: 0.169387549161911
train_iter_loss: 0.20825165510177612
train_iter_loss: 0.2988797128200531
train_iter_loss: 0.45986464619636536
train_iter_loss: 0.34617292881011963
train_iter_loss: 0.3876980245113373
train_iter_loss: 0.08975882083177567
train_iter_loss: 0.24577930569648743
train loss :0.2751
---------------------
Validation seg loss: 0.3673832113151702 at epoch 372
epoch =    373/  1000, exp = train
train_iter_loss: 0.2881743311882019
train_iter_loss: 0.2165784388780594
train_iter_loss: 0.19633589684963226
train_iter_loss: 0.1181979551911354
train_iter_loss: 0.3571013808250427
train_iter_loss: 0.34205731749534607
train_iter_loss: 0.17158132791519165
train_iter_loss: 0.3077702820301056
train_iter_loss: 0.3088099956512451
train_iter_loss: 0.19724532961845398
train_iter_loss: 0.30737826228141785
train_iter_loss: 0.15344786643981934
train_iter_loss: 0.2625104784965515
train_iter_loss: 0.47188615798950195
train_iter_loss: 0.1875983625650406
train_iter_loss: 0.1951255351305008
train_iter_loss: 0.3657230734825134
train_iter_loss: 0.26358935236930847
train_iter_loss: 0.2446427345275879
train_iter_loss: 0.19497917592525482
train_iter_loss: 0.27723708748817444
train_iter_loss: 0.2801657021045685
train_iter_loss: 0.17596349120140076
train_iter_loss: 0.28314489126205444
train_iter_loss: 0.23119020462036133
train_iter_loss: 0.3894892632961273
train_iter_loss: 0.21765448153018951
train_iter_loss: 0.32165729999542236
train_iter_loss: 0.28028929233551025
train_iter_loss: 0.23218467831611633
train_iter_loss: 0.2686122953891754
train_iter_loss: 0.220677450299263
train_iter_loss: 0.10928824543952942
train_iter_loss: 0.2774263620376587
train_iter_loss: 0.3543877601623535
train_iter_loss: 0.2848900258541107
train_iter_loss: 0.24032969772815704
train_iter_loss: 0.2898317575454712
train_iter_loss: 0.2596084177494049
train_iter_loss: 0.1643485426902771
train_iter_loss: 0.29395315051078796
train_iter_loss: 0.17453399300575256
train_iter_loss: 0.4562893509864807
train_iter_loss: 0.3745128810405731
train_iter_loss: 0.23724479973316193
train_iter_loss: 0.5282772779464722
train_iter_loss: 0.35059645771980286
train_iter_loss: 0.20557904243469238
train_iter_loss: 0.4077076315879822
train_iter_loss: 0.293942928314209
train_iter_loss: 0.48222488164901733
train_iter_loss: 0.26509204506874084
train_iter_loss: 0.3016898036003113
train_iter_loss: 0.3303176462650299
train_iter_loss: 0.2593476176261902
train_iter_loss: 0.41222071647644043
train_iter_loss: 0.35536038875579834
train_iter_loss: 0.4309493899345398
train_iter_loss: 0.2345651388168335
train_iter_loss: 0.19303260743618011
train_iter_loss: 0.23534950613975525
train_iter_loss: 0.3203965723514557
train_iter_loss: 0.2305273413658142
train_iter_loss: 0.43918028473854065
train_iter_loss: 0.46188250184059143
train_iter_loss: 0.11533443629741669
train_iter_loss: 0.3400803208351135
train_iter_loss: 0.3338019847869873
train_iter_loss: 0.1536257117986679
train_iter_loss: 0.24904929101467133
train_iter_loss: 0.29041197896003723
train_iter_loss: 0.26877540349960327
train_iter_loss: 0.34689900279045105
train_iter_loss: 0.1592606157064438
train_iter_loss: 0.27318400144577026
train_iter_loss: 0.33216574788093567
train_iter_loss: 0.2528308629989624
train_iter_loss: 0.3323843777179718
train_iter_loss: 0.16087180376052856
train_iter_loss: 0.376046746969223
train_iter_loss: 0.14772289991378784
train_iter_loss: 0.2746249735355377
train_iter_loss: 0.1853046417236328
train_iter_loss: 0.2841758728027344
train_iter_loss: 0.23212383687496185
train_iter_loss: 0.17294852435588837
train_iter_loss: 0.3052915930747986
train_iter_loss: 0.2470390498638153
train_iter_loss: 0.42300117015838623
train_iter_loss: 0.2743102014064789
train_iter_loss: 0.17162972688674927
train_iter_loss: 0.1633327454328537
train_iter_loss: 0.22133196890354156
train_iter_loss: 0.241612046957016
train_iter_loss: 0.34276053309440613
train_iter_loss: 0.2912088930606842
train_iter_loss: 0.11330021917819977
train_iter_loss: 0.4696502983570099
train_iter_loss: 0.21946825087070465
train_iter_loss: 0.20880362391471863
train loss :0.2783
---------------------
Validation seg loss: 0.3797874415451485 at epoch 373
epoch =    374/  1000, exp = train
train_iter_loss: 0.24935173988342285
train_iter_loss: 0.2838398814201355
train_iter_loss: 0.3070942461490631
train_iter_loss: 0.26021215319633484
train_iter_loss: 0.25476139783859253
train_iter_loss: 0.4021771550178528
train_iter_loss: 0.32177674770355225
train_iter_loss: 0.4879203736782074
train_iter_loss: 0.23045732080936432
train_iter_loss: 0.35509997606277466
train_iter_loss: 0.22098633646965027
train_iter_loss: 0.21755440533161163
train_iter_loss: 0.5721567869186401
train_iter_loss: 0.2436482161283493
train_iter_loss: 0.28424257040023804
train_iter_loss: 0.180948406457901
train_iter_loss: 0.24122896790504456
train_iter_loss: 0.10933716595172882
train_iter_loss: 0.36409690976142883
train_iter_loss: 0.3043358027935028
train_iter_loss: 0.2742046117782593
train_iter_loss: 0.2682151794433594
train_iter_loss: 0.2658367156982422
train_iter_loss: 0.11253595352172852
train_iter_loss: 0.19960202276706696
train_iter_loss: 0.26865410804748535
train_iter_loss: 0.23981207609176636
train_iter_loss: 0.2523477375507355
train_iter_loss: 0.23465894162654877
train_iter_loss: 0.18694224953651428
train_iter_loss: 0.3749430179595947
train_iter_loss: 0.2745453417301178
train_iter_loss: 0.18773365020751953
train_iter_loss: 0.16542674601078033
train_iter_loss: 0.17673733830451965
train_iter_loss: 0.2533634901046753
train_iter_loss: 0.31310269236564636
train_iter_loss: 0.22855332493782043
train_iter_loss: 0.18513920903205872
train_iter_loss: 0.19900989532470703
train_iter_loss: 0.39207568764686584
train_iter_loss: 0.29831698536872864
train_iter_loss: 0.2171623259782791
train_iter_loss: 0.2857568860054016
train_iter_loss: 0.2723231315612793
train_iter_loss: 0.2631012499332428
train_iter_loss: 0.1665499061346054
train_iter_loss: 0.35543835163116455
train_iter_loss: 0.29861781001091003
train_iter_loss: 0.3768361508846283
train_iter_loss: 0.33909890055656433
train_iter_loss: 0.19826936721801758
train_iter_loss: 0.16335293650627136
train_iter_loss: 0.24241530895233154
train_iter_loss: 0.05694623664021492
train_iter_loss: 0.2842276096343994
train_iter_loss: 0.1715390682220459
train_iter_loss: 0.3087851405143738
train_iter_loss: 0.2914503514766693
train_iter_loss: 0.2940613925457001
train_iter_loss: 0.25229737162590027
train_iter_loss: 0.26494473218917847
train_iter_loss: 0.24645327031612396
train_iter_loss: 0.29841771721839905
train_iter_loss: 0.22571919858455658
train_iter_loss: 0.27234479784965515
train_iter_loss: 0.3528701663017273
train_iter_loss: 0.24831432104110718
train_iter_loss: 0.4571482837200165
train_iter_loss: 0.27601107954978943
train_iter_loss: 0.6102413535118103
train_iter_loss: 0.15467573702335358
train_iter_loss: 0.3231288194656372
train_iter_loss: 0.1671106368303299
train_iter_loss: 0.16449806094169617
train_iter_loss: 0.20772071182727814
train_iter_loss: 0.3058829605579376
train_iter_loss: 0.28187498450279236
train_iter_loss: 0.29777470231056213
train_iter_loss: 0.4137033522129059
train_iter_loss: 0.6991530656814575
train_iter_loss: 0.280269980430603
train_iter_loss: 0.5708433985710144
train_iter_loss: 0.2843307852745056
train_iter_loss: 0.29569944739341736
train_iter_loss: 0.16134262084960938
train_iter_loss: 0.21329937875270844
train_iter_loss: 0.28010618686676025
train_iter_loss: 0.4355335831642151
train_iter_loss: 0.3475472033023834
train_iter_loss: 0.2952005863189697
train_iter_loss: 0.30690544843673706
train_iter_loss: 0.27017101645469666
train_iter_loss: 0.1764732450246811
train_iter_loss: 0.3124229907989502
train_iter_loss: 0.28581371903419495
train_iter_loss: 0.42647209763526917
train_iter_loss: 0.2402656525373459
train_iter_loss: 0.13455472886562347
train_iter_loss: 0.2153463214635849
train loss :0.2816
---------------------
Validation seg loss: 0.3802770749700941 at epoch 374
epoch =    375/  1000, exp = train
train_iter_loss: 0.17779023945331573
train_iter_loss: 0.30464282631874084
train_iter_loss: 0.3223140239715576
train_iter_loss: 0.2833908498287201
train_iter_loss: 0.1989244520664215
train_iter_loss: 0.39642223715782166
train_iter_loss: 0.23300248384475708
train_iter_loss: 0.31216806173324585
train_iter_loss: 0.17000657320022583
train_iter_loss: 0.20807555317878723
train_iter_loss: 0.33829623460769653
train_iter_loss: 0.41014233231544495
train_iter_loss: 0.4336675703525543
train_iter_loss: 0.24149870872497559
train_iter_loss: 0.13449567556381226
train_iter_loss: 0.21106737852096558
train_iter_loss: 0.34724682569503784
train_iter_loss: 0.13875240087509155
train_iter_loss: 0.26803481578826904
train_iter_loss: 0.33278706669807434
train_iter_loss: 0.30796727538108826
train_iter_loss: 0.25696349143981934
train_iter_loss: 0.3121441602706909
train_iter_loss: 0.07439328730106354
train_iter_loss: 0.20939694344997406
train_iter_loss: 0.25521987676620483
train_iter_loss: 0.32200777530670166
train_iter_loss: 0.271685391664505
train_iter_loss: 0.1944217085838318
train_iter_loss: 0.3995744287967682
train_iter_loss: 0.35767439007759094
train_iter_loss: 0.2513217329978943
train_iter_loss: 0.3403257131576538
train_iter_loss: 0.1919749230146408
train_iter_loss: 0.21889859437942505
train_iter_loss: 0.20893724262714386
train_iter_loss: 0.157929465174675
train_iter_loss: 0.2786560654640198
train_iter_loss: 0.3055025041103363
train_iter_loss: 0.24385637044906616
train_iter_loss: 0.23198750615119934
train_iter_loss: 0.27606621384620667
train_iter_loss: 0.17424306273460388
train_iter_loss: 0.3990916907787323
train_iter_loss: 0.29020583629608154
train_iter_loss: 0.26563650369644165
train_iter_loss: 0.3675088882446289
train_iter_loss: 0.21967405080795288
train_iter_loss: 0.24795490503311157
train_iter_loss: 0.319815993309021
train_iter_loss: 0.21552066504955292
train_iter_loss: 0.3166283965110779
train_iter_loss: 0.36822444200515747
train_iter_loss: 0.37736812233924866
train_iter_loss: 0.3002305030822754
train_iter_loss: 0.2413591891527176
train_iter_loss: 0.14304906129837036
train_iter_loss: 0.26168811321258545
train_iter_loss: 0.22553376853466034
train_iter_loss: 0.2632426917552948
train_iter_loss: 0.2927722930908203
train_iter_loss: 0.26657548546791077
train_iter_loss: 0.35245218873023987
train_iter_loss: 0.20804576575756073
train_iter_loss: 0.36261558532714844
train_iter_loss: 0.2765916883945465
train_iter_loss: 0.2459886074066162
train_iter_loss: 0.30049532651901245
train_iter_loss: 0.3553584814071655
train_iter_loss: 0.14869561791419983
train_iter_loss: 0.4418226182460785
train_iter_loss: 0.4349626302719116
train_iter_loss: 0.2786256968975067
train_iter_loss: 0.4015180170536041
train_iter_loss: 0.1689143180847168
train_iter_loss: 0.3978820741176605
train_iter_loss: 0.32211554050445557
train_iter_loss: 0.48422056436538696
train_iter_loss: 0.3581482768058777
train_iter_loss: 0.2503526508808136
train_iter_loss: 0.18366563320159912
train_iter_loss: 0.23157472908496857
train_iter_loss: 0.22903412580490112
train_iter_loss: 0.29332995414733887
train_iter_loss: 0.2247513383626938
train_iter_loss: 0.17644374072551727
train_iter_loss: 0.17303936183452606
train_iter_loss: 0.1265428215265274
train_iter_loss: 0.3683277368545532
train_iter_loss: 0.2802189886569977
train_iter_loss: 0.335584819316864
train_iter_loss: 0.36076587438583374
train_iter_loss: 0.25987809896469116
train_iter_loss: 0.34543904662132263
train_iter_loss: 0.2727082371711731
train_iter_loss: 0.375774621963501
train_iter_loss: 0.24072305858135223
train_iter_loss: 0.39542800188064575
train_iter_loss: 0.21889284253120422
train_iter_loss: 0.39050236344337463
train loss :0.2824
---------------------
Validation seg loss: 0.3675962503788606 at epoch 375
epoch =    376/  1000, exp = train
train_iter_loss: 0.3914767801761627
train_iter_loss: 0.1619802564382553
train_iter_loss: 0.23579901456832886
train_iter_loss: 0.04751540347933769
train_iter_loss: 0.31610703468322754
train_iter_loss: 0.3745098114013672
train_iter_loss: 0.17799828946590424
train_iter_loss: 0.1268317699432373
train_iter_loss: 0.3227301239967346
train_iter_loss: 0.3803466260433197
train_iter_loss: 0.1735968291759491
train_iter_loss: 0.27842310070991516
train_iter_loss: 0.16455425322055817
train_iter_loss: 0.19388636946678162
train_iter_loss: 0.24510808289051056
train_iter_loss: 0.2418300360441208
train_iter_loss: 0.33507105708122253
train_iter_loss: 0.397676557302475
train_iter_loss: 0.28150421380996704
train_iter_loss: 0.23532702028751373
train_iter_loss: 0.16407480835914612
train_iter_loss: 0.29566705226898193
train_iter_loss: 0.421957790851593
train_iter_loss: 0.21263542771339417
train_iter_loss: 0.30597198009490967
train_iter_loss: 0.2819225788116455
train_iter_loss: 0.36944708228111267
train_iter_loss: 0.2864300310611725
train_iter_loss: 0.41066494584083557
train_iter_loss: 0.19610437750816345
train_iter_loss: 0.24955253303050995
train_iter_loss: 0.19480183720588684
train_iter_loss: 0.3311276435852051
train_iter_loss: 0.31553202867507935
train_iter_loss: 0.3689206838607788
train_iter_loss: 0.2950397729873657
train_iter_loss: 0.3387420177459717
train_iter_loss: 0.26890134811401367
train_iter_loss: 0.3441668450832367
train_iter_loss: 0.22659292817115784
train_iter_loss: 0.15215851366519928
train_iter_loss: 0.5021867752075195
train_iter_loss: 0.25994208455085754
train_iter_loss: 0.3257054388523102
train_iter_loss: 0.28733280301094055
train_iter_loss: 0.2091798633337021
train_iter_loss: 0.2119348794221878
train_iter_loss: 0.32316967844963074
train_iter_loss: 0.14586755633354187
train_iter_loss: 0.13228289783000946
train_iter_loss: 0.22868812084197998
train_iter_loss: 0.1478288620710373
train_iter_loss: 0.237873375415802
train_iter_loss: 0.3088184595108032
train_iter_loss: 0.40262824296951294
train_iter_loss: 0.4945567548274994
train_iter_loss: 0.26349255442619324
train_iter_loss: 0.2540009021759033
train_iter_loss: 0.21927154064178467
train_iter_loss: 0.25381138920783997
train_iter_loss: 0.3099159598350525
train_iter_loss: 0.22792445123195648
train_iter_loss: 0.1899532675743103
train_iter_loss: 0.24610476195812225
train_iter_loss: 0.2317640632390976
train_iter_loss: 0.35862642526626587
train_iter_loss: 0.21384093165397644
train_iter_loss: 0.11920057982206345
train_iter_loss: 0.3064243495464325
train_iter_loss: 0.23772753775119781
train_iter_loss: 0.27561715245246887
train_iter_loss: 0.14573320746421814
train_iter_loss: 0.28631940484046936
train_iter_loss: 0.22532039880752563
train_iter_loss: 0.24473249912261963
train_iter_loss: 0.3418498933315277
train_iter_loss: 0.3209247291088104
train_iter_loss: 0.4995957911014557
train_iter_loss: 0.2601652443408966
train_iter_loss: 0.3572339415550232
train_iter_loss: 0.3028503656387329
train_iter_loss: 0.22982892394065857
train_iter_loss: 0.19747942686080933
train_iter_loss: 0.21618250012397766
train_iter_loss: 0.20281919836997986
train_iter_loss: 0.22530515491962433
train_iter_loss: 0.3497990667819977
train_iter_loss: 0.2017233967781067
train_iter_loss: 0.1820576786994934
train_iter_loss: 0.2741502821445465
train_iter_loss: 0.1645013391971588
train_iter_loss: 0.32607534527778625
train_iter_loss: 0.28742489218711853
train_iter_loss: 0.2761405408382416
train_iter_loss: 0.23035544157028198
train_iter_loss: 0.28033673763275146
train_iter_loss: 0.18576371669769287
train_iter_loss: 0.2277766764163971
train_iter_loss: 0.3293336033821106
train_iter_loss: 0.2978213429450989
train loss :0.2699
---------------------
Validation seg loss: 0.3828681161943472 at epoch 376
epoch =    377/  1000, exp = train
train_iter_loss: 0.28568142652511597
train_iter_loss: 0.18954886496067047
train_iter_loss: 0.3357093036174774
train_iter_loss: 0.3568476438522339
train_iter_loss: 0.2694486081600189
train_iter_loss: 0.19372792541980743
train_iter_loss: 0.256359726190567
train_iter_loss: 0.35352593660354614
train_iter_loss: 0.20828776061534882
train_iter_loss: 0.11105093359947205
train_iter_loss: 0.23956631124019623
train_iter_loss: 0.25038138031959534
train_iter_loss: 0.32448384165763855
train_iter_loss: 0.32890158891677856
train_iter_loss: 0.4043252170085907
train_iter_loss: 0.4178197979927063
train_iter_loss: 0.18803086876869202
train_iter_loss: 0.3174879252910614
train_iter_loss: 0.22882020473480225
train_iter_loss: 0.21999773383140564
train_iter_loss: 0.2818770706653595
train_iter_loss: 0.19923678040504456
train_iter_loss: 0.25373733043670654
train_iter_loss: 0.2757798135280609
train_iter_loss: 0.2622077167034149
train_iter_loss: 0.3399457633495331
train_iter_loss: 0.21064040064811707
train_iter_loss: 0.2671189308166504
train_iter_loss: 0.23707444965839386
train_iter_loss: 0.24325068295001984
train_iter_loss: 0.25759372115135193
train_iter_loss: 0.2591513693332672
train_iter_loss: 0.18173740804195404
train_iter_loss: 0.26942548155784607
train_iter_loss: 0.37035325169563293
train_iter_loss: 0.17666545510292053
train_iter_loss: 0.4765867590904236
train_iter_loss: 0.31228965520858765
train_iter_loss: 0.31959620118141174
train_iter_loss: 0.17831307649612427
train_iter_loss: 0.26869067549705505
train_iter_loss: 0.3527671992778778
train_iter_loss: 0.19177912175655365
train_iter_loss: 0.35563400387763977
train_iter_loss: 0.2562239468097687
train_iter_loss: 0.30814990401268005
train_iter_loss: 0.4945180416107178
train_iter_loss: 0.14399290084838867
train_iter_loss: 0.22687102854251862
train_iter_loss: 0.2792796492576599
train_iter_loss: 0.2925631105899811
train_iter_loss: 0.3875732421875
train_iter_loss: 0.2910926640033722
train_iter_loss: 0.38535311818122864
train_iter_loss: 0.18112143874168396
train_iter_loss: 0.2767515182495117
train_iter_loss: 0.27954214811325073
train_iter_loss: 0.26537418365478516
train_iter_loss: 0.16747711598873138
train_iter_loss: 0.2707648277282715
train_iter_loss: 0.1742323338985443
train_iter_loss: 0.2514718174934387
train_iter_loss: 0.16369307041168213
train_iter_loss: 0.2958929240703583
train_iter_loss: 0.21996918320655823
train_iter_loss: 0.2699636220932007
train_iter_loss: 0.24397549033164978
train_iter_loss: 0.20570410788059235
train_iter_loss: 0.34933117032051086
train_iter_loss: 0.24105501174926758
train_iter_loss: 0.31028667092323303
train_iter_loss: 0.1764204204082489
train_iter_loss: 0.37968453764915466
train_iter_loss: 0.2540038228034973
train_iter_loss: 0.27026933431625366
train_iter_loss: 0.4597509205341339
train_iter_loss: 0.24962766468524933
train_iter_loss: 0.31341540813446045
train_iter_loss: 0.20287850499153137
train_iter_loss: 0.11214447766542435
train_iter_loss: 0.3822005093097687
train_iter_loss: 0.4018627405166626
train_iter_loss: 0.25852200388908386
train_iter_loss: 0.36777228116989136
train_iter_loss: 0.4126058518886566
train_iter_loss: 0.3466663956642151
train_iter_loss: 0.21419085562229156
train_iter_loss: 0.2567080557346344
train_iter_loss: 0.32800835371017456
train_iter_loss: 0.31788551807403564
train_iter_loss: 0.2782335877418518
train_iter_loss: 0.1991388350725174
train_iter_loss: 0.24907483160495758
train_iter_loss: 0.22042308747768402
train_iter_loss: 0.25214123725891113
train_iter_loss: 0.3812873363494873
train_iter_loss: 0.22436214983463287
train_iter_loss: 0.19957812130451202
train_iter_loss: 0.37791094183921814
train_iter_loss: 0.14410464465618134
train loss :0.2775
---------------------
Validation seg loss: 0.43289449208257896 at epoch 377
epoch =    378/  1000, exp = train
train_iter_loss: 0.17364272475242615
train_iter_loss: 0.31742361187934875
train_iter_loss: 0.1873588263988495
train_iter_loss: 0.3115624487400055
train_iter_loss: 0.4730401039123535
train_iter_loss: 0.20605668425559998
train_iter_loss: 0.2439507097005844
train_iter_loss: 0.24921391904354095
train_iter_loss: 0.23876149952411652
train_iter_loss: 0.30979233980178833
train_iter_loss: 0.17877523601055145
train_iter_loss: 0.32318153977394104
train_iter_loss: 0.2595630884170532
train_iter_loss: 0.152821347117424
train_iter_loss: 0.4462364614009857
train_iter_loss: 0.2722272574901581
train_iter_loss: 0.23348616063594818
train_iter_loss: 0.25943467020988464
train_iter_loss: 0.2593182325363159
train_iter_loss: 0.31402045488357544
train_iter_loss: 0.31192681193351746
train_iter_loss: 0.15802033245563507
train_iter_loss: 0.4092130959033966
train_iter_loss: 0.41577985882759094
train_iter_loss: 0.20542123913764954
train_iter_loss: 0.3220581114292145
train_iter_loss: 0.29415643215179443
train_iter_loss: 0.2908999025821686
train_iter_loss: 0.3976539671421051
train_iter_loss: 0.034104980528354645
train_iter_loss: 0.17360879480838776
train_iter_loss: 0.3231140375137329
train_iter_loss: 0.38484251499176025
train_iter_loss: 0.22148151695728302
train_iter_loss: 0.2952685058116913
train_iter_loss: 0.09928634017705917
train_iter_loss: 0.4177315831184387
train_iter_loss: 0.37630560994148254
train_iter_loss: 0.3058132231235504
train_iter_loss: 0.22488395869731903
train_iter_loss: 0.2986816465854645
train_iter_loss: 0.19826090335845947
train_iter_loss: 0.33775395154953003
train_iter_loss: 0.33522796630859375
train_iter_loss: 0.35393086075782776
train_iter_loss: 0.19086194038391113
train_iter_loss: 0.2015359103679657
train_iter_loss: 0.11826706677675247
train_iter_loss: 0.27975913882255554
train_iter_loss: 0.27385032176971436
train_iter_loss: 0.15035384893417358
train_iter_loss: 0.21506014466285706
train_iter_loss: 0.25311827659606934
train_iter_loss: 0.23503130674362183
train_iter_loss: 0.11214098334312439
train_iter_loss: 0.3737868666648865
train_iter_loss: 0.3327108919620514
train_iter_loss: 0.15857887268066406
train_iter_loss: 0.2586001455783844
train_iter_loss: 0.30928510427474976
train_iter_loss: 0.25624507665634155
train_iter_loss: 0.3115402162075043
train_iter_loss: 0.32510098814964294
train_iter_loss: 0.24628575146198273
train_iter_loss: 0.206668421626091
train_iter_loss: 0.47975194454193115
train_iter_loss: 0.27845147252082825
train_iter_loss: 0.175328329205513
train_iter_loss: 0.421304851770401
train_iter_loss: 0.4494430124759674
train_iter_loss: 0.19153478741645813
train_iter_loss: 0.1863400638103485
train_iter_loss: 0.2494160681962967
train_iter_loss: 0.30008724331855774
train_iter_loss: 0.4022448658943176
train_iter_loss: 0.23604723811149597
train_iter_loss: 0.36855605244636536
train_iter_loss: 0.24540552496910095
train_iter_loss: 0.17324098944664001
train_iter_loss: 0.2266354113817215
train_iter_loss: 0.2177993804216385
train_iter_loss: 0.376388281583786
train_iter_loss: 0.31373560428619385
train_iter_loss: 0.22381405532360077
train_iter_loss: 0.15818928182125092
train_iter_loss: 0.17404364049434662
train_iter_loss: 0.3091353178024292
train_iter_loss: 0.2795974910259247
train_iter_loss: 0.27724024653434753
train_iter_loss: 0.234395831823349
train_iter_loss: 0.18090669810771942
train_iter_loss: 0.4000760614871979
train_iter_loss: 0.3813360035419464
train_iter_loss: 0.256747305393219
train_iter_loss: 0.18955017626285553
train_iter_loss: 0.2352643758058548
train_iter_loss: 0.1898709386587143
train_iter_loss: 0.10022644698619843
train_iter_loss: 0.21880826354026794
train_iter_loss: 0.21618300676345825
train loss :0.2700
---------------------
Validation seg loss: 0.3618720236656098 at epoch 378
epoch =    379/  1000, exp = train
train_iter_loss: 0.20869971811771393
train_iter_loss: 0.06854041665792465
train_iter_loss: 0.15069611370563507
train_iter_loss: 0.33652815222740173
train_iter_loss: 0.2280108779668808
train_iter_loss: 0.17557136714458466
train_iter_loss: 0.20495514571666718
train_iter_loss: 0.30483707785606384
train_iter_loss: 0.05804246664047241
train_iter_loss: 0.25091737508773804
train_iter_loss: 0.3044937551021576
train_iter_loss: 0.2562723457813263
train_iter_loss: 0.310791015625
train_iter_loss: 0.20619285106658936
train_iter_loss: 0.3147747218608856
train_iter_loss: 0.3984798491001129
train_iter_loss: 0.27397018671035767
train_iter_loss: 0.29583126306533813
train_iter_loss: 0.23000241816043854
train_iter_loss: 0.3784642517566681
train_iter_loss: 0.1401628851890564
train_iter_loss: 0.2892778813838959
train_iter_loss: 0.28756290674209595
train_iter_loss: 0.37671583890914917
train_iter_loss: 0.47752824425697327
train_iter_loss: 0.24251636862754822
train_iter_loss: 0.3959546983242035
train_iter_loss: 0.3438658118247986
train_iter_loss: 0.3032616078853607
train_iter_loss: 0.20792680978775024
train_iter_loss: 0.445834755897522
train_iter_loss: 0.26953715085983276
train_iter_loss: 0.13831016421318054
train_iter_loss: 0.2801694869995117
train_iter_loss: 0.34038951992988586
train_iter_loss: 0.13350987434387207
train_iter_loss: 0.2477484494447708
train_iter_loss: 0.21824911236763
train_iter_loss: 0.2641366422176361
train_iter_loss: 0.3224126398563385
train_iter_loss: 0.1734795719385147
train_iter_loss: 0.3099237084388733
train_iter_loss: 0.2733689844608307
train_iter_loss: 0.21475426852703094
train_iter_loss: 0.48834657669067383
train_iter_loss: 0.25286954641342163
train_iter_loss: 0.39869141578674316
train_iter_loss: 0.2978990972042084
train_iter_loss: 0.312258780002594
train_iter_loss: 0.31572842597961426
train_iter_loss: 0.30204954743385315
train_iter_loss: 0.1817583590745926
train_iter_loss: 0.2980037331581116
train_iter_loss: 0.3584774434566498
train_iter_loss: 0.1732403039932251
train_iter_loss: 0.2684136629104614
train_iter_loss: 0.39223307371139526
train_iter_loss: 0.14955058693885803
train_iter_loss: 0.33436375856399536
train_iter_loss: 0.2171083241701126
train_iter_loss: 0.33118149638175964
train_iter_loss: 0.19826683402061462
train_iter_loss: 0.2990930676460266
train_iter_loss: 0.30964425206184387
train_iter_loss: 0.283867210149765
train_iter_loss: 0.19388581812381744
train_iter_loss: 0.20461145043373108
train_iter_loss: 0.28065919876098633
train_iter_loss: 0.3047258257865906
train_iter_loss: 0.2826366424560547
train_iter_loss: 0.2824227213859558
train_iter_loss: 0.37296655774116516
train_iter_loss: 0.1854221224784851
train_iter_loss: 0.11012490093708038
train_iter_loss: 0.3188919723033905
train_iter_loss: 0.23913908004760742
train_iter_loss: 0.3674427568912506
train_iter_loss: 0.21143753826618195
train_iter_loss: 0.2376835197210312
train_iter_loss: 0.26577210426330566
train_iter_loss: 0.13732494413852692
train_iter_loss: 0.28302496671676636
train_iter_loss: 0.2872612774372101
train_iter_loss: 0.38785654306411743
train_iter_loss: 0.30223262310028076
train_iter_loss: 0.29207441210746765
train_iter_loss: 0.44542568922042847
train_iter_loss: 0.4207673668861389
train_iter_loss: 0.2484981119632721
train_iter_loss: 0.1745215803384781
train_iter_loss: 0.2716190218925476
train_iter_loss: 0.2163967341184616
train_iter_loss: 0.24233415722846985
train_iter_loss: 0.36014431715011597
train_iter_loss: 0.3674367368221283
train_iter_loss: 0.14393699169158936
train_iter_loss: 0.2998873293399811
train_iter_loss: 0.3017852306365967
train_iter_loss: 0.19220668077468872
train_iter_loss: 0.2954484820365906
train loss :0.2761
---------------------
Validation seg loss: 0.35920609600560843 at epoch 379
epoch =    380/  1000, exp = train
train_iter_loss: 0.2532980442047119
train_iter_loss: 0.2508159875869751
train_iter_loss: 0.16625246405601501
train_iter_loss: 0.1843656599521637
train_iter_loss: 0.32255280017852783
train_iter_loss: 0.18485912680625916
train_iter_loss: 0.38687291741371155
train_iter_loss: 0.37793657183647156
train_iter_loss: 0.3114110827445984
train_iter_loss: 0.46855664253234863
train_iter_loss: 0.23984013497829437
train_iter_loss: 0.23808152973651886
train_iter_loss: 0.17350555956363678
train_iter_loss: 0.2395973801612854
train_iter_loss: 0.33501437306404114
train_iter_loss: 0.35587430000305176
train_iter_loss: 0.29563650488853455
train_iter_loss: 0.30213043093681335
train_iter_loss: 0.21973781287670135
train_iter_loss: 0.2373979538679123
train_iter_loss: 0.3488023281097412
train_iter_loss: 0.3396710455417633
train_iter_loss: 0.31630900502204895
train_iter_loss: 0.35506346821784973
train_iter_loss: 0.310805082321167
train_iter_loss: 0.43747130036354065
train_iter_loss: 0.2438194751739502
train_iter_loss: 0.34815776348114014
train_iter_loss: 0.17543166875839233
train_iter_loss: 0.11657363921403885
train_iter_loss: 0.36286064982414246
train_iter_loss: 0.3036777675151825
train_iter_loss: 0.25285080075263977
train_iter_loss: 0.26596254110336304
train_iter_loss: 0.3207506835460663
train_iter_loss: 0.3055703938007355
train_iter_loss: 0.26920562982559204
train_iter_loss: 0.23016805946826935
train_iter_loss: 0.31341129541397095
train_iter_loss: 0.32862499356269836
train_iter_loss: 0.36148038506507874
train_iter_loss: 0.24818192422389984
train_iter_loss: 0.36311304569244385
train_iter_loss: 0.3044705390930176
train_iter_loss: 0.2814596891403198
train_iter_loss: 0.3357153534889221
train_iter_loss: 0.33972227573394775
train_iter_loss: 0.3691639006137848
train_iter_loss: 0.27136746048927307
train_iter_loss: 0.1798705905675888
train_iter_loss: 0.1938694715499878
train_iter_loss: 0.30545106530189514
train_iter_loss: 0.2048095166683197
train_iter_loss: 0.24931496381759644
train_iter_loss: 0.446045845746994
train_iter_loss: 0.16592733561992645
train_iter_loss: 0.2739553451538086
train_iter_loss: 0.28976619243621826
train_iter_loss: 0.21160653233528137
train_iter_loss: 0.1788535714149475
train_iter_loss: 0.1801847368478775
train_iter_loss: 0.17650523781776428
train_iter_loss: 0.3479595184326172
train_iter_loss: 0.39913830161094666
train_iter_loss: 0.3384685218334198
train_iter_loss: 0.26128295063972473
train_iter_loss: 0.140611469745636
train_iter_loss: 0.3310019373893738
train_iter_loss: 0.25431036949157715
train_iter_loss: 0.25084933638572693
train_iter_loss: 0.10689998418092728
train_iter_loss: 0.36500871181488037
train_iter_loss: 0.3258696496486664
train_iter_loss: 0.2607470452785492
train_iter_loss: 0.1775551438331604
train_iter_loss: 0.3143877387046814
train_iter_loss: 0.2396547645330429
train_iter_loss: 0.15744449198246002
train_iter_loss: 0.315136194229126
train_iter_loss: 0.29903608560562134
train_iter_loss: 0.2969329059123993
train_iter_loss: 0.053084418177604675
train_iter_loss: 0.19906818866729736
train_iter_loss: 0.3855781853199005
train_iter_loss: 0.21015092730522156
train_iter_loss: 0.24157866835594177
train_iter_loss: 0.3774162530899048
train_iter_loss: 0.375270277261734
train_iter_loss: 0.30177611112594604
train_iter_loss: 0.26773878931999207
train_iter_loss: 0.26365604996681213
train_iter_loss: 0.11743219941854477
train_iter_loss: 0.27066582441329956
train_iter_loss: 0.11550518125295639
train_iter_loss: 0.36874276399612427
train_iter_loss: 0.31240877509117126
train_iter_loss: 0.23377889394760132
train_iter_loss: 0.16914011538028717
train_iter_loss: 0.24148808419704437
train_iter_loss: 0.36790063977241516
train loss :0.2775
---------------------
Validation seg loss: 0.3588276465493694 at epoch 380
epoch =    381/  1000, exp = train
train_iter_loss: 0.30586281418800354
train_iter_loss: 0.2528190612792969
train_iter_loss: 0.2479514628648758
train_iter_loss: 0.3467409312725067
train_iter_loss: 0.18693797290325165
train_iter_loss: 0.4816340208053589
train_iter_loss: 0.19532166421413422
train_iter_loss: 0.20976758003234863
train_iter_loss: 0.4056800901889801
train_iter_loss: 0.18857917189598083
train_iter_loss: 0.26554253697395325
train_iter_loss: 0.1852540522813797
train_iter_loss: 0.3504300117492676
train_iter_loss: 0.2523888647556305
train_iter_loss: 0.2230253368616104
train_iter_loss: 0.062293268740177155
train_iter_loss: 0.3842315077781677
train_iter_loss: 0.3771919310092926
train_iter_loss: 0.2514728903770447
train_iter_loss: 0.2794240117073059
train_iter_loss: 0.373550146818161
train_iter_loss: 0.3388992249965668
train_iter_loss: 0.182431161403656
train_iter_loss: 0.27315372228622437
train_iter_loss: 0.31459009647369385
train_iter_loss: 0.31748566031455994
train_iter_loss: 0.31138333678245544
train_iter_loss: 0.24537667632102966
train_iter_loss: 0.24976688623428345
train_iter_loss: 0.1521705687046051
train_iter_loss: 0.18765151500701904
train_iter_loss: 0.3272975981235504
train_iter_loss: 0.0921589806675911
train_iter_loss: 0.16984771192073822
train_iter_loss: 0.2426643669605255
train_iter_loss: 0.2643609046936035
train_iter_loss: 0.45319199562072754
train_iter_loss: 0.36339619755744934
train_iter_loss: 0.18984943628311157
train_iter_loss: 0.3901674151420593
train_iter_loss: 0.35496968030929565
train_iter_loss: 0.23892797529697418
train_iter_loss: 0.2610754668712616
train_iter_loss: 0.2918650805950165
train_iter_loss: 0.2803689241409302
train_iter_loss: 0.35763585567474365
train_iter_loss: 0.2540704905986786
train_iter_loss: 0.2744899094104767
train_iter_loss: 0.2397594302892685
train_iter_loss: 0.3660639524459839
train_iter_loss: 0.16243723034858704
train_iter_loss: 0.15384411811828613
train_iter_loss: 0.16474361717700958
train_iter_loss: 0.276811420917511
train_iter_loss: 0.32400238513946533
train_iter_loss: 0.2086305469274521
train_iter_loss: 0.38546717166900635
train_iter_loss: 0.21026264131069183
train_iter_loss: 0.18810006976127625
train_iter_loss: 0.2406850904226303
train_iter_loss: 0.36937394738197327
train_iter_loss: 0.23772238194942474
train_iter_loss: 0.29500797390937805
train_iter_loss: 0.2165377289056778
train_iter_loss: 0.3223218023777008
train_iter_loss: 0.1471225768327713
train_iter_loss: 0.4299558699131012
train_iter_loss: 0.25963297486305237
train_iter_loss: 0.3778342306613922
train_iter_loss: 0.2255306839942932
train_iter_loss: 0.3841230869293213
train_iter_loss: 0.2653089165687561
train_iter_loss: 0.2447006106376648
train_iter_loss: 0.22833147644996643
train_iter_loss: 0.31605294346809387
train_iter_loss: 0.23394541442394257
train_iter_loss: 0.223581001162529
train_iter_loss: 0.3486013412475586
train_iter_loss: 0.2584621012210846
train_iter_loss: 0.37514829635620117
train_iter_loss: 0.14422591030597687
train_iter_loss: 0.16780400276184082
train_iter_loss: 0.3238731920719147
train_iter_loss: 0.280537873506546
train_iter_loss: 0.3433665931224823
train_iter_loss: 0.22013629972934723
train_iter_loss: 0.2782801389694214
train_iter_loss: 0.10550747066736221
train_iter_loss: 0.20732037723064423
train_iter_loss: 0.3286789357662201
train_iter_loss: 0.2714730501174927
train_iter_loss: 0.3341750204563141
train_iter_loss: 0.5421120524406433
train_iter_loss: 0.20097796618938446
train_iter_loss: 0.3285706043243408
train_iter_loss: 0.22562001645565033
train_iter_loss: 0.43754103779792786
train_iter_loss: 0.15789884328842163
train_iter_loss: 0.2921828329563141
train_iter_loss: 0.12230094522237778
train loss :0.2748
---------------------
Validation seg loss: 0.38288043335712746 at epoch 381
epoch =    382/  1000, exp = train
train_iter_loss: 0.33246830105781555
train_iter_loss: 0.18389159440994263
train_iter_loss: 0.2204430103302002
train_iter_loss: 0.16158874332904816
train_iter_loss: 0.4334907829761505
train_iter_loss: 0.17123523354530334
train_iter_loss: 0.17649085819721222
train_iter_loss: 0.1606290489435196
train_iter_loss: 0.2621638774871826
train_iter_loss: 0.18215656280517578
train_iter_loss: 0.23278337717056274
train_iter_loss: 0.1083587184548378
train_iter_loss: 0.110666923224926
train_iter_loss: 0.3456501364707947
train_iter_loss: 0.2815801501274109
train_iter_loss: 0.22057513892650604
train_iter_loss: 0.28589874505996704
train_iter_loss: 0.3466796278953552
train_iter_loss: 0.4092060327529907
train_iter_loss: 0.2275322675704956
train_iter_loss: 0.27849555015563965
train_iter_loss: 0.0694170817732811
train_iter_loss: 0.21184097230434418
train_iter_loss: 0.49176666140556335
train_iter_loss: 0.31016379594802856
train_iter_loss: 0.21789944171905518
train_iter_loss: 0.3662169873714447
train_iter_loss: 0.3368534445762634
train_iter_loss: 0.42190057039260864
train_iter_loss: 0.32637229561805725
train_iter_loss: 0.18181157112121582
train_iter_loss: 0.2484445869922638
train_iter_loss: 0.23902150988578796
train_iter_loss: 0.22111833095550537
train_iter_loss: 0.49117839336395264
train_iter_loss: 0.24044357240200043
train_iter_loss: 0.2409864217042923
train_iter_loss: 0.25645092129707336
train_iter_loss: 0.21860085427761078
train_iter_loss: 0.23445793986320496
train_iter_loss: 0.23338797688484192
train_iter_loss: 0.31481051445007324
train_iter_loss: 0.3444240093231201
train_iter_loss: 0.29651710391044617
train_iter_loss: 0.2341342270374298
train_iter_loss: 0.08733419328927994
train_iter_loss: 0.19092218577861786
train_iter_loss: 0.41766881942749023
train_iter_loss: 0.2572275400161743
train_iter_loss: 0.22641772031784058
train_iter_loss: 0.2568610906600952
train_iter_loss: 0.22269651293754578
train_iter_loss: 0.415418803691864
train_iter_loss: 0.4264923632144928
train_iter_loss: 0.16637937724590302
train_iter_loss: 0.33973532915115356
train_iter_loss: 0.2315705567598343
train_iter_loss: 0.11065292358398438
train_iter_loss: 0.30709248781204224
train_iter_loss: 0.4842687249183655
train_iter_loss: 0.25760334730148315
train_iter_loss: 0.2397131770849228
train_iter_loss: 0.1279410868883133
train_iter_loss: 0.41314712166786194
train_iter_loss: 0.2122700810432434
train_iter_loss: 0.2796015739440918
train_iter_loss: 0.21386407315731049
train_iter_loss: 0.26257121562957764
train_iter_loss: 0.35545289516448975
train_iter_loss: 0.24731086194515228
train_iter_loss: 0.30922049283981323
train_iter_loss: 0.3272661864757538
train_iter_loss: 0.21229250729084015
train_iter_loss: 0.28102806210517883
train_iter_loss: 0.444486528635025
train_iter_loss: 0.23687484860420227
train_iter_loss: 0.11713740229606628
train_iter_loss: 0.2774331867694855
train_iter_loss: 0.25069257616996765
train_iter_loss: 0.2026398777961731
train_iter_loss: 0.38291990756988525
train_iter_loss: 0.09101980179548264
train_iter_loss: 0.15437068045139313
train_iter_loss: 0.2929118573665619
train_iter_loss: 0.19401630759239197
train_iter_loss: 0.1569623053073883
train_iter_loss: 0.28227338194847107
train_iter_loss: 0.15975289046764374
train_iter_loss: 0.23864920437335968
train_iter_loss: 0.27186399698257446
train_iter_loss: 0.23885059356689453
train_iter_loss: 0.35102224349975586
train_iter_loss: 0.3662858307361603
train_iter_loss: 0.31270742416381836
train_iter_loss: 0.3453509509563446
train_iter_loss: 0.32057759165763855
train_iter_loss: 0.29516980051994324
train_iter_loss: 0.3442251682281494
train_iter_loss: 0.1928657442331314
train_iter_loss: 0.18380044400691986
train loss :0.2674
---------------------
Validation seg loss: 0.3591126574177014 at epoch 382
epoch =    383/  1000, exp = train
train_iter_loss: 0.2260717749595642
train_iter_loss: 0.24953725934028625
train_iter_loss: 0.2854912579059601
train_iter_loss: 0.17976030707359314
train_iter_loss: 0.3943857252597809
train_iter_loss: 0.21462656557559967
train_iter_loss: 0.2701374292373657
train_iter_loss: 0.26198163628578186
train_iter_loss: 0.2941330373287201
train_iter_loss: 0.20322583615779877
train_iter_loss: 0.2615290582180023
train_iter_loss: 0.34361177682876587
train_iter_loss: 0.3734193444252014
train_iter_loss: 0.35289573669433594
train_iter_loss: 0.431364506483078
train_iter_loss: 0.3561062812805176
train_iter_loss: 0.22933752834796906
train_iter_loss: 0.3517265319824219
train_iter_loss: 0.1554824411869049
train_iter_loss: 0.30563637614250183
train_iter_loss: 0.22158505022525787
train_iter_loss: 0.29905813932418823
train_iter_loss: 0.2934589087963104
train_iter_loss: 0.2802030146121979
train_iter_loss: 0.17608091235160828
train_iter_loss: 0.23900562524795532
train_iter_loss: 0.253072589635849
train_iter_loss: 0.28086698055267334
train_iter_loss: 0.2611188590526581
train_iter_loss: 0.09652005881071091
train_iter_loss: 0.10317252576351166
train_iter_loss: 0.19316820800304413
train_iter_loss: 0.23977802693843842
train_iter_loss: 0.2893477976322174
train_iter_loss: 0.3294324278831482
train_iter_loss: 0.23701348900794983
train_iter_loss: 0.16350634396076202
train_iter_loss: 0.3919110596179962
train_iter_loss: 0.36790144443511963
train_iter_loss: 0.24548014998435974
train_iter_loss: 0.3140193521976471
train_iter_loss: 0.2744438350200653
train_iter_loss: 0.35541513562202454
train_iter_loss: 0.41081613302230835
train_iter_loss: 0.19992861151695251
train_iter_loss: 0.4531135559082031
train_iter_loss: 0.2823486626148224
train_iter_loss: 0.13846206665039062
train_iter_loss: 0.24235080182552338
train_iter_loss: 0.2937912046909332
train_iter_loss: 0.3460448086261749
train_iter_loss: 0.03687966614961624
train_iter_loss: 0.3236192464828491
train_iter_loss: 0.3897749185562134
train_iter_loss: 0.15290722250938416
train_iter_loss: 0.3217788636684418
train_iter_loss: 0.3450629413127899
train_iter_loss: 0.26038673520088196
train_iter_loss: 0.31247955560684204
train_iter_loss: 0.40030062198638916
train_iter_loss: 0.42014336585998535
train_iter_loss: 0.28597742319107056
train_iter_loss: 0.33867383003234863
train_iter_loss: 0.35865676403045654
train_iter_loss: 0.2009923756122589
train_iter_loss: 0.2657678425312042
train_iter_loss: 0.23055391013622284
train_iter_loss: 0.3440636992454529
train_iter_loss: 0.34304389357566833
train_iter_loss: 0.30451589822769165
train_iter_loss: 0.28093773126602173
train_iter_loss: 0.35012322664260864
train_iter_loss: 0.1966760754585266
train_iter_loss: 0.2866334319114685
train_iter_loss: 0.30939993262290955
train_iter_loss: 0.1867859810590744
train_iter_loss: 0.2418222576379776
train_iter_loss: 0.42293694615364075
train_iter_loss: 0.2267005443572998
train_iter_loss: 0.15045714378356934
train_iter_loss: 0.33272412419319153
train_iter_loss: 0.30435293912887573
train_iter_loss: 0.19161207973957062
train_iter_loss: 0.3677624762058258
train_iter_loss: 0.2375510334968567
train_iter_loss: 0.22645719349384308
train_iter_loss: 0.17327843606472015
train_iter_loss: 0.2732422351837158
train_iter_loss: 0.2607018053531647
train_iter_loss: 0.3935369849205017
train_iter_loss: 0.3374127745628357
train_iter_loss: 0.14767949283123016
train_iter_loss: 0.30709436535835266
train_iter_loss: 0.316190242767334
train_iter_loss: 0.32947131991386414
train_iter_loss: 0.35555657744407654
train_iter_loss: 0.27390506863594055
train_iter_loss: 0.2545962333679199
train_iter_loss: 0.19855575263500214
train_iter_loss: 0.29638510942459106
train loss :0.2815
---------------------
Validation seg loss: 0.3724340240312916 at epoch 383
epoch =    384/  1000, exp = train
train_iter_loss: 0.30059948563575745
train_iter_loss: 0.29980114102363586
train_iter_loss: 0.36970582604408264
train_iter_loss: 0.3700578808784485
train_iter_loss: 0.3273460566997528
train_iter_loss: 0.30883854627609253
train_iter_loss: 0.2857888340950012
train_iter_loss: 0.26160719990730286
train_iter_loss: 0.3703198730945587
train_iter_loss: 0.2978879511356354
train_iter_loss: 0.30843231081962585
train_iter_loss: 0.40398067235946655
train_iter_loss: 0.34204480051994324
train_iter_loss: 0.23633311688899994
train_iter_loss: 0.2005351036787033
train_iter_loss: 0.31289365887641907
train_iter_loss: 0.10297499597072601
train_iter_loss: 0.2385420799255371
train_iter_loss: 0.27203696966171265
train_iter_loss: 0.18083107471466064
train_iter_loss: 0.10961435735225677
train_iter_loss: 0.25250813364982605
train_iter_loss: 0.2600114047527313
train_iter_loss: 0.18326427042484283
train_iter_loss: 0.28721463680267334
train_iter_loss: 0.34164848923683167
train_iter_loss: 0.3021072745323181
train_iter_loss: 0.3357101380825043
train_iter_loss: 0.1916734129190445
train_iter_loss: 0.17587769031524658
train_iter_loss: 0.24528580904006958
train_iter_loss: 0.4327879548072815
train_iter_loss: 0.2918148636817932
train_iter_loss: 0.22082383930683136
train_iter_loss: 0.30285295844078064
train_iter_loss: 0.36440739035606384
train_iter_loss: 0.17995236814022064
train_iter_loss: 0.33235225081443787
train_iter_loss: 0.2409619837999344
train_iter_loss: 0.29660677909851074
train_iter_loss: 0.29105618596076965
train_iter_loss: 0.3326857388019562
train_iter_loss: 0.32262083888053894
train_iter_loss: 0.3631439805030823
train_iter_loss: 0.1798953264951706
train_iter_loss: 0.2729310095310211
train_iter_loss: 0.36164388060569763
train_iter_loss: 0.3567104935646057
train_iter_loss: 0.20478270947933197
train_iter_loss: 0.23398172855377197
train_iter_loss: 0.29769086837768555
train_iter_loss: 0.19801495969295502
train_iter_loss: 0.3451678454875946
train_iter_loss: 0.22981582581996918
train_iter_loss: 0.13790425658226013
train_iter_loss: 0.4304208755493164
train_iter_loss: 0.27828437089920044
train_iter_loss: 0.20900268852710724
train_iter_loss: 0.26280924677848816
train_iter_loss: 0.13068640232086182
train_iter_loss: 0.3069998621940613
train_iter_loss: 0.09685690701007843
train_iter_loss: 0.2667926251888275
train_iter_loss: 0.24009305238723755
train_iter_loss: 0.2901844084262848
train_iter_loss: 0.21850813925266266
train_iter_loss: 0.3525712788105011
train_iter_loss: 0.33760806918144226
train_iter_loss: 0.2963924705982208
train_iter_loss: 0.29061830043792725
train_iter_loss: 0.2581729292869568
train_iter_loss: 0.17363758385181427
train_iter_loss: 0.3688757121562958
train_iter_loss: 0.3371228575706482
train_iter_loss: 0.23353523015975952
train_iter_loss: 0.2422846555709839
train_iter_loss: 0.29137852787971497
train_iter_loss: 0.28935354948043823
train_iter_loss: 0.13252688944339752
train_iter_loss: 0.3413166403770447
train_iter_loss: 0.16393837332725525
train_iter_loss: 0.24598442018032074
train_iter_loss: 0.27078336477279663
train_iter_loss: 0.3099844753742218
train_iter_loss: 0.3525364398956299
train_iter_loss: 0.22287873923778534
train_iter_loss: 0.08433636277914047
train_iter_loss: 0.23280152678489685
train_iter_loss: 0.3048260509967804
train_iter_loss: 0.23318924009799957
train_iter_loss: 0.3889099657535553
train_iter_loss: 0.237375870347023
train_iter_loss: 0.34852707386016846
train_iter_loss: 0.37716636061668396
train_iter_loss: 0.18298673629760742
train_iter_loss: 0.3374139368534088
train_iter_loss: 0.31138700246810913
train_iter_loss: 0.2642757296562195
train_iter_loss: 0.24930065870285034
train_iter_loss: 0.25746458768844604
train loss :0.2760
---------------------
Validation seg loss: 0.3939061270991586 at epoch 384
epoch =    385/  1000, exp = train
train_iter_loss: 0.24278132617473602
train_iter_loss: 0.3418399393558502
train_iter_loss: 0.3629749119281769
train_iter_loss: 0.44414588809013367
train_iter_loss: 0.2578229010105133
train_iter_loss: 0.21561776101589203
train_iter_loss: 0.30777105689048767
train_iter_loss: 0.3609743118286133
train_iter_loss: 0.45691829919815063
train_iter_loss: 0.22090663015842438
train_iter_loss: 0.2874665856361389
train_iter_loss: 0.1833382248878479
train_iter_loss: 0.2692874073982239
train_iter_loss: 0.15932314097881317
train_iter_loss: 0.1748054176568985
train_iter_loss: 0.2281893938779831
train_iter_loss: 0.37994059920310974
train_iter_loss: 0.3566538393497467
train_iter_loss: 0.20308327674865723
train_iter_loss: 0.1970813274383545
train_iter_loss: 0.33026617765426636
train_iter_loss: 0.3167198896408081
train_iter_loss: 0.41458621621131897
train_iter_loss: 0.40395790338516235
train_iter_loss: 0.3393082022666931
train_iter_loss: 0.3767456114292145
train_iter_loss: 0.3061501681804657
train_iter_loss: 0.20670892298221588
train_iter_loss: 0.4209851324558258
train_iter_loss: 0.1494148224592209
train_iter_loss: 0.24436059594154358
train_iter_loss: 0.21909178793430328
train_iter_loss: 0.3208909332752228
train_iter_loss: 0.29873546957969666
train_iter_loss: 0.2944803535938263
train_iter_loss: 0.19396254420280457
train_iter_loss: 0.19518128037452698
train_iter_loss: 0.26992475986480713
train_iter_loss: 0.2282232791185379
train_iter_loss: 0.22299161553382874
train_iter_loss: 0.28982049226760864
train_iter_loss: 0.21234267950057983
train_iter_loss: 0.18735168874263763
train_iter_loss: 0.23863191902637482
train_iter_loss: 0.10044163465499878
train_iter_loss: 0.3149817883968353
train_iter_loss: 0.20094576478004456
train_iter_loss: 0.25510281324386597
train_iter_loss: 0.32603830099105835
train_iter_loss: 0.22806964814662933
train_iter_loss: 0.2357998937368393
train_iter_loss: 0.17666612565517426
train_iter_loss: 0.20059515535831451
train_iter_loss: 0.3026489019393921
train_iter_loss: 0.16588012874126434
train_iter_loss: 0.22011150419712067
train_iter_loss: 0.40743061900138855
train_iter_loss: 0.18997539579868317
train_iter_loss: 0.13248129189014435
train_iter_loss: 0.25239285826683044
train_iter_loss: 0.15654903650283813
train_iter_loss: 0.23961690068244934
train_iter_loss: 0.35866573452949524
train_iter_loss: 0.43576279282569885
train_iter_loss: 0.2265060991048813
train_iter_loss: 0.19163082540035248
train_iter_loss: 0.26410168409347534
train_iter_loss: 0.45626524090766907
train_iter_loss: 0.1722634732723236
train_iter_loss: 0.23107066750526428
train_iter_loss: 0.3788659870624542
train_iter_loss: 0.21304437518119812
train_iter_loss: 0.30280473828315735
train_iter_loss: 0.27329859137535095
train_iter_loss: 0.20739850401878357
train_iter_loss: 0.17291870713233948
train_iter_loss: 0.3782549798488617
train_iter_loss: 0.2846260964870453
train_iter_loss: 0.19628697633743286
train_iter_loss: 0.20343439280986786
train_iter_loss: 0.2587195634841919
train_iter_loss: 0.15493887662887573
train_iter_loss: 0.3817930519580841
train_iter_loss: 0.2301054298877716
train_iter_loss: 0.32434961199760437
train_iter_loss: 0.40663328766822815
train_iter_loss: 0.3671703636646271
train_iter_loss: 0.2171248346567154
train_iter_loss: 0.17321336269378662
train_iter_loss: 0.2106054574251175
train_iter_loss: 0.1852244883775711
train_iter_loss: 0.3141091763973236
train_iter_loss: 0.22432641685009003
train_iter_loss: 0.34789514541625977
train_iter_loss: 0.42111971974372864
train_iter_loss: 0.3282195031642914
train_iter_loss: 0.5192858576774597
train_iter_loss: 0.3009035885334015
train_iter_loss: 0.21584005653858185
train_iter_loss: 0.2529538571834564
train loss :0.2750
---------------------
Validation seg loss: 0.3519182956057056 at epoch 385
epoch =    386/  1000, exp = train
train_iter_loss: 0.1673055738210678
train_iter_loss: 0.18443743884563446
train_iter_loss: 0.301531046628952
train_iter_loss: 0.31394943594932556
train_iter_loss: 0.2586817443370819
train_iter_loss: 0.28085243701934814
train_iter_loss: 0.23891451954841614
train_iter_loss: 0.14562472701072693
train_iter_loss: 0.2156045138835907
train_iter_loss: 0.4258178770542145
train_iter_loss: 0.21620848774909973
train_iter_loss: 0.2633918523788452
train_iter_loss: 0.2574172914028168
train_iter_loss: 0.2668328881263733
train_iter_loss: 0.397882878780365
train_iter_loss: 0.42007923126220703
train_iter_loss: 0.19639354944229126
train_iter_loss: 0.3867095112800598
train_iter_loss: 0.37508201599121094
train_iter_loss: 0.30211141705513
train_iter_loss: 0.4210679531097412
train_iter_loss: 0.3697090446949005
train_iter_loss: 0.18339166045188904
train_iter_loss: 0.3229745626449585
train_iter_loss: 0.42765793204307556
train_iter_loss: 0.26716649532318115
train_iter_loss: 0.3206894099712372
train_iter_loss: 0.22059763967990875
train_iter_loss: 0.15259024500846863
train_iter_loss: 0.16689404845237732
train_iter_loss: 0.12905387580394745
train_iter_loss: 0.32192689180374146
train_iter_loss: 0.2693929374217987
train_iter_loss: 0.2578493356704712
train_iter_loss: 0.16947436332702637
train_iter_loss: 0.20398648083209991
train_iter_loss: 0.21197105944156647
train_iter_loss: 0.4380553662776947
train_iter_loss: 0.15707315504550934
train_iter_loss: 0.22657394409179688
train_iter_loss: 0.22831571102142334
train_iter_loss: 0.28750869631767273
train_iter_loss: 0.4353145360946655
train_iter_loss: 0.19600358605384827
train_iter_loss: 0.2634086310863495
train_iter_loss: 0.0926973894238472
train_iter_loss: 0.3711817264556885
train_iter_loss: 0.3745442032814026
train_iter_loss: 0.2405257374048233
train_iter_loss: 0.26888784766197205
train_iter_loss: 0.21280667185783386
train_iter_loss: 0.28837552666664124
train_iter_loss: 0.1681421846151352
train_iter_loss: 0.2594412863254547
train_iter_loss: 0.11420182883739471
train_iter_loss: 0.1896388977766037
train_iter_loss: 0.20801226794719696
train_iter_loss: 0.047137025743722916
train_iter_loss: 0.38572028279304504
train_iter_loss: 0.28759729862213135
train_iter_loss: 0.257957398891449
train_iter_loss: 0.26972830295562744
train_iter_loss: 0.33410823345184326
train_iter_loss: 0.28126078844070435
train_iter_loss: 0.1578996479511261
train_iter_loss: 0.17706725001335144
train_iter_loss: 0.21161580085754395
train_iter_loss: 0.26960915327072144
train_iter_loss: 0.26422378420829773
train_iter_loss: 0.4041878283023834
train_iter_loss: 0.35070374608039856
train_iter_loss: 0.33352336287498474
train_iter_loss: 0.2936038672924042
train_iter_loss: 0.22325953841209412
train_iter_loss: 0.2748434245586395
train_iter_loss: 0.16222041845321655
train_iter_loss: 0.29771989583969116
train_iter_loss: 0.30141690373420715
train_iter_loss: 0.3593685030937195
train_iter_loss: 0.24067236483097076
train_iter_loss: 0.2782936692237854
train_iter_loss: 0.27261248230934143
train_iter_loss: 0.348271906375885
train_iter_loss: 0.16815903782844543
train_iter_loss: 0.5209954380989075
train_iter_loss: 0.37333735823631287
train_iter_loss: 0.3333173394203186
train_iter_loss: 0.3233708143234253
train_iter_loss: 0.3611869513988495
train_iter_loss: 0.24235348403453827
train_iter_loss: 0.2234606295824051
train_iter_loss: 0.2762530446052551
train_iter_loss: 0.20813964307308197
train_iter_loss: 0.4574325680732727
train_iter_loss: 0.28599119186401367
train_iter_loss: 0.3899292051792145
train_iter_loss: 0.2140653133392334
train_iter_loss: 0.21944604814052582
train_iter_loss: 0.30126702785491943
train_iter_loss: 0.23505041003227234
train loss :0.2755
---------------------
Validation seg loss: 0.35135900691362487 at epoch 386
epoch =    387/  1000, exp = train
train_iter_loss: 0.26731106638908386
train_iter_loss: 0.149586021900177
train_iter_loss: 0.2509758770465851
train_iter_loss: 0.410916805267334
train_iter_loss: 0.3636418282985687
train_iter_loss: 0.3291410505771637
train_iter_loss: 0.37374356389045715
train_iter_loss: 0.31739041209220886
train_iter_loss: 0.33665555715560913
train_iter_loss: 0.26459258794784546
train_iter_loss: 0.2770789861679077
train_iter_loss: 0.20425677299499512
train_iter_loss: 0.20945732295513153
train_iter_loss: 0.23979875445365906
train_iter_loss: 0.2941427528858185
train_iter_loss: 0.13959939777851105
train_iter_loss: 0.27884986996650696
train_iter_loss: 0.16336457431316376
train_iter_loss: 0.21491777896881104
train_iter_loss: 0.3156004846096039
train_iter_loss: 0.16451454162597656
train_iter_loss: 0.4281052350997925
train_iter_loss: 0.12684950232505798
train_iter_loss: 0.3405727446079254
train_iter_loss: 0.5227504372596741
train_iter_loss: 0.3068064749240875
train_iter_loss: 0.3705389201641083
train_iter_loss: 0.12705448269844055
train_iter_loss: 0.2384546548128128
train_iter_loss: 0.367386132478714
train_iter_loss: 0.22632847726345062
train_iter_loss: 0.3637080490589142
train_iter_loss: 0.1893519163131714
train_iter_loss: 0.2612009644508362
train_iter_loss: 0.2604410648345947
train_iter_loss: 0.3375191390514374
train_iter_loss: 0.37413790822029114
train_iter_loss: 0.25563380122184753
train_iter_loss: 0.2946118116378784
train_iter_loss: 0.3979574739933014
train_iter_loss: 0.419023334980011
train_iter_loss: 0.3804011642932892
train_iter_loss: 0.34401050209999084
train_iter_loss: 0.3489820659160614
train_iter_loss: 0.377759724855423
train_iter_loss: 0.3296210765838623
train_iter_loss: 0.24017783999443054
train_iter_loss: 0.3397294878959656
train_iter_loss: 0.3615601658821106
train_iter_loss: 0.1876533478498459
train_iter_loss: 0.10159490257501602
train_iter_loss: 0.3065667152404785
train_iter_loss: 0.30365100502967834
train_iter_loss: 0.05289090424776077
train_iter_loss: 0.34804201126098633
train_iter_loss: 0.1683967560529709
train_iter_loss: 0.2715383470058441
train_iter_loss: 0.2604764699935913
train_iter_loss: 0.1623370349407196
train_iter_loss: 0.25715410709381104
train_iter_loss: 0.20798301696777344
train_iter_loss: 0.3401399254798889
train_iter_loss: 0.1458018571138382
train_iter_loss: 0.36844706535339355
train_iter_loss: 0.34199759364128113
train_iter_loss: 0.17630521953105927
train_iter_loss: 0.23680943250656128
train_iter_loss: 0.22206391394138336
train_iter_loss: 0.10399860888719559
train_iter_loss: 0.371759831905365
train_iter_loss: 0.3038003146648407
train_iter_loss: 0.2878439724445343
train_iter_loss: 0.29301732778549194
train_iter_loss: 0.0877530500292778
train_iter_loss: 0.3686715364456177
train_iter_loss: 0.3982541561126709
train_iter_loss: 0.34049975872039795
train_iter_loss: 0.12318748980760574
train_iter_loss: 0.42869940400123596
train_iter_loss: 0.3461291491985321
train_iter_loss: 0.27141469717025757
train_iter_loss: 0.1515861302614212
train_iter_loss: 0.286957710981369
train_iter_loss: 0.15362860262393951
train_iter_loss: 0.2821521461009979
train_iter_loss: 0.19978561997413635
train_iter_loss: 0.31196802854537964
train_iter_loss: 0.4335150718688965
train_iter_loss: 0.3440260589122772
train_iter_loss: 0.24926507472991943
train_iter_loss: 0.11880296468734741
train_iter_loss: 0.34900304675102234
train_iter_loss: 0.23076456785202026
train_iter_loss: 0.29223087430000305
train_iter_loss: 0.23493976891040802
train_iter_loss: 0.37700825929641724
train_iter_loss: 0.25665050745010376
train_iter_loss: 0.2440730184316635
train_iter_loss: 0.3164120614528656
train_iter_loss: 0.1999027132987976
train loss :0.2799
---------------------
Validation seg loss: 0.36625123168078233 at epoch 387
epoch =    388/  1000, exp = train
train_iter_loss: 0.31118878722190857
train_iter_loss: 0.2761721611022949
train_iter_loss: 0.24933138489723206
train_iter_loss: 0.32284533977508545
train_iter_loss: 0.27348509430885315
train_iter_loss: 0.24314674735069275
train_iter_loss: 0.23903274536132812
train_iter_loss: 0.2948235869407654
train_iter_loss: 0.27799031138420105
train_iter_loss: 0.2823408544063568
train_iter_loss: 0.4134970009326935
train_iter_loss: 0.2331043779850006
train_iter_loss: 0.3072918951511383
train_iter_loss: 0.25615194439888
train_iter_loss: 0.5198498368263245
train_iter_loss: 0.1443246454000473
train_iter_loss: 0.14190274477005005
train_iter_loss: 0.18940392136573792
train_iter_loss: 0.34650513529777527
train_iter_loss: 0.17592361569404602
train_iter_loss: 0.2413138598203659
train_iter_loss: 0.2733394503593445
train_iter_loss: 0.1185743510723114
train_iter_loss: 0.3169441819190979
train_iter_loss: 0.13275717198848724
train_iter_loss: 0.3473922908306122
train_iter_loss: 0.2018115371465683
train_iter_loss: 0.3475072383880615
train_iter_loss: 0.24606971442699432
train_iter_loss: 0.3486945927143097
train_iter_loss: 0.1277625560760498
train_iter_loss: 0.27451807260513306
train_iter_loss: 0.26588761806488037
train_iter_loss: 0.15075933933258057
train_iter_loss: 0.2589573860168457
train_iter_loss: 0.206096351146698
train_iter_loss: 0.18396005034446716
train_iter_loss: 0.3057044744491577
train_iter_loss: 0.21442803740501404
train_iter_loss: 0.32074686884880066
train_iter_loss: 0.3820650577545166
train_iter_loss: 0.23072750866413116
train_iter_loss: 0.31058937311172485
train_iter_loss: 0.18870596587657928
train_iter_loss: 0.32177966833114624
train_iter_loss: 0.3560273051261902
train_iter_loss: 0.2938787639141083
train_iter_loss: 0.21416504681110382
train_iter_loss: 0.218321293592453
train_iter_loss: 0.1416865885257721
train_iter_loss: 0.4167288839817047
train_iter_loss: 0.2539506256580353
train_iter_loss: 0.3759501874446869
train_iter_loss: 0.38041365146636963
train_iter_loss: 0.21538196504116058
train_iter_loss: 0.21255825459957123
train_iter_loss: 0.348859578371048
train_iter_loss: 0.35201165080070496
train_iter_loss: 0.3191152811050415
train_iter_loss: 0.3224203884601593
train_iter_loss: 0.1811099350452423
train_iter_loss: 0.28067076206207275
train_iter_loss: 0.40573370456695557
train_iter_loss: 0.30481112003326416
train_iter_loss: 0.19776591658592224
train_iter_loss: 0.21394751965999603
train_iter_loss: 0.2147492915391922
train_iter_loss: 0.12299665063619614
train_iter_loss: 0.2800147831439972
train_iter_loss: 0.30412402749061584
train_iter_loss: 0.19137801229953766
train_iter_loss: 0.3011273741722107
train_iter_loss: 0.2553527057170868
train_iter_loss: 0.3282088041305542
train_iter_loss: 0.2395036369562149
train_iter_loss: 0.23013249039649963
train_iter_loss: 0.1214047372341156
train_iter_loss: 0.06451696902513504
train_iter_loss: 0.3069727420806885
train_iter_loss: 0.1766812652349472
train_iter_loss: 0.39584866166114807
train_iter_loss: 0.14281439781188965
train_iter_loss: 0.4224631190299988
train_iter_loss: 0.27562037110328674
train_iter_loss: 0.32741987705230713
train_iter_loss: 0.14600437879562378
train_iter_loss: 0.38120603561401367
train_iter_loss: 0.24454206228256226
train_iter_loss: 0.2681797742843628
train_iter_loss: 0.2875247597694397
train_iter_loss: 0.2933247685432434
train_iter_loss: 0.2804444432258606
train_iter_loss: 0.42314499616622925
train_iter_loss: 0.23966409265995026
train_iter_loss: 0.18701396882534027
train_iter_loss: 0.27267763018608093
train_iter_loss: 0.28106123208999634
train_iter_loss: 0.3049813508987427
train_iter_loss: 0.296992689371109
train_iter_loss: 0.14824272692203522
train loss :0.2686
---------------------
Validation seg loss: 0.42661226317637935 at epoch 388
epoch =    389/  1000, exp = train
train_iter_loss: 0.51374751329422
train_iter_loss: 0.42892250418663025
train_iter_loss: 0.1524764448404312
train_iter_loss: 0.3642346262931824
train_iter_loss: 0.21290640532970428
train_iter_loss: 0.03956407308578491
train_iter_loss: 0.2750900685787201
train_iter_loss: 0.3446725010871887
train_iter_loss: 0.3110925853252411
train_iter_loss: 0.1258368045091629
train_iter_loss: 0.17228659987449646
train_iter_loss: 0.351496160030365
train_iter_loss: 0.44362592697143555
train_iter_loss: 0.2562715411186218
train_iter_loss: 0.17395399510860443
train_iter_loss: 0.1951712667942047
train_iter_loss: 0.20801959931850433
train_iter_loss: 0.129621222615242
train_iter_loss: 0.2573564648628235
train_iter_loss: 0.2416280061006546
train_iter_loss: 0.14403699338436127
train_iter_loss: 0.3583044409751892
train_iter_loss: 0.2567526400089264
train_iter_loss: 0.3089769780635834
train_iter_loss: 0.24359393119812012
train_iter_loss: 0.27142423391342163
train_iter_loss: 0.1802366077899933
train_iter_loss: 0.35137343406677246
train_iter_loss: 0.17927449941635132
train_iter_loss: 0.2664549946784973
train_iter_loss: 0.36117053031921387
train_iter_loss: 0.22523213922977448
train_iter_loss: 0.22593998908996582
train_iter_loss: 0.21250136196613312
train_iter_loss: 0.22730909287929535
train_iter_loss: 0.2653343975543976
train_iter_loss: 0.3045981824398041
train_iter_loss: 0.34377384185791016
train_iter_loss: 0.31853795051574707
train_iter_loss: 0.15747126936912537
train_iter_loss: 0.40631112456321716
train_iter_loss: 0.13588687777519226
train_iter_loss: 0.19427858293056488
train_iter_loss: 0.2893754541873932
train_iter_loss: 0.23999032378196716
train_iter_loss: 0.27234187722206116
train_iter_loss: 0.35466644167900085
train_iter_loss: 0.11072880029678345
train_iter_loss: 0.4391879141330719
train_iter_loss: 0.24912941455841064
train_iter_loss: 0.20401237905025482
train_iter_loss: 0.2422359436750412
train_iter_loss: 0.18609702587127686
train_iter_loss: 0.3403686285018921
train_iter_loss: 0.2239079475402832
train_iter_loss: 0.27645406126976013
train_iter_loss: 0.23386815190315247
train_iter_loss: 0.3313398063182831
train_iter_loss: 0.3277991712093353
train_iter_loss: 0.25949713587760925
train_iter_loss: 0.32916948199272156
train_iter_loss: 0.22609706223011017
train_iter_loss: 0.3081238269805908
train_iter_loss: 0.25334909558296204
train_iter_loss: 0.348272442817688
train_iter_loss: 0.2671263813972473
train_iter_loss: 0.1346636712551117
train_iter_loss: 0.20421656966209412
train_iter_loss: 0.16183215379714966
train_iter_loss: 0.4161977171897888
train_iter_loss: 0.3266856074333191
train_iter_loss: 0.29421254992485046
train_iter_loss: 0.17936192452907562
train_iter_loss: 0.30148351192474365
train_iter_loss: 0.44784897565841675
train_iter_loss: 0.4286248981952667
train_iter_loss: 0.35097813606262207
train_iter_loss: 0.22937841713428497
train_iter_loss: 0.27641335129737854
train_iter_loss: 0.280362993478775
train_iter_loss: 0.13297460973262787
train_iter_loss: 0.3161758780479431
train_iter_loss: 0.24951446056365967
train_iter_loss: 0.3182450234889984
train_iter_loss: 0.20877356827259064
train_iter_loss: 0.3145945072174072
train_iter_loss: 0.3291756212711334
train_iter_loss: 0.405803918838501
train_iter_loss: 0.31261464953422546
train_iter_loss: 0.1942216008901596
train_iter_loss: 0.35972726345062256
train_iter_loss: 0.31543153524398804
train_iter_loss: 0.2352503091096878
train_iter_loss: 0.2615756690502167
train_iter_loss: 0.35021254420280457
train_iter_loss: 0.27725285291671753
train_iter_loss: 0.2188805788755417
train_iter_loss: 0.22859853506088257
train_iter_loss: 0.25658118724823
train_iter_loss: 0.3029577434062958
train loss :0.2734
---------------------
Validation seg loss: 0.3807148990344326 at epoch 389
epoch =    390/  1000, exp = train
train_iter_loss: 0.24187156558036804
train_iter_loss: 0.20953233540058136
train_iter_loss: 0.07354255765676498
train_iter_loss: 0.5367639660835266
train_iter_loss: 0.2313983291387558
train_iter_loss: 0.140783429145813
train_iter_loss: 0.45779943466186523
train_iter_loss: 0.23028500378131866
train_iter_loss: 0.2553046941757202
train_iter_loss: 0.4122011661529541
train_iter_loss: 0.30800333619117737
train_iter_loss: 0.23582394421100616
train_iter_loss: 0.24690692126750946
train_iter_loss: 0.39125052094459534
train_iter_loss: 0.20008675754070282
train_iter_loss: 0.27167171239852905
train_iter_loss: 0.25284653902053833
train_iter_loss: 0.20667803287506104
train_iter_loss: 0.3086983561515808
train_iter_loss: 0.2635852098464966
train_iter_loss: 0.1885991394519806
train_iter_loss: 0.3592391014099121
train_iter_loss: 0.398056298494339
train_iter_loss: 0.28485485911369324
train_iter_loss: 0.3490092158317566
train_iter_loss: 0.2150566130876541
train_iter_loss: 0.14178979396820068
train_iter_loss: 0.2831217050552368
train_iter_loss: 0.2442750185728073
train_iter_loss: 0.21825875341892242
train_iter_loss: 0.29312294721603394
train_iter_loss: 0.16392286121845245
train_iter_loss: 0.2376192808151245
train_iter_loss: 0.25197121500968933
train_iter_loss: 0.19294357299804688
train_iter_loss: 0.29764461517333984
train_iter_loss: 0.18046998977661133
train_iter_loss: 0.28307583928108215
train_iter_loss: 0.20296251773834229
train_iter_loss: 0.27579206228256226
train_iter_loss: 0.40197843313217163
train_iter_loss: 0.20627270638942719
train_iter_loss: 0.18634361028671265
train_iter_loss: 0.23519866168498993
train_iter_loss: 0.30288881063461304
train_iter_loss: 0.28971925377845764
train_iter_loss: 0.27641868591308594
train_iter_loss: 0.16771602630615234
train_iter_loss: 0.13249944150447845
train_iter_loss: 0.23195941746234894
train_iter_loss: 0.3537014126777649
train_iter_loss: 0.14007063210010529
train_iter_loss: 0.40007492899894714
train_iter_loss: 0.2711542844772339
train_iter_loss: 0.3081746995449066
train_iter_loss: 0.22629870474338531
train_iter_loss: 0.38390034437179565
train_iter_loss: 0.15240834653377533
train_iter_loss: 0.3451092839241028
train_iter_loss: 0.3136437237262726
train_iter_loss: 0.33999401330947876
train_iter_loss: 0.3327178359031677
train_iter_loss: 0.09215384721755981
train_iter_loss: 0.23976625502109528
train_iter_loss: 0.3776512145996094
train_iter_loss: 0.20822426676750183
train_iter_loss: 0.3569774031639099
train_iter_loss: 0.2349325567483902
train_iter_loss: 0.2622520327568054
train_iter_loss: 0.26874420046806335
train_iter_loss: 0.3279646039009094
train_iter_loss: 0.3166373670101166
train_iter_loss: 0.2011639028787613
train_iter_loss: 0.33445286750793457
train_iter_loss: 0.181877002120018
train_iter_loss: 0.43500635027885437
train_iter_loss: 0.26471182703971863
train_iter_loss: 0.23358584940433502
train_iter_loss: 0.3094053268432617
train_iter_loss: 0.2849036753177643
train_iter_loss: 0.2522087097167969
train_iter_loss: 0.1853722333908081
train_iter_loss: 0.14375415444374084
train_iter_loss: 0.19444993138313293
train_iter_loss: 0.4356842637062073
train_iter_loss: 0.34859946370124817
train_iter_loss: 0.23021215200424194
train_iter_loss: 0.2550899386405945
train_iter_loss: 0.3221462666988373
train_iter_loss: 0.38133546710014343
train_iter_loss: 0.5572702288627625
train_iter_loss: 0.34197118878364563
train_iter_loss: 0.2558625042438507
train_iter_loss: 0.37390559911727905
train_iter_loss: 0.11113382130861282
train_iter_loss: 0.2641970217227936
train_iter_loss: 0.20498697459697723
train_iter_loss: 0.3598199784755707
train_iter_loss: 0.3404460549354553
train_iter_loss: 0.2403331995010376
train loss :0.2754
---------------------
Validation seg loss: 0.42558979844885336 at epoch 390
epoch =    391/  1000, exp = train
train_iter_loss: 0.31782546639442444
train_iter_loss: 0.21765530109405518
train_iter_loss: 0.45892345905303955
train_iter_loss: 0.4083254039287567
train_iter_loss: 0.22092266380786896
train_iter_loss: 0.3432773947715759
train_iter_loss: 0.31006693840026855
train_iter_loss: 0.4782675504684448
train_iter_loss: 0.12362448126077652
train_iter_loss: 0.258620023727417
train_iter_loss: 0.3049486577510834
train_iter_loss: 0.1941760629415512
train_iter_loss: 0.21503598988056183
train_iter_loss: 0.2980160713195801
train_iter_loss: 0.28626883029937744
train_iter_loss: 0.26280689239501953
train_iter_loss: 0.2409868985414505
train_iter_loss: 0.12499019503593445
train_iter_loss: 0.3271511495113373
train_iter_loss: 0.3889290690422058
train_iter_loss: 0.1965448409318924
train_iter_loss: 0.3037588894367218
train_iter_loss: 0.30539053678512573
train_iter_loss: 0.3608061671257019
train_iter_loss: 0.24176804721355438
train_iter_loss: 0.28885868191719055
train_iter_loss: 0.3823108375072479
train_iter_loss: 0.35384637117385864
train_iter_loss: 0.18103566765785217
train_iter_loss: 0.31617769598960876
train_iter_loss: 0.37630388140678406
train_iter_loss: 0.3027927279472351
train_iter_loss: 0.3811570107936859
train_iter_loss: 0.20424041152000427
train_iter_loss: 0.38672345876693726
train_iter_loss: 0.27185383439064026
train_iter_loss: 0.4576108753681183
train_iter_loss: 0.3298030197620392
train_iter_loss: 0.24736329913139343
train_iter_loss: 0.27156922221183777
train_iter_loss: 0.24555030465126038
train_iter_loss: 0.270537406206131
train_iter_loss: 0.2353452891111374
train_iter_loss: 0.595522403717041
train_iter_loss: 0.31433507800102234
train_iter_loss: 0.21658317744731903
train_iter_loss: 0.385161817073822
train_iter_loss: 0.2388853132724762
train_iter_loss: 0.15453216433525085
train_iter_loss: 0.26549816131591797
train_iter_loss: 0.35872766375541687
train_iter_loss: 0.261035680770874
train_iter_loss: 0.1542992740869522
train_iter_loss: 0.22042475640773773
train_iter_loss: 0.27516067028045654
train_iter_loss: 0.21403631567955017
train_iter_loss: 0.26033732295036316
train_iter_loss: 0.2746080756187439
train_iter_loss: 0.34907010197639465
train_iter_loss: 0.23567479848861694
train_iter_loss: 0.20182408392429352
train_iter_loss: 0.5455973148345947
train_iter_loss: 0.1527753621339798
train_iter_loss: 0.2570335566997528
train_iter_loss: 0.12786900997161865
train_iter_loss: 0.2686757445335388
train_iter_loss: 0.23574519157409668
train_iter_loss: 0.36482441425323486
train_iter_loss: 0.20247136056423187
train_iter_loss: 0.3259255886077881
train_iter_loss: 0.282699316740036
train_iter_loss: 0.24575355648994446
train_iter_loss: 0.2849455177783966
train_iter_loss: 0.20385244488716125
train_iter_loss: 0.302958607673645
train_iter_loss: 0.33069297671318054
train_iter_loss: 0.1734754890203476
train_iter_loss: 0.3003489077091217
train_iter_loss: 0.4445829689502716
train_iter_loss: 0.23483344912528992
train_iter_loss: 0.24734386801719666
train_iter_loss: 0.28101587295532227
train_iter_loss: 0.206826314330101
train_iter_loss: 0.2547849714756012
train_iter_loss: 0.254597544670105
train_iter_loss: 0.187910258769989
train_iter_loss: 0.24229209125041962
train_iter_loss: 0.3461757302284241
train_iter_loss: 0.30082955956459045
train_iter_loss: 0.11550244688987732
train_iter_loss: 0.23886941373348236
train_iter_loss: 0.24565771222114563
train_iter_loss: 0.14149008691310883
train_iter_loss: 0.21471184492111206
train_iter_loss: 0.3585137128829956
train_iter_loss: 0.24575132131576538
train_iter_loss: 0.37100714445114136
train_iter_loss: 0.285165399312973
train_iter_loss: 0.20148475468158722
train_iter_loss: 0.2916513681411743
train loss :0.2826
---------------------
Validation seg loss: 0.36823650969649263 at epoch 391
epoch =    392/  1000, exp = train
train_iter_loss: 0.27332234382629395
train_iter_loss: 0.3638261556625366
train_iter_loss: 0.28409481048583984
train_iter_loss: 0.21848757565021515
train_iter_loss: 0.12598443031311035
train_iter_loss: 0.4149356186389923
train_iter_loss: 0.32841670513153076
train_iter_loss: 0.32556816935539246
train_iter_loss: 0.3914456367492676
train_iter_loss: 0.21878772974014282
train_iter_loss: 0.2627076506614685
train_iter_loss: 0.2664199769496918
train_iter_loss: 0.2836681008338928
train_iter_loss: 0.323271244764328
train_iter_loss: 0.25010982155799866
train_iter_loss: 0.2707609534263611
train_iter_loss: 0.2378435730934143
train_iter_loss: 0.24811697006225586
train_iter_loss: 0.27503320574760437
train_iter_loss: 0.42809024453163147
train_iter_loss: 0.27375537157058716
train_iter_loss: 0.3762548565864563
train_iter_loss: 0.3297651708126068
train_iter_loss: 0.17454646527767181
train_iter_loss: 0.2109661102294922
train_iter_loss: 0.1528070867061615
train_iter_loss: 0.17218482494354248
train_iter_loss: 0.2099846601486206
train_iter_loss: 0.31615638732910156
train_iter_loss: 0.23956264555454254
train_iter_loss: 0.20460137724876404
train_iter_loss: 0.34055185317993164
train_iter_loss: 0.2404082864522934
train_iter_loss: 0.26293012499809265
train_iter_loss: 0.4258536696434021
train_iter_loss: 0.3370879292488098
train_iter_loss: 0.3346884250640869
train_iter_loss: 0.22375543415546417
train_iter_loss: 0.30902785062789917
train_iter_loss: 0.41314586997032166
train_iter_loss: 0.2888581454753876
train_iter_loss: 0.2801513969898224
train_iter_loss: 0.17590346932411194
train_iter_loss: 0.20043113827705383
train_iter_loss: 0.2684043347835541
train_iter_loss: 0.4670882225036621
train_iter_loss: 0.19565322995185852
train_iter_loss: 0.1757667362689972
train_iter_loss: 0.20073629915714264
train_iter_loss: 0.24762848019599915
train_iter_loss: 0.34659841656684875
train_iter_loss: 0.24885405600070953
train_iter_loss: 0.365317165851593
train_iter_loss: 0.4472469687461853
train_iter_loss: 0.20462147891521454
train_iter_loss: 0.31686100363731384
train_iter_loss: 0.3892163634300232
train_iter_loss: 0.19138626754283905
train_iter_loss: 0.3543597459793091
train_iter_loss: 0.21790176630020142
train_iter_loss: 0.22512514889240265
train_iter_loss: 0.23487094044685364
train_iter_loss: 0.17334114015102386
train_iter_loss: 0.4042200744152069
train_iter_loss: 0.34024298191070557
train_iter_loss: 0.28809690475463867
train_iter_loss: 0.1282825469970703
train_iter_loss: 0.2939939796924591
train_iter_loss: 0.28113171458244324
train_iter_loss: 0.28637731075286865
train_iter_loss: 0.3835296928882599
train_iter_loss: 0.4137289822101593
train_iter_loss: 0.23606300354003906
train_iter_loss: 0.372579962015152
train_iter_loss: 0.259628564119339
train_iter_loss: 0.24822410941123962
train_iter_loss: 0.35514533519744873
train_iter_loss: 0.27456384897232056
train_iter_loss: 0.24271173775196075
train_iter_loss: 0.2633315920829773
train_iter_loss: 0.35692551732063293
train_iter_loss: 0.17518331110477448
train_iter_loss: 0.23555566370487213
train_iter_loss: 0.23803289234638214
train_iter_loss: 0.35161691904067993
train_iter_loss: 0.19274766743183136
train_iter_loss: 0.19065873324871063
train_iter_loss: 0.10855195671319962
train_iter_loss: 0.22783465683460236
train_iter_loss: 0.2605889141559601
train_iter_loss: 0.21723127365112305
train_iter_loss: 0.16975094377994537
train_iter_loss: 0.28293976187705994
train_iter_loss: 0.22679954767227173
train_iter_loss: 0.4551662504673004
train_iter_loss: 0.21975241601467133
train_iter_loss: 0.15758129954338074
train_iter_loss: 0.2293573021888733
train_iter_loss: 0.2745516300201416
train_iter_loss: 0.26956138014793396
train loss :0.2775
---------------------
Validation seg loss: 0.3509678536200158 at epoch 392
epoch =    393/  1000, exp = train
train_iter_loss: 0.2766416668891907
train_iter_loss: 0.30804476141929626
train_iter_loss: 0.2568315863609314
train_iter_loss: 0.2654169797897339
train_iter_loss: 0.3862408399581909
train_iter_loss: 0.23976555466651917
train_iter_loss: 0.08615286648273468
train_iter_loss: 0.29338768124580383
train_iter_loss: 0.20394647121429443
train_iter_loss: 0.27051398158073425
train_iter_loss: 0.16290032863616943
train_iter_loss: 0.5928880572319031
train_iter_loss: 0.210133358836174
train_iter_loss: 0.22971871495246887
train_iter_loss: 0.2742372155189514
train_iter_loss: 0.16710308194160461
train_iter_loss: 0.21674177050590515
train_iter_loss: 0.35757818818092346
train_iter_loss: 0.21895349025726318
train_iter_loss: 0.4434691369533539
train_iter_loss: 0.1801815778017044
train_iter_loss: 0.25514116883277893
train_iter_loss: 0.2963477075099945
train_iter_loss: 0.20567424595355988
train_iter_loss: 0.13312077522277832
train_iter_loss: 0.28770363330841064
train_iter_loss: 0.22946178913116455
train_iter_loss: 0.21892289817333221
train_iter_loss: 0.1224130168557167
train_iter_loss: 0.30926012992858887
train_iter_loss: 0.28406259417533875
train_iter_loss: 0.15804660320281982
train_iter_loss: 0.2704056203365326
train_iter_loss: 0.1976553052663803
train_iter_loss: 0.46495553851127625
train_iter_loss: 0.4502326548099518
train_iter_loss: 0.2746790051460266
train_iter_loss: 0.29493609070777893
train_iter_loss: 0.3255538046360016
train_iter_loss: 0.3577212691307068
train_iter_loss: 0.1860230565071106
train_iter_loss: 0.5872193574905396
train_iter_loss: 0.13726122677326202
train_iter_loss: 0.39039796590805054
train_iter_loss: 0.46114954352378845
train_iter_loss: 0.1473267376422882
train_iter_loss: 0.36399492621421814
train_iter_loss: 0.28928545117378235
train_iter_loss: 0.23409099876880646
train_iter_loss: 0.31784602999687195
train_iter_loss: 0.27029597759246826
train_iter_loss: 0.21797887980937958
train_iter_loss: 0.21310676634311676
train_iter_loss: 0.37722134590148926
train_iter_loss: 0.2331772893667221
train_iter_loss: 0.2963980734348297
train_iter_loss: 0.169070303440094
train_iter_loss: 0.28580355644226074
train_iter_loss: 0.22982662916183472
train_iter_loss: 0.36956390738487244
train_iter_loss: 0.4159267842769623
train_iter_loss: 0.35206273198127747
train_iter_loss: 0.3012714087963104
train_iter_loss: 0.3216133713722229
train_iter_loss: 0.43937164545059204
train_iter_loss: 0.3216439187526703
train_iter_loss: 0.34209132194519043
train_iter_loss: 0.22715742886066437
train_iter_loss: 0.13578277826309204
train_iter_loss: 0.3071044385433197
train_iter_loss: 0.19204126298427582
train_iter_loss: 0.4030039608478546
train_iter_loss: 0.2684929370880127
train_iter_loss: 0.473029762506485
train_iter_loss: 0.17253568768501282
train_iter_loss: 0.2718479037284851
train_iter_loss: 0.3486466109752655
train_iter_loss: 0.11076865345239639
train_iter_loss: 0.1935645490884781
train_iter_loss: 0.34056955575942993
train_iter_loss: 0.24321605265140533
train_iter_loss: 0.24086043238639832
train_iter_loss: 0.27555209398269653
train_iter_loss: 0.22934605181217194
train_iter_loss: 0.2485998272895813
train_iter_loss: 0.23302674293518066
train_iter_loss: 0.38061127066612244
train_iter_loss: 0.3568015992641449
train_iter_loss: 0.3015628755092621
train_iter_loss: 0.23758676648139954
train_iter_loss: 0.2533760368824005
train_iter_loss: 0.2305862158536911
train_iter_loss: 0.3272501826286316
train_iter_loss: 0.23317575454711914
train_iter_loss: 0.22871385514736176
train_iter_loss: 0.40024319291114807
train_iter_loss: 0.14847464859485626
train_iter_loss: 0.2983289659023285
train_iter_loss: 0.3700389862060547
train_iter_loss: 0.2991185486316681
train loss :0.2831
---------------------
Validation seg loss: 0.3829505015076753 at epoch 393
epoch =    394/  1000, exp = train
train_iter_loss: 0.2112349569797516
train_iter_loss: 0.44097617268562317
train_iter_loss: 0.4281252920627594
train_iter_loss: 0.3281099200248718
train_iter_loss: 0.25322678685188293
train_iter_loss: 0.5677287578582764
train_iter_loss: 0.27526819705963135
train_iter_loss: 0.28147706389427185
train_iter_loss: 0.21418417990207672
train_iter_loss: 0.2548369765281677
train_iter_loss: 0.1620493233203888
train_iter_loss: 0.23492483794689178
train_iter_loss: 0.24875369668006897
train_iter_loss: 0.31169959902763367
train_iter_loss: 0.25404876470565796
train_iter_loss: 0.23275066912174225
train_iter_loss: 0.16721665859222412
train_iter_loss: 0.18712587654590607
train_iter_loss: 0.2689136564731598
train_iter_loss: 0.29786548018455505
train_iter_loss: 0.1295143961906433
train_iter_loss: 0.4478372037410736
train_iter_loss: 0.24486491084098816
train_iter_loss: 0.1500897854566574
train_iter_loss: 0.3201083242893219
train_iter_loss: 0.23844599723815918
train_iter_loss: 0.22510762512683868
train_iter_loss: 0.1979179084300995
train_iter_loss: 0.256550669670105
train_iter_loss: 0.16286754608154297
train_iter_loss: 0.21439701318740845
train_iter_loss: 0.25566181540489197
train_iter_loss: 0.21213467419147491
train_iter_loss: 0.45045891404151917
train_iter_loss: 0.25423097610473633
train_iter_loss: 0.23898981511592865
train_iter_loss: 0.21626532077789307
train_iter_loss: 0.30255237221717834
train_iter_loss: 0.19743062555789948
train_iter_loss: 0.2241484671831131
train_iter_loss: 0.2300088107585907
train_iter_loss: 0.21225135028362274
train_iter_loss: 0.19420753419399261
train_iter_loss: 0.29666924476623535
train_iter_loss: 0.44997522234916687
train_iter_loss: 0.09265726059675217
train_iter_loss: 0.34070882201194763
train_iter_loss: 0.2687221169471741
train_iter_loss: 0.27938902378082275
train_iter_loss: 0.22487539052963257
train_iter_loss: 0.27518510818481445
train_iter_loss: 0.12318085879087448
train_iter_loss: 0.1990172415971756
train_iter_loss: 0.2102726846933365
train_iter_loss: 0.09021155536174774
train_iter_loss: 0.38442811369895935
train_iter_loss: 0.3055756688117981
train_iter_loss: 0.27243372797966003
train_iter_loss: 0.3358304500579834
train_iter_loss: 0.19607964158058167
train_iter_loss: 0.27662065625190735
train_iter_loss: 0.31555289030075073
train_iter_loss: 0.10347983241081238
train_iter_loss: 0.3534209728240967
train_iter_loss: 0.4024656414985657
train_iter_loss: 0.18033920228481293
train_iter_loss: 0.2708735466003418
train_iter_loss: 0.29731032252311707
train_iter_loss: 0.4314672350883484
train_iter_loss: 0.3680723309516907
train_iter_loss: 0.19496962428092957
train_iter_loss: 0.25715523958206177
train_iter_loss: 0.2956503629684448
train_iter_loss: 0.29311302304267883
train_iter_loss: 0.16782031953334808
train_iter_loss: 0.2441502958536148
train_iter_loss: 0.3934183716773987
train_iter_loss: 0.23824754357337952
train_iter_loss: 0.3071290850639343
train_iter_loss: 0.27794623374938965
train_iter_loss: 0.4796128571033478
train_iter_loss: 0.23898805677890778
train_iter_loss: 0.3585360050201416
train_iter_loss: 0.2034773975610733
train_iter_loss: 0.37296923995018005
train_iter_loss: 0.31165194511413574
train_iter_loss: 0.27221032977104187
train_iter_loss: 0.24183979630470276
train_iter_loss: 0.2977991998195648
train_iter_loss: 0.2532831132411957
train_iter_loss: 0.492802232503891
train_iter_loss: 0.37974604964256287
train_iter_loss: 0.728567361831665
train_iter_loss: 0.18896478414535522
train_iter_loss: 0.37955501675605774
train_iter_loss: 0.22690236568450928
train_iter_loss: 0.09091722220182419
train_iter_loss: 0.3744412958621979
train_iter_loss: 0.34520360827445984
train_iter_loss: 0.17913232743740082
train loss :0.2791
---------------------
Validation seg loss: 0.4062473400548663 at epoch 394
epoch =    395/  1000, exp = train
train_iter_loss: 0.28174200654029846
train_iter_loss: 0.14144451916217804
train_iter_loss: 0.22553108632564545
train_iter_loss: 0.257876992225647
train_iter_loss: 0.23414729535579681
train_iter_loss: 0.24821409583091736
train_iter_loss: 0.30587053298950195
train_iter_loss: 0.3496129512786865
train_iter_loss: 0.16870729625225067
train_iter_loss: 0.3720121681690216
train_iter_loss: 0.19915297627449036
train_iter_loss: 0.4174115061759949
train_iter_loss: 0.23654310405254364
train_iter_loss: 0.18453536927700043
train_iter_loss: 0.321093887090683
train_iter_loss: 0.3646283745765686
train_iter_loss: 0.2765735983848572
train_iter_loss: 0.23586969077587128
train_iter_loss: 0.2727484107017517
train_iter_loss: 0.19251714646816254
train_iter_loss: 0.21421250700950623
train_iter_loss: 0.16851460933685303
train_iter_loss: 0.30435922741889954
train_iter_loss: 0.41124197840690613
train_iter_loss: 0.3591442406177521
train_iter_loss: 0.23900598287582397
train_iter_loss: 0.3056192398071289
train_iter_loss: 0.3799799084663391
train_iter_loss: 0.259132981300354
train_iter_loss: 0.3064799904823303
train_iter_loss: 0.23714815080165863
train_iter_loss: 0.1876038759946823
train_iter_loss: 0.18183861672878265
train_iter_loss: 0.31273001432418823
train_iter_loss: 0.3291780650615692
train_iter_loss: 0.3657354414463043
train_iter_loss: 0.19943468272686005
train_iter_loss: 0.2660805881023407
train_iter_loss: 0.2536841630935669
train_iter_loss: 0.21448175609111786
train_iter_loss: 0.30581507086753845
train_iter_loss: 0.37171804904937744
train_iter_loss: 0.23503102362155914
train_iter_loss: 0.4140280783176422
train_iter_loss: 0.49571681022644043
train_iter_loss: 0.2510378956794739
train_iter_loss: 0.28971123695373535
train_iter_loss: 0.26083752512931824
train_iter_loss: 0.30995452404022217
train_iter_loss: 0.3446994125843048
train_iter_loss: 0.18360914289951324
train_iter_loss: 0.34234192967414856
train_iter_loss: 0.26845023036003113
train_iter_loss: 0.17450739443302155
train_iter_loss: 0.16459789872169495
train_iter_loss: 0.27374863624572754
train_iter_loss: 0.38928937911987305
train_iter_loss: 0.1849825233221054
train_iter_loss: 0.1952143907546997
train_iter_loss: 0.32349780201911926
train_iter_loss: 0.24898870289325714
train_iter_loss: 0.251413494348526
train_iter_loss: 0.4701271951198578
train_iter_loss: 0.3962410092353821
train_iter_loss: 0.3644424080848694
train_iter_loss: 0.21671967208385468
train_iter_loss: 0.1948629915714264
train_iter_loss: 0.51328045129776
train_iter_loss: 0.23459365963935852
train_iter_loss: 0.34884223341941833
train_iter_loss: 0.20137660205364227
train_iter_loss: 0.23222553730010986
train_iter_loss: 0.29310011863708496
train_iter_loss: 0.2801012694835663
train_iter_loss: 0.30147621035575867
train_iter_loss: 0.2522021234035492
train_iter_loss: 0.3093898594379425
train_iter_loss: 0.2346210479736328
train_iter_loss: 0.37241920828819275
train_iter_loss: 0.26365911960601807
train_iter_loss: 0.23218198120594025
train_iter_loss: 0.26865988969802856
train_iter_loss: 0.2381967157125473
train_iter_loss: 0.11915785074234009
train_iter_loss: 0.19883008301258087
train_iter_loss: 0.4488223195075989
train_iter_loss: 0.17803534865379333
train_iter_loss: 0.30574703216552734
train_iter_loss: 0.2917894423007965
train_iter_loss: 0.3731655776500702
train_iter_loss: 0.13972538709640503
train_iter_loss: 0.21247008442878723
train_iter_loss: 0.17169250547885895
train_iter_loss: 0.1266944706439972
train_iter_loss: 0.2767952084541321
train_iter_loss: 0.28357478976249695
train_iter_loss: 0.13566870987415314
train_iter_loss: 0.24496161937713623
train_iter_loss: 0.31086012721061707
train_iter_loss: 0.2652956247329712
train loss :0.2764
---------------------
Validation seg loss: 0.36452348330848905 at epoch 395
epoch =    396/  1000, exp = train
train_iter_loss: 0.5182775259017944
train_iter_loss: 0.37542688846588135
train_iter_loss: 0.2817545235157013
train_iter_loss: 0.2729749381542206
train_iter_loss: 0.34438058733940125
train_iter_loss: 0.2411392629146576
train_iter_loss: 0.23607906699180603
train_iter_loss: 0.2403557449579239
train_iter_loss: 0.34266170859336853
train_iter_loss: 0.40500813722610474
train_iter_loss: 0.38129836320877075
train_iter_loss: 0.37885233759880066
train_iter_loss: 0.23220868408679962
train_iter_loss: 0.11044265329837799
train_iter_loss: 0.402484655380249
train_iter_loss: 0.31557413935661316
train_iter_loss: 0.2988811433315277
train_iter_loss: 0.3111969530582428
train_iter_loss: 0.15290674567222595
train_iter_loss: 0.3123399019241333
train_iter_loss: 0.27455127239227295
train_iter_loss: 0.20906199514865875
train_iter_loss: 0.12829111516475677
train_iter_loss: 0.1938239485025406
train_iter_loss: 0.3546910881996155
train_iter_loss: 0.2554188072681427
train_iter_loss: 0.29841846227645874
train_iter_loss: 0.20885825157165527
train_iter_loss: 0.21510149538516998
train_iter_loss: 0.2370845228433609
train_iter_loss: 0.191348135471344
train_iter_loss: 0.4004306197166443
train_iter_loss: 0.04825371503829956
train_iter_loss: 0.5689479112625122
train_iter_loss: 0.1565614640712738
train_iter_loss: 0.27516037225723267
train_iter_loss: 0.258131742477417
train_iter_loss: 0.13899105787277222
train_iter_loss: 0.2717876732349396
train_iter_loss: 0.1604897677898407
train_iter_loss: 0.31281018257141113
train_iter_loss: 0.23787841200828552
train_iter_loss: 0.396456241607666
train_iter_loss: 0.253523588180542
train_iter_loss: 0.2083476334810257
train_iter_loss: 0.2740265429019928
train_iter_loss: 0.2713966965675354
train_iter_loss: 0.0749349296092987
train_iter_loss: 0.16568288207054138
train_iter_loss: 0.3370814621448517
train_iter_loss: 0.33490708470344543
train_iter_loss: 0.3430336117744446
train_iter_loss: 0.39409080147743225
train_iter_loss: 0.24310143291950226
train_iter_loss: 0.24751338362693787
train_iter_loss: 0.262234091758728
train_iter_loss: 0.2842872738838196
train_iter_loss: 0.3418852388858795
train_iter_loss: 0.2871570289134979
train_iter_loss: 0.37242263555526733
train_iter_loss: 0.2727526128292084
train_iter_loss: 0.2561284303665161
train_iter_loss: 0.10887723416090012
train_iter_loss: 0.30333074927330017
train_iter_loss: 0.3378502130508423
train_iter_loss: 0.17185211181640625
train_iter_loss: 0.26367923617362976
train_iter_loss: 0.1645696461200714
train_iter_loss: 0.2728300988674164
train_iter_loss: 0.29953473806381226
train_iter_loss: 0.2587473690509796
train_iter_loss: 0.34189119935035706
train_iter_loss: 0.2101244181394577
train_iter_loss: 0.2731473743915558
train_iter_loss: 0.24526438117027283
train_iter_loss: 0.28432735800743103
train_iter_loss: 0.19557151198387146
train_iter_loss: 0.2672889232635498
train_iter_loss: 0.2101464569568634
train_iter_loss: 0.3700961172580719
train_iter_loss: 0.27096477150917053
train_iter_loss: 0.23132860660552979
train_iter_loss: 0.3699336349964142
train_iter_loss: 0.3097046911716461
train_iter_loss: 0.332276314496994
train_iter_loss: 0.20142672955989838
train_iter_loss: 0.26998546719551086
train_iter_loss: 0.30393487215042114
train_iter_loss: 0.06289096176624298
train_iter_loss: 0.3232235610485077
train_iter_loss: 0.3301704525947571
train_iter_loss: 0.266220360994339
train_iter_loss: 0.2923254370689392
train_iter_loss: 0.24605713784694672
train_iter_loss: 0.2451164573431015
train_iter_loss: 0.35542887449264526
train_iter_loss: 0.3376855254173279
train_iter_loss: 0.2531098425388336
train_iter_loss: 0.2145397663116455
train_iter_loss: 0.3552665412425995
train loss :0.2760
---------------------
Validation seg loss: 0.3873428934657911 at epoch 396
epoch =    397/  1000, exp = train
train_iter_loss: 0.20312687754631042
train_iter_loss: 0.22085557878017426
train_iter_loss: 0.34898579120635986
train_iter_loss: 0.1721596121788025
train_iter_loss: 0.11534275859594345
train_iter_loss: 0.31022873520851135
train_iter_loss: 0.2827138304710388
train_iter_loss: 0.4027118384838104
train_iter_loss: 0.36966145038604736
train_iter_loss: 0.36009883880615234
train_iter_loss: 0.25080522894859314
train_iter_loss: 0.2852213382720947
train_iter_loss: 0.2819421887397766
train_iter_loss: 0.16652771830558777
train_iter_loss: 0.38877397775650024
train_iter_loss: 0.30223825573921204
train_iter_loss: 0.16742433607578278
train_iter_loss: 0.19975227117538452
train_iter_loss: 0.39845553040504456
train_iter_loss: 0.40797871351242065
train_iter_loss: 0.2698856592178345
train_iter_loss: 0.3471648097038269
train_iter_loss: 0.2901729941368103
train_iter_loss: 0.2919479310512543
train_iter_loss: 0.5010283589363098
train_iter_loss: 0.1697152554988861
train_iter_loss: 0.2986562252044678
train_iter_loss: 0.36866918206214905
train_iter_loss: 0.302455335855484
train_iter_loss: 0.41102373600006104
train_iter_loss: 0.2765105962753296
train_iter_loss: 0.16130168735980988
train_iter_loss: 0.2758442461490631
train_iter_loss: 0.15540696680545807
train_iter_loss: 0.12422895431518555
train_iter_loss: 0.20595310628414154
train_iter_loss: 0.21276284754276276
train_iter_loss: 0.29583999514579773
train_iter_loss: 0.38688039779663086
train_iter_loss: 0.21811257302761078
train_iter_loss: 0.3131386935710907
train_iter_loss: 0.22789210081100464
train_iter_loss: 0.18757562339305878
train_iter_loss: 0.19866974651813507
train_iter_loss: 0.2557923197746277
train_iter_loss: 0.3273937702178955
train_iter_loss: 0.24706177413463593
train_iter_loss: 0.24827106297016144
train_iter_loss: 0.3410254418849945
train_iter_loss: 0.2696882486343384
train_iter_loss: 0.23962412774562836
train_iter_loss: 0.18205325305461884
train_iter_loss: 0.14165133237838745
train_iter_loss: 0.2501537799835205
train_iter_loss: 0.21718177199363708
train_iter_loss: 0.4211276173591614
train_iter_loss: 0.3015954792499542
train_iter_loss: 0.2744435667991638
train_iter_loss: 0.1880033016204834
train_iter_loss: 0.3085605502128601
train_iter_loss: 0.3220764100551605
train_iter_loss: 0.3849279582500458
train_iter_loss: 0.26456236839294434
train_iter_loss: 0.2500946819782257
train_iter_loss: 0.29651516675949097
train_iter_loss: 0.25813180208206177
train_iter_loss: 0.08653047680854797
train_iter_loss: 0.2587403357028961
train_iter_loss: 0.23245516419410706
train_iter_loss: 0.36745673418045044
train_iter_loss: 0.29870352149009705
train_iter_loss: 0.2775470018386841
train_iter_loss: 0.2700081169605255
train_iter_loss: 0.1611718088388443
train_iter_loss: 0.3007180392742157
train_iter_loss: 0.3079760670661926
train_iter_loss: 0.4695657193660736
train_iter_loss: 0.2267398089170456
train_iter_loss: 0.3359428346157074
train_iter_loss: 0.2965228259563446
train_iter_loss: 0.28263890743255615
train_iter_loss: 0.19903378188610077
train_iter_loss: 0.32580825686454773
train_iter_loss: 0.31120094656944275
train_iter_loss: 0.29010507464408875
train_iter_loss: 0.33013471961021423
train_iter_loss: 0.3300758898258209
train_iter_loss: 0.21454191207885742
train_iter_loss: 0.14643098413944244
train_iter_loss: 0.26036518812179565
train_iter_loss: 0.312002956867218
train_iter_loss: 0.2231960892677307
train_iter_loss: 0.49731048941612244
train_iter_loss: 0.2580309212207794
train_iter_loss: 0.26758837699890137
train_iter_loss: 0.21034900844097137
train_iter_loss: 0.37979087233543396
train_iter_loss: 0.21713964641094208
train_iter_loss: 0.27496325969696045
train_iter_loss: 0.19988569617271423
train loss :0.2781
---------------------
Validation seg loss: 0.37341722180926296 at epoch 397
epoch =    398/  1000, exp = train
train_iter_loss: 0.28766104578971863
train_iter_loss: 0.2752619981765747
train_iter_loss: 0.3017512559890747
train_iter_loss: 0.25030919909477234
train_iter_loss: 0.4019254744052887
train_iter_loss: 0.3589003086090088
train_iter_loss: 0.3194367587566376
train_iter_loss: 0.23163031041622162
train_iter_loss: 0.2603209316730499
train_iter_loss: 0.350376158952713
train_iter_loss: 0.345505028963089
train_iter_loss: 0.14626580476760864
train_iter_loss: 0.20868416130542755
train_iter_loss: 0.33148646354675293
train_iter_loss: 0.2737429141998291
train_iter_loss: 0.35442233085632324
train_iter_loss: 0.21616192162036896
train_iter_loss: 0.19018499553203583
train_iter_loss: 0.23397597670555115
train_iter_loss: 0.1538776159286499
train_iter_loss: 0.23482920229434967
train_iter_loss: 0.3279632329940796
train_iter_loss: 0.1821199208498001
train_iter_loss: 0.40467116236686707
train_iter_loss: 0.21722717583179474
train_iter_loss: 0.4930554926395416
train_iter_loss: 0.15681034326553345
train_iter_loss: 0.13793207705020905
train_iter_loss: 0.39382731914520264
train_iter_loss: 0.29074305295944214
train_iter_loss: 0.3344389498233795
train_iter_loss: 0.2263803631067276
train_iter_loss: 0.2502737045288086
train_iter_loss: 0.15670232474803925
train_iter_loss: 0.15644297003746033
train_iter_loss: 0.24044686555862427
train_iter_loss: 0.24994532763957977
train_iter_loss: 0.3310997188091278
train_iter_loss: 0.2974601089954376
train_iter_loss: 0.2233172506093979
train_iter_loss: 0.4494132995605469
train_iter_loss: 0.445806622505188
train_iter_loss: 0.42746439576148987
train_iter_loss: 0.08286646008491516
train_iter_loss: 0.21759606897830963
train_iter_loss: 0.262621134519577
train_iter_loss: 0.1410500705242157
train_iter_loss: 0.11360541731119156
train_iter_loss: 0.3334963023662567
train_iter_loss: 0.33786073327064514
train_iter_loss: 0.30571529269218445
train_iter_loss: 0.05980176478624344
train_iter_loss: 0.2424706369638443
train_iter_loss: 0.27933040261268616
train_iter_loss: 0.32161861658096313
train_iter_loss: 0.26255637407302856
train_iter_loss: 0.2123444378376007
train_iter_loss: 0.21944881975650787
train_iter_loss: 0.27673670649528503
train_iter_loss: 0.2286214828491211
train_iter_loss: 0.31809014081954956
train_iter_loss: 0.13122335076332092
train_iter_loss: 0.236197367310524
train_iter_loss: 0.16446027159690857
train_iter_loss: 0.26120659708976746
train_iter_loss: 0.36122697591781616
train_iter_loss: 0.18837380409240723
train_iter_loss: 0.23861373960971832
train_iter_loss: 0.29626137018203735
train_iter_loss: 0.2514124810695648
train_iter_loss: 0.30754128098487854
train_iter_loss: 0.40361157059669495
train_iter_loss: 0.3733527660369873
train_iter_loss: 0.4000662863254547
train_iter_loss: 0.23552817106246948
train_iter_loss: 0.1968066543340683
train_iter_loss: 0.30267253518104553
train_iter_loss: 0.2825939357280731
train_iter_loss: 0.21613146364688873
train_iter_loss: 0.3153066039085388
train_iter_loss: 0.28715237975120544
train_iter_loss: 0.37251654267311096
train_iter_loss: 0.15242592990398407
train_iter_loss: 0.19118432700634003
train_iter_loss: 0.3316836953163147
train_iter_loss: 0.1884838342666626
train_iter_loss: 0.2304319143295288
train_iter_loss: 0.2085658609867096
train_iter_loss: 0.4776002764701843
train_iter_loss: 0.2785164713859558
train_iter_loss: 0.26879003643989563
train_iter_loss: 0.14319594204425812
train_iter_loss: 0.4146706163883209
train_iter_loss: 0.3148392140865326
train_iter_loss: 0.2732410132884979
train_iter_loss: 0.1599978506565094
train_iter_loss: 0.3560032844543457
train_iter_loss: 0.28166067600250244
train_iter_loss: 0.3204589784145355
train_iter_loss: 0.31924548745155334
train loss :0.2735
---------------------
Validation seg loss: 0.4142958954117208 at epoch 398
epoch =    399/  1000, exp = train
train_iter_loss: 0.2772672772407532
train_iter_loss: 0.2615715563297272
train_iter_loss: 0.15370121598243713
train_iter_loss: 0.2723926305770874
train_iter_loss: 0.3017575144767761
train_iter_loss: 0.2912321984767914
train_iter_loss: 0.1442631185054779
train_iter_loss: 0.2804052531719208
train_iter_loss: 0.3185938894748688
train_iter_loss: 0.23004773259162903
train_iter_loss: 0.2960560619831085
train_iter_loss: 0.3798826038837433
train_iter_loss: 0.3182298541069031
train_iter_loss: 0.2325442135334015
train_iter_loss: 0.3244898319244385
train_iter_loss: 0.33048516511917114
train_iter_loss: 0.44012606143951416
train_iter_loss: 0.29005709290504456
train_iter_loss: 0.236270472407341
train_iter_loss: 0.37297314405441284
train_iter_loss: 0.25408366322517395
train_iter_loss: 0.21694596111774445
train_iter_loss: 0.19075897336006165
train_iter_loss: 0.3040584325790405
train_iter_loss: 0.3498263657093048
train_iter_loss: 0.04650861397385597
train_iter_loss: 0.4477686583995819
train_iter_loss: 0.504115641117096
train_iter_loss: 0.2700886130332947
train_iter_loss: 0.23692402243614197
train_iter_loss: 0.3105216324329376
train_iter_loss: 0.17849451303482056
train_iter_loss: 0.12833978235721588
train_iter_loss: 0.38437601923942566
train_iter_loss: 0.18820664286613464
train_iter_loss: 0.1270057111978531
train_iter_loss: 0.3299423158168793
train_iter_loss: 0.23194275796413422
train_iter_loss: 0.41853705048561096
train_iter_loss: 0.2670574188232422
train_iter_loss: 0.3054669201374054
train_iter_loss: 0.21699582040309906
train_iter_loss: 0.3809473216533661
train_iter_loss: 0.2626951038837433
train_iter_loss: 0.2141004502773285
train_iter_loss: 0.2745557427406311
train_iter_loss: 0.28017860651016235
train_iter_loss: 0.3758896589279175
train_iter_loss: 0.21312837302684784
train_iter_loss: 0.30365490913391113
train_iter_loss: 0.16082285344600677
train_iter_loss: 0.31690242886543274
train_iter_loss: 0.15838420391082764
train_iter_loss: 0.3556821346282959
train_iter_loss: 0.2163742631673813
train_iter_loss: 0.3684006929397583
train_iter_loss: 0.2889038324356079
train_iter_loss: 0.15460160374641418
train_iter_loss: 0.25390395522117615
train_iter_loss: 0.2812887728214264
train_iter_loss: 0.36126503348350525
train_iter_loss: 0.13560403883457184
train_iter_loss: 0.2701110541820526
train_iter_loss: 0.28764697909355164
train_iter_loss: 0.4801461696624756
train_iter_loss: 0.23597115278244019
train_iter_loss: 0.44173023104667664
train_iter_loss: 0.17449776828289032
train_iter_loss: 0.15748807787895203
train_iter_loss: 0.2861606776714325
train_iter_loss: 0.3162316679954529
train_iter_loss: 0.2294882982969284
train_iter_loss: 0.32061856985092163
train_iter_loss: 0.23509541153907776
train_iter_loss: 0.24599449336528778
train_iter_loss: 0.26345908641815186
train_iter_loss: 0.3464013934135437
train_iter_loss: 0.25002506375312805
train_iter_loss: 0.2397497445344925
train_iter_loss: 0.29043859243392944
train_iter_loss: 0.2303217202425003
train_iter_loss: 0.26939094066619873
train_iter_loss: 0.14468951523303986
train_iter_loss: 0.290963739156723
train_iter_loss: 0.2345299869775772
train_iter_loss: 0.1824367642402649
train_iter_loss: 0.43691906332969666
train_iter_loss: 0.28315383195877075
train_iter_loss: 0.1219087615609169
train_iter_loss: 0.18910425901412964
train_iter_loss: 0.35427001118659973
train_iter_loss: 0.26130425930023193
train_iter_loss: 0.27639663219451904
train_iter_loss: 0.30549824237823486
train_iter_loss: 0.2598611116409302
train_iter_loss: 0.331098735332489
train_iter_loss: 0.42374399304389954
train_iter_loss: 0.31005212664604187
train_iter_loss: 0.38048872351646423
train_iter_loss: 0.4050964415073395
train loss :0.2805
---------------------
Validation seg loss: 0.37535814195871353 at epoch 399
epoch =    400/  1000, exp = train
train_iter_loss: 0.2531242072582245
train_iter_loss: 0.31054458022117615
train_iter_loss: 0.32829827070236206
train_iter_loss: 0.2360544353723526
train_iter_loss: 0.16817684471607208
train_iter_loss: 0.2599307894706726
train_iter_loss: 0.2961373031139374
train_iter_loss: 0.2747917175292969
train_iter_loss: 0.3579116463661194
train_iter_loss: 0.32388797402381897
train_iter_loss: 0.2692718207836151
train_iter_loss: 0.3915344178676605
train_iter_loss: 0.3239523470401764
train_iter_loss: 0.41477540135383606
train_iter_loss: 0.44188886880874634
train_iter_loss: 0.26226603984832764
train_iter_loss: 0.21790190041065216
train_iter_loss: 0.3041880428791046
train_iter_loss: 0.38302043080329895
train_iter_loss: 0.30811169743537903
train_iter_loss: 0.3656345009803772
train_iter_loss: 0.33252719044685364
train_iter_loss: 0.2759576141834259
train_iter_loss: 0.27925682067871094
train_iter_loss: 0.1508236825466156
train_iter_loss: 0.1759319305419922
train_iter_loss: 0.1901155412197113
train_iter_loss: 0.25365903973579407
train_iter_loss: 0.1718115359544754
train_iter_loss: 0.17126399278640747
train_iter_loss: 0.20526237785816193
train_iter_loss: 0.3510696291923523
train_iter_loss: 0.17870798707008362
train_iter_loss: 0.21024611592292786
train_iter_loss: 0.3860592544078827
train_iter_loss: 0.21550819277763367
train_iter_loss: 0.33522123098373413
train_iter_loss: 0.21096353232860565
train_iter_loss: 0.3410920798778534
train_iter_loss: 0.23453599214553833
train_iter_loss: 0.20076225697994232
train_iter_loss: 0.18980289995670319
train_iter_loss: 0.3085055649280548
train_iter_loss: 0.26133885979652405
train_iter_loss: 0.3081093728542328
train_iter_loss: 0.28617629408836365
train_iter_loss: 0.24041683971881866
train_iter_loss: 0.15770187973976135
train_iter_loss: 0.2998649477958679
train_iter_loss: 0.23428179323673248
train_iter_loss: 0.5125713348388672
train_iter_loss: 0.33887162804603577
train_iter_loss: 0.20809344947338104
train_iter_loss: 0.39254865050315857
train_iter_loss: 0.2169327586889267
train_iter_loss: 0.20574460923671722
train_iter_loss: 0.2804364860057831
train_iter_loss: 0.11957671493291855
train_iter_loss: 0.09962296485900879
train_iter_loss: 0.12884388864040375
train_iter_loss: 0.27737435698509216
train_iter_loss: 0.12906929850578308
train_iter_loss: 0.14641427993774414
train_iter_loss: 0.2645975947380066
train_iter_loss: 0.28930342197418213
train_iter_loss: 0.2593815326690674
train_iter_loss: 0.19494949281215668
train_iter_loss: 0.2133210003376007
train_iter_loss: 0.3338989019393921
train_iter_loss: 0.30323609709739685
train_iter_loss: 0.34007108211517334
train_iter_loss: 0.2312133014202118
train_iter_loss: 0.30165138840675354
train_iter_loss: 0.41647404432296753
train_iter_loss: 0.20987720787525177
train_iter_loss: 0.3474656343460083
train_iter_loss: 0.2820475995540619
train_iter_loss: 0.3383246660232544
train_iter_loss: 0.29329314827919006
train_iter_loss: 0.24707069993019104
train_iter_loss: 0.3093152642250061
train_iter_loss: 0.2957606911659241
train_iter_loss: 0.1470269113779068
train_iter_loss: 0.48672646284103394
train_iter_loss: 0.1905779242515564
train_iter_loss: 0.44831186532974243
train_iter_loss: 0.1614459902048111
train_iter_loss: 0.3740181624889374
train_iter_loss: 0.18850049376487732
train_iter_loss: 0.2655999958515167
train_iter_loss: 0.23659299314022064
train_iter_loss: 0.3166782259941101
train_iter_loss: 0.2754316031932831
train_iter_loss: 0.1416233777999878
train_iter_loss: 0.2857709527015686
train_iter_loss: 0.21137459576129913
train_iter_loss: 0.19434991478919983
train_iter_loss: 0.25461119413375854
train_iter_loss: 0.2690120041370392
train_iter_loss: 0.366531640291214
train loss :0.2727
---------------------
Validation seg loss: 0.37833448934351216 at epoch 400
epoch =    401/  1000, exp = train
train_iter_loss: 0.10753501206636429
train_iter_loss: 0.28516271710395813
train_iter_loss: 0.44403931498527527
train_iter_loss: 0.4497036039829254
train_iter_loss: 0.4025641977787018
train_iter_loss: 0.1828201413154602
train_iter_loss: 0.303786963224411
train_iter_loss: 0.19943036139011383
train_iter_loss: 0.15681080520153046
train_iter_loss: 0.22111408412456512
train_iter_loss: 0.3616148829460144
train_iter_loss: 0.2451307475566864
train_iter_loss: 0.16322049498558044
train_iter_loss: 0.22499299049377441
train_iter_loss: 0.3687325417995453
train_iter_loss: 0.5474938154220581
train_iter_loss: 0.20590797066688538
train_iter_loss: 0.33399879932403564
train_iter_loss: 0.2128058671951294
train_iter_loss: 0.17795942723751068
train_iter_loss: 0.3680092990398407
train_iter_loss: 0.38078969717025757
train_iter_loss: 0.19923657178878784
train_iter_loss: 0.261274516582489
train_iter_loss: 0.36152446269989014
train_iter_loss: 0.3214762508869171
train_iter_loss: 0.22545866668224335
train_iter_loss: 0.23857446014881134
train_iter_loss: 0.23712816834449768
train_iter_loss: 0.1314147561788559
train_iter_loss: 0.2533559501171112
train_iter_loss: 0.2567670941352844
train_iter_loss: 0.4322603940963745
train_iter_loss: 0.1601714789867401
train_iter_loss: 0.3870340883731842
train_iter_loss: 0.18968886137008667
train_iter_loss: 0.2203490436077118
train_iter_loss: 0.25676581263542175
train_iter_loss: 0.12935876846313477
train_iter_loss: 0.32023635506629944
train_iter_loss: 0.1380736529827118
train_iter_loss: 0.34007832407951355
train_iter_loss: 0.30539241433143616
train_iter_loss: 0.23729854822158813
train_iter_loss: 0.3054403066635132
train_iter_loss: 0.3552255630493164
train_iter_loss: 0.3017890751361847
train_iter_loss: 0.42635104060173035
train_iter_loss: 0.21764475107192993
train_iter_loss: 0.3111935555934906
train_iter_loss: 0.2916756570339203
train_iter_loss: 0.22226399183273315
train_iter_loss: 0.3703262209892273
train_iter_loss: 0.2710850238800049
train_iter_loss: 0.34741708636283875
train_iter_loss: 0.24868659675121307
train_iter_loss: 0.2486853003501892
train_iter_loss: 0.0604085773229599
train_iter_loss: 0.2197735607624054
train_iter_loss: 0.3976827561855316
train_iter_loss: 0.19831858575344086
train_iter_loss: 0.2724733054637909
train_iter_loss: 0.2424389272928238
train_iter_loss: 0.33017274737358093
train_iter_loss: 0.2920825183391571
train_iter_loss: 0.311076283454895
train_iter_loss: 0.23002199828624725
train_iter_loss: 0.1883336454629898
train_iter_loss: 0.14269083738327026
train_iter_loss: 0.35357382893562317
train_iter_loss: 0.3772236108779907
train_iter_loss: 0.2178872674703598
train_iter_loss: 0.3689858317375183
train_iter_loss: 0.2831156551837921
train_iter_loss: 0.28719305992126465
train_iter_loss: 0.18680177628993988
train_iter_loss: 0.224592387676239
train_iter_loss: 0.5293521881103516
train_iter_loss: 0.32586655020713806
train_iter_loss: 0.11031333357095718
train_iter_loss: 0.15420162677764893
train_iter_loss: 0.19542255997657776
train_iter_loss: 0.32442474365234375
train_iter_loss: 0.3267168700695038
train_iter_loss: 0.23341314494609833
train_iter_loss: 0.4343051314353943
train_iter_loss: 0.2501576840877533
train_iter_loss: 0.2884264588356018
train_iter_loss: 0.24837607145309448
train_iter_loss: 0.3922308087348938
train_iter_loss: 0.1508239358663559
train_iter_loss: 0.2334860861301422
train_iter_loss: 0.325196772813797
train_iter_loss: 0.29953300952911377
train_iter_loss: 0.36936619877815247
train_iter_loss: 0.3248125910758972
train_iter_loss: 0.1787879317998886
train_iter_loss: 0.13055852055549622
train_iter_loss: 0.24832874536514282
train_iter_loss: 0.4347071051597595
train loss :0.2783
---------------------
Validation seg loss: 0.3716653723550855 at epoch 401
epoch =    402/  1000, exp = train
train_iter_loss: 0.423798531293869
train_iter_loss: 0.2564862072467804
train_iter_loss: 0.27912160754203796
train_iter_loss: 0.2390972375869751
train_iter_loss: 0.24789075553417206
train_iter_loss: 0.3587946593761444
train_iter_loss: 0.2682991921901703
train_iter_loss: 0.12978650629520416
train_iter_loss: 0.22381925582885742
train_iter_loss: 0.1838962435722351
train_iter_loss: 0.32483506202697754
train_iter_loss: 0.2686291038990021
train_iter_loss: 0.29088294506073
train_iter_loss: 0.15980395674705505
train_iter_loss: 0.3846706449985504
train_iter_loss: 0.31845155358314514
train_iter_loss: 0.38712021708488464
train_iter_loss: 0.3142770826816559
train_iter_loss: 0.2738367021083832
train_iter_loss: 0.26297101378440857
train_iter_loss: 0.1488560289144516
train_iter_loss: 0.10204392671585083
train_iter_loss: 0.1980324387550354
train_iter_loss: 0.2868303954601288
train_iter_loss: 0.24163079261779785
train_iter_loss: 0.32903116941452026
train_iter_loss: 0.26737987995147705
train_iter_loss: 0.3558509349822998
train_iter_loss: 0.31006085872650146
train_iter_loss: 0.36076006293296814
train_iter_loss: 0.14362555742263794
train_iter_loss: 0.37057021260261536
train_iter_loss: 0.3339199125766754
train_iter_loss: 0.2273482084274292
train_iter_loss: 0.24611350893974304
train_iter_loss: 0.23942138254642487
train_iter_loss: 0.23498721420764923
train_iter_loss: 0.1827748715877533
train_iter_loss: 0.33531880378723145
train_iter_loss: 0.3693097233772278
train_iter_loss: 0.21079285442829132
train_iter_loss: 0.24004052579402924
train_iter_loss: 0.2426646202802658
train_iter_loss: 0.24810422956943512
train_iter_loss: 0.29368650913238525
train_iter_loss: 0.2523416578769684
train_iter_loss: 0.2982228994369507
train_iter_loss: 0.266831636428833
train_iter_loss: 0.21608015894889832
train_iter_loss: 0.3318192660808563
train_iter_loss: 0.16201168298721313
train_iter_loss: 0.3089255690574646
train_iter_loss: 0.35283148288726807
train_iter_loss: 0.3541513979434967
train_iter_loss: 0.18948957324028015
train_iter_loss: 0.3738572299480438
train_iter_loss: 0.20717225968837738
train_iter_loss: 0.23056763410568237
train_iter_loss: 0.3517133295536041
train_iter_loss: 0.42163488268852234
train_iter_loss: 0.10587254911661148
train_iter_loss: 0.30088552832603455
train_iter_loss: 0.21476547420024872
train_iter_loss: 0.22089843451976776
train_iter_loss: 0.30931687355041504
train_iter_loss: 0.27103468775749207
train_iter_loss: 0.136498361825943
train_iter_loss: 0.29111647605895996
train_iter_loss: 0.23206095397472382
train_iter_loss: 0.18740518391132355
train_iter_loss: 0.14423562586307526
train_iter_loss: 0.4012880325317383
train_iter_loss: 0.23236162960529327
train_iter_loss: 0.15909627079963684
train_iter_loss: 0.18026793003082275
train_iter_loss: 0.11790414154529572
train_iter_loss: 0.4208669662475586
train_iter_loss: 0.3869740068912506
train_iter_loss: 0.3139484226703644
train_iter_loss: 0.7232833504676819
train_iter_loss: 0.2997952401638031
train_iter_loss: 0.21365931630134583
train_iter_loss: 0.23488783836364746
train_iter_loss: 0.15661649405956268
train_iter_loss: 0.3533799946308136
train_iter_loss: 0.3984118103981018
train_iter_loss: 0.2694579064846039
train_iter_loss: 0.21518200635910034
train_iter_loss: 0.2052289843559265
train_iter_loss: 0.3263005316257477
train_iter_loss: 0.2998925745487213
train_iter_loss: 0.37315523624420166
train_iter_loss: 0.31847360730171204
train_iter_loss: 0.3089157044887543
train_iter_loss: 0.23954249918460846
train_iter_loss: 0.09853003919124603
train_iter_loss: 0.23316590487957
train_iter_loss: 0.17020465433597565
train_iter_loss: 0.2296869158744812
train_iter_loss: 0.25525274872779846
train loss :0.2726
---------------------
Validation seg loss: 0.3634676649938074 at epoch 402
epoch =    403/  1000, exp = train
train_iter_loss: 0.2590877413749695
train_iter_loss: 0.3421729505062103
train_iter_loss: 0.30386731028556824
train_iter_loss: 0.24775175750255585
train_iter_loss: 0.22590681910514832
train_iter_loss: 0.21823161840438843
train_iter_loss: 0.40511244535446167
train_iter_loss: 0.4449921250343323
train_iter_loss: 0.29972314834594727
train_iter_loss: 0.18858791887760162
train_iter_loss: 0.33444827795028687
train_iter_loss: 0.32407689094543457
train_iter_loss: 0.22051028907299042
train_iter_loss: 0.3251078426837921
train_iter_loss: 0.4360368549823761
train_iter_loss: 0.5195668339729309
train_iter_loss: 0.3709614872932434
train_iter_loss: 0.18302001059055328
train_iter_loss: 0.24375081062316895
train_iter_loss: 0.27677208185195923
train_iter_loss: 0.16325969994068146
train_iter_loss: 0.15472863614559174
train_iter_loss: 0.36824509501457214
train_iter_loss: 0.1546308696269989
train_iter_loss: 0.27357563376426697
train_iter_loss: 0.26485198736190796
train_iter_loss: 0.21941730380058289
train_iter_loss: 0.4154662489891052
train_iter_loss: 0.2814447581768036
train_iter_loss: 0.16159670054912567
train_iter_loss: 0.2842530310153961
train_iter_loss: 0.41160571575164795
train_iter_loss: 0.22882762551307678
train_iter_loss: 0.1470341980457306
train_iter_loss: 0.2341146171092987
train_iter_loss: 0.20169760286808014
train_iter_loss: 0.23048625886440277
train_iter_loss: 0.21045340597629547
train_iter_loss: 0.41780024766921997
train_iter_loss: 0.15811680257320404
train_iter_loss: 0.14567217230796814
train_iter_loss: 0.3014269173145294
train_iter_loss: 0.4101700484752655
train_iter_loss: 0.22910813987255096
train_iter_loss: 0.3945263624191284
train_iter_loss: 0.21536007523536682
train_iter_loss: 0.47026416659355164
train_iter_loss: 0.2831256091594696
train_iter_loss: 0.3531448543071747
train_iter_loss: 0.34124162793159485
train_iter_loss: 0.28641971945762634
train_iter_loss: 0.21754568815231323
train_iter_loss: 0.3138549029827118
train_iter_loss: 0.28760308027267456
train_iter_loss: 0.31094300746917725
train_iter_loss: 0.22048430144786835
train_iter_loss: 0.24771931767463684
train_iter_loss: 0.11658115684986115
train_iter_loss: 0.2566860616207123
train_iter_loss: 0.27256712317466736
train_iter_loss: 0.28230398893356323
train_iter_loss: 0.4497779905796051
train_iter_loss: 0.3091698884963989
train_iter_loss: 0.21305708587169647
train_iter_loss: 0.3216840624809265
train_iter_loss: 0.22042571008205414
train_iter_loss: 0.1704045683145523
train_iter_loss: 0.33909785747528076
train_iter_loss: 0.2008209377527237
train_iter_loss: 0.35087916254997253
train_iter_loss: 0.1991591602563858
train_iter_loss: 0.27711108326911926
train_iter_loss: 0.33058038353919983
train_iter_loss: 0.1897123008966446
train_iter_loss: 0.2635851502418518
train_iter_loss: 0.2565591335296631
train_iter_loss: 0.32182034850120544
train_iter_loss: 0.13552314043045044
train_iter_loss: 0.20227639377117157
train_iter_loss: 0.1916126161813736
train_iter_loss: 0.2912342846393585
train_iter_loss: 0.12998010218143463
train_iter_loss: 0.12055056542158127
train_iter_loss: 0.29655757546424866
train_iter_loss: 0.23423494398593903
train_iter_loss: 0.28801485896110535
train_iter_loss: 0.17287513613700867
train_iter_loss: 0.18716074526309967
train_iter_loss: 0.23543024063110352
train_iter_loss: 0.3878563940525055
train_iter_loss: 0.3137412667274475
train_iter_loss: 0.13984501361846924
train_iter_loss: 0.3654116094112396
train_iter_loss: 0.2182805836200714
train_iter_loss: 0.2964293956756592
train_iter_loss: 0.437934935092926
train_iter_loss: 0.2619931101799011
train_iter_loss: 0.11979209631681442
train_iter_loss: 0.6301836371421814
train_iter_loss: 0.10601407289505005
train loss :0.2756
---------------------
Validation seg loss: 0.3770255269043429 at epoch 403
epoch =    404/  1000, exp = train
train_iter_loss: 0.14135371148586273
train_iter_loss: 0.26423409581184387
train_iter_loss: 0.12169978767633438
train_iter_loss: 0.33833712339401245
train_iter_loss: 0.1387077420949936
train_iter_loss: 0.42159727215766907
train_iter_loss: 0.38557761907577515
train_iter_loss: 0.3696746230125427
train_iter_loss: 0.2498103827238083
train_iter_loss: 0.17523141205310822
train_iter_loss: 0.3132033050060272
train_iter_loss: 0.32621440291404724
train_iter_loss: 0.3851967751979828
train_iter_loss: 0.25366419553756714
train_iter_loss: 0.2870301604270935
train_iter_loss: 0.3778406083583832
train_iter_loss: 0.20254643261432648
train_iter_loss: 0.24462485313415527
train_iter_loss: 0.3060598075389862
train_iter_loss: 0.30694594979286194
train_iter_loss: 0.18773752450942993
train_iter_loss: 0.4257512092590332
train_iter_loss: 0.3302801251411438
train_iter_loss: 0.3535720705986023
train_iter_loss: 0.5190386772155762
train_iter_loss: 0.24510596692562103
train_iter_loss: 0.06829969584941864
train_iter_loss: 0.37983664870262146
train_iter_loss: 0.3108528256416321
train_iter_loss: 0.21893808245658875
train_iter_loss: 0.14655572175979614
train_iter_loss: 0.3092677891254425
train_iter_loss: 0.24461773037910461
train_iter_loss: 0.19797666370868683
train_iter_loss: 0.23736120760440826
train_iter_loss: 0.27003395557403564
train_iter_loss: 0.17282967269420624
train_iter_loss: 0.2698221206665039
train_iter_loss: 0.2169652134180069
train_iter_loss: 0.3986857533454895
train_iter_loss: 0.163679301738739
train_iter_loss: 0.37177774310112
train_iter_loss: 0.28819164633750916
train_iter_loss: 0.09115622192621231
train_iter_loss: 0.28601956367492676
train_iter_loss: 0.22685056924819946
train_iter_loss: 0.33817556500434875
train_iter_loss: 0.2588101923465729
train_iter_loss: 0.39410269260406494
train_iter_loss: 0.2954484224319458
train_iter_loss: 0.34241002798080444
train_iter_loss: 0.2821696102619171
train_iter_loss: 0.23918849229812622
train_iter_loss: 0.3541717231273651
train_iter_loss: 0.21047864854335785
train_iter_loss: 0.4346831738948822
train_iter_loss: 0.22067348659038544
train_iter_loss: 0.2564433217048645
train_iter_loss: 0.24379347264766693
train_iter_loss: 0.3461117744445801
train_iter_loss: 0.3408513367176056
train_iter_loss: 0.3312908113002777
train_iter_loss: 0.12077431380748749
train_iter_loss: 0.26649734377861023
train_iter_loss: 0.23114128410816193
train_iter_loss: 0.3488157093524933
train_iter_loss: 0.18743927776813507
train_iter_loss: 0.35541829466819763
train_iter_loss: 0.2923659384250641
train_iter_loss: 0.129794180393219
train_iter_loss: 0.2040194869041443
train_iter_loss: 0.17731046676635742
train_iter_loss: 0.32241353392601013
train_iter_loss: 0.3072570860385895
train_iter_loss: 0.3659138083457947
train_iter_loss: 0.2958548963069916
train_iter_loss: 0.2454938292503357
train_iter_loss: 0.2877747714519501
train_iter_loss: 0.33923327922821045
train_iter_loss: 0.3780863285064697
train_iter_loss: 0.29228994250297546
train_iter_loss: 0.1474861353635788
train_iter_loss: 0.29078006744384766
train_iter_loss: 0.2384020835161209
train_iter_loss: 0.25983884930610657
train_iter_loss: 0.46851882338523865
train_iter_loss: 0.24231818318367004
train_iter_loss: 0.32619327306747437
train_iter_loss: 0.21028582751750946
train_iter_loss: 0.39046964049339294
train_iter_loss: 0.19883368909358978
train_iter_loss: 0.22614961862564087
train_iter_loss: 0.27745291590690613
train_iter_loss: 0.41465047001838684
train_iter_loss: 0.14685054123401642
train_iter_loss: 0.27773138880729675
train_iter_loss: 0.28197675943374634
train_iter_loss: 0.27138516306877136
train_iter_loss: 0.2265111207962036
train_iter_loss: 0.27878251671791077
train loss :0.2803
---------------------
Validation seg loss: 0.38591584335696305 at epoch 404
epoch =    405/  1000, exp = train
train_iter_loss: 0.3470793664455414
train_iter_loss: 0.3060741424560547
train_iter_loss: 0.29826730489730835
train_iter_loss: 0.29843783378601074
train_iter_loss: 0.34377557039260864
train_iter_loss: 0.3614732325077057
train_iter_loss: 0.43491101264953613
train_iter_loss: 0.14127981662750244
train_iter_loss: 0.3110005855560303
train_iter_loss: 0.3310415744781494
train_iter_loss: 0.2077983021736145
train_iter_loss: 0.3836711347103119
train_iter_loss: 0.1976938247680664
train_iter_loss: 0.2603316605091095
train_iter_loss: 0.251176655292511
train_iter_loss: 0.13438384234905243
train_iter_loss: 0.3049379885196686
train_iter_loss: 0.31317663192749023
train_iter_loss: 0.2752714157104492
train_iter_loss: 0.21076148748397827
train_iter_loss: 0.32327064871788025
train_iter_loss: 0.19367310404777527
train_iter_loss: 0.3493344187736511
train_iter_loss: 0.26047050952911377
train_iter_loss: 0.243043452501297
train_iter_loss: 0.1117081269621849
train_iter_loss: 0.4035071134567261
train_iter_loss: 0.35098302364349365
train_iter_loss: 0.38630300760269165
train_iter_loss: 0.274059921503067
train_iter_loss: 0.34661561250686646
train_iter_loss: 0.18074895441532135
train_iter_loss: 0.19735465943813324
train_iter_loss: 0.3242725133895874
train_iter_loss: 0.2837138772010803
train_iter_loss: 0.20505307614803314
train_iter_loss: 0.25439000129699707
train_iter_loss: 0.38227227330207825
train_iter_loss: 0.27922898530960083
train_iter_loss: 0.23523540794849396
train_iter_loss: 0.1951208859682083
train_iter_loss: 0.3043270409107208
train_iter_loss: 0.2950717508792877
train_iter_loss: 0.347604364156723
train_iter_loss: 0.25076645612716675
train_iter_loss: 0.11090215295553207
train_iter_loss: 0.38219279050827026
train_iter_loss: 0.18295462429523468
train_iter_loss: 0.20417238771915436
train_iter_loss: 0.19751623272895813
train_iter_loss: 0.2187969982624054
train_iter_loss: 0.13974392414093018
train_iter_loss: 0.3435957431793213
train_iter_loss: 0.26890674233436584
train_iter_loss: 0.29726213216781616
train_iter_loss: 0.19028833508491516
train_iter_loss: 0.18997792899608612
train_iter_loss: 0.36757832765579224
train_iter_loss: 0.31373023986816406
train_iter_loss: 0.22413575649261475
train_iter_loss: 0.23074717819690704
train_iter_loss: 0.26194778084754944
train_iter_loss: 0.30428260564804077
train_iter_loss: 0.2813124656677246
train_iter_loss: 0.25538167357444763
train_iter_loss: 0.16009464859962463
train_iter_loss: 0.24018719792366028
train_iter_loss: 0.18120940029621124
train_iter_loss: 0.3393399119377136
train_iter_loss: 0.35169118642807007
train_iter_loss: 0.10473691672086716
train_iter_loss: 0.16293787956237793
train_iter_loss: 0.14918960630893707
train_iter_loss: 0.2601417303085327
train_iter_loss: 0.21170349419116974
train_iter_loss: 0.34238946437835693
train_iter_loss: 0.24982057511806488
train_iter_loss: 0.6076182126998901
train_iter_loss: 0.2869221866130829
train_iter_loss: 0.42084214091300964
train_iter_loss: 0.12999892234802246
train_iter_loss: 0.32763105630874634
train_iter_loss: 0.30336910486221313
train_iter_loss: 0.1909947246313095
train_iter_loss: 0.21699175238609314
train_iter_loss: 0.38465574383735657
train_iter_loss: 0.3131010830402374
train_iter_loss: 0.2894326150417328
train_iter_loss: 0.33781954646110535
train_iter_loss: 0.3528517186641693
train_iter_loss: 0.4178982079029083
train_iter_loss: 0.26440590620040894
train_iter_loss: 0.327083945274353
train_iter_loss: 0.30000612139701843
train_iter_loss: 0.2592601478099823
train_iter_loss: 0.2009810358285904
train_iter_loss: 0.13798300921916962
train_iter_loss: 0.3396220803260803
train_iter_loss: 0.3410748839378357
train_iter_loss: 0.16567736864089966
train loss :0.2761
---------------------
Validation seg loss: 0.4053543768295983 at epoch 405
epoch =    406/  1000, exp = train
train_iter_loss: 0.2642005383968353
train_iter_loss: 0.2802921533584595
train_iter_loss: 0.17496944963932037
train_iter_loss: 0.2884911596775055
train_iter_loss: 0.38548001646995544
train_iter_loss: 0.19827693700790405
train_iter_loss: 0.3139818012714386
train_iter_loss: 0.27192628383636475
train_iter_loss: 0.4011850953102112
train_iter_loss: 0.31604844331741333
train_iter_loss: 0.2726536691188812
train_iter_loss: 0.18569940328598022
train_iter_loss: 0.24786511063575745
train_iter_loss: 0.1671830266714096
train_iter_loss: 0.3037697970867157
train_iter_loss: 0.25847846269607544
train_iter_loss: 0.18221519887447357
train_iter_loss: 0.38298070430755615
train_iter_loss: 0.3455340266227722
train_iter_loss: 0.16796018183231354
train_iter_loss: 0.31638190150260925
train_iter_loss: 0.17185993492603302
train_iter_loss: 0.383571594953537
train_iter_loss: 0.2698914706707001
train_iter_loss: 0.3514660596847534
train_iter_loss: 0.12176569551229477
train_iter_loss: 0.3005177676677704
train_iter_loss: 0.5238428711891174
train_iter_loss: 0.44662779569625854
train_iter_loss: 0.19391383230686188
train_iter_loss: 0.09905588626861572
train_iter_loss: 0.5356205105781555
train_iter_loss: 0.3488546907901764
train_iter_loss: 0.4849074184894562
train_iter_loss: 0.17892496287822723
train_iter_loss: 0.2697213292121887
train_iter_loss: 0.24459251761436462
train_iter_loss: 0.212800532579422
train_iter_loss: 0.4008863568305969
train_iter_loss: 0.21834369003772736
train_iter_loss: 0.3188721239566803
train_iter_loss: 0.40449824929237366
train_iter_loss: 0.30449169874191284
train_iter_loss: 0.31846222281455994
train_iter_loss: 0.204534113407135
train_iter_loss: 0.31838321685791016
train_iter_loss: 0.2658955156803131
train_iter_loss: 0.21057085692882538
train_iter_loss: 0.2564505338668823
train_iter_loss: 0.2317761480808258
train_iter_loss: 0.25478166341781616
train_iter_loss: 0.3045717179775238
train_iter_loss: 0.38605213165283203
train_iter_loss: 0.15720629692077637
train_iter_loss: 0.174585223197937
train_iter_loss: 0.4360312819480896
train_iter_loss: 0.25726649165153503
train_iter_loss: 0.2644110321998596
train_iter_loss: 0.20353953540325165
train_iter_loss: 0.2618640661239624
train_iter_loss: 0.16361963748931885
train_iter_loss: 0.32666245102882385
train_iter_loss: 0.4339318871498108
train_iter_loss: 0.3081258535385132
train_iter_loss: 0.1730632781982422
train_iter_loss: 0.38412970304489136
train_iter_loss: 0.2444537729024887
train_iter_loss: 0.2999449670314789
train_iter_loss: 0.20183227956295013
train_iter_loss: 0.2147684097290039
train_iter_loss: 0.15587177872657776
train_iter_loss: 0.2717062830924988
train_iter_loss: 0.14065130054950714
train_iter_loss: 0.10327904671430588
train_iter_loss: 0.2632899880409241
train_iter_loss: 0.27534955739974976
train_iter_loss: 0.4071386158466339
train_iter_loss: 0.23204867541790009
train_iter_loss: 0.2896382808685303
train_iter_loss: 0.2796136140823364
train_iter_loss: 0.4217754304409027
train_iter_loss: 0.12052877992391586
train_iter_loss: 0.30570584535598755
train_iter_loss: 0.24185645580291748
train_iter_loss: 0.25288650393486023
train_iter_loss: 0.31930938363075256
train_iter_loss: 0.2748723328113556
train_iter_loss: 0.2422543466091156
train_iter_loss: 0.3156838119029999
train_iter_loss: 0.14424292743206024
train_iter_loss: 0.2044505476951599
train_iter_loss: 0.28844916820526123
train_iter_loss: 0.14125841856002808
train_iter_loss: 0.21081256866455078
train_iter_loss: 0.3361535370349884
train_iter_loss: 0.2959965765476227
train_iter_loss: 0.35538139939308167
train_iter_loss: 0.2931593358516693
train_iter_loss: 0.29728686809539795
train_iter_loss: 0.22293812036514282
train loss :0.2775
---------------------
Validation seg loss: 0.36899968916726 at epoch 406
epoch =    407/  1000, exp = train
train_iter_loss: 0.302124947309494
train_iter_loss: 0.2669322192668915
train_iter_loss: 0.3208595812320709
train_iter_loss: 0.37375786900520325
train_iter_loss: 0.3843201994895935
train_iter_loss: 0.20554330945014954
train_iter_loss: 0.31830599904060364
train_iter_loss: 0.2664785087108612
train_iter_loss: 0.18161599338054657
train_iter_loss: 0.37063413858413696
train_iter_loss: 0.2698105275630951
train_iter_loss: 0.20736737549304962
train_iter_loss: 0.19772988557815552
train_iter_loss: 0.13043424487113953
train_iter_loss: 0.30692610144615173
train_iter_loss: 0.14784587919712067
train_iter_loss: 0.3194470703601837
train_iter_loss: 0.2641088366508484
train_iter_loss: 0.38862770795822144
train_iter_loss: 0.2240973263978958
train_iter_loss: 0.27680909633636475
train_iter_loss: 0.399711549282074
train_iter_loss: 0.41260218620300293
train_iter_loss: 0.30391931533813477
train_iter_loss: 0.17567546665668488
train_iter_loss: 0.12126261740922928
train_iter_loss: 0.2896662652492523
train_iter_loss: 0.38917064666748047
train_iter_loss: 0.10875029116868973
train_iter_loss: 0.31080251932144165
train_iter_loss: 0.18330392241477966
train_iter_loss: 0.15371333062648773
train_iter_loss: 0.5241720080375671
train_iter_loss: 0.266323059797287
train_iter_loss: 0.23101085424423218
train_iter_loss: 0.2573772966861725
train_iter_loss: 0.25295066833496094
train_iter_loss: 0.23748843371868134
train_iter_loss: 0.292138934135437
train_iter_loss: 0.1862805187702179
train_iter_loss: 0.285017192363739
train_iter_loss: 0.2632672190666199
train_iter_loss: 0.2136487066745758
train_iter_loss: 0.17973634600639343
train_iter_loss: 0.2698216736316681
train_iter_loss: 0.3994753956794739
train_iter_loss: 0.1273278445005417
train_iter_loss: 0.1420750468969345
train_iter_loss: 0.3113156855106354
train_iter_loss: 0.3401634693145752
train_iter_loss: 0.37356093525886536
train_iter_loss: 0.3198319673538208
train_iter_loss: 0.32347050309181213
train_iter_loss: 0.3310359716415405
train_iter_loss: 0.1690216362476349
train_iter_loss: 0.33993101119995117
train_iter_loss: 0.309488981962204
train_iter_loss: 0.1096203401684761
train_iter_loss: 0.2895956039428711
train_iter_loss: 0.22145408391952515
train_iter_loss: 0.3362809419631958
train_iter_loss: 0.4388999044895172
train_iter_loss: 0.23195186257362366
train_iter_loss: 0.19311419129371643
train_iter_loss: 0.31171655654907227
train_iter_loss: 0.2923843264579773
train_iter_loss: 0.33751189708709717
train_iter_loss: 0.35375961661338806
train_iter_loss: 0.17937351763248444
train_iter_loss: 0.3214530944824219
train_iter_loss: 0.2984539270401001
train_iter_loss: 0.14305898547172546
train_iter_loss: 0.17492149770259857
train_iter_loss: 0.17331771552562714
train_iter_loss: 0.32059234380722046
train_iter_loss: 0.3139611482620239
train_iter_loss: 0.33553361892700195
train_iter_loss: 0.26419201493263245
train_iter_loss: 0.11781935393810272
train_iter_loss: 0.25374510884284973
train_iter_loss: 0.19111230969429016
train_iter_loss: 0.39041176438331604
train_iter_loss: 0.1690675914287567
train_iter_loss: 0.337735652923584
train_iter_loss: 0.29142695665359497
train_iter_loss: 0.2853613495826721
train_iter_loss: 0.2044062614440918
train_iter_loss: 0.3528456687927246
train_iter_loss: 0.24475744366645813
train_iter_loss: 0.2705288529396057
train_iter_loss: 0.31986570358276367
train_iter_loss: 0.23365120589733124
train_iter_loss: 0.21874123811721802
train_iter_loss: 0.3878858685493469
train_iter_loss: 0.3081521987915039
train_iter_loss: 0.22712981700897217
train_iter_loss: 0.3043043911457062
train_iter_loss: 0.34313735365867615
train_iter_loss: 0.23847872018814087
train_iter_loss: 0.15756668150424957
train loss :0.2729
---------------------
Validation seg loss: 0.39018472096935475 at epoch 407
epoch =    408/  1000, exp = train
train_iter_loss: 0.2036609947681427
train_iter_loss: 0.2914717495441437
train_iter_loss: 0.31901970505714417
train_iter_loss: 0.14892657101154327
train_iter_loss: 0.18380926549434662
train_iter_loss: 0.19862519204616547
train_iter_loss: 0.29338210821151733
train_iter_loss: 0.3828527331352234
train_iter_loss: 0.16283278167247772
train_iter_loss: 0.2725397050380707
train_iter_loss: 0.2060396522283554
train_iter_loss: 0.23890076577663422
train_iter_loss: 0.25456729531288147
train_iter_loss: 0.4276677370071411
train_iter_loss: 0.2166435569524765
train_iter_loss: 0.241977721452713
train_iter_loss: 0.23254267871379852
train_iter_loss: 0.21642640233039856
train_iter_loss: 0.1926981657743454
train_iter_loss: 0.23939672112464905
train_iter_loss: 0.21033364534378052
train_iter_loss: 0.267788290977478
train_iter_loss: 0.30001580715179443
train_iter_loss: 0.4832293689250946
train_iter_loss: 0.34398153424263
train_iter_loss: 0.23335100710391998
train_iter_loss: 0.19640396535396576
train_iter_loss: 0.19267818331718445
train_iter_loss: 0.32048800587654114
train_iter_loss: 0.3248577117919922
train_iter_loss: 0.3115246295928955
train_iter_loss: 0.29315194487571716
train_iter_loss: 0.24275098741054535
train_iter_loss: 0.22826465964317322
train_iter_loss: 0.36882951855659485
train_iter_loss: 0.20586922764778137
train_iter_loss: 0.27816617488861084
train_iter_loss: 0.290657639503479
train_iter_loss: 0.26976364850997925
train_iter_loss: 0.3344011604785919
train_iter_loss: 0.36546075344085693
train_iter_loss: 0.35704922676086426
train_iter_loss: 0.20791694521903992
train_iter_loss: 0.2409350425004959
train_iter_loss: 0.3938910663127899
train_iter_loss: 0.4232959747314453
train_iter_loss: 0.18010294437408447
train_iter_loss: 0.3182797431945801
train_iter_loss: 0.2282249927520752
train_iter_loss: 0.3158170282840729
train_iter_loss: 0.3394261598587036
train_iter_loss: 0.2793588638305664
train_iter_loss: 0.2726682424545288
train_iter_loss: 0.4080023765563965
train_iter_loss: 0.28457242250442505
train_iter_loss: 0.17540274560451508
train_iter_loss: 0.2143397331237793
train_iter_loss: 0.34220051765441895
train_iter_loss: 0.41767027974128723
train_iter_loss: 0.280561625957489
train_iter_loss: 0.22618456184864044
train_iter_loss: 0.2358495593070984
train_iter_loss: 0.0880645364522934
train_iter_loss: 0.20503415167331696
train_iter_loss: 0.15692859888076782
train_iter_loss: 0.3745458424091339
train_iter_loss: 0.19732658565044403
train_iter_loss: 0.2783583998680115
train_iter_loss: 0.20447883009910583
train_iter_loss: 0.1932661533355713
train_iter_loss: 0.2009444236755371
train_iter_loss: 0.19928430020809174
train_iter_loss: 0.38960587978363037
train_iter_loss: 0.19654805958271027
train_iter_loss: 0.3539131283760071
train_iter_loss: 0.22456681728363037
train_iter_loss: 0.31937557458877563
train_iter_loss: 0.20226232707500458
train_iter_loss: 0.17099542915821075
train_iter_loss: 0.1568131446838379
train_iter_loss: 0.18864233791828156
train_iter_loss: 0.34745025634765625
train_iter_loss: 0.19060277938842773
train_iter_loss: 0.2488355189561844
train_iter_loss: 0.2608641982078552
train_iter_loss: 0.20393504202365875
train_iter_loss: 0.2535608410835266
train_iter_loss: 0.17914949357509613
train_iter_loss: 0.1373346596956253
train_iter_loss: 0.3299407362937927
train_iter_loss: 0.287502646446228
train_iter_loss: 0.15792451798915863
train_iter_loss: 0.13817445933818817
train_iter_loss: 0.40550947189331055
train_iter_loss: 0.4654776155948639
train_iter_loss: 0.16334928572177887
train_iter_loss: 0.3304148316383362
train_iter_loss: 0.29403066635131836
train_iter_loss: 0.24259640276432037
train_iter_loss: 0.37525662779808044
train loss :0.2671
---------------------
Validation seg loss: 0.37083719119886466 at epoch 408
epoch =    409/  1000, exp = train
train_iter_loss: 0.1060495674610138
train_iter_loss: 0.4969181716442108
train_iter_loss: 0.21111571788787842
train_iter_loss: 0.30761808156967163
train_iter_loss: 0.2229653149843216
train_iter_loss: 0.16379763185977936
train_iter_loss: 0.26450058817863464
train_iter_loss: 0.30084556341171265
train_iter_loss: 0.30468446016311646
train_iter_loss: 0.19419921934604645
train_iter_loss: 0.26020801067352295
train_iter_loss: 0.262616366147995
train_iter_loss: 0.34680527448654175
train_iter_loss: 0.2830960154533386
train_iter_loss: 0.33954477310180664
train_iter_loss: 0.2979139983654022
train_iter_loss: 0.19099491834640503
train_iter_loss: 0.14926767349243164
train_iter_loss: 0.39479830861091614
train_iter_loss: 0.3230907917022705
train_iter_loss: 0.2874249815940857
train_iter_loss: 0.2867988646030426
train_iter_loss: 0.19214168190956116
train_iter_loss: 0.24747006595134735
train_iter_loss: 0.25745856761932373
train_iter_loss: 0.2822704613208771
train_iter_loss: 0.2752450108528137
train_iter_loss: 0.22031988203525543
train_iter_loss: 0.25518304109573364
train_iter_loss: 0.20437243580818176
train_iter_loss: 0.28000321984291077
train_iter_loss: 0.28552812337875366
train_iter_loss: 0.21249496936798096
train_iter_loss: 0.37656450271606445
train_iter_loss: 0.3003081977367401
train_iter_loss: 0.2612032890319824
train_iter_loss: 0.3591780960559845
train_iter_loss: 0.22313418984413147
train_iter_loss: 0.2557011842727661
train_iter_loss: 0.24778321385383606
train_iter_loss: 0.21453621983528137
train_iter_loss: 0.3265356123447418
train_iter_loss: 0.503646194934845
train_iter_loss: 0.24517253041267395
train_iter_loss: 0.36610567569732666
train_iter_loss: 0.18356959521770477
train_iter_loss: 0.18295343220233917
train_iter_loss: 0.12138797342777252
train_iter_loss: 0.3655870258808136
train_iter_loss: 0.19343993067741394
train_iter_loss: 0.281451940536499
train_iter_loss: 0.3107263743877411
train_iter_loss: 0.27164822816848755
train_iter_loss: 0.2805146276950836
train_iter_loss: 0.2747081518173218
train_iter_loss: 0.17813639342784882
train_iter_loss: 0.21708214282989502
train_iter_loss: 0.32658737897872925
train_iter_loss: 0.12365251034498215
train_iter_loss: 0.40650245547294617
train_iter_loss: 0.37496304512023926
train_iter_loss: 0.1767236739397049
train_iter_loss: 0.3285638988018036
train_iter_loss: 0.21122223138809204
train_iter_loss: 0.45444220304489136
train_iter_loss: 0.2816256880760193
train_iter_loss: 0.30787599086761475
train_iter_loss: 0.2988084554672241
train_iter_loss: 0.20757372677326202
train_iter_loss: 0.17729219794273376
train_iter_loss: 0.21291209757328033
train_iter_loss: 0.30175721645355225
train_iter_loss: 0.22953876852989197
train_iter_loss: 0.16906137764453888
train_iter_loss: 0.3857317864894867
train_iter_loss: 0.36043134331703186
train_iter_loss: 0.19644495844841003
train_iter_loss: 0.31515759229660034
train_iter_loss: 0.250286340713501
train_iter_loss: 0.22985713183879852
train_iter_loss: 0.43470704555511475
train_iter_loss: 0.16229280829429626
train_iter_loss: 0.4139733910560608
train_iter_loss: 0.19541053473949432
train_iter_loss: 0.1192220002412796
train_iter_loss: 0.16458562016487122
train_iter_loss: 0.31349459290504456
train_iter_loss: 0.2893665134906769
train_iter_loss: 0.20150019228458405
train_iter_loss: 0.30377650260925293
train_iter_loss: 0.27259165048599243
train_iter_loss: 0.11887854337692261
train_iter_loss: 0.2815433144569397
train_iter_loss: 0.27753564715385437
train_iter_loss: 0.3295508325099945
train_iter_loss: 0.3041917681694031
train_iter_loss: 0.33835452795028687
train_iter_loss: 0.4023967385292053
train_iter_loss: 0.4714556932449341
train_iter_loss: 0.272161602973938
train loss :0.2751
---------------------
Validation seg loss: 0.36224529113761095 at epoch 409
epoch =    410/  1000, exp = train
train_iter_loss: 0.31894323229789734
train_iter_loss: 0.09409376233816147
train_iter_loss: 0.2851424813270569
train_iter_loss: 0.3464354872703552
train_iter_loss: 0.30564236640930176
train_iter_loss: 0.30200719833374023
train_iter_loss: 0.2545851767063141
train_iter_loss: 0.3639258146286011
train_iter_loss: 0.12697546184062958
train_iter_loss: 0.37130656838417053
train_iter_loss: 0.2592160403728485
train_iter_loss: 0.36224713921546936
train_iter_loss: 0.22158432006835938
train_iter_loss: 0.1970781832933426
train_iter_loss: 0.15888462960720062
train_iter_loss: 0.39213240146636963
train_iter_loss: 0.22676654160022736
train_iter_loss: 0.1775885671377182
train_iter_loss: 0.2521536946296692
train_iter_loss: 0.2731344997882843
train_iter_loss: 0.24741004407405853
train_iter_loss: 0.5166431069374084
train_iter_loss: 0.40982434153556824
train_iter_loss: 0.2939077317714691
train_iter_loss: 0.32446664571762085
train_iter_loss: 0.4052713215351105
train_iter_loss: 0.22485557198524475
train_iter_loss: 0.3553798198699951
train_iter_loss: 0.04792662709951401
train_iter_loss: 0.13493366539478302
train_iter_loss: 0.13844910264015198
train_iter_loss: 0.3723263144493103
train_iter_loss: 0.2553912401199341
train_iter_loss: 0.3121904730796814
train_iter_loss: 0.17707808315753937
train_iter_loss: 0.3537455201148987
train_iter_loss: 0.1958158016204834
train_iter_loss: 0.31023284792900085
train_iter_loss: 0.3446780741214752
train_iter_loss: 0.24568770825862885
train_iter_loss: 0.2532765567302704
train_iter_loss: 0.13709184527397156
train_iter_loss: 0.4171694219112396
train_iter_loss: 0.2057696282863617
train_iter_loss: 0.37081295251846313
train_iter_loss: 0.24310149252414703
train_iter_loss: 0.20180562138557434
train_iter_loss: 0.4531656801700592
train_iter_loss: 0.27802470326423645
train_iter_loss: 0.3456694185733795
train_iter_loss: 0.18045929074287415
train_iter_loss: 0.25272879004478455
train_iter_loss: 0.23458367586135864
train_iter_loss: 0.13200967013835907
train_iter_loss: 0.1594894677400589
train_iter_loss: 0.19325123727321625
train_iter_loss: 0.3270765244960785
train_iter_loss: 0.1297631859779358
train_iter_loss: 0.37267211079597473
train_iter_loss: 0.26696228981018066
train_iter_loss: 0.14876681566238403
train_iter_loss: 0.30676546692848206
train_iter_loss: 0.2268921583890915
train_iter_loss: 0.2900608479976654
train_iter_loss: 0.19582590460777283
train_iter_loss: 0.391042560338974
train_iter_loss: 0.25431299209594727
train_iter_loss: 0.18018141388893127
train_iter_loss: 0.2407049685716629
train_iter_loss: 0.27038270235061646
train_iter_loss: 0.32223644852638245
train_iter_loss: 0.3212190270423889
train_iter_loss: 0.17691142857074738
train_iter_loss: 0.2679746747016907
train_iter_loss: 0.22837553918361664
train_iter_loss: 0.3840455710887909
train_iter_loss: 0.276647686958313
train_iter_loss: 0.3120211660861969
train_iter_loss: 0.22814999520778656
train_iter_loss: 0.20865704119205475
train_iter_loss: 0.22543995082378387
train_iter_loss: 0.317519873380661
train_iter_loss: 0.23883283138275146
train_iter_loss: 0.41819268465042114
train_iter_loss: 0.30830880999565125
train_iter_loss: 0.24010112881660461
train_iter_loss: 0.34909436106681824
train_iter_loss: 0.31373926997184753
train_iter_loss: 0.32141828536987305
train_iter_loss: 0.3317887783050537
train_iter_loss: 0.23364171385765076
train_iter_loss: 0.2794119119644165
train_iter_loss: 0.3060223460197449
train_iter_loss: 0.21350163221359253
train_iter_loss: 0.27658697962760925
train_iter_loss: 0.30448031425476074
train_iter_loss: 0.20880845189094543
train_iter_loss: 0.3495715856552124
train_iter_loss: 0.3314078152179718
train_iter_loss: 0.15676160156726837
train loss :0.2734
---------------------
Validation seg loss: 0.37231500075145996 at epoch 410
epoch =    411/  1000, exp = train
train_iter_loss: 0.25448840856552124
train_iter_loss: 0.29677894711494446
train_iter_loss: 0.29714834690093994
train_iter_loss: 0.103807732462883
train_iter_loss: 0.33326587080955505
train_iter_loss: 0.20164282619953156
train_iter_loss: 0.19749657809734344
train_iter_loss: 0.23089422285556793
train_iter_loss: 0.3069167733192444
train_iter_loss: 0.29545897245407104
train_iter_loss: 0.3052019476890564
train_iter_loss: 0.19104158878326416
train_iter_loss: 0.19079449772834778
train_iter_loss: 0.3583061695098877
train_iter_loss: 0.2522687613964081
train_iter_loss: 0.265671044588089
train_iter_loss: 0.30791547894477844
train_iter_loss: 0.31942683458328247
train_iter_loss: 0.21977859735488892
train_iter_loss: 0.15849515795707703
train_iter_loss: 0.3292933702468872
train_iter_loss: 0.2827322781085968
train_iter_loss: 0.1621118187904358
train_iter_loss: 0.19501352310180664
train_iter_loss: 0.28572094440460205
train_iter_loss: 0.16718325018882751
train_iter_loss: 0.1974029392004013
train_iter_loss: 0.19732914865016937
train_iter_loss: 0.24670064449310303
train_iter_loss: 0.3468392491340637
train_iter_loss: 0.23035086691379547
train_iter_loss: 0.28813716769218445
train_iter_loss: 0.22841927409172058
train_iter_loss: 0.5792574882507324
train_iter_loss: 0.26073989272117615
train_iter_loss: 0.3632165193557739
train_iter_loss: 0.20163944363594055
train_iter_loss: 0.2834945321083069
train_iter_loss: 0.22275322675704956
train_iter_loss: 0.4779769778251648
train_iter_loss: 0.4229743182659149
train_iter_loss: 0.3068075180053711
train_iter_loss: 0.178423672914505
train_iter_loss: 0.38099217414855957
train_iter_loss: 0.14439624547958374
train_iter_loss: 0.2215891033411026
train_iter_loss: 0.19820331037044525
train_iter_loss: 0.27179715037345886
train_iter_loss: 0.26758310198783875
train_iter_loss: 0.21047836542129517
train_iter_loss: 0.2814851403236389
train_iter_loss: 0.36483511328697205
train_iter_loss: 0.2179833948612213
train_iter_loss: 0.28220298886299133
train_iter_loss: 0.14445222914218903
train_iter_loss: 0.3787229359149933
train_iter_loss: 0.18268130719661713
train_iter_loss: 0.28549912571907043
train_iter_loss: 0.3011993169784546
train_iter_loss: 0.15278398990631104
train_iter_loss: 0.3279275894165039
train_iter_loss: 0.3502121567726135
train_iter_loss: 0.286114901304245
train_iter_loss: 0.28070327639579773
train_iter_loss: 0.28065812587738037
train_iter_loss: 0.30824023485183716
train_iter_loss: 0.6870702505111694
train_iter_loss: 0.24139223992824554
train_iter_loss: 0.2793440818786621
train_iter_loss: 0.3430338501930237
train_iter_loss: 0.3241731524467468
train_iter_loss: 0.3322497010231018
train_iter_loss: 0.20783954858779907
train_iter_loss: 0.25143998861312866
train_iter_loss: 0.2802344560623169
train_iter_loss: 0.09400419890880585
train_iter_loss: 0.3541280925273895
train_iter_loss: 0.2376648336648941
train_iter_loss: 0.2781796455383301
train_iter_loss: 0.3916558623313904
train_iter_loss: 0.15911298990249634
train_iter_loss: 0.44138583540916443
train_iter_loss: 0.45011985301971436
train_iter_loss: 0.09335530549287796
train_iter_loss: 0.32639551162719727
train_iter_loss: 0.2279350310564041
train_iter_loss: 0.32422149181365967
train_iter_loss: 0.19557344913482666
train_iter_loss: 0.28898540139198303
train_iter_loss: 0.2599113881587982
train_iter_loss: 0.23835551738739014
train_iter_loss: 0.3573525846004486
train_iter_loss: 0.1323901265859604
train_iter_loss: 0.3093867003917694
train_iter_loss: 0.23411604762077332
train_iter_loss: 0.15695405006408691
train_iter_loss: 0.4256775975227356
train_iter_loss: 0.3017958700656891
train_iter_loss: 0.2727877199649811
train_iter_loss: 0.14261813461780548
train loss :0.2758
---------------------
Validation seg loss: 0.3763055323517688 at epoch 411
epoch =    412/  1000, exp = train
train_iter_loss: 0.22101722657680511
train_iter_loss: 0.28091374039649963
train_iter_loss: 0.267949640750885
train_iter_loss: 0.5157551169395447
train_iter_loss: 0.18547984957695007
train_iter_loss: 0.18277382850646973
train_iter_loss: 0.23360146582126617
train_iter_loss: 0.32598814368247986
train_iter_loss: 0.29733946919441223
train_iter_loss: 0.31587648391723633
train_iter_loss: 0.3261033296585083
train_iter_loss: 0.23725299537181854
train_iter_loss: 0.21796296536922455
train_iter_loss: 0.24190667271614075
train_iter_loss: 0.17214681208133698
train_iter_loss: 0.17375215888023376
train_iter_loss: 0.49182042479515076
train_iter_loss: 0.20993827283382416
train_iter_loss: 0.08540403842926025
train_iter_loss: 0.2466651350259781
train_iter_loss: 0.294546902179718
train_iter_loss: 0.16622084379196167
train_iter_loss: 0.3469811677932739
train_iter_loss: 0.37242579460144043
train_iter_loss: 0.18382543325424194
train_iter_loss: 0.15018276870250702
train_iter_loss: 0.3190464377403259
train_iter_loss: 0.23211592435836792
train_iter_loss: 0.1677921563386917
train_iter_loss: 0.3559478223323822
train_iter_loss: 0.21331417560577393
train_iter_loss: 0.1845967173576355
train_iter_loss: 0.26079070568084717
train_iter_loss: 0.26470184326171875
train_iter_loss: 0.31274083256721497
train_iter_loss: 0.35331955552101135
train_iter_loss: 0.30945736169815063
train_iter_loss: 0.3701474964618683
train_iter_loss: 0.2604379951953888
train_iter_loss: 0.8421838879585266
train_iter_loss: 0.35508766770362854
train_iter_loss: 0.21160796284675598
train_iter_loss: 0.2973262369632721
train_iter_loss: 0.44349104166030884
train_iter_loss: 0.2782418131828308
train_iter_loss: 0.2061208039522171
train_iter_loss: 0.2390032857656479
train_iter_loss: 0.14139260351657867
train_iter_loss: 0.31603187322616577
train_iter_loss: 0.2151280790567398
train_iter_loss: 0.17153939604759216
train_iter_loss: 0.2645525336265564
train_iter_loss: 0.21953438222408295
train_iter_loss: 0.1814929097890854
train_iter_loss: 0.262351393699646
train_iter_loss: 0.3272080421447754
train_iter_loss: 0.3530384600162506
train_iter_loss: 0.2497352659702301
train_iter_loss: 0.3421584963798523
train_iter_loss: 0.38574862480163574
train_iter_loss: 0.36809438467025757
train_iter_loss: 0.2575337886810303
train_iter_loss: 0.21409699320793152
train_iter_loss: 0.300792932510376
train_iter_loss: 0.32663217186927795
train_iter_loss: 0.12884539365768433
train_iter_loss: 0.13564209640026093
train_iter_loss: 0.2179233580827713
train_iter_loss: 0.14362071454524994
train_iter_loss: 0.2909088730812073
train_iter_loss: 0.23104798793792725
train_iter_loss: 0.21324089169502258
train_iter_loss: 0.3687724769115448
train_iter_loss: 0.19357067346572876
train_iter_loss: 0.28573301434516907
train_iter_loss: 0.2648571729660034
train_iter_loss: 0.27738913893699646
train_iter_loss: 0.3430115878582001
train_iter_loss: 0.09851523488759995
train_iter_loss: 0.35839876532554626
train_iter_loss: 0.29486945271492004
train_iter_loss: 0.32325294613838196
train_iter_loss: 0.315775990486145
train_iter_loss: 0.20146295428276062
train_iter_loss: 0.2339373677968979
train_iter_loss: 0.27523866295814514
train_iter_loss: 0.283020555973053
train_iter_loss: 0.35083386301994324
train_iter_loss: 0.2846636176109314
train_iter_loss: 0.44945088028907776
train_iter_loss: 0.3281114101409912
train_iter_loss: 0.2452310472726822
train_iter_loss: 0.31680387258529663
train_iter_loss: 0.24863648414611816
train_iter_loss: 0.24704180657863617
train_iter_loss: 0.3244272768497467
train_iter_loss: 0.2583741247653961
train_iter_loss: 0.3243279457092285
train_iter_loss: 0.43620193004608154
train_iter_loss: 0.1846829503774643
train loss :0.2787
---------------------
Validation seg loss: 0.36593119081830217 at epoch 412
epoch =    413/  1000, exp = train
train_iter_loss: 0.27851906418800354
train_iter_loss: 0.15519976615905762
train_iter_loss: 0.22953376173973083
train_iter_loss: 0.39853811264038086
train_iter_loss: 0.10987629741430283
train_iter_loss: 0.12999555468559265
train_iter_loss: 0.2519320845603943
train_iter_loss: 0.4619918465614319
train_iter_loss: 0.24942252039909363
train_iter_loss: 0.15247701108455658
train_iter_loss: 0.2768786549568176
train_iter_loss: 0.16513928771018982
train_iter_loss: 0.28550073504447937
train_iter_loss: 0.3307549059391022
train_iter_loss: 0.32690146565437317
train_iter_loss: 0.42757901549339294
train_iter_loss: 0.30457064509391785
train_iter_loss: 0.3308872580528259
train_iter_loss: 0.3156752288341522
train_iter_loss: 0.2557828724384308
train_iter_loss: 0.35372498631477356
train_iter_loss: 0.21666905283927917
train_iter_loss: 0.3349314033985138
train_iter_loss: 0.26926684379577637
train_iter_loss: 0.2007727175951004
train_iter_loss: 0.263936311006546
train_iter_loss: 0.34233325719833374
train_iter_loss: 0.3286517560482025
train_iter_loss: 0.23659126460552216
train_iter_loss: 0.4032480716705322
train_iter_loss: 0.23467513918876648
train_iter_loss: 0.35560113191604614
train_iter_loss: 0.16864892840385437
train_iter_loss: 0.4369391202926636
train_iter_loss: 0.23594850301742554
train_iter_loss: 0.20087113976478577
train_iter_loss: 0.30461376905441284
train_iter_loss: 0.2666947841644287
train_iter_loss: 0.3782467544078827
train_iter_loss: 0.33556196093559265
train_iter_loss: 0.3535165786743164
train_iter_loss: 0.19808778166770935
train_iter_loss: 0.23155440390110016
train_iter_loss: 0.1849682778120041
train_iter_loss: 0.2200774848461151
train_iter_loss: 0.15048536658287048
train_iter_loss: 0.2507716715335846
train_iter_loss: 0.19087933003902435
train_iter_loss: 0.1357024610042572
train_iter_loss: 0.2680875360965729
train_iter_loss: 0.33175137639045715
train_iter_loss: 0.1826660931110382
train_iter_loss: 0.3022288382053375
train_iter_loss: 0.3069113790988922
train_iter_loss: 0.15332819521427155
train_iter_loss: 0.23257826268672943
train_iter_loss: 0.3798600435256958
train_iter_loss: 0.291179895401001
train_iter_loss: 0.12996338307857513
train_iter_loss: 0.3739452064037323
train_iter_loss: 0.3076956570148468
train_iter_loss: 0.2875598073005676
train_iter_loss: 0.24341560900211334
train_iter_loss: 0.2610779404640198
train_iter_loss: 0.224516823887825
train_iter_loss: 0.16603253781795502
train_iter_loss: 0.1711856722831726
train_iter_loss: 0.3238326907157898
train_iter_loss: 0.3319123387336731
train_iter_loss: 0.23839372396469116
train_iter_loss: 0.23048248887062073
train_iter_loss: 0.20431575179100037
train_iter_loss: 0.23582547903060913
train_iter_loss: 0.24393321573734283
train_iter_loss: 0.3767467141151428
train_iter_loss: 0.13991598784923553
train_iter_loss: 0.5045728087425232
train_iter_loss: 0.27499595284461975
train_iter_loss: 0.1463896483182907
train_iter_loss: 0.3232012391090393
train_iter_loss: 0.2819921374320984
train_iter_loss: 0.2301323562860489
train_iter_loss: 0.2911764085292816
train_iter_loss: 0.23326434195041656
train_iter_loss: 0.41468244791030884
train_iter_loss: 0.2155584692955017
train_iter_loss: 0.41260844469070435
train_iter_loss: 0.20087884366512299
train_iter_loss: 0.18647201359272003
train_iter_loss: 0.1504765897989273
train_iter_loss: 0.17308193445205688
train_iter_loss: 0.14531050622463226
train_iter_loss: 0.18454396724700928
train_iter_loss: 0.36965444684028625
train_iter_loss: 0.264558345079422
train_iter_loss: 0.24966828525066376
train_iter_loss: 0.22012370824813843
train_iter_loss: 0.32997050881385803
train_iter_loss: 0.28604280948638916
train_iter_loss: 0.31843504309654236
train loss :0.2685
---------------------
Validation seg loss: 0.3661211228024496 at epoch 413
epoch =    414/  1000, exp = train
train_iter_loss: 0.4442507326602936
train_iter_loss: 0.21402350068092346
train_iter_loss: 0.3590782582759857
train_iter_loss: 0.3805449604988098
train_iter_loss: 0.5075094699859619
train_iter_loss: 0.3432038724422455
train_iter_loss: 0.19619165360927582
train_iter_loss: 0.29214078187942505
train_iter_loss: 0.28172385692596436
train_iter_loss: 0.23526760935783386
train_iter_loss: 0.2678682506084442
train_iter_loss: 0.164479598402977
train_iter_loss: 0.2776881754398346
train_iter_loss: 0.10994789004325867
train_iter_loss: 0.23600918054580688
train_iter_loss: 0.22886906564235687
train_iter_loss: 0.21472816169261932
train_iter_loss: 0.28291070461273193
train_iter_loss: 0.2227688878774643
train_iter_loss: 0.31410613656044006
train_iter_loss: 0.2754833996295929
train_iter_loss: 0.250186949968338
train_iter_loss: 0.23739799857139587
train_iter_loss: 0.18341903388500214
train_iter_loss: 0.12757772207260132
train_iter_loss: 0.3059963881969452
train_iter_loss: 0.36565595865249634
train_iter_loss: 0.16507327556610107
train_iter_loss: 0.3614019453525543
train_iter_loss: 0.3726382255554199
train_iter_loss: 0.1732511669397354
train_iter_loss: 0.3905770778656006
train_iter_loss: 0.26794135570526123
train_iter_loss: 0.19879233837127686
train_iter_loss: 0.23073643445968628
train_iter_loss: 0.2794073522090912
train_iter_loss: 0.4195905327796936
train_iter_loss: 0.26654303073883057
train_iter_loss: 0.10427971929311752
train_iter_loss: 0.2720194458961487
train_iter_loss: 0.1685481071472168
train_iter_loss: 0.20548303425312042
train_iter_loss: 0.21339726448059082
train_iter_loss: 0.2587016224861145
train_iter_loss: 0.22446446120738983
train_iter_loss: 0.12808912992477417
train_iter_loss: 0.27469608187675476
train_iter_loss: 0.1270522028207779
train_iter_loss: 0.36085987091064453
train_iter_loss: 0.34229210019111633
train_iter_loss: 0.25629183650016785
train_iter_loss: 0.26661866903305054
train_iter_loss: 0.39298778772354126
train_iter_loss: 0.21652106940746307
train_iter_loss: 0.3966020941734314
train_iter_loss: 0.24831703305244446
train_iter_loss: 0.3574482202529907
train_iter_loss: 0.37229397892951965
train_iter_loss: 0.274342805147171
train_iter_loss: 0.3799459934234619
train_iter_loss: 0.33507809042930603
train_iter_loss: 0.3718705177307129
train_iter_loss: 0.24879156053066254
train_iter_loss: 0.2748361825942993
train_iter_loss: 0.1669156849384308
train_iter_loss: 0.3965162932872772
train_iter_loss: 0.14462810754776
train_iter_loss: 0.26368868350982666
train_iter_loss: 0.35440558195114136
train_iter_loss: 0.2436477094888687
train_iter_loss: 0.1987435668706894
train_iter_loss: 0.3162761926651001
train_iter_loss: 0.24418053030967712
train_iter_loss: 0.441316157579422
train_iter_loss: 0.13531960546970367
train_iter_loss: 0.25157880783081055
train_iter_loss: 0.17872625589370728
train_iter_loss: 0.22300046682357788
train_iter_loss: 0.3233083486557007
train_iter_loss: 0.3182247579097748
train_iter_loss: 0.26653072237968445
train_iter_loss: 0.2338842898607254
train_iter_loss: 0.3734755516052246
train_iter_loss: 0.31996870040893555
train_iter_loss: 0.3155905604362488
train_iter_loss: 0.14855262637138367
train_iter_loss: 0.2004503756761551
train_iter_loss: 0.2901429831981659
train_iter_loss: 0.338206946849823
train_iter_loss: 0.3965819478034973
train_iter_loss: 0.28294557332992554
train_iter_loss: 0.14871171116828918
train_iter_loss: 0.638105571269989
train_iter_loss: 0.2790553867816925
train_iter_loss: 0.36667540669441223
train_iter_loss: 0.23722031712532043
train_iter_loss: 0.21807070076465607
train_iter_loss: 0.05866558849811554
train_iter_loss: 0.5076755285263062
train_iter_loss: 0.2925543487071991
train loss :0.2790
---------------------
Validation seg loss: 0.35873665301269797 at epoch 414
epoch =    415/  1000, exp = train
train_iter_loss: 0.21348857879638672
train_iter_loss: 0.1898064911365509
train_iter_loss: 0.3461543619632721
train_iter_loss: 0.13070107996463776
train_iter_loss: 0.3395812511444092
train_iter_loss: 0.2327170968055725
train_iter_loss: 0.22433146834373474
train_iter_loss: 0.3075990080833435
train_iter_loss: 0.25917065143585205
train_iter_loss: 0.275835782289505
train_iter_loss: 0.14772439002990723
train_iter_loss: 0.15564261376857758
train_iter_loss: 0.2935333251953125
train_iter_loss: 0.29715821146965027
train_iter_loss: 0.24770715832710266
train_iter_loss: 0.4235893487930298
train_iter_loss: 0.04945005476474762
train_iter_loss: 0.2807731032371521
train_iter_loss: 0.3496541380882263
train_iter_loss: 0.290519654750824
train_iter_loss: 0.3300519585609436
train_iter_loss: 0.35837477445602417
train_iter_loss: 0.22864115238189697
train_iter_loss: 0.4324517548084259
train_iter_loss: 0.3634859025478363
train_iter_loss: 0.23238638043403625
train_iter_loss: 0.3513089418411255
train_iter_loss: 0.27564194798469543
train_iter_loss: 0.16505496203899384
train_iter_loss: 0.23334354162216187
train_iter_loss: 0.23959368467330933
train_iter_loss: 0.2321438193321228
train_iter_loss: 0.29473376274108887
train_iter_loss: 0.16844162344932556
train_iter_loss: 0.10548073053359985
train_iter_loss: 0.2691636383533478
train_iter_loss: 0.32115796208381653
train_iter_loss: 0.2534318268299103
train_iter_loss: 0.32495543360710144
train_iter_loss: 0.16060438752174377
train_iter_loss: 0.3743290901184082
train_iter_loss: 0.16105236113071442
train_iter_loss: 0.2608541250228882
train_iter_loss: 0.2063244879245758
train_iter_loss: 0.3467921018600464
train_iter_loss: 0.32090798020362854
train_iter_loss: 0.38351908326148987
train_iter_loss: 0.3625267744064331
train_iter_loss: 0.1619245707988739
train_iter_loss: 0.3420436978340149
train_iter_loss: 0.36483997106552124
train_iter_loss: 0.45397984981536865
train_iter_loss: 0.2513372600078583
train_iter_loss: 0.2997153401374817
train_iter_loss: 0.3513941764831543
train_iter_loss: 0.2105221450328827
train_iter_loss: 0.29093295335769653
train_iter_loss: 0.2934460937976837
train_iter_loss: 0.33868327736854553
train_iter_loss: 0.2520844042301178
train_iter_loss: 0.19534917175769806
train_iter_loss: 0.24537670612335205
train_iter_loss: 0.23695456981658936
train_iter_loss: 0.28991690278053284
train_iter_loss: 0.24284398555755615
train_iter_loss: 0.2925889194011688
train_iter_loss: 0.37800782918930054
train_iter_loss: 0.18476100265979767
train_iter_loss: 0.405866801738739
train_iter_loss: 0.20643411576747894
train_iter_loss: 0.2331838458776474
train_iter_loss: 0.21646730601787567
train_iter_loss: 0.3197985291481018
train_iter_loss: 0.2176399827003479
train_iter_loss: 0.35607802867889404
train_iter_loss: 0.21990445256233215
train_iter_loss: 0.18742544949054718
train_iter_loss: 0.363363116979599
train_iter_loss: 0.3533259332180023
train_iter_loss: 0.19728755950927734
train_iter_loss: 0.20038893818855286
train_iter_loss: 0.343414306640625
train_iter_loss: 0.2493261694908142
train_iter_loss: 0.3216858506202698
train_iter_loss: 0.3248600661754608
train_iter_loss: 0.314360111951828
train_iter_loss: 0.26640164852142334
train_iter_loss: 0.20313450694084167
train_iter_loss: 0.3539382815361023
train_iter_loss: 0.17197199165821075
train_iter_loss: 0.19751586019992828
train_iter_loss: 0.21133863925933838
train_iter_loss: 0.33255836367607117
train_iter_loss: 0.3628365695476532
train_iter_loss: 0.5047044157981873
train_iter_loss: 0.428590327501297
train_iter_loss: 0.4002167284488678
train_iter_loss: 0.24800828099250793
train_iter_loss: 0.23927298188209534
train_iter_loss: 0.08890356868505478
train loss :0.2781
---------------------
Validation seg loss: 0.40508239384178285 at epoch 415
epoch =    416/  1000, exp = train
train_iter_loss: 0.3210070729255676
train_iter_loss: 0.3717573583126068
train_iter_loss: 0.17127078771591187
train_iter_loss: 0.16782699525356293
train_iter_loss: 0.08995186537504196
train_iter_loss: 0.33708176016807556
train_iter_loss: 0.2536676824092865
train_iter_loss: 0.1841578483581543
train_iter_loss: 0.4374921917915344
train_iter_loss: 0.16489504277706146
train_iter_loss: 0.2728504538536072
train_iter_loss: 0.21595071256160736
train_iter_loss: 0.32448810338974
train_iter_loss: 0.3566863238811493
train_iter_loss: 0.23662623763084412
train_iter_loss: 0.3122239112854004
train_iter_loss: 0.142873153090477
train_iter_loss: 0.28950726985931396
train_iter_loss: 0.1884729266166687
train_iter_loss: 0.25224336981773376
train_iter_loss: 0.3761630058288574
train_iter_loss: 0.30064064264297485
train_iter_loss: 0.05451156944036484
train_iter_loss: 0.16349799931049347
train_iter_loss: 0.30913108587265015
train_iter_loss: 0.4886937439441681
train_iter_loss: 0.1704130321741104
train_iter_loss: 0.42113998532295227
train_iter_loss: 0.34713324904441833
train_iter_loss: 0.2938291132450104
train_iter_loss: 0.2410775125026703
train_iter_loss: 0.18231827020645142
train_iter_loss: 0.3691965639591217
train_iter_loss: 0.3558034598827362
train_iter_loss: 0.2562730610370636
train_iter_loss: 0.2075650840997696
train_iter_loss: 0.20180581510066986
train_iter_loss: 0.3552412986755371
train_iter_loss: 0.30824029445648193
train_iter_loss: 0.17821216583251953
train_iter_loss: 0.24341259896755219
train_iter_loss: 0.24837200343608856
train_iter_loss: 0.28639352321624756
train_iter_loss: 0.20900943875312805
train_iter_loss: 0.2549065351486206
train_iter_loss: 0.3031961917877197
train_iter_loss: 0.2881336808204651
train_iter_loss: 0.2926078140735626
train_iter_loss: 0.3952086865901947
train_iter_loss: 0.33210840821266174
train_iter_loss: 0.3472577929496765
train_iter_loss: 0.2931801974773407
train_iter_loss: 0.2764003872871399
train_iter_loss: 0.23717494308948517
train_iter_loss: 0.32174232602119446
train_iter_loss: 0.16213113069534302
train_iter_loss: 0.23017357289791107
train_iter_loss: 0.15639832615852356
train_iter_loss: 0.3070397973060608
train_iter_loss: 0.37115204334259033
train_iter_loss: 0.2723071575164795
train_iter_loss: 0.2747931182384491
train_iter_loss: 0.5670393705368042
train_iter_loss: 0.29500019550323486
train_iter_loss: 0.3261508345603943
train_iter_loss: 0.2590058445930481
train_iter_loss: 0.2331976741552353
train_iter_loss: 0.20110616087913513
train_iter_loss: 0.3140273690223694
train_iter_loss: 0.30062150955200195
train_iter_loss: 0.1880384087562561
train_iter_loss: 0.08423810452222824
train_iter_loss: 0.22994239628314972
train_iter_loss: 0.35731446743011475
train_iter_loss: 0.27255669236183167
train_iter_loss: 0.2736334800720215
train_iter_loss: 0.24419675767421722
train_iter_loss: 0.344035804271698
train_iter_loss: 0.3604739010334015
train_iter_loss: 0.26709696650505066
train_iter_loss: 0.34050095081329346
train_iter_loss: 0.36392346024513245
train_iter_loss: 0.1586609035730362
train_iter_loss: 0.24853083491325378
train_iter_loss: 0.4327983856201172
train_iter_loss: 0.18604694306850433
train_iter_loss: 0.25684159994125366
train_iter_loss: 0.27102208137512207
train_iter_loss: 0.18057985603809357
train_iter_loss: 0.29698804020881653
train_iter_loss: 0.22469191253185272
train_iter_loss: 0.3499702513217926
train_iter_loss: 0.14913426339626312
train_iter_loss: 0.25404641032218933
train_iter_loss: 0.21123924851417542
train_iter_loss: 0.28288817405700684
train_iter_loss: 0.25547048449516296
train_iter_loss: 0.24094931781291962
train_iter_loss: 0.4511093497276306
train_iter_loss: 0.2790159285068512
train loss :0.2761
---------------------
Validation seg loss: 0.3815054288133979 at epoch 416
epoch =    417/  1000, exp = train
train_iter_loss: 0.20444294810295105
train_iter_loss: 0.17735710740089417
train_iter_loss: 0.19505956768989563
train_iter_loss: 0.19487248361110687
train_iter_loss: 0.23893004655838013
train_iter_loss: 0.087550088763237
train_iter_loss: 0.2070721834897995
train_iter_loss: 0.3298458755016327
train_iter_loss: 0.07168851047754288
train_iter_loss: 0.2841949164867401
train_iter_loss: 0.20304344594478607
train_iter_loss: 0.33767613768577576
train_iter_loss: 0.22310800850391388
train_iter_loss: 0.30849307775497437
train_iter_loss: 0.16047601401805878
train_iter_loss: 0.3080974817276001
train_iter_loss: 0.3402911126613617
train_iter_loss: 0.33213964104652405
train_iter_loss: 0.18393728137016296
train_iter_loss: 0.1148756593465805
train_iter_loss: 0.27396073937416077
train_iter_loss: 0.5195749402046204
train_iter_loss: 0.3564194142818451
train_iter_loss: 0.25470593571662903
train_iter_loss: 0.2692645490169525
train_iter_loss: 0.2022993117570877
train_iter_loss: 0.44231539964675903
train_iter_loss: 0.2357521951198578
train_iter_loss: 0.38707345724105835
train_iter_loss: 0.4443197250366211
train_iter_loss: 0.1905093938112259
train_iter_loss: 0.22515057027339935
train_iter_loss: 0.4226699471473694
train_iter_loss: 0.28975167870521545
train_iter_loss: 0.20531463623046875
train_iter_loss: 0.3072178363800049
train_iter_loss: 0.24221685528755188
train_iter_loss: 0.33704668283462524
train_iter_loss: 0.15148575603961945
train_iter_loss: 0.22192752361297607
train_iter_loss: 0.2725798189640045
train_iter_loss: 0.2547926902770996
train_iter_loss: 0.25910261273384094
train_iter_loss: 0.2164154052734375
train_iter_loss: 0.451717346906662
train_iter_loss: 0.40225350856781006
train_iter_loss: 0.29576030373573303
train_iter_loss: 0.32300880551338196
train_iter_loss: 0.18902596831321716
train_iter_loss: 0.31059786677360535
train_iter_loss: 0.41063061356544495
train_iter_loss: 0.2029641568660736
train_iter_loss: 0.27312830090522766
train_iter_loss: 0.2761940360069275
train_iter_loss: 0.2691528797149658
train_iter_loss: 0.34731316566467285
train_iter_loss: 0.12849082052707672
train_iter_loss: 0.28044992685317993
train_iter_loss: 0.14593005180358887
train_iter_loss: 0.3086743652820587
train_iter_loss: 0.451885849237442
train_iter_loss: 0.392705500125885
train_iter_loss: 0.2696117162704468
train_iter_loss: 0.35398703813552856
train_iter_loss: 0.20412681996822357
train_iter_loss: 0.178524911403656
train_iter_loss: 0.20279647409915924
train_iter_loss: 0.23936112225055695
train_iter_loss: 0.5499761700630188
train_iter_loss: 0.2877742648124695
train_iter_loss: 0.29165104031562805
train_iter_loss: 0.4179389476776123
train_iter_loss: 0.42669257521629333
train_iter_loss: 0.22370751202106476
train_iter_loss: 0.24338410794734955
train_iter_loss: 0.3050130605697632
train_iter_loss: 0.2936107814311981
train_iter_loss: 0.24298158288002014
train_iter_loss: 0.29318803548812866
train_iter_loss: 0.19368872046470642
train_iter_loss: 0.2675113379955292
train_iter_loss: 0.25412970781326294
train_iter_loss: 0.1638454794883728
train_iter_loss: 0.19304616749286652
train_iter_loss: 0.31441953778266907
train_iter_loss: 0.38145264983177185
train_iter_loss: 0.17310254275798798
train_iter_loss: 0.3618507385253906
train_iter_loss: 0.3773093521595001
train_iter_loss: 0.25841376185417175
train_iter_loss: 0.25745511054992676
train_iter_loss: 0.2369849979877472
train_iter_loss: 0.2508561313152313
train_iter_loss: 0.22156532108783722
train_iter_loss: 0.24905332922935486
train_iter_loss: 0.3028753399848938
train_iter_loss: 0.29651254415512085
train_iter_loss: 0.38499966263771057
train_iter_loss: 0.3197169601917267
train_iter_loss: 0.39421117305755615
train loss :0.2810
---------------------
Validation seg loss: 0.41387643668589724 at epoch 417
epoch =    418/  1000, exp = train
train_iter_loss: 0.26563969254493713
train_iter_loss: 0.1539304405450821
train_iter_loss: 0.3469759523868561
train_iter_loss: 0.39713603258132935
train_iter_loss: 0.3432140052318573
train_iter_loss: 0.35035189986228943
train_iter_loss: 0.23941977322101593
train_iter_loss: 0.1862906813621521
train_iter_loss: 0.21927200257778168
train_iter_loss: 0.2966820299625397
train_iter_loss: 0.32738545536994934
train_iter_loss: 0.35542261600494385
train_iter_loss: 0.3152172863483429
train_iter_loss: 0.3404283821582794
train_iter_loss: 0.26122593879699707
train_iter_loss: 0.24170468747615814
train_iter_loss: 0.26945018768310547
train_iter_loss: 0.15244701504707336
train_iter_loss: 0.21960052847862244
train_iter_loss: 0.1809290200471878
train_iter_loss: 0.1692570298910141
train_iter_loss: 0.14685997366905212
train_iter_loss: 0.43178480863571167
train_iter_loss: 0.31917351484298706
train_iter_loss: 0.23476360738277435
train_iter_loss: 0.25473394989967346
train_iter_loss: 0.33061304688453674
train_iter_loss: 0.22092708945274353
train_iter_loss: 0.17616640031337738
train_iter_loss: 0.22912967205047607
train_iter_loss: 0.2027876377105713
train_iter_loss: 0.33128389716148376
train_iter_loss: 0.233455628156662
train_iter_loss: 0.3275548219680786
train_iter_loss: 0.2200678437948227
train_iter_loss: 0.4194316864013672
train_iter_loss: 0.2975771725177765
train_iter_loss: 0.19272609055042267
train_iter_loss: 0.21231719851493835
train_iter_loss: 0.33466508984565735
train_iter_loss: 0.26833227276802063
train_iter_loss: 0.30777180194854736
train_iter_loss: 0.3433232307434082
train_iter_loss: 0.3768331706523895
train_iter_loss: 0.11377019435167313
train_iter_loss: 0.21459364891052246
train_iter_loss: 0.18617941439151764
train_iter_loss: 0.2707888185977936
train_iter_loss: 0.3059079945087433
train_iter_loss: 0.3513815104961395
train_iter_loss: 0.25149187445640564
train_iter_loss: 0.17289207875728607
train_iter_loss: 0.2472376823425293
train_iter_loss: 0.3071530759334564
train_iter_loss: 0.15740381181240082
train_iter_loss: 0.18138933181762695
train_iter_loss: 0.20640724897384644
train_iter_loss: 0.5552352070808411
train_iter_loss: 0.23406076431274414
train_iter_loss: 0.29345640540122986
train_iter_loss: 0.4265751540660858
train_iter_loss: 0.196219801902771
train_iter_loss: 0.17995670437812805
train_iter_loss: 0.19607727229595184
train_iter_loss: 0.26064613461494446
train_iter_loss: 0.3250430226325989
train_iter_loss: 0.21481040120124817
train_iter_loss: 0.46782106161117554
train_iter_loss: 0.3084743320941925
train_iter_loss: 0.2523459792137146
train_iter_loss: 0.2378125935792923
train_iter_loss: 0.2436988204717636
train_iter_loss: 0.11418639868497849
train_iter_loss: 0.41511255502700806
train_iter_loss: 0.31932419538497925
train_iter_loss: 0.3127414882183075
train_iter_loss: 0.3132116496562958
train_iter_loss: 0.30263277888298035
train_iter_loss: 0.2267093062400818
train_iter_loss: 0.2297416627407074
train_iter_loss: 0.279866486787796
train_iter_loss: 0.23319117724895477
train_iter_loss: 0.35521382093429565
train_iter_loss: 0.18645399808883667
train_iter_loss: 0.35735806822776794
train_iter_loss: 0.2744749188423157
train_iter_loss: 0.1970566064119339
train_iter_loss: 0.36285197734832764
train_iter_loss: 0.20680582523345947
train_iter_loss: 0.3789287507534027
train_iter_loss: 0.13875648379325867
train_iter_loss: 0.20841728150844574
train_iter_loss: 0.2587427496910095
train_iter_loss: 0.33020567893981934
train_iter_loss: 0.2641362249851227
train_iter_loss: 0.2349473088979721
train_iter_loss: 0.19191525876522064
train_iter_loss: 0.25911468267440796
train_iter_loss: 0.340040922164917
train_iter_loss: 0.30608895421028137
train loss :0.2728
---------------------
Validation seg loss: 0.37666677740701243 at epoch 418
epoch =    419/  1000, exp = train
train_iter_loss: 0.3287935256958008
train_iter_loss: 0.23629359900951385
train_iter_loss: 0.23045958578586578
train_iter_loss: 0.2675946056842804
train_iter_loss: 0.3695068359375
train_iter_loss: 0.23313994705677032
train_iter_loss: 0.31395629048347473
train_iter_loss: 0.2812187671661377
train_iter_loss: 0.09386283904314041
train_iter_loss: 0.39752334356307983
train_iter_loss: 0.17845888435840607
train_iter_loss: 0.26185306906700134
train_iter_loss: 0.3694198429584503
train_iter_loss: 0.2650522291660309
train_iter_loss: 0.30200913548469543
train_iter_loss: 0.0728759914636612
train_iter_loss: 0.43905845284461975
train_iter_loss: 0.20124734938144684
train_iter_loss: 0.18696579337120056
train_iter_loss: 0.20542632043361664
train_iter_loss: 0.2113354355096817
train_iter_loss: 0.15823736786842346
train_iter_loss: 0.08183205872774124
train_iter_loss: 0.10263748466968536
train_iter_loss: 0.3557201623916626
train_iter_loss: 0.26424920558929443
train_iter_loss: 0.3745841979980469
train_iter_loss: 0.19222164154052734
train_iter_loss: 0.3081998825073242
train_iter_loss: 0.2304762899875641
train_iter_loss: 0.2737349569797516
train_iter_loss: 0.23803697526454926
train_iter_loss: 0.5064412355422974
train_iter_loss: 0.2522639036178589
train_iter_loss: 0.3902537226676941
train_iter_loss: 0.1848761886358261
train_iter_loss: 0.3508301377296448
train_iter_loss: 0.22827179729938507
train_iter_loss: 0.2375037968158722
train_iter_loss: 0.28846418857574463
train_iter_loss: 0.20183904469013214
train_iter_loss: 0.16226285696029663
train_iter_loss: 0.3703196942806244
train_iter_loss: 0.26175448298454285
train_iter_loss: 0.2973807454109192
train_iter_loss: 0.17915984988212585
train_iter_loss: 0.3331547975540161
train_iter_loss: 0.4110836982727051
train_iter_loss: 0.21011807024478912
train_iter_loss: 0.32520732283592224
train_iter_loss: 0.44886988401412964
train_iter_loss: 0.17550541460514069
train_iter_loss: 0.2734704911708832
train_iter_loss: 0.22169627249240875
train_iter_loss: 0.23237386345863342
train_iter_loss: 0.37278318405151367
train_iter_loss: 0.363741934299469
train_iter_loss: 0.21995742619037628
train_iter_loss: 0.35644906759262085
train_iter_loss: 0.31577983498573303
train_iter_loss: 0.34781908988952637
train_iter_loss: 0.4253733456134796
train_iter_loss: 0.23835599422454834
train_iter_loss: 0.19224244356155396
train_iter_loss: 0.36630141735076904
train_iter_loss: 0.3167078495025635
train_iter_loss: 0.29905661940574646
train_iter_loss: 0.3213942050933838
train_iter_loss: 0.1865275651216507
train_iter_loss: 0.2918367385864258
train_iter_loss: 0.3241886496543884
train_iter_loss: 0.24463051557540894
train_iter_loss: 0.1440882384777069
train_iter_loss: 0.055853791534900665
train_iter_loss: 0.2475055307149887
train_iter_loss: 0.2966326177120209
train_iter_loss: 0.25317659974098206
train_iter_loss: 0.3999195992946625
train_iter_loss: 0.4162517488002777
train_iter_loss: 0.22580024600028992
train_iter_loss: 0.2932370603084564
train_iter_loss: 0.24615263938903809
train_iter_loss: 0.3100321888923645
train_iter_loss: 0.22582636773586273
train_iter_loss: 0.09983734041452408
train_iter_loss: 0.2779483199119568
train_iter_loss: 0.20696799457073212
train_iter_loss: 0.3416846990585327
train_iter_loss: 0.3398194909095764
train_iter_loss: 0.19099542498588562
train_iter_loss: 0.23781102895736694
train_iter_loss: 0.3642316460609436
train_iter_loss: 0.09492993354797363
train_iter_loss: 0.234855055809021
train_iter_loss: 0.27514660358428955
train_iter_loss: 0.2575415074825287
train_iter_loss: 0.2672145962715149
train_iter_loss: 0.23588897287845612
train_iter_loss: 0.32428574562072754
train_iter_loss: 0.2823575437068939
train loss :0.2717
---------------------
Validation seg loss: 0.39604502179624756 at epoch 419
epoch =    420/  1000, exp = train
train_iter_loss: 0.23878633975982666
train_iter_loss: 0.2938483655452728
train_iter_loss: 0.2806798219680786
train_iter_loss: 0.1795731633901596
train_iter_loss: 0.3639906942844391
train_iter_loss: 0.39893248677253723
train_iter_loss: 0.1365341693162918
train_iter_loss: 0.3088192045688629
train_iter_loss: 0.17412112653255463
train_iter_loss: 0.3121926486492157
train_iter_loss: 0.24236679077148438
train_iter_loss: 0.37443867325782776
train_iter_loss: 0.28755760192871094
train_iter_loss: 0.23765581846237183
train_iter_loss: 0.3134019374847412
train_iter_loss: 0.21503891050815582
train_iter_loss: 0.2854526937007904
train_iter_loss: 0.36266279220581055
train_iter_loss: 0.06385885924100876
train_iter_loss: 0.2489783614873886
train_iter_loss: 0.2551088035106659
train_iter_loss: 0.2508488595485687
train_iter_loss: 0.36101672053337097
train_iter_loss: 0.2106350064277649
train_iter_loss: 0.22265218198299408
train_iter_loss: 0.3770192563533783
train_iter_loss: 0.3097613751888275
train_iter_loss: 0.2849985361099243
train_iter_loss: 0.24178236722946167
train_iter_loss: 0.2273932248353958
train_iter_loss: 0.3457236886024475
train_iter_loss: 0.2558353543281555
train_iter_loss: 0.27187681198120117
train_iter_loss: 0.16470219194889069
train_iter_loss: 0.20974069833755493
train_iter_loss: 0.10173308104276657
train_iter_loss: 0.32354944944381714
train_iter_loss: 0.23866793513298035
train_iter_loss: 0.2043197602033615
train_iter_loss: 0.3544699251651764
train_iter_loss: 0.3434900641441345
train_iter_loss: 0.27978643774986267
train_iter_loss: 0.2836347818374634
train_iter_loss: 0.3346264362335205
train_iter_loss: 0.208390474319458
train_iter_loss: 0.26941588521003723
train_iter_loss: 0.20538340508937836
train_iter_loss: 0.2953574061393738
train_iter_loss: 0.3209854066371918
train_iter_loss: 0.18741992115974426
train_iter_loss: 0.4051118791103363
train_iter_loss: 0.29756250977516174
train_iter_loss: 0.38220465183258057
train_iter_loss: 0.14464756846427917
train_iter_loss: 0.22455570101737976
train_iter_loss: 0.2203259915113449
train_iter_loss: 0.1977783888578415
train_iter_loss: 0.38017433881759644
train_iter_loss: 0.42037343978881836
train_iter_loss: 0.3405590057373047
train_iter_loss: 0.2514401376247406
train_iter_loss: 0.2543519139289856
train_iter_loss: 0.3183991312980652
train_iter_loss: 0.2718622088432312
train_iter_loss: 0.3017981946468353
train_iter_loss: 0.2992970943450928
train_iter_loss: 0.3496219217777252
train_iter_loss: 0.295067697763443
train_iter_loss: 0.39331966638565063
train_iter_loss: 0.23079903423786163
train_iter_loss: 0.3246452808380127
train_iter_loss: 0.33708781003952026
train_iter_loss: 0.08401515334844589
train_iter_loss: 0.11267837882041931
train_iter_loss: 0.3601318299770355
train_iter_loss: 0.19393491744995117
train_iter_loss: 0.2927893400192261
train_iter_loss: 0.2341385781764984
train_iter_loss: 0.3202075958251953
train_iter_loss: 0.3874950110912323
train_iter_loss: 0.27379360795021057
train_iter_loss: 0.27275651693344116
train_iter_loss: 0.1531626284122467
train_iter_loss: 0.37010514736175537
train_iter_loss: 0.21680349111557007
train_iter_loss: 0.1127098798751831
train_iter_loss: 0.2646293044090271
train_iter_loss: 0.2599240839481354
train_iter_loss: 0.2808094024658203
train_iter_loss: 0.3197793960571289
train_iter_loss: 0.36466851830482483
train_iter_loss: 0.28631019592285156
train_iter_loss: 0.3447858393192291
train_iter_loss: 0.053904298692941666
train_iter_loss: 0.2536777853965759
train_iter_loss: 0.16623163223266602
train_iter_loss: 0.24345768988132477
train_iter_loss: 0.18167944252490997
train_iter_loss: 0.30321481823921204
train_iter_loss: 0.255908340215683
train loss :0.2714
---------------------
Validation seg loss: 0.38205660757485704 at epoch 420
epoch =    421/  1000, exp = train
train_iter_loss: 0.16271542012691498
train_iter_loss: 0.33082759380340576
train_iter_loss: 0.22224240005016327
train_iter_loss: 0.2519319951534271
train_iter_loss: 0.3158649802207947
train_iter_loss: 0.4140917658805847
train_iter_loss: 0.23250170052051544
train_iter_loss: 0.33477768301963806
train_iter_loss: 0.15615449845790863
train_iter_loss: 0.39900240302085876
train_iter_loss: 0.12467532604932785
train_iter_loss: 0.2681720554828644
train_iter_loss: 0.2295830249786377
train_iter_loss: 0.3143156170845032
train_iter_loss: 0.2235487997531891
train_iter_loss: 0.2169845849275589
train_iter_loss: 0.3062601387500763
train_iter_loss: 0.28576868772506714
train_iter_loss: 0.20133738219738007
train_iter_loss: 0.37841862440109253
train_iter_loss: 0.4443453848361969
train_iter_loss: 0.026114078238606453
train_iter_loss: 0.23063667118549347
train_iter_loss: 0.1922772228717804
train_iter_loss: 0.25305113196372986
train_iter_loss: 0.27112942934036255
train_iter_loss: 0.2927842140197754
train_iter_loss: 0.36033323407173157
train_iter_loss: 0.1814446747303009
train_iter_loss: 0.20568808913230896
train_iter_loss: 0.3533726632595062
train_iter_loss: 0.28770583868026733
train_iter_loss: 0.1379050612449646
train_iter_loss: 0.17938056588172913
train_iter_loss: 0.2412247210741043
train_iter_loss: 0.3938985764980316
train_iter_loss: 0.3071860074996948
train_iter_loss: 0.2906358242034912
train_iter_loss: 0.11262492090463638
train_iter_loss: 0.32462188601493835
train_iter_loss: 0.18462134897708893
train_iter_loss: 0.30709075927734375
train_iter_loss: 0.18202091753482819
train_iter_loss: 0.3929276168346405
train_iter_loss: 0.30286988615989685
train_iter_loss: 0.23698024451732635
train_iter_loss: 0.35505327582359314
train_iter_loss: 0.40943798422813416
train_iter_loss: 0.09874902665615082
train_iter_loss: 0.3526555001735687
train_iter_loss: 0.2748142182826996
train_iter_loss: 0.22509892284870148
train_iter_loss: 0.19616548717021942
train_iter_loss: 0.22168497741222382
train_iter_loss: 0.23023764789104462
train_iter_loss: 0.37518343329429626
train_iter_loss: 0.3209146559238434
train_iter_loss: 0.2258404642343521
train_iter_loss: 0.34662026166915894
train_iter_loss: 0.3830990195274353
train_iter_loss: 0.13029804825782776
train_iter_loss: 0.24591444432735443
train_iter_loss: 0.15228094160556793
train_iter_loss: 0.4012059271335602
train_iter_loss: 0.23922671377658844
train_iter_loss: 0.3418334722518921
train_iter_loss: 0.1643505096435547
train_iter_loss: 0.2740899622440338
train_iter_loss: 0.4789564311504364
train_iter_loss: 0.27069091796875
train_iter_loss: 0.4291980564594269
train_iter_loss: 0.13149674236774445
train_iter_loss: 0.18150995671749115
train_iter_loss: 0.21048223972320557
train_iter_loss: 0.2247523069381714
train_iter_loss: 0.48909223079681396
train_iter_loss: 0.16601723432540894
train_iter_loss: 0.24261480569839478
train_iter_loss: 0.1317371279001236
train_iter_loss: 0.16789695620536804
train_iter_loss: 0.36065566539764404
train_iter_loss: 0.19236303865909576
train_iter_loss: 0.36731472611427307
train_iter_loss: 0.2864397168159485
train_iter_loss: 0.28721413016319275
train_iter_loss: 0.28496041893959045
train_iter_loss: 0.20956331491470337
train_iter_loss: 0.1795501559972763
train_iter_loss: 0.3696315288543701
train_iter_loss: 0.23026862740516663
train_iter_loss: 0.33923885226249695
train_iter_loss: 0.26333343982696533
train_iter_loss: 0.3131559491157532
train_iter_loss: 0.3703854978084564
train_iter_loss: 0.28179270029067993
train_iter_loss: 0.3093715012073517
train_iter_loss: 0.349376380443573
train_iter_loss: 0.20508553087711334
train_iter_loss: 0.3601248264312744
train_iter_loss: 0.3866126537322998
train loss :0.2737
---------------------
Validation seg loss: 0.3537929072747675 at epoch 421
epoch =    422/  1000, exp = train
train_iter_loss: 0.12074168771505356
train_iter_loss: 0.21990615129470825
train_iter_loss: 0.39083677530288696
train_iter_loss: 0.09131543338298798
train_iter_loss: 0.2795093059539795
train_iter_loss: 0.28969934582710266
train_iter_loss: 0.2903262972831726
train_iter_loss: 0.29584163427352905
train_iter_loss: 0.24854575097560883
train_iter_loss: 0.30972620844841003
train_iter_loss: 0.44588249921798706
train_iter_loss: 0.20841673016548157
train_iter_loss: 0.3111720383167267
train_iter_loss: 0.5049727559089661
train_iter_loss: 0.2780996263027191
train_iter_loss: 0.26524946093559265
train_iter_loss: 0.23095166683197021
train_iter_loss: 0.29135313630104065
train_iter_loss: 0.4243381917476654
train_iter_loss: 0.2748606503009796
train_iter_loss: 0.33151859045028687
train_iter_loss: 0.3159449100494385
train_iter_loss: 0.23621992766857147
train_iter_loss: 0.27799415588378906
train_iter_loss: 0.2818547785282135
train_iter_loss: 0.22501380741596222
train_iter_loss: 0.21883708238601685
train_iter_loss: 0.3325468897819519
train_iter_loss: 0.2093638926744461
train_iter_loss: 0.3700624406337738
train_iter_loss: 0.39895012974739075
train_iter_loss: 0.09878065437078476
train_iter_loss: 0.3710233271121979
train_iter_loss: 0.26872682571411133
train_iter_loss: 0.34048905968666077
train_iter_loss: 0.1861320286989212
train_iter_loss: 0.40777572989463806
train_iter_loss: 0.3596440553665161
train_iter_loss: 0.22102564573287964
train_iter_loss: 0.1129673421382904
train_iter_loss: 0.4332418441772461
train_iter_loss: 0.1675882190465927
train_iter_loss: 0.37779444456100464
train_iter_loss: 0.2518412172794342
train_iter_loss: 0.20471197366714478
train_iter_loss: 0.2288808673620224
train_iter_loss: 0.13925570249557495
train_iter_loss: 0.26059868931770325
train_iter_loss: 0.17065629363059998
train_iter_loss: 0.29392802715301514
train_iter_loss: 0.18269073963165283
train_iter_loss: 0.2647317945957184
train_iter_loss: 0.16964729130268097
train_iter_loss: 0.19888444244861603
train_iter_loss: 0.19937863945960999
train_iter_loss: 0.375640332698822
train_iter_loss: 0.3156682252883911
train_iter_loss: 0.33927592635154724
train_iter_loss: 0.3331196904182434
train_iter_loss: 0.29992565512657166
train_iter_loss: 0.34099510312080383
train_iter_loss: 0.2764166295528412
train_iter_loss: 0.2525244951248169
train_iter_loss: 0.18004165589809418
train_iter_loss: 0.23153728246688843
train_iter_loss: 0.2523116171360016
train_iter_loss: 0.1987306773662567
train_iter_loss: 0.26420074701309204
train_iter_loss: 0.44532454013824463
train_iter_loss: 0.2352612316608429
train_iter_loss: 0.29611217975616455
train_iter_loss: 0.5625039935112
train_iter_loss: 0.29316461086273193
train_iter_loss: 0.35646530985832214
train_iter_loss: 0.3052358031272888
train_iter_loss: 0.16871590912342072
train_iter_loss: 0.3168732821941376
train_iter_loss: 0.14885519444942474
train_iter_loss: 0.25490763783454895
train_iter_loss: 0.20868077874183655
train_iter_loss: 0.31101396679878235
train_iter_loss: 0.22095414996147156
train_iter_loss: 0.4202495813369751
train_iter_loss: 0.3405448794364929
train_iter_loss: 0.2710115313529968
train_iter_loss: 0.27265340089797974
train_iter_loss: 0.23220203816890717
train_iter_loss: 0.24831357598304749
train_iter_loss: 0.25185638666152954
train_iter_loss: 0.30350640416145325
train_iter_loss: 0.239506796002388
train_iter_loss: 0.2663886547088623
train_iter_loss: 0.2877523899078369
train_iter_loss: 0.24490219354629517
train_iter_loss: 0.07950158417224884
train_iter_loss: 0.25012800097465515
train_iter_loss: 0.2625453472137451
train_iter_loss: 0.21364064514636993
train_iter_loss: 0.301841676235199
train_iter_loss: 0.3374496400356293
train loss :0.2776
---------------------
Validation seg loss: 0.3684136463627922 at epoch 422
epoch =    423/  1000, exp = train
train_iter_loss: 0.15247373282909393
train_iter_loss: 0.19733041524887085
train_iter_loss: 0.37398573756217957
train_iter_loss: 0.24044397473335266
train_iter_loss: 0.22000567615032196
train_iter_loss: 0.263739675283432
train_iter_loss: 0.37154868245124817
train_iter_loss: 0.18835850059986115
train_iter_loss: 0.07434819638729095
train_iter_loss: 0.17234809696674347
train_iter_loss: 0.36890140175819397
train_iter_loss: 0.20826798677444458
train_iter_loss: 0.3630112111568451
train_iter_loss: 0.14731745421886444
train_iter_loss: 0.253984272480011
train_iter_loss: 0.30452266335487366
train_iter_loss: 0.29215574264526367
train_iter_loss: 0.3594267666339874
train_iter_loss: 0.1408018320798874
train_iter_loss: 0.35574838519096375
train_iter_loss: 0.2537662386894226
train_iter_loss: 0.36826443672180176
train_iter_loss: 0.1606493592262268
train_iter_loss: 0.15603961050510406
train_iter_loss: 0.3732563257217407
train_iter_loss: 0.3600214123725891
train_iter_loss: 0.29972973465919495
train_iter_loss: 0.3887134790420532
train_iter_loss: 0.2258949875831604
train_iter_loss: 0.23326587677001953
train_iter_loss: 0.2540728449821472
train_iter_loss: 0.19239000976085663
train_iter_loss: 0.12075863033533096
train_iter_loss: 0.32735416293144226
train_iter_loss: 0.10126391053199768
train_iter_loss: 0.3464933931827545
train_iter_loss: 0.3643111288547516
train_iter_loss: 0.22723837196826935
train_iter_loss: 0.3291027545928955
train_iter_loss: 0.2524351477622986
train_iter_loss: 0.2955988943576813
train_iter_loss: 0.3415954113006592
train_iter_loss: 0.29608672857284546
train_iter_loss: 0.3002607524394989
train_iter_loss: 0.39997541904449463
train_iter_loss: 0.3149700164794922
train_iter_loss: 0.25109806656837463
train_iter_loss: 0.2247007042169571
train_iter_loss: 0.28162360191345215
train_iter_loss: 0.11650195717811584
train_iter_loss: 0.38415178656578064
train_iter_loss: 0.554778516292572
train_iter_loss: 0.2332710474729538
train_iter_loss: 0.3217933475971222
train_iter_loss: 0.2952505350112915
train_iter_loss: 0.19739270210266113
train_iter_loss: 0.25444352626800537
train_iter_loss: 0.18886564671993256
train_iter_loss: 0.24289944767951965
train_iter_loss: 0.1768660545349121
train_iter_loss: 0.31786003708839417
train_iter_loss: 0.43758469820022583
train_iter_loss: 0.281747967004776
train_iter_loss: 0.43606987595558167
train_iter_loss: 0.11874444782733917
train_iter_loss: 0.0944857969880104
train_iter_loss: 0.35454049706459045
train_iter_loss: 0.2116028517484665
train_iter_loss: 0.28228458762168884
train_iter_loss: 0.2029682695865631
train_iter_loss: 0.2902582585811615
train_iter_loss: 0.28228631615638733
train_iter_loss: 0.3260459899902344
train_iter_loss: 0.2384323626756668
train_iter_loss: 0.38280540704727173
train_iter_loss: 0.18820805847644806
train_iter_loss: 0.21322105824947357
train_iter_loss: 0.30028828978538513
train_iter_loss: 0.28569266200065613
train_iter_loss: 0.3135357201099396
train_iter_loss: 0.22665391862392426
train_iter_loss: 0.19909366965293884
train_iter_loss: 0.20211821794509888
train_iter_loss: 0.34139272570610046
train_iter_loss: 0.3331805169582367
train_iter_loss: 0.388420432806015
train_iter_loss: 0.32800018787384033
train_iter_loss: 0.2847047448158264
train_iter_loss: 0.13988454639911652
train_iter_loss: 0.287579745054245
train_iter_loss: 0.2907145321369171
train_iter_loss: 0.2902146279811859
train_iter_loss: 0.10084086656570435
train_iter_loss: 0.2634262144565582
train_iter_loss: 0.25675979256629944
train_iter_loss: 0.27574217319488525
train_iter_loss: 0.267455130815506
train_iter_loss: 0.2155650109052658
train_iter_loss: 0.2520465552806854
train_iter_loss: 0.34042787551879883
train loss :0.2718
---------------------
Validation seg loss: 0.3770033812835672 at epoch 423
epoch =    424/  1000, exp = train
train_iter_loss: 0.33315521478652954
train_iter_loss: 0.18483972549438477
train_iter_loss: 0.27325960993766785
train_iter_loss: 0.29806143045425415
train_iter_loss: 0.18049079179763794
train_iter_loss: 0.23102323710918427
train_iter_loss: 0.2163379192352295
train_iter_loss: 0.12876607477664948
train_iter_loss: 0.2678578794002533
train_iter_loss: 0.3262386620044708
train_iter_loss: 0.10498079657554626
train_iter_loss: 0.2521795928478241
train_iter_loss: 0.2868269085884094
train_iter_loss: 0.20899052917957306
train_iter_loss: 0.3795519471168518
train_iter_loss: 0.35351547598838806
train_iter_loss: 0.27675846219062805
train_iter_loss: 0.17873133718967438
train_iter_loss: 0.2862437665462494
train_iter_loss: 0.46122875809669495
train_iter_loss: 0.11495382338762283
train_iter_loss: 0.2373638153076172
train_iter_loss: 0.3683711588382721
train_iter_loss: 0.13573028147220612
train_iter_loss: 0.288126677274704
train_iter_loss: 0.3591007590293884
train_iter_loss: 0.3310444951057434
train_iter_loss: 0.2899765372276306
train_iter_loss: 0.15108691155910492
train_iter_loss: 0.17479629814624786
train_iter_loss: 0.2510761022567749
train_iter_loss: 0.3970760703086853
train_iter_loss: 0.2579975128173828
train_iter_loss: 0.3098861277103424
train_iter_loss: 0.23543410003185272
train_iter_loss: 0.22765013575553894
train_iter_loss: 0.44757145643234253
train_iter_loss: 0.30611473321914673
train_iter_loss: 0.23357245326042175
train_iter_loss: 0.08675486594438553
train_iter_loss: 0.17135705053806305
train_iter_loss: 0.2825789749622345
train_iter_loss: 0.16762420535087585
train_iter_loss: 0.3336985111236572
train_iter_loss: 0.18409879505634308
train_iter_loss: 0.32946211099624634
train_iter_loss: 0.2769608199596405
train_iter_loss: 0.4032655954360962
train_iter_loss: 0.18050111830234528
train_iter_loss: 0.47739148139953613
train_iter_loss: 0.42273688316345215
train_iter_loss: 0.2507658898830414
train_iter_loss: 0.18268561363220215
train_iter_loss: 0.463155597448349
train_iter_loss: 0.455687016248703
train_iter_loss: 0.20770853757858276
train_iter_loss: 0.28511878848075867
train_iter_loss: 0.3401120603084564
train_iter_loss: 0.32933634519577026
train_iter_loss: 0.4014762043952942
train_iter_loss: 0.3139554262161255
train_iter_loss: 0.36039087176322937
train_iter_loss: 0.29432883858680725
train_iter_loss: 0.2837149500846863
train_iter_loss: 0.5211766362190247
train_iter_loss: 0.40409818291664124
train_iter_loss: 0.10217851400375366
train_iter_loss: 0.2992453873157501
train_iter_loss: 0.2687288522720337
train_iter_loss: 0.25487613677978516
train_iter_loss: 0.2797125279903412
train_iter_loss: 0.30018651485443115
train_iter_loss: 0.32423341274261475
train_iter_loss: 0.27908769249916077
train_iter_loss: 0.2679724395275116
train_iter_loss: 0.24546827375888824
train_iter_loss: 0.2148100584745407
train_iter_loss: 0.2352147400379181
train_iter_loss: 0.246181920170784
train_iter_loss: 0.1999993920326233
train_iter_loss: 0.1677880734205246
train_iter_loss: 0.1632997691631317
train_iter_loss: 0.24942892789840698
train_iter_loss: 0.265558123588562
train_iter_loss: 0.29553014039993286
train_iter_loss: 0.1815146505832672
train_iter_loss: 0.3521919548511505
train_iter_loss: 0.321111261844635
train_iter_loss: 0.24618841707706451
train_iter_loss: 0.15141426026821136
train_iter_loss: 0.13581633567810059
train_iter_loss: 0.29151782393455505
train_iter_loss: 0.3691198229789734
train_iter_loss: 0.21427278220653534
train_iter_loss: 0.3102622330188751
train_iter_loss: 0.36933889985084534
train_iter_loss: 0.2509152293205261
train_iter_loss: 0.2632531523704529
train_iter_loss: 0.33437255024909973
train_iter_loss: 0.3795875608921051
train loss :0.2786
---------------------
Validation seg loss: 0.36614139270002266 at epoch 424
epoch =    425/  1000, exp = train
train_iter_loss: 0.24628876149654388
train_iter_loss: 0.15798909962177277
train_iter_loss: 0.21227745711803436
train_iter_loss: 0.27546072006225586
train_iter_loss: 0.1567174196243286
train_iter_loss: 0.18774549663066864
train_iter_loss: 0.32485076785087585
train_iter_loss: 0.46391090750694275
train_iter_loss: 0.21126499772071838
train_iter_loss: 0.19568908214569092
train_iter_loss: 0.3319666385650635
train_iter_loss: 0.22700650990009308
train_iter_loss: 0.2577151656150818
train_iter_loss: 0.2205183058977127
train_iter_loss: 0.0856328010559082
train_iter_loss: 0.19877919554710388
train_iter_loss: 0.27397486567497253
train_iter_loss: 0.33504295349121094
train_iter_loss: 0.3072707951068878
train_iter_loss: 0.2203199714422226
train_iter_loss: 0.3255843222141266
train_iter_loss: 0.26467326283454895
train_iter_loss: 0.1687195599079132
train_iter_loss: 0.2875943183898926
train_iter_loss: 0.20883813500404358
train_iter_loss: 0.3503257930278778
train_iter_loss: 0.2855032682418823
train_iter_loss: 0.43875303864479065
train_iter_loss: 0.20812006294727325
train_iter_loss: 0.10219784826040268
train_iter_loss: 0.23800338804721832
train_iter_loss: 0.1827717423439026
train_iter_loss: 0.20202812552452087
train_iter_loss: 0.24428309500217438
train_iter_loss: 0.3419283330440521
train_iter_loss: 0.07060695439577103
train_iter_loss: 0.27485641837120056
train_iter_loss: 0.3422400653362274
train_iter_loss: 0.25181207060813904
train_iter_loss: 0.20680803060531616
train_iter_loss: 0.31175661087036133
train_iter_loss: 0.2850355803966522
train_iter_loss: 0.2329849898815155
train_iter_loss: 0.35951855778694153
train_iter_loss: 0.18369467556476593
train_iter_loss: 0.2224130779504776
train_iter_loss: 0.23331625759601593
train_iter_loss: 0.2328191101551056
train_iter_loss: 0.28429391980171204
train_iter_loss: 0.2841469645500183
train_iter_loss: 0.2642715871334076
train_iter_loss: 0.15720339119434357
train_iter_loss: 0.36245742440223694
train_iter_loss: 0.25741979479789734
train_iter_loss: 0.31158751249313354
train_iter_loss: 0.3060016632080078
train_iter_loss: 0.19813701510429382
train_iter_loss: 0.30398017168045044
train_iter_loss: 0.27702596783638
train_iter_loss: 0.10666635632514954
train_iter_loss: 0.3082638084888458
train_iter_loss: 0.32213693857192993
train_iter_loss: 0.29039427638053894
train_iter_loss: 0.5334079265594482
train_iter_loss: 0.15873977541923523
train_iter_loss: 0.043126970529556274
train_iter_loss: 0.30108627676963806
train_iter_loss: 0.2524891495704651
train_iter_loss: 0.27583539485931396
train_iter_loss: 0.267161101102829
train_iter_loss: 0.45561477541923523
train_iter_loss: 0.4097331166267395
train_iter_loss: 0.4523123502731323
train_iter_loss: 0.2517254948616028
train_iter_loss: 0.27398890256881714
train_iter_loss: 0.16677559912204742
train_iter_loss: 0.3312400281429291
train_iter_loss: 0.18576551973819733
train_iter_loss: 0.24540624022483826
train_iter_loss: 0.3258834481239319
train_iter_loss: 0.3524142801761627
train_iter_loss: 0.3117453455924988
train_iter_loss: 0.34607797861099243
train_iter_loss: 0.3848942220211029
train_iter_loss: 0.33614176511764526
train_iter_loss: 0.37515586614608765
train_iter_loss: 0.20063084363937378
train_iter_loss: 0.3212575316429138
train_iter_loss: 0.280567467212677
train_iter_loss: 0.19379854202270508
train_iter_loss: 0.2079564779996872
train_iter_loss: 0.32823124527931213
train_iter_loss: 0.37697768211364746
train_iter_loss: 0.3299019932746887
train_iter_loss: 0.26417437195777893
train_iter_loss: 0.3477545976638794
train_iter_loss: 0.35566556453704834
train_iter_loss: 0.3564114570617676
train_iter_loss: 0.28807178139686584
train_iter_loss: 0.42455360293388367
train loss :0.2757
---------------------
Validation seg loss: 0.36571454001977194 at epoch 425
epoch =    426/  1000, exp = train
train_iter_loss: 0.1488354504108429
train_iter_loss: 0.28911152482032776
train_iter_loss: 0.2570299804210663
train_iter_loss: 0.24253839254379272
train_iter_loss: 0.2971959710121155
train_iter_loss: 0.3186022639274597
train_iter_loss: 0.2667963206768036
train_iter_loss: 0.3480410873889923
train_iter_loss: 0.24342508614063263
train_iter_loss: 0.18567271530628204
train_iter_loss: 0.2572910785675049
train_iter_loss: 0.1669151484966278
train_iter_loss: 0.26416265964508057
train_iter_loss: 0.3312798738479614
train_iter_loss: 0.22562719881534576
train_iter_loss: 0.20791244506835938
train_iter_loss: 0.33445653319358826
train_iter_loss: 0.33475446701049805
train_iter_loss: 0.2361525595188141
train_iter_loss: 0.40943101048469543
train_iter_loss: 0.34150394797325134
train_iter_loss: 0.36215323209762573
train_iter_loss: 0.3749811053276062
train_iter_loss: 0.3198029100894928
train_iter_loss: 0.24878299236297607
train_iter_loss: 0.43223652243614197
train_iter_loss: 0.2851223945617676
train_iter_loss: 0.2672140300273895
train_iter_loss: 0.3275449573993683
train_iter_loss: 0.2571147680282593
train_iter_loss: 0.24813896417617798
train_iter_loss: 0.24689419567584991
train_iter_loss: 0.1359848529100418
train_iter_loss: 0.32623031735420227
train_iter_loss: 0.18229719996452332
train_iter_loss: 0.2270645797252655
train_iter_loss: 0.5374663472175598
train_iter_loss: 0.17671476304531097
train_iter_loss: 0.14176149666309357
train_iter_loss: 0.38990774750709534
train_iter_loss: 0.18913520872592926
train_iter_loss: 0.3283839821815491
train_iter_loss: 0.18885980546474457
train_iter_loss: 0.23836269974708557
train_iter_loss: 0.20438317954540253
train_iter_loss: 0.2703246772289276
train_iter_loss: 0.24467621743679047
train_iter_loss: 0.23709097504615784
train_iter_loss: 0.19229672849178314
train_iter_loss: 0.25978732109069824
train_iter_loss: 0.200725257396698
train_iter_loss: 0.18464328348636627
train_iter_loss: 0.2105889469385147
train_iter_loss: 0.36742350459098816
train_iter_loss: 0.3326178789138794
train_iter_loss: 0.35056379437446594
train_iter_loss: 0.2596830725669861
train_iter_loss: 0.31215745210647583
train_iter_loss: 0.17543336749076843
train_iter_loss: 0.4690195322036743
train_iter_loss: 0.29948535561561584
train_iter_loss: 0.1653508096933365
train_iter_loss: 0.1344897598028183
train_iter_loss: 0.25594598054885864
train_iter_loss: 0.3216223120689392
train_iter_loss: 0.1540931761264801
train_iter_loss: 0.2262866050004959
train_iter_loss: 0.32655197381973267
train_iter_loss: 0.2750682830810547
train_iter_loss: 0.20291072130203247
train_iter_loss: 0.20605739951133728
train_iter_loss: 0.36352747678756714
train_iter_loss: 0.37332046031951904
train_iter_loss: 0.1572236567735672
train_iter_loss: 0.2691720724105835
train_iter_loss: 0.4052315056324005
train_iter_loss: 0.2905377745628357
train_iter_loss: 0.3030124604701996
train_iter_loss: 0.20927691459655762
train_iter_loss: 0.30632439255714417
train_iter_loss: 0.2761903405189514
train_iter_loss: 0.2302245795726776
train_iter_loss: 0.26054543256759644
train_iter_loss: 0.4446616768836975
train_iter_loss: 0.2917786240577698
train_iter_loss: 0.20803354680538177
train_iter_loss: 0.2774525284767151
train_iter_loss: 0.20505453646183014
train_iter_loss: 0.3537764549255371
train_iter_loss: 0.13685426115989685
train_iter_loss: 0.17422673106193542
train_iter_loss: 0.35766032338142395
train_iter_loss: 0.3885745108127594
train_iter_loss: 0.3084401488304138
train_iter_loss: 0.15856799483299255
train_iter_loss: 0.3027600944042206
train_iter_loss: 0.1555994153022766
train_iter_loss: 0.3584025800228119
train_iter_loss: 0.20720192790031433
train_iter_loss: 0.33326342701911926
train loss :0.2736
---------------------
Validation seg loss: 0.35595656939307757 at epoch 426
epoch =    427/  1000, exp = train
train_iter_loss: 0.28010714054107666
train_iter_loss: 0.2707533836364746
train_iter_loss: 0.30683043599128723
train_iter_loss: 0.351023405790329
train_iter_loss: 0.17756183445453644
train_iter_loss: 0.40656524896621704
train_iter_loss: 0.17469248175621033
train_iter_loss: 0.2748035490512848
train_iter_loss: 0.29309433698654175
train_iter_loss: 0.21296998858451843
train_iter_loss: 0.31463858485221863
train_iter_loss: 0.30247557163238525
train_iter_loss: 0.473883718252182
train_iter_loss: 0.22484105825424194
train_iter_loss: 0.1471143513917923
train_iter_loss: 0.5198144316673279
train_iter_loss: 0.27342313528060913
train_iter_loss: 0.22458627820014954
train_iter_loss: 0.4103977084159851
train_iter_loss: 0.4034973382949829
train_iter_loss: 0.2387470155954361
train_iter_loss: 0.34886157512664795
train_iter_loss: 0.29393789172172546
train_iter_loss: 0.2184373289346695
train_iter_loss: 0.2475573867559433
train_iter_loss: 0.3984537720680237
train_iter_loss: 0.269072026014328
train_iter_loss: 0.24246567487716675
train_iter_loss: 0.38937535881996155
train_iter_loss: 0.20235294103622437
train_iter_loss: 0.24059826135635376
train_iter_loss: 0.3647395670413971
train_iter_loss: 0.14927172660827637
train_iter_loss: 0.2906164526939392
train_iter_loss: 0.23011299967765808
train_iter_loss: 0.24275973439216614
train_iter_loss: 0.16751466691493988
train_iter_loss: 0.1648901402950287
train_iter_loss: 0.2100151777267456
train_iter_loss: 0.202621728181839
train_iter_loss: 0.27430036664009094
train_iter_loss: 0.3227667808532715
train_iter_loss: 0.23040767014026642
train_iter_loss: 0.19045628607273102
train_iter_loss: 0.2256534993648529
train_iter_loss: 0.336689829826355
train_iter_loss: 0.35020431876182556
train_iter_loss: 0.20182490348815918
train_iter_loss: 0.29338356852531433
train_iter_loss: 0.19333985447883606
train_iter_loss: 0.3668753504753113
train_iter_loss: 0.21618755161762238
train_iter_loss: 0.20003828406333923
train_iter_loss: 0.1312437206506729
train_iter_loss: 0.23704032599925995
train_iter_loss: 0.3168376684188843
train_iter_loss: 0.22972504794597626
train_iter_loss: 0.3729974329471588
train_iter_loss: 0.32593271136283875
train_iter_loss: 0.25929954648017883
train_iter_loss: 0.2673884630203247
train_iter_loss: 0.44715845584869385
train_iter_loss: 0.37290284037590027
train_iter_loss: 0.16808338463306427
train_iter_loss: 0.1906302571296692
train_iter_loss: 0.2220466285943985
train_iter_loss: 0.25140124559402466
train_iter_loss: 0.29254263639450073
train_iter_loss: 0.20728915929794312
train_iter_loss: 0.3538115620613098
train_iter_loss: 0.5964162349700928
train_iter_loss: 0.20211108028888702
train_iter_loss: 0.2070314735174179
train_iter_loss: 0.3418748676776886
train_iter_loss: 0.24103252589702606
train_iter_loss: 0.356543630361557
train_iter_loss: 0.28929033875465393
train_iter_loss: 0.21861892938613892
train_iter_loss: 0.20584550499916077
train_iter_loss: 0.30346915125846863
train_iter_loss: 0.22216342389583588
train_iter_loss: 0.2192642092704773
train_iter_loss: 0.44623905420303345
train_iter_loss: 0.1239655390381813
train_iter_loss: 0.28774791955947876
train_iter_loss: 0.3300500810146332
train_iter_loss: 0.27117210626602173
train_iter_loss: 0.14396655559539795
train_iter_loss: 0.19540244340896606
train_iter_loss: 0.3798781931400299
train_iter_loss: 0.22479908168315887
train_iter_loss: 0.16969572007656097
train_iter_loss: 0.20154070854187012
train_iter_loss: 0.40355995297431946
train_iter_loss: 0.2556683421134949
train_iter_loss: 0.3124828338623047
train_iter_loss: 0.336062490940094
train_iter_loss: 0.16204605996608734
train_iter_loss: 0.30766353011131287
train_iter_loss: 0.26148226857185364
train loss :0.2769
---------------------
Validation seg loss: 0.3648989202524975 at epoch 427
epoch =    428/  1000, exp = train
train_iter_loss: 0.37198442220687866
train_iter_loss: 0.2792748808860779
train_iter_loss: 0.42649731040000916
train_iter_loss: 0.11601748317480087
train_iter_loss: 0.3119935393333435
train_iter_loss: 0.2399635910987854
train_iter_loss: 0.2007700502872467
train_iter_loss: 0.14657530188560486
train_iter_loss: 0.3550795614719391
train_iter_loss: 0.18426688015460968
train_iter_loss: 0.28390178084373474
train_iter_loss: 0.20196940004825592
train_iter_loss: 0.18250524997711182
train_iter_loss: 0.3487701117992401
train_iter_loss: 0.3990240693092346
train_iter_loss: 0.3214345872402191
train_iter_loss: 0.31787049770355225
train_iter_loss: 0.11310998350381851
train_iter_loss: 0.31470298767089844
train_iter_loss: 0.22455652058124542
train_iter_loss: 0.39303308725357056
train_iter_loss: 0.28106293082237244
train_iter_loss: 0.2759048640727997
train_iter_loss: 0.21974611282348633
train_iter_loss: 0.2342539280653
train_iter_loss: 0.3380603492259979
train_iter_loss: 0.34720706939697266
train_iter_loss: 0.2010011374950409
train_iter_loss: 0.13633953034877777
train_iter_loss: 0.4228001534938812
train_iter_loss: 0.4097943603992462
train_iter_loss: 0.33076080679893494
train_iter_loss: 0.32302677631378174
train_iter_loss: 0.22688916325569153
train_iter_loss: 0.3023357391357422
train_iter_loss: 0.15465758740901947
train_iter_loss: 0.2300979048013687
train_iter_loss: 0.30081090331077576
train_iter_loss: 0.1835421621799469
train_iter_loss: 0.26783716678619385
train_iter_loss: 0.2589879035949707
train_iter_loss: 0.45358702540397644
train_iter_loss: 0.5140820741653442
train_iter_loss: 0.27952778339385986
train_iter_loss: 0.13368745148181915
train_iter_loss: 0.2783036530017853
train_iter_loss: 0.07034553587436676
train_iter_loss: 0.3037196099758148
train_iter_loss: 0.2599993646144867
train_iter_loss: 0.15188376605510712
train_iter_loss: 0.3874110281467438
train_iter_loss: 0.19986623525619507
train_iter_loss: 0.1358618438243866
train_iter_loss: 0.21701475977897644
train_iter_loss: 0.29653313755989075
train_iter_loss: 0.29141393303871155
train_iter_loss: 0.35769304633140564
train_iter_loss: 0.190695121884346
train_iter_loss: 0.3976723849773407
train_iter_loss: 0.2383793145418167
train_iter_loss: 0.37822940945625305
train_iter_loss: 0.22142034769058228
train_iter_loss: 0.39698249101638794
train_iter_loss: 0.4273609220981598
train_iter_loss: 0.24950864911079407
train_iter_loss: 0.2362142950296402
train_iter_loss: 0.3956226706504822
train_iter_loss: 0.23005957901477814
train_iter_loss: 0.18679995834827423
train_iter_loss: 0.23444736003875732
train_iter_loss: 0.18147976696491241
train_iter_loss: 0.22278240323066711
train_iter_loss: 0.2202162891626358
train_iter_loss: 0.23615232110023499
train_iter_loss: 0.4486590027809143
train_iter_loss: 0.3708283603191376
train_iter_loss: 0.2529872953891754
train_iter_loss: 0.3395450711250305
train_iter_loss: 0.570923924446106
train_iter_loss: 0.22475357353687286
train_iter_loss: 0.35099753737449646
train_iter_loss: 0.20895390212535858
train_iter_loss: 0.13818097114562988
train_iter_loss: 0.19984082877635956
train_iter_loss: 0.3396487832069397
train_iter_loss: 0.31026363372802734
train_iter_loss: 0.35288006067276
train_iter_loss: 0.3526342213153839
train_iter_loss: 0.24315349757671356
train_iter_loss: 0.29679855704307556
train_iter_loss: 0.225457102060318
train_iter_loss: 0.21025586128234863
train_iter_loss: 0.17886418104171753
train_iter_loss: 0.25174883008003235
train_iter_loss: 0.1859605610370636
train_iter_loss: 0.17971862852573395
train_iter_loss: 0.26086515188217163
train_iter_loss: 0.21079121530056
train_iter_loss: 0.24277667701244354
train_iter_loss: 0.2751101851463318
train loss :0.2765
---------------------
Validation seg loss: 0.38932928527301214 at epoch 428
epoch =    429/  1000, exp = train
train_iter_loss: 0.4733153283596039
train_iter_loss: 0.31812867522239685
train_iter_loss: 0.3923509120941162
train_iter_loss: 0.2890608310699463
train_iter_loss: 0.27268165349960327
train_iter_loss: 0.30073410272598267
train_iter_loss: 0.2263784259557724
train_iter_loss: 0.2669142484664917
train_iter_loss: 0.13757894933223724
train_iter_loss: 0.47053760290145874
train_iter_loss: 0.22115327417850494
train_iter_loss: 0.3403280973434448
train_iter_loss: 0.30176541209220886
train_iter_loss: 0.3567696511745453
train_iter_loss: 0.28435492515563965
train_iter_loss: 0.2683705985546112
train_iter_loss: 0.25526970624923706
train_iter_loss: 0.15283387899398804
train_iter_loss: 0.23853573203086853
train_iter_loss: 0.2922824025154114
train_iter_loss: 0.3936578631401062
train_iter_loss: 0.1253872960805893
train_iter_loss: 0.3125312328338623
train_iter_loss: 0.3128943145275116
train_iter_loss: 0.4042859375476837
train_iter_loss: 0.18833748996257782
train_iter_loss: 0.35353803634643555
train_iter_loss: 0.33874237537384033
train_iter_loss: 0.29756975173950195
train_iter_loss: 0.3031226694583893
train_iter_loss: 0.30908235907554626
train_iter_loss: 0.18790572881698608
train_iter_loss: 0.20837165415287018
train_iter_loss: 0.18936148285865784
train_iter_loss: 0.3321983218193054
train_iter_loss: 0.29753533005714417
train_iter_loss: 0.0979759693145752
train_iter_loss: 0.20749874413013458
train_iter_loss: 0.27318820357322693
train_iter_loss: 0.1631399542093277
train_iter_loss: 0.2741186320781708
train_iter_loss: 0.3025031089782715
train_iter_loss: 0.30165961384773254
train_iter_loss: 0.0739145576953888
train_iter_loss: 0.3006902039051056
train_iter_loss: 0.35857996344566345
train_iter_loss: 0.2811780571937561
train_iter_loss: 0.25984227657318115
train_iter_loss: 0.23386619985103607
train_iter_loss: 0.30776578187942505
train_iter_loss: 0.35918158292770386
train_iter_loss: 0.2384931445121765
train_iter_loss: 0.17803627252578735
train_iter_loss: 0.2294148951768875
train_iter_loss: 0.3822743892669678
train_iter_loss: 0.42063283920288086
train_iter_loss: 0.37441661953926086
train_iter_loss: 0.24228495359420776
train_iter_loss: 0.30196964740753174
train_iter_loss: 0.1997472196817398
train_iter_loss: 0.41233423352241516
train_iter_loss: 0.247620090842247
train_iter_loss: 0.2486439347267151
train_iter_loss: 0.23646344244480133
train_iter_loss: 0.1959095448255539
train_iter_loss: 0.34823116660118103
train_iter_loss: 0.2386334091424942
train_iter_loss: 0.23121440410614014
train_iter_loss: 0.2409040778875351
train_iter_loss: 0.2294526845216751
train_iter_loss: 0.3754223883152008
train_iter_loss: 0.1968516856431961
train_iter_loss: 0.2722209393978119
train_iter_loss: 0.12843173742294312
train_iter_loss: 0.30628475546836853
train_iter_loss: 0.25539642572402954
train_iter_loss: 0.14117392897605896
train_iter_loss: 0.24718667566776276
train_iter_loss: 0.36836397647857666
train_iter_loss: 0.2569052577018738
train_iter_loss: 0.1656147688627243
train_iter_loss: 0.3161676228046417
train_iter_loss: 0.2519873380661011
train_iter_loss: 0.2421339601278305
train_iter_loss: 0.2539006173610687
train_iter_loss: 0.5155514478683472
train_iter_loss: 0.18894478678703308
train_iter_loss: 0.24493493139743805
train_iter_loss: 0.2689606547355652
train_iter_loss: 0.27512839436531067
train_iter_loss: 0.26113593578338623
train_iter_loss: 0.19955797493457794
train_iter_loss: 0.19141222536563873
train_iter_loss: 0.25973978638648987
train_iter_loss: 0.2274206280708313
train_iter_loss: 0.2575264573097229
train_iter_loss: 0.2306959629058838
train_iter_loss: 0.41128408908843994
train_iter_loss: 0.2577979862689972
train_iter_loss: 0.4161190986633301
train loss :0.2767
---------------------
Validation seg loss: 0.36415290987154225 at epoch 429
epoch =    430/  1000, exp = train
train_iter_loss: 0.19631758332252502
train_iter_loss: 0.25975319743156433
train_iter_loss: 0.23792359232902527
train_iter_loss: 0.21923813223838806
train_iter_loss: 0.28009548783302307
train_iter_loss: 0.20086033642292023
train_iter_loss: 0.3328148126602173
train_iter_loss: 0.1437140852212906
train_iter_loss: 0.3905743956565857
train_iter_loss: 0.2862388789653778
train_iter_loss: 0.2791890501976013
train_iter_loss: 0.13399973511695862
train_iter_loss: 0.2646242380142212
train_iter_loss: 0.2627318203449249
train_iter_loss: 0.3210008144378662
train_iter_loss: 0.33256494998931885
train_iter_loss: 0.33876776695251465
train_iter_loss: 0.1342112123966217
train_iter_loss: 0.2671116888523102
train_iter_loss: 0.31688886880874634
train_iter_loss: 0.2181686908006668
train_iter_loss: 0.24970883131027222
train_iter_loss: 0.13240240514278412
train_iter_loss: 0.1426476240158081
train_iter_loss: 0.30092868208885193
train_iter_loss: 0.3097124993801117
train_iter_loss: 0.2509620487689972
train_iter_loss: 0.2768786549568176
train_iter_loss: 0.28758862614631653
train_iter_loss: 0.27933013439178467
train_iter_loss: 0.3168060779571533
train_iter_loss: 0.22193685173988342
train_iter_loss: 0.3283337354660034
train_iter_loss: 0.3631449043750763
train_iter_loss: 0.20798099040985107
train_iter_loss: 0.3155401945114136
train_iter_loss: 0.19012810289859772
train_iter_loss: 0.20323698222637177
train_iter_loss: 0.27085062861442566
train_iter_loss: 0.21908600628376007
train_iter_loss: 0.2079102098941803
train_iter_loss: 0.24520143866539001
train_iter_loss: 0.23596034944057465
train_iter_loss: 0.36430251598358154
train_iter_loss: 0.4679274260997772
train_iter_loss: 0.34719133377075195
train_iter_loss: 0.24588431417942047
train_iter_loss: 0.33705416321754456
train_iter_loss: 0.16437582671642303
train_iter_loss: 0.09408168494701385
train_iter_loss: 0.18358713388442993
train_iter_loss: 0.2896822392940521
train_iter_loss: 0.22425568103790283
train_iter_loss: 0.1092551052570343
train_iter_loss: 0.3242877125740051
train_iter_loss: 0.2964058220386505
train_iter_loss: 0.2064051777124405
train_iter_loss: 0.31078991293907166
train_iter_loss: 0.23218780755996704
train_iter_loss: 0.2816607654094696
train_iter_loss: 0.28297096490859985
train_iter_loss: 0.2616664171218872
train_iter_loss: 0.2891945242881775
train_iter_loss: 0.3828105330467224
train_iter_loss: 0.13173076510429382
train_iter_loss: 0.8302605748176575
train_iter_loss: 0.3541351854801178
train_iter_loss: 0.3453887403011322
train_iter_loss: 0.22915609180927277
train_iter_loss: 0.3249303996562958
train_iter_loss: 0.22851353883743286
train_iter_loss: 0.19995738565921783
train_iter_loss: 0.12076310068368912
train_iter_loss: 0.21560688316822052
train_iter_loss: 0.34919774532318115
train_iter_loss: 0.34237274527549744
train_iter_loss: 0.35203731060028076
train_iter_loss: 0.2768996059894562
train_iter_loss: 0.2883746027946472
train_iter_loss: 0.10713625699281693
train_iter_loss: 0.2912205159664154
train_iter_loss: 0.358283668756485
train_iter_loss: 0.23306111991405487
train_iter_loss: 0.26413753628730774
train_iter_loss: 0.24957099556922913
train_iter_loss: 0.37209904193878174
train_iter_loss: 0.21928119659423828
train_iter_loss: 0.2251790463924408
train_iter_loss: 0.21241308748722076
train_iter_loss: 0.30076390504837036
train_iter_loss: 0.26398926973342896
train_iter_loss: 0.3539069890975952
train_iter_loss: 0.3861464858055115
train_iter_loss: 0.44199493527412415
train_iter_loss: 0.13664951920509338
train_iter_loss: 0.1721620261669159
train_iter_loss: 0.13031357526779175
train_iter_loss: 0.28922057151794434
train_iter_loss: 0.23367038369178772
train_iter_loss: 0.3428409993648529
train loss :0.2712
---------------------
Validation seg loss: 0.38392873529239363 at epoch 430
epoch =    431/  1000, exp = train
train_iter_loss: 0.3405626714229584
train_iter_loss: 0.1747628152370453
train_iter_loss: 0.3500443994998932
train_iter_loss: 0.24833796918392181
train_iter_loss: 0.27404525876045227
train_iter_loss: 0.25065281987190247
train_iter_loss: 0.3541781008243561
train_iter_loss: 0.373263418674469
train_iter_loss: 0.33528047800064087
train_iter_loss: 0.2894400656223297
train_iter_loss: 0.2223992794752121
train_iter_loss: 0.3051137328147888
train_iter_loss: 0.15567442774772644
train_iter_loss: 0.30094602704048157
train_iter_loss: 0.17456366121768951
train_iter_loss: 0.2299950122833252
train_iter_loss: 0.2547617554664612
train_iter_loss: 0.10701635479927063
train_iter_loss: 0.3231302797794342
train_iter_loss: 0.2783931493759155
train_iter_loss: 0.31093576550483704
train_iter_loss: 0.1406991332769394
train_iter_loss: 0.3704005479812622
train_iter_loss: 0.10936310142278671
train_iter_loss: 0.24049369990825653
train_iter_loss: 0.3637418746948242
train_iter_loss: 0.2583020329475403
train_iter_loss: 0.2277781069278717
train_iter_loss: 0.30379191040992737
train_iter_loss: 0.2428700476884842
train_iter_loss: 0.3847084045410156
train_iter_loss: 0.3059280216693878
train_iter_loss: 0.21003660559654236
train_iter_loss: 0.2599131464958191
train_iter_loss: 0.27639299631118774
train_iter_loss: 0.25830331444740295
train_iter_loss: 0.2570998966693878
train_iter_loss: 0.1951543092727661
train_iter_loss: 0.13767603039741516
train_iter_loss: 0.2670307457447052
train_iter_loss: 0.3861258029937744
train_iter_loss: 0.24777251482009888
train_iter_loss: 0.19243107736110687
train_iter_loss: 0.29931196570396423
train_iter_loss: 0.37004509568214417
train_iter_loss: 0.22807592153549194
train_iter_loss: 0.22125758230686188
train_iter_loss: 0.25542759895324707
train_iter_loss: 0.13906635344028473
train_iter_loss: 0.18781094253063202
train_iter_loss: 0.25737667083740234
train_iter_loss: 0.23907753825187683
train_iter_loss: 0.18709838390350342
train_iter_loss: 0.2543705999851227
train_iter_loss: 0.243273064494133
train_iter_loss: 0.38551297783851624
train_iter_loss: 0.2433098405599594
train_iter_loss: 0.24297569692134857
train_iter_loss: 0.27201130986213684
train_iter_loss: 0.2940342426300049
train_iter_loss: 0.2529996931552887
train_iter_loss: 0.31916043162345886
train_iter_loss: 0.1451818197965622
train_iter_loss: 0.2896089255809784
train_iter_loss: 0.45568984746932983
train_iter_loss: 0.32989197969436646
train_iter_loss: 0.1863066405057907
train_iter_loss: 0.3668068051338196
train_iter_loss: 0.18924430012702942
train_iter_loss: 0.1770515739917755
train_iter_loss: 0.23482583463191986
train_iter_loss: 0.3456660807132721
train_iter_loss: 0.32543930411338806
train_iter_loss: 0.2681964635848999
train_iter_loss: 0.3214607834815979
train_iter_loss: 0.22141580283641815
train_iter_loss: 0.35127323865890503
train_iter_loss: 0.34751206636428833
train_iter_loss: 0.389087975025177
train_iter_loss: 0.15196378529071808
train_iter_loss: 0.24010421335697174
train_iter_loss: 0.35359522700309753
train_iter_loss: 0.45762425661087036
train_iter_loss: 0.25487372279167175
train_iter_loss: 0.2368474006652832
train_iter_loss: 0.33486077189445496
train_iter_loss: 0.3763982355594635
train_iter_loss: 0.4453303813934326
train_iter_loss: 0.27925458550453186
train_iter_loss: 0.22348101437091827
train_iter_loss: 0.2562568783760071
train_iter_loss: 0.20878596603870392
train_iter_loss: 0.2957472503185272
train_iter_loss: 0.30914339423179626
train_iter_loss: 0.358169287443161
train_iter_loss: 0.25127121806144714
train_iter_loss: 0.18967173993587494
train_iter_loss: 0.3197915554046631
train_iter_loss: 0.5105836987495422
train_iter_loss: 0.2876402735710144
train loss :0.2774
---------------------
Validation seg loss: 0.36993033922154384 at epoch 431
epoch =    432/  1000, exp = train
train_iter_loss: 0.15152078866958618
train_iter_loss: 0.1737852394580841
train_iter_loss: 0.16555023193359375
train_iter_loss: 0.3449932336807251
train_iter_loss: 0.38894468545913696
train_iter_loss: 0.2502416670322418
train_iter_loss: 0.34698501229286194
train_iter_loss: 0.18973015248775482
train_iter_loss: 0.20489028096199036
train_iter_loss: 0.2762148380279541
train_iter_loss: 0.3249714970588684
train_iter_loss: 0.15151989459991455
train_iter_loss: 0.2517317235469818
train_iter_loss: 0.24959440529346466
train_iter_loss: 0.3755153715610504
train_iter_loss: 0.24794819951057434
train_iter_loss: 0.21809254586696625
train_iter_loss: 0.35828205943107605
train_iter_loss: 0.17883603274822235
train_iter_loss: 0.09531276673078537
train_iter_loss: 0.3003712296485901
train_iter_loss: 0.12531781196594238
train_iter_loss: 0.273824006319046
train_iter_loss: 0.35485538840293884
train_iter_loss: 0.4854572117328644
train_iter_loss: 0.33713001012802124
train_iter_loss: 0.1681944876909256
train_iter_loss: 0.32693877816200256
train_iter_loss: 0.2023276686668396
train_iter_loss: 0.2435615211725235
train_iter_loss: 0.31728827953338623
train_iter_loss: 0.2366601824760437
train_iter_loss: 0.2705647945404053
train_iter_loss: 0.2685330808162689
train_iter_loss: 0.3984333872795105
train_iter_loss: 0.2729918956756592
train_iter_loss: 0.21292175352573395
train_iter_loss: 0.36822977662086487
train_iter_loss: 0.32106080651283264
train_iter_loss: 0.3689408004283905
train_iter_loss: 0.30599725246429443
train_iter_loss: 0.3206852972507477
train_iter_loss: 0.30937907099723816
train_iter_loss: 0.2675505578517914
train_iter_loss: 0.29022011160850525
train_iter_loss: 0.3140435814857483
train_iter_loss: 0.1769123673439026
train_iter_loss: 0.3724328875541687
train_iter_loss: 0.24871140718460083
train_iter_loss: 0.27101555466651917
train_iter_loss: 0.350079745054245
train_iter_loss: 0.25219255685806274
train_iter_loss: 0.32233256101608276
train_iter_loss: 0.27968573570251465
train_iter_loss: 0.2768245339393616
train_iter_loss: 0.2899114489555359
train_iter_loss: 0.1588234007358551
train_iter_loss: 0.2462502270936966
train_iter_loss: 0.30661457777023315
train_iter_loss: 0.1715761125087738
train_iter_loss: 0.2647700905799866
train_iter_loss: 0.29994991421699524
train_iter_loss: 0.2854900360107422
train_iter_loss: 0.3188643455505371
train_iter_loss: 0.16831763088703156
train_iter_loss: 0.31323766708374023
train_iter_loss: 0.30612775683403015
train_iter_loss: 0.3773743212223053
train_iter_loss: 0.347684383392334
train_iter_loss: 0.3159000277519226
train_iter_loss: 0.352583110332489
train_iter_loss: 0.2764226496219635
train_iter_loss: 0.2112254500389099
train_iter_loss: 0.11324091255664825
train_iter_loss: 0.21073710918426514
train_iter_loss: 0.19550634920597076
train_iter_loss: 0.19036464393138885
train_iter_loss: 0.41059061884880066
train_iter_loss: 0.1856900155544281
train_iter_loss: 0.28354522585868835
train_iter_loss: 0.333280086517334
train_iter_loss: 0.26000115275382996
train_iter_loss: 0.2611808776855469
train_iter_loss: 0.15257573127746582
train_iter_loss: 0.1964792162179947
train_iter_loss: 0.2929880619049072
train_iter_loss: 0.3067609965801239
train_iter_loss: 0.32797253131866455
train_iter_loss: 0.25553664565086365
train_iter_loss: 0.3491891622543335
train_iter_loss: 0.1935586929321289
train_iter_loss: 0.3936131000518799
train_iter_loss: 0.2326548546552658
train_iter_loss: 0.2632989287376404
train_iter_loss: 0.3294017016887665
train_iter_loss: 0.266238808631897
train_iter_loss: 0.4070509672164917
train_iter_loss: 0.22569045424461365
train_iter_loss: 0.23780566453933716
train_iter_loss: 0.22878272831439972
train loss :0.2755
---------------------
Validation seg loss: 0.38082301033274185 at epoch 432
epoch =    433/  1000, exp = train
train_iter_loss: 0.2343825101852417
train_iter_loss: 0.39248892664909363
train_iter_loss: 0.3646961748600006
train_iter_loss: 0.29346978664398193
train_iter_loss: 0.2281433641910553
train_iter_loss: 0.1928033083677292
train_iter_loss: 0.21522952616214752
train_iter_loss: 0.2026958018541336
train_iter_loss: 0.12010116130113602
train_iter_loss: 0.2352215051651001
train_iter_loss: 0.389940470457077
train_iter_loss: 0.23709501326084137
train_iter_loss: 0.2467803657054901
train_iter_loss: 0.2852346897125244
train_iter_loss: 0.3613564074039459
train_iter_loss: 0.19036900997161865
train_iter_loss: 0.2825813591480255
train_iter_loss: 0.3226185441017151
train_iter_loss: 0.15626117587089539
train_iter_loss: 0.3008088171482086
train_iter_loss: 0.2614573836326599
train_iter_loss: 0.3999570608139038
train_iter_loss: 0.40727075934410095
train_iter_loss: 0.3459428548812866
train_iter_loss: 0.3546247184276581
train_iter_loss: 0.3115076422691345
train_iter_loss: 0.3241303861141205
train_iter_loss: 0.2775810658931732
train_iter_loss: 0.290640264749527
train_iter_loss: 0.2546689510345459
train_iter_loss: 0.17844432592391968
train_iter_loss: 0.48647409677505493
train_iter_loss: 0.4569355845451355
train_iter_loss: 0.29046279191970825
train_iter_loss: 0.281103253364563
train_iter_loss: 0.3247455954551697
train_iter_loss: 0.14918571710586548
train_iter_loss: 0.25253018736839294
train_iter_loss: 0.358908474445343
train_iter_loss: 0.26979225873947144
train_iter_loss: 0.1526850312948227
train_iter_loss: 0.20905333757400513
train_iter_loss: 0.23827949166297913
train_iter_loss: 0.11454696208238602
train_iter_loss: 0.295919269323349
train_iter_loss: 0.1916370987892151
train_iter_loss: 0.2682875096797943
train_iter_loss: 0.1617894023656845
train_iter_loss: 0.36266493797302246
train_iter_loss: 0.229299396276474
train_iter_loss: 0.1852128505706787
train_iter_loss: 0.2593770921230316
train_iter_loss: 0.3465687930583954
train_iter_loss: 0.21356309950351715
train_iter_loss: 0.19919243454933167
train_iter_loss: 0.37401020526885986
train_iter_loss: 0.20655354857444763
train_iter_loss: 0.12646347284317017
train_iter_loss: 0.15064087510108948
train_iter_loss: 0.26921752095222473
train_iter_loss: 0.3141120374202728
train_iter_loss: 0.35752201080322266
train_iter_loss: 0.390157014131546
train_iter_loss: 0.27026209235191345
train_iter_loss: 0.13489191234111786
train_iter_loss: 0.3848186731338501
train_iter_loss: 0.1790657937526703
train_iter_loss: 0.22687044739723206
train_iter_loss: 0.2751867473125458
train_iter_loss: 0.24865014851093292
train_iter_loss: 0.3274230659008026
train_iter_loss: 0.32413461804389954
train_iter_loss: 0.2785339653491974
train_iter_loss: 0.24089698493480682
train_iter_loss: 0.2249005138874054
train_iter_loss: 0.22988030314445496
train_iter_loss: 0.37191277742385864
train_iter_loss: 0.283340185880661
train_iter_loss: 0.28299906849861145
train_iter_loss: 0.4015093743801117
train_iter_loss: 0.2407722771167755
train_iter_loss: 0.1743554025888443
train_iter_loss: 0.31283462047576904
train_iter_loss: 0.387751042842865
train_iter_loss: 0.3075072467327118
train_iter_loss: 0.367989718914032
train_iter_loss: 0.2022971510887146
train_iter_loss: 0.20552107691764832
train_iter_loss: 0.1761503666639328
train_iter_loss: 0.21301859617233276
train_iter_loss: 0.44862687587738037
train_iter_loss: 0.18085339665412903
train_iter_loss: 0.20891499519348145
train_iter_loss: 0.28469040989875793
train_iter_loss: 0.1797536462545395
train_iter_loss: 0.22269180417060852
train_iter_loss: 0.3779948055744171
train_iter_loss: 0.2108539491891861
train_iter_loss: 0.434245228767395
train_iter_loss: 0.2822367548942566
train loss :0.2753
---------------------
Validation seg loss: 0.37539375637534933 at epoch 433
epoch =    434/  1000, exp = train
train_iter_loss: 0.24441976845264435
train_iter_loss: 0.15099157392978668
train_iter_loss: 0.23840343952178955
train_iter_loss: 0.17649903893470764
train_iter_loss: 0.2284463346004486
train_iter_loss: 0.08914043009281158
train_iter_loss: 0.3830234110355377
train_iter_loss: 0.189616858959198
train_iter_loss: 0.20836412906646729
train_iter_loss: 0.3575655519962311
train_iter_loss: 0.24332407116889954
train_iter_loss: 0.2564064860343933
train_iter_loss: 0.2507312595844269
train_iter_loss: 0.31917932629585266
train_iter_loss: 0.23214472830295563
train_iter_loss: 0.27963343262672424
train_iter_loss: 0.228825181722641
train_iter_loss: 0.3487108051776886
train_iter_loss: 0.42368417978286743
train_iter_loss: 0.22071340680122375
train_iter_loss: 0.21536488831043243
train_iter_loss: 0.2253512293100357
train_iter_loss: 0.33204495906829834
train_iter_loss: 0.3836299180984497
train_iter_loss: 0.4027983248233795
train_iter_loss: 0.28271886706352234
train_iter_loss: 0.26947611570358276
train_iter_loss: 0.4068301022052765
train_iter_loss: 0.29508092999458313
train_iter_loss: 0.17285621166229248
train_iter_loss: 0.370400995016098
train_iter_loss: 0.1838618516921997
train_iter_loss: 0.17353348433971405
train_iter_loss: 0.2118215560913086
train_iter_loss: 0.1174393892288208
train_iter_loss: 0.45758071541786194
train_iter_loss: 0.22470222413539886
train_iter_loss: 0.21539165079593658
train_iter_loss: 0.20170553028583527
train_iter_loss: 0.16158656775951385
train_iter_loss: 0.33441463112831116
train_iter_loss: 0.23693986237049103
train_iter_loss: 0.2808154225349426
train_iter_loss: 0.38240355253219604
train_iter_loss: 0.34878432750701904
train_iter_loss: 0.29568585753440857
train_iter_loss: 0.2722780406475067
train_iter_loss: 0.32846716046333313
train_iter_loss: 0.2189032882452011
train_iter_loss: 0.3652712106704712
train_iter_loss: 0.35160115361213684
train_iter_loss: 0.2667914032936096
train_iter_loss: 0.28345468640327454
train_iter_loss: 0.28305143117904663
train_iter_loss: 0.3574981391429901
train_iter_loss: 0.32670116424560547
train_iter_loss: 0.3151063919067383
train_iter_loss: 0.5353759527206421
train_iter_loss: 0.567436695098877
train_iter_loss: 0.21394413709640503
train_iter_loss: 0.3246036469936371
train_iter_loss: 0.18776817619800568
train_iter_loss: 0.4769285023212433
train_iter_loss: 0.20141835510730743
train_iter_loss: 0.3432348072528839
train_iter_loss: 0.2392970621585846
train_iter_loss: 0.23113909363746643
train_iter_loss: 0.14813756942749023
train_iter_loss: 0.2948248088359833
train_iter_loss: 0.27686721086502075
train_iter_loss: 0.2559116780757904
train_iter_loss: 0.19692140817642212
train_iter_loss: 0.3012734055519104
train_iter_loss: 0.14106975495815277
train_iter_loss: 0.42229315638542175
train_iter_loss: 0.27309396862983704
train_iter_loss: 0.14028134942054749
train_iter_loss: 0.29752904176712036
train_iter_loss: 0.27395904064178467
train_iter_loss: 0.37755030393600464
train_iter_loss: 0.26776501536369324
train_iter_loss: 0.19921182096004486
train_iter_loss: 0.4159409701824188
train_iter_loss: 0.10817103832960129
train_iter_loss: 0.2055363655090332
train_iter_loss: 0.2489226758480072
train_iter_loss: 0.22547632455825806
train_iter_loss: 0.19337430596351624
train_iter_loss: 0.32346677780151367
train_iter_loss: 0.30821722745895386
train_iter_loss: 0.322375625371933
train_iter_loss: 0.33671310544013977
train_iter_loss: 0.3114840090274811
train_iter_loss: 0.26394322514533997
train_iter_loss: 0.28830486536026
train_iter_loss: 0.28231462836265564
train_iter_loss: 0.08085479587316513
train_iter_loss: 0.4463852047920227
train_iter_loss: 0.29403674602508545
train_iter_loss: 0.16807091236114502
train loss :0.2791
---------------------
Validation seg loss: 0.3714020803641036 at epoch 434
epoch =    435/  1000, exp = train
train_iter_loss: 0.25219208002090454
train_iter_loss: 0.25781944394111633
train_iter_loss: 0.38039761781692505
train_iter_loss: 0.15925666689872742
train_iter_loss: 0.07057513296604156
train_iter_loss: 0.2067839652299881
train_iter_loss: 0.10347878932952881
train_iter_loss: 0.18725433945655823
train_iter_loss: 0.1760258674621582
train_iter_loss: 0.2869223654270172
train_iter_loss: 0.17281530797481537
train_iter_loss: 0.35573610663414
train_iter_loss: 0.19339457154273987
train_iter_loss: 0.23148486018180847
train_iter_loss: 0.22107940912246704
train_iter_loss: 0.4335504174232483
train_iter_loss: 0.25691160559654236
train_iter_loss: 0.19813643395900726
train_iter_loss: 0.24331443011760712
train_iter_loss: 0.20639978349208832
train_iter_loss: 0.27479788661003113
train_iter_loss: 0.1991947889328003
train_iter_loss: 0.19755327701568604
train_iter_loss: 0.21610252559185028
train_iter_loss: 0.384773850440979
train_iter_loss: 0.22382484376430511
train_iter_loss: 0.3321458399295807
train_iter_loss: 0.24613988399505615
train_iter_loss: 0.31923118233680725
train_iter_loss: 0.4867887496948242
train_iter_loss: 0.2927211821079254
train_iter_loss: 0.22561892867088318
train_iter_loss: 0.2122633308172226
train_iter_loss: 0.13650725781917572
train_iter_loss: 0.26492390036582947
train_iter_loss: 0.31040117144584656
train_iter_loss: 0.23786507546901703
train_iter_loss: 0.28022274374961853
train_iter_loss: 0.32825279235839844
train_iter_loss: 0.38852986693382263
train_iter_loss: 0.1876801699399948
train_iter_loss: 0.27669548988342285
train_iter_loss: 0.26399093866348267
train_iter_loss: 0.36707690358161926
train_iter_loss: 0.34665462374687195
train_iter_loss: 0.12615466117858887
train_iter_loss: 0.22833159565925598
train_iter_loss: 0.29956579208374023
train_iter_loss: 0.2977076768875122
train_iter_loss: 0.21097975969314575
train_iter_loss: 0.2926223576068878
train_iter_loss: 0.22218385338783264
train_iter_loss: 0.21048706769943237
train_iter_loss: 0.2524632513523102
train_iter_loss: 0.3000886142253876
train_iter_loss: 0.259317547082901
train_iter_loss: 0.18110018968582153
train_iter_loss: 0.3202506899833679
train_iter_loss: 0.45320355892181396
train_iter_loss: 0.15917158126831055
train_iter_loss: 0.22252678871154785
train_iter_loss: 0.3202452063560486
train_iter_loss: 0.2801487445831299
train_iter_loss: 0.3101540505886078
train_iter_loss: 0.34581735730171204
train_iter_loss: 0.13934661448001862
train_iter_loss: 0.17664624750614166
train_iter_loss: 0.22323490679264069
train_iter_loss: 0.13751590251922607
train_iter_loss: 0.2053593248128891
train_iter_loss: 0.30280017852783203
train_iter_loss: 0.25651320815086365
train_iter_loss: 0.25175029039382935
train_iter_loss: 0.3373178243637085
train_iter_loss: 0.2468557059764862
train_iter_loss: 0.44455939531326294
train_iter_loss: 0.2704544961452484
train_iter_loss: 0.10591378062963486
train_iter_loss: 0.3135128915309906
train_iter_loss: 0.49401330947875977
train_iter_loss: 0.3176581561565399
train_iter_loss: 0.4291371703147888
train_iter_loss: 0.2583976686000824
train_iter_loss: 0.28436848521232605
train_iter_loss: 0.3650032579898834
train_iter_loss: 0.35844841599464417
train_iter_loss: 0.3554840385913849
train_iter_loss: 0.2690927982330322
train_iter_loss: 0.4221426844596863
train_iter_loss: 0.19318093359470367
train_iter_loss: 0.25033172965049744
train_iter_loss: 0.3822534680366516
train_iter_loss: 0.24806293845176697
train_iter_loss: 0.2618054449558258
train_iter_loss: 0.3424842953681946
train_iter_loss: 0.2496016025543213
train_iter_loss: 0.18824709951877594
train_iter_loss: 0.28756287693977356
train_iter_loss: 0.19012022018432617
train_iter_loss: 0.30751562118530273
train loss :0.2703
---------------------
Validation seg loss: 0.36319112653826485 at epoch 435
epoch =    436/  1000, exp = train
train_iter_loss: 0.3249320685863495
train_iter_loss: 0.3036133944988251
train_iter_loss: 0.42291736602783203
train_iter_loss: 0.46865764260292053
train_iter_loss: 0.33305448293685913
train_iter_loss: 0.3065296411514282
train_iter_loss: 0.22631888091564178
train_iter_loss: 0.2826811373233795
train_iter_loss: 0.2239810824394226
train_iter_loss: 0.356812983751297
train_iter_loss: 0.2535321116447449
train_iter_loss: 0.28255248069763184
train_iter_loss: 0.3429170548915863
train_iter_loss: 0.2395881563425064
train_iter_loss: 0.30490246415138245
train_iter_loss: 0.30618375539779663
train_iter_loss: 0.1525816172361374
train_iter_loss: 0.42072468996047974
train_iter_loss: 0.20400670170783997
train_iter_loss: 0.3682832419872284
train_iter_loss: 0.3674516975879669
train_iter_loss: 0.39636048674583435
train_iter_loss: 0.37348559498786926
train_iter_loss: 0.35955068469047546
train_iter_loss: 0.2466556578874588
train_iter_loss: 0.2825980484485626
train_iter_loss: 0.4108571410179138
train_iter_loss: 0.2883846163749695
train_iter_loss: 0.2567552924156189
train_iter_loss: 0.2939373254776001
train_iter_loss: 0.17590948939323425
train_iter_loss: 0.2280983179807663
train_iter_loss: 0.2543811798095703
train_iter_loss: 0.2235003411769867
train_iter_loss: 0.17446467280387878
train_iter_loss: 0.17714467644691467
train_iter_loss: 0.04975505918264389
train_iter_loss: 0.39613550901412964
train_iter_loss: 0.11147404462099075
train_iter_loss: 0.21951867640018463
train_iter_loss: 0.3915907144546509
train_iter_loss: 0.18069547414779663
train_iter_loss: 0.3211536705493927
train_iter_loss: 0.271733820438385
train_iter_loss: 0.2906531095504761
train_iter_loss: 0.28855645656585693
train_iter_loss: 0.25331780314445496
train_iter_loss: 0.20388442277908325
train_iter_loss: 0.17954382300376892
train_iter_loss: 0.31863340735435486
train_iter_loss: 0.38226279616355896
train_iter_loss: 0.325067400932312
train_iter_loss: 0.34415993094444275
train_iter_loss: 0.33365461230278015
train_iter_loss: 0.24875080585479736
train_iter_loss: 0.33752742409706116
train_iter_loss: 0.27595236897468567
train_iter_loss: 0.3339810073375702
train_iter_loss: 0.47158193588256836
train_iter_loss: 0.2543722689151764
train_iter_loss: 0.19221463799476624
train_iter_loss: 0.22562238574028015
train_iter_loss: 0.14458365738391876
train_iter_loss: 0.09182095527648926
train_iter_loss: 0.2021665871143341
train_iter_loss: 0.10647810995578766
train_iter_loss: 0.250033974647522
train_iter_loss: 0.21681758761405945
train_iter_loss: 0.2400815486907959
train_iter_loss: 0.23650212585926056
train_iter_loss: 0.1716310977935791
train_iter_loss: 0.304897278547287
train_iter_loss: 0.24433138966560364
train_iter_loss: 0.3094855546951294
train_iter_loss: 0.34620368480682373
train_iter_loss: 0.37319841980934143
train_iter_loss: 0.2980477511882782
train_iter_loss: 0.34114548563957214
train_iter_loss: 0.1760634332895279
train_iter_loss: 0.19193266332149506
train_iter_loss: 0.4258233606815338
train_iter_loss: 0.29331207275390625
train_iter_loss: 0.2638891041278839
train_iter_loss: 0.19259598851203918
train_iter_loss: 0.12347225099802017
train_iter_loss: 0.16815294325351715
train_iter_loss: 0.2507536709308624
train_iter_loss: 0.2821659445762634
train_iter_loss: 0.2790663242340088
train_iter_loss: 0.3015005588531494
train_iter_loss: 0.12473265826702118
train_iter_loss: 0.27377331256866455
train_iter_loss: 0.33402585983276367
train_iter_loss: 0.2860122323036194
train_iter_loss: 0.37230226397514343
train_iter_loss: 0.16897551715373993
train_iter_loss: 0.14195719361305237
train_iter_loss: 0.31363391876220703
train_iter_loss: 0.3287680149078369
train_iter_loss: 0.19088052213191986
train loss :0.2747
---------------------
Validation seg loss: 0.38562514103140755 at epoch 436
epoch =    437/  1000, exp = train
train_iter_loss: 0.4166940748691559
train_iter_loss: 0.23954884707927704
train_iter_loss: 0.32064980268478394
train_iter_loss: 0.27565398812294006
train_iter_loss: 0.2617052495479584
train_iter_loss: 0.3077802062034607
train_iter_loss: 0.3138938248157501
train_iter_loss: 0.20808111131191254
train_iter_loss: 0.27949926257133484
train_iter_loss: 0.19215112924575806
train_iter_loss: 0.2678830921649933
train_iter_loss: 0.25262096524238586
train_iter_loss: 0.21538180112838745
train_iter_loss: 0.26231348514556885
train_iter_loss: 0.3630528450012207
train_iter_loss: 0.3833022117614746
train_iter_loss: 0.3091137409210205
train_iter_loss: 0.358246773481369
train_iter_loss: 0.3084377348423004
train_iter_loss: 0.3449191451072693
train_iter_loss: 0.21059057116508484
train_iter_loss: 0.2846492528915405
train_iter_loss: 0.3238099217414856
train_iter_loss: 0.4094644784927368
train_iter_loss: 0.27101442217826843
train_iter_loss: 0.2949219048023224
train_iter_loss: 0.2658967077732086
train_iter_loss: 0.46888095140457153
train_iter_loss: 0.09367954730987549
train_iter_loss: 0.1938292384147644
train_iter_loss: 0.33108967542648315
train_iter_loss: 0.23193484544754028
train_iter_loss: 0.2988322675228119
train_iter_loss: 0.0855155810713768
train_iter_loss: 0.3966846764087677
train_iter_loss: 0.2279423475265503
train_iter_loss: 0.23700687289237976
train_iter_loss: 0.4122757315635681
train_iter_loss: 0.2509135603904724
train_iter_loss: 0.17037317156791687
train_iter_loss: 0.28237977623939514
train_iter_loss: 0.27284106612205505
train_iter_loss: 0.19443079829216003
train_iter_loss: 0.25075191259384155
train_iter_loss: 0.21008560061454773
train_iter_loss: 0.3702291250228882
train_iter_loss: 0.15064449608325958
train_iter_loss: 0.1151127964258194
train_iter_loss: 0.49740806221961975
train_iter_loss: 0.1887027621269226
train_iter_loss: 0.14643226563930511
train_iter_loss: 0.34985068440437317
train_iter_loss: 0.1561579406261444
train_iter_loss: 0.3126913011074066
train_iter_loss: 0.3498821258544922
train_iter_loss: 0.191855788230896
train_iter_loss: 0.2226528525352478
train_iter_loss: 0.18006005883216858
train_iter_loss: 0.38431406021118164
train_iter_loss: 0.2649702727794647
train_iter_loss: 0.2822257876396179
train_iter_loss: 0.2958071827888489
train_iter_loss: 0.20719414949417114
train_iter_loss: 0.24357883632183075
train_iter_loss: 0.18233104050159454
train_iter_loss: 0.3472174406051636
train_iter_loss: 0.2868000864982605
train_iter_loss: 0.2789100110530853
train_iter_loss: 0.22765184938907623
train_iter_loss: 0.33230412006378174
train_iter_loss: 0.21388334035873413
train_iter_loss: 0.3654676079750061
train_iter_loss: 0.2422124594449997
train_iter_loss: 0.3337083160877228
train_iter_loss: 0.2660987079143524
train_iter_loss: 0.23884326219558716
train_iter_loss: 0.3645097017288208
train_iter_loss: 0.2337997555732727
train_iter_loss: 0.2747266888618469
train_iter_loss: 0.24750861525535583
train_iter_loss: 0.14390739798545837
train_iter_loss: 0.2560415267944336
train_iter_loss: 0.2558608949184418
train_iter_loss: 0.40603166818618774
train_iter_loss: 0.2752048969268799
train_iter_loss: 0.4570613205432892
train_iter_loss: 0.3039374053478241
train_iter_loss: 0.19340313971042633
train_iter_loss: 0.27060097455978394
train_iter_loss: 0.1508813202381134
train_iter_loss: 0.1447308212518692
train_iter_loss: 0.426681786775589
train_iter_loss: 0.1952602118253708
train_iter_loss: 0.3191346824169159
train_iter_loss: 0.1292850822210312
train_iter_loss: 0.16240207850933075
train_iter_loss: 0.3199780583381653
train_iter_loss: 0.3094826638698578
train_iter_loss: 0.3765653371810913
train_iter_loss: 0.24099019169807434
train loss :0.2751
---------------------
Validation seg loss: 0.3828883754476061 at epoch 437
epoch =    438/  1000, exp = train
train_iter_loss: 0.29328709840774536
train_iter_loss: 0.19018076360225677
train_iter_loss: 0.31060990691185
train_iter_loss: 0.42742928862571716
train_iter_loss: 0.2773455083370209
train_iter_loss: 0.2458762377500534
train_iter_loss: 0.3654096722602844
train_iter_loss: 0.29535412788391113
train_iter_loss: 0.3065085709095001
train_iter_loss: 0.22073164582252502
train_iter_loss: 0.2395380735397339
train_iter_loss: 0.25529512763023376
train_iter_loss: 0.2845885455608368
train_iter_loss: 0.22162781655788422
train_iter_loss: 0.37152183055877686
train_iter_loss: 0.44642531871795654
train_iter_loss: 0.08775627613067627
train_iter_loss: 0.24706071615219116
train_iter_loss: 0.24206528067588806
train_iter_loss: 0.20373839139938354
train_iter_loss: 0.2515580654144287
train_iter_loss: 0.24916639924049377
train_iter_loss: 0.39051949977874756
train_iter_loss: 0.4506819546222687
train_iter_loss: 0.23580826818943024
train_iter_loss: 0.393240362405777
train_iter_loss: 0.3125486671924591
train_iter_loss: 0.131753608584404
train_iter_loss: 0.2437494695186615
train_iter_loss: 0.29466161131858826
train_iter_loss: 0.28596264123916626
train_iter_loss: 0.40186336636543274
train_iter_loss: 0.1783479005098343
train_iter_loss: 0.3031909465789795
train_iter_loss: 0.20619648694992065
train_iter_loss: 0.345681369304657
train_iter_loss: 0.2970317006111145
train_iter_loss: 0.3279533088207245
train_iter_loss: 0.15135066211223602
train_iter_loss: 0.097601518034935
train_iter_loss: 0.2882683575153351
train_iter_loss: 0.18715424835681915
train_iter_loss: 0.367250919342041
train_iter_loss: 0.3912796974182129
train_iter_loss: 0.5573636889457703
train_iter_loss: 0.2567083537578583
train_iter_loss: 0.15653669834136963
train_iter_loss: 0.3101688027381897
train_iter_loss: 0.1976923644542694
train_iter_loss: 0.20624767243862152
train_iter_loss: 0.2810152769088745
train_iter_loss: 0.21094965934753418
train_iter_loss: 0.12021676450967789
train_iter_loss: 0.4887170195579529
train_iter_loss: 0.09713898599147797
train_iter_loss: 0.24381287395954132
train_iter_loss: 0.17329740524291992
train_iter_loss: 0.24000194668769836
train_iter_loss: 0.24225717782974243
train_iter_loss: 0.2604670524597168
train_iter_loss: 0.18378503620624542
train_iter_loss: 0.227948859333992
train_iter_loss: 0.19844220578670502
train_iter_loss: 0.29390010237693787
train_iter_loss: 0.2950569987297058
train_iter_loss: 0.24181528389453888
train_iter_loss: 0.222031369805336
train_iter_loss: 0.26775941252708435
train_iter_loss: 0.30050864815711975
train_iter_loss: 0.3123317360877991
train_iter_loss: 0.3573312759399414
train_iter_loss: 0.20792007446289062
train_iter_loss: 0.3957020342350006
train_iter_loss: 0.382803350687027
train_iter_loss: 0.24357406795024872
train_iter_loss: 0.1596870869398117
train_iter_loss: 0.275661826133728
train_iter_loss: 0.3049514889717102
train_iter_loss: 0.2654930055141449
train_iter_loss: 0.21154090762138367
train_iter_loss: 0.4529496431350708
train_iter_loss: 0.1720701903104782
train_iter_loss: 0.3478175401687622
train_iter_loss: 0.269361287355423
train_iter_loss: 0.37142276763916016
train_iter_loss: 0.24398554861545563
train_iter_loss: 0.37282323837280273
train_iter_loss: 0.20482221245765686
train_iter_loss: 0.13492661714553833
train_iter_loss: 0.2946297228336334
train_iter_loss: 0.3597349524497986
train_iter_loss: 0.3194832503795624
train_iter_loss: 0.17923370003700256
train_iter_loss: 0.2523205578327179
train_iter_loss: 0.2292327731847763
train_iter_loss: 0.22780895233154297
train_iter_loss: 0.312133252620697
train_iter_loss: 0.1744655966758728
train_iter_loss: 0.3169824182987213
train_iter_loss: 0.15566223859786987
train loss :0.2737
---------------------
Validation seg loss: 0.369671895212652 at epoch 438
epoch =    439/  1000, exp = train
train_iter_loss: 0.18705907464027405
train_iter_loss: 0.4554203152656555
train_iter_loss: 0.10302025079727173
train_iter_loss: 0.31848791241645813
train_iter_loss: 0.24155564606189728
train_iter_loss: 0.27450791001319885
train_iter_loss: 0.23139438033103943
train_iter_loss: 0.3040435314178467
train_iter_loss: 0.33024269342422485
train_iter_loss: 0.25286489725112915
train_iter_loss: 0.38414111733436584
train_iter_loss: 0.33391308784484863
train_iter_loss: 0.21833617985248566
train_iter_loss: 0.29990264773368835
train_iter_loss: 0.278171569108963
train_iter_loss: 0.1124555915594101
train_iter_loss: 0.10900850594043732
train_iter_loss: 0.21586769819259644
train_iter_loss: 0.28359517455101013
train_iter_loss: 0.29391783475875854
train_iter_loss: 0.31735044717788696
train_iter_loss: 0.23373980820178986
train_iter_loss: 0.19821061193943024
train_iter_loss: 0.10103641450405121
train_iter_loss: 0.33003365993499756
train_iter_loss: 0.4177631437778473
train_iter_loss: 0.2568707764148712
train_iter_loss: 0.2509475648403168
train_iter_loss: 0.31574708223342896
train_iter_loss: 0.33733072876930237
train_iter_loss: 0.30595365166664124
train_iter_loss: 0.23822806775569916
train_iter_loss: 0.29058602452278137
train_iter_loss: 0.16052542626857758
train_iter_loss: 0.23272927105426788
train_iter_loss: 0.322182297706604
train_iter_loss: 0.3477834463119507
train_iter_loss: 0.18386264145374298
train_iter_loss: 0.25430816411972046
train_iter_loss: 0.24958500266075134
train_iter_loss: 0.18558190762996674
train_iter_loss: 0.25052276253700256
train_iter_loss: 0.25983962416648865
train_iter_loss: 0.26596561074256897
train_iter_loss: 0.290072500705719
train_iter_loss: 0.4205571115016937
train_iter_loss: 0.07878220081329346
train_iter_loss: 0.28841719031333923
train_iter_loss: 0.3238515257835388
train_iter_loss: 0.3600093126296997
train_iter_loss: 0.19902601838111877
train_iter_loss: 0.2388462871313095
train_iter_loss: 0.26317089796066284
train_iter_loss: 0.3035118877887726
train_iter_loss: 0.10811170190572739
train_iter_loss: 0.1474282592535019
train_iter_loss: 0.19896122813224792
train_iter_loss: 0.3269442319869995
train_iter_loss: 0.34727248549461365
train_iter_loss: 0.1506536304950714
train_iter_loss: 0.32143017649650574
train_iter_loss: 0.32726725935935974
train_iter_loss: 0.29707470536231995
train_iter_loss: 0.19488058984279633
train_iter_loss: 0.3901912569999695
train_iter_loss: 0.19585461914539337
train_iter_loss: 0.19145749509334564
train_iter_loss: 0.3037800192832947
train_iter_loss: 0.3197958171367645
train_iter_loss: 0.33739614486694336
train_iter_loss: 0.24150854349136353
train_iter_loss: 0.1757170855998993
train_iter_loss: 0.33123505115509033
train_iter_loss: 0.26543956995010376
train_iter_loss: 0.41030851006507874
train_iter_loss: 0.4601098299026489
train_iter_loss: 0.32055720686912537
train_iter_loss: 0.24948959052562714
train_iter_loss: 0.31186604499816895
train_iter_loss: 0.3598322868347168
train_iter_loss: 0.31425759196281433
train_iter_loss: 0.412310391664505
train_iter_loss: 0.3683474063873291
train_iter_loss: 0.2632978856563568
train_iter_loss: 0.2627377212047577
train_iter_loss: 0.3042455017566681
train_iter_loss: 0.4220632314682007
train_iter_loss: 0.16567784547805786
train_iter_loss: 0.36593276262283325
train_iter_loss: 0.527254581451416
train_iter_loss: 0.15278756618499756
train_iter_loss: 0.17524829506874084
train_iter_loss: 0.17449885606765747
train_iter_loss: 0.21422317624092102
train_iter_loss: 0.28992658853530884
train_iter_loss: 0.2561204135417938
train_iter_loss: 0.2617814838886261
train_iter_loss: 0.3050864040851593
train_iter_loss: 0.3750387728214264
train_iter_loss: 0.32083144783973694
train loss :0.2779
---------------------
Validation seg loss: 0.45860882077963566 at epoch 439
epoch =    440/  1000, exp = train
train_iter_loss: 0.3327779471874237
train_iter_loss: 0.2579844892024994
train_iter_loss: 0.24350762367248535
train_iter_loss: 0.3763052821159363
train_iter_loss: 0.35531091690063477
train_iter_loss: 0.25123682618141174
train_iter_loss: 0.26456451416015625
train_iter_loss: 0.3273976147174835
train_iter_loss: 0.2913307845592499
train_iter_loss: 0.15620765089988708
train_iter_loss: 0.17475131154060364
train_iter_loss: 0.28293198347091675
train_iter_loss: 0.27898284792900085
train_iter_loss: 0.2430036962032318
train_iter_loss: 0.42729973793029785
train_iter_loss: 0.2125309407711029
train_iter_loss: 0.28952914476394653
train_iter_loss: 0.32252541184425354
train_iter_loss: 0.4167959690093994
train_iter_loss: 0.46615302562713623
train_iter_loss: 0.4489419162273407
train_iter_loss: 0.1672724038362503
train_iter_loss: 0.23609952628612518
train_iter_loss: 0.3149822950363159
train_iter_loss: 0.29735058546066284
train_iter_loss: 0.2297234833240509
train_iter_loss: 0.20593571662902832
train_iter_loss: 0.31856536865234375
train_iter_loss: 0.1563660055398941
train_iter_loss: 0.2324281483888626
train_iter_loss: 0.2544982433319092
train_iter_loss: 0.29726746678352356
train_iter_loss: 0.25003719329833984
train_iter_loss: 0.22604411840438843
train_iter_loss: 0.2747634947299957
train_iter_loss: 0.2173740118741989
train_iter_loss: 0.32907965779304504
train_iter_loss: 0.20667488873004913
train_iter_loss: 0.5872058272361755
train_iter_loss: 0.2044444978237152
train_iter_loss: 0.2976677417755127
train_iter_loss: 0.20584151148796082
train_iter_loss: 0.1903669685125351
train_iter_loss: 0.7435095310211182
train_iter_loss: 0.39011290669441223
train_iter_loss: 0.1886189579963684
train_iter_loss: 0.18516992032527924
train_iter_loss: 0.04584445431828499
train_iter_loss: 0.2776930332183838
train_iter_loss: 0.27865368127822876
train_iter_loss: 0.2955945134162903
train_iter_loss: 0.2523302435874939
train_iter_loss: 0.1001913771033287
train_iter_loss: 0.2487897127866745
train_iter_loss: 0.30724093317985535
train_iter_loss: 0.32762232422828674
train_iter_loss: 0.2704460322856903
train_iter_loss: 0.25985926389694214
train_iter_loss: 0.2686869502067566
train_iter_loss: 0.13884954154491425
train_iter_loss: 0.382925808429718
train_iter_loss: 0.3271099627017975
train_iter_loss: 0.2331714779138565
train_iter_loss: 0.2531149983406067
train_iter_loss: 0.1638154238462448
train_iter_loss: 0.2949187457561493
train_iter_loss: 0.1997971534729004
train_iter_loss: 0.3073398768901825
train_iter_loss: 0.266139417886734
train_iter_loss: 0.14813801646232605
train_iter_loss: 0.13907784223556519
train_iter_loss: 0.375844806432724
train_iter_loss: 0.13832807540893555
train_iter_loss: 0.2531892955303192
train_iter_loss: 0.4784969687461853
train_iter_loss: 0.38472849130630493
train_iter_loss: 0.11729246377944946
train_iter_loss: 0.33376744389533997
train_iter_loss: 0.21864531934261322
train_iter_loss: 0.19385023415088654
train_iter_loss: 0.19526922702789307
train_iter_loss: 0.28799664974212646
train_iter_loss: 0.18538421392440796
train_iter_loss: 0.16588585078716278
train_iter_loss: 0.32732778787612915
train_iter_loss: 0.3663012981414795
train_iter_loss: 0.4004019796848297
train_iter_loss: 0.28718316555023193
train_iter_loss: 0.2914028465747833
train_iter_loss: 0.3583103120326996
train_iter_loss: 0.22124995291233063
train_iter_loss: 0.34848251938819885
train_iter_loss: 0.1839379519224167
train_iter_loss: 0.24836412072181702
train_iter_loss: 0.3186042606830597
train_iter_loss: 0.21610255539417267
train_iter_loss: 0.26177236437797546
train_iter_loss: 0.3467176556587219
train_iter_loss: 0.2459561675786972
train_iter_loss: 0.36199596524238586
train loss :0.2781
---------------------
Validation seg loss: 0.36619613124584816 at epoch 440
epoch =    441/  1000, exp = train
train_iter_loss: 0.34347882866859436
train_iter_loss: 0.309781014919281
train_iter_loss: 0.25942862033843994
train_iter_loss: 0.2389811873435974
train_iter_loss: 0.23766157031059265
train_iter_loss: 0.3168407380580902
train_iter_loss: 0.25722819566726685
train_iter_loss: 0.3744089901447296
train_iter_loss: 0.21415553987026215
train_iter_loss: 0.19605004787445068
train_iter_loss: 0.2197430580854416
train_iter_loss: 0.15134136378765106
train_iter_loss: 0.3519878685474396
train_iter_loss: 0.4345860183238983
train_iter_loss: 0.21870215237140656
train_iter_loss: 0.14566171169281006
train_iter_loss: 0.3081297278404236
train_iter_loss: 0.34124377369880676
train_iter_loss: 0.21551306545734406
train_iter_loss: 0.0890219658613205
train_iter_loss: 0.34252306818962097
train_iter_loss: 0.24282380938529968
train_iter_loss: 0.10483980178833008
train_iter_loss: 0.23861677944660187
train_iter_loss: 0.2121989130973816
train_iter_loss: 0.22737781703472137
train_iter_loss: 0.3612775504589081
train_iter_loss: 0.3074513375759125
train_iter_loss: 0.35854974389076233
train_iter_loss: 0.28336361050605774
train_iter_loss: 0.28785690665245056
train_iter_loss: 0.2082216441631317
train_iter_loss: 0.3239913880825043
train_iter_loss: 0.2571580708026886
train_iter_loss: 0.21439963579177856
train_iter_loss: 0.28073447942733765
train_iter_loss: 0.28878530859947205
train_iter_loss: 0.16891279816627502
train_iter_loss: 0.30290156602859497
train_iter_loss: 0.26988041400909424
train_iter_loss: 0.4472050368785858
train_iter_loss: 0.07524372637271881
train_iter_loss: 0.28578776121139526
train_iter_loss: 0.23493315279483795
train_iter_loss: 0.29171523451805115
train_iter_loss: 0.3061213493347168
train_iter_loss: 0.47026145458221436
train_iter_loss: 0.21761161088943481
train_iter_loss: 0.4274088442325592
train_iter_loss: 0.374057799577713
train_iter_loss: 0.3091017007827759
train_iter_loss: 0.274095743894577
train_iter_loss: 0.1938266009092331
train_iter_loss: 0.35192424058914185
train_iter_loss: 0.2587020695209503
train_iter_loss: 0.1938614547252655
train_iter_loss: 0.16076648235321045
train_iter_loss: 0.40074416995048523
train_iter_loss: 0.3257931172847748
train_iter_loss: 0.5203605890274048
train_iter_loss: 0.26005756855010986
train_iter_loss: 0.26703986525535583
train_iter_loss: 0.4562050402164459
train_iter_loss: 0.1290019303560257
train_iter_loss: 0.3206563889980316
train_iter_loss: 0.3299398720264435
train_iter_loss: 0.226076140999794
train_iter_loss: 0.284451425075531
train_iter_loss: 0.2804957628250122
train_iter_loss: 0.1674594283103943
train_iter_loss: 0.23620009422302246
train_iter_loss: 0.26659536361694336
train_iter_loss: 0.26075851917266846
train_iter_loss: 0.3712635040283203
train_iter_loss: 0.27217766642570496
train_iter_loss: 0.25620293617248535
train_iter_loss: 0.3123694062232971
train_iter_loss: 0.09452518075704575
train_iter_loss: 0.2542676031589508
train_iter_loss: 0.35142672061920166
train_iter_loss: 0.27855491638183594
train_iter_loss: 0.21751034259796143
train_iter_loss: 0.1321200579404831
train_iter_loss: 0.12629683315753937
train_iter_loss: 0.3158248960971832
train_iter_loss: 0.23589077591896057
train_iter_loss: 0.3249122202396393
train_iter_loss: 0.17143014073371887
train_iter_loss: 0.27609699964523315
train_iter_loss: 0.29474133253097534
train_iter_loss: 0.31411319971084595
train_iter_loss: 0.2846788465976715
train_iter_loss: 0.27120569348335266
train_iter_loss: 0.24724121391773224
train_iter_loss: 0.22657708823680878
train_iter_loss: 0.2846892476081848
train_iter_loss: 0.21124781668186188
train_iter_loss: 0.31889966130256653
train_iter_loss: 0.3619731366634369
train_iter_loss: 0.14294904470443726
train loss :0.2734
---------------------
Validation seg loss: 0.3913906437327277 at epoch 441
epoch =    442/  1000, exp = train
train_iter_loss: 0.10056518018245697
train_iter_loss: 0.33714091777801514
train_iter_loss: 0.1764858365058899
train_iter_loss: 0.32518136501312256
train_iter_loss: 0.29974600672721863
train_iter_loss: 0.3759332597255707
train_iter_loss: 0.20598171651363373
train_iter_loss: 0.3430309295654297
train_iter_loss: 0.3007482886314392
train_iter_loss: 0.3100424110889435
train_iter_loss: 0.23901978135108948
train_iter_loss: 0.3069922626018524
train_iter_loss: 0.25578930974006653
train_iter_loss: 0.35410892963409424
train_iter_loss: 0.320644736289978
train_iter_loss: 0.30379581451416016
train_iter_loss: 0.14575648307800293
train_iter_loss: 0.23310694098472595
train_iter_loss: 0.14775493741035461
train_iter_loss: 0.14697089791297913
train_iter_loss: 0.3641853928565979
train_iter_loss: 0.3225457966327667
train_iter_loss: 0.3518073558807373
train_iter_loss: 0.17558808624744415
train_iter_loss: 0.3420962989330292
train_iter_loss: 0.28008386492729187
train_iter_loss: 0.4732096195220947
train_iter_loss: 0.2410203516483307
train_iter_loss: 0.20974400639533997
train_iter_loss: 0.17923370003700256
train_iter_loss: 0.18505561351776123
train_iter_loss: 0.519889235496521
train_iter_loss: 0.3310889005661011
train_iter_loss: 0.2569943368434906
train_iter_loss: 0.294286847114563
train_iter_loss: 0.23183156549930573
train_iter_loss: 0.2767353355884552
train_iter_loss: 0.1336338371038437
train_iter_loss: 0.192350372672081
train_iter_loss: 0.3594626188278198
train_iter_loss: 0.38717105984687805
train_iter_loss: 0.19423449039459229
train_iter_loss: 0.22919900715351105
train_iter_loss: 0.3172071576118469
train_iter_loss: 0.16775424778461456
train_iter_loss: 0.16638079285621643
train_iter_loss: 0.45363253355026245
train_iter_loss: 0.1476224809885025
train_iter_loss: 0.28451451659202576
train_iter_loss: 0.19068019092082977
train_iter_loss: 0.2044861763715744
train_iter_loss: 0.299659788608551
train_iter_loss: 0.12187346071004868
train_iter_loss: 0.258495956659317
train_iter_loss: 0.373468816280365
train_iter_loss: 0.3576416075229645
train_iter_loss: 0.1760903149843216
train_iter_loss: 0.372763454914093
train_iter_loss: 0.1566719114780426
train_iter_loss: 0.3148059546947479
train_iter_loss: 0.17751529812812805
train_iter_loss: 0.28159019351005554
train_iter_loss: 0.21496425569057465
train_iter_loss: 0.3826388716697693
train_iter_loss: 0.18298651278018951
train_iter_loss: 0.572787880897522
train_iter_loss: 0.17629368603229523
train_iter_loss: 0.3584693968296051
train_iter_loss: 0.23607902228832245
train_iter_loss: 0.3953031003475189
train_iter_loss: 0.3637735843658447
train_iter_loss: 0.2854767143726349
train_iter_loss: 0.40606942772865295
train_iter_loss: 0.24853454530239105
train_iter_loss: 0.24257895350456238
train_iter_loss: 0.249996617436409
train_iter_loss: 0.29258066415786743
train_iter_loss: 0.26280349493026733
train_iter_loss: 0.4027727246284485
train_iter_loss: 0.3053387701511383
train_iter_loss: 0.2896083891391754
train_iter_loss: 0.18056736886501312
train_iter_loss: 0.36010608077049255
train_iter_loss: 0.2562648355960846
train_iter_loss: 0.2951810657978058
train_iter_loss: 0.15217582881450653
train_iter_loss: 0.18808847665786743
train_iter_loss: 0.28873878717422485
train_iter_loss: 0.18987055122852325
train_iter_loss: 0.1610623151063919
train_iter_loss: 0.31099823117256165
train_iter_loss: 0.278532475233078
train_iter_loss: 0.2050362527370453
train_iter_loss: 0.2675122320652008
train_iter_loss: 0.10754841566085815
train_iter_loss: 0.31936243176460266
train_iter_loss: 0.3621963858604431
train_iter_loss: 0.2232389599084854
train_iter_loss: 0.37601807713508606
train_iter_loss: 0.2893979251384735
train loss :0.2754
---------------------
Validation seg loss: 0.3664813026574985 at epoch 442
epoch =    443/  1000, exp = train
train_iter_loss: 0.30852261185646057
train_iter_loss: 0.250303715467453
train_iter_loss: 0.34812813997268677
train_iter_loss: 0.15224908292293549
train_iter_loss: 0.2671487331390381
train_iter_loss: 0.2784285545349121
train_iter_loss: 0.19600455462932587
train_iter_loss: 0.262572318315506
train_iter_loss: 0.24627277255058289
train_iter_loss: 0.19982364773750305
train_iter_loss: 0.1675787717103958
train_iter_loss: 0.3015316128730774
train_iter_loss: 0.3693312108516693
train_iter_loss: 0.23884283006191254
train_iter_loss: 0.19283941388130188
train_iter_loss: 0.273593544960022
train_iter_loss: 0.4004267752170563
train_iter_loss: 0.21233966946601868
train_iter_loss: 0.23201805353164673
train_iter_loss: 0.3005680441856384
train_iter_loss: 0.3230014145374298
train_iter_loss: 0.22726692259311676
train_iter_loss: 0.3057388961315155
train_iter_loss: 0.24093133211135864
train_iter_loss: 0.3769642412662506
train_iter_loss: 0.33291980624198914
train_iter_loss: 0.38553956151008606
train_iter_loss: 0.24558709561824799
train_iter_loss: 0.37860459089279175
train_iter_loss: 0.25315484404563904
train_iter_loss: 0.2781447768211365
train_iter_loss: 0.27869346737861633
train_iter_loss: 0.22456751763820648
train_iter_loss: 0.31576985120773315
train_iter_loss: 0.20692189037799835
train_iter_loss: 0.20917920768260956
train_iter_loss: 0.05691159889101982
train_iter_loss: 0.23657813668251038
train_iter_loss: 0.25561535358428955
train_iter_loss: 0.1542387306690216
train_iter_loss: 0.11049579083919525
train_iter_loss: 0.26828545331954956
train_iter_loss: 0.29895690083503723
train_iter_loss: 0.23887915909290314
train_iter_loss: 0.22117984294891357
train_iter_loss: 0.2895587086677551
train_iter_loss: 0.4117242395877838
train_iter_loss: 0.2324974238872528
train_iter_loss: 0.3223173916339874
train_iter_loss: 0.18594765663146973
train_iter_loss: 0.35418030619621277
train_iter_loss: 0.17712271213531494
train_iter_loss: 0.3726591169834137
train_iter_loss: 0.09919522702693939
train_iter_loss: 0.23642411828041077
train_iter_loss: 0.21464939415454865
train_iter_loss: 0.3254251182079315
train_iter_loss: 0.22542008757591248
train_iter_loss: 0.3387053608894348
train_iter_loss: 0.28951510787010193
train_iter_loss: 0.37847763299942017
train_iter_loss: 0.30363404750823975
train_iter_loss: 0.4354797303676605
train_iter_loss: 0.18118885159492493
train_iter_loss: 0.2647102177143097
train_iter_loss: 0.45751726627349854
train_iter_loss: 0.3833691477775574
train_iter_loss: 0.28351885080337524
train_iter_loss: 0.1810305416584015
train_iter_loss: 0.2421698421239853
train_iter_loss: 0.35995084047317505
train_iter_loss: 0.26207971572875977
train_iter_loss: 0.304615318775177
train_iter_loss: 0.2804984152317047
train_iter_loss: 0.5791004300117493
train_iter_loss: 0.3124856948852539
train_iter_loss: 0.2122257500886917
train_iter_loss: 0.33892822265625
train_iter_loss: 0.2123786211013794
train_iter_loss: 0.25186824798583984
train_iter_loss: 0.2054837942123413
train_iter_loss: 0.2423732429742813
train_iter_loss: 0.28323280811309814
train_iter_loss: 0.3427468240261078
train_iter_loss: 0.20376832783222198
train_iter_loss: 0.27780455350875854
train_iter_loss: 0.24850238859653473
train_iter_loss: 0.24345850944519043
train_iter_loss: 0.3058902323246002
train_iter_loss: 0.2318592518568039
train_iter_loss: 0.29797041416168213
train_iter_loss: 0.29247307777404785
train_iter_loss: 0.2967890202999115
train_iter_loss: 0.2548202574253082
train_iter_loss: 0.3596124053001404
train_iter_loss: 0.20225916802883148
train_iter_loss: 0.17011912167072296
train_iter_loss: 0.24770061671733856
train_iter_loss: 0.35593560338020325
train_iter_loss: 0.19335035979747772
train loss :0.2745
---------------------
Validation seg loss: 0.4073828668070008 at epoch 443
epoch =    444/  1000, exp = train
train_iter_loss: 0.231378436088562
train_iter_loss: 0.3001643419265747
train_iter_loss: 0.3188070058822632
train_iter_loss: 0.10864917933940887
train_iter_loss: 0.22939835488796234
train_iter_loss: 0.3632798492908478
train_iter_loss: 0.12873181700706482
train_iter_loss: 0.39499667286872864
train_iter_loss: 0.23419737815856934
train_iter_loss: 0.2647976279258728
train_iter_loss: 0.3652234971523285
train_iter_loss: 0.24774542450904846
train_iter_loss: 0.38151848316192627
train_iter_loss: 0.3288716673851013
train_iter_loss: 0.28019067645072937
train_iter_loss: 0.17065390944480896
train_iter_loss: 0.14787673950195312
train_iter_loss: 0.3110547959804535
train_iter_loss: 0.2273504137992859
train_iter_loss: 0.18179063498973846
train_iter_loss: 0.111781544983387
train_iter_loss: 0.2482089400291443
train_iter_loss: 0.1705142706632614
train_iter_loss: 0.43442967534065247
train_iter_loss: 0.2908710241317749
train_iter_loss: 0.24171683192253113
train_iter_loss: 0.13419760763645172
train_iter_loss: 0.3198513686656952
train_iter_loss: 0.21164719760417938
train_iter_loss: 0.2995225191116333
train_iter_loss: 0.18088245391845703
train_iter_loss: 0.24854207038879395
train_iter_loss: 0.269392192363739
train_iter_loss: 0.21271148324012756
train_iter_loss: 0.2018216997385025
train_iter_loss: 0.615329384803772
train_iter_loss: 0.3295736610889435
train_iter_loss: 0.17627854645252228
train_iter_loss: 0.3710358440876007
train_iter_loss: 0.1307961493730545
train_iter_loss: 0.2697662115097046
train_iter_loss: 0.40463531017303467
train_iter_loss: 0.28999388217926025
train_iter_loss: 0.14661026000976562
train_iter_loss: 0.24144889414310455
train_iter_loss: 0.39676201343536377
train_iter_loss: 0.513871431350708
train_iter_loss: 0.3610379993915558
train_iter_loss: 0.17925751209259033
train_iter_loss: 0.20775750279426575
train_iter_loss: 0.2537994086742401
train_iter_loss: 0.33596011996269226
train_iter_loss: 0.3737744092941284
train_iter_loss: 0.2957172691822052
train_iter_loss: 0.3408443331718445
train_iter_loss: 0.22454990446567535
train_iter_loss: 0.1427641063928604
train_iter_loss: 0.3015095293521881
train_iter_loss: 0.23815272748470306
train_iter_loss: 0.33541637659072876
train_iter_loss: 0.233856201171875
train_iter_loss: 0.20742234587669373
train_iter_loss: 0.40613600611686707
train_iter_loss: 0.2999466061592102
train_iter_loss: 0.3248646855354309
train_iter_loss: 0.15855661034584045
train_iter_loss: 0.30317819118499756
train_iter_loss: 0.2377978414297104
train_iter_loss: 0.3707229197025299
train_iter_loss: 0.2305600643157959
train_iter_loss: 0.38763752579689026
train_iter_loss: 0.13556541502475739
train_iter_loss: 0.27848199009895325
train_iter_loss: 0.3470087945461273
train_iter_loss: 0.20965784788131714
train_iter_loss: 0.21330700814723969
train_iter_loss: 0.16546829044818878
train_iter_loss: 0.2769296169281006
train_iter_loss: 0.30799728631973267
train_iter_loss: 0.2833569347858429
train_iter_loss: 0.32226356863975525
train_iter_loss: 0.2336563766002655
train_iter_loss: 0.5387955904006958
train_iter_loss: 0.26652607321739197
train_iter_loss: 0.27190008759498596
train_iter_loss: 0.3863801658153534
train_iter_loss: 0.21302759647369385
train_iter_loss: 0.24154610931873322
train_iter_loss: 0.137619286775589
train_iter_loss: 0.2646656632423401
train_iter_loss: 0.23920392990112305
train_iter_loss: 0.23052720725536346
train_iter_loss: 0.12349215894937515
train_iter_loss: 0.15172497928142548
train_iter_loss: 0.2016199678182602
train_iter_loss: 0.2377142608165741
train_iter_loss: 0.6099221706390381
train_iter_loss: 0.3109530806541443
train_iter_loss: 0.2784070372581482
train_iter_loss: 0.3204505145549774
train loss :0.2750
---------------------
Validation seg loss: 0.36335542100428975 at epoch 444
epoch =    445/  1000, exp = train
train_iter_loss: 0.25272274017333984
train_iter_loss: 0.2728602886199951
train_iter_loss: 0.3128446936607361
train_iter_loss: 0.3218446671962738
train_iter_loss: 0.23908421397209167
train_iter_loss: 0.11906815320253372
train_iter_loss: 0.24115610122680664
train_iter_loss: 0.1754900962114334
train_iter_loss: 0.38962113857269287
train_iter_loss: 0.17768533527851105
train_iter_loss: 0.3141620457172394
train_iter_loss: 0.2194562703371048
train_iter_loss: 0.42881423234939575
train_iter_loss: 0.23794156312942505
train_iter_loss: 0.28703707456588745
train_iter_loss: 0.22486165165901184
train_iter_loss: 0.32131654024124146
train_iter_loss: 0.0703170970082283
train_iter_loss: 0.34237509965896606
train_iter_loss: 0.3261680603027344
train_iter_loss: 0.4063975512981415
train_iter_loss: 0.20615750551223755
train_iter_loss: 0.2834372818470001
train_iter_loss: 0.5775413513183594
train_iter_loss: 0.4007115960121155
train_iter_loss: 0.13485309481620789
train_iter_loss: 0.2773963212966919
train_iter_loss: 0.16618968546390533
train_iter_loss: 0.21553030610084534
train_iter_loss: 0.34580832719802856
train_iter_loss: 0.3383692800998688
train_iter_loss: 0.3964487314224243
train_iter_loss: 0.2871202230453491
train_iter_loss: 0.21865394711494446
train_iter_loss: 0.3497261703014374
train_iter_loss: 0.11389389634132385
train_iter_loss: 0.2852743864059448
train_iter_loss: 0.4072611927986145
train_iter_loss: 0.32961341738700867
train_iter_loss: 0.320816308259964
train_iter_loss: 0.24400587379932404
train_iter_loss: 0.19799472391605377
train_iter_loss: 0.27869993448257446
train_iter_loss: 0.23350267112255096
train_iter_loss: 0.4112483561038971
train_iter_loss: 0.054288268089294434
train_iter_loss: 0.17674195766448975
train_iter_loss: 0.26527151465415955
train_iter_loss: 0.30669236183166504
train_iter_loss: 0.2950272262096405
train_iter_loss: 0.14524394273757935
train_iter_loss: 0.3348534107208252
train_iter_loss: 0.238467738032341
train_iter_loss: 0.20277215540409088
train_iter_loss: 0.2992410957813263
train_iter_loss: 0.32560020685195923
train_iter_loss: 0.17432284355163574
train_iter_loss: 0.29274460673332214
train_iter_loss: 0.1820448935031891
train_iter_loss: 0.16061194241046906
train_iter_loss: 0.1574232131242752
train_iter_loss: 0.3198242783546448
train_iter_loss: 0.23903457820415497
train_iter_loss: 0.3270969092845917
train_iter_loss: 0.2881839871406555
train_iter_loss: 0.2620467245578766
train_iter_loss: 0.2998380661010742
train_iter_loss: 0.2910344898700714
train_iter_loss: 0.30317509174346924
train_iter_loss: 0.29182150959968567
train_iter_loss: 0.289243221282959
train_iter_loss: 0.22403062880039215
train_iter_loss: 0.4165523946285248
train_iter_loss: 0.33573606610298157
train_iter_loss: 0.14495819807052612
train_iter_loss: 0.2866276502609253
train_iter_loss: 0.14231492578983307
train_iter_loss: 0.3366454839706421
train_iter_loss: 0.22243593633174896
train_iter_loss: 0.22800080478191376
train_iter_loss: 0.23968945443630219
train_iter_loss: 0.4137704372406006
train_iter_loss: 0.2525838315486908
train_iter_loss: 0.27374598383903503
train_iter_loss: 0.2722390592098236
train_iter_loss: 0.3354537785053253
train_iter_loss: 0.10367367416620255
train_iter_loss: 0.4547145664691925
train_iter_loss: 0.26828503608703613
train_iter_loss: 0.4241337478160858
train_iter_loss: 0.33882108330726624
train_iter_loss: 0.17691998183727264
train_iter_loss: 0.2350737303495407
train_iter_loss: 0.33231666684150696
train_iter_loss: 0.1052255928516388
train_iter_loss: 0.3985513150691986
train_iter_loss: 0.2888709306716919
train_iter_loss: 0.4050377905368805
train_iter_loss: 0.3065551221370697
train_iter_loss: 0.1716151237487793
train loss :0.2763
---------------------
Validation seg loss: 0.389951348656191 at epoch 445
epoch =    446/  1000, exp = train
train_iter_loss: 0.31139999628067017
train_iter_loss: 0.2782743275165558
train_iter_loss: 0.27021074295043945
train_iter_loss: 0.35129156708717346
train_iter_loss: 0.17501309514045715
train_iter_loss: 0.32394182682037354
train_iter_loss: 0.3330298364162445
train_iter_loss: 0.33357635140419006
train_iter_loss: 0.40128859877586365
train_iter_loss: 0.2189072221517563
train_iter_loss: 0.3127598166465759
train_iter_loss: 0.26459795236587524
train_iter_loss: 0.32891279458999634
train_iter_loss: 0.4115191698074341
train_iter_loss: 0.2086712121963501
train_iter_loss: 0.3105985224246979
train_iter_loss: 0.42385920882225037
train_iter_loss: 0.1786840558052063
train_iter_loss: 0.38811254501342773
train_iter_loss: 0.21863257884979248
train_iter_loss: 0.22574135661125183
train_iter_loss: 0.19599679112434387
train_iter_loss: 0.4823281466960907
train_iter_loss: 0.38064131140708923
train_iter_loss: 0.19889923930168152
train_iter_loss: 0.37779557704925537
train_iter_loss: 0.7576421499252319
train_iter_loss: 0.33162522315979004
train_iter_loss: 0.28807130455970764
train_iter_loss: 0.23204956948757172
train_iter_loss: 0.30042973160743713
train_iter_loss: 0.2614666521549225
train_iter_loss: 0.5032852292060852
train_iter_loss: 0.1657366007566452
train_iter_loss: 0.21089796721935272
train_iter_loss: 0.2605118453502655
train_iter_loss: 0.19780856370925903
train_iter_loss: 0.27758756279945374
train_iter_loss: 0.20735979080200195
train_iter_loss: 0.26225748658180237
train_iter_loss: 0.1982516497373581
train_iter_loss: 0.30903708934783936
train_iter_loss: 0.2666377127170563
train_iter_loss: 0.2607695460319519
train_iter_loss: 0.14917303621768951
train_iter_loss: 0.25396206974983215
train_iter_loss: 0.3651673495769501
train_iter_loss: 0.2733677327632904
train_iter_loss: 0.23181168735027313
train_iter_loss: 0.27785012125968933
train_iter_loss: 0.2879871428012848
train_iter_loss: 0.19159841537475586
train_iter_loss: 0.2949204742908478
train_iter_loss: 0.19727246463298798
train_iter_loss: 0.24148496985435486
train_iter_loss: 0.19971267879009247
train_iter_loss: 0.4588375389575958
train_iter_loss: 0.11103767156600952
train_iter_loss: 0.10864643007516861
train_iter_loss: 0.3723701536655426
train_iter_loss: 0.24695415794849396
train_iter_loss: 0.2741490304470062
train_iter_loss: 0.284878134727478
train_iter_loss: 0.14078783988952637
train_iter_loss: 0.27220216393470764
train_iter_loss: 0.2010580599308014
train_iter_loss: 0.31381916999816895
train_iter_loss: 0.12936633825302124
train_iter_loss: 0.35277748107910156
train_iter_loss: 0.3780612051486969
train_iter_loss: 0.31263959407806396
train_iter_loss: 0.16645924746990204
train_iter_loss: 0.2665846645832062
train_iter_loss: 0.24132713675498962
train_iter_loss: 0.24808864295482635
train_iter_loss: 0.11546678841114044
train_iter_loss: 0.20652544498443604
train_iter_loss: 0.4127126634120941
train_iter_loss: 0.23994311690330505
train_iter_loss: 0.272203266620636
train_iter_loss: 0.2573087513446808
train_iter_loss: 0.14197245240211487
train_iter_loss: 0.22187942266464233
train_iter_loss: 0.32393398880958557
train_iter_loss: 0.2589888870716095
train_iter_loss: 0.29704588651657104
train_iter_loss: 0.17308129370212555
train_iter_loss: 0.2503487765789032
train_iter_loss: 0.187804713845253
train_iter_loss: 0.18456315994262695
train_iter_loss: 0.30531075596809387
train_iter_loss: 0.1834804117679596
train_iter_loss: 0.16852201521396637
train_iter_loss: 0.3933607339859009
train_iter_loss: 0.32878628373146057
train_iter_loss: 0.1752948760986328
train_iter_loss: 0.12318403273820877
train_iter_loss: 0.31778204441070557
train_iter_loss: 0.19170303642749786
train_iter_loss: 0.315586119890213
train loss :0.2736
---------------------
Validation seg loss: 0.39054240157195136 at epoch 446
epoch =    447/  1000, exp = train
train_iter_loss: 0.13776037096977234
train_iter_loss: 0.24127216637134552
train_iter_loss: 0.22643429040908813
train_iter_loss: 0.24914416670799255
train_iter_loss: 0.375806987285614
train_iter_loss: 0.3362191319465637
train_iter_loss: 0.19383151829242706
train_iter_loss: 0.14920301735401154
train_iter_loss: 0.4243811368942261
train_iter_loss: 0.2684682309627533
train_iter_loss: 0.41470202803611755
train_iter_loss: 0.2350517213344574
train_iter_loss: 0.19503851234912872
train_iter_loss: 0.17804008722305298
train_iter_loss: 0.17707586288452148
train_iter_loss: 0.32003724575042725
train_iter_loss: 0.26931044459342957
train_iter_loss: 0.26872900128364563
train_iter_loss: 0.17307615280151367
train_iter_loss: 0.23636242747306824
train_iter_loss: 0.26506268978118896
train_iter_loss: 0.40570881962776184
train_iter_loss: 0.28609415888786316
train_iter_loss: 0.27257025241851807
train_iter_loss: 0.25075003504753113
train_iter_loss: 0.09632612764835358
train_iter_loss: 0.2916753888130188
train_iter_loss: 0.2574045956134796
train_iter_loss: 0.31882980465888977
train_iter_loss: 0.2804718613624573
train_iter_loss: 0.3061048090457916
train_iter_loss: 0.37305745482444763
train_iter_loss: 0.2308482974767685
train_iter_loss: 0.12407644093036652
train_iter_loss: 0.3664272725582123
train_iter_loss: 0.15829937160015106
train_iter_loss: 0.38069748878479004
train_iter_loss: 0.2858162522315979
train_iter_loss: 0.31053274869918823
train_iter_loss: 0.25730642676353455
train_iter_loss: 0.34917744994163513
train_iter_loss: 0.2428705245256424
train_iter_loss: 0.15185801684856415
train_iter_loss: 0.23366816341876984
train_iter_loss: 0.14479148387908936
train_iter_loss: 0.15251955389976501
train_iter_loss: 0.3392235338687897
train_iter_loss: 0.18336839973926544
train_iter_loss: 0.1883278787136078
train_iter_loss: 0.4781529903411865
train_iter_loss: 0.3134390413761139
train_iter_loss: 0.2315692901611328
train_iter_loss: 0.09074153751134872
train_iter_loss: 0.17993076145648956
train_iter_loss: 0.31122905015945435
train_iter_loss: 0.36002257466316223
train_iter_loss: 0.1786460429430008
train_iter_loss: 0.2661600708961487
train_iter_loss: 0.19784052670001984
train_iter_loss: 0.20103468000888824
train_iter_loss: 0.18003585934638977
train_iter_loss: 0.08140920847654343
train_iter_loss: 0.30424147844314575
train_iter_loss: 0.34737515449523926
train_iter_loss: 0.33493170142173767
train_iter_loss: 0.24846816062927246
train_iter_loss: 0.34098541736602783
train_iter_loss: 0.35085752606391907
train_iter_loss: 0.3195354640483856
train_iter_loss: 0.24372076988220215
train_iter_loss: 0.4850331246852875
train_iter_loss: 0.06943287700414658
train_iter_loss: 0.24816954135894775
train_iter_loss: 0.3750802278518677
train_iter_loss: 0.24694152176380157
train_iter_loss: 0.3561970293521881
train_iter_loss: 0.3143492639064789
train_iter_loss: 0.2971312403678894
train_iter_loss: 0.30980855226516724
train_iter_loss: 0.35306212306022644
train_iter_loss: 0.19383060932159424
train_iter_loss: 0.3189404606819153
train_iter_loss: 0.2411319762468338
train_iter_loss: 0.4902852177619934
train_iter_loss: 0.2185753732919693
train_iter_loss: 0.4075019955635071
train_iter_loss: 0.2610418498516083
train_iter_loss: 0.29838407039642334
train_iter_loss: 0.3798418939113617
train_iter_loss: 0.14919687807559967
train_iter_loss: 0.22596345841884613
train_iter_loss: 0.3013020157814026
train_iter_loss: 0.25316059589385986
train_iter_loss: 0.34783318638801575
train_iter_loss: 0.2924465239048004
train_iter_loss: 0.27139347791671753
train_iter_loss: 0.30237460136413574
train_iter_loss: 0.37893569469451904
train_iter_loss: 0.2233128845691681
train_iter_loss: 0.1418810486793518
train loss :0.2716
---------------------
Validation seg loss: 0.3589563488538535 at epoch 447
epoch =    448/  1000, exp = train
train_iter_loss: 0.4085833728313446
train_iter_loss: 0.3652060031890869
train_iter_loss: 0.2218884974718094
train_iter_loss: 0.2841518521308899
train_iter_loss: 0.2697076201438904
train_iter_loss: 0.23703323304653168
train_iter_loss: 0.2784292995929718
train_iter_loss: 0.36813831329345703
train_iter_loss: 0.3875548541545868
train_iter_loss: 0.22206030786037445
train_iter_loss: 0.23952218890190125
train_iter_loss: 0.22556635737419128
train_iter_loss: 0.2697669565677643
train_iter_loss: 0.2552623152732849
train_iter_loss: 0.17582878470420837
train_iter_loss: 0.22324471175670624
train_iter_loss: 0.3377533555030823
train_iter_loss: 0.43647482991218567
train_iter_loss: 0.17941389977931976
train_iter_loss: 0.2974328398704529
train_iter_loss: 0.37870702147483826
train_iter_loss: 0.11975884437561035
train_iter_loss: 0.16368786990642548
train_iter_loss: 0.27714595198631287
train_iter_loss: 0.19645360112190247
train_iter_loss: 0.169657364487648
train_iter_loss: 0.20200270414352417
train_iter_loss: 0.28132355213165283
train_iter_loss: 0.30791544914245605
train_iter_loss: 0.2150019258260727
train_iter_loss: 0.3343185484409332
train_iter_loss: 0.28819894790649414
train_iter_loss: 0.24208998680114746
train_iter_loss: 0.26508060097694397
train_iter_loss: 0.33919966220855713
train_iter_loss: 0.24107769131660461
train_iter_loss: 0.3377825617790222
train_iter_loss: 0.2440434992313385
train_iter_loss: 0.32237619161605835
train_iter_loss: 0.07862458378076553
train_iter_loss: 0.2115197330713272
train_iter_loss: 0.12973925471305847
train_iter_loss: 0.23692800104618073
train_iter_loss: 0.18928982317447662
train_iter_loss: 0.268710196018219
train_iter_loss: 0.31758415699005127
train_iter_loss: 0.2669569253921509
train_iter_loss: 0.17124566435813904
train_iter_loss: 0.1531895399093628
train_iter_loss: 0.34914442896842957
train_iter_loss: 0.3462490737438202
train_iter_loss: 0.3055788278579712
train_iter_loss: 0.24610833823680878
train_iter_loss: 0.2672073245048523
train_iter_loss: 0.4839431941509247
train_iter_loss: 0.20665231347084045
train_iter_loss: 0.3153051435947418
train_iter_loss: 0.2316926121711731
train_iter_loss: 0.30200034379959106
train_iter_loss: 0.2410653829574585
train_iter_loss: 0.3165544271469116
train_iter_loss: 0.2596789598464966
train_iter_loss: 0.1120385229587555
train_iter_loss: 0.1312255710363388
train_iter_loss: 0.22818730771541595
train_iter_loss: 0.37275874614715576
train_iter_loss: 0.19174911081790924
train_iter_loss: 0.45100146532058716
train_iter_loss: 0.2654607594013214
train_iter_loss: 0.2999936640262604
train_iter_loss: 0.34494656324386597
train_iter_loss: 0.2146352082490921
train_iter_loss: 0.3213077485561371
train_iter_loss: 0.17630869150161743
train_iter_loss: 0.2135429084300995
train_iter_loss: 0.3053235709667206
train_iter_loss: 0.26853156089782715
train_iter_loss: 0.17058847844600677
train_iter_loss: 0.21398858726024628
train_iter_loss: 0.30867117643356323
train_iter_loss: 0.3642706573009491
train_iter_loss: 0.21724854409694672
train_iter_loss: 0.2848931849002838
train_iter_loss: 0.2556941509246826
train_iter_loss: 0.2923816442489624
train_iter_loss: 0.3517608940601349
train_iter_loss: 0.25903481245040894
train_iter_loss: 0.5902887582778931
train_iter_loss: 0.2759808599948883
train_iter_loss: 0.28456881642341614
train_iter_loss: 0.24612830579280853
train_iter_loss: 0.3104571998119354
train_iter_loss: 0.20925003290176392
train_iter_loss: 0.21903419494628906
train_iter_loss: 0.22186700999736786
train_iter_loss: 0.23044630885124207
train_iter_loss: 0.3214035630226135
train_iter_loss: 0.49456101655960083
train_iter_loss: 0.23052988946437836
train_iter_loss: 0.30488693714141846
train loss :0.2733
---------------------
Validation seg loss: 0.38191698902761034 at epoch 448
epoch =    449/  1000, exp = train
train_iter_loss: 0.19655396044254303
train_iter_loss: 0.41650304198265076
train_iter_loss: 0.15788228809833527
train_iter_loss: 0.30135586857795715
train_iter_loss: 0.47534096240997314
train_iter_loss: 0.426279217004776
train_iter_loss: 0.134995236992836
train_iter_loss: 0.08553820848464966
train_iter_loss: 0.3088243901729584
train_iter_loss: 0.1880292147397995
train_iter_loss: 0.28123176097869873
train_iter_loss: 0.1200856864452362
train_iter_loss: 0.24694935977458954
train_iter_loss: 0.21153469383716583
train_iter_loss: 0.18396319448947906
train_iter_loss: 0.12383664399385452
train_iter_loss: 0.4932454824447632
train_iter_loss: 0.2540563941001892
train_iter_loss: 0.20289835333824158
train_iter_loss: 0.3445218801498413
train_iter_loss: 0.17016763985157013
train_iter_loss: 0.21521738171577454
train_iter_loss: 0.16940312087535858
train_iter_loss: 0.23794811964035034
train_iter_loss: 0.3153960108757019
train_iter_loss: 0.2002914994955063
train_iter_loss: 0.2971588373184204
train_iter_loss: 0.24345523118972778
train_iter_loss: 0.42493659257888794
train_iter_loss: 0.17086130380630493
train_iter_loss: 0.2586539685726166
train_iter_loss: 0.1952020674943924
train_iter_loss: 0.19632172584533691
train_iter_loss: 0.1278878003358841
train_iter_loss: 0.27104347944259644
train_iter_loss: 0.3475973606109619
train_iter_loss: 0.3118043839931488
train_iter_loss: 0.3050472140312195
train_iter_loss: 0.2102777510881424
train_iter_loss: 0.3559286296367645
train_iter_loss: 0.3075679540634155
train_iter_loss: 0.32154524326324463
train_iter_loss: 0.22854261100292206
train_iter_loss: 0.07827643305063248
train_iter_loss: 0.33662474155426025
train_iter_loss: 0.09693824499845505
train_iter_loss: 0.23165246844291687
train_iter_loss: 0.2929879426956177
train_iter_loss: 0.28239694237709045
train_iter_loss: 0.17031678557395935
train_iter_loss: 0.3279433846473694
train_iter_loss: 0.34174853563308716
train_iter_loss: 0.328754186630249
train_iter_loss: 0.3467724323272705
train_iter_loss: 0.11257472634315491
train_iter_loss: 0.1900884360074997
train_iter_loss: 0.20324921607971191
train_iter_loss: 0.3838590085506439
train_iter_loss: 0.17541101574897766
train_iter_loss: 0.2199842631816864
train_iter_loss: 0.27269989252090454
train_iter_loss: 0.29887789487838745
train_iter_loss: 0.2691119611263275
train_iter_loss: 0.3839625120162964
train_iter_loss: 0.1428447663784027
train_iter_loss: 0.22404468059539795
train_iter_loss: 0.1926046758890152
train_iter_loss: 0.41822415590286255
train_iter_loss: 0.36737680435180664
train_iter_loss: 0.20053035020828247
train_iter_loss: 0.29625871777534485
train_iter_loss: 0.24098287522792816
train_iter_loss: 0.3182741105556488
train_iter_loss: 0.20071397721767426
train_iter_loss: 0.26539820432662964
train_iter_loss: 0.2697350084781647
train_iter_loss: 0.6155024766921997
train_iter_loss: 0.13987259566783905
train_iter_loss: 0.14834634959697723
train_iter_loss: 0.2898062467575073
train_iter_loss: 0.29533156752586365
train_iter_loss: 0.384138286113739
train_iter_loss: 0.21780678629875183
train_iter_loss: 0.2512859106063843
train_iter_loss: 0.19636094570159912
train_iter_loss: 0.19534435868263245
train_iter_loss: 0.4200787842273712
train_iter_loss: 0.16701140999794006
train_iter_loss: 0.1427154839038849
train_iter_loss: 0.44734933972358704
train_iter_loss: 0.2580682933330536
train_iter_loss: 0.3281727731227875
train_iter_loss: 0.13891591131687164
train_iter_loss: 0.19812174141407013
train_iter_loss: 0.23095963895320892
train_iter_loss: 0.4398796856403351
train_iter_loss: 0.4283064901828766
train_iter_loss: 0.36199507117271423
train_iter_loss: 0.2930258512496948
train_iter_loss: 0.30250632762908936
train loss :0.2668
---------------------
Validation seg loss: 0.36764707805518554 at epoch 449
epoch =    450/  1000, exp = train
train_iter_loss: 0.32737407088279724
train_iter_loss: 0.24496929347515106
train_iter_loss: 0.3360242545604706
train_iter_loss: 0.24129940569400787
train_iter_loss: 0.2874102294445038
train_iter_loss: 0.2478269636631012
train_iter_loss: 0.29061928391456604
train_iter_loss: 0.24936163425445557
train_iter_loss: 0.24885301291942596
train_iter_loss: 0.20174147188663483
train_iter_loss: 0.1854141801595688
train_iter_loss: 0.15327563881874084
train_iter_loss: 0.20361176133155823
train_iter_loss: 0.3551817536354065
train_iter_loss: 0.18193258345127106
train_iter_loss: 0.2531281113624573
train_iter_loss: 0.2372368723154068
train_iter_loss: 0.33746138215065
train_iter_loss: 0.18687167763710022
train_iter_loss: 0.3906317949295044
train_iter_loss: 0.2911800146102905
train_iter_loss: 0.4733918607234955
train_iter_loss: 0.3284012973308563
train_iter_loss: 0.16760534048080444
train_iter_loss: 0.4526260495185852
train_iter_loss: 0.21898329257965088
train_iter_loss: 0.2431599646806717
train_iter_loss: 0.27512112259864807
train_iter_loss: 0.2219710350036621
train_iter_loss: 0.2798837423324585
train_iter_loss: 0.33928045630455017
train_iter_loss: 0.3096071481704712
train_iter_loss: 0.2259092777967453
train_iter_loss: 0.3292035460472107
train_iter_loss: 0.2535206079483032
train_iter_loss: 0.2833048403263092
train_iter_loss: 0.2649002969264984
train_iter_loss: 0.3267834782600403
train_iter_loss: 0.38512495160102844
train_iter_loss: 0.2583215832710266
train_iter_loss: 0.21850860118865967
train_iter_loss: 0.1391637772321701
train_iter_loss: 0.5452397465705872
train_iter_loss: 0.14159640669822693
train_iter_loss: 0.35368654131889343
train_iter_loss: 0.21507446467876434
train_iter_loss: 0.2832372486591339
train_iter_loss: 0.21762531995773315
train_iter_loss: 0.1591568887233734
train_iter_loss: 0.4058181941509247
train_iter_loss: 0.2849366068840027
train_iter_loss: 0.12962733209133148
train_iter_loss: 0.1791873574256897
train_iter_loss: 0.25704512000083923
train_iter_loss: 0.2942257523536682
train_iter_loss: 0.1683483123779297
train_iter_loss: 0.25366485118865967
train_iter_loss: 0.24247613549232483
train_iter_loss: 0.23385772109031677
train_iter_loss: 0.30984970927238464
train_iter_loss: 0.3753542900085449
train_iter_loss: 0.33571457862854004
train_iter_loss: 0.3219442367553711
train_iter_loss: 0.2756190299987793
train_iter_loss: 0.3009685277938843
train_iter_loss: 0.1945258229970932
train_iter_loss: 0.24513396620750427
train_iter_loss: 0.2837021052837372
train_iter_loss: 0.3090521991252899
train_iter_loss: 0.2672375440597534
train_iter_loss: 0.31540006399154663
train_iter_loss: 0.18009305000305176
train_iter_loss: 0.30593207478523254
train_iter_loss: 0.4629333019256592
train_iter_loss: 0.25624555349349976
train_iter_loss: 0.4258270263671875
train_iter_loss: 0.24129000306129456
train_iter_loss: 0.3973272442817688
train_iter_loss: 0.3153182566165924
train_iter_loss: 0.21055088937282562
train_iter_loss: 0.29722073674201965
train_iter_loss: 0.2616870105266571
train_iter_loss: 0.12857797741889954
train_iter_loss: 0.17599943280220032
train_iter_loss: 0.17163428664207458
train_iter_loss: 0.34176400303840637
train_iter_loss: 0.31791096925735474
train_iter_loss: 0.36910027265548706
train_iter_loss: 0.14583350718021393
train_iter_loss: 0.37306979298591614
train_iter_loss: 0.3769102990627289
train_iter_loss: 0.10692863166332245
train_iter_loss: 0.3279132544994354
train_iter_loss: 0.11907202750444412
train_iter_loss: 0.28667885065078735
train_iter_loss: 0.1729287952184677
train_iter_loss: 0.31546103954315186
train_iter_loss: 0.3924316167831421
train_iter_loss: 0.19948430359363556
train_iter_loss: 0.38394010066986084
train loss :0.2767
---------------------
Validation seg loss: 0.36961013881735644 at epoch 450
epoch =    451/  1000, exp = train
train_iter_loss: 0.15799514949321747
train_iter_loss: 0.19134916365146637
train_iter_loss: 0.3353640139102936
train_iter_loss: 0.22983022034168243
train_iter_loss: 0.3616120517253876
train_iter_loss: 0.3280114233493805
train_iter_loss: 0.3469058871269226
train_iter_loss: 0.30674511194229126
train_iter_loss: 0.20358584821224213
train_iter_loss: 0.30601248145103455
train_iter_loss: 0.15704649686813354
train_iter_loss: 0.2572360932826996
train_iter_loss: 0.3988843560218811
train_iter_loss: 0.29149168729782104
train_iter_loss: 0.2546655535697937
train_iter_loss: 0.3953511416912079
train_iter_loss: 0.3123842477798462
train_iter_loss: 0.28754982352256775
train_iter_loss: 0.24769675731658936
train_iter_loss: 0.3381061851978302
train_iter_loss: 0.18509957194328308
train_iter_loss: 0.2195003181695938
train_iter_loss: 0.1421934962272644
train_iter_loss: 0.2434103786945343
train_iter_loss: 0.13122382760047913
train_iter_loss: 0.13742117583751678
train_iter_loss: 0.37489771842956543
train_iter_loss: 0.24172136187553406
train_iter_loss: 0.08040983974933624
train_iter_loss: 0.372180312871933
train_iter_loss: 0.28805458545684814
train_iter_loss: 0.21106451749801636
train_iter_loss: 0.17458179593086243
train_iter_loss: 0.3035169541835785
train_iter_loss: 0.12009506672620773
train_iter_loss: 0.2606883645057678
train_iter_loss: 0.32403409481048584
train_iter_loss: 0.3397884964942932
train_iter_loss: 0.10292010009288788
train_iter_loss: 0.35672837495803833
train_iter_loss: 0.26516494154930115
train_iter_loss: 0.25102633237838745
train_iter_loss: 0.3543975055217743
train_iter_loss: 0.11112463474273682
train_iter_loss: 0.17699040472507477
train_iter_loss: 0.14583581686019897
train_iter_loss: 0.29904457926750183
train_iter_loss: 0.3618185818195343
train_iter_loss: 0.43265199661254883
train_iter_loss: 0.2987636923789978
train_iter_loss: 0.3696814179420471
train_iter_loss: 0.3448483943939209
train_iter_loss: 0.4263443946838379
train_iter_loss: 0.29004374146461487
train_iter_loss: 0.32014575600624084
train_iter_loss: 0.1784599870443344
train_iter_loss: 0.3608013391494751
train_iter_loss: 0.19583524763584137
train_iter_loss: 0.3381197154521942
train_iter_loss: 0.5744127631187439
train_iter_loss: 0.1779961884021759
train_iter_loss: 0.418122798204422
train_iter_loss: 0.4149700105190277
train_iter_loss: 0.318893700838089
train_iter_loss: 0.27585312724113464
train_iter_loss: 0.4576549828052521
train_iter_loss: 0.23483353853225708
train_iter_loss: 0.31776225566864014
train_iter_loss: 0.29870399832725525
train_iter_loss: 0.3492051959037781
train_iter_loss: 0.1972113698720932
train_iter_loss: 0.17788442969322205
train_iter_loss: 0.20402224361896515
train_iter_loss: 0.28992024064064026
train_iter_loss: 0.3791797161102295
train_iter_loss: 0.1645222008228302
train_iter_loss: 0.3484082520008087
train_iter_loss: 0.2127777338027954
train_iter_loss: 0.4276992380619049
train_iter_loss: 0.3018989562988281
train_iter_loss: 0.40227195620536804
train_iter_loss: 0.2469901591539383
train_iter_loss: 0.3382369875907898
train_iter_loss: 0.237306147813797
train_iter_loss: 0.3052867650985718
train_iter_loss: 0.329062819480896
train_iter_loss: 0.1380874216556549
train_iter_loss: 0.42083290219306946
train_iter_loss: 0.3253541886806488
train_iter_loss: 0.16403749585151672
train_iter_loss: 0.35871797800064087
train_iter_loss: 0.13502514362335205
train_iter_loss: 0.36066922545433044
train_iter_loss: 0.299986332654953
train_iter_loss: 0.31878724694252014
train_iter_loss: 0.16835004091262817
train_iter_loss: 0.10438715666532516
train_iter_loss: 0.2799617350101471
train_iter_loss: 0.29017436504364014
train_iter_loss: 0.24804569780826569
train loss :0.2813
---------------------
Validation seg loss: 0.36342915193230474 at epoch 451
epoch =    452/  1000, exp = train
train_iter_loss: 0.15906153619289398
train_iter_loss: 0.3250597417354584
train_iter_loss: 0.3727949857711792
train_iter_loss: 0.1814814954996109
train_iter_loss: 0.4321921169757843
train_iter_loss: 0.24623510241508484
train_iter_loss: 0.26792579889297485
train_iter_loss: 0.22720934450626373
train_iter_loss: 0.31461864709854126
train_iter_loss: 0.40614962577819824
train_iter_loss: 0.32303348183631897
train_iter_loss: 0.2167998105287552
train_iter_loss: 0.19550268352031708
train_iter_loss: 0.2351665496826172
train_iter_loss: 0.22003406286239624
train_iter_loss: 0.16779394447803497
train_iter_loss: 0.34561771154403687
train_iter_loss: 0.2165021449327469
train_iter_loss: 0.6360079050064087
train_iter_loss: 0.1337079256772995
train_iter_loss: 0.25816836953163147
train_iter_loss: 0.30389246344566345
train_iter_loss: 0.39966118335723877
train_iter_loss: 0.3517320156097412
train_iter_loss: 0.2204548716545105
train_iter_loss: 0.2950892448425293
train_iter_loss: 0.25116395950317383
train_iter_loss: 0.25174328684806824
train_iter_loss: 0.2863767445087433
train_iter_loss: 0.35909804701805115
train_iter_loss: 0.24266181886196136
train_iter_loss: 0.23042871057987213
train_iter_loss: 0.2472577542066574
train_iter_loss: 0.1223970428109169
train_iter_loss: 0.26737335324287415
train_iter_loss: 0.3185269236564636
train_iter_loss: 0.18073385953903198
train_iter_loss: 0.27775171399116516
train_iter_loss: 0.37842702865600586
train_iter_loss: 0.3014502227306366
train_iter_loss: 0.2923493981361389
train_iter_loss: 0.18219400942325592
train_iter_loss: 0.24993638694286346
train_iter_loss: 0.20361365377902985
train_iter_loss: 0.3791303038597107
train_iter_loss: 0.1800539791584015
train_iter_loss: 0.28079962730407715
train_iter_loss: 0.3351115584373474
train_iter_loss: 0.4040991961956024
train_iter_loss: 0.1314387321472168
train_iter_loss: 0.3339974880218506
train_iter_loss: 0.39305636286735535
train_iter_loss: 0.38628703355789185
train_iter_loss: 0.285009503364563
train_iter_loss: 0.378434419631958
train_iter_loss: 0.22186698019504547
train_iter_loss: 0.16882012784481049
train_iter_loss: 0.3609393239021301
train_iter_loss: 0.20183509588241577
train_iter_loss: 0.17102567851543427
train_iter_loss: 0.2189406454563141
train_iter_loss: 0.24028930068016052
train_iter_loss: 0.31822770833969116
train_iter_loss: 0.21786299347877502
train_iter_loss: 0.2228255271911621
train_iter_loss: 0.5368560552597046
train_iter_loss: 0.27523839473724365
train_iter_loss: 0.34593555331230164
train_iter_loss: 0.3306604325771332
train_iter_loss: 0.2237229347229004
train_iter_loss: 0.39326030015945435
train_iter_loss: 0.38283273577690125
train_iter_loss: 0.12920883297920227
train_iter_loss: 0.38057345151901245
train_iter_loss: 0.3196967542171478
train_iter_loss: 0.25614503026008606
train_iter_loss: 0.12349440902471542
train_iter_loss: 0.13085287809371948
train_iter_loss: 0.1933479756116867
train_iter_loss: 0.20399656891822815
train_iter_loss: 0.3152937591075897
train_iter_loss: 0.12290193885564804
train_iter_loss: 0.3516850173473358
train_iter_loss: 0.39847809076309204
train_iter_loss: 0.2981121242046356
train_iter_loss: 0.18026995658874512
train_iter_loss: 0.24771206080913544
train_iter_loss: 0.3204255700111389
train_iter_loss: 0.27905184030532837
train_iter_loss: 0.37402185797691345
train_iter_loss: 0.24490778148174286
train_iter_loss: 0.2844708561897278
train_iter_loss: 0.2617880702018738
train_iter_loss: 0.3081054985523224
train_iter_loss: 0.15580792725086212
train_iter_loss: 0.22709991037845612
train_iter_loss: 0.2210133820772171
train_iter_loss: 0.26094934344291687
train_iter_loss: 0.26289159059524536
train_iter_loss: 0.48985570669174194
train loss :0.2804
---------------------
Validation seg loss: 0.3698695518234569 at epoch 452
epoch =    453/  1000, exp = train
train_iter_loss: 0.2670840322971344
train_iter_loss: 0.22279272973537445
train_iter_loss: 0.2676223814487457
train_iter_loss: 0.36671414971351624
train_iter_loss: 0.19341495633125305
train_iter_loss: 0.44562387466430664
train_iter_loss: 0.3747541308403015
train_iter_loss: 0.37863889336586
train_iter_loss: 0.30410370230674744
train_iter_loss: 0.19834043085575104
train_iter_loss: 0.3275798261165619
train_iter_loss: 0.4513506293296814
train_iter_loss: 0.22298316657543182
train_iter_loss: 0.25512224435806274
train_iter_loss: 0.25669407844543457
train_iter_loss: 0.20518428087234497
train_iter_loss: 0.2544902563095093
train_iter_loss: 0.3278697431087494
train_iter_loss: 0.21128755807876587
train_iter_loss: 0.2440699189901352
train_iter_loss: 0.14910940825939178
train_iter_loss: 0.363298237323761
train_iter_loss: 0.3943750262260437
train_iter_loss: 0.1424040049314499
train_iter_loss: 0.04802192747592926
train_iter_loss: 0.08663159608840942
train_iter_loss: 0.3427557647228241
train_iter_loss: 0.3672800064086914
train_iter_loss: 0.36099106073379517
train_iter_loss: 0.3144470155239105
train_iter_loss: 0.32961079478263855
train_iter_loss: 0.2683195173740387
train_iter_loss: 0.28675976395606995
train_iter_loss: 0.23821957409381866
train_iter_loss: 0.38977527618408203
train_iter_loss: 0.2312880903482437
train_iter_loss: 0.1878979355096817
train_iter_loss: 0.07787768542766571
train_iter_loss: 0.2642183005809784
train_iter_loss: 0.17390084266662598
train_iter_loss: 0.41542142629623413
train_iter_loss: 0.25964802503585815
train_iter_loss: 0.23936155438423157
train_iter_loss: 0.28904202580451965
train_iter_loss: 0.2748558819293976
train_iter_loss: 0.3196078836917877
train_iter_loss: 0.4119711220264435
train_iter_loss: 0.34406495094299316
train_iter_loss: 0.33294323086738586
train_iter_loss: 0.27428361773490906
train_iter_loss: 0.20636188983917236
train_iter_loss: 0.2896623909473419
train_iter_loss: 0.28896090388298035
train_iter_loss: 0.30729034543037415
train_iter_loss: 0.3543168902397156
train_iter_loss: 0.2899942398071289
train_iter_loss: 0.3313644230365753
train_iter_loss: 0.3234781324863434
train_iter_loss: 0.2533113360404968
train_iter_loss: 0.23701505362987518
train_iter_loss: 0.1816999614238739
train_iter_loss: 0.3500218391418457
train_iter_loss: 0.2512350380420685
train_iter_loss: 0.28910738229751587
train_iter_loss: 0.1725875586271286
train_iter_loss: 0.26032182574272156
train_iter_loss: 0.2505243122577667
train_iter_loss: 0.3780220150947571
train_iter_loss: 0.18258531391620636
train_iter_loss: 0.20289547741413116
train_iter_loss: 0.13402824103832245
train_iter_loss: 0.21790745854377747
train_iter_loss: 0.25134560465812683
train_iter_loss: 0.16790355741977692
train_iter_loss: 0.44191575050354004
train_iter_loss: 0.2568519115447998
train_iter_loss: 0.2253619283437729
train_iter_loss: 0.3682575821876526
train_iter_loss: 0.3129003345966339
train_iter_loss: 0.1900349259376526
train_iter_loss: 0.0432860367000103
train_iter_loss: 0.15165559947490692
train_iter_loss: 0.19542433321475983
train_iter_loss: 0.40750616788864136
train_iter_loss: 0.15847215056419373
train_iter_loss: 0.28883060812950134
train_iter_loss: 0.23972508311271667
train_iter_loss: 0.33415043354034424
train_iter_loss: 0.2855541706085205
train_iter_loss: 0.22428612411022186
train_iter_loss: 0.2770991027355194
train_iter_loss: 0.17888537049293518
train_iter_loss: 0.2586210370063782
train_iter_loss: 0.12834066152572632
train_iter_loss: 0.2270587533712387
train_iter_loss: 0.19403548538684845
train_iter_loss: 0.27529630064964294
train_iter_loss: 0.2032926082611084
train_iter_loss: 0.38075390458106995
train_iter_loss: 0.28344154357910156
train loss :0.2694
---------------------
Validation seg loss: 0.3924632389361988 at epoch 453
epoch =    454/  1000, exp = train
train_iter_loss: 0.40292736887931824
train_iter_loss: 0.212640181183815
train_iter_loss: 0.24689564108848572
train_iter_loss: 0.3715555965900421
train_iter_loss: 0.13299398124217987
train_iter_loss: 0.23415888845920563
train_iter_loss: 0.2329368144273758
train_iter_loss: 0.25807255506515503
train_iter_loss: 0.12417112290859222
train_iter_loss: 0.269866943359375
train_iter_loss: 0.350810170173645
train_iter_loss: 0.2003016620874405
train_iter_loss: 0.2682417929172516
train_iter_loss: 0.3320120871067047
train_iter_loss: 0.29984328150749207
train_iter_loss: 0.33365511894226074
train_iter_loss: 0.3462451994419098
train_iter_loss: 0.39295366406440735
train_iter_loss: 0.22717319428920746
train_iter_loss: 0.05654248222708702
train_iter_loss: 0.3737827241420746
train_iter_loss: 0.1540479063987732
train_iter_loss: 0.2705536186695099
train_iter_loss: 0.27629968523979187
train_iter_loss: 0.3264525830745697
train_iter_loss: 0.4287794232368469
train_iter_loss: 0.381035178899765
train_iter_loss: 0.3209376633167267
train_iter_loss: 0.330200731754303
train_iter_loss: 0.3414236307144165
train_iter_loss: 0.28067538142204285
train_iter_loss: 0.23092928528785706
train_iter_loss: 0.19708646833896637
train_iter_loss: 0.24697467684745789
train_iter_loss: 0.3438737392425537
train_iter_loss: 0.24444127082824707
train_iter_loss: 0.19665183126926422
train_iter_loss: 0.20115850865840912
train_iter_loss: 0.26941099762916565
train_iter_loss: 0.1015859991312027
train_iter_loss: 0.15828274190425873
train_iter_loss: 0.23926152288913727
train_iter_loss: 0.14586372673511505
train_iter_loss: 0.14906935393810272
train_iter_loss: 0.30108746886253357
train_iter_loss: 0.34159716963768005
train_iter_loss: 0.14947032928466797
train_iter_loss: 0.3562714457511902
train_iter_loss: 0.3110327422618866
train_iter_loss: 0.2337331622838974
train_iter_loss: 0.2957976460456848
train_iter_loss: 0.4000639319419861
train_iter_loss: 0.27736178040504456
train_iter_loss: 0.21296238899230957
train_iter_loss: 0.45717892050743103
train_iter_loss: 0.5388134121894836
train_iter_loss: 0.1031709536910057
train_iter_loss: 0.18585717678070068
train_iter_loss: 0.20785562694072723
train_iter_loss: 0.383579820394516
train_iter_loss: 0.23708291351795197
train_iter_loss: 0.18523529171943665
train_iter_loss: 0.3701438307762146
train_iter_loss: 0.33601802587509155
train_iter_loss: 0.26853933930397034
train_iter_loss: 0.32629233598709106
train_iter_loss: 0.1393604278564453
train_iter_loss: 0.2156476527452469
train_iter_loss: 0.271165132522583
train_iter_loss: 0.1836559921503067
train_iter_loss: 0.33509063720703125
train_iter_loss: 0.2846498489379883
train_iter_loss: 0.3598268926143646
train_iter_loss: 0.2528690695762634
train_iter_loss: 0.24627278745174408
train_iter_loss: 0.30218735337257385
train_iter_loss: 0.22661146521568298
train_iter_loss: 0.2503741383552551
train_iter_loss: 0.44248735904693604
train_iter_loss: 0.25340479612350464
train_iter_loss: 0.32328617572784424
train_iter_loss: 0.19055069983005524
train_iter_loss: 0.24407704174518585
train_iter_loss: 0.40572378039360046
train_iter_loss: 0.28238052129745483
train_iter_loss: 0.4484161138534546
train_iter_loss: 0.2668676972389221
train_iter_loss: 0.2453635185956955
train_iter_loss: 0.30967193841934204
train_iter_loss: 0.2961080074310303
train_iter_loss: 0.21373361349105835
train_iter_loss: 0.2574159801006317
train_iter_loss: 0.3450677692890167
train_iter_loss: 0.27857640385627747
train_iter_loss: 0.16218824684619904
train_iter_loss: 0.27580758929252625
train_iter_loss: 0.3727663457393646
train_iter_loss: 0.22534692287445068
train_iter_loss: 0.21778717637062073
train_iter_loss: 0.09501369297504425
train loss :0.2747
---------------------
Validation seg loss: 0.4128273361195582 at epoch 454
epoch =    455/  1000, exp = train
train_iter_loss: 0.20653586089611053
train_iter_loss: 0.25150638818740845
train_iter_loss: 0.2600947916507721
train_iter_loss: 0.24076217412948608
train_iter_loss: 0.23460137844085693
train_iter_loss: 0.2714661955833435
train_iter_loss: 0.24812671542167664
train_iter_loss: 0.2909763753414154
train_iter_loss: 0.2964712977409363
train_iter_loss: 0.3254491686820984
train_iter_loss: 0.3035019040107727
train_iter_loss: 0.31886401772499084
train_iter_loss: 0.3634253442287445
train_iter_loss: 0.1838587522506714
train_iter_loss: 0.31831398606300354
train_iter_loss: 0.28752633929252625
train_iter_loss: 0.423816055059433
train_iter_loss: 0.2644632160663605
train_iter_loss: 0.2880691587924957
train_iter_loss: 0.2053004801273346
train_iter_loss: 0.18627582490444183
train_iter_loss: 0.38893699645996094
train_iter_loss: 0.15138648450374603
train_iter_loss: 0.21908901631832123
train_iter_loss: 0.26193323731422424
train_iter_loss: 0.46270522475242615
train_iter_loss: 0.2699938714504242
train_iter_loss: 0.28913962841033936
train_iter_loss: 0.26039424538612366
train_iter_loss: 0.14978688955307007
train_iter_loss: 0.4172145128250122
train_iter_loss: 0.3348942995071411
train_iter_loss: 0.2220616638660431
train_iter_loss: 0.5710780024528503
train_iter_loss: 0.38986194133758545
train_iter_loss: 0.28926312923431396
train_iter_loss: 0.2571822702884674
train_iter_loss: 0.23266294598579407
train_iter_loss: 0.3384723663330078
train_iter_loss: 0.35603225231170654
train_iter_loss: 0.27835074067115784
train_iter_loss: 0.2395840734243393
train_iter_loss: 0.29441505670547485
train_iter_loss: 0.23754248023033142
train_iter_loss: 0.21262049674987793
train_iter_loss: 0.23593097925186157
train_iter_loss: 0.173263818025589
train_iter_loss: 0.26992079615592957
train_iter_loss: 0.23148098587989807
train_iter_loss: 0.3025945723056793
train_iter_loss: 0.38408219814300537
train_iter_loss: 0.40476953983306885
train_iter_loss: 0.2653212249279022
train_iter_loss: 0.3381882905960083
train_iter_loss: 0.3547455370426178
train_iter_loss: 0.1042616218328476
train_iter_loss: 0.35078251361846924
train_iter_loss: 0.33073657751083374
train_iter_loss: 0.3155979514122009
train_iter_loss: 0.17081418633460999
train_iter_loss: 0.3130023777484894
train_iter_loss: 0.13340811431407928
train_iter_loss: 0.254199355840683
train_iter_loss: 0.2331458479166031
train_iter_loss: 0.20515844225883484
train_iter_loss: 0.21472874283790588
train_iter_loss: 0.20560503005981445
train_iter_loss: 0.29384610056877136
train_iter_loss: 0.28027966618537903
train_iter_loss: 0.16015051305294037
train_iter_loss: 0.2693934142589569
train_iter_loss: 0.27828195691108704
train_iter_loss: 0.34850752353668213
train_iter_loss: 0.4102282226085663
train_iter_loss: 0.12863540649414062
train_iter_loss: 0.3593961000442505
train_iter_loss: 0.04951348528265953
train_iter_loss: 0.2285519242286682
train_iter_loss: 0.38801172375679016
train_iter_loss: 0.21596680581569672
train_iter_loss: 0.3174957036972046
train_iter_loss: 0.24780748784542084
train_iter_loss: 0.18322430551052094
train_iter_loss: 0.2378169447183609
train_iter_loss: 0.1972435861825943
train_iter_loss: 0.1847008466720581
train_iter_loss: 0.16228429973125458
train_iter_loss: 0.1899433583021164
train_iter_loss: 0.19571104645729065
train_iter_loss: 0.27545034885406494
train_iter_loss: 0.20543955266475677
train_iter_loss: 0.27611464262008667
train_iter_loss: 0.2713405191898346
train_iter_loss: 0.1525186151266098
train_iter_loss: 0.3514309525489807
train_iter_loss: 0.22790487110614777
train_iter_loss: 0.2657749056816101
train_iter_loss: 0.1194678544998169
train_iter_loss: 0.2555502951145172
train_iter_loss: 0.30723607540130615
train loss :0.2700
---------------------
Validation seg loss: 0.37628267630758994 at epoch 455
epoch =    456/  1000, exp = train
train_iter_loss: 0.3092593848705292
train_iter_loss: 0.28114622831344604
train_iter_loss: 0.1854608654975891
train_iter_loss: 0.20544297993183136
train_iter_loss: 0.3407081961631775
train_iter_loss: 0.20252591371536255
train_iter_loss: 0.2060861736536026
train_iter_loss: 0.22784602642059326
train_iter_loss: 0.24034211039543152
train_iter_loss: 0.1703384816646576
train_iter_loss: 0.2599813640117645
train_iter_loss: 0.3002513349056244
train_iter_loss: 0.307327002286911
train_iter_loss: 0.36648741364479065
train_iter_loss: 0.1830136477947235
train_iter_loss: 0.27545008063316345
train_iter_loss: 0.4270915687084198
train_iter_loss: 0.39132532477378845
train_iter_loss: 0.33662736415863037
train_iter_loss: 0.12516650557518005
train_iter_loss: 0.2784077525138855
train_iter_loss: 0.28007879853248596
train_iter_loss: 0.19385108351707458
train_iter_loss: 0.5081889629364014
train_iter_loss: 0.28356850147247314
train_iter_loss: 0.3148452937602997
train_iter_loss: 0.22975407540798187
train_iter_loss: 0.20130765438079834
train_iter_loss: 0.16911837458610535
train_iter_loss: 0.32179200649261475
train_iter_loss: 0.2672886848449707
train_iter_loss: 0.24704678356647491
train_iter_loss: 0.24923211336135864
train_iter_loss: 0.26084572076797485
train_iter_loss: 0.20399096608161926
train_iter_loss: 0.34411415457725525
train_iter_loss: 0.030973410233855247
train_iter_loss: 0.25186941027641296
train_iter_loss: 0.23053547739982605
train_iter_loss: 0.3414115607738495
train_iter_loss: 0.24104556441307068
train_iter_loss: 0.19681036472320557
train_iter_loss: 0.30001360177993774
train_iter_loss: 0.21894937753677368
train_iter_loss: 0.2872088551521301
train_iter_loss: 0.2284555733203888
train_iter_loss: 0.2909191846847534
train_iter_loss: 0.18309591710567474
train_iter_loss: 0.12798377871513367
train_iter_loss: 0.1912386566400528
train_iter_loss: 0.1875041276216507
train_iter_loss: 0.265072226524353
train_iter_loss: 0.19700737297534943
train_iter_loss: 0.3967415392398834
train_iter_loss: 0.29142677783966064
train_iter_loss: 0.22652770578861237
train_iter_loss: 0.28385940194129944
train_iter_loss: 0.28022435307502747
train_iter_loss: 0.2628990113735199
train_iter_loss: 0.14282575249671936
train_iter_loss: 0.2260933816432953
train_iter_loss: 0.09803679585456848
train_iter_loss: 0.3138059079647064
train_iter_loss: 0.23011237382888794
train_iter_loss: 0.3123725354671478
train_iter_loss: 0.29336827993392944
train_iter_loss: 0.19209979474544525
train_iter_loss: 0.32656049728393555
train_iter_loss: 0.7832644581794739
train_iter_loss: 0.23243454098701477
train_iter_loss: 0.4533037543296814
train_iter_loss: 0.24759025871753693
train_iter_loss: 0.14540760219097137
train_iter_loss: 0.26663821935653687
train_iter_loss: 0.1962437480688095
train_iter_loss: 0.3304925262928009
train_iter_loss: 0.24755893647670746
train_iter_loss: 0.29660627245903015
train_iter_loss: 0.37085452675819397
train_iter_loss: 0.2783215641975403
train_iter_loss: 0.2986774742603302
train_iter_loss: 0.2951134443283081
train_iter_loss: 0.2765826880931854
train_iter_loss: 0.2601267695426941
train_iter_loss: 0.2476942092180252
train_iter_loss: 0.3778751492500305
train_iter_loss: 0.178229421377182
train_iter_loss: 0.3035052716732025
train_iter_loss: 0.3820193111896515
train_iter_loss: 0.33946940302848816
train_iter_loss: 0.227303609251976
train_iter_loss: 0.05879225954413414
train_iter_loss: 0.2551582455635071
train_iter_loss: 0.27073878049850464
train_iter_loss: 0.1463533341884613
train_iter_loss: 0.2462625503540039
train_iter_loss: 0.22251349687576294
train_iter_loss: 0.3737823963165283
train_iter_loss: 0.2790798544883728
train_iter_loss: 0.2810598313808441
train loss :0.2678
---------------------
Validation seg loss: 0.3554330583714511 at epoch 456
epoch =    457/  1000, exp = train
train_iter_loss: 0.17783331871032715
train_iter_loss: 0.3402441442012787
train_iter_loss: 0.16175490617752075
train_iter_loss: 0.20789162814617157
train_iter_loss: 0.25311151146888733
train_iter_loss: 0.21254512667655945
train_iter_loss: 0.10251306742429733
train_iter_loss: 0.18994180858135223
train_iter_loss: 0.1597142070531845
train_iter_loss: 0.4515407979488373
train_iter_loss: 0.14521844685077667
train_iter_loss: 0.23297448456287384
train_iter_loss: 0.22011294960975647
train_iter_loss: 0.24734047055244446
train_iter_loss: 0.2778206169605255
train_iter_loss: 0.3025267720222473
train_iter_loss: 0.2763734459877014
train_iter_loss: 0.19565580785274506
train_iter_loss: 0.38540810346603394
train_iter_loss: 0.22832471132278442
train_iter_loss: 0.34335413575172424
train_iter_loss: 0.37703046202659607
train_iter_loss: 0.2574528455734253
train_iter_loss: 0.2912066876888275
train_iter_loss: 0.2085840106010437
train_iter_loss: 0.23045596480369568
train_iter_loss: 0.34855446219444275
train_iter_loss: 0.26745688915252686
train_iter_loss: 0.2309015989303589
train_iter_loss: 0.2883446216583252
train_iter_loss: 0.34385862946510315
train_iter_loss: 0.4011560380458832
train_iter_loss: 0.08407018333673477
train_iter_loss: 0.27082201838493347
train_iter_loss: 0.2843427360057831
train_iter_loss: 0.1454509049654007
train_iter_loss: 0.35527992248535156
train_iter_loss: 0.20160408318042755
train_iter_loss: 0.1638512760400772
train_iter_loss: 0.23401451110839844
train_iter_loss: 0.32318931818008423
train_iter_loss: 0.35263898968696594
train_iter_loss: 0.23768927156925201
train_iter_loss: 0.2599519193172455
train_iter_loss: 0.2830321490764618
train_iter_loss: 0.24248486757278442
train_iter_loss: 0.2727260887622833
train_iter_loss: 0.32642805576324463
train_iter_loss: 0.3985503315925598
train_iter_loss: 0.36028870940208435
train_iter_loss: 0.47835177183151245
train_iter_loss: 0.27843931317329407
train_iter_loss: 0.1588546633720398
train_iter_loss: 0.2659621834754944
train_iter_loss: 0.26490628719329834
train_iter_loss: 0.23177354037761688
train_iter_loss: 0.2850579619407654
train_iter_loss: 0.2166583389043808
train_iter_loss: 0.2854214310646057
train_iter_loss: 0.24492235481739044
train_iter_loss: 0.27616414427757263
train_iter_loss: 0.22834719717502594
train_iter_loss: 0.2722071707248688
train_iter_loss: 0.2790396213531494
train_iter_loss: 0.2436826378107071
train_iter_loss: 0.2558649778366089
train_iter_loss: 0.13590027391910553
train_iter_loss: 0.20804482698440552
train_iter_loss: 0.266777366399765
train_iter_loss: 0.2560548484325409
train_iter_loss: 0.23856863379478455
train_iter_loss: 0.352529913187027
train_iter_loss: 0.32642629742622375
train_iter_loss: 0.3772011697292328
train_iter_loss: 0.30904465913772583
train_iter_loss: 0.3615383505821228
train_iter_loss: 0.2833230495452881
train_iter_loss: 0.18510472774505615
train_iter_loss: 0.293678343296051
train_iter_loss: 0.3372070789337158
train_iter_loss: 0.2501790523529053
train_iter_loss: 0.23992207646369934
train_iter_loss: 0.3541826009750366
train_iter_loss: 0.3595431447029114
train_iter_loss: 0.3059670925140381
train_iter_loss: 0.2686244547367096
train_iter_loss: 0.3005082905292511
train_iter_loss: 0.22285909950733185
train_iter_loss: 0.25102612376213074
train_iter_loss: 0.3127553462982178
train_iter_loss: 0.1846284419298172
train_iter_loss: 0.26851311326026917
train_iter_loss: 0.17738695442676544
train_iter_loss: 0.12207943201065063
train_iter_loss: 0.2836451232433319
train_iter_loss: 0.17350326478481293
train_iter_loss: 0.11527325212955475
train_iter_loss: 0.27958065271377563
train_iter_loss: 0.5158799886703491
train_iter_loss: 0.18909664452075958
train loss :0.2679
---------------------
Validation seg loss: 0.36784858992850444 at epoch 457
epoch =    458/  1000, exp = train
train_iter_loss: 0.1778487116098404
train_iter_loss: 0.27875879406929016
train_iter_loss: 0.2980697751045227
train_iter_loss: 0.32150810956954956
train_iter_loss: 0.28256091475486755
train_iter_loss: 0.28215235471725464
train_iter_loss: 0.2348448634147644
train_iter_loss: 0.3956698179244995
train_iter_loss: 0.3757629990577698
train_iter_loss: 0.20104792714118958
train_iter_loss: 0.14629800617694855
train_iter_loss: 0.3180158734321594
train_iter_loss: 0.12552155554294586
train_iter_loss: 0.252595454454422
train_iter_loss: 0.18458309769630432
train_iter_loss: 0.297924280166626
train_iter_loss: 0.2688520550727844
train_iter_loss: 0.22080782055854797
train_iter_loss: 0.1398959755897522
train_iter_loss: 0.19506150484085083
train_iter_loss: 0.30719539523124695
train_iter_loss: 0.24048517644405365
train_iter_loss: 0.2833970785140991
train_iter_loss: 0.3370043635368347
train_iter_loss: 0.2797517478466034
train_iter_loss: 0.22093479335308075
train_iter_loss: 0.5540281534194946
train_iter_loss: 0.29281723499298096
train_iter_loss: 0.27376821637153625
train_iter_loss: 0.45854830741882324
train_iter_loss: 0.23643633723258972
train_iter_loss: 0.36950477957725525
train_iter_loss: 0.2346901148557663
train_iter_loss: 0.31676778197288513
train_iter_loss: 0.24516300857067108
train_iter_loss: 0.42370182275772095
train_iter_loss: 0.23830440640449524
train_iter_loss: 0.3760579526424408
train_iter_loss: 0.25901514291763306
train_iter_loss: 0.3239189386367798
train_iter_loss: 0.10920267552137375
train_iter_loss: 0.2955816984176636
train_iter_loss: 0.1572762429714203
train_iter_loss: 0.22905851900577545
train_iter_loss: 0.27782729268074036
train_iter_loss: 0.3107011318206787
train_iter_loss: 0.3288068473339081
train_iter_loss: 0.33803799748420715
train_iter_loss: 0.2834766209125519
train_iter_loss: 0.1692458838224411
train_iter_loss: 0.2740308940410614
train_iter_loss: 0.36054596304893494
train_iter_loss: 0.18809804320335388
train_iter_loss: 0.17489315569400787
train_iter_loss: 0.3474535644054413
train_iter_loss: 0.22855250537395477
train_iter_loss: 0.3253343999385834
train_iter_loss: 0.2432902753353119
train_iter_loss: 0.15724195539951324
train_iter_loss: 0.3777996599674225
train_iter_loss: 0.2470657080411911
train_iter_loss: 0.3717459440231323
train_iter_loss: 0.1568772941827774
train_iter_loss: 0.48854514956474304
train_iter_loss: 0.4702475965023041
train_iter_loss: 0.27453163266181946
train_iter_loss: 0.2116026133298874
train_iter_loss: 0.13434027135372162
train_iter_loss: 0.33046868443489075
train_iter_loss: 0.22853823006153107
train_iter_loss: 0.2294992357492447
train_iter_loss: 0.1139225959777832
train_iter_loss: 0.2193794548511505
train_iter_loss: 0.40279847383499146
train_iter_loss: 0.2705022990703583
train_iter_loss: 0.4065935015678406
train_iter_loss: 0.4551529288291931
train_iter_loss: 0.37682971358299255
train_iter_loss: 0.2737749516963959
train_iter_loss: 0.3134680986404419
train_iter_loss: 0.23084406554698944
train_iter_loss: 0.17853625118732452
train_iter_loss: 0.3481198251247406
train_iter_loss: 0.20401565730571747
train_iter_loss: 0.22416555881500244
train_iter_loss: 0.30683276057243347
train_iter_loss: 0.2490132749080658
train_iter_loss: 0.25343891978263855
train_iter_loss: 0.3563070297241211
train_iter_loss: 0.26934146881103516
train_iter_loss: 0.07745685428380966
train_iter_loss: 0.24815477430820465
train_iter_loss: 0.23530170321464539
train_iter_loss: 0.4137987196445465
train_iter_loss: 0.1264929622411728
train_iter_loss: 0.08259118348360062
train_iter_loss: 0.22543616592884064
train_iter_loss: 0.27988559007644653
train_iter_loss: 0.2914813458919525
train_iter_loss: 0.33946701884269714
train loss :0.2763
---------------------
Validation seg loss: 0.4157030184858672 at epoch 458
epoch =    459/  1000, exp = train
train_iter_loss: 0.23725855350494385
train_iter_loss: 0.1277102380990982
train_iter_loss: 0.33250486850738525
train_iter_loss: 0.281025230884552
train_iter_loss: 0.39574134349823
train_iter_loss: 0.4171854853630066
train_iter_loss: 0.3457717001438141
train_iter_loss: 0.17734761536121368
train_iter_loss: 0.25902432203292847
train_iter_loss: 0.28541862964630127
train_iter_loss: 0.20449432730674744
train_iter_loss: 0.3638697862625122
train_iter_loss: 0.23935995995998383
train_iter_loss: 0.23000772297382355
train_iter_loss: 0.17037132382392883
train_iter_loss: 0.3507976830005646
train_iter_loss: 0.31321874260902405
train_iter_loss: 0.2768290936946869
train_iter_loss: 0.3332371115684509
train_iter_loss: 0.2596527636051178
train_iter_loss: 0.4279380738735199
train_iter_loss: 0.29011771082878113
train_iter_loss: 0.22045865654945374
train_iter_loss: 0.3421967327594757
train_iter_loss: 0.27172285318374634
train_iter_loss: 0.5393310189247131
train_iter_loss: 0.25347211956977844
train_iter_loss: 0.2381865382194519
train_iter_loss: 0.3104609549045563
train_iter_loss: 0.1822771579027176
train_iter_loss: 0.26053890585899353
train_iter_loss: 0.19853854179382324
train_iter_loss: 0.38074609637260437
train_iter_loss: 0.34064212441444397
train_iter_loss: 0.4462744891643524
train_iter_loss: 0.17597182095050812
train_iter_loss: 0.20369204878807068
train_iter_loss: 0.2134549468755722
train_iter_loss: 0.25097692012786865
train_iter_loss: 0.21127082407474518
train_iter_loss: 0.2445909082889557
train_iter_loss: 0.11286791414022446
train_iter_loss: 0.39100727438926697
train_iter_loss: 0.20553497970104218
train_iter_loss: 0.2997000813484192
train_iter_loss: 0.2855466604232788
train_iter_loss: 0.18986138701438904
train_iter_loss: 0.20033636689186096
train_iter_loss: 0.23832282423973083
train_iter_loss: 0.2886313796043396
train_iter_loss: 0.2230301797389984
train_iter_loss: 0.28351303935050964
train_iter_loss: 0.19920261204242706
train_iter_loss: 0.20834943652153015
train_iter_loss: 0.18040090799331665
train_iter_loss: 0.35779452323913574
train_iter_loss: 0.3865346908569336
train_iter_loss: 0.12187059968709946
train_iter_loss: 0.24751943349838257
train_iter_loss: 0.24372491240501404
train_iter_loss: 0.22883588075637817
train_iter_loss: 0.16510118544101715
train_iter_loss: 0.2985597550868988
train_iter_loss: 0.2319323718547821
train_iter_loss: 0.18965847790241241
train_iter_loss: 0.3831961452960968
train_iter_loss: 0.30100199580192566
train_iter_loss: 0.3150576055049896
train_iter_loss: 0.35798943042755127
train_iter_loss: 0.26442116498947144
train_iter_loss: 0.16901889443397522
train_iter_loss: 0.12643064558506012
train_iter_loss: 0.21773995459079742
train_iter_loss: 0.2674257755279541
train_iter_loss: 0.2743506133556366
train_iter_loss: 0.22239726781845093
train_iter_loss: 0.2803448438644409
train_iter_loss: 0.2334291785955429
train_iter_loss: 0.17253023386001587
train_iter_loss: 0.35951197147369385
train_iter_loss: 0.2577543258666992
train_iter_loss: 0.16370835900306702
train_iter_loss: 0.44524210691452026
train_iter_loss: 0.22268612682819366
train_iter_loss: 0.2553624212741852
train_iter_loss: 0.32031258940696716
train_iter_loss: 0.4710828363895416
train_iter_loss: 0.25746333599090576
train_iter_loss: 0.22437210381031036
train_iter_loss: 0.257715106010437
train_iter_loss: 0.3156898021697998
train_iter_loss: 0.14804987609386444
train_iter_loss: 0.2136421799659729
train_iter_loss: 0.29956480860710144
train_iter_loss: 0.16712045669555664
train_iter_loss: 0.4599606394767761
train_iter_loss: 0.2719559967517853
train_iter_loss: 0.3656711280345917
train_iter_loss: 0.2487923949956894
train_iter_loss: 0.21505174040794373
train loss :0.2718
---------------------
Validation seg loss: 0.3729686335801094 at epoch 459
epoch =    460/  1000, exp = train
train_iter_loss: 0.2532534897327423
train_iter_loss: 0.39160796999931335
train_iter_loss: 0.20886318385601044
train_iter_loss: 0.29072481393814087
train_iter_loss: 0.34936389327049255
train_iter_loss: 0.394040584564209
train_iter_loss: 0.30813324451446533
train_iter_loss: 0.13113681972026825
train_iter_loss: 0.19674330949783325
train_iter_loss: 0.3447642922401428
train_iter_loss: 0.32144254446029663
train_iter_loss: 0.2668379843235016
train_iter_loss: 0.2744056284427643
train_iter_loss: 0.27593207359313965
train_iter_loss: 0.2843519449234009
train_iter_loss: 0.3217136561870575
train_iter_loss: 0.29443997144699097
train_iter_loss: 0.36226707696914673
train_iter_loss: 0.35825175046920776
train_iter_loss: 0.38918671011924744
train_iter_loss: 0.3118107318878174
train_iter_loss: 0.2611074447631836
train_iter_loss: 0.24048417806625366
train_iter_loss: 0.2283567488193512
train_iter_loss: 0.18695688247680664
train_iter_loss: 0.18863026797771454
train_iter_loss: 0.26047393679618835
train_iter_loss: 0.17898434400558472
train_iter_loss: 0.3525542914867401
train_iter_loss: 0.42826852202415466
train_iter_loss: 0.21838504076004028
train_iter_loss: 0.18344800174236298
train_iter_loss: 0.25785568356513977
train_iter_loss: 0.24499426782131195
train_iter_loss: 0.3576371669769287
train_iter_loss: 0.2988569736480713
train_iter_loss: 0.2613847851753235
train_iter_loss: 0.3967702090740204
train_iter_loss: 0.24871623516082764
train_iter_loss: 0.4519624412059784
train_iter_loss: 0.2679321765899658
train_iter_loss: 0.3035883605480194
train_iter_loss: 0.14731188118457794
train_iter_loss: 0.268556147813797
train_iter_loss: 0.3892034590244293
train_iter_loss: 0.18452668190002441
train_iter_loss: 0.17662732303142548
train_iter_loss: 0.38483577966690063
train_iter_loss: 0.33125436305999756
train_iter_loss: 0.18080826103687286
train_iter_loss: 0.21508249640464783
train_iter_loss: 0.3109157979488373
train_iter_loss: 0.26956698298454285
train_iter_loss: 0.3417414128780365
train_iter_loss: 0.44005903601646423
train_iter_loss: 0.28984344005584717
train_iter_loss: 0.2675442695617676
train_iter_loss: 0.24615983664989471
train_iter_loss: 0.18430176377296448
train_iter_loss: 0.0791381299495697
train_iter_loss: 0.21651458740234375
train_iter_loss: 0.17361320555210114
train_iter_loss: 0.23960112035274506
train_iter_loss: 0.2154340296983719
train_iter_loss: 0.2538990080356598
train_iter_loss: 0.30246320366859436
train_iter_loss: 0.18667007982730865
train_iter_loss: 0.3153465688228607
train_iter_loss: 0.31498634815216064
train_iter_loss: 0.27301132678985596
train_iter_loss: 0.32786694169044495
train_iter_loss: 0.3031650185585022
train_iter_loss: 0.19743600487709045
train_iter_loss: 0.3604802191257477
train_iter_loss: 0.33348730206489563
train_iter_loss: 0.22611576318740845
train_iter_loss: 0.3278196156024933
train_iter_loss: 0.3789443373680115
train_iter_loss: 0.17534244060516357
train_iter_loss: 0.11382996290922165
train_iter_loss: 0.24405136704444885
train_iter_loss: 0.207768976688385
train_iter_loss: 0.3091907799243927
train_iter_loss: 0.3139955997467041
train_iter_loss: 0.3376239538192749
train_iter_loss: 0.31071844696998596
train_iter_loss: 0.37718164920806885
train_iter_loss: 0.28266414999961853
train_iter_loss: 0.16648265719413757
train_iter_loss: 0.23326095938682556
train_iter_loss: 0.21416477859020233
train_iter_loss: 0.20244215428829193
train_iter_loss: 0.20431168377399445
train_iter_loss: 0.29886001348495483
train_iter_loss: 0.30837273597717285
train_iter_loss: 0.3450239896774292
train_iter_loss: 0.4776577949523926
train_iter_loss: 0.2875995337963104
train_iter_loss: 0.2583147883415222
train_iter_loss: 0.10023842751979828
train loss :0.2784
---------------------
Validation seg loss: 0.35585313920318234 at epoch 460
epoch =    461/  1000, exp = train
train_iter_loss: 0.44662782549858093
train_iter_loss: 0.46373823285102844
train_iter_loss: 0.31676486134529114
train_iter_loss: 0.3507509231567383
train_iter_loss: 0.338184654712677
train_iter_loss: 0.42083263397216797
train_iter_loss: 0.1758248656988144
train_iter_loss: 0.30183809995651245
train_iter_loss: 0.21496663987636566
train_iter_loss: 0.2933014929294586
train_iter_loss: 0.1606658697128296
train_iter_loss: 0.19151705503463745
train_iter_loss: 0.2784414291381836
train_iter_loss: 0.3911304175853729
train_iter_loss: 0.26143020391464233
train_iter_loss: 0.08266198635101318
train_iter_loss: 0.20794978737831116
train_iter_loss: 0.18810369074344635
train_iter_loss: 0.36942943930625916
train_iter_loss: 0.23470740020275116
train_iter_loss: 0.22564852237701416
train_iter_loss: 0.2900207042694092
train_iter_loss: 0.4682525098323822
train_iter_loss: 0.22445298731327057
train_iter_loss: 0.16907820105552673
train_iter_loss: 0.22576461732387543
train_iter_loss: 0.3010837435722351
train_iter_loss: 0.25895652174949646
train_iter_loss: 0.2993534207344055
train_iter_loss: 0.3758108913898468
train_iter_loss: 0.17098703980445862
train_iter_loss: 0.2579292356967926
train_iter_loss: 0.11444103717803955
train_iter_loss: 0.25638848543167114
train_iter_loss: 0.1829269975423813
train_iter_loss: 0.26079338788986206
train_iter_loss: 0.17590558528900146
train_iter_loss: 0.6102327704429626
train_iter_loss: 0.23958899080753326
train_iter_loss: 0.20395846664905548
train_iter_loss: 0.3716326951980591
train_iter_loss: 0.19153648614883423
train_iter_loss: 0.35774117708206177
train_iter_loss: 0.20054078102111816
train_iter_loss: 0.07634978741407394
train_iter_loss: 0.26434963941574097
train_iter_loss: 0.36040782928466797
train_iter_loss: 0.41919246315956116
train_iter_loss: 0.3655029833316803
train_iter_loss: 0.2674228847026825
train_iter_loss: 0.22939647734165192
train_iter_loss: 0.22007060050964355
train_iter_loss: 0.31540200114250183
train_iter_loss: 0.2835150361061096
train_iter_loss: 0.17065879702568054
train_iter_loss: 0.2396717518568039
train_iter_loss: 0.2496533840894699
train_iter_loss: 0.24246500432491302
train_iter_loss: 0.24149392545223236
train_iter_loss: 0.37767913937568665
train_iter_loss: 0.46362218260765076
train_iter_loss: 0.16509808599948883
train_iter_loss: 0.32860544323921204
train_iter_loss: 0.34130287170410156
train_iter_loss: 0.15338248014450073
train_iter_loss: 0.14571905136108398
train_iter_loss: 0.16041211783885956
train_iter_loss: 0.07487387210130692
train_iter_loss: 0.22891324758529663
train_iter_loss: 0.3486212193965912
train_iter_loss: 0.3464427590370178
train_iter_loss: 0.21195557713508606
train_iter_loss: 0.30834025144577026
train_iter_loss: 0.22279562056064606
train_iter_loss: 0.14148619771003723
train_iter_loss: 0.2247105836868286
train_iter_loss: 0.12167392671108246
train_iter_loss: 0.3001605272293091
train_iter_loss: 0.2706570625305176
train_iter_loss: 0.24397072196006775
train_iter_loss: 0.3315928876399994
train_iter_loss: 0.30189675092697144
train_iter_loss: 0.1909814327955246
train_iter_loss: 0.26738283038139343
train_iter_loss: 0.1708516776561737
train_iter_loss: 0.30939075350761414
train_iter_loss: 0.30542826652526855
train_iter_loss: 0.4356074035167694
train_iter_loss: 0.1886981725692749
train_iter_loss: 0.37135550379753113
train_iter_loss: 0.291314959526062
train_iter_loss: 0.2927566170692444
train_iter_loss: 0.11936640739440918
train_iter_loss: 0.39608830213546753
train_iter_loss: 0.23433613777160645
train_iter_loss: 0.35690709948539734
train_iter_loss: 0.41829925775527954
train_iter_loss: 0.2514427602291107
train_iter_loss: 0.3505021035671234
train_iter_loss: 0.518022894859314
train loss :0.2763
---------------------
Validation seg loss: 0.3726775751027437 at epoch 461
epoch =    462/  1000, exp = train
train_iter_loss: 0.1002112627029419
train_iter_loss: 0.15612465143203735
train_iter_loss: 0.24676179885864258
train_iter_loss: 0.23536095023155212
train_iter_loss: 0.3585094213485718
train_iter_loss: 0.29025599360466003
train_iter_loss: 0.2030223309993744
train_iter_loss: 0.2914249897003174
train_iter_loss: 0.20241324603557587
train_iter_loss: 0.21485424041748047
train_iter_loss: 0.3918668031692505
train_iter_loss: 0.21762610971927643
train_iter_loss: 0.28089281916618347
train_iter_loss: 0.21322788298130035
train_iter_loss: 0.3306654989719391
train_iter_loss: 0.27604880928993225
train_iter_loss: 0.17937467992305756
train_iter_loss: 0.3043552339076996
train_iter_loss: 0.3229556381702423
train_iter_loss: 0.2689213454723358
train_iter_loss: 0.19831044971942902
train_iter_loss: 0.42860057950019836
train_iter_loss: 0.31557413935661316
train_iter_loss: 0.24508197605609894
train_iter_loss: 0.26348450779914856
train_iter_loss: 0.2125914841890335
train_iter_loss: 0.2416437715291977
train_iter_loss: 0.39550381898880005
train_iter_loss: 0.1889149397611618
train_iter_loss: 0.2822529673576355
train_iter_loss: 0.32927176356315613
train_iter_loss: 0.14922161400318146
train_iter_loss: 0.46734875440597534
train_iter_loss: 0.10769879817962646
train_iter_loss: 0.3892829716205597
train_iter_loss: 0.2793143689632416
train_iter_loss: 0.35893404483795166
train_iter_loss: 0.32976287603378296
train_iter_loss: 0.23291967809200287
train_iter_loss: 0.14983001351356506
train_iter_loss: 0.26704588532447815
train_iter_loss: 0.22085054218769073
train_iter_loss: 0.2868044972419739
train_iter_loss: 0.17964661121368408
train_iter_loss: 0.14681553840637207
train_iter_loss: 0.11621683835983276
train_iter_loss: 0.46326228976249695
train_iter_loss: 0.37501490116119385
train_iter_loss: 0.34384503960609436
train_iter_loss: 0.32118701934814453
train_iter_loss: 0.29541823267936707
train_iter_loss: 0.43789607286453247
train_iter_loss: 0.1611638069152832
train_iter_loss: 0.4431249499320984
train_iter_loss: 0.21936073899269104
train_iter_loss: 0.32947057485580444
train_iter_loss: 0.2963753342628479
train_iter_loss: 0.24358317255973816
train_iter_loss: 0.11097933351993561
train_iter_loss: 0.42346715927124023
train_iter_loss: 0.16803115606307983
train_iter_loss: 0.2660101354122162
train_iter_loss: 0.2284296303987503
train_iter_loss: 0.16953203082084656
train_iter_loss: 0.2590814530849457
train_iter_loss: 0.3424437940120697
train_iter_loss: 0.26011934876441956
train_iter_loss: 0.32469111680984497
train_iter_loss: 0.28946906328201294
train_iter_loss: 0.2987323999404907
train_iter_loss: 0.3495692312717438
train_iter_loss: 0.2316271811723709
train_iter_loss: 0.1015368402004242
train_iter_loss: 0.10537899285554886
train_iter_loss: 0.39526933431625366
train_iter_loss: 0.3358767330646515
train_iter_loss: 0.2865547239780426
train_iter_loss: 0.2750549018383026
train_iter_loss: 0.14126303791999817
train_iter_loss: 0.3489322364330292
train_iter_loss: 0.35999202728271484
train_iter_loss: 0.2583206295967102
train_iter_loss: 0.17868302762508392
train_iter_loss: 0.3263382613658905
train_iter_loss: 0.3684089779853821
train_iter_loss: 0.4083004891872406
train_iter_loss: 0.2138984501361847
train_iter_loss: 0.32681214809417725
train_iter_loss: 0.28037238121032715
train_iter_loss: 0.2545270621776581
train_iter_loss: 0.3738361895084381
train_iter_loss: 0.2048707753419876
train_iter_loss: 0.28835755586624146
train_iter_loss: 0.3006730377674103
train_iter_loss: 0.2142641693353653
train_iter_loss: 0.2225719839334488
train_iter_loss: 0.2564985454082489
train_iter_loss: 0.3092958629131317
train_iter_loss: 0.4086245000362396
train_iter_loss: 0.19653534889221191
train loss :0.2754
---------------------
Validation seg loss: 0.3496337263089306 at epoch 462
********************
best_val_epoch_loss:  0.3496337263089306
MODEL UPDATED
epoch =    463/  1000, exp = train
train_iter_loss: 0.30980274081230164
train_iter_loss: 0.39248207211494446
train_iter_loss: 0.3642122745513916
train_iter_loss: 0.23125982284545898
train_iter_loss: 0.30246007442474365
train_iter_loss: 0.3631902039051056
train_iter_loss: 0.2904667258262634
train_iter_loss: 0.22553183138370514
train_iter_loss: 0.3552272915840149
train_iter_loss: 0.1765192598104477
train_iter_loss: 0.3180082142353058
train_iter_loss: 0.33048248291015625
train_iter_loss: 0.2463105171918869
train_iter_loss: 0.4122570753097534
train_iter_loss: 0.3356686234474182
train_iter_loss: 0.31187760829925537
train_iter_loss: 0.3083097040653229
train_iter_loss: 0.11141800880432129
train_iter_loss: 0.259141206741333
train_iter_loss: 0.2499283403158188
train_iter_loss: 0.3009834289550781
train_iter_loss: 0.21453407406806946
train_iter_loss: 0.25773218274116516
train_iter_loss: 0.3346342146396637
train_iter_loss: 0.20865187048912048
train_iter_loss: 0.20564047992229462
train_iter_loss: 0.32750460505485535
train_iter_loss: 0.1997482180595398
train_iter_loss: 0.12109220772981644
train_iter_loss: 0.23031996190547943
train_iter_loss: 0.11235295236110687
train_iter_loss: 0.35901981592178345
train_iter_loss: 0.2784005105495453
train_iter_loss: 0.3155611455440521
train_iter_loss: 0.29064345359802246
train_iter_loss: 0.23312653601169586
train_iter_loss: 0.2920921742916107
train_iter_loss: 0.34287533164024353
train_iter_loss: 0.16704244911670685
train_iter_loss: 0.2295742630958557
train_iter_loss: 0.18093279004096985
train_iter_loss: 0.3254890441894531
train_iter_loss: 0.32768774032592773
train_iter_loss: 0.3240605592727661
train_iter_loss: 0.14885777235031128
train_iter_loss: 0.2654483914375305
train_iter_loss: 0.2108551412820816
train_iter_loss: 0.2185775190591812
train_iter_loss: 0.4451103210449219
train_iter_loss: 0.25382915139198303
train_iter_loss: 0.3337531089782715
train_iter_loss: 0.2295425832271576
train_iter_loss: 0.3510358929634094
train_iter_loss: 0.2068520039319992
train_iter_loss: 0.3157133162021637
train_iter_loss: 0.31273582577705383
train_iter_loss: 0.12286423146724701
train_iter_loss: 0.28385040163993835
train_iter_loss: 0.3627104163169861
train_iter_loss: 0.3365216553211212
train_iter_loss: 0.36628925800323486
train_iter_loss: 0.1588435024023056
train_iter_loss: 0.08033809810876846
train_iter_loss: 0.27141278982162476
train_iter_loss: 0.2159428596496582
train_iter_loss: 0.19703848659992218
train_iter_loss: 0.2977292239665985
train_iter_loss: 0.2791782021522522
train_iter_loss: 0.382625013589859
train_iter_loss: 0.34531205892562866
train_iter_loss: 0.4212788939476013
train_iter_loss: 0.25811851024627686
train_iter_loss: 0.19594533741474152
train_iter_loss: 0.25635629892349243
train_iter_loss: 0.2593514323234558
train_iter_loss: 0.31042012572288513
train_iter_loss: 0.13520202040672302
train_iter_loss: 0.3984091877937317
train_iter_loss: 0.28768694400787354
train_iter_loss: 0.36425429582595825
train_iter_loss: 0.32259121537208557
train_iter_loss: 0.24107950925827026
train_iter_loss: 0.3539120852947235
train_iter_loss: 0.1895921379327774
train_iter_loss: 0.1350276619195938
train_iter_loss: 0.21633195877075195
train_iter_loss: 0.14378999173641205
train_iter_loss: 0.26966896653175354
train_iter_loss: 0.269792377948761
train_iter_loss: 0.2136663943529129
train_iter_loss: 0.41131287813186646
train_iter_loss: 0.2305186241865158
train_iter_loss: 0.2687080502510071
train_iter_loss: 0.296090692281723
train_iter_loss: 0.3486584424972534
train_iter_loss: 0.3961281478404999
train_iter_loss: 0.30629026889801025
train_iter_loss: 0.254729688167572
train_iter_loss: 0.2881837785243988
train_iter_loss: 0.198972687125206
train loss :0.2758
---------------------
Validation seg loss: 0.37056916272970586 at epoch 463
epoch =    464/  1000, exp = train
train_iter_loss: 0.23818503320217133
train_iter_loss: 0.23546764254570007
train_iter_loss: 0.2749383747577667
train_iter_loss: 0.27640870213508606
train_iter_loss: 0.18039411306381226
train_iter_loss: 0.34320658445358276
train_iter_loss: 0.2693476378917694
train_iter_loss: 0.2913784980773926
train_iter_loss: 0.4608357846736908
train_iter_loss: 0.2848421633243561
train_iter_loss: 0.1279652714729309
train_iter_loss: 0.29564979672431946
train_iter_loss: 0.28949087858200073
train_iter_loss: 0.23305989801883698
train_iter_loss: 0.34098947048187256
train_iter_loss: 0.4245867133140564
train_iter_loss: 0.1801133006811142
train_iter_loss: 0.24339933693408966
train_iter_loss: 0.38857677578926086
train_iter_loss: 0.18187293410301208
train_iter_loss: 0.23710374534130096
train_iter_loss: 0.23249854147434235
train_iter_loss: 0.21769608557224274
train_iter_loss: 0.27869606018066406
train_iter_loss: 0.2455127239227295
train_iter_loss: 0.38540926575660706
train_iter_loss: 0.1992252916097641
train_iter_loss: 0.041583869606256485
train_iter_loss: 0.26450756192207336
train_iter_loss: 0.30104145407676697
train_iter_loss: 0.2228202223777771
train_iter_loss: 0.2449931502342224
train_iter_loss: 0.2894616723060608
train_iter_loss: 0.29188019037246704
train_iter_loss: 0.1898796409368515
train_iter_loss: 0.16770240664482117
train_iter_loss: 0.36507558822631836
train_iter_loss: 0.31871655583381653
train_iter_loss: 0.4039180874824524
train_iter_loss: 0.307312548160553
train_iter_loss: 0.15655821561813354
train_iter_loss: 0.39359283447265625
train_iter_loss: 0.31335392594337463
train_iter_loss: 0.34063732624053955
train_iter_loss: 0.31760740280151367
train_iter_loss: 0.36231887340545654
train_iter_loss: 0.23934940993785858
train_iter_loss: 0.14959421753883362
train_iter_loss: 0.23679164052009583
train_iter_loss: 0.38454970717430115
train_iter_loss: 0.22634604573249817
train_iter_loss: 0.34633222222328186
train_iter_loss: 0.3294450342655182
train_iter_loss: 0.30651986598968506
train_iter_loss: 0.45003437995910645
train_iter_loss: 0.332873672246933
train_iter_loss: 0.4464399814605713
train_iter_loss: 0.30526959896087646
train_iter_loss: 0.1418878436088562
train_iter_loss: 0.09259051084518433
train_iter_loss: 0.36046913266181946
train_iter_loss: 0.23578600585460663
train_iter_loss: 0.33686086535453796
train_iter_loss: 0.30186861753463745
train_iter_loss: 0.046600375324487686
train_iter_loss: 0.19578847289085388
train_iter_loss: 0.21750901639461517
train_iter_loss: 0.3017648756504059
train_iter_loss: 0.17477521300315857
train_iter_loss: 0.26197049021720886
train_iter_loss: 0.2417561411857605
train_iter_loss: 0.4563845098018646
train_iter_loss: 0.14405763149261475
train_iter_loss: 0.077295683324337
train_iter_loss: 0.25724294781684875
train_iter_loss: 0.4887217581272125
train_iter_loss: 0.36394548416137695
train_iter_loss: 0.252560019493103
train_iter_loss: 0.2965315878391266
train_iter_loss: 0.32909440994262695
train_iter_loss: 0.2391088902950287
train_iter_loss: 0.4683864116668701
train_iter_loss: 0.21135351061820984
train_iter_loss: 0.22471290826797485
train_iter_loss: 0.12392785400152206
train_iter_loss: 0.43420523405075073
train_iter_loss: 0.2643319368362427
train_iter_loss: 0.19699092209339142
train_iter_loss: 0.43714210391044617
train_iter_loss: 0.2310156524181366
train_iter_loss: 0.32279741764068604
train_iter_loss: 0.21409763395786285
train_iter_loss: 0.23931998014450073
train_iter_loss: 0.247921422123909
train_iter_loss: 0.19518235325813293
train_iter_loss: 0.2629179358482361
train_iter_loss: 0.23502983152866364
train_iter_loss: 0.17675012350082397
train_iter_loss: 0.4266834557056427
train_iter_loss: 0.2593667507171631
train loss :0.2767
---------------------
Validation seg loss: 0.397868984765461 at epoch 464
epoch =    465/  1000, exp = train
train_iter_loss: 0.35875552892684937
train_iter_loss: 0.2263219654560089
train_iter_loss: 0.2620450258255005
train_iter_loss: 0.28206464648246765
train_iter_loss: 0.2441292405128479
train_iter_loss: 0.17840838432312012
train_iter_loss: 0.21139506995677948
train_iter_loss: 0.22075486183166504
train_iter_loss: 0.191404789686203
train_iter_loss: 0.3137223422527313
train_iter_loss: 0.32172033190727234
train_iter_loss: 0.18845981359481812
train_iter_loss: 0.20803320407867432
train_iter_loss: 0.028115289285779
train_iter_loss: 0.20185472071170807
train_iter_loss: 0.48052889108657837
train_iter_loss: 0.42142313718795776
train_iter_loss: 0.248977392911911
train_iter_loss: 0.3249291479587555
train_iter_loss: 0.3927323818206787
train_iter_loss: 0.14023995399475098
train_iter_loss: 0.20825490355491638
train_iter_loss: 0.22372780740261078
train_iter_loss: 0.1878587007522583
train_iter_loss: 0.34486421942710876
train_iter_loss: 0.22462548315525055
train_iter_loss: 0.3042833209037781
train_iter_loss: 0.3004441559314728
train_iter_loss: 0.38810107111930847
train_iter_loss: 0.24934150278568268
train_iter_loss: 0.2593786120414734
train_iter_loss: 0.27382606267929077
train_iter_loss: 0.24448032677173615
train_iter_loss: 0.23603719472885132
train_iter_loss: 0.34678757190704346
train_iter_loss: 0.2022310495376587
train_iter_loss: 0.24056968092918396
train_iter_loss: 0.35580265522003174
train_iter_loss: 0.3008923828601837
train_iter_loss: 0.25847452878952026
train_iter_loss: 0.36605963110923767
train_iter_loss: 0.33671292662620544
train_iter_loss: 0.22008368372917175
train_iter_loss: 0.2753565013408661
train_iter_loss: 0.0743737742304802
train_iter_loss: 0.052217964082956314
train_iter_loss: 0.3115774095058441
train_iter_loss: 0.22506485879421234
train_iter_loss: 0.29749375581741333
train_iter_loss: 0.3453563153743744
train_iter_loss: 0.3145909905433655
train_iter_loss: 0.3221904933452606
train_iter_loss: 0.3151053190231323
train_iter_loss: 0.23226188123226166
train_iter_loss: 0.5313168168067932
train_iter_loss: 0.2707623243331909
train_iter_loss: 0.33638158440589905
train_iter_loss: 0.25754597783088684
train_iter_loss: 0.30186405777931213
train_iter_loss: 0.3069024980068207
train_iter_loss: 0.1631622612476349
train_iter_loss: 0.21238656342029572
train_iter_loss: 0.23339757323265076
train_iter_loss: 0.4193820059299469
train_iter_loss: 0.30765023827552795
train_iter_loss: 0.43291032314300537
train_iter_loss: 0.19382131099700928
train_iter_loss: 0.2608971893787384
train_iter_loss: 0.14618021249771118
train_iter_loss: 0.27296364307403564
train_iter_loss: 0.3853880763053894
train_iter_loss: 0.22953398525714874
train_iter_loss: 0.3039858341217041
train_iter_loss: 0.4025368094444275
train_iter_loss: 0.20647156238555908
train_iter_loss: 0.19898296892642975
train_iter_loss: 0.23214982450008392
train_iter_loss: 0.26808130741119385
train_iter_loss: 0.14440588653087616
train_iter_loss: 0.21177899837493896
train_iter_loss: 0.2638210654258728
train_iter_loss: 0.2716546952724457
train_iter_loss: 0.4657396376132965
train_iter_loss: 0.14628535509109497
train_iter_loss: 0.22388076782226562
train_iter_loss: 0.3081386089324951
train_iter_loss: 0.22221587598323822
train_iter_loss: 0.2710239589214325
train_iter_loss: 0.24392083287239075
train_iter_loss: 0.4143076539039612
train_iter_loss: 0.2520540654659271
train_iter_loss: 0.2274392694234848
train_iter_loss: 0.21285343170166016
train_iter_loss: 0.22981436550617218
train_iter_loss: 0.13511307537555695
train_iter_loss: 0.2396640181541443
train_iter_loss: 0.36396926641464233
train_iter_loss: 0.18469378352165222
train_iter_loss: 0.216934472322464
train_iter_loss: 0.2388061136007309
train loss :0.2692
---------------------
Validation seg loss: 0.4472981743681473 at epoch 465
epoch =    466/  1000, exp = train
train_iter_loss: 0.11334795504808426
train_iter_loss: 0.336904376745224
train_iter_loss: 0.3572234511375427
train_iter_loss: 0.44843611121177673
train_iter_loss: 0.307810515165329
train_iter_loss: 0.3171030580997467
train_iter_loss: 0.16994908452033997
train_iter_loss: 0.4917936325073242
train_iter_loss: 0.25407615303993225
train_iter_loss: 0.3216826319694519
train_iter_loss: 0.26287227869033813
train_iter_loss: 0.2410147339105606
train_iter_loss: 0.18832002580165863
train_iter_loss: 0.27385053038597107
train_iter_loss: 0.23919403553009033
train_iter_loss: 0.2547034025192261
train_iter_loss: 0.2749103307723999
train_iter_loss: 0.22109177708625793
train_iter_loss: 0.24484451115131378
train_iter_loss: 0.30572590231895447
train_iter_loss: 0.2428816705942154
train_iter_loss: 0.33400872349739075
train_iter_loss: 0.23522815108299255
train_iter_loss: 0.2845516800880432
train_iter_loss: 0.28991270065307617
train_iter_loss: 0.3286939561367035
train_iter_loss: 0.3301773965358734
train_iter_loss: 0.3379037380218506
train_iter_loss: 0.13730184733867645
train_iter_loss: 0.2842988669872284
train_iter_loss: 0.2998339831829071
train_iter_loss: 0.1965285688638687
train_iter_loss: 0.27876704931259155
train_iter_loss: 0.24738992750644684
train_iter_loss: 0.2583105266094208
train_iter_loss: 0.28375762701034546
train_iter_loss: 0.2695480287075043
train_iter_loss: 0.16483938694000244
train_iter_loss: 0.33276328444480896
train_iter_loss: 0.12858456373214722
train_iter_loss: 0.32340848445892334
train_iter_loss: 0.2701930105686188
train_iter_loss: 0.27127712965011597
train_iter_loss: 0.13446153700351715
train_iter_loss: 0.30915096402168274
train_iter_loss: 0.35913199186325073
train_iter_loss: 0.41038501262664795
train_iter_loss: 0.22169145941734314
train_iter_loss: 0.36902642250061035
train_iter_loss: 0.3122503161430359
train_iter_loss: 0.18444189429283142
train_iter_loss: 0.46898478269577026
train_iter_loss: 0.24179959297180176
train_iter_loss: 0.30157095193862915
train_iter_loss: 0.29522275924682617
train_iter_loss: 0.19990277290344238
train_iter_loss: 0.3564152419567108
train_iter_loss: 0.26469749212265015
train_iter_loss: 0.20552977919578552
train_iter_loss: 0.35637256503105164
train_iter_loss: 0.3137042224407196
train_iter_loss: 0.33699831366539
train_iter_loss: 0.2746327519416809
train_iter_loss: 0.32306531071662903
train_iter_loss: 0.29496389627456665
train_iter_loss: 0.1811920553445816
train_iter_loss: 0.2872043251991272
train_iter_loss: 0.21829432249069214
train_iter_loss: 0.2100146859884262
train_iter_loss: 0.19089564681053162
train_iter_loss: 0.21186286211013794
train_iter_loss: 0.2526829242706299
train_iter_loss: 0.40544193983078003
train_iter_loss: 0.15588220953941345
train_iter_loss: 0.23930752277374268
train_iter_loss: 0.26781338453292847
train_iter_loss: 0.11047656089067459
train_iter_loss: 0.14098259806632996
train_iter_loss: 0.14340493083000183
train_iter_loss: 0.416397362947464
train_iter_loss: 0.1642375886440277
train_iter_loss: 0.19128455221652985
train_iter_loss: 0.23300491273403168
train_iter_loss: 0.2475292980670929
train_iter_loss: 0.3150709271430969
train_iter_loss: 0.16241465508937836
train_iter_loss: 0.35746246576309204
train_iter_loss: 0.17812030017375946
train_iter_loss: 0.19859464466571808
train_iter_loss: 0.20494233071804047
train_iter_loss: 0.26815536618232727
train_iter_loss: 0.4340721368789673
train_iter_loss: 0.3083864450454712
train_iter_loss: 0.3108399510383606
train_iter_loss: 0.326060026884079
train_iter_loss: 0.14174199104309082
train_iter_loss: 0.14503853023052216
train_iter_loss: 0.2809242606163025
train_iter_loss: 0.24437227845191956
train_iter_loss: 0.3364425599575043
train loss :0.2704
---------------------
Validation seg loss: 0.3947030414985317 at epoch 466
epoch =    467/  1000, exp = train
train_iter_loss: 0.28159597516059875
train_iter_loss: 0.148995503783226
train_iter_loss: 0.16285544633865356
train_iter_loss: 0.37462711334228516
train_iter_loss: 0.26288941502571106
train_iter_loss: 0.4879448115825653
train_iter_loss: 0.17912572622299194
train_iter_loss: 0.26579567790031433
train_iter_loss: 0.25433218479156494
train_iter_loss: 0.2287256121635437
train_iter_loss: 0.23234644532203674
train_iter_loss: 0.2157045304775238
train_iter_loss: 0.29799407720565796
train_iter_loss: 0.19882701337337494
train_iter_loss: 0.1804247498512268
train_iter_loss: 0.18366442620754242
train_iter_loss: 0.31737837195396423
train_iter_loss: 0.3767276108264923
train_iter_loss: 0.29253053665161133
train_iter_loss: 0.3537742793560028
train_iter_loss: 0.2857052981853485
train_iter_loss: 0.23617713153362274
train_iter_loss: 0.22789707779884338
train_iter_loss: 0.2784610688686371
train_iter_loss: 0.30614373087882996
train_iter_loss: 0.26217156648635864
train_iter_loss: 0.19904188811779022
train_iter_loss: 0.28845345973968506
train_iter_loss: 0.34274035692214966
train_iter_loss: 0.15427181124687195
train_iter_loss: 0.23420450091362
train_iter_loss: 0.15724660456180573
train_iter_loss: 0.35496261715888977
train_iter_loss: 0.3401922583580017
train_iter_loss: 0.318108469247818
train_iter_loss: 0.1308881938457489
train_iter_loss: 0.28844746947288513
train_iter_loss: 0.13708363473415375
train_iter_loss: 0.3123289942741394
train_iter_loss: 0.1592685431241989
train_iter_loss: 0.1500931978225708
train_iter_loss: 0.40917399525642395
train_iter_loss: 0.2932336926460266
train_iter_loss: 0.23886550962924957
train_iter_loss: 0.23189258575439453
train_iter_loss: 0.3421827256679535
train_iter_loss: 0.43068233132362366
train_iter_loss: 0.24998736381530762
train_iter_loss: 0.18573302030563354
train_iter_loss: 0.3586142361164093
train_iter_loss: 0.26398026943206787
train_iter_loss: 0.28024429082870483
train_iter_loss: 0.15892204642295837
train_iter_loss: 0.3074463903903961
train_iter_loss: 0.41477420926094055
train_iter_loss: 0.3643086552619934
train_iter_loss: 0.4374638497829437
train_iter_loss: 0.37058424949645996
train_iter_loss: 0.21563076972961426
train_iter_loss: 0.13965366780757904
train_iter_loss: 0.2285851091146469
train_iter_loss: 0.4099411964416504
train_iter_loss: 0.3868718147277832
train_iter_loss: 0.26977190375328064
train_iter_loss: 0.3156459331512451
train_iter_loss: 0.44950374960899353
train_iter_loss: 0.34482046961784363
train_iter_loss: 0.29684698581695557
train_iter_loss: 0.29726600646972656
train_iter_loss: 0.3512260913848877
train_iter_loss: 0.3064284920692444
train_iter_loss: 0.2721182405948639
train_iter_loss: 0.4062507152557373
train_iter_loss: 0.2642228901386261
train_iter_loss: 0.19231879711151123
train_iter_loss: 0.3193444609642029
train_iter_loss: 0.24190540611743927
train_iter_loss: 0.19348160922527313
train_iter_loss: 0.4424743056297302
train_iter_loss: 0.2957712709903717
train_iter_loss: 0.21877969801425934
train_iter_loss: 0.2811083197593689
train_iter_loss: 0.3084963262081146
train_iter_loss: 0.13310207426548004
train_iter_loss: 0.3383161127567291
train_iter_loss: 0.29463303089141846
train_iter_loss: 0.11820364743471146
train_iter_loss: 0.4548812806606293
train_iter_loss: 0.20248207449913025
train_iter_loss: 0.24475938081741333
train_iter_loss: 0.18115071952342987
train_iter_loss: 0.22861388325691223
train_iter_loss: 0.19497144222259521
train_iter_loss: 0.3250676393508911
train_iter_loss: 0.25307294726371765
train_iter_loss: 0.32139939069747925
train_iter_loss: 0.23024530708789825
train_iter_loss: 0.1557365357875824
train_iter_loss: 0.2652234733104706
train_iter_loss: 0.3392038643360138
train loss :0.2780
---------------------
Validation seg loss: 0.3949069569309084 at epoch 467
epoch =    468/  1000, exp = train
train_iter_loss: 0.28513678908348083
train_iter_loss: 0.23362146317958832
train_iter_loss: 0.3197980225086212
train_iter_loss: 0.23010925948619843
train_iter_loss: 0.2226550281047821
train_iter_loss: 0.3196350336074829
train_iter_loss: 0.3079851567745209
train_iter_loss: 0.23373015224933624
train_iter_loss: 0.28654518723487854
train_iter_loss: 0.22744208574295044
train_iter_loss: 0.36155638098716736
train_iter_loss: 0.22920279204845428
train_iter_loss: 0.15739569067955017
train_iter_loss: 0.36265799403190613
train_iter_loss: 0.3309866189956665
train_iter_loss: 0.22676394879817963
train_iter_loss: 0.1968129575252533
train_iter_loss: 0.301047146320343
train_iter_loss: 0.41099968552589417
train_iter_loss: 0.37272632122039795
train_iter_loss: 0.06853987276554108
train_iter_loss: 0.37319517135620117
train_iter_loss: 0.19295468926429749
train_iter_loss: 0.235992431640625
train_iter_loss: 0.2953970432281494
train_iter_loss: 0.11439056694507599
train_iter_loss: 0.44342440366744995
train_iter_loss: 0.2577766478061676
train_iter_loss: 0.2109820544719696
train_iter_loss: 0.34223923087120056
train_iter_loss: 0.3585459291934967
train_iter_loss: 0.20770643651485443
train_iter_loss: 0.17207124829292297
train_iter_loss: 0.2442212849855423
train_iter_loss: 0.23151110112667084
train_iter_loss: 0.255622535943985
train_iter_loss: 0.3669441342353821
train_iter_loss: 0.3212243616580963
train_iter_loss: 0.3141530752182007
train_iter_loss: 0.27706170082092285
train_iter_loss: 0.2729273736476898
train_iter_loss: 0.24609346687793732
train_iter_loss: 0.38391876220703125
train_iter_loss: 0.27022725343704224
train_iter_loss: 0.30135199427604675
train_iter_loss: 0.21500861644744873
train_iter_loss: 0.3175281584262848
train_iter_loss: 0.34333518147468567
train_iter_loss: 0.4156785309314728
train_iter_loss: 0.40661972761154175
train_iter_loss: 0.2127716988325119
train_iter_loss: 0.32286903262138367
train_iter_loss: 0.21404960751533508
train_iter_loss: 0.28474506735801697
train_iter_loss: 0.3587508499622345
train_iter_loss: 0.22942285239696503
train_iter_loss: 0.28613290190696716
train_iter_loss: 0.22034984827041626
train_iter_loss: 0.15087050199508667
train_iter_loss: 0.3678351938724518
train_iter_loss: 0.39279502630233765
train_iter_loss: 0.4465405344963074
train_iter_loss: 0.3661985695362091
train_iter_loss: 0.18482248485088348
train_iter_loss: 0.07205978035926819
train_iter_loss: 0.14857089519500732
train_iter_loss: 0.2205154299736023
train_iter_loss: 0.32591482996940613
train_iter_loss: 0.13064734637737274
train_iter_loss: 0.2641395032405853
train_iter_loss: 0.151768758893013
train_iter_loss: 0.14592894911766052
train_iter_loss: 0.12059998512268066
train_iter_loss: 0.30209189653396606
train_iter_loss: 0.24714887142181396
train_iter_loss: 0.22665005922317505
train_iter_loss: 0.27414408326148987
train_iter_loss: 0.3694861829280853
train_iter_loss: 0.3582451641559601
train_iter_loss: 0.2361379861831665
train_iter_loss: 0.1688849776983261
train_iter_loss: 0.44642993807792664
train_iter_loss: 0.21482811868190765
train_iter_loss: 0.1963934749364853
train_iter_loss: 0.3496687114238739
train_iter_loss: 0.3679875433444977
train_iter_loss: 0.3435049057006836
train_iter_loss: 0.22471077740192413
train_iter_loss: 0.3846968114376068
train_iter_loss: 0.20903122425079346
train_iter_loss: 0.22399763762950897
train_iter_loss: 0.3621670603752136
train_iter_loss: 0.3694855868816376
train_iter_loss: 0.23720453679561615
train_iter_loss: 0.21326510608196259
train_iter_loss: 0.1450170874595642
train_iter_loss: 0.20613273978233337
train_iter_loss: 0.25310173630714417
train_iter_loss: 0.21324610710144043
train_iter_loss: 0.1159818172454834
train loss :0.2723
---------------------
Validation seg loss: 0.3987681161313546 at epoch 468
epoch =    469/  1000, exp = train
train_iter_loss: 0.29948192834854126
train_iter_loss: 0.2794061303138733
train_iter_loss: 0.13114632666110992
train_iter_loss: 0.4857366383075714
train_iter_loss: 0.37135812640190125
train_iter_loss: 0.28012996912002563
train_iter_loss: 0.3178355097770691
train_iter_loss: 0.20779819786548615
train_iter_loss: 0.4181075990200043
train_iter_loss: 0.39968985319137573
train_iter_loss: 0.19431529939174652
train_iter_loss: 0.3491199016571045
train_iter_loss: 0.6297524571418762
train_iter_loss: 0.26429077982902527
train_iter_loss: 0.17764903604984283
train_iter_loss: 0.3051515817642212
train_iter_loss: 0.32065123319625854
train_iter_loss: 0.035328637808561325
train_iter_loss: 0.17763298749923706
train_iter_loss: 0.27730950713157654
train_iter_loss: 0.23299899697303772
train_iter_loss: 0.3686903417110443
train_iter_loss: 0.28899091482162476
train_iter_loss: 0.24642927944660187
train_iter_loss: 0.30556753277778625
train_iter_loss: 0.12777461111545563
train_iter_loss: 0.27642300724983215
train_iter_loss: 0.2558498680591583
train_iter_loss: 0.39650827646255493
train_iter_loss: 0.22840723395347595
train_iter_loss: 0.3276936709880829
train_iter_loss: 0.35778263211250305
train_iter_loss: 0.3953094780445099
train_iter_loss: 0.2630806565284729
train_iter_loss: 0.26026520133018494
train_iter_loss: 0.27389290928840637
train_iter_loss: 0.3678258955478668
train_iter_loss: 0.25542953610420227
train_iter_loss: 0.25676387548446655
train_iter_loss: 0.18527722358703613
train_iter_loss: 0.3762962222099304
train_iter_loss: 0.16960281133651733
train_iter_loss: 0.2665407657623291
train_iter_loss: 0.4134311079978943
train_iter_loss: 0.10981263220310211
train_iter_loss: 0.35057851672172546
train_iter_loss: 0.0946354791522026
train_iter_loss: 0.3563791811466217
train_iter_loss: 0.4077056050300598
train_iter_loss: 0.2686507999897003
train_iter_loss: 0.1274748593568802
train_iter_loss: 0.17186018824577332
train_iter_loss: 0.31456273794174194
train_iter_loss: 0.29761287569999695
train_iter_loss: 0.43242597579956055
train_iter_loss: 0.263265997171402
train_iter_loss: 0.4105105996131897
train_iter_loss: 0.37784284353256226
train_iter_loss: 0.30529001355171204
train_iter_loss: 0.260014146566391
train_iter_loss: 0.1894141286611557
train_iter_loss: 0.31325680017471313
train_iter_loss: 0.37450435757637024
train_iter_loss: 0.36243849992752075
train_iter_loss: 0.27077004313468933
train_iter_loss: 0.26099592447280884
train_iter_loss: 0.16937552392482758
train_iter_loss: 0.31986844539642334
train_iter_loss: 0.27180492877960205
train_iter_loss: 0.18649378418922424
train_iter_loss: 0.22739453613758087
train_iter_loss: 0.266287624835968
train_iter_loss: 0.3285517990589142
train_iter_loss: 0.2681288421154022
train_iter_loss: 0.23321956396102905
train_iter_loss: 0.3170829117298126
train_iter_loss: 0.16255396604537964
train_iter_loss: 0.305023729801178
train_iter_loss: 0.2978879511356354
train_iter_loss: 0.21196283400058746
train_iter_loss: 0.2777714431285858
train_iter_loss: 0.15809059143066406
train_iter_loss: 0.39213865995407104
train_iter_loss: 0.3834262490272522
train_iter_loss: 0.21332363784313202
train_iter_loss: 0.291093111038208
train_iter_loss: 0.328698068857193
train_iter_loss: 0.10487930476665497
train_iter_loss: 0.07454927265644073
train_iter_loss: 0.262787401676178
train_iter_loss: 0.28752851486206055
train_iter_loss: 0.2772078216075897
train_iter_loss: 0.1526186466217041
train_iter_loss: 0.2778814733028412
train_iter_loss: 0.2206469178199768
train_iter_loss: 0.2538784444332123
train_iter_loss: 0.2365163266658783
train_iter_loss: 0.18838025629520416
train_iter_loss: 0.2637826204299927
train_iter_loss: 0.24163629114627838
train loss :0.2786
---------------------
Validation seg loss: 0.372148337387392 at epoch 469
epoch =    470/  1000, exp = train
train_iter_loss: 0.27433347702026367
train_iter_loss: 0.16857175529003143
train_iter_loss: 0.10801928490400314
train_iter_loss: 0.27241289615631104
train_iter_loss: 0.28510722517967224
train_iter_loss: 0.12805719673633575
train_iter_loss: 0.22163259983062744
train_iter_loss: 0.19431926310062408
train_iter_loss: 0.2508530020713806
train_iter_loss: 0.2897023558616638
train_iter_loss: 0.299571692943573
train_iter_loss: 0.331150621175766
train_iter_loss: 0.28288620710372925
train_iter_loss: 0.34720468521118164
train_iter_loss: 0.255037784576416
train_iter_loss: 0.2757936418056488
train_iter_loss: 0.1008770614862442
train_iter_loss: 0.3354264497756958
train_iter_loss: 0.3664211928844452
train_iter_loss: 0.23185890913009644
train_iter_loss: 0.27771008014678955
train_iter_loss: 0.22798414528369904
train_iter_loss: 0.2838314175605774
train_iter_loss: 0.3355277478694916
train_iter_loss: 0.19398336112499237
train_iter_loss: 0.22142097353935242
train_iter_loss: 0.22518649697303772
train_iter_loss: 0.2466757893562317
train_iter_loss: 0.3439396619796753
train_iter_loss: 0.05989982560276985
train_iter_loss: 0.400238573551178
train_iter_loss: 0.2538432776927948
train_iter_loss: 0.33208951354026794
train_iter_loss: 0.27932313084602356
train_iter_loss: 0.12818731367588043
train_iter_loss: 0.45624420046806335
train_iter_loss: 0.20225024223327637
train_iter_loss: 0.18798337876796722
train_iter_loss: 0.16652904450893402
train_iter_loss: 0.26037001609802246
train_iter_loss: 0.3643288016319275
train_iter_loss: 0.35404112935066223
train_iter_loss: 0.48481541872024536
train_iter_loss: 0.3695855438709259
train_iter_loss: 0.3944089412689209
train_iter_loss: 0.2734045386314392
train_iter_loss: 0.3251912295818329
train_iter_loss: 0.39023253321647644
train_iter_loss: 0.24695304036140442
train_iter_loss: 0.22048941254615784
train_iter_loss: 0.26693567633628845
train_iter_loss: 0.22172509133815765
train_iter_loss: 0.1914457529783249
train_iter_loss: 0.353473037481308
train_iter_loss: 0.2596786618232727
train_iter_loss: 0.2219352424144745
train_iter_loss: 0.21869462728500366
train_iter_loss: 0.22012558579444885
train_iter_loss: 0.3351350724697113
train_iter_loss: 0.33042070269584656
train_iter_loss: 0.21797500550746918
train_iter_loss: 0.29520371556282043
train_iter_loss: 0.2790449559688568
train_iter_loss: 0.18132783472537994
train_iter_loss: 0.34267091751098633
train_iter_loss: 0.6145239472389221
train_iter_loss: 0.15606874227523804
train_iter_loss: 0.1703866571187973
train_iter_loss: 0.38238129019737244
train_iter_loss: 0.23239795863628387
train_iter_loss: 0.44815629720687866
train_iter_loss: 0.2822788953781128
train_iter_loss: 0.24081383645534515
train_iter_loss: 0.1877848207950592
train_iter_loss: 0.31020504236221313
train_iter_loss: 0.268842488527298
train_iter_loss: 0.2279520481824875
train_iter_loss: 0.4532977044582367
train_iter_loss: 0.24461151659488678
train_iter_loss: 0.3495365381240845
train_iter_loss: 0.3966458737850189
train_iter_loss: 0.32220834493637085
train_iter_loss: 0.1452624797821045
train_iter_loss: 0.24234317243099213
train_iter_loss: 0.2755146920681
train_iter_loss: 0.1986016184091568
train_iter_loss: 0.2772786021232605
train_iter_loss: 0.18050937354564667
train_iter_loss: 0.28392407298088074
train_iter_loss: 0.137090802192688
train_iter_loss: 0.24520693719387054
train_iter_loss: 0.13893376290798187
train_iter_loss: 0.23095230758190155
train_iter_loss: 0.41364291310310364
train_iter_loss: 0.05495017394423485
train_iter_loss: 0.3830966353416443
train_iter_loss: 0.39420703053474426
train_iter_loss: 0.22708965837955475
train_iter_loss: 0.1905001699924469
train_iter_loss: 0.19560375809669495
train loss :0.2724
---------------------
Validation seg loss: 0.3648582803313884 at epoch 470
epoch =    471/  1000, exp = train
train_iter_loss: 0.45470133423805237
train_iter_loss: 0.39067840576171875
train_iter_loss: 0.26701146364212036
train_iter_loss: 0.25697067379951477
train_iter_loss: 0.26843804121017456
train_iter_loss: 0.26030486822128296
train_iter_loss: 0.16116562485694885
train_iter_loss: 0.23627370595932007
train_iter_loss: 0.2633320093154907
train_iter_loss: 0.5371376872062683
train_iter_loss: 0.29989853501319885
train_iter_loss: 0.34136196970939636
train_iter_loss: 0.19280469417572021
train_iter_loss: 0.3046955466270447
train_iter_loss: 0.3430849611759186
train_iter_loss: 0.16892589628696442
train_iter_loss: 0.46855562925338745
train_iter_loss: 0.34589919447898865
train_iter_loss: 0.3004697263240814
train_iter_loss: 0.22728630900382996
train_iter_loss: 0.16959287226200104
train_iter_loss: 0.4070099890232086
train_iter_loss: 0.3285585939884186
train_iter_loss: 0.25045913457870483
train_iter_loss: 0.2231496423482895
train_iter_loss: 0.38213449716567993
train_iter_loss: 0.29592251777648926
train_iter_loss: 0.2088431864976883
train_iter_loss: 0.26201894879341125
train_iter_loss: 0.36124274134635925
train_iter_loss: 0.1985100656747818
train_iter_loss: 0.25797876715660095
train_iter_loss: 0.26871588826179504
train_iter_loss: 0.39799273014068604
train_iter_loss: 0.267591655254364
train_iter_loss: 0.32102835178375244
train_iter_loss: 0.16567057371139526
train_iter_loss: 0.3759002387523651
train_iter_loss: 0.22991955280303955
train_iter_loss: 0.1946820765733719
train_iter_loss: 0.3507613241672516
train_iter_loss: 0.3104020357131958
train_iter_loss: 0.16298867762088776
train_iter_loss: 0.3262174129486084
train_iter_loss: 0.23304982483386993
train_iter_loss: 0.3309328854084015
train_iter_loss: 0.4760356843471527
train_iter_loss: 0.3794359564781189
train_iter_loss: 0.22845083475112915
train_iter_loss: 0.14417904615402222
train_iter_loss: 0.27139878273010254
train_iter_loss: 0.19682851433753967
train_iter_loss: 0.2995797395706177
train_iter_loss: 0.38307908177375793
train_iter_loss: 0.09959985315799713
train_iter_loss: 0.23044557869434357
train_iter_loss: 0.18936394155025482
train_iter_loss: 0.30064478516578674
train_iter_loss: 0.41798636317253113
train_iter_loss: 0.18975627422332764
train_iter_loss: 0.3012775778770447
train_iter_loss: 0.18433727324008942
train_iter_loss: 0.26470500230789185
train_iter_loss: 0.22783417999744415
train_iter_loss: 0.24254246056079865
train_iter_loss: 0.2617148458957672
train_iter_loss: 0.22202365100383759
train_iter_loss: 0.2281067967414856
train_iter_loss: 0.21778051555156708
train_iter_loss: 0.23786722123622894
train_iter_loss: 0.42356613278388977
train_iter_loss: 0.2150232195854187
train_iter_loss: 0.29511120915412903
train_iter_loss: 0.3099260628223419
train_iter_loss: 0.23572099208831787
train_iter_loss: 0.23508410155773163
train_iter_loss: 0.18325437605381012
train_iter_loss: 0.25559118390083313
train_iter_loss: 0.3164961338043213
train_iter_loss: 0.18164370954036713
train_iter_loss: 0.16219882667064667
train_iter_loss: 0.15818814933300018
train_iter_loss: 0.11819595098495483
train_iter_loss: 0.46810659766197205
train_iter_loss: 0.4124685227870941
train_iter_loss: 0.2951790988445282
train_iter_loss: 0.16276854276657104
train_iter_loss: 0.2710235118865967
train_iter_loss: 0.3277965784072876
train_iter_loss: 0.32199251651763916
train_iter_loss: 0.28947970271110535
train_iter_loss: 0.27972275018692017
train_iter_loss: 0.21360139548778534
train_iter_loss: 0.29643383622169495
train_iter_loss: 0.3626802861690521
train_iter_loss: 0.17520031332969666
train_iter_loss: 0.07124613225460052
train_iter_loss: 0.2482277750968933
train_iter_loss: 0.6079609394073486
train_iter_loss: 0.21985620260238647
train loss :0.2795
---------------------
Validation seg loss: 0.3737952061840948 at epoch 471
epoch =    472/  1000, exp = train
train_iter_loss: 0.20105750858783722
train_iter_loss: 0.25696811079978943
train_iter_loss: 0.4098067283630371
train_iter_loss: 0.19368870556354523
train_iter_loss: 0.23764686286449432
train_iter_loss: 0.3692990243434906
train_iter_loss: 0.15615397691726685
train_iter_loss: 0.28531742095947266
train_iter_loss: 0.25701838731765747
train_iter_loss: 0.20728705823421478
train_iter_loss: 0.25331777334213257
train_iter_loss: 0.22985725104808807
train_iter_loss: 0.2202412188053131
train_iter_loss: 0.275788813829422
train_iter_loss: 0.5238415598869324
train_iter_loss: 0.2707298994064331
train_iter_loss: 0.24059416353702545
train_iter_loss: 0.4094562828540802
train_iter_loss: 0.12935656309127808
train_iter_loss: 0.2515559196472168
train_iter_loss: 0.508173406124115
train_iter_loss: 0.2164730429649353
train_iter_loss: 0.3223028779029846
train_iter_loss: 0.2354346364736557
train_iter_loss: 0.19696855545043945
train_iter_loss: 0.2704719007015228
train_iter_loss: 0.19301633536815643
train_iter_loss: 0.2734059691429138
train_iter_loss: 0.25081270933151245
train_iter_loss: 0.29375290870666504
train_iter_loss: 0.3842378258705139
train_iter_loss: 0.17798328399658203
train_iter_loss: 0.23213623464107513
train_iter_loss: 0.20009534060955048
train_iter_loss: 0.18607234954833984
train_iter_loss: 0.2707619369029999
train_iter_loss: 0.21977576613426208
train_iter_loss: 0.3135188817977905
train_iter_loss: 0.28523269295692444
train_iter_loss: 0.2988167405128479
train_iter_loss: 0.22617073357105255
train_iter_loss: 0.28652751445770264
train_iter_loss: 0.18803463876247406
train_iter_loss: 0.20895159244537354
train_iter_loss: 0.20425885915756226
train_iter_loss: 0.3348081707954407
train_iter_loss: 0.212455153465271
train_iter_loss: 0.12739604711532593
train_iter_loss: 0.1335902214050293
train_iter_loss: 0.17177532613277435
train_iter_loss: 0.2807055413722992
train_iter_loss: 0.20991668105125427
train_iter_loss: 0.33938685059547424
train_iter_loss: 0.25391682982444763
train_iter_loss: 0.3252827823162079
train_iter_loss: 0.15838348865509033
train_iter_loss: 0.5384816527366638
train_iter_loss: 0.41811883449554443
train_iter_loss: 0.34998875856399536
train_iter_loss: 0.1707356721162796
train_iter_loss: 0.1880841702222824
train_iter_loss: 0.34147244691848755
train_iter_loss: 0.33044397830963135
train_iter_loss: 0.1655368208885193
train_iter_loss: 0.21511301398277283
train_iter_loss: 0.185121089220047
train_iter_loss: 0.17756009101867676
train_iter_loss: 0.28694722056388855
train_iter_loss: 0.1348799765110016
train_iter_loss: 0.3731064796447754
train_iter_loss: 0.24319146573543549
train_iter_loss: 0.35319212079048157
train_iter_loss: 0.23150625824928284
train_iter_loss: 0.4406345784664154
train_iter_loss: 0.2916712164878845
train_iter_loss: 0.18913403153419495
train_iter_loss: 0.28236302733421326
train_iter_loss: 0.2422524243593216
train_iter_loss: 0.29714521765708923
train_iter_loss: 0.2543763816356659
train_iter_loss: 0.03962494432926178
train_iter_loss: 0.3037606477737427
train_iter_loss: 0.3483012616634369
train_iter_loss: 0.3036714196205139
train_iter_loss: 0.41620486974716187
train_iter_loss: 0.1518796980381012
train_iter_loss: 0.23477883636951447
train_iter_loss: 0.33927208185195923
train_iter_loss: 0.35340791940689087
train_iter_loss: 0.3327561914920807
train_iter_loss: 0.18341682851314545
train_iter_loss: 0.29232093691825867
train_iter_loss: 0.4120234549045563
train_iter_loss: 0.058767933398485184
train_iter_loss: 0.20078660547733307
train_iter_loss: 0.25640925765037537
train_iter_loss: 0.37814104557037354
train_iter_loss: 0.3342430591583252
train_iter_loss: 0.2524625360965729
train_iter_loss: 0.28301405906677246
train loss :0.2683
---------------------
Validation seg loss: 0.3878999141062487 at epoch 472
epoch =    473/  1000, exp = train
train_iter_loss: 0.27229294180870056
train_iter_loss: 0.19545769691467285
train_iter_loss: 0.34871044754981995
train_iter_loss: 0.30568695068359375
train_iter_loss: 0.3782273232936859
train_iter_loss: 0.43558046221733093
train_iter_loss: 0.13166449964046478
train_iter_loss: 0.271460622549057
train_iter_loss: 0.2732795476913452
train_iter_loss: 0.20536646246910095
train_iter_loss: 0.23190204799175262
train_iter_loss: 0.3115934133529663
train_iter_loss: 0.2343589961528778
train_iter_loss: 0.26573508977890015
train_iter_loss: 0.41338446736335754
train_iter_loss: 0.16951008141040802
train_iter_loss: 0.3678346276283264
train_iter_loss: 0.2249877154827118
train_iter_loss: 0.1758335530757904
train_iter_loss: 0.27109673619270325
train_iter_loss: 0.2860199809074402
train_iter_loss: 0.3864169418811798
train_iter_loss: 0.20120467245578766
train_iter_loss: 0.3697972297668457
train_iter_loss: 0.3295779526233673
train_iter_loss: 0.31237807869911194
train_iter_loss: 0.18288536369800568
train_iter_loss: 0.2778207063674927
train_iter_loss: 0.1354907900094986
train_iter_loss: 0.35696762800216675
train_iter_loss: 0.38375672698020935
train_iter_loss: 0.5967885255813599
train_iter_loss: 0.14273884892463684
train_iter_loss: 0.2008594572544098
train_iter_loss: 0.1230349987745285
train_iter_loss: 0.21805495023727417
train_iter_loss: 0.18879655003547668
train_iter_loss: 0.31618112325668335
train_iter_loss: 0.23015958070755005
train_iter_loss: 0.3089703917503357
train_iter_loss: 0.33074426651000977
train_iter_loss: 0.1314660608768463
train_iter_loss: 0.21016941964626312
train_iter_loss: 0.3399302065372467
train_iter_loss: 0.23922961950302124
train_iter_loss: 0.23370864987373352
train_iter_loss: 0.2700164318084717
train_iter_loss: 0.24739259481430054
train_iter_loss: 0.20411518216133118
train_iter_loss: 0.33707720041275024
train_iter_loss: 0.18485605716705322
train_iter_loss: 0.3041685223579407
train_iter_loss: 0.19937945902347565
train_iter_loss: 0.32969653606414795
train_iter_loss: 0.2957839369773865
train_iter_loss: 0.24806183576583862
train_iter_loss: 0.21993474662303925
train_iter_loss: 0.21224817633628845
train_iter_loss: 0.3178357481956482
train_iter_loss: 0.18966072797775269
train_iter_loss: 0.29770761728286743
train_iter_loss: 0.18684279918670654
train_iter_loss: 0.10279746353626251
train_iter_loss: 0.35810378193855286
train_iter_loss: 0.20659880340099335
train_iter_loss: 0.44897350668907166
train_iter_loss: 0.24571625888347626
train_iter_loss: 0.35019710659980774
train_iter_loss: 0.2851566970348358
train_iter_loss: 0.40126511454582214
train_iter_loss: 0.1731538474559784
train_iter_loss: 0.18973477184772491
train_iter_loss: 0.2959696352481842
train_iter_loss: 0.22211354970932007
train_iter_loss: 0.5405223369598389
train_iter_loss: 0.3337167203426361
train_iter_loss: 0.030809279531240463
train_iter_loss: 0.3140096366405487
train_iter_loss: 0.2558514177799225
train_iter_loss: 0.25508812069892883
train_iter_loss: 0.34438058733940125
train_iter_loss: 0.14250300824642181
train_iter_loss: 0.24371859431266785
train_iter_loss: 0.1977127641439438
train_iter_loss: 0.30789563059806824
train_iter_loss: 0.2637259066104889
train_iter_loss: 0.34448376297950745
train_iter_loss: 0.24778854846954346
train_iter_loss: 0.379226416349411
train_iter_loss: 0.25246843695640564
train_iter_loss: 0.3643086850643158
train_iter_loss: 0.26169872283935547
train_iter_loss: 0.21020697057247162
train_iter_loss: 0.39575859904289246
train_iter_loss: 0.13491086661815643
train_iter_loss: 0.33390408754348755
train_iter_loss: 0.18191055953502655
train_iter_loss: 0.263931006193161
train_iter_loss: 0.22545470297336578
train_iter_loss: 0.2491995245218277
train loss :0.2719
---------------------
Validation seg loss: 0.36529164064570135 at epoch 473
epoch =    474/  1000, exp = train
train_iter_loss: 0.13939031958580017
train_iter_loss: 0.4192672669887543
train_iter_loss: 0.32719823718070984
train_iter_loss: 0.1053345650434494
train_iter_loss: 0.25116726756095886
train_iter_loss: 0.26078319549560547
train_iter_loss: 0.12865731120109558
train_iter_loss: 0.4730477035045624
train_iter_loss: 0.2736440598964691
train_iter_loss: 0.16665761172771454
train_iter_loss: 0.202323317527771
train_iter_loss: 0.08597502112388611
train_iter_loss: 0.2955947518348694
train_iter_loss: 0.16045497357845306
train_iter_loss: 0.2902337610721588
train_iter_loss: 0.2758484184741974
train_iter_loss: 0.3183353841304779
train_iter_loss: 0.25377777218818665
train_iter_loss: 0.2657410502433777
train_iter_loss: 0.26137298345565796
train_iter_loss: 0.1025550365447998
train_iter_loss: 0.17042650282382965
train_iter_loss: 0.3621536195278168
train_iter_loss: 0.06424885243177414
train_iter_loss: 0.24199075996875763
train_iter_loss: 0.11924362927675247
train_iter_loss: 0.2438487410545349
train_iter_loss: 0.23807062208652496
train_iter_loss: 0.32976770401000977
train_iter_loss: 0.30562254786491394
train_iter_loss: 0.2533670961856842
train_iter_loss: 0.2576829493045807
train_iter_loss: 0.36892178654670715
train_iter_loss: 0.34185051918029785
train_iter_loss: 0.3258912265300751
train_iter_loss: 0.35275998711586
train_iter_loss: 0.32099294662475586
train_iter_loss: 0.3316154181957245
train_iter_loss: 0.13078708946704865
train_iter_loss: 0.45938247442245483
train_iter_loss: 0.11519365012645721
train_iter_loss: 0.32125669717788696
train_iter_loss: 0.3247145414352417
train_iter_loss: 0.2916072607040405
train_iter_loss: 0.21113087236881256
train_iter_loss: 0.2016289085149765
train_iter_loss: 0.2250543087720871
train_iter_loss: 0.36934083700180054
train_iter_loss: 0.2671385705471039
train_iter_loss: 0.187674880027771
train_iter_loss: 0.40105879306793213
train_iter_loss: 0.23827214539051056
train_iter_loss: 0.25658944249153137
train_iter_loss: 0.32813066244125366
train_iter_loss: 0.22102968394756317
train_iter_loss: 0.1650429368019104
train_iter_loss: 0.2655160427093506
train_iter_loss: 0.4268058240413666
train_iter_loss: 0.25956135988235474
train_iter_loss: 0.2510991096496582
train_iter_loss: 0.27110493183135986
train_iter_loss: 0.2916242182254791
train_iter_loss: 0.2552117705345154
train_iter_loss: 0.2260778695344925
train_iter_loss: 0.3793788552284241
train_iter_loss: 0.24859094619750977
train_iter_loss: 0.28432393074035645
train_iter_loss: 0.19046877324581146
train_iter_loss: 0.24968324601650238
train_iter_loss: 0.2556571960449219
train_iter_loss: 0.1483592391014099
train_iter_loss: 0.33334827423095703
train_iter_loss: 0.3371695280075073
train_iter_loss: 0.23301807045936584
train_iter_loss: 0.3711681365966797
train_iter_loss: 0.2998182475566864
train_iter_loss: 0.2540826201438904
train_iter_loss: 0.5163832902908325
train_iter_loss: 0.12347066402435303
train_iter_loss: 0.2598590552806854
train_iter_loss: 0.1930815577507019
train_iter_loss: 0.28628864884376526
train_iter_loss: 0.2926478087902069
train_iter_loss: 0.25355958938598633
train_iter_loss: 0.39630326628685
train_iter_loss: 0.3478594720363617
train_iter_loss: 0.2968985140323639
train_iter_loss: 0.17961740493774414
train_iter_loss: 0.32748302817344666
train_iter_loss: 0.14817136526107788
train_iter_loss: 0.4040165841579437
train_iter_loss: 0.3125355839729309
train_iter_loss: 0.3565608859062195
train_iter_loss: 0.3287733793258667
train_iter_loss: 0.19680507481098175
train_iter_loss: 0.08870738744735718
train_iter_loss: 0.3539835512638092
train_iter_loss: 0.3713100850582123
train_iter_loss: 0.28539279103279114
train_iter_loss: 0.27877476811408997
train loss :0.2710
---------------------
Validation seg loss: 0.35782914923257986 at epoch 474
epoch =    475/  1000, exp = train
train_iter_loss: 0.1456068605184555
train_iter_loss: 0.3303823471069336
train_iter_loss: 0.22469495236873627
train_iter_loss: 0.09422490745782852
train_iter_loss: 0.21488137543201447
train_iter_loss: 0.40709882974624634
train_iter_loss: 0.22800731658935547
train_iter_loss: 0.18736295402050018
train_iter_loss: 0.238936647772789
train_iter_loss: 0.29444169998168945
train_iter_loss: 0.369601845741272
train_iter_loss: 0.29698899388313293
train_iter_loss: 0.27493706345558167
train_iter_loss: 0.3157700002193451
train_iter_loss: 0.2743542194366455
train_iter_loss: 0.08769480884075165
train_iter_loss: 0.35539379715919495
train_iter_loss: 0.37425917387008667
train_iter_loss: 0.270094096660614
train_iter_loss: 0.31800901889801025
train_iter_loss: 0.4753825068473816
train_iter_loss: 0.26324477791786194
train_iter_loss: 0.2766951322555542
train_iter_loss: 0.24557043612003326
train_iter_loss: 0.2863701283931732
train_iter_loss: 0.2310328483581543
train_iter_loss: 0.3650340735912323
train_iter_loss: 0.34485554695129395
train_iter_loss: 0.19511815905570984
train_iter_loss: 0.15715943276882172
train_iter_loss: 0.40400415658950806
train_iter_loss: 0.3379029631614685
train_iter_loss: 0.235702246427536
train_iter_loss: 0.19190368056297302
train_iter_loss: 0.39672788977622986
train_iter_loss: 0.3042505383491516
train_iter_loss: 0.04164552688598633
train_iter_loss: 0.47456836700439453
train_iter_loss: 0.17558380961418152
train_iter_loss: 0.2850300967693329
train_iter_loss: 0.4322812855243683
train_iter_loss: 0.257598340511322
train_iter_loss: 0.2577233910560608
train_iter_loss: 0.27116817235946655
train_iter_loss: 0.4004075527191162
train_iter_loss: 0.2572806775569916
train_iter_loss: 0.27472031116485596
train_iter_loss: 0.31305381655693054
train_iter_loss: 0.2331112027168274
train_iter_loss: 0.38952967524528503
train_iter_loss: 0.4192523658275604
train_iter_loss: 0.27999618649482727
train_iter_loss: 0.26164746284484863
train_iter_loss: 0.3009716272354126
train_iter_loss: 0.29232102632522583
train_iter_loss: 0.3072015643119812
train_iter_loss: 0.31856685876846313
train_iter_loss: 0.1788450926542282
train_iter_loss: 0.37690994143486023
train_iter_loss: 0.30179086327552795
train_iter_loss: 0.39691025018692017
train_iter_loss: 0.26707473397254944
train_iter_loss: 0.25311022996902466
train_iter_loss: 0.2736033499240875
train_iter_loss: 0.20507468283176422
train_iter_loss: 0.20285317301750183
train_iter_loss: 0.4342673718929291
train_iter_loss: 0.12356287986040115
train_iter_loss: 0.534542441368103
train_iter_loss: 0.13138289749622345
train_iter_loss: 0.16223792731761932
train_iter_loss: 0.23200562596321106
train_iter_loss: 0.39479362964630127
train_iter_loss: 0.2638534605503082
train_iter_loss: 0.5800873637199402
train_iter_loss: 0.1549685001373291
train_iter_loss: 0.5371366739273071
train_iter_loss: 0.27938276529312134
train_iter_loss: 0.22942379117012024
train_iter_loss: 0.18333253264427185
train_iter_loss: 0.13040582835674286
train_iter_loss: 0.18121300637722015
train_iter_loss: 0.2696053087711334
train_iter_loss: 0.3281460702419281
train_iter_loss: 0.19493959844112396
train_iter_loss: 0.220625638961792
train_iter_loss: 0.4517100751399994
train_iter_loss: 0.22295480966567993
train_iter_loss: 0.36882680654525757
train_iter_loss: 0.2705698013305664
train_iter_loss: 0.2969823479652405
train_iter_loss: 0.2533985376358032
train_iter_loss: 0.13008002936840057
train_iter_loss: 0.16047556698322296
train_iter_loss: 0.1663326919078827
train_iter_loss: 0.21999062597751617
train_iter_loss: 0.16476835310459137
train_iter_loss: 0.3294874131679535
train_iter_loss: 0.1553221493959427
train_iter_loss: 0.26181408762931824
train loss :0.2800
---------------------
Validation seg loss: 0.3498144876387603 at epoch 475
epoch =    476/  1000, exp = train
train_iter_loss: 0.2962549924850464
train_iter_loss: 0.18329864740371704
train_iter_loss: 0.31914496421813965
train_iter_loss: 0.20463381707668304
train_iter_loss: 0.1917431354522705
train_iter_loss: 0.39450570940971375
train_iter_loss: 0.34230250120162964
train_iter_loss: 0.14266979694366455
train_iter_loss: 0.2800712585449219
train_iter_loss: 0.45300599932670593
train_iter_loss: 0.30436503887176514
train_iter_loss: 0.2735145688056946
train_iter_loss: 0.2788551449775696
train_iter_loss: 0.15548940002918243
train_iter_loss: 0.2684115469455719
train_iter_loss: 0.22390343248844147
train_iter_loss: 0.25532373785972595
train_iter_loss: 0.25684112310409546
train_iter_loss: 0.3478948473930359
train_iter_loss: 0.2892516851425171
train_iter_loss: 0.38391241431236267
train_iter_loss: 0.27340054512023926
train_iter_loss: 0.12345097213983536
train_iter_loss: 0.18278194963932037
train_iter_loss: 0.0866798609495163
train_iter_loss: 0.2521175146102905
train_iter_loss: 0.17756325006484985
train_iter_loss: 0.126484677195549
train_iter_loss: 0.4207972586154938
train_iter_loss: 0.17914028465747833
train_iter_loss: 0.2223060578107834
train_iter_loss: 0.35518813133239746
train_iter_loss: 0.31425386667251587
train_iter_loss: 0.2381485104560852
train_iter_loss: 0.277081161737442
train_iter_loss: 0.43487975001335144
train_iter_loss: 0.17513321340084076
train_iter_loss: 0.3802320063114166
train_iter_loss: 0.26232731342315674
train_iter_loss: 0.05976545438170433
train_iter_loss: 0.36196666955947876
train_iter_loss: 0.40288496017456055
train_iter_loss: 0.19453111290931702
train_iter_loss: 0.16733670234680176
train_iter_loss: 0.40069296956062317
train_iter_loss: 0.2251742035150528
train_iter_loss: 0.23076488077640533
train_iter_loss: 0.12433676421642303
train_iter_loss: 0.3655260503292084
train_iter_loss: 0.31056737899780273
train_iter_loss: 0.4488973617553711
train_iter_loss: 0.1925356388092041
train_iter_loss: 0.21088670194149017
train_iter_loss: 0.33444440364837646
train_iter_loss: 0.2553892433643341
train_iter_loss: 0.2682005763053894
train_iter_loss: 0.1369062215089798
train_iter_loss: 0.1299821436405182
train_iter_loss: 0.2663727402687073
train_iter_loss: 0.21684055030345917
train_iter_loss: 0.2549300789833069
train_iter_loss: 0.27247756719589233
train_iter_loss: 0.44883716106414795
train_iter_loss: 0.3571118414402008
train_iter_loss: 0.29647308588027954
train_iter_loss: 0.21055875718593597
train_iter_loss: 0.3470103442668915
train_iter_loss: 0.1525479406118393
train_iter_loss: 0.2412792444229126
train_iter_loss: 0.16249769926071167
train_iter_loss: 0.20320379734039307
train_iter_loss: 0.2875133156776428
train_iter_loss: 0.19634504616260529
train_iter_loss: 0.28972601890563965
train_iter_loss: 0.3797628581523895
train_iter_loss: 0.27532467246055603
train_iter_loss: 0.31975817680358887
train_iter_loss: 0.20953558385372162
train_iter_loss: 0.35019105672836304
train_iter_loss: 0.20031648874282837
train_iter_loss: 0.2539365887641907
train_iter_loss: 0.3608086407184601
train_iter_loss: 0.3559914231300354
train_iter_loss: 0.2265147864818573
train_iter_loss: 0.22718234360218048
train_iter_loss: 0.22175708413124084
train_iter_loss: 0.12677522003650665
train_iter_loss: 0.17287200689315796
train_iter_loss: 0.36533546447753906
train_iter_loss: 0.22576092183589935
train_iter_loss: 0.26243215799331665
train_iter_loss: 0.3505900800228119
train_iter_loss: 0.20724409818649292
train_iter_loss: 0.25556322932243347
train_iter_loss: 0.4285629391670227
train_iter_loss: 0.2803340554237366
train_iter_loss: 0.2322624921798706
train_iter_loss: 0.2370743602514267
train_iter_loss: 0.22749769687652588
train_iter_loss: 0.233530193567276
train loss :0.2662
---------------------
Validation seg loss: 0.4184576012369878 at epoch 476
epoch =    477/  1000, exp = train
train_iter_loss: 0.40178588032722473
train_iter_loss: 0.1991885006427765
train_iter_loss: 0.21870483458042145
train_iter_loss: 0.20896275341510773
train_iter_loss: 0.17332316935062408
train_iter_loss: 0.1950005441904068
train_iter_loss: 0.365497887134552
train_iter_loss: 0.3200555145740509
train_iter_loss: 0.3086826205253601
train_iter_loss: 0.3646976947784424
train_iter_loss: 0.38788077235221863
train_iter_loss: 0.3302425444126129
train_iter_loss: 0.41158556938171387
train_iter_loss: 0.17579296231269836
train_iter_loss: 0.2045210897922516
train_iter_loss: 0.362386554479599
train_iter_loss: 0.3517287075519562
train_iter_loss: 0.2760728895664215
train_iter_loss: 0.39461228251457214
train_iter_loss: 0.40046074986457825
train_iter_loss: 0.20000921189785004
train_iter_loss: 0.305829793214798
train_iter_loss: 0.2087973952293396
train_iter_loss: 0.280085027217865
train_iter_loss: 0.294664591550827
train_iter_loss: 0.3427824079990387
train_iter_loss: 0.2474983036518097
train_iter_loss: 0.32125917077064514
train_iter_loss: 0.20430049300193787
train_iter_loss: 0.24056538939476013
train_iter_loss: 0.38019227981567383
train_iter_loss: 0.2631194293498993
train_iter_loss: 0.1594488024711609
train_iter_loss: 0.48936179280281067
train_iter_loss: 0.3765465021133423
train_iter_loss: 0.26741018891334534
train_iter_loss: 0.18808628618717194
train_iter_loss: 0.27059662342071533
train_iter_loss: 0.3675295114517212
train_iter_loss: 0.11139398068189621
train_iter_loss: 0.455539733171463
train_iter_loss: 0.19810545444488525
train_iter_loss: 0.12867553532123566
train_iter_loss: 0.16412176191806793
train_iter_loss: 0.1244608610868454
train_iter_loss: 0.22894245386123657
train_iter_loss: 0.35702160000801086
train_iter_loss: 0.23609885573387146
train_iter_loss: 0.24563531577587128
train_iter_loss: 0.19305582344532013
train_iter_loss: 0.21104572713375092
train_iter_loss: 0.2318038046360016
train_iter_loss: 0.18544164299964905
train_iter_loss: 0.21040770411491394
train_iter_loss: 0.21019843220710754
train_iter_loss: 0.29363760352134705
train_iter_loss: 0.22908617556095123
train_iter_loss: 0.32744717597961426
train_iter_loss: 0.32495957612991333
train_iter_loss: 0.2521297335624695
train_iter_loss: 0.3329114615917206
train_iter_loss: 0.16306211054325104
train_iter_loss: 0.22414542734622955
train_iter_loss: 0.22806960344314575
train_iter_loss: 0.07348174601793289
train_iter_loss: 0.1768178939819336
train_iter_loss: 0.3308219015598297
train_iter_loss: 0.2569167912006378
train_iter_loss: 0.26527437567710876
train_iter_loss: 0.2923881411552429
train_iter_loss: 0.24136994779109955
train_iter_loss: 0.3000693619251251
train_iter_loss: 0.2684207856655121
train_iter_loss: 0.23716948926448822
train_iter_loss: 0.3213236629962921
train_iter_loss: 0.2023676037788391
train_iter_loss: 0.26774105429649353
train_iter_loss: 0.2192610204219818
train_iter_loss: 0.23765535652637482
train_iter_loss: 0.21299220621585846
train_iter_loss: 0.2663639485836029
train_iter_loss: 0.13957983255386353
train_iter_loss: 0.1250178962945938
train_iter_loss: 0.21871908009052277
train_iter_loss: 0.2917155921459198
train_iter_loss: 0.2844677269458771
train_iter_loss: 0.1326896846294403
train_iter_loss: 0.2871137261390686
train_iter_loss: 0.26194050908088684
train_iter_loss: 0.31451213359832764
train_iter_loss: 0.2093532532453537
train_iter_loss: 0.2515609860420227
train_iter_loss: 0.2857411801815033
train_iter_loss: 0.33683890104293823
train_iter_loss: 0.24623771011829376
train_iter_loss: 0.28344398736953735
train_iter_loss: 0.2614166736602783
train_iter_loss: 0.33488544821739197
train_iter_loss: 0.2497088611125946
train_iter_loss: 0.25679802894592285
train loss :0.2654
---------------------
Validation seg loss: 0.3690040565253991 at epoch 477
epoch =    478/  1000, exp = train
train_iter_loss: 0.15659497678279877
train_iter_loss: 0.15346848964691162
train_iter_loss: 0.13529731333255768
train_iter_loss: 0.3017103970050812
train_iter_loss: 0.36226096749305725
train_iter_loss: 0.22082148492336273
train_iter_loss: 0.22389818727970123
train_iter_loss: 0.4362712502479553
train_iter_loss: 0.1323636919260025
train_iter_loss: 0.22235381603240967
train_iter_loss: 0.3147209584712982
train_iter_loss: 0.26875433325767517
train_iter_loss: 0.3805399537086487
train_iter_loss: 0.32201650738716125
train_iter_loss: 0.34983208775520325
train_iter_loss: 0.5936955213546753
train_iter_loss: 0.09815015643835068
train_iter_loss: 0.252898246049881
train_iter_loss: 0.14887799322605133
train_iter_loss: 0.22695621848106384
train_iter_loss: 0.2626221776008606
train_iter_loss: 0.3006833791732788
train_iter_loss: 0.39384791254997253
train_iter_loss: 0.16672171652317047
train_iter_loss: 0.24430006742477417
train_iter_loss: 0.182694211602211
train_iter_loss: 0.3699861168861389
train_iter_loss: 0.20122317969799042
train_iter_loss: 0.2873179614543915
train_iter_loss: 0.26562538743019104
train_iter_loss: 0.27303338050842285
train_iter_loss: 0.17643539607524872
train_iter_loss: 0.35019075870513916
train_iter_loss: 0.24445584416389465
train_iter_loss: 0.30279219150543213
train_iter_loss: 0.3480375409126282
train_iter_loss: 0.20799174904823303
train_iter_loss: 0.17821434140205383
train_iter_loss: 0.19187667965888977
train_iter_loss: 0.4797848165035248
train_iter_loss: 0.33857765793800354
train_iter_loss: 0.26350733637809753
train_iter_loss: 0.3749426603317261
train_iter_loss: 0.2015731781721115
train_iter_loss: 0.2323487550020218
train_iter_loss: 0.36177054047584534
train_iter_loss: 0.38864612579345703
train_iter_loss: 0.3556486964225769
train_iter_loss: 0.180650532245636
train_iter_loss: 0.2055520862340927
train_iter_loss: 0.29658910632133484
train_iter_loss: 0.2802301049232483
train_iter_loss: 0.3563694953918457
train_iter_loss: 0.23641349375247955
train_iter_loss: 0.3208922743797302
train_iter_loss: 0.3376297652721405
train_iter_loss: 0.14278560876846313
train_iter_loss: 0.2567465007305145
train_iter_loss: 0.35531187057495117
train_iter_loss: 0.10579517483711243
train_iter_loss: 0.4209331274032593
train_iter_loss: 0.2381267547607422
train_iter_loss: 0.3067513406276703
train_iter_loss: 0.2036835104227066
train_iter_loss: 0.23797480762004852
train_iter_loss: 0.2924704849720001
train_iter_loss: 0.20904755592346191
train_iter_loss: 0.2583981156349182
train_iter_loss: 0.3080492317676544
train_iter_loss: 0.26762041449546814
train_iter_loss: 0.33429378271102905
train_iter_loss: 0.3297159671783447
train_iter_loss: 0.332375168800354
train_iter_loss: 0.3249935507774353
train_iter_loss: 0.25358864665031433
train_iter_loss: 0.22829046845436096
train_iter_loss: 0.169997438788414
train_iter_loss: 0.2575564384460449
train_iter_loss: 0.32004302740097046
train_iter_loss: 0.33388420939445496
train_iter_loss: 0.4315377473831177
train_iter_loss: 0.48585548996925354
train_iter_loss: 0.16053621470928192
train_iter_loss: 0.4431565999984741
train_iter_loss: 0.2955667972564697
train_iter_loss: 0.2854662835597992
train_iter_loss: 0.16477195918560028
train_iter_loss: 0.21096482872962952
train_iter_loss: 0.23227889835834503
train_iter_loss: 0.3344421088695526
train_iter_loss: 0.24539729952812195
train_iter_loss: 0.19601444900035858
train_iter_loss: 0.2274981290102005
train_iter_loss: 0.21793803572654724
train_iter_loss: 0.30261462926864624
train_iter_loss: 0.292227178812027
train_iter_loss: 0.3194846510887146
train_iter_loss: 0.27541396021842957
train_iter_loss: 0.18076825141906738
train_iter_loss: 0.2070450633764267
train loss :0.2772
---------------------
Validation seg loss: 0.3494330016228388 at epoch 478
********************
best_val_epoch_loss:  0.3494330016228388
MODEL UPDATED
epoch =    479/  1000, exp = train
train_iter_loss: 0.20848116278648376
train_iter_loss: 0.223307266831398
train_iter_loss: 0.2751511335372925
train_iter_loss: 0.10394039005041122
train_iter_loss: 0.20927704870700836
train_iter_loss: 0.09898370504379272
train_iter_loss: 0.33853471279144287
train_iter_loss: 0.2523404359817505
train_iter_loss: 0.3456129729747772
train_iter_loss: 0.287913054227829
train_iter_loss: 0.2778078019618988
train_iter_loss: 0.290689080953598
train_iter_loss: 0.5071777105331421
train_iter_loss: 0.0891471803188324
train_iter_loss: 0.3027150630950928
train_iter_loss: 0.29292500019073486
train_iter_loss: 0.22454629838466644
train_iter_loss: 0.15234024822711945
train_iter_loss: 0.2322581261396408
train_iter_loss: 0.14876006543636322
train_iter_loss: 0.38242107629776
train_iter_loss: 0.3726534843444824
train_iter_loss: 0.19946570694446564
train_iter_loss: 0.15572568774223328
train_iter_loss: 0.21305127441883087
train_iter_loss: 0.3207947313785553
train_iter_loss: 0.28426167368888855
train_iter_loss: 0.22925856709480286
train_iter_loss: 0.2527739107608795
train_iter_loss: 0.2709636092185974
train_iter_loss: 0.35514333844184875
train_iter_loss: 0.15295864641666412
train_iter_loss: 0.36901792883872986
train_iter_loss: 0.42486733198165894
train_iter_loss: 0.308393269777298
train_iter_loss: 0.2631152272224426
train_iter_loss: 0.2637633681297302
train_iter_loss: 0.18023774027824402
train_iter_loss: 0.17195628583431244
train_iter_loss: 0.13795897364616394
train_iter_loss: 0.32399630546569824
train_iter_loss: 0.3617057204246521
train_iter_loss: 0.295380175113678
train_iter_loss: 0.4091876149177551
train_iter_loss: 0.23388446867465973
train_iter_loss: 0.206789031624794
train_iter_loss: 0.16529616713523865
train_iter_loss: 0.33246874809265137
train_iter_loss: 0.36316439509391785
train_iter_loss: 0.23700273036956787
train_iter_loss: 0.2809988558292389
train_iter_loss: 0.33981987833976746
train_iter_loss: 0.3257998526096344
train_iter_loss: 0.3419606685638428
train_iter_loss: 0.16359873116016388
train_iter_loss: 0.34557780623435974
train_iter_loss: 0.24449609220027924
train_iter_loss: 0.311531662940979
train_iter_loss: 0.3543263375759125
train_iter_loss: 0.28862184286117554
train_iter_loss: 0.2681486904621124
train_iter_loss: 0.23263850808143616
train_iter_loss: 0.24256546795368195
train_iter_loss: 0.17630214989185333
train_iter_loss: 0.26735466718673706
train_iter_loss: 0.2640693783760071
train_iter_loss: 0.42488205432891846
train_iter_loss: 0.3982877731323242
train_iter_loss: 0.25964275002479553
train_iter_loss: 0.1646675318479538
train_iter_loss: 0.4855298697948456
train_iter_loss: 0.19476918876171112
train_iter_loss: 0.30792635679244995
train_iter_loss: 0.30188703536987305
train_iter_loss: 0.2235412448644638
train_iter_loss: 0.26194924116134644
train_iter_loss: 0.145213320851326
train_iter_loss: 0.33994439244270325
train_iter_loss: 0.22808091342449188
train_iter_loss: 0.3643397092819214
train_iter_loss: 0.2957019805908203
train_iter_loss: 0.11333906650543213
train_iter_loss: 0.25742003321647644
train_iter_loss: 0.2838446795940399
train_iter_loss: 0.2989101707935333
train_iter_loss: 0.41471120715141296
train_iter_loss: 0.3964308202266693
train_iter_loss: 0.2434786856174469
train_iter_loss: 0.21557338535785675
train_iter_loss: 0.04432477802038193
train_iter_loss: 0.21046385169029236
train_iter_loss: 0.2820330858230591
train_iter_loss: 0.38124874234199524
train_iter_loss: 0.2597511112689972
train_iter_loss: 0.1249072477221489
train_iter_loss: 0.2247570902109146
train_iter_loss: 0.26981353759765625
train_iter_loss: 0.22074732184410095
train_iter_loss: 0.45728129148483276
train_iter_loss: 0.30937597155570984
train loss :0.2723
---------------------
Validation seg loss: 0.38284226932633175 at epoch 479
epoch =    480/  1000, exp = train
train_iter_loss: 0.2808278501033783
train_iter_loss: 0.1907077133655548
train_iter_loss: 0.23033401370048523
train_iter_loss: 0.15094253420829773
train_iter_loss: 0.059060994535684586
train_iter_loss: 0.16090108454227448
train_iter_loss: 0.23867854475975037
train_iter_loss: 0.08768915385007858
train_iter_loss: 0.30683669447898865
train_iter_loss: 0.27940088510513306
train_iter_loss: 0.33505979180336
train_iter_loss: 0.21823382377624512
train_iter_loss: 0.24300163984298706
train_iter_loss: 0.12044777721166611
train_iter_loss: 0.22051924467086792
train_iter_loss: 0.22438549995422363
train_iter_loss: 0.2817595899105072
train_iter_loss: 0.3287029564380646
train_iter_loss: 0.22710056602954865
train_iter_loss: 0.2670438885688782
train_iter_loss: 0.5639663934707642
train_iter_loss: 0.2973160445690155
train_iter_loss: 0.28763219714164734
train_iter_loss: 0.13094745576381683
train_iter_loss: 0.26700451970100403
train_iter_loss: 0.25786280632019043
train_iter_loss: 0.34088119864463806
train_iter_loss: 0.2868686020374298
train_iter_loss: 0.16077807545661926
train_iter_loss: 0.40816158056259155
train_iter_loss: 0.33475202322006226
train_iter_loss: 0.41904687881469727
train_iter_loss: 0.3257903456687927
train_iter_loss: 0.24170590937137604
train_iter_loss: 0.22602784633636475
train_iter_loss: 0.3842375874519348
train_iter_loss: 0.18400342762470245
train_iter_loss: 0.3800589144229889
train_iter_loss: 0.1894969493150711
train_iter_loss: 0.3537481129169464
train_iter_loss: 0.284336119890213
train_iter_loss: 0.31557637453079224
train_iter_loss: 0.3272162675857544
train_iter_loss: 0.1975357085466385
train_iter_loss: 0.24592828750610352
train_iter_loss: 0.2649007737636566
train_iter_loss: 0.21818870306015015
train_iter_loss: 0.2549452483654022
train_iter_loss: 0.3025987446308136
train_iter_loss: 0.33594152331352234
train_iter_loss: 0.20728984475135803
train_iter_loss: 0.32673317193984985
train_iter_loss: 0.3120068609714508
train_iter_loss: 0.29233700037002563
train_iter_loss: 0.21020428836345673
train_iter_loss: 0.3014204204082489
train_iter_loss: 0.25656193494796753
train_iter_loss: 0.291968435049057
train_iter_loss: 0.1795719712972641
train_iter_loss: 0.21070615947246552
train_iter_loss: 0.2255474179983139
train_iter_loss: 0.28276383876800537
train_iter_loss: 0.2659279406070709
train_iter_loss: 0.33475035429000854
train_iter_loss: 0.32617929577827454
train_iter_loss: 0.3695613443851471
train_iter_loss: 0.19074393808841705
train_iter_loss: 0.38682615756988525
train_iter_loss: 0.28619033098220825
train_iter_loss: 0.39307019114494324
train_iter_loss: 0.3786299228668213
train_iter_loss: 0.22006087005138397
train_iter_loss: 0.24026469886302948
train_iter_loss: 0.27830228209495544
train_iter_loss: 0.2793451249599457
train_iter_loss: 0.20867857336997986
train_iter_loss: 0.30015549063682556
train_iter_loss: 0.3085717260837555
train_iter_loss: 0.214136004447937
train_iter_loss: 0.2849414050579071
train_iter_loss: 0.2045312374830246
train_iter_loss: 0.21314135193824768
train_iter_loss: 0.22935570776462555
train_iter_loss: 0.14015519618988037
train_iter_loss: 0.363227903842926
train_iter_loss: 0.18201442062854767
train_iter_loss: 0.1661314219236374
train_iter_loss: 0.2443004846572876
train_iter_loss: 0.31973397731781006
train_iter_loss: 0.29923275113105774
train_iter_loss: 0.1898648738861084
train_iter_loss: 0.11915326863527298
train_iter_loss: 0.16405706107616425
train_iter_loss: 0.15184848010540009
train_iter_loss: 0.3034520447254181
train_iter_loss: 0.3426826000213623
train_iter_loss: 0.2814333438873291
train_iter_loss: 0.29027625918388367
train_iter_loss: 0.2806561589241028
train_iter_loss: 0.37408122420310974
train loss :0.2673
---------------------
Validation seg loss: 0.35685872516752976 at epoch 480
epoch =    481/  1000, exp = train
train_iter_loss: 0.22771351039409637
train_iter_loss: 0.2865471839904785
train_iter_loss: 0.15355831384658813
train_iter_loss: 0.37872931361198425
train_iter_loss: 0.27650347352027893
train_iter_loss: 0.14606168866157532
train_iter_loss: 0.28559762239456177
train_iter_loss: 0.29827049374580383
train_iter_loss: 0.10133006423711777
train_iter_loss: 0.4349135756492615
train_iter_loss: 0.3605995774269104
train_iter_loss: 0.36671575903892517
train_iter_loss: 0.24094046652317047
train_iter_loss: 0.1994788944721222
train_iter_loss: 0.25139904022216797
train_iter_loss: 0.18032288551330566
train_iter_loss: 0.24810828268527985
train_iter_loss: 0.16099496185779572
train_iter_loss: 0.10769820958375931
train_iter_loss: 0.34178730845451355
train_iter_loss: 0.4505995810031891
train_iter_loss: 0.19461920857429504
train_iter_loss: 0.15597772598266602
train_iter_loss: 0.13844826817512512
train_iter_loss: 0.21899394690990448
train_iter_loss: 0.3576851189136505
train_iter_loss: 0.1274702101945877
train_iter_loss: 0.22489000856876373
train_iter_loss: 0.1508442908525467
train_iter_loss: 0.47042760252952576
train_iter_loss: 0.1824893355369568
train_iter_loss: 0.308438777923584
train_iter_loss: 0.1665702909231186
train_iter_loss: 0.37354856729507446
train_iter_loss: 0.2808573842048645
train_iter_loss: 0.2037685215473175
train_iter_loss: 0.1580796241760254
train_iter_loss: 0.4140348434448242
train_iter_loss: 0.37355881929397583
train_iter_loss: 0.3761928677558899
train_iter_loss: 0.29651400446891785
train_iter_loss: 0.3191916346549988
train_iter_loss: 0.32406166195869446
train_iter_loss: 0.38158294558525085
train_iter_loss: 0.27802225947380066
train_iter_loss: 0.34882017970085144
train_iter_loss: 0.36634987592697144
train_iter_loss: 0.3169960677623749
train_iter_loss: 0.2788182199001312
train_iter_loss: 0.2337498664855957
train_iter_loss: 0.13986867666244507
train_iter_loss: 0.3485347330570221
train_iter_loss: 0.21227756142616272
train_iter_loss: 0.27761197090148926
train_iter_loss: 0.34537845849990845
train_iter_loss: 0.5194433331489563
train_iter_loss: 0.1906214952468872
train_iter_loss: 0.26787668466567993
train_iter_loss: 0.29892024397850037
train_iter_loss: 0.39077091217041016
train_iter_loss: 0.2670156955718994
train_iter_loss: 0.2920204699039459
train_iter_loss: 0.38360634446144104
train_iter_loss: 0.35383158922195435
train_iter_loss: 0.18346278369426727
train_iter_loss: 0.24124775826931
train_iter_loss: 0.19216598570346832
train_iter_loss: 0.15403984487056732
train_iter_loss: 0.4184598922729492
train_iter_loss: 0.24418073892593384
train_iter_loss: 0.28722143173217773
train_iter_loss: 0.19939330220222473
train_iter_loss: 0.23184055089950562
train_iter_loss: 0.2792817950248718
train_iter_loss: 0.3381752669811249
train_iter_loss: 0.19401733577251434
train_iter_loss: 0.23166078329086304
train_iter_loss: 0.19061718881130219
train_iter_loss: 0.30300816893577576
train_iter_loss: 0.3921821415424347
train_iter_loss: 0.3426128923892975
train_iter_loss: 0.24610623717308044
train_iter_loss: 0.2748221457004547
train_iter_loss: 0.27076253294944763
train_iter_loss: 0.2864508628845215
train_iter_loss: 0.12683118879795074
train_iter_loss: 0.2930947244167328
train_iter_loss: 0.41366103291511536
train_iter_loss: 0.35026103258132935
train_iter_loss: 0.20669801533222198
train_iter_loss: 0.2034958004951477
train_iter_loss: 0.4111992120742798
train_iter_loss: 0.28350332379341125
train_iter_loss: 0.23867467045783997
train_iter_loss: 0.29904210567474365
train_iter_loss: 0.09307035058736801
train_iter_loss: 0.2836609482765198
train_iter_loss: 0.19318389892578125
train_iter_loss: 0.3597777783870697
train_iter_loss: 0.20300841331481934
train loss :0.2753
---------------------
Validation seg loss: 0.35935210777481774 at epoch 481
epoch =    482/  1000, exp = train
train_iter_loss: 0.24860453605651855
train_iter_loss: 0.23150920867919922
train_iter_loss: 0.3973035514354706
train_iter_loss: 0.33123186230659485
train_iter_loss: 0.25794461369514465
train_iter_loss: 0.3278287351131439
train_iter_loss: 0.21801157295703888
train_iter_loss: 0.261686772108078
train_iter_loss: 0.36257198452949524
train_iter_loss: 0.32909852266311646
train_iter_loss: 0.20361819863319397
train_iter_loss: 0.07289562374353409
train_iter_loss: 0.19999849796295166
train_iter_loss: 0.40831106901168823
train_iter_loss: 0.20948320627212524
train_iter_loss: 0.2531253695487976
train_iter_loss: 0.4232918322086334
train_iter_loss: 0.3115610182285309
train_iter_loss: 0.25459882616996765
train_iter_loss: 0.2970256805419922
train_iter_loss: 0.12912113964557648
train_iter_loss: 0.10216084867715836
train_iter_loss: 0.24758334457874298
train_iter_loss: 0.32528451085090637
train_iter_loss: 0.17274914681911469
train_iter_loss: 0.4598744213581085
train_iter_loss: 0.3297348916530609
train_iter_loss: 0.197951540350914
train_iter_loss: 0.38166674971580505
train_iter_loss: 0.28453004360198975
train_iter_loss: 0.16473601758480072
train_iter_loss: 0.31850990653038025
train_iter_loss: 0.3003695011138916
train_iter_loss: 0.20998336374759674
train_iter_loss: 0.1043580174446106
train_iter_loss: 0.2732365131378174
train_iter_loss: 0.16735991835594177
train_iter_loss: 0.21565572917461395
train_iter_loss: 0.43630659580230713
train_iter_loss: 0.2862287163734436
train_iter_loss: 0.3154633939266205
train_iter_loss: 0.16675958037376404
train_iter_loss: 0.3420572876930237
train_iter_loss: 0.2587188482284546
train_iter_loss: 0.2905077040195465
train_iter_loss: 0.2636691927909851
train_iter_loss: 0.18990293145179749
train_iter_loss: 0.30390408635139465
train_iter_loss: 0.2374945878982544
train_iter_loss: 0.27738216519355774
train_iter_loss: 0.18958735466003418
train_iter_loss: 0.3107360005378723
train_iter_loss: 0.4194551110267639
train_iter_loss: 0.31419637799263
train_iter_loss: 0.13951678574085236
train_iter_loss: 0.30919620394706726
train_iter_loss: 0.34980571269989014
train_iter_loss: 0.21444885432720184
train_iter_loss: 0.29528725147247314
train_iter_loss: 0.3324314057826996
train_iter_loss: 0.42380785942077637
train_iter_loss: 0.34651532769203186
train_iter_loss: 0.28673747181892395
train_iter_loss: 0.30757248401641846
train_iter_loss: 0.18674422800540924
train_iter_loss: 0.27390721440315247
train_iter_loss: 0.18718361854553223
train_iter_loss: 0.17519263923168182
train_iter_loss: 0.3073032796382904
train_iter_loss: 0.4294368326663971
train_iter_loss: 0.4062618911266327
train_iter_loss: 0.33061543107032776
train_iter_loss: 0.24273565411567688
train_iter_loss: 0.2798948287963867
train_iter_loss: 0.2627513110637665
train_iter_loss: 0.22503404319286346
train_iter_loss: 0.24732866883277893
train_iter_loss: 0.37697359919548035
train_iter_loss: 0.2293032556772232
train_iter_loss: 0.1816166341304779
train_iter_loss: 0.3514840602874756
train_iter_loss: 0.45628654956817627
train_iter_loss: 0.2458304613828659
train_iter_loss: 0.4271182119846344
train_iter_loss: 0.3132767379283905
train_iter_loss: 0.2020416259765625
train_iter_loss: 0.09488628059625626
train_iter_loss: 0.12043096870183945
train_iter_loss: 0.2878837287425995
train_iter_loss: 0.354403018951416
train_iter_loss: 0.26515015959739685
train_iter_loss: 0.21416038274765015
train_iter_loss: 0.2084459662437439
train_iter_loss: 0.19633124768733978
train_iter_loss: 0.2746483087539673
train_iter_loss: 0.23350028693675995
train_iter_loss: 0.21453942358493805
train_iter_loss: 0.2814253866672516
train_iter_loss: 0.2200356423854828
train_iter_loss: 0.20286758244037628
train loss :0.2737
---------------------
Validation seg loss: 0.35636038481662013 at epoch 482
epoch =    483/  1000, exp = train
train_iter_loss: 0.2177291363477707
train_iter_loss: 0.2227851301431656
train_iter_loss: 0.22920896112918854
train_iter_loss: 0.3027609884738922
train_iter_loss: 0.14199428260326385
train_iter_loss: 0.3733082413673401
train_iter_loss: 0.26377981901168823
train_iter_loss: 0.13392490148544312
train_iter_loss: 0.19282770156860352
train_iter_loss: 0.36923253536224365
train_iter_loss: 0.3363115191459656
train_iter_loss: 0.18748924136161804
train_iter_loss: 0.32418885827064514
train_iter_loss: 0.28771573305130005
train_iter_loss: 0.3271457254886627
train_iter_loss: 0.2775396406650543
train_iter_loss: 0.25773561000823975
train_iter_loss: 0.3458186686038971
train_iter_loss: 0.3232688009738922
train_iter_loss: 0.4063369035720825
train_iter_loss: 0.11176198720932007
train_iter_loss: 0.23482777178287506
train_iter_loss: 0.3165661096572876
train_iter_loss: 0.3053239583969116
train_iter_loss: 0.1732141375541687
train_iter_loss: 0.1828266680240631
train_iter_loss: 0.28940707445144653
train_iter_loss: 0.28499725461006165
train_iter_loss: 0.31173720955848694
train_iter_loss: 0.2323637753725052
train_iter_loss: 0.3338499069213867
train_iter_loss: 0.22188150882720947
train_iter_loss: 0.22600162029266357
train_iter_loss: 0.17234604060649872
train_iter_loss: 0.18005689978599548
train_iter_loss: 0.1597127467393875
train_iter_loss: 0.4285956621170044
train_iter_loss: 0.16258253157138824
train_iter_loss: 0.24127522110939026
train_iter_loss: 0.23040147125720978
train_iter_loss: 0.29737550020217896
train_iter_loss: 0.36645764112472534
train_iter_loss: 0.3380897641181946
train_iter_loss: 0.20733404159545898
train_iter_loss: 0.24731987714767456
train_iter_loss: 0.1834453046321869
train_iter_loss: 0.348131388425827
train_iter_loss: 0.32524600625038147
train_iter_loss: 0.28667062520980835
train_iter_loss: 0.2064482867717743
train_iter_loss: 0.18105632066726685
train_iter_loss: 0.16116589307785034
train_iter_loss: 0.0974399670958519
train_iter_loss: 0.27008000016212463
train_iter_loss: 0.32640326023101807
train_iter_loss: 0.31511443853378296
train_iter_loss: 0.19155064225196838
train_iter_loss: 0.06462356448173523
train_iter_loss: 0.21839025616645813
train_iter_loss: 0.1897376924753189
train_iter_loss: 0.3428981602191925
train_iter_loss: 0.2651500105857849
train_iter_loss: 0.3829313814640045
train_iter_loss: 0.301950603723526
train_iter_loss: 0.27965742349624634
train_iter_loss: 0.39109286665916443
train_iter_loss: 0.2710435390472412
train_iter_loss: 0.41414982080459595
train_iter_loss: 0.33814212679862976
train_iter_loss: 0.27452176809310913
train_iter_loss: 0.3296414315700531
train_iter_loss: 0.5644906759262085
train_iter_loss: 0.24685831367969513
train_iter_loss: 0.251741498708725
train_iter_loss: 0.35112449526786804
train_iter_loss: 0.18411032855510712
train_iter_loss: 0.11322320252656937
train_iter_loss: 0.19274680316448212
train_iter_loss: 0.1871623694896698
train_iter_loss: 0.13077498972415924
train_iter_loss: 0.22663863003253937
train_iter_loss: 0.27388298511505127
train_iter_loss: 0.34926167130470276
train_iter_loss: 0.32023414969444275
train_iter_loss: 0.3205847442150116
train_iter_loss: 0.47587907314300537
train_iter_loss: 0.22034789621829987
train_iter_loss: 0.13734070956707
train_iter_loss: 0.25149381160736084
train_iter_loss: 0.3014165461063385
train_iter_loss: 0.21036814153194427
train_iter_loss: 0.3233163058757782
train_iter_loss: 0.2992272973060608
train_iter_loss: 0.12523503601551056
train_iter_loss: 0.34353554248809814
train_iter_loss: 0.3156406581401825
train_iter_loss: 0.28833431005477905
train_iter_loss: 0.23610174655914307
train_iter_loss: 0.19840867817401886
train_iter_loss: 0.33421194553375244
train loss :0.2678
---------------------
Validation seg loss: 0.38208389135499327 at epoch 483
epoch =    484/  1000, exp = train
train_iter_loss: 0.3396185338497162
train_iter_loss: 0.2424100637435913
train_iter_loss: 0.3519137501716614
train_iter_loss: 0.33675476908683777
train_iter_loss: 0.5257182717323303
train_iter_loss: 0.31377801299095154
train_iter_loss: 0.14575861394405365
train_iter_loss: 0.12920494377613068
train_iter_loss: 0.31359049677848816
train_iter_loss: 0.19249071180820465
train_iter_loss: 0.26635149121284485
train_iter_loss: 0.24720291793346405
train_iter_loss: 0.22255340218544006
train_iter_loss: 0.19623509049415588
train_iter_loss: 0.2412366271018982
train_iter_loss: 0.3114608824253082
train_iter_loss: 0.1985025256872177
train_iter_loss: 0.29344168305397034
train_iter_loss: 0.4541080892086029
train_iter_loss: 0.33095449209213257
train_iter_loss: 0.2811456024646759
train_iter_loss: 0.21163471043109894
train_iter_loss: 0.377307653427124
train_iter_loss: 0.3291197419166565
train_iter_loss: 0.2833496034145355
train_iter_loss: 0.34716349840164185
train_iter_loss: 0.3036540746688843
train_iter_loss: 0.25048404932022095
train_iter_loss: 0.27413445711135864
train_iter_loss: 0.2419067770242691
train_iter_loss: 0.3055655062198639
train_iter_loss: 0.22808396816253662
train_iter_loss: 0.2708335518836975
train_iter_loss: 0.1806274652481079
train_iter_loss: 0.3320442736148834
train_iter_loss: 0.38324910402297974
train_iter_loss: 0.2907850742340088
train_iter_loss: 0.15010525286197662
train_iter_loss: 0.2221919745206833
train_iter_loss: 0.2526180148124695
train_iter_loss: 0.23619110882282257
train_iter_loss: 0.29797595739364624
train_iter_loss: 0.20162364840507507
train_iter_loss: 0.24930933117866516
train_iter_loss: 0.18608202040195465
train_iter_loss: 0.39131519198417664
train_iter_loss: 0.29383471608161926
train_iter_loss: 0.35114872455596924
train_iter_loss: 0.32037127017974854
train_iter_loss: 0.129071444272995
train_iter_loss: 0.22729678452014923
train_iter_loss: 0.2631082236766815
train_iter_loss: 0.22303910553455353
train_iter_loss: 0.26909664273262024
train_iter_loss: 0.41819754242897034
train_iter_loss: 0.18608038127422333
train_iter_loss: 0.3470803201198578
train_iter_loss: 0.3290824294090271
train_iter_loss: 0.20542111992835999
train_iter_loss: 0.3566366136074066
train_iter_loss: 0.2801860570907593
train_iter_loss: 0.14967213571071625
train_iter_loss: 0.163677379488945
train_iter_loss: 0.32923492789268494
train_iter_loss: 0.08060115575790405
train_iter_loss: 0.08801588416099548
train_iter_loss: 0.31037893891334534
train_iter_loss: 0.36337533593177795
train_iter_loss: 0.20610584318637848
train_iter_loss: 0.2506555914878845
train_iter_loss: 0.32591918110847473
train_iter_loss: 0.4017874002456665
train_iter_loss: 0.15710683166980743
train_iter_loss: 0.397819846868515
train_iter_loss: 0.3117985129356384
train_iter_loss: 0.30697953701019287
train_iter_loss: 0.06832949072122574
train_iter_loss: 0.08043806999921799
train_iter_loss: 0.3224732577800751
train_iter_loss: 0.23966512084007263
train_iter_loss: 0.3085026443004608
train_iter_loss: 0.27486321330070496
train_iter_loss: 0.20731425285339355
train_iter_loss: 0.30917593836784363
train_iter_loss: 0.24661748111248016
train_iter_loss: 0.0789971873164177
train_iter_loss: 0.3385918140411377
train_iter_loss: 0.3232678771018982
train_iter_loss: 0.3010820746421814
train_iter_loss: 0.1971907615661621
train_iter_loss: 0.199960395693779
train_iter_loss: 0.22974060475826263
train_iter_loss: 0.2710011303424835
train_iter_loss: 0.41907554864883423
train_iter_loss: 0.13414406776428223
train_iter_loss: 0.26340094208717346
train_iter_loss: 0.302949994802475
train_iter_loss: 0.30675995349884033
train_iter_loss: 0.26380348205566406
train_iter_loss: 0.26441892981529236
train loss :0.2699
---------------------
Validation seg loss: 0.4307467311770075 at epoch 484
epoch =    485/  1000, exp = train
train_iter_loss: 0.15744565427303314
train_iter_loss: 0.0989551991224289
train_iter_loss: 0.28041860461235046
train_iter_loss: 0.24025221168994904
train_iter_loss: 0.35488569736480713
train_iter_loss: 0.2109859138727188
train_iter_loss: 0.29085806012153625
train_iter_loss: 0.2788546085357666
train_iter_loss: 0.12386918067932129
train_iter_loss: 0.44766923785209656
train_iter_loss: 0.304375559091568
train_iter_loss: 0.3258858919143677
train_iter_loss: 0.23687657713890076
train_iter_loss: 0.25596052408218384
train_iter_loss: 0.37437009811401367
train_iter_loss: 0.26960885524749756
train_iter_loss: 0.25722092390060425
train_iter_loss: 0.17233143746852875
train_iter_loss: 0.338735967874527
train_iter_loss: 0.24553994834423065
train_iter_loss: 0.24739983677864075
train_iter_loss: 0.20009973645210266
train_iter_loss: 0.2148924320936203
train_iter_loss: 0.24964343011379242
train_iter_loss: 0.17033357918262482
train_iter_loss: 0.17721138894557953
train_iter_loss: 0.21446716785430908
train_iter_loss: 0.26565462350845337
train_iter_loss: 0.24708883464336395
train_iter_loss: 0.20391534268856049
train_iter_loss: 0.16455914080142975
train_iter_loss: 0.26934072375297546
train_iter_loss: 0.2449486404657364
train_iter_loss: 0.22306181490421295
train_iter_loss: 0.2665250897407532
train_iter_loss: 0.3234947919845581
train_iter_loss: 0.32064753770828247
train_iter_loss: 0.18932344019412994
train_iter_loss: 0.28670552372932434
train_iter_loss: 0.25494563579559326
train_iter_loss: 0.20669963955879211
train_iter_loss: 0.3214135468006134
train_iter_loss: 0.2549842596054077
train_iter_loss: 0.17265640199184418
train_iter_loss: 0.2447790652513504
train_iter_loss: 0.3376941978931427
train_iter_loss: 0.23726078867912292
train_iter_loss: 0.3185485303401947
train_iter_loss: 0.21389295160770416
train_iter_loss: 0.5632051229476929
train_iter_loss: 0.34329789876937866
train_iter_loss: 0.2790541648864746
train_iter_loss: 0.28724613785743713
train_iter_loss: 0.26722967624664307
train_iter_loss: 0.2999589145183563
train_iter_loss: 0.31140509247779846
train_iter_loss: 0.1917412281036377
train_iter_loss: 0.20534224808216095
train_iter_loss: 0.116884745657444
train_iter_loss: 0.17556189000606537
train_iter_loss: 0.17650434374809265
train_iter_loss: 0.48997658491134644
train_iter_loss: 0.149736687541008
train_iter_loss: 0.3806826174259186
train_iter_loss: 0.31342247128486633
train_iter_loss: 0.274815171957016
train_iter_loss: 0.3406570851802826
train_iter_loss: 0.2350430190563202
train_iter_loss: 0.32769066095352173
train_iter_loss: 0.27070003747940063
train_iter_loss: 0.3511728346347809
train_iter_loss: 0.4596113860607147
train_iter_loss: 0.37457075715065
train_iter_loss: 0.16778403520584106
train_iter_loss: 0.21646162867546082
train_iter_loss: 0.3146575391292572
train_iter_loss: 0.19893330335617065
train_iter_loss: 0.21445827186107635
train_iter_loss: 0.20073001086711884
train_iter_loss: 0.2686370015144348
train_iter_loss: 0.2537878453731537
train_iter_loss: 0.31217238306999207
train_iter_loss: 0.286099910736084
train_iter_loss: 0.20709393918514252
train_iter_loss: 0.13342295587062836
train_iter_loss: 0.3226000666618347
train_iter_loss: 0.1428791880607605
train_iter_loss: 0.31689679622650146
train_iter_loss: 0.17934073507785797
train_iter_loss: 0.24046877026557922
train_iter_loss: 0.3193458020687103
train_iter_loss: 0.2666538953781128
train_iter_loss: 0.43029898405075073
train_iter_loss: 0.10605417191982269
train_iter_loss: 0.26924368739128113
train_iter_loss: 0.251313179731369
train_iter_loss: 0.2693193256855011
train_iter_loss: 0.35994791984558105
train_iter_loss: 0.13819633424282074
train_iter_loss: 0.09691526740789413
train loss :0.2625
---------------------
Validation seg loss: 0.35487857840533527 at epoch 485
epoch =    486/  1000, exp = train
train_iter_loss: 0.4222361445426941
train_iter_loss: 0.3679148852825165
train_iter_loss: 0.31961292028427124
train_iter_loss: 0.2215680181980133
train_iter_loss: 0.1183508113026619
train_iter_loss: 0.2980354130268097
train_iter_loss: 0.3744550943374634
train_iter_loss: 0.3156805634498596
train_iter_loss: 0.27287498116493225
train_iter_loss: 0.20063894987106323
train_iter_loss: 0.42846032977104187
train_iter_loss: 0.23262299597263336
train_iter_loss: 0.29269373416900635
train_iter_loss: 0.21140305697917938
train_iter_loss: 0.32608410716056824
train_iter_loss: 0.2735271155834198
train_iter_loss: 0.3906346261501312
train_iter_loss: 0.3780651092529297
train_iter_loss: 0.29887956380844116
train_iter_loss: 0.42029452323913574
train_iter_loss: 0.3304176330566406
train_iter_loss: 0.3578685224056244
train_iter_loss: 0.1757129728794098
train_iter_loss: 0.3640873432159424
train_iter_loss: 0.3792303204536438
train_iter_loss: 0.19883480668067932
train_iter_loss: 0.3089653253555298
train_iter_loss: 0.2713855504989624
train_iter_loss: 0.3262063264846802
train_iter_loss: 0.1810658872127533
train_iter_loss: 0.31939807534217834
train_iter_loss: 0.2539936602115631
train_iter_loss: 0.33875247836112976
train_iter_loss: 0.27153152227401733
train_iter_loss: 0.25674429535865784
train_iter_loss: 0.2909125089645386
train_iter_loss: 0.2417413592338562
train_iter_loss: 0.2508116066455841
train_iter_loss: 0.19521205127239227
train_iter_loss: 0.3988557755947113
train_iter_loss: 0.4661371111869812
train_iter_loss: 0.38214370608329773
train_iter_loss: 0.3014216721057892
train_iter_loss: 0.12133194506168365
train_iter_loss: 0.2618104815483093
train_iter_loss: 0.18608272075653076
train_iter_loss: 0.33282244205474854
train_iter_loss: 0.42628446221351624
train_iter_loss: 0.1968478262424469
train_iter_loss: 0.3008681535720825
train_iter_loss: 0.23729854822158813
train_iter_loss: 0.2232527732849121
train_iter_loss: 0.30938050150871277
train_iter_loss: 0.24706821143627167
train_iter_loss: 0.34541580080986023
train_iter_loss: 0.19832797348499298
train_iter_loss: 0.3409464657306671
train_iter_loss: 0.32559099793434143
train_iter_loss: 0.24755294620990753
train_iter_loss: 0.3197130262851715
train_iter_loss: 0.25832945108413696
train_iter_loss: 0.2816535532474518
train_iter_loss: 0.2864783704280853
train_iter_loss: 0.26428332924842834
train_iter_loss: 0.2493748962879181
train_iter_loss: 0.4264409840106964
train_iter_loss: 0.2478221207857132
train_iter_loss: 0.09054799377918243
train_iter_loss: 0.15930110216140747
train_iter_loss: 0.2624342739582062
train_iter_loss: 0.026091357693076134
train_iter_loss: 0.1521155834197998
train_iter_loss: 0.23987163603305817
train_iter_loss: 0.37949562072753906
train_iter_loss: 0.2767854928970337
train_iter_loss: 0.14153946936130524
train_iter_loss: 0.16473664343357086
train_iter_loss: 0.3250512182712555
train_iter_loss: 0.1711297482252121
train_iter_loss: 0.20976698398590088
train_iter_loss: 0.27750855684280396
train_iter_loss: 0.3172612190246582
train_iter_loss: 0.21035419404506683
train_iter_loss: 0.3187163770198822
train_iter_loss: 0.36189013719558716
train_iter_loss: 0.2933981120586395
train_iter_loss: 0.16217103600502014
train_iter_loss: 0.1391569972038269
train_iter_loss: 0.3842473030090332
train_iter_loss: 0.2905174195766449
train_iter_loss: 0.27282872796058655
train_iter_loss: 0.2212725430727005
train_iter_loss: 0.20230087637901306
train_iter_loss: 0.2859994173049927
train_iter_loss: 0.12253696471452713
train_iter_loss: 0.17580215632915497
train_iter_loss: 0.22439689934253693
train_iter_loss: 0.3094942271709442
train_iter_loss: 0.41300538182258606
train_iter_loss: 0.1377672404050827
train loss :0.2764
---------------------
Validation seg loss: 0.3575657721398012 at epoch 486
epoch =    487/  1000, exp = train
train_iter_loss: 0.2071291208267212
train_iter_loss: 0.28337132930755615
train_iter_loss: 0.35004669427871704
train_iter_loss: 0.313591867685318
train_iter_loss: 0.23751601576805115
train_iter_loss: 0.18271277844905853
train_iter_loss: 0.25838974118232727
train_iter_loss: 0.12173717468976974
train_iter_loss: 0.24055878818035126
train_iter_loss: 0.17310413718223572
train_iter_loss: 0.36566728353500366
train_iter_loss: 0.3693588376045227
train_iter_loss: 0.26606494188308716
train_iter_loss: 0.27381882071495056
train_iter_loss: 0.20585277676582336
train_iter_loss: 0.18414749205112457
train_iter_loss: 0.31340140104293823
train_iter_loss: 0.3341981768608093
train_iter_loss: 0.2315329611301422
train_iter_loss: 0.23661097884178162
train_iter_loss: 0.24674387276172638
train_iter_loss: 0.15557023882865906
train_iter_loss: 0.20693489909172058
train_iter_loss: 0.1779503971338272
train_iter_loss: 0.4140472412109375
train_iter_loss: 0.31836414337158203
train_iter_loss: 0.40355977416038513
train_iter_loss: 0.28751108050346375
train_iter_loss: 0.3781011998653412
train_iter_loss: 0.21234968304634094
train_iter_loss: 0.3086898624897003
train_iter_loss: 0.32050564885139465
train_iter_loss: 0.13413402438163757
train_iter_loss: 0.2504010796546936
train_iter_loss: 0.4099431037902832
train_iter_loss: 0.32621872425079346
train_iter_loss: 0.22059957683086395
train_iter_loss: 0.16252028942108154
train_iter_loss: 0.2927058935165405
train_iter_loss: 0.41243311762809753
train_iter_loss: 0.34350836277008057
train_iter_loss: 0.3813701272010803
train_iter_loss: 0.2654108703136444
train_iter_loss: 0.2876792848110199
train_iter_loss: 0.144272580742836
train_iter_loss: 0.19483354687690735
train_iter_loss: 0.2807689607143402
train_iter_loss: 0.3013318181037903
train_iter_loss: 0.19209083914756775
train_iter_loss: 0.2996900677680969
train_iter_loss: 0.23610347509384155
train_iter_loss: 0.15263669192790985
train_iter_loss: 0.18523898720741272
train_iter_loss: 0.18419809639453888
train_iter_loss: 0.33604004979133606
train_iter_loss: 0.3800530433654785
train_iter_loss: 0.44526025652885437
train_iter_loss: 0.13602472841739655
train_iter_loss: 0.2340525984764099
train_iter_loss: 0.260772168636322
train_iter_loss: 0.17200131714344025
train_iter_loss: 0.3371864855289459
train_iter_loss: 0.1814429610967636
train_iter_loss: 0.33999866247177124
train_iter_loss: 0.2781069874763489
train_iter_loss: 0.3018743693828583
train_iter_loss: 0.17065677046775818
train_iter_loss: 0.2405393421649933
train_iter_loss: 0.2721174657344818
train_iter_loss: 0.34798920154571533
train_iter_loss: 0.30302077531814575
train_iter_loss: 0.2942984402179718
train_iter_loss: 0.32234495878219604
train_iter_loss: 0.31860312819480896
train_iter_loss: 0.3510131537914276
train_iter_loss: 0.20649351179599762
train_iter_loss: 0.13374342024326324
train_iter_loss: 0.24448251724243164
train_iter_loss: 0.2712474763393402
train_iter_loss: 0.3403877913951874
train_iter_loss: 0.3477390706539154
train_iter_loss: 0.27303144335746765
train_iter_loss: 0.3428471088409424
train_iter_loss: 0.24049381911754608
train_iter_loss: 0.2513534128665924
train_iter_loss: 0.30716031789779663
train_iter_loss: 0.4672683775424957
train_iter_loss: 0.17703121900558472
train_iter_loss: 0.2444249987602234
train_iter_loss: 0.48776793479919434
train_iter_loss: 0.33337292075157166
train_iter_loss: 0.2109304666519165
train_iter_loss: 0.1547241508960724
train_iter_loss: 0.34493187069892883
train_iter_loss: 0.23959879577159882
train_iter_loss: 0.27847379446029663
train_iter_loss: 0.25417160987854004
train_iter_loss: 0.1747252643108368
train_iter_loss: 0.20295271277427673
train_iter_loss: 0.2027849704027176
train loss :0.2727
---------------------
Validation seg loss: 0.3714683346344896 at epoch 487
epoch =    488/  1000, exp = train
train_iter_loss: 0.3360674977302551
train_iter_loss: 0.17384585738182068
train_iter_loss: 0.22004300355911255
train_iter_loss: 0.3289819657802582
train_iter_loss: 0.25728827714920044
train_iter_loss: 0.3254651725292206
train_iter_loss: 0.2319779247045517
train_iter_loss: 0.50676029920578
train_iter_loss: 0.28457966446876526
train_iter_loss: 0.2587635815143585
train_iter_loss: 0.12460698187351227
train_iter_loss: 0.3349084258079529
train_iter_loss: 0.18061035871505737
train_iter_loss: 0.29680976271629333
train_iter_loss: 0.2133115530014038
train_iter_loss: 0.43493369221687317
train_iter_loss: 0.19262821972370148
train_iter_loss: 0.25409671664237976
train_iter_loss: 0.3309105932712555
train_iter_loss: 0.2901562452316284
train_iter_loss: 0.35208722949028015
train_iter_loss: 0.21283279359340668
train_iter_loss: 0.2769518494606018
train_iter_loss: 0.07868999987840652
train_iter_loss: 0.2189299315214157
train_iter_loss: 0.3388488292694092
train_iter_loss: 0.17514380812644958
train_iter_loss: 0.41036325693130493
train_iter_loss: 0.16913442313671112
train_iter_loss: 0.26024580001831055
train_iter_loss: 0.27640166878700256
train_iter_loss: 0.22626134753227234
train_iter_loss: 0.06609255075454712
train_iter_loss: 0.25488513708114624
train_iter_loss: 0.3660804033279419
train_iter_loss: 0.3275754451751709
train_iter_loss: 0.276627779006958
train_iter_loss: 0.3080153167247772
train_iter_loss: 0.23692375421524048
train_iter_loss: 0.28107643127441406
train_iter_loss: 0.20487158000469208
train_iter_loss: 0.38743534684181213
train_iter_loss: 0.10117312520742416
train_iter_loss: 0.45475631952285767
train_iter_loss: 0.21886588633060455
train_iter_loss: 0.2667608857154846
train_iter_loss: 0.21352727711200714
train_iter_loss: 0.26259708404541016
train_iter_loss: 0.36863675713539124
train_iter_loss: 0.38035303354263306
train_iter_loss: 0.28166893124580383
train_iter_loss: 0.13830548524856567
train_iter_loss: 0.270628422498703
train_iter_loss: 0.19730724394321442
train_iter_loss: 0.29686886072158813
train_iter_loss: 0.44273266196250916
train_iter_loss: 0.2559165954589844
train_iter_loss: 0.1331620216369629
train_iter_loss: 0.39162465929985046
train_iter_loss: 0.21840283274650574
train_iter_loss: 0.33371108770370483
train_iter_loss: 0.1654726266860962
train_iter_loss: 0.24371172487735748
train_iter_loss: 0.2805178761482239
train_iter_loss: 0.2155754417181015
train_iter_loss: 0.32112064957618713
train_iter_loss: 0.2836594581604004
train_iter_loss: 0.318837970495224
train_iter_loss: 0.27342909574508667
train_iter_loss: 0.25716814398765564
train_iter_loss: 0.2830944359302521
train_iter_loss: 0.3498338460922241
train_iter_loss: 0.2895479202270508
train_iter_loss: 0.269943505525589
train_iter_loss: 0.2018241584300995
train_iter_loss: 0.2951408326625824
train_iter_loss: 0.29667916893959045
train_iter_loss: 0.2769809067249298
train_iter_loss: 0.4014405608177185
train_iter_loss: 0.3324960172176361
train_iter_loss: 0.14874590933322906
train_iter_loss: 0.22333411872386932
train_iter_loss: 0.1723695546388626
train_iter_loss: 0.3091416358947754
train_iter_loss: 0.33739250898361206
train_iter_loss: 0.27931323647499084
train_iter_loss: 0.2527354955673218
train_iter_loss: 0.3139382302761078
train_iter_loss: 0.18978117406368256
train_iter_loss: 0.11271997541189194
train_iter_loss: 0.23540087044239044
train_iter_loss: 0.2926856577396393
train_iter_loss: 0.4257158935070038
train_iter_loss: 0.37013381719589233
train_iter_loss: 0.22353199124336243
train_iter_loss: 0.29407986998558044
train_iter_loss: 0.11596266180276871
train_iter_loss: 0.20796702802181244
train_iter_loss: 0.2449001967906952
train_iter_loss: 0.25885745882987976
train loss :0.2715
---------------------
Validation seg loss: 0.3802082972152967 at epoch 488
epoch =    489/  1000, exp = train
train_iter_loss: 0.22300803661346436
train_iter_loss: 0.023559562861919403
train_iter_loss: 0.30716636776924133
train_iter_loss: 0.39642783999443054
train_iter_loss: 0.1690341979265213
train_iter_loss: 0.24976307153701782
train_iter_loss: 0.3642881214618683
train_iter_loss: 0.35303357243537903
train_iter_loss: 0.13057798147201538
train_iter_loss: 0.3295818865299225
train_iter_loss: 0.21768489480018616
train_iter_loss: 0.24387812614440918
train_iter_loss: 0.38896262645721436
train_iter_loss: 0.37072744965553284
train_iter_loss: 0.18931394815444946
train_iter_loss: 0.2968449890613556
train_iter_loss: 0.13482283055782318
train_iter_loss: 0.37866365909576416
train_iter_loss: 0.2982008755207062
train_iter_loss: 0.2952202260494232
train_iter_loss: 0.3459751605987549
train_iter_loss: 0.3429604172706604
train_iter_loss: 0.17913398146629333
train_iter_loss: 0.13172601163387299
train_iter_loss: 0.23585864901542664
train_iter_loss: 0.3017846941947937
train_iter_loss: 0.09420350193977356
train_iter_loss: 0.33470213413238525
train_iter_loss: 0.25563138723373413
train_iter_loss: 0.2818267047405243
train_iter_loss: 0.30240532755851746
train_iter_loss: 0.4489520192146301
train_iter_loss: 0.13091889023780823
train_iter_loss: 0.34052711725234985
train_iter_loss: 0.13187278807163239
train_iter_loss: 0.18543551862239838
train_iter_loss: 0.1432075947523117
train_iter_loss: 0.06000564992427826
train_iter_loss: 0.2565363049507141
train_iter_loss: 0.20974035561084747
train_iter_loss: 0.31135475635528564
train_iter_loss: 0.12537060678005219
train_iter_loss: 0.16739775240421295
train_iter_loss: 0.19361424446105957
train_iter_loss: 0.3714224100112915
train_iter_loss: 0.2980661988258362
train_iter_loss: 0.30924785137176514
train_iter_loss: 0.42590683698654175
train_iter_loss: 0.35749611258506775
train_iter_loss: 0.1832232028245926
train_iter_loss: 0.2688760459423065
train_iter_loss: 0.32568952441215515
train_iter_loss: 0.1289670318365097
train_iter_loss: 0.32625141739845276
train_iter_loss: 0.1512765735387802
train_iter_loss: 0.35502684116363525
train_iter_loss: 0.1782015562057495
train_iter_loss: 0.18628935515880585
train_iter_loss: 0.2852829396724701
train_iter_loss: 0.21907295286655426
train_iter_loss: 0.3468230664730072
train_iter_loss: 0.34793588519096375
train_iter_loss: 0.15639150142669678
train_iter_loss: 0.30843374133110046
train_iter_loss: 0.39065438508987427
train_iter_loss: 0.3253086507320404
train_iter_loss: 0.3568619191646576
train_iter_loss: 0.4023685157299042
train_iter_loss: 0.3215283155441284
train_iter_loss: 0.3611225187778473
train_iter_loss: 0.36559027433395386
train_iter_loss: 0.27716168761253357
train_iter_loss: 0.21522431075572968
train_iter_loss: 0.41879114508628845
train_iter_loss: 0.21856771409511566
train_iter_loss: 0.29009464383125305
train_iter_loss: 0.2867664396762848
train_iter_loss: 0.19406098127365112
train_iter_loss: 0.14538748562335968
train_iter_loss: 0.28876641392707825
train_iter_loss: 0.25657379627227783
train_iter_loss: 0.16309288144111633
train_iter_loss: 0.3534356355667114
train_iter_loss: 0.3070857524871826
train_iter_loss: 0.21815291047096252
train_iter_loss: 0.2667178809642792
train_iter_loss: 0.25494185090065
train_iter_loss: 0.30808573961257935
train_iter_loss: 0.11046411097049713
train_iter_loss: 0.24852442741394043
train_iter_loss: 0.17429517209529877
train_iter_loss: 0.15413671731948853
train_iter_loss: 0.3967277705669403
train_iter_loss: 0.2506042420864105
train_iter_loss: 0.2149624228477478
train_iter_loss: 0.16704556345939636
train_iter_loss: 0.30637165904045105
train_iter_loss: 0.21881167590618134
train_iter_loss: 0.3335477411746979
train_iter_loss: 0.33880138397216797
train loss :0.2657
---------------------
Validation seg loss: 0.3576087051088799 at epoch 489
epoch =    490/  1000, exp = train
train_iter_loss: 0.32814037799835205
train_iter_loss: 0.48303335905075073
train_iter_loss: 0.1694730669260025
train_iter_loss: 0.23139537870883942
train_iter_loss: 0.15347349643707275
train_iter_loss: 0.4687405228614807
train_iter_loss: 0.27876392006874084
train_iter_loss: 0.43082693219184875
train_iter_loss: 0.31706616282463074
train_iter_loss: 0.2425922155380249
train_iter_loss: 0.381369948387146
train_iter_loss: 0.18505753576755524
train_iter_loss: 0.1738174557685852
train_iter_loss: 0.3237845003604889
train_iter_loss: 0.1720481663942337
train_iter_loss: 0.3003440797328949
train_iter_loss: 0.2785928547382355
train_iter_loss: 0.3106189966201782
train_iter_loss: 0.42934316396713257
train_iter_loss: 0.30608806014060974
train_iter_loss: 0.29337289929389954
train_iter_loss: 0.26645663380622864
train_iter_loss: 0.2211194634437561
train_iter_loss: 0.3296626806259155
train_iter_loss: 0.2832990288734436
train_iter_loss: 0.1854507476091385
train_iter_loss: 0.20260611176490784
train_iter_loss: 0.16714176535606384
train_iter_loss: 0.2077445238828659
train_iter_loss: 0.38363173604011536
train_iter_loss: 0.217911496758461
train_iter_loss: 0.33419445157051086
train_iter_loss: 0.232683464884758
train_iter_loss: 0.19052307307720184
train_iter_loss: 0.3405863344669342
train_iter_loss: 0.43116647005081177
train_iter_loss: 0.16814535856246948
train_iter_loss: 0.2132009118795395
train_iter_loss: 0.25128045678138733
train_iter_loss: 0.2645510137081146
train_iter_loss: 0.17397236824035645
train_iter_loss: 0.3007088601589203
train_iter_loss: 0.25736337900161743
train_iter_loss: 0.08497270196676254
train_iter_loss: 0.13980664312839508
train_iter_loss: 0.1525219976902008
train_iter_loss: 0.20443616807460785
train_iter_loss: 0.22529146075248718
train_iter_loss: 0.29594242572784424
train_iter_loss: 0.2560543119907379
train_iter_loss: 0.2409035563468933
train_iter_loss: 0.255988210439682
train_iter_loss: 0.1934841275215149
train_iter_loss: 0.2879107892513275
train_iter_loss: 0.24939250946044922
train_iter_loss: 0.31203600764274597
train_iter_loss: 0.17135579884052277
train_iter_loss: 0.3388083279132843
train_iter_loss: 0.21903076767921448
train_iter_loss: 0.34024763107299805
train_iter_loss: 0.23765607178211212
train_iter_loss: 0.23801732063293457
train_iter_loss: 0.2383088618516922
train_iter_loss: 0.18348772823810577
train_iter_loss: 0.20614412426948547
train_iter_loss: 0.29459282755851746
train_iter_loss: 0.46407392621040344
train_iter_loss: 0.37604737281799316
train_iter_loss: 0.3901691436767578
train_iter_loss: 0.34824615716934204
train_iter_loss: 0.3047444522380829
train_iter_loss: 0.3669828772544861
train_iter_loss: 0.34694039821624756
train_iter_loss: 0.28455764055252075
train_iter_loss: 0.15674178302288055
train_iter_loss: 0.3565613329410553
train_iter_loss: 0.3498152792453766
train_iter_loss: 0.14581526815891266
train_iter_loss: 0.36263224482536316
train_iter_loss: 0.20478734374046326
train_iter_loss: 0.17956098914146423
train_iter_loss: 0.3206053078174591
train_iter_loss: 0.12573924660682678
train_iter_loss: 0.14963248372077942
train_iter_loss: 0.3448137044906616
train_iter_loss: 0.22270210087299347
train_iter_loss: 0.3134762644767761
train_iter_loss: 0.32651564478874207
train_iter_loss: 0.3816247880458832
train_iter_loss: 0.18239271640777588
train_iter_loss: 0.26916801929473877
train_iter_loss: 0.23383744060993195
train_iter_loss: 0.35084086656570435
train_iter_loss: 0.4274827837944031
train_iter_loss: 0.2860402762889862
train_iter_loss: 0.23759715259075165
train_iter_loss: 0.39414483308792114
train_iter_loss: 0.20775888860225677
train_iter_loss: 0.2560495138168335
train_iter_loss: 0.2467336803674698
train loss :0.2743
---------------------
Validation seg loss: 0.4107529526479753 at epoch 490
epoch =    491/  1000, exp = train
train_iter_loss: 0.22017285227775574
train_iter_loss: 0.21404829621315002
train_iter_loss: 0.5711756944656372
train_iter_loss: 0.24594242870807648
train_iter_loss: 0.34415704011917114
train_iter_loss: 0.1560925394296646
train_iter_loss: 0.2695445120334625
train_iter_loss: 0.2647955119609833
train_iter_loss: 0.24974635243415833
train_iter_loss: 0.0991484671831131
train_iter_loss: 0.19686239957809448
train_iter_loss: 0.2188873142004013
train_iter_loss: 0.28929537534713745
train_iter_loss: 0.396684855222702
train_iter_loss: 0.35696619749069214
train_iter_loss: 0.1539631485939026
train_iter_loss: 0.27375584840774536
train_iter_loss: 0.13421045243740082
train_iter_loss: 0.22066019475460052
train_iter_loss: 0.29463350772857666
train_iter_loss: 0.19785277545452118
train_iter_loss: 0.2072322964668274
train_iter_loss: 0.32692477107048035
train_iter_loss: 0.3536437153816223
train_iter_loss: 0.27340778708457947
train_iter_loss: 0.2944777011871338
train_iter_loss: 0.28168001770973206
train_iter_loss: 0.47882214188575745
train_iter_loss: 0.21200479567050934
train_iter_loss: 0.22773714363574982
train_iter_loss: 0.4058326780796051
train_iter_loss: 0.3536096215248108
train_iter_loss: 0.30399563908576965
train_iter_loss: 0.32712405920028687
train_iter_loss: 0.2324535995721817
train_iter_loss: 0.26544439792633057
train_iter_loss: 0.5177364945411682
train_iter_loss: 0.34056800603866577
train_iter_loss: 0.2502211034297943
train_iter_loss: 0.29734107851982117
train_iter_loss: 0.2156403809785843
train_iter_loss: 0.2558145523071289
train_iter_loss: 0.2799178957939148
train_iter_loss: 0.21607431769371033
train_iter_loss: 0.28032591938972473
train_iter_loss: 0.20658329129219055
train_iter_loss: 0.2558704912662506
train_iter_loss: 0.38201388716697693
train_iter_loss: 0.4833388924598694
train_iter_loss: 0.2853871285915375
train_iter_loss: 0.07864492386579514
train_iter_loss: 0.4864175319671631
train_iter_loss: 0.05758102238178253
train_iter_loss: 0.20868901908397675
train_iter_loss: 0.284884512424469
train_iter_loss: 0.3261815309524536
train_iter_loss: 0.3030568063259125
train_iter_loss: 0.13797429203987122
train_iter_loss: 0.1109723150730133
train_iter_loss: 0.359527587890625
train_iter_loss: 0.40019622445106506
train_iter_loss: 0.23241053521633148
train_iter_loss: 0.2757839560508728
train_iter_loss: 0.271860271692276
train_iter_loss: 0.23061974346637726
train_iter_loss: 0.2637162208557129
train_iter_loss: 0.2266847938299179
train_iter_loss: 0.23002885282039642
train_iter_loss: 0.28652170300483704
train_iter_loss: 0.3674379885196686
train_iter_loss: 0.15332698822021484
train_iter_loss: 0.3764491677284241
train_iter_loss: 0.31026870012283325
train_iter_loss: 0.24353240430355072
train_iter_loss: 0.30003979802131653
train_iter_loss: 0.5239226818084717
train_iter_loss: 0.19710668921470642
train_iter_loss: 0.2730465531349182
train_iter_loss: 0.4167788028717041
train_iter_loss: 0.19888052344322205
train_iter_loss: 0.5480349063873291
train_iter_loss: 0.24313873052597046
train_iter_loss: 0.09972359985113144
train_iter_loss: 0.28685110807418823
train_iter_loss: 0.13798189163208008
train_iter_loss: 0.1981041431427002
train_iter_loss: 0.2973807454109192
train_iter_loss: 0.23391994833946228
train_iter_loss: 0.27175605297088623
train_iter_loss: 0.29395127296447754
train_iter_loss: 0.35009267926216125
train_iter_loss: 0.3410198986530304
train_iter_loss: 0.27070754766464233
train_iter_loss: 0.15364331007003784
train_iter_loss: 0.4069461524486542
train_iter_loss: 0.18422921001911163
train_iter_loss: 0.23820966482162476
train_iter_loss: 0.11615698784589767
train_iter_loss: 0.19835476577281952
train_iter_loss: 0.37623387575149536
train loss :0.2783
---------------------
Validation seg loss: 0.3822287294029627 at epoch 491
epoch =    492/  1000, exp = train
train_iter_loss: 0.29668885469436646
train_iter_loss: 0.41969385743141174
train_iter_loss: 0.2976703941822052
train_iter_loss: 0.30584239959716797
train_iter_loss: 0.18281759321689606
train_iter_loss: 0.38440635800361633
train_iter_loss: 0.22240221500396729
train_iter_loss: 0.25413691997528076
train_iter_loss: 0.14550793170928955
train_iter_loss: 0.27636927366256714
train_iter_loss: 0.13849873840808868
train_iter_loss: 0.0990748479962349
train_iter_loss: 0.24841411411762238
train_iter_loss: 0.17576055228710175
train_iter_loss: 0.37556299567222595
train_iter_loss: 0.2357456088066101
train_iter_loss: 0.29366785287857056
train_iter_loss: 0.4102480113506317
train_iter_loss: 0.30089902877807617
train_iter_loss: 0.26629704236984253
train_iter_loss: 0.201601043343544
train_iter_loss: 0.2737782299518585
train_iter_loss: 0.5854594707489014
train_iter_loss: 0.24096840620040894
train_iter_loss: 0.21353565156459808
train_iter_loss: 0.10410097986459732
train_iter_loss: 0.21588872373104095
train_iter_loss: 0.3875715136528015
train_iter_loss: 0.35571733117103577
train_iter_loss: 0.24003265798091888
train_iter_loss: 0.1752622127532959
train_iter_loss: 0.27184605598449707
train_iter_loss: 0.30898839235305786
train_iter_loss: 0.25742107629776
train_iter_loss: 0.23803047835826874
train_iter_loss: 0.4950122535228729
train_iter_loss: 0.2781710624694824
train_iter_loss: 0.2716601490974426
train_iter_loss: 0.3761594891548157
train_iter_loss: 0.2966735064983368
train_iter_loss: 0.3159944713115692
train_iter_loss: 0.25237199664115906
train_iter_loss: 0.1977211982011795
train_iter_loss: 0.3255581557750702
train_iter_loss: 0.27497929334640503
train_iter_loss: 0.2952144145965576
train_iter_loss: 0.24340775609016418
train_iter_loss: 0.2722029387950897
train_iter_loss: 0.1656591296195984
train_iter_loss: 0.20620644092559814
train_iter_loss: 0.17146320641040802
train_iter_loss: 0.29208090901374817
train_iter_loss: 0.43552082777023315
train_iter_loss: 0.29617393016815186
train_iter_loss: 0.2719961404800415
train_iter_loss: 0.16620689630508423
train_iter_loss: 0.33340218663215637
train_iter_loss: 0.20793333649635315
train_iter_loss: 0.3080277740955353
train_iter_loss: 0.2545357346534729
train_iter_loss: 0.39761173725128174
train_iter_loss: 0.16786956787109375
train_iter_loss: 0.10159683227539062
train_iter_loss: 0.2742798924446106
train_iter_loss: 0.32272741198539734
train_iter_loss: 0.4761146903038025
train_iter_loss: 0.2867048978805542
train_iter_loss: 0.6360086798667908
train_iter_loss: 0.46229246258735657
train_iter_loss: 0.23976697027683258
train_iter_loss: 0.2655309736728668
train_iter_loss: 0.12322043627500534
train_iter_loss: 0.42994505167007446
train_iter_loss: 0.20209655165672302
train_iter_loss: 0.2990480065345764
train_iter_loss: 0.22420614957809448
train_iter_loss: 0.3067708909511566
train_iter_loss: 0.31910231709480286
train_iter_loss: 0.30743661522865295
train_iter_loss: 0.163687065243721
train_iter_loss: 0.2896396815776825
train_iter_loss: 0.257834255695343
train_iter_loss: 0.31251636147499084
train_iter_loss: 0.30831488966941833
train_iter_loss: 0.20943084359169006
train_iter_loss: 0.3817867338657379
train_iter_loss: 0.15190266072750092
train_iter_loss: 0.28601697087287903
train_iter_loss: 0.2818417251110077
train_iter_loss: 0.10948193818330765
train_iter_loss: 0.27748218178749084
train_iter_loss: 0.3639293909072876
train_iter_loss: 0.2800713777542114
train_iter_loss: 0.3604700565338135
train_iter_loss: 0.1862591952085495
train_iter_loss: 0.18315373361110687
train_iter_loss: 0.2002909779548645
train_iter_loss: 0.5399702191352844
train_iter_loss: 0.29392287135124207
train_iter_loss: 0.1828158050775528
train loss :0.2815
---------------------
Validation seg loss: 0.3688622844484547 at epoch 492
epoch =    493/  1000, exp = train
train_iter_loss: 0.21876895427703857
train_iter_loss: 0.35587671399116516
train_iter_loss: 0.2840748727321625
train_iter_loss: 0.3356088697910309
train_iter_loss: 0.21594439446926117
train_iter_loss: 0.356240838766098
train_iter_loss: 0.20166610181331635
train_iter_loss: 0.28384360671043396
train_iter_loss: 0.1835944503545761
train_iter_loss: 0.19522131979465485
train_iter_loss: 0.2057725489139557
train_iter_loss: 0.42690154910087585
train_iter_loss: 0.360956072807312
train_iter_loss: 0.14594325423240662
train_iter_loss: 0.6048046350479126
train_iter_loss: 0.23084214329719543
train_iter_loss: 0.2805373966693878
train_iter_loss: 0.34581366181373596
train_iter_loss: 0.15671993792057037
train_iter_loss: 0.4127321243286133
train_iter_loss: 0.20746351778507233
train_iter_loss: 0.3463526964187622
train_iter_loss: 0.3872179388999939
train_iter_loss: 0.22221776843070984
train_iter_loss: 0.2695886790752411
train_iter_loss: 0.38409775495529175
train_iter_loss: 0.2708362638950348
train_iter_loss: 0.2511502504348755
train_iter_loss: 0.15388664603233337
train_iter_loss: 0.3738916516304016
train_iter_loss: 0.19735106825828552
train_iter_loss: 0.27549758553504944
train_iter_loss: 0.19859206676483154
train_iter_loss: 0.18175825476646423
train_iter_loss: 0.3644590973854065
train_iter_loss: 0.2745112180709839
train_iter_loss: 0.14419054985046387
train_iter_loss: 0.24502524733543396
train_iter_loss: 0.276742547750473
train_iter_loss: 0.1752537041902542
train_iter_loss: 0.22748614847660065
train_iter_loss: 0.3244000971317291
train_iter_loss: 0.43935972452163696
train_iter_loss: 0.2440355271100998
train_iter_loss: 0.3091311752796173
train_iter_loss: 0.15681323409080505
train_iter_loss: 0.3710413873195648
train_iter_loss: 0.36547744274139404
train_iter_loss: 0.301547646522522
train_iter_loss: 0.1736593395471573
train_iter_loss: 0.14982178807258606
train_iter_loss: 0.2548259198665619
train_iter_loss: 0.31610527634620667
train_iter_loss: 0.2773413360118866
train_iter_loss: 0.29470184445381165
train_iter_loss: 0.14148595929145813
train_iter_loss: 0.33706018328666687
train_iter_loss: 0.23647400736808777
train_iter_loss: 0.3087140917778015
train_iter_loss: 0.2947007715702057
train_iter_loss: 0.22009670734405518
train_iter_loss: 0.307850182056427
train_iter_loss: 0.24739287793636322
train_iter_loss: 0.24879245460033417
train_iter_loss: 0.27826499938964844
train_iter_loss: 0.27119725942611694
train_iter_loss: 0.34905126690864563
train_iter_loss: 0.3028402626514435
train_iter_loss: 0.25000062584877014
train_iter_loss: 0.13585980236530304
train_iter_loss: 0.2602718472480774
train_iter_loss: 0.3691590428352356
train_iter_loss: 0.2659512758255005
train_iter_loss: 0.22842635214328766
train_iter_loss: 0.32673022150993347
train_iter_loss: 0.19273433089256287
train_iter_loss: 0.2564621567726135
train_iter_loss: 0.18901781737804413
train_iter_loss: 0.2543649971485138
train_iter_loss: 0.3168056309223175
train_iter_loss: 0.1258918195962906
train_iter_loss: 0.21537305414676666
train_iter_loss: 0.3596363067626953
train_iter_loss: 0.34544092416763306
train_iter_loss: 0.23714160919189453
train_iter_loss: 0.33174702525138855
train_iter_loss: 0.2600199580192566
train_iter_loss: 0.08885227143764496
train_iter_loss: 0.19220106303691864
train_iter_loss: 0.21392680704593658
train_iter_loss: 0.3210192918777466
train_iter_loss: 0.24252845346927643
train_iter_loss: 0.41063350439071655
train_iter_loss: 0.21383270621299744
train_iter_loss: 0.1719091832637787
train_iter_loss: 0.26558396220207214
train_iter_loss: 0.14731891453266144
train_iter_loss: 0.35546019673347473
train_iter_loss: 0.21294014155864716
train_iter_loss: 0.1782246083021164
train loss :0.2697
---------------------
Validation seg loss: 0.3541239147285386 at epoch 493
epoch =    494/  1000, exp = train
train_iter_loss: 0.3107476234436035
train_iter_loss: 0.23558953404426575
train_iter_loss: 0.43225589394569397
train_iter_loss: 0.31911641359329224
train_iter_loss: 0.37962475419044495
train_iter_loss: 0.15159155428409576
train_iter_loss: 0.10278567671775818
train_iter_loss: 0.16019661724567413
train_iter_loss: 0.31325194239616394
train_iter_loss: 0.3908731937408447
train_iter_loss: 0.25508174300193787
train_iter_loss: 0.3443840444087982
train_iter_loss: 0.2280806005001068
train_iter_loss: 0.2684623897075653
train_iter_loss: 0.332881897687912
train_iter_loss: 0.22702465951442719
train_iter_loss: 0.30592286586761475
train_iter_loss: 0.2483784705400467
train_iter_loss: 0.37211596965789795
train_iter_loss: 0.3258579969406128
train_iter_loss: 0.3066561222076416
train_iter_loss: 0.2890799045562744
train_iter_loss: 0.2722378075122833
train_iter_loss: 0.45910659432411194
train_iter_loss: 0.3642306923866272
train_iter_loss: 0.1573004573583603
train_iter_loss: 0.0909760519862175
train_iter_loss: 0.10785581916570663
train_iter_loss: 0.2163839191198349
train_iter_loss: 0.17541928589344025
train_iter_loss: 0.4107348620891571
train_iter_loss: 0.17051014304161072
train_iter_loss: 0.3562399744987488
train_iter_loss: 0.22143156826496124
train_iter_loss: 0.3455105125904083
train_iter_loss: 0.18118926882743835
train_iter_loss: 0.20544612407684326
train_iter_loss: 0.219090074300766
train_iter_loss: 0.34290915727615356
train_iter_loss: 0.26693421602249146
train_iter_loss: 0.27222368121147156
train_iter_loss: 0.1872209757566452
train_iter_loss: 0.202360600233078
train_iter_loss: 0.25949376821517944
train_iter_loss: 0.3082961142063141
train_iter_loss: 0.3440801799297333
train_iter_loss: 0.2085411548614502
train_iter_loss: 0.2753131091594696
train_iter_loss: 0.3392188549041748
train_iter_loss: 0.1363881379365921
train_iter_loss: 0.35458481311798096
train_iter_loss: 0.19638878107070923
train_iter_loss: 0.2059275060892105
train_iter_loss: 0.2776826024055481
train_iter_loss: 0.22623108327388763
train_iter_loss: 0.28606903553009033
train_iter_loss: 0.17371690273284912
train_iter_loss: 0.44125205278396606
train_iter_loss: 0.342708945274353
train_iter_loss: 0.15684624016284943
train_iter_loss: 0.23568706214427948
train_iter_loss: 0.36671775579452515
train_iter_loss: 0.33665966987609863
train_iter_loss: 0.21279145777225494
train_iter_loss: 0.35261839628219604
train_iter_loss: 0.23040230572223663
train_iter_loss: 0.3748963177204132
train_iter_loss: 0.17520351707935333
train_iter_loss: 0.2555384933948517
train_iter_loss: 0.4004784822463989
train_iter_loss: 0.3086928129196167
train_iter_loss: 0.2786979675292969
train_iter_loss: 0.14732632040977478
train_iter_loss: 0.20727480947971344
train_iter_loss: 0.3603319823741913
train_iter_loss: 0.48018819093704224
train_iter_loss: 0.24624550342559814
train_iter_loss: 0.4452821612358093
train_iter_loss: 0.25349050760269165
train_iter_loss: 0.23840662837028503
train_iter_loss: 0.3062315881252289
train_iter_loss: 0.2739432454109192
train_iter_loss: 0.20660552382469177
train_iter_loss: 0.1863911896944046
train_iter_loss: 0.2918756902217865
train_iter_loss: 0.3602358102798462
train_iter_loss: 0.25104132294654846
train_iter_loss: 0.30333635210990906
train_iter_loss: 0.26017600297927856
train_iter_loss: 0.41911736130714417
train_iter_loss: 0.15556098520755768
train_iter_loss: 0.20152433216571808
train_iter_loss: 0.41574713587760925
train_iter_loss: 0.24684856832027435
train_iter_loss: 0.07431938499212265
train_iter_loss: 0.1312916874885559
train_iter_loss: 0.18938082456588745
train_iter_loss: 0.15867488086223602
train_iter_loss: 0.25739383697509766
train_iter_loss: 0.2011856585741043
train loss :0.2713
---------------------
Validation seg loss: 0.3907542330177747 at epoch 494
epoch =    495/  1000, exp = train
train_iter_loss: 0.18318919837474823
train_iter_loss: 0.17625577747821808
train_iter_loss: 0.4173619747161865
train_iter_loss: 0.3266526758670807
train_iter_loss: 0.2836548984050751
train_iter_loss: 0.46752652525901794
train_iter_loss: 0.30558112263679504
train_iter_loss: 0.1874648481607437
train_iter_loss: 0.19445651769638062
train_iter_loss: 0.2688133716583252
train_iter_loss: 0.2442699521780014
train_iter_loss: 0.3150753080844879
train_iter_loss: 0.3807713985443115
train_iter_loss: 0.11825112998485565
train_iter_loss: 0.1991395801305771
train_iter_loss: 0.24025404453277588
train_iter_loss: 0.2694462537765503
train_iter_loss: 0.3170934021472931
train_iter_loss: 0.14001500606536865
train_iter_loss: 0.30896246433258057
train_iter_loss: 0.10299983620643616
train_iter_loss: 0.4015493392944336
train_iter_loss: 0.16728585958480835
train_iter_loss: 0.2666962742805481
train_iter_loss: 0.2281959503889084
train_iter_loss: 0.2911183834075928
train_iter_loss: 0.38586071133613586
train_iter_loss: 0.2519504725933075
train_iter_loss: 0.24366815388202667
train_iter_loss: 0.15847451984882355
train_iter_loss: 0.11681703478097916
train_iter_loss: 0.3454342782497406
train_iter_loss: 0.2626124918460846
train_iter_loss: 0.4225365221500397
train_iter_loss: 0.28988054394721985
train_iter_loss: 0.32409194111824036
train_iter_loss: 0.5185118317604065
train_iter_loss: 0.14678722620010376
train_iter_loss: 0.3330085873603821
train_iter_loss: 0.27838993072509766
train_iter_loss: 0.32357266545295715
train_iter_loss: 0.4365941882133484
train_iter_loss: 0.37920093536376953
train_iter_loss: 0.18246354162693024
train_iter_loss: 0.3409128785133362
train_iter_loss: 0.35358354449272156
train_iter_loss: 0.2733362019062042
train_iter_loss: 0.30575141310691833
train_iter_loss: 0.2933579683303833
train_iter_loss: 0.2257440835237503
train_iter_loss: 0.16398975253105164
train_iter_loss: 0.26469823718070984
train_iter_loss: 0.36006075143814087
train_iter_loss: 0.11597856879234314
train_iter_loss: 0.3644948899745941
train_iter_loss: 0.3447212278842926
train_iter_loss: 0.5330538153648376
train_iter_loss: 0.1864793300628662
train_iter_loss: 0.2812959551811218
train_iter_loss: 0.4206368625164032
train_iter_loss: 0.17416316270828247
train_iter_loss: 0.19968242943286896
train_iter_loss: 0.2693517804145813
train_iter_loss: 0.15880577266216278
train_iter_loss: 0.15717482566833496
train_iter_loss: 0.20603616535663605
train_iter_loss: 0.23377305269241333
train_iter_loss: 0.23119843006134033
train_iter_loss: 0.36023303866386414
train_iter_loss: 0.30494070053100586
train_iter_loss: 0.23836266994476318
train_iter_loss: 0.2947141230106354
train_iter_loss: 0.19444023072719574
train_iter_loss: 0.34249091148376465
train_iter_loss: 0.38077011704444885
train_iter_loss: 0.1473500281572342
train_iter_loss: 0.1774984747171402
train_iter_loss: 0.2087082713842392
train_iter_loss: 0.4402461051940918
train_iter_loss: 0.34780359268188477
train_iter_loss: 0.35714012384414673
train_iter_loss: 0.10628088563680649
train_iter_loss: 0.13658103346824646
train_iter_loss: 0.3570406138896942
train_iter_loss: 0.27688491344451904
train_iter_loss: 0.4242280125617981
train_iter_loss: 0.3098304569721222
train_iter_loss: 0.1732441484928131
train_iter_loss: 0.34906691312789917
train_iter_loss: 0.26586204767227173
train_iter_loss: 0.14469943940639496
train_iter_loss: 0.3556998372077942
train_iter_loss: 0.12190567702054977
train_iter_loss: 0.10315351188182831
train_iter_loss: 0.2476401925086975
train_iter_loss: 0.23393268883228302
train_iter_loss: 0.36054545640945435
train_iter_loss: 0.2558943033218384
train_iter_loss: 0.3176349699497223
train_iter_loss: 0.2501935660839081
train loss :0.2752
---------------------
Validation seg loss: 0.35500212730664127 at epoch 495
epoch =    496/  1000, exp = train
train_iter_loss: 0.16948960721492767
train_iter_loss: 0.2957110106945038
train_iter_loss: 0.23662897944450378
train_iter_loss: 0.20438838005065918
train_iter_loss: 0.26575547456741333
train_iter_loss: 0.34647515416145325
train_iter_loss: 0.2500064969062805
train_iter_loss: 0.22402779757976532
train_iter_loss: 0.44518423080444336
train_iter_loss: 0.3887253701686859
train_iter_loss: 0.28549924492836
train_iter_loss: 0.34295183420181274
train_iter_loss: 0.2743838131427765
train_iter_loss: 0.33930498361587524
train_iter_loss: 0.37702682614326477
train_iter_loss: 0.21925696730613708
train_iter_loss: 0.23081126809120178
train_iter_loss: 0.16564211249351501
train_iter_loss: 0.23406457901000977
train_iter_loss: 0.2752287685871124
train_iter_loss: 0.3102349042892456
train_iter_loss: 0.3948221802711487
train_iter_loss: 0.420545756816864
train_iter_loss: 0.3583053946495056
train_iter_loss: 0.30948373675346375
train_iter_loss: 0.4069828391075134
train_iter_loss: 0.25485196709632874
train_iter_loss: 0.36295977234840393
train_iter_loss: 0.19602298736572266
train_iter_loss: 0.26364749670028687
train_iter_loss: 0.196271151304245
train_iter_loss: 0.3297635614871979
train_iter_loss: 0.1651795506477356
train_iter_loss: 0.1580352932214737
train_iter_loss: 0.20169411599636078
train_iter_loss: 0.3556104302406311
train_iter_loss: 0.12367811799049377
train_iter_loss: 0.17359212040901184
train_iter_loss: 0.2479390799999237
train_iter_loss: 0.34923824667930603
train_iter_loss: 0.1721860021352768
train_iter_loss: 0.24140769243240356
train_iter_loss: 0.28449657559394836
train_iter_loss: 0.12468889355659485
train_iter_loss: 0.30207541584968567
train_iter_loss: 0.17560738325119019
train_iter_loss: 0.24712319672107697
train_iter_loss: 0.3032609522342682
train_iter_loss: 0.2665776312351227
train_iter_loss: 0.2504004240036011
train_iter_loss: 0.17427699267864227
train_iter_loss: 0.28435811400413513
train_iter_loss: 0.5159792304039001
train_iter_loss: 0.2683134078979492
train_iter_loss: 0.2514219582080841
train_iter_loss: 0.20577478408813477
train_iter_loss: 0.1812107414007187
train_iter_loss: 0.05665748193860054
train_iter_loss: 0.2602565586566925
train_iter_loss: 0.28792157769203186
train_iter_loss: 0.48177629709243774
train_iter_loss: 0.3772447109222412
train_iter_loss: 0.3785981237888336
train_iter_loss: 0.26553890109062195
train_iter_loss: 0.2803686261177063
train_iter_loss: 0.39862632751464844
train_iter_loss: 0.21183104813098907
train_iter_loss: 0.3835057020187378
train_iter_loss: 0.28198736906051636
train_iter_loss: 0.14329683780670166
train_iter_loss: 0.247340127825737
train_iter_loss: 0.31399598717689514
train_iter_loss: 0.2077738642692566
train_iter_loss: 0.3637739419937134
train_iter_loss: 0.24958869814872742
train_iter_loss: 0.2871420383453369
train_iter_loss: 0.3635975420475006
train_iter_loss: 0.31801605224609375
train_iter_loss: 0.1605144739151001
train_iter_loss: 0.3175114095211029
train_iter_loss: 0.3043866455554962
train_iter_loss: 0.23749062418937683
train_iter_loss: 0.2624724209308624
train_iter_loss: 0.29849347472190857
train_iter_loss: 0.334552526473999
train_iter_loss: 0.18142502009868622
train_iter_loss: 0.18125924468040466
train_iter_loss: 0.1674628108739853
train_iter_loss: 0.2583024501800537
train_iter_loss: 0.23363789916038513
train_iter_loss: 0.29692691564559937
train_iter_loss: 0.43252435326576233
train_iter_loss: 0.35343146324157715
train_iter_loss: 0.23849377036094666
train_iter_loss: 0.3972608745098114
train_iter_loss: 0.26691874861717224
train_iter_loss: 0.21359504759311676
train_iter_loss: 0.131053164601326
train_iter_loss: 0.20847974717617035
train_iter_loss: 0.2086053490638733
train loss :0.2752
---------------------
Validation seg loss: 0.3550685915639097 at epoch 496
epoch =    497/  1000, exp = train
train_iter_loss: 0.2422935962677002
train_iter_loss: 0.14974944293498993
train_iter_loss: 0.36880454421043396
train_iter_loss: 0.3084622323513031
train_iter_loss: 0.21514512598514557
train_iter_loss: 0.1856086701154709
train_iter_loss: 0.22850249707698822
train_iter_loss: 0.2617701590061188
train_iter_loss: 0.30304262042045593
train_iter_loss: 0.3770715296268463
train_iter_loss: 0.2659474015235901
train_iter_loss: 0.2739294767379761
train_iter_loss: 0.2043086290359497
train_iter_loss: 0.3249247968196869
train_iter_loss: 0.1621101349592209
train_iter_loss: 0.24740329384803772
train_iter_loss: 0.2881287634372711
train_iter_loss: 0.21537435054779053
train_iter_loss: 0.20058658719062805
train_iter_loss: 0.20311523973941803
train_iter_loss: 0.1661524772644043
train_iter_loss: 0.32954320311546326
train_iter_loss: 0.35783979296684265
train_iter_loss: 0.16208940744400024
train_iter_loss: 0.33113962411880493
train_iter_loss: 0.31516537070274353
train_iter_loss: 0.3475233316421509
train_iter_loss: 0.3088996410369873
train_iter_loss: 0.3863529860973358
train_iter_loss: 0.30352452397346497
train_iter_loss: 0.16786962747573853
train_iter_loss: 0.20416224002838135
train_iter_loss: 0.22578084468841553
train_iter_loss: 0.2867524027824402
train_iter_loss: 0.26970648765563965
train_iter_loss: 0.29929977655410767
train_iter_loss: 0.3266156315803528
train_iter_loss: 0.2462174892425537
train_iter_loss: 0.17018276453018188
train_iter_loss: 0.26399844884872437
train_iter_loss: 0.22802652418613434
train_iter_loss: 0.27005207538604736
train_iter_loss: 0.30043184757232666
train_iter_loss: 0.15012095868587494
train_iter_loss: 0.49970632791519165
train_iter_loss: 0.10295426100492477
train_iter_loss: 0.267336905002594
train_iter_loss: 0.37062081694602966
train_iter_loss: 0.1557176113128662
train_iter_loss: 0.370429128408432
train_iter_loss: 0.28949886560440063
train_iter_loss: 0.2584629952907562
train_iter_loss: 0.23975637555122375
train_iter_loss: 0.38252219557762146
train_iter_loss: 0.13925796747207642
train_iter_loss: 0.35460081696510315
train_iter_loss: 0.311592161655426
train_iter_loss: 0.08119682222604752
train_iter_loss: 0.24343319237232208
train_iter_loss: 0.24315595626831055
train_iter_loss: 0.14709806442260742
train_iter_loss: 0.26836687326431274
train_iter_loss: 0.2792791724205017
train_iter_loss: 0.2669292092323303
train_iter_loss: 0.3193272352218628
train_iter_loss: 0.1502024382352829
train_iter_loss: 0.40978604555130005
train_iter_loss: 0.3339158594608307
train_iter_loss: 0.17967385053634644
train_iter_loss: 0.20774461328983307
train_iter_loss: 0.29960525035858154
train_iter_loss: 0.11244117468595505
train_iter_loss: 0.255479633808136
train_iter_loss: 0.4222181439399719
train_iter_loss: 0.22291874885559082
train_iter_loss: 0.4530068337917328
train_iter_loss: 0.3225495219230652
train_iter_loss: 0.26518282294273376
train_iter_loss: 0.25645604729652405
train_iter_loss: 0.33102571964263916
train_iter_loss: 0.30837637186050415
train_iter_loss: 0.23623308539390564
train_iter_loss: 0.24201470613479614
train_iter_loss: 0.22229409217834473
train_iter_loss: 0.13015955686569214
train_iter_loss: 0.3763550817966461
train_iter_loss: 0.1889069378376007
train_iter_loss: 0.3244982659816742
train_iter_loss: 0.3243919312953949
train_iter_loss: 0.4175909459590912
train_iter_loss: 0.3498658537864685
train_iter_loss: 0.18026183545589447
train_iter_loss: 0.21067848801612854
train_iter_loss: 0.4985140860080719
train_iter_loss: 0.3342680335044861
train_iter_loss: 0.27224868535995483
train_iter_loss: 0.2979797422885895
train_iter_loss: 0.35631775856018066
train_iter_loss: 0.2204330861568451
train_iter_loss: 0.44698429107666016
train loss :0.2747
---------------------
Validation seg loss: 0.34743467501347075 at epoch 497
********************
best_val_epoch_loss:  0.34743467501347075
MODEL UPDATED
epoch =    498/  1000, exp = train
train_iter_loss: 0.22669649124145508
train_iter_loss: 0.13270112872123718
train_iter_loss: 0.2819676995277405
train_iter_loss: 0.34443843364715576
train_iter_loss: 0.24932409822940826
train_iter_loss: 0.23623840510845184
train_iter_loss: 0.49655672907829285
train_iter_loss: 0.30014458298683167
train_iter_loss: 0.23068001866340637
train_iter_loss: 0.2974032759666443
train_iter_loss: 0.2577112019062042
train_iter_loss: 0.22940681874752045
train_iter_loss: 0.3013874888420105
train_iter_loss: 0.22150197625160217
train_iter_loss: 0.3180725574493408
train_iter_loss: 0.38355347514152527
train_iter_loss: 0.36120644211769104
train_iter_loss: 0.21180066466331482
train_iter_loss: 0.33138951659202576
train_iter_loss: 0.2691439390182495
train_iter_loss: 0.2307790219783783
train_iter_loss: 0.18734411895275116
train_iter_loss: 0.28978192806243896
train_iter_loss: 0.3249672055244446
train_iter_loss: 0.35737037658691406
train_iter_loss: 0.24189727008342743
train_iter_loss: 0.26639968156814575
train_iter_loss: 0.4271771311759949
train_iter_loss: 0.283834844827652
train_iter_loss: 0.2116410881280899
train_iter_loss: 0.19478970766067505
train_iter_loss: 0.2612425684928894
train_iter_loss: 0.32639095187187195
train_iter_loss: 0.22424423694610596
train_iter_loss: 0.12192298471927643
train_iter_loss: 0.29884305596351624
train_iter_loss: 0.3547075688838959
train_iter_loss: 0.3349343240261078
train_iter_loss: 0.18603290617465973
train_iter_loss: 0.2575385570526123
train_iter_loss: 0.09283409267663956
train_iter_loss: 0.16297647356987
train_iter_loss: 0.09239152073860168
train_iter_loss: 0.18739669024944305
train_iter_loss: 0.38962194323539734
train_iter_loss: 0.1035362109541893
train_iter_loss: 0.41806337237358093
train_iter_loss: 0.11220946162939072
train_iter_loss: 0.1972871720790863
train_iter_loss: 0.2270614206790924
train_iter_loss: 0.24237404763698578
train_iter_loss: 0.29694581031799316
train_iter_loss: 0.17122873663902283
train_iter_loss: 0.1335141360759735
train_iter_loss: 0.21260005235671997
train_iter_loss: 0.40990132093429565
train_iter_loss: 0.3024121820926666
train_iter_loss: 0.35355547070503235
train_iter_loss: 0.16489143669605255
train_iter_loss: 0.2242080271244049
train_iter_loss: 0.29827430844306946
train_iter_loss: 0.28073030710220337
train_iter_loss: 0.2818996012210846
train_iter_loss: 0.31226012110710144
train_iter_loss: 0.25703126192092896
train_iter_loss: 0.2864205539226532
train_iter_loss: 0.27687346935272217
train_iter_loss: 0.4897821247577667
train_iter_loss: 0.34988096356391907
train_iter_loss: 0.5010485053062439
train_iter_loss: 0.2898150682449341
train_iter_loss: 0.13233374059200287
train_iter_loss: 0.24220794439315796
train_iter_loss: 0.31192582845687866
train_iter_loss: 0.23988942801952362
train_iter_loss: 0.2821577489376068
train_iter_loss: 0.29255369305610657
train_iter_loss: 0.22771933674812317
train_iter_loss: 0.24483928084373474
train_iter_loss: 0.35050442814826965
train_iter_loss: 0.2315157651901245
train_iter_loss: 0.28266072273254395
train_iter_loss: 0.23577742278575897
train_iter_loss: 0.34490907192230225
train_iter_loss: 0.18670500814914703
train_iter_loss: 0.26853424310684204
train_iter_loss: 0.4392501711845398
train_iter_loss: 0.2425888180732727
train_iter_loss: 0.29680123925209045
train_iter_loss: 0.47972479462623596
train_iter_loss: 0.279723197221756
train_iter_loss: 0.2544616162776947
train_iter_loss: 0.1874961107969284
train_iter_loss: 0.21976622939109802
train_iter_loss: 0.25411227345466614
train_iter_loss: 0.4079762399196625
train_iter_loss: 0.3021782636642456
train_iter_loss: 0.08580806106328964
train_iter_loss: 0.2944748103618622
train_iter_loss: 0.4408423602581024
train loss :0.2752
---------------------
Validation seg loss: 0.3727892153667954 at epoch 498
epoch =    499/  1000, exp = train
train_iter_loss: 0.24945972859859467
train_iter_loss: 0.28359854221343994
train_iter_loss: 0.2597946226596832
train_iter_loss: 0.4598783254623413
train_iter_loss: 0.326770156621933
train_iter_loss: 0.19854623079299927
train_iter_loss: 0.28440412878990173
train_iter_loss: 0.4342097043991089
train_iter_loss: 0.1902756243944168
train_iter_loss: 0.38396650552749634
train_iter_loss: 0.25719162821769714
train_iter_loss: 0.177080899477005
train_iter_loss: 0.29632094502449036
train_iter_loss: 0.3288635313510895
train_iter_loss: 0.26308783888816833
train_iter_loss: 0.4243658781051636
train_iter_loss: 0.3052064776420593
train_iter_loss: 0.3536880314350128
train_iter_loss: 0.23504795134067535
train_iter_loss: 0.26024240255355835
train_iter_loss: 0.21246472001075745
train_iter_loss: 0.3150715231895447
train_iter_loss: 0.16121222078800201
train_iter_loss: 0.29188039898872375
train_iter_loss: 0.35853302478790283
train_iter_loss: 0.15592291951179504
train_iter_loss: 0.13935703039169312
train_iter_loss: 0.2506416440010071
train_iter_loss: 0.26284247636795044
train_iter_loss: 0.2218729853630066
train_iter_loss: 0.49707886576652527
train_iter_loss: 0.24314366281032562
train_iter_loss: 0.24835605919361115
train_iter_loss: 0.3359590172767639
train_iter_loss: 0.18854638934135437
train_iter_loss: 0.22511692345142365
train_iter_loss: 0.3496928811073303
train_iter_loss: 0.23385033011436462
train_iter_loss: 0.1642456203699112
train_iter_loss: 0.1816263645887375
train_iter_loss: 0.17319002747535706
train_iter_loss: 0.3467051088809967
train_iter_loss: 0.1673552691936493
train_iter_loss: 0.09169982373714447
train_iter_loss: 0.18247582018375397
train_iter_loss: 0.26701435446739197
train_iter_loss: 0.38655102252960205
train_iter_loss: 0.203877255320549
train_iter_loss: 0.33045822381973267
train_iter_loss: 0.21293780207633972
train_iter_loss: 0.2882154881954193
train_iter_loss: 0.2630440890789032
train_iter_loss: 0.09877936542034149
train_iter_loss: 0.2488212138414383
train_iter_loss: 0.24859842658042908
train_iter_loss: 0.31390300393104553
train_iter_loss: 0.39073753356933594
train_iter_loss: 0.3246805667877197
train_iter_loss: 0.45920372009277344
train_iter_loss: 0.266441285610199
train_iter_loss: 0.39227578043937683
train_iter_loss: 0.26264962553977966
train_iter_loss: 0.4272558093070984
train_iter_loss: 0.2066221386194229
train_iter_loss: 0.3462829887866974
train_iter_loss: 0.23222988843917847
train_iter_loss: 0.3422412872314453
train_iter_loss: 0.5607243776321411
train_iter_loss: 0.26980483531951904
train_iter_loss: 0.3441072404384613
train_iter_loss: 0.14627596735954285
train_iter_loss: 0.11311720311641693
train_iter_loss: 0.17531226575374603
train_iter_loss: 0.5297035574913025
train_iter_loss: 0.299654483795166
train_iter_loss: 0.26820608973503113
train_iter_loss: 0.2370917648077011
train_iter_loss: 0.1661044806241989
train_iter_loss: 0.202501580119133
train_iter_loss: 0.2967870533466339
train_iter_loss: 0.23325780034065247
train_iter_loss: 0.34704262018203735
train_iter_loss: 0.3624999225139618
train_iter_loss: 0.22919806838035583
train_iter_loss: 0.1575542837381363
train_iter_loss: 0.3944287598133087
train_iter_loss: 0.5940669775009155
train_iter_loss: 0.3494226634502411
train_iter_loss: 0.28837069869041443
train_iter_loss: 0.057896364480257034
train_iter_loss: 0.29174041748046875
train_iter_loss: 0.182358056306839
train_iter_loss: 0.19388718903064728
train_iter_loss: 0.41304853558540344
train_iter_loss: 0.14610812067985535
train_iter_loss: 0.2463214099407196
train_iter_loss: 0.31409046053886414
train_iter_loss: 0.15959015488624573
train_iter_loss: 0.07999350875616074
train_iter_loss: 0.26378369331359863
train loss :0.2767
---------------------
Validation seg loss: 0.3601322625324411 at epoch 499
epoch =    500/  1000, exp = train
train_iter_loss: 0.12529705464839935
train_iter_loss: 0.19564692676067352
train_iter_loss: 0.30757468938827515
train_iter_loss: 0.11341872811317444
train_iter_loss: 0.4054487943649292
train_iter_loss: 0.25428643822669983
train_iter_loss: 0.1573917269706726
train_iter_loss: 0.40359362959861755
train_iter_loss: 0.25932198762893677
train_iter_loss: 0.21170154213905334
train_iter_loss: 0.36746567487716675
train_iter_loss: 0.18666106462478638
train_iter_loss: 0.37677979469299316
train_iter_loss: 0.32884088158607483
train_iter_loss: 0.2876540422439575
train_iter_loss: 0.1665162593126297
train_iter_loss: 0.3701291084289551
train_iter_loss: 0.2155212014913559
train_iter_loss: 0.1538238227367401
train_iter_loss: 0.10609126836061478
train_iter_loss: 0.23420116305351257
train_iter_loss: 0.28078699111938477
train_iter_loss: 0.38372504711151123
train_iter_loss: 0.26773884892463684
train_iter_loss: 0.3549862205982208
train_iter_loss: 0.27252739667892456
train_iter_loss: 0.32841721177101135
train_iter_loss: 0.27236253023147583
train_iter_loss: 0.07640194147825241
train_iter_loss: 0.24413274228572845
train_iter_loss: 0.25724661350250244
train_iter_loss: 0.37087538838386536
train_iter_loss: 0.35403844714164734
train_iter_loss: 0.2583867609500885
train_iter_loss: 0.1952584832906723
train_iter_loss: 0.2826310694217682
train_iter_loss: 0.16402290761470795
train_iter_loss: 0.24560879170894623
train_iter_loss: 0.2307979315519333
train_iter_loss: 0.21367813646793365
train_iter_loss: 0.3355906903743744
train_iter_loss: 0.16279898583889008
train_iter_loss: 0.23772752285003662
train_iter_loss: 0.30385345220565796
train_iter_loss: 0.25274085998535156
train_iter_loss: 0.22734056413173676
train_iter_loss: 0.4379243552684784
train_iter_loss: 0.2727944254875183
train_iter_loss: 0.28586894273757935
train_iter_loss: 0.25555258989334106
train_iter_loss: 0.35036152601242065
train_iter_loss: 0.38984203338623047
train_iter_loss: 0.20425942540168762
train_iter_loss: 0.272077739238739
train_iter_loss: 0.14852258563041687
train_iter_loss: 0.37895074486732483
train_iter_loss: 0.3601776659488678
train_iter_loss: 0.22655436396598816
train_iter_loss: 0.33027541637420654
train_iter_loss: 0.31089067459106445
train_iter_loss: 0.3065236508846283
train_iter_loss: 0.2206898182630539
train_iter_loss: 0.21328617632389069
train_iter_loss: 0.39645513892173767
train_iter_loss: 0.23714353144168854
train_iter_loss: 0.4053097367286682
train_iter_loss: 0.10206085443496704
train_iter_loss: 0.251272588968277
train_iter_loss: 0.2971285879611969
train_iter_loss: 0.2844690978527069
train_iter_loss: 0.18054379522800446
train_iter_loss: 0.223393514752388
train_iter_loss: 0.2709219753742218
train_iter_loss: 0.18242628872394562
train_iter_loss: 0.1406850963830948
train_iter_loss: 0.27716147899627686
train_iter_loss: 0.2785164415836334
train_iter_loss: 0.3571786880493164
train_iter_loss: 0.21132677793502808
train_iter_loss: 0.3372846841812134
train_iter_loss: 0.19706228375434875
train_iter_loss: 0.21143855154514313
train_iter_loss: 0.13212458789348602
train_iter_loss: 0.397392213344574
train_iter_loss: 0.11587797105312347
train_iter_loss: 0.24033212661743164
train_iter_loss: 0.200834721326828
train_iter_loss: 0.23743681609630585
train_iter_loss: 0.21786580979824066
train_iter_loss: 0.228623628616333
train_iter_loss: 0.3959586024284363
train_iter_loss: 0.2920021116733551
train_iter_loss: 0.1833975911140442
train_iter_loss: 0.3580493628978729
train_iter_loss: 0.24793927371501923
train_iter_loss: 0.2795608341693878
train_iter_loss: 0.3652276396751404
train_iter_loss: 0.28528377413749695
train_iter_loss: 0.3312472403049469
train_iter_loss: 0.2881111800670624
train loss :0.2668
---------------------
Validation seg loss: 0.3584316232030825 at epoch 500
epoch =    501/  1000, exp = train
train_iter_loss: 0.280879408121109
train_iter_loss: 0.35383251309394836
train_iter_loss: 0.27544790506362915
train_iter_loss: 0.2636624872684479
train_iter_loss: 0.41369882225990295
train_iter_loss: 0.25641003251075745
train_iter_loss: 0.42678120732307434
train_iter_loss: 0.20636887848377228
train_iter_loss: 0.34133484959602356
train_iter_loss: 0.26270556449890137
train_iter_loss: 0.18751484155654907
train_iter_loss: 0.13313569128513336
train_iter_loss: 0.5186642408370972
train_iter_loss: 0.330078661441803
train_iter_loss: 0.3186921775341034
train_iter_loss: 0.2879466116428375
train_iter_loss: 0.3144092857837677
train_iter_loss: 0.30098438262939453
train_iter_loss: 0.4059538245201111
train_iter_loss: 0.1215854063630104
train_iter_loss: 0.26812225580215454
train_iter_loss: 0.25514763593673706
train_iter_loss: 0.24555087089538574
train_iter_loss: 0.10630939900875092
train_iter_loss: 0.3833221197128296
train_iter_loss: 0.24768200516700745
train_iter_loss: 0.29881665110588074
train_iter_loss: 0.2285141795873642
train_iter_loss: 0.26584962010383606
train_iter_loss: 0.3236803412437439
train_iter_loss: 0.21506085991859436
train_iter_loss: 0.17595064640045166
train_iter_loss: 0.2545589804649353
train_iter_loss: 0.275304913520813
train_iter_loss: 0.3633173704147339
train_iter_loss: 0.1306048184633255
train_iter_loss: 0.34711241722106934
train_iter_loss: 0.12307320535182953
train_iter_loss: 0.11922264099121094
train_iter_loss: 0.20023444294929504
train_iter_loss: 0.1626822054386139
train_iter_loss: 0.2534814178943634
train_iter_loss: 0.14407402276992798
train_iter_loss: 0.23329603672027588
train_iter_loss: 0.38274604082107544
train_iter_loss: 0.2176266461610794
train_iter_loss: 0.19973863661289215
train_iter_loss: 0.26772037148475647
train_iter_loss: 0.30941763520240784
train_iter_loss: 0.155608132481575
train_iter_loss: 0.28799450397491455
train_iter_loss: 0.16510926187038422
train_iter_loss: 0.2626754939556122
train_iter_loss: 0.27466145157814026
train_iter_loss: 0.26878854632377625
train_iter_loss: 0.1689702570438385
train_iter_loss: 0.2055564969778061
train_iter_loss: 0.20157490670681
train_iter_loss: 0.1141979843378067
train_iter_loss: 0.2693922519683838
train_iter_loss: 0.1884361058473587
train_iter_loss: 0.3088493347167969
train_iter_loss: 0.15696915984153748
train_iter_loss: 0.21973247826099396
train_iter_loss: 0.32173749804496765
train_iter_loss: 0.27158698439598083
train_iter_loss: 0.23312005400657654
train_iter_loss: 0.28630760312080383
train_iter_loss: 0.47513413429260254
train_iter_loss: 0.30842339992523193
train_iter_loss: 0.19581125676631927
train_iter_loss: 0.29944664239883423
train_iter_loss: 0.2732661962509155
train_iter_loss: 0.4135693907737732
train_iter_loss: 0.1423773169517517
train_iter_loss: 0.1987466961145401
train_iter_loss: 0.1924026757478714
train_iter_loss: 0.22984163463115692
train_iter_loss: 0.20104441046714783
train_iter_loss: 0.29427704215049744
train_iter_loss: 0.28822875022888184
train_iter_loss: 0.43695133924484253
train_iter_loss: 0.2242678701877594
train_iter_loss: 0.20681488513946533
train_iter_loss: 0.2592988908290863
train_iter_loss: 0.32194289565086365
train_iter_loss: 0.3661879599094391
train_iter_loss: 0.124080128967762
train_iter_loss: 0.1795129030942917
train_iter_loss: 0.39383387565612793
train_iter_loss: 0.5336218476295471
train_iter_loss: 0.3446580469608307
train_iter_loss: 0.1943681240081787
train_iter_loss: 0.27313438057899475
train_iter_loss: 0.434587687253952
train_iter_loss: 0.21729503571987152
train_iter_loss: 0.3200873136520386
train_iter_loss: 0.24987563490867615
train_iter_loss: 0.44132596254348755
train_iter_loss: 0.3534453809261322
train loss :0.2701
---------------------
Validation seg loss: 0.35840676803985294 at epoch 501
epoch =    502/  1000, exp = train
train_iter_loss: 0.27438732981681824
train_iter_loss: 0.1974652111530304
train_iter_loss: 0.3108194172382355
train_iter_loss: 0.13179552555084229
train_iter_loss: 0.11967822164297104
train_iter_loss: 0.2864435911178589
train_iter_loss: 0.25349482893943787
train_iter_loss: 0.2978503406047821
train_iter_loss: 0.41316497325897217
train_iter_loss: 0.22962628304958344
train_iter_loss: 0.4209117591381073
train_iter_loss: 0.13542324304580688
train_iter_loss: 0.3643549382686615
train_iter_loss: 0.25541216135025024
train_iter_loss: 0.11023327708244324
train_iter_loss: 0.3494575321674347
train_iter_loss: 0.24116355180740356
train_iter_loss: 0.2506803870201111
train_iter_loss: 0.29749858379364014
train_iter_loss: 0.3451061248779297
train_iter_loss: 0.19087880849838257
train_iter_loss: 0.22983458638191223
train_iter_loss: 0.22929514944553375
train_iter_loss: 0.23395608365535736
train_iter_loss: 0.2901341915130615
train_iter_loss: 0.20970562100410461
train_iter_loss: 0.39534512162208557
train_iter_loss: 0.21622835099697113
train_iter_loss: 0.24226662516593933
train_iter_loss: 0.2775452136993408
train_iter_loss: 0.23414097726345062
train_iter_loss: 0.4219316840171814
train_iter_loss: 0.4313270151615143
train_iter_loss: 0.43832874298095703
train_iter_loss: 0.3442124128341675
train_iter_loss: 0.2776237428188324
train_iter_loss: 0.29354771971702576
train_iter_loss: 0.41884762048721313
train_iter_loss: 0.11136012524366379
train_iter_loss: 0.2831690013408661
train_iter_loss: 0.21690469980239868
train_iter_loss: 0.35158267617225647
train_iter_loss: 0.2912697196006775
train_iter_loss: 0.26393306255340576
train_iter_loss: 0.27757447957992554
train_iter_loss: 0.2925332188606262
train_iter_loss: 0.49204781651496887
train_iter_loss: 0.3255462050437927
train_iter_loss: 0.3626452088356018
train_iter_loss: 0.20205125212669373
train_iter_loss: 0.28859570622444153
train_iter_loss: 0.2669259309768677
train_iter_loss: 0.11407987773418427
train_iter_loss: 0.23069733381271362
train_iter_loss: 0.16639229655265808
train_iter_loss: 0.18584708869457245
train_iter_loss: 0.2212532013654709
train_iter_loss: 0.18398629128932953
train_iter_loss: 0.1328296959400177
train_iter_loss: 0.256621778011322
train_iter_loss: 0.2878274917602539
train_iter_loss: 0.31839221715927124
train_iter_loss: 0.18295976519584656
train_iter_loss: 0.411617249250412
train_iter_loss: 0.24285703897476196
train_iter_loss: 0.2749300003051758
train_iter_loss: 0.21375611424446106
train_iter_loss: 0.20867988467216492
train_iter_loss: 0.2894277572631836
train_iter_loss: 0.20786996185779572
train_iter_loss: 0.2931711971759796
train_iter_loss: 0.2353399693965912
train_iter_loss: 0.21766303479671478
train_iter_loss: 0.28421059250831604
train_iter_loss: 0.15189586579799652
train_iter_loss: 0.3282085657119751
train_iter_loss: 0.35090163350105286
train_iter_loss: 0.2652933895587921
train_iter_loss: 0.17875811457633972
train_iter_loss: 0.2586003541946411
train_iter_loss: 0.28607797622680664
train_iter_loss: 0.18625937402248383
train_iter_loss: 0.3005266785621643
train_iter_loss: 0.30252301692962646
train_iter_loss: 0.2715243101119995
train_iter_loss: 0.2806309461593628
train_iter_loss: 0.15007224678993225
train_iter_loss: 0.3682149350643158
train_iter_loss: 0.2692467272281647
train_iter_loss: 0.24289488792419434
train_iter_loss: 0.3662261366844177
train_iter_loss: 0.22701983153820038
train_iter_loss: 0.26487594842910767
train_iter_loss: 0.2559537887573242
train_iter_loss: 0.19375170767307281
train_iter_loss: 0.27138158679008484
train_iter_loss: 0.22488319873809814
train_iter_loss: 0.3115607798099518
train_iter_loss: 0.23171588778495789
train_iter_loss: 0.26054301857948303
train loss :0.2692
---------------------
Validation seg loss: 0.36570451661663234 at epoch 502
epoch =    503/  1000, exp = train
train_iter_loss: 0.24972526729106903
train_iter_loss: 0.40107762813568115
train_iter_loss: 0.23228684067726135
train_iter_loss: 0.34081411361694336
train_iter_loss: 0.13852958381175995
train_iter_loss: 0.13916067779064178
train_iter_loss: 0.18983644247055054
train_iter_loss: 0.1284235566854477
train_iter_loss: 0.2330895960330963
train_iter_loss: 0.25579941272735596
train_iter_loss: 0.34226787090301514
train_iter_loss: 0.2953866124153137
train_iter_loss: 0.20431913435459137
train_iter_loss: 0.46815574169158936
train_iter_loss: 0.15032657980918884
train_iter_loss: 0.21102818846702576
train_iter_loss: 0.4357319176197052
train_iter_loss: 0.09979112446308136
train_iter_loss: 0.20747441053390503
train_iter_loss: 0.14244180917739868
train_iter_loss: 0.22199876606464386
train_iter_loss: 0.25171828269958496
train_iter_loss: 0.3888041377067566
train_iter_loss: 0.34725823998451233
train_iter_loss: 0.37458306550979614
train_iter_loss: 0.22214941680431366
train_iter_loss: 0.3534010350704193
train_iter_loss: 0.24135053157806396
train_iter_loss: 0.18999478220939636
train_iter_loss: 0.3191271722316742
train_iter_loss: 0.2813023328781128
train_iter_loss: 0.26532793045043945
train_iter_loss: 0.22713081538677216
train_iter_loss: 0.36769285798072815
train_iter_loss: 0.34693288803100586
train_iter_loss: 0.24456317722797394
train_iter_loss: 0.3050433099269867
train_iter_loss: 0.4122280180454254
train_iter_loss: 0.18941150605678558
train_iter_loss: 0.14643044769763947
train_iter_loss: 0.3581679165363312
train_iter_loss: 0.22267304360866547
train_iter_loss: 0.3032723069190979
train_iter_loss: 0.18726105988025665
train_iter_loss: 0.24144375324249268
train_iter_loss: 0.25636038184165955
train_iter_loss: 0.2406640499830246
train_iter_loss: 0.2981717884540558
train_iter_loss: 0.34314435720443726
train_iter_loss: 0.29164162278175354
train_iter_loss: 0.21014761924743652
train_iter_loss: 0.2771115303039551
train_iter_loss: 0.11898017674684525
train_iter_loss: 0.12414911389350891
train_iter_loss: 0.20493432879447937
train_iter_loss: 0.14621751010417938
train_iter_loss: 0.4591858685016632
train_iter_loss: 0.2153748720884323
train_iter_loss: 0.384713739156723
train_iter_loss: 0.4747152030467987
train_iter_loss: 0.20438776910305023
train_iter_loss: 0.31818148493766785
train_iter_loss: 0.3496010899543762
train_iter_loss: 0.4937893748283386
train_iter_loss: 0.2370857149362564
train_iter_loss: 0.1805141419172287
train_iter_loss: 0.3397877812385559
train_iter_loss: 0.1485777497291565
train_iter_loss: 0.2925344705581665
train_iter_loss: 0.281723290681839
train_iter_loss: 0.355495810508728
train_iter_loss: 0.1660832166671753
train_iter_loss: 0.35151034593582153
train_iter_loss: 0.4188317656517029
train_iter_loss: 0.13658766448497772
train_iter_loss: 0.21958880126476288
train_iter_loss: 0.3392132818698883
train_iter_loss: 0.14675697684288025
train_iter_loss: 0.20819813013076782
train_iter_loss: 0.1429787278175354
train_iter_loss: 0.20740781724452972
train_iter_loss: 0.18303453922271729
train_iter_loss: 0.2793370187282562
train_iter_loss: 0.10844752192497253
train_iter_loss: 0.33160316944122314
train_iter_loss: 0.25954362750053406
train_iter_loss: 0.1750984489917755
train_iter_loss: 0.2594814598560333
train_iter_loss: 0.24137748777866364
train_iter_loss: 0.39499199390411377
train_iter_loss: 0.23936249315738678
train_iter_loss: 0.25879502296447754
train_iter_loss: 0.29400238394737244
train_iter_loss: 0.2915578782558441
train_iter_loss: 0.4013455808162689
train_iter_loss: 0.19951987266540527
train_iter_loss: 0.3465604782104492
train_iter_loss: 0.305433988571167
train_iter_loss: 0.24931256473064423
train_iter_loss: 0.268241286277771
train loss :0.2682
---------------------
Validation seg loss: 0.3616906705593585 at epoch 503
epoch =    504/  1000, exp = train
train_iter_loss: 0.2858591079711914
train_iter_loss: 0.13790816068649292
train_iter_loss: 0.2612511217594147
train_iter_loss: 0.2304508090019226
train_iter_loss: 0.29060426354408264
train_iter_loss: 0.46563008427619934
train_iter_loss: 0.3896327614784241
train_iter_loss: 0.23108024895191193
train_iter_loss: 0.19573646783828735
train_iter_loss: 0.22521518170833588
train_iter_loss: 0.26412704586982727
train_iter_loss: 0.3004527688026428
train_iter_loss: 0.1823621392250061
train_iter_loss: 0.12788033485412598
train_iter_loss: 0.26505187153816223
train_iter_loss: 0.258240282535553
train_iter_loss: 0.17068485915660858
train_iter_loss: 0.2648845314979553
train_iter_loss: 0.11255891621112823
train_iter_loss: 0.32403114438056946
train_iter_loss: 0.3039618730545044
train_iter_loss: 0.22486162185668945
train_iter_loss: 0.22796210646629333
train_iter_loss: 0.30198755860328674
train_iter_loss: 0.2666222155094147
train_iter_loss: 0.23289711773395538
train_iter_loss: 0.31235212087631226
train_iter_loss: 0.26902586221694946
train_iter_loss: 0.13237571716308594
train_iter_loss: 0.3050658404827118
train_iter_loss: 0.22030965983867645
train_iter_loss: 0.43384018540382385
train_iter_loss: 0.08076933771371841
train_iter_loss: 0.4108971953392029
train_iter_loss: 0.3524222671985626
train_iter_loss: 0.29283779859542847
train_iter_loss: 0.3605439066886902
train_iter_loss: 0.20650121569633484
train_iter_loss: 0.2740693688392639
train_iter_loss: 0.20043517649173737
train_iter_loss: 0.49125751852989197
train_iter_loss: 0.29598116874694824
train_iter_loss: 0.24311800301074982
train_iter_loss: 0.19977425038814545
train_iter_loss: 0.2390141636133194
train_iter_loss: 0.47090795636177063
train_iter_loss: 0.18496103584766388
train_iter_loss: 0.2501387894153595
train_iter_loss: 0.1920280009508133
train_iter_loss: 0.1789269894361496
train_iter_loss: 0.0375896580517292
train_iter_loss: 0.3522191345691681
train_iter_loss: 0.2641202509403229
train_iter_loss: 0.3181345760822296
train_iter_loss: 0.29336777329444885
train_iter_loss: 0.20754790306091309
train_iter_loss: 0.6460452675819397
train_iter_loss: 0.22741222381591797
train_iter_loss: 0.26149192452430725
train_iter_loss: 0.11524497717618942
train_iter_loss: 0.33817094564437866
train_iter_loss: 0.27826550602912903
train_iter_loss: 0.2595747113227844
train_iter_loss: 0.28314661979675293
train_iter_loss: 0.12800554931163788
train_iter_loss: 0.2327198088169098
train_iter_loss: 0.4294776916503906
train_iter_loss: 0.3251076936721802
train_iter_loss: 0.35417208075523376
train_iter_loss: 0.35962533950805664
train_iter_loss: 0.4738301932811737
train_iter_loss: 0.31071901321411133
train_iter_loss: 0.2331204116344452
train_iter_loss: 0.25466111302375793
train_iter_loss: 0.1663701981306076
train_iter_loss: 0.21974045038223267
train_iter_loss: 0.23607641458511353
train_iter_loss: 0.29277753829956055
train_iter_loss: 0.25187212228775024
train_iter_loss: 0.5030484795570374
train_iter_loss: 0.23751279711723328
train_iter_loss: 0.5525726675987244
train_iter_loss: 0.1489093154668808
train_iter_loss: 0.32450541853904724
train_iter_loss: 0.2830016613006592
train_iter_loss: 0.24547745287418365
train_iter_loss: 0.16958194971084595
train_iter_loss: 0.19933544099330902
train_iter_loss: 0.40724387764930725
train_iter_loss: 0.36160922050476074
train_iter_loss: 0.4446820318698883
train_iter_loss: 0.17426665127277374
train_iter_loss: 0.3237060010433197
train_iter_loss: 0.2473408430814743
train_iter_loss: 0.34909388422966003
train_iter_loss: 0.27887991070747375
train_iter_loss: 0.2938232719898224
train_iter_loss: 0.283133864402771
train_iter_loss: 0.14039172232151031
train_iter_loss: 0.4130639433860779
train loss :0.2794
---------------------
Validation seg loss: 0.3495120581169173 at epoch 504
epoch =    505/  1000, exp = train
train_iter_loss: 0.2036411166191101
train_iter_loss: 0.15630099177360535
train_iter_loss: 0.3173491060733795
train_iter_loss: 0.1205381453037262
train_iter_loss: 0.3612166941165924
train_iter_loss: 0.18687662482261658
train_iter_loss: 0.2960825562477112
train_iter_loss: 0.3382378816604614
train_iter_loss: 0.15910272300243378
train_iter_loss: 0.1315619945526123
train_iter_loss: 0.2766968011856079
train_iter_loss: 0.1902376264333725
train_iter_loss: 0.3306896984577179
train_iter_loss: 0.21567992866039276
train_iter_loss: 0.21581728756427765
train_iter_loss: 0.31877371668815613
train_iter_loss: 0.23534174263477325
train_iter_loss: 0.45579150319099426
train_iter_loss: 0.33149614930152893
train_iter_loss: 0.5088496804237366
train_iter_loss: 0.231157124042511
train_iter_loss: 0.20462515950202942
train_iter_loss: 0.5122029185295105
train_iter_loss: 0.15396863222122192
train_iter_loss: 0.1535276174545288
train_iter_loss: 0.3688117563724518
train_iter_loss: 0.41725268959999084
train_iter_loss: 0.21068891882896423
train_iter_loss: 0.12755794823169708
train_iter_loss: 0.3277159035205841
train_iter_loss: 0.32433557510375977
train_iter_loss: 0.305742472410202
train_iter_loss: 0.37968507409095764
train_iter_loss: 0.26719221472740173
train_iter_loss: 0.2649409770965576
train_iter_loss: 0.5077721476554871
train_iter_loss: 0.37918365001678467
train_iter_loss: 0.2596820592880249
train_iter_loss: 0.22253823280334473
train_iter_loss: 0.1804945319890976
train_iter_loss: 0.24550779163837433
train_iter_loss: 0.18630559742450714
train_iter_loss: 0.14211367070674896
train_iter_loss: 0.2458115965127945
train_iter_loss: 0.29085591435432434
train_iter_loss: 0.2339809685945511
train_iter_loss: 0.32074084877967834
train_iter_loss: 0.3838634788990021
train_iter_loss: 0.1754554659128189
train_iter_loss: 0.3004913628101349
train_iter_loss: 0.33076900243759155
train_iter_loss: 0.40158358216285706
train_iter_loss: 0.3043355941772461
train_iter_loss: 0.13590241968631744
train_iter_loss: 0.16386665403842926
train_iter_loss: 0.20466706156730652
train_iter_loss: 0.235421821475029
train_iter_loss: 0.23524236679077148
train_iter_loss: 0.3356161117553711
train_iter_loss: 0.32994344830513
train_iter_loss: 0.3089413046836853
train_iter_loss: 0.2382250279188156
train_iter_loss: 0.20002077519893646
train_iter_loss: 0.277006596326828
train_iter_loss: 0.135252445936203
train_iter_loss: 0.3080091178417206
train_iter_loss: 0.287998765707016
train_iter_loss: 0.3037225604057312
train_iter_loss: 0.25805607438087463
train_iter_loss: 0.3802180290222168
train_iter_loss: 0.16028960049152374
train_iter_loss: 0.12959681451320648
train_iter_loss: 0.1978534758090973
train_iter_loss: 0.3199582099914551
train_iter_loss: 0.24077646434307098
train_iter_loss: 0.49818190932273865
train_iter_loss: 0.31380027532577515
train_iter_loss: 0.2814079523086548
train_iter_loss: 0.2991112172603607
train_iter_loss: 0.3332136869430542
train_iter_loss: 0.29762083292007446
train_iter_loss: 0.3685635030269623
train_iter_loss: 0.31889504194259644
train_iter_loss: 0.2697201371192932
train_iter_loss: 0.1985486000776291
train_iter_loss: 0.275115042924881
train_iter_loss: 0.2526346743106842
train_iter_loss: 0.41492441296577454
train_iter_loss: 0.1689797341823578
train_iter_loss: 0.3137603998184204
train_iter_loss: 0.19384247064590454
train_iter_loss: 0.23911483585834503
train_iter_loss: 0.20425407588481903
train_iter_loss: 0.30091387033462524
train_iter_loss: 0.2736707329750061
train_iter_loss: 0.11511155962944031
train_iter_loss: 0.24826230108737946
train_iter_loss: 0.3274802267551422
train_iter_loss: 0.20347368717193604
train_iter_loss: 0.44073784351348877
train loss :0.2753
---------------------
Validation seg loss: 0.3744317119290947 at epoch 505
epoch =    506/  1000, exp = train
train_iter_loss: 0.38510408997535706
train_iter_loss: 0.19692519307136536
train_iter_loss: 0.26854756474494934
train_iter_loss: 0.24370290338993073
train_iter_loss: 0.19444705545902252
train_iter_loss: 0.4330010414123535
train_iter_loss: 0.35428497195243835
train_iter_loss: 0.3945981562137604
train_iter_loss: 0.08394201844930649
train_iter_loss: 0.47380900382995605
train_iter_loss: 0.10161137580871582
train_iter_loss: 0.3135548233985901
train_iter_loss: 0.3236093819141388
train_iter_loss: 0.18750575184822083
train_iter_loss: 0.21121534705162048
train_iter_loss: 0.2731259763240814
train_iter_loss: 0.3044195771217346
train_iter_loss: 0.3901366591453552
train_iter_loss: 0.2224585860967636
train_iter_loss: 0.36061134934425354
train_iter_loss: 0.275223970413208
train_iter_loss: 0.2970992624759674
train_iter_loss: 0.19813427329063416
train_iter_loss: 0.33915001153945923
train_iter_loss: 0.2852855324745178
train_iter_loss: 0.4310954213142395
train_iter_loss: 0.24786750972270966
train_iter_loss: 0.32782837748527527
train_iter_loss: 0.22225597500801086
train_iter_loss: 0.2914724051952362
train_iter_loss: 0.2227497696876526
train_iter_loss: 0.25323784351348877
train_iter_loss: 0.42899829149246216
train_iter_loss: 0.345280259847641
train_iter_loss: 0.14984230697155
train_iter_loss: 0.3463650047779083
train_iter_loss: 0.20836740732192993
train_iter_loss: 0.4073517620563507
train_iter_loss: 0.16014191508293152
train_iter_loss: 0.06900753825902939
train_iter_loss: 0.3328005373477936
train_iter_loss: 0.3117515444755554
train_iter_loss: 0.28314751386642456
train_iter_loss: 0.29183170199394226
train_iter_loss: 0.23554310202598572
train_iter_loss: 0.322315514087677
train_iter_loss: 0.2576937675476074
train_iter_loss: 0.29046136140823364
train_iter_loss: 0.14162477850914001
train_iter_loss: 0.1353447139263153
train_iter_loss: 0.2570740878582001
train_iter_loss: 0.2342740148305893
train_iter_loss: 0.666469156742096
train_iter_loss: 0.3084629476070404
train_iter_loss: 0.16584759950637817
train_iter_loss: 0.2695479393005371
train_iter_loss: 0.2803843319416046
train_iter_loss: 0.185430645942688
train_iter_loss: 0.1847451776266098
train_iter_loss: 0.2722155749797821
train_iter_loss: 0.42127686738967896
train_iter_loss: 0.26828694343566895
train_iter_loss: 0.3617388904094696
train_iter_loss: 0.20623773336410522
train_iter_loss: 0.31676068902015686
train_iter_loss: 0.2076922208070755
train_iter_loss: 0.2720797657966614
train_iter_loss: 0.20236167311668396
train_iter_loss: 0.3414948880672455
train_iter_loss: 0.3164423406124115
train_iter_loss: 0.28623732924461365
train_iter_loss: 0.18869712948799133
train_iter_loss: 0.17696928977966309
train_iter_loss: 0.17314401268959045
train_iter_loss: 0.34438356757164
train_iter_loss: 0.24573251605033875
train_iter_loss: 0.3247411549091339
train_iter_loss: 0.34261319041252136
train_iter_loss: 0.40281569957733154
train_iter_loss: 0.1911645382642746
train_iter_loss: 0.1442531943321228
train_iter_loss: 0.25589609146118164
train_iter_loss: 0.28786125779151917
train_iter_loss: 0.3189031183719635
train_iter_loss: 0.16564898192882538
train_iter_loss: 0.25732311606407166
train_iter_loss: 0.30415064096450806
train_iter_loss: 0.1750929206609726
train_iter_loss: 0.18292230367660522
train_iter_loss: 0.3489898443222046
train_iter_loss: 0.1703168749809265
train_iter_loss: 0.2578372359275818
train_iter_loss: 0.2710774540901184
train_iter_loss: 0.18253621459007263
train_iter_loss: 0.28386232256889343
train_iter_loss: 0.215802863240242
train_iter_loss: 0.37442323565483093
train_iter_loss: 0.19925454258918762
train_iter_loss: 0.3553745448589325
train_iter_loss: 0.2893144190311432
train loss :0.2756
---------------------
Validation seg loss: 0.35387725426094996 at epoch 506
epoch =    507/  1000, exp = train
train_iter_loss: 0.43640950322151184
train_iter_loss: 0.3407139182090759
train_iter_loss: 0.2766698896884918
train_iter_loss: 0.2610977590084076
train_iter_loss: 0.2384149134159088
train_iter_loss: 0.16219587624073029
train_iter_loss: 0.33038562536239624
train_iter_loss: 0.25429853796958923
train_iter_loss: 0.22404739260673523
train_iter_loss: 0.12435787916183472
train_iter_loss: 0.3432904779911041
train_iter_loss: 0.33966773748397827
train_iter_loss: 0.290452241897583
train_iter_loss: 0.13691969215869904
train_iter_loss: 0.47613298892974854
train_iter_loss: 0.2425101101398468
train_iter_loss: 0.2228611558675766
train_iter_loss: 0.33347973227500916
train_iter_loss: 0.3293231427669525
train_iter_loss: 0.2506217360496521
train_iter_loss: 0.32647010684013367
train_iter_loss: 0.22172267735004425
train_iter_loss: 0.2540660798549652
train_iter_loss: 0.31549975275993347
train_iter_loss: 0.3154791295528412
train_iter_loss: 0.2711317539215088
train_iter_loss: 0.31516405940055847
train_iter_loss: 0.1858188807964325
train_iter_loss: 0.24165193736553192
train_iter_loss: 0.2718542218208313
train_iter_loss: 0.2565560042858124
train_iter_loss: 0.24997565150260925
train_iter_loss: 0.1338929831981659
train_iter_loss: 0.2765118181705475
train_iter_loss: 0.33007460832595825
train_iter_loss: 0.25608304142951965
train_iter_loss: 0.2735525667667389
train_iter_loss: 0.4581245183944702
train_iter_loss: 0.2685607075691223
train_iter_loss: 0.15799938142299652
train_iter_loss: 0.3565259575843811
train_iter_loss: 0.1522013545036316
train_iter_loss: 0.3142364025115967
train_iter_loss: 0.302836149930954
train_iter_loss: 0.13060520589351654
train_iter_loss: 0.18733777105808258
train_iter_loss: 0.26276522874832153
train_iter_loss: 0.1665429323911667
train_iter_loss: 0.27518245577812195
train_iter_loss: 0.17599833011627197
train_iter_loss: 0.18613533675670624
train_iter_loss: 0.27543941140174866
train_iter_loss: 0.1950712949037552
train_iter_loss: 0.2910183072090149
train_iter_loss: 0.17858393490314484
train_iter_loss: 0.35797110199928284
train_iter_loss: 0.25740712881088257
train_iter_loss: 0.44448164105415344
train_iter_loss: 0.20371267199516296
train_iter_loss: 0.15073741972446442
train_iter_loss: 0.37072429060935974
train_iter_loss: 0.3452737331390381
train_iter_loss: 0.29459792375564575
train_iter_loss: 0.14959821105003357
train_iter_loss: 0.2588125765323639
train_iter_loss: 0.1893167793750763
train_iter_loss: 0.33890649676322937
train_iter_loss: 0.3355541527271271
train_iter_loss: 0.17705053091049194
train_iter_loss: 0.318087637424469
train_iter_loss: 0.29509225487709045
train_iter_loss: 0.26951509714126587
train_iter_loss: 0.07092352211475372
train_iter_loss: 0.09685379266738892
train_iter_loss: 0.18995141983032227
train_iter_loss: 0.275574266910553
train_iter_loss: 0.552510142326355
train_iter_loss: 0.39781713485717773
train_iter_loss: 0.22195163369178772
train_iter_loss: 0.31486931443214417
train_iter_loss: 0.24564307928085327
train_iter_loss: 0.268551766872406
train_iter_loss: 0.3568653464317322
train_iter_loss: 0.22449521720409393
train_iter_loss: 0.20924007892608643
train_iter_loss: 0.22960913181304932
train_iter_loss: 0.23728933930397034
train_iter_loss: 0.24688740074634552
train_iter_loss: 0.2652209401130676
train_iter_loss: 0.23050694167613983
train_iter_loss: 0.45047903060913086
train_iter_loss: 0.22450435161590576
train_iter_loss: 0.25793692469596863
train_iter_loss: 0.33136898279190063
train_iter_loss: 0.39811041951179504
train_iter_loss: 0.18387560546398163
train_iter_loss: 0.20687109231948853
train_iter_loss: 0.1638902723789215
train_iter_loss: 0.1565701961517334
train_iter_loss: 0.23562732338905334
train loss :0.2672
---------------------
Validation seg loss: 0.35875870156945344 at epoch 507
epoch =    508/  1000, exp = train
train_iter_loss: 0.12437894195318222
train_iter_loss: 0.16764989495277405
train_iter_loss: 0.4521436095237732
train_iter_loss: 0.29446208477020264
train_iter_loss: 0.2083951085805893
train_iter_loss: 0.11731329560279846
train_iter_loss: 0.25619885325431824
train_iter_loss: 0.3567398190498352
train_iter_loss: 0.24706901609897614
train_iter_loss: 0.3033128082752228
train_iter_loss: 0.32147926092147827
train_iter_loss: 0.21754826605319977
train_iter_loss: 0.3656918406486511
train_iter_loss: 0.18650709092617035
train_iter_loss: 0.2070208489894867
train_iter_loss: 0.16027352213859558
train_iter_loss: 0.43861979246139526
train_iter_loss: 0.19641228020191193
train_iter_loss: 0.09536321461200714
train_iter_loss: 0.17467565834522247
train_iter_loss: 0.3184329867362976
train_iter_loss: 0.1990249902009964
train_iter_loss: 0.28866708278656006
train_iter_loss: 0.39340266585350037
train_iter_loss: 0.2497372180223465
train_iter_loss: 0.28763335943222046
train_iter_loss: 0.2703169584274292
train_iter_loss: 0.28442853689193726
train_iter_loss: 0.23974435031414032
train_iter_loss: 0.2323557734489441
train_iter_loss: 0.20728155970573425
train_iter_loss: 0.3404959440231323
train_iter_loss: 0.3354504704475403
train_iter_loss: 0.2755054533481598
train_iter_loss: 0.2896105945110321
train_iter_loss: 0.26250869035720825
train_iter_loss: 0.32839974761009216
train_iter_loss: 0.11946524679660797
train_iter_loss: 0.43822047114372253
train_iter_loss: 0.3282082974910736
train_iter_loss: 0.3515329360961914
train_iter_loss: 0.26422029733657837
train_iter_loss: 0.24269677698612213
train_iter_loss: 0.2992955148220062
train_iter_loss: 0.46986791491508484
train_iter_loss: 0.3077620267868042
train_iter_loss: 0.23175406455993652
train_iter_loss: 0.19726476073265076
train_iter_loss: 0.2372644990682602
train_iter_loss: 0.215916708111763
train_iter_loss: 0.5116273760795593
train_iter_loss: 0.2022300511598587
train_iter_loss: 0.38713979721069336
train_iter_loss: 0.3147699236869812
train_iter_loss: 0.20794908702373505
train_iter_loss: 0.19288292527198792
train_iter_loss: 0.27149373292922974
train_iter_loss: 0.35205793380737305
train_iter_loss: 0.22912214696407318
train_iter_loss: 0.361812561750412
train_iter_loss: 0.2783852517604828
train_iter_loss: 0.5268384218215942
train_iter_loss: 0.2616841495037079
train_iter_loss: 0.39233556389808655
train_iter_loss: 0.16575680673122406
train_iter_loss: 0.2964911162853241
train_iter_loss: 0.35938453674316406
train_iter_loss: 0.24086545407772064
train_iter_loss: 0.4548804461956024
train_iter_loss: 0.1405714452266693
train_iter_loss: 0.11014147847890854
train_iter_loss: 0.27604740858078003
train_iter_loss: 0.29447826743125916
train_iter_loss: 0.15683944523334503
train_iter_loss: 0.3065711259841919
train_iter_loss: 0.3110339045524597
train_iter_loss: 0.09645803272724152
train_iter_loss: 0.17737802863121033
train_iter_loss: 0.22034813463687897
train_iter_loss: 0.18335410952568054
train_iter_loss: 0.2211294025182724
train_iter_loss: 0.189656600356102
train_iter_loss: 0.14829035103321075
train_iter_loss: 0.19578279554843903
train_iter_loss: 0.23887352645397186
train_iter_loss: 0.36018484830856323
train_iter_loss: 0.3557569980621338
train_iter_loss: 0.3709794580936432
train_iter_loss: 0.1729833483695984
train_iter_loss: 0.2973061800003052
train_iter_loss: 0.1897585391998291
train_iter_loss: 0.42384618520736694
train_iter_loss: 0.3134876489639282
train_iter_loss: 0.3474847972393036
train_iter_loss: 0.3230223059654236
train_iter_loss: 0.2344924360513687
train_iter_loss: 0.12775561213493347
train_iter_loss: 0.2702430188655853
train_iter_loss: 0.152593195438385
train_iter_loss: 0.3655497133731842
train loss :0.2725
---------------------
Validation seg loss: 0.4092441877077843 at epoch 508
epoch =    509/  1000, exp = train
train_iter_loss: 0.315066397190094
train_iter_loss: 0.20573392510414124
train_iter_loss: 0.06606016308069229
train_iter_loss: 0.16340100765228271
train_iter_loss: 0.18503786623477936
train_iter_loss: 0.08626702427864075
train_iter_loss: 0.2587924003601074
train_iter_loss: 0.3093659281730652
train_iter_loss: 0.29160547256469727
train_iter_loss: 0.1272454410791397
train_iter_loss: 0.27522504329681396
train_iter_loss: 0.17241331934928894
train_iter_loss: 0.1026969850063324
train_iter_loss: 0.33631929755210876
train_iter_loss: 0.33252009749412537
train_iter_loss: 0.24715082347393036
train_iter_loss: 0.25887003540992737
train_iter_loss: 0.2533077597618103
train_iter_loss: 0.18552866578102112
train_iter_loss: 0.15304550528526306
train_iter_loss: 0.19260524213314056
train_iter_loss: 0.21036532521247864
train_iter_loss: 0.2140687108039856
train_iter_loss: 0.3786165714263916
train_iter_loss: 0.32567858695983887
train_iter_loss: 0.19261875748634338
train_iter_loss: 0.28602054715156555
train_iter_loss: 0.31079351902008057
train_iter_loss: 0.2302049845457077
train_iter_loss: 0.2675846219062805
train_iter_loss: 0.43409979343414307
train_iter_loss: 0.4572695791721344
train_iter_loss: 0.2644646465778351
train_iter_loss: 0.15699663758277893
train_iter_loss: 0.47565609216690063
train_iter_loss: 0.286297470331192
train_iter_loss: 0.23219691216945648
train_iter_loss: 0.3478052318096161
train_iter_loss: 0.32930150628089905
train_iter_loss: 0.17957933247089386
train_iter_loss: 0.27761074900627136
train_iter_loss: 0.25377917289733887
train_iter_loss: 0.21911057829856873
train_iter_loss: 0.3776454031467438
train_iter_loss: 0.4281547963619232
train_iter_loss: 0.11138103157281876
train_iter_loss: 0.255499929189682
train_iter_loss: 0.20163854956626892
train_iter_loss: 0.2819706201553345
train_iter_loss: 0.25350016355514526
train_iter_loss: 0.31461474299430847
train_iter_loss: 0.2161921262741089
train_iter_loss: 0.2509325444698334
train_iter_loss: 0.24262604117393494
train_iter_loss: 0.3957079350948334
train_iter_loss: 0.43765461444854736
train_iter_loss: 0.3145853877067566
train_iter_loss: 0.2615242898464203
train_iter_loss: 0.35319051146507263
train_iter_loss: 0.22823089361190796
train_iter_loss: 0.41369810700416565
train_iter_loss: 0.2692543566226959
train_iter_loss: 0.2526542842388153
train_iter_loss: 0.6106261610984802
train_iter_loss: 0.17359086871147156
train_iter_loss: 0.27931755781173706
train_iter_loss: 0.2981211841106415
train_iter_loss: 0.216714009642601
train_iter_loss: 0.27115389704704285
train_iter_loss: 0.23606078326702118
train_iter_loss: 0.292709082365036
train_iter_loss: 0.14061379432678223
train_iter_loss: 0.18208357691764832
train_iter_loss: 0.2922544479370117
train_iter_loss: 0.25978243350982666
train_iter_loss: 0.4015321433544159
train_iter_loss: 0.2476305216550827
train_iter_loss: 0.2230144590139389
train_iter_loss: 0.3135325610637665
train_iter_loss: 0.3145723342895508
train_iter_loss: 0.16730301082134247
train_iter_loss: 0.2484130561351776
train_iter_loss: 0.35495299100875854
train_iter_loss: 0.18903447687625885
train_iter_loss: 0.236532062292099
train_iter_loss: 0.27602845430374146
train_iter_loss: 0.076640285551548
train_iter_loss: 0.3656165897846222
train_iter_loss: 0.26213544607162476
train_iter_loss: 0.18106725811958313
train_iter_loss: 0.37589260935783386
train_iter_loss: 0.26643624901771545
train_iter_loss: 0.30509111285209656
train_iter_loss: 0.4494197964668274
train_iter_loss: 0.3300183117389679
train_iter_loss: 0.13641737401485443
train_iter_loss: 0.377683162689209
train_iter_loss: 0.12123455852270126
train_iter_loss: 0.28514668345451355
train_iter_loss: 0.3400116264820099
train loss :0.2708
---------------------
Validation seg loss: 0.3516743850525258 at epoch 509
epoch =    510/  1000, exp = train
train_iter_loss: 0.26873284578323364
train_iter_loss: 0.1902446448802948
train_iter_loss: 0.27462446689605713
train_iter_loss: 0.2842080891132355
train_iter_loss: 0.2512449324131012
train_iter_loss: 0.4365364909172058
train_iter_loss: 0.2555340528488159
train_iter_loss: 0.2689748704433441
train_iter_loss: 0.4288083016872406
train_iter_loss: 0.2228187769651413
train_iter_loss: 0.25741344690322876
train_iter_loss: 0.4143691062927246
train_iter_loss: 0.19736512005329132
train_iter_loss: 0.2537355422973633
train_iter_loss: 0.2260945588350296
train_iter_loss: 0.2609151303768158
train_iter_loss: 0.11202774941921234
train_iter_loss: 0.36338385939598083
train_iter_loss: 0.3694086968898773
train_iter_loss: 0.4020119607448578
train_iter_loss: 0.42214885354042053
train_iter_loss: 0.23796679079532623
train_iter_loss: 0.18205492198467255
train_iter_loss: 0.06686428189277649
train_iter_loss: 0.17196246981620789
train_iter_loss: 0.31986889243125916
train_iter_loss: 0.2483680695295334
train_iter_loss: 0.27810031175613403
train_iter_loss: 0.38664180040359497
train_iter_loss: 0.2743881046772003
train_iter_loss: 0.3170756697654724
train_iter_loss: 0.23586811125278473
train_iter_loss: 0.2332661747932434
train_iter_loss: 0.2566760182380676
train_iter_loss: 0.3806194067001343
train_iter_loss: 0.28113657236099243
train_iter_loss: 0.290060818195343
train_iter_loss: 0.3410952091217041
train_iter_loss: 0.23861292004585266
train_iter_loss: 0.192128524184227
train_iter_loss: 0.2736477553844452
train_iter_loss: 0.4272059202194214
train_iter_loss: 0.23727592825889587
train_iter_loss: 0.2167157381772995
train_iter_loss: 0.16392509639263153
train_iter_loss: 0.33821341395378113
train_iter_loss: 0.07347565144300461
train_iter_loss: 0.20518548786640167
train_iter_loss: 0.47067520022392273
train_iter_loss: 0.2652926743030548
train_iter_loss: 0.24744123220443726
train_iter_loss: 0.28288063406944275
train_iter_loss: 0.1623763144016266
train_iter_loss: 0.2592937648296356
train_iter_loss: 0.22433507442474365
train_iter_loss: 0.18801933526992798
train_iter_loss: 0.3095463812351227
train_iter_loss: 0.2410498559474945
train_iter_loss: 0.13636869192123413
train_iter_loss: 0.2856678366661072
train_iter_loss: 0.02921557053923607
train_iter_loss: 0.25494033098220825
train_iter_loss: 0.2658211588859558
train_iter_loss: 0.2891996204853058
train_iter_loss: 0.3563401997089386
train_iter_loss: 0.11717232316732407
train_iter_loss: 0.26491254568099976
train_iter_loss: 0.17347535490989685
train_iter_loss: 0.3861102759838104
train_iter_loss: 0.20943616330623627
train_iter_loss: 0.18992653489112854
train_iter_loss: 0.25642943382263184
train_iter_loss: 0.255342572927475
train_iter_loss: 0.24924898147583008
train_iter_loss: 0.25818440318107605
train_iter_loss: 0.22386229038238525
train_iter_loss: 0.28676801919937134
train_iter_loss: 0.4121990203857422
train_iter_loss: 0.18637715280056
train_iter_loss: 0.4287257492542267
train_iter_loss: 0.3166477382183075
train_iter_loss: 0.1743146926164627
train_iter_loss: 0.35723695158958435
train_iter_loss: 0.25457435846328735
train_iter_loss: 0.2512437403202057
train_iter_loss: 0.1829318255186081
train_iter_loss: 0.14575666189193726
train_iter_loss: 0.2905924916267395
train_iter_loss: 0.3456690013408661
train_iter_loss: 0.17765112221240997
train_iter_loss: 0.3260820209980011
train_iter_loss: 0.540307879447937
train_iter_loss: 0.23646944761276245
train_iter_loss: 0.27348944544792175
train_iter_loss: 0.20157654583454132
train_iter_loss: 0.29301148653030396
train_iter_loss: 0.22518682479858398
train_iter_loss: 0.38826265931129456
train_iter_loss: 0.38483354449272156
train_iter_loss: 0.25606444478034973
train loss :0.2709
---------------------
Validation seg loss: 0.37161773512631935 at epoch 510
epoch =    511/  1000, exp = train
train_iter_loss: 0.2331315577030182
train_iter_loss: 0.3051311671733856
train_iter_loss: 0.24628904461860657
train_iter_loss: 0.2915050685405731
train_iter_loss: 0.14888016879558563
train_iter_loss: 0.21799035370349884
train_iter_loss: 0.09235186874866486
train_iter_loss: 0.33246397972106934
train_iter_loss: 0.2480902075767517
train_iter_loss: 0.3952864706516266
train_iter_loss: 0.3086128532886505
train_iter_loss: 0.25679242610931396
train_iter_loss: 0.33034878969192505
train_iter_loss: 0.1963174194097519
train_iter_loss: 0.3124447762966156
train_iter_loss: 0.14268268644809723
train_iter_loss: 0.22554747760295868
train_iter_loss: 0.2104310542345047
train_iter_loss: 0.19037722051143646
train_iter_loss: 0.3211429715156555
train_iter_loss: 0.2834818363189697
train_iter_loss: 0.31278952956199646
train_iter_loss: 0.22945377230644226
train_iter_loss: 0.24913527071475983
train_iter_loss: 0.322330504655838
train_iter_loss: 0.2873936891555786
train_iter_loss: 0.15612812340259552
train_iter_loss: 0.1635778248310089
train_iter_loss: 0.48810631036758423
train_iter_loss: 0.2774650454521179
train_iter_loss: 0.39855194091796875
train_iter_loss: 0.2917150557041168
train_iter_loss: 0.26183122396469116
train_iter_loss: 0.28605973720550537
train_iter_loss: 0.22212590277194977
train_iter_loss: 0.2111104130744934
train_iter_loss: 0.40356141328811646
train_iter_loss: 0.17320391535758972
train_iter_loss: 0.30803564190864563
train_iter_loss: 0.36162999272346497
train_iter_loss: 0.1896117776632309
train_iter_loss: 0.21082590520381927
train_iter_loss: 0.278287410736084
train_iter_loss: 0.2462656944990158
train_iter_loss: 0.08373633772134781
train_iter_loss: 0.42359545826911926
train_iter_loss: 0.20893755555152893
train_iter_loss: 0.24709026515483856
train_iter_loss: 0.3252863883972168
train_iter_loss: 0.23255208134651184
train_iter_loss: 0.21765144169330597
train_iter_loss: 0.28847745060920715
train_iter_loss: 0.36946749687194824
train_iter_loss: 0.32179147005081177
train_iter_loss: 0.38301143050193787
train_iter_loss: 0.2572929859161377
train_iter_loss: 0.19340011477470398
train_iter_loss: 0.435753732919693
train_iter_loss: 0.3039829730987549
train_iter_loss: 0.23727570474147797
train_iter_loss: 0.2010883092880249
train_iter_loss: 0.3465779423713684
train_iter_loss: 0.3184545934200287
train_iter_loss: 0.2878898084163666
train_iter_loss: 0.28132206201553345
train_iter_loss: 0.3789946734905243
train_iter_loss: 0.27331632375717163
train_iter_loss: 0.22586767375469208
train_iter_loss: 0.17975245416164398
train_iter_loss: 0.31744474172592163
train_iter_loss: 0.07642276585102081
train_iter_loss: 0.2569740414619446
train_iter_loss: 0.178946390748024
train_iter_loss: 0.16574789583683014
train_iter_loss: 0.3045802712440491
train_iter_loss: 0.27154287695884705
train_iter_loss: 0.10688095539808273
train_iter_loss: 0.19643612205982208
train_iter_loss: 0.3747062683105469
train_iter_loss: 0.28506433963775635
train_iter_loss: 0.31664350628852844
train_iter_loss: 0.20951269567012787
train_iter_loss: 0.28709524869918823
train_iter_loss: 0.19574961066246033
train_iter_loss: 0.18987195193767548
train_iter_loss: 0.23115769028663635
train_iter_loss: 0.395596444606781
train_iter_loss: 0.26797306537628174
train_iter_loss: 0.42936474084854126
train_iter_loss: 0.19520077109336853
train_iter_loss: 0.3548869490623474
train_iter_loss: 0.2999539077281952
train_iter_loss: 0.16484659910202026
train_iter_loss: 0.31857791543006897
train_iter_loss: 0.17558923363685608
train_iter_loss: 0.30980047583580017
train_iter_loss: 0.25921568274497986
train_iter_loss: 0.38838711380958557
train_iter_loss: 0.23225125670433044
train_iter_loss: 0.36875200271606445
train loss :0.2704
---------------------
Validation seg loss: 0.4144029162103697 at epoch 511
epoch =    512/  1000, exp = train
train_iter_loss: 0.2045273631811142
train_iter_loss: 0.05713270977139473
train_iter_loss: 0.265572726726532
train_iter_loss: 0.25741350650787354
train_iter_loss: 0.3710949122905731
train_iter_loss: 0.42915427684783936
train_iter_loss: 0.1504499912261963
train_iter_loss: 0.1737121045589447
train_iter_loss: 0.28549912571907043
train_iter_loss: 0.18379786610603333
train_iter_loss: 0.22025413811206818
train_iter_loss: 0.37326857447624207
train_iter_loss: 0.37383800745010376
train_iter_loss: 0.32250526547431946
train_iter_loss: 0.305334210395813
train_iter_loss: 0.3289371132850647
train_iter_loss: 0.2159489244222641
train_iter_loss: 0.2553230822086334
train_iter_loss: 0.2560988664627075
train_iter_loss: 0.22852209210395813
train_iter_loss: 0.23380030691623688
train_iter_loss: 0.21226555109024048
train_iter_loss: 0.27980145812034607
train_iter_loss: 0.34962475299835205
train_iter_loss: 0.2833954095840454
train_iter_loss: 0.3463919758796692
train_iter_loss: 0.2607457935810089
train_iter_loss: 0.2285560816526413
train_iter_loss: 0.21706978976726532
train_iter_loss: 0.3333410620689392
train_iter_loss: 0.1283225268125534
train_iter_loss: 0.23413848876953125
train_iter_loss: 0.27129432559013367
train_iter_loss: 0.20787934958934784
train_iter_loss: 0.2708919942378998
train_iter_loss: 0.38782158493995667
train_iter_loss: 0.1743946224451065
train_iter_loss: 0.1426403522491455
train_iter_loss: 0.11667735874652863
train_iter_loss: 0.25718241930007935
train_iter_loss: 0.5309765338897705
train_iter_loss: 0.4153878390789032
train_iter_loss: 0.1495274007320404
train_iter_loss: 0.3296526074409485
train_iter_loss: 0.3576400578022003
train_iter_loss: 0.29449936747550964
train_iter_loss: 0.30291470885276794
train_iter_loss: 0.3805561661720276
train_iter_loss: 0.21719291806221008
train_iter_loss: 0.10080572217702866
train_iter_loss: 0.31127649545669556
train_iter_loss: 0.11044216901063919
train_iter_loss: 0.20319952070713043
train_iter_loss: 0.2647375166416168
train_iter_loss: 0.3172222077846527
train_iter_loss: 0.35714563727378845
train_iter_loss: 0.3100588917732239
train_iter_loss: 0.22318309545516968
train_iter_loss: 0.19052864611148834
train_iter_loss: 0.17038430273532867
train_iter_loss: 0.24526752531528473
train_iter_loss: 0.2880311608314514
train_iter_loss: 0.1210639551281929
train_iter_loss: 0.37556248903274536
train_iter_loss: 0.30593475699424744
train_iter_loss: 0.29682785272598267
train_iter_loss: 0.1404331475496292
train_iter_loss: 0.25881636142730713
train_iter_loss: 0.16310763359069824
train_iter_loss: 0.2465360313653946
train_iter_loss: 0.34534984827041626
train_iter_loss: 0.33134254813194275
train_iter_loss: 0.1263316422700882
train_iter_loss: 0.23313692212104797
train_iter_loss: 0.2606367766857147
train_iter_loss: 0.21164588630199432
train_iter_loss: 0.4261488914489746
train_iter_loss: 0.6139088273048401
train_iter_loss: 0.15265081822872162
train_iter_loss: 0.22902058064937592
train_iter_loss: 0.33483341336250305
train_iter_loss: 0.18686260282993317
train_iter_loss: 0.2429332137107849
train_iter_loss: 0.2874726951122284
train_iter_loss: 0.28981339931488037
train_iter_loss: 0.2671355903148651
train_iter_loss: 0.2284552901983261
train_iter_loss: 0.05558933690190315
train_iter_loss: 0.16733752191066742
train_iter_loss: 0.31005361676216125
train_iter_loss: 0.26945945620536804
train_iter_loss: 0.18840740621089935
train_iter_loss: 0.6155151128768921
train_iter_loss: 0.24733470380306244
train_iter_loss: 0.22639432549476624
train_iter_loss: 0.1985902339220047
train_iter_loss: 0.4548824727535248
train_iter_loss: 0.30049943923950195
train_iter_loss: 0.27684006094932556
train_iter_loss: 0.22731009125709534
train loss :0.2675
---------------------
Validation seg loss: 0.3533143908277435 at epoch 512
epoch =    513/  1000, exp = train
train_iter_loss: 0.3468681573867798
train_iter_loss: 0.18989410996437073
train_iter_loss: 0.3131614625453949
train_iter_loss: 0.5366491675376892
train_iter_loss: 0.26715216040611267
train_iter_loss: 0.07796609401702881
train_iter_loss: 0.3950139880180359
train_iter_loss: 0.16119419038295746
train_iter_loss: 0.18642105162143707
train_iter_loss: 0.22806555032730103
train_iter_loss: 0.20140868425369263
train_iter_loss: 0.26291701197624207
train_iter_loss: 0.22828850150108337
train_iter_loss: 0.14157579839229584
train_iter_loss: 0.27847981452941895
train_iter_loss: 0.43431296944618225
train_iter_loss: 0.3169287145137787
train_iter_loss: 0.21914367377758026
train_iter_loss: 0.2883635461330414
train_iter_loss: 0.25878873467445374
train_iter_loss: 0.23605531454086304
train_iter_loss: 0.2722209095954895
train_iter_loss: 0.382622092962265
train_iter_loss: 0.26215529441833496
train_iter_loss: 0.28188788890838623
train_iter_loss: 0.35431841015815735
train_iter_loss: 0.2799075245857239
train_iter_loss: 0.32230231165885925
train_iter_loss: 0.25506773591041565
train_iter_loss: 0.1937473714351654
train_iter_loss: 0.2791220247745514
train_iter_loss: 0.3742224872112274
train_iter_loss: 0.2649385333061218
train_iter_loss: 0.23072291910648346
train_iter_loss: 0.2151932269334793
train_iter_loss: 0.2838510274887085
train_iter_loss: 0.30488789081573486
train_iter_loss: 0.24580921232700348
train_iter_loss: 0.2901875674724579
train_iter_loss: 0.30993160605430603
train_iter_loss: 0.26882556080818176
train_iter_loss: 0.05162244662642479
train_iter_loss: 0.21928073465824127
train_iter_loss: 0.24489668011665344
train_iter_loss: 0.15321850776672363
train_iter_loss: 0.3251122236251831
train_iter_loss: 0.2129392921924591
train_iter_loss: 0.3219822347164154
train_iter_loss: 0.25995612144470215
train_iter_loss: 0.27954739332199097
train_iter_loss: 0.2713000178337097
train_iter_loss: 0.36695346236228943
train_iter_loss: 0.30195170640945435
train_iter_loss: 0.1308436542749405
train_iter_loss: 0.2391815185546875
train_iter_loss: 0.16202408075332642
train_iter_loss: 0.2491353452205658
train_iter_loss: 0.25979918241500854
train_iter_loss: 0.3090067207813263
train_iter_loss: 0.06808856129646301
train_iter_loss: 0.36811965703964233
train_iter_loss: 0.14052268862724304
train_iter_loss: 0.3194763660430908
train_iter_loss: 0.35646042227745056
train_iter_loss: 0.34107857942581177
train_iter_loss: 0.28616559505462646
train_iter_loss: 0.1415322721004486
train_iter_loss: 0.2238878756761551
train_iter_loss: 0.19778573513031006
train_iter_loss: 0.2794446349143982
train_iter_loss: 0.27239155769348145
train_iter_loss: 0.3353555202484131
train_iter_loss: 0.46214035153388977
train_iter_loss: 0.18839803338050842
train_iter_loss: 0.31147852540016174
train_iter_loss: 0.3014785051345825
train_iter_loss: 0.21811936795711517
train_iter_loss: 0.22119532525539398
train_iter_loss: 0.2941112518310547
train_iter_loss: 0.4261346161365509
train_iter_loss: 0.3187154233455658
train_iter_loss: 0.14668388664722443
train_iter_loss: 0.3899904787540436
train_iter_loss: 0.30043283104896545
train_iter_loss: 0.19403286278247833
train_iter_loss: 0.1164005696773529
train_iter_loss: 0.19640102982521057
train_iter_loss: 0.20354613661766052
train_iter_loss: 0.39070820808410645
train_iter_loss: 0.44372278451919556
train_iter_loss: 0.2605191171169281
train_iter_loss: 0.2682412564754486
train_iter_loss: 0.32648780941963196
train_iter_loss: 0.2432234287261963
train_iter_loss: 0.1936352699995041
train_iter_loss: 0.15972736477851868
train_iter_loss: 0.23863698542118073
train_iter_loss: 0.3566875755786896
train_iter_loss: 0.26837655901908875
train_iter_loss: 0.3569696843624115
train loss :0.2699
---------------------
Validation seg loss: 0.36421060646479986 at epoch 513
epoch =    514/  1000, exp = train
train_iter_loss: 0.29217633605003357
train_iter_loss: 0.22108083963394165
train_iter_loss: 0.2232663929462433
train_iter_loss: 0.11631901562213898
train_iter_loss: 0.30502083897590637
train_iter_loss: 0.21611402928829193
train_iter_loss: 0.2320989966392517
train_iter_loss: 0.30631887912750244
train_iter_loss: 0.23139230906963348
train_iter_loss: 0.1934998482465744
train_iter_loss: 0.29768311977386475
train_iter_loss: 0.3078753352165222
train_iter_loss: 0.10520146042108536
train_iter_loss: 0.3683018684387207
train_iter_loss: 0.28466153144836426
train_iter_loss: 0.43658697605133057
train_iter_loss: 0.3168664276599884
train_iter_loss: 0.3388495147228241
train_iter_loss: 0.16669043898582458
train_iter_loss: 0.4817034602165222
train_iter_loss: 0.26016226410865784
train_iter_loss: 0.33807680010795593
train_iter_loss: 0.16686616837978363
train_iter_loss: 0.38114094734191895
train_iter_loss: 0.4253127872943878
train_iter_loss: 0.22237545251846313
train_iter_loss: 0.14094139635562897
train_iter_loss: 0.2584281861782074
train_iter_loss: 0.39775875210762024
train_iter_loss: 0.2888745069503784
train_iter_loss: 0.29807087779045105
train_iter_loss: 0.2228241115808487
train_iter_loss: 0.37173911929130554
train_iter_loss: 0.2089395374059677
train_iter_loss: 0.30680447816848755
train_iter_loss: 0.42609331011772156
train_iter_loss: 0.24892553687095642
train_iter_loss: 0.08296152949333191
train_iter_loss: 0.27841371297836304
train_iter_loss: 0.3300707936286926
train_iter_loss: 0.30570048093795776
train_iter_loss: 0.3471178412437439
train_iter_loss: 0.21839481592178345
train_iter_loss: 0.32888057827949524
train_iter_loss: 0.2877002954483032
train_iter_loss: 0.3123953938484192
train_iter_loss: 0.27671438455581665
train_iter_loss: 0.1185673251748085
train_iter_loss: 0.22317174077033997
train_iter_loss: 0.22934165596961975
train_iter_loss: 0.17006005346775055
train_iter_loss: 0.2101670205593109
train_iter_loss: 0.3126389682292938
train_iter_loss: 0.2575972378253937
train_iter_loss: 0.361187219619751
train_iter_loss: 0.24819503724575043
train_iter_loss: 0.1633397340774536
train_iter_loss: 0.1897508203983307
train_iter_loss: 0.28583118319511414
train_iter_loss: 0.4134586453437805
train_iter_loss: 0.1429293155670166
train_iter_loss: 0.12489651143550873
train_iter_loss: 0.3243652582168579
train_iter_loss: 0.3949655592441559
train_iter_loss: 0.12954142689704895
train_iter_loss: 0.3549462854862213
train_iter_loss: 0.16633374989032745
train_iter_loss: 0.2694271504878998
train_iter_loss: 0.270278662443161
train_iter_loss: 0.312654584646225
train_iter_loss: 0.38982006907463074
train_iter_loss: 0.18124385178089142
train_iter_loss: 0.3849339485168457
train_iter_loss: 0.2869616448879242
train_iter_loss: 0.2357068508863449
train_iter_loss: 0.14925061166286469
train_iter_loss: 0.2927606701850891
train_iter_loss: 0.27120980620384216
train_iter_loss: 0.3547741174697876
train_iter_loss: 0.47083356976509094
train_iter_loss: 0.2637495696544647
train_iter_loss: 0.17917843163013458
train_iter_loss: 0.25792548060417175
train_iter_loss: 0.36130520701408386
train_iter_loss: 0.4270554482936859
train_iter_loss: 0.2144659012556076
train_iter_loss: 0.2986270785331726
train_iter_loss: 0.32914894819259644
train_iter_loss: 0.20129868388175964
train_iter_loss: 0.28347474336624146
train_iter_loss: 0.24927812814712524
train_iter_loss: 0.04882820323109627
train_iter_loss: 0.405551552772522
train_iter_loss: 0.1615181565284729
train_iter_loss: 0.22910922765731812
train_iter_loss: 0.33368027210235596
train_iter_loss: 0.2516852617263794
train_iter_loss: 0.18524500727653503
train_iter_loss: 0.336392879486084
train_iter_loss: 0.18518969416618347
train loss :0.2734
---------------------
Validation seg loss: 0.36465740381335876 at epoch 514
epoch =    515/  1000, exp = train
train_iter_loss: 0.24873149394989014
train_iter_loss: 0.27778470516204834
train_iter_loss: 0.21683689951896667
train_iter_loss: 0.15721459686756134
train_iter_loss: 0.17903167009353638
train_iter_loss: 0.155570849776268
train_iter_loss: 0.26953426003456116
train_iter_loss: 0.3619975745677948
train_iter_loss: 0.27808651328086853
train_iter_loss: 0.21018323302268982
train_iter_loss: 0.3659454584121704
train_iter_loss: 0.38211944699287415
train_iter_loss: 0.14307673275470734
train_iter_loss: 0.2326088845729828
train_iter_loss: 0.3163279891014099
train_iter_loss: 0.25695210695266724
train_iter_loss: 0.1897517889738083
train_iter_loss: 0.18841777741909027
train_iter_loss: 0.13784268498420715
train_iter_loss: 0.22631198167800903
train_iter_loss: 0.3739079236984253
train_iter_loss: 0.38139525055885315
train_iter_loss: 0.2623838782310486
train_iter_loss: 0.22029957175254822
train_iter_loss: 0.16951453685760498
train_iter_loss: 0.28314265608787537
train_iter_loss: 0.20016835629940033
train_iter_loss: 0.3121621608734131
train_iter_loss: 0.13537494838237762
train_iter_loss: 0.3071127235889435
train_iter_loss: 0.28030505776405334
train_iter_loss: 0.330062597990036
train_iter_loss: 0.2947081923484802
train_iter_loss: 0.29790499806404114
train_iter_loss: 0.12485234439373016
train_iter_loss: 0.5931907892227173
train_iter_loss: 0.2662290334701538
train_iter_loss: 0.23124516010284424
train_iter_loss: 0.12313175946474075
train_iter_loss: 0.2672479450702667
train_iter_loss: 0.17652064561843872
train_iter_loss: 0.20923733711242676
train_iter_loss: 0.20195461809635162
train_iter_loss: 0.6655389666557312
train_iter_loss: 0.07244378328323364
train_iter_loss: 0.3173997700214386
train_iter_loss: 0.2240111380815506
train_iter_loss: 0.16618509590625763
train_iter_loss: 0.19031310081481934
train_iter_loss: 0.2746458351612091
train_iter_loss: 0.15921065211296082
train_iter_loss: 0.2761678397655487
train_iter_loss: 0.37022843956947327
train_iter_loss: 0.1929071843624115
train_iter_loss: 0.37513601779937744
train_iter_loss: 0.1816023737192154
train_iter_loss: 0.23046137392520905
train_iter_loss: 0.2179207056760788
train_iter_loss: 0.4548967480659485
train_iter_loss: 0.3506298065185547
train_iter_loss: 0.26972585916519165
train_iter_loss: 0.40460678935050964
train_iter_loss: 0.38062652945518494
train_iter_loss: 0.24278287589550018
train_iter_loss: 0.2185715287923813
train_iter_loss: 0.25989705324172974
train_iter_loss: 0.37078067660331726
train_iter_loss: 0.17829464375972748
train_iter_loss: 0.2976893484592438
train_iter_loss: 0.34997138381004333
train_iter_loss: 0.35432207584381104
train_iter_loss: 0.22103111445903778
train_iter_loss: 0.3267437517642975
train_iter_loss: 0.3064080774784088
train_iter_loss: 0.22928878664970398
train_iter_loss: 0.12630660831928253
train_iter_loss: 0.2827788293361664
train_iter_loss: 0.27509835362434387
train_iter_loss: 0.03825987130403519
train_iter_loss: 0.23946896195411682
train_iter_loss: 0.334960013628006
train_iter_loss: 0.31800293922424316
train_iter_loss: 0.34591788053512573
train_iter_loss: 0.142679363489151
train_iter_loss: 0.2039560079574585
train_iter_loss: 0.3695136606693268
train_iter_loss: 0.3803163766860962
train_iter_loss: 0.297148197889328
train_iter_loss: 0.12843000888824463
train_iter_loss: 0.35307133197784424
train_iter_loss: 0.27109676599502563
train_iter_loss: 0.23787687718868256
train_iter_loss: 0.2755235731601715
train_iter_loss: 0.2505585253238678
train_iter_loss: 0.3743813633918762
train_iter_loss: 0.20490744709968567
train_iter_loss: 0.15281148254871368
train_iter_loss: 0.2459382563829422
train_iter_loss: 0.1409330815076828
train_iter_loss: 0.20815834403038025
train loss :0.2644
---------------------
Validation seg loss: 0.367956696750315 at epoch 515
epoch =    516/  1000, exp = train
train_iter_loss: 0.23081280291080475
train_iter_loss: 0.13153940439224243
train_iter_loss: 0.16642846167087555
train_iter_loss: 0.42247113585472107
train_iter_loss: 0.41599467396736145
train_iter_loss: 0.29681330919265747
train_iter_loss: 0.4319930970668793
train_iter_loss: 0.23596450686454773
train_iter_loss: 0.31682536005973816
train_iter_loss: 0.3118067979812622
train_iter_loss: 0.3612367510795593
train_iter_loss: 0.1941506415605545
train_iter_loss: 0.30885788798332214
train_iter_loss: 0.2744368016719818
train_iter_loss: 0.26652413606643677
train_iter_loss: 0.2782452404499054
train_iter_loss: 0.12288525700569153
train_iter_loss: 0.23569142818450928
train_iter_loss: 0.37003692984580994
train_iter_loss: 0.25238144397735596
train_iter_loss: 0.15124115347862244
train_iter_loss: 0.22309789061546326
train_iter_loss: 0.3558005690574646
train_iter_loss: 0.4148367643356323
train_iter_loss: 0.2137930989265442
train_iter_loss: 0.2492038905620575
train_iter_loss: 0.19402992725372314
train_iter_loss: 0.21002374589443207
train_iter_loss: 0.46779993176460266
train_iter_loss: 0.19716106355190277
train_iter_loss: 0.22832061350345612
train_iter_loss: 0.2734137773513794
train_iter_loss: 0.39592060446739197
train_iter_loss: 0.18649142980575562
train_iter_loss: 0.18702809512615204
train_iter_loss: 0.23096397519111633
train_iter_loss: 0.11522841453552246
train_iter_loss: 0.32951289415359497
train_iter_loss: 0.2891992926597595
train_iter_loss: 0.26667487621307373
train_iter_loss: 0.3121729791164398
train_iter_loss: 0.38705214858055115
train_iter_loss: 0.16223467886447906
train_iter_loss: 0.2384389191865921
train_iter_loss: 0.1465272754430771
train_iter_loss: 0.3275168538093567
train_iter_loss: 0.30089372396469116
train_iter_loss: 0.24521559476852417
train_iter_loss: 0.2219538539648056
train_iter_loss: 0.30545711517333984
train_iter_loss: 0.289241760969162
train_iter_loss: 0.22278036177158356
train_iter_loss: 0.16544152796268463
train_iter_loss: 0.22673086822032928
train_iter_loss: 0.269829124212265
train_iter_loss: 0.22263339161872864
train_iter_loss: 0.4042735695838928
train_iter_loss: 0.24900084733963013
train_iter_loss: 0.10344313830137253
train_iter_loss: 0.36467617750167847
train_iter_loss: 0.30997833609580994
train_iter_loss: 0.2597363591194153
train_iter_loss: 0.27805212140083313
train_iter_loss: 0.18935416638851166
train_iter_loss: 0.3529888987541199
train_iter_loss: 0.29908090829849243
train_iter_loss: 0.25189611315727234
train_iter_loss: 0.18332478404045105
train_iter_loss: 0.17904777824878693
train_iter_loss: 0.24536444246768951
train_iter_loss: 0.2829623520374298
train_iter_loss: 0.2869073748588562
train_iter_loss: 0.22325260937213898
train_iter_loss: 0.17081235349178314
train_iter_loss: 0.4133038818836212
train_iter_loss: 0.29806655645370483
train_iter_loss: 0.08212880045175552
train_iter_loss: 0.3525460958480835
train_iter_loss: 0.3187536299228668
train_iter_loss: 0.14434315264225006
train_iter_loss: 0.2566278576850891
train_iter_loss: 0.3134572505950928
train_iter_loss: 0.23147812485694885
train_iter_loss: 0.25726982951164246
train_iter_loss: 0.2934020459651947
train_iter_loss: 0.161588653922081
train_iter_loss: 0.21074718236923218
train_iter_loss: 0.26992398500442505
train_iter_loss: 0.3860335350036621
train_iter_loss: 0.3772204518318176
train_iter_loss: 0.17146974802017212
train_iter_loss: 0.26709362864494324
train_iter_loss: 0.2646431028842926
train_iter_loss: 0.24907246232032776
train_iter_loss: 0.14747150242328644
train_iter_loss: 0.2640162408351898
train_iter_loss: 0.3387932777404785
train_iter_loss: 0.5298165082931519
train_iter_loss: 0.28245413303375244
train_iter_loss: 0.3704729974269867
train loss :0.2700
---------------------
Validation seg loss: 0.36471134322291276 at epoch 516
epoch =    517/  1000, exp = train
train_iter_loss: 0.38251474499702454
train_iter_loss: 0.18069180846214294
train_iter_loss: 0.41371312737464905
train_iter_loss: 0.32662081718444824
train_iter_loss: 0.1827172189950943
train_iter_loss: 0.2931850254535675
train_iter_loss: 0.3345241844654083
train_iter_loss: 0.3209659159183502
train_iter_loss: 0.19779656827449799
train_iter_loss: 0.19319561123847961
train_iter_loss: 0.15279428660869598
train_iter_loss: 0.25768181681632996
train_iter_loss: 0.13074037432670593
train_iter_loss: 0.24687853455543518
train_iter_loss: 0.2709808349609375
train_iter_loss: 0.24674399197101593
train_iter_loss: 0.25756365060806274
train_iter_loss: 0.38934212923049927
train_iter_loss: 0.2701723277568817
train_iter_loss: 0.20955944061279297
train_iter_loss: 0.30976706743240356
train_iter_loss: 0.33582794666290283
train_iter_loss: 0.2655111849308014
train_iter_loss: 0.36874619126319885
train_iter_loss: 0.32511502504348755
train_iter_loss: 0.3199358582496643
train_iter_loss: 0.32700788974761963
train_iter_loss: 0.38397058844566345
train_iter_loss: 0.28689485788345337
train_iter_loss: 0.2836717963218689
train_iter_loss: 0.1449209749698639
train_iter_loss: 0.26832038164138794
train_iter_loss: 0.31462907791137695
train_iter_loss: 0.2998875379562378
train_iter_loss: 0.18845577538013458
train_iter_loss: 0.19548186659812927
train_iter_loss: 0.4241706132888794
train_iter_loss: 0.11987054347991943
train_iter_loss: 0.2606406807899475
train_iter_loss: 0.2276417464017868
train_iter_loss: 0.38419458270072937
train_iter_loss: 0.22394198179244995
train_iter_loss: 0.350806325674057
train_iter_loss: 0.1884414106607437
train_iter_loss: 0.33370664715766907
train_iter_loss: 0.12385106831789017
train_iter_loss: 0.3409257233142853
train_iter_loss: 0.1884910762310028
train_iter_loss: 0.2606198489665985
train_iter_loss: 0.2854575514793396
train_iter_loss: 0.2715209126472473
train_iter_loss: 0.33454155921936035
train_iter_loss: 0.2314283698797226
train_iter_loss: 0.2383643388748169
train_iter_loss: 0.372507244348526
train_iter_loss: 0.21354719996452332
train_iter_loss: 0.05509505420923233
train_iter_loss: 0.09599562734365463
train_iter_loss: 0.3467300534248352
train_iter_loss: 0.22902463376522064
train_iter_loss: 0.19678553938865662
train_iter_loss: 0.5053884387016296
train_iter_loss: 0.32524579763412476
train_iter_loss: 0.5582699775695801
train_iter_loss: 0.30648910999298096
train_iter_loss: 0.22732558846473694
train_iter_loss: 0.21923618018627167
train_iter_loss: 0.3875899314880371
train_iter_loss: 0.2607739269733429
train_iter_loss: 0.1963556855916977
train_iter_loss: 0.2637157142162323
train_iter_loss: 0.2039289027452469
train_iter_loss: 0.14218740165233612
train_iter_loss: 0.3242116868495941
train_iter_loss: 0.22693197429180145
train_iter_loss: 0.24799975752830505
train_iter_loss: 0.37950778007507324
train_iter_loss: 0.26972803473472595
train_iter_loss: 0.23675654828548431
train_iter_loss: 0.27175524830818176
train_iter_loss: 0.37189069390296936
train_iter_loss: 0.24418362975120544
train_iter_loss: 0.39758560061454773
train_iter_loss: 0.2571535110473633
train_iter_loss: 0.2137407511472702
train_iter_loss: 0.32452163100242615
train_iter_loss: 0.3531462252140045
train_iter_loss: 0.32270368933677673
train_iter_loss: 0.31082749366760254
train_iter_loss: 0.39764559268951416
train_iter_loss: 0.2187846153974533
train_iter_loss: 0.3774683475494385
train_iter_loss: 0.12675711512565613
train_iter_loss: 0.27732884883880615
train_iter_loss: 0.12719428539276123
train_iter_loss: 0.038279805332422256
train_iter_loss: 0.14446642994880676
train_iter_loss: 0.13264404237270355
train_iter_loss: 0.1804075986146927
train_iter_loss: 0.21260523796081543
train loss :0.2703
---------------------
Validation seg loss: 0.4179907804442886 at epoch 517
epoch =    518/  1000, exp = train
train_iter_loss: 0.2212171107530594
train_iter_loss: 0.3130427598953247
train_iter_loss: 0.15978501737117767
train_iter_loss: 0.11501746624708176
train_iter_loss: 0.31514307856559753
train_iter_loss: 0.35413017868995667
train_iter_loss: 0.25028663873672485
train_iter_loss: 0.21196582913398743
train_iter_loss: 0.38027235865592957
train_iter_loss: 0.46405866742134094
train_iter_loss: 0.29175373911857605
train_iter_loss: 0.1878299117088318
train_iter_loss: 0.3706749975681305
train_iter_loss: 0.13270556926727295
train_iter_loss: 0.24129195511341095
train_iter_loss: 0.18898089230060577
train_iter_loss: 0.1938372701406479
train_iter_loss: 0.33463388681411743
train_iter_loss: 0.26718559861183167
train_iter_loss: 0.37629422545433044
train_iter_loss: 0.18361340463161469
train_iter_loss: 0.12468798458576202
train_iter_loss: 0.24889737367630005
train_iter_loss: 0.28290030360221863
train_iter_loss: 0.3155267834663391
train_iter_loss: 0.25413578748703003
train_iter_loss: 0.2415110319852829
train_iter_loss: 0.23918795585632324
train_iter_loss: 0.29665958881378174
train_iter_loss: 0.33656272292137146
train_iter_loss: 0.36744382977485657
train_iter_loss: 0.297194242477417
train_iter_loss: 0.37183356285095215
train_iter_loss: 0.35267552733421326
train_iter_loss: 0.23062226176261902
train_iter_loss: 0.22420619428157806
train_iter_loss: 0.06953465938568115
train_iter_loss: 0.303830623626709
train_iter_loss: 0.3217346668243408
train_iter_loss: 0.24534377455711365
train_iter_loss: 0.1669771373271942
train_iter_loss: 0.2942861318588257
train_iter_loss: 0.25979214906692505
train_iter_loss: 0.3042530119419098
train_iter_loss: 0.36279740929603577
train_iter_loss: 0.07717770338058472
train_iter_loss: 0.23048391938209534
train_iter_loss: 0.19380611181259155
train_iter_loss: 0.3040080964565277
train_iter_loss: 0.3579246699810028
train_iter_loss: 0.1868598461151123
train_iter_loss: 0.3404216468334198
train_iter_loss: 0.2750224471092224
train_iter_loss: 0.39393287897109985
train_iter_loss: 0.16029006242752075
train_iter_loss: 0.44029656052589417
train_iter_loss: 0.4154297411441803
train_iter_loss: 0.14810235798358917
train_iter_loss: 0.3368569314479828
train_iter_loss: 0.3099195659160614
train_iter_loss: 0.16785621643066406
train_iter_loss: 0.372094988822937
train_iter_loss: 0.21991731226444244
train_iter_loss: 0.5266640782356262
train_iter_loss: 0.24655649065971375
train_iter_loss: 0.18229959905147552
train_iter_loss: 0.42608344554901123
train_iter_loss: 0.22869403660297394
train_iter_loss: 0.24543136358261108
train_iter_loss: 0.23791654407978058
train_iter_loss: 0.2891483008861542
train_iter_loss: 0.23063553869724274
train_iter_loss: 0.27054354548454285
train_iter_loss: 0.2859403192996979
train_iter_loss: 0.19793780148029327
train_iter_loss: 0.2114596962928772
train_iter_loss: 0.2485881745815277
train_iter_loss: 0.26832935214042664
train_iter_loss: 0.30264267325401306
train_iter_loss: 0.32885873317718506
train_iter_loss: 0.2682739496231079
train_iter_loss: 0.32875895500183105
train_iter_loss: 0.2856887876987457
train_iter_loss: 0.09855067729949951
train_iter_loss: 0.49415433406829834
train_iter_loss: 0.34835460782051086
train_iter_loss: 0.15899167954921722
train_iter_loss: 0.22064802050590515
train_iter_loss: 0.22674055397510529
train_iter_loss: 0.2370861917734146
train_iter_loss: 0.2204698622226715
train_iter_loss: 0.09449758380651474
train_iter_loss: 0.22840449213981628
train_iter_loss: 0.35609108209609985
train_iter_loss: 0.3021078109741211
train_iter_loss: 0.2633788585662842
train_iter_loss: 0.20786823332309723
train_iter_loss: 0.1890416443347931
train_iter_loss: 0.5224234461784363
train_iter_loss: 0.19773700833320618
train loss :0.2724
---------------------
Validation seg loss: 0.35784655488711203 at epoch 518
epoch =    519/  1000, exp = train
train_iter_loss: 0.23881469666957855
train_iter_loss: 0.26244843006134033
train_iter_loss: 0.3422650396823883
train_iter_loss: 0.43811413645744324
train_iter_loss: 0.11308480054140091
train_iter_loss: 0.40917181968688965
train_iter_loss: 0.3588947653770447
train_iter_loss: 0.22200289368629456
train_iter_loss: 0.21772341430187225
train_iter_loss: 0.21169357001781464
train_iter_loss: 0.4254143536090851
train_iter_loss: 0.17918509244918823
train_iter_loss: 0.26743921637535095
train_iter_loss: 0.33502623438835144
train_iter_loss: 0.16799569129943848
train_iter_loss: 0.15429313480854034
train_iter_loss: 0.07192406803369522
train_iter_loss: 0.1397578865289688
train_iter_loss: 0.2682005763053894
train_iter_loss: 0.33925890922546387
train_iter_loss: 0.25315436720848083
train_iter_loss: 0.19334380328655243
train_iter_loss: 0.2999708354473114
train_iter_loss: 0.24317359924316406
train_iter_loss: 0.22355620563030243
train_iter_loss: 0.3279164731502533
train_iter_loss: 0.18318264186382294
train_iter_loss: 0.21944350004196167
train_iter_loss: 0.28865891695022583
train_iter_loss: 0.3206498324871063
train_iter_loss: 0.19578933715820312
train_iter_loss: 0.25420787930488586
train_iter_loss: 0.22033268213272095
train_iter_loss: 0.3169736862182617
train_iter_loss: 0.46164554357528687
train_iter_loss: 0.2215561866760254
train_iter_loss: 0.218734011054039
train_iter_loss: 0.2599094808101654
train_iter_loss: 0.1772482991218567
train_iter_loss: 0.31075483560562134
train_iter_loss: 0.2258756160736084
train_iter_loss: 0.3025366961956024
train_iter_loss: 0.44725826382637024
train_iter_loss: 0.2917509377002716
train_iter_loss: 0.26333874464035034
train_iter_loss: 0.22691793739795685
train_iter_loss: 0.3112885355949402
train_iter_loss: 0.17690126597881317
train_iter_loss: 0.22955256700515747
train_iter_loss: 0.21349743008613586
train_iter_loss: 0.20840410888195038
train_iter_loss: 0.3605111539363861
train_iter_loss: 0.2113744020462036
train_iter_loss: 0.2446274310350418
train_iter_loss: 0.39623522758483887
train_iter_loss: 0.3077605962753296
train_iter_loss: 0.3084944188594818
train_iter_loss: 0.30926254391670227
train_iter_loss: 0.13048170506954193
train_iter_loss: 0.24064518511295319
train_iter_loss: 0.1811070591211319
train_iter_loss: 0.26227107644081116
train_iter_loss: 0.23260968923568726
train_iter_loss: 0.3466320335865021
train_iter_loss: 0.13336917757987976
train_iter_loss: 0.2831727862358093
train_iter_loss: 0.2956197261810303
train_iter_loss: 0.16477137804031372
train_iter_loss: 0.3838285207748413
train_iter_loss: 0.25416409969329834
train_iter_loss: 0.21146924793720245
train_iter_loss: 0.3634721040725708
train_iter_loss: 0.17423494160175323
train_iter_loss: 0.3724437952041626
train_iter_loss: 0.30510008335113525
train_iter_loss: 0.2163102775812149
train_iter_loss: 0.21155059337615967
train_iter_loss: 0.3310721814632416
train_iter_loss: 0.37817373871803284
train_iter_loss: 0.1902158409357071
train_iter_loss: 0.32567283511161804
train_iter_loss: 0.2852734327316284
train_iter_loss: 0.274299293756485
train_iter_loss: 0.281887024641037
train_iter_loss: 0.4513636529445648
train_iter_loss: 0.35037878155708313
train_iter_loss: 0.26944640278816223
train_iter_loss: 0.1963222324848175
train_iter_loss: 0.4362921416759491
train_iter_loss: 0.15656445920467377
train_iter_loss: 0.20837105810642242
train_iter_loss: 0.14736808836460114
train_iter_loss: 0.05815918743610382
train_iter_loss: 0.2395612895488739
train_iter_loss: 0.30413851141929626
train_iter_loss: 0.328370600938797
train_iter_loss: 0.2165801227092743
train_iter_loss: 0.18915940821170807
train_iter_loss: 0.40954387187957764
train_iter_loss: 0.1800970435142517
train loss :0.2663
---------------------
Validation seg loss: 0.38207097848842164 at epoch 519
epoch =    520/  1000, exp = train
train_iter_loss: 0.14007316529750824
train_iter_loss: 0.3874415457248688
train_iter_loss: 0.26329728960990906
train_iter_loss: 0.39960774779319763
train_iter_loss: 0.2731534540653229
train_iter_loss: 0.21797826886177063
train_iter_loss: 0.3880673944950104
train_iter_loss: 0.18995627760887146
train_iter_loss: 0.3208393454551697
train_iter_loss: 0.18142971396446228
train_iter_loss: 0.22869357466697693
train_iter_loss: 0.2416321039199829
train_iter_loss: 0.37414970993995667
train_iter_loss: 0.4097960591316223
train_iter_loss: 0.24368350207805634
train_iter_loss: 0.20441055297851562
train_iter_loss: 0.27178066968917847
train_iter_loss: 0.15778662264347076
train_iter_loss: 0.32155126333236694
train_iter_loss: 0.23217694461345673
train_iter_loss: 0.21833334863185883
train_iter_loss: 0.19401384890079498
train_iter_loss: 0.12347117066383362
train_iter_loss: 0.3027597665786743
train_iter_loss: 0.2027863711118698
train_iter_loss: 0.1486852467060089
train_iter_loss: 0.35456451773643494
train_iter_loss: 0.05292275547981262
train_iter_loss: 0.3033967912197113
train_iter_loss: 0.18149036169052124
train_iter_loss: 0.24033036828041077
train_iter_loss: 0.19549748301506042
train_iter_loss: 0.16688334941864014
train_iter_loss: 0.3000199496746063
train_iter_loss: 0.17674480378627777
train_iter_loss: 0.41157346963882446
train_iter_loss: 0.3502671420574188
train_iter_loss: 0.3311921954154968
train_iter_loss: 0.2866305410861969
train_iter_loss: 0.4120408892631531
train_iter_loss: 0.32847052812576294
train_iter_loss: 0.3255259096622467
train_iter_loss: 0.2980133295059204
train_iter_loss: 0.21049277484416962
train_iter_loss: 0.33585771918296814
train_iter_loss: 0.044623963534832
train_iter_loss: 0.41083937883377075
train_iter_loss: 0.4110824763774872
train_iter_loss: 0.22000004351139069
train_iter_loss: 0.39703166484832764
train_iter_loss: 0.41980522871017456
train_iter_loss: 0.09595977514982224
train_iter_loss: 0.25716111063957214
train_iter_loss: 0.42554545402526855
train_iter_loss: 0.24402332305908203
train_iter_loss: 0.28769487142562866
train_iter_loss: 0.3684699237346649
train_iter_loss: 0.27835264801979065
train_iter_loss: 0.19710485637187958
train_iter_loss: 0.3978942930698395
train_iter_loss: 0.32457298040390015
train_iter_loss: 0.1867503523826599
train_iter_loss: 0.1772407740354538
train_iter_loss: 0.43008169531822205
train_iter_loss: 0.19135794043540955
train_iter_loss: 0.18582391738891602
train_iter_loss: 0.3142601549625397
train_iter_loss: 0.34227752685546875
train_iter_loss: 0.3147087097167969
train_iter_loss: 0.28406354784965515
train_iter_loss: 0.41140618920326233
train_iter_loss: 0.3756471276283264
train_iter_loss: 0.5467759966850281
train_iter_loss: 0.3734184801578522
train_iter_loss: 0.25765085220336914
train_iter_loss: 0.2578205466270447
train_iter_loss: 0.10321008414030075
train_iter_loss: 0.27634158730506897
train_iter_loss: 0.24974371492862701
train_iter_loss: 0.45281124114990234
train_iter_loss: 0.3250604271888733
train_iter_loss: 0.29712241888046265
train_iter_loss: 0.3534294664859772
train_iter_loss: 0.1870710849761963
train_iter_loss: 0.3706394135951996
train_iter_loss: 0.5105762481689453
train_iter_loss: 0.27342385053634644
train_iter_loss: 0.2458774298429489
train_iter_loss: 0.2725904881954193
train_iter_loss: 0.31489700078964233
train_iter_loss: 0.2560175359249115
train_iter_loss: 0.2175760418176651
train_iter_loss: 0.19341211020946503
train_iter_loss: 0.15968634188175201
train_iter_loss: 0.17728525400161743
train_iter_loss: 0.2551332712173462
train_iter_loss: 0.2153777778148651
train_iter_loss: 0.27566590905189514
train_iter_loss: 0.24171629548072815
train_iter_loss: 0.20963019132614136
train loss :0.2803
---------------------
Validation seg loss: 0.37100407401240376 at epoch 520
epoch =    521/  1000, exp = train
train_iter_loss: 0.21268752217292786
train_iter_loss: 0.24226711690425873
train_iter_loss: 0.24957197904586792
train_iter_loss: 0.31263187527656555
train_iter_loss: 0.30121245980262756
train_iter_loss: 0.32371780276298523
train_iter_loss: 0.22022725641727448
train_iter_loss: 0.271457314491272
train_iter_loss: 0.20705994963645935
train_iter_loss: 0.2125164419412613
train_iter_loss: 0.19458620250225067
train_iter_loss: 0.2565934956073761
train_iter_loss: 0.29650264978408813
train_iter_loss: 0.305494099855423
train_iter_loss: 0.19680368900299072
train_iter_loss: 0.2522985339164734
train_iter_loss: 0.40286746621131897
train_iter_loss: 0.22601842880249023
train_iter_loss: 0.4141102731227875
train_iter_loss: 0.16690555214881897
train_iter_loss: 0.5224725604057312
train_iter_loss: 0.3309001624584198
train_iter_loss: 0.2580735683441162
train_iter_loss: 0.2463284134864807
train_iter_loss: 0.28215742111206055
train_iter_loss: 0.23318536579608917
train_iter_loss: 0.18685272336006165
train_iter_loss: 0.1798907220363617
train_iter_loss: 0.14564862847328186
train_iter_loss: 0.3876914083957672
train_iter_loss: 0.13269934058189392
train_iter_loss: 0.24361345171928406
train_iter_loss: 0.30632534623146057
train_iter_loss: 0.3334823250770569
train_iter_loss: 0.2112128734588623
train_iter_loss: 0.17688030004501343
train_iter_loss: 0.08097974956035614
train_iter_loss: 0.2663787603378296
train_iter_loss: 0.2944026291370392
train_iter_loss: 0.35463109612464905
train_iter_loss: 0.08829180151224136
train_iter_loss: 0.2789947986602783
train_iter_loss: 0.23963531851768494
train_iter_loss: 0.2851450443267822
train_iter_loss: 0.24886684119701385
train_iter_loss: 0.29609474539756775
train_iter_loss: 0.2557692229747772
train_iter_loss: 0.2801320254802704
train_iter_loss: 0.28550249338150024
train_iter_loss: 0.19954626262187958
train_iter_loss: 0.3588946759700775
train_iter_loss: 0.3563610911369324
train_iter_loss: 0.31140580773353577
train_iter_loss: 0.38253065943717957
train_iter_loss: 0.21649585664272308
train_iter_loss: 0.3340640962123871
train_iter_loss: 0.18672747910022736
train_iter_loss: 0.27593058347702026
train_iter_loss: 0.34112533926963806
train_iter_loss: 0.19417737424373627
train_iter_loss: 0.27360886335372925
train_iter_loss: 0.367723286151886
train_iter_loss: 0.35859793424606323
train_iter_loss: 0.2893567383289337
train_iter_loss: 0.24668605625629425
train_iter_loss: 0.3306441605091095
train_iter_loss: 0.34513241052627563
train_iter_loss: 0.23971372842788696
train_iter_loss: 0.23355938494205475
train_iter_loss: 0.20982131361961365
train_iter_loss: 0.2142447531223297
train_iter_loss: 0.25948968529701233
train_iter_loss: 0.28297433257102966
train_iter_loss: 0.16930842399597168
train_iter_loss: 0.5634234547615051
train_iter_loss: 0.22358660399913788
train_iter_loss: 0.26855236291885376
train_iter_loss: 0.3424586355686188
train_iter_loss: 0.3094984292984009
train_iter_loss: 0.3328799903392792
train_iter_loss: 0.319856733083725
train_iter_loss: 0.21446579694747925
train_iter_loss: 0.2945319712162018
train_iter_loss: 0.20584215223789215
train_iter_loss: 0.2725237011909485
train_iter_loss: 0.2631617486476898
train_iter_loss: 0.2082260102033615
train_iter_loss: 0.16339312493801117
train_iter_loss: 0.2221274971961975
train_iter_loss: 0.46806323528289795
train_iter_loss: 0.5147188901901245
train_iter_loss: 0.504244327545166
train_iter_loss: 0.027025792747735977
train_iter_loss: 0.22630678117275238
train_iter_loss: 0.2122643142938614
train_iter_loss: 0.34767967462539673
train_iter_loss: 0.25696679949760437
train_iter_loss: 0.28740358352661133
train_iter_loss: 0.41273233294487
train_iter_loss: 0.37871304154396057
train loss :0.2780
---------------------
Validation seg loss: 0.35269866865304 at epoch 521
epoch =    522/  1000, exp = train
train_iter_loss: 0.26099833846092224
train_iter_loss: 0.45513734221458435
train_iter_loss: 0.14340932667255402
train_iter_loss: 0.2909966707229614
train_iter_loss: 0.258575975894928
train_iter_loss: 0.26793134212493896
train_iter_loss: 0.12534259259700775
train_iter_loss: 0.5726739764213562
train_iter_loss: 0.38233962655067444
train_iter_loss: 0.3479149043560028
train_iter_loss: 0.29686564207077026
train_iter_loss: 0.27766528725624084
train_iter_loss: 0.26931115984916687
train_iter_loss: 0.23904725909233093
train_iter_loss: 0.23623330891132355
train_iter_loss: 0.19848273694515228
train_iter_loss: 0.4482199549674988
train_iter_loss: 0.36279115080833435
train_iter_loss: 0.29884710907936096
train_iter_loss: 0.13416653871536255
train_iter_loss: 0.2311505526304245
train_iter_loss: 0.36262771487236023
train_iter_loss: 0.12704835832118988
train_iter_loss: 0.20811614394187927
train_iter_loss: 0.3004409372806549
train_iter_loss: 0.38089069724082947
train_iter_loss: 0.16641834378242493
train_iter_loss: 0.3490002751350403
train_iter_loss: 0.1992933303117752
train_iter_loss: 0.15822339057922363
train_iter_loss: 0.3471168577671051
train_iter_loss: 0.24042057991027832
train_iter_loss: 0.29426243901252747
train_iter_loss: 0.22728700935840607
train_iter_loss: 0.4009143114089966
train_iter_loss: 0.4139416217803955
train_iter_loss: 0.3275376558303833
train_iter_loss: 0.24369139969348907
train_iter_loss: 0.2586362361907959
train_iter_loss: 0.28668126463890076
train_iter_loss: 0.2563053369522095
train_iter_loss: 0.26468339562416077
train_iter_loss: 0.2882728576660156
train_iter_loss: 0.3675130307674408
train_iter_loss: 0.2365909218788147
train_iter_loss: 0.313892126083374
train_iter_loss: 0.3350565731525421
train_iter_loss: 0.07421103864908218
train_iter_loss: 0.3238009810447693
train_iter_loss: 0.35826703906059265
train_iter_loss: 0.3629113733768463
train_iter_loss: 0.2711679935455322
train_iter_loss: 0.2390531599521637
train_iter_loss: 0.286966472864151
train_iter_loss: 0.16621217131614685
train_iter_loss: 0.28692877292633057
train_iter_loss: 0.2506347894668579
train_iter_loss: 0.15705347061157227
train_iter_loss: 0.344684898853302
train_iter_loss: 0.36103755235671997
train_iter_loss: 0.2906375527381897
train_iter_loss: 0.25637540221214294
train_iter_loss: 0.26329484581947327
train_iter_loss: 0.17050126194953918
train_iter_loss: 0.3479994833469391
train_iter_loss: 0.22663161158561707
train_iter_loss: 0.29060694575309753
train_iter_loss: 0.2674224078655243
train_iter_loss: 0.22952710092067719
train_iter_loss: 0.10541887581348419
train_iter_loss: 0.28645166754722595
train_iter_loss: 0.1978956013917923
train_iter_loss: 0.31124061346054077
train_iter_loss: 0.05120503902435303
train_iter_loss: 0.19008469581604004
train_iter_loss: 0.1167515218257904
train_iter_loss: 0.23240123689174652
train_iter_loss: 0.2627624571323395
train_iter_loss: 0.15593492984771729
train_iter_loss: 0.3545885980129242
train_iter_loss: 0.1617165207862854
train_iter_loss: 0.25435787439346313
train_iter_loss: 0.22127898037433624
train_iter_loss: 0.26760441064834595
train_iter_loss: 0.36864250898361206
train_iter_loss: 0.24389982223510742
train_iter_loss: 0.4301503896713257
train_iter_loss: 0.2671884298324585
train_iter_loss: 0.2545669376850128
train_iter_loss: 0.2921658158302307
train_iter_loss: 0.3116574287414551
train_iter_loss: 0.13536253571510315
train_iter_loss: 0.3131721615791321
train_iter_loss: 0.08690859377384186
train_iter_loss: 0.2547249495983124
train_iter_loss: 0.26632627844810486
train_iter_loss: 0.2924979627132416
train_iter_loss: 0.3059392273426056
train_iter_loss: 0.16942918300628662
train_iter_loss: 0.3733375072479248
train loss :0.2709
---------------------
Validation seg loss: 0.36430130516758785 at epoch 522
epoch =    523/  1000, exp = train
train_iter_loss: 0.18276911973953247
train_iter_loss: 0.3594841957092285
train_iter_loss: 0.3279785215854645
train_iter_loss: 0.2656245529651642
train_iter_loss: 0.3425523638725281
train_iter_loss: 0.2746189534664154
train_iter_loss: 0.26594483852386475
train_iter_loss: 0.5331440567970276
train_iter_loss: 0.19837692379951477
train_iter_loss: 0.42222481966018677
train_iter_loss: 0.29433777928352356
train_iter_loss: 0.4313807785511017
train_iter_loss: 0.20842647552490234
train_iter_loss: 0.3185409605503082
train_iter_loss: 0.19776375591754913
train_iter_loss: 0.27337077260017395
train_iter_loss: 0.09223919361829758
train_iter_loss: 0.3405124545097351
train_iter_loss: 0.11427483707666397
train_iter_loss: 0.22423982620239258
train_iter_loss: 0.22840291261672974
train_iter_loss: 0.21339882910251617
train_iter_loss: 0.31943243741989136
train_iter_loss: 0.2721363306045532
train_iter_loss: 0.1815616488456726
train_iter_loss: 0.3495239317417145
train_iter_loss: 0.3391987681388855
train_iter_loss: 0.15795676410198212
train_iter_loss: 0.1572781205177307
train_iter_loss: 0.2627711296081543
train_iter_loss: 0.24476221203804016
train_iter_loss: 0.21729308366775513
train_iter_loss: 0.3954881727695465
train_iter_loss: 0.33590754866600037
train_iter_loss: 0.17176122963428497
train_iter_loss: 0.3613470196723938
train_iter_loss: 0.30385181307792664
train_iter_loss: 0.45199519395828247
train_iter_loss: 0.2858870029449463
train_iter_loss: 0.48963263630867004
train_iter_loss: 0.2552361786365509
train_iter_loss: 0.28667980432510376
train_iter_loss: 0.3266468644142151
train_iter_loss: 0.31306082010269165
train_iter_loss: 0.14546258747577667
train_iter_loss: 0.32389453053474426
train_iter_loss: 0.4398708939552307
train_iter_loss: 0.21479278802871704
train_iter_loss: 0.27952197194099426
train_iter_loss: 0.18581347167491913
train_iter_loss: 0.351444810628891
train_iter_loss: 0.20666463673114777
train_iter_loss: 0.2340158373117447
train_iter_loss: 0.2262246161699295
train_iter_loss: 0.40416669845581055
train_iter_loss: 0.28241896629333496
train_iter_loss: 0.29836615920066833
train_iter_loss: 0.1095862090587616
train_iter_loss: 0.3653518557548523
train_iter_loss: 0.13565757870674133
train_iter_loss: 0.2627490758895874
train_iter_loss: 0.07917123287916183
train_iter_loss: 0.19925962388515472
train_iter_loss: 0.36163389682769775
train_iter_loss: 0.36488619446754456
train_iter_loss: 0.23250912129878998
train_iter_loss: 0.2457994818687439
train_iter_loss: 0.3635227084159851
train_iter_loss: 0.08503212779760361
train_iter_loss: 0.31106075644493103
train_iter_loss: 0.25597652792930603
train_iter_loss: 0.21183307468891144
train_iter_loss: 0.08824764937162399
train_iter_loss: 0.2978922724723816
train_iter_loss: 0.27099207043647766
train_iter_loss: 0.3514891266822815
train_iter_loss: 0.1968984603881836
train_iter_loss: 0.3358987867832184
train_iter_loss: 0.3316118121147156
train_iter_loss: 0.2695421278476715
train_iter_loss: 0.2518688142299652
train_iter_loss: 0.22807124257087708
train_iter_loss: 0.26950308680534363
train_iter_loss: 0.2729739844799042
train_iter_loss: 0.19086304306983948
train_iter_loss: 0.21810738742351532
train_iter_loss: 0.3176825940608978
train_iter_loss: 0.28870466351509094
train_iter_loss: 0.18409903347492218
train_iter_loss: 0.31130915880203247
train_iter_loss: 0.42404401302337646
train_iter_loss: 0.030777527019381523
train_iter_loss: 0.21184472739696503
train_iter_loss: 0.10966573655605316
train_iter_loss: 0.2793644368648529
train_iter_loss: 0.4851110577583313
train_iter_loss: 0.30622273683547974
train_iter_loss: 0.3328557014465332
train_iter_loss: 0.40339240431785583
train_iter_loss: 0.27870774269104004
train loss :0.2757
---------------------
Validation seg loss: 0.3518129730737715 at epoch 523
epoch =    524/  1000, exp = train
train_iter_loss: 0.2017863392829895
train_iter_loss: 0.11968725174665451
train_iter_loss: 0.19943362474441528
train_iter_loss: 0.31249624490737915
train_iter_loss: 0.3863769471645355
train_iter_loss: 0.17228993773460388
train_iter_loss: 0.2683372497558594
train_iter_loss: 0.1441531628370285
train_iter_loss: 0.2759295701980591
train_iter_loss: 0.20156985521316528
train_iter_loss: 0.2166537493467331
train_iter_loss: 0.16293904185295105
train_iter_loss: 0.14225313067436218
train_iter_loss: 0.27773576974868774
train_iter_loss: 0.22473640739917755
train_iter_loss: 0.24955321848392487
train_iter_loss: 0.29401707649230957
train_iter_loss: 0.2904251515865326
train_iter_loss: 0.3739107549190521
train_iter_loss: 0.4668610394001007
train_iter_loss: 0.31297576427459717
train_iter_loss: 0.4621017575263977
train_iter_loss: 0.27374857664108276
train_iter_loss: 0.30365708470344543
train_iter_loss: 0.44110503792762756
train_iter_loss: 0.24954958260059357
train_iter_loss: 0.2938542366027832
train_iter_loss: 0.2656380534172058
train_iter_loss: 0.3930545151233673
train_iter_loss: 0.322112113237381
train_iter_loss: 0.1548715978860855
train_iter_loss: 0.30784860253334045
train_iter_loss: 0.29614514112472534
train_iter_loss: 0.2173704355955124
train_iter_loss: 0.34178829193115234
train_iter_loss: 0.17270608246326447
train_iter_loss: 0.1973569095134735
train_iter_loss: 0.20810484886169434
train_iter_loss: 0.28243064880371094
train_iter_loss: 0.21457435190677643
train_iter_loss: 0.2968187928199768
train_iter_loss: 0.24483613669872284
train_iter_loss: 0.3171619474887848
train_iter_loss: 0.1558765172958374
train_iter_loss: 0.3793357312679291
train_iter_loss: 0.25887569785118103
train_iter_loss: 0.25564634799957275
train_iter_loss: 0.170836940407753
train_iter_loss: 0.2764427363872528
train_iter_loss: 0.28631922602653503
train_iter_loss: 0.21765033900737762
train_iter_loss: 0.30884718894958496
train_iter_loss: 0.19828404486179352
train_iter_loss: 0.22221852838993073
train_iter_loss: 0.32060784101486206
train_iter_loss: 0.2729806900024414
train_iter_loss: 0.3358083963394165
train_iter_loss: 0.3226900100708008
train_iter_loss: 0.23233938217163086
train_iter_loss: 0.11588510870933533
train_iter_loss: 0.4745701253414154
train_iter_loss: 0.29581910371780396
train_iter_loss: 0.23127126693725586
train_iter_loss: 0.27816295623779297
train_iter_loss: 0.27705174684524536
train_iter_loss: 0.29443037509918213
train_iter_loss: 0.11442991346120834
train_iter_loss: 0.302689790725708
train_iter_loss: 0.41119304299354553
train_iter_loss: 0.3080102205276489
train_iter_loss: 0.18356743454933167
train_iter_loss: 0.37650635838508606
train_iter_loss: 0.36049532890319824
train_iter_loss: 0.26817646622657776
train_iter_loss: 0.2992764711380005
train_iter_loss: 0.10793304443359375
train_iter_loss: 0.2642298936843872
train_iter_loss: 0.3519926965236664
train_iter_loss: 0.29867541790008545
train_iter_loss: 0.3024294674396515
train_iter_loss: 0.1716412603855133
train_iter_loss: 0.1359614133834839
train_iter_loss: 0.2905787229537964
train_iter_loss: 0.09386882185935974
train_iter_loss: 0.2471006065607071
train_iter_loss: 0.1765386015176773
train_iter_loss: 0.15462107956409454
train_iter_loss: 0.3122325539588928
train_iter_loss: 0.36418405175209045
train_iter_loss: 0.3134164810180664
train_iter_loss: 0.1779925525188446
train_iter_loss: 0.3859441578388214
train_iter_loss: 0.4393928050994873
train_iter_loss: 0.13671667873859406
train_iter_loss: 0.2487943172454834
train_iter_loss: 0.41893428564071655
train_iter_loss: 0.12771238386631012
train_iter_loss: 0.24037602543830872
train_iter_loss: 0.25317150354385376
train_iter_loss: 0.2999841272830963
train loss :0.2695
---------------------
Validation seg loss: 0.366634918745818 at epoch 524
epoch =    525/  1000, exp = train
train_iter_loss: 0.3423727750778198
train_iter_loss: 0.505114734172821
train_iter_loss: 0.294874906539917
train_iter_loss: 0.17032863199710846
train_iter_loss: 0.21821442246437073
train_iter_loss: 0.21925196051597595
train_iter_loss: 0.2790064513683319
train_iter_loss: 0.15325775742530823
train_iter_loss: 0.19088879227638245
train_iter_loss: 0.37023624777793884
train_iter_loss: 0.21618619561195374
train_iter_loss: 0.2851884961128235
train_iter_loss: 0.2871757745742798
train_iter_loss: 0.32001423835754395
train_iter_loss: 0.36795851588249207
train_iter_loss: 0.35306793451309204
train_iter_loss: 0.22504070401191711
train_iter_loss: 0.10870720446109772
train_iter_loss: 0.23697708547115326
train_iter_loss: 0.3143308162689209
train_iter_loss: 0.27657201886177063
train_iter_loss: 0.20195001363754272
train_iter_loss: 0.2916702628135681
train_iter_loss: 0.20760266482830048
train_iter_loss: 0.2320544570684433
train_iter_loss: 0.3827652931213379
train_iter_loss: 0.292591392993927
train_iter_loss: 0.2230134755373001
train_iter_loss: 0.1914743036031723
train_iter_loss: 0.08995083719491959
train_iter_loss: 0.36393165588378906
train_iter_loss: 0.2984631657600403
train_iter_loss: 0.18265242874622345
train_iter_loss: 0.261636346578598
train_iter_loss: 0.23493501543998718
train_iter_loss: 0.24752648174762726
train_iter_loss: 0.11140154302120209
train_iter_loss: 0.2954791486263275
train_iter_loss: 0.3419472277164459
train_iter_loss: 0.18506471812725067
train_iter_loss: 0.31696662306785583
train_iter_loss: 0.16575630009174347
train_iter_loss: 0.18657608330249786
train_iter_loss: 0.28887882828712463
train_iter_loss: 0.11587770283222198
train_iter_loss: 0.2941077649593353
train_iter_loss: 0.2939424514770508
train_iter_loss: 0.2775570750236511
train_iter_loss: 0.22101128101348877
train_iter_loss: 0.2969192564487457
train_iter_loss: 0.3228572905063629
train_iter_loss: 0.31089869141578674
train_iter_loss: 0.4207714796066284
train_iter_loss: 0.13841113448143005
train_iter_loss: 0.22993230819702148
train_iter_loss: 0.3423486649990082
train_iter_loss: 0.14080150425434113
train_iter_loss: 0.15440307557582855
train_iter_loss: 0.3915407061576843
train_iter_loss: 0.228948712348938
train_iter_loss: 0.271045058965683
train_iter_loss: 0.19128035008907318
train_iter_loss: 0.12030541151762009
train_iter_loss: 0.1690773218870163
train_iter_loss: 0.35703685879707336
train_iter_loss: 0.2265835851430893
train_iter_loss: 0.30948010087013245
train_iter_loss: 0.32023584842681885
train_iter_loss: 0.3087204694747925
train_iter_loss: 0.1331537812948227
train_iter_loss: 0.38584551215171814
train_iter_loss: 0.19469311833381653
train_iter_loss: 0.3002234995365143
train_iter_loss: 0.3177156448364258
train_iter_loss: 0.3633638322353363
train_iter_loss: 0.2112910896539688
train_iter_loss: 0.2456550896167755
train_iter_loss: 0.23528774082660675
train_iter_loss: 0.2316390872001648
train_iter_loss: 0.2530900537967682
train_iter_loss: 0.3120940029621124
train_iter_loss: 0.2876458764076233
train_iter_loss: 0.24587254226207733
train_iter_loss: 0.31494390964508057
train_iter_loss: 0.26890474557876587
train_iter_loss: 0.36316239833831787
train_iter_loss: 0.3475487530231476
train_iter_loss: 0.29363325238227844
train_iter_loss: 0.4041888415813446
train_iter_loss: 0.1959819793701172
train_iter_loss: 0.14431515336036682
train_iter_loss: 0.3688918352127075
train_iter_loss: 0.4311065375804901
train_iter_loss: 0.20962148904800415
train_iter_loss: 0.40229159593582153
train_iter_loss: 0.31959885358810425
train_iter_loss: 0.3225101828575134
train_iter_loss: 0.1646018773317337
train_iter_loss: 0.23073051869869232
train_iter_loss: 0.504544734954834
train loss :0.2706
---------------------
Validation seg loss: 0.3860839715772221 at epoch 525
epoch =    526/  1000, exp = train
train_iter_loss: 0.2911282479763031
train_iter_loss: 0.3367907702922821
train_iter_loss: 0.509315550327301
train_iter_loss: 0.400674045085907
train_iter_loss: 0.3301765024662018
train_iter_loss: 0.29151323437690735
train_iter_loss: 0.30322015285491943
train_iter_loss: 0.2444322258234024
train_iter_loss: 0.0656379759311676
train_iter_loss: 0.26057446002960205
train_iter_loss: 0.2273755967617035
train_iter_loss: 0.3401026129722595
train_iter_loss: 0.1252617985010147
train_iter_loss: 0.21502931416034698
train_iter_loss: 0.12872301042079926
train_iter_loss: 0.20116637647151947
train_iter_loss: 0.23253458738327026
train_iter_loss: 0.43671315908432007
train_iter_loss: 0.14224666357040405
train_iter_loss: 0.34830957651138306
train_iter_loss: 0.19619528949260712
train_iter_loss: 0.2501171827316284
train_iter_loss: 0.23144657909870148
train_iter_loss: 0.28097301721572876
train_iter_loss: 0.21545198559761047
train_iter_loss: 0.19073717296123505
train_iter_loss: 0.26491615176200867
train_iter_loss: 0.13613979518413544
train_iter_loss: 0.1558219939470291
train_iter_loss: 0.24311311542987823
train_iter_loss: 0.3034939169883728
train_iter_loss: 0.4572017192840576
train_iter_loss: 0.10331311076879501
train_iter_loss: 0.2962719798088074
train_iter_loss: 0.34537333250045776
train_iter_loss: 0.30205345153808594
train_iter_loss: 0.2227839082479477
train_iter_loss: 0.39184680581092834
train_iter_loss: 0.19077247381210327
train_iter_loss: 0.3224274218082428
train_iter_loss: 0.20735099911689758
train_iter_loss: 0.23990949988365173
train_iter_loss: 0.1882968544960022
train_iter_loss: 0.6030855178833008
train_iter_loss: 0.19147804379463196
train_iter_loss: 0.36980369687080383
train_iter_loss: 0.22894659638404846
train_iter_loss: 0.2584894895553589
train_iter_loss: 0.27656665444374084
train_iter_loss: 0.4389753043651581
train_iter_loss: 0.22226135432720184
train_iter_loss: 0.28902649879455566
train_iter_loss: 0.3027504086494446
train_iter_loss: 0.33153849840164185
train_iter_loss: 0.33866459131240845
train_iter_loss: 0.4200570285320282
train_iter_loss: 0.10917989909648895
train_iter_loss: 0.2535479664802551
train_iter_loss: 0.32140782475471497
train_iter_loss: 0.37315094470977783
train_iter_loss: 0.3449023365974426
train_iter_loss: 0.1860799342393875
train_iter_loss: 0.07722093909978867
train_iter_loss: 0.18501976132392883
train_iter_loss: 0.16661135852336884
train_iter_loss: 0.30714505910873413
train_iter_loss: 0.20690016448497772
train_iter_loss: 0.13898691534996033
train_iter_loss: 0.41285377740859985
train_iter_loss: 0.417319118976593
train_iter_loss: 0.15931692719459534
train_iter_loss: 0.15129251778125763
train_iter_loss: 0.2609344720840454
train_iter_loss: 0.34952887892723083
train_iter_loss: 0.5474313497543335
train_iter_loss: 0.3884314298629761
train_iter_loss: 0.23254898190498352
train_iter_loss: 0.2449195384979248
train_iter_loss: 0.23434270918369293
train_iter_loss: 0.09224046766757965
train_iter_loss: 0.23918060958385468
train_iter_loss: 0.35579606890678406
train_iter_loss: 0.2325361967086792
train_iter_loss: 0.32904163002967834
train_iter_loss: 0.09349376708269119
train_iter_loss: 0.28899693489074707
train_iter_loss: 0.2024206966161728
train_iter_loss: 0.3512357175350189
train_iter_loss: 0.2782690227031708
train_iter_loss: 0.14727386832237244
train_iter_loss: 0.39258578419685364
train_iter_loss: 0.2560916841030121
train_iter_loss: 0.18178926408290863
train_iter_loss: 0.2619515359401703
train_iter_loss: 0.3027583062648773
train_iter_loss: 0.3321852385997772
train_iter_loss: 0.2609526515007019
train_iter_loss: 0.2803001403808594
train_iter_loss: 0.27755317091941833
train_iter_loss: 0.2612571120262146
train loss :0.2720
---------------------
Validation seg loss: 0.3798822634326259 at epoch 526
epoch =    527/  1000, exp = train
train_iter_loss: 0.2793443500995636
train_iter_loss: 0.2893741726875305
train_iter_loss: 0.4467743933200836
train_iter_loss: 0.13535848259925842
train_iter_loss: 0.14076539874076843
train_iter_loss: 0.19900444149971008
train_iter_loss: 0.17411406338214874
train_iter_loss: 0.23889189958572388
train_iter_loss: 0.22995597124099731
train_iter_loss: 0.27174854278564453
train_iter_loss: 0.18662819266319275
train_iter_loss: 0.1848885416984558
train_iter_loss: 0.20854318141937256
train_iter_loss: 0.21651361882686615
train_iter_loss: 0.3032723069190979
train_iter_loss: 0.25798583030700684
train_iter_loss: 0.14828389883041382
train_iter_loss: 0.24635197222232819
train_iter_loss: 0.12447583675384521
train_iter_loss: 0.3740612268447876
train_iter_loss: 0.20523546636104584
train_iter_loss: 0.4305722117424011
train_iter_loss: 0.22322125732898712
train_iter_loss: 0.1021759957075119
train_iter_loss: 0.2322123646736145
train_iter_loss: 0.40905359387397766
train_iter_loss: 0.3324863314628601
train_iter_loss: 0.28077560663223267
train_iter_loss: 0.1818239837884903
train_iter_loss: 0.38955262303352356
train_iter_loss: 0.2014620453119278
train_iter_loss: 0.22319236397743225
train_iter_loss: 0.22045692801475525
train_iter_loss: 0.3743849992752075
train_iter_loss: 0.27097755670547485
train_iter_loss: 0.24336107075214386
train_iter_loss: 0.2517603933811188
train_iter_loss: 0.28595584630966187
train_iter_loss: 0.26755985617637634
train_iter_loss: 0.1921612024307251
train_iter_loss: 0.26856890320777893
train_iter_loss: 0.22052231431007385
train_iter_loss: 0.1440294086933136
train_iter_loss: 0.28266939520835876
train_iter_loss: 0.29290133714675903
train_iter_loss: 0.6540592312812805
train_iter_loss: 0.16681012511253357
train_iter_loss: 0.29434898495674133
train_iter_loss: 0.33499428629875183
train_iter_loss: 0.10351219773292542
train_iter_loss: 0.38179585337638855
train_iter_loss: 0.28735989332199097
train_iter_loss: 0.2664732038974762
train_iter_loss: 0.2476574182510376
train_iter_loss: 0.21553297340869904
train_iter_loss: 0.43359535932540894
train_iter_loss: 0.31113573908805847
train_iter_loss: 0.33847951889038086
train_iter_loss: 0.2119562178850174
train_iter_loss: 0.3468433618545532
train_iter_loss: 0.11841659247875214
train_iter_loss: 0.228345587849617
train_iter_loss: 0.2202443778514862
train_iter_loss: 0.15213319659233093
train_iter_loss: 0.2761547863483429
train_iter_loss: 0.4136551022529602
train_iter_loss: 0.2948147654533386
train_iter_loss: 0.39899373054504395
train_iter_loss: 0.2928384840488434
train_iter_loss: 0.18219256401062012
train_iter_loss: 0.314879447221756
train_iter_loss: 0.2903265953063965
train_iter_loss: 0.35530316829681396
train_iter_loss: 0.34716707468032837
train_iter_loss: 0.23438994586467743
train_iter_loss: 0.3385506272315979
train_iter_loss: 0.28137803077697754
train_iter_loss: 0.3076116442680359
train_iter_loss: 0.3000012934207916
train_iter_loss: 0.3473314046859741
train_iter_loss: 0.40696775913238525
train_iter_loss: 0.2472316026687622
train_iter_loss: 0.2472246289253235
train_iter_loss: 0.3050512969493866
train_iter_loss: 0.2626078724861145
train_iter_loss: 0.4266628623008728
train_iter_loss: 0.38147830963134766
train_iter_loss: 0.1485140323638916
train_iter_loss: 0.3789965808391571
train_iter_loss: 0.26787903904914856
train_iter_loss: 0.22994163632392883
train_iter_loss: 0.27840572595596313
train_iter_loss: 0.18913762271404266
train_iter_loss: 0.2733491063117981
train_iter_loss: 0.3202895224094391
train_iter_loss: 0.4327096939086914
train_iter_loss: 0.314353883266449
train_iter_loss: 0.28265413641929626
train_iter_loss: 0.21566514670848846
train_iter_loss: 0.24946923553943634
train loss :0.2760
---------------------
Validation seg loss: 0.3505649927054655 at epoch 527
epoch =    528/  1000, exp = train
train_iter_loss: 0.1530986875295639
train_iter_loss: 0.23464345932006836
train_iter_loss: 0.48397988080978394
train_iter_loss: 0.33484625816345215
train_iter_loss: 0.2586040496826172
train_iter_loss: 0.23538419604301453
train_iter_loss: 0.20390571653842926
train_iter_loss: 0.3566911518573761
train_iter_loss: 0.2435000091791153
train_iter_loss: 0.20940406620502472
train_iter_loss: 0.3233221769332886
train_iter_loss: 0.16976720094680786
train_iter_loss: 0.3277488350868225
train_iter_loss: 0.22337552905082703
train_iter_loss: 0.4365065097808838
train_iter_loss: 0.34718599915504456
train_iter_loss: 0.2523244023323059
train_iter_loss: 0.19927453994750977
train_iter_loss: 0.23137667775154114
train_iter_loss: 0.2238433063030243
train_iter_loss: 0.2780133783817291
train_iter_loss: 0.4286704659461975
train_iter_loss: 0.2934986352920532
train_iter_loss: 0.07836612313985825
train_iter_loss: 0.34344351291656494
train_iter_loss: 0.30732354521751404
train_iter_loss: 0.31139644980430603
train_iter_loss: 0.21378087997436523
train_iter_loss: 0.30392542481422424
train_iter_loss: 0.24850398302078247
train_iter_loss: 0.2566496729850769
train_iter_loss: 0.21665312349796295
train_iter_loss: 0.3058876097202301
train_iter_loss: 0.30316877365112305
train_iter_loss: 0.15449269115924835
train_iter_loss: 0.1579727828502655
train_iter_loss: 0.13756975531578064
train_iter_loss: 0.2897453010082245
train_iter_loss: 0.1753532737493515
train_iter_loss: 0.32702091336250305
train_iter_loss: 0.4631160795688629
train_iter_loss: 0.301726371049881
train_iter_loss: 0.20936328172683716
train_iter_loss: 0.2007090300321579
train_iter_loss: 0.23033949732780457
train_iter_loss: 0.17126208543777466
train_iter_loss: 0.320163369178772
train_iter_loss: 0.30229872465133667
train_iter_loss: 0.05682147294282913
train_iter_loss: 0.29288366436958313
train_iter_loss: 0.21366946399211884
train_iter_loss: 0.26795369386672974
train_iter_loss: 0.23747719824314117
train_iter_loss: 0.37032800912857056
train_iter_loss: 0.17927303910255432
train_iter_loss: 0.3265025317668915
train_iter_loss: 0.35538119077682495
train_iter_loss: 0.3570350408554077
train_iter_loss: 0.3103916645050049
train_iter_loss: 0.2503262162208557
train_iter_loss: 0.2702423632144928
train_iter_loss: 0.16523030400276184
train_iter_loss: 0.24380087852478027
train_iter_loss: 0.3255573511123657
train_iter_loss: 0.3606320321559906
train_iter_loss: 0.2152886986732483
train_iter_loss: 0.3637079894542694
train_iter_loss: 0.2280512899160385
train_iter_loss: 0.333315908908844
train_iter_loss: 0.23294849693775177
train_iter_loss: 0.2287815362215042
train_iter_loss: 0.45598042011260986
train_iter_loss: 0.2837716341018677
train_iter_loss: 0.3089328110218048
train_iter_loss: 0.3743070662021637
train_iter_loss: 0.11952735483646393
train_iter_loss: 0.18547706305980682
train_iter_loss: 0.23887988924980164
train_iter_loss: 0.40896493196487427
train_iter_loss: 0.22995492815971375
train_iter_loss: 0.25538966059684753
train_iter_loss: 0.21608616411685944
train_iter_loss: 0.3649040460586548
train_iter_loss: 0.2894520163536072
train_iter_loss: 0.2073126882314682
train_iter_loss: 0.10644227266311646
train_iter_loss: 0.2369382530450821
train_iter_loss: 0.2084593027830124
train_iter_loss: 0.45950379967689514
train_iter_loss: 0.12001138925552368
train_iter_loss: 0.2587584853172302
train_iter_loss: 0.2590302526950836
train_iter_loss: 0.4586356580257416
train_iter_loss: 0.2641052007675171
train_iter_loss: 0.41826242208480835
train_iter_loss: 0.26084262132644653
train_iter_loss: 0.2789272367954254
train_iter_loss: 0.22028464078903198
train_iter_loss: 0.3309108018875122
train_iter_loss: 0.24140340089797974
train loss :0.2730
---------------------
Validation seg loss: 0.35935661266997176 at epoch 528
epoch =    529/  1000, exp = train
train_iter_loss: 0.0972488597035408
train_iter_loss: 0.2173198163509369
train_iter_loss: 0.1740390807390213
train_iter_loss: 0.07567139714956284
train_iter_loss: 0.30197471380233765
train_iter_loss: 0.38178130984306335
train_iter_loss: 0.2505553066730499
train_iter_loss: 0.35407769680023193
train_iter_loss: 0.1809977889060974
train_iter_loss: 0.18738599121570587
train_iter_loss: 0.24949876964092255
train_iter_loss: 0.25181904435157776
train_iter_loss: 0.2976902723312378
train_iter_loss: 0.2247452437877655
train_iter_loss: 0.24331989884376526
train_iter_loss: 0.30945608019828796
train_iter_loss: 0.20898254215717316
train_iter_loss: 0.3414633274078369
train_iter_loss: 0.16654722392559052
train_iter_loss: 0.3235909342765808
train_iter_loss: 0.24617482721805573
train_iter_loss: 0.34014102816581726
train_iter_loss: 0.19009087979793549
train_iter_loss: 0.25390735268592834
train_iter_loss: 0.16728399693965912
train_iter_loss: 0.09376182407140732
train_iter_loss: 0.2536083459854126
train_iter_loss: 0.2183881402015686
train_iter_loss: 0.2922753691673279
train_iter_loss: 0.16174356639385223
train_iter_loss: 0.31518256664276123
train_iter_loss: 0.23381511867046356
train_iter_loss: 0.3922526240348816
train_iter_loss: 0.17985780537128448
train_iter_loss: 0.26612329483032227
train_iter_loss: 0.1982126384973526
train_iter_loss: 0.35285285115242004
train_iter_loss: 0.3069341480731964
train_iter_loss: 0.35110512375831604
train_iter_loss: 0.22462905943393707
train_iter_loss: 0.43732282519340515
train_iter_loss: 0.2912549674510956
train_iter_loss: 0.25422653555870056
train_iter_loss: 0.07189776748418808
train_iter_loss: 0.45063215494155884
train_iter_loss: 0.16365747153759003
train_iter_loss: 0.18878261744976044
train_iter_loss: 0.2693227231502533
train_iter_loss: 0.3278905153274536
train_iter_loss: 0.28445762395858765
train_iter_loss: 0.1952430158853531
train_iter_loss: 0.2929534614086151
train_iter_loss: 0.40699923038482666
train_iter_loss: 0.19814731180667877
train_iter_loss: 0.32432013750076294
train_iter_loss: 0.14981256425380707
train_iter_loss: 0.42261332273483276
train_iter_loss: 0.22476820647716522
train_iter_loss: 0.3190337121486664
train_iter_loss: 0.29407691955566406
train_iter_loss: 0.2742351293563843
train_iter_loss: 0.3754040002822876
train_iter_loss: 0.15447254478931427
train_iter_loss: 0.20793670415878296
train_iter_loss: 0.3027789294719696
train_iter_loss: 0.2689347267150879
train_iter_loss: 0.292358934879303
train_iter_loss: 0.24883733689785004
train_iter_loss: 0.32111281156539917
train_iter_loss: 0.3464466333389282
train_iter_loss: 0.3368431031703949
train_iter_loss: 0.2719103693962097
train_iter_loss: 0.37083500623703003
train_iter_loss: 0.3240559995174408
train_iter_loss: 0.0852917730808258
train_iter_loss: 0.37221264839172363
train_iter_loss: 0.25643032789230347
train_iter_loss: 0.43566229939460754
train_iter_loss: 0.2784369885921478
train_iter_loss: 0.23419170081615448
train_iter_loss: 0.24733226001262665
train_iter_loss: 0.35955628752708435
train_iter_loss: 0.24054966866970062
train_iter_loss: 0.47090470790863037
train_iter_loss: 0.20718874037265778
train_iter_loss: 0.15343694388866425
train_iter_loss: 0.31405022740364075
train_iter_loss: 0.364300012588501
train_iter_loss: 0.19785231351852417
train_iter_loss: 0.30845290422439575
train_iter_loss: 0.16606828570365906
train_iter_loss: 0.017719710245728493
train_iter_loss: 0.2989366054534912
train_iter_loss: 0.4017890691757202
train_iter_loss: 0.3162631690502167
train_iter_loss: 0.2563708424568176
train_iter_loss: 0.29120171070098877
train_iter_loss: 0.2794540226459503
train_iter_loss: 0.2087223082780838
train_iter_loss: 0.3794776499271393
train loss :0.2695
---------------------
Validation seg loss: 0.3833552346705406 at epoch 529
epoch =    530/  1000, exp = train
train_iter_loss: 0.2522504925727844
train_iter_loss: 0.38440248370170593
train_iter_loss: 0.21221211552619934
train_iter_loss: 0.41241347789764404
train_iter_loss: 0.11880620568990707
train_iter_loss: 0.24594824016094208
train_iter_loss: 0.40256986021995544
train_iter_loss: 0.28388795256614685
train_iter_loss: 0.24374254047870636
train_iter_loss: 0.3108045756816864
train_iter_loss: 0.14746522903442383
train_iter_loss: 0.28859880566596985
train_iter_loss: 0.36052921414375305
train_iter_loss: 0.266323983669281
train_iter_loss: 0.2892002761363983
train_iter_loss: 0.3203733265399933
train_iter_loss: 0.10682225227355957
train_iter_loss: 0.24280677735805511
train_iter_loss: 0.25120630860328674
train_iter_loss: 0.10487012565135956
train_iter_loss: 0.034360628575086594
train_iter_loss: 0.30343112349510193
train_iter_loss: 0.255332350730896
train_iter_loss: 0.2764800190925598
train_iter_loss: 0.24240539968013763
train_iter_loss: 0.3520282804965973
train_iter_loss: 0.2576632797718048
train_iter_loss: 0.20783734321594238
train_iter_loss: 0.25611260533332825
train_iter_loss: 0.2660336494445801
train_iter_loss: 0.39060166478157043
train_iter_loss: 0.2592785656452179
train_iter_loss: 0.3101647198200226
train_iter_loss: 0.2114846557378769
train_iter_loss: 0.3806670308113098
train_iter_loss: 0.21938738226890564
train_iter_loss: 0.37807565927505493
train_iter_loss: 0.20363150537014008
train_iter_loss: 0.32187792658805847
train_iter_loss: 0.23222412168979645
train_iter_loss: 0.209976464509964
train_iter_loss: 0.2254602164030075
train_iter_loss: 0.2679484784603119
train_iter_loss: 0.30880725383758545
train_iter_loss: 0.2237531542778015
train_iter_loss: 0.3081248104572296
train_iter_loss: 0.12967708706855774
train_iter_loss: 0.23277439177036285
train_iter_loss: 0.3186720013618469
train_iter_loss: 0.18397916853427887
train_iter_loss: 0.1933833658695221
train_iter_loss: 0.20372845232486725
train_iter_loss: 0.2883740961551666
train_iter_loss: 0.23720088601112366
train_iter_loss: 0.22291690111160278
train_iter_loss: 0.31883788108825684
train_iter_loss: 0.48590508103370667
train_iter_loss: 0.49135279655456543
train_iter_loss: 0.31184202432632446
train_iter_loss: 0.34096619486808777
train_iter_loss: 0.42378249764442444
train_iter_loss: 0.41053807735443115
train_iter_loss: 0.3153163492679596
train_iter_loss: 0.2970082759857178
train_iter_loss: 0.4108599126338959
train_iter_loss: 0.22551023960113525
train_iter_loss: 0.353906512260437
train_iter_loss: 0.2381867617368698
train_iter_loss: 0.16523489356040955
train_iter_loss: 0.21794646978378296
train_iter_loss: 0.3950057327747345
train_iter_loss: 0.22116535902023315
train_iter_loss: 0.17046527564525604
train_iter_loss: 0.21630878746509552
train_iter_loss: 0.2770509421825409
train_iter_loss: 0.3770025372505188
train_iter_loss: 0.20360501110553741
train_iter_loss: 0.27223527431488037
train_iter_loss: 0.33848920464515686
train_iter_loss: 0.24583718180656433
train_iter_loss: 0.12508167326450348
train_iter_loss: 0.31542861461639404
train_iter_loss: 0.1594967544078827
train_iter_loss: 0.40280377864837646
train_iter_loss: 0.3211156725883484
train_iter_loss: 0.17649175226688385
train_iter_loss: 0.3381161093711853
train_iter_loss: 0.1240990087389946
train_iter_loss: 0.37802284955978394
train_iter_loss: 0.24812768399715424
train_iter_loss: 0.20955568552017212
train_iter_loss: 0.32385656237602234
train_iter_loss: 0.29070478677749634
train_iter_loss: 0.1245766282081604
train_iter_loss: 0.3582976460456848
train_iter_loss: 0.19496074318885803
train_iter_loss: 0.24507541954517365
train_iter_loss: 0.2668832838535309
train_iter_loss: 0.33987078070640564
train_iter_loss: 0.19299232959747314
train loss :0.2729
---------------------
Validation seg loss: 0.3476165688754815 at epoch 530
epoch =    531/  1000, exp = train
train_iter_loss: 0.1699891835451126
train_iter_loss: 0.2142936736345291
train_iter_loss: 0.1650564968585968
train_iter_loss: 0.2038496881723404
train_iter_loss: 0.22085385024547577
train_iter_loss: 0.2200050950050354
train_iter_loss: 0.26548585295677185
train_iter_loss: 0.14956767857074738
train_iter_loss: 0.2759132385253906
train_iter_loss: 0.3060440421104431
train_iter_loss: 0.19857904314994812
train_iter_loss: 0.3357636630535126
train_iter_loss: 0.16213291883468628
train_iter_loss: 0.39768606424331665
train_iter_loss: 0.3214598298072815
train_iter_loss: 0.305960476398468
train_iter_loss: 0.23203040659427643
train_iter_loss: 0.1172436997294426
train_iter_loss: 0.2993118464946747
train_iter_loss: 0.2645244598388672
train_iter_loss: 0.3392854928970337
train_iter_loss: 0.23501528799533844
train_iter_loss: 0.2917119860649109
train_iter_loss: 0.18529532849788666
train_iter_loss: 0.2746279239654541
train_iter_loss: 0.20466899871826172
train_iter_loss: 0.3151780366897583
train_iter_loss: 0.26350101828575134
train_iter_loss: 0.21957749128341675
train_iter_loss: 0.15387512743473053
train_iter_loss: 0.2505096197128296
train_iter_loss: 0.09626112878322601
train_iter_loss: 0.29920724034309387
train_iter_loss: 0.22174133360385895
train_iter_loss: 0.2761370837688446
train_iter_loss: 0.18990875780582428
train_iter_loss: 0.33420732617378235
train_iter_loss: 0.2677985429763794
train_iter_loss: 0.15029661357402802
train_iter_loss: 0.32130905985832214
train_iter_loss: 0.1970355361700058
train_iter_loss: 0.20276720821857452
train_iter_loss: 0.19681960344314575
train_iter_loss: 0.38259366154670715
train_iter_loss: 0.35202717781066895
train_iter_loss: 0.2662465572357178
train_iter_loss: 0.4031139314174652
train_iter_loss: 0.15435652434825897
train_iter_loss: 0.4199460744857788
train_iter_loss: 0.36756882071495056
train_iter_loss: 0.2234324961900711
train_iter_loss: 0.21055246889591217
train_iter_loss: 0.4189033508300781
train_iter_loss: 0.26218488812446594
train_iter_loss: 0.22247780859470367
train_iter_loss: 0.17017905414104462
train_iter_loss: 0.1436796635389328
train_iter_loss: 0.353758841753006
train_iter_loss: 0.3025033175945282
train_iter_loss: 0.23484763503074646
train_iter_loss: 0.2899371087551117
train_iter_loss: 0.24627448618412018
train_iter_loss: 0.19224673509597778
train_iter_loss: 0.45714592933654785
train_iter_loss: 0.27957800030708313
train_iter_loss: 0.321587473154068
train_iter_loss: 0.2943480312824249
train_iter_loss: 0.2739132046699524
train_iter_loss: 0.22241458296775818
train_iter_loss: 0.11205730587244034
train_iter_loss: 0.160965234041214
train_iter_loss: 0.37414273619651794
train_iter_loss: 0.24168136715888977
train_iter_loss: 0.28736206889152527
train_iter_loss: 0.08677906543016434
train_iter_loss: 0.20869746804237366
train_iter_loss: 0.24390728771686554
train_iter_loss: 0.2510652542114258
train_iter_loss: 0.1827852427959442
train_iter_loss: 0.31681251525878906
train_iter_loss: 0.23156361281871796
train_iter_loss: 0.28112104535102844
train_iter_loss: 0.20364341139793396
train_iter_loss: 0.31229740381240845
train_iter_loss: 0.3867873549461365
train_iter_loss: 0.27792081236839294
train_iter_loss: 0.3636285066604614
train_iter_loss: 0.25489872694015503
train_iter_loss: 0.20393037796020508
train_iter_loss: 0.3163035213947296
train_iter_loss: 0.19493116438388824
train_iter_loss: 0.33034607768058777
train_iter_loss: 0.22387927770614624
train_iter_loss: 0.35348236560821533
train_iter_loss: 0.15244491398334503
train_iter_loss: 0.35727667808532715
train_iter_loss: 0.4017051160335541
train_iter_loss: 0.4184882342815399
train_iter_loss: 0.2742334008216858
train_iter_loss: 0.4809943735599518
train loss :0.2654
---------------------
Validation seg loss: 0.4606498630743755 at epoch 531
epoch =    532/  1000, exp = train
train_iter_loss: 0.35201919078826904
train_iter_loss: 0.31332749128341675
train_iter_loss: 0.15504108369350433
train_iter_loss: 0.30358853936195374
train_iter_loss: 0.2801336944103241
train_iter_loss: 0.32212385535240173
train_iter_loss: 0.18263296782970428
train_iter_loss: 0.27280551195144653
train_iter_loss: 0.27592357993125916
train_iter_loss: 0.20415644347667694
train_iter_loss: 0.3902081549167633
train_iter_loss: 0.27488937973976135
train_iter_loss: 0.16467149555683136
train_iter_loss: 0.3419555127620697
train_iter_loss: 0.2807086706161499
train_iter_loss: 0.21550846099853516
train_iter_loss: 0.09574448317289352
train_iter_loss: 0.1565384417772293
train_iter_loss: 0.3068564832210541
train_iter_loss: 0.2553298771381378
train_iter_loss: 0.2872885763645172
train_iter_loss: 0.3369773030281067
train_iter_loss: 0.35074129700660706
train_iter_loss: 0.24491815268993378
train_iter_loss: 0.22867603600025177
train_iter_loss: 0.28752201795578003
train_iter_loss: 0.1743791401386261
train_iter_loss: 0.21734751760959625
train_iter_loss: 0.2044668197631836
train_iter_loss: 0.2441062331199646
train_iter_loss: 0.19895483553409576
train_iter_loss: 0.2584531605243683
train_iter_loss: 0.23864027857780457
train_iter_loss: 0.34302234649658203
train_iter_loss: 0.2434537261724472
train_iter_loss: 0.28603509068489075
train_iter_loss: 0.183262899518013
train_iter_loss: 0.3100453317165375
train_iter_loss: 0.2661493122577667
train_iter_loss: 0.2386925369501114
train_iter_loss: 0.3159560561180115
train_iter_loss: 0.2803630232810974
train_iter_loss: 0.21342086791992188
train_iter_loss: 0.3174625337123871
train_iter_loss: 0.2064402997493744
train_iter_loss: 0.34350642561912537
train_iter_loss: 0.19021862745285034
train_iter_loss: 0.33371901512145996
train_iter_loss: 0.2688441276550293
train_iter_loss: 0.1637255996465683
train_iter_loss: 0.2803991734981537
train_iter_loss: 0.33901602029800415
train_iter_loss: 0.491803914308548
train_iter_loss: 0.28255128860473633
train_iter_loss: 0.11430476605892181
train_iter_loss: 0.2791242003440857
train_iter_loss: 0.25888776779174805
train_iter_loss: 0.2175610363483429
train_iter_loss: 0.3427920937538147
train_iter_loss: 0.1554923951625824
train_iter_loss: 0.1843835711479187
train_iter_loss: 0.4222499132156372
train_iter_loss: 0.274862140417099
train_iter_loss: 0.035945191979408264
train_iter_loss: 0.10640363395214081
train_iter_loss: 0.19273367524147034
train_iter_loss: 0.2569344639778137
train_iter_loss: 0.14722959697246552
train_iter_loss: 0.20978541672229767
train_iter_loss: 0.24900057911872864
train_iter_loss: 0.23324330151081085
train_iter_loss: 0.3217698931694031
train_iter_loss: 0.2999809682369232
train_iter_loss: 0.38442936539649963
train_iter_loss: 0.2596827745437622
train_iter_loss: 0.2068374752998352
train_iter_loss: 0.23663802444934845
train_iter_loss: 0.3140128254890442
train_iter_loss: 0.3810321092605591
train_iter_loss: 0.04683960601687431
train_iter_loss: 0.3207324147224426
train_iter_loss: 0.2803746461868286
train_iter_loss: 0.2567998766899109
train_iter_loss: 0.39045166969299316
train_iter_loss: 0.31171852350234985
train_iter_loss: 0.2358849048614502
train_iter_loss: 0.2061362862586975
train_iter_loss: 0.3333335816860199
train_iter_loss: 0.19148461520671844
train_iter_loss: 0.27453702688217163
train_iter_loss: 0.100949227809906
train_iter_loss: 0.12715472280979156
train_iter_loss: 0.2703888416290283
train_iter_loss: 0.39949822425842285
train_iter_loss: 0.2685735523700714
train_iter_loss: 0.2968699336051941
train_iter_loss: 0.27196428179740906
train_iter_loss: 0.2635840177536011
train_iter_loss: 0.23656505346298218
train_iter_loss: 0.28418388962745667
train loss :0.2603
---------------------
Validation seg loss: 0.3707364977750365 at epoch 532
epoch =    533/  1000, exp = train
train_iter_loss: 0.2535701394081116
train_iter_loss: 0.15421399474143982
train_iter_loss: 0.29593271017074585
train_iter_loss: 0.21984818577766418
train_iter_loss: 0.2987043857574463
train_iter_loss: 0.2975262701511383
train_iter_loss: 0.17198672890663147
train_iter_loss: 0.2746557891368866
train_iter_loss: 0.43824708461761475
train_iter_loss: 0.28741157054901123
train_iter_loss: 0.15996454656124115
train_iter_loss: 0.242656409740448
train_iter_loss: 0.3362189829349518
train_iter_loss: 0.3799974322319031
train_iter_loss: 0.23361161351203918
train_iter_loss: 0.16894860565662384
train_iter_loss: 0.22437331080436707
train_iter_loss: 0.33148542046546936
train_iter_loss: 0.29648733139038086
train_iter_loss: 0.23760533332824707
train_iter_loss: 0.33621010184288025
train_iter_loss: 0.23908542096614838
train_iter_loss: 0.31960996985435486
train_iter_loss: 0.3028447926044464
train_iter_loss: 0.18796463310718536
train_iter_loss: 0.2226884961128235
train_iter_loss: 0.1873256117105484
train_iter_loss: 0.3762982487678528
train_iter_loss: 0.27500224113464355
train_iter_loss: 0.3390664756298065
train_iter_loss: 0.22172945737838745
train_iter_loss: 0.21417734026908875
train_iter_loss: 0.3705625832080841
train_iter_loss: 0.23294903337955475
train_iter_loss: 0.29468899965286255
train_iter_loss: 0.2560628652572632
train_iter_loss: 0.08463764190673828
train_iter_loss: 0.1938951462507248
train_iter_loss: 0.3766695559024811
train_iter_loss: 0.5181365013122559
train_iter_loss: 0.18633081018924713
train_iter_loss: 0.25423818826675415
train_iter_loss: 0.4082237780094147
train_iter_loss: 0.09133242815732956
train_iter_loss: 0.19612954556941986
train_iter_loss: 0.4877120554447174
train_iter_loss: 0.260803759098053
train_iter_loss: 0.2560911178588867
train_iter_loss: 0.49913716316223145
train_iter_loss: 0.18692094087600708
train_iter_loss: 0.1984471082687378
train_iter_loss: 0.3962004482746124
train_iter_loss: 0.33923330903053284
train_iter_loss: 0.25173261761665344
train_iter_loss: 0.2133910059928894
train_iter_loss: 0.3247632086277008
train_iter_loss: 0.2224610596895218
train_iter_loss: 0.27842265367507935
train_iter_loss: 0.31698858737945557
train_iter_loss: 0.39101073145866394
train_iter_loss: 0.3292385935783386
train_iter_loss: 0.28654029965400696
train_iter_loss: 0.60176020860672
train_iter_loss: 0.18849778175354004
train_iter_loss: 0.3588131070137024
train_iter_loss: 0.20897521078586578
train_iter_loss: 0.2859683632850647
train_iter_loss: 0.2522052526473999
train_iter_loss: 0.2222791612148285
train_iter_loss: 0.12688569724559784
train_iter_loss: 0.30492672324180603
train_iter_loss: 0.24877360463142395
train_iter_loss: 0.31076598167419434
train_iter_loss: 0.23758234083652496
train_iter_loss: 0.3190454840660095
train_iter_loss: 0.37459564208984375
train_iter_loss: 0.42991068959236145
train_iter_loss: 0.32917320728302
train_iter_loss: 0.33719927072525024
train_iter_loss: 0.09346990287303925
train_iter_loss: 0.15275561809539795
train_iter_loss: 0.3052069842815399
train_iter_loss: 0.2770150899887085
train_iter_loss: 0.27983367443084717
train_iter_loss: 0.27008360624313354
train_iter_loss: 0.37753766775131226
train_iter_loss: 0.03700342774391174
train_iter_loss: 0.25759363174438477
train_iter_loss: 0.2237616330385208
train_iter_loss: 0.2448534518480301
train_iter_loss: 0.31605276465415955
train_iter_loss: 0.2398737668991089
train_iter_loss: 0.25272276997566223
train_iter_loss: 0.20957574248313904
train_iter_loss: 0.22604332864284515
train_iter_loss: 0.27290964126586914
train_iter_loss: 0.18659664690494537
train_iter_loss: 0.11724916100502014
train_iter_loss: 0.24147029221057892
train_iter_loss: 0.20175623893737793
train loss :0.2741
---------------------
Validation seg loss: 0.3499058035448334 at epoch 533
epoch =    534/  1000, exp = train
train_iter_loss: 0.2630806267261505
train_iter_loss: 0.2715347707271576
train_iter_loss: 0.2284352332353592
train_iter_loss: 0.21435168385505676
train_iter_loss: 0.4412751793861389
train_iter_loss: 0.19806431233882904
train_iter_loss: 0.18459771573543549
train_iter_loss: 0.4361918568611145
train_iter_loss: 0.14741137623786926
train_iter_loss: 0.17332036793231964
train_iter_loss: 0.3928081691265106
train_iter_loss: 0.3272346258163452
train_iter_loss: 0.3490823805332184
train_iter_loss: 0.30758094787597656
train_iter_loss: 0.20988474786281586
train_iter_loss: 0.29065749049186707
train_iter_loss: 0.39546310901641846
train_iter_loss: 0.2681220471858978
train_iter_loss: 0.2577315866947174
train_iter_loss: 0.2011198103427887
train_iter_loss: 0.33871400356292725
train_iter_loss: 0.2936888039112091
train_iter_loss: 0.2642454206943512
train_iter_loss: 0.29009121656417847
train_iter_loss: 0.2961263656616211
train_iter_loss: 0.2209869921207428
train_iter_loss: 0.1776939481496811
train_iter_loss: 0.37719112634658813
train_iter_loss: 0.2553325593471527
train_iter_loss: 0.21332982182502747
train_iter_loss: 0.3270125389099121
train_iter_loss: 0.04513372480869293
train_iter_loss: 0.14841265976428986
train_iter_loss: 0.19552846252918243
train_iter_loss: 0.2042379081249237
train_iter_loss: 0.24060146510601044
train_iter_loss: 0.28421369194984436
train_iter_loss: 0.39742353558540344
train_iter_loss: 0.21856717765331268
train_iter_loss: 0.27126947045326233
train_iter_loss: 0.31604981422424316
train_iter_loss: 0.3317054212093353
train_iter_loss: 0.14859598875045776
train_iter_loss: 0.37421715259552
train_iter_loss: 0.2334555983543396
train_iter_loss: 0.16007433831691742
train_iter_loss: 0.33872953057289124
train_iter_loss: 0.3203086853027344
train_iter_loss: 0.4255182147026062
train_iter_loss: 0.1726815551519394
train_iter_loss: 0.2961978018283844
train_iter_loss: 0.30894285440444946
train_iter_loss: 0.2784676253795624
train_iter_loss: 0.2390478104352951
train_iter_loss: 0.17083612084388733
train_iter_loss: 0.18688315153121948
train_iter_loss: 0.3014322519302368
train_iter_loss: 0.46030667424201965
train_iter_loss: 0.28333234786987305
train_iter_loss: 0.242549329996109
train_iter_loss: 0.3626789152622223
train_iter_loss: 0.23191207647323608
train_iter_loss: 0.28216123580932617
train_iter_loss: 0.28674623370170593
train_iter_loss: 0.23423461616039276
train_iter_loss: 0.42313387989997864
train_iter_loss: 0.18662025034427643
train_iter_loss: 0.3497340977191925
train_iter_loss: 0.16393791139125824
train_iter_loss: 0.2488168179988861
train_iter_loss: 0.3033437132835388
train_iter_loss: 0.23890089988708496
train_iter_loss: 0.2415345013141632
train_iter_loss: 0.16152317821979523
train_iter_loss: 0.20198439061641693
train_iter_loss: 0.11369255185127258
train_iter_loss: 0.29206088185310364
train_iter_loss: 0.26888641715049744
train_iter_loss: 0.3183589577674866
train_iter_loss: 0.2561285197734833
train_iter_loss: 0.35569754242897034
train_iter_loss: 0.3021431267261505
train_iter_loss: 0.26007309556007385
train_iter_loss: 0.30646973848342896
train_iter_loss: 0.2806595265865326
train_iter_loss: 0.2888075113296509
train_iter_loss: 0.3753076195716858
train_iter_loss: 0.25555992126464844
train_iter_loss: 0.12279926240444183
train_iter_loss: 0.21791568398475647
train_iter_loss: 0.34357479214668274
train_iter_loss: 0.19628633558750153
train_iter_loss: 0.36418667435646057
train_iter_loss: 0.270216166973114
train_iter_loss: 0.31358733773231506
train_iter_loss: 0.15591251850128174
train_iter_loss: 0.1261463165283203
train_iter_loss: 0.3332282602787018
train_iter_loss: 0.2956410348415375
train_iter_loss: 0.3247879445552826
train loss :0.2714
---------------------
Validation seg loss: 0.36341480337048193 at epoch 534
epoch =    535/  1000, exp = train
train_iter_loss: 0.2096179574728012
train_iter_loss: 0.14998799562454224
train_iter_loss: 0.2532389163970947
train_iter_loss: 0.3226873576641083
train_iter_loss: 0.26266616582870483
train_iter_loss: 0.2924366593360901
train_iter_loss: 0.340188592672348
train_iter_loss: 0.24436819553375244
train_iter_loss: 0.4019632935523987
train_iter_loss: 0.25642451643943787
train_iter_loss: 0.29995566606521606
train_iter_loss: 0.1170889139175415
train_iter_loss: 0.3077114522457123
train_iter_loss: 0.4846249222755432
train_iter_loss: 0.19294553995132446
train_iter_loss: 0.175767719745636
train_iter_loss: 0.3955383896827698
train_iter_loss: 0.20019404590129852
train_iter_loss: 0.3068592846393585
train_iter_loss: 0.3385905921459198
train_iter_loss: 0.1549525111913681
train_iter_loss: 0.25543031096458435
train_iter_loss: 0.28343668580055237
train_iter_loss: 0.2942976653575897
train_iter_loss: 0.29457855224609375
train_iter_loss: 0.19464707374572754
train_iter_loss: 0.1834108680486679
train_iter_loss: 0.13707506656646729
train_iter_loss: 0.27424830198287964
train_iter_loss: 0.37428781390190125
train_iter_loss: 0.23428423702716827
train_iter_loss: 0.11535833775997162
train_iter_loss: 0.3653661012649536
train_iter_loss: 0.31687188148498535
train_iter_loss: 0.12026447057723999
train_iter_loss: 0.2709442675113678
train_iter_loss: 0.22954826056957245
train_iter_loss: 0.2140790820121765
train_iter_loss: 0.2770935297012329
train_iter_loss: 0.16735845804214478
train_iter_loss: 0.26588207483291626
train_iter_loss: 0.32076460123062134
train_iter_loss: 0.316695898771286
train_iter_loss: 0.13092079758644104
train_iter_loss: 0.35972175002098083
train_iter_loss: 0.13261856138706207
train_iter_loss: 0.17996764183044434
train_iter_loss: 0.27022501826286316
train_iter_loss: 0.16156408190727234
train_iter_loss: 0.3248083293437958
train_iter_loss: 0.2023162990808487
train_iter_loss: 0.202863872051239
train_iter_loss: 0.2286699116230011
train_iter_loss: 0.29244160652160645
train_iter_loss: 0.26815107464790344
train_iter_loss: 0.27892419695854187
train_iter_loss: 0.4005868136882782
train_iter_loss: 0.2762508690357208
train_iter_loss: 0.3924984633922577
train_iter_loss: 0.25868165493011475
train_iter_loss: 0.33658134937286377
train_iter_loss: 0.20623789727687836
train_iter_loss: 0.2995694577693939
train_iter_loss: 0.15107469260692596
train_iter_loss: 0.3443629741668701
train_iter_loss: 0.24749703705310822
train_iter_loss: 0.15390966832637787
train_iter_loss: 0.1319059282541275
train_iter_loss: 0.29643771052360535
train_iter_loss: 0.2804262936115265
train_iter_loss: 0.3145342171192169
train_iter_loss: 0.3240946829319
train_iter_loss: 0.2869627773761749
train_iter_loss: 0.17596837878227234
train_iter_loss: 0.16664525866508484
train_iter_loss: 0.20777854323387146
train_iter_loss: 0.2474503517150879
train_iter_loss: 0.3355512022972107
train_iter_loss: 0.2644011080265045
train_iter_loss: 0.34580859541893005
train_iter_loss: 0.39148640632629395
train_iter_loss: 0.3016788959503174
train_iter_loss: 0.18279926478862762
train_iter_loss: 0.1812884509563446
train_iter_loss: 0.3530028164386749
train_iter_loss: 0.24366062879562378
train_iter_loss: 0.37891632318496704
train_iter_loss: 0.24773187935352325
train_iter_loss: 0.31912410259246826
train_iter_loss: 0.2567424774169922
train_iter_loss: 0.19701626896858215
train_iter_loss: 0.19786599278450012
train_iter_loss: 0.3234984278678894
train_iter_loss: 0.3571896553039551
train_iter_loss: 0.30264079570770264
train_iter_loss: 0.22966204583644867
train_iter_loss: 0.32406100630760193
train_iter_loss: 0.21428419649600983
train_iter_loss: 0.2278224229812622
train_iter_loss: 0.36948153376579285
train loss :0.2656
---------------------
Validation seg loss: 0.37546369028365556 at epoch 535
epoch =    536/  1000, exp = train
train_iter_loss: 0.26060619950294495
train_iter_loss: 0.3120274245738983
train_iter_loss: 0.25502341985702515
train_iter_loss: 0.22901080548763275
train_iter_loss: 0.19841226935386658
train_iter_loss: 0.4397660493850708
train_iter_loss: 0.1913650780916214
train_iter_loss: 0.21139468252658844
train_iter_loss: 0.4454791247844696
train_iter_loss: 0.2805657386779785
train_iter_loss: 0.1620481014251709
train_iter_loss: 0.2923394739627838
train_iter_loss: 0.18456144630908966
train_iter_loss: 0.2933887243270874
train_iter_loss: 0.1860061138868332
train_iter_loss: 0.21790924668312073
train_iter_loss: 0.36517009139060974
train_iter_loss: 0.31521478295326233
train_iter_loss: 0.21424226462841034
train_iter_loss: 0.1559837907552719
train_iter_loss: 0.2410064935684204
train_iter_loss: 0.34964069724082947
train_iter_loss: 0.23367440700531006
train_iter_loss: 0.3643175959587097
train_iter_loss: 0.4129665791988373
train_iter_loss: 0.19708359241485596
train_iter_loss: 0.244714617729187
train_iter_loss: 0.36290279030799866
train_iter_loss: 0.24768635630607605
train_iter_loss: 0.2526029348373413
train_iter_loss: 0.33609580993652344
train_iter_loss: 0.18333862721920013
train_iter_loss: 0.245931014418602
train_iter_loss: 0.1334380805492401
train_iter_loss: 0.3422252833843231
train_iter_loss: 0.28354811668395996
train_iter_loss: 0.19120262563228607
train_iter_loss: 0.4488910138607025
train_iter_loss: 0.24768099188804626
train_iter_loss: 0.20458786189556122
train_iter_loss: 0.3372727334499359
train_iter_loss: 0.22355423867702484
train_iter_loss: 0.34969863295555115
train_iter_loss: 0.36074602603912354
train_iter_loss: 0.24548037350177765
train_iter_loss: 0.1869935393333435
train_iter_loss: 0.25876471400260925
train_iter_loss: 0.3151271939277649
train_iter_loss: 0.3741239011287689
train_iter_loss: 0.35021546483039856
train_iter_loss: 0.20888198912143707
train_iter_loss: 0.11653506755828857
train_iter_loss: 0.19874294102191925
train_iter_loss: 0.09230692684650421
train_iter_loss: 0.3351998031139374
train_iter_loss: 0.3339798152446747
train_iter_loss: 0.3486177325248718
train_iter_loss: 0.23184925317764282
train_iter_loss: 0.2526794373989105
train_iter_loss: 0.34429240226745605
train_iter_loss: 0.3740326464176178
train_iter_loss: 0.307598352432251
train_iter_loss: 0.24117904901504517
train_iter_loss: 0.3409431278705597
train_iter_loss: 0.22333279252052307
train_iter_loss: 0.3377630114555359
train_iter_loss: 0.07742520421743393
train_iter_loss: 0.35489916801452637
train_iter_loss: 0.29435062408447266
train_iter_loss: 0.21714434027671814
train_iter_loss: 0.2703266441822052
train_iter_loss: 0.1952315866947174
train_iter_loss: 0.10095185041427612
train_iter_loss: 0.2385784387588501
train_iter_loss: 0.1621764600276947
train_iter_loss: 0.192241370677948
train_iter_loss: 0.2317723035812378
train_iter_loss: 0.346758633852005
train_iter_loss: 0.3512450158596039
train_iter_loss: 0.21684446930885315
train_iter_loss: 0.27234163880348206
train_iter_loss: 0.2773933708667755
train_iter_loss: 0.10188795626163483
train_iter_loss: 0.24622496962547302
train_iter_loss: 0.18886633217334747
train_iter_loss: 0.2745071351528168
train_iter_loss: 0.3086579740047455
train_iter_loss: 0.25301820039749146
train_iter_loss: 0.2852722108364105
train_iter_loss: 0.3991744816303253
train_iter_loss: 0.2972997725009918
train_iter_loss: 0.43388816714286804
train_iter_loss: 0.3625852167606354
train_iter_loss: 0.2954981327056885
train_iter_loss: 0.14451180398464203
train_iter_loss: 0.1069764718413353
train_iter_loss: 0.29651397466659546
train_iter_loss: 0.2586570084095001
train_iter_loss: 0.2893761694431305
train_iter_loss: 0.28394246101379395
train loss :0.2692
---------------------
Validation seg loss: 0.36088882876946676 at epoch 536
epoch =    537/  1000, exp = train
train_iter_loss: 0.14191323518753052
train_iter_loss: 0.1792314648628235
train_iter_loss: 0.4046090245246887
train_iter_loss: 0.2928721606731415
train_iter_loss: 0.025277676060795784
train_iter_loss: 0.22526119649410248
train_iter_loss: 0.20054171979427338
train_iter_loss: 0.17549873888492584
train_iter_loss: 0.19974108040332794
train_iter_loss: 0.29724055528640747
train_iter_loss: 0.0251605287194252
train_iter_loss: 0.20032702386379242
train_iter_loss: 0.3521519899368286
train_iter_loss: 0.1309642344713211
train_iter_loss: 0.2762189507484436
train_iter_loss: 0.24327068030834198
train_iter_loss: 0.3252525329589844
train_iter_loss: 0.3152766525745392
train_iter_loss: 0.2074442058801651
train_iter_loss: 0.22159266471862793
train_iter_loss: 0.1580175906419754
train_iter_loss: 0.10871195048093796
train_iter_loss: 0.286072701215744
train_iter_loss: 0.30020076036453247
train_iter_loss: 0.25687509775161743
train_iter_loss: 0.17218975722789764
train_iter_loss: 0.3930853009223938
train_iter_loss: 0.3044130802154541
train_iter_loss: 0.13677455484867096
train_iter_loss: 0.35589495301246643
train_iter_loss: 0.17345969378948212
train_iter_loss: 0.37671908736228943
train_iter_loss: 0.34280067682266235
train_iter_loss: 0.1576004922389984
train_iter_loss: 0.4641101658344269
train_iter_loss: 0.47075268626213074
train_iter_loss: 0.28684383630752563
train_iter_loss: 0.3969721496105194
train_iter_loss: 0.1981874257326126
train_iter_loss: 0.10034499317407608
train_iter_loss: 0.308366596698761
train_iter_loss: 0.2810235619544983
train_iter_loss: 0.3277265727519989
train_iter_loss: 0.3665989935398102
train_iter_loss: 0.26838234066963196
train_iter_loss: 0.39897504448890686
train_iter_loss: 0.2532135546207428
train_iter_loss: 0.28401878476142883
train_iter_loss: 0.3123630881309509
train_iter_loss: 0.4019768238067627
train_iter_loss: 0.22315296530723572
train_iter_loss: 0.22995871305465698
train_iter_loss: 0.2673647105693817
train_iter_loss: 0.2354350984096527
train_iter_loss: 0.3087446689605713
train_iter_loss: 0.17428821325302124
train_iter_loss: 0.29177650809288025
train_iter_loss: 0.15433533489704132
train_iter_loss: 0.1906222403049469
train_iter_loss: 0.21166662871837616
train_iter_loss: 0.34417200088500977
train_iter_loss: 0.3352774679660797
train_iter_loss: 0.17987380921840668
train_iter_loss: 0.2520178258419037
train_iter_loss: 0.13031752407550812
train_iter_loss: 0.36006301641464233
train_iter_loss: 0.16821296513080597
train_iter_loss: 0.35019224882125854
train_iter_loss: 0.32974693179130554
train_iter_loss: 0.11189719289541245
train_iter_loss: 0.3467586934566498
train_iter_loss: 0.55122309923172
train_iter_loss: 0.2498779296875
train_iter_loss: 0.28278470039367676
train_iter_loss: 0.34197843074798584
train_iter_loss: 0.22714155912399292
train_iter_loss: 0.2580641806125641
train_iter_loss: 0.2730340361595154
train_iter_loss: 0.20453110337257385
train_iter_loss: 0.3149619400501251
train_iter_loss: 0.10984531044960022
train_iter_loss: 0.2764510214328766
train_iter_loss: 0.11882388591766357
train_iter_loss: 0.3986477553844452
train_iter_loss: 0.2875712215900421
train_iter_loss: 0.3770378530025482
train_iter_loss: 0.31446167826652527
train_iter_loss: 0.20368707180023193
train_iter_loss: 0.22411128878593445
train_iter_loss: 0.2565757632255554
train_iter_loss: 0.3145386874675751
train_iter_loss: 0.09197923541069031
train_iter_loss: 0.29839470982551575
train_iter_loss: 0.25905731320381165
train_iter_loss: 0.20330891013145447
train_iter_loss: 0.25340110063552856
train_iter_loss: 0.3233950734138489
train_iter_loss: 0.277199923992157
train_iter_loss: 0.2532395124435425
train_iter_loss: 0.21061569452285767
train loss :0.2630
---------------------
Validation seg loss: 0.36050060727053657 at epoch 537
epoch =    538/  1000, exp = train
train_iter_loss: 0.3898344337940216
train_iter_loss: 0.29385173320770264
train_iter_loss: 0.3412639796733856
train_iter_loss: 0.22111135721206665
train_iter_loss: 0.086591936647892
train_iter_loss: 0.32276079058647156
train_iter_loss: 0.05572116747498512
train_iter_loss: 0.6458650231361389
train_iter_loss: 0.3482922315597534
train_iter_loss: 0.4878557026386261
train_iter_loss: 0.3610069751739502
train_iter_loss: 0.1715337038040161
train_iter_loss: 0.22893543541431427
train_iter_loss: 0.2782130539417267
train_iter_loss: 0.08239119499921799
train_iter_loss: 0.23455750942230225
train_iter_loss: 0.2963027060031891
train_iter_loss: 0.3078991770744324
train_iter_loss: 0.10515785962343216
train_iter_loss: 0.3493184745311737
train_iter_loss: 0.3303122818470001
train_iter_loss: 0.16021494567394257
train_iter_loss: 0.250826895236969
train_iter_loss: 0.2211454063653946
train_iter_loss: 0.2844892144203186
train_iter_loss: 0.1973295509815216
train_iter_loss: 0.2724955976009369
train_iter_loss: 0.23568467795848846
train_iter_loss: 0.12111974507570267
train_iter_loss: 0.10100073367357254
train_iter_loss: 0.5802901983261108
train_iter_loss: 0.23258668184280396
train_iter_loss: 0.2688863277435303
train_iter_loss: 0.22809098660945892
train_iter_loss: 0.20587535202503204
train_iter_loss: 0.1940985471010208
train_iter_loss: 0.3912270963191986
train_iter_loss: 0.4259607493877411
train_iter_loss: 0.3571181297302246
train_iter_loss: 0.2116701900959015
train_iter_loss: 0.19603890180587769
train_iter_loss: 0.29187557101249695
train_iter_loss: 0.4104275703430176
train_iter_loss: 0.2344941794872284
train_iter_loss: 0.48652780055999756
train_iter_loss: 0.15231764316558838
train_iter_loss: 0.10954877734184265
train_iter_loss: 0.2157321274280548
train_iter_loss: 0.48126208782196045
train_iter_loss: 0.1440492868423462
train_iter_loss: 0.2401471734046936
train_iter_loss: 0.29390278458595276
train_iter_loss: 0.24463577568531036
train_iter_loss: 0.2626079320907593
train_iter_loss: 0.40569427609443665
train_iter_loss: 0.1763002872467041
train_iter_loss: 0.24895523488521576
train_iter_loss: 0.2124110907316208
train_iter_loss: 0.20912913978099823
train_iter_loss: 0.3215150833129883
train_iter_loss: 0.23796366155147552
train_iter_loss: 0.2867278456687927
train_iter_loss: 0.2869851589202881
train_iter_loss: 0.24152472615242004
train_iter_loss: 0.1237058937549591
train_iter_loss: 0.31073927879333496
train_iter_loss: 0.4491097629070282
train_iter_loss: 0.2708989977836609
train_iter_loss: 0.3906984329223633
train_iter_loss: 0.20333214104175568
train_iter_loss: 0.33532267808914185
train_iter_loss: 0.2578381597995758
train_iter_loss: 0.3283098340034485
train_iter_loss: 0.11053013801574707
train_iter_loss: 0.3278103768825531
train_iter_loss: 0.16305258870124817
train_iter_loss: 0.35750460624694824
train_iter_loss: 0.3618127107620239
train_iter_loss: 0.19752758741378784
train_iter_loss: 0.31320101022720337
train_iter_loss: 0.1853601038455963
train_iter_loss: 0.20286402106285095
train_iter_loss: 0.33024922013282776
train_iter_loss: 0.22419844567775726
train_iter_loss: 0.19027702510356903
train_iter_loss: 0.20462779700756073
train_iter_loss: 0.5151246190071106
train_iter_loss: 0.22390909492969513
train_iter_loss: 0.2357504814863205
train_iter_loss: 0.43607744574546814
train_iter_loss: 0.23462314903736115
train_iter_loss: 0.3584434390068054
train_iter_loss: 0.3072821795940399
train_iter_loss: 0.26152485609054565
train_iter_loss: 0.22803448140621185
train_iter_loss: 0.42233145236968994
train_iter_loss: 0.19371776282787323
train_iter_loss: 0.21576064825057983
train_iter_loss: 0.19721300899982452
train_iter_loss: 0.37041646242141724
train loss :0.2757
---------------------
Validation seg loss: 0.3682375514071505 at epoch 538
epoch =    539/  1000, exp = train
train_iter_loss: 0.31827306747436523
train_iter_loss: 0.15612398087978363
train_iter_loss: 0.26809972524642944
train_iter_loss: 0.25628945231437683
train_iter_loss: 0.15930631756782532
train_iter_loss: 0.45020610094070435
train_iter_loss: 0.33486393094062805
train_iter_loss: 0.19285042583942413
train_iter_loss: 0.37760964035987854
train_iter_loss: 0.26030921936035156
train_iter_loss: 0.19205909967422485
train_iter_loss: 0.21686670184135437
train_iter_loss: 0.20768065750598907
train_iter_loss: 0.3230568766593933
train_iter_loss: 0.1655314713716507
train_iter_loss: 0.14085252583026886
train_iter_loss: 0.2777211368083954
train_iter_loss: 0.34315699338912964
train_iter_loss: 0.18260449171066284
train_iter_loss: 0.11745540797710419
train_iter_loss: 0.31558462977409363
train_iter_loss: 0.32869917154312134
train_iter_loss: 0.34848442673683167
train_iter_loss: 0.17766964435577393
train_iter_loss: 0.3298063278198242
train_iter_loss: 0.26483598351478577
train_iter_loss: 0.30063045024871826
train_iter_loss: 0.21084630489349365
train_iter_loss: 0.245449498295784
train_iter_loss: 0.37929460406303406
train_iter_loss: 0.25354593992233276
train_iter_loss: 0.2312745898962021
train_iter_loss: 0.10378715395927429
train_iter_loss: 0.28065767884254456
train_iter_loss: 0.12215176224708557
train_iter_loss: 0.23004285991191864
train_iter_loss: 0.33246180415153503
train_iter_loss: 0.3266375958919525
train_iter_loss: 0.2586439847946167
train_iter_loss: 0.31675422191619873
train_iter_loss: 0.17803819477558136
train_iter_loss: 0.1984211951494217
train_iter_loss: 0.11906392872333527
train_iter_loss: 0.24930305778980255
train_iter_loss: 0.23762938380241394
train_iter_loss: 0.1702931523323059
train_iter_loss: 0.28658655285835266
train_iter_loss: 0.24658188223838806
train_iter_loss: 0.264536052942276
train_iter_loss: 0.2923562526702881
train_iter_loss: 0.448345810174942
train_iter_loss: 0.349959135055542
train_iter_loss: 0.23275220394134521
train_iter_loss: 0.21749745309352875
train_iter_loss: 0.2582525908946991
train_iter_loss: 0.24922031164169312
train_iter_loss: 0.21559374034404755
train_iter_loss: 0.24045267701148987
train_iter_loss: 0.2751663625240326
train_iter_loss: 0.3911267817020416
train_iter_loss: 0.3953092396259308
train_iter_loss: 0.10756286233663559
train_iter_loss: 0.2166169136762619
train_iter_loss: 0.5842273831367493
train_iter_loss: 0.33099010586738586
train_iter_loss: 0.13959868252277374
train_iter_loss: 0.410672664642334
train_iter_loss: 0.3166908323764801
train_iter_loss: 0.22311337292194366
train_iter_loss: 0.2939453125
train_iter_loss: 0.19211320579051971
train_iter_loss: 0.16381724178791046
train_iter_loss: 0.3530597388744354
train_iter_loss: 0.12579384446144104
train_iter_loss: 0.34699806571006775
train_iter_loss: 0.23986567556858063
train_iter_loss: 0.3303491175174713
train_iter_loss: 0.20316390693187714
train_iter_loss: 0.2288202941417694
train_iter_loss: 0.33600670099258423
train_iter_loss: 0.273393452167511
train_iter_loss: 0.30311912298202515
train_iter_loss: 0.46557560563087463
train_iter_loss: 0.14481939375400543
train_iter_loss: 0.3022782504558563
train_iter_loss: 0.330111026763916
train_iter_loss: 0.20644229650497437
train_iter_loss: 0.38442322611808777
train_iter_loss: 0.33982914686203003
train_iter_loss: 0.18966816365718842
train_iter_loss: 0.32275789976119995
train_iter_loss: 0.16891568899154663
train_iter_loss: 0.4174290597438812
train_iter_loss: 0.10339465737342834
train_iter_loss: 0.17823170125484467
train_iter_loss: 0.3686189651489258
train_iter_loss: 0.14621411263942719
train_iter_loss: 0.609191358089447
train_iter_loss: 0.16230270266532898
train_iter_loss: 0.1985359489917755
train loss :0.2685
---------------------
Validation seg loss: 0.3746414349660418 at epoch 539
epoch =    540/  1000, exp = train
train_iter_loss: 0.1772465705871582
train_iter_loss: 0.2881856858730316
train_iter_loss: 0.319405734539032
train_iter_loss: 0.25329914689064026
train_iter_loss: 0.31098154187202454
train_iter_loss: 0.37086474895477295
train_iter_loss: 0.15037794411182404
train_iter_loss: 0.16299644112586975
train_iter_loss: 0.38348427414894104
train_iter_loss: 0.2047129124403
train_iter_loss: 0.2908637225627899
train_iter_loss: 0.5234428644180298
train_iter_loss: 0.36454474925994873
train_iter_loss: 0.2787422835826874
train_iter_loss: 0.3269050121307373
train_iter_loss: 0.3765485882759094
train_iter_loss: 0.1362658441066742
train_iter_loss: 0.26188692450523376
train_iter_loss: 0.36747264862060547
train_iter_loss: 0.3219411373138428
train_iter_loss: 0.32772570848464966
train_iter_loss: 0.24080614745616913
train_iter_loss: 0.3872853219509125
train_iter_loss: 0.38238784670829773
train_iter_loss: 0.3460966944694519
train_iter_loss: 0.21050840616226196
train_iter_loss: 0.23867842555046082
train_iter_loss: 0.33629217743873596
train_iter_loss: 0.2207651436328888
train_iter_loss: 0.13932831585407257
train_iter_loss: 0.32018744945526123
train_iter_loss: 0.39322301745414734
train_iter_loss: 0.26200759410858154
train_iter_loss: 0.1828257292509079
train_iter_loss: 0.34164759516716003
train_iter_loss: 0.11820919811725616
train_iter_loss: 0.08603082597255707
train_iter_loss: 0.2852032482624054
train_iter_loss: 0.3163232207298279
train_iter_loss: 0.1891472488641739
train_iter_loss: 0.5157836675643921
train_iter_loss: 0.2802245020866394
train_iter_loss: 0.33861038088798523
train_iter_loss: 0.32015305757522583
train_iter_loss: 0.28067412972450256
train_iter_loss: 0.3204260766506195
train_iter_loss: 0.2860095202922821
train_iter_loss: 0.38165587186813354
train_iter_loss: 0.1793375015258789
train_iter_loss: 0.20096637308597565
train_iter_loss: 0.33471888303756714
train_iter_loss: 0.2344685196876526
train_iter_loss: 0.2262510061264038
train_iter_loss: 0.07117820531129837
train_iter_loss: 0.2566056251525879
train_iter_loss: 0.2911193370819092
train_iter_loss: 0.3279956877231598
train_iter_loss: 0.32181140780448914
train_iter_loss: 0.13070344924926758
train_iter_loss: 0.2687765657901764
train_iter_loss: 0.32253509759902954
train_iter_loss: 0.3032841384410858
train_iter_loss: 0.2543730139732361
train_iter_loss: 0.2761862277984619
train_iter_loss: 0.5044368505477905
train_iter_loss: 0.22328616678714752
train_iter_loss: 0.30138638615608215
train_iter_loss: 0.23002474009990692
train_iter_loss: 0.10466878116130829
train_iter_loss: 0.2740616798400879
train_iter_loss: 0.2111176997423172
train_iter_loss: 0.2447722852230072
train_iter_loss: 0.21966767311096191
train_iter_loss: 0.31388017535209656
train_iter_loss: 0.40692800283432007
train_iter_loss: 0.3234769105911255
train_iter_loss: 0.1728290468454361
train_iter_loss: 0.1599782109260559
train_iter_loss: 0.4288005530834198
train_iter_loss: 0.08857423812150955
train_iter_loss: 0.2625056207180023
train_iter_loss: 0.274081289768219
train_iter_loss: 0.19537238776683807
train_iter_loss: 0.2622509300708771
train_iter_loss: 0.26898452639579773
train_iter_loss: 0.2831721901893616
train_iter_loss: 0.13545329868793488
train_iter_loss: 0.5609754323959351
train_iter_loss: 0.30652081966400146
train_iter_loss: 0.19241711497306824
train_iter_loss: 0.3204370141029358
train_iter_loss: 0.20324169099330902
train_iter_loss: 0.23901061713695526
train_iter_loss: 0.2823779582977295
train_iter_loss: 0.4449753761291504
train_iter_loss: 0.25581926107406616
train_iter_loss: 0.03646429255604744
train_iter_loss: 0.191824272274971
train_iter_loss: 0.37299931049346924
train_iter_loss: 0.2640748620033264
train loss :0.2775
---------------------
Validation seg loss: 0.3674198906288816 at epoch 540
epoch =    541/  1000, exp = train
train_iter_loss: 0.40364769101142883
train_iter_loss: 0.0944354385137558
train_iter_loss: 0.22099272906780243
train_iter_loss: 0.19807115197181702
train_iter_loss: 0.26052573323249817
train_iter_loss: 0.2774181663990021
train_iter_loss: 0.37209200859069824
train_iter_loss: 0.3135361969470978
train_iter_loss: 0.33430027961730957
train_iter_loss: 0.2788553535938263
train_iter_loss: 0.33188876509666443
train_iter_loss: 0.34933391213417053
train_iter_loss: 0.5157346725463867
train_iter_loss: 0.30445870757102966
train_iter_loss: 0.13330811262130737
train_iter_loss: 0.19329027831554413
train_iter_loss: 0.33754634857177734
train_iter_loss: 0.29894715547561646
train_iter_loss: 0.09168136864900589
train_iter_loss: 0.19813202321529388
train_iter_loss: 0.3823830485343933
train_iter_loss: 0.19236572086811066
train_iter_loss: 0.10119760781526566
train_iter_loss: 0.24875319004058838
train_iter_loss: 0.1996651291847229
train_iter_loss: 0.4799860119819641
train_iter_loss: 0.23924045264720917
train_iter_loss: 0.23113030195236206
train_iter_loss: 0.30144432187080383
train_iter_loss: 0.2514866888523102
train_iter_loss: 0.2542932629585266
train_iter_loss: 0.4687327444553375
train_iter_loss: 0.40715956687927246
train_iter_loss: 0.394477903842926
train_iter_loss: 0.2775058448314667
train_iter_loss: 0.22202253341674805
train_iter_loss: 0.35366737842559814
train_iter_loss: 0.3152954876422882
train_iter_loss: 0.30891650915145874
train_iter_loss: 0.23617714643478394
train_iter_loss: 0.32570329308509827
train_iter_loss: 0.3012310266494751
train_iter_loss: 0.23871904611587524
train_iter_loss: 0.27822554111480713
train_iter_loss: 0.20131506025791168
train_iter_loss: 0.259387344121933
train_iter_loss: 0.2154291719198227
train_iter_loss: 0.450512558221817
train_iter_loss: 0.21930506825447083
train_iter_loss: 0.24649469554424286
train_iter_loss: 0.374019056558609
train_iter_loss: 0.2380458563566208
train_iter_loss: 0.34116309881210327
train_iter_loss: 0.2683952748775482
train_iter_loss: 0.15405234694480896
train_iter_loss: 0.2153702825307846
train_iter_loss: 0.18178966641426086
train_iter_loss: 0.2599968910217285
train_iter_loss: 0.2532883286476135
train_iter_loss: 0.027903828769922256
train_iter_loss: 0.21945831179618835
train_iter_loss: 0.24021576344966888
train_iter_loss: 0.7329314947128296
train_iter_loss: 0.22116316854953766
train_iter_loss: 0.20092684030532837
train_iter_loss: 0.43503260612487793
train_iter_loss: 0.2411184012889862
train_iter_loss: 0.20323540270328522
train_iter_loss: 0.17178520560264587
train_iter_loss: 0.1042204350233078
train_iter_loss: 0.1136062815785408
train_iter_loss: 0.19356440007686615
train_iter_loss: 0.2131725549697876
train_iter_loss: 0.3568730354309082
train_iter_loss: 0.23514947295188904
train_iter_loss: 0.3214721083641052
train_iter_loss: 0.4107009172439575
train_iter_loss: 0.3042148947715759
train_iter_loss: 0.2545410096645355
train_iter_loss: 0.21464042365550995
train_iter_loss: 0.16790468990802765
train_iter_loss: 0.15561087429523468
train_iter_loss: 0.23436559736728668
train_iter_loss: 0.33251479268074036
train_iter_loss: 0.18152590095996857
train_iter_loss: 0.1570882350206375
train_iter_loss: 0.28763797879219055
train_iter_loss: 0.26666581630706787
train_iter_loss: 0.25411349534988403
train_iter_loss: 0.17504675686359406
train_iter_loss: 0.3516567349433899
train_iter_loss: 0.0717388242483139
train_iter_loss: 0.19845187664031982
train_iter_loss: 0.259472131729126
train_iter_loss: 0.42835742235183716
train_iter_loss: 0.25210466980934143
train_iter_loss: 0.2780340313911438
train_iter_loss: 0.3054962456226349
train_iter_loss: 0.3149850070476532
train_iter_loss: 0.42269518971443176
train loss :0.2719
---------------------
Validation seg loss: 0.3561391751281917 at epoch 541
epoch =    542/  1000, exp = train
train_iter_loss: 0.2831990718841553
train_iter_loss: 0.3254818916320801
train_iter_loss: 0.31443414092063904
train_iter_loss: 0.2757070064544678
train_iter_loss: 0.22493456304073334
train_iter_loss: 0.2663191854953766
train_iter_loss: 0.25004878640174866
train_iter_loss: 0.37417227029800415
train_iter_loss: 0.30324119329452515
train_iter_loss: 0.13603751361370087
train_iter_loss: 0.2534469962120056
train_iter_loss: 0.2704271674156189
train_iter_loss: 0.17655594646930695
train_iter_loss: 0.154195174574852
train_iter_loss: 0.23699654638767242
train_iter_loss: 0.40619608759880066
train_iter_loss: 0.20993182063102722
train_iter_loss: 0.2392922341823578
train_iter_loss: 0.2150890976190567
train_iter_loss: 0.16359680891036987
train_iter_loss: 0.34659379720687866
train_iter_loss: 0.14642968773841858
train_iter_loss: 0.25390273332595825
train_iter_loss: 0.3782010078430176
train_iter_loss: 0.0775422677397728
train_iter_loss: 0.23186419904232025
train_iter_loss: 0.2443467229604721
train_iter_loss: 0.2632157802581787
train_iter_loss: 0.3264111578464508
train_iter_loss: 0.4469485580921173
train_iter_loss: 0.16184675693511963
train_iter_loss: 0.40302106738090515
train_iter_loss: 0.24660779535770416
train_iter_loss: 0.3314424753189087
train_iter_loss: 0.3341335654258728
train_iter_loss: 0.24888908863067627
train_iter_loss: 0.25908440351486206
train_iter_loss: 0.3204531967639923
train_iter_loss: 0.23086240887641907
train_iter_loss: 0.19485007226467133
train_iter_loss: 0.23510679602622986
train_iter_loss: 0.19041211903095245
train_iter_loss: 0.2531960606575012
train_iter_loss: 0.27261602878570557
train_iter_loss: 0.3449136018753052
train_iter_loss: 0.22793500125408173
train_iter_loss: 0.4250170886516571
train_iter_loss: 0.2185063511133194
train_iter_loss: 0.2140553593635559
train_iter_loss: 0.1746898591518402
train_iter_loss: 0.14264856278896332
train_iter_loss: 0.2617575228214264
train_iter_loss: 0.2791287302970886
train_iter_loss: 0.24686439335346222
train_iter_loss: 0.1713610589504242
train_iter_loss: 0.22861099243164062
train_iter_loss: 0.2434452325105667
train_iter_loss: 0.3061036467552185
train_iter_loss: 0.26769888401031494
train_iter_loss: 0.45724406838417053
train_iter_loss: 0.2725425958633423
train_iter_loss: 0.5520682334899902
train_iter_loss: 0.45541834831237793
train_iter_loss: 0.43787866830825806
train_iter_loss: 0.18740861117839813
train_iter_loss: 0.1386110782623291
train_iter_loss: 0.419321745634079
train_iter_loss: 0.2520315945148468
train_iter_loss: 0.1935271918773651
train_iter_loss: 0.1855005919933319
train_iter_loss: 0.28816238045692444
train_iter_loss: 0.3641723096370697
train_iter_loss: 0.3798927366733551
train_iter_loss: 0.346527636051178
train_iter_loss: 0.3009209632873535
train_iter_loss: 0.2196333110332489
train_iter_loss: 0.10596662014722824
train_iter_loss: 0.17712032794952393
train_iter_loss: 0.2590574324131012
train_iter_loss: 0.4775574207305908
train_iter_loss: 0.2667360007762909
train_iter_loss: 0.114448182284832
train_iter_loss: 0.29709893465042114
train_iter_loss: 0.21705353260040283
train_iter_loss: 0.2603782117366791
train_iter_loss: 0.20785380899906158
train_iter_loss: 0.08210999518632889
train_iter_loss: 0.18238268792629242
train_iter_loss: 0.36632177233695984
train_iter_loss: 0.20965754985809326
train_iter_loss: 0.3146982789039612
train_iter_loss: 0.35825464129447937
train_iter_loss: 0.3517870604991913
train_iter_loss: 0.3513270616531372
train_iter_loss: 0.2581578493118286
train_iter_loss: 0.24259665608406067
train_iter_loss: 0.35490167140960693
train_iter_loss: 0.24953879415988922
train_iter_loss: 0.2844807207584381
train_iter_loss: 0.2762632369995117
train loss :0.2722
---------------------
Validation seg loss: 0.3639498824464544 at epoch 542
epoch =    543/  1000, exp = train
train_iter_loss: 0.094632089138031
train_iter_loss: 0.27953797578811646
train_iter_loss: 0.25318488478660583
train_iter_loss: 0.17130924761295319
train_iter_loss: 0.21409986913204193
train_iter_loss: 0.18564704060554504
train_iter_loss: 0.32358306646347046
train_iter_loss: 0.37486323714256287
train_iter_loss: 0.14182201027870178
train_iter_loss: 0.3855997323989868
train_iter_loss: 0.1768711358308792
train_iter_loss: 0.3534393608570099
train_iter_loss: 0.16766303777694702
train_iter_loss: 0.19104179739952087
train_iter_loss: 0.43666285276412964
train_iter_loss: 0.19725251197814941
train_iter_loss: 0.19107644259929657
train_iter_loss: 0.29416152834892273
train_iter_loss: 0.277687132358551
train_iter_loss: 0.34321725368499756
train_iter_loss: 0.30121535062789917
train_iter_loss: 0.2879936099052429
train_iter_loss: 0.3209662139415741
train_iter_loss: 0.26002904772758484
train_iter_loss: 0.34295743703842163
train_iter_loss: 0.13439758121967316
train_iter_loss: 0.46034619212150574
train_iter_loss: 0.26860857009887695
train_iter_loss: 0.20321956276893616
train_iter_loss: 0.2698087990283966
train_iter_loss: 0.18193939328193665
train_iter_loss: 0.3537319004535675
train_iter_loss: 0.24871891736984253
train_iter_loss: 0.29829248785972595
train_iter_loss: 0.3673931658267975
train_iter_loss: 0.2722774147987366
train_iter_loss: 0.3585340082645416
train_iter_loss: 0.3904319107532501
train_iter_loss: 0.3023509681224823
train_iter_loss: 0.26258477568626404
train_iter_loss: 0.25101351737976074
train_iter_loss: 0.19770140945911407
train_iter_loss: 0.22968758642673492
train_iter_loss: 0.2504197657108307
train_iter_loss: 0.3031276762485504
train_iter_loss: 0.2939918637275696
train_iter_loss: 0.16749896109104156
train_iter_loss: 0.16481168568134308
train_iter_loss: 0.38912591338157654
train_iter_loss: 0.21172039210796356
train_iter_loss: 0.42301538586616516
train_iter_loss: 0.3694499135017395
train_iter_loss: 0.1273714154958725
train_iter_loss: 0.15986166894435883
train_iter_loss: 0.18375004827976227
train_iter_loss: 0.2291228473186493
train_iter_loss: 0.19979815185070038
train_iter_loss: 0.23348304629325867
train_iter_loss: 0.46478480100631714
train_iter_loss: 0.3515971601009369
train_iter_loss: 0.19646768271923065
train_iter_loss: 0.3154216408729553
train_iter_loss: 0.2821991741657257
train_iter_loss: 0.17217113077640533
train_iter_loss: 0.28530174493789673
train_iter_loss: 0.3230541944503784
train_iter_loss: 0.2520931363105774
train_iter_loss: 0.27688780426979065
train_iter_loss: 0.1998208612203598
train_iter_loss: 0.12832745909690857
train_iter_loss: 0.2930058240890503
train_iter_loss: 0.31616687774658203
train_iter_loss: 0.2693796455860138
train_iter_loss: 0.21347594261169434
train_iter_loss: 0.3466093838214874
train_iter_loss: 0.2870870530605316
train_iter_loss: 0.08638936281204224
train_iter_loss: 0.39489758014678955
train_iter_loss: 0.35228246450424194
train_iter_loss: 0.28894296288490295
train_iter_loss: 0.3687577545642853
train_iter_loss: 0.09846220910549164
train_iter_loss: 0.1706603318452835
train_iter_loss: 0.3015443682670593
train_iter_loss: 0.328229695558548
train_iter_loss: 0.26110517978668213
train_iter_loss: 0.3844029903411865
train_iter_loss: 0.30960187315940857
train_iter_loss: 0.18102537095546722
train_iter_loss: 0.21800415217876434
train_iter_loss: 0.261659175157547
train_iter_loss: 0.3066037893295288
train_iter_loss: 0.1923646628856659
train_iter_loss: 0.4154379367828369
train_iter_loss: 0.3493501842021942
train_iter_loss: 0.2200794517993927
train_iter_loss: 0.17575199902057648
train_iter_loss: 0.17558301985263824
train_iter_loss: 0.3202742338180542
train_iter_loss: 0.4128572940826416
train loss :0.2714
---------------------
Validation seg loss: 0.37519069747858735 at epoch 543
epoch =    544/  1000, exp = train
train_iter_loss: 0.24364539980888367
train_iter_loss: 0.39867401123046875
train_iter_loss: 0.3659743666648865
train_iter_loss: 0.3662683665752411
train_iter_loss: 0.24601317942142487
train_iter_loss: 0.401155024766922
train_iter_loss: 0.3329603374004364
train_iter_loss: 0.38644877076148987
train_iter_loss: 0.3069763779640198
train_iter_loss: 0.33569133281707764
train_iter_loss: 0.422137588262558
train_iter_loss: 0.5662685632705688
train_iter_loss: 0.17522373795509338
train_iter_loss: 0.20821039378643036
train_iter_loss: 0.1448545753955841
train_iter_loss: 0.35145246982574463
train_iter_loss: 0.33500683307647705
train_iter_loss: 0.39271965622901917
train_iter_loss: 0.15574300289154053
train_iter_loss: 0.34940189123153687
train_iter_loss: 0.19689957797527313
train_iter_loss: 0.14256511628627777
train_iter_loss: 0.3413240909576416
train_iter_loss: 0.43009430170059204
train_iter_loss: 0.16883091628551483
train_iter_loss: 0.282421737909317
train_iter_loss: 0.293454647064209
train_iter_loss: 0.2841794490814209
train_iter_loss: 0.3011537194252014
train_iter_loss: 0.1609058529138565
train_iter_loss: 0.2726626992225647
train_iter_loss: 0.2084665298461914
train_iter_loss: 0.3803027868270874
train_iter_loss: 0.3030056357383728
train_iter_loss: 0.44080278277397156
train_iter_loss: 0.14207638800144196
train_iter_loss: 0.2569495439529419
train_iter_loss: 0.41267484426498413
train_iter_loss: 0.17578721046447754
train_iter_loss: 0.17370377480983734
train_iter_loss: 0.24608077108860016
train_iter_loss: 0.2684737741947174
train_iter_loss: 0.28701165318489075
train_iter_loss: 0.47840970754623413
train_iter_loss: 0.27818071842193604
train_iter_loss: 0.30691030621528625
train_iter_loss: 0.24922984838485718
train_iter_loss: 0.30939164757728577
train_iter_loss: 0.33289095759391785
train_iter_loss: 0.4155997931957245
train_iter_loss: 0.2620021104812622
train_iter_loss: 0.20131926238536835
train_iter_loss: 0.16407476365566254
train_iter_loss: 0.2556770145893097
train_iter_loss: 0.21940293908119202
train_iter_loss: 0.20944711565971375
train_iter_loss: 0.13877379894256592
train_iter_loss: 0.2409532070159912
train_iter_loss: 0.3427356481552124
train_iter_loss: 0.34189748764038086
train_iter_loss: 0.32238996028900146
train_iter_loss: 0.15858659148216248
train_iter_loss: 0.243817999958992
train_iter_loss: 0.19158108532428741
train_iter_loss: 0.24025660753250122
train_iter_loss: 0.293454647064209
train_iter_loss: 0.28992339968681335
train_iter_loss: 0.23988044261932373
train_iter_loss: 0.20474401116371155
train_iter_loss: 0.286175012588501
train_iter_loss: 0.1448737382888794
train_iter_loss: 0.21239817142486572
train_iter_loss: 0.38003480434417725
train_iter_loss: 0.16618850827217102
train_iter_loss: 0.30380764603614807
train_iter_loss: 0.23181748390197754
train_iter_loss: 0.23106488585472107
train_iter_loss: 0.32544344663619995
train_iter_loss: 0.24831826984882355
train_iter_loss: 0.16216948628425598
train_iter_loss: 0.21118992567062378
train_iter_loss: 0.12048812210559845
train_iter_loss: 0.28702256083488464
train_iter_loss: 0.17440569400787354
train_iter_loss: 0.26593685150146484
train_iter_loss: 0.09161075949668884
train_iter_loss: 0.2664546072483063
train_iter_loss: 0.3887368440628052
train_iter_loss: 0.3281106948852539
train_iter_loss: 0.4012061357498169
train_iter_loss: 0.28690850734710693
train_iter_loss: 0.14386001229286194
train_iter_loss: 0.374581903219223
train_iter_loss: 0.20641063153743744
train_iter_loss: 0.1903582066297531
train_iter_loss: 0.2812212109565735
train_iter_loss: 0.16913823783397675
train_iter_loss: 0.32299330830574036
train_iter_loss: 0.24630598723888397
train_iter_loss: 0.10181082785129547
train loss :0.2741
---------------------
Validation seg loss: 0.362173841844471 at epoch 544
epoch =    545/  1000, exp = train
train_iter_loss: 0.19330132007598877
train_iter_loss: 0.026154659688472748
train_iter_loss: 0.43337327241897583
train_iter_loss: 0.1546560674905777
train_iter_loss: 0.2624484598636627
train_iter_loss: 0.3449093997478485
train_iter_loss: 0.3205162286758423
train_iter_loss: 0.3718331456184387
train_iter_loss: 0.15379241108894348
train_iter_loss: 0.21494174003601074
train_iter_loss: 0.323447048664093
train_iter_loss: 0.2821934223175049
train_iter_loss: 0.2177073359489441
train_iter_loss: 0.32628580927848816
train_iter_loss: 0.2502076029777527
train_iter_loss: 0.12620092928409576
train_iter_loss: 0.20786665380001068
train_iter_loss: 0.268769234418869
train_iter_loss: 0.27368006110191345
train_iter_loss: 0.38775575160980225
train_iter_loss: 0.20637521147727966
train_iter_loss: 0.3602887690067291
train_iter_loss: 0.23931778967380524
train_iter_loss: 0.1371316760778427
train_iter_loss: 0.3868095278739929
train_iter_loss: 0.3619328737258911
train_iter_loss: 0.12891696393489838
train_iter_loss: 0.272577166557312
train_iter_loss: 0.18994644284248352
train_iter_loss: 0.25388863682746887
train_iter_loss: 0.2022705376148224
train_iter_loss: 0.11557438969612122
train_iter_loss: 0.13156655430793762
train_iter_loss: 0.2290075421333313
train_iter_loss: 0.280464768409729
train_iter_loss: 0.3985808491706848
train_iter_loss: 0.12087123841047287
train_iter_loss: 0.3162650465965271
train_iter_loss: 0.33160004019737244
train_iter_loss: 0.35912856459617615
train_iter_loss: 0.3081534802913666
train_iter_loss: 0.3523302674293518
train_iter_loss: 0.19943344593048096
train_iter_loss: 0.36338484287261963
train_iter_loss: 0.3513067960739136
train_iter_loss: 0.24458587169647217
train_iter_loss: 0.2312411218881607
train_iter_loss: 0.25204581022262573
train_iter_loss: 0.13835230469703674
train_iter_loss: 0.17758159339427948
train_iter_loss: 0.19766171276569366
train_iter_loss: 0.22998589277267456
train_iter_loss: 0.2435707151889801
train_iter_loss: 0.45965367555618286
train_iter_loss: 0.2058706283569336
train_iter_loss: 0.1625795066356659
train_iter_loss: 0.42369702458381653
train_iter_loss: 0.3145303726196289
train_iter_loss: 0.2968563139438629
train_iter_loss: 0.2362567037343979
train_iter_loss: 0.4688146114349365
train_iter_loss: 0.27872493863105774
train_iter_loss: 0.23319606482982635
train_iter_loss: 0.1508701890707016
train_iter_loss: 0.37312018871307373
train_iter_loss: 0.29337504506111145
train_iter_loss: 0.16870376467704773
train_iter_loss: 0.3317483067512512
train_iter_loss: 0.47557857632637024
train_iter_loss: 0.1893993467092514
train_iter_loss: 0.2587752342224121
train_iter_loss: 0.14343629777431488
train_iter_loss: 0.2417537122964859
train_iter_loss: 0.39100661873817444
train_iter_loss: 0.15346506237983704
train_iter_loss: 0.21476426720619202
train_iter_loss: 0.2182118147611618
train_iter_loss: 0.35616549849510193
train_iter_loss: 0.2891082167625427
train_iter_loss: 0.21562348306179047
train_iter_loss: 0.26457005739212036
train_iter_loss: 0.3453798294067383
train_iter_loss: 0.3670041859149933
train_iter_loss: 0.2756105661392212
train_iter_loss: 0.20630232989788055
train_iter_loss: 0.09767649322748184
train_iter_loss: 0.4312257468700409
train_iter_loss: 0.18769027292728424
train_iter_loss: 0.11669477820396423
train_iter_loss: 0.33309945464134216
train_iter_loss: 0.31400543451309204
train_iter_loss: 0.33716049790382385
train_iter_loss: 0.29692667722702026
train_iter_loss: 0.24650566279888153
train_iter_loss: 0.29713940620422363
train_iter_loss: 0.24673791229724884
train_iter_loss: 0.19716617465019226
train_iter_loss: 0.3477329611778259
train_iter_loss: 0.29479122161865234
train_iter_loss: 0.18977418541908264
train loss :0.2666
---------------------
Validation seg loss: 0.368232483791082 at epoch 545
epoch =    546/  1000, exp = train
train_iter_loss: 0.3907893896102905
train_iter_loss: 0.29398706555366516
train_iter_loss: 0.17707061767578125
train_iter_loss: 0.27485060691833496
train_iter_loss: 0.12048448622226715
train_iter_loss: 0.26413822174072266
train_iter_loss: 0.33614060282707214
train_iter_loss: 0.4486773610115051
train_iter_loss: 0.17982488870620728
train_iter_loss: 0.12926805019378662
train_iter_loss: 0.24184557795524597
train_iter_loss: 0.2406267523765564
train_iter_loss: 0.35282790660858154
train_iter_loss: 0.22965064644813538
train_iter_loss: 0.18342062830924988
train_iter_loss: 0.3253094255924225
train_iter_loss: 0.17142829298973083
train_iter_loss: 0.223829448223114
train_iter_loss: 0.21680130064487457
train_iter_loss: 0.2105807214975357
train_iter_loss: 0.07809889316558838
train_iter_loss: 0.5046204924583435
train_iter_loss: 0.2833996117115021
train_iter_loss: 0.29712679982185364
train_iter_loss: 0.2034624218940735
train_iter_loss: 0.3758341073989868
train_iter_loss: 0.3187096118927002
train_iter_loss: 0.10289653390645981
train_iter_loss: 0.2998855710029602
train_iter_loss: 0.23983795940876007
train_iter_loss: 0.23775270581245422
train_iter_loss: 0.2275668829679489
train_iter_loss: 0.1416448950767517
train_iter_loss: 0.3402734696865082
train_iter_loss: 0.2571067214012146
train_iter_loss: 0.29832953214645386
train_iter_loss: 0.20513884723186493
train_iter_loss: 0.397599458694458
train_iter_loss: 0.3218371570110321
train_iter_loss: 0.12584401667118073
train_iter_loss: 0.24907921254634857
train_iter_loss: 0.2198089361190796
train_iter_loss: 0.17628611624240875
train_iter_loss: 0.23524285852909088
train_iter_loss: 0.19438552856445312
train_iter_loss: 0.2736964225769043
train_iter_loss: 0.39623314142227173
train_iter_loss: 0.5136324167251587
train_iter_loss: 0.20329885184764862
train_iter_loss: 0.3882533311843872
train_iter_loss: 0.34654709696769714
train_iter_loss: 0.35189583897590637
train_iter_loss: 0.1632019579410553
train_iter_loss: 0.2721325159072876
train_iter_loss: 0.3372657895088196
train_iter_loss: 0.23416082561016083
train_iter_loss: 0.25624653697013855
train_iter_loss: 0.2566361129283905
train_iter_loss: 0.4736484885215759
train_iter_loss: 0.20157910883426666
train_iter_loss: 0.1786080151796341
train_iter_loss: 0.19847261905670166
train_iter_loss: 0.20443442463874817
train_iter_loss: 0.3194812834262848
train_iter_loss: 0.2757287621498108
train_iter_loss: 0.21180389821529388
train_iter_loss: 0.29345816373825073
train_iter_loss: 0.16522549092769623
train_iter_loss: 0.3631172478199005
train_iter_loss: 0.24748028814792633
train_iter_loss: 0.13756349682807922
train_iter_loss: 0.35701125860214233
train_iter_loss: 0.3242108225822449
train_iter_loss: 0.3416474163532257
train_iter_loss: 0.28120577335357666
train_iter_loss: 0.2650890052318573
train_iter_loss: 0.11656538397073746
train_iter_loss: 0.3930211067199707
train_iter_loss: 0.1149749830365181
train_iter_loss: 0.17100636661052704
train_iter_loss: 0.2334050089120865
train_iter_loss: 0.22350138425827026
train_iter_loss: 0.25462403893470764
train_iter_loss: 0.3346405029296875
train_iter_loss: 0.30693891644477844
train_iter_loss: 0.18103650212287903
train_iter_loss: 0.3215791583061218
train_iter_loss: 0.2409893423318863
train_iter_loss: 0.3533909320831299
train_iter_loss: 0.27379441261291504
train_iter_loss: 0.2627025246620178
train_iter_loss: 0.3400530219078064
train_iter_loss: 0.11485383659601212
train_iter_loss: 0.26928743720054626
train_iter_loss: 0.35976237058639526
train_iter_loss: 0.2977156937122345
train_iter_loss: 0.24661017954349518
train_iter_loss: 0.2027372419834137
train_iter_loss: 0.21898256242275238
train_iter_loss: 0.2551175653934479
train loss :0.2653
---------------------
Validation seg loss: 0.3918382109951158 at epoch 546
epoch =    547/  1000, exp = train
train_iter_loss: 0.07080553472042084
train_iter_loss: 0.2647501528263092
train_iter_loss: 0.22566479444503784
train_iter_loss: 0.28322142362594604
train_iter_loss: 0.37653082609176636
train_iter_loss: 0.3495139181613922
train_iter_loss: 0.2043629139661789
train_iter_loss: 0.18192045390605927
train_iter_loss: 0.18266865611076355
train_iter_loss: 0.27891090512275696
train_iter_loss: 0.20631928741931915
train_iter_loss: 0.2622450590133667
train_iter_loss: 0.325621634721756
train_iter_loss: 0.3126436769962311
train_iter_loss: 0.2866774797439575
train_iter_loss: 0.2271503508090973
train_iter_loss: 0.43198922276496887
train_iter_loss: 0.3015212118625641
train_iter_loss: 0.3000667691230774
train_iter_loss: 0.4225626587867737
train_iter_loss: 0.2809813618659973
train_iter_loss: 0.17498144507408142
train_iter_loss: 0.19460022449493408
train_iter_loss: 0.27165940403938293
train_iter_loss: 0.09647900611162186
train_iter_loss: 0.20234467089176178
train_iter_loss: 0.23203736543655396
train_iter_loss: 0.373893141746521
train_iter_loss: 0.37652066349983215
train_iter_loss: 0.15275117754936218
train_iter_loss: 0.35352370142936707
train_iter_loss: 0.2196294516324997
train_iter_loss: 0.3232523202896118
train_iter_loss: 0.3182688057422638
train_iter_loss: 0.37493276596069336
train_iter_loss: 0.2970411777496338
train_iter_loss: 0.3849276602268219
train_iter_loss: 0.2751182019710541
train_iter_loss: 0.3436165153980255
train_iter_loss: 0.27818989753723145
train_iter_loss: 0.21588559448719025
train_iter_loss: 0.25719019770622253
train_iter_loss: 0.3039338290691376
train_iter_loss: 0.41776978969573975
train_iter_loss: 0.350205659866333
train_iter_loss: 0.314003586769104
train_iter_loss: 0.3329209089279175
train_iter_loss: 0.22826695442199707
train_iter_loss: 0.2504901885986328
train_iter_loss: 0.20192976295948029
train_iter_loss: 0.27113020420074463
train_iter_loss: 0.28385111689567566
train_iter_loss: 0.3784681558609009
train_iter_loss: 0.3359617590904236
train_iter_loss: 0.33711501955986023
train_iter_loss: 0.3316504955291748
train_iter_loss: 0.29948997497558594
train_iter_loss: 0.38690754771232605
train_iter_loss: 0.2665603756904602
train_iter_loss: 0.28336217999458313
train_iter_loss: 0.17745709419250488
train_iter_loss: 0.33115342259407043
train_iter_loss: 0.2076815515756607
train_iter_loss: 0.35435691475868225
train_iter_loss: 0.34986597299575806
train_iter_loss: 0.4015763998031616
train_iter_loss: 0.2643485963344574
train_iter_loss: 0.2902349531650543
train_iter_loss: 0.29525211453437805
train_iter_loss: 0.1523299366235733
train_iter_loss: 0.3208662271499634
train_iter_loss: 0.38607603311538696
train_iter_loss: 0.17525845766067505
train_iter_loss: 0.39214226603507996
train_iter_loss: 0.1947537213563919
train_iter_loss: 0.25131237506866455
train_iter_loss: 0.18927691876888275
train_iter_loss: 0.23739230632781982
train_iter_loss: 0.2944330871105194
train_iter_loss: 0.33504003286361694
train_iter_loss: 0.19747480750083923
train_iter_loss: 0.28704917430877686
train_iter_loss: 0.2294853776693344
train_iter_loss: 0.3296346366405487
train_iter_loss: 0.2691441774368286
train_iter_loss: 0.2627876400947571
train_iter_loss: 0.2626034915447235
train_iter_loss: 0.27943480014801025
train_iter_loss: 0.27551794052124023
train_iter_loss: 0.17406737804412842
train_iter_loss: 0.1207096055150032
train_iter_loss: 0.14888858795166016
train_iter_loss: 0.14405052363872528
train_iter_loss: 0.22372202575206757
train_iter_loss: 0.12450353801250458
train_iter_loss: 0.19520825147628784
train_iter_loss: 0.16363242268562317
train_iter_loss: 0.23234227299690247
train_iter_loss: 0.07163029909133911
train_iter_loss: 0.2916531264781952
train loss :0.2722
---------------------
Validation seg loss: 0.36084996112007295 at epoch 547
epoch =    548/  1000, exp = train
train_iter_loss: 0.15592095255851746
train_iter_loss: 0.22852586209774017
train_iter_loss: 0.2738059461116791
train_iter_loss: 0.11661287397146225
train_iter_loss: 0.25338080525398254
train_iter_loss: 0.3979797065258026
train_iter_loss: 0.44968387484550476
train_iter_loss: 0.40004411339759827
train_iter_loss: 0.251996248960495
train_iter_loss: 0.287659615278244
train_iter_loss: 0.23549121618270874
train_iter_loss: 0.1099695935845375
train_iter_loss: 0.2324109673500061
train_iter_loss: 0.3010016977787018
train_iter_loss: 0.18714483082294464
train_iter_loss: 0.34735986590385437
train_iter_loss: 0.11636654287576675
train_iter_loss: 0.28408244252204895
train_iter_loss: 0.24496294558048248
train_iter_loss: 0.30123311281204224
train_iter_loss: 0.24957965314388275
train_iter_loss: 0.3134680390357971
train_iter_loss: 0.326002299785614
train_iter_loss: 0.23765501379966736
train_iter_loss: 0.14737579226493835
train_iter_loss: 0.22433677315711975
train_iter_loss: 0.32560306787490845
train_iter_loss: 0.14858675003051758
train_iter_loss: 0.3004240095615387
train_iter_loss: 0.17722317576408386
train_iter_loss: 0.10575299710035324
train_iter_loss: 0.3620896339416504
train_iter_loss: 0.18224304914474487
train_iter_loss: 0.37157702445983887
train_iter_loss: 0.23244893550872803
train_iter_loss: 0.3296138346195221
train_iter_loss: 0.3380699157714844
train_iter_loss: 0.24817855656147003
train_iter_loss: 0.4783313274383545
train_iter_loss: 0.14390240609645844
train_iter_loss: 0.18158701062202454
train_iter_loss: 0.263953298330307
train_iter_loss: 0.14177624881267548
train_iter_loss: 0.1902620494365692
train_iter_loss: 0.16335391998291016
train_iter_loss: 0.32578715682029724
train_iter_loss: 0.3594604730606079
train_iter_loss: 0.2898547351360321
train_iter_loss: 0.20837055146694183
train_iter_loss: 0.2842356264591217
train_iter_loss: 0.23514586687088013
train_iter_loss: 0.3714378774166107
train_iter_loss: 0.27429330348968506
train_iter_loss: 0.2376404106616974
train_iter_loss: 0.30574166774749756
train_iter_loss: 0.26258423924446106
train_iter_loss: 0.23923158645629883
train_iter_loss: 0.22053423523902893
train_iter_loss: 0.32520821690559387
train_iter_loss: 0.31160610914230347
train_iter_loss: 0.32823804020881653
train_iter_loss: 0.26153886318206787
train_iter_loss: 0.29462742805480957
train_iter_loss: 0.3314175605773926
train_iter_loss: 0.26886776089668274
train_iter_loss: 0.15428584814071655
train_iter_loss: 0.20679578185081482
train_iter_loss: 0.30436375737190247
train_iter_loss: 0.18602095544338226
train_iter_loss: 0.09714401513338089
train_iter_loss: 0.3278125524520874
train_iter_loss: 0.28659048676490784
train_iter_loss: 0.1861865073442459
train_iter_loss: 0.3638418912887573
train_iter_loss: 0.25429844856262207
train_iter_loss: 0.1461891382932663
train_iter_loss: 0.3621179461479187
train_iter_loss: 0.11102105677127838
train_iter_loss: 0.24941575527191162
train_iter_loss: 0.36502060294151306
train_iter_loss: 0.3882363438606262
train_iter_loss: 0.28767120838165283
train_iter_loss: 0.21893207728862762
train_iter_loss: 0.34310516715049744
train_iter_loss: 0.2479015290737152
train_iter_loss: 0.30911925435066223
train_iter_loss: 0.28844061493873596
train_iter_loss: 0.4418477714061737
train_iter_loss: 0.40543922781944275
train_iter_loss: 0.1636238545179367
train_iter_loss: 0.344792902469635
train_iter_loss: 0.1296585649251938
train_iter_loss: 0.38235390186309814
train_iter_loss: 0.3225369453430176
train_iter_loss: 0.23061886429786682
train_iter_loss: 0.25198012590408325
train_iter_loss: 0.30698442459106445
train_iter_loss: 0.26684075593948364
train_iter_loss: 0.2462998926639557
train_iter_loss: 0.286563515663147
train loss :0.2683
---------------------
Validation seg loss: 0.377461714239545 at epoch 548
epoch =    549/  1000, exp = train
train_iter_loss: 0.21375232934951782
train_iter_loss: 0.37960731983184814
train_iter_loss: 0.1984502226114273
train_iter_loss: 0.4646432101726532
train_iter_loss: 0.2393353134393692
train_iter_loss: 0.2392738312482834
train_iter_loss: 0.28701043128967285
train_iter_loss: 0.20800885558128357
train_iter_loss: 0.15539605915546417
train_iter_loss: 0.2431674301624298
train_iter_loss: 0.3018820285797119
train_iter_loss: 0.3427838683128357
train_iter_loss: 0.2596663534641266
train_iter_loss: 0.23770387470722198
train_iter_loss: 0.20228126645088196
train_iter_loss: 0.182053342461586
train_iter_loss: 0.45651671290397644
train_iter_loss: 0.4093901813030243
train_iter_loss: 0.21113401651382446
train_iter_loss: 0.26320818066596985
train_iter_loss: 0.21501684188842773
train_iter_loss: 0.22625873982906342
train_iter_loss: 0.12915991246700287
train_iter_loss: 0.3097634017467499
train_iter_loss: 0.21694409847259521
train_iter_loss: 0.3085011839866638
train_iter_loss: 0.24896240234375
train_iter_loss: 0.2828958034515381
train_iter_loss: 0.2852053940296173
train_iter_loss: 0.32553598284721375
train_iter_loss: 0.20627819001674652
train_iter_loss: 0.3656752407550812
train_iter_loss: 0.15825802087783813
train_iter_loss: 0.20892389118671417
train_iter_loss: 0.3212277591228485
train_iter_loss: 0.24159133434295654
train_iter_loss: 0.3050256371498108
train_iter_loss: 0.26570066809654236
train_iter_loss: 0.31326988339424133
train_iter_loss: 0.239751398563385
train_iter_loss: 0.28374361991882324
train_iter_loss: 0.4236195385456085
train_iter_loss: 0.2581840753555298
train_iter_loss: 0.2896512448787689
train_iter_loss: 0.36913999915122986
train_iter_loss: 0.20420490205287933
train_iter_loss: 0.2685897648334503
train_iter_loss: 0.28131911158561707
train_iter_loss: 0.2581903636455536
train_iter_loss: 0.2845083773136139
train_iter_loss: 0.1811113953590393
train_iter_loss: 0.27209028601646423
train_iter_loss: 0.3744283616542816
train_iter_loss: 0.18946705758571625
train_iter_loss: 0.46358802914619446
train_iter_loss: 0.30558669567108154
train_iter_loss: 0.33698853850364685
train_iter_loss: 0.2665088176727295
train_iter_loss: 0.368392676115036
train_iter_loss: 0.2508251667022705
train_iter_loss: 0.31957677006721497
train_iter_loss: 0.17309533059597015
train_iter_loss: 0.31313055753707886
train_iter_loss: 0.33862584829330444
train_iter_loss: 0.1672094315290451
train_iter_loss: 0.1950162798166275
train_iter_loss: 0.2546698749065399
train_iter_loss: 0.4279382824897766
train_iter_loss: 0.2871161699295044
train_iter_loss: 0.4017263650894165
train_iter_loss: 0.22290995717048645
train_iter_loss: 0.31268417835235596
train_iter_loss: 0.2171628475189209
train_iter_loss: 0.183554008603096
train_iter_loss: 0.15846514701843262
train_iter_loss: 0.4375632405281067
train_iter_loss: 0.15033727884292603
train_iter_loss: 0.17998917400836945
train_iter_loss: 0.17786969244480133
train_iter_loss: 0.3210572600364685
train_iter_loss: 0.08374050259590149
train_iter_loss: 0.24236997961997986
train_iter_loss: 0.22084741294384003
train_iter_loss: 0.28285500407218933
train_iter_loss: 0.18060237169265747
train_iter_loss: 0.46741804480552673
train_iter_loss: 0.20335908234119415
train_iter_loss: 0.22062020003795624
train_iter_loss: 0.040077585726976395
train_iter_loss: 0.42516884207725525
train_iter_loss: 0.325033575296402
train_iter_loss: 0.16671259701251984
train_iter_loss: 0.502507746219635
train_iter_loss: 0.09086555987596512
train_iter_loss: 0.23997542262077332
train_iter_loss: 0.27083900570869446
train_iter_loss: 0.34323614835739136
train_iter_loss: 0.2921483516693115
train_iter_loss: 0.2785560190677643
train_iter_loss: 0.20921708643436432
train loss :0.2719
---------------------
Validation seg loss: 0.35595328645464386 at epoch 549
epoch =    550/  1000, exp = train
train_iter_loss: 0.2633407711982727
train_iter_loss: 0.342287540435791
train_iter_loss: 0.2622688114643097
train_iter_loss: 0.225584015250206
train_iter_loss: 0.3353918492794037
train_iter_loss: 0.15799415111541748
train_iter_loss: 0.22694286704063416
train_iter_loss: 0.2003123164176941
train_iter_loss: 0.2646282911300659
train_iter_loss: 0.21048514544963837
train_iter_loss: 0.2854697108268738
train_iter_loss: 0.23677994310855865
train_iter_loss: 0.21753399074077606
train_iter_loss: 0.29123982787132263
train_iter_loss: 0.20809441804885864
train_iter_loss: 0.25566455721855164
train_iter_loss: 0.16219493746757507
train_iter_loss: 0.3415793180465698
train_iter_loss: 0.2502976357936859
train_iter_loss: 0.360729455947876
train_iter_loss: 0.24959459900856018
train_iter_loss: 0.21961647272109985
train_iter_loss: 0.3419928252696991
train_iter_loss: 0.358968049287796
train_iter_loss: 0.3149580955505371
train_iter_loss: 0.338360071182251
train_iter_loss: 0.17415767908096313
train_iter_loss: 0.433893084526062
train_iter_loss: 0.20998600125312805
train_iter_loss: 0.2375856190919876
train_iter_loss: 0.30847296118736267
train_iter_loss: 0.24997907876968384
train_iter_loss: 0.24445602297782898
train_iter_loss: 0.39039117097854614
train_iter_loss: 0.3358917832374573
train_iter_loss: 0.3007275462150574
train_iter_loss: 0.10941837728023529
train_iter_loss: 0.38830897212028503
train_iter_loss: 0.31263628602027893
train_iter_loss: 0.27227485179901123
train_iter_loss: 0.3412120044231415
train_iter_loss: 0.18126648664474487
train_iter_loss: 0.2860095202922821
train_iter_loss: 0.2900344729423523
train_iter_loss: 0.18921270966529846
train_iter_loss: 0.1961841881275177
train_iter_loss: 0.29123952984809875
train_iter_loss: 0.40587109327316284
train_iter_loss: 0.061297498643398285
train_iter_loss: 0.2988022267818451
train_iter_loss: 0.31227707862854004
train_iter_loss: 0.3341268301010132
train_iter_loss: 0.2748936414718628
train_iter_loss: 0.2663905918598175
train_iter_loss: 0.26786506175994873
train_iter_loss: 0.31533029675483704
train_iter_loss: 0.3979174494743347
train_iter_loss: 0.22079285979270935
train_iter_loss: 0.2507820129394531
train_iter_loss: 0.16355487704277039
train_iter_loss: 0.4501115381717682
train_iter_loss: 0.2956415116786957
train_iter_loss: 0.14009618759155273
train_iter_loss: 0.3166852295398712
train_iter_loss: 0.2288672924041748
train_iter_loss: 0.21927322447299957
train_iter_loss: 0.23558297753334045
train_iter_loss: 0.17914891242980957
train_iter_loss: 0.2236589938402176
train_iter_loss: 0.17642967402935028
train_iter_loss: 0.34170883893966675
train_iter_loss: 0.20687690377235413
train_iter_loss: 0.3169683814048767
train_iter_loss: 0.37178561091423035
train_iter_loss: 0.20376114547252655
train_iter_loss: 0.17427103221416473
train_iter_loss: 0.2928130030632019
train_iter_loss: 0.37526750564575195
train_iter_loss: 0.18995356559753418
train_iter_loss: 0.27107617259025574
train_iter_loss: 0.1518924981355667
train_iter_loss: 0.31522682309150696
train_iter_loss: 0.19651716947555542
train_iter_loss: 0.30396217107772827
train_iter_loss: 0.27750054001808167
train_iter_loss: 0.28857100009918213
train_iter_loss: 0.14958219230175018
train_iter_loss: 0.28217655420303345
train_iter_loss: 0.2965537905693054
train_iter_loss: 0.1742371916770935
train_iter_loss: 0.359732449054718
train_iter_loss: 0.31065866351127625
train_iter_loss: 0.04927005246281624
train_iter_loss: 0.18950311839580536
train_iter_loss: 0.34075963497161865
train_iter_loss: 0.3198407292366028
train_iter_loss: 0.21649472415447235
train_iter_loss: 0.3266567587852478
train_iter_loss: 0.10852225869894028
train_iter_loss: 0.3620930314064026
train loss :0.2674
---------------------
Validation seg loss: 0.36923580144022433 at epoch 550
epoch =    551/  1000, exp = train
train_iter_loss: 0.22951894998550415
train_iter_loss: 0.2511990964412689
train_iter_loss: 0.10598097741603851
train_iter_loss: 0.28123313188552856
train_iter_loss: 0.4292640686035156
train_iter_loss: 0.4289332330226898
train_iter_loss: 0.511962354183197
train_iter_loss: 0.22195467352867126
train_iter_loss: 0.27556759119033813
train_iter_loss: 0.2836639881134033
train_iter_loss: 0.2711316645145416
train_iter_loss: 0.2847106158733368
train_iter_loss: 0.4171140491962433
train_iter_loss: 0.1628948152065277
train_iter_loss: 0.4301048219203949
train_iter_loss: 0.2231702357530594
train_iter_loss: 0.31975480914115906
train_iter_loss: 0.14150916039943695
train_iter_loss: 0.3960916996002197
train_iter_loss: 0.23904694616794586
train_iter_loss: 0.2637769877910614
train_iter_loss: 0.4479166567325592
train_iter_loss: 0.21511457860469818
train_iter_loss: 0.196502223610878
train_iter_loss: 0.20871150493621826
train_iter_loss: 0.2245759814977646
train_iter_loss: 0.3421897888183594
train_iter_loss: 0.17494292557239532
train_iter_loss: 0.35663673281669617
train_iter_loss: 0.35215404629707336
train_iter_loss: 0.20218563079833984
train_iter_loss: 0.19965319335460663
train_iter_loss: 0.14344434440135956
train_iter_loss: 0.292887419462204
train_iter_loss: 0.18673312664031982
train_iter_loss: 0.20891933143138885
train_iter_loss: 0.2135280966758728
train_iter_loss: 0.1691967248916626
train_iter_loss: 0.16483516991138458
train_iter_loss: 0.31994181871414185
train_iter_loss: 0.16421666741371155
train_iter_loss: 0.2390621304512024
train_iter_loss: 0.15140601992607117
train_iter_loss: 0.2909378409385681
train_iter_loss: 0.20636984705924988
train_iter_loss: 0.38506558537483215
train_iter_loss: 0.2782478332519531
train_iter_loss: 0.20672310888767242
train_iter_loss: 0.25918951630592346
train_iter_loss: 0.22140657901763916
train_iter_loss: 0.2451811134815216
train_iter_loss: 0.3335336744785309
train_iter_loss: 0.1758510023355484
train_iter_loss: 0.3634255826473236
train_iter_loss: 0.1369570791721344
train_iter_loss: 0.3017578423023224
train_iter_loss: 0.3081318140029907
train_iter_loss: 0.29227131605148315
train_iter_loss: 0.36145836114883423
train_iter_loss: 0.22837932407855988
train_iter_loss: 0.35438111424446106
train_iter_loss: 0.37880927324295044
train_iter_loss: 0.22888684272766113
train_iter_loss: 0.4075847864151001
train_iter_loss: 0.21447816491127014
train_iter_loss: 0.24694442749023438
train_iter_loss: 0.29690277576446533
train_iter_loss: 0.10315696895122528
train_iter_loss: 0.23185987770557404
train_iter_loss: 0.29652872681617737
train_iter_loss: 0.11965058743953705
train_iter_loss: 0.3319346606731415
train_iter_loss: 0.40151864290237427
train_iter_loss: 0.2691322863101959
train_iter_loss: 0.25819069147109985
train_iter_loss: 0.5130649209022522
train_iter_loss: 0.2719350755214691
train_iter_loss: 0.26897141337394714
train_iter_loss: 0.3285609483718872
train_iter_loss: 0.22353146970272064
train_iter_loss: 0.15914469957351685
train_iter_loss: 0.4084983170032501
train_iter_loss: 0.36856749653816223
train_iter_loss: 0.3451065719127655
train_iter_loss: 0.1723427027463913
train_iter_loss: 0.23119625449180603
train_iter_loss: 0.14382344484329224
train_iter_loss: 0.4669802188873291
train_iter_loss: 0.1832858920097351
train_iter_loss: 0.20178021490573883
train_iter_loss: 0.1591375172138214
train_iter_loss: 0.3475419878959656
train_iter_loss: 0.40519821643829346
train_iter_loss: 0.15338443219661713
train_iter_loss: 0.20517344772815704
train_iter_loss: 0.3411462604999542
train_iter_loss: 0.24250848591327667
train_iter_loss: 0.3277272880077362
train_iter_loss: 0.3086254894733429
train_iter_loss: 0.2575530409812927
train loss :0.2738
---------------------
Validation seg loss: 0.3707573483083046 at epoch 551
epoch =    552/  1000, exp = train
train_iter_loss: 0.38151830434799194
train_iter_loss: 0.18315598368644714
train_iter_loss: 0.09544634073972702
train_iter_loss: 0.2662537097930908
train_iter_loss: 0.30519694089889526
train_iter_loss: 0.25880923867225647
train_iter_loss: 0.22039589285850525
train_iter_loss: 0.19889503717422485
train_iter_loss: 0.2429279386997223
train_iter_loss: 0.23686663806438446
train_iter_loss: 0.20870886743068695
train_iter_loss: 0.2325754314661026
train_iter_loss: 0.4062834084033966
train_iter_loss: 0.3708783984184265
train_iter_loss: 0.24687020480632782
train_iter_loss: 0.2617402970790863
train_iter_loss: 0.3054829239845276
train_iter_loss: 0.20554214715957642
train_iter_loss: 0.3708937168121338
train_iter_loss: 0.2201293557882309
train_iter_loss: 0.3414585590362549
train_iter_loss: 0.17071720957756042
train_iter_loss: 0.18299032747745514
train_iter_loss: 0.2903139889240265
train_iter_loss: 0.14547422528266907
train_iter_loss: 0.22270327806472778
train_iter_loss: 0.210105299949646
train_iter_loss: 0.1790744662284851
train_iter_loss: 0.23257572948932648
train_iter_loss: 0.33911994099617004
train_iter_loss: 0.31637004017829895
train_iter_loss: 0.23693573474884033
train_iter_loss: 0.33738282322883606
train_iter_loss: 0.37816521525382996
train_iter_loss: 0.2744753360748291
train_iter_loss: 0.3727104663848877
train_iter_loss: 0.39655429124832153
train_iter_loss: 0.2631755769252777
train_iter_loss: 0.16787077486515045
train_iter_loss: 0.2871014475822449
train_iter_loss: 0.2603045403957367
train_iter_loss: 0.34974199533462524
train_iter_loss: 0.34910205006599426
train_iter_loss: 0.32765015959739685
train_iter_loss: 0.2193627506494522
train_iter_loss: 0.20272819697856903
train_iter_loss: 0.16422194242477417
train_iter_loss: 0.2221621870994568
train_iter_loss: 0.319627583026886
train_iter_loss: 0.2774410843849182
train_iter_loss: 0.16361741721630096
train_iter_loss: 0.21169599890708923
train_iter_loss: 0.4171225428581238
train_iter_loss: 0.34829050302505493
train_iter_loss: 0.2907710373401642
train_iter_loss: 0.22146934270858765
train_iter_loss: 0.25414150953292847
train_iter_loss: 0.09508809447288513
train_iter_loss: 0.2652062177658081
train_iter_loss: 0.2881412208080292
train_iter_loss: 0.13387717306613922
train_iter_loss: 0.2213238775730133
train_iter_loss: 0.3048619329929352
train_iter_loss: 0.1711646318435669
train_iter_loss: 0.0832684114575386
train_iter_loss: 0.27565646171569824
train_iter_loss: 0.18780417740345
train_iter_loss: 0.36264485120773315
train_iter_loss: 0.2690955400466919
train_iter_loss: 0.31140241026878357
train_iter_loss: 0.3674979507923126
train_iter_loss: 0.3161267638206482
train_iter_loss: 0.2535027265548706
train_iter_loss: 0.2614225745201111
train_iter_loss: 0.22599443793296814
train_iter_loss: 0.21964532136917114
train_iter_loss: 0.25665372610092163
train_iter_loss: 0.2250100076198578
train_iter_loss: 0.33009687066078186
train_iter_loss: 0.2426280528306961
train_iter_loss: 0.28848758339881897
train_iter_loss: 0.2542133629322052
train_iter_loss: 0.1054658517241478
train_iter_loss: 0.23113122582435608
train_iter_loss: 0.35124674439430237
train_iter_loss: 0.3994230329990387
train_iter_loss: 0.38609692454338074
train_iter_loss: 0.2759456932544708
train_iter_loss: 0.3573840260505676
train_iter_loss: 0.2572645843029022
train_iter_loss: 0.27658364176750183
train_iter_loss: 0.14467836916446686
train_iter_loss: 0.25941115617752075
train_iter_loss: 0.302827924489975
train_iter_loss: 0.313048779964447
train_iter_loss: 0.34836119413375854
train_iter_loss: 0.2368546575307846
train_iter_loss: 0.20107297599315643
train_iter_loss: 0.28552955389022827
train_iter_loss: 0.2914515733718872
train loss :0.2667
---------------------
Validation seg loss: 0.38519417043810467 at epoch 552
epoch =    553/  1000, exp = train
train_iter_loss: 0.16745705902576447
train_iter_loss: 0.2516198754310608
train_iter_loss: 0.39332807064056396
train_iter_loss: 0.1522391140460968
train_iter_loss: 0.2786364257335663
train_iter_loss: 0.21540327370166779
train_iter_loss: 0.2213081568479538
train_iter_loss: 0.15074004232883453
train_iter_loss: 0.2174704223871231
train_iter_loss: 0.36768946051597595
train_iter_loss: 0.12014104425907135
train_iter_loss: 0.15240611135959625
train_iter_loss: 0.2536371648311615
train_iter_loss: 0.2060902714729309
train_iter_loss: 0.3410871922969818
train_iter_loss: 0.34577125310897827
train_iter_loss: 0.25100240111351013
train_iter_loss: 0.1843394786119461
train_iter_loss: 0.32660946249961853
train_iter_loss: 0.27695029973983765
train_iter_loss: 0.3336747884750366
train_iter_loss: 0.4048275649547577
train_iter_loss: 0.2518121600151062
train_iter_loss: 0.30562180280685425
train_iter_loss: 0.301973432302475
train_iter_loss: 0.4893883466720581
train_iter_loss: 0.16764648258686066
train_iter_loss: 0.4983530342578888
train_iter_loss: 0.23219716548919678
train_iter_loss: 0.3596039116382599
train_iter_loss: 0.23598524928092957
train_iter_loss: 0.3327353894710541
train_iter_loss: 0.32219892740249634
train_iter_loss: 0.22946356236934662
train_iter_loss: 0.2235286831855774
train_iter_loss: 0.34668025374412537
train_iter_loss: 0.33564913272857666
train_iter_loss: 0.2618887424468994
train_iter_loss: 0.31351178884506226
train_iter_loss: 0.32371482253074646
train_iter_loss: 0.19983913004398346
train_iter_loss: 0.23903410136699677
train_iter_loss: 0.2948579490184784
train_iter_loss: 0.33949366211891174
train_iter_loss: 0.27708113193511963
train_iter_loss: 0.23941506445407867
train_iter_loss: 0.14504973590373993
train_iter_loss: 0.23006916046142578
train_iter_loss: 0.0911688283085823
train_iter_loss: 0.2405947893857956
train_iter_loss: 0.2732618451118469
train_iter_loss: 0.3368975818157196
train_iter_loss: 0.30107930302619934
train_iter_loss: 0.4991665780544281
train_iter_loss: 0.29626142978668213
train_iter_loss: 0.1451355665922165
train_iter_loss: 0.22237952053546906
train_iter_loss: 0.12899896502494812
train_iter_loss: 0.2980230152606964
train_iter_loss: 0.31669002771377563
train_iter_loss: 0.2085312306880951
train_iter_loss: 0.17160703241825104
train_iter_loss: 0.2744627296924591
train_iter_loss: 0.2442430555820465
train_iter_loss: 0.3688880503177643
train_iter_loss: 0.1836792230606079
train_iter_loss: 0.2807873785495758
train_iter_loss: 0.4400418996810913
train_iter_loss: 0.2770595848560333
train_iter_loss: 0.3266964256763458
train_iter_loss: 0.16931329667568207
train_iter_loss: 0.2263728529214859
train_iter_loss: 0.18353645503520966
train_iter_loss: 0.25554683804512024
train_iter_loss: 0.2760353684425354
train_iter_loss: 0.380350261926651
train_iter_loss: 0.22512856125831604
train_iter_loss: 0.30719661712646484
train_iter_loss: 0.22858086228370667
train_iter_loss: 0.2959052622318268
train_iter_loss: 0.25563451647758484
train_iter_loss: 0.2688814401626587
train_iter_loss: 0.17201456427574158
train_iter_loss: 0.3193759322166443
train_iter_loss: 0.2806600034236908
train_iter_loss: 0.20713511109352112
train_iter_loss: 0.2204807698726654
train_iter_loss: 0.4303382933139801
train_iter_loss: 0.4030340611934662
train_iter_loss: 0.07356161624193192
train_iter_loss: 0.2351740449666977
train_iter_loss: 0.26706409454345703
train_iter_loss: 0.2791360020637512
train_iter_loss: 0.2899606227874756
train_iter_loss: 0.2486935555934906
train_iter_loss: 0.26983755826950073
train_iter_loss: 0.23682191967964172
train_iter_loss: 0.28382059931755066
train_iter_loss: 0.2529647946357727
train_iter_loss: 0.44301897287368774
train loss :0.2729
---------------------
Validation seg loss: 0.3592505860889225 at epoch 553
epoch =    554/  1000, exp = train
train_iter_loss: 0.286982923746109
train_iter_loss: 0.3029533326625824
train_iter_loss: 0.27676939964294434
train_iter_loss: 0.4161185920238495
train_iter_loss: 0.34982889890670776
train_iter_loss: 0.299724817276001
train_iter_loss: 0.2194892317056656
train_iter_loss: 0.17655092477798462
train_iter_loss: 0.458315372467041
train_iter_loss: 0.2256454974412918
train_iter_loss: 0.1739751696586609
train_iter_loss: 0.4996285140514374
train_iter_loss: 0.23155395686626434
train_iter_loss: 0.25856128334999084
train_iter_loss: 0.15915681421756744
train_iter_loss: 0.10908456146717072
train_iter_loss: 0.2916688024997711
train_iter_loss: 0.33319345116615295
train_iter_loss: 0.16608178615570068
train_iter_loss: 0.27748966217041016
train_iter_loss: 0.28935033082962036
train_iter_loss: 0.38017356395721436
train_iter_loss: 0.23137061297893524
train_iter_loss: 0.26346156001091003
train_iter_loss: 0.2945016622543335
train_iter_loss: 0.25692325830459595
train_iter_loss: 0.4758760631084442
train_iter_loss: 0.39690855145454407
train_iter_loss: 0.17512907087802887
train_iter_loss: 0.11055266857147217
train_iter_loss: 0.14617809653282166
train_iter_loss: 0.5491595268249512
train_iter_loss: 0.16848543286323547
train_iter_loss: 0.38320937752723694
train_iter_loss: 0.3259536027908325
train_iter_loss: 0.26336660981178284
train_iter_loss: 0.174563929438591
train_iter_loss: 0.43076837062835693
train_iter_loss: 0.1339452862739563
train_iter_loss: 0.30662378668785095
train_iter_loss: 0.08554019778966904
train_iter_loss: 0.3551172614097595
train_iter_loss: 0.32527869939804077
train_iter_loss: 0.1553182750940323
train_iter_loss: 0.2348242700099945
train_iter_loss: 0.20446988940238953
train_iter_loss: 0.24833175539970398
train_iter_loss: 0.23797233402729034
train_iter_loss: 0.44444721937179565
train_iter_loss: 0.3023098409175873
train_iter_loss: 0.3035176396369934
train_iter_loss: 0.1724184900522232
train_iter_loss: 0.17126688361167908
train_iter_loss: 0.2634541094303131
train_iter_loss: 0.4200400412082672
train_iter_loss: 0.37839797139167786
train_iter_loss: 0.3481235206127167
train_iter_loss: 0.14889371395111084
train_iter_loss: 0.29105690121650696
train_iter_loss: 0.2715567350387573
train_iter_loss: 0.3983672261238098
train_iter_loss: 0.3058271110057831
train_iter_loss: 0.30720746517181396
train_iter_loss: 0.29735925793647766
train_iter_loss: 0.2994630038738251
train_iter_loss: 0.19146530330181122
train_iter_loss: 0.31644606590270996
train_iter_loss: 0.29824507236480713
train_iter_loss: 0.2552698850631714
train_iter_loss: 0.37568289041519165
train_iter_loss: 0.1258123517036438
train_iter_loss: 0.41743579506874084
train_iter_loss: 0.17781277000904083
train_iter_loss: 0.20572111010551453
train_iter_loss: 0.15142744779586792
train_iter_loss: 0.2398277223110199
train_iter_loss: 0.1159510388970375
train_iter_loss: 0.19829054176807404
train_iter_loss: 0.3603600561618805
train_iter_loss: 0.11762699484825134
train_iter_loss: 0.21628022193908691
train_iter_loss: 0.7499115467071533
train_iter_loss: 0.4297601878643036
train_iter_loss: 0.04816335439682007
train_iter_loss: 0.15273964405059814
train_iter_loss: 0.230342298746109
train_iter_loss: 0.3152869641780853
train_iter_loss: 0.24419444799423218
train_iter_loss: 0.21679896116256714
train_iter_loss: 0.23272132873535156
train_iter_loss: 0.2907972037792206
train_iter_loss: 0.21102337539196014
train_iter_loss: 0.25853201746940613
train_iter_loss: 0.18455109000205994
train_iter_loss: 0.44409865140914917
train_iter_loss: 0.11914289742708206
train_iter_loss: 0.2227250337600708
train_iter_loss: 0.4310888350009918
train_iter_loss: 0.26409196853637695
train_iter_loss: 0.3292808532714844
train loss :0.2765
---------------------
Validation seg loss: 0.3546392145953229 at epoch 554
epoch =    555/  1000, exp = train
train_iter_loss: 0.25845447182655334
train_iter_loss: 0.3056730329990387
train_iter_loss: 0.3725201487541199
train_iter_loss: 0.33078891038894653
train_iter_loss: 0.35017383098602295
train_iter_loss: 0.17279280722141266
train_iter_loss: 0.27945613861083984
train_iter_loss: 0.18151132762432098
train_iter_loss: 0.10825680941343307
train_iter_loss: 0.38015294075012207
train_iter_loss: 0.27938568592071533
train_iter_loss: 0.2650393843650818
train_iter_loss: 0.3142857551574707
train_iter_loss: 0.2976016104221344
train_iter_loss: 0.12294233590364456
train_iter_loss: 0.2902304232120514
train_iter_loss: 0.31136420369148254
train_iter_loss: 0.2076023817062378
train_iter_loss: 0.3527292013168335
train_iter_loss: 0.2721858024597168
train_iter_loss: 0.30406227707862854
train_iter_loss: 0.27449190616607666
train_iter_loss: 0.24994391202926636
train_iter_loss: 0.20453448593616486
train_iter_loss: 0.289064884185791
train_iter_loss: 0.3067883551120758
train_iter_loss: 0.1543240249156952
train_iter_loss: 0.28575414419174194
train_iter_loss: 0.11844027042388916
train_iter_loss: 0.341478168964386
train_iter_loss: 0.4037913978099823
train_iter_loss: 0.1054287776350975
train_iter_loss: 0.0966385006904602
train_iter_loss: 0.2798311114311218
train_iter_loss: 0.279556006193161
train_iter_loss: 0.27815380692481995
train_iter_loss: 0.28940248489379883
train_iter_loss: 0.2531009614467621
train_iter_loss: 0.14981766045093536
train_iter_loss: 0.15639570355415344
train_iter_loss: 0.3239901065826416
train_iter_loss: 0.3013477027416229
train_iter_loss: 0.18297316133975983
train_iter_loss: 0.3804915249347687
train_iter_loss: 0.3925599753856659
train_iter_loss: 0.25683122873306274
train_iter_loss: 0.2864064872264862
train_iter_loss: 0.25396695733070374
train_iter_loss: 0.3043660521507263
train_iter_loss: 0.2009337693452835
train_iter_loss: 0.2416568547487259
train_iter_loss: 0.24474455416202545
train_iter_loss: 0.11776112765073776
train_iter_loss: 0.20606887340545654
train_iter_loss: 0.3532258868217468
train_iter_loss: 0.3112519681453705
train_iter_loss: 0.43904900550842285
train_iter_loss: 0.42573538422584534
train_iter_loss: 0.2573467493057251
train_iter_loss: 0.28440529108047485
train_iter_loss: 0.2539597749710083
train_iter_loss: 0.3474838435649872
train_iter_loss: 0.29160669445991516
train_iter_loss: 0.2516966164112091
train_iter_loss: 0.2536654472351074
train_iter_loss: 0.2721843719482422
train_iter_loss: 0.3323865532875061
train_iter_loss: 0.35095956921577454
train_iter_loss: 0.2883802354335785
train_iter_loss: 0.16193599998950958
train_iter_loss: 0.23710328340530396
train_iter_loss: 0.239723801612854
train_iter_loss: 0.38774213194847107
train_iter_loss: 0.2216453105211258
train_iter_loss: 0.15770559012889862
train_iter_loss: 0.4127374291419983
train_iter_loss: 0.1902369260787964
train_iter_loss: 0.35754358768463135
train_iter_loss: 0.20811016857624054
train_iter_loss: 0.29524242877960205
train_iter_loss: 0.27297478914260864
train_iter_loss: 0.14378489553928375
train_iter_loss: 0.13264715671539307
train_iter_loss: 0.1479881852865219
train_iter_loss: 0.2785176634788513
train_iter_loss: 0.16935668885707855
train_iter_loss: 0.19662173092365265
train_iter_loss: 0.4045483469963074
train_iter_loss: 0.28154247999191284
train_iter_loss: 0.25185540318489075
train_iter_loss: 0.292795330286026
train_iter_loss: 0.31586533784866333
train_iter_loss: 0.3634880781173706
train_iter_loss: 0.22329358756542206
train_iter_loss: 0.1788731962442398
train_iter_loss: 0.20905517041683197
train_iter_loss: 0.2290719449520111
train_iter_loss: 0.23892030119895935
train_iter_loss: 0.4437418282032013
train_iter_loss: 0.2969101667404175
train loss :0.2690
---------------------
Validation seg loss: 0.3715582875675469 at epoch 555
epoch =    556/  1000, exp = train
train_iter_loss: 0.3480208218097687
train_iter_loss: 0.2774641215801239
train_iter_loss: 0.2218003273010254
train_iter_loss: 0.17620962858200073
train_iter_loss: 0.25650325417518616
train_iter_loss: 0.1899302452802658
train_iter_loss: 0.2053629606962204
train_iter_loss: 0.1644689291715622
train_iter_loss: 0.3306770622730255
train_iter_loss: 0.19200913608074188
train_iter_loss: 0.2257709801197052
train_iter_loss: 0.361687570810318
train_iter_loss: 0.30780160427093506
train_iter_loss: 0.18686331808567047
train_iter_loss: 0.3682714104652405
train_iter_loss: 0.23272880911827087
train_iter_loss: 0.26803380250930786
train_iter_loss: 0.3268447816371918
train_iter_loss: 0.3721831738948822
train_iter_loss: 0.36193105578422546
train_iter_loss: 0.22263528406620026
train_iter_loss: 0.20011727511882782
train_iter_loss: 0.11858703196048737
train_iter_loss: 0.1552688330411911
train_iter_loss: 0.2493627816438675
train_iter_loss: 0.33633318543434143
train_iter_loss: 0.3749585747718811
train_iter_loss: 0.2703343331813812
train_iter_loss: 0.3013901114463806
train_iter_loss: 0.293321430683136
train_iter_loss: 0.17015500366687775
train_iter_loss: 0.1938871145248413
train_iter_loss: 0.23574355244636536
train_iter_loss: 0.130107119679451
train_iter_loss: 0.12740547955036163
train_iter_loss: 0.4489908516407013
train_iter_loss: 0.25870561599731445
train_iter_loss: 0.23409582674503326
train_iter_loss: 0.24177229404449463
train_iter_loss: 0.21286378800868988
train_iter_loss: 0.23252350091934204
train_iter_loss: 0.32304584980010986
train_iter_loss: 0.26489928364753723
train_iter_loss: 0.11558341979980469
train_iter_loss: 0.35247620940208435
train_iter_loss: 0.2991032302379608
train_iter_loss: 0.20617027580738068
train_iter_loss: 0.2949187755584717
train_iter_loss: 0.20054903626441956
train_iter_loss: 0.20235411822795868
train_iter_loss: 0.21827638149261475
train_iter_loss: 0.2522927224636078
train_iter_loss: 0.27812543511390686
train_iter_loss: 0.38171228766441345
train_iter_loss: 0.19666787981987
train_iter_loss: 0.40527692437171936
train_iter_loss: 0.18530547618865967
train_iter_loss: 0.2672024667263031
train_iter_loss: 0.29521507024765015
train_iter_loss: 0.24346503615379333
train_iter_loss: 0.3440222442150116
train_iter_loss: 0.08894295245409012
train_iter_loss: 0.461584210395813
train_iter_loss: 0.29367733001708984
train_iter_loss: 0.4755876660346985
train_iter_loss: 0.08617572486400604
train_iter_loss: 0.33860141038894653
train_iter_loss: 0.09336957335472107
train_iter_loss: 0.45023560523986816
train_iter_loss: 0.26051053404808044
train_iter_loss: 0.3254743218421936
train_iter_loss: 0.21373336017131805
train_iter_loss: 0.27409058809280396
train_iter_loss: 0.2876047194004059
train_iter_loss: 0.38602879643440247
train_iter_loss: 0.18914607167243958
train_iter_loss: 0.35167381167411804
train_iter_loss: 0.24570982158184052
train_iter_loss: 0.35926276445388794
train_iter_loss: 0.3099929690361023
train_iter_loss: 0.18486064672470093
train_iter_loss: 0.07390865683555603
train_iter_loss: 0.24581362307071686
train_iter_loss: 0.17790526151657104
train_iter_loss: 0.29472222924232483
train_iter_loss: 0.3500918745994568
train_iter_loss: 0.2122475504875183
train_iter_loss: 0.2550286650657654
train_iter_loss: 0.35485440492630005
train_iter_loss: 0.31628262996673584
train_iter_loss: 0.19916881620883942
train_iter_loss: 0.32434409856796265
train_iter_loss: 0.16514895856380463
train_iter_loss: 0.26797372102737427
train_iter_loss: 0.12587353587150574
train_iter_loss: 0.2907060384750366
train_iter_loss: 0.1966700255870819
train_iter_loss: 0.25855332612991333
train_iter_loss: 0.21042294800281525
train_iter_loss: 0.24351081252098083
train loss :0.2622
---------------------
Validation seg loss: 0.3557602085858443 at epoch 556
epoch =    557/  1000, exp = train
train_iter_loss: 0.41815486550331116
train_iter_loss: 0.25698092579841614
train_iter_loss: 0.2224964201450348
train_iter_loss: 0.43003785610198975
train_iter_loss: 0.3930685818195343
train_iter_loss: 0.3023669719696045
train_iter_loss: 0.3664145767688751
train_iter_loss: 0.2704790532588959
train_iter_loss: 0.21001246571540833
train_iter_loss: 0.20670098066329956
train_iter_loss: 0.36436042189598083
train_iter_loss: 0.1041421964764595
train_iter_loss: 0.2900281548500061
train_iter_loss: 0.294598788022995
train_iter_loss: 0.33410975337028503
train_iter_loss: 0.045899733901023865
train_iter_loss: 0.3203935921192169
train_iter_loss: 0.30990689992904663
train_iter_loss: 0.29712527990341187
train_iter_loss: 0.2581202983856201
train_iter_loss: 0.21439172327518463
train_iter_loss: 0.2944332957267761
train_iter_loss: 0.1199626475572586
train_iter_loss: 0.31703945994377136
train_iter_loss: 0.3167613446712494
train_iter_loss: 0.30450257658958435
train_iter_loss: 0.3968223035335541
train_iter_loss: 0.29106178879737854
train_iter_loss: 0.3331792950630188
train_iter_loss: 0.2891419231891632
train_iter_loss: 0.19920922815799713
train_iter_loss: 0.1937926858663559
train_iter_loss: 0.21062421798706055
train_iter_loss: 0.30746063590049744
train_iter_loss: 0.2309359759092331
train_iter_loss: 0.0925363153219223
train_iter_loss: 0.2670956552028656
train_iter_loss: 0.22668300569057465
train_iter_loss: 0.31493258476257324
train_iter_loss: 0.30314725637435913
train_iter_loss: 0.253299355506897
train_iter_loss: 0.16214247047901154
train_iter_loss: 0.2713955342769623
train_iter_loss: 0.22271092236042023
train_iter_loss: 0.4365575313568115
train_iter_loss: 0.2976565361022949
train_iter_loss: 0.16019974648952484
train_iter_loss: 0.33588042855262756
train_iter_loss: 0.4791512191295624
train_iter_loss: 0.3063035011291504
train_iter_loss: 0.1591397374868393
train_iter_loss: 0.14278768002986908
train_iter_loss: 0.20552602410316467
train_iter_loss: 0.30562686920166016
train_iter_loss: 0.15790897607803345
train_iter_loss: 0.14145104587078094
train_iter_loss: 0.15044204890727997
train_iter_loss: 0.30880463123321533
train_iter_loss: 0.22198477387428284
train_iter_loss: 0.21204718947410583
train_iter_loss: 0.2531968653202057
train_iter_loss: 0.19466754794120789
train_iter_loss: 0.1995331048965454
train_iter_loss: 0.38484519720077515
train_iter_loss: 0.20723462104797363
train_iter_loss: 0.14962169528007507
train_iter_loss: 0.26220136880874634
train_iter_loss: 0.24039939045906067
train_iter_loss: 0.2686135768890381
train_iter_loss: 0.33979809284210205
train_iter_loss: 0.30125823616981506
train_iter_loss: 0.31915122270584106
train_iter_loss: 0.4278547465801239
train_iter_loss: 0.25064364075660706
train_iter_loss: 0.1808730512857437
train_iter_loss: 0.3477783501148224
train_iter_loss: 0.23124292492866516
train_iter_loss: 0.20092061161994934
train_iter_loss: 0.10506416112184525
train_iter_loss: 0.28724461793899536
train_iter_loss: 0.24782009422779083
train_iter_loss: 0.41959014534950256
train_iter_loss: 0.2737060487270355
train_iter_loss: 0.3342570662498474
train_iter_loss: 0.3720886707305908
train_iter_loss: 0.07105020433664322
train_iter_loss: 0.36920976638793945
train_iter_loss: 0.32366928458213806
train_iter_loss: 0.1730251908302307
train_iter_loss: 0.16350433230400085
train_iter_loss: 0.22423647344112396
train_iter_loss: 0.08709433674812317
train_iter_loss: 0.2105834186077118
train_iter_loss: 0.3084934651851654
train_iter_loss: 0.34893596172332764
train_iter_loss: 0.2522670030593872
train_iter_loss: 0.24310722947120667
train_iter_loss: 0.3285521864891052
train_iter_loss: 0.508655309677124
train_iter_loss: 0.2841843068599701
train loss :0.2681
---------------------
Validation seg loss: 0.3762927841272135 at epoch 557
epoch =    558/  1000, exp = train
train_iter_loss: 0.18895192444324493
train_iter_loss: 0.34567320346832275
train_iter_loss: 0.20865488052368164
train_iter_loss: 0.23948881030082703
train_iter_loss: 0.24721026420593262
train_iter_loss: 0.3726392090320587
train_iter_loss: 0.18036434054374695
train_iter_loss: 0.1896125227212906
train_iter_loss: 0.16932347416877747
train_iter_loss: 0.2691364884376526
train_iter_loss: 0.28014716506004333
train_iter_loss: 0.22713689506053925
train_iter_loss: 0.3563373386859894
train_iter_loss: 0.267403244972229
train_iter_loss: 0.2975567877292633
train_iter_loss: 0.38540786504745483
train_iter_loss: 0.2536050081253052
train_iter_loss: 0.15928632020950317
train_iter_loss: 0.31624743342399597
train_iter_loss: 0.32951411604881287
train_iter_loss: 0.2753281891345978
train_iter_loss: 0.6494559645652771
train_iter_loss: 0.2826429605484009
train_iter_loss: 0.2360963076353073
train_iter_loss: 0.3553001880645752
train_iter_loss: 0.19905735552310944
train_iter_loss: 0.2737997770309448
train_iter_loss: 0.19115088880062103
train_iter_loss: 0.22389452159404755
train_iter_loss: 0.285958856344223
train_iter_loss: 0.4540433883666992
train_iter_loss: 0.20896188914775848
train_iter_loss: 0.4188983738422394
train_iter_loss: 0.3470848798751831
train_iter_loss: 0.37823978066444397
train_iter_loss: 0.31185275316238403
train_iter_loss: 0.1250045895576477
train_iter_loss: 0.4791320562362671
train_iter_loss: 0.11282450705766678
train_iter_loss: 0.2689632177352905
train_iter_loss: 0.2615317702293396
train_iter_loss: 0.2321823537349701
train_iter_loss: 0.2460600733757019
train_iter_loss: 0.1468796730041504
train_iter_loss: 0.23298266530036926
train_iter_loss: 0.18865346908569336
train_iter_loss: 0.35832706093788147
train_iter_loss: 0.23831820487976074
train_iter_loss: 0.1772838532924652
train_iter_loss: 0.2914887070655823
train_iter_loss: 0.2563594579696655
train_iter_loss: 0.252607524394989
train_iter_loss: 0.381732314825058
train_iter_loss: 0.3105598986148834
train_iter_loss: 0.28733229637145996
train_iter_loss: 0.4295685291290283
train_iter_loss: 0.0822216048836708
train_iter_loss: 0.2976101040840149
train_iter_loss: 0.36716145277023315
train_iter_loss: 0.3320745825767517
train_iter_loss: 0.1331191062927246
train_iter_loss: 0.0887480080127716
train_iter_loss: 0.3074672222137451
train_iter_loss: 0.3565209209918976
train_iter_loss: 0.40541255474090576
train_iter_loss: 0.23255029320716858
train_iter_loss: 0.2679467499256134
train_iter_loss: 0.19955743849277496
train_iter_loss: 0.24123595654964447
train_iter_loss: 0.32884469628334045
train_iter_loss: 0.399039626121521
train_iter_loss: 0.1928407996892929
train_iter_loss: 0.1885565221309662
train_iter_loss: 0.2764314115047455
train_iter_loss: 0.16908542811870575
train_iter_loss: 0.2501260042190552
train_iter_loss: 0.10576146095991135
train_iter_loss: 0.3210201561450958
train_iter_loss: 0.4062272012233734
train_iter_loss: 0.39414048194885254
train_iter_loss: 0.3200471103191376
train_iter_loss: 0.40651944279670715
train_iter_loss: 0.36401888728141785
train_iter_loss: 0.22004415094852448
train_iter_loss: 0.291174978017807
train_iter_loss: 0.14440609514713287
train_iter_loss: 0.3321298062801361
train_iter_loss: 0.24439175426959991
train_iter_loss: 0.5175230503082275
train_iter_loss: 0.37755173444747925
train_iter_loss: 0.3445492386817932
train_iter_loss: 0.25090494751930237
train_iter_loss: 0.27149713039398193
train_iter_loss: 0.26679205894470215
train_iter_loss: 0.1560080498456955
train_iter_loss: 0.28763341903686523
train_iter_loss: 0.2638796269893646
train_iter_loss: 0.17941883206367493
train_iter_loss: 0.33915212750434875
train_iter_loss: 0.19563108682632446
train loss :0.2804
---------------------
Validation seg loss: 0.3474759763317569 at epoch 558
epoch =    559/  1000, exp = train
train_iter_loss: 0.18561534583568573
train_iter_loss: 0.21357543766498566
train_iter_loss: 0.1349710077047348
train_iter_loss: 0.26237770915031433
train_iter_loss: 0.4993222653865814
train_iter_loss: 0.3419017195701599
train_iter_loss: 0.3262731730937958
train_iter_loss: 0.2518932819366455
train_iter_loss: 0.2933335602283478
train_iter_loss: 0.35749736428260803
train_iter_loss: 0.21170580387115479
train_iter_loss: 0.3051629960536957
train_iter_loss: 0.19198600947856903
train_iter_loss: 0.2954704165458679
train_iter_loss: 0.4267578125
train_iter_loss: 0.2687969505786896
train_iter_loss: 0.4010564386844635
train_iter_loss: 0.27823805809020996
train_iter_loss: 0.18335990607738495
train_iter_loss: 0.2000000774860382
train_iter_loss: 0.1720571368932724
train_iter_loss: 0.23512917757034302
train_iter_loss: 0.26289987564086914
train_iter_loss: 0.24102383852005005
train_iter_loss: 0.18800853192806244
train_iter_loss: 0.26392385363578796
train_iter_loss: 0.2943894863128662
train_iter_loss: 0.36073777079582214
train_iter_loss: 0.2628197968006134
train_iter_loss: 0.2164231240749359
train_iter_loss: 0.33290475606918335
train_iter_loss: 0.22486260533332825
train_iter_loss: 0.22865551710128784
train_iter_loss: 0.39742887020111084
train_iter_loss: 0.27906176447868347
train_iter_loss: 0.2182348519563675
train_iter_loss: 0.1724228709936142
train_iter_loss: 0.3677588403224945
train_iter_loss: 0.2562076151371002
train_iter_loss: 0.2464459389448166
train_iter_loss: 0.21403571963310242
train_iter_loss: 0.14056861400604248
train_iter_loss: 0.10613922774791718
train_iter_loss: 0.26665791869163513
train_iter_loss: 0.2193746268749237
train_iter_loss: 0.19235903024673462
train_iter_loss: 0.18442676961421967
train_iter_loss: 0.1294354349374771
train_iter_loss: 0.24157913029193878
train_iter_loss: 0.12544889748096466
train_iter_loss: 0.3305392563343048
train_iter_loss: 0.26644736528396606
train_iter_loss: 0.27763479948043823
train_iter_loss: 0.15977106988430023
train_iter_loss: 0.33707520365715027
train_iter_loss: 0.08970688283443451
train_iter_loss: 0.3397541642189026
train_iter_loss: 0.20949062705039978
train_iter_loss: 0.21031779050827026
train_iter_loss: 0.19295114278793335
train_iter_loss: 0.45246702432632446
train_iter_loss: 0.22073018550872803
train_iter_loss: 0.2561473250389099
train_iter_loss: 0.2511041462421417
train_iter_loss: 0.1324913650751114
train_iter_loss: 0.29223620891571045
train_iter_loss: 0.3628275990486145
train_iter_loss: 0.25731831789016724
train_iter_loss: 0.18247312307357788
train_iter_loss: 0.09557611495256424
train_iter_loss: 0.07801835983991623
train_iter_loss: 0.19391800463199615
train_iter_loss: 0.22585073113441467
train_iter_loss: 0.22392041981220245
train_iter_loss: 0.2234845906496048
train_iter_loss: 0.3337094187736511
train_iter_loss: 0.2806498110294342
train_iter_loss: 0.37745413184165955
train_iter_loss: 0.3009564280509949
train_iter_loss: 0.1688065379858017
train_iter_loss: 0.24371132254600525
train_iter_loss: 0.22929948568344116
train_iter_loss: 0.4005486071109772
train_iter_loss: 0.4399348199367523
train_iter_loss: 0.3072834014892578
train_iter_loss: 0.2469370812177658
train_iter_loss: 0.25390544533729553
train_iter_loss: 0.35353851318359375
train_iter_loss: 0.28968513011932373
train_iter_loss: 0.2671336531639099
train_iter_loss: 0.21997980773448944
train_iter_loss: 0.1025722473859787
train_iter_loss: 0.2523214817047119
train_iter_loss: 0.2989208996295929
train_iter_loss: 0.2855056822299957
train_iter_loss: 0.30706164240837097
train_iter_loss: 0.1837184727191925
train_iter_loss: 0.34984180331230164
train_iter_loss: 0.3775905668735504
train_iter_loss: 0.24110305309295654
train loss :0.2593
---------------------
Validation seg loss: 0.4457108447424857 at epoch 559
epoch =    560/  1000, exp = train
train_iter_loss: 0.2364232838153839
train_iter_loss: 0.32396572828292847
train_iter_loss: 0.15711413323879242
train_iter_loss: 0.2887047231197357
train_iter_loss: 0.2200029343366623
train_iter_loss: 0.24288448691368103
train_iter_loss: 0.14657488465309143
train_iter_loss: 0.20592309534549713
train_iter_loss: 0.35368865728378296
train_iter_loss: 0.3404077887535095
train_iter_loss: 0.21257175505161285
train_iter_loss: 0.4647665023803711
train_iter_loss: 0.14438144862651825
train_iter_loss: 0.24320374429225922
train_iter_loss: 0.29734471440315247
train_iter_loss: 0.2271183580160141
train_iter_loss: 0.12314969301223755
train_iter_loss: 0.2210872769355774
train_iter_loss: 0.1490086019039154
train_iter_loss: 0.1495915651321411
train_iter_loss: 0.3871775269508362
train_iter_loss: 0.28440070152282715
train_iter_loss: 0.3357686400413513
train_iter_loss: 0.29270514845848083
train_iter_loss: 0.28193360567092896
train_iter_loss: 0.19908879697322845
train_iter_loss: 0.2331896424293518
train_iter_loss: 0.2441570907831192
train_iter_loss: 0.28467535972595215
train_iter_loss: 0.36342954635620117
train_iter_loss: 0.1766246110200882
train_iter_loss: 0.16731636226177216
train_iter_loss: 0.29662296175956726
train_iter_loss: 0.22485361993312836
train_iter_loss: 0.2926037013530731
train_iter_loss: 0.19026531279087067
train_iter_loss: 0.23492036759853363
train_iter_loss: 0.18411962687969208
train_iter_loss: 0.2492581456899643
train_iter_loss: 0.32786837220191956
train_iter_loss: 0.21504557132720947
train_iter_loss: 0.2877029776573181
train_iter_loss: 0.307049036026001
train_iter_loss: 0.3155977725982666
train_iter_loss: 0.17006756365299225
train_iter_loss: 0.42014703154563904
train_iter_loss: 0.21938586235046387
train_iter_loss: 0.13716930150985718
train_iter_loss: 0.25920554995536804
train_iter_loss: 0.4239600598812103
train_iter_loss: 0.3960415720939636
train_iter_loss: 0.2890433371067047
train_iter_loss: 0.21208322048187256
train_iter_loss: 0.4845336973667145
train_iter_loss: 0.3543495237827301
train_iter_loss: 0.39642781019210815
train_iter_loss: 0.2584032416343689
train_iter_loss: 0.20992690324783325
train_iter_loss: 0.2251518815755844
train_iter_loss: 0.1703750491142273
train_iter_loss: 0.26592546701431274
train_iter_loss: 0.12957847118377686
train_iter_loss: 0.27039608359336853
train_iter_loss: 0.4640124440193176
train_iter_loss: 0.20784221589565277
train_iter_loss: 0.12063831835985184
train_iter_loss: 0.2867121398448944
train_iter_loss: 0.19401848316192627
train_iter_loss: 0.3517838716506958
train_iter_loss: 0.27933043241500854
train_iter_loss: 0.22347798943519592
train_iter_loss: 0.3514322340488434
train_iter_loss: 0.27537405490875244
train_iter_loss: 0.2006780505180359
train_iter_loss: 0.26945239305496216
train_iter_loss: 0.34148743748664856
train_iter_loss: 0.09438939392566681
train_iter_loss: 0.3271680176258087
train_iter_loss: 0.3590681850910187
train_iter_loss: 0.3584457337856293
train_iter_loss: 0.24552562832832336
train_iter_loss: 0.29372820258140564
train_iter_loss: 0.3788139224052429
train_iter_loss: 0.2080679088830948
train_iter_loss: 0.3667888641357422
train_iter_loss: 0.23548009991645813
train_iter_loss: 0.3313375413417816
train_iter_loss: 0.3056665062904358
train_iter_loss: 0.23326118290424347
train_iter_loss: 0.32393980026245117
train_iter_loss: 0.2964971661567688
train_iter_loss: 0.23440532386302948
train_iter_loss: 0.36275845766067505
train_iter_loss: 0.1960756778717041
train_iter_loss: 0.2853218913078308
train_iter_loss: 0.2494063824415207
train_iter_loss: 0.25503894686698914
train_iter_loss: 0.2767389118671417
train_iter_loss: 0.09123913943767548
train_iter_loss: 0.13877080380916595
train loss :0.2669
---------------------
Validation seg loss: 0.3839913216981826 at epoch 560
epoch =    561/  1000, exp = train
train_iter_loss: 0.19181737303733826
train_iter_loss: 0.19140943884849548
train_iter_loss: 0.36687925457954407
train_iter_loss: 0.2641492486000061
train_iter_loss: 0.3133591413497925
train_iter_loss: 0.3094398081302643
train_iter_loss: 0.31565871834754944
train_iter_loss: 0.3986503779888153
train_iter_loss: 0.47146502137184143
train_iter_loss: 0.3720293641090393
train_iter_loss: 0.230741947889328
train_iter_loss: 0.10032203793525696
train_iter_loss: 0.22242091596126556
train_iter_loss: 0.19240860641002655
train_iter_loss: 0.1993536353111267
train_iter_loss: 0.2901715338230133
train_iter_loss: 0.25784438848495483
train_iter_loss: 0.4198041260242462
train_iter_loss: 0.28129565715789795
train_iter_loss: 0.04137970879673958
train_iter_loss: 0.3019559681415558
train_iter_loss: 0.4104138910770416
train_iter_loss: 0.34344661235809326
train_iter_loss: 0.3006293475627899
train_iter_loss: 0.34729593992233276
train_iter_loss: 0.22346334159374237
train_iter_loss: 0.08750149607658386
train_iter_loss: 0.1749112606048584
train_iter_loss: 0.20087693631649017
train_iter_loss: 0.24583686888217926
train_iter_loss: 0.26172417402267456
train_iter_loss: 0.23122625052928925
train_iter_loss: 0.29633471369743347
train_iter_loss: 0.24808992445468903
train_iter_loss: 0.20126459002494812
train_iter_loss: 0.3399873673915863
train_iter_loss: 0.5067834258079529
train_iter_loss: 0.3154926896095276
train_iter_loss: 0.33446255326271057
train_iter_loss: 0.2259688377380371
train_iter_loss: 0.22819256782531738
train_iter_loss: 0.2719796299934387
train_iter_loss: 0.2738608121871948
train_iter_loss: 0.3599838316440582
train_iter_loss: 0.28606411814689636
train_iter_loss: 0.3129350244998932
train_iter_loss: 0.21852387487888336
train_iter_loss: 0.34235090017318726
train_iter_loss: 0.5678253173828125
train_iter_loss: 0.3304138481616974
train_iter_loss: 0.23280014097690582
train_iter_loss: 0.34850361943244934
train_iter_loss: 0.15352043509483337
train_iter_loss: 0.26757878065109253
train_iter_loss: 0.26536986231803894
train_iter_loss: 0.1793709695339203
train_iter_loss: 0.37640389800071716
train_iter_loss: 0.13935579359531403
train_iter_loss: 0.33687493205070496
train_iter_loss: 0.37652820348739624
train_iter_loss: 0.1657492220401764
train_iter_loss: 0.3864632248878479
train_iter_loss: 0.19689923524856567
train_iter_loss: 0.28909194469451904
train_iter_loss: 0.3000411093235016
train_iter_loss: 0.2664701044559479
train_iter_loss: 0.32227158546447754
train_iter_loss: 0.4081270694732666
train_iter_loss: 0.25475358963012695
train_iter_loss: 0.20614434778690338
train_iter_loss: 0.12657330930233002
train_iter_loss: 0.21113231778144836
train_iter_loss: 0.11969400942325592
train_iter_loss: 0.2502802610397339
train_iter_loss: 0.23580078780651093
train_iter_loss: 0.19020764529705048
train_iter_loss: 0.39887896180152893
train_iter_loss: 0.29219189286231995
train_iter_loss: 0.19787155091762543
train_iter_loss: 0.3037073612213135
train_iter_loss: 0.07643919438123703
train_iter_loss: 0.36660292744636536
train_iter_loss: 0.5542162656784058
train_iter_loss: 0.2951521575450897
train_iter_loss: 0.20457254350185394
train_iter_loss: 0.3657819628715515
train_iter_loss: 0.2448378950357437
train_iter_loss: 0.31409451365470886
train_iter_loss: 0.2235076129436493
train_iter_loss: 0.27426114678382874
train_iter_loss: 0.17935973405838013
train_iter_loss: 0.25344064831733704
train_iter_loss: 0.18796777725219727
train_iter_loss: 0.22509720921516418
train_iter_loss: 0.3569444417953491
train_iter_loss: 0.2528105676174164
train_iter_loss: 0.29221636056900024
train_iter_loss: 0.3318038582801819
train_iter_loss: 0.3955516517162323
train_iter_loss: 0.19254843890666962
train loss :0.2787
---------------------
Validation seg loss: 0.36034104671715844 at epoch 561
epoch =    562/  1000, exp = train
train_iter_loss: 0.3553944230079651
train_iter_loss: 0.27106231451034546
train_iter_loss: 0.32280153036117554
train_iter_loss: 0.24576058983802795
train_iter_loss: 0.20991046726703644
train_iter_loss: 0.28571292757987976
train_iter_loss: 0.1908504217863083
train_iter_loss: 0.3982487618923187
train_iter_loss: 0.22157514095306396
train_iter_loss: 0.3737238645553589
train_iter_loss: 0.3640232980251312
train_iter_loss: 0.3396216332912445
train_iter_loss: 0.30803006887435913
train_iter_loss: 0.27691689133644104
train_iter_loss: 0.2368108630180359
train_iter_loss: 0.16804422438144684
train_iter_loss: 0.19367560744285583
train_iter_loss: 0.21547825634479523
train_iter_loss: 0.3340897262096405
train_iter_loss: 0.1943984031677246
train_iter_loss: 0.16490554809570312
train_iter_loss: 0.2426084578037262
train_iter_loss: 0.25659459829330444
train_iter_loss: 0.24623467028141022
train_iter_loss: 0.3057561218738556
train_iter_loss: 0.235306516289711
train_iter_loss: 0.3530629873275757
train_iter_loss: 0.28040382266044617
train_iter_loss: 0.35233405232429504
train_iter_loss: 0.20805662870407104
train_iter_loss: 0.27140137553215027
train_iter_loss: 0.3556968569755554
train_iter_loss: 0.22812491655349731
train_iter_loss: 0.323212593793869
train_iter_loss: 0.17219531536102295
train_iter_loss: 0.3311271071434021
train_iter_loss: 0.33801740407943726
train_iter_loss: 0.2823105752468109
train_iter_loss: 0.19363237917423248
train_iter_loss: 0.3125424087047577
train_iter_loss: 0.31220993399620056
train_iter_loss: 0.34964364767074585
train_iter_loss: 0.2456946074962616
train_iter_loss: 0.24355316162109375
train_iter_loss: 0.35075974464416504
train_iter_loss: 0.20266450941562653
train_iter_loss: 0.2884502410888672
train_iter_loss: 0.23472259938716888
train_iter_loss: 0.22217096388339996
train_iter_loss: 0.28491997718811035
train_iter_loss: 0.26673051714897156
train_iter_loss: 0.1350964903831482
train_iter_loss: 0.26225656270980835
train_iter_loss: 0.33095601201057434
train_iter_loss: 0.3274129033088684
train_iter_loss: 0.17239710688591003
train_iter_loss: 0.360779732465744
train_iter_loss: 0.3435450792312622
train_iter_loss: 0.23980383574962616
train_iter_loss: 0.19772139191627502
train_iter_loss: 0.192636176943779
train_iter_loss: 0.14324764907360077
train_iter_loss: 0.3335159420967102
train_iter_loss: 0.24949495494365692
train_iter_loss: 0.11124735325574875
train_iter_loss: 0.17102448642253876
train_iter_loss: 0.16259920597076416
train_iter_loss: 0.3524114191532135
train_iter_loss: 0.11153054982423782
train_iter_loss: 0.3157011866569519
train_iter_loss: 0.2019636631011963
train_iter_loss: 0.15462331473827362
train_iter_loss: 0.5429717898368835
train_iter_loss: 0.25904494524002075
train_iter_loss: 0.3603835999965668
train_iter_loss: 0.35428497195243835
train_iter_loss: 0.13948875665664673
train_iter_loss: 0.14165399968624115
train_iter_loss: 0.49075859785079956
train_iter_loss: 0.09050458669662476
train_iter_loss: 0.18070954084396362
train_iter_loss: 0.1920628547668457
train_iter_loss: 0.5009781718254089
train_iter_loss: 0.3041609227657318
train_iter_loss: 0.23017089068889618
train_iter_loss: 0.23555536568164825
train_iter_loss: 0.09553463757038116
train_iter_loss: 0.2562781572341919
train_iter_loss: 0.22109456360340118
train_iter_loss: 0.3398335874080658
train_iter_loss: 0.3393413722515106
train_iter_loss: 0.22385281324386597
train_iter_loss: 0.14404253661632538
train_iter_loss: 0.3237071633338928
train_iter_loss: 0.2427939474582672
train_iter_loss: 0.17127756774425507
train_iter_loss: 0.23572130501270294
train_iter_loss: 0.30994996428489685
train_iter_loss: 0.17563815414905548
train_iter_loss: 0.2556603252887726
train loss :0.2647
---------------------
Validation seg loss: 0.36865720227536447 at epoch 562
epoch =    563/  1000, exp = train
train_iter_loss: 0.19283457100391388
train_iter_loss: 0.49149784445762634
train_iter_loss: 0.38246726989746094
train_iter_loss: 0.3113254904747009
train_iter_loss: 0.2689289450645447
train_iter_loss: 0.14286741614341736
train_iter_loss: 0.3354615569114685
train_iter_loss: 0.47119373083114624
train_iter_loss: 0.20998992025852203
train_iter_loss: 0.3019985854625702
train_iter_loss: 0.29815301299095154
train_iter_loss: 0.31593456864356995
train_iter_loss: 0.2524162530899048
train_iter_loss: 0.34921228885650635
train_iter_loss: 0.1092897579073906
train_iter_loss: 0.2197537124156952
train_iter_loss: 0.29784077405929565
train_iter_loss: 0.46715065836906433
train_iter_loss: 0.24554800987243652
train_iter_loss: 0.09951680153608322
train_iter_loss: 0.3988026976585388
train_iter_loss: 0.21142904460430145
train_iter_loss: 0.38353511691093445
train_iter_loss: 0.22590546309947968
train_iter_loss: 0.2859249413013458
train_iter_loss: 0.2429533749818802
train_iter_loss: 0.22007037699222565
train_iter_loss: 0.17752431333065033
train_iter_loss: 0.2094043791294098
train_iter_loss: 0.2301943302154541
train_iter_loss: 0.3362649381160736
train_iter_loss: 0.22573508322238922
train_iter_loss: 0.26157814264297485
train_iter_loss: 0.28707730770111084
train_iter_loss: 0.2849634289741516
train_iter_loss: 0.20286047458648682
train_iter_loss: 0.3521145284175873
train_iter_loss: 0.35776427388191223
train_iter_loss: 0.0704243928194046
train_iter_loss: 0.288345605134964
train_iter_loss: 0.16987790167331696
train_iter_loss: 0.22701561450958252
train_iter_loss: 0.17306794226169586
train_iter_loss: 0.22613103687763214
train_iter_loss: 0.21781395375728607
train_iter_loss: 0.19465979933738708
train_iter_loss: 0.3503662049770355
train_iter_loss: 0.2482280731201172
train_iter_loss: 0.08948992937803268
train_iter_loss: 0.375870019197464
train_iter_loss: 0.24084749817848206
train_iter_loss: 0.21314536035060883
train_iter_loss: 0.17220932245254517
train_iter_loss: 0.3761279881000519
train_iter_loss: 0.2653060555458069
train_iter_loss: 0.5301991105079651
train_iter_loss: 0.3392915725708008
train_iter_loss: 0.3377586901187897
train_iter_loss: 0.16088810563087463
train_iter_loss: 0.3360348641872406
train_iter_loss: 0.2067130208015442
train_iter_loss: 0.11279667913913727
train_iter_loss: 0.2407578080892563
train_iter_loss: 0.23962455987930298
train_iter_loss: 0.2819501459598541
train_iter_loss: 0.3500881791114807
train_iter_loss: 0.15385015308856964
train_iter_loss: 0.11266615986824036
train_iter_loss: 0.16745221614837646
train_iter_loss: 0.27096816897392273
train_iter_loss: 0.28076866269111633
train_iter_loss: 0.26583391427993774
train_iter_loss: 0.3282790780067444
train_iter_loss: 0.1645689457654953
train_iter_loss: 0.18848958611488342
train_iter_loss: 0.35797563195228577
train_iter_loss: 0.4002346992492676
train_iter_loss: 0.3658599853515625
train_iter_loss: 0.369914710521698
train_iter_loss: 0.3181765377521515
train_iter_loss: 0.31771954894065857
train_iter_loss: 0.1832369565963745
train_iter_loss: 0.22478939592838287
train_iter_loss: 0.14409875869750977
train_iter_loss: 0.37534427642822266
train_iter_loss: 0.3538454473018646
train_iter_loss: 0.2801301181316376
train_iter_loss: 0.27299198508262634
train_iter_loss: 0.2957020103931427
train_iter_loss: 0.267535924911499
train_iter_loss: 0.33199360966682434
train_iter_loss: 0.2868429720401764
train_iter_loss: 0.26108866930007935
train_iter_loss: 0.33121415972709656
train_iter_loss: 0.10970839858055115
train_iter_loss: 0.3112649619579315
train_iter_loss: 0.3254151940345764
train_iter_loss: 0.14735855162143707
train_iter_loss: 0.21410055458545685
train_iter_loss: 0.24869440495967865
train loss :0.2699
---------------------
Validation seg loss: 0.3530490935938257 at epoch 563
epoch =    564/  1000, exp = train
train_iter_loss: 0.37058043479919434
train_iter_loss: 0.2605646252632141
train_iter_loss: 0.36659860610961914
train_iter_loss: 0.36354371905326843
train_iter_loss: 0.1400492638349533
train_iter_loss: 0.5097553133964539
train_iter_loss: 0.20796725153923035
train_iter_loss: 0.1982688307762146
train_iter_loss: 0.2964020371437073
train_iter_loss: 0.31521520018577576
train_iter_loss: 0.1442524939775467
train_iter_loss: 0.27376848459243774
train_iter_loss: 0.16315332055091858
train_iter_loss: 0.41214266419410706
train_iter_loss: 0.25714248418807983
train_iter_loss: 0.3296346664428711
train_iter_loss: 0.2455650418996811
train_iter_loss: 0.19285643100738525
train_iter_loss: 0.18139563500881195
train_iter_loss: 0.307376503944397
train_iter_loss: 0.2404860407114029
train_iter_loss: 0.2710624933242798
train_iter_loss: 0.2596082389354706
train_iter_loss: 0.22697319090366364
train_iter_loss: 0.3102395832538605
train_iter_loss: 0.1653529554605484
train_iter_loss: 0.3048734962940216
train_iter_loss: 0.42060279846191406
train_iter_loss: 0.2628512978553772
train_iter_loss: 0.14758463203907013
train_iter_loss: 0.2444899082183838
train_iter_loss: 0.23130737245082855
train_iter_loss: 0.21247093379497528
train_iter_loss: 0.4434810280799866
train_iter_loss: 0.29353955388069153
train_iter_loss: 0.32993820309638977
train_iter_loss: 0.2400193214416504
train_iter_loss: 0.43763870000839233
train_iter_loss: 0.15439234673976898
train_iter_loss: 0.25598812103271484
train_iter_loss: 0.35179683566093445
train_iter_loss: 0.3819839358329773
train_iter_loss: 0.27980658411979675
train_iter_loss: 0.1596468985080719
train_iter_loss: 0.2760205566883087
train_iter_loss: 0.14997859299182892
train_iter_loss: 0.2953159511089325
train_iter_loss: 0.4256194233894348
train_iter_loss: 0.25995904207229614
train_iter_loss: 0.23490557074546814
train_iter_loss: 0.29825085401535034
train_iter_loss: 0.31887146830558777
train_iter_loss: 0.25473687052726746
train_iter_loss: 0.2615317702293396
train_iter_loss: 0.14872120320796967
train_iter_loss: 0.21754543483257294
train_iter_loss: 0.2381700575351715
train_iter_loss: 0.2540927529335022
train_iter_loss: 0.18552401661872864
train_iter_loss: 0.19710148870944977
train_iter_loss: 0.19915080070495605
train_iter_loss: 0.23197510838508606
train_iter_loss: 0.28194376826286316
train_iter_loss: 0.22307121753692627
train_iter_loss: 0.18533119559288025
train_iter_loss: 0.22589011490345
train_iter_loss: 0.4048994183540344
train_iter_loss: 0.2489696443080902
train_iter_loss: 0.11903450638055801
train_iter_loss: 0.12858422100543976
train_iter_loss: 0.18926505744457245
train_iter_loss: 0.26626282930374146
train_iter_loss: 0.34734126925468445
train_iter_loss: 0.2947453260421753
train_iter_loss: 0.4254659414291382
train_iter_loss: 0.25275591015815735
train_iter_loss: 0.1937282681465149
train_iter_loss: 0.1587872952222824
train_iter_loss: 0.3923174738883972
train_iter_loss: 0.32801681756973267
train_iter_loss: 0.33679649233818054
train_iter_loss: 0.2363191694021225
train_iter_loss: 0.4102446436882019
train_iter_loss: 0.11480511724948883
train_iter_loss: 0.14534521102905273
train_iter_loss: 0.2612636685371399
train_iter_loss: 0.17322617769241333
train_iter_loss: 0.2839760184288025
train_iter_loss: 0.06837816536426544
train_iter_loss: 0.1576213389635086
train_iter_loss: 0.26986807584762573
train_iter_loss: 0.33979207277297974
train_iter_loss: 0.22788268327713013
train_iter_loss: 0.29696932435035706
train_iter_loss: 0.2761770188808441
train_iter_loss: 0.3157408535480499
train_iter_loss: 0.3637741804122925
train_iter_loss: 0.20081771910190582
train_iter_loss: 0.087459035217762
train_iter_loss: 0.3568003475666046
train loss :0.2647
---------------------
Validation seg loss: 0.36614174843010194 at epoch 564
epoch =    565/  1000, exp = train
train_iter_loss: 0.37669965624809265
train_iter_loss: 0.1976308524608612
train_iter_loss: 0.28959527611732483
train_iter_loss: 0.3432263433933258
train_iter_loss: 0.38816604018211365
train_iter_loss: 0.14853882789611816
train_iter_loss: 0.3753865957260132
train_iter_loss: 0.3056018650531769
train_iter_loss: 0.20495255291461945
train_iter_loss: 0.23362715542316437
train_iter_loss: 0.3023366928100586
train_iter_loss: 0.319284588098526
train_iter_loss: 0.27547094225883484
train_iter_loss: 0.2555621564388275
train_iter_loss: 0.29078131914138794
train_iter_loss: 0.14780884981155396
train_iter_loss: 0.38835546374320984
train_iter_loss: 0.17965643107891083
train_iter_loss: 0.25076615810394287
train_iter_loss: 0.2791285216808319
train_iter_loss: 0.22231978178024292
train_iter_loss: 0.42466306686401367
train_iter_loss: 0.2785124182701111
train_iter_loss: 0.19661274552345276
train_iter_loss: 0.22357819974422455
train_iter_loss: 0.22075501084327698
train_iter_loss: 0.36676889657974243
train_iter_loss: 0.3175494074821472
train_iter_loss: 0.28249531984329224
train_iter_loss: 0.17605382204055786
train_iter_loss: 0.43204042315483093
train_iter_loss: 0.33350685238838196
train_iter_loss: 0.22610901296138763
train_iter_loss: 0.22684849798679352
train_iter_loss: 0.4239014685153961
train_iter_loss: 0.1601286679506302
train_iter_loss: 0.3125774562358856
train_iter_loss: 0.29466667771339417
train_iter_loss: 0.3353594243526459
train_iter_loss: 0.3336145877838135
train_iter_loss: 0.1836584359407425
train_iter_loss: 0.20663608610630035
train_iter_loss: 0.05613371729850769
train_iter_loss: 0.1927683800458908
train_iter_loss: 0.22730939090251923
train_iter_loss: 0.25049808621406555
train_iter_loss: 0.1737828552722931
train_iter_loss: 0.3058820068836212
train_iter_loss: 0.29585665464401245
train_iter_loss: 0.23222284018993378
train_iter_loss: 0.38111501932144165
train_iter_loss: 0.17731818556785583
train_iter_loss: 0.19505928456783295
train_iter_loss: 0.3521500825881958
train_iter_loss: 0.24580897390842438
train_iter_loss: 0.31032896041870117
train_iter_loss: 0.283145010471344
train_iter_loss: 0.36787235736846924
train_iter_loss: 0.2768976092338562
train_iter_loss: 0.3099307119846344
train_iter_loss: 0.23283150792121887
train_iter_loss: 0.2738036811351776
train_iter_loss: 0.17749238014221191
train_iter_loss: 0.1973825991153717
train_iter_loss: 0.29768961668014526
train_iter_loss: 0.14728842675685883
train_iter_loss: 0.11459332704544067
train_iter_loss: 0.23458975553512573
train_iter_loss: 0.28410571813583374
train_iter_loss: 0.18219897150993347
train_iter_loss: 0.2450130134820938
train_iter_loss: 0.08015277981758118
train_iter_loss: 0.2586956322193146
train_iter_loss: 0.20646092295646667
train_iter_loss: 0.29468080401420593
train_iter_loss: 0.28621432185173035
train_iter_loss: 0.07412844151258469
train_iter_loss: 0.3118220269680023
train_iter_loss: 0.342746764421463
train_iter_loss: 0.23735648393630981
train_iter_loss: 0.3977963328361511
train_iter_loss: 0.4041135013103485
train_iter_loss: 0.18273600935935974
train_iter_loss: 0.22506465017795563
train_iter_loss: 0.2925820052623749
train_iter_loss: 0.26629582047462463
train_iter_loss: 0.2605278491973877
train_iter_loss: 0.2293158620595932
train_iter_loss: 0.38789868354797363
train_iter_loss: 0.15235865116119385
train_iter_loss: 0.41764336824417114
train_iter_loss: 0.21180720627307892
train_iter_loss: 0.23141583800315857
train_iter_loss: 0.3397447466850281
train_iter_loss: 0.23551149666309357
train_iter_loss: 0.14297328889369965
train_iter_loss: 0.20712313055992126
train_iter_loss: 0.13400104641914368
train_iter_loss: 0.23159842193126678
train_iter_loss: 0.22539125382900238
train loss :0.2628
---------------------
Validation seg loss: 0.40238560198472356 at epoch 565
epoch =    566/  1000, exp = train
train_iter_loss: 0.2550497353076935
train_iter_loss: 0.3006149232387543
train_iter_loss: 0.21630394458770752
train_iter_loss: 0.19650551676750183
train_iter_loss: 0.4058389365673065
train_iter_loss: 0.3722483515739441
train_iter_loss: 0.08385910838842392
train_iter_loss: 0.13142339885234833
train_iter_loss: 0.2497602254152298
train_iter_loss: 0.19535906612873077
train_iter_loss: 0.39203205704689026
train_iter_loss: 0.373049795627594
train_iter_loss: 0.37610095739364624
train_iter_loss: 0.2558412551879883
train_iter_loss: 0.36812305450439453
train_iter_loss: 0.12583386898040771
train_iter_loss: 0.1949928104877472
train_iter_loss: 0.24754531681537628
train_iter_loss: 0.325321227312088
train_iter_loss: 0.31240981817245483
train_iter_loss: 0.19959688186645508
train_iter_loss: 0.22639617323875427
train_iter_loss: 0.39486849308013916
train_iter_loss: 0.29064884781837463
train_iter_loss: 0.17452727258205414
train_iter_loss: 0.2206840217113495
train_iter_loss: 0.18089528381824493
train_iter_loss: 0.24307619035243988
train_iter_loss: 0.2156432718038559
train_iter_loss: 0.41251546144485474
train_iter_loss: 0.2547045946121216
train_iter_loss: 0.1834995001554489
train_iter_loss: 0.24477222561836243
train_iter_loss: 0.26460182666778564
train_iter_loss: 0.1798432171344757
train_iter_loss: 0.27336621284484863
train_iter_loss: 0.15220876038074493
train_iter_loss: 0.22684364020824432
train_iter_loss: 0.2574109733104706
train_iter_loss: 0.1315997838973999
train_iter_loss: 0.2978903353214264
train_iter_loss: 0.19319842755794525
train_iter_loss: 0.33453795313835144
train_iter_loss: 0.32809171080589294
train_iter_loss: 0.2979167103767395
train_iter_loss: 0.27445679903030396
train_iter_loss: 0.36026808619499207
train_iter_loss: 0.28794947266578674
train_iter_loss: 0.3476053774356842
train_iter_loss: 0.2516453266143799
train_iter_loss: 0.27811452746391296
train_iter_loss: 0.18960489332675934
train_iter_loss: 0.24817298352718353
train_iter_loss: 0.33953964710235596
train_iter_loss: 0.30254194140434265
train_iter_loss: 0.17086145281791687
train_iter_loss: 0.2649717628955841
train_iter_loss: 0.17643582820892334
train_iter_loss: 0.1385820060968399
train_iter_loss: 0.2837425768375397
train_iter_loss: 0.3314451575279236
train_iter_loss: 0.22459784150123596
train_iter_loss: 0.28716281056404114
train_iter_loss: 0.2373865842819214
train_iter_loss: 0.28040844202041626
train_iter_loss: 0.34384360909461975
train_iter_loss: 0.32634907960891724
train_iter_loss: 0.36856693029403687
train_iter_loss: 0.1288597583770752
train_iter_loss: 0.34966325759887695
train_iter_loss: 0.1768612116575241
train_iter_loss: 0.14683005213737488
train_iter_loss: 0.21762602031230927
train_iter_loss: 0.43538206815719604
train_iter_loss: 0.2947613596916199
train_iter_loss: 0.2136416882276535
train_iter_loss: 0.2844449579715729
train_iter_loss: 0.2379790097475052
train_iter_loss: 0.3289608359336853
train_iter_loss: 0.2888447344303131
train_iter_loss: 0.35826247930526733
train_iter_loss: 0.24366694688796997
train_iter_loss: 0.20871084928512573
train_iter_loss: 0.3658771216869354
train_iter_loss: 0.3052196800708771
train_iter_loss: 0.2967773973941803
train_iter_loss: 0.3762957453727722
train_iter_loss: 0.45281732082366943
train_iter_loss: 0.2936941683292389
train_iter_loss: 0.30943048000335693
train_iter_loss: 0.31339606642723083
train_iter_loss: 0.2332918494939804
train_iter_loss: 0.25452595949172974
train_iter_loss: 0.2322361171245575
train_iter_loss: 0.15540973842144012
train_iter_loss: 0.43511348962783813
train_iter_loss: 0.220925971865654
train_iter_loss: 0.24355842173099518
train_iter_loss: 0.2991598844528198
train_iter_loss: 0.27862823009490967
train loss :0.2712
---------------------
Validation seg loss: 0.3670640548445144 at epoch 566
epoch =    567/  1000, exp = train
train_iter_loss: 0.10030412673950195
train_iter_loss: 0.2835655212402344
train_iter_loss: 0.5209127068519592
train_iter_loss: 0.41752517223358154
train_iter_loss: 0.24189087748527527
train_iter_loss: 0.39707329869270325
train_iter_loss: 0.1880498081445694
train_iter_loss: 0.34518352150917053
train_iter_loss: 0.21208633482456207
train_iter_loss: 0.2753347158432007
train_iter_loss: 0.3356611132621765
train_iter_loss: 0.2588472366333008
train_iter_loss: 0.29843515157699585
train_iter_loss: 0.32975655794143677
train_iter_loss: 0.21462813019752502
train_iter_loss: 0.1634083241224289
train_iter_loss: 0.29582086205482483
train_iter_loss: 0.20154236257076263
train_iter_loss: 0.21083441376686096
train_iter_loss: 0.14002186059951782
train_iter_loss: 0.26918646693229675
train_iter_loss: 0.263660728931427
train_iter_loss: 0.14567379653453827
train_iter_loss: 0.26812875270843506
train_iter_loss: 0.20089967548847198
train_iter_loss: 0.17556868493556976
train_iter_loss: 0.2564018964767456
train_iter_loss: 0.383278489112854
train_iter_loss: 0.17349037528038025
train_iter_loss: 0.3016641139984131
train_iter_loss: 0.13833598792552948
train_iter_loss: 0.1351873278617859
train_iter_loss: 0.37886175513267517
train_iter_loss: 0.36272355914115906
train_iter_loss: 0.48052501678466797
train_iter_loss: 0.37629464268684387
train_iter_loss: 0.2348327934741974
train_iter_loss: 0.42748716473579407
train_iter_loss: 0.17644178867340088
train_iter_loss: 0.24208366870880127
train_iter_loss: 0.3189074397087097
train_iter_loss: 0.10314241051673889
train_iter_loss: 0.2957252562046051
train_iter_loss: 0.440860778093338
train_iter_loss: 0.2658906877040863
train_iter_loss: 0.1560148298740387
train_iter_loss: 0.3180665373802185
train_iter_loss: 0.17313963174819946
train_iter_loss: 0.2281268984079361
train_iter_loss: 0.25099101662635803
train_iter_loss: 0.16582930088043213
train_iter_loss: 0.2720473110675812
train_iter_loss: 0.2092086672782898
train_iter_loss: 0.3164044916629791
train_iter_loss: 0.1953980028629303
train_iter_loss: 0.20808078348636627
train_iter_loss: 0.26764678955078125
train_iter_loss: 0.2189410775899887
train_iter_loss: 0.19997119903564453
train_iter_loss: 0.38114893436431885
train_iter_loss: 0.13209889829158783
train_iter_loss: 0.1688375622034073
train_iter_loss: 0.18683935701847076
train_iter_loss: 0.27687007188796997
train_iter_loss: 0.19161845743656158
train_iter_loss: 0.2943769097328186
train_iter_loss: 0.07058822363615036
train_iter_loss: 0.2500452697277069
train_iter_loss: 0.46590715646743774
train_iter_loss: 0.265484094619751
train_iter_loss: 0.2538549602031708
train_iter_loss: 0.2357987016439438
train_iter_loss: 0.2737272381782532
train_iter_loss: 0.4289374053478241
train_iter_loss: 0.2606363296508789
train_iter_loss: 0.25717735290527344
train_iter_loss: 0.1777530014514923
train_iter_loss: 0.2693760097026825
train_iter_loss: 0.4148261845111847
train_iter_loss: 0.16586704552173615
train_iter_loss: 0.263776957988739
train_iter_loss: 0.3449161648750305
train_iter_loss: 0.22273147106170654
train_iter_loss: 0.17412157356739044
train_iter_loss: 0.30237647891044617
train_iter_loss: 0.2703886926174164
train_iter_loss: 0.293075829744339
train_iter_loss: 0.3376885950565338
train_iter_loss: 0.2891055643558502
train_iter_loss: 0.337156742811203
train_iter_loss: 0.300935298204422
train_iter_loss: 0.43180036544799805
train_iter_loss: 0.36020782589912415
train_iter_loss: 0.30397874116897583
train_iter_loss: 0.15953315794467926
train_iter_loss: 0.20935209095478058
train_iter_loss: 0.3059336543083191
train_iter_loss: 0.42254334688186646
train_iter_loss: 0.19553978741168976
train_iter_loss: 0.2780294418334961
train loss :0.2692
---------------------
Validation seg loss: 0.36503967442461904 at epoch 567
epoch =    568/  1000, exp = train
train_iter_loss: 0.41547098755836487
train_iter_loss: 0.1789214015007019
train_iter_loss: 0.30993568897247314
train_iter_loss: 0.3118821978569031
train_iter_loss: 0.3177095949649811
train_iter_loss: 0.1452093869447708
train_iter_loss: 0.23614563047885895
train_iter_loss: 0.484466016292572
train_iter_loss: 0.4496762454509735
train_iter_loss: 0.228328675031662
train_iter_loss: 0.5078906416893005
train_iter_loss: 0.2594067454338074
train_iter_loss: 0.18593113124370575
train_iter_loss: 0.20798541605472565
train_iter_loss: 0.2103273570537567
train_iter_loss: 0.276388555765152
train_iter_loss: 0.22337034344673157
train_iter_loss: 0.18252843618392944
train_iter_loss: 0.2934337854385376
train_iter_loss: 0.23955243825912476
train_iter_loss: 0.22822251915931702
train_iter_loss: 0.2694586217403412
train_iter_loss: 0.2707180380821228
train_iter_loss: 0.20178361237049103
train_iter_loss: 0.5119131803512573
train_iter_loss: 0.21356676518917084
train_iter_loss: 0.11630052328109741
train_iter_loss: 0.2574973404407501
train_iter_loss: 0.18694423139095306
train_iter_loss: 0.22729595005512238
train_iter_loss: 0.2005946785211563
train_iter_loss: 0.1957857459783554
train_iter_loss: 0.22347266972064972
train_iter_loss: 0.4136536121368408
train_iter_loss: 0.25361016392707825
train_iter_loss: 0.39667433500289917
train_iter_loss: 0.4221748113632202
train_iter_loss: 0.2511533200740814
train_iter_loss: 0.1744777262210846
train_iter_loss: 0.1208883672952652
train_iter_loss: 0.17994628846645355
train_iter_loss: 0.5934069752693176
train_iter_loss: 0.2210417538881302
train_iter_loss: 0.20356003940105438
train_iter_loss: 0.33863767981529236
train_iter_loss: 0.18020132184028625
train_iter_loss: 0.2142839878797531
train_iter_loss: 0.3446332514286041
train_iter_loss: 0.23987866938114166
train_iter_loss: 0.37879711389541626
train_iter_loss: 0.2699538767337799
train_iter_loss: 0.3742590546607971
train_iter_loss: 0.35609006881713867
train_iter_loss: 0.2787376642227173
train_iter_loss: 0.3142544627189636
train_iter_loss: 0.18030326068401337
train_iter_loss: 0.3581981062889099
train_iter_loss: 0.3172919750213623
train_iter_loss: 0.31550467014312744
train_iter_loss: 0.13586744666099548
train_iter_loss: 0.12515397369861603
train_iter_loss: 0.19006554782390594
train_iter_loss: 0.2787517011165619
train_iter_loss: 0.3172977864742279
train_iter_loss: 0.26664435863494873
train_iter_loss: 0.16614855825901031
train_iter_loss: 0.29425081610679626
train_iter_loss: 0.23155923187732697
train_iter_loss: 0.22355632483959198
train_iter_loss: 0.22405748069286346
train_iter_loss: 0.2635399401187897
train_iter_loss: 0.23529580235481262
train_iter_loss: 0.25015825033187866
train_iter_loss: 0.27414244413375854
train_iter_loss: 0.4093210995197296
train_iter_loss: 0.3020859956741333
train_iter_loss: 0.11681399494409561
train_iter_loss: 0.29891571402549744
train_iter_loss: 0.2789383828639984
train_iter_loss: 0.5130597949028015
train_iter_loss: 0.35290488600730896
train_iter_loss: 0.21278144419193268
train_iter_loss: 0.2554912865161896
train_iter_loss: 0.31357041001319885
train_iter_loss: 0.42143306136131287
train_iter_loss: 0.09267869591712952
train_iter_loss: 0.4367351531982422
train_iter_loss: 0.20273157954216003
train_iter_loss: 0.4141298532485962
train_iter_loss: 0.18241587281227112
train_iter_loss: 0.14940142631530762
train_iter_loss: 0.25672417879104614
train_iter_loss: 0.18513014912605286
train_iter_loss: 0.3174179494380951
train_iter_loss: 0.4073392450809479
train_iter_loss: 0.38725847005844116
train_iter_loss: 0.24077853560447693
train_iter_loss: 0.18624438345432281
train_iter_loss: 0.31627967953681946
train_iter_loss: 0.1559136062860489
train loss :0.2761
---------------------
Validation seg loss: 0.3466861197145058 at epoch 568
********************
best_val_epoch_loss:  0.3466861197145058
MODEL UPDATED
epoch =    569/  1000, exp = train
train_iter_loss: 0.11549827456474304
train_iter_loss: 0.237842857837677
train_iter_loss: 0.10683222115039825
train_iter_loss: 0.3601633310317993
train_iter_loss: 0.22826054692268372
train_iter_loss: 0.16792404651641846
train_iter_loss: 0.33368080854415894
train_iter_loss: 0.27280721068382263
train_iter_loss: 0.3656140863895416
train_iter_loss: 0.28023841977119446
train_iter_loss: 0.17141814529895782
train_iter_loss: 0.25736525654792786
train_iter_loss: 0.27797165513038635
train_iter_loss: 0.34601065516471863
train_iter_loss: 0.22563308477401733
train_iter_loss: 0.43727514147758484
train_iter_loss: 0.2800969183444977
train_iter_loss: 0.2769351303577423
train_iter_loss: 0.2845432162284851
train_iter_loss: 0.2199486941099167
train_iter_loss: 0.41023123264312744
train_iter_loss: 0.31802093982696533
train_iter_loss: 0.28759080171585083
train_iter_loss: 0.2483413815498352
train_iter_loss: 0.3308509290218353
train_iter_loss: 0.24568945169448853
train_iter_loss: 0.25337594747543335
train_iter_loss: 0.31541958451271057
train_iter_loss: 0.19166028499603271
train_iter_loss: 0.3145255446434021
train_iter_loss: 0.32386571168899536
train_iter_loss: 0.28797227144241333
train_iter_loss: 0.24115583300590515
train_iter_loss: 0.19040562212467194
train_iter_loss: 0.2372428923845291
train_iter_loss: 0.20509397983551025
train_iter_loss: 0.37066197395324707
train_iter_loss: 0.195697620511055
train_iter_loss: 0.0824192613363266
train_iter_loss: 0.33753466606140137
train_iter_loss: 0.3001105487346649
train_iter_loss: 0.16477997601032257
train_iter_loss: 0.2176090031862259
train_iter_loss: 0.3558658957481384
train_iter_loss: 0.3905435800552368
train_iter_loss: 0.2014206200838089
train_iter_loss: 0.3717309832572937
train_iter_loss: 0.2605850100517273
train_iter_loss: 0.17205151915550232
train_iter_loss: 0.30687597393989563
train_iter_loss: 0.29884007573127747
train_iter_loss: 0.3523889482021332
train_iter_loss: 0.3871721625328064
train_iter_loss: 0.25094330310821533
train_iter_loss: 0.161857008934021
train_iter_loss: 0.3585098385810852
train_iter_loss: 0.263644278049469
train_iter_loss: 0.2084762006998062
train_iter_loss: 0.33523792028427124
train_iter_loss: 0.22207918763160706
train_iter_loss: 0.23953275382518768
train_iter_loss: 0.18101899325847626
train_iter_loss: 0.23886293172836304
train_iter_loss: 0.2560197114944458
train_iter_loss: 0.25772181153297424
train_iter_loss: 0.5759049654006958
train_iter_loss: 0.24457810819149017
train_iter_loss: 0.21910206973552704
train_iter_loss: 0.2791973948478699
train_iter_loss: 0.26778268814086914
train_iter_loss: 0.2829377055168152
train_iter_loss: 0.31857171654701233
train_iter_loss: 0.3240453898906708
train_iter_loss: 0.360219806432724
train_iter_loss: 0.3281385898590088
train_iter_loss: 0.1499219536781311
train_iter_loss: 0.2479565292596817
train_iter_loss: 0.2794252634048462
train_iter_loss: 0.3593752682209015
train_iter_loss: 0.12134355306625366
train_iter_loss: 0.28179284930229187
train_iter_loss: 0.2392229437828064
train_iter_loss: 0.13349077105522156
train_iter_loss: 0.18652556836605072
train_iter_loss: 0.3796447813510895
train_iter_loss: 0.14690503478050232
train_iter_loss: 0.2927335500717163
train_iter_loss: 0.22971411049365997
train_iter_loss: 0.3624012768268585
train_iter_loss: 0.4747503697872162
train_iter_loss: 0.32070738077163696
train_iter_loss: 0.3339484632015228
train_iter_loss: 0.16505137085914612
train_iter_loss: 0.17051619291305542
train_iter_loss: 0.28272756934165955
train_iter_loss: 0.11018942296504974
train_iter_loss: 0.2636916935443878
train_iter_loss: 0.18123048543930054
train_iter_loss: 0.24045802652835846
train_iter_loss: 0.19037535786628723
train loss :0.2701
---------------------
Validation seg loss: 0.4362962546073041 at epoch 569
epoch =    570/  1000, exp = train
train_iter_loss: 0.41803693771362305
train_iter_loss: 0.28106966614723206
train_iter_loss: 0.2846709191799164
train_iter_loss: 0.14814229309558868
train_iter_loss: 0.3127882778644562
train_iter_loss: 0.30985507369041443
train_iter_loss: 0.25916406512260437
train_iter_loss: 0.19649994373321533
train_iter_loss: 0.28493577241897583
train_iter_loss: 0.37783122062683105
train_iter_loss: 0.24771402776241302
train_iter_loss: 0.2081054151058197
train_iter_loss: 0.41485074162483215
train_iter_loss: 0.32519325613975525
train_iter_loss: 0.3446621596813202
train_iter_loss: 0.3337075114250183
train_iter_loss: 0.2855333387851715
train_iter_loss: 0.38488516211509705
train_iter_loss: 0.21427536010742188
train_iter_loss: 0.23771128058433533
train_iter_loss: 0.2755318880081177
train_iter_loss: 0.22862553596496582
train_iter_loss: 0.26701509952545166
train_iter_loss: 0.3223106265068054
train_iter_loss: 0.21624743938446045
train_iter_loss: 0.2100355178117752
train_iter_loss: 0.17345717549324036
train_iter_loss: 0.2615242898464203
train_iter_loss: 0.4022137522697449
train_iter_loss: 0.08094406127929688
train_iter_loss: 0.18559537827968597
train_iter_loss: 0.17789186537265778
train_iter_loss: 0.3522964119911194
train_iter_loss: 0.22224090993404388
train_iter_loss: 0.20039448142051697
train_iter_loss: 0.2117774933576584
train_iter_loss: 0.14107051491737366
train_iter_loss: 0.27286607027053833
train_iter_loss: 0.25615087151527405
train_iter_loss: 0.19414736330509186
train_iter_loss: 0.1676199734210968
train_iter_loss: 0.19888733327388763
train_iter_loss: 0.3044453263282776
train_iter_loss: 0.20387479662895203
train_iter_loss: 0.3558970093727112
train_iter_loss: 0.3800864815711975
train_iter_loss: 0.25001052021980286
train_iter_loss: 0.11338842660188675
train_iter_loss: 0.29005593061447144
train_iter_loss: 0.34732985496520996
train_iter_loss: 0.40190982818603516
train_iter_loss: 0.22497081756591797
train_iter_loss: 0.2750740051269531
train_iter_loss: 0.17817309498786926
train_iter_loss: 0.3059578537940979
train_iter_loss: 0.2211332470178604
train_iter_loss: 0.4063747823238373
train_iter_loss: 0.3063439726829529
train_iter_loss: 0.2800471782684326
train_iter_loss: 0.34321513772010803
train_iter_loss: 0.28435230255126953
train_iter_loss: 0.38231635093688965
train_iter_loss: 0.16766341030597687
train_iter_loss: 0.329386442899704
train_iter_loss: 0.3135014772415161
train_iter_loss: 0.13736945390701294
train_iter_loss: 0.40700381994247437
train_iter_loss: 0.3319280445575714
train_iter_loss: 0.19962330162525177
train_iter_loss: 0.37660637497901917
train_iter_loss: 0.17399096488952637
train_iter_loss: 0.3618641197681427
train_iter_loss: 0.1480972170829773
train_iter_loss: 0.1162116602063179
train_iter_loss: 0.3326680362224579
train_iter_loss: 0.40188589692115784
train_iter_loss: 0.1481178253889084
train_iter_loss: 0.2904210388660431
train_iter_loss: 0.27676519751548767
train_iter_loss: 0.24260547757148743
train_iter_loss: 0.3178558349609375
train_iter_loss: 0.290798157453537
train_iter_loss: 0.07723749428987503
train_iter_loss: 0.28204891085624695
train_iter_loss: 0.48905321955680847
train_iter_loss: 0.10972213745117188
train_iter_loss: 0.20387955009937286
train_iter_loss: 0.3032034933567047
train_iter_loss: 0.32033562660217285
train_iter_loss: 0.2048678696155548
train_iter_loss: 0.2629367709159851
train_iter_loss: 0.15487128496170044
train_iter_loss: 0.1188206896185875
train_iter_loss: 0.32355013489723206
train_iter_loss: 0.29298293590545654
train_iter_loss: 0.22950156033039093
train_iter_loss: 0.2783697247505188
train_iter_loss: 0.2864571809768677
train_iter_loss: 0.4875830113887787
train_iter_loss: 0.07587017118930817
train loss :0.2688
---------------------
Validation seg loss: 0.3541270989379933 at epoch 570
epoch =    571/  1000, exp = train
train_iter_loss: 0.21682851016521454
train_iter_loss: 0.23610952496528625
train_iter_loss: 0.2650758624076843
train_iter_loss: 0.1803160309791565
train_iter_loss: 0.2779535949230194
train_iter_loss: 0.33470022678375244
train_iter_loss: 0.32800552248954773
train_iter_loss: 0.19543690979480743
train_iter_loss: 0.19512055814266205
train_iter_loss: 0.36798688769340515
train_iter_loss: 0.4532473683357239
train_iter_loss: 0.27083325386047363
train_iter_loss: 0.30235257744789124
train_iter_loss: 0.2954055964946747
train_iter_loss: 0.2589423954486847
train_iter_loss: 0.3800355792045593
train_iter_loss: 0.26367247104644775
train_iter_loss: 0.3125273883342743
train_iter_loss: 0.22530056536197662
train_iter_loss: 0.3498798906803131
train_iter_loss: 0.39813661575317383
train_iter_loss: 0.11338620632886887
train_iter_loss: 0.17091892659664154
train_iter_loss: 0.31146082282066345
train_iter_loss: 0.2294115573167801
train_iter_loss: 0.2659013867378235
train_iter_loss: 0.19793933629989624
train_iter_loss: 0.17323900759220123
train_iter_loss: 0.4245111048221588
train_iter_loss: 0.19001546502113342
train_iter_loss: 0.25918176770210266
train_iter_loss: 0.18104399740695953
train_iter_loss: 0.2099643349647522
train_iter_loss: 0.3181966245174408
train_iter_loss: 0.2848008871078491
train_iter_loss: 0.2644246816635132
train_iter_loss: 0.21945691108703613
train_iter_loss: 0.15738606452941895
train_iter_loss: 0.27280256152153015
train_iter_loss: 0.37858709692955017
train_iter_loss: 0.31727340817451477
train_iter_loss: 0.283933162689209
train_iter_loss: 0.22262398898601532
train_iter_loss: 0.24090643227100372
train_iter_loss: 0.35205328464508057
train_iter_loss: 0.2812419831752777
train_iter_loss: 0.2911704182624817
train_iter_loss: 0.25451746582984924
train_iter_loss: 0.13821496069431305
train_iter_loss: 0.24982884526252747
train_iter_loss: 0.2830987274646759
train_iter_loss: 0.37464553117752075
train_iter_loss: 0.3051745295524597
train_iter_loss: 0.25014743208885193
train_iter_loss: 0.26917532086372375
train_iter_loss: 0.27125877141952515
train_iter_loss: 0.1583443135023117
train_iter_loss: 0.29472067952156067
train_iter_loss: 0.2031889110803604
train_iter_loss: 0.4099452793598175
train_iter_loss: 0.2003854662179947
train_iter_loss: 0.15571826696395874
train_iter_loss: 0.157030388712883
train_iter_loss: 0.24504123628139496
train_iter_loss: 0.28593677282333374
train_iter_loss: 0.1829000562429428
train_iter_loss: 0.6167367100715637
train_iter_loss: 0.15406262874603271
train_iter_loss: 0.3286718428134918
train_iter_loss: 0.2364506870508194
train_iter_loss: 0.1997663527727127
train_iter_loss: 0.3890898823738098
train_iter_loss: 0.23817621171474457
train_iter_loss: 0.2153320461511612
train_iter_loss: 0.3988342583179474
train_iter_loss: 0.2859911620616913
train_iter_loss: 0.2154432237148285
train_iter_loss: 0.22112013399600983
train_iter_loss: 0.24747049808502197
train_iter_loss: 0.31329530477523804
train_iter_loss: 0.22772476077079773
train_iter_loss: 0.43457186222076416
train_iter_loss: 0.22763410210609436
train_iter_loss: 0.23873889446258545
train_iter_loss: 0.3666260242462158
train_iter_loss: 0.1682923585176468
train_iter_loss: 0.15786482393741608
train_iter_loss: 0.41552847623825073
train_iter_loss: 0.24672849476337433
train_iter_loss: 0.3368411362171173
train_iter_loss: 0.40206220746040344
train_iter_loss: 0.35937580466270447
train_iter_loss: 0.18473568558692932
train_iter_loss: 0.12811243534088135
train_iter_loss: 0.3411693572998047
train_iter_loss: 0.23474083840847015
train_iter_loss: 0.2232649326324463
train_iter_loss: 0.11878771334886551
train_iter_loss: 0.3160535395145416
train_iter_loss: 0.23665963113307953
train loss :0.2710
---------------------
Validation seg loss: 0.35409105042839106 at epoch 571
epoch =    572/  1000, exp = train
train_iter_loss: 0.3948253393173218
train_iter_loss: 0.3082139790058136
train_iter_loss: 0.1745126098394394
train_iter_loss: 0.36504316329956055
train_iter_loss: 0.2812242805957794
train_iter_loss: 0.28130242228507996
train_iter_loss: 0.39556553959846497
train_iter_loss: 0.28857842087745667
train_iter_loss: 0.2671512961387634
train_iter_loss: 0.1940491795539856
train_iter_loss: 0.28884628415107727
train_iter_loss: 0.18453873693943024
train_iter_loss: 0.3457169830799103
train_iter_loss: 0.417436808347702
train_iter_loss: 0.29945072531700134
train_iter_loss: 0.18610763549804688
train_iter_loss: 0.23874826729297638
train_iter_loss: 0.2877159118652344
train_iter_loss: 0.36573225259780884
train_iter_loss: 0.3233923614025116
train_iter_loss: 0.2761826813220978
train_iter_loss: 0.16848242282867432
train_iter_loss: 0.2528780996799469
train_iter_loss: 0.47369763255119324
train_iter_loss: 0.22277897596359253
train_iter_loss: 0.16235047578811646
train_iter_loss: 0.2690891623497009
train_iter_loss: 0.29807573556900024
train_iter_loss: 0.19679871201515198
train_iter_loss: 0.26564720273017883
train_iter_loss: 0.2441946268081665
train_iter_loss: 0.2408684343099594
train_iter_loss: 0.37396439909935
train_iter_loss: 0.21664133667945862
train_iter_loss: 0.2488134354352951
train_iter_loss: 0.23098094761371613
train_iter_loss: 0.42045992612838745
train_iter_loss: 0.22597059607505798
train_iter_loss: 0.1382424533367157
train_iter_loss: 0.3488536775112152
train_iter_loss: 0.3565188944339752
train_iter_loss: 0.2543986141681671
train_iter_loss: 0.29513585567474365
train_iter_loss: 0.19527487456798553
train_iter_loss: 0.3534274697303772
train_iter_loss: 0.1336606740951538
train_iter_loss: 0.3165854811668396
train_iter_loss: 0.2210703343153
train_iter_loss: 0.387597918510437
train_iter_loss: 0.16972634196281433
train_iter_loss: 0.05447802692651749
train_iter_loss: 0.32302847504615784
train_iter_loss: 0.18501518666744232
train_iter_loss: 0.31686532497406006
train_iter_loss: 0.3782373070716858
train_iter_loss: 0.17139935493469238
train_iter_loss: 0.29320400953292847
train_iter_loss: 0.14670784771442413
train_iter_loss: 0.17779655754566193
train_iter_loss: 0.2060687243938446
train_iter_loss: 0.2960440218448639
train_iter_loss: 0.25021684169769287
train_iter_loss: 0.3142912685871124
train_iter_loss: 0.5407113432884216
train_iter_loss: 0.3613887131214142
train_iter_loss: 0.19900743663311005
train_iter_loss: 0.22152936458587646
train_iter_loss: 0.29584309458732605
train_iter_loss: 0.19011420011520386
train_iter_loss: 0.3479509651660919
train_iter_loss: 0.36637839674949646
train_iter_loss: 0.14821872115135193
train_iter_loss: 0.4256269931793213
train_iter_loss: 0.24849896132946014
train_iter_loss: 0.25410744547843933
train_iter_loss: 0.21151457726955414
train_iter_loss: 0.22539803385734558
train_iter_loss: 0.3806767761707306
train_iter_loss: 0.24365390837192535
train_iter_loss: 0.16578204929828644
train_iter_loss: 0.3447328507900238
train_iter_loss: 0.21084529161453247
train_iter_loss: 0.06600590795278549
train_iter_loss: 0.22541221976280212
train_iter_loss: 0.35514378547668457
train_iter_loss: 0.08864825963973999
train_iter_loss: 0.18106356263160706
train_iter_loss: 0.2755841314792633
train_iter_loss: 0.3828623294830322
train_iter_loss: 0.23169702291488647
train_iter_loss: 0.18959873914718628
train_iter_loss: 0.3293352425098419
train_iter_loss: 0.30304592847824097
train_iter_loss: 0.15437008440494537
train_iter_loss: 0.4130476415157318
train_iter_loss: 0.10920015722513199
train_iter_loss: 0.33864980936050415
train_iter_loss: 0.2949611246585846
train_iter_loss: 0.3083955645561218
train_iter_loss: 0.2842932939529419
train loss :0.2714
---------------------
Validation seg loss: 0.3745248517733208 at epoch 572
epoch =    573/  1000, exp = train
train_iter_loss: 0.2878189980983734
train_iter_loss: 0.30474281311035156
train_iter_loss: 0.24013085663318634
train_iter_loss: 0.2041063904762268
train_iter_loss: 0.3278887867927551
train_iter_loss: 0.16320553421974182
train_iter_loss: 0.3812403976917267
train_iter_loss: 0.18349714577198029
train_iter_loss: 0.22924120724201202
train_iter_loss: 0.4576897919178009
train_iter_loss: 0.38722631335258484
train_iter_loss: 0.3144903779029846
train_iter_loss: 0.2285608947277069
train_iter_loss: 0.2698397934436798
train_iter_loss: 0.2789214551448822
train_iter_loss: 0.2863936722278595
train_iter_loss: 0.5504494309425354
train_iter_loss: 0.2517293095588684
train_iter_loss: 0.22572362422943115
train_iter_loss: 0.21678955852985382
train_iter_loss: 0.21559502184391022
train_iter_loss: 0.21677739918231964
train_iter_loss: 0.36332353949546814
train_iter_loss: 0.34968194365501404
train_iter_loss: 0.3383333683013916
train_iter_loss: 0.17116203904151917
train_iter_loss: 0.21358461678028107
train_iter_loss: 0.2759239077568054
train_iter_loss: 0.18586547672748566
train_iter_loss: 0.2282947152853012
train_iter_loss: 0.0876154899597168
train_iter_loss: 0.20257948338985443
train_iter_loss: 0.26939114928245544
train_iter_loss: 0.30018123984336853
train_iter_loss: 0.3153766989707947
train_iter_loss: 0.37592247128486633
train_iter_loss: 0.4370881915092468
train_iter_loss: 0.4553283751010895
train_iter_loss: 0.27294719219207764
train_iter_loss: 0.4428344666957855
train_iter_loss: 0.302768737077713
train_iter_loss: 0.15661205351352692
train_iter_loss: 0.1900387555360794
train_iter_loss: 0.4139832556247711
train_iter_loss: 0.31339165568351746
train_iter_loss: 0.13491132855415344
train_iter_loss: 0.03544830530881882
train_iter_loss: 0.4134489893913269
train_iter_loss: 0.2932749390602112
train_iter_loss: 0.1804446578025818
train_iter_loss: 0.2639549970626831
train_iter_loss: 0.11715833842754364
train_iter_loss: 0.24399304389953613
train_iter_loss: 0.2553498148918152
train_iter_loss: 0.366066038608551
train_iter_loss: 0.3008827865123749
train_iter_loss: 0.22617299854755402
train_iter_loss: 0.2644004225730896
train_iter_loss: 0.2556402385234833
train_iter_loss: 0.3464566469192505
train_iter_loss: 0.1443156599998474
train_iter_loss: 0.2357209473848343
train_iter_loss: 0.2426912635564804
train_iter_loss: 0.16225840151309967
train_iter_loss: 0.25521838665008545
train_iter_loss: 0.3350198566913605
train_iter_loss: 0.2916114926338196
train_iter_loss: 0.30027303099632263
train_iter_loss: 0.27883070707321167
train_iter_loss: 0.49105867743492126
train_iter_loss: 0.32196730375289917
train_iter_loss: 0.2288394421339035
train_iter_loss: 0.1987873762845993
train_iter_loss: 0.20578111708164215
train_iter_loss: 0.16550473868846893
train_iter_loss: 0.3029853403568268
train_iter_loss: 0.27754899859428406
train_iter_loss: 0.17625457048416138
train_iter_loss: 0.2081356793642044
train_iter_loss: 0.3863128423690796
train_iter_loss: 0.42857682704925537
train_iter_loss: 0.29483991861343384
train_iter_loss: 0.26956233382225037
train_iter_loss: 0.2704160809516907
train_iter_loss: 0.19344554841518402
train_iter_loss: 0.2877390384674072
train_iter_loss: 0.1878003627061844
train_iter_loss: 0.09087423235177994
train_iter_loss: 0.1701677292585373
train_iter_loss: 0.30310943722724915
train_iter_loss: 0.34281498193740845
train_iter_loss: 0.23003382980823517
train_iter_loss: 0.29770296812057495
train_iter_loss: 0.3841380476951599
train_iter_loss: 0.28714874386787415
train_iter_loss: 0.2586289942264557
train_iter_loss: 0.2193400114774704
train_iter_loss: 0.393973708152771
train_iter_loss: 0.1023789793252945
train_iter_loss: 0.08802734315395355
train loss :0.2716
---------------------
Validation seg loss: 0.3809046344392283 at epoch 573
epoch =    574/  1000, exp = train
train_iter_loss: 0.23468685150146484
train_iter_loss: 0.29955002665519714
train_iter_loss: 0.3083305060863495
train_iter_loss: 0.35292091965675354
train_iter_loss: 0.2734262943267822
train_iter_loss: 0.26791810989379883
train_iter_loss: 0.21350690722465515
train_iter_loss: 0.20771470665931702
train_iter_loss: 0.2031726837158203
train_iter_loss: 0.2969963252544403
train_iter_loss: 0.3090972900390625
train_iter_loss: 0.21632689237594604
train_iter_loss: 0.2519761323928833
train_iter_loss: 0.2688990831375122
train_iter_loss: 0.24504674971103668
train_iter_loss: 0.33671537041664124
train_iter_loss: 0.15265879034996033
train_iter_loss: 0.3270851969718933
train_iter_loss: 0.2544686496257782
train_iter_loss: 0.24412134289741516
train_iter_loss: 0.3622190058231354
train_iter_loss: 0.25455203652381897
train_iter_loss: 0.20692938566207886
train_iter_loss: 0.34161972999572754
train_iter_loss: 0.11349643021821976
train_iter_loss: 0.22644071280956268
train_iter_loss: 0.17951644957065582
train_iter_loss: 0.3255915939807892
train_iter_loss: 0.27690669894218445
train_iter_loss: 0.3027828335762024
train_iter_loss: 0.3406059443950653
train_iter_loss: 0.23115096986293793
train_iter_loss: 0.3554099500179291
train_iter_loss: 0.38898295164108276
train_iter_loss: 0.3261507749557495
train_iter_loss: 0.18806061148643494
train_iter_loss: 0.33704787492752075
train_iter_loss: 0.3541906177997589
train_iter_loss: 0.17596435546875
train_iter_loss: 0.4363524615764618
train_iter_loss: 0.29129424691200256
train_iter_loss: 0.2057420164346695
train_iter_loss: 0.3082732558250427
train_iter_loss: 0.3945702910423279
train_iter_loss: 0.2041262984275818
train_iter_loss: 0.13659358024597168
train_iter_loss: 0.24482126533985138
train_iter_loss: 0.20771844685077667
train_iter_loss: 0.2934890687465668
train_iter_loss: 0.16873589158058167
train_iter_loss: 0.34233367443084717
train_iter_loss: 0.3749067187309265
train_iter_loss: 0.3564694821834564
train_iter_loss: 0.1916486769914627
train_iter_loss: 0.3097377419471741
train_iter_loss: 0.3060878813266754
train_iter_loss: 0.35838207602500916
train_iter_loss: 0.30286705493927
train_iter_loss: 0.23313495516777039
train_iter_loss: 0.23850013315677643
train_iter_loss: 0.2642110586166382
train_iter_loss: 0.20431183278560638
train_iter_loss: 0.26385074853897095
train_iter_loss: 0.2302374690771103
train_iter_loss: 0.4363681375980377
train_iter_loss: 0.46178728342056274
train_iter_loss: 0.31739360094070435
train_iter_loss: 0.040931496769189835
train_iter_loss: 0.3042329251766205
train_iter_loss: 0.21305252611637115
train_iter_loss: 0.16818760335445404
train_iter_loss: 0.22776198387145996
train_iter_loss: 0.11861450970172882
train_iter_loss: 0.12181369215250015
train_iter_loss: 0.15059873461723328
train_iter_loss: 0.24748724699020386
train_iter_loss: 0.16361550986766815
train_iter_loss: 0.21295839548110962
train_iter_loss: 0.2573205828666687
train_iter_loss: 0.18142502009868622
train_iter_loss: 0.20194843411445618
train_iter_loss: 0.14070679247379303
train_iter_loss: 0.2759770452976227
train_iter_loss: 0.2445671111345291
train_iter_loss: 0.17277908325195312
train_iter_loss: 0.18141822516918182
train_iter_loss: 0.1063159704208374
train_iter_loss: 0.3283807933330536
train_iter_loss: 0.26176825165748596
train_iter_loss: 0.3472714126110077
train_iter_loss: 0.3077605366706848
train_iter_loss: 0.22568769752979279
train_iter_loss: 0.3887196481227875
train_iter_loss: 0.15206089615821838
train_iter_loss: 0.4223117232322693
train_iter_loss: 0.161781907081604
train_iter_loss: 0.3004097044467926
train_iter_loss: 0.25648611783981323
train_iter_loss: 0.2826289236545563
train_iter_loss: 0.309076189994812
train loss :0.2638
---------------------
Validation seg loss: 0.36714916785669355 at epoch 574
epoch =    575/  1000, exp = train
train_iter_loss: 0.2008017748594284
train_iter_loss: 0.3702055811882019
train_iter_loss: 0.23945201933383942
train_iter_loss: 0.2063850462436676
train_iter_loss: 0.16286124289035797
train_iter_loss: 0.1156034842133522
train_iter_loss: 0.14889824390411377
train_iter_loss: 0.3906123638153076
train_iter_loss: 0.17901378870010376
train_iter_loss: 0.3076838552951813
train_iter_loss: 0.22175991535186768
train_iter_loss: 0.19985510408878326
train_iter_loss: 0.21328294277191162
train_iter_loss: 0.2924592196941376
train_iter_loss: 0.11128883063793182
train_iter_loss: 0.24963697791099548
train_iter_loss: 0.2263571321964264
train_iter_loss: 0.19345220923423767
train_iter_loss: 0.5104882121086121
train_iter_loss: 0.23991338908672333
train_iter_loss: 0.20566721260547638
train_iter_loss: 0.36884209513664246
train_iter_loss: 0.18689361214637756
train_iter_loss: 0.3470083773136139
train_iter_loss: 0.37318316102027893
train_iter_loss: 0.17447294294834137
train_iter_loss: 0.14788828790187836
train_iter_loss: 0.18090686202049255
train_iter_loss: 0.19378340244293213
train_iter_loss: 0.15105880796909332
train_iter_loss: 0.10112426429986954
train_iter_loss: 0.195426344871521
train_iter_loss: 0.27510687708854675
train_iter_loss: 0.21651463210582733
train_iter_loss: 0.3188919723033905
train_iter_loss: 0.3073582351207733
train_iter_loss: 0.17512471973896027
train_iter_loss: 0.33124926686286926
train_iter_loss: 0.2640535533428192
train_iter_loss: 0.1822274625301361
train_iter_loss: 0.2682040333747864
train_iter_loss: 0.2779815196990967
train_iter_loss: 0.24933655560016632
train_iter_loss: 0.3112983703613281
train_iter_loss: 0.45255550742149353
train_iter_loss: 0.5032482743263245
train_iter_loss: 0.25965195894241333
train_iter_loss: 0.22642335295677185
train_iter_loss: 0.3231217563152313
train_iter_loss: 0.2949122488498688
train_iter_loss: 0.31564784049987793
train_iter_loss: 0.16939948499202728
train_iter_loss: 0.12479658424854279
train_iter_loss: 0.32132861018180847
train_iter_loss: 0.3555864989757538
train_iter_loss: 0.15933658182621002
train_iter_loss: 0.12064214795827866
train_iter_loss: 0.3791164755821228
train_iter_loss: 0.22001180052757263
train_iter_loss: 0.29745084047317505
train_iter_loss: 0.39477285742759705
train_iter_loss: 0.3741194009780884
train_iter_loss: 0.411123126745224
train_iter_loss: 0.23209795355796814
train_iter_loss: 0.3152250051498413
train_iter_loss: 0.29540032148361206
train_iter_loss: 0.17945009469985962
train_iter_loss: 0.20920626819133759
train_iter_loss: 0.36236676573753357
train_iter_loss: 0.3958013653755188
train_iter_loss: 0.28857505321502686
train_iter_loss: 0.3764571249485016
train_iter_loss: 0.4127053916454315
train_iter_loss: 0.30464521050453186
train_iter_loss: 0.25071099400520325
train_iter_loss: 0.2914455533027649
train_iter_loss: 0.3210699260234833
train_iter_loss: 0.262273371219635
train_iter_loss: 0.1452336311340332
train_iter_loss: 0.22451339662075043
train_iter_loss: 0.3394097685813904
train_iter_loss: 0.15410691499710083
train_iter_loss: 0.0929541364312172
train_iter_loss: 0.48147645592689514
train_iter_loss: 0.3525438904762268
train_iter_loss: 0.34718891978263855
train_iter_loss: 0.2183380424976349
train_iter_loss: 0.2530651092529297
train_iter_loss: 0.12640319764614105
train_iter_loss: 0.2702941596508026
train_iter_loss: 0.3107905983924866
train_iter_loss: 0.2601970136165619
train_iter_loss: 0.325661301612854
train_iter_loss: 0.22734516859054565
train_iter_loss: 0.25453299283981323
train_iter_loss: 0.032212965190410614
train_iter_loss: 0.3425343930721283
train_iter_loss: 0.19046525657176971
train_iter_loss: 0.2758246958255768
train_iter_loss: 0.28329434990882874
train loss :0.2656
---------------------
Validation seg loss: 0.3729964367483022 at epoch 575
epoch =    576/  1000, exp = train
train_iter_loss: 0.18766309320926666
train_iter_loss: 0.16667763888835907
train_iter_loss: 0.2038940191268921
train_iter_loss: 0.327266126871109
train_iter_loss: 0.32477328181266785
train_iter_loss: 0.3575076758861542
train_iter_loss: 0.32749852538108826
train_iter_loss: 0.24258914589881897
train_iter_loss: 0.16655787825584412
train_iter_loss: 0.21581241488456726
train_iter_loss: 0.2463698834180832
train_iter_loss: 0.3244099020957947
train_iter_loss: 0.24392512440681458
train_iter_loss: 0.3044181764125824
train_iter_loss: 0.24274754524230957
train_iter_loss: 0.27253106236457825
train_iter_loss: 0.16118115186691284
train_iter_loss: 0.2124621719121933
train_iter_loss: 0.31120800971984863
train_iter_loss: 0.2412489652633667
train_iter_loss: 0.35753822326660156
train_iter_loss: 0.3969590663909912
train_iter_loss: 0.18096686899662018
train_iter_loss: 0.284024715423584
train_iter_loss: 0.2796148359775543
train_iter_loss: 0.2534269392490387
train_iter_loss: 0.25385919213294983
train_iter_loss: 0.32005801796913147
train_iter_loss: 0.1711353212594986
train_iter_loss: 0.15178941190242767
train_iter_loss: 0.30090853571891785
train_iter_loss: 0.15177389979362488
train_iter_loss: 0.21377049386501312
train_iter_loss: 0.31208157539367676
train_iter_loss: 0.3644450604915619
train_iter_loss: 0.15508583188056946
train_iter_loss: 0.12087051570415497
train_iter_loss: 0.3122027814388275
train_iter_loss: 0.21435213088989258
train_iter_loss: 0.20674458146095276
train_iter_loss: 0.2400529980659485
train_iter_loss: 0.12159817665815353
train_iter_loss: 0.28683188557624817
train_iter_loss: 0.412870317697525
train_iter_loss: 0.17761042714118958
train_iter_loss: 0.38590991497039795
train_iter_loss: 0.21561862528324127
train_iter_loss: 0.31275883316993713
train_iter_loss: 0.2592701017856598
train_iter_loss: 0.19177372753620148
train_iter_loss: 0.25690263509750366
train_iter_loss: 0.30537647008895874
train_iter_loss: 0.13078784942626953
train_iter_loss: 0.18384547531604767
train_iter_loss: 0.18700997531414032
train_iter_loss: 0.5379173159599304
train_iter_loss: 0.1855211853981018
train_iter_loss: 0.2680351734161377
train_iter_loss: 0.24774397909641266
train_iter_loss: 0.2248598337173462
train_iter_loss: 0.24558468163013458
train_iter_loss: 0.2920316755771637
train_iter_loss: 0.24014849960803986
train_iter_loss: 0.12369389832019806
train_iter_loss: 0.17391611635684967
train_iter_loss: 0.26114654541015625
train_iter_loss: 0.2651446461677551
train_iter_loss: 0.22762808203697205
train_iter_loss: 0.24175196886062622
train_iter_loss: 0.23882898688316345
train_iter_loss: 0.11358366906642914
train_iter_loss: 0.22519686818122864
train_iter_loss: 0.3008202910423279
train_iter_loss: 0.37078896164894104
train_iter_loss: 0.3364987373352051
train_iter_loss: 0.25563061237335205
train_iter_loss: 0.23777993023395538
train_iter_loss: 0.28247252106666565
train_iter_loss: 0.24586081504821777
train_iter_loss: 0.4042615294456482
train_iter_loss: 0.21895870566368103
train_iter_loss: 0.3556898236274719
train_iter_loss: 0.2795441746711731
train_iter_loss: 0.3707681894302368
train_iter_loss: 0.29101160168647766
train_iter_loss: 0.3470408022403717
train_iter_loss: 0.26739615201950073
train_iter_loss: 0.4333566725254059
train_iter_loss: 0.30360451340675354
train_iter_loss: 0.3829607665538788
train_iter_loss: 0.28626057505607605
train_iter_loss: 0.3228713870048523
train_iter_loss: 0.3416605591773987
train_iter_loss: 0.3181491792201996
train_iter_loss: 0.280872106552124
train_iter_loss: 0.11730662733316422
train_iter_loss: 0.24375206232070923
train_iter_loss: 0.25914984941482544
train_iter_loss: 0.3418130576610565
train_iter_loss: 0.3005808889865875
train loss :0.2663
---------------------
Validation seg loss: 0.376634664174591 at epoch 576
epoch =    577/  1000, exp = train
train_iter_loss: 0.22865957021713257
train_iter_loss: 0.3578041195869446
train_iter_loss: 0.21615031361579895
train_iter_loss: 0.26926884055137634
train_iter_loss: 0.2849512994289398
train_iter_loss: 0.33412572741508484
train_iter_loss: 0.3512594997882843
train_iter_loss: 0.2301032990217209
train_iter_loss: 0.22909489274024963
train_iter_loss: 0.21247339248657227
train_iter_loss: 0.19174209237098694
train_iter_loss: 0.444950133562088
train_iter_loss: 0.17918027937412262
train_iter_loss: 0.3670724928379059
train_iter_loss: 0.36847665905952454
train_iter_loss: 0.2794160842895508
train_iter_loss: 0.32697293162345886
train_iter_loss: 0.3529928922653198
train_iter_loss: 0.4287068843841553
train_iter_loss: 0.3373768627643585
train_iter_loss: 0.3601921796798706
train_iter_loss: 0.2608520984649658
train_iter_loss: 0.28492459654808044
train_iter_loss: 0.15897518396377563
train_iter_loss: 0.24818547070026398
train_iter_loss: 0.261679470539093
train_iter_loss: 0.18193970620632172
train_iter_loss: 0.3101186156272888
train_iter_loss: 0.12645232677459717
train_iter_loss: 0.2830350995063782
train_iter_loss: 0.22748510539531708
train_iter_loss: 0.3217926323413849
train_iter_loss: 0.4761134684085846
train_iter_loss: 0.24159547686576843
train_iter_loss: 0.32505735754966736
train_iter_loss: 0.2669638991355896
train_iter_loss: 0.2843213975429535
train_iter_loss: 0.2759146988391876
train_iter_loss: 0.20865517854690552
train_iter_loss: 0.22173196077346802
train_iter_loss: 0.24410194158554077
train_iter_loss: 0.3007708489894867
train_iter_loss: 0.2742168605327606
train_iter_loss: 0.2675386667251587
train_iter_loss: 0.22660230100154877
train_iter_loss: 0.2508431673049927
train_iter_loss: 0.3375352621078491
train_iter_loss: 0.18180276453495026
train_iter_loss: 0.2480868399143219
train_iter_loss: 0.16473200917243958
train_iter_loss: 0.26447612047195435
train_iter_loss: 0.37553802132606506
train_iter_loss: 0.15750186145305634
train_iter_loss: 0.3608736991882324
train_iter_loss: 0.2886927127838135
train_iter_loss: 0.19039376080036163
train_iter_loss: 0.03301322087645531
train_iter_loss: 0.27239224314689636
train_iter_loss: 0.27671149373054504
train_iter_loss: 0.25177985429763794
train_iter_loss: 0.16061414778232574
train_iter_loss: 0.244808167219162
train_iter_loss: 0.21041974425315857
train_iter_loss: 0.18571242690086365
train_iter_loss: 0.09323601424694061
train_iter_loss: 0.1794712245464325
train_iter_loss: 0.14681574702262878
train_iter_loss: 0.1798325777053833
train_iter_loss: 0.20773544907569885
train_iter_loss: 0.24689854681491852
train_iter_loss: 0.5098429322242737
train_iter_loss: 0.3491004705429077
train_iter_loss: 0.3157810568809509
train_iter_loss: 0.24681825935840607
train_iter_loss: 0.3880305886268616
train_iter_loss: 0.32403701543807983
train_iter_loss: 0.2471473664045334
train_iter_loss: 0.19713306427001953
train_iter_loss: 0.37851664423942566
train_iter_loss: 0.10095056891441345
train_iter_loss: 0.3272705078125
train_iter_loss: 0.3665691912174225
train_iter_loss: 0.17242756485939026
train_iter_loss: 0.27723124623298645
train_iter_loss: 0.3484443128108978
train_iter_loss: 0.3748281002044678
train_iter_loss: 0.38436099886894226
train_iter_loss: 0.3386797308921814
train_iter_loss: 0.27099037170410156
train_iter_loss: 0.18321186304092407
train_iter_loss: 0.20267824828624725
train_iter_loss: 0.3148564100265503
train_iter_loss: 0.29152190685272217
train_iter_loss: 0.17897984385490417
train_iter_loss: 0.1833724081516266
train_iter_loss: 0.29778891801834106
train_iter_loss: 0.21739916503429413
train_iter_loss: 0.17414432764053345
train_iter_loss: 0.44288569688796997
train_iter_loss: 0.32949724793434143
train loss :0.2714
---------------------
Validation seg loss: 0.352394020676894 at epoch 577
epoch =    578/  1000, exp = train
train_iter_loss: 0.22980996966362
train_iter_loss: 0.1778792291879654
train_iter_loss: 0.41123852133750916
train_iter_loss: 0.723576545715332
train_iter_loss: 0.2598544657230377
train_iter_loss: 0.25745832920074463
train_iter_loss: 0.162689208984375
train_iter_loss: 0.513663649559021
train_iter_loss: 0.2107645571231842
train_iter_loss: 0.25508540868759155
train_iter_loss: 0.32853806018829346
train_iter_loss: 0.25538989901542664
train_iter_loss: 0.25338438153266907
train_iter_loss: 0.3427964746952057
train_iter_loss: 0.2552080750465393
train_iter_loss: 0.2782839238643646
train_iter_loss: 0.22624048590660095
train_iter_loss: 0.264201819896698
train_iter_loss: 0.33650967478752136
train_iter_loss: 0.281161904335022
train_iter_loss: 0.2133897840976715
train_iter_loss: 0.1313140094280243
train_iter_loss: 0.38097766041755676
train_iter_loss: 0.1317184865474701
train_iter_loss: 0.3293004333972931
train_iter_loss: 0.4115649461746216
train_iter_loss: 0.33455395698547363
train_iter_loss: 0.16620208323001862
train_iter_loss: 0.23713664710521698
train_iter_loss: 0.24622593820095062
train_iter_loss: 0.38066768646240234
train_iter_loss: 0.21491564810276031
train_iter_loss: 0.3163628876209259
train_iter_loss: 0.49083369970321655
train_iter_loss: 0.30073851346969604
train_iter_loss: 0.3162331283092499
train_iter_loss: 0.24884888529777527
train_iter_loss: 0.12948936223983765
train_iter_loss: 0.4184509813785553
train_iter_loss: 0.26038187742233276
train_iter_loss: 0.23639896512031555
train_iter_loss: 0.2598402500152588
train_iter_loss: 0.22892048954963684
train_iter_loss: 0.3293859660625458
train_iter_loss: 0.19137316942214966
train_iter_loss: 0.24185244739055634
train_iter_loss: 0.13596601784229279
train_iter_loss: 0.504214882850647
train_iter_loss: 0.20080594718456268
train_iter_loss: 0.2609768211841583
train_iter_loss: 0.22106508910655975
train_iter_loss: 0.22970041632652283
train_iter_loss: 0.18351516127586365
train_iter_loss: 0.054432764649391174
train_iter_loss: 0.19671672582626343
train_iter_loss: 0.17460453510284424
train_iter_loss: 0.3720631003379822
train_iter_loss: 0.30907413363456726
train_iter_loss: 0.22099922597408295
train_iter_loss: 0.18103721737861633
train_iter_loss: 0.16172969341278076
train_iter_loss: 0.24799828231334686
train_iter_loss: 0.21604497730731964
train_iter_loss: 0.3450819253921509
train_iter_loss: 0.131829634308815
train_iter_loss: 0.3540515899658203
train_iter_loss: 0.20740902423858643
train_iter_loss: 0.23449590802192688
train_iter_loss: 0.25417712330818176
train_iter_loss: 0.1681959480047226
train_iter_loss: 0.21195964515209198
train_iter_loss: 0.28100165724754333
train_iter_loss: 0.3103220760822296
train_iter_loss: 0.2245500385761261
train_iter_loss: 0.28062742948532104
train_iter_loss: 0.3455038368701935
train_iter_loss: 0.45386141538619995
train_iter_loss: 0.3862305283546448
train_iter_loss: 0.2249326854944229
train_iter_loss: 0.2865586578845978
train_iter_loss: 0.19633549451828003
train_iter_loss: 0.33926188945770264
train_iter_loss: 0.3560979962348938
train_iter_loss: 0.3310173451900482
train_iter_loss: 0.18340620398521423
train_iter_loss: 0.18390478193759918
train_iter_loss: 0.21506288647651672
train_iter_loss: 0.29293784499168396
train_iter_loss: 0.23333680629730225
train_iter_loss: 0.22073553502559662
train_iter_loss: 0.3572923243045807
train_iter_loss: 0.3786381781101227
train_iter_loss: 0.2778832018375397
train_iter_loss: 0.3218817710876465
train_iter_loss: 0.3535153567790985
train_iter_loss: 0.13999438285827637
train_iter_loss: 0.2778763175010681
train_iter_loss: 0.3910354971885681
train_iter_loss: 0.21006691455841064
train_iter_loss: 0.15676595270633698
train loss :0.2743
---------------------
Validation seg loss: 0.35792034389099703 at epoch 578
epoch =    579/  1000, exp = train
train_iter_loss: 0.14576655626296997
train_iter_loss: 0.35237011313438416
train_iter_loss: 0.25748953223228455
train_iter_loss: 0.1419876664876938
train_iter_loss: 0.40801259875297546
train_iter_loss: 0.1427808701992035
train_iter_loss: 0.35618114471435547
train_iter_loss: 0.16639725863933563
train_iter_loss: 0.4299503266811371
train_iter_loss: 0.20400916039943695
train_iter_loss: 0.2198929339647293
train_iter_loss: 0.09072475135326385
train_iter_loss: 0.2688717842102051
train_iter_loss: 0.2880041301250458
train_iter_loss: 0.15755094587802887
train_iter_loss: 0.3250589966773987
train_iter_loss: 0.19355478882789612
train_iter_loss: 0.3723768889904022
train_iter_loss: 0.21812449395656586
train_iter_loss: 0.2418680340051651
train_iter_loss: 0.3839421272277832
train_iter_loss: 0.17100748419761658
train_iter_loss: 0.10819745063781738
train_iter_loss: 0.0688573494553566
train_iter_loss: 0.4579574167728424
train_iter_loss: 0.33025848865509033
train_iter_loss: 0.21684551239013672
train_iter_loss: 0.21781791746616364
train_iter_loss: 0.24414928257465363
train_iter_loss: 0.17187117040157318
train_iter_loss: 0.28890669345855713
train_iter_loss: 0.418692022562027
train_iter_loss: 0.1941855400800705
train_iter_loss: 0.26605501770973206
train_iter_loss: 0.31101512908935547
train_iter_loss: 0.14433333277702332
train_iter_loss: 0.2515691816806793
train_iter_loss: 0.3998906910419464
train_iter_loss: 0.22926345467567444
train_iter_loss: 0.3302167057991028
train_iter_loss: 0.29020941257476807
train_iter_loss: 0.2502841651439667
train_iter_loss: 0.25768303871154785
train_iter_loss: 0.18739326298236847
train_iter_loss: 0.20465727150440216
train_iter_loss: 0.44740837812423706
train_iter_loss: 0.22864431142807007
train_iter_loss: 0.3191448450088501
train_iter_loss: 0.3765718340873718
train_iter_loss: 0.26610901951789856
train_iter_loss: 0.29688963294029236
train_iter_loss: 0.2701081931591034
train_iter_loss: 0.3440050184726715
train_iter_loss: 0.12002186477184296
train_iter_loss: 0.177650585770607
train_iter_loss: 0.2468552589416504
train_iter_loss: 0.1348101645708084
train_iter_loss: 0.4155038893222809
train_iter_loss: 0.3021644055843353
train_iter_loss: 0.1617354154586792
train_iter_loss: 0.1880604326725006
train_iter_loss: 0.13060086965560913
train_iter_loss: 0.295978307723999
train_iter_loss: 0.4847789704799652
train_iter_loss: 0.34805288910865784
train_iter_loss: 0.3546389937400818
train_iter_loss: 0.3577953279018402
train_iter_loss: 0.2699655592441559
train_iter_loss: 0.2802664041519165
train_iter_loss: 0.2439662218093872
train_iter_loss: 0.4129352867603302
train_iter_loss: 0.44140657782554626
train_iter_loss: 0.19296395778656006
train_iter_loss: 0.2978046238422394
train_iter_loss: 0.2825149893760681
train_iter_loss: 0.19984030723571777
train_iter_loss: 0.2117656022310257
train_iter_loss: 0.18785566091537476
train_iter_loss: 0.27586209774017334
train_iter_loss: 0.2534308433532715
train_iter_loss: 0.2827301621437073
train_iter_loss: 0.2846126854419708
train_iter_loss: 0.23173625767230988
train_iter_loss: 0.39079809188842773
train_iter_loss: 0.2411176711320877
train_iter_loss: 0.2886900007724762
train_iter_loss: 0.12524308264255524
train_iter_loss: 0.29514870047569275
train_iter_loss: 0.20854546129703522
train_iter_loss: 0.3677878677845001
train_iter_loss: 0.2518608272075653
train_iter_loss: 0.21102547645568848
train_iter_loss: 0.1471017450094223
train_iter_loss: 0.15750454366207123
train_iter_loss: 0.12319930642843246
train_iter_loss: 0.3631356954574585
train_iter_loss: 0.2893527150154114
train_iter_loss: 0.2544558644294739
train_iter_loss: 0.41188180446624756
train_iter_loss: 0.25598153471946716
train loss :0.2665
---------------------
Validation seg loss: 0.35847104607009384 at epoch 579
epoch =    580/  1000, exp = train
train_iter_loss: 0.38025224208831787
train_iter_loss: 0.34197473526000977
train_iter_loss: 0.3283500671386719
train_iter_loss: 0.21370641887187958
train_iter_loss: 0.194889634847641
train_iter_loss: 0.26599711179733276
train_iter_loss: 0.3444598913192749
train_iter_loss: 0.258823037147522
train_iter_loss: 0.14415961503982544
train_iter_loss: 0.24856902658939362
train_iter_loss: 0.16300085186958313
train_iter_loss: 0.2736850082874298
train_iter_loss: 0.2312052994966507
train_iter_loss: 0.2148103564977646
train_iter_loss: 0.2721794843673706
train_iter_loss: 0.3319898545742035
train_iter_loss: 0.22015319764614105
train_iter_loss: 0.5771340727806091
train_iter_loss: 0.3191615045070648
train_iter_loss: 0.2959970533847809
train_iter_loss: 0.3536038100719452
train_iter_loss: 0.04050403833389282
train_iter_loss: 0.2971653640270233
train_iter_loss: 0.2086268812417984
train_iter_loss: 0.14912095665931702
train_iter_loss: 0.20657990872859955
train_iter_loss: 0.44787487387657166
train_iter_loss: 0.2910583019256592
train_iter_loss: 0.26059088110923767
train_iter_loss: 0.54292231798172
train_iter_loss: 0.22807064652442932
train_iter_loss: 0.2625710964202881
train_iter_loss: 0.3004187047481537
train_iter_loss: 0.13574518263339996
train_iter_loss: 0.18889778852462769
train_iter_loss: 0.2873394787311554
train_iter_loss: 0.2346016764640808
train_iter_loss: 0.16918282210826874
train_iter_loss: 0.3872752785682678
train_iter_loss: 0.25775277614593506
train_iter_loss: 0.32101956009864807
train_iter_loss: 0.3125098645687103
train_iter_loss: 0.35576942563056946
train_iter_loss: 0.2861414849758148
train_iter_loss: 0.3326048254966736
train_iter_loss: 0.25718575716018677
train_iter_loss: 0.13096021115779877
train_iter_loss: 0.29910188913345337
train_iter_loss: 0.2820231318473816
train_iter_loss: 0.16972151398658752
train_iter_loss: 0.26359134912490845
train_iter_loss: 0.27324438095092773
train_iter_loss: 0.13967961072921753
train_iter_loss: 0.24556349217891693
train_iter_loss: 0.21808049082756042
train_iter_loss: 0.19639720022678375
train_iter_loss: 0.24721702933311462
train_iter_loss: 0.3020082116127014
train_iter_loss: 0.28002867102622986
train_iter_loss: 0.19265447556972504
train_iter_loss: 0.4469470679759979
train_iter_loss: 0.16465459764003754
train_iter_loss: 0.25638091564178467
train_iter_loss: 0.12227693945169449
train_iter_loss: 0.30607062578201294
train_iter_loss: 0.27539822459220886
train_iter_loss: 0.20306596159934998
train_iter_loss: 0.26633986830711365
train_iter_loss: 0.16927587985992432
train_iter_loss: 0.3418966233730316
train_iter_loss: 0.17295967042446136
train_iter_loss: 0.24868567287921906
train_iter_loss: 0.2157917022705078
train_iter_loss: 0.2291848510503769
train_iter_loss: 0.09170758724212646
train_iter_loss: 0.2455732673406601
train_iter_loss: 0.21784669160842896
train_iter_loss: 0.3776845633983612
train_iter_loss: 0.19806064665317535
train_iter_loss: 0.09973039478063583
train_iter_loss: 0.25464874505996704
train_iter_loss: 0.1982245296239853
train_iter_loss: 0.5184813141822815
train_iter_loss: 0.24021776020526886
train_iter_loss: 0.2916359603404999
train_iter_loss: 0.12432678788900375
train_iter_loss: 0.24157480895519257
train_iter_loss: 0.40772297978401184
train_iter_loss: 0.1693384051322937
train_iter_loss: 0.3771374821662903
train_iter_loss: 0.2055714875459671
train_iter_loss: 0.3243456184864044
train_iter_loss: 0.26736512780189514
train_iter_loss: 0.30072468519210815
train_iter_loss: 0.3366331160068512
train_iter_loss: 0.18228447437286377
train_iter_loss: 0.32350561022758484
train_iter_loss: 0.38500043749809265
train_iter_loss: 0.22617299854755402
train_iter_loss: 0.2442576289176941
train loss :0.2651
---------------------
Validation seg loss: 0.3751940833501307 at epoch 580
epoch =    581/  1000, exp = train
train_iter_loss: 0.10506892949342728
train_iter_loss: 0.24309608340263367
train_iter_loss: 0.22050288319587708
train_iter_loss: 0.3162909746170044
train_iter_loss: 0.27632778882980347
train_iter_loss: 0.19584886729717255
train_iter_loss: 0.10127405822277069
train_iter_loss: 0.31076914072036743
train_iter_loss: 0.292236864566803
train_iter_loss: 0.33126506209373474
train_iter_loss: 0.2401195764541626
train_iter_loss: 0.2174636423587799
train_iter_loss: 0.35071200132369995
train_iter_loss: 0.164407417178154
train_iter_loss: 0.2841084897518158
train_iter_loss: 0.3789355754852295
train_iter_loss: 0.29924046993255615
train_iter_loss: 0.2817651927471161
train_iter_loss: 0.29306861758232117
train_iter_loss: 0.17431412637233734
train_iter_loss: 0.2776526212692261
train_iter_loss: 0.46124136447906494
train_iter_loss: 0.22995924949645996
train_iter_loss: 0.17651137709617615
train_iter_loss: 0.24133791029453278
train_iter_loss: 0.21764448285102844
train_iter_loss: 0.34097370505332947
train_iter_loss: 0.376838356256485
train_iter_loss: 0.1759704351425171
train_iter_loss: 0.27429330348968506
train_iter_loss: 0.2652107775211334
train_iter_loss: 0.435539186000824
train_iter_loss: 0.1844855397939682
train_iter_loss: 0.33883294463157654
train_iter_loss: 0.2410522997379303
train_iter_loss: 0.21616441011428833
train_iter_loss: 0.17898190021514893
train_iter_loss: 0.2508297860622406
train_iter_loss: 0.13386331498622894
train_iter_loss: 0.32848498225212097
train_iter_loss: 0.2530101537704468
train_iter_loss: 0.1598016917705536
train_iter_loss: 0.1247323676943779
train_iter_loss: 0.1313709318637848
train_iter_loss: 0.3395825922489166
train_iter_loss: 0.181302011013031
train_iter_loss: 0.254763662815094
train_iter_loss: 0.2630807161331177
train_iter_loss: 0.20750993490219116
train_iter_loss: 0.10218524932861328
train_iter_loss: 0.23398800194263458
train_iter_loss: 0.20457987487316132
train_iter_loss: 0.32446110248565674
train_iter_loss: 0.09729383885860443
train_iter_loss: 0.35202768445014954
train_iter_loss: 0.196895569562912
train_iter_loss: 0.3601028323173523
train_iter_loss: 0.30673083662986755
train_iter_loss: 0.19869829714298248
train_iter_loss: 0.2735495865345001
train_iter_loss: 0.19712406396865845
train_iter_loss: 0.26941660046577454
train_iter_loss: 0.23747006058692932
train_iter_loss: 0.3172450661659241
train_iter_loss: 0.3819146752357483
train_iter_loss: 0.11240853369235992
train_iter_loss: 0.11276127398014069
train_iter_loss: 0.23119662702083588
train_iter_loss: 0.31489869952201843
train_iter_loss: 0.31011465191841125
train_iter_loss: 0.3190211355686188
train_iter_loss: 0.4121180474758148
train_iter_loss: 0.43334972858428955
train_iter_loss: 0.29698827862739563
train_iter_loss: 0.1525111198425293
train_iter_loss: 0.21577198803424835
train_iter_loss: 0.2548481225967407
train_iter_loss: 0.3231937885284424
train_iter_loss: 0.33754971623420715
train_iter_loss: 0.2337821125984192
train_iter_loss: 0.4757198989391327
train_iter_loss: 0.17919524013996124
train_iter_loss: 0.4217858910560608
train_iter_loss: 0.339055597782135
train_iter_loss: 0.37928125262260437
train_iter_loss: 0.292158842086792
train_iter_loss: 0.5038080215454102
train_iter_loss: 0.34424322843551636
train_iter_loss: 0.19266480207443237
train_iter_loss: 0.2681395709514618
train_iter_loss: 0.37961897253990173
train_iter_loss: 0.3452507257461548
train_iter_loss: 0.1827564835548401
train_iter_loss: 0.32965996861457825
train_iter_loss: 0.20138752460479736
train_iter_loss: 0.2773704528808594
train_iter_loss: 0.338717520236969
train_iter_loss: 0.2735050916671753
train_iter_loss: 0.23949047923088074
train_iter_loss: 0.24622248113155365
train loss :0.2693
---------------------
Validation seg loss: 0.35418178909018916 at epoch 581
epoch =    582/  1000, exp = train
train_iter_loss: 0.21706585586071014
train_iter_loss: 0.08695665746927261
train_iter_loss: 0.2654310166835785
train_iter_loss: 0.3099042475223541
train_iter_loss: 0.2551482319831848
train_iter_loss: 0.16653862595558167
train_iter_loss: 0.4521532654762268
train_iter_loss: 0.13942961394786835
train_iter_loss: 0.2953461706638336
train_iter_loss: 0.15001888573169708
train_iter_loss: 0.26448914408683777
train_iter_loss: 0.25672638416290283
train_iter_loss: 0.1760583370923996
train_iter_loss: 0.2698165774345398
train_iter_loss: 0.34547680616378784
train_iter_loss: 0.23011748492717743
train_iter_loss: 0.5664253234863281
train_iter_loss: 0.21206329762935638
train_iter_loss: 0.2233853042125702
train_iter_loss: 0.22005833685398102
train_iter_loss: 0.27738943696022034
train_iter_loss: 0.3983808159828186
train_iter_loss: 0.399544358253479
train_iter_loss: 0.3050597906112671
train_iter_loss: 0.4198494553565979
train_iter_loss: 0.3195599615573883
train_iter_loss: 0.3046260476112366
train_iter_loss: 0.2908477783203125
train_iter_loss: 0.21966250240802765
train_iter_loss: 0.24939097464084625
train_iter_loss: 0.27777159214019775
train_iter_loss: 0.4862070679664612
train_iter_loss: 0.2255541980266571
train_iter_loss: 0.25633808970451355
train_iter_loss: 0.31516575813293457
train_iter_loss: 0.14610391855239868
train_iter_loss: 0.36156970262527466
train_iter_loss: 0.29963377118110657
train_iter_loss: 0.3259332776069641
train_iter_loss: 0.16530336439609528
train_iter_loss: 0.20391085743904114
train_iter_loss: 0.3236280679702759
train_iter_loss: 0.21924500167369843
train_iter_loss: 0.10160794854164124
train_iter_loss: 0.28641048073768616
train_iter_loss: 0.29228827357292175
train_iter_loss: 0.17074155807495117
train_iter_loss: 0.36835381388664246
train_iter_loss: 0.11572115123271942
train_iter_loss: 0.293709933757782
train_iter_loss: 0.32210978865623474
train_iter_loss: 0.2168933004140854
train_iter_loss: 0.26114001870155334
train_iter_loss: 0.27355992794036865
train_iter_loss: 0.13713271915912628
train_iter_loss: 0.33469483256340027
train_iter_loss: 0.2914600074291229
train_iter_loss: 0.08035070449113846
train_iter_loss: 0.16632986068725586
train_iter_loss: 0.3610919117927551
train_iter_loss: 0.26416099071502686
train_iter_loss: 0.3387574255466461
train_iter_loss: 0.35084402561187744
train_iter_loss: 0.4563310444355011
train_iter_loss: 0.21938107907772064
train_iter_loss: 0.2600507438182831
train_iter_loss: 0.31485515832901
train_iter_loss: 0.2459315061569214
train_iter_loss: 0.12604737281799316
train_iter_loss: 0.17983590066432953
train_iter_loss: 0.27118977904319763
train_iter_loss: 0.23724886775016785
train_iter_loss: 0.1762312799692154
train_iter_loss: 0.20679454505443573
train_iter_loss: 0.23414701223373413
train_iter_loss: 0.5239007472991943
train_iter_loss: 0.2628188133239746
train_iter_loss: 0.33285629749298096
train_iter_loss: 0.18608920276165009
train_iter_loss: 0.3409268856048584
train_iter_loss: 0.33310067653656006
train_iter_loss: 0.15883663296699524
train_iter_loss: 0.2979232370853424
train_iter_loss: 0.3954848647117615
train_iter_loss: 0.1438732147216797
train_iter_loss: 0.268208384513855
train_iter_loss: 0.2930985987186432
train_iter_loss: 0.2962222099304199
train_iter_loss: 0.377023309469223
train_iter_loss: 0.33014968037605286
train_iter_loss: 0.23962311446666718
train_iter_loss: 0.30460259318351746
train_iter_loss: 0.3115656077861786
train_iter_loss: 0.20528365671634674
train_iter_loss: 0.32557982206344604
train_iter_loss: 0.40701764822006226
train_iter_loss: 0.11015623062849045
train_iter_loss: 0.2197646051645279
train_iter_loss: 0.2942001223564148
train_iter_loss: 0.29205748438835144
train loss :0.2737
---------------------
Validation seg loss: 0.3478836346470382 at epoch 582
epoch =    583/  1000, exp = train
train_iter_loss: 0.2010963410139084
train_iter_loss: 0.289889931678772
train_iter_loss: 0.24924543499946594
train_iter_loss: 0.33743903040885925
train_iter_loss: 0.17589329183101654
train_iter_loss: 0.24693702161312103
train_iter_loss: 0.16869550943374634
train_iter_loss: 0.2689122259616852
train_iter_loss: 0.24050836265087128
train_iter_loss: 0.27658405900001526
train_iter_loss: 0.23886068165302277
train_iter_loss: 0.3299780488014221
train_iter_loss: 0.30609849095344543
train_iter_loss: 0.3245568871498108
train_iter_loss: 0.25416141748428345
train_iter_loss: 0.2315165400505066
train_iter_loss: 0.2840682864189148
train_iter_loss: 0.3339509069919586
train_iter_loss: 0.20293781161308289
train_iter_loss: 0.2731729745864868
train_iter_loss: 0.14917118847370148
train_iter_loss: 0.22121188044548035
train_iter_loss: 0.1754951775074005
train_iter_loss: 0.14700248837471008
train_iter_loss: 0.1558765172958374
train_iter_loss: 0.21528267860412598
train_iter_loss: 0.34228965640068054
train_iter_loss: 0.34129440784454346
train_iter_loss: 0.30391600728034973
train_iter_loss: 0.18753473460674286
train_iter_loss: 0.3278307616710663
train_iter_loss: 0.24628232419490814
train_iter_loss: 0.35959190130233765
train_iter_loss: 0.18187279999256134
train_iter_loss: 0.2923566401004791
train_iter_loss: 0.2693358361721039
train_iter_loss: 0.24094736576080322
train_iter_loss: 0.29083696007728577
train_iter_loss: 0.3016395568847656
train_iter_loss: 0.2793160676956177
train_iter_loss: 0.2601211369037628
train_iter_loss: 0.337159663438797
train_iter_loss: 0.2999535799026489
train_iter_loss: 0.17209002375602722
train_iter_loss: 0.24789437651634216
train_iter_loss: 0.44700899720191956
train_iter_loss: 0.17155717313289642
train_iter_loss: 0.23835861682891846
train_iter_loss: 0.1248190701007843
train_iter_loss: 0.2982248365879059
train_iter_loss: 0.41064533591270447
train_iter_loss: 0.17692436277866364
train_iter_loss: 0.25491660833358765
train_iter_loss: 0.24087442457675934
train_iter_loss: 0.35852470993995667
train_iter_loss: 0.1737769991159439
train_iter_loss: 0.2508147656917572
train_iter_loss: 0.2725059986114502
train_iter_loss: 0.5207024216651917
train_iter_loss: 0.2452605813741684
train_iter_loss: 0.2600899040699005
train_iter_loss: 0.3251977562904358
train_iter_loss: 0.26910796761512756
train_iter_loss: 0.1459292471408844
train_iter_loss: 0.1765209585428238
train_iter_loss: 0.36840924620628357
train_iter_loss: 0.3234109878540039
train_iter_loss: 0.3910766541957855
train_iter_loss: 0.23803842067718506
train_iter_loss: 0.3142640292644501
train_iter_loss: 0.10544546693563461
train_iter_loss: 0.24077515304088593
train_iter_loss: 0.24480746686458588
train_iter_loss: 0.16784709692001343
train_iter_loss: 0.09472435712814331
train_iter_loss: 0.23111267387866974
train_iter_loss: 0.24673083424568176
train_iter_loss: 0.20744185149669647
train_iter_loss: 0.34700438380241394
train_iter_loss: 0.32514283061027527
train_iter_loss: 0.30863457918167114
train_iter_loss: 0.2930993437767029
train_iter_loss: 0.2874283194541931
train_iter_loss: 0.39037731289863586
train_iter_loss: 0.16422101855278015
train_iter_loss: 0.2053864747285843
train_iter_loss: 0.42524367570877075
train_iter_loss: 0.11820732802152634
train_iter_loss: 0.22307753562927246
train_iter_loss: 0.3079701066017151
train_iter_loss: 0.3429884612560272
train_iter_loss: 0.1966562271118164
train_iter_loss: 0.22952264547348022
train_iter_loss: 0.19665533304214478
train_iter_loss: 0.6046872735023499
train_iter_loss: 0.34181323647499084
train_iter_loss: 0.4662327468395233
train_iter_loss: 0.4465015232563019
train_iter_loss: 0.3672344386577606
train_iter_loss: 0.239571675658226
train loss :0.2722
---------------------
Validation seg loss: 0.3477925726317994 at epoch 583
epoch =    584/  1000, exp = train
train_iter_loss: 0.3092276155948639
train_iter_loss: 0.2552329897880554
train_iter_loss: 0.16905900835990906
train_iter_loss: 0.17182126641273499
train_iter_loss: 0.21068161725997925
train_iter_loss: 0.18262124061584473
train_iter_loss: 0.2634029984474182
train_iter_loss: 0.19346946477890015
train_iter_loss: 0.20829719305038452
train_iter_loss: 0.2305171638727188
train_iter_loss: 0.39491117000579834
train_iter_loss: 0.20654068887233734
train_iter_loss: 0.4019710123538971
train_iter_loss: 0.2424491047859192
train_iter_loss: 0.24463710188865662
train_iter_loss: 0.2993306517601013
train_iter_loss: 0.2681891918182373
train_iter_loss: 0.19522814452648163
train_iter_loss: 0.17286652326583862
train_iter_loss: 0.34498217701911926
train_iter_loss: 0.23831409215927124
train_iter_loss: 0.17234985530376434
train_iter_loss: 0.2666316032409668
train_iter_loss: 0.25143101811408997
train_iter_loss: 0.1528877466917038
train_iter_loss: 0.1971091479063034
train_iter_loss: 0.27251821756362915
train_iter_loss: 0.25888726115226746
train_iter_loss: 0.2853778302669525
train_iter_loss: 0.22214165329933167
train_iter_loss: 0.2815229594707489
train_iter_loss: 0.2626059651374817
train_iter_loss: 0.31064438819885254
train_iter_loss: 0.2895652651786804
train_iter_loss: 0.1756272315979004
train_iter_loss: 0.10307728499174118
train_iter_loss: 0.1368248015642166
train_iter_loss: 0.24421459436416626
train_iter_loss: 0.20817801356315613
train_iter_loss: 0.2799857556819916
train_iter_loss: 0.2752242684364319
train_iter_loss: 0.2825907766819
train_iter_loss: 0.3616311252117157
train_iter_loss: 0.14671632647514343
train_iter_loss: 0.23691295087337494
train_iter_loss: 0.1891660839319229
train_iter_loss: 0.2306210845708847
train_iter_loss: 0.23043042421340942
train_iter_loss: 0.21136082708835602
train_iter_loss: 0.3852986693382263
train_iter_loss: 0.36250853538513184
train_iter_loss: 0.24841490387916565
train_iter_loss: 0.2739005386829376
train_iter_loss: 0.12548507750034332
train_iter_loss: 0.19618894159793854
train_iter_loss: 0.2142658680677414
train_iter_loss: 0.1798252910375595
train_iter_loss: 0.2194322794675827
train_iter_loss: 0.3668963611125946
train_iter_loss: 0.4882124364376068
train_iter_loss: 0.43721503019332886
train_iter_loss: 0.06697838008403778
train_iter_loss: 0.3697066605091095
train_iter_loss: 0.2530500292778015
train_iter_loss: 0.19721351563930511
train_iter_loss: 0.36495959758758545
train_iter_loss: 0.24384163320064545
train_iter_loss: 0.1565699428319931
train_iter_loss: 0.26274383068084717
train_iter_loss: 0.29780587553977966
train_iter_loss: 0.239527627825737
train_iter_loss: 0.16030381619930267
train_iter_loss: 0.29930004477500916
train_iter_loss: 0.15555687248706818
train_iter_loss: 0.26755768060684204
train_iter_loss: 0.4433947503566742
train_iter_loss: 0.3088555634021759
train_iter_loss: 0.25582966208457947
train_iter_loss: 0.16446350514888763
train_iter_loss: 0.2877936065196991
train_iter_loss: 0.280485600233078
train_iter_loss: 0.3042556941509247
train_iter_loss: 0.35673460364341736
train_iter_loss: 0.16989929974079132
train_iter_loss: 0.44921064376831055
train_iter_loss: 0.17288880050182343
train_iter_loss: 0.2349015176296234
train_iter_loss: 0.31341028213500977
train_iter_loss: 0.40314316749572754
train_iter_loss: 0.44408026337623596
train_iter_loss: 0.4409148693084717
train_iter_loss: 0.3098315894603729
train_iter_loss: 0.2576987147331238
train_iter_loss: 0.20352888107299805
train_iter_loss: 0.24313662946224213
train_iter_loss: 0.29346418380737305
train_iter_loss: 0.24315237998962402
train_iter_loss: 0.2748391330242157
train_iter_loss: 0.2044713795185089
train_iter_loss: 0.4425206482410431
train loss :0.2635
---------------------
Validation seg loss: 0.4436631884056864 at epoch 584
epoch =    585/  1000, exp = train
train_iter_loss: 0.1764768362045288
train_iter_loss: 0.33794674277305603
train_iter_loss: 0.29458925127983093
train_iter_loss: 0.13539312779903412
train_iter_loss: 0.29060694575309753
train_iter_loss: 0.17005006968975067
train_iter_loss: 0.32451194524765015
train_iter_loss: 0.2614871561527252
train_iter_loss: 0.36100175976753235
train_iter_loss: 0.237435445189476
train_iter_loss: 0.22729863226413727
train_iter_loss: 0.26807689666748047
train_iter_loss: 0.22499306499958038
train_iter_loss: 0.30970698595046997
train_iter_loss: 0.16278204321861267
train_iter_loss: 0.3622764050960541
train_iter_loss: 0.5135872960090637
train_iter_loss: 0.3369670510292053
train_iter_loss: 0.26263344287872314
train_iter_loss: 0.21984438598155975
train_iter_loss: 0.2753017544746399
train_iter_loss: 0.4359094798564911
train_iter_loss: 0.43079978227615356
train_iter_loss: 0.1469321846961975
train_iter_loss: 0.19009006023406982
train_iter_loss: 0.24303096532821655
train_iter_loss: 0.23888523876667023
train_iter_loss: 0.20773985981941223
train_iter_loss: 0.32274556159973145
train_iter_loss: 0.12498961389064789
train_iter_loss: 0.18607434630393982
train_iter_loss: 0.18011175096035004
train_iter_loss: 0.4075542986392975
train_iter_loss: 0.23212039470672607
train_iter_loss: 0.3373136818408966
train_iter_loss: 0.38781988620758057
train_iter_loss: 0.19176644086837769
train_iter_loss: 0.21823236346244812
train_iter_loss: 0.2794763147830963
train_iter_loss: 0.27770793437957764
train_iter_loss: 0.21115921437740326
train_iter_loss: 0.3094193637371063
train_iter_loss: 0.3553626537322998
train_iter_loss: 0.2226496934890747
train_iter_loss: 0.24170462787151337
train_iter_loss: 0.46716880798339844
train_iter_loss: 0.244264155626297
train_iter_loss: 0.2755277752876282
train_iter_loss: 0.2032259851694107
train_iter_loss: 0.44032102823257446
train_iter_loss: 0.28746670484542847
train_iter_loss: 0.14660601317882538
train_iter_loss: 0.12411075085401535
train_iter_loss: 0.2318842113018036
train_iter_loss: 0.21456502377986908
train_iter_loss: 0.2966674566268921
train_iter_loss: 0.2636451721191406
train_iter_loss: 0.47693437337875366
train_iter_loss: 0.3218082785606384
train_iter_loss: 0.29081156849861145
train_iter_loss: 0.3223252594470978
train_iter_loss: 0.1404593139886856
train_iter_loss: 0.10263573378324509
train_iter_loss: 0.2252848744392395
train_iter_loss: 0.21867379546165466
train_iter_loss: 0.3644546866416931
train_iter_loss: 0.2202984094619751
train_iter_loss: 0.2542814612388611
train_iter_loss: 0.2801129221916199
train_iter_loss: 0.2968580722808838
train_iter_loss: 0.2880741357803345
train_iter_loss: 0.3452668786048889
train_iter_loss: 0.3000068664550781
train_iter_loss: 0.21311171352863312
train_iter_loss: 0.17203199863433838
train_iter_loss: 0.2751177251338959
train_iter_loss: 0.359249472618103
train_iter_loss: 0.2598315477371216
train_iter_loss: 0.17573101818561554
train_iter_loss: 0.2330886423587799
train_iter_loss: 0.27629759907722473
train_iter_loss: 0.2634001076221466
train_iter_loss: 0.4278814196586609
train_iter_loss: 0.37226447463035583
train_iter_loss: 0.3895617127418518
train_iter_loss: 0.3636588752269745
train_iter_loss: 0.15497246384620667
train_iter_loss: 0.28298184275627136
train_iter_loss: 0.2759745717048645
train_iter_loss: 0.2304380238056183
train_iter_loss: 0.11295570433139801
train_iter_loss: 0.35228919982910156
train_iter_loss: 0.2584156394004822
train_iter_loss: 0.2705664038658142
train_iter_loss: 0.16057968139648438
train_iter_loss: 0.2328839898109436
train_iter_loss: 0.15562313795089722
train_iter_loss: 0.1618737429380417
train_iter_loss: 0.12482340633869171
train_iter_loss: 0.2551707327365875
train loss :0.2685
---------------------
Validation seg loss: 0.35242609297505245 at epoch 585
epoch =    586/  1000, exp = train
train_iter_loss: 0.3687824010848999
train_iter_loss: 0.2895474433898926
train_iter_loss: 0.12236637622117996
train_iter_loss: 0.5218334794044495
train_iter_loss: 0.235757976770401
train_iter_loss: 0.23916254937648773
train_iter_loss: 0.3480609357357025
train_iter_loss: 0.2556077837944031
train_iter_loss: 0.20050694048404694
train_iter_loss: 0.3016342222690582
train_iter_loss: 0.12674270570278168
train_iter_loss: 0.16091197729110718
train_iter_loss: 0.3911929726600647
train_iter_loss: 0.3621128499507904
train_iter_loss: 0.40065765380859375
train_iter_loss: 0.3893970847129822
train_iter_loss: 0.43806254863739014
train_iter_loss: 0.2817704379558563
train_iter_loss: 0.4351983666419983
train_iter_loss: 0.33846771717071533
train_iter_loss: 0.2776623070240021
train_iter_loss: 0.21303807199001312
train_iter_loss: 0.14511212706565857
train_iter_loss: 0.32131195068359375
train_iter_loss: 0.20917731523513794
train_iter_loss: 0.18198685348033905
train_iter_loss: 0.2555186152458191
train_iter_loss: 0.36549511551856995
train_iter_loss: 0.33423325419425964
train_iter_loss: 0.4023876488208771
train_iter_loss: 0.5232544541358948
train_iter_loss: 0.1656903177499771
train_iter_loss: 0.26872748136520386
train_iter_loss: 0.17021360993385315
train_iter_loss: 0.18776975572109222
train_iter_loss: 0.187367781996727
train_iter_loss: 0.34221574664115906
train_iter_loss: 0.25109702348709106
train_iter_loss: 0.2858887314796448
train_iter_loss: 0.21786759793758392
train_iter_loss: 0.22788600623607635
train_iter_loss: 0.39870133996009827
train_iter_loss: 0.20282872021198273
train_iter_loss: 0.12656418979167938
train_iter_loss: 0.1646105796098709
train_iter_loss: 0.13723456859588623
train_iter_loss: 0.09718240797519684
train_iter_loss: 0.2919796109199524
train_iter_loss: 0.1600988805294037
train_iter_loss: 0.17478401958942413
train_iter_loss: 0.0418357327580452
train_iter_loss: 0.31654587388038635
train_iter_loss: 0.4915517270565033
train_iter_loss: 0.18314559757709503
train_iter_loss: 0.373896986246109
train_iter_loss: 0.2610294818878174
train_iter_loss: 0.20968186855316162
train_iter_loss: 0.120782770216465
train_iter_loss: 0.33617037534713745
train_iter_loss: 0.11859285831451416
train_iter_loss: 0.2387690246105194
train_iter_loss: 0.2812570333480835
train_iter_loss: 0.2057790458202362
train_iter_loss: 0.2520989179611206
train_iter_loss: 0.36657869815826416
train_iter_loss: 0.2619083821773529
train_iter_loss: 0.2428126484155655
train_iter_loss: 0.28181371092796326
train_iter_loss: 0.3517835736274719
train_iter_loss: 0.23925518989562988
train_iter_loss: 0.27438217401504517
train_iter_loss: 0.2826651334762573
train_iter_loss: 0.22870296239852905
train_iter_loss: 0.19140388071537018
train_iter_loss: 0.374321848154068
train_iter_loss: 0.23629634082317352
train_iter_loss: 0.46065425872802734
train_iter_loss: 0.4047737121582031
train_iter_loss: 0.21831104159355164
train_iter_loss: 0.27343881130218506
train_iter_loss: 0.354166716337204
train_iter_loss: 0.2581741213798523
train_iter_loss: 0.23108525574207306
train_iter_loss: 0.17974503338336945
train_iter_loss: 0.28758543729782104
train_iter_loss: 0.21998055279254913
train_iter_loss: 0.26607662439346313
train_iter_loss: 0.3991689682006836
train_iter_loss: 0.3231203258037567
train_iter_loss: 0.1873200386762619
train_iter_loss: 0.16785821318626404
train_iter_loss: 0.32485124468803406
train_iter_loss: 0.16620059311389923
train_iter_loss: 0.38599732518196106
train_iter_loss: 0.3425684869289398
train_iter_loss: 0.29817384481430054
train_iter_loss: 0.19258323311805725
train_iter_loss: 0.1993117779493332
train_iter_loss: 0.18544229865074158
train_iter_loss: 0.05586431547999382
train loss :0.2691
---------------------
Validation seg loss: 0.35712363253751733 at epoch 586
epoch =    587/  1000, exp = train
train_iter_loss: 0.34969690442085266
train_iter_loss: 0.23994570970535278
train_iter_loss: 0.17971916496753693
train_iter_loss: 0.19649699330329895
train_iter_loss: 0.37826696038246155
train_iter_loss: 0.25299766659736633
train_iter_loss: 0.34511473774909973
train_iter_loss: 0.4149790406227112
train_iter_loss: 0.2456655353307724
train_iter_loss: 0.40206781029701233
train_iter_loss: 0.217568501830101
train_iter_loss: 0.3465184271335602
train_iter_loss: 0.3817105293273926
train_iter_loss: 0.28785809874534607
train_iter_loss: 0.30378568172454834
train_iter_loss: 0.22609294950962067
train_iter_loss: 0.16317957639694214
train_iter_loss: 0.32555392384529114
train_iter_loss: 0.38155144453048706
train_iter_loss: 0.37142977118492126
train_iter_loss: 0.46773195266723633
train_iter_loss: 0.3450677692890167
train_iter_loss: 0.32378172874450684
train_iter_loss: 0.22592277824878693
train_iter_loss: 0.16804729402065277
train_iter_loss: 0.3801272213459015
train_iter_loss: 0.180363729596138
train_iter_loss: 0.2897813320159912
train_iter_loss: 0.3436275124549866
train_iter_loss: 0.310914546251297
train_iter_loss: 0.3262118995189667
train_iter_loss: 0.12172389775514603
train_iter_loss: 0.3572099804878235
train_iter_loss: 0.21275031566619873
train_iter_loss: 0.09395341575145721
train_iter_loss: 0.24486669898033142
train_iter_loss: 0.3034263551235199
train_iter_loss: 0.34090495109558105
train_iter_loss: 0.23324920237064362
train_iter_loss: 0.2921493351459503
train_iter_loss: 0.23888608813285828
train_iter_loss: 0.28915899991989136
train_iter_loss: 0.1606777012348175
train_iter_loss: 0.3548552691936493
train_iter_loss: 0.13875330984592438
train_iter_loss: 0.24604569375514984
train_iter_loss: 0.22086015343666077
train_iter_loss: 0.2742260694503784
train_iter_loss: 0.22633090615272522
train_iter_loss: 0.25453129410743713
train_iter_loss: 0.07617178559303284
train_iter_loss: 0.22643610835075378
train_iter_loss: 0.2989109754562378
train_iter_loss: 0.1220022588968277
train_iter_loss: 0.27476879954338074
train_iter_loss: 0.27634894847869873
train_iter_loss: 0.3134766221046448
train_iter_loss: 0.2586565613746643
train_iter_loss: 0.1740749627351761
train_iter_loss: 0.22657498717308044
train_iter_loss: 0.31983137130737305
train_iter_loss: 0.2021373212337494
train_iter_loss: 0.30365440249443054
train_iter_loss: 0.16473154723644257
train_iter_loss: 0.4425410330295563
train_iter_loss: 0.197222501039505
train_iter_loss: 0.17216776311397552
train_iter_loss: 0.3208405375480652
train_iter_loss: 0.2709115445613861
train_iter_loss: 0.20662786066532135
train_iter_loss: 0.30063480138778687
train_iter_loss: 0.16158950328826904
train_iter_loss: 0.15475690364837646
train_iter_loss: 0.425203800201416
train_iter_loss: 0.1344507932662964
train_iter_loss: 0.1967342346906662
train_iter_loss: 0.26864057779312134
train_iter_loss: 0.3140503168106079
train_iter_loss: 0.25322723388671875
train_iter_loss: 0.19909225404262543
train_iter_loss: 0.3068366050720215
train_iter_loss: 0.18430718779563904
train_iter_loss: 0.19378741085529327
train_iter_loss: 0.2910284996032715
train_iter_loss: 0.282153457403183
train_iter_loss: 0.3251933157444
train_iter_loss: 0.2591187655925751
train_iter_loss: 0.47870996594429016
train_iter_loss: 0.1744217425584793
train_iter_loss: 0.23887379467487335
train_iter_loss: 0.3221935033798218
train_iter_loss: 0.252736896276474
train_iter_loss: 0.30566391348838806
train_iter_loss: 0.3404039740562439
train_iter_loss: 0.23731982707977295
train_iter_loss: 0.26325827836990356
train_iter_loss: 0.12849083542823792
train_iter_loss: 0.2895142138004303
train_iter_loss: 0.21026869118213654
train_iter_loss: 0.16388842463493347
train loss :0.2675
---------------------
Validation seg loss: 0.359767546033803 at epoch 587
epoch =    588/  1000, exp = train
train_iter_loss: 0.2134895622730255
train_iter_loss: 0.22552452981472015
train_iter_loss: 0.2974199056625366
train_iter_loss: 0.2338089495897293
train_iter_loss: 0.13534486293792725
train_iter_loss: 0.24804426729679108
train_iter_loss: 0.21898749470710754
train_iter_loss: 0.1048085168004036
train_iter_loss: 0.25844234228134155
train_iter_loss: 0.22939155995845795
train_iter_loss: 0.2206062525510788
train_iter_loss: 0.2444872111082077
train_iter_loss: 0.23474980890750885
train_iter_loss: 0.7833622097969055
train_iter_loss: 0.24602966010570526
train_iter_loss: 0.3749702274799347
train_iter_loss: 0.1947411745786667
train_iter_loss: 0.15474095940589905
train_iter_loss: 0.13344596326351166
train_iter_loss: 0.3376987874507904
train_iter_loss: 0.1013798788189888
train_iter_loss: 0.25027698278427124
train_iter_loss: 0.31994274258613586
train_iter_loss: 0.15054641664028168
train_iter_loss: 0.18650801479816437
train_iter_loss: 0.23939953744411469
train_iter_loss: 0.35295990109443665
train_iter_loss: 0.1887311041355133
train_iter_loss: 0.13218463957309723
train_iter_loss: 0.15257294476032257
train_iter_loss: 0.527652382850647
train_iter_loss: 0.3806068003177643
train_iter_loss: 0.10478232800960541
train_iter_loss: 0.3528231382369995
train_iter_loss: 0.15111827850341797
train_iter_loss: 0.23686808347702026
train_iter_loss: 0.22237250208854675
train_iter_loss: 0.2540690004825592
train_iter_loss: 0.171308696269989
train_iter_loss: 0.37364861369132996
train_iter_loss: 0.26880475878715515
train_iter_loss: 0.11993972957134247
train_iter_loss: 0.22518764436244965
train_iter_loss: 0.20167340338230133
train_iter_loss: 0.24094811081886292
train_iter_loss: 0.3041740953922272
train_iter_loss: 0.33539554476737976
train_iter_loss: 0.23029571771621704
train_iter_loss: 0.3894275426864624
train_iter_loss: 0.35835790634155273
train_iter_loss: 0.4487445652484894
train_iter_loss: 0.27032166719436646
train_iter_loss: 0.42502114176750183
train_iter_loss: 0.2590617835521698
train_iter_loss: 0.34034463763237
train_iter_loss: 0.29366016387939453
train_iter_loss: 0.14864438772201538
train_iter_loss: 0.3452221751213074
train_iter_loss: 0.1503213793039322
train_iter_loss: 0.3175974488258362
train_iter_loss: 0.23617224395275116
train_iter_loss: 0.2442307025194168
train_iter_loss: 0.29957327246665955
train_iter_loss: 0.2639848291873932
train_iter_loss: 0.34755486249923706
train_iter_loss: 0.32382577657699585
train_iter_loss: 0.21499726176261902
train_iter_loss: 0.3066742420196533
train_iter_loss: 0.21045851707458496
train_iter_loss: 0.34439998865127563
train_iter_loss: 0.3091205060482025
train_iter_loss: 0.2650150954723358
train_iter_loss: 0.2214811146259308
train_iter_loss: 0.23147368431091309
train_iter_loss: 0.17126759886741638
train_iter_loss: 0.23573161661624908
train_iter_loss: 0.3353527784347534
train_iter_loss: 0.2110918164253235
train_iter_loss: 0.3630262315273285
train_iter_loss: 0.14285585284233093
train_iter_loss: 0.29434090852737427
train_iter_loss: 0.31392738223075867
train_iter_loss: 0.39596572518348694
train_iter_loss: 0.18884646892547607
train_iter_loss: 0.19663172960281372
train_iter_loss: 0.23859253525733948
train_iter_loss: 0.14903278648853302
train_iter_loss: 0.3198796510696411
train_iter_loss: 0.3205169141292572
train_iter_loss: 0.24968938529491425
train_iter_loss: 0.23385773599147797
train_iter_loss: 0.2929550111293793
train_iter_loss: 0.42314833402633667
train_iter_loss: 0.3874940276145935
train_iter_loss: 0.23373885452747345
train_iter_loss: 0.22868061065673828
train_iter_loss: 0.399260550737381
train_iter_loss: 0.35114285349845886
train_iter_loss: 0.23337483406066895
train_iter_loss: 0.22316016256809235
train loss :0.2683
---------------------
Validation seg loss: 0.3530490720020783 at epoch 588
epoch =    589/  1000, exp = train
train_iter_loss: 0.2905341386795044
train_iter_loss: 0.33396804332733154
train_iter_loss: 0.3074079751968384
train_iter_loss: 0.49614715576171875
train_iter_loss: 0.3412952721118927
train_iter_loss: 0.3576151132583618
train_iter_loss: 0.151676744222641
train_iter_loss: 0.39186275005340576
train_iter_loss: 0.11478432267904282
train_iter_loss: 0.24240222573280334
train_iter_loss: 0.3751879036426544
train_iter_loss: 0.1451181173324585
train_iter_loss: 0.2868648171424866
train_iter_loss: 0.3789951205253601
train_iter_loss: 0.17175373435020447
train_iter_loss: 0.23240771889686584
train_iter_loss: 0.2338346242904663
train_iter_loss: 0.35948410630226135
train_iter_loss: 0.17373844981193542
train_iter_loss: 0.1283501237630844
train_iter_loss: 0.19511713087558746
train_iter_loss: 0.30627766251564026
train_iter_loss: 0.1752714067697525
train_iter_loss: 0.226619690656662
train_iter_loss: 0.2553267180919647
train_iter_loss: 0.1632688343524933
train_iter_loss: 0.1982482224702835
train_iter_loss: 0.12632322311401367
train_iter_loss: 0.43273597955703735
train_iter_loss: 0.2457723766565323
train_iter_loss: 0.29728272557258606
train_iter_loss: 0.26562410593032837
train_iter_loss: 0.22895632684230804
train_iter_loss: 0.2022797018289566
train_iter_loss: 0.27800342440605164
train_iter_loss: 0.29447025060653687
train_iter_loss: 0.23133130371570587
train_iter_loss: 0.3231379985809326
train_iter_loss: 0.32725390791893005
train_iter_loss: 0.26900413632392883
train_iter_loss: 0.4885826110839844
train_iter_loss: 0.16744133830070496
train_iter_loss: 0.1673293560743332
train_iter_loss: 0.20050117373466492
train_iter_loss: 0.12672187387943268
train_iter_loss: 0.22295229136943817
train_iter_loss: 0.3803326487541199
train_iter_loss: 0.2788955271244049
train_iter_loss: 0.28982770442962646
train_iter_loss: 0.31154534220695496
train_iter_loss: 0.09124352782964706
train_iter_loss: 0.2319076955318451
train_iter_loss: 0.34092405438423157
train_iter_loss: 0.2344689965248108
train_iter_loss: 0.24175751209259033
train_iter_loss: 0.21951924264431
train_iter_loss: 0.2982206344604492
train_iter_loss: 0.2049417942762375
train_iter_loss: 0.21732649207115173
train_iter_loss: 0.14763706922531128
train_iter_loss: 0.2682963013648987
train_iter_loss: 0.20873937010765076
train_iter_loss: 0.32533183693885803
train_iter_loss: 0.30968478322029114
train_iter_loss: 0.22780171036720276
train_iter_loss: 0.2552756071090698
train_iter_loss: 0.29673150181770325
train_iter_loss: 0.3070395886898041
train_iter_loss: 0.11498504877090454
train_iter_loss: 0.38852375745773315
train_iter_loss: 0.34725818037986755
train_iter_loss: 0.20043301582336426
train_iter_loss: 0.2871011793613434
train_iter_loss: 0.1714548021554947
train_iter_loss: 0.3162883520126343
train_iter_loss: 0.32491862773895264
train_iter_loss: 0.2419857382774353
train_iter_loss: 0.2290819138288498
train_iter_loss: 0.6269735097885132
train_iter_loss: 0.3024984896183014
train_iter_loss: 0.2942441999912262
train_iter_loss: 0.36216408014297485
train_iter_loss: 0.2576330304145813
train_iter_loss: 0.20622166991233826
train_iter_loss: 0.4089513123035431
train_iter_loss: 0.3765913248062134
train_iter_loss: 0.25859734416007996
train_iter_loss: 0.126152902841568
train_iter_loss: 0.2359752655029297
train_iter_loss: 0.35495927929878235
train_iter_loss: 0.19701144099235535
train_iter_loss: 0.20437468588352203
train_iter_loss: 0.11819171160459518
train_iter_loss: 0.25500601530075073
train_iter_loss: 0.2683785855770111
train_iter_loss: 0.2329813838005066
train_iter_loss: 0.10545134544372559
train_iter_loss: 0.3747998774051666
train_iter_loss: 0.1709996461868286
train_iter_loss: 0.26605191826820374
train loss :0.2651
---------------------
Validation seg loss: 0.3594496163785598 at epoch 589
epoch =    590/  1000, exp = train
train_iter_loss: 0.32987818121910095
train_iter_loss: 0.2666924297809601
train_iter_loss: 0.3570082187652588
train_iter_loss: 0.3784312605857849
train_iter_loss: 0.3313942551612854
train_iter_loss: 0.34015175700187683
train_iter_loss: 0.3502134680747986
train_iter_loss: 0.38209477066993713
train_iter_loss: 0.150961771607399
train_iter_loss: 0.12635669112205505
train_iter_loss: 0.20737284421920776
train_iter_loss: 0.16800226271152496
train_iter_loss: 0.24347615242004395
train_iter_loss: 0.4064551293849945
train_iter_loss: 0.20806005597114563
train_iter_loss: 0.2662935256958008
train_iter_loss: 0.40472474694252014
train_iter_loss: 0.27977579832077026
train_iter_loss: 0.3945300877094269
train_iter_loss: 0.1309712827205658
train_iter_loss: 0.313001811504364
train_iter_loss: 0.26924198865890503
train_iter_loss: 0.1792280226945877
train_iter_loss: 0.1533249467611313
train_iter_loss: 0.16754716634750366
train_iter_loss: 0.3521626591682434
train_iter_loss: 0.3198339641094208
train_iter_loss: 0.3006543219089508
train_iter_loss: 0.2614761292934418
train_iter_loss: 0.26941582560539246
train_iter_loss: 0.2596365809440613
train_iter_loss: 0.39668986201286316
train_iter_loss: 0.13069331645965576
train_iter_loss: 0.3236302435398102
train_iter_loss: 0.31617698073387146
train_iter_loss: 0.3084449768066406
train_iter_loss: 0.3337564170360565
train_iter_loss: 0.1486085057258606
train_iter_loss: 0.34601515531539917
train_iter_loss: 0.2938852906227112
train_iter_loss: 0.3395520746707916
train_iter_loss: 0.19485388696193695
train_iter_loss: 0.11577444523572922
train_iter_loss: 0.19981132447719574
train_iter_loss: 0.4179163873195648
train_iter_loss: 0.3261394500732422
train_iter_loss: 0.21440626680850983
train_iter_loss: 0.25795841217041016
train_iter_loss: 0.28186002373695374
train_iter_loss: 0.11472261697053909
train_iter_loss: 0.20707184076309204
train_iter_loss: 0.28534650802612305
train_iter_loss: 0.29615873098373413
train_iter_loss: 0.2448115348815918
train_iter_loss: 0.34286484122276306
train_iter_loss: 0.33014044165611267
train_iter_loss: 0.2415543794631958
train_iter_loss: 0.13114085793495178
train_iter_loss: 0.2549354135990143
train_iter_loss: 0.14694370329380035
train_iter_loss: 0.18896420300006866
train_iter_loss: 0.3369998037815094
train_iter_loss: 0.38001906871795654
train_iter_loss: 0.21565842628479004
train_iter_loss: 0.3831705152988434
train_iter_loss: 0.40134695172309875
train_iter_loss: 0.39451345801353455
train_iter_loss: 0.17106367647647858
train_iter_loss: 0.20839278399944305
train_iter_loss: 0.1426241397857666
train_iter_loss: 0.2834842801094055
train_iter_loss: 0.29132378101348877
train_iter_loss: 0.19630667567253113
train_iter_loss: 0.026628563180565834
train_iter_loss: 0.26090312004089355
train_iter_loss: 0.21828490495681763
train_iter_loss: 0.3750933110713959
train_iter_loss: 0.358226478099823
train_iter_loss: 0.2570805251598358
train_iter_loss: 0.282417356967926
train_iter_loss: 0.3330800533294678
train_iter_loss: 0.2545170783996582
train_iter_loss: 0.39369216561317444
train_iter_loss: 0.3000900149345398
train_iter_loss: 0.30437982082366943
train_iter_loss: 0.28183719515800476
train_iter_loss: 0.07800571620464325
train_iter_loss: 0.3042541444301605
train_iter_loss: 0.2086876481771469
train_iter_loss: 0.3439321517944336
train_iter_loss: 0.32804933190345764
train_iter_loss: 0.21509969234466553
train_iter_loss: 0.3042573928833008
train_iter_loss: 0.1894809603691101
train_iter_loss: 0.26491695642471313
train_iter_loss: 0.29875612258911133
train_iter_loss: 0.07819757610559464
train_iter_loss: 0.20555460453033447
train_iter_loss: 0.1922289878129959
train_iter_loss: 0.2725334167480469
train loss :0.2690
---------------------
Validation seg loss: 0.3866788541001953 at epoch 590
epoch =    591/  1000, exp = train
train_iter_loss: 0.2084401249885559
train_iter_loss: 0.2458612024784088
train_iter_loss: 0.21202437579631805
train_iter_loss: 0.23999592661857605
train_iter_loss: 0.43965744972229004
train_iter_loss: 0.19130992889404297
train_iter_loss: 0.12549543380737305
train_iter_loss: 0.42920729517936707
train_iter_loss: 0.14492551982402802
train_iter_loss: 0.3699610233306885
train_iter_loss: 0.28486528992652893
train_iter_loss: 0.29114967584609985
train_iter_loss: 0.11358348280191422
train_iter_loss: 0.2749252915382385
train_iter_loss: 0.332241028547287
train_iter_loss: 0.22358502447605133
train_iter_loss: 0.39850446581840515
train_iter_loss: 0.2741159498691559
train_iter_loss: 0.1796804964542389
train_iter_loss: 0.32274776697158813
train_iter_loss: 0.3772831857204437
train_iter_loss: 0.2952292561531067
train_iter_loss: 0.200560063123703
train_iter_loss: 0.15790051221847534
train_iter_loss: 0.30799809098243713
train_iter_loss: 0.3102603256702423
train_iter_loss: 0.2435392439365387
train_iter_loss: 0.23796960711479187
train_iter_loss: 0.30328673124313354
train_iter_loss: 0.27169960737228394
train_iter_loss: 0.23372820019721985
train_iter_loss: 0.1900259107351303
train_iter_loss: 0.45286688208580017
train_iter_loss: 0.23371721804141998
train_iter_loss: 0.4394919276237488
train_iter_loss: 0.34564489126205444
train_iter_loss: 0.315582275390625
train_iter_loss: 0.32130110263824463
train_iter_loss: 0.2702799439430237
train_iter_loss: 0.1770477592945099
train_iter_loss: 0.2737826108932495
train_iter_loss: 0.13344697654247284
train_iter_loss: 0.5098904371261597
train_iter_loss: 0.17268887162208557
train_iter_loss: 0.37436643242836
train_iter_loss: 0.2537054419517517
train_iter_loss: 0.3035712242126465
train_iter_loss: 0.22832795977592468
train_iter_loss: 0.283181756734848
train_iter_loss: 0.25497570633888245
train_iter_loss: 0.36943522095680237
train_iter_loss: 0.1839185655117035
train_iter_loss: 0.19068288803100586
train_iter_loss: 0.3039097487926483
train_iter_loss: 0.11680591106414795
train_iter_loss: 0.18715551495552063
train_iter_loss: 0.32363367080688477
train_iter_loss: 0.2845538854598999
train_iter_loss: 0.3565121293067932
train_iter_loss: 0.221652552485466
train_iter_loss: 0.11159207671880722
train_iter_loss: 0.26180845499038696
train_iter_loss: 0.25673580169677734
train_iter_loss: 0.28798025846481323
train_iter_loss: 0.17680111527442932
train_iter_loss: 0.20834238827228546
train_iter_loss: 0.25008901953697205
train_iter_loss: 0.26876014471054077
train_iter_loss: 0.31653058528900146
train_iter_loss: 0.17591309547424316
train_iter_loss: 0.2903374135494232
train_iter_loss: 0.255745530128479
train_iter_loss: 0.2010021060705185
train_iter_loss: 0.38950517773628235
train_iter_loss: 0.11975381523370743
train_iter_loss: 0.22000271081924438
train_iter_loss: 0.2892650365829468
train_iter_loss: 0.20673677325248718
train_iter_loss: 0.37533119320869446
train_iter_loss: 0.3292446732521057
train_iter_loss: 0.23984892666339874
train_iter_loss: 0.25753024220466614
train_iter_loss: 0.29712140560150146
train_iter_loss: 0.32724159955978394
train_iter_loss: 0.1855078637599945
train_iter_loss: 0.3109688460826874
train_iter_loss: 0.18090544641017914
train_iter_loss: 0.35376548767089844
train_iter_loss: 0.20775964856147766
train_iter_loss: 0.23187944293022156
train_iter_loss: 0.32264244556427
train_iter_loss: 0.24037471413612366
train_iter_loss: 0.26122012734413147
train_iter_loss: 0.09917186200618744
train_iter_loss: 0.24533355236053467
train_iter_loss: 0.24528174102306366
train_iter_loss: 0.4281311631202698
train_iter_loss: 0.19136746227741241
train_iter_loss: 0.3189205825328827
train_iter_loss: 0.205018550157547
train loss :0.2672
---------------------
Validation seg loss: 0.3653296262974728 at epoch 591
epoch =    592/  1000, exp = train
train_iter_loss: 0.4440135657787323
train_iter_loss: 0.41584596037864685
train_iter_loss: 0.18666961789131165
train_iter_loss: 0.02331535704433918
train_iter_loss: 0.2746717035770416
train_iter_loss: 0.2114904820919037
train_iter_loss: 0.0788894072175026
train_iter_loss: 0.5157251954078674
train_iter_loss: 0.2402457892894745
train_iter_loss: 0.2134665697813034
train_iter_loss: 0.3360021710395813
train_iter_loss: 0.14860031008720398
train_iter_loss: 0.1561761200428009
train_iter_loss: 0.24625910818576813
train_iter_loss: 0.18775883316993713
train_iter_loss: 0.36462002992630005
train_iter_loss: 0.3538702130317688
train_iter_loss: 0.3133369982242584
train_iter_loss: 0.2003277689218521
train_iter_loss: 0.17821025848388672
train_iter_loss: 0.31114840507507324
train_iter_loss: 0.22928772866725922
train_iter_loss: 0.45082876086235046
train_iter_loss: 0.20891720056533813
train_iter_loss: 0.29401877522468567
train_iter_loss: 0.33121389150619507
train_iter_loss: 0.3527452349662781
train_iter_loss: 0.20995987951755524
train_iter_loss: 0.44625595211982727
train_iter_loss: 0.1495135873556137
train_iter_loss: 0.3609791100025177
train_iter_loss: 0.25901371240615845
train_iter_loss: 0.25343552231788635
train_iter_loss: 0.31172630190849304
train_iter_loss: 0.17426934838294983
train_iter_loss: 0.15914246439933777
train_iter_loss: 0.13238288462162018
train_iter_loss: 0.15797604620456696
train_iter_loss: 0.2534380555152893
train_iter_loss: 0.15219208598136902
train_iter_loss: 0.46527740359306335
train_iter_loss: 0.15521779656410217
train_iter_loss: 0.2206815779209137
train_iter_loss: 0.3583647608757019
train_iter_loss: 0.23630617558956146
train_iter_loss: 0.18712516129016876
train_iter_loss: 0.25151562690734863
train_iter_loss: 0.3225485384464264
train_iter_loss: 0.46886390447616577
train_iter_loss: 0.2954877018928528
train_iter_loss: 0.25316959619522095
train_iter_loss: 0.23192325234413147
train_iter_loss: 0.31734704971313477
train_iter_loss: 0.2799712121486664
train_iter_loss: 0.1228184700012207
train_iter_loss: 0.2483990341424942
train_iter_loss: 0.3183230459690094
train_iter_loss: 0.2564900815486908
train_iter_loss: 0.14117015898227692
train_iter_loss: 0.2633332312107086
train_iter_loss: 0.15030771493911743
train_iter_loss: 0.21279993653297424
train_iter_loss: 0.18962092697620392
train_iter_loss: 0.24738188087940216
train_iter_loss: 0.29380109906196594
train_iter_loss: 0.24068112671375275
train_iter_loss: 0.2049916684627533
train_iter_loss: 0.3019569516181946
train_iter_loss: 0.26728156208992004
train_iter_loss: 0.29056283831596375
train_iter_loss: 0.46835535764694214
train_iter_loss: 0.23897558450698853
train_iter_loss: 0.1029718741774559
train_iter_loss: 0.26571419835090637
train_iter_loss: 0.20785221457481384
train_iter_loss: 0.25999993085861206
train_iter_loss: 0.1895170360803604
train_iter_loss: 0.1633724868297577
train_iter_loss: 0.24622181057929993
train_iter_loss: 0.24918097257614136
train_iter_loss: 0.194387286901474
train_iter_loss: 0.3720005452632904
train_iter_loss: 0.2720814347267151
train_iter_loss: 0.23879851400852203
train_iter_loss: 0.46385031938552856
train_iter_loss: 0.13822267949581146
train_iter_loss: 0.3962371051311493
train_iter_loss: 0.2663140296936035
train_iter_loss: 0.37821170687675476
train_iter_loss: 0.15301887691020966
train_iter_loss: 0.333638459444046
train_iter_loss: 0.2549078166484833
train_iter_loss: 0.25058287382125854
train_iter_loss: 0.37758222222328186
train_iter_loss: 0.37299734354019165
train_iter_loss: 0.28152236342430115
train_iter_loss: 0.29082924127578735
train_iter_loss: 0.23383961617946625
train_iter_loss: 0.3572523593902588
train_iter_loss: 0.3167850375175476
train loss :0.2668
---------------------
Validation seg loss: 0.36894589848816395 at epoch 592
epoch =    593/  1000, exp = train
train_iter_loss: 0.2080940306186676
train_iter_loss: 0.16768737137317657
train_iter_loss: 0.27203255891799927
train_iter_loss: 0.35237836837768555
train_iter_loss: 0.25020867586135864
train_iter_loss: 0.11383933573961258
train_iter_loss: 0.26663053035736084
train_iter_loss: 0.399045467376709
train_iter_loss: 0.1837797611951828
train_iter_loss: 0.3425948917865753
train_iter_loss: 0.1395559161901474
train_iter_loss: 0.287243515253067
train_iter_loss: 0.31120193004608154
train_iter_loss: 0.3740827739238739
train_iter_loss: 0.2194443792104721
train_iter_loss: 0.17261524498462677
train_iter_loss: 0.2382509559392929
train_iter_loss: 0.19331708550453186
train_iter_loss: 0.31396251916885376
train_iter_loss: 0.39642244577407837
train_iter_loss: 0.3082532286643982
train_iter_loss: 0.2984853684902191
train_iter_loss: 0.31342992186546326
train_iter_loss: 0.2843382656574249
train_iter_loss: 0.2948003113269806
train_iter_loss: 0.14607161283493042
train_iter_loss: 0.35876592993736267
train_iter_loss: 0.1451391577720642
train_iter_loss: 0.222365602850914
train_iter_loss: 0.2680191695690155
train_iter_loss: 0.2636561989784241
train_iter_loss: 0.1839527040719986
train_iter_loss: 0.3927602767944336
train_iter_loss: 0.14234091341495514
train_iter_loss: 0.250863641500473
train_iter_loss: 0.10075698047876358
train_iter_loss: 0.2874666750431061
train_iter_loss: 0.34940025210380554
train_iter_loss: 0.2865545153617859
train_iter_loss: 0.11171714216470718
train_iter_loss: 0.1721160113811493
train_iter_loss: 0.328248530626297
train_iter_loss: 0.20904240012168884
train_iter_loss: 0.13510951399803162
train_iter_loss: 0.3016135096549988
train_iter_loss: 0.2232818901538849
train_iter_loss: 0.13812664151191711
train_iter_loss: 0.33228880167007446
train_iter_loss: 0.2420232594013214
train_iter_loss: 0.5232671499252319
train_iter_loss: 0.10790009796619415
train_iter_loss: 0.4542834460735321
train_iter_loss: 0.2534046173095703
train_iter_loss: 0.34781041741371155
train_iter_loss: 0.3251288831233978
train_iter_loss: 0.23724447190761566
train_iter_loss: 0.3700140714645386
train_iter_loss: 0.2911473214626312
train_iter_loss: 0.3102213740348816
train_iter_loss: 0.19851526618003845
train_iter_loss: 0.19678986072540283
train_iter_loss: 0.22661446034908295
train_iter_loss: 0.32409197092056274
train_iter_loss: 0.3335156738758087
train_iter_loss: 0.2602429986000061
train_iter_loss: 0.30710896849632263
train_iter_loss: 0.13393095135688782
train_iter_loss: 0.18281370401382446
train_iter_loss: 0.2969127595424652
train_iter_loss: 0.39397427439689636
train_iter_loss: 0.37314462661743164
train_iter_loss: 0.3896249234676361
train_iter_loss: 0.3834415376186371
train_iter_loss: 0.20497901737689972
train_iter_loss: 0.18264085054397583
train_iter_loss: 0.33138877153396606
train_iter_loss: 0.2359236180782318
train_iter_loss: 0.3102976083755493
train_iter_loss: 0.1411084085702896
train_iter_loss: 0.3214442729949951
train_iter_loss: 0.3229810893535614
train_iter_loss: 0.17687571048736572
train_iter_loss: 0.2630120515823364
train_iter_loss: 0.30894359946250916
train_iter_loss: 0.2853820323944092
train_iter_loss: 0.38686642050743103
train_iter_loss: 0.2304549664258957
train_iter_loss: 0.41179507970809937
train_iter_loss: 0.2561918795108795
train_iter_loss: 0.24464553594589233
train_iter_loss: 0.2969798743724823
train_iter_loss: 0.169746533036232
train_iter_loss: 0.3767431676387787
train_iter_loss: 0.24003274738788605
train_iter_loss: 0.41773396730422974
train_iter_loss: 0.2923808991909027
train_iter_loss: 0.25202319025993347
train_iter_loss: 0.28874871134757996
train_iter_loss: 0.3249778151512146
train_iter_loss: 0.06739970296621323
train loss :0.2712
---------------------
Validation seg loss: 0.3547214127479578 at epoch 593
epoch =    594/  1000, exp = train
train_iter_loss: 0.20761829614639282
train_iter_loss: 0.4532419443130493
train_iter_loss: 0.36386820673942566
train_iter_loss: 0.1335337907075882
train_iter_loss: 0.2892656922340393
train_iter_loss: 0.2765258848667145
train_iter_loss: 0.3034103214740753
train_iter_loss: 0.2802828252315521
train_iter_loss: 0.22548818588256836
train_iter_loss: 0.325547993183136
train_iter_loss: 0.2739109694957733
train_iter_loss: 0.17982034385204315
train_iter_loss: 0.1625455915927887
train_iter_loss: 0.08947643637657166
train_iter_loss: 0.3381689190864563
train_iter_loss: 0.3131767511367798
train_iter_loss: 0.18104031682014465
train_iter_loss: 0.33432725071907043
train_iter_loss: 0.29130133986473083
train_iter_loss: 0.3122974932193756
train_iter_loss: 0.22022302448749542
train_iter_loss: 0.15063124895095825
train_iter_loss: 0.24967028200626373
train_iter_loss: 0.09027823060750961
train_iter_loss: 0.3531591594219208
train_iter_loss: 0.24659311771392822
train_iter_loss: 0.22089193761348724
train_iter_loss: 0.36954426765441895
train_iter_loss: 0.23735210299491882
train_iter_loss: 0.19365912675857544
train_iter_loss: 0.2594588100910187
train_iter_loss: 0.3083646893501282
train_iter_loss: 0.22559843957424164
train_iter_loss: 0.18297584354877472
train_iter_loss: 0.1997649073600769
train_iter_loss: 0.18123957514762878
train_iter_loss: 0.19540758430957794
train_iter_loss: 0.25906747579574585
train_iter_loss: 0.3432570695877075
train_iter_loss: 0.1177695095539093
train_iter_loss: 0.15255196392536163
train_iter_loss: 0.5068600177764893
train_iter_loss: 0.2418760359287262
train_iter_loss: 0.27946168184280396
train_iter_loss: 0.48177456855773926
train_iter_loss: 0.14642557501792908
train_iter_loss: 0.292465478181839
train_iter_loss: 0.35313862562179565
train_iter_loss: 0.3027136027812958
train_iter_loss: 0.15863411128520966
train_iter_loss: 0.20197561383247375
train_iter_loss: 0.34957820177078247
train_iter_loss: 0.3899620771408081
train_iter_loss: 0.31749892234802246
train_iter_loss: 0.3699153959751129
train_iter_loss: 0.3820340037345886
train_iter_loss: 0.11401765048503876
train_iter_loss: 0.37652039527893066
train_iter_loss: 0.24052461981773376
train_iter_loss: 0.3116569221019745
train_iter_loss: 0.08881278336048126
train_iter_loss: 0.2524856626987457
train_iter_loss: 0.2929776608943939
train_iter_loss: 0.2633016109466553
train_iter_loss: 0.2150074690580368
train_iter_loss: 0.29171326756477356
train_iter_loss: 0.3263307809829712
train_iter_loss: 0.28473642468452454
train_iter_loss: 0.40642601251602173
train_iter_loss: 0.13551217317581177
train_iter_loss: 0.3436376750469208
train_iter_loss: 0.22236597537994385
train_iter_loss: 0.12438388168811798
train_iter_loss: 0.27195554971694946
train_iter_loss: 0.31535017490386963
train_iter_loss: 0.33703669905662537
train_iter_loss: 0.32200756669044495
train_iter_loss: 0.258263498544693
train_iter_loss: 0.1740165501832962
train_iter_loss: 0.2125854641199112
train_iter_loss: 0.38883721828460693
train_iter_loss: 0.16283366084098816
train_iter_loss: 0.27907609939575195
train_iter_loss: 0.24760626256465912
train_iter_loss: 0.18225215375423431
train_iter_loss: 0.31313252449035645
train_iter_loss: 0.22170518338680267
train_iter_loss: 0.26956281065940857
train_iter_loss: 0.3752281367778778
train_iter_loss: 0.3305463492870331
train_iter_loss: 0.22491709887981415
train_iter_loss: 0.33816638588905334
train_iter_loss: 0.17460910975933075
train_iter_loss: 0.3417923152446747
train_iter_loss: 0.30279937386512756
train_iter_loss: 0.2753034830093384
train_iter_loss: 0.3783136308193207
train_iter_loss: 0.21900054812431335
train_iter_loss: 0.31342294812202454
train_iter_loss: 0.3181869387626648
train loss :0.2698
---------------------
Validation seg loss: 0.37210903120367733 at epoch 594
epoch =    595/  1000, exp = train
train_iter_loss: 0.26497769355773926
train_iter_loss: 0.1266053467988968
train_iter_loss: 0.42538365721702576
train_iter_loss: 0.282382994890213
train_iter_loss: 0.37054169178009033
train_iter_loss: 0.19832274317741394
train_iter_loss: 0.2684265673160553
train_iter_loss: 0.31817826628685
train_iter_loss: 0.23855099081993103
train_iter_loss: 0.2243143618106842
train_iter_loss: 0.2467850148677826
train_iter_loss: 0.14467038214206696
train_iter_loss: 0.2525942623615265
train_iter_loss: 0.40776509046554565
train_iter_loss: 0.16104869544506073
train_iter_loss: 0.1905469447374344
train_iter_loss: 0.1255757361650467
train_iter_loss: 0.29328665137290955
train_iter_loss: 0.17302457988262177
train_iter_loss: 0.18893606960773468
train_iter_loss: 0.25125348567962646
train_iter_loss: 0.24000109732151031
train_iter_loss: 0.3136546313762665
train_iter_loss: 0.310770720243454
train_iter_loss: 0.12896303832530975
train_iter_loss: 0.16843082010746002
train_iter_loss: 0.26321443915367126
train_iter_loss: 0.2737743854522705
train_iter_loss: 0.29133644700050354
train_iter_loss: 0.34338921308517456
train_iter_loss: 0.20027202367782593
train_iter_loss: 0.19176869094371796
train_iter_loss: 0.23264062404632568
train_iter_loss: 0.3190995752811432
train_iter_loss: 0.1600501984357834
train_iter_loss: 0.15391334891319275
train_iter_loss: 0.41861292719841003
train_iter_loss: 0.4465150535106659
train_iter_loss: 0.22850561141967773
train_iter_loss: 0.2148554027080536
train_iter_loss: 0.3117963969707489
train_iter_loss: 0.2344435304403305
train_iter_loss: 0.30933448672294617
train_iter_loss: 0.1998036503791809
train_iter_loss: 0.19161716103553772
train_iter_loss: 0.3301527202129364
train_iter_loss: 0.2867276668548584
train_iter_loss: 0.3011675179004669
train_iter_loss: 0.46213051676750183
train_iter_loss: 0.2942660450935364
train_iter_loss: 0.15105365216732025
train_iter_loss: 0.26614490151405334
train_iter_loss: 0.16666565835475922
train_iter_loss: 0.2881743907928467
train_iter_loss: 0.34187883138656616
train_iter_loss: 0.27482470870018005
train_iter_loss: 0.19192533195018768
train_iter_loss: 0.24803468585014343
train_iter_loss: 0.2104668915271759
train_iter_loss: 0.2414936125278473
train_iter_loss: 0.29765284061431885
train_iter_loss: 0.3730732202529907
train_iter_loss: 0.15218544006347656
train_iter_loss: 0.36884641647338867
train_iter_loss: 0.2836899757385254
train_iter_loss: 0.08874580264091492
train_iter_loss: 0.3898155689239502
train_iter_loss: 0.17957335710525513
train_iter_loss: 0.20250678062438965
train_iter_loss: 0.369836688041687
train_iter_loss: 0.2867189943790436
train_iter_loss: 0.1469329297542572
train_iter_loss: 0.18372507393360138
train_iter_loss: 0.2785350978374481
train_iter_loss: 0.2708594799041748
train_iter_loss: 0.2690991759300232
train_iter_loss: 0.14559397101402283
train_iter_loss: 0.2174675613641739
train_iter_loss: 0.3256201446056366
train_iter_loss: 0.3768603801727295
train_iter_loss: 0.2702626883983612
train_iter_loss: 0.320791631937027
train_iter_loss: 0.32127895951271057
train_iter_loss: 0.26934006810188293
train_iter_loss: 0.3709101676940918
train_iter_loss: 0.3486998379230499
train_iter_loss: 0.25692543387413025
train_iter_loss: 0.21895115077495575
train_iter_loss: 0.2704838514328003
train_iter_loss: 0.2240607887506485
train_iter_loss: 0.24161577224731445
train_iter_loss: 0.28825637698173523
train_iter_loss: 0.1555020660161972
train_iter_loss: 0.32814615964889526
train_iter_loss: 0.2528710961341858
train_iter_loss: 0.27672892808914185
train_iter_loss: 0.257394403219223
train_iter_loss: 0.13211755454540253
train_iter_loss: 0.3810689449310303
train_iter_loss: 0.3177243173122406
train loss :0.2634
---------------------
Validation seg loss: 0.3659669542860872 at epoch 595
epoch =    596/  1000, exp = train
train_iter_loss: 0.05472401902079582
train_iter_loss: 0.26872122287750244
train_iter_loss: 0.11226899921894073
train_iter_loss: 0.30044490098953247
train_iter_loss: 0.12907123565673828
train_iter_loss: 0.25235337018966675
train_iter_loss: 0.25514206290245056
train_iter_loss: 0.42240527272224426
train_iter_loss: 0.14992061257362366
train_iter_loss: 0.2311333268880844
train_iter_loss: 0.23047871887683868
train_iter_loss: 0.37438392639160156
train_iter_loss: 0.42275723814964294
train_iter_loss: 0.2291286736726761
train_iter_loss: 0.2017613649368286
train_iter_loss: 0.22562278807163239
train_iter_loss: 0.1797581911087036
train_iter_loss: 0.26621726155281067
train_iter_loss: 0.28470224142074585
train_iter_loss: 0.2500925362110138
train_iter_loss: 0.10502204298973083
train_iter_loss: 0.3150847852230072
train_iter_loss: 0.20024119317531586
train_iter_loss: 0.28797677159309387
train_iter_loss: 0.23242603242397308
train_iter_loss: 0.32090312242507935
train_iter_loss: 0.3020741045475006
train_iter_loss: 0.2426680028438568
train_iter_loss: 0.27992770075798035
train_iter_loss: 0.08904636651277542
train_iter_loss: 0.2852664887905121
train_iter_loss: 0.2819020748138428
train_iter_loss: 0.03382837027311325
train_iter_loss: 0.3205702602863312
train_iter_loss: 0.33002784848213196
train_iter_loss: 0.17077317833900452
train_iter_loss: 0.16996462643146515
train_iter_loss: 0.280165433883667
train_iter_loss: 0.2506228983402252
train_iter_loss: 0.3407570719718933
train_iter_loss: 0.2018292397260666
train_iter_loss: 0.32266929745674133
train_iter_loss: 0.2943738102912903
train_iter_loss: 0.3020901083946228
train_iter_loss: 0.3104415237903595
train_iter_loss: 0.14461848139762878
train_iter_loss: 0.24795159697532654
train_iter_loss: 0.2219489961862564
train_iter_loss: 0.1945796012878418
train_iter_loss: 0.2857581377029419
train_iter_loss: 0.224058598279953
train_iter_loss: 0.14001643657684326
train_iter_loss: 0.2768633961677551
train_iter_loss: 0.30153635144233704
train_iter_loss: 0.24421438574790955
train_iter_loss: 0.15812847018241882
train_iter_loss: 0.14211499691009521
train_iter_loss: 0.29206153750419617
train_iter_loss: 0.373457133769989
train_iter_loss: 0.3512386381626129
train_iter_loss: 0.376213014125824
train_iter_loss: 0.15379826724529266
train_iter_loss: 0.1765424758195877
train_iter_loss: 0.35973724722862244
train_iter_loss: 0.2132578194141388
train_iter_loss: 0.3248344361782074
train_iter_loss: 0.32898157835006714
train_iter_loss: 0.32577162981033325
train_iter_loss: 0.23993955552577972
train_iter_loss: 0.2816932499408722
train_iter_loss: 0.2919209599494934
train_iter_loss: 0.20267848670482635
train_iter_loss: 0.3508759140968323
train_iter_loss: 0.28117045760154724
train_iter_loss: 0.27367472648620605
train_iter_loss: 0.35961055755615234
train_iter_loss: 0.31987902522087097
train_iter_loss: 0.3055651783943176
train_iter_loss: 0.12327584624290466
train_iter_loss: 0.27385905385017395
train_iter_loss: 0.36747634410858154
train_iter_loss: 0.20859916508197784
train_iter_loss: 0.27907946705818176
train_iter_loss: 0.2838791310787201
train_iter_loss: 0.27996042370796204
train_iter_loss: 0.17034004628658295
train_iter_loss: 0.3274114727973938
train_iter_loss: 0.3422510027885437
train_iter_loss: 0.2390642613172531
train_iter_loss: 0.4422728717327118
train_iter_loss: 0.28463420271873474
train_iter_loss: 0.2206854522228241
train_iter_loss: 0.37687385082244873
train_iter_loss: 0.19504904747009277
train_iter_loss: 0.5091171860694885
train_iter_loss: 0.40167006850242615
train_iter_loss: 0.2927688956260681
train_iter_loss: 0.27226945757865906
train_iter_loss: 0.300016313791275
train_iter_loss: 0.33200904726982117
train loss :0.2666
---------------------
Validation seg loss: 0.3485177499505709 at epoch 596
epoch =    597/  1000, exp = train
train_iter_loss: 0.23873434960842133
train_iter_loss: 0.285001665353775
train_iter_loss: 0.24205981194972992
train_iter_loss: 0.2981298565864563
train_iter_loss: 0.2511720061302185
train_iter_loss: 0.2509095370769501
train_iter_loss: 0.14623034000396729
train_iter_loss: 0.23003888130187988
train_iter_loss: 0.16865985095500946
train_iter_loss: 0.19473692774772644
train_iter_loss: 0.10420504212379456
train_iter_loss: 0.2682785391807556
train_iter_loss: 0.39845481514930725
train_iter_loss: 0.29356130957603455
train_iter_loss: 0.3789096176624298
train_iter_loss: 0.34751713275909424
train_iter_loss: 0.17135849595069885
train_iter_loss: 0.37690097093582153
train_iter_loss: 0.497674822807312
train_iter_loss: 0.28714412450790405
train_iter_loss: 0.26524361968040466
train_iter_loss: 0.23914746940135956
train_iter_loss: 0.243374302983284
train_iter_loss: 0.11848480999469757
train_iter_loss: 0.3124188482761383
train_iter_loss: 0.27849170565605164
train_iter_loss: 0.21760672330856323
train_iter_loss: 0.23093688488006592
train_iter_loss: 0.30661529302597046
train_iter_loss: 0.20627115666866302
train_iter_loss: 0.17591305077075958
train_iter_loss: 0.2556915283203125
train_iter_loss: 0.08084987103939056
train_iter_loss: 0.4256357252597809
train_iter_loss: 0.16176407039165497
train_iter_loss: 0.1878242939710617
train_iter_loss: 0.13310453295707703
train_iter_loss: 0.19372302293777466
train_iter_loss: 0.5140638947486877
train_iter_loss: 0.3500136733055115
train_iter_loss: 0.31678253412246704
train_iter_loss: 0.31384602189064026
train_iter_loss: 0.12943333387374878
train_iter_loss: 0.22427305579185486
train_iter_loss: 0.2985946834087372
train_iter_loss: 0.5149136185646057
train_iter_loss: 0.1311049610376358
train_iter_loss: 0.28245800733566284
train_iter_loss: 0.34352195262908936
train_iter_loss: 0.29523250460624695
train_iter_loss: 0.28807657957077026
train_iter_loss: 0.26054394245147705
train_iter_loss: 0.15701480209827423
train_iter_loss: 0.21885541081428528
train_iter_loss: 0.27331891655921936
train_iter_loss: 0.2289201021194458
train_iter_loss: 0.25726351141929626
train_iter_loss: 0.20757968723773956
train_iter_loss: 0.3837678134441376
train_iter_loss: 0.26427215337753296
train_iter_loss: 0.2214645892381668
train_iter_loss: 0.39399808645248413
train_iter_loss: 0.26358869671821594
train_iter_loss: 0.2829826772212982
train_iter_loss: 0.3319535553455353
train_iter_loss: 0.340030699968338
train_iter_loss: 0.3352779448032379
train_iter_loss: 0.20729053020477295
train_iter_loss: 0.3108311593532562
train_iter_loss: 0.24793225526809692
train_iter_loss: 0.20129357278347015
train_iter_loss: 0.30305641889572144
train_iter_loss: 0.24362404644489288
train_iter_loss: 0.156052365899086
train_iter_loss: 0.4041977524757385
train_iter_loss: 0.08165749907493591
train_iter_loss: 0.24326695501804352
train_iter_loss: 0.23173831403255463
train_iter_loss: 0.15750105679035187
train_iter_loss: 0.19579096138477325
train_iter_loss: 0.32192420959472656
train_iter_loss: 0.2212579995393753
train_iter_loss: 0.2818865478038788
train_iter_loss: 0.36617985367774963
train_iter_loss: 0.28269320726394653
train_iter_loss: 0.4316658079624176
train_iter_loss: 0.21560026705265045
train_iter_loss: 0.17765279114246368
train_iter_loss: 0.33869802951812744
train_iter_loss: 0.26385849714279175
train_iter_loss: 0.28860682249069214
train_iter_loss: 0.20822472870349884
train_iter_loss: 0.3775424659252167
train_iter_loss: 0.31216421723365784
train_iter_loss: 0.18439286947250366
train_iter_loss: 0.30793121457099915
train_iter_loss: 0.3563655614852905
train_iter_loss: 0.14428256452083588
train_iter_loss: 0.24006956815719604
train_iter_loss: 0.3060358166694641
train loss :0.2676
---------------------
Validation seg loss: 0.3594939128961414 at epoch 597
epoch =    598/  1000, exp = train
train_iter_loss: 0.28980696201324463
train_iter_loss: 0.28448769450187683
train_iter_loss: 0.28738659620285034
train_iter_loss: 0.13696813583374023
train_iter_loss: 0.643593966960907
train_iter_loss: 0.24698077142238617
train_iter_loss: 0.33278512954711914
train_iter_loss: 0.1967233419418335
train_iter_loss: 0.24093352258205414
train_iter_loss: 0.02202773094177246
train_iter_loss: 0.15502646565437317
train_iter_loss: 0.18560227751731873
train_iter_loss: 0.2390655279159546
train_iter_loss: 0.2875009775161743
train_iter_loss: 0.29480603337287903
train_iter_loss: 0.21198414266109467
train_iter_loss: 0.23115484416484833
train_iter_loss: 0.2998438775539398
train_iter_loss: 0.25581616163253784
train_iter_loss: 0.15443220734596252
train_iter_loss: 0.07099862396717072
train_iter_loss: 0.15153005719184875
train_iter_loss: 0.151004359126091
train_iter_loss: 0.13132862746715546
train_iter_loss: 0.23505103588104248
train_iter_loss: 0.3514799177646637
train_iter_loss: 0.3576304614543915
train_iter_loss: 0.28067663311958313
train_iter_loss: 0.25085631012916565
train_iter_loss: 0.25942423939704895
train_iter_loss: 0.2621563971042633
train_iter_loss: 0.2382412999868393
train_iter_loss: 0.348417192697525
train_iter_loss: 0.033436622470617294
train_iter_loss: 0.4104093909263611
train_iter_loss: 0.2018003761768341
train_iter_loss: 0.2158609926700592
train_iter_loss: 0.385968416929245
train_iter_loss: 0.36846455931663513
train_iter_loss: 0.2930116057395935
train_iter_loss: 0.3172697126865387
train_iter_loss: 0.240550234913826
train_iter_loss: 0.41442787647247314
train_iter_loss: 0.22351089119911194
train_iter_loss: 0.24115139245986938
train_iter_loss: 0.3133872449398041
train_iter_loss: 0.3826345205307007
train_iter_loss: 0.3003416061401367
train_iter_loss: 0.2749355435371399
train_iter_loss: 0.18232965469360352
train_iter_loss: 0.3292795419692993
train_iter_loss: 0.35580626130104065
train_iter_loss: 0.14592929184436798
train_iter_loss: 0.2515972852706909
train_iter_loss: 0.3296077847480774
train_iter_loss: 0.30158671736717224
train_iter_loss: 0.32251524925231934
train_iter_loss: 0.2656588554382324
train_iter_loss: 0.1739846169948578
train_iter_loss: 0.12744015455245972
train_iter_loss: 0.29419997334480286
train_iter_loss: 0.6549673080444336
train_iter_loss: 0.2707764804363251
train_iter_loss: 0.25777262449264526
train_iter_loss: 0.32866764068603516
train_iter_loss: 0.36327606439590454
train_iter_loss: 0.3055165112018585
train_iter_loss: 0.2352432906627655
train_iter_loss: 0.361330509185791
train_iter_loss: 0.16407018899917603
train_iter_loss: 0.26295340061187744
train_iter_loss: 0.1714743673801422
train_iter_loss: 0.2650512754917145
train_iter_loss: 0.3099699318408966
train_iter_loss: 0.21146586537361145
train_iter_loss: 0.28420305252075195
train_iter_loss: 0.31164565682411194
train_iter_loss: 0.31192904710769653
train_iter_loss: 0.3574039041996002
train_iter_loss: 0.10370837152004242
train_iter_loss: 0.2801996171474457
train_iter_loss: 0.11154887080192566
train_iter_loss: 0.2487730085849762
train_iter_loss: 0.35904744267463684
train_iter_loss: 0.5200324058532715
train_iter_loss: 0.29390040040016174
train_iter_loss: 0.27321940660476685
train_iter_loss: 0.25845828652381897
train_iter_loss: 0.43524593114852905
train_iter_loss: 0.4387786090373993
train_iter_loss: 0.39715877175331116
train_iter_loss: 0.2040134072303772
train_iter_loss: 0.2635065019130707
train_iter_loss: 0.308807909488678
train_iter_loss: 0.14249339699745178
train_iter_loss: 0.25179630517959595
train_iter_loss: 0.2615545094013214
train_iter_loss: 0.1254080981016159
train_iter_loss: 0.3799118101596832
train_iter_loss: 0.28655508160591125
train loss :0.2742
---------------------
Validation seg loss: 0.3595096983525129 at epoch 598
epoch =    599/  1000, exp = train
train_iter_loss: 0.28009673953056335
train_iter_loss: 0.1966906636953354
train_iter_loss: 0.12509700655937195
train_iter_loss: 0.24847465753555298
train_iter_loss: 0.2495856136083603
train_iter_loss: 0.30041199922561646
train_iter_loss: 0.20357392728328705
train_iter_loss: 0.29754480719566345
train_iter_loss: 0.2758010923862457
train_iter_loss: 0.1849660724401474
train_iter_loss: 0.08983764052391052
train_iter_loss: 0.33433613181114197
train_iter_loss: 0.23168852925300598
train_iter_loss: 0.39228612184524536
train_iter_loss: 0.13912664353847504
train_iter_loss: 0.22742941975593567
train_iter_loss: 0.36703377962112427
train_iter_loss: 0.18297874927520752
train_iter_loss: 0.3467251658439636
train_iter_loss: 0.22854316234588623
train_iter_loss: 0.21872830390930176
train_iter_loss: 0.34817245602607727
train_iter_loss: 0.11893172562122345
train_iter_loss: 0.19191910326480865
train_iter_loss: 0.3194288909435272
train_iter_loss: 0.36867377161979675
train_iter_loss: 0.34843897819519043
train_iter_loss: 0.18034255504608154
train_iter_loss: 0.2562441825866699
train_iter_loss: 0.23064497113227844
train_iter_loss: 0.1362101286649704
train_iter_loss: 0.4036412537097931
train_iter_loss: 0.22173479199409485
train_iter_loss: 0.37489843368530273
train_iter_loss: 0.15788324177265167
train_iter_loss: 0.23349501192569733
train_iter_loss: 0.2501063942909241
train_iter_loss: 0.29626375436782837
train_iter_loss: 0.25126001238822937
train_iter_loss: 0.12387342751026154
train_iter_loss: 0.42798882722854614
train_iter_loss: 0.1969008594751358
train_iter_loss: 0.25257205963134766
train_iter_loss: 0.19399161636829376
train_iter_loss: 0.24116194248199463
train_iter_loss: 0.2829730212688446
train_iter_loss: 0.38479623198509216
train_iter_loss: 0.3446446657180786
train_iter_loss: 0.37709930539131165
train_iter_loss: 0.20678207278251648
train_iter_loss: 0.3527430593967438
train_iter_loss: 0.1915426403284073
train_iter_loss: 0.43302926421165466
train_iter_loss: 0.33141690492630005
train_iter_loss: 0.3437243103981018
train_iter_loss: 0.4217682480812073
train_iter_loss: 0.31339365243911743
train_iter_loss: 0.28566452860832214
train_iter_loss: 0.4382968544960022
train_iter_loss: 0.3640936315059662
train_iter_loss: 0.25523272156715393
train_iter_loss: 0.36660444736480713
train_iter_loss: 0.25671741366386414
train_iter_loss: 0.23650862276554108
train_iter_loss: 0.1786917895078659
train_iter_loss: 0.1369633674621582
train_iter_loss: 0.31393134593963623
train_iter_loss: 0.3055364191532135
train_iter_loss: 0.34397751092910767
train_iter_loss: 0.27115926146507263
train_iter_loss: 0.3670024871826172
train_iter_loss: 0.15543843805789948
train_iter_loss: 0.3641546964645386
train_iter_loss: 0.2563655376434326
train_iter_loss: 0.20258298516273499
train_iter_loss: 0.24381478130817413
train_iter_loss: 0.17356814444065094
train_iter_loss: 0.3473060727119446
train_iter_loss: 0.2883983254432678
train_iter_loss: 0.26863422989845276
train_iter_loss: 0.2719246447086334
train_iter_loss: 0.3213824927806854
train_iter_loss: 0.2996065616607666
train_iter_loss: 0.1970650851726532
train_iter_loss: 0.23630113899707794
train_iter_loss: 0.2143913358449936
train_iter_loss: 0.16696342825889587
train_iter_loss: 0.1295643448829651
train_iter_loss: 0.2298225313425064
train_iter_loss: 0.2628032863140106
train_iter_loss: 0.13010169565677643
train_iter_loss: 0.32992711663246155
train_iter_loss: 0.1437842845916748
train_iter_loss: 0.3695021867752075
train_iter_loss: 0.32065534591674805
train_iter_loss: 0.260191947221756
train_iter_loss: 0.27675560116767883
train_iter_loss: 0.4678930938243866
train_iter_loss: 0.18892721831798553
train_iter_loss: 0.22831757366657257
train loss :0.2697
---------------------
Validation seg loss: 0.3508329208114378 at epoch 599
epoch =    600/  1000, exp = train
train_iter_loss: 0.12995335459709167
train_iter_loss: 0.596787691116333
train_iter_loss: 0.29911482334136963
train_iter_loss: 0.1605009287595749
train_iter_loss: 0.21933548152446747
train_iter_loss: 0.24371522665023804
train_iter_loss: 0.12382757663726807
train_iter_loss: 0.1626000851392746
train_iter_loss: 0.19335773587226868
train_iter_loss: 0.29582831263542175
train_iter_loss: 0.33901798725128174
train_iter_loss: 0.14755187928676605
train_iter_loss: 0.3015531897544861
train_iter_loss: 0.37555480003356934
train_iter_loss: 0.07071651518344879
train_iter_loss: 0.31059932708740234
train_iter_loss: 0.24574390053749084
train_iter_loss: 0.1782705932855606
train_iter_loss: 0.1382867842912674
train_iter_loss: 0.15734799206256866
train_iter_loss: 0.1655530333518982
train_iter_loss: 0.11061710864305496
train_iter_loss: 0.12816676497459412
train_iter_loss: 0.32497304677963257
train_iter_loss: 0.18643246591091156
train_iter_loss: 0.22972257435321808
train_iter_loss: 0.4078100919723511
train_iter_loss: 0.16487574577331543
train_iter_loss: 0.2306787371635437
train_iter_loss: 0.08749499171972275
train_iter_loss: 0.2570042908191681
train_iter_loss: 0.19177772104740143
train_iter_loss: 0.19551891088485718
train_iter_loss: 0.2691500186920166
train_iter_loss: 0.3544144928455353
train_iter_loss: 0.2301618754863739
train_iter_loss: 0.2553364634513855
train_iter_loss: 0.5001466870307922
train_iter_loss: 0.36613741517066956
train_iter_loss: 0.3420572280883789
train_iter_loss: 0.3655470311641693
train_iter_loss: 0.15730656683444977
train_iter_loss: 0.16046904027462006
train_iter_loss: 0.2894706130027771
train_iter_loss: 0.3053218722343445
train_iter_loss: 0.24279887974262238
train_iter_loss: 0.488442599773407
train_iter_loss: 0.1752665787935257
train_iter_loss: 0.2127901166677475
train_iter_loss: 0.17113584280014038
train_iter_loss: 0.34620603919029236
train_iter_loss: 0.19909289479255676
train_iter_loss: 0.2765411138534546
train_iter_loss: 0.2348892092704773
train_iter_loss: 0.2950434386730194
train_iter_loss: 0.20778360962867737
train_iter_loss: 0.34807315468788147
train_iter_loss: 0.24274387955665588
train_iter_loss: 0.2799283564090729
train_iter_loss: 0.1640467643737793
train_iter_loss: 0.27708056569099426
train_iter_loss: 0.277064710855484
train_iter_loss: 0.3440447151660919
train_iter_loss: 0.05898892134428024
train_iter_loss: 0.2551068067550659
train_iter_loss: 0.21815475821495056
train_iter_loss: 0.29945865273475647
train_iter_loss: 0.25455647706985474
train_iter_loss: 0.27135974168777466
train_iter_loss: 0.22848811745643616
train_iter_loss: 0.29946088790893555
train_iter_loss: 0.43072593212127686
train_iter_loss: 0.29929664731025696
train_iter_loss: 0.3275517523288727
train_iter_loss: 0.34618687629699707
train_iter_loss: 0.20523755252361298
train_iter_loss: 0.2208663672208786
train_iter_loss: 0.36116746068000793
train_iter_loss: 0.31571823358535767
train_iter_loss: 0.33214330673217773
train_iter_loss: 0.27916038036346436
train_iter_loss: 0.24978917837142944
train_iter_loss: 0.4194094240665436
train_iter_loss: 0.33933961391448975
train_iter_loss: 0.26248201727867126
train_iter_loss: 0.3798058331012726
train_iter_loss: 0.2855681777000427
train_iter_loss: 0.2131577432155609
train_iter_loss: 0.19489534199237823
train_iter_loss: 0.08380550146102905
train_iter_loss: 0.2518793046474457
train_iter_loss: 0.31012290716171265
train_iter_loss: 0.27771562337875366
train_iter_loss: 0.36322885751724243
train_iter_loss: 0.5437066555023193
train_iter_loss: 0.18358029425144196
train_iter_loss: 0.1788349598646164
train_iter_loss: 0.4064551889896393
train_iter_loss: 0.320973664522171
train_iter_loss: 0.29590851068496704
train loss :0.2658
---------------------
Validation seg loss: 0.36679543487569494 at epoch 600
epoch =    601/  1000, exp = train
train_iter_loss: 0.2973994314670563
train_iter_loss: 0.4005548655986786
train_iter_loss: 0.25252050161361694
train_iter_loss: 0.3245147466659546
train_iter_loss: 0.3026447892189026
train_iter_loss: 0.33923017978668213
train_iter_loss: 0.32110971212387085
train_iter_loss: 0.3079802989959717
train_iter_loss: 0.17676542699337006
train_iter_loss: 0.25215572118759155
train_iter_loss: 0.3201259672641754
train_iter_loss: 0.2770041227340698
train_iter_loss: 0.27384740114212036
train_iter_loss: 0.22289735078811646
train_iter_loss: 0.09455310553312302
train_iter_loss: 0.30558761954307556
train_iter_loss: 0.3440966010093689
train_iter_loss: 0.15547491610050201
train_iter_loss: 0.265185683965683
train_iter_loss: 0.1785685420036316
train_iter_loss: 0.2970132529735565
train_iter_loss: 0.2723239064216614
train_iter_loss: 0.300273060798645
train_iter_loss: 0.2366446554660797
train_iter_loss: 0.21914777159690857
train_iter_loss: 0.19798704981803894
train_iter_loss: 0.15645718574523926
train_iter_loss: 0.1728832721710205
train_iter_loss: 0.2907300293445587
train_iter_loss: 0.2838919162750244
train_iter_loss: 0.3158201277256012
train_iter_loss: 0.25557103753089905
train_iter_loss: 0.3008541762828827
train_iter_loss: 0.25609397888183594
train_iter_loss: 0.2849305272102356
train_iter_loss: 0.18861043453216553
train_iter_loss: 0.22955107688903809
train_iter_loss: 0.218763068318367
train_iter_loss: 0.2245696485042572
train_iter_loss: 0.2857203483581543
train_iter_loss: 0.360115647315979
train_iter_loss: 0.4917256534099579
train_iter_loss: 0.2060570865869522
train_iter_loss: 0.3349311053752899
train_iter_loss: 0.13873377442359924
train_iter_loss: 0.18474125862121582
train_iter_loss: 0.16564296185970306
train_iter_loss: 0.2362123280763626
train_iter_loss: 0.09130234271287918
train_iter_loss: 0.2829325497150421
train_iter_loss: 0.171551913022995
train_iter_loss: 0.28837472200393677
train_iter_loss: 0.16155734658241272
train_iter_loss: 0.2465900480747223
train_iter_loss: 0.13907822966575623
train_iter_loss: 0.3455410897731781
train_iter_loss: 0.28601378202438354
train_iter_loss: 0.2705046534538269
train_iter_loss: 0.34635129570961
train_iter_loss: 0.30708661675453186
train_iter_loss: 0.3342539668083191
train_iter_loss: 0.16445258259773254
train_iter_loss: 0.28456950187683105
train_iter_loss: 0.30849581956863403
train_iter_loss: 0.37904301285743713
train_iter_loss: 0.3108556568622589
train_iter_loss: 0.2262723296880722
train_iter_loss: 0.2807255685329437
train_iter_loss: 0.15019363164901733
train_iter_loss: 0.40374353528022766
train_iter_loss: 0.3062533736228943
train_iter_loss: 0.2508634328842163
train_iter_loss: 0.14370585978031158
train_iter_loss: 0.41779178380966187
train_iter_loss: 0.2572233974933624
train_iter_loss: 0.294806569814682
train_iter_loss: 0.24253739416599274
train_iter_loss: 0.23543670773506165
train_iter_loss: 0.24071983993053436
train_iter_loss: 0.3661230206489563
train_iter_loss: 0.12366411834955215
train_iter_loss: 0.31041163206100464
train_iter_loss: 0.15833228826522827
train_iter_loss: 0.3546736538410187
train_iter_loss: 0.32140201330184937
train_iter_loss: 0.24375732243061066
train_iter_loss: 0.2038465291261673
train_iter_loss: 0.24160759150981903
train_iter_loss: 0.40216362476348877
train_iter_loss: 0.27638548612594604
train_iter_loss: 0.08503101766109467
train_iter_loss: 0.34522107243537903
train_iter_loss: 0.19661380350589752
train_iter_loss: 0.37954479455947876
train_iter_loss: 0.18198567628860474
train_iter_loss: 0.3335427939891815
train_iter_loss: 0.26385658979415894
train_iter_loss: 0.2575993537902832
train_iter_loss: 0.39196982979774475
train_iter_loss: 0.11137109249830246
train loss :0.2650
---------------------
Validation seg loss: 0.3507772709752591 at epoch 601
epoch =    602/  1000, exp = train
train_iter_loss: 0.3343814015388489
train_iter_loss: 0.41589823365211487
train_iter_loss: 0.1628142148256302
train_iter_loss: 0.3134520351886749
train_iter_loss: 0.39436522126197815
train_iter_loss: 0.20208391547203064
train_iter_loss: 0.20741784572601318
train_iter_loss: 0.394135445356369
train_iter_loss: 0.20801731944084167
train_iter_loss: 0.4612325131893158
train_iter_loss: 0.21694140136241913
train_iter_loss: 0.3278576731681824
train_iter_loss: 0.2112574577331543
train_iter_loss: 0.27882120013237
train_iter_loss: 0.10966678708791733
train_iter_loss: 0.2689460813999176
train_iter_loss: 0.15713542699813843
train_iter_loss: 0.21511447429656982
train_iter_loss: 0.2581540644168854
train_iter_loss: 0.21966789662837982
train_iter_loss: 0.24109533429145813
train_iter_loss: 0.3398643732070923
train_iter_loss: 0.34100571274757385
train_iter_loss: 0.3539871275424957
train_iter_loss: 0.3060225546360016
train_iter_loss: 0.2397458851337433
train_iter_loss: 0.3304726183414459
train_iter_loss: 0.33058980107307434
train_iter_loss: 0.24116860330104828
train_iter_loss: 0.24079976975917816
train_iter_loss: 0.3401556611061096
train_iter_loss: 0.23509277403354645
train_iter_loss: 0.3292589485645294
train_iter_loss: 0.2646099627017975
train_iter_loss: 0.2887921929359436
train_iter_loss: 0.2656577229499817
train_iter_loss: 0.08582483977079391
train_iter_loss: 0.27421045303344727
train_iter_loss: 0.4085862338542938
train_iter_loss: 0.2769058346748352
train_iter_loss: 0.2814122140407562
train_iter_loss: 0.12440601736307144
train_iter_loss: 0.29638758301734924
train_iter_loss: 0.208719402551651
train_iter_loss: 0.30015048384666443
train_iter_loss: 0.2572481632232666
train_iter_loss: 0.45651113986968994
train_iter_loss: 0.26649218797683716
train_iter_loss: 0.38401246070861816
train_iter_loss: 0.2293478101491928
train_iter_loss: 0.3573807179927826
train_iter_loss: 0.20029191672801971
train_iter_loss: 0.2944341003894806
train_iter_loss: 0.3731999099254608
train_iter_loss: 0.2848941385746002
train_iter_loss: 0.40682709217071533
train_iter_loss: 0.35530874133110046
train_iter_loss: 0.4755527973175049
train_iter_loss: 0.28067272901535034
train_iter_loss: 0.2018839716911316
train_iter_loss: 0.3874908983707428
train_iter_loss: 0.1987869292497635
train_iter_loss: 0.22575612366199493
train_iter_loss: 0.2505088746547699
train_iter_loss: 0.19402599334716797
train_iter_loss: 0.1182364895939827
train_iter_loss: 0.3150857090950012
train_iter_loss: 0.44589343667030334
train_iter_loss: 0.42361024022102356
train_iter_loss: 0.5111324787139893
train_iter_loss: 0.04233710840344429
train_iter_loss: 0.27768412232398987
train_iter_loss: 0.21134218573570251
train_iter_loss: 0.08921299874782562
train_iter_loss: 0.22416521608829498
train_iter_loss: 0.1702558994293213
train_iter_loss: 0.2389654815196991
train_iter_loss: 0.17079941928386688
train_iter_loss: 0.31881585717201233
train_iter_loss: 0.4063548743724823
train_iter_loss: 0.11428820341825485
train_iter_loss: 0.20092733204364777
train_iter_loss: 0.2947019338607788
train_iter_loss: 0.32979288697242737
train_iter_loss: 0.17033226788043976
train_iter_loss: 0.3654089868068695
train_iter_loss: 0.28377747535705566
train_iter_loss: 0.2089879959821701
train_iter_loss: 0.2532476782798767
train_iter_loss: 0.2815409302711487
train_iter_loss: 0.22557105123996735
train_iter_loss: 0.20787525177001953
train_iter_loss: 0.23136907815933228
train_iter_loss: 0.3163188099861145
train_iter_loss: 0.2887677550315857
train_iter_loss: 0.17477168142795563
train_iter_loss: 0.24828669428825378
train_iter_loss: 0.19572363793849945
train_iter_loss: 0.2770116329193115
train_iter_loss: 0.31849583983421326
train loss :0.2760
---------------------
Validation seg loss: 0.36245095070433925 at epoch 602
epoch =    603/  1000, exp = train
train_iter_loss: 0.17493687570095062
train_iter_loss: 0.3085188567638397
train_iter_loss: 0.608302891254425
train_iter_loss: 0.30689936876296997
train_iter_loss: 0.23834167420864105
train_iter_loss: 0.2851763367652893
train_iter_loss: 0.177412748336792
train_iter_loss: 0.22210001945495605
train_iter_loss: 0.14986921846866608
train_iter_loss: 0.2034754455089569
train_iter_loss: 0.32409998774528503
train_iter_loss: 0.4025447964668274
train_iter_loss: 0.2539490759372711
train_iter_loss: 0.3324609696865082
train_iter_loss: 0.18138650059700012
train_iter_loss: 0.279808908700943
train_iter_loss: 0.356315553188324
train_iter_loss: 0.35542502999305725
train_iter_loss: 0.13088607788085938
train_iter_loss: 0.2770445942878723
train_iter_loss: 0.2612180709838867
train_iter_loss: 0.3041537404060364
train_iter_loss: 0.21820761263370514
train_iter_loss: 0.4361548125743866
train_iter_loss: 0.3308486044406891
train_iter_loss: 0.15017420053482056
train_iter_loss: 0.2923574447631836
train_iter_loss: 0.43832024931907654
train_iter_loss: 0.20497682690620422
train_iter_loss: 0.3380534052848816
train_iter_loss: 0.20445264875888824
train_iter_loss: 0.2389405369758606
train_iter_loss: 0.20239156484603882
train_iter_loss: 0.3753929138183594
train_iter_loss: 0.16569550335407257
train_iter_loss: 0.1905284970998764
train_iter_loss: 0.18432579934597015
train_iter_loss: 0.20510095357894897
train_iter_loss: 0.2546554505825043
train_iter_loss: 0.2505815923213959
train_iter_loss: 0.22202272713184357
train_iter_loss: 0.2788592278957367
train_iter_loss: 0.30373749136924744
train_iter_loss: 0.2807989716529846
train_iter_loss: 0.2624753713607788
train_iter_loss: 0.20774389803409576
train_iter_loss: 0.3667626678943634
train_iter_loss: 0.2436872273683548
train_iter_loss: 0.29782232642173767
train_iter_loss: 0.34767860174179077
train_iter_loss: 0.13509942591190338
train_iter_loss: 0.36142948269844055
train_iter_loss: 0.12623943388462067
train_iter_loss: 0.3092881441116333
train_iter_loss: 0.3488166034221649
train_iter_loss: 0.24849092960357666
train_iter_loss: 0.24712419509887695
train_iter_loss: 0.15685361623764038
train_iter_loss: 0.20459894835948944
train_iter_loss: 0.3775215744972229
train_iter_loss: 0.3232482373714447
train_iter_loss: 0.20783382654190063
train_iter_loss: 0.28466710448265076
train_iter_loss: 0.2567689120769501
train_iter_loss: 0.2650248110294342
train_iter_loss: 0.3002067506313324
train_iter_loss: 0.36657148599624634
train_iter_loss: 0.35190603137016296
train_iter_loss: 0.19862668216228485
train_iter_loss: 0.11384753882884979
train_iter_loss: 0.13608402013778687
train_iter_loss: 0.258362352848053
train_iter_loss: 0.2093353122472763
train_iter_loss: 0.43631526827812195
train_iter_loss: 0.3426898419857025
train_iter_loss: 0.1930602490901947
train_iter_loss: 0.2875553369522095
train_iter_loss: 0.40063294768333435
train_iter_loss: 0.23720329999923706
train_iter_loss: 0.3367454409599304
train_iter_loss: 0.3346288204193115
train_iter_loss: 0.17428146302700043
train_iter_loss: 0.2785140872001648
train_iter_loss: 0.2328507900238037
train_iter_loss: 0.29289472103118896
train_iter_loss: 0.2834676504135132
train_iter_loss: 0.20766675472259521
train_iter_loss: 0.31929630041122437
train_iter_loss: 0.22079409658908844
train_iter_loss: 0.4211388826370239
train_iter_loss: 0.3452475666999817
train_iter_loss: 0.4100436866283417
train_iter_loss: 0.10060440003871918
train_iter_loss: 0.14609074592590332
train_iter_loss: 0.1813429743051529
train_iter_loss: 0.15690752863883972
train_iter_loss: 0.1944418102502823
train_iter_loss: 0.18879486620426178
train_iter_loss: 0.321662575006485
train_iter_loss: 0.24148619174957275
train loss :0.2698
---------------------
Validation seg loss: 0.36460474952352495 at epoch 603
epoch =    604/  1000, exp = train
train_iter_loss: 0.2623264193534851
train_iter_loss: 0.2622576355934143
train_iter_loss: 0.18592683970928192
train_iter_loss: 0.31856679916381836
train_iter_loss: 0.21191105246543884
train_iter_loss: 0.2966793179512024
train_iter_loss: 0.2582148313522339
train_iter_loss: 0.16969437897205353
train_iter_loss: 0.37134745717048645
train_iter_loss: 0.25931981205940247
train_iter_loss: 0.2678880989551544
train_iter_loss: 0.20110930502414703
train_iter_loss: 0.2821377217769623
train_iter_loss: 0.2850196957588196
train_iter_loss: 0.2987120747566223
train_iter_loss: 0.28859743475914
train_iter_loss: 0.2305886149406433
train_iter_loss: 0.22615742683410645
train_iter_loss: 0.26071664690971375
train_iter_loss: 0.3973822593688965
train_iter_loss: 0.29152727127075195
train_iter_loss: 0.18726226687431335
train_iter_loss: 0.35875117778778076
train_iter_loss: 0.30949801206588745
train_iter_loss: 0.3059844374656677
train_iter_loss: 0.24052269756793976
train_iter_loss: 0.16171972453594208
train_iter_loss: 0.2826974093914032
train_iter_loss: 0.24008677899837494
train_iter_loss: 0.3952648341655731
train_iter_loss: 0.2884930372238159
train_iter_loss: 0.24221280217170715
train_iter_loss: 0.32975059747695923
train_iter_loss: 0.07988980412483215
train_iter_loss: 0.2523084580898285
train_iter_loss: 0.13365258276462555
train_iter_loss: 0.313894659280777
train_iter_loss: 0.19163312017917633
train_iter_loss: 0.20661814510822296
train_iter_loss: 0.2253086268901825
train_iter_loss: 0.09406910836696625
train_iter_loss: 0.34175848960876465
train_iter_loss: 0.3078225255012512
train_iter_loss: 0.4451427459716797
train_iter_loss: 0.2145003378391266
train_iter_loss: 0.2108689546585083
train_iter_loss: 0.1731841266155243
train_iter_loss: 0.2862480878829956
train_iter_loss: 0.3094828724861145
train_iter_loss: 0.3000261187553406
train_iter_loss: 0.19684405624866486
train_iter_loss: 0.25171351432800293
train_iter_loss: 0.1226273849606514
train_iter_loss: 0.281946063041687
train_iter_loss: 0.29982954263687134
train_iter_loss: 0.2357047200202942
train_iter_loss: 0.19791552424430847
train_iter_loss: 0.2692660689353943
train_iter_loss: 0.33524999022483826
train_iter_loss: 0.17627596855163574
train_iter_loss: 0.06681807339191437
train_iter_loss: 0.2640175521373749
train_iter_loss: 0.17795678973197937
train_iter_loss: 0.3061474561691284
train_iter_loss: 0.42733365297317505
train_iter_loss: 0.28631603717803955
train_iter_loss: 0.16906112432479858
train_iter_loss: 0.13710463047027588
train_iter_loss: 0.22692809998989105
train_iter_loss: 0.16560672223567963
train_iter_loss: 0.22455266118049622
train_iter_loss: 0.39367133378982544
train_iter_loss: 0.2842961549758911
train_iter_loss: 0.20136100053787231
train_iter_loss: 0.20130319893360138
train_iter_loss: 0.34405508637428284
train_iter_loss: 0.15276136994361877
train_iter_loss: 0.2577175796031952
train_iter_loss: 0.18202145397663116
train_iter_loss: 0.2254597544670105
train_iter_loss: 0.42309480905532837
train_iter_loss: 0.3052646815776825
train_iter_loss: 0.36409303545951843
train_iter_loss: 0.30259808897972107
train_iter_loss: 0.2590235471725464
train_iter_loss: 0.22973863780498505
train_iter_loss: 0.40409013628959656
train_iter_loss: 0.19624681770801544
train_iter_loss: 0.39274683594703674
train_iter_loss: 0.32389703392982483
train_iter_loss: 0.39726734161376953
train_iter_loss: 0.47244295477867126
train_iter_loss: 0.3268720507621765
train_iter_loss: 0.16493703424930573
train_iter_loss: 0.2988039553165436
train_iter_loss: 0.3034106194972992
train_iter_loss: 0.22521474957466125
train_iter_loss: 0.29477936029434204
train_iter_loss: 0.27348312735557556
train_iter_loss: 0.3411322832107544
train loss :0.2669
---------------------
Validation seg loss: 0.359356134339183 at epoch 604
epoch =    605/  1000, exp = train
train_iter_loss: 0.25302645564079285
train_iter_loss: 0.24306714534759521
train_iter_loss: 0.08887195587158203
train_iter_loss: 0.14524789154529572
train_iter_loss: 0.4066388010978699
train_iter_loss: 0.4062774181365967
train_iter_loss: 0.3133470416069031
train_iter_loss: 0.37884944677352905
train_iter_loss: 0.25175419449806213
train_iter_loss: 0.2831307649612427
train_iter_loss: 0.17380604147911072
train_iter_loss: 0.21928225457668304
train_iter_loss: 0.2809576690196991
train_iter_loss: 0.2712579667568207
train_iter_loss: 0.20587055385112762
train_iter_loss: 0.3374790549278259
train_iter_loss: 0.2771611511707306
train_iter_loss: 0.32635602355003357
train_iter_loss: 0.14457710087299347
train_iter_loss: 0.11753052473068237
train_iter_loss: 0.21983222663402557
train_iter_loss: 0.41564974188804626
train_iter_loss: 0.42901813983917236
train_iter_loss: 0.17762598395347595
train_iter_loss: 0.2525804042816162
train_iter_loss: 0.11442882567644119
train_iter_loss: 0.24430203437805176
train_iter_loss: 0.2890170216560364
train_iter_loss: 0.29341021180152893
train_iter_loss: 0.2195538729429245
train_iter_loss: 0.17897242307662964
train_iter_loss: 0.3333929181098938
train_iter_loss: 0.27846041321754456
train_iter_loss: 0.1816566288471222
train_iter_loss: 0.18815380334854126
train_iter_loss: 0.34332260489463806
train_iter_loss: 0.28720006346702576
train_iter_loss: 0.5067668557167053
train_iter_loss: 0.2863946855068207
train_iter_loss: 0.3245534896850586
train_iter_loss: 0.1761326789855957
train_iter_loss: 0.19720573723316193
train_iter_loss: 0.36198633909225464
train_iter_loss: 0.0662401095032692
train_iter_loss: 0.3124536871910095
train_iter_loss: 0.25625306367874146
train_iter_loss: 0.27810752391815186
train_iter_loss: 0.2472105175256729
train_iter_loss: 0.26941806077957153
train_iter_loss: 0.1889134645462036
train_iter_loss: 0.15477588772773743
train_iter_loss: 0.26274019479751587
train_iter_loss: 0.3017275035381317
train_iter_loss: 0.28962627053260803
train_iter_loss: 0.1316775679588318
train_iter_loss: 0.349630743265152
train_iter_loss: 0.2627580165863037
train_iter_loss: 0.11031833291053772
train_iter_loss: 0.35060369968414307
train_iter_loss: 0.22735607624053955
train_iter_loss: 0.36125004291534424
train_iter_loss: 0.08073496073484421
train_iter_loss: 0.1554001420736313
train_iter_loss: 0.2096230536699295
train_iter_loss: 0.23542118072509766
train_iter_loss: 0.24538379907608032
train_iter_loss: 0.430807501077652
train_iter_loss: 0.1780560463666916
train_iter_loss: 0.48864349722862244
train_iter_loss: 0.2713647186756134
train_iter_loss: 0.229929119348526
train_iter_loss: 0.30920708179473877
train_iter_loss: 0.12487020343542099
train_iter_loss: 0.4369133412837982
train_iter_loss: 0.17219100892543793
train_iter_loss: 0.3723939061164856
train_iter_loss: 0.5478948354721069
train_iter_loss: 0.420138418674469
train_iter_loss: 0.19574086368083954
train_iter_loss: 0.33983176946640015
train_iter_loss: 0.13126160204410553
train_iter_loss: 0.16093166172504425
train_iter_loss: 0.24591314792633057
train_iter_loss: 0.380634605884552
train_iter_loss: 0.2518397867679596
train_iter_loss: 0.06461828947067261
train_iter_loss: 0.2831330895423889
train_iter_loss: 0.44158312678337097
train_iter_loss: 0.15104220807552338
train_iter_loss: 0.19582104682922363
train_iter_loss: 0.30372336506843567
train_iter_loss: 0.18598206341266632
train_iter_loss: 0.27184951305389404
train_iter_loss: 0.35054776072502136
train_iter_loss: 0.25153648853302
train_iter_loss: 0.16210120916366577
train_iter_loss: 0.39935600757598877
train_iter_loss: 0.2225240021944046
train_iter_loss: 0.1337060183286667
train_iter_loss: 0.35658228397369385
train loss :0.2650
---------------------
Validation seg loss: 0.36402349190716193 at epoch 605
epoch =    606/  1000, exp = train
train_iter_loss: 0.2951274812221527
train_iter_loss: 0.32544994354248047
train_iter_loss: 0.25705868005752563
train_iter_loss: 0.27579012513160706
train_iter_loss: 0.09052024781703949
train_iter_loss: 0.1282881200313568
train_iter_loss: 0.32033565640449524
train_iter_loss: 0.31558096408843994
train_iter_loss: 0.36971065402030945
train_iter_loss: 0.1739279180765152
train_iter_loss: 0.2303481549024582
train_iter_loss: 0.22352300584316254
train_iter_loss: 0.2440844625234604
train_iter_loss: 0.3388351798057556
train_iter_loss: 0.1783093512058258
train_iter_loss: 0.35708871483802795
train_iter_loss: 0.17200948297977448
train_iter_loss: 0.27351418137550354
train_iter_loss: 0.12328385561704636
train_iter_loss: 0.2944202721118927
train_iter_loss: 0.38720446825027466
train_iter_loss: 0.3262389302253723
train_iter_loss: 0.21972601115703583
train_iter_loss: 0.1876729428768158
train_iter_loss: 0.17262808978557587
train_iter_loss: 0.19163312017917633
train_iter_loss: 0.2997967302799225
train_iter_loss: 0.3335222899913788
train_iter_loss: 0.47978752851486206
train_iter_loss: 0.1600767970085144
train_iter_loss: 0.38375768065452576
train_iter_loss: 0.1540614813566208
train_iter_loss: 0.29105326533317566
train_iter_loss: 0.21513520181179047
train_iter_loss: 0.16002704203128815
train_iter_loss: 0.2908506691455841
train_iter_loss: 0.5167669057846069
train_iter_loss: 0.11914447695016861
train_iter_loss: 0.2941005825996399
train_iter_loss: 0.4505314230918884
train_iter_loss: 0.3378518521785736
train_iter_loss: 0.2554079294204712
train_iter_loss: 0.19018174707889557
train_iter_loss: 0.2547244429588318
train_iter_loss: 0.18698333203792572
train_iter_loss: 0.16238616406917572
train_iter_loss: 0.06172424927353859
train_iter_loss: 0.33565884828567505
train_iter_loss: 0.29659056663513184
train_iter_loss: 0.31717997789382935
train_iter_loss: 0.28330403566360474
train_iter_loss: 0.5252306461334229
train_iter_loss: 0.26102909445762634
train_iter_loss: 0.30745673179626465
train_iter_loss: 0.3351525664329529
train_iter_loss: 0.26561516523361206
train_iter_loss: 0.18335096538066864
train_iter_loss: 0.10579825192689896
train_iter_loss: 0.29692715406417847
train_iter_loss: 0.24031028151512146
train_iter_loss: 0.24854831397533417
train_iter_loss: 0.14386314153671265
train_iter_loss: 0.20081396400928497
train_iter_loss: 0.25564825534820557
train_iter_loss: 0.412140429019928
train_iter_loss: 0.37308189272880554
train_iter_loss: 0.22926075756549835
train_iter_loss: 0.1696268618106842
train_iter_loss: 0.13317078351974487
train_iter_loss: 0.26131677627563477
train_iter_loss: 0.175805002450943
train_iter_loss: 0.31923708319664
train_iter_loss: 0.20813870429992676
train_iter_loss: 0.35963261127471924
train_iter_loss: 0.33356523513793945
train_iter_loss: 0.31930506229400635
train_iter_loss: 0.2754596173763275
train_iter_loss: 0.3474693298339844
train_iter_loss: 0.34833598136901855
train_iter_loss: 0.28553345799446106
train_iter_loss: 0.023929383605718613
train_iter_loss: 0.33604738116264343
train_iter_loss: 0.25254517793655396
train_iter_loss: 0.21535547077655792
train_iter_loss: 0.2841244041919708
train_iter_loss: 0.37717753648757935
train_iter_loss: 0.32438924908638
train_iter_loss: 0.3719453513622284
train_iter_loss: 0.19750583171844482
train_iter_loss: 0.3603576123714447
train_iter_loss: 0.18369153141975403
train_iter_loss: 0.40150851011276245
train_iter_loss: 0.23321832716464996
train_iter_loss: 0.29145002365112305
train_iter_loss: 0.16659246385097504
train_iter_loss: 0.3575286865234375
train_iter_loss: 0.09934189915657043
train_iter_loss: 0.2942710220813751
train_iter_loss: 0.3654358983039856
train_iter_loss: 0.27258211374282837
train loss :0.2690
---------------------
Validation seg loss: 0.35684575627224063 at epoch 606
epoch =    607/  1000, exp = train
train_iter_loss: 0.2550331950187683
train_iter_loss: 0.35216957330703735
train_iter_loss: 0.2046084851026535
train_iter_loss: 0.31932029128074646
train_iter_loss: 0.554085373878479
train_iter_loss: 0.2912677824497223
train_iter_loss: 0.17523716390132904
train_iter_loss: 0.169129878282547
train_iter_loss: 0.1392679661512375
train_iter_loss: 0.4696723520755768
train_iter_loss: 0.11340312659740448
train_iter_loss: 0.354515939950943
train_iter_loss: 0.3354061245918274
train_iter_loss: 0.2848654091358185
train_iter_loss: 0.35265836119651794
train_iter_loss: 0.47106796503067017
train_iter_loss: 0.2500988245010376
train_iter_loss: 0.2130173295736313
train_iter_loss: 0.12544190883636475
train_iter_loss: 0.22599084675312042
train_iter_loss: 0.1445063352584839
train_iter_loss: 0.26173657178878784
train_iter_loss: 0.28816598653793335
train_iter_loss: 0.24626795947551727
train_iter_loss: 0.21590441465377808
train_iter_loss: 0.2959728538990021
train_iter_loss: 0.28276526927948
train_iter_loss: 0.3375517427921295
train_iter_loss: 0.19626305997371674
train_iter_loss: 0.3354479670524597
train_iter_loss: 0.1305711418390274
train_iter_loss: 0.15165232121944427
train_iter_loss: 0.2676212191581726
train_iter_loss: 0.32840460538864136
train_iter_loss: 0.22553783655166626
train_iter_loss: 0.4326920211315155
train_iter_loss: 0.25565940141677856
train_iter_loss: 0.24800695478916168
train_iter_loss: 0.23971515893936157
train_iter_loss: 0.3374347984790802
train_iter_loss: 0.257897287607193
train_iter_loss: 0.18325376510620117
train_iter_loss: 0.21335910260677338
train_iter_loss: 0.39997124671936035
train_iter_loss: 0.2554224729537964
train_iter_loss: 0.16343480348587036
train_iter_loss: 0.258355051279068
train_iter_loss: 0.1420222967863083
train_iter_loss: 0.17440558969974518
train_iter_loss: 0.0810534879565239
train_iter_loss: 0.2885727882385254
train_iter_loss: 0.22821550071239471
train_iter_loss: 0.24176333844661713
train_iter_loss: 0.3305656611919403
train_iter_loss: 0.26638877391815186
train_iter_loss: 0.409268856048584
train_iter_loss: 0.26758360862731934
train_iter_loss: 0.24508534371852875
train_iter_loss: 0.18357855081558228
train_iter_loss: 0.28019171953201294
train_iter_loss: 0.22073423862457275
train_iter_loss: 0.36258676648139954
train_iter_loss: 0.30049648880958557
train_iter_loss: 0.27708426117897034
train_iter_loss: 0.3108026087284088
train_iter_loss: 0.23318324983119965
train_iter_loss: 0.2001328021287918
train_iter_loss: 0.2725988030433655
train_iter_loss: 0.2461549937725067
train_iter_loss: 0.2352936714887619
train_iter_loss: 0.15839441120624542
train_iter_loss: 0.31248921155929565
train_iter_loss: 0.23955023288726807
train_iter_loss: 0.3496587872505188
train_iter_loss: 0.27020442485809326
train_iter_loss: 0.19045448303222656
train_iter_loss: 0.2195429652929306
train_iter_loss: 0.21004915237426758
train_iter_loss: 0.3563091456890106
train_iter_loss: 0.29411011934280396
train_iter_loss: 0.4228787422180176
train_iter_loss: 0.3328952193260193
train_iter_loss: 0.31483176350593567
train_iter_loss: 0.3132408559322357
train_iter_loss: 0.1931631863117218
train_iter_loss: 0.18106117844581604
train_iter_loss: 0.4087788164615631
train_iter_loss: 0.21181900799274445
train_iter_loss: 0.22511114180088043
train_iter_loss: 0.2571311891078949
train_iter_loss: 0.17163604497909546
train_iter_loss: 0.24682584404945374
train_iter_loss: 0.2200859785079956
train_iter_loss: 0.3116719424724579
train_iter_loss: 0.2046041190624237
train_iter_loss: 0.19065535068511963
train_iter_loss: 0.17970257997512817
train_iter_loss: 0.20661693811416626
train_iter_loss: 0.19231462478637695
train_iter_loss: 0.38838639855384827
train loss :0.2645
---------------------
Validation seg loss: 0.35704847908455806 at epoch 607
epoch =    608/  1000, exp = train
train_iter_loss: 0.2297511100769043
train_iter_loss: 0.27324178814888
train_iter_loss: 0.3441002368927002
train_iter_loss: 0.29300394654273987
train_iter_loss: 0.1870424896478653
train_iter_loss: 0.26554185152053833
train_iter_loss: 0.34023433923721313
train_iter_loss: 0.1624521017074585
train_iter_loss: 0.3470936119556427
train_iter_loss: 0.17983606457710266
train_iter_loss: 0.4122479259967804
train_iter_loss: 0.3905823230743408
train_iter_loss: 0.1517445147037506
train_iter_loss: 0.25300681591033936
train_iter_loss: 0.27359530329704285
train_iter_loss: 0.21308766305446625
train_iter_loss: 0.3100356161594391
train_iter_loss: 0.22191739082336426
train_iter_loss: 0.23206594586372375
train_iter_loss: 0.6307599544525146
train_iter_loss: 0.2341170758008957
train_iter_loss: 0.2575724124908447
train_iter_loss: 0.3432161509990692
train_iter_loss: 0.2558821439743042
train_iter_loss: 0.24173004925251007
train_iter_loss: 0.12054047733545303
train_iter_loss: 0.23031410574913025
train_iter_loss: 0.21263429522514343
train_iter_loss: 0.2009628564119339
train_iter_loss: 0.21439088881015778
train_iter_loss: 0.14647488296031952
train_iter_loss: 0.27770015597343445
train_iter_loss: 0.1892859786748886
train_iter_loss: 0.29248785972595215
train_iter_loss: 0.29070597887039185
train_iter_loss: 0.19958385825157166
train_iter_loss: 0.187333881855011
train_iter_loss: 0.22736889123916626
train_iter_loss: 0.28564393520355225
train_iter_loss: 0.3703780770301819
train_iter_loss: 0.31458422541618347
train_iter_loss: 0.16408759355545044
train_iter_loss: 0.21661259233951569
train_iter_loss: 0.23928837478160858
train_iter_loss: 0.2653345465660095
train_iter_loss: 0.3418225347995758
train_iter_loss: 0.2898768484592438
train_iter_loss: 0.3040466606616974
train_iter_loss: 0.18107706308364868
train_iter_loss: 0.20875561237335205
train_iter_loss: 0.24134917557239532
train_iter_loss: 0.18055762350559235
train_iter_loss: 0.13267368078231812
train_iter_loss: 0.2546673119068146
train_iter_loss: 0.28167539834976196
train_iter_loss: 0.22227981686592102
train_iter_loss: 0.26978349685668945
train_iter_loss: 0.27460363507270813
train_iter_loss: 0.3268256187438965
train_iter_loss: 0.09503887593746185
train_iter_loss: 0.2039811909198761
train_iter_loss: 0.15630273520946503
train_iter_loss: 0.16982850432395935
train_iter_loss: 0.23001424968242645
train_iter_loss: 0.21725432574748993
train_iter_loss: 0.1312800645828247
train_iter_loss: 0.14741389453411102
train_iter_loss: 0.15328629314899445
train_iter_loss: 0.4044562876224518
train_iter_loss: 0.13937152922153473
train_iter_loss: 0.34448307752609253
train_iter_loss: 0.43179094791412354
train_iter_loss: 0.28964221477508545
train_iter_loss: 0.2778956890106201
train_iter_loss: 0.28403162956237793
train_iter_loss: 0.22277118265628815
train_iter_loss: 0.26604005694389343
train_iter_loss: 0.19722093641757965
train_iter_loss: 0.2996775209903717
train_iter_loss: 0.3500064015388489
train_iter_loss: 0.2675682008266449
train_iter_loss: 0.26028206944465637
train_iter_loss: 0.2544184923171997
train_iter_loss: 0.3228706121444702
train_iter_loss: 0.3156120181083679
train_iter_loss: 0.36355260014533997
train_iter_loss: 0.18302148580551147
train_iter_loss: 0.10705692321062088
train_iter_loss: 0.23604828119277954
train_iter_loss: 0.39547932147979736
train_iter_loss: 0.37244874238967896
train_iter_loss: 0.3831220269203186
train_iter_loss: 0.25830551981925964
train_iter_loss: 0.3438558578491211
train_iter_loss: 0.16774678230285645
train_iter_loss: 0.29158473014831543
train_iter_loss: 0.3168838322162628
train_iter_loss: 0.31523600220680237
train_iter_loss: 0.2549934685230255
train_iter_loss: 0.3794354200363159
train loss :0.2627
---------------------
Validation seg loss: 0.35683612800466846 at epoch 608
epoch =    609/  1000, exp = train
train_iter_loss: 0.36403512954711914
train_iter_loss: 0.2127694934606552
train_iter_loss: 0.34385886788368225
train_iter_loss: 0.2131594568490982
train_iter_loss: 0.18061430752277374
train_iter_loss: 0.2568024694919586
train_iter_loss: 0.32886654138565063
train_iter_loss: 0.16665223240852356
train_iter_loss: 0.3209307789802551
train_iter_loss: 0.31286945939064026
train_iter_loss: 0.27474355697631836
train_iter_loss: 0.33951839804649353
train_iter_loss: 0.20846697688102722
train_iter_loss: 0.23290511965751648
train_iter_loss: 0.2948029339313507
train_iter_loss: 0.20503897964954376
train_iter_loss: 0.22540883719921112
train_iter_loss: 0.3385714888572693
train_iter_loss: 0.08751916885375977
train_iter_loss: 0.2243761569261551
train_iter_loss: 0.2229783535003662
train_iter_loss: 0.44356536865234375
train_iter_loss: 0.15549001097679138
train_iter_loss: 0.37320446968078613
train_iter_loss: 0.3334418535232544
train_iter_loss: 0.2651924192905426
train_iter_loss: 0.2566753029823303
train_iter_loss: 0.2540430724620819
train_iter_loss: 0.3458392918109894
train_iter_loss: 0.47345298528671265
train_iter_loss: 0.1674923300743103
train_iter_loss: 0.2962867319583893
train_iter_loss: 0.2663416862487793
train_iter_loss: 0.2869139611721039
train_iter_loss: 0.12223169952630997
train_iter_loss: 0.1694086194038391
train_iter_loss: 0.20438824594020844
train_iter_loss: 0.25834372639656067
train_iter_loss: 0.28254273533821106
train_iter_loss: 0.1557215303182602
train_iter_loss: 0.2490386664867401
train_iter_loss: 0.3572573661804199
train_iter_loss: 0.2901034355163574
train_iter_loss: 0.24800153076648712
train_iter_loss: 0.22472381591796875
train_iter_loss: 0.15077416598796844
train_iter_loss: 0.3406289517879486
train_iter_loss: 0.34246405959129333
train_iter_loss: 0.203364297747612
train_iter_loss: 0.1185067668557167
train_iter_loss: 0.3645777702331543
train_iter_loss: 0.36143946647644043
train_iter_loss: 0.24893566966056824
train_iter_loss: 0.3804915249347687
train_iter_loss: 0.26727166771888733
train_iter_loss: 0.07789575308561325
train_iter_loss: 0.1403045952320099
train_iter_loss: 0.1818724274635315
train_iter_loss: 0.2265070676803589
train_iter_loss: 0.2593498229980469
train_iter_loss: 0.3169446289539337
train_iter_loss: 0.2520807981491089
train_iter_loss: 0.22077171504497528
train_iter_loss: 0.2845996022224426
train_iter_loss: 0.2940923571586609
train_iter_loss: 0.3192237913608551
train_iter_loss: 0.22731195390224457
train_iter_loss: 0.29506102204322815
train_iter_loss: 0.3976747691631317
train_iter_loss: 0.3994922637939453
train_iter_loss: 0.2566814422607422
train_iter_loss: 0.3544108271598816
train_iter_loss: 0.1848868578672409
train_iter_loss: 0.29712027311325073
train_iter_loss: 0.12815821170806885
train_iter_loss: 0.2743900716304779
train_iter_loss: 0.3645283877849579
train_iter_loss: 0.27590516209602356
train_iter_loss: 0.2593722939491272
train_iter_loss: 0.41065099835395813
train_iter_loss: 0.30203139781951904
train_iter_loss: 0.2828231751918793
train_iter_loss: 0.3125378489494324
train_iter_loss: 0.2934607267379761
train_iter_loss: 0.2201305776834488
train_iter_loss: 0.3221602439880371
train_iter_loss: 0.14674267172813416
train_iter_loss: 0.3698054254055023
train_iter_loss: 0.21420197188854218
train_iter_loss: 0.20262083411216736
train_iter_loss: 0.2039566934108734
train_iter_loss: 0.2466851770877838
train_iter_loss: 0.2770432233810425
train_iter_loss: 0.21212181448936462
train_iter_loss: 0.2144596129655838
train_iter_loss: 0.18682453036308289
train_iter_loss: 0.36243122816085815
train_iter_loss: 0.24729230999946594
train_iter_loss: 0.23940236866474152
train_iter_loss: 0.3192768692970276
train loss :0.2675
---------------------
Validation seg loss: 0.39340776719047493 at epoch 609
epoch =    610/  1000, exp = train
train_iter_loss: 0.1650596559047699
train_iter_loss: 0.2946401834487915
train_iter_loss: 0.3583410382270813
train_iter_loss: 0.34241804480552673
train_iter_loss: 0.5359447598457336
train_iter_loss: 0.2932893931865692
train_iter_loss: 0.23624879121780396
train_iter_loss: 0.2950901389122009
train_iter_loss: 0.36835530400276184
train_iter_loss: 0.23824085295200348
train_iter_loss: 0.23044899106025696
train_iter_loss: 0.30788132548332214
train_iter_loss: 0.2206588089466095
train_iter_loss: 0.277423232793808
train_iter_loss: 0.5198037028312683
train_iter_loss: 0.14803782105445862
train_iter_loss: 0.43368324637413025
train_iter_loss: 0.1074831411242485
train_iter_loss: 0.3134072422981262
train_iter_loss: 0.3483234941959381
train_iter_loss: 0.25819310545921326
train_iter_loss: 0.21867650747299194
train_iter_loss: 0.3342919647693634
train_iter_loss: 0.2568731904029846
train_iter_loss: 0.31781044602394104
train_iter_loss: 0.27168843150138855
train_iter_loss: 0.2952345609664917
train_iter_loss: 0.27215659618377686
train_iter_loss: 0.22295090556144714
train_iter_loss: 0.1963006854057312
train_iter_loss: 0.21158508956432343
train_iter_loss: 0.2612546384334564
train_iter_loss: 0.2900521755218506
train_iter_loss: 0.29389771819114685
train_iter_loss: 0.25646716356277466
train_iter_loss: 0.2699511647224426
train_iter_loss: 0.07412178069353104
train_iter_loss: 0.15705011785030365
train_iter_loss: 0.29927998781204224
train_iter_loss: 0.19726237654685974
train_iter_loss: 0.33243831992149353
train_iter_loss: 0.05010959878563881
train_iter_loss: 0.2208704799413681
train_iter_loss: 0.3191121518611908
train_iter_loss: 0.1753479689359665
train_iter_loss: 0.14762668311595917
train_iter_loss: 0.24677647650241852
train_iter_loss: 0.3088618218898773
train_iter_loss: 0.3147101402282715
train_iter_loss: 0.22916562855243683
train_iter_loss: 0.22644585371017456
train_iter_loss: 0.2639446556568146
train_iter_loss: 0.22394731640815735
train_iter_loss: 0.2800157070159912
train_iter_loss: 0.17502036690711975
train_iter_loss: 0.2841801345348358
train_iter_loss: 0.395248144865036
train_iter_loss: 0.26505914330482483
train_iter_loss: 0.2713107764720917
train_iter_loss: 0.30749383568763733
train_iter_loss: 0.24299633502960205
train_iter_loss: 0.5990379452705383
train_iter_loss: 0.2887069582939148
train_iter_loss: 0.22249813377857208
train_iter_loss: 0.1965913027524948
train_iter_loss: 0.37149953842163086
train_iter_loss: 0.2478523999452591
train_iter_loss: 0.22076694667339325
train_iter_loss: 0.44555097818374634
train_iter_loss: 0.2791980504989624
train_iter_loss: 0.2739391326904297
train_iter_loss: 0.2525267004966736
train_iter_loss: 0.1378811001777649
train_iter_loss: 0.30632472038269043
train_iter_loss: 0.2797711491584778
train_iter_loss: 0.14397990703582764
train_iter_loss: 0.26040202379226685
train_iter_loss: 0.34969332814216614
train_iter_loss: 0.3763013482093811
train_iter_loss: 0.27064815163612366
train_iter_loss: 0.20044448971748352
train_iter_loss: 0.0978882908821106
train_iter_loss: 0.15290068089962006
train_iter_loss: 0.29488644003868103
train_iter_loss: 0.20292173326015472
train_iter_loss: 0.24561072885990143
train_iter_loss: 0.14563234150409698
train_iter_loss: 0.23160980641841888
train_iter_loss: 0.24427981674671173
train_iter_loss: 0.2638165056705475
train_iter_loss: 0.27779996395111084
train_iter_loss: 0.27546554803848267
train_iter_loss: 0.10279891639947891
train_iter_loss: 0.3304167687892914
train_iter_loss: 0.4422089457511902
train_iter_loss: 0.24898038804531097
train_iter_loss: 0.21316829323768616
train_iter_loss: 0.20738354325294495
train_iter_loss: 0.3724936842918396
train_iter_loss: 0.19736461341381073
train loss :0.2681
---------------------
Validation seg loss: 0.3456711218217915 at epoch 610
********************
best_val_epoch_loss:  0.3456711218217915
MODEL UPDATED
epoch =    611/  1000, exp = train
train_iter_loss: 0.11496467888355255
train_iter_loss: 0.22639088332653046
train_iter_loss: 0.159631609916687
train_iter_loss: 0.3694976568222046
train_iter_loss: 0.22931289672851562
train_iter_loss: 0.38522347807884216
train_iter_loss: 0.19885565340518951
train_iter_loss: 0.4985479712486267
train_iter_loss: 0.1947181522846222
train_iter_loss: 0.27885469794273376
train_iter_loss: 0.3048021197319031
train_iter_loss: 0.3693799674510956
train_iter_loss: 0.2193114012479782
train_iter_loss: 0.3350520730018616
train_iter_loss: 0.264768123626709
train_iter_loss: 0.23202770948410034
train_iter_loss: 0.14651471376419067
train_iter_loss: 0.2618713676929474
train_iter_loss: 0.2901016175746918
train_iter_loss: 0.1270976960659027
train_iter_loss: 0.19780413806438446
train_iter_loss: 0.2603721618652344
train_iter_loss: 0.26758408546447754
train_iter_loss: 0.32525208592414856
train_iter_loss: 0.195184126496315
train_iter_loss: 0.3612608313560486
train_iter_loss: 0.37624526023864746
train_iter_loss: 0.28923603892326355
train_iter_loss: 0.2111915647983551
train_iter_loss: 0.23968514800071716
train_iter_loss: 0.15589608252048492
train_iter_loss: 0.2520163357257843
train_iter_loss: 0.15437325835227966
train_iter_loss: 0.33704525232315063
train_iter_loss: 0.12452160567045212
train_iter_loss: 0.23826545476913452
train_iter_loss: 0.3050021529197693
train_iter_loss: 0.2239767163991928
train_iter_loss: 0.10278289765119553
train_iter_loss: 0.44170036911964417
train_iter_loss: 0.36299648880958557
train_iter_loss: 0.08344443887472153
train_iter_loss: 0.1705096811056137
train_iter_loss: 0.1829579919576645
train_iter_loss: 0.4039381146430969
train_iter_loss: 0.11455179750919342
train_iter_loss: 0.37661615014076233
train_iter_loss: 0.32707178592681885
train_iter_loss: 0.53139728307724
train_iter_loss: 0.22327713668346405
train_iter_loss: 0.29892459511756897
train_iter_loss: 0.13482750952243805
train_iter_loss: 0.3072374165058136
train_iter_loss: 0.20381854474544525
train_iter_loss: 0.17515206336975098
train_iter_loss: 0.36722978949546814
train_iter_loss: 0.20824575424194336
train_iter_loss: 0.21113410592079163
train_iter_loss: 0.21565750241279602
train_iter_loss: 0.2875846028327942
train_iter_loss: 0.204987570643425
train_iter_loss: 0.26605984568595886
train_iter_loss: 0.08766897022724152
train_iter_loss: 0.5078977942466736
train_iter_loss: 0.36884716153144836
train_iter_loss: 0.23470185697078705
train_iter_loss: 0.3557841181755066
train_iter_loss: 0.1744246482849121
train_iter_loss: 0.34017807245254517
train_iter_loss: 0.3658623695373535
train_iter_loss: 0.16608498990535736
train_iter_loss: 0.2872113287448883
train_iter_loss: 0.29997846484184265
train_iter_loss: 0.16124948859214783
train_iter_loss: 0.21419380605220795
train_iter_loss: 0.1631043404340744
train_iter_loss: 0.22885477542877197
train_iter_loss: 0.5252469182014465
train_iter_loss: 0.2010667324066162
train_iter_loss: 0.19064921140670776
train_iter_loss: 0.21349218487739563
train_iter_loss: 0.25476914644241333
train_iter_loss: 0.40546414256095886
train_iter_loss: 0.33387115597724915
train_iter_loss: 0.3234299421310425
train_iter_loss: 0.2765839993953705
train_iter_loss: 0.36880767345428467
train_iter_loss: 0.11792490631341934
train_iter_loss: 0.30160465836524963
train_iter_loss: 0.4738420248031616
train_iter_loss: 0.16294921934604645
train_iter_loss: 0.1527140885591507
train_iter_loss: 0.19481855630874634
train_iter_loss: 0.37164682149887085
train_iter_loss: 0.41255536675453186
train_iter_loss: 0.3098330497741699
train_iter_loss: 0.18995913863182068
train_iter_loss: 0.1208738461136818
train_iter_loss: 0.282064825296402
train_iter_loss: 0.22497408092021942
train loss :0.2656
---------------------
Validation seg loss: 0.3524764896159605 at epoch 611
epoch =    612/  1000, exp = train
train_iter_loss: 0.37887102365493774
train_iter_loss: 0.0808509960770607
train_iter_loss: 0.16814984381198883
train_iter_loss: 0.27804914116859436
train_iter_loss: 0.20928482711315155
train_iter_loss: 0.22448374330997467
train_iter_loss: 0.28110113739967346
train_iter_loss: 0.21688182651996613
train_iter_loss: 0.3062223494052887
train_iter_loss: 0.14118191599845886
train_iter_loss: 0.40925198793411255
train_iter_loss: 0.1940745860338211
train_iter_loss: 0.1309557855129242
train_iter_loss: 0.1411551833152771
train_iter_loss: 0.1406397670507431
train_iter_loss: 0.2507259249687195
train_iter_loss: 0.3206596076488495
train_iter_loss: 0.08985662460327148
train_iter_loss: 0.18198683857917786
train_iter_loss: 0.3013118505477905
train_iter_loss: 0.3655182123184204
train_iter_loss: 0.2837553322315216
train_iter_loss: 0.20684944093227386
train_iter_loss: 0.2951277196407318
train_iter_loss: 0.2546703517436981
train_iter_loss: 0.20296697318553925
train_iter_loss: 0.4019731283187866
train_iter_loss: 0.21840988099575043
train_iter_loss: 0.24083761870861053
train_iter_loss: 0.26392143964767456
train_iter_loss: 0.39022761583328247
train_iter_loss: 0.2622571587562561
train_iter_loss: 0.31289559602737427
train_iter_loss: 0.21289090812206268
train_iter_loss: 0.22068718075752258
train_iter_loss: 0.3672296702861786
train_iter_loss: 0.33613407611846924
train_iter_loss: 0.25601059198379517
train_iter_loss: 0.3083898425102234
train_iter_loss: 0.31123194098472595
train_iter_loss: 0.2160726934671402
train_iter_loss: 0.31330418586730957
train_iter_loss: 0.09408649802207947
train_iter_loss: 0.26554015278816223
train_iter_loss: 0.35348033905029297
train_iter_loss: 0.02850925549864769
train_iter_loss: 0.40817010402679443
train_iter_loss: 0.23979949951171875
train_iter_loss: 0.3315812647342682
train_iter_loss: 0.30443647503852844
train_iter_loss: 0.21786144375801086
train_iter_loss: 0.15075147151947021
train_iter_loss: 0.34789302945137024
train_iter_loss: 0.3387034833431244
train_iter_loss: 0.21016570925712585
train_iter_loss: 0.2954602539539337
train_iter_loss: 0.32513901591300964
train_iter_loss: 0.37447795271873474
train_iter_loss: 0.3500846326351166
train_iter_loss: 0.13909408450126648
train_iter_loss: 0.4051794409751892
train_iter_loss: 0.27911093831062317
train_iter_loss: 0.32604676485061646
train_iter_loss: 0.2020663619041443
train_iter_loss: 0.2999614477157593
train_iter_loss: 0.37820982933044434
train_iter_loss: 0.2727207839488983
train_iter_loss: 0.31056201457977295
train_iter_loss: 0.28216126561164856
train_iter_loss: 0.21798284351825714
train_iter_loss: 0.3062187731266022
train_iter_loss: 0.18369436264038086
train_iter_loss: 0.3918670415878296
train_iter_loss: 0.20732958614826202
train_iter_loss: 0.4199436604976654
train_iter_loss: 0.30070093274116516
train_iter_loss: 0.2817401587963104
train_iter_loss: 0.2094181478023529
train_iter_loss: 0.4690100848674774
train_iter_loss: 0.27687975764274597
train_iter_loss: 0.18489037454128265
train_iter_loss: 0.13868553936481476
train_iter_loss: 0.3168197274208069
train_iter_loss: 0.35243940353393555
train_iter_loss: 0.29276397824287415
train_iter_loss: 0.3374609649181366
train_iter_loss: 0.40354880690574646
train_iter_loss: 0.23935899138450623
train_iter_loss: 0.3259090781211853
train_iter_loss: 0.3340810537338257
train_iter_loss: 0.2782459259033203
train_iter_loss: 0.273385614156723
train_iter_loss: 0.22827191650867462
train_iter_loss: 0.16296711564064026
train_iter_loss: 0.27322831749916077
train_iter_loss: 0.23095305263996124
train_iter_loss: 0.19981585443019867
train_iter_loss: 0.3394087851047516
train_iter_loss: 0.3181871473789215
train_iter_loss: 0.26993635296821594
train loss :0.2725
---------------------
Validation seg loss: 0.3721093593235567 at epoch 612
epoch =    613/  1000, exp = train
train_iter_loss: 0.4147242307662964
train_iter_loss: 0.23408626019954681
train_iter_loss: 0.3170810341835022
train_iter_loss: 0.3638142943382263
train_iter_loss: 0.15975359082221985
train_iter_loss: 0.32670214772224426
train_iter_loss: 0.34637168049812317
train_iter_loss: 0.21710951626300812
train_iter_loss: 0.26930686831474304
train_iter_loss: 0.3169719874858856
train_iter_loss: 0.14642685651779175
train_iter_loss: 0.2765192985534668
train_iter_loss: 0.34968504309654236
train_iter_loss: 0.22547850012779236
train_iter_loss: 0.26471301913261414
train_iter_loss: 0.22449274361133575
train_iter_loss: 0.33358296751976013
train_iter_loss: 0.3492249548435211
train_iter_loss: 0.317533940076828
train_iter_loss: 0.3457474112510681
train_iter_loss: 0.20816724002361298
train_iter_loss: 0.32975149154663086
train_iter_loss: 0.2841246426105499
train_iter_loss: 0.19960421323776245
train_iter_loss: 0.21997633576393127
train_iter_loss: 0.2212267518043518
train_iter_loss: 0.2662815749645233
train_iter_loss: 0.38460907340049744
train_iter_loss: 0.1457347869873047
train_iter_loss: 0.35450461506843567
train_iter_loss: 0.2719738483428955
train_iter_loss: 0.14680980145931244
train_iter_loss: 0.42824238538742065
train_iter_loss: 0.20661045610904694
train_iter_loss: 0.293562650680542
train_iter_loss: 0.29187408089637756
train_iter_loss: 0.23705509305000305
train_iter_loss: 0.3057476580142975
train_iter_loss: 0.373989462852478
train_iter_loss: 0.08697747439146042
train_iter_loss: 0.23184673488140106
train_iter_loss: 0.4352019131183624
train_iter_loss: 0.2956755459308624
train_iter_loss: 0.34436875581741333
train_iter_loss: 0.2694152891635895
train_iter_loss: 0.3257453143596649
train_iter_loss: 0.1520652174949646
train_iter_loss: 0.257245272397995
train_iter_loss: 0.3551080822944641
train_iter_loss: 0.2718818783760071
train_iter_loss: 0.3679506778717041
train_iter_loss: 0.26316627860069275
train_iter_loss: 0.29198944568634033
train_iter_loss: 0.15205605328083038
train_iter_loss: 0.37466341257095337
train_iter_loss: 0.26492634415626526
train_iter_loss: 0.3026222884654999
train_iter_loss: 0.2703816592693329
train_iter_loss: 0.13472582399845123
train_iter_loss: 0.3205038607120514
train_iter_loss: 0.1468900740146637
train_iter_loss: 0.23713688552379608
train_iter_loss: 0.24847711622714996
train_iter_loss: 0.15636205673217773
train_iter_loss: 0.19687362015247345
train_iter_loss: 0.2909906804561615
train_iter_loss: 0.18711526691913605
train_iter_loss: 0.26603642106056213
train_iter_loss: 0.3471505045890808
train_iter_loss: 0.2070915699005127
train_iter_loss: 0.3777363896369934
train_iter_loss: 0.15638479590415955
train_iter_loss: 0.4120367169380188
train_iter_loss: 0.2691514492034912
train_iter_loss: 0.14855222404003143
train_iter_loss: 0.08055873960256577
train_iter_loss: 0.18321490287780762
train_iter_loss: 0.2538478970527649
train_iter_loss: 0.18630364537239075
train_iter_loss: 0.07769255340099335
train_iter_loss: 0.40214842557907104
train_iter_loss: 0.14500369131565094
train_iter_loss: 0.121120885014534
train_iter_loss: 0.24413052201271057
train_iter_loss: 0.5137014389038086
train_iter_loss: 0.10553590953350067
train_iter_loss: 0.2876760959625244
train_iter_loss: 0.4051225781440735
train_iter_loss: 0.4462333619594574
train_iter_loss: 0.23741240799427032
train_iter_loss: 0.18888063728809357
train_iter_loss: 0.26111936569213867
train_iter_loss: 0.20867471396923065
train_iter_loss: 0.3801902234554291
train_iter_loss: 0.16623555123806
train_iter_loss: 0.16351601481437683
train_iter_loss: 0.1834333837032318
train_iter_loss: 0.2522723078727722
train_iter_loss: 0.13961926102638245
train_iter_loss: 0.24290727078914642
train loss :0.2653
---------------------
Validation seg loss: 0.398337622524573 at epoch 613
epoch =    614/  1000, exp = train
train_iter_loss: 0.32308971881866455
train_iter_loss: 0.2550390660762787
train_iter_loss: 0.3675188422203064
train_iter_loss: 0.27477580308914185
train_iter_loss: 0.23958760499954224
train_iter_loss: 0.20371058583259583
train_iter_loss: 0.41563543677330017
train_iter_loss: 0.14005327224731445
train_iter_loss: 0.3954368531703949
train_iter_loss: 0.32648277282714844
train_iter_loss: 0.24230095744132996
train_iter_loss: 0.4040907621383667
train_iter_loss: 0.2018531709909439
train_iter_loss: 0.2869293689727783
train_iter_loss: 0.2732548713684082
train_iter_loss: 0.22332069277763367
train_iter_loss: 0.19003573060035706
train_iter_loss: 0.3916087746620178
train_iter_loss: 0.12290076911449432
train_iter_loss: 0.11377864331007004
train_iter_loss: 0.3216443955898285
train_iter_loss: 0.16419431567192078
train_iter_loss: 0.09262816607952118
train_iter_loss: 0.2305060178041458
train_iter_loss: 0.33414262533187866
train_iter_loss: 0.22803781926631927
train_iter_loss: 0.2380765676498413
train_iter_loss: 0.16897065937519073
train_iter_loss: 0.13727401196956635
train_iter_loss: 0.2371847927570343
train_iter_loss: 0.21179217100143433
train_iter_loss: 0.20781391859054565
train_iter_loss: 0.16938523948192596
train_iter_loss: 0.1889343112707138
train_iter_loss: 0.1799321472644806
train_iter_loss: 0.3219057619571686
train_iter_loss: 0.38756388425827026
train_iter_loss: 0.1968347728252411
train_iter_loss: 0.4100430905818939
train_iter_loss: 0.2728768289089203
train_iter_loss: 0.43442824482917786
train_iter_loss: 0.47168684005737305
train_iter_loss: 0.2717055380344391
train_iter_loss: 0.0714455172419548
train_iter_loss: 0.34490206837654114
train_iter_loss: 0.2784072458744049
train_iter_loss: 0.30693480372428894
train_iter_loss: 0.32521477341651917
train_iter_loss: 0.28942808508872986
train_iter_loss: 0.41487452387809753
train_iter_loss: 0.28886309266090393
train_iter_loss: 0.20598050951957703
train_iter_loss: 0.26243171095848083
train_iter_loss: 0.34435945749282837
train_iter_loss: 0.2616315186023712
train_iter_loss: 0.3595229387283325
train_iter_loss: 0.20388086140155792
train_iter_loss: 0.4250733256340027
train_iter_loss: 0.10182343423366547
train_iter_loss: 0.23537677526474
train_iter_loss: 0.3441462814807892
train_iter_loss: 0.1424173265695572
train_iter_loss: 0.3437109887599945
train_iter_loss: 0.10390422493219376
train_iter_loss: 0.22516827285289764
train_iter_loss: 0.2524304986000061
train_iter_loss: 0.3426493704319
train_iter_loss: 0.27661842107772827
train_iter_loss: 0.21012845635414124
train_iter_loss: 0.2748676538467407
train_iter_loss: 0.0451119989156723
train_iter_loss: 0.2909291982650757
train_iter_loss: 0.2238055169582367
train_iter_loss: 0.22270290553569794
train_iter_loss: 0.1641736775636673
train_iter_loss: 0.26675236225128174
train_iter_loss: 0.15603302419185638
train_iter_loss: 0.24522264301776886
train_iter_loss: 0.30661433935165405
train_iter_loss: 0.2827360928058624
train_iter_loss: 0.29330888390541077
train_iter_loss: 0.22102326154708862
train_iter_loss: 0.30146580934524536
train_iter_loss: 0.2177998423576355
train_iter_loss: 0.2047075778245926
train_iter_loss: 0.41153788566589355
train_iter_loss: 0.33209195733070374
train_iter_loss: 0.2577662765979767
train_iter_loss: 0.07101325690746307
train_iter_loss: 0.15883250534534454
train_iter_loss: 0.464103639125824
train_iter_loss: 0.4393462538719177
train_iter_loss: 0.21862387657165527
train_iter_loss: 0.3821899890899658
train_iter_loss: 0.1679335981607437
train_iter_loss: 0.17283762991428375
train_iter_loss: 0.427340030670166
train_iter_loss: 0.34243878722190857
train_iter_loss: 0.2644782066345215
train_iter_loss: 0.2820587456226349
train loss :0.2661
---------------------
Validation seg loss: 0.4047832827452781 at epoch 614
epoch =    615/  1000, exp = train
train_iter_loss: 0.23622512817382812
train_iter_loss: 0.23803770542144775
train_iter_loss: 0.2942168414592743
train_iter_loss: 0.2142430543899536
train_iter_loss: 0.4381350874900818
train_iter_loss: 0.34466487169265747
train_iter_loss: 0.23432795703411102
train_iter_loss: 0.1847066581249237
train_iter_loss: 0.22872230410575867
train_iter_loss: 0.23188963532447815
train_iter_loss: 0.3265630304813385
train_iter_loss: 0.38222551345825195
train_iter_loss: 0.22591054439544678
train_iter_loss: 0.3600834906101227
train_iter_loss: 0.4123724102973938
train_iter_loss: 0.2578046917915344
train_iter_loss: 0.2901282012462616
train_iter_loss: 0.24588482081890106
train_iter_loss: 0.1532755196094513
train_iter_loss: 0.30155742168426514
train_iter_loss: 0.08422915637493134
train_iter_loss: 0.3062930405139923
train_iter_loss: 0.37669381499290466
train_iter_loss: 0.25729483366012573
train_iter_loss: 0.33459943532943726
train_iter_loss: 0.3274492621421814
train_iter_loss: 0.27614912390708923
train_iter_loss: 0.3129703402519226
train_iter_loss: 0.24327634274959564
train_iter_loss: 0.1541578471660614
train_iter_loss: 0.16685916483402252
train_iter_loss: 0.3019259572029114
train_iter_loss: 0.37371474504470825
train_iter_loss: 0.19051046669483185
train_iter_loss: 0.20937605202198029
train_iter_loss: 0.3385900557041168
train_iter_loss: 0.16183234751224518
train_iter_loss: 0.1778041273355484
train_iter_loss: 0.1527549773454666
train_iter_loss: 0.22384318709373474
train_iter_loss: 0.3502862751483917
train_iter_loss: 0.1346665918827057
train_iter_loss: 0.3586083948612213
train_iter_loss: 0.24630028009414673
train_iter_loss: 0.21037739515304565
train_iter_loss: 0.2985159456729889
train_iter_loss: 0.25323349237442017
train_iter_loss: 0.21973973512649536
train_iter_loss: 0.3143005669116974
train_iter_loss: 0.2897692918777466
train_iter_loss: 0.293003648519516
train_iter_loss: 0.21394208073616028
train_iter_loss: 0.18623700737953186
train_iter_loss: 0.20645911991596222
train_iter_loss: 0.08380590379238129
train_iter_loss: 0.33389154076576233
train_iter_loss: 0.22189489006996155
train_iter_loss: 0.2636691927909851
train_iter_loss: 0.2016710638999939
train_iter_loss: 0.2165241539478302
train_iter_loss: 0.27181798219680786
train_iter_loss: 0.2976599633693695
train_iter_loss: 0.26967141032218933
train_iter_loss: 0.27412477135658264
train_iter_loss: 0.2639589011669159
train_iter_loss: 0.1949862241744995
train_iter_loss: 0.2568829655647278
train_iter_loss: 0.27692365646362305
train_iter_loss: 0.3556743264198303
train_iter_loss: 0.4562970995903015
train_iter_loss: 0.15633685886859894
train_iter_loss: 0.3194921910762787
train_iter_loss: 0.36467036604881287
train_iter_loss: 0.30238813161849976
train_iter_loss: 0.298662930727005
train_iter_loss: 0.2543924152851105
train_iter_loss: 0.17958326637744904
train_iter_loss: 0.2893906533718109
train_iter_loss: 0.2618781328201294
train_iter_loss: 0.23911155760288239
train_iter_loss: 0.31379997730255127
train_iter_loss: 0.3491983711719513
train_iter_loss: 0.12981514632701874
train_iter_loss: 0.22524791955947876
train_iter_loss: 0.14484626054763794
train_iter_loss: 0.2855578362941742
train_iter_loss: 0.2600061595439911
train_iter_loss: 0.33166971802711487
train_iter_loss: 0.33327716588974
train_iter_loss: 0.31432175636291504
train_iter_loss: 0.25603753328323364
train_iter_loss: 0.3154909014701843
train_iter_loss: 0.27890604734420776
train_iter_loss: 0.32263273000717163
train_iter_loss: 0.48008471727371216
train_iter_loss: 0.22646726667881012
train_iter_loss: 0.36647188663482666
train_iter_loss: 0.2720445692539215
train_iter_loss: 0.1859615296125412
train_iter_loss: 0.3332054018974304
train loss :0.2704
---------------------
Validation seg loss: 0.3513988797236585 at epoch 615
epoch =    616/  1000, exp = train
train_iter_loss: 0.22991745173931122
train_iter_loss: 0.23568609356880188
train_iter_loss: 0.3315308392047882
train_iter_loss: 0.3990056812763214
train_iter_loss: 0.18423815071582794
train_iter_loss: 0.22348414361476898
train_iter_loss: 0.3496752977371216
train_iter_loss: 0.16232828795909882
train_iter_loss: 0.17355278134346008
train_iter_loss: 0.4761081337928772
train_iter_loss: 0.23137427866458893
train_iter_loss: 0.3896090090274811
train_iter_loss: 0.2087496668100357
train_iter_loss: 0.1936231404542923
train_iter_loss: 0.1988578885793686
train_iter_loss: 0.2713193893432617
train_iter_loss: 0.1599803864955902
train_iter_loss: 0.3694039285182953
train_iter_loss: 0.23236116766929626
train_iter_loss: 0.37908244132995605
train_iter_loss: 0.29742446541786194
train_iter_loss: 0.18658706545829773
train_iter_loss: 0.21681925654411316
train_iter_loss: 0.2842910885810852
train_iter_loss: 0.20030127465724945
train_iter_loss: 0.20442508161067963
train_iter_loss: 0.16431711614131927
train_iter_loss: 0.22332097589969635
train_iter_loss: 0.24776668846607208
train_iter_loss: 0.19105935096740723
train_iter_loss: 0.19789062440395355
train_iter_loss: 0.17700941860675812
train_iter_loss: 0.265802264213562
train_iter_loss: 0.225923553109169
train_iter_loss: 0.300320029258728
train_iter_loss: 0.3333107531070709
train_iter_loss: 0.38190314173698425
train_iter_loss: 0.2866290211677551
train_iter_loss: 0.2913428246974945
train_iter_loss: 0.17727625370025635
train_iter_loss: 0.2487393170595169
train_iter_loss: 0.25003179907798767
train_iter_loss: 0.4703052043914795
train_iter_loss: 0.18198110163211823
train_iter_loss: 0.373842716217041
train_iter_loss: 0.17162998020648956
train_iter_loss: 0.2555949091911316
train_iter_loss: 0.2642635703086853
train_iter_loss: 0.39933955669403076
train_iter_loss: 0.35246220231056213
train_iter_loss: 0.30206629633903503
train_iter_loss: 0.08749587833881378
train_iter_loss: 0.2176521122455597
train_iter_loss: 0.34351375699043274
train_iter_loss: 0.3696748912334442
train_iter_loss: 0.25315162539482117
train_iter_loss: 0.22808146476745605
train_iter_loss: 0.16234995424747467
train_iter_loss: 0.44431817531585693
train_iter_loss: 0.22232264280319214
train_iter_loss: 0.25419703125953674
train_iter_loss: 0.22248248755931854
train_iter_loss: 0.14894619584083557
train_iter_loss: 0.2703878581523895
train_iter_loss: 0.4145837128162384
train_iter_loss: 0.33293044567108154
train_iter_loss: 0.32833361625671387
train_iter_loss: 0.3214433491230011
train_iter_loss: 0.21626530587673187
train_iter_loss: 0.16146987676620483
train_iter_loss: 0.3034431040287018
train_iter_loss: 0.1293102502822876
train_iter_loss: 0.30958521366119385
train_iter_loss: 0.25597891211509705
train_iter_loss: 0.21345274150371552
train_iter_loss: 0.3653140962123871
train_iter_loss: 0.23384436964988708
train_iter_loss: 0.36772266030311584
train_iter_loss: 0.22388026118278503
train_iter_loss: 0.17117418348789215
train_iter_loss: 0.26891830563545227
train_iter_loss: 0.3279620409011841
train_iter_loss: 0.29653432965278625
train_iter_loss: 0.15954545140266418
train_iter_loss: 0.45766332745552063
train_iter_loss: 0.2703549265861511
train_iter_loss: 0.21282073855400085
train_iter_loss: 0.3535197079181671
train_iter_loss: 0.27738094329833984
train_iter_loss: 0.33945348858833313
train_iter_loss: 0.24563077092170715
train_iter_loss: 0.23156212270259857
train_iter_loss: 0.22665293514728546
train_iter_loss: 0.21520934998989105
train_iter_loss: 0.0752924457192421
train_iter_loss: 0.11734121292829514
train_iter_loss: 0.2211904227733612
train_iter_loss: 0.20955313742160797
train_iter_loss: 0.280457079410553
train_iter_loss: 0.23809389770030975
train loss :0.2639
---------------------
Validation seg loss: 0.36011316059245113 at epoch 616
epoch =    617/  1000, exp = train
train_iter_loss: 0.31113699078559875
train_iter_loss: 0.42607682943344116
train_iter_loss: 0.2847807705402374
train_iter_loss: 0.38833868503570557
train_iter_loss: 0.11962972581386566
train_iter_loss: 0.15272556245326996
train_iter_loss: 0.3274688124656677
train_iter_loss: 0.36111703515052795
train_iter_loss: 0.23762623965740204
train_iter_loss: 0.1945226937532425
train_iter_loss: 0.28385937213897705
train_iter_loss: 0.33637532591819763
train_iter_loss: 0.29146161675453186
train_iter_loss: 0.319717675447464
train_iter_loss: 0.3537726104259491
train_iter_loss: 0.42957666516304016
train_iter_loss: 0.21528634428977966
train_iter_loss: 0.27976933121681213
train_iter_loss: 0.16836845874786377
train_iter_loss: 0.25829896330833435
train_iter_loss: 0.40987488627433777
train_iter_loss: 0.286005437374115
train_iter_loss: 0.22964301705360413
train_iter_loss: 0.16327877342700958
train_iter_loss: 0.24329523742198944
train_iter_loss: 0.3006840646266937
train_iter_loss: 0.14102478325366974
train_iter_loss: 0.19596970081329346
train_iter_loss: 0.22278855741024017
train_iter_loss: 0.16311755776405334
train_iter_loss: 0.16688236594200134
train_iter_loss: 0.33195215463638306
train_iter_loss: 0.2993195950984955
train_iter_loss: 0.29851752519607544
train_iter_loss: 0.32968512177467346
train_iter_loss: 0.21529914438724518
train_iter_loss: 0.21438081562519073
train_iter_loss: 0.19356851279735565
train_iter_loss: 0.0883011668920517
train_iter_loss: 0.23966290056705475
train_iter_loss: 0.24733202159404755
train_iter_loss: 0.33106324076652527
train_iter_loss: 0.28124064207077026
train_iter_loss: 0.17116592824459076
train_iter_loss: 0.2563532590866089
train_iter_loss: 0.30547428131103516
train_iter_loss: 0.1841803342103958
train_iter_loss: 0.4373609721660614
train_iter_loss: 0.27616164088249207
train_iter_loss: 0.2315438687801361
train_iter_loss: 0.3147672712802887
train_iter_loss: 0.4072808623313904
train_iter_loss: 0.20022961497306824
train_iter_loss: 0.0962534174323082
train_iter_loss: 0.22586344182491302
train_iter_loss: 0.5469509959220886
train_iter_loss: 0.25186413526535034
train_iter_loss: 0.36103755235671997
train_iter_loss: 0.2496187537908554
train_iter_loss: 0.14286845922470093
train_iter_loss: 0.14778311550617218
train_iter_loss: 0.313854843378067
train_iter_loss: 0.3277980089187622
train_iter_loss: 0.24108436703681946
train_iter_loss: 0.15595823526382446
train_iter_loss: 0.4151897728443146
train_iter_loss: 0.362667053937912
train_iter_loss: 0.17881126701831818
train_iter_loss: 0.28992927074432373
train_iter_loss: 0.15740954875946045
train_iter_loss: 0.41150715947151184
train_iter_loss: 0.27997931838035583
train_iter_loss: 0.1580982208251953
train_iter_loss: 0.32264482975006104
train_iter_loss: 0.20839963853359222
train_iter_loss: 0.19373510777950287
train_iter_loss: 0.23609386384487152
train_iter_loss: 0.2650374174118042
train_iter_loss: 0.23304973542690277
train_iter_loss: 0.15569092333316803
train_iter_loss: 0.19565381109714508
train_iter_loss: 0.28626301884651184
train_iter_loss: 0.3384269177913666
train_iter_loss: 0.20059673488140106
train_iter_loss: 0.32051584124565125
train_iter_loss: 0.3198339641094208
train_iter_loss: 0.23058708012104034
train_iter_loss: 0.1748191863298416
train_iter_loss: 0.2163415104150772
train_iter_loss: 0.28037458658218384
train_iter_loss: 0.2986866533756256
train_iter_loss: 0.14763250946998596
train_iter_loss: 0.24870261549949646
train_iter_loss: 0.20413988828659058
train_iter_loss: 0.18072420358657837
train_iter_loss: 0.18765303492546082
train_iter_loss: 0.1441469043493271
train_iter_loss: 0.13818669319152832
train_iter_loss: 0.2526429295539856
train_iter_loss: 0.24782530963420868
train loss :0.2592
---------------------
Validation seg loss: 0.3634682520696858 at epoch 617
epoch =    618/  1000, exp = train
train_iter_loss: 0.19215086102485657
train_iter_loss: 0.18144576251506805
train_iter_loss: 0.2680836617946625
train_iter_loss: 0.3263270854949951
train_iter_loss: 0.23863311111927032
train_iter_loss: 0.13920661807060242
train_iter_loss: 0.3683496415615082
train_iter_loss: 0.23654314875602722
train_iter_loss: 0.29481300711631775
train_iter_loss: 0.3219834864139557
train_iter_loss: 0.2592596709728241
train_iter_loss: 0.22544507682323456
train_iter_loss: 0.3401767313480377
train_iter_loss: 0.1005401462316513
train_iter_loss: 0.4131653904914856
train_iter_loss: 0.2183317095041275
train_iter_loss: 0.34370309114456177
train_iter_loss: 0.2394087016582489
train_iter_loss: 0.19544710218906403
train_iter_loss: 0.41843605041503906
train_iter_loss: 0.2760237157344818
train_iter_loss: 0.2755965292453766
train_iter_loss: 0.30662620067596436
train_iter_loss: 0.3102110028266907
train_iter_loss: 0.20036166906356812
train_iter_loss: 0.23120643198490143
train_iter_loss: 0.2166830450296402
train_iter_loss: 0.19615353643894196
train_iter_loss: 0.19840802252292633
train_iter_loss: 0.33212411403656006
train_iter_loss: 0.140530064702034
train_iter_loss: 0.37302958965301514
train_iter_loss: 0.2222488820552826
train_iter_loss: 0.3183991611003876
train_iter_loss: 0.19657936692237854
train_iter_loss: 0.31819117069244385
train_iter_loss: 0.27185094356536865
train_iter_loss: 0.36750832200050354
train_iter_loss: 0.33621200919151306
train_iter_loss: 0.11873579770326614
train_iter_loss: 0.3277880549430847
train_iter_loss: 0.2464611530303955
train_iter_loss: 0.18209151923656464
train_iter_loss: 0.15520696341991425
train_iter_loss: 0.24222411215305328
train_iter_loss: 0.3565443754196167
train_iter_loss: 0.16558875143527985
train_iter_loss: 0.3449137508869171
train_iter_loss: 0.3234292268753052
train_iter_loss: 0.4683801531791687
train_iter_loss: 0.441249281167984
train_iter_loss: 0.37864431738853455
train_iter_loss: 0.30786338448524475
train_iter_loss: 0.23100021481513977
train_iter_loss: 0.11545844376087189
train_iter_loss: 0.3830144703388214
train_iter_loss: 0.2534257471561432
train_iter_loss: 0.3198529779911041
train_iter_loss: 0.4145791530609131
train_iter_loss: 0.3795486390590668
train_iter_loss: 0.216908261179924
train_iter_loss: 0.22652682662010193
train_iter_loss: 0.381631463766098
train_iter_loss: 0.3156420588493347
train_iter_loss: 0.2890852987766266
train_iter_loss: 0.1888621747493744
train_iter_loss: 0.3800995647907257
train_iter_loss: 0.3446483314037323
train_iter_loss: 0.27505919337272644
train_iter_loss: 0.2798723578453064
train_iter_loss: 0.3008558452129364
train_iter_loss: 0.28025108575820923
train_iter_loss: 0.35788750648498535
train_iter_loss: 0.2030244767665863
train_iter_loss: 0.22422157227993011
train_iter_loss: 0.22950580716133118
train_iter_loss: 0.28731465339660645
train_iter_loss: 0.3440052568912506
train_iter_loss: 0.1219949722290039
train_iter_loss: 0.11846919357776642
train_iter_loss: 0.2683314085006714
train_iter_loss: 0.14810314774513245
train_iter_loss: 0.17178259789943695
train_iter_loss: 0.15382440388202667
train_iter_loss: 0.49760547280311584
train_iter_loss: 0.35157525539398193
train_iter_loss: 0.2957001328468323
train_iter_loss: 0.387382447719574
train_iter_loss: 0.21468870341777802
train_iter_loss: 0.3435598909854889
train_iter_loss: 0.2416123002767563
train_iter_loss: 0.2517046332359314
train_iter_loss: 0.2358824759721756
train_iter_loss: 0.19423584640026093
train_iter_loss: 0.40826892852783203
train_iter_loss: 0.2348792403936386
train_iter_loss: 0.17392458021640778
train_iter_loss: 0.3433702886104584
train_iter_loss: 0.16840164363384247
train_iter_loss: 0.266365647315979
train loss :0.2754
---------------------
Validation seg loss: 0.3694294151797328 at epoch 618
epoch =    619/  1000, exp = train
train_iter_loss: 0.2999531924724579
train_iter_loss: 0.2262525111436844
train_iter_loss: 0.3152305483818054
train_iter_loss: 0.3855224549770355
train_iter_loss: 0.26340413093566895
train_iter_loss: 0.5475913882255554
train_iter_loss: 0.2591301202774048
train_iter_loss: 0.37890705466270447
train_iter_loss: 0.27029991149902344
train_iter_loss: 0.2782871723175049
train_iter_loss: 0.2673330307006836
train_iter_loss: 0.29030051827430725
train_iter_loss: 0.20421932637691498
train_iter_loss: 0.2352915108203888
train_iter_loss: 0.19539391994476318
train_iter_loss: 0.148018941283226
train_iter_loss: 0.25010931491851807
train_iter_loss: 0.14220087230205536
train_iter_loss: 0.4509238004684448
train_iter_loss: 0.335559219121933
train_iter_loss: 0.23301830887794495
train_iter_loss: 0.15052062273025513
train_iter_loss: 0.2902124226093292
train_iter_loss: 0.35634320974349976
train_iter_loss: 0.11620064824819565
train_iter_loss: 0.40080803632736206
train_iter_loss: 0.19718512892723083
train_iter_loss: 0.34516435861587524
train_iter_loss: 0.28472328186035156
train_iter_loss: 0.22421719133853912
train_iter_loss: 0.34491682052612305
train_iter_loss: 0.35060468316078186
train_iter_loss: 0.20403636991977692
train_iter_loss: 0.137681782245636
train_iter_loss: 0.2895345985889435
train_iter_loss: 0.24549290537834167
train_iter_loss: 0.23496071994304657
train_iter_loss: 0.26755812764167786
train_iter_loss: 0.18972499668598175
train_iter_loss: 0.23386378586292267
train_iter_loss: 0.31588783860206604
train_iter_loss: 0.17804507911205292
train_iter_loss: 0.28275012969970703
train_iter_loss: 0.17681030929088593
train_iter_loss: 0.15929090976715088
train_iter_loss: 0.27166202664375305
train_iter_loss: 0.24270236492156982
train_iter_loss: 0.33683305978775024
train_iter_loss: 0.17127974331378937
train_iter_loss: 0.3945310711860657
train_iter_loss: 0.40749964118003845
train_iter_loss: 0.2515435218811035
train_iter_loss: 0.10986869037151337
train_iter_loss: 0.28901755809783936
train_iter_loss: 0.17916378378868103
train_iter_loss: 0.12899677455425262
train_iter_loss: 0.326820969581604
train_iter_loss: 0.3324298858642578
train_iter_loss: 0.17123375833034515
train_iter_loss: 0.377878338098526
train_iter_loss: 0.22778482735157013
train_iter_loss: 0.3277634382247925
train_iter_loss: 0.23636677861213684
train_iter_loss: 0.3174003064632416
train_iter_loss: 0.27716532349586487
train_iter_loss: 0.13241860270500183
train_iter_loss: 0.29306337237358093
train_iter_loss: 0.3425036668777466
train_iter_loss: 0.36796993017196655
train_iter_loss: 0.22248049080371857
train_iter_loss: 0.23625119030475616
train_iter_loss: 0.436981201171875
train_iter_loss: 0.30168285965919495
train_iter_loss: 0.3513185381889343
train_iter_loss: 0.3208722770214081
train_iter_loss: 0.16017934679985046
train_iter_loss: 0.36663275957107544
train_iter_loss: 0.18967881798744202
train_iter_loss: 0.3193558156490326
train_iter_loss: 0.2654828429222107
train_iter_loss: 0.3295151889324188
train_iter_loss: 0.18753771483898163
train_iter_loss: 0.2985050082206726
train_iter_loss: 0.256559818983078
train_iter_loss: 0.2654464840888977
train_iter_loss: 0.2614935338497162
train_iter_loss: 0.22888867557048798
train_iter_loss: 0.18236488103866577
train_iter_loss: 0.14113342761993408
train_iter_loss: 0.2293773889541626
train_iter_loss: 0.1724383682012558
train_iter_loss: 0.3193312883377075
train_iter_loss: 0.19918200373649597
train_iter_loss: 0.18036143481731415
train_iter_loss: 0.21983222663402557
train_iter_loss: 0.26556605100631714
train_iter_loss: 0.23011042177677155
train_iter_loss: 0.29282355308532715
train_iter_loss: 0.21466465294361115
train_iter_loss: 0.1858038604259491
train loss :0.2657
---------------------
Validation seg loss: 0.3588028201780651 at epoch 619
epoch =    620/  1000, exp = train
train_iter_loss: 0.26313304901123047
train_iter_loss: 0.28184348344802856
train_iter_loss: 0.1420051008462906
train_iter_loss: 0.2028191238641739
train_iter_loss: 0.1467394232749939
train_iter_loss: 0.30436837673187256
train_iter_loss: 0.1974167376756668
train_iter_loss: 0.5346676111221313
train_iter_loss: 0.42245936393737793
train_iter_loss: 0.30153027176856995
train_iter_loss: 0.2560264766216278
train_iter_loss: 0.1884608417749405
train_iter_loss: 0.15421126782894135
train_iter_loss: 0.3178561329841614
train_iter_loss: 0.31315284967422485
train_iter_loss: 0.2141563892364502
train_iter_loss: 0.24440997838974
train_iter_loss: 0.4042113721370697
train_iter_loss: 0.25822553038597107
train_iter_loss: 0.04122340679168701
train_iter_loss: 0.2550484538078308
train_iter_loss: 0.3045588731765747
train_iter_loss: 0.30447739362716675
train_iter_loss: 0.21059104800224304
train_iter_loss: 0.18545955419540405
train_iter_loss: 0.26450321078300476
train_iter_loss: 0.3207826316356659
train_iter_loss: 0.2557494342327118
train_iter_loss: 0.22107094526290894
train_iter_loss: 0.377695232629776
train_iter_loss: 0.22074726223945618
train_iter_loss: 0.2052842676639557
train_iter_loss: 0.31785547733306885
train_iter_loss: 0.15332874655723572
train_iter_loss: 0.27748623490333557
train_iter_loss: 0.4188544452190399
train_iter_loss: 0.27239808440208435
train_iter_loss: 0.2004258781671524
train_iter_loss: 0.3686926066875458
train_iter_loss: 0.13732019066810608
train_iter_loss: 0.2029184103012085
train_iter_loss: 0.2393822818994522
train_iter_loss: 0.17278502881526947
train_iter_loss: 0.3062923550605774
train_iter_loss: 0.30430537462234497
train_iter_loss: 0.3362588882446289
train_iter_loss: 0.2542223334312439
train_iter_loss: 0.31187716126441956
train_iter_loss: 0.23891885578632355
train_iter_loss: 0.25384026765823364
train_iter_loss: 0.37889450788497925
train_iter_loss: 0.1621464490890503
train_iter_loss: 0.3371986150741577
train_iter_loss: 0.16595864295959473
train_iter_loss: 0.19038906693458557
train_iter_loss: 0.33364608883857727
train_iter_loss: 0.4070797264575958
train_iter_loss: 0.26460686326026917
train_iter_loss: 0.22757762670516968
train_iter_loss: 0.1461603045463562
train_iter_loss: 0.333877831697464
train_iter_loss: 0.19605471193790436
train_iter_loss: 0.2665402591228485
train_iter_loss: 0.6346152424812317
train_iter_loss: 0.31785377860069275
train_iter_loss: 0.1445283442735672
train_iter_loss: 0.10919615626335144
train_iter_loss: 0.12718753516674042
train_iter_loss: 0.1127675473690033
train_iter_loss: 0.19977743923664093
train_iter_loss: 0.19741958379745483
train_iter_loss: 0.18976403772830963
train_iter_loss: 0.18429940938949585
train_iter_loss: 0.13955070078372955
train_iter_loss: 0.3644656538963318
train_iter_loss: 0.2982078790664673
train_iter_loss: 0.15553167462348938
train_iter_loss: 0.31056609749794006
train_iter_loss: 0.35785916447639465
train_iter_loss: 0.23151691257953644
train_iter_loss: 0.300862580537796
train_iter_loss: 0.32700762152671814
train_iter_loss: 0.19012579321861267
train_iter_loss: 0.1474180370569229
train_iter_loss: 0.2633090615272522
train_iter_loss: 0.30275678634643555
train_iter_loss: 0.1717144101858139
train_iter_loss: 0.2846800982952118
train_iter_loss: 0.3125523030757904
train_iter_loss: 0.3065817952156067
train_iter_loss: 0.20991812646389008
train_iter_loss: 0.3760092258453369
train_iter_loss: 0.24499040842056274
train_iter_loss: 0.30373379588127136
train_iter_loss: 0.2867605686187744
train_iter_loss: 0.3696698546409607
train_iter_loss: 0.3306965231895447
train_iter_loss: 0.17725156247615814
train_iter_loss: 0.3370044529438019
train_iter_loss: 0.1446908861398697
train loss :0.2622
---------------------
Validation seg loss: 0.3574008814867516 at epoch 620
epoch =    621/  1000, exp = train
train_iter_loss: 0.23463167250156403
train_iter_loss: 0.26225951313972473
train_iter_loss: 0.20316395163536072
train_iter_loss: 0.4801795780658722
train_iter_loss: 0.2218015044927597
train_iter_loss: 0.25377196073532104
train_iter_loss: 0.12832874059677124
train_iter_loss: 0.31836095452308655
train_iter_loss: 0.2380175143480301
train_iter_loss: 0.5244796872138977
train_iter_loss: 0.13220179080963135
train_iter_loss: 0.13957133889198303
train_iter_loss: 0.19251707196235657
train_iter_loss: 0.21474632620811462
train_iter_loss: 0.2533155679702759
train_iter_loss: 0.25070321559906006
train_iter_loss: 0.2212156504392624
train_iter_loss: 0.04912543296813965
train_iter_loss: 0.3169601559638977
train_iter_loss: 0.13662992417812347
train_iter_loss: 0.25184348225593567
train_iter_loss: 0.20922094583511353
train_iter_loss: 0.22356264293193817
train_iter_loss: 0.2137477993965149
train_iter_loss: 0.2312917709350586
train_iter_loss: 0.2878289520740509
train_iter_loss: 0.23144109547138214
train_iter_loss: 0.3170483410358429
train_iter_loss: 0.365140825510025
train_iter_loss: 0.22164872288703918
train_iter_loss: 0.17831820249557495
train_iter_loss: 0.3053160011768341
train_iter_loss: 0.34718042612075806
train_iter_loss: 0.09925469756126404
train_iter_loss: 0.24179236590862274
train_iter_loss: 0.2510147988796234
train_iter_loss: 0.3378887176513672
train_iter_loss: 0.32876962423324585
train_iter_loss: 0.48360010981559753
train_iter_loss: 0.33018583059310913
train_iter_loss: 0.1862318217754364
train_iter_loss: 0.4453336000442505
train_iter_loss: 0.266781210899353
train_iter_loss: 0.31238922476768494
train_iter_loss: 0.19974423944950104
train_iter_loss: 0.2969357371330261
train_iter_loss: 0.5538029074668884
train_iter_loss: 0.2504439055919647
train_iter_loss: 0.10669643431901932
train_iter_loss: 0.12839406728744507
train_iter_loss: 0.16191399097442627
train_iter_loss: 0.3798615038394928
train_iter_loss: 0.381264328956604
train_iter_loss: 0.2974793314933777
train_iter_loss: 0.21399006247520447
train_iter_loss: 0.3781091868877411
train_iter_loss: 0.261651873588562
train_iter_loss: 0.37018510699272156
train_iter_loss: 0.14782512187957764
train_iter_loss: 0.3386017680168152
train_iter_loss: 0.2242071032524109
train_iter_loss: 0.29911473393440247
train_iter_loss: 0.383577823638916
train_iter_loss: 0.1609400510787964
train_iter_loss: 0.30777549743652344
train_iter_loss: 0.2706233561038971
train_iter_loss: 0.29945072531700134
train_iter_loss: 0.18463581800460815
train_iter_loss: 0.3139907717704773
train_iter_loss: 0.3140580654144287
train_iter_loss: 0.21956685185432434
train_iter_loss: 0.15435944497585297
train_iter_loss: 0.297135591506958
train_iter_loss: 0.2541342079639435
train_iter_loss: 0.2779253125190735
train_iter_loss: 0.2763080298900604
train_iter_loss: 0.31862765550613403
train_iter_loss: 0.2724118232727051
train_iter_loss: 0.14007213711738586
train_iter_loss: 0.3375822901725769
train_iter_loss: 0.1807299554347992
train_iter_loss: 0.3053102493286133
train_iter_loss: 0.17600572109222412
train_iter_loss: 0.31298914551734924
train_iter_loss: 0.17697106301784515
train_iter_loss: 0.2196192741394043
train_iter_loss: 0.2108365297317505
train_iter_loss: 0.4050484597682953
train_iter_loss: 0.08033658564090729
train_iter_loss: 0.33797386288642883
train_iter_loss: 0.25676968693733215
train_iter_loss: 0.20920050144195557
train_iter_loss: 0.35716816782951355
train_iter_loss: 0.22067192196846008
train_iter_loss: 0.19108526408672333
train_iter_loss: 0.2734498679637909
train_iter_loss: 0.31941014528274536
train_iter_loss: 0.2841351628303528
train_iter_loss: 0.32219240069389343
train_iter_loss: 0.25094208121299744
train loss :0.2657
---------------------
Validation seg loss: 0.359069249100702 at epoch 621
epoch =    622/  1000, exp = train
train_iter_loss: 0.32295745611190796
train_iter_loss: 0.4098591208457947
train_iter_loss: 0.11701399087905884
train_iter_loss: 0.3430514633655548
train_iter_loss: 0.27698811888694763
train_iter_loss: 0.3030082583427429
train_iter_loss: 0.17574582993984222
train_iter_loss: 0.18956413865089417
train_iter_loss: 0.2748139798641205
train_iter_loss: 0.4797438383102417
train_iter_loss: 0.2927313446998596
train_iter_loss: 0.2649182081222534
train_iter_loss: 0.31423360109329224
train_iter_loss: 0.26426899433135986
train_iter_loss: 0.2217758297920227
train_iter_loss: 0.25856316089630127
train_iter_loss: 0.2950400412082672
train_iter_loss: 0.18351992964744568
train_iter_loss: 0.27620112895965576
train_iter_loss: 0.25867345929145813
train_iter_loss: 0.22107765078544617
train_iter_loss: 0.31646180152893066
train_iter_loss: 0.09597059339284897
train_iter_loss: 0.24615558981895447
train_iter_loss: 0.18917684257030487
train_iter_loss: 0.10839381814002991
train_iter_loss: 0.35467565059661865
train_iter_loss: 0.2489050179719925
train_iter_loss: 0.1551559716463089
train_iter_loss: 0.2641882598400116
train_iter_loss: 0.254545122385025
train_iter_loss: 0.3233225345611572
train_iter_loss: 0.3275299668312073
train_iter_loss: 0.28215211629867554
train_iter_loss: 0.18975971639156342
train_iter_loss: 0.29452916979789734
train_iter_loss: 0.25236889719963074
train_iter_loss: 0.3240340054035187
train_iter_loss: 0.24550434947013855
train_iter_loss: 0.4231862425804138
train_iter_loss: 0.1987244039773941
train_iter_loss: 0.1637006402015686
train_iter_loss: 0.20488795638084412
train_iter_loss: 0.25067824125289917
train_iter_loss: 0.2395307421684265
train_iter_loss: 0.30650487542152405
train_iter_loss: 0.19856034219264984
train_iter_loss: 0.16173464059829712
train_iter_loss: 0.2650725543498993
train_iter_loss: 0.3847258687019348
train_iter_loss: 0.31817859411239624
train_iter_loss: 0.24322310090065002
train_iter_loss: 0.28624236583709717
train_iter_loss: 0.26120319962501526
train_iter_loss: 0.3932950496673584
train_iter_loss: 0.3882997930049896
train_iter_loss: 0.1815553456544876
train_iter_loss: 0.2388593852519989
train_iter_loss: 0.3037501275539398
train_iter_loss: 0.36493581533432007
train_iter_loss: 0.2352267950773239
train_iter_loss: 0.3548450767993927
train_iter_loss: 0.25800660252571106
train_iter_loss: 0.08481021225452423
train_iter_loss: 0.36957645416259766
train_iter_loss: 0.24166052043437958
train_iter_loss: 0.17408743500709534
train_iter_loss: 0.4165433645248413
train_iter_loss: 0.3452998399734497
train_iter_loss: 0.288047730922699
train_iter_loss: 0.20563437044620514
train_iter_loss: 0.36266231536865234
train_iter_loss: 0.26860687136650085
train_iter_loss: 0.25042036175727844
train_iter_loss: 0.30466315150260925
train_iter_loss: 0.2986782193183899
train_iter_loss: 0.22699512541294098
train_iter_loss: 0.3450348377227783
train_iter_loss: 0.30106306076049805
train_iter_loss: 0.32686305046081543
train_iter_loss: 0.1971333920955658
train_iter_loss: 0.19092485308647156
train_iter_loss: 0.3365577161312103
train_iter_loss: 0.08806617558002472
train_iter_loss: 0.23408909142017365
train_iter_loss: 0.15309493243694305
train_iter_loss: 0.32569822669029236
train_iter_loss: 0.5790913701057434
train_iter_loss: 0.3186596632003784
train_iter_loss: 0.27776098251342773
train_iter_loss: 0.15847934782505035
train_iter_loss: 0.3122487962245941
train_iter_loss: 0.2040978968143463
train_iter_loss: 0.23127047717571259
train_iter_loss: 0.08859358727931976
train_iter_loss: 0.2785446345806122
train_iter_loss: 0.3047432005405426
train_iter_loss: 0.22004900872707367
train_iter_loss: 0.07765073329210281
train_iter_loss: 0.16596557199954987
train loss :0.2666
---------------------
Validation seg loss: 0.38501105168482885 at epoch 622
epoch =    623/  1000, exp = train
train_iter_loss: 0.21948599815368652
train_iter_loss: 0.39608997106552124
train_iter_loss: 0.2576717436313629
train_iter_loss: 0.22376367449760437
train_iter_loss: 0.24215267598628998
train_iter_loss: 0.3787159323692322
train_iter_loss: 0.3394622802734375
train_iter_loss: 0.14721189439296722
train_iter_loss: 0.2993369698524475
train_iter_loss: 0.256693035364151
train_iter_loss: 0.13717639446258545
train_iter_loss: 0.2994859516620636
train_iter_loss: 0.25353920459747314
train_iter_loss: 0.22601792216300964
train_iter_loss: 0.2546856105327606
train_iter_loss: 0.26984187960624695
train_iter_loss: 0.23004812002182007
train_iter_loss: 0.22481265664100647
train_iter_loss: 0.1546408236026764
train_iter_loss: 0.3705427646636963
train_iter_loss: 0.26142871379852295
train_iter_loss: 0.09009555727243423
train_iter_loss: 0.2788524925708771
train_iter_loss: 0.2655825912952423
train_iter_loss: 0.2350044548511505
train_iter_loss: 0.1852729469537735
train_iter_loss: 0.26307055354118347
train_iter_loss: 0.1651211529970169
train_iter_loss: 0.30380570888519287
train_iter_loss: 0.2446666806936264
train_iter_loss: 0.15228863060474396
train_iter_loss: 0.2282298058271408
train_iter_loss: 0.23006859421730042
train_iter_loss: 0.4286740720272064
train_iter_loss: 0.25809434056282043
train_iter_loss: 0.13406871259212494
train_iter_loss: 0.23059003055095673
train_iter_loss: 0.3009066581726074
train_iter_loss: 0.32257595658302307
train_iter_loss: 0.14822757244110107
train_iter_loss: 0.5548389554023743
train_iter_loss: 0.18509219586849213
train_iter_loss: 0.28406888246536255
train_iter_loss: 0.09414810687303543
train_iter_loss: 0.3080829381942749
train_iter_loss: 0.20869877934455872
train_iter_loss: 0.31739601492881775
train_iter_loss: 0.2971792221069336
train_iter_loss: 0.31288573145866394
train_iter_loss: 0.29210641980171204
train_iter_loss: 0.39114388823509216
train_iter_loss: 0.1860557645559311
train_iter_loss: 0.33941659331321716
train_iter_loss: 0.13441938161849976
train_iter_loss: 0.3025803864002228
train_iter_loss: 0.2123011350631714
train_iter_loss: 0.11478804051876068
train_iter_loss: 0.365248441696167
train_iter_loss: 0.2905224561691284
train_iter_loss: 0.24443696439266205
train_iter_loss: 0.46057185530662537
train_iter_loss: 0.43562155961990356
train_iter_loss: 0.370622456073761
train_iter_loss: 0.43597355484962463
train_iter_loss: 0.2021074891090393
train_iter_loss: 0.24715638160705566
train_iter_loss: 0.2869114875793457
train_iter_loss: 0.22158896923065186
train_iter_loss: 0.13854245841503143
train_iter_loss: 0.2961621582508087
train_iter_loss: 0.1098908931016922
train_iter_loss: 0.36415013670921326
train_iter_loss: 0.3047477900981903
train_iter_loss: 0.34124287962913513
train_iter_loss: 0.4190405309200287
train_iter_loss: 0.15112534165382385
train_iter_loss: 0.18958736956119537
train_iter_loss: 0.20050309598445892
train_iter_loss: 0.36513522267341614
train_iter_loss: 0.2351028323173523
train_iter_loss: 0.3431311547756195
train_iter_loss: 0.2617504298686981
train_iter_loss: 0.3078559935092926
train_iter_loss: 0.41241616010665894
train_iter_loss: 0.2367219626903534
train_iter_loss: 0.2746533751487732
train_iter_loss: 0.18983876705169678
train_iter_loss: 0.16423149406909943
train_iter_loss: 0.15447789430618286
train_iter_loss: 0.3042316734790802
train_iter_loss: 0.3175926208496094
train_iter_loss: 0.26178544759750366
train_iter_loss: 0.25582578778266907
train_iter_loss: 0.4102572500705719
train_iter_loss: 0.24915707111358643
train_iter_loss: 0.22810164093971252
train_iter_loss: 0.24543024599552155
train_iter_loss: 0.10110288858413696
train_iter_loss: 0.3866312503814697
train_iter_loss: 0.2500080168247223
train loss :0.2674
---------------------
Validation seg loss: 0.37001623787899623 at epoch 623
epoch =    624/  1000, exp = train
train_iter_loss: 0.284175843000412
train_iter_loss: 0.19065700471401215
train_iter_loss: 0.2977619469165802
train_iter_loss: 0.25445160269737244
train_iter_loss: 0.30292999744415283
train_iter_loss: 0.2549775540828705
train_iter_loss: 0.3735204041004181
train_iter_loss: 0.272966206073761
train_iter_loss: 0.1588553786277771
train_iter_loss: 0.2095348984003067
train_iter_loss: 0.2328544557094574
train_iter_loss: 0.2911204695701599
train_iter_loss: 0.4266003370285034
train_iter_loss: 0.21161946654319763
train_iter_loss: 0.21710672974586487
train_iter_loss: 0.263668954372406
train_iter_loss: 0.4138857126235962
train_iter_loss: 0.29429078102111816
train_iter_loss: 0.24309560656547546
train_iter_loss: 0.15379001200199127
train_iter_loss: 0.2048734426498413
train_iter_loss: 0.3051789104938507
train_iter_loss: 0.45809268951416016
train_iter_loss: 0.27333614230155945
train_iter_loss: 0.1499944031238556
train_iter_loss: 0.35540783405303955
train_iter_loss: 0.3450513780117035
train_iter_loss: 0.21641843020915985
train_iter_loss: 0.27434366941452026
train_iter_loss: 0.2714597284793854
train_iter_loss: 0.32746586203575134
train_iter_loss: 0.12324098497629166
train_iter_loss: 0.4344576299190521
train_iter_loss: 0.2511744499206543
train_iter_loss: 0.11784670501947403
train_iter_loss: 0.24422217905521393
train_iter_loss: 0.19735243916511536
train_iter_loss: 0.26211532950401306
train_iter_loss: 0.303384929895401
train_iter_loss: 0.15676014125347137
train_iter_loss: 0.33954519033432007
train_iter_loss: 0.17132851481437683
train_iter_loss: 0.2546466290950775
train_iter_loss: 0.27424585819244385
train_iter_loss: 0.24860680103302002
train_iter_loss: 0.42308828234672546
train_iter_loss: 0.30750277638435364
train_iter_loss: 0.2992995083332062
train_iter_loss: 0.31665346026420593
train_iter_loss: 0.34447693824768066
train_iter_loss: 0.13309542834758759
train_iter_loss: 0.2716301381587982
train_iter_loss: 0.22119532525539398
train_iter_loss: 0.27743542194366455
train_iter_loss: 0.18581826984882355
train_iter_loss: 0.334941029548645
train_iter_loss: 0.33181509375572205
train_iter_loss: 0.18545493483543396
train_iter_loss: 0.28497540950775146
train_iter_loss: 0.2550826370716095
train_iter_loss: 0.13834795355796814
train_iter_loss: 0.20832926034927368
train_iter_loss: 0.2562623620033264
train_iter_loss: 0.2799720764160156
train_iter_loss: 0.12850983440876007
train_iter_loss: 0.2774627208709717
train_iter_loss: 0.27041763067245483
train_iter_loss: 0.18054358661174774
train_iter_loss: 0.27283918857574463
train_iter_loss: 0.27125051617622375
train_iter_loss: 0.24831703305244446
train_iter_loss: 0.30897974967956543
train_iter_loss: 0.2710677683353424
train_iter_loss: 0.25187352299690247
train_iter_loss: 0.3361610174179077
train_iter_loss: 0.2743304669857025
train_iter_loss: 0.13590995967388153
train_iter_loss: 0.3262348473072052
train_iter_loss: 0.26548001170158386
train_iter_loss: 0.4828846752643585
train_iter_loss: 0.2733694612979889
train_iter_loss: 0.2561202049255371
train_iter_loss: 0.2074594348669052
train_iter_loss: 0.3599676787853241
train_iter_loss: 0.3847081661224365
train_iter_loss: 0.395138680934906
train_iter_loss: 0.25570181012153625
train_iter_loss: 0.26994550228118896
train_iter_loss: 0.04769071564078331
train_iter_loss: 0.3629266619682312
train_iter_loss: 0.27879637479782104
train_iter_loss: 0.47985541820526123
train_iter_loss: 0.32740309834480286
train_iter_loss: 0.3393159806728363
train_iter_loss: 0.1536249816417694
train_iter_loss: 0.22679442167282104
train_iter_loss: 0.44629812240600586
train_iter_loss: 0.1998331993818283
train_iter_loss: 0.2890963852405548
train_iter_loss: 0.15306997299194336
train loss :0.2724
---------------------
Validation seg loss: 0.36067033085515193 at epoch 624
epoch =    625/  1000, exp = train
train_iter_loss: 0.2988130748271942
train_iter_loss: 0.2701915502548218
train_iter_loss: 0.2622913420200348
train_iter_loss: 0.14348039031028748
train_iter_loss: 0.34824395179748535
train_iter_loss: 0.32090309262275696
train_iter_loss: 0.17718318104743958
train_iter_loss: 0.2873717248439789
train_iter_loss: 0.1644992083311081
train_iter_loss: 0.29562893509864807
train_iter_loss: 0.29003337025642395
train_iter_loss: 0.14150887727737427
train_iter_loss: 0.08747106045484543
train_iter_loss: 0.2298901230096817
train_iter_loss: 0.2247505486011505
train_iter_loss: 0.0953744649887085
train_iter_loss: 0.1806608885526657
train_iter_loss: 0.3672436773777008
train_iter_loss: 0.4631636142730713
train_iter_loss: 0.2798803746700287
train_iter_loss: 0.26669663190841675
train_iter_loss: 0.23381666839122772
train_iter_loss: 0.10774435847997665
train_iter_loss: 0.19373786449432373
train_iter_loss: 0.44278261065483093
train_iter_loss: 0.22384914755821228
train_iter_loss: 0.24157486855983734
train_iter_loss: 0.12010390311479568
train_iter_loss: 0.3525286614894867
train_iter_loss: 0.2044844925403595
train_iter_loss: 0.24615371227264404
train_iter_loss: 0.286786288022995
train_iter_loss: 0.18721292912960052
train_iter_loss: 0.2660669982433319
train_iter_loss: 0.20281362533569336
train_iter_loss: 0.14490585029125214
train_iter_loss: 0.4182779788970947
train_iter_loss: 0.2072894275188446
train_iter_loss: 0.07889673113822937
train_iter_loss: 0.5038567185401917
train_iter_loss: 0.2919883131980896
train_iter_loss: 0.40822717547416687
train_iter_loss: 0.32407623529434204
train_iter_loss: 0.33665671944618225
train_iter_loss: 0.21933016180992126
train_iter_loss: 0.2824714779853821
train_iter_loss: 0.3742806017398834
train_iter_loss: 0.23312979936599731
train_iter_loss: 0.3197082579135895
train_iter_loss: 0.20176437497138977
train_iter_loss: 0.35382649302482605
train_iter_loss: 0.1930645853281021
train_iter_loss: 0.36166301369667053
train_iter_loss: 0.2150488942861557
train_iter_loss: 0.1593416929244995
train_iter_loss: 0.3002626597881317
train_iter_loss: 0.3134402930736542
train_iter_loss: 0.28775444626808167
train_iter_loss: 0.3019264042377472
train_iter_loss: 0.11035805940628052
train_iter_loss: 0.28940051794052124
train_iter_loss: 0.3922988474369049
train_iter_loss: 0.373781681060791
train_iter_loss: 0.2093476504087448
train_iter_loss: 0.5244801044464111
train_iter_loss: 0.18751274049282074
train_iter_loss: 0.239376500248909
train_iter_loss: 0.12741786241531372
train_iter_loss: 0.2750929594039917
train_iter_loss: 0.27779459953308105
train_iter_loss: 0.11035982519388199
train_iter_loss: 0.22757138311862946
train_iter_loss: 0.31664568185806274
train_iter_loss: 0.4195539653301239
train_iter_loss: 0.2227052003145218
train_iter_loss: 0.36398011445999146
train_iter_loss: 0.16955527663230896
train_iter_loss: 0.16140012443065643
train_iter_loss: 0.151968851685524
train_iter_loss: 0.23960188031196594
train_iter_loss: 0.30836406350135803
train_iter_loss: 0.3217151463031769
train_iter_loss: 0.2586352229118347
train_iter_loss: 0.2130449414253235
train_iter_loss: 0.45176196098327637
train_iter_loss: 0.20559252798557281
train_iter_loss: 0.24605320394039154
train_iter_loss: 0.24335622787475586
train_iter_loss: 0.22206085920333862
train_iter_loss: 0.4515761137008667
train_iter_loss: 0.16774068772792816
train_iter_loss: 0.31693151593208313
train_iter_loss: 0.40533384680747986
train_iter_loss: 0.28858551383018494
train_iter_loss: 0.32791125774383545
train_iter_loss: 0.3423140048980713
train_iter_loss: 0.2162935584783554
train_iter_loss: 0.27703356742858887
train_iter_loss: 0.15935836732387543
train_iter_loss: 0.34962016344070435
train loss :0.2677
---------------------
Validation seg loss: 0.4040581411945651 at epoch 625
epoch =    626/  1000, exp = train
train_iter_loss: 0.24757064878940582
train_iter_loss: 0.23425619304180145
train_iter_loss: 0.30697527527809143
train_iter_loss: 0.23236355185508728
train_iter_loss: 0.2307077795267105
train_iter_loss: 0.24507345259189606
train_iter_loss: 0.2316841185092926
train_iter_loss: 0.22824379801750183
train_iter_loss: 0.3332945704460144
train_iter_loss: 0.2399584949016571
train_iter_loss: 0.29322150349617004
train_iter_loss: 0.2119341492652893
train_iter_loss: 0.390972763299942
train_iter_loss: 0.4120197594165802
train_iter_loss: 0.26962733268737793
train_iter_loss: 0.2379276603460312
train_iter_loss: 0.31156495213508606
train_iter_loss: 0.4242716431617737
train_iter_loss: 0.4005836546421051
train_iter_loss: 0.15259292721748352
train_iter_loss: 0.19396045804023743
train_iter_loss: 0.19386805593967438
train_iter_loss: 0.2453671544790268
train_iter_loss: 0.43378880620002747
train_iter_loss: 0.15692828595638275
train_iter_loss: 0.35214078426361084
train_iter_loss: 0.15416596829891205
train_iter_loss: 0.34637659788131714
train_iter_loss: 0.23458632826805115
train_iter_loss: 0.33666256070137024
train_iter_loss: 0.20715074241161346
train_iter_loss: 0.20085984468460083
train_iter_loss: 0.23113882541656494
train_iter_loss: 0.2541179358959198
train_iter_loss: 0.2775009274482727
train_iter_loss: 0.2678149938583374
train_iter_loss: 0.3809587359428406
train_iter_loss: 0.21017691493034363
train_iter_loss: 0.39029067754745483
train_iter_loss: 0.1373090147972107
train_iter_loss: 0.24077676236629486
train_iter_loss: 0.15072926878929138
train_iter_loss: 0.21668212115764618
train_iter_loss: 0.18025130033493042
train_iter_loss: 0.20984631776809692
train_iter_loss: 0.33091840147972107
train_iter_loss: 0.22620797157287598
train_iter_loss: 0.30618396401405334
train_iter_loss: 0.19946011900901794
train_iter_loss: 0.4196186065673828
train_iter_loss: 0.24397216737270355
train_iter_loss: 0.33225053548812866
train_iter_loss: 0.18985314667224884
train_iter_loss: 0.2641320526599884
train_iter_loss: 0.3456796109676361
train_iter_loss: 0.32959240674972534
train_iter_loss: 0.3077133893966675
train_iter_loss: 0.3685155212879181
train_iter_loss: 0.0533096008002758
train_iter_loss: 0.18494248390197754
train_iter_loss: 0.2410910427570343
train_iter_loss: 0.19843241572380066
train_iter_loss: 0.362131804227829
train_iter_loss: 0.30932530760765076
train_iter_loss: 0.3983716666698456
train_iter_loss: 0.2083728313446045
train_iter_loss: 0.14430159330368042
train_iter_loss: 0.40010425448417664
train_iter_loss: 0.15543702244758606
train_iter_loss: 0.260079562664032
train_iter_loss: 0.2059611976146698
train_iter_loss: 0.19660012423992157
train_iter_loss: 0.32603269815444946
train_iter_loss: 0.12540313601493835
train_iter_loss: 0.30325180292129517
train_iter_loss: 0.24542415142059326
train_iter_loss: 0.374478816986084
train_iter_loss: 0.3747307360172272
train_iter_loss: 0.34392523765563965
train_iter_loss: 0.30545350909233093
train_iter_loss: 0.18644948303699493
train_iter_loss: 0.12910008430480957
train_iter_loss: 0.16551373898983002
train_iter_loss: 0.27403080463409424
train_iter_loss: 0.1916026920080185
train_iter_loss: 0.32995620369911194
train_iter_loss: 0.27527329325675964
train_iter_loss: 0.2754985988140106
train_iter_loss: 0.19014348089694977
train_iter_loss: 0.26170963048934937
train_iter_loss: 0.19139525294303894
train_iter_loss: 0.3180674612522125
train_iter_loss: 0.2735477685928345
train_iter_loss: 0.38388484716415405
train_iter_loss: 0.4853792190551758
train_iter_loss: 0.2786681056022644
train_iter_loss: 0.255700021982193
train_iter_loss: 0.1565558761358261
train_iter_loss: 0.2668805420398712
train_iter_loss: 0.16323043406009674
train loss :0.2674
---------------------
Validation seg loss: 0.3910064238237815 at epoch 626
epoch =    627/  1000, exp = train
train_iter_loss: 0.12244661897420883
train_iter_loss: 0.15439780056476593
train_iter_loss: 0.34987887740135193
train_iter_loss: 0.13831458985805511
train_iter_loss: 0.34647008776664734
train_iter_loss: 0.08089769631624222
train_iter_loss: 0.2620301842689514
train_iter_loss: 0.18585291504859924
train_iter_loss: 0.22847947478294373
train_iter_loss: 0.2175099402666092
train_iter_loss: 0.22948072850704193
train_iter_loss: 0.22556529939174652
train_iter_loss: 0.12953966856002808
train_iter_loss: 0.3067423105239868
train_iter_loss: 0.20262782275676727
train_iter_loss: 0.31289777159690857
train_iter_loss: 0.4023388922214508
train_iter_loss: 0.2449786514043808
train_iter_loss: 0.25613293051719666
train_iter_loss: 0.2326473444700241
train_iter_loss: 0.5265964269638062
train_iter_loss: 0.32636919617652893
train_iter_loss: 0.22997941076755524
train_iter_loss: 0.20819304883480072
train_iter_loss: 0.32768726348876953
train_iter_loss: 0.39502665400505066
train_iter_loss: 0.40023520588874817
train_iter_loss: 0.3125324249267578
train_iter_loss: 0.15883032977581024
train_iter_loss: 0.13462002575397491
train_iter_loss: 0.282450795173645
train_iter_loss: 0.17581823468208313
train_iter_loss: 0.33068352937698364
train_iter_loss: 0.216683492064476
train_iter_loss: 0.2911980152130127
train_iter_loss: 0.2565740942955017
train_iter_loss: 0.14118216931819916
train_iter_loss: 0.15205951035022736
train_iter_loss: 0.3167603015899658
train_iter_loss: 0.3254673182964325
train_iter_loss: 0.38822779059410095
train_iter_loss: 0.37709522247314453
train_iter_loss: 0.31200844049453735
train_iter_loss: 0.30090516805648804
train_iter_loss: 0.3292861878871918
train_iter_loss: 0.2110893726348877
train_iter_loss: 0.31261536478996277
train_iter_loss: 0.27421122789382935
train_iter_loss: 0.2963569164276123
train_iter_loss: 0.1062479093670845
train_iter_loss: 0.1316450536251068
train_iter_loss: 0.35241854190826416
train_iter_loss: 0.4084109961986542
train_iter_loss: 0.1857692450284958
train_iter_loss: 0.4437221586704254
train_iter_loss: 0.21892139315605164
train_iter_loss: 0.13248486816883087
train_iter_loss: 0.33254745602607727
train_iter_loss: 0.26206642389297485
train_iter_loss: 0.10009094327688217
train_iter_loss: 0.2899441719055176
train_iter_loss: 0.23611971735954285
train_iter_loss: 0.21262037754058838
train_iter_loss: 0.1106582060456276
train_iter_loss: 0.2695099413394928
train_iter_loss: 0.20025067031383514
train_iter_loss: 0.3208075165748596
train_iter_loss: 0.2023230940103531
train_iter_loss: 0.29694628715515137
train_iter_loss: 0.31003352999687195
train_iter_loss: 0.27705976366996765
train_iter_loss: 0.37466809153556824
train_iter_loss: 0.1867133527994156
train_iter_loss: 0.3144186735153198
train_iter_loss: 0.27607738971710205
train_iter_loss: 0.25071579217910767
train_iter_loss: 0.14086604118347168
train_iter_loss: 0.21695244312286377
train_iter_loss: 0.3323439061641693
train_iter_loss: 0.2860298454761505
train_iter_loss: 0.33361440896987915
train_iter_loss: 0.23743431270122528
train_iter_loss: 0.17339959740638733
train_iter_loss: 0.1962483674287796
train_iter_loss: 0.3173832893371582
train_iter_loss: 0.34690165519714355
train_iter_loss: 0.19801464676856995
train_iter_loss: 0.26356709003448486
train_iter_loss: 0.23608988523483276
train_iter_loss: 0.41226842999458313
train_iter_loss: 0.3601774573326111
train_iter_loss: 0.12060052156448364
train_iter_loss: 0.15929925441741943
train_iter_loss: 0.3680537939071655
train_iter_loss: 0.2899532616138458
train_iter_loss: 0.2812519371509552
train_iter_loss: 0.41133788228034973
train_iter_loss: 0.30981871485710144
train_iter_loss: 0.23351499438285828
train_iter_loss: 0.306702196598053
train loss :0.2654
---------------------
Validation seg loss: 0.38820659551101755 at epoch 627
epoch =    628/  1000, exp = train
train_iter_loss: 0.36071136593818665
train_iter_loss: 0.2673681676387787
train_iter_loss: 0.37199339270591736
train_iter_loss: 0.15949083864688873
train_iter_loss: 0.28975608944892883
train_iter_loss: 0.19223342835903168
train_iter_loss: 0.23020511865615845
train_iter_loss: 0.13115176558494568
train_iter_loss: 0.3604499101638794
train_iter_loss: 0.3736509680747986
train_iter_loss: 0.197008416056633
train_iter_loss: 0.3346201479434967
train_iter_loss: 0.29423388838768005
train_iter_loss: 0.3157651722431183
train_iter_loss: 0.18111515045166016
train_iter_loss: 0.23242077231407166
train_iter_loss: 0.4718969464302063
train_iter_loss: 0.04819453880190849
train_iter_loss: 0.33104944229125977
train_iter_loss: 0.32954251766204834
train_iter_loss: 0.2511484622955322
train_iter_loss: 0.2755740284919739
train_iter_loss: 0.24847114086151123
train_iter_loss: 0.2976049780845642
train_iter_loss: 0.3806573748588562
train_iter_loss: 0.15841041505336761
train_iter_loss: 0.20629970729351044
train_iter_loss: 0.37456974387168884
train_iter_loss: 0.27157774567604065
train_iter_loss: 0.11207538843154907
train_iter_loss: 0.23799316585063934
train_iter_loss: 0.17206963896751404
train_iter_loss: 0.36805054545402527
train_iter_loss: 0.3007428050041199
train_iter_loss: 0.18345506489276886
train_iter_loss: 0.2575720548629761
train_iter_loss: 0.25286874175071716
train_iter_loss: 0.29880139231681824
train_iter_loss: 0.3855414390563965
train_iter_loss: 0.1782148778438568
train_iter_loss: 0.5045613646507263
train_iter_loss: 0.31192871928215027
train_iter_loss: 0.3133309781551361
train_iter_loss: 0.24774175882339478
train_iter_loss: 0.18700182437896729
train_iter_loss: 0.2370009422302246
train_iter_loss: 0.3630877137184143
train_iter_loss: 0.2512471675872803
train_iter_loss: 0.13704414665699005
train_iter_loss: 0.3433513343334198
train_iter_loss: 0.14294172823429108
train_iter_loss: 0.2519029974937439
train_iter_loss: 0.19999302923679352
train_iter_loss: 0.1827319860458374
train_iter_loss: 0.17006658017635345
train_iter_loss: 0.10480933636426926
train_iter_loss: 0.2287125140428543
train_iter_loss: 0.23675796389579773
train_iter_loss: 0.2611444890499115
train_iter_loss: 0.3319648802280426
train_iter_loss: 0.32441246509552
train_iter_loss: 0.14403127133846283
train_iter_loss: 0.19721579551696777
train_iter_loss: 0.08907563239336014
train_iter_loss: 0.3076905608177185
train_iter_loss: 0.27271923422813416
train_iter_loss: 0.3259457051753998
train_iter_loss: 0.3389754295349121
train_iter_loss: 0.1686188280582428
train_iter_loss: 0.17055284976959229
train_iter_loss: 0.20815183222293854
train_iter_loss: 0.22234177589416504
train_iter_loss: 0.31778809428215027
train_iter_loss: 0.24997106194496155
train_iter_loss: 0.35465386509895325
train_iter_loss: 0.2570207417011261
train_iter_loss: 0.20437242090702057
train_iter_loss: 0.2514699101448059
train_iter_loss: 0.424658864736557
train_iter_loss: 0.2704846262931824
train_iter_loss: 0.4078289866447449
train_iter_loss: 0.14241130650043488
train_iter_loss: 0.2617034316062927
train_iter_loss: 0.31846797466278076
train_iter_loss: 0.27628540992736816
train_iter_loss: 0.21241983771324158
train_iter_loss: 0.2705455720424652
train_iter_loss: 0.2921692132949829
train_iter_loss: 0.2530656158924103
train_iter_loss: 0.29133832454681396
train_iter_loss: 0.1449698954820633
train_iter_loss: 0.28450778126716614
train_iter_loss: 0.26055777072906494
train_iter_loss: 0.21015335619449615
train_iter_loss: 0.20990979671478271
train_iter_loss: 0.29444798827171326
train_iter_loss: 0.31928300857543945
train_iter_loss: 0.26933929324150085
train_iter_loss: 0.3856581449508667
train_iter_loss: 0.17839878797531128
train loss :0.2634
---------------------
Validation seg loss: 0.36575630945065674 at epoch 628
epoch =    629/  1000, exp = train
train_iter_loss: 0.38372349739074707
train_iter_loss: 0.2554161548614502
train_iter_loss: 0.18935008347034454
train_iter_loss: 0.3723222315311432
train_iter_loss: 0.21058128774166107
train_iter_loss: 0.3167339563369751
train_iter_loss: 0.35499733686447144
train_iter_loss: 0.25160348415374756
train_iter_loss: 0.306637704372406
train_iter_loss: 0.2053951472043991
train_iter_loss: 0.21134480834007263
train_iter_loss: 0.27033454179763794
train_iter_loss: 0.36493828892707825
train_iter_loss: 0.06866162270307541
train_iter_loss: 0.2569178342819214
train_iter_loss: 0.20783433318138123
train_iter_loss: 0.40651941299438477
train_iter_loss: 0.20705163478851318
train_iter_loss: 0.3094533383846283
train_iter_loss: 0.21465171873569489
train_iter_loss: 0.3157053589820862
train_iter_loss: 0.20623300969600677
train_iter_loss: 0.13769081234931946
train_iter_loss: 0.33208367228507996
train_iter_loss: 0.3647584319114685
train_iter_loss: 0.13548524677753448
train_iter_loss: 0.2739880084991455
train_iter_loss: 0.2774827778339386
train_iter_loss: 0.31683290004730225
train_iter_loss: 0.24013832211494446
train_iter_loss: 0.2900170683860779
train_iter_loss: 0.23649470508098602
train_iter_loss: 0.3691977560520172
train_iter_loss: 0.3616509437561035
train_iter_loss: 0.10460857301950455
train_iter_loss: 0.1549321860074997
train_iter_loss: 0.33492687344551086
train_iter_loss: 0.25293347239494324
train_iter_loss: 0.3127801716327667
train_iter_loss: 0.3501456379890442
train_iter_loss: 0.1133575290441513
train_iter_loss: 0.199815034866333
train_iter_loss: 0.33887895941734314
train_iter_loss: 0.16329963505268097
train_iter_loss: 0.37637174129486084
train_iter_loss: 0.15742146968841553
train_iter_loss: 0.24000206589698792
train_iter_loss: 0.26100626587867737
train_iter_loss: 0.18352845311164856
train_iter_loss: 0.3077189028263092
train_iter_loss: 0.1633632630109787
train_iter_loss: 0.2831898331642151
train_iter_loss: 0.5023590922355652
train_iter_loss: 0.14161477982997894
train_iter_loss: 0.3240400552749634
train_iter_loss: 0.34301048517227173
train_iter_loss: 0.39772579073905945
train_iter_loss: 0.32633379101753235
train_iter_loss: 0.43058809638023376
train_iter_loss: 0.25427597761154175
train_iter_loss: 0.24545034766197205
train_iter_loss: 0.11496195942163467
train_iter_loss: 0.2815247178077698
train_iter_loss: 0.2599678635597229
train_iter_loss: 0.38657107949256897
train_iter_loss: 0.2580477297306061
train_iter_loss: 0.12089665979146957
train_iter_loss: 0.30440500378608704
train_iter_loss: 0.25578585267066956
train_iter_loss: 0.22324806451797485
train_iter_loss: 0.46324023604393005
train_iter_loss: 0.2947331964969635
train_iter_loss: 0.2796260714530945
train_iter_loss: 0.024827685207128525
train_iter_loss: 0.31066063046455383
train_iter_loss: 0.290793240070343
train_iter_loss: 0.31393975019454956
train_iter_loss: 0.3221437335014343
train_iter_loss: 0.15154030919075012
train_iter_loss: 0.2343815267086029
train_iter_loss: 0.22605514526367188
train_iter_loss: 0.2246725857257843
train_iter_loss: 0.2409559041261673
train_iter_loss: 0.3351615369319916
train_iter_loss: 0.3338950276374817
train_iter_loss: 0.2805238962173462
train_iter_loss: 0.2754867672920227
train_iter_loss: 0.07738702744245529
train_iter_loss: 0.24774762988090515
train_iter_loss: 0.10437621921300888
train_iter_loss: 0.36869779229164124
train_iter_loss: 0.22094546258449554
train_iter_loss: 0.21165886521339417
train_iter_loss: 0.4346834719181061
train_iter_loss: 0.30005398392677307
train_iter_loss: 0.17895764112472534
train_iter_loss: 0.12820513546466827
train_iter_loss: 0.16496571898460388
train_iter_loss: 0.4250190258026123
train_iter_loss: 0.27093595266342163
train loss :0.2669
---------------------
Validation seg loss: 0.3495093551025076 at epoch 629
epoch =    630/  1000, exp = train
train_iter_loss: 0.18744078278541565
train_iter_loss: 0.2534290850162506
train_iter_loss: 0.2618463635444641
train_iter_loss: 0.3980676233768463
train_iter_loss: 0.38323143124580383
train_iter_loss: 0.5113171935081482
train_iter_loss: 0.2984403371810913
train_iter_loss: 0.20029032230377197
train_iter_loss: 0.19708387553691864
train_iter_loss: 0.41086816787719727
train_iter_loss: 0.3153028190135956
train_iter_loss: 0.29374897480010986
train_iter_loss: 0.23108331859111786
train_iter_loss: 0.29400360584259033
train_iter_loss: 0.23488405346870422
train_iter_loss: 0.3962254822254181
train_iter_loss: 0.1754900962114334
train_iter_loss: 0.32938218116760254
train_iter_loss: 0.2393120676279068
train_iter_loss: 0.30387023091316223
train_iter_loss: 0.28100553154945374
train_iter_loss: 0.28192511200904846
train_iter_loss: 0.1943562775850296
train_iter_loss: 0.1994214653968811
train_iter_loss: 0.36840319633483887
train_iter_loss: 0.3803166449069977
train_iter_loss: 0.18954172730445862
train_iter_loss: 0.21955057978630066
train_iter_loss: 0.21482212841510773
train_iter_loss: 0.23787245154380798
train_iter_loss: 0.5464779734611511
train_iter_loss: 0.23454585671424866
train_iter_loss: 0.2279130518436432
train_iter_loss: 0.3464432954788208
train_iter_loss: 0.3251165747642517
train_iter_loss: 0.4501233398914337
train_iter_loss: 0.24069973826408386
train_iter_loss: 0.2152356505393982
train_iter_loss: 0.30050942301750183
train_iter_loss: 0.16589531302452087
train_iter_loss: 0.40876126289367676
train_iter_loss: 0.12236199527978897
train_iter_loss: 0.3452973961830139
train_iter_loss: 0.21443887054920197
train_iter_loss: 0.21816551685333252
train_iter_loss: 0.23424062132835388
train_iter_loss: 0.3977389335632324
train_iter_loss: 0.2791256606578827
train_iter_loss: 0.4546149969100952
train_iter_loss: 0.2186163067817688
train_iter_loss: 0.2910434603691101
train_iter_loss: 0.19434766471385956
train_iter_loss: 0.36866357922554016
train_iter_loss: 0.17426051199436188
train_iter_loss: 0.17575545608997345
train_iter_loss: 0.2838784456253052
train_iter_loss: 0.11863987892866135
train_iter_loss: 0.143992081284523
train_iter_loss: 0.22397753596305847
train_iter_loss: 0.3214178681373596
train_iter_loss: 0.12597960233688354
train_iter_loss: 0.26850080490112305
train_iter_loss: 0.23875069618225098
train_iter_loss: 0.16271676123142242
train_iter_loss: 0.2691490352153778
train_iter_loss: 0.31733086705207825
train_iter_loss: 0.1386626660823822
train_iter_loss: 0.22477391362190247
train_iter_loss: 0.4063929319381714
train_iter_loss: 0.2994435429573059
train_iter_loss: 0.3376932144165039
train_iter_loss: 0.21313412487506866
train_iter_loss: 0.16724249720573425
train_iter_loss: 0.33578288555145264
train_iter_loss: 0.2746209502220154
train_iter_loss: 0.20407599210739136
train_iter_loss: 0.30621740221977234
train_iter_loss: 0.1073315441608429
train_iter_loss: 0.29060104489326477
train_iter_loss: 0.17322112619876862
train_iter_loss: 0.1897118240594864
train_iter_loss: 0.0961950421333313
train_iter_loss: 0.35625627636909485
train_iter_loss: 0.2102912813425064
train_iter_loss: 0.21208228170871735
train_iter_loss: 0.32190048694610596
train_iter_loss: 0.33052191138267517
train_iter_loss: 0.24775831401348114
train_iter_loss: 0.295980840921402
train_iter_loss: 0.227363720536232
train_iter_loss: 0.21637171506881714
train_iter_loss: 0.34607210755348206
train_iter_loss: 0.151173397898674
train_iter_loss: 0.27836093306541443
train_iter_loss: 0.2565959393978119
train_iter_loss: 0.3059062957763672
train_iter_loss: 0.21347051858901978
train_iter_loss: 0.1379862129688263
train_iter_loss: 0.14747722446918488
train_iter_loss: 0.07814865559339523
train loss :0.2647
---------------------
Validation seg loss: 0.3765553912710188 at epoch 630
epoch =    631/  1000, exp = train
train_iter_loss: 0.2631450295448303
train_iter_loss: 0.2199980765581131
train_iter_loss: 0.37096741795539856
train_iter_loss: 0.2600660026073456
train_iter_loss: 0.3508591651916504
train_iter_loss: 0.2430143803358078
train_iter_loss: 0.19357983767986298
train_iter_loss: 0.20846334099769592
train_iter_loss: 0.15673711895942688
train_iter_loss: 0.21736237406730652
train_iter_loss: 0.17228993773460388
train_iter_loss: 0.22160731256008148
train_iter_loss: 0.18471749126911163
train_iter_loss: 0.1900060623884201
train_iter_loss: 0.19072692096233368
train_iter_loss: 0.1515686810016632
train_iter_loss: 0.14132246375083923
train_iter_loss: 0.31695449352264404
train_iter_loss: 0.08682724833488464
train_iter_loss: 0.1169130727648735
train_iter_loss: 0.2693931460380554
train_iter_loss: 0.12548066675662994
train_iter_loss: 0.4387017786502838
train_iter_loss: 0.1949906051158905
train_iter_loss: 0.32394611835479736
train_iter_loss: 0.24523156881332397
train_iter_loss: 0.19572614133358002
train_iter_loss: 0.23749421536922455
train_iter_loss: 0.3562985062599182
train_iter_loss: 0.140963613986969
train_iter_loss: 0.2005113959312439
train_iter_loss: 0.3667885363101959
train_iter_loss: 0.27550995349884033
train_iter_loss: 0.11758098006248474
train_iter_loss: 0.20853614807128906
train_iter_loss: 0.10836295783519745
train_iter_loss: 0.3787180185317993
train_iter_loss: 0.09712239354848862
train_iter_loss: 0.4312228858470917
train_iter_loss: 0.23163758218288422
train_iter_loss: 0.1269567608833313
train_iter_loss: 0.44721895456314087
train_iter_loss: 0.3816823661327362
train_iter_loss: 0.294114351272583
train_iter_loss: 0.22165383398532867
train_iter_loss: 0.3103656470775604
train_iter_loss: 0.2206459492444992
train_iter_loss: 0.4202813506126404
train_iter_loss: 0.23933495581150055
train_iter_loss: 0.23677131533622742
train_iter_loss: 0.42605260014533997
train_iter_loss: 0.29909348487854004
train_iter_loss: 0.15472231805324554
train_iter_loss: 0.3249105215072632
train_iter_loss: 0.2815280854701996
train_iter_loss: 0.23173271119594574
train_iter_loss: 0.2543754279613495
train_iter_loss: 0.4063495099544525
train_iter_loss: 0.16253018379211426
train_iter_loss: 0.23870205879211426
train_iter_loss: 0.3020457327365875
train_iter_loss: 0.36315247416496277
train_iter_loss: 0.2068587988615036
train_iter_loss: 0.2029011845588684
train_iter_loss: 0.3648522198200226
train_iter_loss: 0.29106295108795166
train_iter_loss: 0.2049933820962906
train_iter_loss: 0.24484144151210785
train_iter_loss: 0.2147890329360962
train_iter_loss: 0.26070383191108704
train_iter_loss: 0.276265025138855
train_iter_loss: 0.2463553249835968
train_iter_loss: 0.28239205479621887
train_iter_loss: 0.30326491594314575
train_iter_loss: 0.4607754349708557
train_iter_loss: 0.33533376455307007
train_iter_loss: 0.25955820083618164
train_iter_loss: 0.3618156611919403
train_iter_loss: 0.3697119951248169
train_iter_loss: 0.3624969720840454
train_iter_loss: 0.1862844079732895
train_iter_loss: 0.2613348066806793
train_iter_loss: 0.19684047996997833
train_iter_loss: 0.3127446472644806
train_iter_loss: 0.24859119951725006
train_iter_loss: 0.41291218996047974
train_iter_loss: 0.10389374941587448
train_iter_loss: 0.3120212256908417
train_iter_loss: 0.3145459294319153
train_iter_loss: 0.2461043894290924
train_iter_loss: 0.28193172812461853
train_iter_loss: 0.4341340959072113
train_iter_loss: 0.32766321301460266
train_iter_loss: 0.37776702642440796
train_iter_loss: 0.17026673257350922
train_iter_loss: 0.2883790135383606
train_iter_loss: 0.28333351016044617
train_iter_loss: 0.3666468858718872
train_iter_loss: 0.3124173581600189
train_iter_loss: 0.14657790958881378
train loss :0.2664
---------------------
Validation seg loss: 0.353198889238795 at epoch 631
epoch =    632/  1000, exp = train
train_iter_loss: 0.33271273970603943
train_iter_loss: 0.16217276453971863
train_iter_loss: 0.23623310029506683
train_iter_loss: 0.26895320415496826
train_iter_loss: 0.07615584880113602
train_iter_loss: 0.2654714286327362
train_iter_loss: 0.2982209324836731
train_iter_loss: 0.3108457922935486
train_iter_loss: 0.29615649580955505
train_iter_loss: 0.2864287793636322
train_iter_loss: 0.14273591339588165
train_iter_loss: 0.39843642711639404
train_iter_loss: 0.35027533769607544
train_iter_loss: 0.4322393834590912
train_iter_loss: 0.18393641710281372
train_iter_loss: 0.330234169960022
train_iter_loss: 0.1537484973669052
train_iter_loss: 0.1628326177597046
train_iter_loss: 0.21707677841186523
train_iter_loss: 0.2841717004776001
train_iter_loss: 0.26121094822883606
train_iter_loss: 0.2294236570596695
train_iter_loss: 0.22711515426635742
train_iter_loss: 0.18914027512073517
train_iter_loss: 0.22704529762268066
train_iter_loss: 0.09777150303125381
train_iter_loss: 0.23457343876361847
train_iter_loss: 0.39986029267311096
train_iter_loss: 0.3495405912399292
train_iter_loss: 0.29612308740615845
train_iter_loss: 0.19835937023162842
train_iter_loss: 0.4250549077987671
train_iter_loss: 0.18066051602363586
train_iter_loss: 0.2655140459537506
train_iter_loss: 0.44028517603874207
train_iter_loss: 0.24726669490337372
train_iter_loss: 0.23910939693450928
train_iter_loss: 0.2070988416671753
train_iter_loss: 0.16385439038276672
train_iter_loss: 0.3735904395580292
train_iter_loss: 0.3475356996059418
train_iter_loss: 0.4291345179080963
train_iter_loss: 0.2507093548774719
train_iter_loss: 0.3335078954696655
train_iter_loss: 0.263411283493042
train_iter_loss: 0.19882388412952423
train_iter_loss: 0.23145101964473724
train_iter_loss: 0.4362977147102356
train_iter_loss: 0.34747037291526794
train_iter_loss: 0.3060106039047241
train_iter_loss: 0.29621827602386475
train_iter_loss: 0.25979992747306824
train_iter_loss: 0.26722627878189087
train_iter_loss: 0.24579238891601562
train_iter_loss: 0.3716764450073242
train_iter_loss: 0.2199702113866806
train_iter_loss: 0.21288323402404785
train_iter_loss: 0.38730332255363464
train_iter_loss: 0.41977858543395996
train_iter_loss: 0.26617881655693054
train_iter_loss: 0.5192168354988098
train_iter_loss: 0.4156511425971985
train_iter_loss: 0.2410091906785965
train_iter_loss: 0.32908082008361816
train_iter_loss: 0.20053227245807648
train_iter_loss: 0.3077109456062317
train_iter_loss: 0.30413946509361267
train_iter_loss: 0.2570565938949585
train_iter_loss: 0.3035432696342468
train_iter_loss: 0.384746789932251
train_iter_loss: 0.10068312287330627
train_iter_loss: 0.25644078850746155
train_iter_loss: 0.46220919489860535
train_iter_loss: 0.40771475434303284
train_iter_loss: 0.16136851906776428
train_iter_loss: 0.13764792680740356
train_iter_loss: 0.14618389308452606
train_iter_loss: 0.21085500717163086
train_iter_loss: 0.1879601627588272
train_iter_loss: 0.1066066175699234
train_iter_loss: 0.22579102218151093
train_iter_loss: 0.36917346715927124
train_iter_loss: 0.23035702109336853
train_iter_loss: 0.2971092164516449
train_iter_loss: 0.19782871007919312
train_iter_loss: 0.26150912046432495
train_iter_loss: 0.4388071596622467
train_iter_loss: 0.3100549876689911
train_iter_loss: 0.3007810115814209
train_iter_loss: 0.24342353641986847
train_iter_loss: 0.4315975308418274
train_iter_loss: 0.39776909351348877
train_iter_loss: 0.11286355555057526
train_iter_loss: 0.21085090935230255
train_iter_loss: 0.16732670366764069
train_iter_loss: 0.19112733006477356
train_iter_loss: 0.2262093424797058
train_iter_loss: 0.2600841224193573
train_iter_loss: 0.16811682283878326
train_iter_loss: 0.3716326653957367
train loss :0.2765
---------------------
Validation seg loss: 0.35328292420116375 at epoch 632
epoch =    633/  1000, exp = train
train_iter_loss: 0.38715288043022156
train_iter_loss: 0.23479123413562775
train_iter_loss: 0.18789346516132355
train_iter_loss: 0.22560042142868042
train_iter_loss: 0.31582269072532654
train_iter_loss: 0.32275888323783875
train_iter_loss: 0.1409996598958969
train_iter_loss: 0.3445224165916443
train_iter_loss: 0.3024766445159912
train_iter_loss: 0.10903409123420715
train_iter_loss: 0.2576063871383667
train_iter_loss: 0.22435396909713745
train_iter_loss: 0.3041105568408966
train_iter_loss: 0.4697220027446747
train_iter_loss: 0.1948825716972351
train_iter_loss: 0.4464401602745056
train_iter_loss: 0.3239443302154541
train_iter_loss: 0.12417487055063248
train_iter_loss: 0.14421600103378296
train_iter_loss: 0.21895857155323029
train_iter_loss: 0.27483975887298584
train_iter_loss: 0.21204018592834473
train_iter_loss: 0.36356988549232483
train_iter_loss: 0.18101634085178375
train_iter_loss: 0.20035795867443085
train_iter_loss: 0.17669090628623962
train_iter_loss: 0.19806495308876038
train_iter_loss: 0.1439019739627838
train_iter_loss: 0.3204638957977295
train_iter_loss: 0.3316879868507385
train_iter_loss: 0.24431665241718292
train_iter_loss: 0.3970421552658081
train_iter_loss: 0.17496420443058014
train_iter_loss: 0.34678375720977783
train_iter_loss: 0.12010642886161804
train_iter_loss: 0.39786839485168457
train_iter_loss: 0.17666281759738922
train_iter_loss: 0.1737375408411026
train_iter_loss: 0.29386070370674133
train_iter_loss: 0.275525838136673
train_iter_loss: 0.21070034801959991
train_iter_loss: 0.2175735980272293
train_iter_loss: 0.35265088081359863
train_iter_loss: 0.3142412006855011
train_iter_loss: 0.24409256875514984
train_iter_loss: 0.32578611373901367
train_iter_loss: 0.28574395179748535
train_iter_loss: 0.3746722638607025
train_iter_loss: 0.27519404888153076
train_iter_loss: 0.3560347855091095
train_iter_loss: 0.35157525539398193
train_iter_loss: 0.29068002104759216
train_iter_loss: 0.28670668601989746
train_iter_loss: 0.12709100544452667
train_iter_loss: 0.2887272834777832
train_iter_loss: 0.3267683982849121
train_iter_loss: 0.3734687268733978
train_iter_loss: 0.22569791972637177
train_iter_loss: 0.16892142593860626
train_iter_loss: 0.13462190330028534
train_iter_loss: 0.25975221395492554
train_iter_loss: 0.13810455799102783
train_iter_loss: 0.32103511691093445
train_iter_loss: 0.27793648838996887
train_iter_loss: 0.23799775540828705
train_iter_loss: 0.2625821530818939
train_iter_loss: 0.1966620683670044
train_iter_loss: 0.18066781759262085
train_iter_loss: 0.1690153181552887
train_iter_loss: 0.31910863518714905
train_iter_loss: 0.17172777652740479
train_iter_loss: 0.1204981878399849
train_iter_loss: 0.4026925265789032
train_iter_loss: 0.25169146060943604
train_iter_loss: 0.48931506276130676
train_iter_loss: 0.19044500589370728
train_iter_loss: 0.36796748638153076
train_iter_loss: 0.290901243686676
train_iter_loss: 0.22073376178741455
train_iter_loss: 0.33002951741218567
train_iter_loss: 0.21430371701717377
train_iter_loss: 0.33135539293289185
train_iter_loss: 0.3072814643383026
train_iter_loss: 0.2570379078388214
train_iter_loss: 0.25324589014053345
train_iter_loss: 0.23711198568344116
train_iter_loss: 0.28116151690483093
train_iter_loss: 0.21585515141487122
train_iter_loss: 0.23525837063789368
train_iter_loss: 0.07901155203580856
train_iter_loss: 0.377830445766449
train_iter_loss: 0.148747518658638
train_iter_loss: 0.28252020478248596
train_iter_loss: 0.08196507394313812
train_iter_loss: 0.17070022225379944
train_iter_loss: 0.3477088212966919
train_iter_loss: 0.37184563279151917
train_iter_loss: 0.1529276818037033
train_iter_loss: 0.4393778145313263
train_iter_loss: 0.343167245388031
train loss :0.2642
---------------------
Validation seg loss: 0.3748800364006662 at epoch 633
epoch =    634/  1000, exp = train
train_iter_loss: 0.1278243511915207
train_iter_loss: 0.3012774586677551
train_iter_loss: 0.4458673298358917
train_iter_loss: 0.2681376039981842
train_iter_loss: 0.2058357447385788
train_iter_loss: 0.21930533647537231
train_iter_loss: 0.15450993180274963
train_iter_loss: 0.18225842714309692
train_iter_loss: 0.3021402060985565
train_iter_loss: 0.23859712481498718
train_iter_loss: 0.0469028577208519
train_iter_loss: 0.21242345869541168
train_iter_loss: 0.26399898529052734
train_iter_loss: 0.32288438081741333
train_iter_loss: 0.1881331205368042
train_iter_loss: 0.27976295351982117
train_iter_loss: 0.47565481066703796
train_iter_loss: 0.18064145743846893
train_iter_loss: 0.31909698247909546
train_iter_loss: 0.3576880693435669
train_iter_loss: 0.3333777189254761
train_iter_loss: 0.3740178644657135
train_iter_loss: 0.24846716225147247
train_iter_loss: 0.33546242117881775
train_iter_loss: 0.32321324944496155
train_iter_loss: 0.4623083770275116
train_iter_loss: 0.22396254539489746
train_iter_loss: 0.15341272950172424
train_iter_loss: 0.2502102255821228
train_iter_loss: 0.2740531265735626
train_iter_loss: 0.2591269016265869
train_iter_loss: 0.24467845261096954
train_iter_loss: 0.25131505727767944
train_iter_loss: 0.18706734478473663
train_iter_loss: 0.10044395923614502
train_iter_loss: 0.3242434859275818
train_iter_loss: 0.2118065506219864
train_iter_loss: 0.26808440685272217
train_iter_loss: 0.2655333876609802
train_iter_loss: 0.27724015712738037
train_iter_loss: 0.19658049941062927
train_iter_loss: 0.3310873210430145
train_iter_loss: 0.3597966432571411
train_iter_loss: 0.2507467269897461
train_iter_loss: 0.3124316334724426
train_iter_loss: 0.32631832361221313
train_iter_loss: 0.32186082005500793
train_iter_loss: 0.23045611381530762
train_iter_loss: 0.30839550495147705
train_iter_loss: 0.3896845579147339
train_iter_loss: 0.2777271270751953
train_iter_loss: 0.18730498850345612
train_iter_loss: 0.3352629244327545
train_iter_loss: 0.26703810691833496
train_iter_loss: 0.2853766083717346
train_iter_loss: 0.30185839533805847
train_iter_loss: 0.167564257979393
train_iter_loss: 0.46245983242988586
train_iter_loss: 0.2663664221763611
train_iter_loss: 0.18567372858524323
train_iter_loss: 0.26028841733932495
train_iter_loss: 0.1469699591398239
train_iter_loss: 0.28286340832710266
train_iter_loss: 0.3566717803478241
train_iter_loss: 0.1889893263578415
train_iter_loss: 0.16737805306911469
train_iter_loss: 0.3009645342826843
train_iter_loss: 0.2802674472332001
train_iter_loss: 0.4709024727344513
train_iter_loss: 0.13008038699626923
train_iter_loss: 0.28770244121551514
train_iter_loss: 0.10686464607715607
train_iter_loss: 0.10731202363967896
train_iter_loss: 0.14301134645938873
train_iter_loss: 0.22119028866291046
train_iter_loss: 0.42620009183883667
train_iter_loss: 0.31961700320243835
train_iter_loss: 0.16668236255645752
train_iter_loss: 0.30322790145874023
train_iter_loss: 0.2952982485294342
train_iter_loss: 0.37072545289993286
train_iter_loss: 0.21551811695098877
train_iter_loss: 0.16262175142765045
train_iter_loss: 0.2824867367744446
train_iter_loss: 0.237153559923172
train_iter_loss: 0.37154555320739746
train_iter_loss: 0.0972357913851738
train_iter_loss: 0.27505621314048767
train_iter_loss: 0.2614314556121826
train_iter_loss: 0.24269334971904755
train_iter_loss: 0.31184113025665283
train_iter_loss: 0.31667572259902954
train_iter_loss: 0.40524938702583313
train_iter_loss: 0.1557351052761078
train_iter_loss: 0.22307917475700378
train_iter_loss: 0.23262305557727814
train_iter_loss: 0.3038524389266968
train_iter_loss: 0.2643834352493286
train_iter_loss: 0.19402436912059784
train_iter_loss: 0.4349110424518585
train loss :0.2681
---------------------
Validation seg loss: 0.39047777075496204 at epoch 634
epoch =    635/  1000, exp = train
train_iter_loss: 0.23193609714508057
train_iter_loss: 0.2664470672607422
train_iter_loss: 0.19125691056251526
train_iter_loss: 0.24826928973197937
train_iter_loss: 0.4108235836029053
train_iter_loss: 0.24527671933174133
train_iter_loss: 0.13942894339561462
train_iter_loss: 0.32613101601600647
train_iter_loss: 0.2050478607416153
train_iter_loss: 0.2740955352783203
train_iter_loss: 0.1326826512813568
train_iter_loss: 0.2366010993719101
train_iter_loss: 0.28727564215660095
train_iter_loss: 0.25887855887413025
train_iter_loss: 0.28515228629112244
train_iter_loss: 0.3892160952091217
train_iter_loss: 0.36185747385025024
train_iter_loss: 0.2794046401977539
train_iter_loss: 0.1918022334575653
train_iter_loss: 0.3845847547054291
train_iter_loss: 0.21365320682525635
train_iter_loss: 0.3508032560348511
train_iter_loss: 0.10537046939134598
train_iter_loss: 0.4282890558242798
train_iter_loss: 0.2756001651287079
train_iter_loss: 0.3207160532474518
train_iter_loss: 0.24945543706417084
train_iter_loss: 0.3045390248298645
train_iter_loss: 0.2157808393239975
train_iter_loss: 0.22070613503456116
train_iter_loss: 0.26743847131729126
train_iter_loss: 0.09892813861370087
train_iter_loss: 0.24948656558990479
train_iter_loss: 0.2826283872127533
train_iter_loss: 0.2670336663722992
train_iter_loss: 0.3912234306335449
train_iter_loss: 0.287302166223526
train_iter_loss: 0.27226030826568604
train_iter_loss: 0.1538008600473404
train_iter_loss: 0.3445485830307007
train_iter_loss: 0.23664924502372742
train_iter_loss: 0.38639068603515625
train_iter_loss: 0.096051886677742
train_iter_loss: 0.25648945569992065
train_iter_loss: 0.4148108959197998
train_iter_loss: 0.3989439904689789
train_iter_loss: 0.3547556400299072
train_iter_loss: 0.2977643609046936
train_iter_loss: 0.14498698711395264
train_iter_loss: 0.23071034252643585
train_iter_loss: 0.2854636013507843
train_iter_loss: 0.24456483125686646
train_iter_loss: 0.3023384213447571
train_iter_loss: 0.20102088153362274
train_iter_loss: 0.2627813220024109
train_iter_loss: 0.12225762009620667
train_iter_loss: 0.33033233880996704
train_iter_loss: 0.3664393424987793
train_iter_loss: 0.3029102683067322
train_iter_loss: 0.25631022453308105
train_iter_loss: 0.22259876132011414
train_iter_loss: 0.38470542430877686
train_iter_loss: 0.3610117435455322
train_iter_loss: 0.2687426805496216
train_iter_loss: 0.2419571429491043
train_iter_loss: 0.1319037228822708
train_iter_loss: 0.2659178078174591
train_iter_loss: 0.16167767345905304
train_iter_loss: 0.20824427902698517
train_iter_loss: 0.3236065208911896
train_iter_loss: 0.23910395801067352
train_iter_loss: 0.1303398758172989
train_iter_loss: 0.2530452311038971
train_iter_loss: 0.30884531140327454
train_iter_loss: 0.32679110765457153
train_iter_loss: 0.4572388827800751
train_iter_loss: 0.2760502099990845
train_iter_loss: 0.24963489174842834
train_iter_loss: 0.12371382117271423
train_iter_loss: 0.21419590711593628
train_iter_loss: 0.2727632224559784
train_iter_loss: 0.1574874073266983
train_iter_loss: 0.3461436927318573
train_iter_loss: 0.3130216896533966
train_iter_loss: 0.17264750599861145
train_iter_loss: 0.27631816267967224
train_iter_loss: 0.29764339327812195
train_iter_loss: 0.2600961923599243
train_iter_loss: 0.26203691959381104
train_iter_loss: 0.3225694000720978
train_iter_loss: 0.2558974325656891
train_iter_loss: 0.2650638520717621
train_iter_loss: 0.1465752273797989
train_iter_loss: 0.1474824845790863
train_iter_loss: 0.34102603793144226
train_iter_loss: 0.19939960539340973
train_iter_loss: 0.25464725494384766
train_iter_loss: 0.22664602100849152
train_iter_loss: 0.19290758669376373
train_iter_loss: 0.2839243710041046
train loss :0.2655
---------------------
Validation seg loss: 0.34653951865972354 at epoch 635
epoch =    636/  1000, exp = train
train_iter_loss: 0.17711320519447327
train_iter_loss: 0.28360071778297424
train_iter_loss: 0.18560239672660828
train_iter_loss: 0.2876186966896057
train_iter_loss: 0.3482268750667572
train_iter_loss: 0.1796034425497055
train_iter_loss: 0.21420396864414215
train_iter_loss: 0.3033958375453949
train_iter_loss: 0.18572112917900085
train_iter_loss: 0.1491849720478058
train_iter_loss: 0.14099347591400146
train_iter_loss: 0.08695574104785919
train_iter_loss: 0.3394213318824768
train_iter_loss: 0.18241456151008606
train_iter_loss: 0.2740634083747864
train_iter_loss: 0.2384612262248993
train_iter_loss: 0.23145590722560883
train_iter_loss: 0.27260228991508484
train_iter_loss: 0.2813575267791748
train_iter_loss: 0.43941977620124817
train_iter_loss: 0.20473003387451172
train_iter_loss: 0.3018623888492584
train_iter_loss: 0.21894583106040955
train_iter_loss: 0.2964616119861603
train_iter_loss: 0.3909345269203186
train_iter_loss: 0.37431955337524414
train_iter_loss: 0.29786837100982666
train_iter_loss: 0.2135981023311615
train_iter_loss: 0.2606433629989624
train_iter_loss: 0.21014679968357086
train_iter_loss: 0.3238925337791443
train_iter_loss: 0.318692147731781
train_iter_loss: 0.18562886118888855
train_iter_loss: 0.1750389039516449
train_iter_loss: 0.18167203664779663
train_iter_loss: 0.39204391837120056
train_iter_loss: 0.3781448304653168
train_iter_loss: 0.2826507091522217
train_iter_loss: 0.2380763590335846
train_iter_loss: 0.0926336795091629
train_iter_loss: 0.33679959177970886
train_iter_loss: 0.2879955768585205
train_iter_loss: 0.1879153549671173
train_iter_loss: 0.29189902544021606
train_iter_loss: 0.30774590373039246
train_iter_loss: 0.16673898696899414
train_iter_loss: 0.3280024528503418
train_iter_loss: 0.329921156167984
train_iter_loss: 0.2317301332950592
train_iter_loss: 0.23861457407474518
train_iter_loss: 0.31992506980895996
train_iter_loss: 0.22729092836380005
train_iter_loss: 0.20257557928562164
train_iter_loss: 0.3250103294849396
train_iter_loss: 0.39245572686195374
train_iter_loss: 0.6481640338897705
train_iter_loss: 0.3031243085861206
train_iter_loss: 0.3193546235561371
train_iter_loss: 0.3729531764984131
train_iter_loss: 0.12846946716308594
train_iter_loss: 0.35564085841178894
train_iter_loss: 0.36466166377067566
train_iter_loss: 0.20606446266174316
train_iter_loss: 0.2411496937274933
train_iter_loss: 0.36945778131484985
train_iter_loss: 0.2027665674686432
train_iter_loss: 0.24139991402626038
train_iter_loss: 0.26852360367774963
train_iter_loss: 0.40978434681892395
train_iter_loss: 0.37743881344795227
train_iter_loss: 0.29508668184280396
train_iter_loss: 0.23209494352340698
train_iter_loss: 0.17993193864822388
train_iter_loss: 0.15385158360004425
train_iter_loss: 0.317574143409729
train_iter_loss: 0.45722562074661255
train_iter_loss: 0.3165007531642914
train_iter_loss: 0.24955213069915771
train_iter_loss: 0.2906031310558319
train_iter_loss: 0.20793384313583374
train_iter_loss: 0.3391711413860321
train_iter_loss: 0.18889346718788147
train_iter_loss: 0.24307337403297424
train_iter_loss: 0.21210800111293793
train_iter_loss: 0.24152828752994537
train_iter_loss: 0.23769915103912354
train_iter_loss: 0.30614399909973145
train_iter_loss: 0.22181469202041626
train_iter_loss: 0.2679134011268616
train_iter_loss: 0.1748412847518921
train_iter_loss: 0.3244912922382355
train_iter_loss: 0.15581724047660828
train_iter_loss: 0.15123116970062256
train_iter_loss: 0.2507058084011078
train_iter_loss: 0.28680938482284546
train_iter_loss: 0.37746667861938477
train_iter_loss: 0.21584148705005646
train_iter_loss: 0.27591148018836975
train_iter_loss: 0.20385214686393738
train_iter_loss: 0.16697655618190765
train loss :0.2693
---------------------
Validation seg loss: 0.36241087010833173 at epoch 636
epoch =    637/  1000, exp = train
train_iter_loss: 0.17693917453289032
train_iter_loss: 0.2044733762741089
train_iter_loss: 0.12644727528095245
train_iter_loss: 0.36576828360557556
train_iter_loss: 0.25753530859947205
train_iter_loss: 0.31481918692588806
train_iter_loss: 0.42326033115386963
train_iter_loss: 0.2730325162410736
train_iter_loss: 0.33046236634254456
train_iter_loss: 0.21988055109977722
train_iter_loss: 0.19435331225395203
train_iter_loss: 0.21466559171676636
train_iter_loss: 0.1200723871588707
train_iter_loss: 0.302383154630661
train_iter_loss: 0.26559388637542725
train_iter_loss: 0.2403850257396698
train_iter_loss: 0.25799375772476196
train_iter_loss: 0.19277426600456238
train_iter_loss: 0.398077130317688
train_iter_loss: 0.3026238977909088
train_iter_loss: 0.32456013560295105
train_iter_loss: 0.32185524702072144
train_iter_loss: 0.27018997073173523
train_iter_loss: 0.11396799236536026
train_iter_loss: 0.1431235522031784
train_iter_loss: 0.3281190097332001
train_iter_loss: 0.18800762295722961
train_iter_loss: 0.1665744036436081
train_iter_loss: 0.2772265076637268
train_iter_loss: 0.30390459299087524
train_iter_loss: 0.2772652804851532
train_iter_loss: 0.2532576620578766
train_iter_loss: 0.3139055073261261
train_iter_loss: 0.25702789425849915
train_iter_loss: 0.3326939344406128
train_iter_loss: 0.1267765909433365
train_iter_loss: 0.18726612627506256
train_iter_loss: 0.12924320995807648
train_iter_loss: 0.2080330103635788
train_iter_loss: 0.3500232994556427
train_iter_loss: 0.26401787996292114
train_iter_loss: 0.25483351945877075
train_iter_loss: 0.25634074211120605
train_iter_loss: 0.26889896392822266
train_iter_loss: 0.3211546242237091
train_iter_loss: 0.17304886877536774
train_iter_loss: 0.2508741319179535
train_iter_loss: 0.25832536816596985
train_iter_loss: 0.2620711624622345
train_iter_loss: 0.17561006546020508
train_iter_loss: 0.2875724732875824
train_iter_loss: 0.25104761123657227
train_iter_loss: 0.30417880415916443
train_iter_loss: 0.20540091395378113
train_iter_loss: 0.28028208017349243
train_iter_loss: 0.42543357610702515
train_iter_loss: 0.21503043174743652
train_iter_loss: 0.4156688451766968
train_iter_loss: 0.35354626178741455
train_iter_loss: 0.1864343136548996
train_iter_loss: 0.2971183657646179
train_iter_loss: 0.402654230594635
train_iter_loss: 0.23600374162197113
train_iter_loss: 0.23983505368232727
train_iter_loss: 0.15713775157928467
train_iter_loss: 0.3565143644809723
train_iter_loss: 0.2382938414812088
train_iter_loss: 0.24122090637683868
train_iter_loss: 0.2913873493671417
train_iter_loss: 0.19794932007789612
train_iter_loss: 0.23268964886665344
train_iter_loss: 0.19244948029518127
train_iter_loss: 0.24824325740337372
train_iter_loss: 0.17214196920394897
train_iter_loss: 0.18738865852355957
train_iter_loss: 0.33337703347206116
train_iter_loss: 0.1328534334897995
train_iter_loss: 0.2934608459472656
train_iter_loss: 0.516360342502594
train_iter_loss: 0.20456109941005707
train_iter_loss: 0.13367457687854767
train_iter_loss: 0.30951976776123047
train_iter_loss: 0.36176809668540955
train_iter_loss: 0.29521265625953674
train_iter_loss: 0.3631112277507782
train_iter_loss: 0.30173003673553467
train_iter_loss: 0.20760348439216614
train_iter_loss: 0.334329217672348
train_iter_loss: 0.29441845417022705
train_iter_loss: 0.29993265867233276
train_iter_loss: 0.38254567980766296
train_iter_loss: 0.2546200752258301
train_iter_loss: 0.3388999402523041
train_iter_loss: 0.11137854307889938
train_iter_loss: 0.19014357030391693
train_iter_loss: 0.28878408670425415
train_iter_loss: 0.35455963015556335
train_iter_loss: 0.34469074010849
train_iter_loss: 0.3849621117115021
train_iter_loss: 0.291116327047348
train loss :0.2675
---------------------
Validation seg loss: 0.36045097687088373 at epoch 637
epoch =    638/  1000, exp = train
train_iter_loss: 0.30586904287338257
train_iter_loss: 0.13580968976020813
train_iter_loss: 0.2704668343067169
train_iter_loss: 0.31572410464286804
train_iter_loss: 0.2888140082359314
train_iter_loss: 0.14459723234176636
train_iter_loss: 0.13746026158332825
train_iter_loss: 0.17751944065093994
train_iter_loss: 0.36686772108078003
train_iter_loss: 0.3385552763938904
train_iter_loss: 0.2606784701347351
train_iter_loss: 0.2203885018825531
train_iter_loss: 0.3062555491924286
train_iter_loss: 0.26502662897109985
train_iter_loss: 0.23231929540634155
train_iter_loss: 0.32012829184532166
train_iter_loss: 0.2193935513496399
train_iter_loss: 0.25929713249206543
train_iter_loss: 0.32599011063575745
train_iter_loss: 0.29214388132095337
train_iter_loss: 0.32799071073532104
train_iter_loss: 0.07053173333406448
train_iter_loss: 0.3319651484489441
train_iter_loss: 0.22288595139980316
train_iter_loss: 0.2818479835987091
train_iter_loss: 0.21969175338745117
train_iter_loss: 0.2203344702720642
train_iter_loss: 0.3614984154701233
train_iter_loss: 0.31005552411079407
train_iter_loss: 0.2235737293958664
train_iter_loss: 0.19932390749454498
train_iter_loss: 0.3573167026042938
train_iter_loss: 0.16978862881660461
train_iter_loss: 0.1525166928768158
train_iter_loss: 0.28955936431884766
train_iter_loss: 0.3360876739025116
train_iter_loss: 0.3797803521156311
train_iter_loss: 0.13554702699184418
train_iter_loss: 0.3201923072338104
train_iter_loss: 0.17583607137203217
train_iter_loss: 0.31238657236099243
train_iter_loss: 0.2963694930076599
train_iter_loss: 0.21679086983203888
train_iter_loss: 0.23505769670009613
train_iter_loss: 0.22303235530853271
train_iter_loss: 0.16925235092639923
train_iter_loss: 0.24586369097232819
train_iter_loss: 0.20563814043998718
train_iter_loss: 0.23421534895896912
train_iter_loss: 0.3482554256916046
train_iter_loss: 0.2960948050022125
train_iter_loss: 0.29902219772338867
train_iter_loss: 0.309854656457901
train_iter_loss: 0.14607857167720795
train_iter_loss: 0.17114439606666565
train_iter_loss: 0.3656812310218811
train_iter_loss: 0.21401043236255646
train_iter_loss: 0.24665330350399017
train_iter_loss: 0.38873010873794556
train_iter_loss: 0.27613723278045654
train_iter_loss: 0.3044659197330475
train_iter_loss: 0.33958664536476135
train_iter_loss: 0.09528716653585434
train_iter_loss: 0.20630131661891937
train_iter_loss: 0.35879483819007874
train_iter_loss: 0.23149164021015167
train_iter_loss: 0.15534915030002594
train_iter_loss: 0.3873010277748108
train_iter_loss: 0.21858054399490356
train_iter_loss: 0.16782698035240173
train_iter_loss: 0.23116078972816467
train_iter_loss: 0.1570354700088501
train_iter_loss: 0.3900124430656433
train_iter_loss: 0.14145660400390625
train_iter_loss: 0.32703539729118347
train_iter_loss: 0.28510284423828125
train_iter_loss: 0.37336310744285583
train_iter_loss: 0.0954621434211731
train_iter_loss: 0.20641876757144928
train_iter_loss: 0.3234623372554779
train_iter_loss: 0.1476147621870041
train_iter_loss: 0.5213878750801086
train_iter_loss: 0.2868281900882721
train_iter_loss: 0.27168989181518555
train_iter_loss: 0.27346062660217285
train_iter_loss: 0.2604333460330963
train_iter_loss: 0.29837894439697266
train_iter_loss: 0.3189235031604767
train_iter_loss: 0.33159902691841125
train_iter_loss: 0.14603421092033386
train_iter_loss: 0.2979937791824341
train_iter_loss: 0.19020847976207733
train_iter_loss: 0.2370585948228836
train_iter_loss: 0.35530218482017517
train_iter_loss: 0.3153591454029083
train_iter_loss: 0.3813989460468292
train_iter_loss: 0.523174524307251
train_iter_loss: 0.2457551211118698
train_iter_loss: 0.28383371233940125
train_iter_loss: 0.1994287371635437
train loss :0.2662
---------------------
Validation seg loss: 0.3596841461675347 at epoch 638
epoch =    639/  1000, exp = train
train_iter_loss: 0.2668021321296692
train_iter_loss: 0.27350062131881714
train_iter_loss: 0.2444223016500473
train_iter_loss: 0.3941704034805298
train_iter_loss: 0.44578292965888977
train_iter_loss: 0.2770061790943146
train_iter_loss: 0.19078870117664337
train_iter_loss: 0.16318592429161072
train_iter_loss: 0.42331650853157043
train_iter_loss: 0.1348133236169815
train_iter_loss: 0.25668516755104065
train_iter_loss: 0.4086635112762451
train_iter_loss: 0.23490825295448303
train_iter_loss: 0.42062291502952576
train_iter_loss: 0.1825130432844162
train_iter_loss: 0.27820464968681335
train_iter_loss: 0.34476813673973083
train_iter_loss: 0.20572307705879211
train_iter_loss: 0.3060982823371887
train_iter_loss: 0.2344382256269455
train_iter_loss: 0.2804531157016754
train_iter_loss: 0.40734535455703735
train_iter_loss: 0.2049657702445984
train_iter_loss: 0.2536786198616028
train_iter_loss: 0.2436172366142273
train_iter_loss: 0.12181247025728226
train_iter_loss: 0.335393488407135
train_iter_loss: 0.38326311111450195
train_iter_loss: 0.28987061977386475
train_iter_loss: 0.4345027506351471
train_iter_loss: 0.3104798495769501
train_iter_loss: 0.29936301708221436
train_iter_loss: 0.30410340428352356
train_iter_loss: 0.2735818326473236
train_iter_loss: 0.1964881271123886
train_iter_loss: 0.3251975178718567
train_iter_loss: 0.2383943498134613
train_iter_loss: 0.19054123759269714
train_iter_loss: 0.16483083367347717
train_iter_loss: 0.12945878505706787
train_iter_loss: 0.31321263313293457
train_iter_loss: 0.2878052294254303
train_iter_loss: 0.20666775107383728
train_iter_loss: 0.2844483554363251
train_iter_loss: 0.28127142786979675
train_iter_loss: 0.11421332508325577
train_iter_loss: 0.38427674770355225
train_iter_loss: 0.32769691944122314
train_iter_loss: 0.2169555425643921
train_iter_loss: 0.10853708535432816
train_iter_loss: 0.20503482222557068
train_iter_loss: 0.13636323809623718
train_iter_loss: 0.2795659899711609
train_iter_loss: 0.2160012573003769
train_iter_loss: 0.2923130393028259
train_iter_loss: 0.14596956968307495
train_iter_loss: 0.13058654963970184
train_iter_loss: 0.2368396371603012
train_iter_loss: 0.2654479742050171
train_iter_loss: 0.26982519030570984
train_iter_loss: 0.2616901099681854
train_iter_loss: 0.35867294669151306
train_iter_loss: 0.2063712179660797
train_iter_loss: 0.2297058403491974
train_iter_loss: 0.1960834115743637
train_iter_loss: 0.45225849747657776
train_iter_loss: 0.1853403002023697
train_iter_loss: 0.19929885864257812
train_iter_loss: 0.22043725848197937
train_iter_loss: 0.4077723026275635
train_iter_loss: 0.2964630424976349
train_iter_loss: 0.3767760097980499
train_iter_loss: 0.3365870714187622
train_iter_loss: 0.23844334483146667
train_iter_loss: 0.1363314688205719
train_iter_loss: 0.3032319247722626
train_iter_loss: 0.12476028501987457
train_iter_loss: 0.3790538012981415
train_iter_loss: 0.24981513619422913
train_iter_loss: 0.38160648941993713
train_iter_loss: 0.22367160022258759
train_iter_loss: 0.21491315960884094
train_iter_loss: 0.27520325779914856
train_iter_loss: 0.12438644468784332
train_iter_loss: 0.12266445904970169
train_iter_loss: 0.35716357827186584
train_iter_loss: 0.11196637898683548
train_iter_loss: 0.2998103201389313
train_iter_loss: 0.22341324388980865
train_iter_loss: 0.31715136766433716
train_iter_loss: 0.2856653928756714
train_iter_loss: 0.25082871317863464
train_iter_loss: 0.23216663300991058
train_iter_loss: 0.3086174428462982
train_iter_loss: 0.25661933422088623
train_iter_loss: 0.23447257280349731
train_iter_loss: 0.30099111795425415
train_iter_loss: 0.1708451360464096
train_iter_loss: 0.2250506430864334
train_iter_loss: 0.2717427611351013
train loss :0.2639
---------------------
Validation seg loss: 0.37184900821204175 at epoch 639
epoch =    640/  1000, exp = train
train_iter_loss: 0.33540043234825134
train_iter_loss: 0.3438938856124878
train_iter_loss: 0.33052924275398254
train_iter_loss: 0.31011754274368286
train_iter_loss: 0.3930818438529968
train_iter_loss: 0.4910453259944916
train_iter_loss: 0.2289837896823883
train_iter_loss: 0.3567925989627838
train_iter_loss: 0.3849300146102905
train_iter_loss: 0.2028878927230835
train_iter_loss: 0.41333702206611633
train_iter_loss: 0.35203275084495544
train_iter_loss: 0.24890145659446716
train_iter_loss: 0.15764063596725464
train_iter_loss: 0.3576626479625702
train_iter_loss: 0.10755836963653564
train_iter_loss: 0.1382611244916916
train_iter_loss: 0.288632333278656
train_iter_loss: 0.22107043862342834
train_iter_loss: 0.2420119196176529
train_iter_loss: 0.3373468220233917
train_iter_loss: 0.14118316769599915
train_iter_loss: 0.09843426197767258
train_iter_loss: 0.19913160800933838
train_iter_loss: 0.4520323574542999
train_iter_loss: 0.31816455721855164
train_iter_loss: 0.22281384468078613
train_iter_loss: 0.24500352144241333
train_iter_loss: 0.2964954078197479
train_iter_loss: 0.19534903764724731
train_iter_loss: 0.1497286856174469
train_iter_loss: 0.1878311187028885
train_iter_loss: 0.22008618712425232
train_iter_loss: 0.3106084167957306
train_iter_loss: 0.2531734108924866
train_iter_loss: 0.2550676763057709
train_iter_loss: 0.36978569626808167
train_iter_loss: 0.24263261258602142
train_iter_loss: 0.1909838765859604
train_iter_loss: 0.1839141845703125
train_iter_loss: 0.26394739747047424
train_iter_loss: 0.2431347668170929
train_iter_loss: 0.2728462517261505
train_iter_loss: 0.10713135451078415
train_iter_loss: 0.2754802703857422
train_iter_loss: 0.31513914465904236
train_iter_loss: 0.21586957573890686
train_iter_loss: 0.19343949854373932
train_iter_loss: 0.1962127536535263
train_iter_loss: 0.16719761490821838
train_iter_loss: 0.3009909391403198
train_iter_loss: 0.3619035482406616
train_iter_loss: 0.2909282445907593
train_iter_loss: 0.20470651984214783
train_iter_loss: 0.11033079028129578
train_iter_loss: 0.2249637395143509
train_iter_loss: 0.2663387060165405
train_iter_loss: 0.1878134161233902
train_iter_loss: 0.18114826083183289
train_iter_loss: 0.23455581068992615
train_iter_loss: 0.20198467373847961
train_iter_loss: 0.4383774399757385
train_iter_loss: 0.4899350702762604
train_iter_loss: 0.30898648500442505
train_iter_loss: 0.19219721853733063
train_iter_loss: 0.3505975604057312
train_iter_loss: 0.2323697954416275
train_iter_loss: 0.3543190062046051
train_iter_loss: 0.2395583540201187
train_iter_loss: 0.14282892644405365
train_iter_loss: 0.22281816601753235
train_iter_loss: 0.2705507278442383
train_iter_loss: 0.30125826597213745
train_iter_loss: 0.10705351084470749
train_iter_loss: 0.35989704728126526
train_iter_loss: 0.36697566509246826
train_iter_loss: 0.13838468492031097
train_iter_loss: 0.23198790848255157
train_iter_loss: 0.2657548487186432
train_iter_loss: 0.21023808419704437
train_iter_loss: 0.22839732468128204
train_iter_loss: 0.2232440710067749
train_iter_loss: 0.17947174608707428
train_iter_loss: 0.2704389691352844
train_iter_loss: 0.31247106194496155
train_iter_loss: 0.2527155578136444
train_iter_loss: 0.3155174255371094
train_iter_loss: 0.20407262444496155
train_iter_loss: 0.13004593551158905
train_iter_loss: 0.11252538859844208
train_iter_loss: 0.4562850892543793
train_iter_loss: 0.1417570561170578
train_iter_loss: 0.3593306243419647
train_iter_loss: 0.30181193351745605
train_iter_loss: 0.23515887558460236
train_iter_loss: 0.4143903851509094
train_iter_loss: 0.19809050858020782
train_iter_loss: 0.2721622586250305
train_iter_loss: 0.2963331341743469
train_iter_loss: 0.16798706352710724
train loss :0.2615
---------------------
Validation seg loss: 0.40083172502224595 at epoch 640
epoch =    641/  1000, exp = train
train_iter_loss: 0.3423187732696533
train_iter_loss: 0.2130654752254486
train_iter_loss: 0.2924501299858093
train_iter_loss: 0.23873528838157654
train_iter_loss: 0.27029237151145935
train_iter_loss: 0.41051173210144043
train_iter_loss: 0.16162440180778503
train_iter_loss: 0.2831404507160187
train_iter_loss: 0.16715238988399506
train_iter_loss: 0.25522616505622864
train_iter_loss: 0.4075987935066223
train_iter_loss: 0.23711051046848297
train_iter_loss: 0.3015233278274536
train_iter_loss: 0.3023357689380646
train_iter_loss: 0.3144329786300659
train_iter_loss: 0.229303777217865
train_iter_loss: 0.2910842001438141
train_iter_loss: 0.21566863358020782
train_iter_loss: 0.2164493203163147
train_iter_loss: 0.24378621578216553
train_iter_loss: 0.3636894226074219
train_iter_loss: 0.3791133463382721
train_iter_loss: 0.15481604635715485
train_iter_loss: 0.2446223795413971
train_iter_loss: 0.3585500717163086
train_iter_loss: 0.42774179577827454
train_iter_loss: 0.23292185366153717
train_iter_loss: 0.1314476579427719
train_iter_loss: 0.18980106711387634
train_iter_loss: 0.24594564735889435
train_iter_loss: 0.17710541188716888
train_iter_loss: 0.26208779215812683
train_iter_loss: 0.26152658462524414
train_iter_loss: 0.1946684569120407
train_iter_loss: 0.2909502685070038
train_iter_loss: 0.3173985481262207
train_iter_loss: 0.631226122379303
train_iter_loss: 0.2207435667514801
train_iter_loss: 0.3487212657928467
train_iter_loss: 0.2760167121887207
train_iter_loss: 0.2771144509315491
train_iter_loss: 0.18693777918815613
train_iter_loss: 0.24748514592647552
train_iter_loss: 0.15599101781845093
train_iter_loss: 0.23681870102882385
train_iter_loss: 0.1038903146982193
train_iter_loss: 0.23746319115161896
train_iter_loss: 0.4417487382888794
train_iter_loss: 0.10452795773744583
train_iter_loss: 0.3075561821460724
train_iter_loss: 0.18761268258094788
train_iter_loss: 0.19993264973163605
train_iter_loss: 0.26528400182724
train_iter_loss: 0.32285749912261963
train_iter_loss: 0.28173166513442993
train_iter_loss: 0.3011145293712616
train_iter_loss: 0.2150123566389084
train_iter_loss: 0.24326008558273315
train_iter_loss: 0.3209042251110077
train_iter_loss: 0.2892412543296814
train_iter_loss: 0.24315014481544495
train_iter_loss: 0.3494343161582947
train_iter_loss: 0.15235725045204163
train_iter_loss: 0.2918846011161804
train_iter_loss: 0.3430555760860443
train_iter_loss: 0.27362585067749023
train_iter_loss: 0.2627890110015869
train_iter_loss: 0.2801482081413269
train_iter_loss: 0.24642221629619598
train_iter_loss: 0.1966797113418579
train_iter_loss: 0.18523162603378296
train_iter_loss: 0.18713729083538055
train_iter_loss: 0.5097420811653137
train_iter_loss: 0.1992400735616684
train_iter_loss: 0.3960198760032654
train_iter_loss: 0.2835167348384857
train_iter_loss: 0.28248706459999084
train_iter_loss: 0.22644078731536865
train_iter_loss: 0.14489232003688812
train_iter_loss: 0.23719513416290283
train_iter_loss: 0.20257683098316193
train_iter_loss: 0.23629160225391388
train_iter_loss: 0.40733012557029724
train_iter_loss: 0.2573286294937134
train_iter_loss: 0.25981539487838745
train_iter_loss: 0.23485665023326874
train_iter_loss: 0.36008715629577637
train_iter_loss: 0.2982214689254761
train_iter_loss: 0.21266911923885345
train_iter_loss: 0.1909390687942505
train_iter_loss: 0.3977969288825989
train_iter_loss: 0.22464339435100555
train_iter_loss: 0.3140931725502014
train_iter_loss: 0.24619051814079285
train_iter_loss: 0.25789937376976013
train_iter_loss: 0.24793069064617157
train_iter_loss: 0.2476119101047516
train_iter_loss: 0.25333288311958313
train_iter_loss: 0.2569171190261841
train_iter_loss: 0.1956353783607483
train loss :0.2688
---------------------
Validation seg loss: 0.3514446653469624 at epoch 641
epoch =    642/  1000, exp = train
train_iter_loss: 0.31826403737068176
train_iter_loss: 0.3396871089935303
train_iter_loss: 0.1482023298740387
train_iter_loss: 0.1628274917602539
train_iter_loss: 0.38007619976997375
train_iter_loss: 0.18218828737735748
train_iter_loss: 0.29965734481811523
train_iter_loss: 0.28032684326171875
train_iter_loss: 0.30666741728782654
train_iter_loss: 0.14361417293548584
train_iter_loss: 0.23531126976013184
train_iter_loss: 0.21012315154075623
train_iter_loss: 0.30556032061576843
train_iter_loss: 0.2439226508140564
train_iter_loss: 0.2014855295419693
train_iter_loss: 0.22653017938137054
train_iter_loss: 0.2586135268211365
train_iter_loss: 0.38500797748565674
train_iter_loss: 0.24264416098594666
train_iter_loss: 0.3024071753025055
train_iter_loss: 0.21495315432548523
train_iter_loss: 0.21842899918556213
train_iter_loss: 0.19426760077476501
train_iter_loss: 0.25796860456466675
train_iter_loss: 0.41268712282180786
train_iter_loss: 0.1853673905134201
train_iter_loss: 0.22472794353961945
train_iter_loss: 0.29438576102256775
train_iter_loss: 0.25875672698020935
train_iter_loss: 0.17733164131641388
train_iter_loss: 0.3051227331161499
train_iter_loss: 0.11510798335075378
train_iter_loss: 0.2585385739803314
train_iter_loss: 0.31151318550109863
train_iter_loss: 0.23277240991592407
train_iter_loss: 0.38971513509750366
train_iter_loss: 0.21428503096103668
train_iter_loss: 0.18152320384979248
train_iter_loss: 0.21040010452270508
train_iter_loss: 0.13909901678562164
train_iter_loss: 0.2843623459339142
train_iter_loss: 0.49551016092300415
train_iter_loss: 0.37977638840675354
train_iter_loss: 0.2847674787044525
train_iter_loss: 0.30382049083709717
train_iter_loss: 0.3674827516078949
train_iter_loss: 0.20986652374267578
train_iter_loss: 0.22407083213329315
train_iter_loss: 0.21203380823135376
train_iter_loss: 0.3758750259876251
train_iter_loss: 0.20883934199810028
train_iter_loss: 0.2832740843296051
train_iter_loss: 0.2616308033466339
train_iter_loss: 0.19057871401309967
train_iter_loss: 0.08409710228443146
train_iter_loss: 0.368559330701828
train_iter_loss: 0.24874384701251984
train_iter_loss: 0.3940044939517975
train_iter_loss: 0.42527464032173157
train_iter_loss: 0.22351717948913574
train_iter_loss: 0.46995261311531067
train_iter_loss: 0.2329670935869217
train_iter_loss: 0.3106524646282196
train_iter_loss: 0.11694995313882828
train_iter_loss: 0.39662691950798035
train_iter_loss: 0.3273925185203552
train_iter_loss: 0.1949618011713028
train_iter_loss: 0.244496688246727
train_iter_loss: 0.36534425616264343
train_iter_loss: 0.23433026671409607
train_iter_loss: 0.07720126956701279
train_iter_loss: 0.23167312145233154
train_iter_loss: 0.27972322702407837
train_iter_loss: 0.3546085059642792
train_iter_loss: 0.27219176292419434
train_iter_loss: 0.23116779327392578
train_iter_loss: 0.4347057044506073
train_iter_loss: 0.2524120807647705
train_iter_loss: 0.3392479419708252
train_iter_loss: 0.21163050830364227
train_iter_loss: 0.2453349232673645
train_iter_loss: 0.472069650888443
train_iter_loss: 0.13658399879932404
train_iter_loss: 0.20450562238693237
train_iter_loss: 0.15813979506492615
train_iter_loss: 0.2799590826034546
train_iter_loss: 0.2912510931491852
train_iter_loss: 0.23371152579784393
train_iter_loss: 0.2817728519439697
train_iter_loss: 0.3228800594806671
train_iter_loss: 0.3029935657978058
train_iter_loss: 0.20837095379829407
train_iter_loss: 0.3476284444332123
train_iter_loss: 0.31370851397514343
train_iter_loss: 0.3034742474555969
train_iter_loss: 0.3344542384147644
train_iter_loss: 0.34693706035614014
train_iter_loss: 0.38886842131614685
train_iter_loss: 0.2685893774032593
train_iter_loss: 0.24326017498970032
train loss :0.2733
---------------------
Validation seg loss: 0.3546197883529975 at epoch 642
epoch =    643/  1000, exp = train
train_iter_loss: 0.24576336145401
train_iter_loss: 0.3199138641357422
train_iter_loss: 0.09153822064399719
train_iter_loss: 0.16170930862426758
train_iter_loss: 0.3741270899772644
train_iter_loss: 0.4353063702583313
train_iter_loss: 0.3236105144023895
train_iter_loss: 0.38812536001205444
train_iter_loss: 0.12837369740009308
train_iter_loss: 0.2657010853290558
train_iter_loss: 0.17121385037899017
train_iter_loss: 0.18764163553714752
train_iter_loss: 0.2582174837589264
train_iter_loss: 0.27502530813217163
train_iter_loss: 0.28042471408843994
train_iter_loss: 0.19607853889465332
train_iter_loss: 0.27225056290626526
train_iter_loss: 0.3980153799057007
train_iter_loss: 0.313544899225235
train_iter_loss: 0.3721875548362732
train_iter_loss: 0.1641969531774521
train_iter_loss: 0.28899502754211426
train_iter_loss: 0.22937750816345215
train_iter_loss: 0.27962982654571533
train_iter_loss: 0.4952313005924225
train_iter_loss: 0.26436346769332886
train_iter_loss: 0.2995641231536865
train_iter_loss: 0.43964579701423645
train_iter_loss: 0.20470954477787018
train_iter_loss: 0.35573551058769226
train_iter_loss: 0.3616138696670532
train_iter_loss: 0.3517438769340515
train_iter_loss: 0.17389142513275146
train_iter_loss: 0.27250081300735474
train_iter_loss: 0.2675148844718933
train_iter_loss: 0.2894628643989563
train_iter_loss: 0.3131777048110962
train_iter_loss: 0.20940440893173218
train_iter_loss: 0.26405787467956543
train_iter_loss: 0.4806964695453644
train_iter_loss: 0.12144969403743744
train_iter_loss: 0.5000367760658264
train_iter_loss: 0.18786180019378662
train_iter_loss: 0.323975533246994
train_iter_loss: 0.26865237951278687
train_iter_loss: 0.21400491893291473
train_iter_loss: 0.29216498136520386
train_iter_loss: 0.3877049684524536
train_iter_loss: 0.2261747419834137
train_iter_loss: 0.4212169945240021
train_iter_loss: 0.17900143563747406
train_iter_loss: 0.042126622051000595
train_iter_loss: 0.15596285462379456
train_iter_loss: 0.18966299295425415
train_iter_loss: 0.2643258273601532
train_iter_loss: 0.18637427687644958
train_iter_loss: 0.19752098619937897
train_iter_loss: 0.2896411716938019
train_iter_loss: 0.26758822798728943
train_iter_loss: 0.31286028027534485
train_iter_loss: 0.3890349268913269
train_iter_loss: 0.12284298241138458
train_iter_loss: 0.33032429218292236
train_iter_loss: 0.25723257660865784
train_iter_loss: 0.19433890283107758
train_iter_loss: 0.2633697986602783
train_iter_loss: 0.274668425321579
train_iter_loss: 0.27622878551483154
train_iter_loss: 0.4556000530719757
train_iter_loss: 0.3291851282119751
train_iter_loss: 0.19838616251945496
train_iter_loss: 0.26567673683166504
train_iter_loss: 0.17186591029167175
train_iter_loss: 0.1741151362657547
train_iter_loss: 0.2741321921348572
train_iter_loss: 0.11759331822395325
train_iter_loss: 0.3388085961341858
train_iter_loss: 0.2710626423358917
train_iter_loss: 0.19705244898796082
train_iter_loss: 0.1394168585538864
train_iter_loss: 0.30767443776130676
train_iter_loss: 0.19481374323368073
train_iter_loss: 0.27973347902297974
train_iter_loss: 0.17783726751804352
train_iter_loss: 0.209702730178833
train_iter_loss: 0.20935995876789093
train_iter_loss: 0.21770842373371124
train_iter_loss: 0.14384248852729797
train_iter_loss: 0.1995088756084442
train_iter_loss: 0.23316115140914917
train_iter_loss: 0.21908535063266754
train_iter_loss: 0.19221201539039612
train_iter_loss: 0.20844866335391998
train_iter_loss: 0.2734855115413666
train_iter_loss: 0.2610130310058594
train_iter_loss: 0.21279308199882507
train_iter_loss: 0.24731485545635223
train_iter_loss: 0.39414939284324646
train_iter_loss: 0.23644335567951202
train_iter_loss: 0.18683107197284698
train loss :0.2641
---------------------
Validation seg loss: 0.3639159140721807 at epoch 643
epoch =    644/  1000, exp = train
train_iter_loss: 0.2257109433412552
train_iter_loss: 0.32914453744888306
train_iter_loss: 0.33575940132141113
train_iter_loss: 0.3365798890590668
train_iter_loss: 0.24070501327514648
train_iter_loss: 0.23358823359012604
train_iter_loss: 0.1991242915391922
train_iter_loss: 0.2948187589645386
train_iter_loss: 0.3302883505821228
train_iter_loss: 0.2595245838165283
train_iter_loss: 0.3943241834640503
train_iter_loss: 0.24702078104019165
train_iter_loss: 0.23577986657619476
train_iter_loss: 0.3391110301017761
train_iter_loss: 0.2797889709472656
train_iter_loss: 0.11523175984621048
train_iter_loss: 0.16869708895683289
train_iter_loss: 0.27095094323158264
train_iter_loss: 0.24233251810073853
train_iter_loss: 0.2708771228790283
train_iter_loss: 0.21451520919799805
train_iter_loss: 0.2620093822479248
train_iter_loss: 0.14087513089179993
train_iter_loss: 0.20735196769237518
train_iter_loss: 0.28067919611930847
train_iter_loss: 0.32855555415153503
train_iter_loss: 0.22208040952682495
train_iter_loss: 0.3763093650341034
train_iter_loss: 0.2607443034648895
train_iter_loss: 0.3807981014251709
train_iter_loss: 0.218300461769104
train_iter_loss: 0.17969247698783875
train_iter_loss: 0.31279319524765015
train_iter_loss: 0.2954413890838623
train_iter_loss: 0.3050854504108429
train_iter_loss: 0.2725873291492462
train_iter_loss: 0.2156180590391159
train_iter_loss: 0.30432429909706116
train_iter_loss: 0.1843353658914566
train_iter_loss: 0.2518816292285919
train_iter_loss: 0.40690916776657104
train_iter_loss: 0.24570374190807343
train_iter_loss: 0.31009164452552795
train_iter_loss: 0.2559445798397064
train_iter_loss: 0.2927517592906952
train_iter_loss: 0.11815816909074783
train_iter_loss: 0.2739853262901306
train_iter_loss: 0.2650672197341919
train_iter_loss: 0.29729917645454407
train_iter_loss: 0.12473705410957336
train_iter_loss: 0.5475821495056152
train_iter_loss: 0.19844558835029602
train_iter_loss: 0.34177425503730774
train_iter_loss: 0.22836600244045258
train_iter_loss: 0.293086439371109
train_iter_loss: 0.3855876624584198
train_iter_loss: 0.28751906752586365
train_iter_loss: 0.1581134796142578
train_iter_loss: 0.34711503982543945
train_iter_loss: 0.27735406160354614
train_iter_loss: 0.31784704327583313
train_iter_loss: 0.28921788930892944
train_iter_loss: 0.40907829999923706
train_iter_loss: 0.266915887594223
train_iter_loss: 0.3365870416164398
train_iter_loss: 0.1820012480020523
train_iter_loss: 0.3212219774723053
train_iter_loss: 0.3385465741157532
train_iter_loss: 0.4427487552165985
train_iter_loss: 0.20061156153678894
train_iter_loss: 0.14809876680374146
train_iter_loss: 0.29491639137268066
train_iter_loss: 0.2192537635564804
train_iter_loss: 0.1410631388425827
train_iter_loss: 0.22529350221157074
train_iter_loss: 0.28483912348747253
train_iter_loss: 0.24951443076133728
train_iter_loss: 0.20945823192596436
train_iter_loss: 0.1797717958688736
train_iter_loss: 0.3060905635356903
train_iter_loss: 0.26535311341285706
train_iter_loss: 0.2546263039112091
train_iter_loss: 0.3685866892337799
train_iter_loss: 0.25870099663734436
train_iter_loss: 0.2079930454492569
train_iter_loss: 0.2976487874984741
train_iter_loss: 0.28658944368362427
train_iter_loss: 0.22678926587104797
train_iter_loss: 0.23756122589111328
train_iter_loss: 0.2983548939228058
train_iter_loss: 0.22916635870933533
train_iter_loss: 0.24458256363868713
train_iter_loss: 0.2910619080066681
train_iter_loss: 0.20035356283187866
train_iter_loss: 0.26693859696388245
train_iter_loss: 0.165629580616951
train_iter_loss: 0.5443792343139648
train_iter_loss: 0.1697562038898468
train_iter_loss: 0.21157026290893555
train_iter_loss: 0.09934461116790771
train loss :0.2697
---------------------
Validation seg loss: 0.3832188735645756 at epoch 644
epoch =    645/  1000, exp = train
train_iter_loss: 0.20512142777442932
train_iter_loss: 0.26381731033325195
train_iter_loss: 0.48444128036499023
train_iter_loss: 0.26202064752578735
train_iter_loss: 0.24034814536571503
train_iter_loss: 0.3001716732978821
train_iter_loss: 0.3576388657093048
train_iter_loss: 0.2911674380302429
train_iter_loss: 0.15554390847682953
train_iter_loss: 0.13258425891399384
train_iter_loss: 0.19993561506271362
train_iter_loss: 0.3099003732204437
train_iter_loss: 0.46146872639656067
train_iter_loss: 0.39131179451942444
train_iter_loss: 0.14081047475337982
train_iter_loss: 0.26992475986480713
train_iter_loss: 0.3686441481113434
train_iter_loss: 0.21524128317832947
train_iter_loss: 0.17766480147838593
train_iter_loss: 0.3750292658805847
train_iter_loss: 0.2690161168575287
train_iter_loss: 0.18283669650554657
train_iter_loss: 0.1140613779425621
train_iter_loss: 0.22357027232646942
train_iter_loss: 0.24039272964000702
train_iter_loss: 0.39100635051727295
train_iter_loss: 0.20768049359321594
train_iter_loss: 0.30071160197257996
train_iter_loss: 0.19877713918685913
train_iter_loss: 0.3166882395744324
train_iter_loss: 0.2391914427280426
train_iter_loss: 0.24974584579467773
train_iter_loss: 0.4732225239276886
train_iter_loss: 0.4163268804550171
train_iter_loss: 0.10308142006397247
train_iter_loss: 0.24022120237350464
train_iter_loss: 0.10087461769580841
train_iter_loss: 0.20193645358085632
train_iter_loss: 0.27338215708732605
train_iter_loss: 0.15661385655403137
train_iter_loss: 0.3933369517326355
train_iter_loss: 0.27800554037094116
train_iter_loss: 0.26862645149230957
train_iter_loss: 0.30113714933395386
train_iter_loss: 0.21411488950252533
train_iter_loss: 0.35187381505966187
train_iter_loss: 0.24946776032447815
train_iter_loss: 0.12231025099754333
train_iter_loss: 0.24076934158802032
train_iter_loss: 0.4156022369861603
train_iter_loss: 0.3469352722167969
train_iter_loss: 0.2789566218852997
train_iter_loss: 0.08920709043741226
train_iter_loss: 0.31044718623161316
train_iter_loss: 0.17706558108329773
train_iter_loss: 0.2629868686199188
train_iter_loss: 0.3538002669811249
train_iter_loss: 0.2898058593273163
train_iter_loss: 0.29449471831321716
train_iter_loss: 0.3390646278858185
train_iter_loss: 0.2838248312473297
train_iter_loss: 0.1467357575893402
train_iter_loss: 0.39317551255226135
train_iter_loss: 0.13185951113700867
train_iter_loss: 0.23867279291152954
train_iter_loss: 0.27066415548324585
train_iter_loss: 0.14181657135486603
train_iter_loss: 0.2600872814655304
train_iter_loss: 0.32501137256622314
train_iter_loss: 0.3355250060558319
train_iter_loss: 0.2808915674686432
train_iter_loss: 0.24516813457012177
train_iter_loss: 0.34472858905792236
train_iter_loss: 0.15407808125019073
train_iter_loss: 0.33735737204551697
train_iter_loss: 0.1720677614212036
train_iter_loss: 0.3381527066230774
train_iter_loss: 0.2923832833766937
train_iter_loss: 0.42830997705459595
train_iter_loss: 0.20320361852645874
train_iter_loss: 0.3283957540988922
train_iter_loss: 0.24336422979831696
train_iter_loss: 0.3154911994934082
train_iter_loss: 0.173423171043396
train_iter_loss: 0.24523282051086426
train_iter_loss: 0.32193446159362793
train_iter_loss: 0.2579694986343384
train_iter_loss: 0.3072153627872467
train_iter_loss: 0.21352729201316833
train_iter_loss: 0.2359151989221573
train_iter_loss: 0.2323317974805832
train_iter_loss: 0.33643901348114014
train_iter_loss: 0.24233189225196838
train_iter_loss: 0.14781872928142548
train_iter_loss: 0.2949368953704834
train_iter_loss: 0.3042061924934387
train_iter_loss: 0.2161695957183838
train_iter_loss: 0.20243395864963531
train_iter_loss: 0.16838686168193817
train_iter_loss: 0.303116112947464
train loss :0.2676
---------------------
Validation seg loss: 0.3453241428536064 at epoch 645
********************
best_val_epoch_loss:  0.3453241428536064
MODEL UPDATED
epoch =    646/  1000, exp = train
train_iter_loss: 0.3158290386199951
train_iter_loss: 0.44541284441947937
train_iter_loss: 0.38930466771125793
train_iter_loss: 0.2976391315460205
train_iter_loss: 0.30090761184692383
train_iter_loss: 0.24003781378269196
train_iter_loss: 0.1567673236131668
train_iter_loss: 0.2702280282974243
train_iter_loss: 0.28094977140426636
train_iter_loss: 0.18120448291301727
train_iter_loss: 0.18532931804656982
train_iter_loss: 0.25740307569503784
train_iter_loss: 0.3174613416194916
train_iter_loss: 0.22070398926734924
train_iter_loss: 0.14300818741321564
train_iter_loss: 0.437967985868454
train_iter_loss: 0.2751615345478058
train_iter_loss: 0.3407251536846161
train_iter_loss: 0.26519155502319336
train_iter_loss: 0.4001246988773346
train_iter_loss: 0.1738426834344864
train_iter_loss: 0.38744398951530457
train_iter_loss: 0.19639408588409424
train_iter_loss: 0.22300471365451813
train_iter_loss: 0.2261936068534851
train_iter_loss: 0.1564399152994156
train_iter_loss: 0.24132488667964935
train_iter_loss: 0.1907796710729599
train_iter_loss: 0.25178003311157227
train_iter_loss: 0.29718253016471863
train_iter_loss: 0.2614540457725525
train_iter_loss: 0.15408314764499664
train_iter_loss: 0.2595468759536743
train_iter_loss: 0.17230814695358276
train_iter_loss: 0.3465369641780853
train_iter_loss: 0.23597927391529083
train_iter_loss: 0.35596171021461487
train_iter_loss: 0.05023187771439552
train_iter_loss: 0.33758556842803955
train_iter_loss: 0.3047485053539276
train_iter_loss: 0.29280778765678406
train_iter_loss: 0.21981807053089142
train_iter_loss: 0.37326478958129883
train_iter_loss: 0.36956849694252014
train_iter_loss: 0.17812232673168182
train_iter_loss: 0.6529572606086731
train_iter_loss: 0.18143142759799957
train_iter_loss: 0.18809811770915985
train_iter_loss: 0.732185423374176
train_iter_loss: 0.43267861008644104
train_iter_loss: 0.2574136257171631
train_iter_loss: 0.22378355264663696
train_iter_loss: 0.13469141721725464
train_iter_loss: 0.22531048953533173
train_iter_loss: 0.227692112326622
train_iter_loss: 0.2387513369321823
train_iter_loss: 0.3235100209712982
train_iter_loss: 0.34179842472076416
train_iter_loss: 0.26397502422332764
train_iter_loss: 0.34800970554351807
train_iter_loss: 0.25092020630836487
train_iter_loss: 0.3380148410797119
train_iter_loss: 0.10849516093730927
train_iter_loss: 0.26314133405685425
train_iter_loss: 0.28493988513946533
train_iter_loss: 0.30007410049438477
train_iter_loss: 0.06791769713163376
train_iter_loss: 0.34426185488700867
train_iter_loss: 0.2334618866443634
train_iter_loss: 0.16379369795322418
train_iter_loss: 0.3248954117298126
train_iter_loss: 0.28097400069236755
train_iter_loss: 0.13348303735256195
train_iter_loss: 0.21079327166080475
train_iter_loss: 0.252537339925766
train_iter_loss: 0.2566182017326355
train_iter_loss: 0.215030238032341
train_iter_loss: 0.31717097759246826
train_iter_loss: 0.31843146681785583
train_iter_loss: 0.28371983766555786
train_iter_loss: 0.2575465738773346
train_iter_loss: 0.24803218245506287
train_iter_loss: 0.39367955923080444
train_iter_loss: 0.15055924654006958
train_iter_loss: 0.458713561296463
train_iter_loss: 0.16532018780708313
train_iter_loss: 0.17595209181308746
train_iter_loss: 0.14104926586151123
train_iter_loss: 0.324134886264801
train_iter_loss: 0.2959093749523163
train_iter_loss: 0.1736406832933426
train_iter_loss: 0.20834395289421082
train_iter_loss: 0.2949473559856415
train_iter_loss: 0.3689142167568207
train_iter_loss: 0.20054879784584045
train_iter_loss: 0.17144893109798431
train_iter_loss: 0.43283239006996155
train_iter_loss: 0.2953838109970093
train_iter_loss: 0.179501011967659
train_iter_loss: 0.38949325680732727
train loss :0.2729
---------------------
Validation seg loss: 0.3547897315320541 at epoch 646
epoch =    647/  1000, exp = train
train_iter_loss: 0.34260502457618713
train_iter_loss: 0.30929210782051086
train_iter_loss: 0.21087564527988434
train_iter_loss: 0.22802965342998505
train_iter_loss: 0.3234749138355255
train_iter_loss: 0.2950724959373474
train_iter_loss: 0.16584789752960205
train_iter_loss: 0.28264734148979187
train_iter_loss: 0.18787476420402527
train_iter_loss: 0.5460924506187439
train_iter_loss: 0.09933512657880783
train_iter_loss: 0.3387354016304016
train_iter_loss: 0.1861569881439209
train_iter_loss: 0.2857072055339813
train_iter_loss: 0.3430310785770416
train_iter_loss: 0.2947097718715668
train_iter_loss: 0.10148252546787262
train_iter_loss: 0.17860707640647888
train_iter_loss: 0.3449481427669525
train_iter_loss: 0.2700791358947754
train_iter_loss: 0.5728235244750977
train_iter_loss: 0.30167320370674133
train_iter_loss: 0.18955445289611816
train_iter_loss: 0.27054563164711
train_iter_loss: 0.14363880455493927
train_iter_loss: 0.2501917779445648
train_iter_loss: 0.4164622128009796
train_iter_loss: 0.16121581196784973
train_iter_loss: 0.2655761241912842
train_iter_loss: 0.2249261736869812
train_iter_loss: 0.21737273037433624
train_iter_loss: 0.22608385980129242
train_iter_loss: 0.2921200096607208
train_iter_loss: 0.3126533031463623
train_iter_loss: 0.20058031380176544
train_iter_loss: 0.3463079333305359
train_iter_loss: 0.4479094445705414
train_iter_loss: 0.2064567655324936
train_iter_loss: 0.30287235975265503
train_iter_loss: 0.268302857875824
train_iter_loss: 0.4298456013202667
train_iter_loss: 0.28099608421325684
train_iter_loss: 0.19306103885173798
train_iter_loss: 0.31483137607574463
train_iter_loss: 0.13461464643478394
train_iter_loss: 0.41825807094573975
train_iter_loss: 0.1907859742641449
train_iter_loss: 0.29235878586769104
train_iter_loss: 0.271239697933197
train_iter_loss: 0.27055102586746216
train_iter_loss: 0.2984306514263153
train_iter_loss: 0.2141267955303192
train_iter_loss: 0.3005456030368805
train_iter_loss: 0.3195658028125763
train_iter_loss: 0.23780854046344757
train_iter_loss: 0.186653733253479
train_iter_loss: 0.18352802097797394
train_iter_loss: 0.29260215163230896
train_iter_loss: 0.31060177087783813
train_iter_loss: 0.3126952648162842
train_iter_loss: 0.39617204666137695
train_iter_loss: 0.2941274046897888
train_iter_loss: 0.2509600520133972
train_iter_loss: 0.2195325642824173
train_iter_loss: 0.23929087817668915
train_iter_loss: 0.22188079357147217
train_iter_loss: 0.2417856603860855
train_iter_loss: 0.32597804069519043
train_iter_loss: 0.21164944767951965
train_iter_loss: 0.2537868320941925
train_iter_loss: 0.34193602204322815
train_iter_loss: 0.26979073882102966
train_iter_loss: 0.3305943012237549
train_iter_loss: 0.1340629905462265
train_iter_loss: 0.25584876537323
train_iter_loss: 0.28893953561782837
train_iter_loss: 0.15061913430690765
train_iter_loss: 0.18270309269428253
train_iter_loss: 0.24387335777282715
train_iter_loss: 0.34325242042541504
train_iter_loss: 0.36803072690963745
train_iter_loss: 0.26905491948127747
train_iter_loss: 0.2116066962480545
train_iter_loss: 0.3178219497203827
train_iter_loss: 0.17764508724212646
train_iter_loss: 0.1623368263244629
train_iter_loss: 0.2471952885389328
train_iter_loss: 0.26471376419067383
train_iter_loss: 0.1467510461807251
train_iter_loss: 0.2777794599533081
train_iter_loss: 0.34668177366256714
train_iter_loss: 0.7225558757781982
train_iter_loss: 0.2902931571006775
train_iter_loss: 0.16059353947639465
train_iter_loss: 0.15410318970680237
train_iter_loss: 0.17745526134967804
train_iter_loss: 0.2369244247674942
train_iter_loss: 0.3637803792953491
train_iter_loss: 0.4473944902420044
train_iter_loss: 0.2023719847202301
train loss :0.2742
---------------------
Validation seg loss: 0.40168561811014164 at epoch 647
epoch =    648/  1000, exp = train
train_iter_loss: 0.2219797968864441
train_iter_loss: 0.22479088604450226
train_iter_loss: 0.2061171680688858
train_iter_loss: 0.18791082501411438
train_iter_loss: 0.28834739327430725
train_iter_loss: 0.30610066652297974
train_iter_loss: 0.28181564807891846
train_iter_loss: 0.2584529519081116
train_iter_loss: 0.3522314429283142
train_iter_loss: 0.19985100626945496
train_iter_loss: 0.175851508975029
train_iter_loss: 0.27540168166160583
train_iter_loss: 0.23570220172405243
train_iter_loss: 0.2977551817893982
train_iter_loss: 0.30177614092826843
train_iter_loss: 0.3212581276893616
train_iter_loss: 0.2165687531232834
train_iter_loss: 0.3336864709854126
train_iter_loss: 0.26988279819488525
train_iter_loss: 0.27406439185142517
train_iter_loss: 0.41789937019348145
train_iter_loss: 0.3808671236038208
train_iter_loss: 0.23827530443668365
train_iter_loss: 0.2255195528268814
train_iter_loss: 0.2626691162586212
train_iter_loss: 0.3539411425590515
train_iter_loss: 0.3374702036380768
train_iter_loss: 0.2398257553577423
train_iter_loss: 0.2585589289665222
train_iter_loss: 0.142093688249588
train_iter_loss: 0.2941475808620453
train_iter_loss: 0.24368394911289215
train_iter_loss: 0.19644658267498016
train_iter_loss: 0.322182297706604
train_iter_loss: 0.14779190719127655
train_iter_loss: 0.12096495926380157
train_iter_loss: 0.2762086093425751
train_iter_loss: 0.3963780701160431
train_iter_loss: 0.21101422607898712
train_iter_loss: 0.25726303458213806
train_iter_loss: 0.37657874822616577
train_iter_loss: 0.35238176584243774
train_iter_loss: 0.2889503836631775
train_iter_loss: 0.24850527942180634
train_iter_loss: 0.2864278554916382
train_iter_loss: 0.2938512861728668
train_iter_loss: 0.28083276748657227
train_iter_loss: 0.11954755336046219
train_iter_loss: 0.22424986958503723
train_iter_loss: 0.3252290189266205
train_iter_loss: 0.2644234895706177
train_iter_loss: 0.11877840757369995
train_iter_loss: 0.41668421030044556
train_iter_loss: 0.2748025357723236
train_iter_loss: 0.41547155380249023
train_iter_loss: 0.21476858854293823
train_iter_loss: 0.28072190284729004
train_iter_loss: 0.21223050355911255
train_iter_loss: 0.23678314685821533
train_iter_loss: 0.12056531757116318
train_iter_loss: 0.34209224581718445
train_iter_loss: 0.19909052550792694
train_iter_loss: 0.20640987157821655
train_iter_loss: 0.19991910457611084
train_iter_loss: 0.30253297090530396
train_iter_loss: 0.3966429829597473
train_iter_loss: 0.3559790849685669
train_iter_loss: 0.18107429146766663
train_iter_loss: 0.15014228224754333
train_iter_loss: 0.39684075117111206
train_iter_loss: 0.2352035492658615
train_iter_loss: 0.26242947578430176
train_iter_loss: 0.06791885197162628
train_iter_loss: 0.31492409110069275
train_iter_loss: 0.331249475479126
train_iter_loss: 0.45103922486305237
train_iter_loss: 0.2969825863838196
train_iter_loss: 0.21612679958343506
train_iter_loss: 0.2198210060596466
train_iter_loss: 0.2493840754032135
train_iter_loss: 0.391709566116333
train_iter_loss: 0.23872078955173492
train_iter_loss: 0.30886080861091614
train_iter_loss: 0.25943252444267273
train_iter_loss: 0.21966788172721863
train_iter_loss: 0.22466570138931274
train_iter_loss: 0.1729777455329895
train_iter_loss: 0.1765221506357193
train_iter_loss: 0.13393813371658325
train_iter_loss: 0.1929006427526474
train_iter_loss: 0.1504887044429779
train_iter_loss: 0.29605138301849365
train_iter_loss: 0.19626878201961517
train_iter_loss: 0.18155963718891144
train_iter_loss: 0.44976022839546204
train_iter_loss: 0.2735813856124878
train_iter_loss: 0.2762529253959656
train_iter_loss: 0.4160519540309906
train_iter_loss: 0.14020131528377533
train_iter_loss: 0.3015158772468567
train loss :0.2656
---------------------
Validation seg loss: 0.3518615403950636 at epoch 648
epoch =    649/  1000, exp = train
train_iter_loss: 0.18491975963115692
train_iter_loss: 0.24895811080932617
train_iter_loss: 0.10636506974697113
train_iter_loss: 0.19257989525794983
train_iter_loss: 0.2739357352256775
train_iter_loss: 0.38947513699531555
train_iter_loss: 0.3055635690689087
train_iter_loss: 0.22338198125362396
train_iter_loss: 0.3546607494354248
train_iter_loss: 0.2925681471824646
train_iter_loss: 0.2974183261394501
train_iter_loss: 0.16891315579414368
train_iter_loss: 0.2818189561367035
train_iter_loss: 0.27550727128982544
train_iter_loss: 0.16423477232456207
train_iter_loss: 0.31677550077438354
train_iter_loss: 0.32392585277557373
train_iter_loss: 0.2742701768875122
train_iter_loss: 0.20915813744068146
train_iter_loss: 0.2795396149158478
train_iter_loss: 0.26440465450286865
train_iter_loss: 0.2771083414554596
train_iter_loss: 0.35963597893714905
train_iter_loss: 0.32958415150642395
train_iter_loss: 0.18510250747203827
train_iter_loss: 0.3354836106300354
train_iter_loss: 0.32953110337257385
train_iter_loss: 0.1422349512577057
train_iter_loss: 0.33951038122177124
train_iter_loss: 0.3660055994987488
train_iter_loss: 0.143708735704422
train_iter_loss: 0.28304824233055115
train_iter_loss: 0.2682703137397766
train_iter_loss: 0.2601757347583771
train_iter_loss: 0.14447623491287231
train_iter_loss: 0.2663480043411255
train_iter_loss: 0.2896178960800171
train_iter_loss: 0.2699245810508728
train_iter_loss: 0.1542455106973648
train_iter_loss: 0.08316875249147415
train_iter_loss: 0.2988918423652649
train_iter_loss: 0.31486600637435913
train_iter_loss: 0.2791520059108734
train_iter_loss: 0.21876709163188934
train_iter_loss: 0.16325338184833527
train_iter_loss: 0.23816494643688202
train_iter_loss: 0.2697196900844574
train_iter_loss: 0.25058817863464355
train_iter_loss: 0.22540158033370972
train_iter_loss: 0.1701446771621704
train_iter_loss: 0.2669985890388489
train_iter_loss: 0.222639262676239
train_iter_loss: 0.1714671552181244
train_iter_loss: 0.30079683661460876
train_iter_loss: 0.5595331192016602
train_iter_loss: 0.15437352657318115
train_iter_loss: 0.32450103759765625
train_iter_loss: 0.3255155384540558
train_iter_loss: 0.16268229484558105
train_iter_loss: 0.1808571070432663
train_iter_loss: 0.19546788930892944
train_iter_loss: 0.1666857749223709
train_iter_loss: 0.2145339846611023
train_iter_loss: 0.3307068943977356
train_iter_loss: 0.13417091965675354
train_iter_loss: 0.36349794268608093
train_iter_loss: 0.18845288455486298
train_iter_loss: 0.2500297725200653
train_iter_loss: 0.32897302508354187
train_iter_loss: 0.3148375153541565
train_iter_loss: 0.23811376094818115
train_iter_loss: 0.28082501888275146
train_iter_loss: 0.18575100600719452
train_iter_loss: 0.3008382320404053
train_iter_loss: 0.316280335187912
train_iter_loss: 0.16436131298542023
train_iter_loss: 0.15052269399166107
train_iter_loss: 0.25508177280426025
train_iter_loss: 0.17995916306972504
train_iter_loss: 0.2683023512363434
train_iter_loss: 0.28536370396614075
train_iter_loss: 0.31414690613746643
train_iter_loss: 0.24881573021411896
train_iter_loss: 0.21821780502796173
train_iter_loss: 0.24077251553535461
train_iter_loss: 0.1862614005804062
train_iter_loss: 0.3366069495677948
train_iter_loss: 0.14091186225414276
train_iter_loss: 0.23288175463676453
train_iter_loss: 0.39193445444107056
train_iter_loss: 0.25502899289131165
train_iter_loss: 0.28260090947151184
train_iter_loss: 0.3170897364616394
train_iter_loss: 0.19510994851589203
train_iter_loss: 0.23650896549224854
train_iter_loss: 0.1365634948015213
train_iter_loss: 0.15121713280677795
train_iter_loss: 0.2254980057477951
train_iter_loss: 0.2159227728843689
train_iter_loss: 0.29864752292633057
train loss :0.2536
---------------------
Validation seg loss: 0.35884285458135157 at epoch 649
epoch =    650/  1000, exp = train
train_iter_loss: 0.16606003046035767
train_iter_loss: 0.2817818522453308
train_iter_loss: 0.3173885643482208
train_iter_loss: 0.36495503783226013
train_iter_loss: 0.2604549527168274
train_iter_loss: 0.3824879825115204
train_iter_loss: 0.2460583746433258
train_iter_loss: 0.19415266811847687
train_iter_loss: 0.31457796692848206
train_iter_loss: 0.3028823733329773
train_iter_loss: 0.33637580275535583
train_iter_loss: 0.33234983682632446
train_iter_loss: 0.2343885600566864
train_iter_loss: 0.16679048538208008
train_iter_loss: 0.41793736815452576
train_iter_loss: 0.20932483673095703
train_iter_loss: 0.22815409302711487
train_iter_loss: 0.2554680109024048
train_iter_loss: 0.2974099814891815
train_iter_loss: 0.28316640853881836
train_iter_loss: 0.20710159838199615
train_iter_loss: 0.22339533269405365
train_iter_loss: 0.30206143856048584
train_iter_loss: 0.26334649324417114
train_iter_loss: 0.39721599221229553
train_iter_loss: 0.28291746973991394
train_iter_loss: 0.4212034046649933
train_iter_loss: 0.3174794316291809
train_iter_loss: 0.13776151835918427
train_iter_loss: 0.056649670004844666
train_iter_loss: 0.23269112408161163
train_iter_loss: 0.1990891844034195
train_iter_loss: 0.23707951605319977
train_iter_loss: 0.30083996057510376
train_iter_loss: 0.2702594995498657
train_iter_loss: 0.24486291408538818
train_iter_loss: 0.27714070677757263
train_iter_loss: 0.1761235147714615
train_iter_loss: 0.24667033553123474
train_iter_loss: 0.6378481388092041
train_iter_loss: 0.1614866852760315
train_iter_loss: 0.3849617838859558
train_iter_loss: 0.27312609553337097
train_iter_loss: 0.27878114581108093
train_iter_loss: 0.28539708256721497
train_iter_loss: 0.25521302223205566
train_iter_loss: 0.316125750541687
train_iter_loss: 0.3559962809085846
train_iter_loss: 0.2170080542564392
train_iter_loss: 0.3154953122138977
train_iter_loss: 0.2571823298931122
train_iter_loss: 0.41885724663734436
train_iter_loss: 0.279792845249176
train_iter_loss: 0.07104696333408356
train_iter_loss: 0.3223104178905487
train_iter_loss: 0.2582603394985199
train_iter_loss: 0.29848021268844604
train_iter_loss: 0.3712047040462494
train_iter_loss: 0.16669262945652008
train_iter_loss: 0.34102100133895874
train_iter_loss: 0.2893240749835968
train_iter_loss: 0.2864531874656677
train_iter_loss: 0.31330785155296326
train_iter_loss: 0.28834328055381775
train_iter_loss: 0.15797346830368042
train_iter_loss: 0.2112705409526825
train_iter_loss: 0.3759832978248596
train_iter_loss: 0.18175777792930603
train_iter_loss: 0.2785244286060333
train_iter_loss: 0.19576461613178253
train_iter_loss: 0.33993521332740784
train_iter_loss: 0.19029751420021057
train_iter_loss: 0.27056676149368286
train_iter_loss: 0.12820293009281158
train_iter_loss: 0.1997305452823639
train_iter_loss: 0.2290331870317459
train_iter_loss: 0.28691190481185913
train_iter_loss: 0.3350616991519928
train_iter_loss: 0.17141610383987427
train_iter_loss: 0.3703981637954712
train_iter_loss: 0.16945388913154602
train_iter_loss: 0.3738766014575958
train_iter_loss: 0.1914055049419403
train_iter_loss: 0.38521263003349304
train_iter_loss: 0.15930919349193573
train_iter_loss: 0.2307741791009903
train_iter_loss: 0.42328599095344543
train_iter_loss: 0.18966548144817352
train_iter_loss: 0.39355993270874023
train_iter_loss: 0.2754319906234741
train_iter_loss: 0.33787691593170166
train_iter_loss: 0.1219334527850151
train_iter_loss: 0.19553795456886292
train_iter_loss: 0.29018110036849976
train_iter_loss: 0.13320446014404297
train_iter_loss: 0.3138920068740845
train_iter_loss: 0.3959476053714752
train_iter_loss: 0.12499082833528519
train_iter_loss: 0.20956437289714813
train_iter_loss: 0.2026088982820511
train loss :0.2712
---------------------
Validation seg loss: 0.3490079796152576 at epoch 650
epoch =    651/  1000, exp = train
train_iter_loss: 0.24399785697460175
train_iter_loss: 0.2605442702770233
train_iter_loss: 0.31740784645080566
train_iter_loss: 0.30556151270866394
train_iter_loss: 0.2547549307346344
train_iter_loss: 0.19819879531860352
train_iter_loss: 0.2551986575126648
train_iter_loss: 0.3924303352832794
train_iter_loss: 0.19529888033866882
train_iter_loss: 0.3913272023200989
train_iter_loss: 0.14566941559314728
train_iter_loss: 0.18321366608142853
train_iter_loss: 0.17401957511901855
train_iter_loss: 0.3924218714237213
train_iter_loss: 0.21448315680027008
train_iter_loss: 0.25016701221466064
train_iter_loss: 0.32843372225761414
train_iter_loss: 0.3017047643661499
train_iter_loss: 0.31004443764686584
train_iter_loss: 0.23057258129119873
train_iter_loss: 0.22787652909755707
train_iter_loss: 0.36082392930984497
train_iter_loss: 0.3069174885749817
train_iter_loss: 0.3331168293952942
train_iter_loss: 0.3326396346092224
train_iter_loss: 0.2804889678955078
train_iter_loss: 0.26706594228744507
train_iter_loss: 0.20764723420143127
train_iter_loss: 0.22780199348926544
train_iter_loss: 0.18272364139556885
train_iter_loss: 0.2627888321876526
train_iter_loss: 0.28938931226730347
train_iter_loss: 0.17853322625160217
train_iter_loss: 0.21672706305980682
train_iter_loss: 0.22096912562847137
train_iter_loss: 0.30470260977745056
train_iter_loss: 0.3503807485103607
train_iter_loss: 0.132351353764534
train_iter_loss: 0.22482137382030487
train_iter_loss: 0.35602903366088867
train_iter_loss: 0.12488919496536255
train_iter_loss: 0.40432634949684143
train_iter_loss: 0.3435458838939667
train_iter_loss: 0.13993529975414276
train_iter_loss: 0.3566213846206665
train_iter_loss: 0.1516771912574768
train_iter_loss: 0.45520949363708496
train_iter_loss: 0.2382248044013977
train_iter_loss: 0.48305898904800415
train_iter_loss: 0.28962448239326477
train_iter_loss: 0.3697871267795563
train_iter_loss: 0.14349104464054108
train_iter_loss: 0.36731863021850586
train_iter_loss: 0.20853616297245026
train_iter_loss: 0.2286234349012375
train_iter_loss: 0.24913442134857178
train_iter_loss: 0.23493172228336334
train_iter_loss: 0.43286147713661194
train_iter_loss: 0.3154382109642029
train_iter_loss: 0.14748157560825348
train_iter_loss: 0.23974551260471344
train_iter_loss: 0.23371337354183197
train_iter_loss: 0.26802146434783936
train_iter_loss: 0.21180842816829681
train_iter_loss: 0.1908043920993805
train_iter_loss: 0.24282009899616241
train_iter_loss: 0.2249695062637329
train_iter_loss: 0.24211108684539795
train_iter_loss: 0.3541466295719147
train_iter_loss: 0.14196793735027313
train_iter_loss: 0.19618554413318634
train_iter_loss: 0.13851690292358398
train_iter_loss: 0.44127243757247925
train_iter_loss: 0.31923961639404297
train_iter_loss: 0.18780715763568878
train_iter_loss: 0.3131384551525116
train_iter_loss: 0.17539261281490326
train_iter_loss: 0.2029576301574707
train_iter_loss: 0.2847292423248291
train_iter_loss: 0.22876715660095215
train_iter_loss: 0.24428197741508484
train_iter_loss: 0.23372307419776917
train_iter_loss: 0.2376052290201187
train_iter_loss: 0.20983751118183136
train_iter_loss: 0.19571997225284576
train_iter_loss: 0.18436014652252197
train_iter_loss: 0.20060335099697113
train_iter_loss: 0.244685098528862
train_iter_loss: 0.3130839765071869
train_iter_loss: 0.171264186501503
train_iter_loss: 0.32871225476264954
train_iter_loss: 0.24499309062957764
train_iter_loss: 0.31167370080947876
train_iter_loss: 0.5188249349594116
train_iter_loss: 0.22858060896396637
train_iter_loss: 0.2787521481513977
train_iter_loss: 0.13691525161266327
train_iter_loss: 0.3671381175518036
train_iter_loss: 0.1290997415781021
train_iter_loss: 0.19166617095470428
train loss :0.2637
---------------------
Validation seg loss: 0.3519702812059308 at epoch 651
epoch =    652/  1000, exp = train
train_iter_loss: 0.4241345524787903
train_iter_loss: 0.10551974177360535
train_iter_loss: 0.2591739594936371
train_iter_loss: 0.24775265157222748
train_iter_loss: 0.1494428813457489
train_iter_loss: 0.4502491354942322
train_iter_loss: 0.2090415358543396
train_iter_loss: 0.195477694272995
train_iter_loss: 0.2500252425670624
train_iter_loss: 0.3071293830871582
train_iter_loss: 0.2580892741680145
train_iter_loss: 0.26295363903045654
train_iter_loss: 0.2241390198469162
train_iter_loss: 0.1713821291923523
train_iter_loss: 0.3270980417728424
train_iter_loss: 0.3373337686061859
train_iter_loss: 0.3041163384914398
train_iter_loss: 0.409674733877182
train_iter_loss: 0.16504313051700592
train_iter_loss: 0.38340625166893005
train_iter_loss: 0.24231192469596863
train_iter_loss: 0.12187158316373825
train_iter_loss: 0.2048908919095993
train_iter_loss: 0.33582639694213867
train_iter_loss: 0.1590488702058792
train_iter_loss: 0.18314914405345917
train_iter_loss: 0.2138146311044693
train_iter_loss: 0.26530227065086365
train_iter_loss: 0.28246769309043884
train_iter_loss: 0.11619847267866135
train_iter_loss: 0.21656940877437592
train_iter_loss: 0.11738242208957672
train_iter_loss: 0.20451952517032623
train_iter_loss: 0.3138803243637085
train_iter_loss: 0.21210628747940063
train_iter_loss: 0.18230845034122467
train_iter_loss: 0.29557621479034424
train_iter_loss: 0.30105334520339966
train_iter_loss: 0.21084171533584595
train_iter_loss: 0.36588409543037415
train_iter_loss: 0.1745714694261551
train_iter_loss: 0.256894052028656
train_iter_loss: 0.32675623893737793
train_iter_loss: 0.3374263048171997
train_iter_loss: 0.35473597049713135
train_iter_loss: 0.2026272714138031
train_iter_loss: 0.2395309954881668
train_iter_loss: 0.19291329383850098
train_iter_loss: 0.42273056507110596
train_iter_loss: 0.32741865515708923
train_iter_loss: 0.24640510976314545
train_iter_loss: 0.22255828976631165
train_iter_loss: 0.2617262005805969
train_iter_loss: 0.2910853326320648
train_iter_loss: 0.46359512209892273
train_iter_loss: 0.28391948342323303
train_iter_loss: 0.3315744996070862
train_iter_loss: 0.26500263810157776
train_iter_loss: 0.3064546287059784
train_iter_loss: 0.3114834725856781
train_iter_loss: 0.2553047239780426
train_iter_loss: 0.29311344027519226
train_iter_loss: 0.2797948122024536
train_iter_loss: 0.3109677731990814
train_iter_loss: 0.2857392430305481
train_iter_loss: 0.27233219146728516
train_iter_loss: 0.26534050703048706
train_iter_loss: 0.2718428373336792
train_iter_loss: 0.20328602194786072
train_iter_loss: 0.22436602413654327
train_iter_loss: 0.25882184505462646
train_iter_loss: 0.08302751183509827
train_iter_loss: 0.35789740085601807
train_iter_loss: 0.15827009081840515
train_iter_loss: 0.27339446544647217
train_iter_loss: 0.352470338344574
train_iter_loss: 0.4293266534805298
train_iter_loss: 0.37519916892051697
train_iter_loss: 0.26816898584365845
train_iter_loss: 0.12079646438360214
train_iter_loss: 0.10171917825937271
train_iter_loss: 0.28080353140830994
train_iter_loss: 0.06890584528446198
train_iter_loss: 0.35716840624809265
train_iter_loss: 0.23517581820487976
train_iter_loss: 0.09733355045318604
train_iter_loss: 0.25520697236061096
train_iter_loss: 0.12952381372451782
train_iter_loss: 0.16563640534877777
train_iter_loss: 0.32380276918411255
train_iter_loss: 0.38135042786598206
train_iter_loss: 0.25241535902023315
train_iter_loss: 0.37566497921943665
train_iter_loss: 0.3247993290424347
train_iter_loss: 0.25276416540145874
train_iter_loss: 0.2449459582567215
train_iter_loss: 0.29495611786842346
train_iter_loss: 0.35078221559524536
train_iter_loss: 0.22842586040496826
train_iter_loss: 0.44318482279777527
train loss :0.2657
---------------------
Validation seg loss: 0.3522381344810128 at epoch 652
epoch =    653/  1000, exp = train
train_iter_loss: 0.14866861701011658
train_iter_loss: 0.18000735342502594
train_iter_loss: 0.22747181355953217
train_iter_loss: 0.20136253535747528
train_iter_loss: 0.24363471567630768
train_iter_loss: 0.1814214289188385
train_iter_loss: 0.26808375120162964
train_iter_loss: 0.32829806208610535
train_iter_loss: 0.2405659705400467
train_iter_loss: 0.294097363948822
train_iter_loss: 0.12445708364248276
train_iter_loss: 0.18173189461231232
train_iter_loss: 0.35100334882736206
train_iter_loss: 0.275083988904953
train_iter_loss: 0.24322599172592163
train_iter_loss: 0.21993690729141235
train_iter_loss: 0.387814462184906
train_iter_loss: 0.2469702959060669
train_iter_loss: 0.14743004739284515
train_iter_loss: 0.26475200057029724
train_iter_loss: 0.09318079799413681
train_iter_loss: 0.3879334032535553
train_iter_loss: 0.20180514454841614
train_iter_loss: 0.19700288772583008
train_iter_loss: 0.3205690383911133
train_iter_loss: 0.34719082713127136
train_iter_loss: 0.23220066726207733
train_iter_loss: 0.47230249643325806
train_iter_loss: 0.37333282828330994
train_iter_loss: 0.13810290396213531
train_iter_loss: 0.16175711154937744
train_iter_loss: 0.19387394189834595
train_iter_loss: 0.22797022759914398
train_iter_loss: 0.19195769727230072
train_iter_loss: 0.4268529713153839
train_iter_loss: 0.28156161308288574
train_iter_loss: 0.257902592420578
train_iter_loss: 0.2262423038482666
train_iter_loss: 0.453232079744339
train_iter_loss: 0.2295072227716446
train_iter_loss: 0.23434045910835266
train_iter_loss: 0.2956770062446594
train_iter_loss: 0.3257772624492645
train_iter_loss: 0.12753421068191528
train_iter_loss: 0.3504306375980377
train_iter_loss: 0.1807255744934082
train_iter_loss: 0.24175629019737244
train_iter_loss: 0.2097090780735016
train_iter_loss: 0.1675768345594406
train_iter_loss: 0.2495398372411728
train_iter_loss: 0.2223319262266159
train_iter_loss: 0.2580887973308563
train_iter_loss: 0.38377806544303894
train_iter_loss: 0.262805312871933
train_iter_loss: 0.29649999737739563
train_iter_loss: 0.21004050970077515
train_iter_loss: 0.37300145626068115
train_iter_loss: 0.2699752151966095
train_iter_loss: 0.30584320425987244
train_iter_loss: 0.2275090366601944
train_iter_loss: 0.20435863733291626
train_iter_loss: 0.3263135850429535
train_iter_loss: 0.3473140299320221
train_iter_loss: 0.20644840598106384
train_iter_loss: 0.06849295645952225
train_iter_loss: 0.19756834208965302
train_iter_loss: 0.2111968845129013
train_iter_loss: 0.4319826066493988
train_iter_loss: 0.262001097202301
train_iter_loss: 0.16556118428707123
train_iter_loss: 0.21902622282505035
train_iter_loss: 0.2905412018299103
train_iter_loss: 0.1917726695537567
train_iter_loss: 0.29841843247413635
train_iter_loss: 0.33365434408187866
train_iter_loss: 0.32262828946113586
train_iter_loss: 0.3732912242412567
train_iter_loss: 0.4453796148300171
train_iter_loss: 0.3404795825481415
train_iter_loss: 0.2364380955696106
train_iter_loss: 0.26723676919937134
train_iter_loss: 0.504075825214386
train_iter_loss: 0.3195490539073944
train_iter_loss: 0.2676602900028229
train_iter_loss: 0.21833066642284393
train_iter_loss: 0.33931005001068115
train_iter_loss: 0.2299412041902542
train_iter_loss: 0.2944776117801666
train_iter_loss: 0.3575071692466736
train_iter_loss: 0.23970571160316467
train_iter_loss: 0.3911636173725128
train_iter_loss: 0.40903550386428833
train_iter_loss: 0.3520269989967346
train_iter_loss: 0.3424474596977234
train_iter_loss: 0.12536750733852386
train_iter_loss: 0.10385914146900177
train_iter_loss: 0.24132825434207916
train_iter_loss: 0.15477944910526276
train_iter_loss: 0.16147944331169128
train_iter_loss: 0.2328699827194214
train loss :0.2665
---------------------
Validation seg loss: 0.36677802729262216 at epoch 653
epoch =    654/  1000, exp = train
train_iter_loss: 0.3256422281265259
train_iter_loss: 0.251266211271286
train_iter_loss: 0.3206310570240021
train_iter_loss: 0.2529122233390808
train_iter_loss: 0.2191886454820633
train_iter_loss: 0.1616561859846115
train_iter_loss: 0.2982253432273865
train_iter_loss: 0.19978591799736023
train_iter_loss: 0.32095903158187866
train_iter_loss: 0.1993332803249359
train_iter_loss: 0.13683205842971802
train_iter_loss: 0.2460283786058426
train_iter_loss: 0.12958505749702454
train_iter_loss: 0.2652912437915802
train_iter_loss: 0.18339812755584717
train_iter_loss: 0.12429410964250565
train_iter_loss: 0.45710447430610657
train_iter_loss: 0.41064146161079407
train_iter_loss: 0.2781372666358948
train_iter_loss: 0.1677969992160797
train_iter_loss: 0.3105808198451996
train_iter_loss: 0.3255567252635956
train_iter_loss: 0.4804026782512665
train_iter_loss: 0.20701250433921814
train_iter_loss: 0.2550438642501831
train_iter_loss: 0.18588261306285858
train_iter_loss: 0.22836735844612122
train_iter_loss: 0.3539879024028778
train_iter_loss: 0.29216399788856506
train_iter_loss: 0.13221058249473572
train_iter_loss: 0.2376280277967453
train_iter_loss: 0.3267606198787689
train_iter_loss: 0.22876782715320587
train_iter_loss: 0.43664029240608215
train_iter_loss: 0.14468105137348175
train_iter_loss: 0.2714921534061432
train_iter_loss: 0.1851274073123932
train_iter_loss: 0.33825773000717163
train_iter_loss: 0.1625560075044632
train_iter_loss: 0.3138948380947113
train_iter_loss: 0.4606640934944153
train_iter_loss: 0.17506642639636993
train_iter_loss: 0.27857106924057007
train_iter_loss: 0.1378258913755417
train_iter_loss: 0.27558156847953796
train_iter_loss: 0.21366269886493683
train_iter_loss: 0.1783190220594406
train_iter_loss: 0.18646685779094696
train_iter_loss: 0.29889121651649475
train_iter_loss: 0.25422242283821106
train_iter_loss: 0.1889282912015915
train_iter_loss: 0.08000869303941727
train_iter_loss: 0.16112419962882996
train_iter_loss: 0.23083356022834778
train_iter_loss: 0.29345113039016724
train_iter_loss: 0.2558830678462982
train_iter_loss: 0.20956625044345856
train_iter_loss: 0.12705379724502563
train_iter_loss: 0.3160676062107086
train_iter_loss: 0.22164759039878845
train_iter_loss: 0.21612803637981415
train_iter_loss: 0.3295430839061737
train_iter_loss: 0.3556053340435028
train_iter_loss: 0.3090001046657562
train_iter_loss: 0.24949109554290771
train_iter_loss: 0.42630526423454285
train_iter_loss: 0.21916845440864563
train_iter_loss: 0.25810369849205017
train_iter_loss: 0.35337257385253906
train_iter_loss: 0.19904275238513947
train_iter_loss: 0.32528120279312134
train_iter_loss: 0.11208686977624893
train_iter_loss: 0.32274362444877625
train_iter_loss: 0.3868428170681
train_iter_loss: 0.3041742146015167
train_iter_loss: 0.31223928928375244
train_iter_loss: 0.34770655632019043
train_iter_loss: 0.2458488494157791
train_iter_loss: 0.2463196963071823
train_iter_loss: 0.22015155851840973
train_iter_loss: 0.3897426128387451
train_iter_loss: 0.29955223202705383
train_iter_loss: 0.30222341418266296
train_iter_loss: 0.2093987911939621
train_iter_loss: 0.3641197085380554
train_iter_loss: 0.20658139884471893
train_iter_loss: 0.34779027104377747
train_iter_loss: 0.25110459327697754
train_iter_loss: 0.10469512641429901
train_iter_loss: 0.22003361582756042
train_iter_loss: 0.18526068329811096
train_iter_loss: 0.22576802968978882
train_iter_loss: 0.3072197735309601
train_iter_loss: 0.33919888734817505
train_iter_loss: 0.2956445813179016
train_iter_loss: 0.13347211480140686
train_iter_loss: 0.38568592071533203
train_iter_loss: 0.33517971634864807
train_iter_loss: 0.3432982861995697
train_iter_loss: 0.3505679666996002
train loss :0.2651
---------------------
Validation seg loss: 0.35295094926577975 at epoch 654
epoch =    655/  1000, exp = train
train_iter_loss: 0.1576521396636963
train_iter_loss: 0.2848415672779083
train_iter_loss: 0.34910717606544495
train_iter_loss: 0.20797045528888702
train_iter_loss: 0.3109080195426941
train_iter_loss: 0.31291458010673523
train_iter_loss: 0.19470001757144928
train_iter_loss: 0.4028073251247406
train_iter_loss: 0.25976166129112244
train_iter_loss: 0.2312060445547104
train_iter_loss: 0.22111985087394714
train_iter_loss: 0.11806164681911469
train_iter_loss: 0.41790422797203064
train_iter_loss: 0.25318291783332825
train_iter_loss: 0.19904641807079315
train_iter_loss: 0.34342607855796814
train_iter_loss: 0.24758082628250122
train_iter_loss: 0.1428557187318802
train_iter_loss: 0.1820371001958847
train_iter_loss: 0.17888659238815308
train_iter_loss: 0.2937077581882477
train_iter_loss: 0.33694735169410706
train_iter_loss: 0.19288370013237
train_iter_loss: 0.31059566140174866
train_iter_loss: 0.2267467975616455
train_iter_loss: 0.12752282619476318
train_iter_loss: 0.24700960516929626
train_iter_loss: 0.2764110863208771
train_iter_loss: 0.5247488021850586
train_iter_loss: 0.38348257541656494
train_iter_loss: 0.2681775391101837
train_iter_loss: 0.2915307879447937
train_iter_loss: 0.2946791350841522
train_iter_loss: 0.38660410046577454
train_iter_loss: 0.33711904287338257
train_iter_loss: 0.28642186522483826
train_iter_loss: 0.15575218200683594
train_iter_loss: 0.3758530914783478
train_iter_loss: 0.3767477869987488
train_iter_loss: 0.06548915803432465
train_iter_loss: 0.3657718896865845
train_iter_loss: 0.24529936909675598
train_iter_loss: 0.29127568006515503
train_iter_loss: 0.23868368566036224
train_iter_loss: 0.32340291142463684
train_iter_loss: 0.3668069541454315
train_iter_loss: 0.18994098901748657
train_iter_loss: 0.21420644223690033
train_iter_loss: 0.18463577330112457
train_iter_loss: 0.36938905715942383
train_iter_loss: 0.18100027740001678
train_iter_loss: 0.3114323318004608
train_iter_loss: 0.27090778946876526
train_iter_loss: 0.3221740424633026
train_iter_loss: 0.2587636411190033
train_iter_loss: 0.17479290068149567
train_iter_loss: 0.325417160987854
train_iter_loss: 0.3727107644081116
train_iter_loss: 0.20374925434589386
train_iter_loss: 0.20804999768733978
train_iter_loss: 0.3256053328514099
train_iter_loss: 0.2012299746274948
train_iter_loss: 0.3017207086086273
train_iter_loss: 0.3279770612716675
train_iter_loss: 0.2546626031398773
train_iter_loss: 0.1625874638557434
train_iter_loss: 0.354575514793396
train_iter_loss: 0.33142268657684326
train_iter_loss: 0.2900508940219879
train_iter_loss: 0.1320161670446396
train_iter_loss: 0.34053918719291687
train_iter_loss: 0.31787803769111633
train_iter_loss: 0.19035457074642181
train_iter_loss: 0.3133493959903717
train_iter_loss: 0.3767293691635132
train_iter_loss: 0.19534781575202942
train_iter_loss: 0.30829498171806335
train_iter_loss: 0.08252670615911484
train_iter_loss: 0.3204798400402069
train_iter_loss: 0.29711756110191345
train_iter_loss: 0.30360352993011475
train_iter_loss: 0.34112709760665894
train_iter_loss: 0.21389268338680267
train_iter_loss: 0.3046877682209015
train_iter_loss: 0.3003518879413605
train_iter_loss: 0.19916921854019165
train_iter_loss: 0.2519863247871399
train_iter_loss: 0.19377073645591736
train_iter_loss: 0.28841227293014526
train_iter_loss: 0.26745542883872986
train_iter_loss: 0.2631603479385376
train_iter_loss: 0.387322336435318
train_iter_loss: 0.2867845296859741
train_iter_loss: 0.24968834221363068
train_iter_loss: 0.1590208262205124
train_iter_loss: 0.14894330501556396
train_iter_loss: 0.2598116099834442
train_iter_loss: 0.1437913030385971
train_iter_loss: 0.2581345736980438
train_iter_loss: 0.36389482021331787
train loss :0.2707
---------------------
Validation seg loss: 0.3990190699960123 at epoch 655
epoch =    656/  1000, exp = train
train_iter_loss: 0.2022022306919098
train_iter_loss: 0.22162801027297974
train_iter_loss: 0.16472332179546356
train_iter_loss: 0.3598477840423584
train_iter_loss: 0.33869704604148865
train_iter_loss: 0.3655385673046112
train_iter_loss: 0.22563134133815765
train_iter_loss: 0.3137534558773041
train_iter_loss: 0.30146273970603943
train_iter_loss: 0.15294404327869415
train_iter_loss: 0.2950783371925354
train_iter_loss: 0.18078599870204926
train_iter_loss: 0.3359280228614807
train_iter_loss: 0.30249640345573425
train_iter_loss: 0.3102332055568695
train_iter_loss: 0.32586947083473206
train_iter_loss: 0.1813451498746872
train_iter_loss: 0.03699139878153801
train_iter_loss: 0.3183499276638031
train_iter_loss: 0.17227955162525177
train_iter_loss: 0.2467615157365799
train_iter_loss: 0.11532250046730042
train_iter_loss: 0.12170980125665665
train_iter_loss: 0.23197731375694275
train_iter_loss: 0.21877038478851318
train_iter_loss: 0.37441423535346985
train_iter_loss: 0.24174360930919647
train_iter_loss: 0.5269202589988708
train_iter_loss: 0.316506028175354
train_iter_loss: 0.2445221245288849
train_iter_loss: 0.2500321567058563
train_iter_loss: 0.4072776436805725
train_iter_loss: 0.09247966855764389
train_iter_loss: 0.12133156508207321
train_iter_loss: 0.18077358603477478
train_iter_loss: 0.1827223151922226
train_iter_loss: 0.17257273197174072
train_iter_loss: 0.34797945618629456
train_iter_loss: 0.17551489174365997
train_iter_loss: 0.14921477437019348
train_iter_loss: 0.15135301649570465
train_iter_loss: 0.3423997759819031
train_iter_loss: 0.32771870493888855
train_iter_loss: 0.3139491379261017
train_iter_loss: 0.306887686252594
train_iter_loss: 0.1793346405029297
train_iter_loss: 0.2867859899997711
train_iter_loss: 0.05866099148988724
train_iter_loss: 0.3540663421154022
train_iter_loss: 0.4770521819591522
train_iter_loss: 0.20713573694229126
train_iter_loss: 0.21072469651699066
train_iter_loss: 0.4340464174747467
train_iter_loss: 0.4095223546028137
train_iter_loss: 0.29443639516830444
train_iter_loss: 0.20592062175273895
train_iter_loss: 0.4264310598373413
train_iter_loss: 0.2670213282108307
train_iter_loss: 0.18269053101539612
train_iter_loss: 0.0484357625246048
train_iter_loss: 0.26547971367836
train_iter_loss: 0.2480403333902359
train_iter_loss: 0.24945878982543945
train_iter_loss: 0.13165593147277832
train_iter_loss: 0.3868412673473358
train_iter_loss: 0.24419289827346802
train_iter_loss: 0.3499700129032135
train_iter_loss: 0.27032893896102905
train_iter_loss: 0.14698418974876404
train_iter_loss: 0.2043684720993042
train_iter_loss: 0.12927943468093872
train_iter_loss: 0.34551092982292175
train_iter_loss: 0.22919858992099762
train_iter_loss: 0.2992623746395111
train_iter_loss: 0.22071436047554016
train_iter_loss: 0.2787301540374756
train_iter_loss: 0.181052565574646
train_iter_loss: 0.3821386396884918
train_iter_loss: 0.22028441727161407
train_iter_loss: 0.3134307563304901
train_iter_loss: 0.45393887162208557
train_iter_loss: 0.2504933774471283
train_iter_loss: 0.15336479246616364
train_iter_loss: 0.37891581654548645
train_iter_loss: 0.15268298983573914
train_iter_loss: 0.23301063477993011
train_iter_loss: 0.23512282967567444
train_iter_loss: 0.2786851227283478
train_iter_loss: 0.2677009701728821
train_iter_loss: 0.2689705491065979
train_iter_loss: 0.29646503925323486
train_iter_loss: 0.16628706455230713
train_iter_loss: 0.1880967766046524
train_iter_loss: 0.10404542833566666
train_iter_loss: 0.25542041659355164
train_iter_loss: 0.4291034936904907
train_iter_loss: 0.26456785202026367
train_iter_loss: 0.25442683696746826
train_iter_loss: 0.3533410131931305
train_iter_loss: 0.4789245128631592
train loss :0.2614
---------------------
Validation seg loss: 0.3839305173298168 at epoch 656
epoch =    657/  1000, exp = train
train_iter_loss: 0.26961585879325867
train_iter_loss: 0.15700942277908325
train_iter_loss: 0.3861449956893921
train_iter_loss: 0.2522260546684265
train_iter_loss: 0.20575061440467834
train_iter_loss: 0.19089354574680328
train_iter_loss: 0.15354162454605103
train_iter_loss: 0.3088071644306183
train_iter_loss: 0.37962284684181213
train_iter_loss: 0.44637173414230347
train_iter_loss: 0.323886901140213
train_iter_loss: 0.2289779782295227
train_iter_loss: 0.16715121269226074
train_iter_loss: 0.29574885964393616
train_iter_loss: 0.3398059010505676
train_iter_loss: 0.16884443163871765
train_iter_loss: 0.2216212898492813
train_iter_loss: 0.10719112306833267
train_iter_loss: 0.16321271657943726
train_iter_loss: 0.39748910069465637
train_iter_loss: 0.393852174282074
train_iter_loss: 0.23492677509784698
train_iter_loss: 0.28389325737953186
train_iter_loss: 0.23585036396980286
train_iter_loss: 0.1543973833322525
train_iter_loss: 0.20746976137161255
train_iter_loss: 0.20011262595653534
train_iter_loss: 0.2985400855541229
train_iter_loss: 0.30847278237342834
train_iter_loss: 0.16686739027500153
train_iter_loss: 0.2995322644710541
train_iter_loss: 0.3465365171432495
train_iter_loss: 0.25934287905693054
train_iter_loss: 0.15762853622436523
train_iter_loss: 0.25201600790023804
train_iter_loss: 0.22433121502399445
train_iter_loss: 0.12334803491830826
train_iter_loss: 0.24768754839897156
train_iter_loss: 0.34978652000427246
train_iter_loss: 0.23099815845489502
train_iter_loss: 0.28417277336120605
train_iter_loss: 0.2357834130525589
train_iter_loss: 0.3409982919692993
train_iter_loss: 0.20499709248542786
train_iter_loss: 0.28469783067703247
train_iter_loss: 0.2741016745567322
train_iter_loss: 0.4614511728286743
train_iter_loss: 0.3411969542503357
train_iter_loss: 0.3460306227207184
train_iter_loss: 0.26491814851760864
train_iter_loss: 0.31159380078315735
train_iter_loss: 0.17998671531677246
train_iter_loss: 0.275203675031662
train_iter_loss: 0.44883307814598083
train_iter_loss: 0.22546350955963135
train_iter_loss: 0.17223110795021057
train_iter_loss: 0.10348762571811676
train_iter_loss: 0.21286745369434357
train_iter_loss: 0.32494840025901794
train_iter_loss: 0.25249016284942627
train_iter_loss: 0.3105774521827698
train_iter_loss: 0.29713550209999084
train_iter_loss: 0.33971256017684937
train_iter_loss: 0.3212643265724182
train_iter_loss: 0.23063382506370544
train_iter_loss: 0.31517091393470764
train_iter_loss: 0.19437247514724731
train_iter_loss: 0.30996939539909363
train_iter_loss: 0.34022825956344604
train_iter_loss: 0.7195558547973633
train_iter_loss: 0.06635208427906036
train_iter_loss: 0.26840776205062866
train_iter_loss: 0.24093367159366608
train_iter_loss: 0.12894073128700256
train_iter_loss: 0.23774999380111694
train_iter_loss: 0.3735879063606262
train_iter_loss: 0.44238293170928955
train_iter_loss: 0.2566532790660858
train_iter_loss: 0.28588607907295227
train_iter_loss: 0.20267744362354279
train_iter_loss: 0.2401401847600937
train_iter_loss: 0.2481050193309784
train_iter_loss: 0.17484763264656067
train_iter_loss: 0.36500293016433716
train_iter_loss: 0.2747228443622589
train_iter_loss: 0.1907341480255127
train_iter_loss: 0.3946501612663269
train_iter_loss: 0.3015355169773102
train_iter_loss: 0.23322437703609467
train_iter_loss: 0.2525748908519745
train_iter_loss: 0.19101594388484955
train_iter_loss: 0.1323680281639099
train_iter_loss: 0.21812285482883453
train_iter_loss: 0.20894013345241547
train_iter_loss: 0.0967315211892128
train_iter_loss: 0.34171417355537415
train_iter_loss: 0.41789019107818604
train_iter_loss: 0.2921625077724457
train_iter_loss: 0.28764456510543823
train_iter_loss: 0.3003711998462677
train loss :0.2699
---------------------
Validation seg loss: 0.35690267234249917 at epoch 657
epoch =    658/  1000, exp = train
train_iter_loss: 0.2712608575820923
train_iter_loss: 0.2737092673778534
train_iter_loss: 0.19159631431102753
train_iter_loss: 0.0670304223895073
train_iter_loss: 0.23935824632644653
train_iter_loss: 0.10736455768346786
train_iter_loss: 0.2844194173812866
train_iter_loss: 0.2042652815580368
train_iter_loss: 0.31647488474845886
train_iter_loss: 0.4751807153224945
train_iter_loss: 0.07421594858169556
train_iter_loss: 0.19409392774105072
train_iter_loss: 0.31168654561042786
train_iter_loss: 0.31398701667785645
train_iter_loss: 0.30437377095222473
train_iter_loss: 0.4506034851074219
train_iter_loss: 0.26741135120391846
train_iter_loss: 0.13466504216194153
train_iter_loss: 0.21998156607151031
train_iter_loss: 0.1950855851173401
train_iter_loss: 0.4449518620967865
train_iter_loss: 0.3129338026046753
train_iter_loss: 0.3216487169265747
train_iter_loss: 0.2820712924003601
train_iter_loss: 0.17618869245052338
train_iter_loss: 0.3578763008117676
train_iter_loss: 0.16155622899532318
train_iter_loss: 0.48408228158950806
train_iter_loss: 0.09841702878475189
train_iter_loss: 0.2581008970737457
train_iter_loss: 0.40981587767601013
train_iter_loss: 0.21020421385765076
train_iter_loss: 0.22457094490528107
train_iter_loss: 0.11369836330413818
train_iter_loss: 0.24066981673240662
train_iter_loss: 0.11655449122190475
train_iter_loss: 0.2836219072341919
train_iter_loss: 0.3251052498817444
train_iter_loss: 0.20255541801452637
train_iter_loss: 0.32597681879997253
train_iter_loss: 0.2204892486333847
train_iter_loss: 0.20781327784061432
train_iter_loss: 0.25884079933166504
train_iter_loss: 0.3760318160057068
train_iter_loss: 0.2204686850309372
train_iter_loss: 0.25738325715065
train_iter_loss: 0.14810550212860107
train_iter_loss: 0.6340805292129517
train_iter_loss: 0.1383628100156784
train_iter_loss: 0.1694590002298355
train_iter_loss: 0.2882857024669647
train_iter_loss: 0.14991582930088043
train_iter_loss: 0.30095890164375305
train_iter_loss: 0.2890659272670746
train_iter_loss: 0.36359477043151855
train_iter_loss: 0.2946637272834778
train_iter_loss: 0.2090321034193039
train_iter_loss: 0.17419223487377167
train_iter_loss: 0.3357817530632019
train_iter_loss: 0.1860160529613495
train_iter_loss: 0.17517545819282532
train_iter_loss: 0.2162127047777176
train_iter_loss: 0.31921836733818054
train_iter_loss: 0.24965587258338928
train_iter_loss: 0.1873975396156311
train_iter_loss: 0.18173345923423767
train_iter_loss: 0.32552093267440796
train_iter_loss: 0.37908124923706055
train_iter_loss: 0.3266879618167877
train_iter_loss: 0.373024046421051
train_iter_loss: 0.21020524203777313
train_iter_loss: 0.2814379036426544
train_iter_loss: 0.4600549340248108
train_iter_loss: 0.22272717952728271
train_iter_loss: 0.34438470005989075
train_iter_loss: 0.3181591331958771
train_iter_loss: 0.16130059957504272
train_iter_loss: 0.2420782595872879
train_iter_loss: 0.2695092260837555
train_iter_loss: 0.38595515489578247
train_iter_loss: 0.3064524531364441
train_iter_loss: 0.21873779594898224
train_iter_loss: 0.20160123705863953
train_iter_loss: 0.2999194264411926
train_iter_loss: 0.3364194929599762
train_iter_loss: 0.13755753636360168
train_iter_loss: 0.15386033058166504
train_iter_loss: 0.32173049449920654
train_iter_loss: 0.23943093419075012
train_iter_loss: 0.4407973289489746
train_iter_loss: 0.269752562046051
train_iter_loss: 0.25941601395606995
train_iter_loss: 0.2456819862127304
train_iter_loss: 0.24911044538021088
train_iter_loss: 0.3590972423553467
train_iter_loss: 0.19799858331680298
train_iter_loss: 0.2623830735683441
train_iter_loss: 0.12883688509464264
train_iter_loss: 0.32075828313827515
train_iter_loss: 0.31464192271232605
train loss :0.2663
---------------------
Validation seg loss: 0.35106187058120686 at epoch 658
epoch =    659/  1000, exp = train
train_iter_loss: 0.3031149208545685
train_iter_loss: 0.30131813883781433
train_iter_loss: 0.09633856266736984
train_iter_loss: 0.27755025029182434
train_iter_loss: 0.2874903380870819
train_iter_loss: 0.31276655197143555
train_iter_loss: 0.28770962357521057
train_iter_loss: 0.259907603263855
train_iter_loss: 0.15786467492580414
train_iter_loss: 0.28110527992248535
train_iter_loss: 0.2911362946033478
train_iter_loss: 0.32132652401924133
train_iter_loss: 0.1895400583744049
train_iter_loss: 0.17649416625499725
train_iter_loss: 0.28540682792663574
train_iter_loss: 0.24856030941009521
train_iter_loss: 0.26732856035232544
train_iter_loss: 0.3608663082122803
train_iter_loss: 0.10349131375551224
train_iter_loss: 0.26430442929267883
train_iter_loss: 0.3207160532474518
train_iter_loss: 0.29683157801628113
train_iter_loss: 0.34273576736450195
train_iter_loss: 0.23848378658294678
train_iter_loss: 0.37996503710746765
train_iter_loss: 0.2674808204174042
train_iter_loss: 0.0920652523636818
train_iter_loss: 0.27054378390312195
train_iter_loss: 0.3043617904186249
train_iter_loss: 0.18698179721832275
train_iter_loss: 0.23819968104362488
train_iter_loss: 0.275582492351532
train_iter_loss: 0.3100743889808655
train_iter_loss: 0.35769349336624146
train_iter_loss: 0.08977002650499344
train_iter_loss: 0.39709538221359253
train_iter_loss: 0.2538340091705322
train_iter_loss: 0.4225974380970001
train_iter_loss: 0.2548438310623169
train_iter_loss: 0.2628818452358246
train_iter_loss: 0.15412424504756927
train_iter_loss: 0.4020434319972992
train_iter_loss: 0.24882714450359344
train_iter_loss: 0.3096543252468109
train_iter_loss: 0.2279217690229416
train_iter_loss: 0.25799983739852905
train_iter_loss: 0.37974175810813904
train_iter_loss: 0.2219613492488861
train_iter_loss: 0.1438990980386734
train_iter_loss: 0.25604093074798584
train_iter_loss: 0.19733262062072754
train_iter_loss: 0.22454869747161865
train_iter_loss: 0.21917356550693512
train_iter_loss: 0.19265100359916687
train_iter_loss: 0.22120435535907745
train_iter_loss: 0.22848807275295258
train_iter_loss: 0.31228330731391907
train_iter_loss: 0.049290236085653305
train_iter_loss: 0.21352633833885193
train_iter_loss: 0.21167290210723877
train_iter_loss: 0.2399556040763855
train_iter_loss: 0.42894405126571655
train_iter_loss: 0.29879435896873474
train_iter_loss: 0.18476463854312897
train_iter_loss: 0.4189223051071167
train_iter_loss: 0.2906482517719269
train_iter_loss: 0.1519266963005066
train_iter_loss: 0.2681070566177368
train_iter_loss: 0.32562002539634705
train_iter_loss: 0.257661372423172
train_iter_loss: 0.3170815408229828
train_iter_loss: 0.33093875646591187
train_iter_loss: 0.4328227639198303
train_iter_loss: 0.30000147223472595
train_iter_loss: 0.10551563650369644
train_iter_loss: 0.15900316834449768
train_iter_loss: 0.20316211879253387
train_iter_loss: 0.1809580773115158
train_iter_loss: 0.24933107197284698
train_iter_loss: 0.1802489310503006
train_iter_loss: 0.40717336535453796
train_iter_loss: 0.2611302435398102
train_iter_loss: 0.32699212431907654
train_iter_loss: 0.4192059636116028
train_iter_loss: 0.23759150505065918
train_iter_loss: 0.19610005617141724
train_iter_loss: 0.3457329571247101
train_iter_loss: 0.21596533060073853
train_iter_loss: 0.2350843995809555
train_iter_loss: 0.28507450222969055
train_iter_loss: 0.23986397683620453
train_iter_loss: 0.19852304458618164
train_iter_loss: 0.09650886803865433
train_iter_loss: 0.3052024841308594
train_iter_loss: 0.28649720549583435
train_iter_loss: 0.26838281750679016
train_iter_loss: 0.2758413255214691
train_iter_loss: 0.21534734964370728
train_iter_loss: 0.3032318651676178
train_iter_loss: 0.21534274518489838
train loss :0.2623
---------------------
Validation seg loss: 0.34233008356729766 at epoch 659
********************
best_val_epoch_loss:  0.34233008356729766
MODEL UPDATED
epoch =    660/  1000, exp = train
train_iter_loss: 0.10605917870998383
train_iter_loss: 0.21421673893928528
train_iter_loss: 0.2295995056629181
train_iter_loss: 0.29280412197113037
train_iter_loss: 0.3114776611328125
train_iter_loss: 0.4830651879310608
train_iter_loss: 0.26111650466918945
train_iter_loss: 0.1797337830066681
train_iter_loss: 0.3624723553657532
train_iter_loss: 0.24252724647521973
train_iter_loss: 0.33291923999786377
train_iter_loss: 0.3034611642360687
train_iter_loss: 0.10596910864114761
train_iter_loss: 0.3216513395309448
train_iter_loss: 0.4051448106765747
train_iter_loss: 0.14719653129577637
train_iter_loss: 0.30010345578193665
train_iter_loss: 0.4786723256111145
train_iter_loss: 0.2637801170349121
train_iter_loss: 0.3726186752319336
train_iter_loss: 0.149311363697052
train_iter_loss: 0.2948903739452362
train_iter_loss: 0.2674580216407776
train_iter_loss: 0.2819521725177765
train_iter_loss: 0.25816377997398376
train_iter_loss: 0.48313599824905396
train_iter_loss: 0.31608685851097107
train_iter_loss: 0.4904446601867676
train_iter_loss: 0.2726433575153351
train_iter_loss: 0.21257726848125458
train_iter_loss: 0.33077090978622437
train_iter_loss: 0.24036605656147003
train_iter_loss: 0.21241489052772522
train_iter_loss: 0.11584041267633438
train_iter_loss: 0.1608608514070511
train_iter_loss: 0.23195511102676392
train_iter_loss: 0.28232768177986145
train_iter_loss: 0.10665487498044968
train_iter_loss: 0.33435508608818054
train_iter_loss: 0.3661262094974518
train_iter_loss: 0.3003136217594147
train_iter_loss: 0.29140952229499817
train_iter_loss: 0.1938837170600891
train_iter_loss: 0.4265666604042053
train_iter_loss: 0.23032331466674805
train_iter_loss: 0.33170852065086365
train_iter_loss: 0.14388978481292725
train_iter_loss: 0.30351394414901733
train_iter_loss: 0.27995067834854126
train_iter_loss: 0.3928995132446289
train_iter_loss: 0.30995455384254456
train_iter_loss: 0.2131890058517456
train_iter_loss: 0.27076083421707153
train_iter_loss: 0.262085497379303
train_iter_loss: 0.17509576678276062
train_iter_loss: 0.20337174832820892
train_iter_loss: 0.3420543670654297
train_iter_loss: 0.21489059925079346
train_iter_loss: 0.13533824682235718
train_iter_loss: 0.13429082930088043
train_iter_loss: 0.24881692230701447
train_iter_loss: 0.2229103147983551
train_iter_loss: 0.2932174801826477
train_iter_loss: 0.2765118479728699
train_iter_loss: 0.2162877470254898
train_iter_loss: 0.22009018063545227
train_iter_loss: 0.3037368357181549
train_iter_loss: 0.42697834968566895
train_iter_loss: 0.32050108909606934
train_iter_loss: 0.26459234952926636
train_iter_loss: 0.1357230246067047
train_iter_loss: 0.11635632812976837
train_iter_loss: 0.2183718979358673
train_iter_loss: 0.25403109192848206
train_iter_loss: 0.2219742089509964
train_iter_loss: 0.17495931684970856
train_iter_loss: 0.3607504069805145
train_iter_loss: 0.3923441767692566
train_iter_loss: 0.21306021511554718
train_iter_loss: 0.21368910372257233
train_iter_loss: 0.2885788083076477
train_iter_loss: 0.3080046474933624
train_iter_loss: 0.21919499337673187
train_iter_loss: 0.3612480163574219
train_iter_loss: 0.2884818911552429
train_iter_loss: 0.23016853630542755
train_iter_loss: 0.31148406863212585
train_iter_loss: 0.32292312383651733
train_iter_loss: 0.13216464221477509
train_iter_loss: 0.15340009331703186
train_iter_loss: 0.1388801783323288
train_iter_loss: 0.295855849981308
train_iter_loss: 0.18631599843502045
train_iter_loss: 0.3897460699081421
train_iter_loss: 0.27314305305480957
train_iter_loss: 0.20189113914966583
train_iter_loss: 0.2536425292491913
train_iter_loss: 0.28430798649787903
train_iter_loss: 0.3382126986980438
train_iter_loss: 0.2972748279571533
train loss :0.2691
---------------------
Validation seg loss: 0.3705933898037194 at epoch 660
epoch =    661/  1000, exp = train
train_iter_loss: 0.18579812347888947
train_iter_loss: 0.1565704047679901
train_iter_loss: 0.1913297176361084
train_iter_loss: 0.22933954000473022
train_iter_loss: 0.16754615306854248
train_iter_loss: 0.3022535741329193
train_iter_loss: 0.12751153111457825
train_iter_loss: 0.25879326462745667
train_iter_loss: 0.24985167384147644
train_iter_loss: 0.21156634390354156
train_iter_loss: 0.2577749490737915
train_iter_loss: 0.2215365171432495
train_iter_loss: 0.13802525401115417
train_iter_loss: 0.19968309998512268
train_iter_loss: 0.2506749927997589
train_iter_loss: 0.08069077879190445
train_iter_loss: 0.309131383895874
train_iter_loss: 0.30807173252105713
train_iter_loss: 0.34025460481643677
train_iter_loss: 0.36942005157470703
train_iter_loss: 0.3352148234844208
train_iter_loss: 0.5875433683395386
train_iter_loss: 0.38021886348724365
train_iter_loss: 0.15070606768131256
train_iter_loss: 0.2687220871448517
train_iter_loss: 0.32826530933380127
train_iter_loss: 0.18743896484375
train_iter_loss: 0.1275845766067505
train_iter_loss: 0.2690800130367279
train_iter_loss: 0.24861107766628265
train_iter_loss: 0.15897014737129211
train_iter_loss: 0.26572784781455994
train_iter_loss: 0.14885438978672028
train_iter_loss: 0.14847637712955475
train_iter_loss: 0.38219696283340454
train_iter_loss: 0.26103675365448
train_iter_loss: 0.25325649976730347
train_iter_loss: 0.21775873005390167
train_iter_loss: 0.3357110321521759
train_iter_loss: 0.25367271900177
train_iter_loss: 0.29913759231567383
train_iter_loss: 0.39013800024986267
train_iter_loss: 0.22319255769252777
train_iter_loss: 0.21380336582660675
train_iter_loss: 0.32420772314071655
train_iter_loss: 0.22092685103416443
train_iter_loss: 0.13442496955394745
train_iter_loss: 0.34810253977775574
train_iter_loss: 0.3689926862716675
train_iter_loss: 0.3463796079158783
train_iter_loss: 0.21980465948581696
train_iter_loss: 0.09925747662782669
train_iter_loss: 0.1684238612651825
train_iter_loss: 0.2016972005367279
train_iter_loss: 0.29619574546813965
train_iter_loss: 0.16459383070468903
train_iter_loss: 0.3287682831287384
train_iter_loss: 0.23399053514003754
train_iter_loss: 0.158782958984375
train_iter_loss: 0.3160657584667206
train_iter_loss: 0.1769995242357254
train_iter_loss: 0.08091121912002563
train_iter_loss: 0.28479695320129395
train_iter_loss: 0.3541508615016937
train_iter_loss: 0.2145911157131195
train_iter_loss: 0.4393749535083771
train_iter_loss: 0.27630671858787537
train_iter_loss: 0.24101297557353973
train_iter_loss: 0.18592946231365204
train_iter_loss: 0.24763895571231842
train_iter_loss: 0.30476856231689453
train_iter_loss: 0.40911146998405457
train_iter_loss: 0.2507484555244446
train_iter_loss: 0.5126106142997742
train_iter_loss: 0.28892672061920166
train_iter_loss: 0.10898476839065552
train_iter_loss: 0.2878047823905945
train_iter_loss: 0.33523523807525635
train_iter_loss: 0.7259281277656555
train_iter_loss: 0.17825819551944733
train_iter_loss: 0.31596463918685913
train_iter_loss: 0.25318488478660583
train_iter_loss: 0.31962165236473083
train_iter_loss: 0.29564082622528076
train_iter_loss: 0.31894350051879883
train_iter_loss: 0.26392483711242676
train_iter_loss: 0.2945733368396759
train_iter_loss: 0.25274398922920227
train_iter_loss: 0.26915693283081055
train_iter_loss: 0.31799551844596863
train_iter_loss: 0.2629997730255127
train_iter_loss: 0.4928049147129059
train_iter_loss: 0.24922333657741547
train_iter_loss: 0.2878928482532501
train_iter_loss: 0.3040291965007782
train_iter_loss: 0.27330291271209717
train_iter_loss: 0.3093203604221344
train_iter_loss: 0.1399882733821869
train_iter_loss: 0.1656235158443451
train_iter_loss: 0.23169291019439697
train loss :0.2671
---------------------
Validation seg loss: 0.34774477074344484 at epoch 661
epoch =    662/  1000, exp = train
train_iter_loss: 0.1836608350276947
train_iter_loss: 0.22658689320087433
train_iter_loss: 0.2398875206708908
train_iter_loss: 0.4636880159378052
train_iter_loss: 0.24480776488780975
train_iter_loss: 0.29252246022224426
train_iter_loss: 0.4077521562576294
train_iter_loss: 0.19074532389640808
train_iter_loss: 0.13236185908317566
train_iter_loss: 0.2181646227836609
train_iter_loss: 0.27604740858078003
train_iter_loss: 0.36279770731925964
train_iter_loss: 0.0978836864233017
train_iter_loss: 0.33596041798591614
train_iter_loss: 0.3077681064605713
train_iter_loss: 0.2955228388309479
train_iter_loss: 0.38390210270881653
train_iter_loss: 0.20640642940998077
train_iter_loss: 0.3246190547943115
train_iter_loss: 0.24296271800994873
train_iter_loss: 0.1549052894115448
train_iter_loss: 0.34862643480300903
train_iter_loss: 0.23835475742816925
train_iter_loss: 0.19919177889823914
train_iter_loss: 0.42090195417404175
train_iter_loss: 0.09589727967977524
train_iter_loss: 0.23149989545345306
train_iter_loss: 0.17383252084255219
train_iter_loss: 0.21667367219924927
train_iter_loss: 0.35056954622268677
train_iter_loss: 0.16498561203479767
train_iter_loss: 0.190728098154068
train_iter_loss: 0.3047267496585846
train_iter_loss: 0.45862093567848206
train_iter_loss: 0.10346279293298721
train_iter_loss: 0.25713086128234863
train_iter_loss: 0.2967892587184906
train_iter_loss: 0.13309018313884735
train_iter_loss: 0.1004367247223854
train_iter_loss: 0.3055940866470337
train_iter_loss: 0.2834213078022003
train_iter_loss: 0.3204267621040344
train_iter_loss: 0.10070467740297318
train_iter_loss: 0.3299787640571594
train_iter_loss: 0.24816778302192688
train_iter_loss: 0.10613958537578583
train_iter_loss: 0.37964949011802673
train_iter_loss: 0.24949845671653748
train_iter_loss: 0.45915886759757996
train_iter_loss: 0.36901506781578064
train_iter_loss: 0.3369879126548767
train_iter_loss: 0.27040034532546997
train_iter_loss: 0.18218927085399628
train_iter_loss: 0.2495565563440323
train_iter_loss: 0.25523245334625244
train_iter_loss: 0.2616807818412781
train_iter_loss: 0.2932029366493225
train_iter_loss: 0.21667887270450592
train_iter_loss: 0.2205815613269806
train_iter_loss: 0.09956840425729752
train_iter_loss: 0.1602078080177307
train_iter_loss: 0.17257282137870789
train_iter_loss: 0.3538530766963959
train_iter_loss: 0.17029938101768494
train_iter_loss: 0.3049827814102173
train_iter_loss: 0.38099947571754456
train_iter_loss: 0.3325105309486389
train_iter_loss: 0.26842403411865234
train_iter_loss: 0.2010347694158554
train_iter_loss: 0.19270920753479004
train_iter_loss: 0.317844957113266
train_iter_loss: 0.3980947434902191
train_iter_loss: 0.3093727231025696
train_iter_loss: 0.14096024632453918
train_iter_loss: 0.2819095849990845
train_iter_loss: 0.26275816559791565
train_iter_loss: 0.1598678082227707
train_iter_loss: 0.28216516971588135
train_iter_loss: 0.22711493074893951
train_iter_loss: 0.2637762129306793
train_iter_loss: 0.34021010994911194
train_iter_loss: 0.3059440553188324
train_iter_loss: 0.19720147550106049
train_iter_loss: 0.28025954961776733
train_iter_loss: 0.32277122139930725
train_iter_loss: 0.22357843816280365
train_iter_loss: 0.30786195397377014
train_iter_loss: 0.14241145551204681
train_iter_loss: 0.34157535433769226
train_iter_loss: 0.3854355216026306
train_iter_loss: 0.3326377868652344
train_iter_loss: 0.4525195062160492
train_iter_loss: 0.32557934522628784
train_iter_loss: 0.0875668153166771
train_iter_loss: 0.14844051003456116
train_iter_loss: 0.41670098900794983
train_iter_loss: 0.2693049907684326
train_iter_loss: 0.16900597512722015
train_iter_loss: 0.09472306072711945
train_iter_loss: 0.24981369078159332
train loss :0.2625
---------------------
Validation seg loss: 0.3847943641957036 at epoch 662
epoch =    663/  1000, exp = train
train_iter_loss: 0.1509915292263031
train_iter_loss: 0.14055898785591125
train_iter_loss: 0.0911046639084816
train_iter_loss: 0.24973146617412567
train_iter_loss: 0.10165185481309891
train_iter_loss: 0.2758013904094696
train_iter_loss: 0.2378610521554947
train_iter_loss: 0.16454461216926575
train_iter_loss: 0.48292359709739685
train_iter_loss: 0.13392315804958344
train_iter_loss: 0.22932779788970947
train_iter_loss: 0.1425028145313263
train_iter_loss: 0.2716265916824341
train_iter_loss: 0.26978644728660583
train_iter_loss: 0.2672169804573059
train_iter_loss: 0.3061445951461792
train_iter_loss: 0.33670496940612793
train_iter_loss: 0.28230589628219604
train_iter_loss: 0.2049999088048935
train_iter_loss: 0.4735724925994873
train_iter_loss: 0.25077033042907715
train_iter_loss: 0.10479265451431274
train_iter_loss: 0.2698846161365509
train_iter_loss: 0.3512735664844513
train_iter_loss: 0.12068544328212738
train_iter_loss: 0.33595404028892517
train_iter_loss: 0.17001493275165558
train_iter_loss: 0.2537493109703064
train_iter_loss: 0.21663329005241394
train_iter_loss: 0.3736487925052643
train_iter_loss: 0.35148435831069946
train_iter_loss: 0.3129211664199829
train_iter_loss: 0.3065422475337982
train_iter_loss: 0.16938595473766327
train_iter_loss: 0.3134188652038574
train_iter_loss: 0.2684679329395294
train_iter_loss: 0.2706146538257599
train_iter_loss: 0.3650819957256317
train_iter_loss: 0.14713647961616516
train_iter_loss: 0.30350080132484436
train_iter_loss: 0.41266024112701416
train_iter_loss: 0.17886945605278015
train_iter_loss: 0.2678341269493103
train_iter_loss: 0.31379827857017517
train_iter_loss: 0.14883919060230255
train_iter_loss: 0.142595112323761
train_iter_loss: 0.32622596621513367
train_iter_loss: 0.20764368772506714
train_iter_loss: 0.2688670754432678
train_iter_loss: 0.29527512192726135
train_iter_loss: 0.2294507473707199
train_iter_loss: 0.3493153154850006
train_iter_loss: 0.10283020883798599
train_iter_loss: 0.3715694844722748
train_iter_loss: 0.26770609617233276
train_iter_loss: 0.29877910017967224
train_iter_loss: 0.12868545949459076
train_iter_loss: 0.3119266927242279
train_iter_loss: 0.2868627607822418
train_iter_loss: 0.3163613975048065
train_iter_loss: 0.19250896573066711
train_iter_loss: 0.33376747369766235
train_iter_loss: 0.259959876537323
train_iter_loss: 0.2824794054031372
train_iter_loss: 0.2715359926223755
train_iter_loss: 0.11546003818511963
train_iter_loss: 0.22998474538326263
train_iter_loss: 0.3249979615211487
train_iter_loss: 0.22068050503730774
train_iter_loss: 0.1739448755979538
train_iter_loss: 0.3772336542606354
train_iter_loss: 0.2984698414802551
train_iter_loss: 0.2552066445350647
train_iter_loss: 0.2632080018520355
train_iter_loss: 0.41377079486846924
train_iter_loss: 0.3097294867038727
train_iter_loss: 0.22485317289829254
train_iter_loss: 0.30647343397140503
train_iter_loss: 0.2166016697883606
train_iter_loss: 0.035302866250276566
train_iter_loss: 0.24873985350131989
train_iter_loss: 0.32736948132514954
train_iter_loss: 0.11047610640525818
train_iter_loss: 0.2434898167848587
train_iter_loss: 0.36731770634651184
train_iter_loss: 0.25111863017082214
train_iter_loss: 0.3745836317539215
train_iter_loss: 0.1636440008878708
train_iter_loss: 0.22144529223442078
train_iter_loss: 0.22528383135795593
train_iter_loss: 0.33121493458747864
train_iter_loss: 0.37269359827041626
train_iter_loss: 0.25483256578445435
train_iter_loss: 0.4238613247871399
train_iter_loss: 0.3472622036933899
train_iter_loss: 0.22386139631271362
train_iter_loss: 0.2766386866569519
train_iter_loss: 0.2547004222869873
train_iter_loss: 0.23726841807365417
train_iter_loss: 0.3078049123287201
train loss :0.2623
---------------------
Validation seg loss: 0.3582168167922646 at epoch 663
epoch =    664/  1000, exp = train
train_iter_loss: 0.27046841382980347
train_iter_loss: 0.33013299107551575
train_iter_loss: 0.2352449595928192
train_iter_loss: 0.24473515152931213
train_iter_loss: 0.3039700984954834
train_iter_loss: 0.2531258165836334
train_iter_loss: 0.28959235548973083
train_iter_loss: 0.21966804563999176
train_iter_loss: 0.2774333953857422
train_iter_loss: 0.21947696805000305
train_iter_loss: 0.3176330029964447
train_iter_loss: 0.16836002469062805
train_iter_loss: 0.33154454827308655
train_iter_loss: 0.20882001519203186
train_iter_loss: 0.2425408810377121
train_iter_loss: 0.14716562628746033
train_iter_loss: 0.20051155984401703
train_iter_loss: 0.12254522740840912
train_iter_loss: 0.2288040965795517
train_iter_loss: 0.18277129530906677
train_iter_loss: 0.20381027460098267
train_iter_loss: 0.2968304455280304
train_iter_loss: 0.31369099020957947
train_iter_loss: 0.09415504336357117
train_iter_loss: 0.3567667305469513
train_iter_loss: 0.28714558482170105
train_iter_loss: 0.2979048490524292
train_iter_loss: 0.2525739371776581
train_iter_loss: 0.445351779460907
train_iter_loss: 0.24911826848983765
train_iter_loss: 0.3066219687461853
train_iter_loss: 0.300586998462677
train_iter_loss: 0.21810245513916016
train_iter_loss: 0.19168147444725037
train_iter_loss: 0.29194343090057373
train_iter_loss: 0.19543369114398956
train_iter_loss: 0.20598210394382477
train_iter_loss: 0.3763197660446167
train_iter_loss: 0.4340924620628357
train_iter_loss: 0.27157852053642273
train_iter_loss: 0.22384443879127502
train_iter_loss: 0.1278485357761383
train_iter_loss: 0.3137957751750946
train_iter_loss: 0.3945092558860779
train_iter_loss: 0.2170439064502716
train_iter_loss: 0.29122912883758545
train_iter_loss: 0.38140836358070374
train_iter_loss: 0.21844404935836792
train_iter_loss: 0.3636564016342163
train_iter_loss: 0.34501731395721436
train_iter_loss: 0.6640470623970032
train_iter_loss: 0.2309541553258896
train_iter_loss: 0.12782296538352966
train_iter_loss: 0.32330864667892456
train_iter_loss: 0.11636719107627869
train_iter_loss: 0.2727944254875183
train_iter_loss: 0.22249314188957214
train_iter_loss: 0.2087307721376419
train_iter_loss: 0.20751351118087769
train_iter_loss: 0.21456189453601837
train_iter_loss: 0.19080404937267303
train_iter_loss: 0.23874938488006592
train_iter_loss: 0.2417944371700287
train_iter_loss: 0.27255433797836304
train_iter_loss: 0.2019835263490677
train_iter_loss: 0.3760986328125
train_iter_loss: 0.4004470705986023
train_iter_loss: 0.23308897018432617
train_iter_loss: 0.18299037218093872
train_iter_loss: 0.2665955424308777
train_iter_loss: 0.16309939324855804
train_iter_loss: 0.3008127212524414
train_iter_loss: 0.2687622904777527
train_iter_loss: 0.29560181498527527
train_iter_loss: 0.306978702545166
train_iter_loss: 0.16861358284950256
train_iter_loss: 0.29674312472343445
train_iter_loss: 0.2797624468803406
train_iter_loss: 0.32951489090919495
train_iter_loss: 0.25690048933029175
train_iter_loss: 0.20550794899463654
train_iter_loss: 0.27662575244903564
train_iter_loss: 0.26379379630088806
train_iter_loss: 0.09477076679468155
train_iter_loss: 0.31268560886383057
train_iter_loss: 0.24344871938228607
train_iter_loss: 0.21793898940086365
train_iter_loss: 0.258454829454422
train_iter_loss: 0.2994307279586792
train_iter_loss: 0.25813668966293335
train_iter_loss: 0.38745713233947754
train_iter_loss: 0.34619206190109253
train_iter_loss: 0.3120032250881195
train_iter_loss: 0.20983320474624634
train_iter_loss: 0.2030169516801834
train_iter_loss: 0.26809781789779663
train_iter_loss: 0.3884720802307129
train_iter_loss: 0.21186785399913788
train_iter_loss: 0.28857025504112244
train_iter_loss: 0.327930212020874
train loss :0.2676
---------------------
Validation seg loss: 0.3573156879195627 at epoch 664
epoch =    665/  1000, exp = train
train_iter_loss: 0.2834177017211914
train_iter_loss: 0.32173874974250793
train_iter_loss: 0.026196036487817764
train_iter_loss: 0.07268759608268738
train_iter_loss: 0.2300589680671692
train_iter_loss: 0.34445497393608093
train_iter_loss: 0.4097118377685547
train_iter_loss: 0.2693779766559601
train_iter_loss: 0.22061879932880402
train_iter_loss: 0.2431955188512802
train_iter_loss: 0.27751418948173523
train_iter_loss: 0.3479418158531189
train_iter_loss: 0.3400413393974304
train_iter_loss: 0.15251925587654114
train_iter_loss: 0.2899477481842041
train_iter_loss: 0.17860186100006104
train_iter_loss: 0.28061115741729736
train_iter_loss: 0.30610010027885437
train_iter_loss: 0.2515551745891571
train_iter_loss: 0.15307198464870453
train_iter_loss: 0.4309864044189453
train_iter_loss: 0.18794792890548706
train_iter_loss: 0.314193993806839
train_iter_loss: 0.2997515797615051
train_iter_loss: 0.4004613757133484
train_iter_loss: 0.2451799213886261
train_iter_loss: 0.30373615026474
train_iter_loss: 0.13215592503547668
train_iter_loss: 0.21228426694869995
train_iter_loss: 0.25762835144996643
train_iter_loss: 0.14575155079364777
train_iter_loss: 0.3704560101032257
train_iter_loss: 0.22034430503845215
train_iter_loss: 0.21569441258907318
train_iter_loss: 0.22095531225204468
train_iter_loss: 0.30180519819259644
train_iter_loss: 0.3212589621543884
train_iter_loss: 0.27382323145866394
train_iter_loss: 0.28018903732299805
train_iter_loss: 0.24986904859542847
train_iter_loss: 0.2787728011608124
train_iter_loss: 0.20135939121246338
train_iter_loss: 0.3529321849346161
train_iter_loss: 0.21659690141677856
train_iter_loss: 0.3728402853012085
train_iter_loss: 0.15555021166801453
train_iter_loss: 0.25228890776634216
train_iter_loss: 0.45786523818969727
train_iter_loss: 0.27568575739860535
train_iter_loss: 0.10184464603662491
train_iter_loss: 0.32601016759872437
train_iter_loss: 0.1871826946735382
train_iter_loss: 0.2833312153816223
train_iter_loss: 0.24079979956150055
train_iter_loss: 0.17488184571266174
train_iter_loss: 0.19118185341358185
train_iter_loss: 0.4153643846511841
train_iter_loss: 0.17261779308319092
train_iter_loss: 0.14507652819156647
train_iter_loss: 0.20498014986515045
train_iter_loss: 0.250023752450943
train_iter_loss: 0.27077946066856384
train_iter_loss: 0.26618465781211853
train_iter_loss: 0.21956835687160492
train_iter_loss: 0.19362235069274902
train_iter_loss: 0.46353334188461304
train_iter_loss: 0.14870692789554596
train_iter_loss: 0.2079506814479828
train_iter_loss: 0.5122886300086975
train_iter_loss: 0.35252824425697327
train_iter_loss: 0.21224358677864075
train_iter_loss: 0.27537137269973755
train_iter_loss: 0.2991216480731964
train_iter_loss: 0.2691655457019806
train_iter_loss: 0.2796741724014282
train_iter_loss: 0.2822013199329376
train_iter_loss: 0.34444645047187805
train_iter_loss: 0.25814327597618103
train_iter_loss: 0.33071044087409973
train_iter_loss: 0.14281629025936127
train_iter_loss: 0.4421313405036926
train_iter_loss: 0.2749389708042145
train_iter_loss: 0.298448771238327
train_iter_loss: 0.11148387938737869
train_iter_loss: 0.29122990369796753
train_iter_loss: 0.4588627219200134
train_iter_loss: 0.23657283186912537
train_iter_loss: 0.42319297790527344
train_iter_loss: 0.1974932998418808
train_iter_loss: 0.34978634119033813
train_iter_loss: 0.33086052536964417
train_iter_loss: 0.2652723491191864
train_iter_loss: 0.09712418168783188
train_iter_loss: 0.33104872703552246
train_iter_loss: 0.28036928176879883
train_iter_loss: 0.2510593831539154
train_iter_loss: 0.46313348412513733
train_iter_loss: 0.183208629488945
train_iter_loss: 0.2514425814151764
train_iter_loss: 0.4132140278816223
train loss :0.2719
---------------------
Validation seg loss: 0.35714232513927063 at epoch 665
epoch =    666/  1000, exp = train
train_iter_loss: 0.1888531595468521
train_iter_loss: 0.21938110888004303
train_iter_loss: 0.3123032748699188
train_iter_loss: 0.2407461255788803
train_iter_loss: 0.23070105910301208
train_iter_loss: 0.2232867032289505
train_iter_loss: 0.23550283908843994
train_iter_loss: 0.2847476303577423
train_iter_loss: 0.3290612995624542
train_iter_loss: 0.08079199492931366
train_iter_loss: 0.20184196531772614
train_iter_loss: 0.16180120408535004
train_iter_loss: 0.18713946640491486
train_iter_loss: 0.35789111256599426
train_iter_loss: 0.4232165515422821
train_iter_loss: 0.23510056734085083
train_iter_loss: 0.2608371078968048
train_iter_loss: 0.1309180110692978
train_iter_loss: 0.2914404273033142
train_iter_loss: 0.3731811046600342
train_iter_loss: 0.13435572385787964
train_iter_loss: 0.26142656803131104
train_iter_loss: 0.2542131245136261
train_iter_loss: 0.22156095504760742
train_iter_loss: 0.3494124412536621
train_iter_loss: 0.3809187114238739
train_iter_loss: 0.3459336459636688
train_iter_loss: 0.32825517654418945
train_iter_loss: 0.22485582530498505
train_iter_loss: 0.28259116411209106
train_iter_loss: 0.22474339604377747
train_iter_loss: 0.216147318482399
train_iter_loss: 0.31884944438934326
train_iter_loss: 0.23337315022945404
train_iter_loss: 0.26198264956474304
train_iter_loss: 0.3275068700313568
train_iter_loss: 0.31942087411880493
train_iter_loss: 0.24469871819019318
train_iter_loss: 0.18993410468101501
train_iter_loss: 0.3213665783405304
train_iter_loss: 0.3967973589897156
train_iter_loss: 0.1620130091905594
train_iter_loss: 0.2765156924724579
train_iter_loss: 0.34240463376045227
train_iter_loss: 0.3160061240196228
train_iter_loss: 0.1085532009601593
train_iter_loss: 0.319543719291687
train_iter_loss: 0.29987454414367676
train_iter_loss: 0.3008330762386322
train_iter_loss: 0.3191383481025696
train_iter_loss: 0.25925305485725403
train_iter_loss: 0.10281097143888474
train_iter_loss: 0.2523251175880432
train_iter_loss: 0.27495306730270386
train_iter_loss: 0.23264500498771667
train_iter_loss: 0.3778853118419647
train_iter_loss: 0.2688903510570526
train_iter_loss: 0.17150399088859558
train_iter_loss: 0.26047641038894653
train_iter_loss: 0.43742406368255615
train_iter_loss: 0.24903510510921478
train_iter_loss: 0.11209841817617416
train_iter_loss: 0.30998581647872925
train_iter_loss: 0.3128385543823242
train_iter_loss: 0.25748205184936523
train_iter_loss: 0.2957564890384674
train_iter_loss: 0.2158549726009369
train_iter_loss: 0.23854786157608032
train_iter_loss: 0.35908520221710205
train_iter_loss: 0.30434873700141907
train_iter_loss: 0.14085663855075836
train_iter_loss: 0.2213740050792694
train_iter_loss: 0.2955954670906067
train_iter_loss: 0.34180375933647156
train_iter_loss: 0.17305247485637665
train_iter_loss: 0.37953004240989685
train_iter_loss: 0.2049812376499176
train_iter_loss: 0.2633640766143799
train_iter_loss: 0.245858296751976
train_iter_loss: 0.21587476134300232
train_iter_loss: 0.19727011024951935
train_iter_loss: 0.23425790667533875
train_iter_loss: 0.17198090255260468
train_iter_loss: 0.32441645860671997
train_iter_loss: 0.2697879672050476
train_iter_loss: 0.2570464611053467
train_iter_loss: 0.160484179854393
train_iter_loss: 0.20980918407440186
train_iter_loss: 0.3922784626483917
train_iter_loss: 0.16146601736545563
train_iter_loss: 0.26698780059814453
train_iter_loss: 0.24358312785625458
train_iter_loss: 0.35268232226371765
train_iter_loss: 0.42404428124427795
train_iter_loss: 0.2800249755382538
train_iter_loss: 0.341769814491272
train_iter_loss: 0.3149447739124298
train_iter_loss: 0.18080388009548187
train_iter_loss: 0.1941051036119461
train_iter_loss: 0.41042661666870117
train loss :0.2669
---------------------
Validation seg loss: 0.35758687198636524 at epoch 666
epoch =    667/  1000, exp = train
train_iter_loss: 0.22070901095867157
train_iter_loss: 0.30059877038002014
train_iter_loss: 0.2616707980632782
train_iter_loss: 0.2688267230987549
train_iter_loss: 0.25448063015937805
train_iter_loss: 0.20201444625854492
train_iter_loss: 0.24786534905433655
train_iter_loss: 0.19486103951931
train_iter_loss: 0.1293400079011917
train_iter_loss: 0.245030015707016
train_iter_loss: 0.3694400489330292
train_iter_loss: 0.2685065269470215
train_iter_loss: 0.22161118686199188
train_iter_loss: 0.2754117250442505
train_iter_loss: 0.17685538530349731
train_iter_loss: 0.41574999690055847
train_iter_loss: 0.3236221671104431
train_iter_loss: 0.22071993350982666
train_iter_loss: 0.3167102038860321
train_iter_loss: 0.23243962228298187
train_iter_loss: 0.2640990614891052
train_iter_loss: 0.3566686511039734
train_iter_loss: 0.12762312591075897
train_iter_loss: 0.2470858097076416
train_iter_loss: 0.30461448431015015
train_iter_loss: 0.3056049942970276
train_iter_loss: 0.29850515723228455
train_iter_loss: 0.3153689205646515
train_iter_loss: 0.3384227752685547
train_iter_loss: 0.23501379787921906
train_iter_loss: 0.3159555494785309
train_iter_loss: 0.32092270255088806
train_iter_loss: 0.2015000581741333
train_iter_loss: 0.24177640676498413
train_iter_loss: 0.027794256806373596
train_iter_loss: 0.22549715638160706
train_iter_loss: 0.4028414487838745
train_iter_loss: 0.17225690186023712
train_iter_loss: 0.32981032133102417
train_iter_loss: 0.28840792179107666
train_iter_loss: 0.3226425349712372
train_iter_loss: 0.026501301676034927
train_iter_loss: 0.356553852558136
train_iter_loss: 0.2110154777765274
train_iter_loss: 0.20554611086845398
train_iter_loss: 0.21330086886882782
train_iter_loss: 0.22135384380817413
train_iter_loss: 0.38425329327583313
train_iter_loss: 0.16713903844356537
train_iter_loss: 0.1898748278617859
train_iter_loss: 0.24906504154205322
train_iter_loss: 0.08072517067193985
train_iter_loss: 0.1497802734375
train_iter_loss: 0.5054364204406738
train_iter_loss: 0.30502602458000183
train_iter_loss: 0.22523918747901917
train_iter_loss: 0.30017122626304626
train_iter_loss: 0.22419115900993347
train_iter_loss: 0.14529313147068024
train_iter_loss: 0.20249707996845245
train_iter_loss: 0.2107456475496292
train_iter_loss: 0.30831626057624817
train_iter_loss: 0.21052990853786469
train_iter_loss: 0.2194226086139679
train_iter_loss: 0.16431796550750732
train_iter_loss: 0.41735222935676575
train_iter_loss: 0.21026729047298431
train_iter_loss: 0.31312233209609985
train_iter_loss: 0.28326570987701416
train_iter_loss: 0.317252516746521
train_iter_loss: 0.21824683248996735
train_iter_loss: 0.31085261702537537
train_iter_loss: 0.2591271996498108
train_iter_loss: 0.23539140820503235
train_iter_loss: 0.3616369664669037
train_iter_loss: 0.22080475091934204
train_iter_loss: 0.33754271268844604
train_iter_loss: 0.22606319189071655
train_iter_loss: 0.08420387655496597
train_iter_loss: 0.1955902874469757
train_iter_loss: 0.18854652345180511
train_iter_loss: 0.3865639567375183
train_iter_loss: 0.2767655849456787
train_iter_loss: 0.3598162531852722
train_iter_loss: 0.19123849272727966
train_iter_loss: 0.2778264582157135
train_iter_loss: 0.27791351079940796
train_iter_loss: 0.1661824882030487
train_iter_loss: 0.2978420853614807
train_iter_loss: 0.45559608936309814
train_iter_loss: 0.22931870818138123
train_iter_loss: 0.34517180919647217
train_iter_loss: 0.15603169798851013
train_iter_loss: 0.2991143763065338
train_iter_loss: 0.7775192260742188
train_iter_loss: 0.1776985079050064
train_iter_loss: 0.34585919976234436
train_iter_loss: 0.29403308033943176
train_iter_loss: 0.20996734499931335
train_iter_loss: 0.26212799549102783
train loss :0.2650
---------------------
Validation seg loss: 0.35717414015517485 at epoch 667
epoch =    668/  1000, exp = train
train_iter_loss: 0.2539094090461731
train_iter_loss: 0.13147088885307312
train_iter_loss: 0.2670622766017914
train_iter_loss: 0.3377447724342346
train_iter_loss: 0.19124376773834229
train_iter_loss: 0.28998637199401855
train_iter_loss: 0.3154180645942688
train_iter_loss: 0.28363046050071716
train_iter_loss: 0.1179540604352951
train_iter_loss: 0.24727226793766022
train_iter_loss: 0.1771959662437439
train_iter_loss: 0.24767257273197174
train_iter_loss: 0.30157309770584106
train_iter_loss: 0.3517283797264099
train_iter_loss: 0.18247286975383759
train_iter_loss: 0.25312596559524536
train_iter_loss: 0.2055971324443817
train_iter_loss: 0.303378164768219
train_iter_loss: 0.23330102860927582
train_iter_loss: 0.2835545539855957
train_iter_loss: 0.4249860346317291
train_iter_loss: 0.35573893785476685
train_iter_loss: 0.303043007850647
train_iter_loss: 0.24842949211597443
train_iter_loss: 0.29931843280792236
train_iter_loss: 0.26167696714401245
train_iter_loss: 0.2873780131340027
train_iter_loss: 0.14690914750099182
train_iter_loss: 0.19848571717739105
train_iter_loss: 0.24191449582576752
train_iter_loss: 0.23802177608013153
train_iter_loss: 0.30123504996299744
train_iter_loss: 0.27510905265808105
train_iter_loss: 0.4150337874889374
train_iter_loss: 0.28864264488220215
train_iter_loss: 0.2654068171977997
train_iter_loss: 0.4553993344306946
train_iter_loss: 0.291838139295578
train_iter_loss: 0.15603981912136078
train_iter_loss: 0.39824485778808594
train_iter_loss: 0.2791792154312134
train_iter_loss: 0.13458819687366486
train_iter_loss: 0.24076402187347412
train_iter_loss: 0.1332767754793167
train_iter_loss: 0.20001959800720215
train_iter_loss: 0.277560293674469
train_iter_loss: 0.228889599442482
train_iter_loss: 0.2709489166736603
train_iter_loss: 0.18035824596881866
train_iter_loss: 0.23719708621501923
train_iter_loss: 0.25786423683166504
train_iter_loss: 0.2911531925201416
train_iter_loss: 0.27039164304733276
train_iter_loss: 0.2725257873535156
train_iter_loss: 0.30568960309028625
train_iter_loss: 0.10488247871398926
train_iter_loss: 0.14505107700824738
train_iter_loss: 0.2834078073501587
train_iter_loss: 0.3739936649799347
train_iter_loss: 0.34261494874954224
train_iter_loss: 0.19660557806491852
train_iter_loss: 0.3784569203853607
train_iter_loss: 0.2679188549518585
train_iter_loss: 0.4010603725910187
train_iter_loss: 0.2724011540412903
train_iter_loss: 0.19454284012317657
train_iter_loss: 0.5257536172866821
train_iter_loss: 0.1473623812198639
train_iter_loss: 0.32380712032318115
train_iter_loss: 0.28157082200050354
train_iter_loss: 0.14248622953891754
train_iter_loss: 0.2378782033920288
train_iter_loss: 0.3964473307132721
train_iter_loss: 0.328484445810318
train_iter_loss: 0.4855417013168335
train_iter_loss: 0.371080219745636
train_iter_loss: 0.24492569267749786
train_iter_loss: 0.04868423193693161
train_iter_loss: 0.150477796792984
train_iter_loss: 0.25542396306991577
train_iter_loss: 0.34076884388923645
train_iter_loss: 0.23917286098003387
train_iter_loss: 0.2569761276245117
train_iter_loss: 0.17277511954307556
train_iter_loss: 0.12389929592609406
train_iter_loss: 0.2421427071094513
train_iter_loss: 0.35607659816741943
train_iter_loss: 0.47910845279693604
train_iter_loss: 0.13483192026615143
train_iter_loss: 0.2030773162841797
train_iter_loss: 0.14256487786769867
train_iter_loss: 0.24639597535133362
train_iter_loss: 0.21709850430488586
train_iter_loss: 0.2992405593395233
train_iter_loss: 0.2121914178133011
train_iter_loss: 0.13265705108642578
train_iter_loss: 0.47170528769493103
train_iter_loss: 0.1447281837463379
train_iter_loss: 0.15795117616653442
train_iter_loss: 0.3302666246891022
train loss :0.2648
---------------------
Validation seg loss: 0.3653875362909979 at epoch 668
epoch =    669/  1000, exp = train
train_iter_loss: 0.38866347074508667
train_iter_loss: 0.14860764145851135
train_iter_loss: 0.2908366322517395
train_iter_loss: 0.2877167761325836
train_iter_loss: 0.1609267294406891
train_iter_loss: 0.22577354311943054
train_iter_loss: 0.33171147108078003
train_iter_loss: 0.22080151736736298
train_iter_loss: 0.3360300064086914
train_iter_loss: 0.3128262162208557
train_iter_loss: 0.21356190741062164
train_iter_loss: 0.2758297324180603
train_iter_loss: 0.2076091170310974
train_iter_loss: 0.2882460951805115
train_iter_loss: 0.3214166462421417
train_iter_loss: 0.35090914368629456
train_iter_loss: 0.3206688463687897
train_iter_loss: 0.14672498404979706
train_iter_loss: 0.2217094898223877
train_iter_loss: 0.2964348793029785
train_iter_loss: 0.36001792550086975
train_iter_loss: 0.25373750925064087
train_iter_loss: 0.2810121178627014
train_iter_loss: 0.24594715237617493
train_iter_loss: 0.25954723358154297
train_iter_loss: 0.27419689297676086
train_iter_loss: 0.2644248604774475
train_iter_loss: 0.26248693466186523
train_iter_loss: 0.20931221544742584
train_iter_loss: 0.18785451352596283
train_iter_loss: 0.31490638852119446
train_iter_loss: 0.2262456715106964
train_iter_loss: 0.13613073527812958
train_iter_loss: 0.19637919962406158
train_iter_loss: 0.12231506407260895
train_iter_loss: 0.3621236979961395
train_iter_loss: 0.34643125534057617
train_iter_loss: 0.16127416491508484
train_iter_loss: 0.2406284213066101
train_iter_loss: 0.21394848823547363
train_iter_loss: 0.2763908803462982
train_iter_loss: 0.4259045720100403
train_iter_loss: 0.3121158182621002
train_iter_loss: 0.31200653314590454
train_iter_loss: 0.3761783540248871
train_iter_loss: 0.28300338983535767
train_iter_loss: 0.16433115303516388
train_iter_loss: 0.29966580867767334
train_iter_loss: 0.3536219000816345
train_iter_loss: 0.2556406557559967
train_iter_loss: 0.40035364031791687
train_iter_loss: 0.10048054158687592
train_iter_loss: 0.16279616951942444
train_iter_loss: 0.3674229681491852
train_iter_loss: 0.2885332703590393
train_iter_loss: 0.12347718328237534
train_iter_loss: 0.285938560962677
train_iter_loss: 0.33817723393440247
train_iter_loss: 0.3850163519382477
train_iter_loss: 0.26644301414489746
train_iter_loss: 0.17107614874839783
train_iter_loss: 0.3165131211280823
train_iter_loss: 0.24571320414543152
train_iter_loss: 0.2554926574230194
train_iter_loss: 0.22533699870109558
train_iter_loss: 0.22295495867729187
train_iter_loss: 0.2360154390335083
train_iter_loss: 0.32098811864852905
train_iter_loss: 0.2727796137332916
train_iter_loss: 0.26587188243865967
train_iter_loss: 0.3556046783924103
train_iter_loss: 0.275760680437088
train_iter_loss: 0.3159377872943878
train_iter_loss: 0.25816208124160767
train_iter_loss: 0.1989641934633255
train_iter_loss: 0.38228747248649597
train_iter_loss: 0.3386271595954895
train_iter_loss: 0.20998215675354004
train_iter_loss: 0.3349688947200775
train_iter_loss: 0.26323163509368896
train_iter_loss: 0.27093783020973206
train_iter_loss: 0.2179374396800995
train_iter_loss: 0.11762002855539322
train_iter_loss: 0.32957783341407776
train_iter_loss: 0.39256271719932556
train_iter_loss: 0.21512694656848907
train_iter_loss: 0.24050098657608032
train_iter_loss: 0.350667268037796
train_iter_loss: 0.4460372030735016
train_iter_loss: 0.3197842538356781
train_iter_loss: 0.2768131196498871
train_iter_loss: 0.35336118936538696
train_iter_loss: 0.1274489313364029
train_iter_loss: 0.18550480902194977
train_iter_loss: 0.24995096027851105
train_iter_loss: 0.24842172861099243
train_iter_loss: 0.2163768708705902
train_iter_loss: 0.25847217440605164
train_iter_loss: 0.06574136018753052
train_iter_loss: 0.2767399549484253
train loss :0.2693
---------------------
Validation seg loss: 0.349435532729159 at epoch 669
epoch =    670/  1000, exp = train
train_iter_loss: 0.26186680793762207
train_iter_loss: 0.31542399525642395
train_iter_loss: 0.11197492480278015
train_iter_loss: 0.10972761362791061
train_iter_loss: 0.3374102711677551
train_iter_loss: 0.04233722388744354
train_iter_loss: 0.20827114582061768
train_iter_loss: 0.20801155269145966
train_iter_loss: 0.2131209820508957
train_iter_loss: 0.4481081962585449
train_iter_loss: 0.2683243751525879
train_iter_loss: 0.4116342067718506
train_iter_loss: 0.20073851943016052
train_iter_loss: 0.21559228003025055
train_iter_loss: 0.36958742141723633
train_iter_loss: 0.3249463737010956
train_iter_loss: 0.22164690494537354
train_iter_loss: 0.14402103424072266
train_iter_loss: 0.24565598368644714
train_iter_loss: 0.23092851042747498
train_iter_loss: 0.3013802766799927
train_iter_loss: 0.21283647418022156
train_iter_loss: 0.3211710453033447
train_iter_loss: 0.2834955155849457
train_iter_loss: 0.24798861145973206
train_iter_loss: 0.1577879637479782
train_iter_loss: 0.272705614566803
train_iter_loss: 0.27819517254829407
train_iter_loss: 0.328110009431839
train_iter_loss: 0.20038443803787231
train_iter_loss: 0.3673856556415558
train_iter_loss: 0.07927075773477554
train_iter_loss: 0.33676469326019287
train_iter_loss: 0.3640455901622772
train_iter_loss: 0.17985977232456207
train_iter_loss: 0.34995007514953613
train_iter_loss: 0.22766856849193573
train_iter_loss: 0.18235893547534943
train_iter_loss: 0.2854847013950348
train_iter_loss: 0.23895063996315002
train_iter_loss: 0.24129030108451843
train_iter_loss: 0.32163652777671814
train_iter_loss: 0.2409796118736267
train_iter_loss: 0.27866312861442566
train_iter_loss: 0.2465687394142151
train_iter_loss: 0.2580220699310303
train_iter_loss: 0.3820006847381592
train_iter_loss: 0.15725360810756683
train_iter_loss: 0.3437347114086151
train_iter_loss: 0.3492399752140045
train_iter_loss: 0.14032061398029327
train_iter_loss: 0.30128857493400574
train_iter_loss: 0.10879155993461609
train_iter_loss: 0.25787028670310974
train_iter_loss: 0.36149731278419495
train_iter_loss: 0.2762240171432495
train_iter_loss: 0.09575662016868591
train_iter_loss: 0.2814391255378723
train_iter_loss: 0.26926764845848083
train_iter_loss: 0.23201584815979004
train_iter_loss: 0.27318108081817627
train_iter_loss: 0.5966043472290039
train_iter_loss: 0.21476198732852936
train_iter_loss: 0.3004761040210724
train_iter_loss: 0.10778539627790451
train_iter_loss: 0.2547881603240967
train_iter_loss: 0.14580199122428894
train_iter_loss: 0.31104251742362976
train_iter_loss: 0.2277648001909256
train_iter_loss: 0.27157214283943176
train_iter_loss: 0.39300471544265747
train_iter_loss: 0.23540088534355164
train_iter_loss: 0.20517598092556
train_iter_loss: 0.394401490688324
train_iter_loss: 0.23763607442378998
train_iter_loss: 0.32605865597724915
train_iter_loss: 0.35099515318870544
train_iter_loss: 0.1970086246728897
train_iter_loss: 0.1999238133430481
train_iter_loss: 0.11067985743284225
train_iter_loss: 0.35554319620132446
train_iter_loss: 0.1736033409833908
train_iter_loss: 0.16433335840702057
train_iter_loss: 0.18881481885910034
train_iter_loss: 0.32417112588882446
train_iter_loss: 0.26774919033050537
train_iter_loss: 0.2467629760503769
train_iter_loss: 0.22924920916557312
train_iter_loss: 0.12483927607536316
train_iter_loss: 0.38141322135925293
train_iter_loss: 0.16551056504249573
train_iter_loss: 0.21945667266845703
train_iter_loss: 0.4563104510307312
train_iter_loss: 0.14317065477371216
train_iter_loss: 0.20687954127788544
train_iter_loss: 0.38907375931739807
train_iter_loss: 0.33259424567222595
train_iter_loss: 0.3164253830909729
train_iter_loss: 0.43147459626197815
train_iter_loss: 0.44214579463005066
train loss :0.2643
---------------------
Validation seg loss: 0.3512351021680208 at epoch 670
epoch =    671/  1000, exp = train
train_iter_loss: 0.24508699774742126
train_iter_loss: 0.22802403569221497
train_iter_loss: 0.10174474120140076
train_iter_loss: 0.20320537686347961
train_iter_loss: 0.382936030626297
train_iter_loss: 0.26096171140670776
train_iter_loss: 0.22119566798210144
train_iter_loss: 0.2673606276512146
train_iter_loss: 0.21035896241664886
train_iter_loss: 0.22998081147670746
train_iter_loss: 0.4348892867565155
train_iter_loss: 0.3627173900604248
train_iter_loss: 0.4183090627193451
train_iter_loss: 0.1723753958940506
train_iter_loss: 0.40380623936653137
train_iter_loss: 0.24200056493282318
train_iter_loss: 0.2679809033870697
train_iter_loss: 0.23455902934074402
train_iter_loss: 0.32823044061660767
train_iter_loss: 0.28745827078819275
train_iter_loss: 0.22226789593696594
train_iter_loss: 0.18128100037574768
train_iter_loss: 0.2746097445487976
train_iter_loss: 0.44437241554260254
train_iter_loss: 0.3314453363418579
train_iter_loss: 0.2825589179992676
train_iter_loss: 0.2166489213705063
train_iter_loss: 0.24924999475479126
train_iter_loss: 0.22362366318702698
train_iter_loss: 0.14863894879817963
train_iter_loss: 0.2715410590171814
train_iter_loss: 0.10946086049079895
train_iter_loss: 0.19849900901317596
train_iter_loss: 0.18986999988555908
train_iter_loss: 0.1960483193397522
train_iter_loss: 0.17592401802539825
train_iter_loss: 0.3180030584335327
train_iter_loss: 0.33238551020622253
train_iter_loss: 0.2003507763147354
train_iter_loss: 0.22384695708751678
train_iter_loss: 0.310872882604599
train_iter_loss: 0.12514211237430573
train_iter_loss: 0.13408099114894867
train_iter_loss: 0.29478827118873596
train_iter_loss: 0.2981109321117401
train_iter_loss: 0.20481356978416443
train_iter_loss: 0.23371398448944092
train_iter_loss: 0.1874554455280304
train_iter_loss: 0.3712955415248871
train_iter_loss: 0.2776922583580017
train_iter_loss: 0.2965211868286133
train_iter_loss: 0.19138416647911072
train_iter_loss: 0.25796446204185486
train_iter_loss: 0.20433752238750458
train_iter_loss: 0.19201306998729706
train_iter_loss: 0.33546870946884155
train_iter_loss: 0.32009226083755493
train_iter_loss: 0.35906797647476196
train_iter_loss: 0.435859352350235
train_iter_loss: 0.2858298718929291
train_iter_loss: 0.3430454432964325
train_iter_loss: 0.22849556803703308
train_iter_loss: 0.3198035657405853
train_iter_loss: 0.44230249524116516
train_iter_loss: 0.20711570978164673
train_iter_loss: 0.32785743474960327
train_iter_loss: 0.2120717316865921
train_iter_loss: 0.1567574143409729
train_iter_loss: 0.153772234916687
train_iter_loss: 0.28690361976623535
train_iter_loss: 0.2424917072057724
train_iter_loss: 0.3750922977924347
train_iter_loss: 0.34362471103668213
train_iter_loss: 0.3845077455043793
train_iter_loss: 0.24945540726184845
train_iter_loss: 0.21536938846111298
train_iter_loss: 0.3864695131778717
train_iter_loss: 0.22715339064598083
train_iter_loss: 0.26313430070877075
train_iter_loss: 0.20555467903614044
train_iter_loss: 0.18994539976119995
train_iter_loss: 0.20817072689533234
train_iter_loss: 0.27001431584358215
train_iter_loss: 0.2714572250843048
train_iter_loss: 0.26458877325057983
train_iter_loss: 0.28396716713905334
train_iter_loss: 0.21803827583789825
train_iter_loss: 0.10911233723163605
train_iter_loss: 0.14585576951503754
train_iter_loss: 0.1563049554824829
train_iter_loss: 0.46898552775382996
train_iter_loss: 0.46724510192871094
train_iter_loss: 0.21840643882751465
train_iter_loss: 0.3478359282016754
train_iter_loss: 0.2169821560382843
train_iter_loss: 0.465546190738678
train_iter_loss: 0.31794705986976624
train_iter_loss: 0.3315298855304718
train_iter_loss: 0.3520459830760956
train_iter_loss: 0.18972165882587433
train loss :0.2694
---------------------
Validation seg loss: 0.352145314392335 at epoch 671
epoch =    672/  1000, exp = train
train_iter_loss: 0.2905787527561188
train_iter_loss: 0.27907755970954895
train_iter_loss: 0.21700483560562134
train_iter_loss: 0.2536030113697052
train_iter_loss: 0.1785293072462082
train_iter_loss: 0.17949354648590088
train_iter_loss: 0.22239656746387482
train_iter_loss: 0.3084881603717804
train_iter_loss: 0.20985817909240723
train_iter_loss: 0.3677021563053131
train_iter_loss: 0.41997286677360535
train_iter_loss: 0.2889408767223358
train_iter_loss: 0.3524318337440491
train_iter_loss: 0.25262972712516785
train_iter_loss: 0.40656453371047974
train_iter_loss: 0.39297041296958923
train_iter_loss: 0.15631842613220215
train_iter_loss: 0.26243916153907776
train_iter_loss: 0.2663895785808563
train_iter_loss: 0.09022605419158936
train_iter_loss: 0.30412206053733826
train_iter_loss: 0.22760732471942902
train_iter_loss: 0.2715795934200287
train_iter_loss: 0.14576955139636993
train_iter_loss: 0.3268480598926544
train_iter_loss: 0.23246942460536957
train_iter_loss: 0.22214272618293762
train_iter_loss: 0.4419192969799042
train_iter_loss: 0.19305561482906342
train_iter_loss: 0.34141796827316284
train_iter_loss: 0.16481035947799683
train_iter_loss: 0.5828798413276672
train_iter_loss: 0.2370472401380539
train_iter_loss: 0.2131703943014145
train_iter_loss: 0.3145865499973297
train_iter_loss: 0.22909000515937805
train_iter_loss: 0.21475042402744293
train_iter_loss: 0.23323708772659302
train_iter_loss: 0.1534532755613327
train_iter_loss: 0.32136720418930054
train_iter_loss: 0.1719799041748047
train_iter_loss: 0.24020564556121826
train_iter_loss: 0.2684108316898346
train_iter_loss: 0.22757163643836975
train_iter_loss: 0.2263396829366684
train_iter_loss: 0.39021334052085876
train_iter_loss: 0.43700382113456726
train_iter_loss: 0.3737291991710663
train_iter_loss: 0.23005211353302002
train_iter_loss: 0.2348303496837616
train_iter_loss: 0.32867568731307983
train_iter_loss: 0.29229092597961426
train_iter_loss: 0.29926571249961853
train_iter_loss: 0.2227933555841446
train_iter_loss: 0.15836194157600403
train_iter_loss: 0.21790574491024017
train_iter_loss: 0.15868733823299408
train_iter_loss: 0.21908032894134521
train_iter_loss: 0.27984848618507385
train_iter_loss: 0.27989912033081055
train_iter_loss: 0.31089842319488525
train_iter_loss: 0.10650483518838882
train_iter_loss: 0.33055317401885986
train_iter_loss: 0.2655062675476074
train_iter_loss: 0.21459002792835236
train_iter_loss: 0.5050112009048462
train_iter_loss: 0.4034639000892639
train_iter_loss: 0.2088068276643753
train_iter_loss: 0.2475171983242035
train_iter_loss: 0.24674862623214722
train_iter_loss: 0.19403426349163055
train_iter_loss: 0.3148357570171356
train_iter_loss: 0.19757771492004395
train_iter_loss: 0.2870059907436371
train_iter_loss: 0.2622470557689667
train_iter_loss: 0.2565125823020935
train_iter_loss: 0.16695508360862732
train_iter_loss: 0.21291473507881165
train_iter_loss: 0.33634164929389954
train_iter_loss: 0.2974849343299866
train_iter_loss: 0.19897617399692535
train_iter_loss: 0.3297819495201111
train_iter_loss: 0.2606285512447357
train_iter_loss: 0.27470162510871887
train_iter_loss: 0.324275940656662
train_iter_loss: 0.2218683809041977
train_iter_loss: 0.14436203241348267
train_iter_loss: 0.3562885820865631
train_iter_loss: 0.19816827774047852
train_iter_loss: 0.4322020709514618
train_iter_loss: 0.3726482391357422
train_iter_loss: 0.3921118676662445
train_iter_loss: 0.21112903952598572
train_iter_loss: 0.15362317860126495
train_iter_loss: 0.23535166680812836
train_iter_loss: 0.20190030336380005
train_iter_loss: 0.4005053639411926
train_iter_loss: 0.28398627042770386
train_iter_loss: 0.16470655798912048
train_iter_loss: 0.2355331927537918
train loss :0.2705
---------------------
Validation seg loss: 0.36554649059291716 at epoch 672
epoch =    673/  1000, exp = train
train_iter_loss: 0.2880372107028961
train_iter_loss: 0.20504344999790192
train_iter_loss: 0.29455623030662537
train_iter_loss: 0.30961307883262634
train_iter_loss: 0.03789675235748291
train_iter_loss: 0.29949110746383667
train_iter_loss: 0.12825040519237518
train_iter_loss: 0.3197804391384125
train_iter_loss: 0.33259040117263794
train_iter_loss: 0.3344019055366516
train_iter_loss: 0.2974673807621002
train_iter_loss: 0.2895876169204712
train_iter_loss: 0.21482513844966888
train_iter_loss: 0.1222618892788887
train_iter_loss: 0.23339521884918213
train_iter_loss: 0.32211533188819885
train_iter_loss: 0.2615363299846649
train_iter_loss: 0.17042329907417297
train_iter_loss: 0.2760752737522125
train_iter_loss: 0.21763138473033905
train_iter_loss: 0.26021891832351685
train_iter_loss: 0.19364352524280548
train_iter_loss: 0.2396930754184723
train_iter_loss: 0.27740293741226196
train_iter_loss: 0.2538372278213501
train_iter_loss: 0.2574639916419983
train_iter_loss: 0.1640487164258957
train_iter_loss: 0.4046002924442291
train_iter_loss: 0.3218059539794922
train_iter_loss: 0.2503458261489868
train_iter_loss: 0.30666840076446533
train_iter_loss: 0.13632160425186157
train_iter_loss: 0.1698719710111618
train_iter_loss: 0.15526658296585083
train_iter_loss: 0.12566693127155304
train_iter_loss: 0.1819225698709488
train_iter_loss: 0.43371254205703735
train_iter_loss: 0.3574028015136719
train_iter_loss: 0.27653977274894714
train_iter_loss: 0.1816481351852417
train_iter_loss: 0.3016894459724426
train_iter_loss: 0.2837049067020416
train_iter_loss: 0.4117991328239441
train_iter_loss: 0.2623535096645355
train_iter_loss: 0.24332739412784576
train_iter_loss: 0.43931514024734497
train_iter_loss: 0.4239853322505951
train_iter_loss: 0.24737495183944702
train_iter_loss: 0.37277114391326904
train_iter_loss: 0.2583770751953125
train_iter_loss: 0.23091764748096466
train_iter_loss: 0.1969100385904312
train_iter_loss: 0.19009484350681305
train_iter_loss: 0.24029552936553955
train_iter_loss: 0.35469502210617065
train_iter_loss: 0.23692546784877777
train_iter_loss: 0.2753797769546509
train_iter_loss: 0.27076008915901184
train_iter_loss: 0.14338380098342896
train_iter_loss: 0.13644886016845703
train_iter_loss: 0.20679324865341187
train_iter_loss: 0.34623679518699646
train_iter_loss: 0.42926549911499023
train_iter_loss: 0.25957348942756653
train_iter_loss: 0.21426555514335632
train_iter_loss: 0.3301026523113251
train_iter_loss: 0.18650473654270172
train_iter_loss: 0.25557783246040344
train_iter_loss: 0.3927514851093292
train_iter_loss: 0.3129262924194336
train_iter_loss: 0.23932532966136932
train_iter_loss: 0.18788230419158936
train_iter_loss: 0.2878088057041168
train_iter_loss: 0.2364874631166458
train_iter_loss: 0.3078537881374359
train_iter_loss: 0.19980773329734802
train_iter_loss: 0.3938387930393219
train_iter_loss: 0.19449065625667572
train_iter_loss: 0.24869251251220703
train_iter_loss: 0.16704246401786804
train_iter_loss: 0.30126067996025085
train_iter_loss: 0.34478434920310974
train_iter_loss: 0.27979859709739685
train_iter_loss: 0.24253122508525848
train_iter_loss: 0.26195117831230164
train_iter_loss: 0.28723254799842834
train_iter_loss: 0.40143683552742004
train_iter_loss: 0.30864277482032776
train_iter_loss: 0.20968475937843323
train_iter_loss: 0.3393879532814026
train_iter_loss: 0.37755250930786133
train_iter_loss: 0.22201474010944366
train_iter_loss: 0.19962498545646667
train_iter_loss: 0.18036623299121857
train_iter_loss: 0.2734336853027344
train_iter_loss: 0.1316131055355072
train_iter_loss: 0.18964307010173798
train_iter_loss: 0.19662092626094818
train_iter_loss: 0.2618062198162079
train_iter_loss: 0.3644241690635681
train loss :0.2647
---------------------
Validation seg loss: 0.3681688943069498 at epoch 673
epoch =    674/  1000, exp = train
train_iter_loss: 0.20528696477413177
train_iter_loss: 0.49358245730400085
train_iter_loss: 0.13787366449832916
train_iter_loss: 0.25058630108833313
train_iter_loss: 0.040510546416044235
train_iter_loss: 0.2392723262310028
train_iter_loss: 0.2868337333202362
train_iter_loss: 0.28928592801094055
train_iter_loss: 0.3266524374485016
train_iter_loss: 0.3359706401824951
train_iter_loss: 0.3127516806125641
train_iter_loss: 0.1774548888206482
train_iter_loss: 0.2605202794075012
train_iter_loss: 0.22754648327827454
train_iter_loss: 0.19700086116790771
train_iter_loss: 0.3486522138118744
train_iter_loss: 0.33359968662261963
train_iter_loss: 0.19942408800125122
train_iter_loss: 0.14481301605701447
train_iter_loss: 0.28598517179489136
train_iter_loss: 0.2955430746078491
train_iter_loss: 0.19774286448955536
train_iter_loss: 0.1743486076593399
train_iter_loss: 0.4088969826698303
train_iter_loss: 0.22472672164440155
train_iter_loss: 0.2204562872648239
train_iter_loss: 0.229178786277771
train_iter_loss: 0.14745980501174927
train_iter_loss: 0.364751398563385
train_iter_loss: 0.2554602026939392
train_iter_loss: 0.25660157203674316
train_iter_loss: 0.30770203471183777
train_iter_loss: 0.3139582574367523
train_iter_loss: 0.1805751919746399
train_iter_loss: 0.4119977056980133
train_iter_loss: 0.40036556124687195
train_iter_loss: 0.2646099627017975
train_iter_loss: 0.37345418334007263
train_iter_loss: 0.2262432873249054
train_iter_loss: 0.12631280720233917
train_iter_loss: 0.14420469105243683
train_iter_loss: 0.2540552616119385
train_iter_loss: 0.30858442187309265
train_iter_loss: 0.28626731038093567
train_iter_loss: 0.388141006231308
train_iter_loss: 0.3247661292552948
train_iter_loss: 0.17259037494659424
train_iter_loss: 0.20148317515850067
train_iter_loss: 0.247109055519104
train_iter_loss: 0.2950305938720703
train_iter_loss: 0.26290491223335266
train_iter_loss: 0.2328319102525711
train_iter_loss: 0.39298373460769653
train_iter_loss: 0.13535793125629425
train_iter_loss: 0.2030535191297531
train_iter_loss: 0.19270959496498108
train_iter_loss: 0.17541587352752686
train_iter_loss: 0.38297536969184875
train_iter_loss: 0.2132706493139267
train_iter_loss: 0.22877778112888336
train_iter_loss: 0.1874375194311142
train_iter_loss: 0.37705177068710327
train_iter_loss: 0.08543664962053299
train_iter_loss: 0.2770233452320099
train_iter_loss: 0.48684900999069214
train_iter_loss: 0.19191980361938477
train_iter_loss: 0.2941955327987671
train_iter_loss: 0.23961128294467926
train_iter_loss: 0.264678418636322
train_iter_loss: 0.2512640953063965
train_iter_loss: 0.3284396827220917
train_iter_loss: 0.2141072005033493
train_iter_loss: 0.19297192990779877
train_iter_loss: 0.26472166180610657
train_iter_loss: 0.18109464645385742
train_iter_loss: 0.2522719204425812
train_iter_loss: 0.16746598482131958
train_iter_loss: 0.2722896635532379
train_iter_loss: 0.22247456014156342
train_iter_loss: 0.2789669334888458
train_iter_loss: 0.24981413781642914
train_iter_loss: 0.3682780861854553
train_iter_loss: 0.15479925274848938
train_iter_loss: 0.22609123587608337
train_iter_loss: 0.2069086879491806
train_iter_loss: 0.3460659086704254
train_iter_loss: 0.27373838424682617
train_iter_loss: 0.39798393845558167
train_iter_loss: 0.3353439271450043
train_iter_loss: 0.22748534381389618
train_iter_loss: 0.2449956238269806
train_iter_loss: 0.3577338457107544
train_iter_loss: 0.1732475608587265
train_iter_loss: 0.38928666710853577
train_iter_loss: 0.20585601031780243
train_iter_loss: 0.45547720789909363
train_iter_loss: 0.29192981123924255
train_iter_loss: 0.19754736125469208
train_iter_loss: 0.23979532718658447
train_iter_loss: 0.3301434814929962
train loss :0.2648
---------------------
Validation seg loss: 0.35748768650118334 at epoch 674
epoch =    675/  1000, exp = train
train_iter_loss: 0.19802983105182648
train_iter_loss: 0.24352726340293884
train_iter_loss: 0.41906803846359253
train_iter_loss: 0.3535544276237488
train_iter_loss: 0.17829278111457825
train_iter_loss: 0.31893888115882874
train_iter_loss: 0.27574220299720764
train_iter_loss: 0.24840421974658966
train_iter_loss: 0.2804211378097534
train_iter_loss: 0.3938924968242645
train_iter_loss: 0.06548137217760086
train_iter_loss: 0.30299749970436096
train_iter_loss: 0.2171963006258011
train_iter_loss: 0.23630473017692566
train_iter_loss: 0.4056677222251892
train_iter_loss: 0.271042138338089
train_iter_loss: 0.2927454113960266
train_iter_loss: 0.22766032814979553
train_iter_loss: 0.2395375818014145
train_iter_loss: 0.16041842103004456
train_iter_loss: 0.27539023756980896
train_iter_loss: 0.4256642162799835
train_iter_loss: 0.4052529036998749
train_iter_loss: 0.22620005905628204
train_iter_loss: 0.2585957944393158
train_iter_loss: 0.17123830318450928
train_iter_loss: 0.1461615413427353
train_iter_loss: 0.19010692834854126
train_iter_loss: 0.3601142168045044
train_iter_loss: 0.19293157756328583
train_iter_loss: 0.23634018003940582
train_iter_loss: 0.22035612165927887
train_iter_loss: 0.1181836724281311
train_iter_loss: 0.27016833424568176
train_iter_loss: 0.25158238410949707
train_iter_loss: 0.2531105875968933
train_iter_loss: 0.32314208149909973
train_iter_loss: 0.38399267196655273
train_iter_loss: 0.3022152781486511
train_iter_loss: 0.17409633100032806
train_iter_loss: 0.1909479796886444
train_iter_loss: 0.19904620945453644
train_iter_loss: 0.24875232577323914
train_iter_loss: 0.24930842220783234
train_iter_loss: 0.27245649695396423
train_iter_loss: 0.3356333374977112
train_iter_loss: 0.33895304799079895
train_iter_loss: 0.17349113523960114
train_iter_loss: 0.3308946192264557
train_iter_loss: 0.21825088560581207
train_iter_loss: 0.1315452754497528
train_iter_loss: 0.21827258169651031
train_iter_loss: 0.15401244163513184
train_iter_loss: 0.24348559975624084
train_iter_loss: 0.29775044322013855
train_iter_loss: 0.30438458919525146
train_iter_loss: 0.13223865628242493
train_iter_loss: 0.17327895760536194
train_iter_loss: 0.15138283371925354
train_iter_loss: 0.40294915437698364
train_iter_loss: 0.2327575385570526
train_iter_loss: 0.4009454846382141
train_iter_loss: 0.24289073050022125
train_iter_loss: 0.2839473783969879
train_iter_loss: 0.28837156295776367
train_iter_loss: 0.366751104593277
train_iter_loss: 0.31997543573379517
train_iter_loss: 0.2091444432735443
train_iter_loss: 0.2514072060585022
train_iter_loss: 0.2460479885339737
train_iter_loss: 0.2398722916841507
train_iter_loss: 0.17915286123752594
train_iter_loss: 0.2340831160545349
train_iter_loss: 0.32568272948265076
train_iter_loss: 0.18437519669532776
train_iter_loss: 0.16501252353191376
train_iter_loss: 0.2865818440914154
train_iter_loss: 0.19962377846240997
train_iter_loss: 0.29833826422691345
train_iter_loss: 0.415392130613327
train_iter_loss: 0.4075230658054352
train_iter_loss: 0.19700172543525696
train_iter_loss: 0.21856383979320526
train_iter_loss: 0.23089581727981567
train_iter_loss: 0.2966127395629883
train_iter_loss: 0.2561081051826477
train_iter_loss: 0.2052728682756424
train_iter_loss: 0.08899884670972824
train_iter_loss: 0.10449870675802231
train_iter_loss: 0.30238819122314453
train_iter_loss: 0.3980761170387268
train_iter_loss: 0.389973521232605
train_iter_loss: 0.24413996934890747
train_iter_loss: 0.23834183812141418
train_iter_loss: 0.16603340208530426
train_iter_loss: 0.3728013336658478
train_iter_loss: 0.3869163990020752
train_iter_loss: 0.2067538946866989
train_iter_loss: 0.15058571100234985
train_iter_loss: 0.22014477849006653
train loss :0.2600
---------------------
Validation seg loss: 0.3580722356795 at epoch 675
epoch =    676/  1000, exp = train
train_iter_loss: 0.2004682719707489
train_iter_loss: 0.20689663290977478
train_iter_loss: 0.38700875639915466
train_iter_loss: 0.29606786370277405
train_iter_loss: 0.2724054157733917
train_iter_loss: 0.18715065717697144
train_iter_loss: 0.4115137457847595
train_iter_loss: 0.059899814426898956
train_iter_loss: 0.25633248686790466
train_iter_loss: 0.1882728487253189
train_iter_loss: 0.2650500535964966
train_iter_loss: 0.29784974455833435
train_iter_loss: 0.22780992090702057
train_iter_loss: 0.28174328804016113
train_iter_loss: 0.17897093296051025
train_iter_loss: 0.14031951129436493
train_iter_loss: 0.2512314021587372
train_iter_loss: 0.30483490228652954
train_iter_loss: 0.28578656911849976
train_iter_loss: 0.2982040047645569
train_iter_loss: 0.2630046606063843
train_iter_loss: 0.3315103352069855
train_iter_loss: 0.2963739037513733
train_iter_loss: 0.20678159594535828
train_iter_loss: 0.18160583078861237
train_iter_loss: 0.38026973605155945
train_iter_loss: 0.40089714527130127
train_iter_loss: 0.0851568952202797
train_iter_loss: 0.19259068369865417
train_iter_loss: 0.21284419298171997
train_iter_loss: 0.2401714324951172
train_iter_loss: 0.36889952421188354
train_iter_loss: 0.3576149344444275
train_iter_loss: 0.18382830917835236
train_iter_loss: 0.4093061089515686
train_iter_loss: 0.31120577454566956
train_iter_loss: 0.09708952903747559
train_iter_loss: 0.19023866951465607
train_iter_loss: 0.29856961965560913
train_iter_loss: 0.3111706078052521
train_iter_loss: 0.2934209108352661
train_iter_loss: 0.18365378677845
train_iter_loss: 0.27232423424720764
train_iter_loss: 0.3661872148513794
train_iter_loss: 0.23686398565769196
train_iter_loss: 0.05362097918987274
train_iter_loss: 0.30686822533607483
train_iter_loss: 0.18932458758354187
train_iter_loss: 0.3979186713695526
train_iter_loss: 0.2990088164806366
train_iter_loss: 0.34712088108062744
train_iter_loss: 0.19854098558425903
train_iter_loss: 0.2245231717824936
train_iter_loss: 0.22575992345809937
train_iter_loss: 0.1250321865081787
train_iter_loss: 0.26313716173171997
train_iter_loss: 0.17165490984916687
train_iter_loss: 0.07815313339233398
train_iter_loss: 0.23840855062007904
train_iter_loss: 0.22027216851711273
train_iter_loss: 0.1746402531862259
train_iter_loss: 0.33618995547294617
train_iter_loss: 0.32324540615081787
train_iter_loss: 0.24016590416431427
train_iter_loss: 0.327071875333786
train_iter_loss: 0.31457003951072693
train_iter_loss: 0.25676873326301575
train_iter_loss: 0.352036714553833
train_iter_loss: 0.25254788994789124
train_iter_loss: 0.3165375590324402
train_iter_loss: 0.4045926332473755
train_iter_loss: 0.38335105776786804
train_iter_loss: 0.32552003860473633
train_iter_loss: 0.16123148798942566
train_iter_loss: 0.13236740231513977
train_iter_loss: 0.09025516360998154
train_iter_loss: 0.18586502969264984
train_iter_loss: 0.2979719340801239
train_iter_loss: 0.33724504709243774
train_iter_loss: 0.3779527544975281
train_iter_loss: 0.245283842086792
train_iter_loss: 0.23327918350696564
train_iter_loss: 0.2100253850221634
train_iter_loss: 0.11762682348489761
train_iter_loss: 0.2943200170993805
train_iter_loss: 0.3261658549308777
train_iter_loss: 0.6351883411407471
train_iter_loss: 0.25471410155296326
train_iter_loss: 0.13412342965602875
train_iter_loss: 0.17069923877716064
train_iter_loss: 0.33305788040161133
train_iter_loss: 0.23260848224163055
train_iter_loss: 0.3230753540992737
train_iter_loss: 0.2927941679954529
train_iter_loss: 0.1531853973865509
train_iter_loss: 0.3703274726867676
train_iter_loss: 0.23142875730991364
train_iter_loss: 0.26535752415657043
train_iter_loss: 0.2754804491996765
train_iter_loss: 0.28351351618766785
train loss :0.2624
---------------------
Validation seg loss: 0.3704757888555386 at epoch 676
epoch =    677/  1000, exp = train
train_iter_loss: 0.3491111993789673
train_iter_loss: 0.3416772484779358
train_iter_loss: 0.2988068759441376
train_iter_loss: 0.2576063871383667
train_iter_loss: 0.23286324739456177
train_iter_loss: 0.2817845046520233
train_iter_loss: 0.33948564529418945
train_iter_loss: 0.06835925579071045
train_iter_loss: 0.43373236060142517
train_iter_loss: 0.37788575887680054
train_iter_loss: 0.25330832600593567
train_iter_loss: 0.3422617018222809
train_iter_loss: 0.2555549442768097
train_iter_loss: 0.3190455734729767
train_iter_loss: 0.23044203221797943
train_iter_loss: 0.263063907623291
train_iter_loss: 0.4621739685535431
train_iter_loss: 0.2456812709569931
train_iter_loss: 0.2805371582508087
train_iter_loss: 0.28115731477737427
train_iter_loss: 0.3738994300365448
train_iter_loss: 0.28367018699645996
train_iter_loss: 0.26717036962509155
train_iter_loss: 0.23010121285915375
train_iter_loss: 0.22088424861431122
train_iter_loss: 0.2034105807542801
train_iter_loss: 0.1769658476114273
train_iter_loss: 0.20310631394386292
train_iter_loss: 0.19329935312271118
train_iter_loss: 0.36359280347824097
train_iter_loss: 0.2466079294681549
train_iter_loss: 0.17262376844882965
train_iter_loss: 0.20846685767173767
train_iter_loss: 0.4045098125934601
train_iter_loss: 0.19491000473499298
train_iter_loss: 0.21876327693462372
train_iter_loss: 0.2839905023574829
train_iter_loss: 0.2594861686229706
train_iter_loss: 0.1333153396844864
train_iter_loss: 0.2857651710510254
train_iter_loss: 0.23298801481723785
train_iter_loss: 0.18493665754795074
train_iter_loss: 0.21940650045871735
train_iter_loss: 0.3554094135761261
train_iter_loss: 0.28577521443367004
train_iter_loss: 0.23803329467773438
train_iter_loss: 0.22675883769989014
train_iter_loss: 0.27349793910980225
train_iter_loss: 0.2191699892282486
train_iter_loss: 0.36302655935287476
train_iter_loss: 0.2783081531524658
train_iter_loss: 0.4558373689651489
train_iter_loss: 0.310470312833786
train_iter_loss: 0.3414677381515503
train_iter_loss: 0.17072750627994537
train_iter_loss: 0.24555820226669312
train_iter_loss: 0.1306704729795456
train_iter_loss: 0.3217739760875702
train_iter_loss: 0.4218243956565857
train_iter_loss: 0.2861810624599457
train_iter_loss: 0.23706968128681183
train_iter_loss: 0.4431129992008209
train_iter_loss: 0.12856319546699524
train_iter_loss: 0.13721895217895508
train_iter_loss: 0.2956344187259674
train_iter_loss: 0.37273526191711426
train_iter_loss: 0.24785767495632172
train_iter_loss: 0.5301324725151062
train_iter_loss: 0.0605086125433445
train_iter_loss: 0.2754514813423157
train_iter_loss: 0.2907197177410126
train_iter_loss: 0.19544430077075958
train_iter_loss: 0.28705066442489624
train_iter_loss: 0.1764509379863739
train_iter_loss: 0.25622493028640747
train_iter_loss: 0.2730698883533478
train_iter_loss: 0.3432294428348541
train_iter_loss: 0.24705830216407776
train_iter_loss: 0.17670510709285736
train_iter_loss: 0.3496662676334381
train_iter_loss: 0.2800724506378174
train_iter_loss: 0.2773084044456482
train_iter_loss: 0.13872137665748596
train_iter_loss: 0.16958600282669067
train_iter_loss: 0.22102603316307068
train_iter_loss: 0.2223931849002838
train_iter_loss: 0.3105428218841553
train_iter_loss: 0.22339680790901184
train_iter_loss: 0.2622610926628113
train_iter_loss: 0.22124765813350677
train_iter_loss: 0.43321019411087036
train_iter_loss: 0.20047971606254578
train_iter_loss: 0.2426552027463913
train_iter_loss: 0.2621272802352905
train_iter_loss: 0.27752813696861267
train_iter_loss: 0.22247029840946198
train_iter_loss: 0.2626902461051941
train_iter_loss: 0.3312666416168213
train_iter_loss: 0.20332126319408417
train_iter_loss: 0.09681381285190582
train loss :0.2685
---------------------
Validation seg loss: 0.3653479802624305 at epoch 677
epoch =    678/  1000, exp = train
train_iter_loss: 0.32853448390960693
train_iter_loss: 0.17962153255939484
train_iter_loss: 0.2028520703315735
train_iter_loss: 0.21213920414447784
train_iter_loss: 0.24233804643154144
train_iter_loss: 0.17319554090499878
train_iter_loss: 0.2351873815059662
train_iter_loss: 0.2404831349849701
train_iter_loss: 0.23466461896896362
train_iter_loss: 0.28768813610076904
train_iter_loss: 0.2511495053768158
train_iter_loss: 0.3546530306339264
train_iter_loss: 0.15536750853061676
train_iter_loss: 0.10087750852108002
train_iter_loss: 0.2059859037399292
train_iter_loss: 0.22676780819892883
train_iter_loss: 0.2462742179632187
train_iter_loss: 0.2153131365776062
train_iter_loss: 0.3336873948574066
train_iter_loss: 0.3281700313091278
train_iter_loss: 0.2704724669456482
train_iter_loss: 0.38268789649009705
train_iter_loss: 0.13736853003501892
train_iter_loss: 0.1824355572462082
train_iter_loss: 0.2756173610687256
train_iter_loss: 0.31834566593170166
train_iter_loss: 0.32701411843299866
train_iter_loss: 0.2076142281293869
train_iter_loss: 0.3334498703479767
train_iter_loss: 0.30177557468414307
train_iter_loss: 0.2879643142223358
train_iter_loss: 0.3150458335876465
train_iter_loss: 0.15432880818843842
train_iter_loss: 0.3244759142398834
train_iter_loss: 0.15528737008571625
train_iter_loss: 0.4400458037853241
train_iter_loss: 0.21558816730976105
train_iter_loss: 0.11723814159631729
train_iter_loss: 0.3481408655643463
train_iter_loss: 0.33408594131469727
train_iter_loss: 0.31557151675224304
train_iter_loss: 0.2203104943037033
train_iter_loss: 0.47789594531059265
train_iter_loss: 0.28938817977905273
train_iter_loss: 0.2940382659435272
train_iter_loss: 0.3517548739910126
train_iter_loss: 0.10656125098466873
train_iter_loss: 0.19572100043296814
train_iter_loss: 0.25982949137687683
train_iter_loss: 0.16083671152591705
train_iter_loss: 0.22643332183361053
train_iter_loss: 0.15297044813632965
train_iter_loss: 0.24330903589725494
train_iter_loss: 0.20346081256866455
train_iter_loss: 0.21815885603427887
train_iter_loss: 0.27251505851745605
train_iter_loss: 0.35063299536705017
train_iter_loss: 0.13954424858093262
train_iter_loss: 0.14957357943058014
train_iter_loss: 0.40457388758659363
train_iter_loss: 0.2119395136833191
train_iter_loss: 0.27267175912857056
train_iter_loss: 0.3436092734336853
train_iter_loss: 0.1759573221206665
train_iter_loss: 0.1211613267660141
train_iter_loss: 0.34111538529396057
train_iter_loss: 0.20106157660484314
train_iter_loss: 0.28432196378707886
train_iter_loss: 0.1343422532081604
train_iter_loss: 0.46497660875320435
train_iter_loss: 0.34263089299201965
train_iter_loss: 0.24269843101501465
train_iter_loss: 0.3305847644805908
train_iter_loss: 0.5166193246841431
train_iter_loss: 0.2646395266056061
train_iter_loss: 0.17445321381092072
train_iter_loss: 0.33715203404426575
train_iter_loss: 0.3271936774253845
train_iter_loss: 0.2735227346420288
train_iter_loss: 0.2848317325115204
train_iter_loss: 0.2605084180831909
train_iter_loss: 0.2686363756656647
train_iter_loss: 0.3852824568748474
train_iter_loss: 0.09829185903072357
train_iter_loss: 0.3425676226615906
train_iter_loss: 0.17941971123218536
train_iter_loss: 0.26121920347213745
train_iter_loss: 0.32401740550994873
train_iter_loss: 0.3090190291404724
train_iter_loss: 0.19378642737865448
train_iter_loss: 0.2452099323272705
train_iter_loss: 0.4505234956741333
train_iter_loss: 0.2921202480792999
train_iter_loss: 0.23991067707538605
train_iter_loss: 0.21916693449020386
train_iter_loss: 0.3368089497089386
train_iter_loss: 0.2940165102481842
train_iter_loss: 0.2288503646850586
train_iter_loss: 0.29732921719551086
train_iter_loss: 0.3113354742527008
train loss :0.2667
---------------------
Validation seg loss: 0.3521819972731876 at epoch 678
epoch =    679/  1000, exp = train
train_iter_loss: 0.2364303171634674
train_iter_loss: 0.32461899518966675
train_iter_loss: 0.1745872050523758
train_iter_loss: 0.30394247174263
train_iter_loss: 0.30925047397613525
train_iter_loss: 0.3025670051574707
train_iter_loss: 0.24462276697158813
train_iter_loss: 0.06033296510577202
train_iter_loss: 0.3518342077732086
train_iter_loss: 0.22088898718357086
train_iter_loss: 0.2633848488330841
train_iter_loss: 0.12600499391555786
train_iter_loss: 0.2806515395641327
train_iter_loss: 0.2931740880012512
train_iter_loss: 0.08990944921970367
train_iter_loss: 0.231624037027359
train_iter_loss: 0.2182794064283371
train_iter_loss: 0.26140668988227844
train_iter_loss: 0.28541111946105957
train_iter_loss: 0.23155376315116882
train_iter_loss: 0.38217777013778687
train_iter_loss: 0.36775004863739014
train_iter_loss: 0.23105910420417786
train_iter_loss: 0.290778249502182
train_iter_loss: 0.27593275904655457
train_iter_loss: 0.13765709102153778
train_iter_loss: 0.1535932719707489
train_iter_loss: 0.20267368853092194
train_iter_loss: 0.39305949211120605
train_iter_loss: 0.22303032875061035
train_iter_loss: 0.0916866585612297
train_iter_loss: 0.3150835335254669
train_iter_loss: 0.13266631960868835
train_iter_loss: 0.19170531630516052
train_iter_loss: 0.27866846323013306
train_iter_loss: 0.3492460250854492
train_iter_loss: 0.3492870032787323
train_iter_loss: 0.23402923345565796
train_iter_loss: 0.4451129138469696
train_iter_loss: 0.19663679599761963
train_iter_loss: 0.2175694853067398
train_iter_loss: 0.21385662257671356
train_iter_loss: 0.32273662090301514
train_iter_loss: 0.16400891542434692
train_iter_loss: 0.15331213176250458
train_iter_loss: 0.16298380494117737
train_iter_loss: 0.47374314069747925
train_iter_loss: 0.21616609394550323
train_iter_loss: 0.19516634941101074
train_iter_loss: 0.37486401200294495
train_iter_loss: 0.2603275179862976
train_iter_loss: 0.18316666781902313
train_iter_loss: 0.3686787188053131
train_iter_loss: 0.3513132929801941
train_iter_loss: 0.2873079776763916
train_iter_loss: 0.19031357765197754
train_iter_loss: 0.25837087631225586
train_iter_loss: 0.4991641342639923
train_iter_loss: 0.17831951379776
train_iter_loss: 0.42186224460601807
train_iter_loss: 0.19910801947116852
train_iter_loss: 0.1436166614294052
train_iter_loss: 0.2546696662902832
train_iter_loss: 0.15138715505599976
train_iter_loss: 0.3077389597892761
train_iter_loss: 0.1868678778409958
train_iter_loss: 0.14150497317314148
train_iter_loss: 0.34957706928253174
train_iter_loss: 0.2662378251552582
train_iter_loss: 0.31101226806640625
train_iter_loss: 0.35342293977737427
train_iter_loss: 0.126469686627388
train_iter_loss: 0.24769553542137146
train_iter_loss: 0.33569109439849854
train_iter_loss: 0.23255884647369385
train_iter_loss: 0.3558107614517212
train_iter_loss: 0.2713784873485565
train_iter_loss: 0.22211037576198578
train_iter_loss: 0.2989254295825958
train_iter_loss: 0.3692597448825836
train_iter_loss: 0.1985449641942978
train_iter_loss: 0.4207707643508911
train_iter_loss: 0.21694283187389374
train_iter_loss: 0.2557472884654999
train_iter_loss: 0.2996058166027069
train_iter_loss: 0.332012802362442
train_iter_loss: 0.1260828971862793
train_iter_loss: 0.15839675068855286
train_iter_loss: 0.22183285653591156
train_iter_loss: 0.4911321997642517
train_iter_loss: 0.28093117475509644
train_iter_loss: 0.3278805613517761
train_iter_loss: 0.22831600904464722
train_iter_loss: 0.24391360580921173
train_iter_loss: 0.13230250775814056
train_iter_loss: 0.3702465891838074
train_iter_loss: 0.337116003036499
train_iter_loss: 0.38497793674468994
train_iter_loss: 0.300886869430542
train_iter_loss: 0.4413217306137085
train loss :0.2671
---------------------
Validation seg loss: 0.36112626909963647 at epoch 679
epoch =    680/  1000, exp = train
train_iter_loss: 0.3498007655143738
train_iter_loss: 0.3109743893146515
train_iter_loss: 0.38880428671836853
train_iter_loss: 0.11241423338651657
train_iter_loss: 0.2779034674167633
train_iter_loss: 0.28100860118865967
train_iter_loss: 0.36103200912475586
train_iter_loss: 0.5031960010528564
train_iter_loss: 0.2649593949317932
train_iter_loss: 0.10166984796524048
train_iter_loss: 0.2187911719083786
train_iter_loss: 0.11699865013360977
train_iter_loss: 0.3203178942203522
train_iter_loss: 0.26588478684425354
train_iter_loss: 0.2254599779844284
train_iter_loss: 0.3317425549030304
train_iter_loss: 0.23046323657035828
train_iter_loss: 0.23235516250133514
train_iter_loss: 0.2791001498699188
train_iter_loss: 0.3131312429904938
train_iter_loss: 0.3035944402217865
train_iter_loss: 0.21676231920719147
train_iter_loss: 0.20960025489330292
train_iter_loss: 0.22698599100112915
train_iter_loss: 0.15577170252799988
train_iter_loss: 0.21345993876457214
train_iter_loss: 0.28662219643592834
train_iter_loss: 0.2857566475868225
train_iter_loss: 0.2074384242296219
train_iter_loss: 0.21730701625347137
train_iter_loss: 0.3778899013996124
train_iter_loss: 0.2084457129240036
train_iter_loss: 0.26606398820877075
train_iter_loss: 0.3214501440525055
train_iter_loss: 0.2849476635456085
train_iter_loss: 0.3485390245914459
train_iter_loss: 0.22809851169586182
train_iter_loss: 0.20543326437473297
train_iter_loss: 0.37898528575897217
train_iter_loss: 0.3433244526386261
train_iter_loss: 0.1780879646539688
train_iter_loss: 0.2961484491825104
train_iter_loss: 0.2794976532459259
train_iter_loss: 0.3075418770313263
train_iter_loss: 0.18308421969413757
train_iter_loss: 0.26680228114128113
train_iter_loss: 0.21152402460575104
train_iter_loss: 0.22292909026145935
train_iter_loss: 0.2344132661819458
train_iter_loss: 0.31295013427734375
train_iter_loss: 0.17593419551849365
train_iter_loss: 0.2722736597061157
train_iter_loss: 0.2986186742782593
train_iter_loss: 0.3117217421531677
train_iter_loss: 0.393392413854599
train_iter_loss: 0.21392054855823517
train_iter_loss: 0.278433620929718
train_iter_loss: 0.3563145697116852
train_iter_loss: 0.3619284927845001
train_iter_loss: 0.2918168604373932
train_iter_loss: 0.246358260512352
train_iter_loss: 0.23316611349582672
train_iter_loss: 0.2986910939216614
train_iter_loss: 0.21724964678287506
train_iter_loss: 0.4092417061328888
train_iter_loss: 0.27576881647109985
train_iter_loss: 0.4798155128955841
train_iter_loss: 0.25614070892333984
train_iter_loss: 0.35229501128196716
train_iter_loss: 0.2506347894668579
train_iter_loss: 0.27631741762161255
train_iter_loss: 0.23782415688037872
train_iter_loss: 0.09841697663068771
train_iter_loss: 0.3419744372367859
train_iter_loss: 0.28280553221702576
train_iter_loss: 0.2575145959854126
train_iter_loss: 0.09825097024440765
train_iter_loss: 0.37679344415664673
train_iter_loss: 0.2035902887582779
train_iter_loss: 0.3208925426006317
train_iter_loss: 0.21919570863246918
train_iter_loss: 0.2349511682987213
train_iter_loss: 0.18071706593036652
train_iter_loss: 0.167277991771698
train_iter_loss: 0.28781360387802124
train_iter_loss: 0.2254839390516281
train_iter_loss: 0.183336541056633
train_iter_loss: 0.35153117775917053
train_iter_loss: 0.11334812641143799
train_iter_loss: 0.3510044515132904
train_iter_loss: 0.37654995918273926
train_iter_loss: 0.2355206310749054
train_iter_loss: 0.14153435826301575
train_iter_loss: 0.2155451774597168
train_iter_loss: 0.2948997914791107
train_iter_loss: 0.17529793083667755
train_iter_loss: 0.13599278032779694
train_iter_loss: 0.26303479075431824
train_iter_loss: 0.302835077047348
train_iter_loss: 0.2460174411535263
train loss :0.2670
---------------------
Validation seg loss: 0.3744658039715844 at epoch 680
epoch =    681/  1000, exp = train
train_iter_loss: 0.3084658682346344
train_iter_loss: 0.4667617380619049
train_iter_loss: 0.31184816360473633
train_iter_loss: 0.28346818685531616
train_iter_loss: 0.15580877661705017
train_iter_loss: 0.3499631881713867
train_iter_loss: 0.26155415177345276
train_iter_loss: 0.33166012167930603
train_iter_loss: 0.21421049535274506
train_iter_loss: 0.07787726819515228
train_iter_loss: 0.16268578171730042
train_iter_loss: 0.37689337134361267
train_iter_loss: 0.22277851402759552
train_iter_loss: 0.19947534799575806
train_iter_loss: 0.32055798172950745
train_iter_loss: 0.2851864695549011
train_iter_loss: 0.2645125985145569
train_iter_loss: 0.32915857434272766
train_iter_loss: 0.29099899530410767
train_iter_loss: 0.3439525365829468
train_iter_loss: 0.29587841033935547
train_iter_loss: 0.28216707706451416
train_iter_loss: 0.39779147505760193
train_iter_loss: 0.2436414361000061
train_iter_loss: 0.2520555853843689
train_iter_loss: 0.3586748242378235
train_iter_loss: 0.2612476646900177
train_iter_loss: 0.4402528703212738
train_iter_loss: 0.3190550208091736
train_iter_loss: 0.35850465297698975
train_iter_loss: 0.13915760815143585
train_iter_loss: 0.2656360864639282
train_iter_loss: 0.19713206589221954
train_iter_loss: 0.15928277373313904
train_iter_loss: 0.48912471532821655
train_iter_loss: 0.26775869727134705
train_iter_loss: 0.252204567193985
train_iter_loss: 0.24939319491386414
train_iter_loss: 0.2782043516635895
train_iter_loss: 0.2397056370973587
train_iter_loss: 0.2488604038953781
train_iter_loss: 0.2066916972398758
train_iter_loss: 0.16448186337947845
train_iter_loss: 0.2160675972700119
train_iter_loss: 0.3086135685443878
train_iter_loss: 0.20785364508628845
train_iter_loss: 0.2633802592754364
train_iter_loss: 0.16362673044204712
train_iter_loss: 0.12566863000392914
train_iter_loss: 0.2635895609855652
train_iter_loss: 0.23341551423072815
train_iter_loss: 0.2169368863105774
train_iter_loss: 0.33199355006217957
train_iter_loss: 0.2851315140724182
train_iter_loss: 0.3053200840950012
train_iter_loss: 0.23263093829154968
train_iter_loss: 0.44996270537376404
train_iter_loss: 0.1628764569759369
train_iter_loss: 0.48413777351379395
train_iter_loss: 0.2507624924182892
train_iter_loss: 0.3079630136489868
train_iter_loss: 0.238427072763443
train_iter_loss: 0.2182973176240921
train_iter_loss: 0.19791720807552338
train_iter_loss: 0.09693557024002075
train_iter_loss: 0.3451383411884308
train_iter_loss: 0.21018563210964203
train_iter_loss: 0.25076496601104736
train_iter_loss: 0.2686876058578491
train_iter_loss: 0.2664041817188263
train_iter_loss: 0.09831354022026062
train_iter_loss: 0.2355276197195053
train_iter_loss: 0.1987040638923645
train_iter_loss: 0.3304261267185211
train_iter_loss: 0.17114529013633728
train_iter_loss: 0.2710101008415222
train_iter_loss: 0.2751897871494293
train_iter_loss: 0.08042854815721512
train_iter_loss: 0.30364012718200684
train_iter_loss: 0.3299052119255066
train_iter_loss: 0.29951366782188416
train_iter_loss: 0.1829405277967453
train_iter_loss: 0.16071787476539612
train_iter_loss: 0.28913941979408264
train_iter_loss: 0.27897459268569946
train_iter_loss: 0.30194681882858276
train_iter_loss: 0.370436429977417
train_iter_loss: 0.36479660868644714
train_iter_loss: 0.25847160816192627
train_iter_loss: 0.2903802692890167
train_iter_loss: 0.20548678934574127
train_iter_loss: 0.23515266180038452
train_iter_loss: 0.22878588736057281
train_iter_loss: 0.21437762677669525
train_iter_loss: 0.26130402088165283
train_iter_loss: 0.21659277379512787
train_iter_loss: 0.226202592253685
train_iter_loss: 0.32750535011291504
train_iter_loss: 0.32012683153152466
train_iter_loss: 0.34088996052742004
train loss :0.2676
---------------------
Validation seg loss: 0.3743940151278984 at epoch 681
epoch =    682/  1000, exp = train
train_iter_loss: 0.33218058943748474
train_iter_loss: 0.14323438704013824
train_iter_loss: 0.24441945552825928
train_iter_loss: 0.27294835448265076
train_iter_loss: 0.405527263879776
train_iter_loss: 0.297722190618515
train_iter_loss: 0.32433268427848816
train_iter_loss: 0.19440630078315735
train_iter_loss: 0.2697804272174835
train_iter_loss: 0.06546874344348907
train_iter_loss: 0.14331990480422974
train_iter_loss: 0.32952308654785156
train_iter_loss: 0.3136760890483856
train_iter_loss: 0.3247673511505127
train_iter_loss: 0.45652198791503906
train_iter_loss: 0.32292330265045166
train_iter_loss: 0.11728568375110626
train_iter_loss: 0.39238250255584717
train_iter_loss: 0.22698765993118286
train_iter_loss: 0.16198740899562836
train_iter_loss: 0.19915850460529327
train_iter_loss: 0.20957882702350616
train_iter_loss: 0.2612917423248291
train_iter_loss: 0.3464159071445465
train_iter_loss: 0.12129249423742294
train_iter_loss: 0.3380247950553894
train_iter_loss: 0.3929217457771301
train_iter_loss: 0.5063717365264893
train_iter_loss: 0.2658289968967438
train_iter_loss: 0.3811626732349396
train_iter_loss: 0.1511831432580948
train_iter_loss: 0.12709380686283112
train_iter_loss: 0.24073565006256104
train_iter_loss: 0.2758127450942993
train_iter_loss: 0.1862841099500656
train_iter_loss: 0.2998136579990387
train_iter_loss: 0.22525346279144287
train_iter_loss: 0.23193472623825073
train_iter_loss: 0.33651289343833923
train_iter_loss: 0.1118490919470787
train_iter_loss: 0.2108311653137207
train_iter_loss: 0.365115225315094
train_iter_loss: 0.2721339166164398
train_iter_loss: 0.34264689683914185
train_iter_loss: 0.22361193597316742
train_iter_loss: 0.1781609058380127
train_iter_loss: 0.2683917284011841
train_iter_loss: 0.3364684581756592
train_iter_loss: 0.2878445088863373
train_iter_loss: 0.1647936850786209
train_iter_loss: 0.12977151572704315
train_iter_loss: 0.2951109707355499
train_iter_loss: 0.3993098735809326
train_iter_loss: 0.11708846688270569
train_iter_loss: 0.24664485454559326
train_iter_loss: 0.20020318031311035
train_iter_loss: 0.2723052203655243
train_iter_loss: 0.14192724227905273
train_iter_loss: 0.31751805543899536
train_iter_loss: 0.14337538182735443
train_iter_loss: 0.263335257768631
train_iter_loss: 0.29312899708747864
train_iter_loss: 0.11625959724187851
train_iter_loss: 0.3247162401676178
train_iter_loss: 0.4422582983970642
train_iter_loss: 0.17995944619178772
train_iter_loss: 0.24624575674533844
train_iter_loss: 0.2836945056915283
train_iter_loss: 0.35034507513046265
train_iter_loss: 0.25829797983169556
train_iter_loss: 0.03129630908370018
train_iter_loss: 0.17763356864452362
train_iter_loss: 0.16798824071884155
train_iter_loss: 0.4514915645122528
train_iter_loss: 0.34199005365371704
train_iter_loss: 0.2579697370529175
train_iter_loss: 0.38075119256973267
train_iter_loss: 0.27696511149406433
train_iter_loss: 0.2337729036808014
train_iter_loss: 0.197514608502388
train_iter_loss: 0.29670917987823486
train_iter_loss: 0.2552976906299591
train_iter_loss: 0.22942161560058594
train_iter_loss: 0.23602542281150818
train_iter_loss: 0.27120140194892883
train_iter_loss: 0.2414926290512085
train_iter_loss: 0.42874008417129517
train_iter_loss: 0.28903689980506897
train_iter_loss: 0.4393114149570465
train_iter_loss: 0.3695031702518463
train_iter_loss: 0.32120129466056824
train_iter_loss: 0.11559128761291504
train_iter_loss: 0.1781160533428192
train_iter_loss: 0.18625174462795258
train_iter_loss: 0.19115287065505981
train_iter_loss: 0.2691965401172638
train_iter_loss: 0.36752015352249146
train_iter_loss: 0.392719030380249
train_iter_loss: 0.2015589475631714
train_iter_loss: 0.27300190925598145
train loss :0.2656
---------------------
Validation seg loss: 0.3786306300657397 at epoch 682
epoch =    683/  1000, exp = train
train_iter_loss: 0.34516748785972595
train_iter_loss: 0.09162609279155731
train_iter_loss: 0.3409069776535034
train_iter_loss: 0.3127017617225647
train_iter_loss: 0.16057352721691132
train_iter_loss: 0.2851163446903229
train_iter_loss: 0.33292123675346375
train_iter_loss: 0.17565463483333588
train_iter_loss: 0.23798763751983643
train_iter_loss: 0.17015548050403595
train_iter_loss: 0.38175272941589355
train_iter_loss: 0.08185920119285583
train_iter_loss: 0.17440305650234222
train_iter_loss: 0.2922841012477875
train_iter_loss: 0.17632301151752472
train_iter_loss: 0.35637322068214417
train_iter_loss: 0.30516526103019714
train_iter_loss: 0.25834494829177856
train_iter_loss: 0.260549396276474
train_iter_loss: 0.33645984530448914
train_iter_loss: 0.2495485246181488
train_iter_loss: 0.33552253246307373
train_iter_loss: 0.2726707458496094
train_iter_loss: 0.39367616176605225
train_iter_loss: 0.22202248871326447
train_iter_loss: 0.2665412724018097
train_iter_loss: 0.37252622842788696
train_iter_loss: 0.15141862630844116
train_iter_loss: 0.22645556926727295
train_iter_loss: 0.22185517847537994
train_iter_loss: 0.25343185663223267
train_iter_loss: 0.2795545756816864
train_iter_loss: 0.30315786600112915
train_iter_loss: 0.1639862358570099
train_iter_loss: 0.3348948657512665
train_iter_loss: 0.19918543100357056
train_iter_loss: 0.30212217569351196
train_iter_loss: 0.3390173017978668
train_iter_loss: 0.20581573247909546
train_iter_loss: 0.13330624997615814
train_iter_loss: 0.3297880291938782
train_iter_loss: 0.305640310049057
train_iter_loss: 0.24667583405971527
train_iter_loss: 0.17724858224391937
train_iter_loss: 0.28344282507896423
train_iter_loss: 0.2714173197746277
train_iter_loss: 0.21798373758792877
train_iter_loss: 0.1990479975938797
train_iter_loss: 0.19284141063690186
train_iter_loss: 0.41281944513320923
train_iter_loss: 0.3607269525527954
train_iter_loss: 0.24953246116638184
train_iter_loss: 0.2710578143596649
train_iter_loss: 0.38859522342681885
train_iter_loss: 0.39647433161735535
train_iter_loss: 0.30394676327705383
train_iter_loss: 0.26962029933929443
train_iter_loss: 0.23830725252628326
train_iter_loss: 0.2588922679424286
train_iter_loss: 0.14163005352020264
train_iter_loss: 0.2839197516441345
train_iter_loss: 0.20967712998390198
train_iter_loss: 0.41726866364479065
train_iter_loss: 0.30290013551712036
train_iter_loss: 0.29441434144973755
train_iter_loss: 0.4825412929058075
train_iter_loss: 0.37199750542640686
train_iter_loss: 0.1391306072473526
train_iter_loss: 0.1841522604227066
train_iter_loss: 0.23630695044994354
train_iter_loss: 0.1895466446876526
train_iter_loss: 0.31305596232414246
train_iter_loss: 0.2679468095302582
train_iter_loss: 0.14768806099891663
train_iter_loss: 0.22848646342754364
train_iter_loss: 0.2261628359556198
train_iter_loss: 0.115707628428936
train_iter_loss: 0.21219536662101746
train_iter_loss: 0.30012011528015137
train_iter_loss: 0.23941732943058014
train_iter_loss: 0.25331398844718933
train_iter_loss: 0.35016223788261414
train_iter_loss: 0.2380785197019577
train_iter_loss: 0.3140023350715637
train_iter_loss: 0.30709677934646606
train_iter_loss: 0.37082791328430176
train_iter_loss: 0.2616804242134094
train_iter_loss: 0.216411292552948
train_iter_loss: 0.11362665891647339
train_iter_loss: 0.3784179389476776
train_iter_loss: 0.10772857815027237
train_iter_loss: 0.11445432156324387
train_iter_loss: 0.24911020696163177
train_iter_loss: 0.21449494361877441
train_iter_loss: 0.24129174649715424
train_iter_loss: 0.20534482598304749
train_iter_loss: 0.24604694545269012
train_iter_loss: 0.2948402464389801
train_iter_loss: 0.2788464426994324
train_iter_loss: 0.16727207601070404
train loss :0.2617
---------------------
Validation seg loss: 0.4036741850388078 at epoch 683
epoch =    684/  1000, exp = train
train_iter_loss: 0.229239359498024
train_iter_loss: 0.19766299426555634
train_iter_loss: 0.38764867186546326
train_iter_loss: 0.34175533056259155
train_iter_loss: 0.32315704226493835
train_iter_loss: 0.23483774065971375
train_iter_loss: 0.26424288749694824
train_iter_loss: 0.2952461540699005
train_iter_loss: 0.12190765887498856
train_iter_loss: 0.3457605242729187
train_iter_loss: 0.2479488104581833
train_iter_loss: 0.20816810429096222
train_iter_loss: 0.25629758834838867
train_iter_loss: 0.0891774520277977
train_iter_loss: 0.22197765111923218
train_iter_loss: 0.35321205854415894
train_iter_loss: 0.3188207447528839
train_iter_loss: 0.37905505299568176
train_iter_loss: 0.188879132270813
train_iter_loss: 0.18761475384235382
train_iter_loss: 0.2552095949649811
train_iter_loss: 0.24937205016613007
train_iter_loss: 0.3488689064979553
train_iter_loss: 0.22785593569278717
train_iter_loss: 0.13613583147525787
train_iter_loss: 0.40574467182159424
train_iter_loss: 0.3576832413673401
train_iter_loss: 0.19273534417152405
train_iter_loss: 0.27131760120391846
train_iter_loss: 0.24190108478069305
train_iter_loss: 0.16126400232315063
train_iter_loss: 0.4020290672779083
train_iter_loss: 0.2031814306974411
train_iter_loss: 0.2968219518661499
train_iter_loss: 0.2463579624891281
train_iter_loss: 0.35549047589302063
train_iter_loss: 0.22893130779266357
train_iter_loss: 0.20395798981189728
train_iter_loss: 0.36973297595977783
train_iter_loss: 0.26950833201408386
train_iter_loss: 0.22895842790603638
train_iter_loss: 0.06846411526203156
train_iter_loss: 0.21732111275196075
train_iter_loss: 0.3702861964702606
train_iter_loss: 0.24113766849040985
train_iter_loss: 0.1973193734884262
train_iter_loss: 0.11681126803159714
train_iter_loss: 0.2587016522884369
train_iter_loss: 0.22971297800540924
train_iter_loss: 0.19377724826335907
train_iter_loss: 0.2590963542461395
train_iter_loss: 0.1778634786605835
train_iter_loss: 0.19591639935970306
train_iter_loss: 0.1556173413991928
train_iter_loss: 0.3226514160633087
train_iter_loss: 0.17564935982227325
train_iter_loss: 0.08758485317230225
train_iter_loss: 0.37450188398361206
train_iter_loss: 0.33098408579826355
train_iter_loss: 0.41067707538604736
train_iter_loss: 0.2603113651275635
train_iter_loss: 0.23586753010749817
train_iter_loss: 0.25917986035346985
train_iter_loss: 0.2623237371444702
train_iter_loss: 0.24289223551750183
train_iter_loss: 0.23431558907032013
train_iter_loss: 0.19158455729484558
train_iter_loss: 0.4135104715824127
train_iter_loss: 0.20147982239723206
train_iter_loss: 0.21884381771087646
train_iter_loss: 0.42029643058776855
train_iter_loss: 0.34646230936050415
train_iter_loss: 0.2301906943321228
train_iter_loss: 0.3401297330856323
train_iter_loss: 0.38690218329429626
train_iter_loss: 0.323550283908844
train_iter_loss: 0.28199994564056396
train_iter_loss: 0.08102356642484665
train_iter_loss: 0.22041113674640656
train_iter_loss: 0.2443089336156845
train_iter_loss: 0.21397492289543152
train_iter_loss: 0.1391814649105072
train_iter_loss: 0.11536373198032379
train_iter_loss: 0.28034818172454834
train_iter_loss: 0.22053910791873932
train_iter_loss: 0.20729365944862366
train_iter_loss: 0.1755627691745758
train_iter_loss: 0.3444768786430359
train_iter_loss: 0.22353187203407288
train_iter_loss: 0.2854974865913391
train_iter_loss: 0.34199896454811096
train_iter_loss: 0.22452975809574127
train_iter_loss: 0.23051683604717255
train_iter_loss: 0.3517056107521057
train_iter_loss: 0.2997533082962036
train_iter_loss: 0.1189129501581192
train_iter_loss: 0.45583444833755493
train_iter_loss: 0.27789628505706787
train_iter_loss: 0.27650076150894165
train_iter_loss: 0.2348008006811142
train loss :0.2591
---------------------
Validation seg loss: 0.3816610800875527 at epoch 684
epoch =    685/  1000, exp = train
train_iter_loss: 0.20370014011859894
train_iter_loss: 0.1447719931602478
train_iter_loss: 0.2182369828224182
train_iter_loss: 0.3904705345630646
train_iter_loss: 0.149300217628479
train_iter_loss: 0.29841744899749756
train_iter_loss: 0.2833169102668762
train_iter_loss: 0.06502862274646759
train_iter_loss: 0.26546332240104675
train_iter_loss: 0.2066623717546463
train_iter_loss: 0.3341524004936218
train_iter_loss: 0.27631452679634094
train_iter_loss: 0.23837566375732422
train_iter_loss: 0.21381841599941254
train_iter_loss: 0.3099701702594757
train_iter_loss: 0.277688205242157
train_iter_loss: 0.22056977450847626
train_iter_loss: 0.20409749448299408
train_iter_loss: 0.37113961577415466
train_iter_loss: 0.193430095911026
train_iter_loss: 0.2970607578754425
train_iter_loss: 0.30329152941703796
train_iter_loss: 0.24074211716651917
train_iter_loss: 0.07698243111371994
train_iter_loss: 0.27547550201416016
train_iter_loss: 0.4060836732387543
train_iter_loss: 0.264475017786026
train_iter_loss: 0.24239395558834076
train_iter_loss: 0.5000293254852295
train_iter_loss: 0.30215999484062195
train_iter_loss: 0.2608806788921356
train_iter_loss: 0.37732693552970886
train_iter_loss: 0.11633776873350143
train_iter_loss: 0.28674277663230896
train_iter_loss: 0.35214605927467346
train_iter_loss: 0.11695525795221329
train_iter_loss: 0.28023096919059753
train_iter_loss: 0.1860787272453308
train_iter_loss: 0.32108473777770996
train_iter_loss: 0.3134942650794983
train_iter_loss: 0.16506671905517578
train_iter_loss: 0.25736749172210693
train_iter_loss: 0.49599406123161316
train_iter_loss: 0.2014867663383484
train_iter_loss: 0.28982624411582947
train_iter_loss: 0.279328316450119
train_iter_loss: 0.31431877613067627
train_iter_loss: 0.032962337136268616
train_iter_loss: 0.3359468877315521
train_iter_loss: 0.3611505627632141
train_iter_loss: 0.25003546476364136
train_iter_loss: 0.3503081798553467
train_iter_loss: 0.1703316569328308
train_iter_loss: 0.26054468750953674
train_iter_loss: 0.46179038286209106
train_iter_loss: 0.3519502282142639
train_iter_loss: 0.4179322421550751
train_iter_loss: 0.27562767267227173
train_iter_loss: 0.3575669229030609
train_iter_loss: 0.2603762149810791
train_iter_loss: 0.2358410656452179
train_iter_loss: 0.30450162291526794
train_iter_loss: 0.15657857060432434
train_iter_loss: 0.1721651405096054
train_iter_loss: 0.3006945550441742
train_iter_loss: 0.0756756067276001
train_iter_loss: 0.3976418077945709
train_iter_loss: 0.2995593249797821
train_iter_loss: 0.402721107006073
train_iter_loss: 0.21889398992061615
train_iter_loss: 0.3284166753292084
train_iter_loss: 0.08947032690048218
train_iter_loss: 0.10723157227039337
train_iter_loss: 0.34045809507369995
train_iter_loss: 0.21954596042633057
train_iter_loss: 0.26450252532958984
train_iter_loss: 0.23774287104606628
train_iter_loss: 0.1296999603509903
train_iter_loss: 0.28135576844215393
train_iter_loss: 0.40292471647262573
train_iter_loss: 0.12372792512178421
train_iter_loss: 0.31820377707481384
train_iter_loss: 0.2738213539123535
train_iter_loss: 0.2380857914686203
train_iter_loss: 0.22971771657466888
train_iter_loss: 0.27118048071861267
train_iter_loss: 0.2007445991039276
train_iter_loss: 0.2557823657989502
train_iter_loss: 0.27196556329727173
train_iter_loss: 0.34436604380607605
train_iter_loss: 0.19276589155197144
train_iter_loss: 0.15160302817821503
train_iter_loss: 0.29153212904930115
train_iter_loss: 0.2939484715461731
train_iter_loss: 0.2869965136051178
train_iter_loss: 0.08615139871835709
train_iter_loss: 0.1061461865901947
train_iter_loss: 0.19180002808570862
train_iter_loss: 0.4864262342453003
train_iter_loss: 0.3639995753765106
train loss :0.2648
---------------------
Validation seg loss: 0.36834688539581617 at epoch 685
epoch =    686/  1000, exp = train
train_iter_loss: 0.17008088529109955
train_iter_loss: 0.19593268632888794
train_iter_loss: 0.3285427987575531
train_iter_loss: 0.3802347183227539
train_iter_loss: 0.23503004014492035
train_iter_loss: 0.1699242889881134
train_iter_loss: 0.34323737025260925
train_iter_loss: 0.2679465711116791
train_iter_loss: 0.35293346643447876
train_iter_loss: 0.2544739544391632
train_iter_loss: 0.14618180692195892
train_iter_loss: 0.20457488298416138
train_iter_loss: 0.3834889233112335
train_iter_loss: 0.11870332062244415
train_iter_loss: 0.2663283050060272
train_iter_loss: 0.24551762640476227
train_iter_loss: 0.37119561433792114
train_iter_loss: 0.24020841717720032
train_iter_loss: 0.27672886848449707
train_iter_loss: 0.19855445623397827
train_iter_loss: 0.3310425579547882
train_iter_loss: 0.40144479274749756
train_iter_loss: 0.33753514289855957
train_iter_loss: 0.22633343935012817
train_iter_loss: 0.1198442354798317
train_iter_loss: 0.3195854723453522
train_iter_loss: 0.3205743432044983
train_iter_loss: 0.291489839553833
train_iter_loss: 0.4041005074977875
train_iter_loss: 0.5009923577308655
train_iter_loss: 0.29927268624305725
train_iter_loss: 0.2487875521183014
train_iter_loss: 0.43614351749420166
train_iter_loss: 0.15497291088104248
train_iter_loss: 0.1953996866941452
train_iter_loss: 0.1224578320980072
train_iter_loss: 0.37926653027534485
train_iter_loss: 0.2672874629497528
train_iter_loss: 0.19307248294353485
train_iter_loss: 0.32840287685394287
train_iter_loss: 0.35340213775634766
train_iter_loss: 0.2092750370502472
train_iter_loss: 0.290492445230484
train_iter_loss: 0.2217796891927719
train_iter_loss: 0.2439800500869751
train_iter_loss: 0.24817991256713867
train_iter_loss: 0.3580503761768341
train_iter_loss: 0.13561637699604034
train_iter_loss: 0.3501022756099701
train_iter_loss: 0.3096641004085541
train_iter_loss: 0.27340635657310486
train_iter_loss: 0.11337842792272568
train_iter_loss: 0.2023269534111023
train_iter_loss: 0.3458566963672638
train_iter_loss: 0.37162983417510986
train_iter_loss: 0.2115839272737503
train_iter_loss: 0.3555685579776764
train_iter_loss: 0.401130348443985
train_iter_loss: 0.42755651473999023
train_iter_loss: 0.1323542296886444
train_iter_loss: 0.24744346737861633
train_iter_loss: 0.19438806176185608
train_iter_loss: 0.15115882456302643
train_iter_loss: 0.42285022139549255
train_iter_loss: 0.2032875269651413
train_iter_loss: 0.1504327803850174
train_iter_loss: 0.35482603311538696
train_iter_loss: 0.23374319076538086
train_iter_loss: 0.22687342762947083
train_iter_loss: 0.27595043182373047
train_iter_loss: 0.28184500336647034
train_iter_loss: 0.242933452129364
train_iter_loss: 0.4162651598453522
train_iter_loss: 0.24351000785827637
train_iter_loss: 0.335185170173645
train_iter_loss: 0.21375717222690582
train_iter_loss: 0.22972886264324188
train_iter_loss: 0.25001290440559387
train_iter_loss: 0.5408295392990112
train_iter_loss: 0.2497587651014328
train_iter_loss: 0.4192001521587372
train_iter_loss: 0.1899917721748352
train_iter_loss: 0.35852161049842834
train_iter_loss: 0.2926314175128937
train_iter_loss: 0.31697264313697815
train_iter_loss: 0.17040085792541504
train_iter_loss: 0.2553253769874573
train_iter_loss: 0.1500030755996704
train_iter_loss: 0.29464638233184814
train_iter_loss: 0.1970834583044052
train_iter_loss: 0.2419750839471817
train_iter_loss: 0.23730577528476715
train_iter_loss: 0.39201533794403076
train_iter_loss: 0.15109118819236755
train_iter_loss: 0.10558272898197174
train_iter_loss: 0.3524814248085022
train_iter_loss: 0.16205917298793793
train_iter_loss: 0.07782664149999619
train_iter_loss: 0.1610143482685089
train_iter_loss: 0.3023627698421478
train loss :0.2714
---------------------
Validation seg loss: 0.3604343587857724 at epoch 686
epoch =    687/  1000, exp = train
train_iter_loss: 0.24653032422065735
train_iter_loss: 0.2769347131252289
train_iter_loss: 0.2811594307422638
train_iter_loss: 0.25842931866645813
train_iter_loss: 0.2745857238769531
train_iter_loss: 0.21145424246788025
train_iter_loss: 0.21947909891605377
train_iter_loss: 0.19299212098121643
train_iter_loss: 0.32835879921913147
train_iter_loss: 0.22948090732097626
train_iter_loss: 0.3954152762889862
train_iter_loss: 0.26219257712364197
train_iter_loss: 0.1720193475484848
train_iter_loss: 0.3572278916835785
train_iter_loss: 0.2852333188056946
train_iter_loss: 0.18719233572483063
train_iter_loss: 0.28800609707832336
train_iter_loss: 0.13047248125076294
train_iter_loss: 0.1595229059457779
train_iter_loss: 0.27813711762428284
train_iter_loss: 0.29553884267807007
train_iter_loss: 0.24800050258636475
train_iter_loss: 0.2584227919578552
train_iter_loss: 0.26736071705818176
train_iter_loss: 0.3662760555744171
train_iter_loss: 0.03324449434876442
train_iter_loss: 0.09108548611402512
train_iter_loss: 0.1715204268693924
train_iter_loss: 0.1276969313621521
train_iter_loss: 0.3320254385471344
train_iter_loss: 0.1351173371076584
train_iter_loss: 0.1662624627351761
train_iter_loss: 0.23658201098442078
train_iter_loss: 0.3532569110393524
train_iter_loss: 0.3410545289516449
train_iter_loss: 0.24967260658740997
train_iter_loss: 0.15356335043907166
train_iter_loss: 0.08357983827590942
train_iter_loss: 0.24947068095207214
train_iter_loss: 0.22134682536125183
train_iter_loss: 0.3281930983066559
train_iter_loss: 0.4551919102668762
train_iter_loss: 0.2454008311033249
train_iter_loss: 0.3258613049983978
train_iter_loss: 0.25948405265808105
train_iter_loss: 0.23946069180965424
train_iter_loss: 0.16355860233306885
train_iter_loss: 0.20084838569164276
train_iter_loss: 0.13503606617450714
train_iter_loss: 0.23411120474338531
train_iter_loss: 0.28692862391471863
train_iter_loss: 0.4410019814968109
train_iter_loss: 0.16213124990463257
train_iter_loss: 0.3615695536136627
train_iter_loss: 0.36555051803588867
train_iter_loss: 0.12686783075332642
train_iter_loss: 0.15083789825439453
train_iter_loss: 0.4336690306663513
train_iter_loss: 0.22858954966068268
train_iter_loss: 0.3448503017425537
train_iter_loss: 0.15614937245845795
train_iter_loss: 0.26777663826942444
train_iter_loss: 0.3936695456504822
train_iter_loss: 0.2871040403842926
train_iter_loss: 0.3160424530506134
train_iter_loss: 0.40130332112312317
train_iter_loss: 0.24317684769630432
train_iter_loss: 0.2453402429819107
train_iter_loss: 0.3135048449039459
train_iter_loss: 0.2101995050907135
train_iter_loss: 0.23585322499275208
train_iter_loss: 0.19497740268707275
train_iter_loss: 0.4177251160144806
train_iter_loss: 0.2762872576713562
train_iter_loss: 0.27589571475982666
train_iter_loss: 0.2505423128604889
train_iter_loss: 0.160447895526886
train_iter_loss: 0.2653363049030304
train_iter_loss: 0.20467737317085266
train_iter_loss: 0.25521475076675415
train_iter_loss: 0.40066495537757874
train_iter_loss: 0.37111446261405945
train_iter_loss: 0.2432568222284317
train_iter_loss: 0.22669543325901031
train_iter_loss: 0.46696192026138306
train_iter_loss: 0.15578536689281464
train_iter_loss: 0.09948649257421494
train_iter_loss: 0.21975383162498474
train_iter_loss: 0.41233325004577637
train_iter_loss: 0.37296587228775024
train_iter_loss: 0.25026166439056396
train_iter_loss: 0.2699720561504364
train_iter_loss: 0.2401144653558731
train_iter_loss: 0.29273152351379395
train_iter_loss: 0.22002430260181427
train_iter_loss: 0.21093639731407166
train_iter_loss: 0.22605159878730774
train_iter_loss: 0.13780349493026733
train_iter_loss: 0.2243209332227707
train_iter_loss: 0.22996923327445984
train loss :0.2582
---------------------
Validation seg loss: 0.38332216155645 at epoch 687
epoch =    688/  1000, exp = train
train_iter_loss: 0.26028725504875183
train_iter_loss: 0.36139360070228577
train_iter_loss: 0.2556581497192383
train_iter_loss: 0.18233248591423035
train_iter_loss: 0.16673405468463898
train_iter_loss: 0.23050691187381744
train_iter_loss: 0.18714244663715363
train_iter_loss: 0.1473858803510666
train_iter_loss: 0.16570216417312622
train_iter_loss: 0.21224136650562286
train_iter_loss: 0.24365338683128357
train_iter_loss: 0.21155007183551788
train_iter_loss: 0.12548479437828064
train_iter_loss: 0.23178482055664062
train_iter_loss: 0.12679286301136017
train_iter_loss: 0.11596448719501495
train_iter_loss: 0.30029839277267456
train_iter_loss: 0.2655373811721802
train_iter_loss: 0.6900917291641235
train_iter_loss: 0.17100463807582855
train_iter_loss: 0.4150092899799347
train_iter_loss: 0.43200838565826416
train_iter_loss: 0.22495640814304352
train_iter_loss: 0.3220181465148926
train_iter_loss: 0.15536890923976898
train_iter_loss: 0.30138465762138367
train_iter_loss: 0.33573949337005615
train_iter_loss: 0.25930893421173096
train_iter_loss: 0.293110728263855
train_iter_loss: 0.1920398622751236
train_iter_loss: 0.2387252151966095
train_iter_loss: 0.28271135687828064
train_iter_loss: 0.19092606008052826
train_iter_loss: 0.1903064250946045
train_iter_loss: 0.10237688571214676
train_iter_loss: 0.1965721994638443
train_iter_loss: 0.26401665806770325
train_iter_loss: 0.23319317400455475
train_iter_loss: 0.2756255567073822
train_iter_loss: 0.4049673080444336
train_iter_loss: 0.11240722984075546
train_iter_loss: 0.258289635181427
train_iter_loss: 0.1865512579679489
train_iter_loss: 0.17160363495349884
train_iter_loss: 0.3407193124294281
train_iter_loss: 0.2484796941280365
train_iter_loss: 0.44851383566856384
train_iter_loss: 0.4500080347061157
train_iter_loss: 0.2631334960460663
train_iter_loss: 0.19788892567157745
train_iter_loss: 0.26398801803588867
train_iter_loss: 0.22879962623119354
train_iter_loss: 0.41437774896621704
train_iter_loss: 0.28789541125297546
train_iter_loss: 0.15304060280323029
train_iter_loss: 0.2869202196598053
train_iter_loss: 0.2923262417316437
train_iter_loss: 0.21662423014640808
train_iter_loss: 0.1469089835882187
train_iter_loss: 0.18997636437416077
train_iter_loss: 0.24363523721694946
train_iter_loss: 0.31674322485923767
train_iter_loss: 0.39447855949401855
train_iter_loss: 0.16617698967456818
train_iter_loss: 0.4091172218322754
train_iter_loss: 0.4400794208049774
train_iter_loss: 0.23834389448165894
train_iter_loss: 0.22223655879497528
train_iter_loss: 0.23040936887264252
train_iter_loss: 0.2559741139411926
train_iter_loss: 0.26541244983673096
train_iter_loss: 0.4630114436149597
train_iter_loss: 0.39514490962028503
train_iter_loss: 0.08353563398122787
train_iter_loss: 0.27000680565834045
train_iter_loss: 0.1473681926727295
train_iter_loss: 0.3898603916168213
train_iter_loss: 0.1691974550485611
train_iter_loss: 0.32281678915023804
train_iter_loss: 0.36369428038597107
train_iter_loss: 0.2374574989080429
train_iter_loss: 0.17896929383277893
train_iter_loss: 0.2521398067474365
train_iter_loss: 0.2448877990245819
train_iter_loss: 0.3631935119628906
train_iter_loss: 0.37944138050079346
train_iter_loss: 0.22507157921791077
train_iter_loss: 0.279658704996109
train_iter_loss: 0.17932632565498352
train_iter_loss: 0.3590226471424103
train_iter_loss: 0.40591859817504883
train_iter_loss: 0.20374664664268494
train_iter_loss: 0.4679667353630066
train_iter_loss: 0.22078755497932434
train_iter_loss: 0.2507748305797577
train_iter_loss: 0.18088145554065704
train_iter_loss: 0.3133354187011719
train_iter_loss: 0.23762132227420807
train_iter_loss: 0.36358124017715454
train_iter_loss: 0.2740052044391632
train loss :0.2678
---------------------
Validation seg loss: 0.39157646292609705 at epoch 688
epoch =    689/  1000, exp = train
train_iter_loss: 0.20118722319602966
train_iter_loss: 0.35874515771865845
train_iter_loss: 0.1985471546649933
train_iter_loss: 0.3314114212989807
train_iter_loss: 0.3128805458545685
train_iter_loss: 0.33786681294441223
train_iter_loss: 0.20964719355106354
train_iter_loss: 0.5079696774482727
train_iter_loss: 0.2669825851917267
train_iter_loss: 0.32459789514541626
train_iter_loss: 0.39904001355171204
train_iter_loss: 0.23801833391189575
train_iter_loss: 0.3134562075138092
train_iter_loss: 0.22660867869853973
train_iter_loss: 0.3106553256511688
train_iter_loss: 0.4126480221748352
train_iter_loss: 0.1634383648633957
train_iter_loss: 0.22318831086158752
train_iter_loss: 0.2716640830039978
train_iter_loss: 0.2911570966243744
train_iter_loss: 0.35690516233444214
train_iter_loss: 0.25396907329559326
train_iter_loss: 0.2864457070827484
train_iter_loss: 0.34384459257125854
train_iter_loss: 0.36151885986328125
train_iter_loss: 0.222087562084198
train_iter_loss: 0.2208685427904129
train_iter_loss: 0.35210299491882324
train_iter_loss: 0.21308346092700958
train_iter_loss: 0.16180802881717682
train_iter_loss: 0.26452773809432983
train_iter_loss: 0.25118935108184814
train_iter_loss: 0.2764313817024231
train_iter_loss: 0.352528840303421
train_iter_loss: 0.30831319093704224
train_iter_loss: 0.24462638795375824
train_iter_loss: 0.25078707933425903
train_iter_loss: 0.2732137441635132
train_iter_loss: 0.3260096609592438
train_iter_loss: 0.036163896322250366
train_iter_loss: 0.11475791782140732
train_iter_loss: 0.31118032336235046
train_iter_loss: 0.3084423243999481
train_iter_loss: 0.3426460921764374
train_iter_loss: 0.1805630773305893
train_iter_loss: 0.27802494168281555
train_iter_loss: 0.19710718095302582
train_iter_loss: 0.1801796406507492
train_iter_loss: 0.15578827261924744
train_iter_loss: 0.3353971540927887
train_iter_loss: 0.3948744535446167
train_iter_loss: 0.21367380023002625
train_iter_loss: 0.3280832767486572
train_iter_loss: 0.20545142889022827
train_iter_loss: 0.3184872567653656
train_iter_loss: 0.13936984539031982
train_iter_loss: 0.34661781787872314
train_iter_loss: 0.20332342386245728
train_iter_loss: 0.20667125284671783
train_iter_loss: 0.16400013864040375
train_iter_loss: 0.24787209928035736
train_iter_loss: 0.32311880588531494
train_iter_loss: 0.23772665858268738
train_iter_loss: 0.2316066473722458
train_iter_loss: 0.28215768933296204
train_iter_loss: 0.3260195553302765
train_iter_loss: 0.24456632137298584
train_iter_loss: 0.25121286511421204
train_iter_loss: 0.2615007758140564
train_iter_loss: 0.30967503786087036
train_iter_loss: 0.2774355411529541
train_iter_loss: 0.30391281843185425
train_iter_loss: 0.26982414722442627
train_iter_loss: 0.2105066031217575
train_iter_loss: 0.18110668659210205
train_iter_loss: 0.19617387652397156
train_iter_loss: 0.18317975103855133
train_iter_loss: 0.29601025581359863
train_iter_loss: 0.23133493959903717
train_iter_loss: 0.1798201948404312
train_iter_loss: 0.20489893853664398
train_iter_loss: 0.3716256320476532
train_iter_loss: 0.2563411593437195
train_iter_loss: 0.30128422379493713
train_iter_loss: 0.3490261137485504
train_iter_loss: 0.24836646020412445
train_iter_loss: 0.40048983693122864
train_iter_loss: 0.1874169558286667
train_iter_loss: 0.1643589437007904
train_iter_loss: 0.18794314563274384
train_iter_loss: 0.22853845357894897
train_iter_loss: 0.06357989460229874
train_iter_loss: 0.40675100684165955
train_iter_loss: 0.30479440093040466
train_iter_loss: 0.28126782178878784
train_iter_loss: 0.2671440839767456
train_iter_loss: 0.3685329854488373
train_iter_loss: 0.35530921816825867
train_iter_loss: 0.20499323308467865
train_iter_loss: 0.1720033437013626
train loss :0.2682
---------------------
Validation seg loss: 0.36452564582193514 at epoch 689
epoch =    690/  1000, exp = train
train_iter_loss: 0.377531498670578
train_iter_loss: 0.19551293551921844
train_iter_loss: 0.34570053219795227
train_iter_loss: 0.15651917457580566
train_iter_loss: 0.18779312074184418
train_iter_loss: 0.4071451425552368
train_iter_loss: 0.3380461037158966
train_iter_loss: 0.269428014755249
train_iter_loss: 0.31739312410354614
train_iter_loss: 0.22321219742298126
train_iter_loss: 0.2880083918571472
train_iter_loss: 0.37607666850090027
train_iter_loss: 0.19043655693531036
train_iter_loss: 0.19440999627113342
train_iter_loss: 0.47476744651794434
train_iter_loss: 0.14998501539230347
train_iter_loss: 0.19834043085575104
train_iter_loss: 0.3862782120704651
train_iter_loss: 0.2624635696411133
train_iter_loss: 0.29270321130752563
train_iter_loss: 0.43535763025283813
train_iter_loss: 0.2612572908401489
train_iter_loss: 0.23002290725708008
train_iter_loss: 0.2673126459121704
train_iter_loss: 0.24086229503154755
train_iter_loss: 0.2563995122909546
train_iter_loss: 0.09581241756677628
train_iter_loss: 0.3320029079914093
train_iter_loss: 0.1960049718618393
train_iter_loss: 0.3470733165740967
train_iter_loss: 0.18410329520702362
train_iter_loss: 0.2889275848865509
train_iter_loss: 0.2511535584926605
train_iter_loss: 0.3387751579284668
train_iter_loss: 0.20212823152542114
train_iter_loss: 0.15447089076042175
train_iter_loss: 0.2566435933113098
train_iter_loss: 0.39725935459136963
train_iter_loss: 0.3748365640640259
train_iter_loss: 0.176767036318779
train_iter_loss: 0.3272874355316162
train_iter_loss: 0.4680534899234772
train_iter_loss: 0.23616519570350647
train_iter_loss: 0.19268690049648285
train_iter_loss: 0.35515296459198
train_iter_loss: 0.3154236376285553
train_iter_loss: 0.35832294821739197
train_iter_loss: 0.24111364781856537
train_iter_loss: 0.1695837527513504
train_iter_loss: 0.2939082384109497
train_iter_loss: 0.16068217158317566
train_iter_loss: 0.3017392158508301
train_iter_loss: 0.3468438386917114
train_iter_loss: 0.35787349939346313
train_iter_loss: 0.15901151299476624
train_iter_loss: 0.18199022114276886
train_iter_loss: 0.17192727327346802
train_iter_loss: 0.2004615068435669
train_iter_loss: 0.2506289482116699
train_iter_loss: 0.31429776549339294
train_iter_loss: 0.22227132320404053
train_iter_loss: 0.2233472764492035
train_iter_loss: 0.17022483050823212
train_iter_loss: 0.18216031789779663
train_iter_loss: 0.2451012283563614
train_iter_loss: 0.17373864352703094
train_iter_loss: 0.344527006149292
train_iter_loss: 0.20237642526626587
train_iter_loss: 0.2045535147190094
train_iter_loss: 0.2356477975845337
train_iter_loss: 0.0650973692536354
train_iter_loss: 0.5147060751914978
train_iter_loss: 0.11916504800319672
train_iter_loss: 0.1879829466342926
train_iter_loss: 0.3157109320163727
train_iter_loss: 0.16890089213848114
train_iter_loss: 0.14304666221141815
train_iter_loss: 0.26327285170555115
train_iter_loss: 0.1924067586660385
train_iter_loss: 0.18582718074321747
train_iter_loss: 0.08926398307085037
train_iter_loss: 0.2306123822927475
train_iter_loss: 0.45605507493019104
train_iter_loss: 0.3755519390106201
train_iter_loss: 0.17275647819042206
train_iter_loss: 0.4682323634624481
train_iter_loss: 0.25174281001091003
train_iter_loss: 0.18859046697616577
train_iter_loss: 0.3413510322570801
train_iter_loss: 0.2896488308906555
train_iter_loss: 0.19320859014987946
train_iter_loss: 0.20206087827682495
train_iter_loss: 0.31480252742767334
train_iter_loss: 0.4147707521915436
train_iter_loss: 0.2817445993423462
train_iter_loss: 0.34640055894851685
train_iter_loss: 0.36386436223983765
train_iter_loss: 0.35574600100517273
train_iter_loss: 0.2086145132780075
train_iter_loss: 0.20956575870513916
train loss :0.2670
---------------------
Validation seg loss: 0.39619672521158067 at epoch 690
epoch =    691/  1000, exp = train
train_iter_loss: 0.31491217017173767
train_iter_loss: 0.18080954253673553
train_iter_loss: 0.3632224202156067
train_iter_loss: 0.38578763604164124
train_iter_loss: 0.37662404775619507
train_iter_loss: 0.37302663922309875
train_iter_loss: 0.13743028044700623
train_iter_loss: 0.1789063662290573
train_iter_loss: 0.17217233777046204
train_iter_loss: 0.15644744038581848
train_iter_loss: 0.24312946200370789
train_iter_loss: 0.2772704064846039
train_iter_loss: 0.6560442447662354
train_iter_loss: 0.16827702522277832
train_iter_loss: 0.1559484302997589
train_iter_loss: 0.16814737021923065
train_iter_loss: 0.1716996729373932
train_iter_loss: 0.21804802119731903
train_iter_loss: 0.11747583001852036
train_iter_loss: 0.29665306210517883
train_iter_loss: 0.24480120837688446
train_iter_loss: 0.10192054510116577
train_iter_loss: 0.19478127360343933
train_iter_loss: 0.43332943320274353
train_iter_loss: 0.1905020922422409
train_iter_loss: 0.25365278124809265
train_iter_loss: 0.3167797029018402
train_iter_loss: 0.11477132141590118
train_iter_loss: 0.19211632013320923
train_iter_loss: 0.13564658164978027
train_iter_loss: 0.4294217526912689
train_iter_loss: 0.3463481366634369
train_iter_loss: 0.13096410036087036
train_iter_loss: 0.26920661330223083
train_iter_loss: 0.23266099393367767
train_iter_loss: 0.30156874656677246
train_iter_loss: 0.2686500549316406
train_iter_loss: 0.32845303416252136
train_iter_loss: 0.38505518436431885
train_iter_loss: 0.20390430092811584
train_iter_loss: 0.36037537455558777
train_iter_loss: 0.3912419378757477
train_iter_loss: 0.12468519061803818
train_iter_loss: 0.30620789527893066
train_iter_loss: 0.3838452696800232
train_iter_loss: 0.2603543996810913
train_iter_loss: 0.25146856904029846
train_iter_loss: 0.29866161942481995
train_iter_loss: 0.203574538230896
train_iter_loss: 0.19177326560020447
train_iter_loss: 0.19800017774105072
train_iter_loss: 0.09297491610050201
train_iter_loss: 0.39811667799949646
train_iter_loss: 0.35653141140937805
train_iter_loss: 0.14593125879764557
train_iter_loss: 0.40987902879714966
train_iter_loss: 0.2735397517681122
train_iter_loss: 0.1707509309053421
train_iter_loss: 0.23067377507686615
train_iter_loss: 0.2923809885978699
train_iter_loss: 0.14785072207450867
train_iter_loss: 0.28152650594711304
train_iter_loss: 0.3306993246078491
train_iter_loss: 0.13130737841129303
train_iter_loss: 0.2340952754020691
train_iter_loss: 0.4185071587562561
train_iter_loss: 0.27691343426704407
train_iter_loss: 0.16433922946453094
train_iter_loss: 0.1651303470134735
train_iter_loss: 0.28924959897994995
train_iter_loss: 0.3205425441265106
train_iter_loss: 0.3609582483768463
train_iter_loss: 0.19596655666828156
train_iter_loss: 0.3230217397212982
train_iter_loss: 0.3373408615589142
train_iter_loss: 0.19070442020893097
train_iter_loss: 0.3006325662136078
train_iter_loss: 0.32302960753440857
train_iter_loss: 0.1895347535610199
train_iter_loss: 0.16582223773002625
train_iter_loss: 0.2608287036418915
train_iter_loss: 0.25450587272644043
train_iter_loss: 0.2936784327030182
train_iter_loss: 0.35860228538513184
train_iter_loss: 0.23766174912452698
train_iter_loss: 0.3574542999267578
train_iter_loss: 0.19024288654327393
train_iter_loss: 0.3550354838371277
train_iter_loss: 0.30434131622314453
train_iter_loss: 0.2612811326980591
train_iter_loss: 0.16188322007656097
train_iter_loss: 0.5037131309509277
train_iter_loss: 0.2090645134449005
train_iter_loss: 0.305130273103714
train_iter_loss: 0.4197615385055542
train_iter_loss: 0.28996092081069946
train_iter_loss: 0.17712238430976868
train_iter_loss: 0.2327098250389099
train_iter_loss: 0.23685301840305328
train_iter_loss: 0.33355167508125305
train loss :0.2669
---------------------
Validation seg loss: 0.3769057757915261 at epoch 691
epoch =    692/  1000, exp = train
train_iter_loss: 0.3069174885749817
train_iter_loss: 0.4781133234500885
train_iter_loss: 0.3242146670818329
train_iter_loss: 0.2185264527797699
train_iter_loss: 0.28102388978004456
train_iter_loss: 0.18644827604293823
train_iter_loss: 0.0988033264875412
train_iter_loss: 0.3243165612220764
train_iter_loss: 0.3325670063495636
train_iter_loss: 0.21882468461990356
train_iter_loss: 0.11152229458093643
train_iter_loss: 0.22886024415493011
train_iter_loss: 0.4438329041004181
train_iter_loss: 0.15307702124118805
train_iter_loss: 0.2171078324317932
train_iter_loss: 0.11965195089578629
train_iter_loss: 0.2347707599401474
train_iter_loss: 0.2664867043495178
train_iter_loss: 0.2722283601760864
train_iter_loss: 0.29375946521759033
train_iter_loss: 0.10856714844703674
train_iter_loss: 0.33449798822402954
train_iter_loss: 0.16917726397514343
train_iter_loss: 0.18246391415596008
train_iter_loss: 0.3057604730129242
train_iter_loss: 0.32132238149642944
train_iter_loss: 0.21499475836753845
train_iter_loss: 0.1971617043018341
train_iter_loss: 0.13532014191150665
train_iter_loss: 0.32247862219810486
train_iter_loss: 0.27179867029190063
train_iter_loss: 0.024074699729681015
train_iter_loss: 0.329176664352417
train_iter_loss: 0.252299040555954
train_iter_loss: 0.3226856291294098
train_iter_loss: 0.05187017843127251
train_iter_loss: 0.1889261156320572
train_iter_loss: 0.27344807982444763
train_iter_loss: 0.2519381642341614
train_iter_loss: 0.3433617949485779
train_iter_loss: 0.22465740144252777
train_iter_loss: 0.3171662390232086
train_iter_loss: 0.31235429644584656
train_iter_loss: 0.2274416983127594
train_iter_loss: 0.2929712235927582
train_iter_loss: 0.0971309170126915
train_iter_loss: 0.3203149139881134
train_iter_loss: 0.33752426505088806
train_iter_loss: 0.36490964889526367
train_iter_loss: 0.2627951204776764
train_iter_loss: 0.3238678574562073
train_iter_loss: 0.17589370906352997
train_iter_loss: 0.4256952106952667
train_iter_loss: 0.19269020855426788
train_iter_loss: 0.21954765915870667
train_iter_loss: 0.33798789978027344
train_iter_loss: 0.33177030086517334
train_iter_loss: 0.2781061828136444
train_iter_loss: 0.120155468583107
train_iter_loss: 0.3207007646560669
train_iter_loss: 0.3075627088546753
train_iter_loss: 0.337206095457077
train_iter_loss: 0.23197133839130402
train_iter_loss: 0.3370135426521301
train_iter_loss: 0.32400766015052795
train_iter_loss: 0.16558139026165009
train_iter_loss: 0.34558480978012085
train_iter_loss: 0.23138932883739471
train_iter_loss: 0.1512528508901596
train_iter_loss: 0.21778316795825958
train_iter_loss: 0.14535817503929138
train_iter_loss: 0.2763112783432007
train_iter_loss: 0.4642479717731476
train_iter_loss: 0.1779467761516571
train_iter_loss: 0.38005584478378296
train_iter_loss: 0.23656968772411346
train_iter_loss: 0.33687299489974976
train_iter_loss: 0.3344300091266632
train_iter_loss: 0.2570858895778656
train_iter_loss: 0.2740064561367035
train_iter_loss: 0.13885706663131714
train_iter_loss: 0.1110551655292511
train_iter_loss: 0.29601144790649414
train_iter_loss: 0.16765213012695312
train_iter_loss: 0.37736883759498596
train_iter_loss: 0.39971816539764404
train_iter_loss: 0.4765965938568115
train_iter_loss: 0.2610672116279602
train_iter_loss: 0.21468161046504974
train_iter_loss: 0.36876624822616577
train_iter_loss: 0.31584909558296204
train_iter_loss: 0.3349951207637787
train_iter_loss: 0.399117648601532
train_iter_loss: 0.26762333512306213
train_iter_loss: 0.3390617072582245
train_iter_loss: 0.22912926971912384
train_iter_loss: 0.16863848268985748
train_iter_loss: 0.25948774814605713
train_iter_loss: 0.15567076206207275
train_iter_loss: 0.2701563239097595
train loss :0.2655
---------------------
Validation seg loss: 0.38596627197035077 at epoch 692
epoch =    693/  1000, exp = train
train_iter_loss: 0.29144570231437683
train_iter_loss: 0.313463419675827
train_iter_loss: 0.16402162611484528
train_iter_loss: 0.3282480239868164
train_iter_loss: 0.2283095270395279
train_iter_loss: 0.22343122959136963
train_iter_loss: 0.3805684745311737
train_iter_loss: 0.3617677390575409
train_iter_loss: 0.3539811670780182
train_iter_loss: 0.09575317054986954
train_iter_loss: 0.26590272784233093
train_iter_loss: 0.32229745388031006
train_iter_loss: 0.29747822880744934
train_iter_loss: 0.20770378410816193
train_iter_loss: 0.1859927624464035
train_iter_loss: 0.22701482474803925
train_iter_loss: 0.3731817603111267
train_iter_loss: 0.2990582585334778
train_iter_loss: 0.1947307288646698
train_iter_loss: 0.23090633749961853
train_iter_loss: 0.32373589277267456
train_iter_loss: 0.35001036524772644
train_iter_loss: 0.34774306416511536
train_iter_loss: 0.12341327220201492
train_iter_loss: 0.3649945557117462
train_iter_loss: 0.1384381204843521
train_iter_loss: 0.20450490713119507
train_iter_loss: 0.18526344001293182
train_iter_loss: 0.21156944334506989
train_iter_loss: 0.2062762975692749
train_iter_loss: 0.17572803795337677
train_iter_loss: 0.2115459442138672
train_iter_loss: 0.13596095144748688
train_iter_loss: 0.47678342461586
train_iter_loss: 0.132835254073143
train_iter_loss: 0.186420738697052
train_iter_loss: 0.1889648139476776
train_iter_loss: 0.29276353120803833
train_iter_loss: 0.15043985843658447
train_iter_loss: 0.21748322248458862
train_iter_loss: 0.21891961991786957
train_iter_loss: 0.18326495587825775
train_iter_loss: 0.36342132091522217
train_iter_loss: 0.2519279420375824
train_iter_loss: 0.42101266980171204
train_iter_loss: 0.25902268290519714
train_iter_loss: 0.2213367521762848
train_iter_loss: 0.2740875780582428
train_iter_loss: 0.22879499197006226
train_iter_loss: 0.4905482828617096
train_iter_loss: 0.3229037821292877
train_iter_loss: 0.21468934416770935
train_iter_loss: 0.33046817779541016
train_iter_loss: 0.26391059160232544
train_iter_loss: 0.07031016796827316
train_iter_loss: 0.22791336476802826
train_iter_loss: 0.22630704939365387
train_iter_loss: 0.38174253702163696
train_iter_loss: 0.3078920841217041
train_iter_loss: 0.3203459084033966
train_iter_loss: 0.261892706155777
train_iter_loss: 0.3042244613170624
train_iter_loss: 0.24134036898612976
train_iter_loss: 0.4042646288871765
train_iter_loss: 0.2667957544326782
train_iter_loss: 0.2289706915616989
train_iter_loss: 0.29125291109085083
train_iter_loss: 0.3408280611038208
train_iter_loss: 0.1751641184091568
train_iter_loss: 0.3362731337547302
train_iter_loss: 0.21056410670280457
train_iter_loss: 0.2984834909439087
train_iter_loss: 0.23288467526435852
train_iter_loss: 0.2550763785839081
train_iter_loss: 0.35756590962409973
train_iter_loss: 0.35978278517723083
train_iter_loss: 0.17326946556568146
train_iter_loss: 0.2468780130147934
train_iter_loss: 0.35671061277389526
train_iter_loss: 0.08010875433683395
train_iter_loss: 0.36588749289512634
train_iter_loss: 0.2782292366027832
train_iter_loss: 0.2194877713918686
train_iter_loss: 0.3077751696109772
train_iter_loss: 0.3092160224914551
train_iter_loss: 0.13585533201694489
train_iter_loss: 0.23009680211544037
train_iter_loss: 0.2003360241651535
train_iter_loss: 0.15844140946865082
train_iter_loss: 0.23640543222427368
train_iter_loss: 0.2697446048259735
train_iter_loss: 0.2678854167461395
train_iter_loss: 0.35156312584877014
train_iter_loss: 0.061419934034347534
train_iter_loss: 0.28138628602027893
train_iter_loss: 0.1911291778087616
train_iter_loss: 0.24851231276988983
train_iter_loss: 0.2721574306488037
train_iter_loss: 0.2416953444480896
train_iter_loss: 0.21774597465991974
train loss :0.2608
---------------------
Validation seg loss: 0.35911959775213925 at epoch 693
epoch =    694/  1000, exp = train
train_iter_loss: 0.1812390238046646
train_iter_loss: 0.3109288811683655
train_iter_loss: 0.2380417138338089
train_iter_loss: 0.1653241068124771
train_iter_loss: 0.310504287481308
train_iter_loss: 0.2527867555618286
train_iter_loss: 0.29590344429016113
train_iter_loss: 0.328021764755249
train_iter_loss: 0.21494661271572113
train_iter_loss: 0.24432720243930817
train_iter_loss: 0.25452303886413574
train_iter_loss: 0.23008359968662262
train_iter_loss: 0.26424720883369446
train_iter_loss: 0.19727753102779388
train_iter_loss: 0.27761682868003845
train_iter_loss: 0.27421852946281433
train_iter_loss: 0.3659309446811676
train_iter_loss: 0.1901310682296753
train_iter_loss: 0.4589103162288666
train_iter_loss: 0.23950311541557312
train_iter_loss: 0.18156854808330536
train_iter_loss: 0.21717515587806702
train_iter_loss: 0.28558987379074097
train_iter_loss: 0.2563639283180237
train_iter_loss: 0.5041486620903015
train_iter_loss: 0.17542368173599243
train_iter_loss: 0.21165303885936737
train_iter_loss: 0.3498777747154236
train_iter_loss: 0.3269273340702057
train_iter_loss: 0.18777328729629517
train_iter_loss: 0.3244136571884155
train_iter_loss: 0.3114684224128723
train_iter_loss: 0.1708526313304901
train_iter_loss: 0.14821380376815796
train_iter_loss: 0.2498529553413391
train_iter_loss: 0.21979697048664093
train_iter_loss: 0.2638688385486603
train_iter_loss: 0.2621922194957733
train_iter_loss: 0.2240675389766693
train_iter_loss: 0.17978939414024353
train_iter_loss: 0.2623945474624634
train_iter_loss: 0.27102357149124146
train_iter_loss: 0.2601154148578644
train_iter_loss: 0.17715351283550262
train_iter_loss: 0.27206045389175415
train_iter_loss: 0.35810962319374084
train_iter_loss: 0.3714101314544678
train_iter_loss: 0.24569235742092133
train_iter_loss: 0.25812116265296936
train_iter_loss: 0.25591737031936646
train_iter_loss: 0.2445659637451172
train_iter_loss: 0.1524772197008133
train_iter_loss: 0.22434189915657043
train_iter_loss: 0.18111303448677063
train_iter_loss: 0.30386027693748474
train_iter_loss: 0.3627091348171234
train_iter_loss: 0.25050896406173706
train_iter_loss: 0.27778205275535583
train_iter_loss: 0.12724138796329498
train_iter_loss: 0.24309629201889038
train_iter_loss: 0.42001959681510925
train_iter_loss: 0.30414271354675293
train_iter_loss: 0.216800257563591
train_iter_loss: 0.2308848649263382
train_iter_loss: 0.32592061161994934
train_iter_loss: 0.36472222208976746
train_iter_loss: 0.23719508945941925
train_iter_loss: 0.289502888917923
train_iter_loss: 0.20077170431613922
train_iter_loss: 0.29253318905830383
train_iter_loss: 0.16883686184883118
train_iter_loss: 0.15641266107559204
train_iter_loss: 0.23323646187782288
train_iter_loss: 0.26150625944137573
train_iter_loss: 0.3204798996448517
train_iter_loss: 0.18480899930000305
train_iter_loss: 0.32063356041908264
train_iter_loss: 0.2628070116043091
train_iter_loss: 0.3587815761566162
train_iter_loss: 0.1767899990081787
train_iter_loss: 0.3891123831272125
train_iter_loss: 0.4736124575138092
train_iter_loss: 0.31842103600502014
train_iter_loss: 0.2719196677207947
train_iter_loss: 0.2027779072523117
train_iter_loss: 0.19838860630989075
train_iter_loss: 0.39964741468429565
train_iter_loss: 0.32525140047073364
train_iter_loss: 0.24953851103782654
train_iter_loss: 0.25478890538215637
train_iter_loss: 0.10353336483240128
train_iter_loss: 0.23400576412677765
train_iter_loss: 0.14133909344673157
train_iter_loss: 0.4650556147098541
train_iter_loss: 0.27590012550354004
train_iter_loss: 0.30841559171676636
train_iter_loss: 0.17082440853118896
train_iter_loss: 0.3588138818740845
train_iter_loss: 0.31464534997940063
train_iter_loss: 0.08895576745271683
train loss :0.2658
---------------------
Validation seg loss: 0.3740589931290948 at epoch 694
epoch =    695/  1000, exp = train
train_iter_loss: 0.2795509696006775
train_iter_loss: 0.2572823464870453
train_iter_loss: 0.3106658160686493
train_iter_loss: 0.32909083366394043
train_iter_loss: 0.14438201487064362
train_iter_loss: 0.3513268232345581
train_iter_loss: 0.426480233669281
train_iter_loss: 0.1920509934425354
train_iter_loss: 0.25586506724357605
train_iter_loss: 0.14957576990127563
train_iter_loss: 0.18842056393623352
train_iter_loss: 0.23249328136444092
train_iter_loss: 0.3604986369609833
train_iter_loss: 0.34531548619270325
train_iter_loss: 0.17745396494865417
train_iter_loss: 0.3089950680732727
train_iter_loss: 0.2859632968902588
train_iter_loss: 0.29866787791252136
train_iter_loss: 0.29733413457870483
train_iter_loss: 0.10939595848321915
train_iter_loss: 0.2175585776567459
train_iter_loss: 0.4503301978111267
train_iter_loss: 0.36096319556236267
train_iter_loss: 0.1949673593044281
train_iter_loss: 0.21962377429008484
train_iter_loss: 0.23701708018779755
train_iter_loss: 0.45842471718788147
train_iter_loss: 0.1784459352493286
train_iter_loss: 0.2297624796628952
train_iter_loss: 0.21545471251010895
train_iter_loss: 0.2689770758152008
train_iter_loss: 0.0771629586815834
train_iter_loss: 0.28930243849754333
train_iter_loss: 0.17202727496623993
train_iter_loss: 0.31002435088157654
train_iter_loss: 0.37237316370010376
train_iter_loss: 0.19571149349212646
train_iter_loss: 0.2853430211544037
train_iter_loss: 0.2051018923521042
train_iter_loss: 0.24898605048656464
train_iter_loss: 0.18434318900108337
train_iter_loss: 0.3008781671524048
train_iter_loss: 0.1390713006258011
train_iter_loss: 0.30513909459114075
train_iter_loss: 0.20078317821025848
train_iter_loss: 0.23513054847717285
train_iter_loss: 0.2850256562232971
train_iter_loss: 0.17019499838352203
train_iter_loss: 0.11487249284982681
train_iter_loss: 0.36051151156425476
train_iter_loss: 0.145241841673851
train_iter_loss: 0.2227429747581482
train_iter_loss: 0.3437541127204895
train_iter_loss: 0.26930180191993713
train_iter_loss: 0.28264501690864563
train_iter_loss: 0.0825369656085968
train_iter_loss: 0.23842911422252655
train_iter_loss: 0.23087161779403687
train_iter_loss: 0.28270864486694336
train_iter_loss: 0.2849303185939789
train_iter_loss: 0.3171188235282898
train_iter_loss: 0.20284076035022736
train_iter_loss: 0.26641353964805603
train_iter_loss: 0.3344157636165619
train_iter_loss: 0.2792093753814697
train_iter_loss: 0.03192838281393051
train_iter_loss: 0.33875274658203125
train_iter_loss: 0.30733004212379456
train_iter_loss: 0.15314914286136627
train_iter_loss: 0.12895932793617249
train_iter_loss: 0.23336626589298248
train_iter_loss: 0.21468347311019897
train_iter_loss: 0.24814251065254211
train_iter_loss: 0.3410700857639313
train_iter_loss: 0.2901022732257843
train_iter_loss: 0.33764076232910156
train_iter_loss: 0.35986676812171936
train_iter_loss: 0.2453424036502838
train_iter_loss: 0.21006618440151215
train_iter_loss: 0.18344298005104065
train_iter_loss: 0.22629311680793762
train_iter_loss: 0.1448362171649933
train_iter_loss: 0.2951725423336029
train_iter_loss: 0.24384145438671112
train_iter_loss: 0.4526882767677307
train_iter_loss: 0.13365432620048523
train_iter_loss: 0.13871711492538452
train_iter_loss: 0.44023290276527405
train_iter_loss: 0.28399011492729187
train_iter_loss: 0.35385647416114807
train_iter_loss: 0.2879519462585449
train_iter_loss: 0.3176936209201813
train_iter_loss: 0.36586615443229675
train_iter_loss: 0.3070492446422577
train_iter_loss: 0.32233282923698425
train_iter_loss: 0.25628113746643066
train_iter_loss: 0.3773975074291229
train_iter_loss: 0.1887543797492981
train_iter_loss: 0.1721011847257614
train_iter_loss: 0.35365691781044006
train loss :0.2612
---------------------
Validation seg loss: 0.3670287777599439 at epoch 695
epoch =    696/  1000, exp = train
train_iter_loss: 0.2243073582649231
train_iter_loss: 0.21649684011936188
train_iter_loss: 0.4174439013004303
train_iter_loss: 0.22174401581287384
train_iter_loss: 0.313175767660141
train_iter_loss: 0.09127175807952881
train_iter_loss: 0.3929586112499237
train_iter_loss: 0.27378618717193604
train_iter_loss: 0.3162583112716675
train_iter_loss: 0.01942726969718933
train_iter_loss: 0.12639503180980682
train_iter_loss: 0.30888405442237854
train_iter_loss: 0.4030035138130188
train_iter_loss: 0.2695411741733551
train_iter_loss: 0.0699077844619751
train_iter_loss: 0.32729554176330566
train_iter_loss: 0.4402218163013458
train_iter_loss: 0.45705339312553406
train_iter_loss: 0.2543300986289978
train_iter_loss: 0.30012497305870056
train_iter_loss: 0.2832975387573242
train_iter_loss: 0.2700057029724121
train_iter_loss: 0.3184940218925476
train_iter_loss: 0.16395200788974762
train_iter_loss: 0.21988609433174133
train_iter_loss: 0.18879146873950958
train_iter_loss: 0.24387584626674652
train_iter_loss: 0.22379465401172638
train_iter_loss: 0.15168379247188568
train_iter_loss: 0.20777203142642975
train_iter_loss: 0.12268661707639694
train_iter_loss: 0.26727595925331116
train_iter_loss: 0.36376821994781494
train_iter_loss: 0.20360170304775238
train_iter_loss: 0.5637016296386719
train_iter_loss: 0.3328671455383301
train_iter_loss: 0.4385821521282196
train_iter_loss: 0.3334197998046875
train_iter_loss: 0.24500496685504913
train_iter_loss: 0.18824777007102966
train_iter_loss: 0.15664519369602203
train_iter_loss: 0.2745952904224396
train_iter_loss: 0.47674816846847534
train_iter_loss: 0.29078638553619385
train_iter_loss: 0.38707655668258667
train_iter_loss: 0.4083864986896515
train_iter_loss: 0.184358611702919
train_iter_loss: 0.4452686905860901
train_iter_loss: 0.24043962359428406
train_iter_loss: 0.21604497730731964
train_iter_loss: 0.23725037276744843
train_iter_loss: 0.27831611037254333
train_iter_loss: 0.2859956622123718
train_iter_loss: 0.2305625081062317
train_iter_loss: 0.35382726788520813
train_iter_loss: 0.1348956972360611
train_iter_loss: 0.3363693654537201
train_iter_loss: 0.1770068258047104
train_iter_loss: 0.3295225203037262
train_iter_loss: 0.2074478268623352
train_iter_loss: 0.3107692301273346
train_iter_loss: 0.1750355213880539
train_iter_loss: 0.19295021891593933
train_iter_loss: 0.1525152027606964
train_iter_loss: 0.2707754969596863
train_iter_loss: 0.17933844029903412
train_iter_loss: 0.15370362997055054
train_iter_loss: 0.25840890407562256
train_iter_loss: 0.37328484654426575
train_iter_loss: 0.37950989603996277
train_iter_loss: 0.1916380226612091
train_iter_loss: 0.17565056681632996
train_iter_loss: 0.28650468587875366
train_iter_loss: 0.18218111991882324
train_iter_loss: 0.3199465870857239
train_iter_loss: 0.2205582559108734
train_iter_loss: 0.2338728904724121
train_iter_loss: 0.28029897809028625
train_iter_loss: 0.2565399706363678
train_iter_loss: 0.19382621347904205
train_iter_loss: 0.31218746304512024
train_iter_loss: 0.2872418761253357
train_iter_loss: 0.33394500613212585
train_iter_loss: 0.23136058449745178
train_iter_loss: 0.17822004854679108
train_iter_loss: 0.2739943861961365
train_iter_loss: 0.1732378453016281
train_iter_loss: 0.2288176566362381
train_iter_loss: 0.07272151857614517
train_iter_loss: 0.15560144186019897
train_iter_loss: 0.3601449131965637
train_iter_loss: 0.23606018722057343
train_iter_loss: 0.13884270191192627
train_iter_loss: 0.37167462706565857
train_iter_loss: 0.30953291058540344
train_iter_loss: 0.3928585648536682
train_iter_loss: 0.20445241034030914
train_iter_loss: 0.3701731562614441
train_iter_loss: 0.23032279312610626
train_iter_loss: 0.149840846657753
train loss :0.2649
---------------------
Validation seg loss: 0.3664306281222347 at epoch 696
epoch =    697/  1000, exp = train
train_iter_loss: 0.30892905592918396
train_iter_loss: 0.15521927177906036
train_iter_loss: 0.30396804213523865
train_iter_loss: 0.2784044146537781
train_iter_loss: 0.11253620684146881
train_iter_loss: 0.26494964957237244
train_iter_loss: 0.23814770579338074
train_iter_loss: 0.3798424303531647
train_iter_loss: 0.38099682331085205
train_iter_loss: 0.4752640128135681
train_iter_loss: 0.3798576295375824
train_iter_loss: 0.22623001039028168
train_iter_loss: 0.20165246725082397
train_iter_loss: 0.2347377985715866
train_iter_loss: 0.1953495740890503
train_iter_loss: 0.31206128001213074
train_iter_loss: 0.17656269669532776
train_iter_loss: 0.12758590281009674
train_iter_loss: 0.2230798304080963
train_iter_loss: 0.2295583337545395
train_iter_loss: 0.30574461817741394
train_iter_loss: 0.28195539116859436
train_iter_loss: 0.31645840406417847
train_iter_loss: 0.2907121777534485
train_iter_loss: 0.17289124429225922
train_iter_loss: 0.20286227762699127
train_iter_loss: 0.3128924071788788
train_iter_loss: 0.06715984642505646
train_iter_loss: 0.15733464062213898
train_iter_loss: 0.11987555772066116
train_iter_loss: 0.1860615611076355
train_iter_loss: 0.2704811692237854
train_iter_loss: 0.3266107141971588
train_iter_loss: 0.3032814562320709
train_iter_loss: 0.36373797059059143
train_iter_loss: 0.2691810727119446
train_iter_loss: 0.13263987004756927
train_iter_loss: 0.2739866077899933
train_iter_loss: 0.15059912204742432
train_iter_loss: 0.18228451907634735
train_iter_loss: 0.28666529059410095
train_iter_loss: 0.40445780754089355
train_iter_loss: 0.4626895785331726
train_iter_loss: 0.3869938552379608
train_iter_loss: 0.3136333227157593
train_iter_loss: 0.16331008076667786
train_iter_loss: 0.33037805557250977
train_iter_loss: 0.2735072374343872
train_iter_loss: 0.2273728847503662
train_iter_loss: 0.2970844507217407
train_iter_loss: 0.14869147539138794
train_iter_loss: 0.14815469086170197
train_iter_loss: 0.290303498506546
train_iter_loss: 0.23444189131259918
train_iter_loss: 0.23096002638339996
train_iter_loss: 0.25740480422973633
train_iter_loss: 0.2962344288825989
train_iter_loss: 0.31142738461494446
train_iter_loss: 0.29666611552238464
train_iter_loss: 0.31583699584007263
train_iter_loss: 0.12772928178310394
train_iter_loss: 0.32734906673431396
train_iter_loss: 0.30829140543937683
train_iter_loss: 0.22626648843288422
train_iter_loss: 0.2887214720249176
train_iter_loss: 0.18306802213191986
train_iter_loss: 0.28693899512290955
train_iter_loss: 0.22305786609649658
train_iter_loss: 0.21366290748119354
train_iter_loss: 0.30000442266464233
train_iter_loss: 0.37077003717422485
train_iter_loss: 0.287234902381897
train_iter_loss: 0.2891407310962677
train_iter_loss: 0.19054044783115387
train_iter_loss: 0.28475421667099
train_iter_loss: 0.13526424765586853
train_iter_loss: 0.3204066753387451
train_iter_loss: 0.2760518789291382
train_iter_loss: 0.37181150913238525
train_iter_loss: 0.19958777725696564
train_iter_loss: 0.2640305757522583
train_iter_loss: 0.31935518980026245
train_iter_loss: 0.15079721808433533
train_iter_loss: 0.18929967284202576
train_iter_loss: 0.5269062519073486
train_iter_loss: 0.2732371687889099
train_iter_loss: 0.114395871758461
train_iter_loss: 0.22061720490455627
train_iter_loss: 0.3538178503513336
train_iter_loss: 0.20434468984603882
train_iter_loss: 0.30002695322036743
train_iter_loss: 0.2785329520702362
train_iter_loss: 0.27779752016067505
train_iter_loss: 0.16478905081748962
train_iter_loss: 0.3740118443965912
train_iter_loss: 0.16012196242809296
train_iter_loss: 0.26303282380104065
train_iter_loss: 0.3872706890106201
train_iter_loss: 0.20274049043655396
train_iter_loss: 0.3079899251461029
train loss :0.2638
---------------------
Validation seg loss: 0.34619105153910396 at epoch 697
epoch =    698/  1000, exp = train
train_iter_loss: 0.24379536509513855
train_iter_loss: 0.37949010729789734
train_iter_loss: 0.32359808683395386
train_iter_loss: 0.4475575089454651
train_iter_loss: 0.3097771406173706
train_iter_loss: 0.4622548520565033
train_iter_loss: 0.21534213423728943
train_iter_loss: 0.19129858911037445
train_iter_loss: 0.27845272421836853
train_iter_loss: 0.18857651948928833
train_iter_loss: 0.17820025980472565
train_iter_loss: 0.1960299164056778
train_iter_loss: 0.26097530126571655
train_iter_loss: 0.3922678828239441
train_iter_loss: 0.1747247725725174
train_iter_loss: 0.19118531048297882
train_iter_loss: 0.42290809750556946
train_iter_loss: 0.28074291348457336
train_iter_loss: 0.13413278758525848
train_iter_loss: 0.24370263516902924
train_iter_loss: 0.3504543900489807
train_iter_loss: 0.2947944402694702
train_iter_loss: 0.20201376080513
train_iter_loss: 0.23334869742393494
train_iter_loss: 0.3599458932876587
train_iter_loss: 0.13842438161373138
train_iter_loss: 0.19495229423046112
train_iter_loss: 0.23203526437282562
train_iter_loss: 0.1833016276359558
train_iter_loss: 0.22536981105804443
train_iter_loss: 0.1831090748310089
train_iter_loss: 0.21513180434703827
train_iter_loss: 0.2164451777935028
train_iter_loss: 0.32416072487831116
train_iter_loss: 0.35593312978744507
train_iter_loss: 0.2770029306411743
train_iter_loss: 0.2356380969285965
train_iter_loss: 0.15520910918712616
train_iter_loss: 0.19896000623703003
train_iter_loss: 0.37096866965293884
train_iter_loss: 0.3455098271369934
train_iter_loss: 0.2509651184082031
train_iter_loss: 0.27819621562957764
train_iter_loss: 0.09665002673864365
train_iter_loss: 0.1854884773492813
train_iter_loss: 0.25238555669784546
train_iter_loss: 0.26849815249443054
train_iter_loss: 0.3908783793449402
train_iter_loss: 0.26603448390960693
train_iter_loss: 0.27147194743156433
train_iter_loss: 0.32763513922691345
train_iter_loss: 0.3668422996997833
train_iter_loss: 0.27507543563842773
train_iter_loss: 0.2868685722351074
train_iter_loss: 0.2305551916360855
train_iter_loss: 0.16317220032215118
train_iter_loss: 0.22063498198986053
train_iter_loss: 0.1198422834277153
train_iter_loss: 0.2584720551967621
train_iter_loss: 0.27903953194618225
train_iter_loss: 0.20126059651374817
train_iter_loss: 0.30472657084465027
train_iter_loss: 0.30968695878982544
train_iter_loss: 0.42235052585601807
train_iter_loss: 0.20875844359397888
train_iter_loss: 0.18825657665729523
train_iter_loss: 0.16770264506340027
train_iter_loss: 0.2757384479045868
train_iter_loss: 0.21414066851139069
train_iter_loss: 0.20676136016845703
train_iter_loss: 0.26071223616600037
train_iter_loss: 0.23672206699848175
train_iter_loss: 0.35053250193595886
train_iter_loss: 0.2437773048877716
train_iter_loss: 0.2152380645275116
train_iter_loss: 0.4113040566444397
train_iter_loss: 0.1903066635131836
train_iter_loss: 0.2683025300502777
train_iter_loss: 0.26313772797584534
train_iter_loss: 0.41403257846832275
train_iter_loss: 0.31828922033309937
train_iter_loss: 0.1424729973077774
train_iter_loss: 0.30389443039894104
train_iter_loss: 0.153667151927948
train_iter_loss: 0.20181617140769958
train_iter_loss: 0.18650370836257935
train_iter_loss: 0.4232979714870453
train_iter_loss: 0.28264307975769043
train_iter_loss: 0.19321191310882568
train_iter_loss: 0.28393101692199707
train_iter_loss: 0.34822532534599304
train_iter_loss: 0.16613711416721344
train_iter_loss: 0.1917807161808014
train_iter_loss: 0.3277413845062256
train_iter_loss: 0.16044943034648895
train_iter_loss: 0.3150671422481537
train_iter_loss: 0.17376963794231415
train_iter_loss: 0.26860618591308594
train_iter_loss: 0.3609679341316223
train_iter_loss: 0.3005449175834656
train loss :0.2632
---------------------
Validation seg loss: 0.3722813811618835 at epoch 698
epoch =    699/  1000, exp = train
train_iter_loss: 0.2623489499092102
train_iter_loss: 0.1349620670080185
train_iter_loss: 0.33015361428260803
train_iter_loss: 0.3220207691192627
train_iter_loss: 0.21790853142738342
train_iter_loss: 0.10998378694057465
train_iter_loss: 0.30013716220855713
train_iter_loss: 0.18274444341659546
train_iter_loss: 0.13202522695064545
train_iter_loss: 0.16646277904510498
train_iter_loss: 0.28463107347488403
train_iter_loss: 0.16805386543273926
train_iter_loss: 0.23373273015022278
train_iter_loss: 0.2813658118247986
train_iter_loss: 0.22594664990901947
train_iter_loss: 0.22977827489376068
train_iter_loss: 0.22079408168792725
train_iter_loss: 0.3496021330356598
train_iter_loss: 0.3685130178928375
train_iter_loss: 0.3393254578113556
train_iter_loss: 0.35406625270843506
train_iter_loss: 0.2675668001174927
train_iter_loss: 0.2957583963871002
train_iter_loss: 0.26178985834121704
train_iter_loss: 0.42077845335006714
train_iter_loss: 0.27244651317596436
train_iter_loss: 0.230014368891716
train_iter_loss: 0.2687744200229645
train_iter_loss: 0.33971112966537476
train_iter_loss: 0.27384111285209656
train_iter_loss: 0.17904771864414215
train_iter_loss: 0.26718002557754517
train_iter_loss: 0.2443108856678009
train_iter_loss: 0.2735333740711212
train_iter_loss: 0.1028522402048111
train_iter_loss: 0.2834136486053467
train_iter_loss: 0.14299744367599487
train_iter_loss: 0.3048344552516937
train_iter_loss: 0.031710173934698105
train_iter_loss: 0.19277621805667877
train_iter_loss: 0.16783910989761353
train_iter_loss: 0.1283816546201706
train_iter_loss: 0.25578439235687256
train_iter_loss: 0.24904346466064453
train_iter_loss: 0.42947372794151306
train_iter_loss: 0.3491734266281128
train_iter_loss: 0.2313871681690216
train_iter_loss: 0.2429870218038559
train_iter_loss: 0.2220829725265503
train_iter_loss: 0.2055339366197586
train_iter_loss: 0.3288348615169525
train_iter_loss: 0.37547120451927185
train_iter_loss: 0.4630966782569885
train_iter_loss: 0.3965434730052948
train_iter_loss: 0.28314608335494995
train_iter_loss: 0.2998901903629303
train_iter_loss: 0.4209626317024231
train_iter_loss: 0.3246939778327942
train_iter_loss: 0.24490022659301758
train_iter_loss: 0.4471697211265564
train_iter_loss: 0.306633859872818
train_iter_loss: 0.311743825674057
train_iter_loss: 0.24660125374794006
train_iter_loss: 0.20514142513275146
train_iter_loss: 0.2500351667404175
train_iter_loss: 0.28958263993263245
train_iter_loss: 0.2762448787689209
train_iter_loss: 0.16989976167678833
train_iter_loss: 0.3466557562351227
train_iter_loss: 0.46810540556907654
train_iter_loss: 0.30220022797584534
train_iter_loss: 0.10922179371118546
train_iter_loss: 0.24812832474708557
train_iter_loss: 0.08084546774625778
train_iter_loss: 0.23376716673374176
train_iter_loss: 0.22333097457885742
train_iter_loss: 0.24393853545188904
train_iter_loss: 0.2069062441587448
train_iter_loss: 0.26875242590904236
train_iter_loss: 0.32120418548583984
train_iter_loss: 0.19314536452293396
train_iter_loss: 0.35066771507263184
train_iter_loss: 0.2755882740020752
train_iter_loss: 0.36453136801719666
train_iter_loss: 0.2644464373588562
train_iter_loss: 0.27240532636642456
train_iter_loss: 0.28926095366477966
train_iter_loss: 0.25623083114624023
train_iter_loss: 0.23362746834754944
train_iter_loss: 0.29589012265205383
train_iter_loss: 0.33091726899147034
train_iter_loss: 0.21595197916030884
train_iter_loss: 0.23503080010414124
train_iter_loss: 0.23361581563949585
train_iter_loss: 0.22056925296783447
train_iter_loss: 0.37035176157951355
train_iter_loss: 0.2701896131038666
train_iter_loss: 0.24297431111335754
train_iter_loss: 0.3777565658092499
train_iter_loss: 0.4698478877544403
train loss :0.2707
---------------------
Validation seg loss: 0.3655002488368103 at epoch 699
epoch =    700/  1000, exp = train
train_iter_loss: 0.19953499734401703
train_iter_loss: 0.27278321981430054
train_iter_loss: 0.2511390745639801
train_iter_loss: 0.23688894510269165
train_iter_loss: 0.2840796709060669
train_iter_loss: 0.33648622035980225
train_iter_loss: 0.024435462430119514
train_iter_loss: 0.33880558609962463
train_iter_loss: 0.3268143832683563
train_iter_loss: 0.1437765657901764
train_iter_loss: 0.18057051301002502
train_iter_loss: 0.20532304048538208
train_iter_loss: 0.10515956580638885
train_iter_loss: 0.35142970085144043
train_iter_loss: 0.3181270658969879
train_iter_loss: 0.22751396894454956
train_iter_loss: 0.31710895895957947
train_iter_loss: 0.17648693919181824
train_iter_loss: 0.24153095483779907
train_iter_loss: 0.2695673108100891
train_iter_loss: 0.25448307394981384
train_iter_loss: 0.1964958757162094
train_iter_loss: 0.24205142259597778
train_iter_loss: 0.2700064182281494
train_iter_loss: 0.42657071352005005
train_iter_loss: 0.35334140062332153
train_iter_loss: 0.25264832377433777
train_iter_loss: 0.13102903962135315
train_iter_loss: 0.2336375117301941
train_iter_loss: 0.20509257912635803
train_iter_loss: 0.32299187779426575
train_iter_loss: 0.35997694730758667
train_iter_loss: 0.4120860993862152
train_iter_loss: 0.3377290666103363
train_iter_loss: 0.28624215722084045
train_iter_loss: 0.2816067636013031
train_iter_loss: 0.29404470324516296
train_iter_loss: 0.5158132314682007
train_iter_loss: 0.1470971256494522
train_iter_loss: 0.3035184442996979
train_iter_loss: 0.29385489225387573
train_iter_loss: 0.10887724161148071
train_iter_loss: 0.3020437955856323
train_iter_loss: 0.21926698088645935
train_iter_loss: 0.3057638108730316
train_iter_loss: 0.24910618364810944
train_iter_loss: 0.03646925464272499
train_iter_loss: 0.1789843887090683
train_iter_loss: 0.3118954300880432
train_iter_loss: 0.38680848479270935
train_iter_loss: 0.20070329308509827
train_iter_loss: 0.4067474603652954
train_iter_loss: 0.3835467994213104
train_iter_loss: 0.2351575791835785
train_iter_loss: 0.23021940886974335
train_iter_loss: 0.2970607578754425
train_iter_loss: 0.2949957251548767
train_iter_loss: 0.12937332689762115
train_iter_loss: 0.13765737414360046
train_iter_loss: 0.31049489974975586
train_iter_loss: 0.26412707567214966
train_iter_loss: 0.24767589569091797
train_iter_loss: 0.32069718837738037
train_iter_loss: 0.3267364203929901
train_iter_loss: 0.2964363992214203
train_iter_loss: 0.23558957874774933
train_iter_loss: 0.29718804359436035
train_iter_loss: 0.2780997157096863
train_iter_loss: 0.26410701870918274
train_iter_loss: 0.3841795325279236
train_iter_loss: 0.313978910446167
train_iter_loss: 0.33815065026283264
train_iter_loss: 0.22904618084430695
train_iter_loss: 0.2364843338727951
train_iter_loss: 0.23306626081466675
train_iter_loss: 0.3824586868286133
train_iter_loss: 0.2953847050666809
train_iter_loss: 0.3548974394798279
train_iter_loss: 0.29904451966285706
train_iter_loss: 0.3429170846939087
train_iter_loss: 0.19141589105129242
train_iter_loss: 0.17281149327754974
train_iter_loss: 0.13954158127307892
train_iter_loss: 0.21807444095611572
train_iter_loss: 0.21183423697948456
train_iter_loss: 0.3247963488101959
train_iter_loss: 0.09073301404714584
train_iter_loss: 0.34285974502563477
train_iter_loss: 0.13821277022361755
train_iter_loss: 0.23758383095264435
train_iter_loss: 0.3062194585800171
train_iter_loss: 0.11674442887306213
train_iter_loss: 0.17018862068653107
train_iter_loss: 0.17527960240840912
train_iter_loss: 0.3539727032184601
train_iter_loss: 0.112542524933815
train_iter_loss: 0.1825975626707077
train_iter_loss: 0.34248417615890503
train_iter_loss: 0.25061389803886414
train_iter_loss: 0.30497196316719055
train loss :0.2625
---------------------
Validation seg loss: 0.3929733229748342 at epoch 700
epoch =    701/  1000, exp = train
train_iter_loss: 0.27546536922454834
train_iter_loss: 0.2996928095817566
train_iter_loss: 0.19626815617084503
train_iter_loss: 0.07620756328105927
train_iter_loss: 0.19864793121814728
train_iter_loss: 0.22440309822559357
train_iter_loss: 0.2706703841686249
train_iter_loss: 0.2665087878704071
train_iter_loss: 0.23832571506500244
train_iter_loss: 0.21861223876476288
train_iter_loss: 0.2958270013332367
train_iter_loss: 0.40252089500427246
train_iter_loss: 0.2513105273246765
train_iter_loss: 0.24345222115516663
train_iter_loss: 0.2519192397594452
train_iter_loss: 0.29931753873825073
train_iter_loss: 0.3777002990245819
train_iter_loss: 0.22682106494903564
train_iter_loss: 0.2661946415901184
train_iter_loss: 0.2925075888633728
train_iter_loss: 0.20995953679084778
train_iter_loss: 0.4242415428161621
train_iter_loss: 0.3028511703014374
train_iter_loss: 0.3138464689254761
train_iter_loss: 0.17029501497745514
train_iter_loss: 0.17678219079971313
train_iter_loss: 0.37269315123558044
train_iter_loss: 0.29680007696151733
train_iter_loss: 0.3406614065170288
train_iter_loss: 0.41009432077407837
train_iter_loss: 0.3575798273086548
train_iter_loss: 0.22702504694461823
train_iter_loss: 0.18969698250293732
train_iter_loss: 0.1589624434709549
train_iter_loss: 0.4213079810142517
train_iter_loss: 0.29410380125045776
train_iter_loss: 0.27787041664123535
train_iter_loss: 0.18271763622760773
train_iter_loss: 0.21016168594360352
train_iter_loss: 0.30449506640434265
train_iter_loss: 0.29259225726127625
train_iter_loss: 0.1759689897298813
train_iter_loss: 0.30876126885414124
train_iter_loss: 0.27837100625038147
train_iter_loss: 0.23645827174186707
train_iter_loss: 0.20713315904140472
train_iter_loss: 0.26841044425964355
train_iter_loss: 0.21932634711265564
train_iter_loss: 0.2899087965488434
train_iter_loss: 0.16584493219852448
train_iter_loss: 0.27130281925201416
train_iter_loss: 0.3587345778942108
train_iter_loss: 0.12723296880722046
train_iter_loss: 0.237699955701828
train_iter_loss: 0.4931107759475708
train_iter_loss: 0.17057032883167267
train_iter_loss: 0.3652159869670868
train_iter_loss: 0.23479291796684265
train_iter_loss: 0.23953720927238464
train_iter_loss: 0.3193780481815338
train_iter_loss: 0.44447293877601624
train_iter_loss: 0.23141402006149292
train_iter_loss: 0.2669046223163605
train_iter_loss: 0.30264419317245483
train_iter_loss: 0.18698063492774963
train_iter_loss: 0.16416072845458984
train_iter_loss: 0.08624816685914993
train_iter_loss: 0.4722636044025421
train_iter_loss: 0.5516555905342102
train_iter_loss: 0.39206960797309875
train_iter_loss: 0.21570830047130585
train_iter_loss: 0.1291496604681015
train_iter_loss: 0.16833198070526123
train_iter_loss: 0.18235674500465393
train_iter_loss: 0.22508592903614044
train_iter_loss: 0.30948662757873535
train_iter_loss: 0.14384828507900238
train_iter_loss: 0.15792955458164215
train_iter_loss: 0.06739991158246994
train_iter_loss: 0.18374398350715637
train_iter_loss: 0.18014194071292877
train_iter_loss: 0.25435182452201843
train_iter_loss: 0.2825556695461273
train_iter_loss: 0.3584253787994385
train_iter_loss: 0.16986972093582153
train_iter_loss: 0.2154483199119568
train_iter_loss: 0.035799767822027206
train_iter_loss: 0.25541070103645325
train_iter_loss: 0.4301348924636841
train_iter_loss: 0.19434018433094025
train_iter_loss: 0.329470694065094
train_iter_loss: 0.20303811132907867
train_iter_loss: 0.2677564024925232
train_iter_loss: 0.3277393579483032
train_iter_loss: 0.13762341439723969
train_iter_loss: 0.09346310794353485
train_iter_loss: 0.33675411343574524
train_iter_loss: 0.21626007556915283
train_iter_loss: 0.22423358261585236
train_iter_loss: 0.3499113917350769
train loss :0.2608
---------------------
Validation seg loss: 0.3458908205887057 at epoch 701
epoch =    702/  1000, exp = train
train_iter_loss: 0.2421570122241974
train_iter_loss: 0.28494754433631897
train_iter_loss: 0.19119851291179657
train_iter_loss: 0.36285659670829773
train_iter_loss: 0.17966869473457336
train_iter_loss: 0.24555760622024536
train_iter_loss: 0.4934929609298706
train_iter_loss: 0.4473773241043091
train_iter_loss: 0.19207629561424255
train_iter_loss: 0.2559095323085785
train_iter_loss: 0.20612241327762604
train_iter_loss: 0.20559784770011902
train_iter_loss: 0.4211725890636444
train_iter_loss: 0.27740585803985596
train_iter_loss: 0.29756230115890503
train_iter_loss: 0.13650375604629517
train_iter_loss: 0.26342588663101196
train_iter_loss: 0.3418331444263458
train_iter_loss: 0.17852705717086792
train_iter_loss: 0.21113471686840057
train_iter_loss: 0.27874231338500977
train_iter_loss: 0.22276048362255096
train_iter_loss: 0.21958549320697784
train_iter_loss: 0.33903267979621887
train_iter_loss: 0.29532286524772644
train_iter_loss: 0.1824597865343094
train_iter_loss: 0.22225938737392426
train_iter_loss: 0.20097529888153076
train_iter_loss: 0.18696504831314087
train_iter_loss: 0.4375837743282318
train_iter_loss: 0.29203441739082336
train_iter_loss: 0.3581652045249939
train_iter_loss: 0.2864513695240021
train_iter_loss: 0.25501060485839844
train_iter_loss: 0.2638540267944336
train_iter_loss: 0.35847651958465576
train_iter_loss: 0.425443559885025
train_iter_loss: 0.20079150795936584
train_iter_loss: 0.4585490822792053
train_iter_loss: 0.20904403924942017
train_iter_loss: 0.25199803709983826
train_iter_loss: 0.25047120451927185
train_iter_loss: 0.14923079311847687
train_iter_loss: 0.2884710133075714
train_iter_loss: 0.24899017810821533
train_iter_loss: 0.19826169312000275
train_iter_loss: 0.19527775049209595
train_iter_loss: 0.2754865288734436
train_iter_loss: 0.3877227008342743
train_iter_loss: 0.34184834361076355
train_iter_loss: 0.2177005112171173
train_iter_loss: 0.17644920945167542
train_iter_loss: 0.20257169008255005
train_iter_loss: 0.26845383644104004
train_iter_loss: 0.21485701203346252
train_iter_loss: 0.21961547434329987
train_iter_loss: 0.4056999981403351
train_iter_loss: 0.2539691925048828
train_iter_loss: 0.2961561381816864
train_iter_loss: 0.3550451993942261
train_iter_loss: 0.42233195900917053
train_iter_loss: 0.2619515657424927
train_iter_loss: 0.2291446030139923
train_iter_loss: 0.266202449798584
train_iter_loss: 0.13638761639595032
train_iter_loss: 0.41147178411483765
train_iter_loss: 0.2741526961326599
train_iter_loss: 0.2864442467689514
train_iter_loss: 0.16885702311992645
train_iter_loss: 0.29668283462524414
train_iter_loss: 0.3960855007171631
train_iter_loss: 0.14842110872268677
train_iter_loss: 0.18484826385974884
train_iter_loss: 0.44478514790534973
train_iter_loss: 0.2753720283508301
train_iter_loss: 0.16128338873386383
train_iter_loss: 0.30843809247016907
train_iter_loss: 0.27793270349502563
train_iter_loss: 0.34953880310058594
train_iter_loss: 0.3346327841281891
train_iter_loss: 0.15309560298919678
train_iter_loss: 0.26301294565200806
train_iter_loss: 0.09404026716947556
train_iter_loss: 0.1644674837589264
train_iter_loss: 0.22833998501300812
train_iter_loss: 0.20792417228221893
train_iter_loss: 0.31846755743026733
train_iter_loss: 0.0988534539937973
train_iter_loss: 0.3068420886993408
train_iter_loss: 0.31814587116241455
train_iter_loss: 0.4088610112667084
train_iter_loss: 0.17497922480106354
train_iter_loss: 0.117471843957901
train_iter_loss: 0.2911578118801117
train_iter_loss: 0.21632128953933716
train_iter_loss: 0.16058462858200073
train_iter_loss: 0.24516655504703522
train_iter_loss: 0.22371569275856018
train_iter_loss: 0.3782815635204315
train_iter_loss: 0.28063681721687317
train loss :0.2687
---------------------
Validation seg loss: 0.3540795248107247 at epoch 702
epoch =    703/  1000, exp = train
train_iter_loss: 0.09773105382919312
train_iter_loss: 0.22522303462028503
train_iter_loss: 0.3537895083427429
train_iter_loss: 0.22337473928928375
train_iter_loss: 0.3075273036956787
train_iter_loss: 0.30753788352012634
train_iter_loss: 0.3032958507537842
train_iter_loss: 0.30617207288742065
train_iter_loss: 0.4699750244617462
train_iter_loss: 0.36475157737731934
train_iter_loss: 0.07340452820062637
train_iter_loss: 0.24273602664470673
train_iter_loss: 0.29145553708076477
train_iter_loss: 0.23916280269622803
train_iter_loss: 0.23699964582920074
train_iter_loss: 0.33981096744537354
train_iter_loss: 0.19279281795024872
train_iter_loss: 0.12339271605014801
train_iter_loss: 0.254681795835495
train_iter_loss: 0.21970489621162415
train_iter_loss: 0.2952733635902405
train_iter_loss: 0.2332942634820938
train_iter_loss: 0.25700289011001587
train_iter_loss: 0.3537466824054718
train_iter_loss: 0.2645115852355957
train_iter_loss: 0.26331427693367004
train_iter_loss: 0.2469722479581833
train_iter_loss: 0.18526344001293182
train_iter_loss: 0.2045147866010666
train_iter_loss: 0.30515387654304504
train_iter_loss: 0.4439171254634857
train_iter_loss: 0.11330415308475494
train_iter_loss: 0.33685773611068726
train_iter_loss: 0.2739231586456299
train_iter_loss: 0.2868170440196991
train_iter_loss: 0.2316189408302307
train_iter_loss: 0.12379113584756851
train_iter_loss: 0.34816521406173706
train_iter_loss: 0.3733658492565155
train_iter_loss: 0.20281876623630524
train_iter_loss: 0.4562816023826599
train_iter_loss: 0.027241678908467293
train_iter_loss: 0.2926921546459198
train_iter_loss: 0.14377327263355255
train_iter_loss: 0.2877017557621002
train_iter_loss: 0.11433123052120209
train_iter_loss: 0.318881094455719
train_iter_loss: 0.35815176367759705
train_iter_loss: 0.25673598051071167
train_iter_loss: 0.22982998192310333
train_iter_loss: 0.308599054813385
train_iter_loss: 0.20079080760478973
train_iter_loss: 0.14638830721378326
train_iter_loss: 0.19077745079994202
train_iter_loss: 0.29610398411750793
train_iter_loss: 0.37391167879104614
train_iter_loss: 0.3412248492240906
train_iter_loss: 0.17406071722507477
train_iter_loss: 0.2705402672290802
train_iter_loss: 0.2778998017311096
train_iter_loss: 0.25867459177970886
train_iter_loss: 0.24728058278560638
train_iter_loss: 0.32411476969718933
train_iter_loss: 0.16060718894004822
train_iter_loss: 0.400730162858963
train_iter_loss: 0.1303700953722
train_iter_loss: 0.37886831164360046
train_iter_loss: 0.3117983043193817
train_iter_loss: 0.24621528387069702
train_iter_loss: 0.2773417532444
train_iter_loss: 0.2741886377334595
train_iter_loss: 0.23160921037197113
train_iter_loss: 0.29055607318878174
train_iter_loss: 0.20736190676689148
train_iter_loss: 0.1919521689414978
train_iter_loss: 0.3141137957572937
train_iter_loss: 0.27061212062835693
train_iter_loss: 0.1229424998164177
train_iter_loss: 0.25893494486808777
train_iter_loss: 0.2362779974937439
train_iter_loss: 0.31165802478790283
train_iter_loss: 0.26304566860198975
train_iter_loss: 0.2781769335269928
train_iter_loss: 0.1860494166612625
train_iter_loss: 0.25882187485694885
train_iter_loss: 0.3379436433315277
train_iter_loss: 0.17303599417209625
train_iter_loss: 0.44641968607902527
train_iter_loss: 0.09393791854381561
train_iter_loss: 0.3178430497646332
train_iter_loss: 0.26557090878486633
train_iter_loss: 0.35618284344673157
train_iter_loss: 0.24995078146457672
train_iter_loss: 0.336486279964447
train_iter_loss: 0.26924797892570496
train_iter_loss: 0.21425791084766388
train_iter_loss: 0.14283491671085358
train_iter_loss: 0.2104870229959488
train_iter_loss: 0.23553109169006348
train_iter_loss: 0.19009941816329956
train loss :0.2612
---------------------
Validation seg loss: 0.3646506927380303 at epoch 703
epoch =    704/  1000, exp = train
train_iter_loss: 0.20523910224437714
train_iter_loss: 0.24585005640983582
train_iter_loss: 0.20944127440452576
train_iter_loss: 0.28508448600769043
train_iter_loss: 0.22339870035648346
train_iter_loss: 0.24329416453838348
train_iter_loss: 0.45089343190193176
train_iter_loss: 0.12119118124246597
train_iter_loss: 0.21020303666591644
train_iter_loss: 0.3094186782836914
train_iter_loss: 0.34481868147850037
train_iter_loss: 0.2308402806520462
train_iter_loss: 0.20185500383377075
train_iter_loss: 0.4757382273674011
train_iter_loss: 0.21850401163101196
train_iter_loss: 0.2586136758327484
train_iter_loss: 0.16753073036670685
train_iter_loss: 0.2531261444091797
train_iter_loss: 0.2546617090702057
train_iter_loss: 0.20554859936237335
train_iter_loss: 0.30639204382896423
train_iter_loss: 0.2319130152463913
train_iter_loss: 0.2583616077899933
train_iter_loss: 0.19736729562282562
train_iter_loss: 0.23637309670448303
train_iter_loss: 0.18744605779647827
train_iter_loss: 0.23382611572742462
train_iter_loss: 0.12497647106647491
train_iter_loss: 0.3938423991203308
train_iter_loss: 0.2808740735054016
train_iter_loss: 0.2787211537361145
train_iter_loss: 0.23272103071212769
train_iter_loss: 0.30013495683670044
train_iter_loss: 0.2502708435058594
train_iter_loss: 0.2967852056026459
train_iter_loss: 0.19126418232917786
train_iter_loss: 0.28071385622024536
train_iter_loss: 0.2766367793083191
train_iter_loss: 0.3482455015182495
train_iter_loss: 0.15614035725593567
train_iter_loss: 0.34825193881988525
train_iter_loss: 0.08899931609630585
train_iter_loss: 0.25451985001564026
train_iter_loss: 0.0822843685746193
train_iter_loss: 0.21424266695976257
train_iter_loss: 0.13380280137062073
train_iter_loss: 0.2191203236579895
train_iter_loss: 0.2543041408061981
train_iter_loss: 0.46603187918663025
train_iter_loss: 0.14644071459770203
train_iter_loss: 0.09408541768789291
train_iter_loss: 0.4202677309513092
train_iter_loss: 0.33368727564811707
train_iter_loss: 0.21373379230499268
train_iter_loss: 0.08032932877540588
train_iter_loss: 0.5866044163703918
train_iter_loss: 0.4171411991119385
train_iter_loss: 0.1269616037607193
train_iter_loss: 0.3940812349319458
train_iter_loss: 0.13801193237304688
train_iter_loss: 0.17426657676696777
train_iter_loss: 0.28426092863082886
train_iter_loss: 0.19161465764045715
train_iter_loss: 0.4115641713142395
train_iter_loss: 0.21752876043319702
train_iter_loss: 0.255711168050766
train_iter_loss: 0.40745818614959717
train_iter_loss: 0.34895816445350647
train_iter_loss: 0.17584672570228577
train_iter_loss: 0.2393287569284439
train_iter_loss: 0.0684569776058197
train_iter_loss: 0.29674234986305237
train_iter_loss: 0.27678799629211426
train_iter_loss: 0.20253652334213257
train_iter_loss: 0.20588044822216034
train_iter_loss: 0.3160538971424103
train_iter_loss: 0.2468767762184143
train_iter_loss: 0.12669584155082703
train_iter_loss: 0.23807382583618164
train_iter_loss: 0.17938503623008728
train_iter_loss: 0.22964079678058624
train_iter_loss: 0.4030534625053406
train_iter_loss: 0.40870150923728943
train_iter_loss: 0.19122523069381714
train_iter_loss: 0.21742120385169983
train_iter_loss: 0.2300177663564682
train_iter_loss: 0.1962733119726181
train_iter_loss: 0.495026171207428
train_iter_loss: 0.42116084694862366
train_iter_loss: 0.24811513721942902
train_iter_loss: 0.15432493388652802
train_iter_loss: 0.2837691605091095
train_iter_loss: 0.3932321071624756
train_iter_loss: 0.2797947824001312
train_iter_loss: 0.3156934678554535
train_iter_loss: 0.2838103175163269
train_iter_loss: 0.1596939116716385
train_iter_loss: 0.18865938484668732
train_iter_loss: 0.27353009581565857
train_iter_loss: 0.39361056685447693
train loss :0.2608
---------------------
Validation seg loss: 0.3617111027926066 at epoch 704
epoch =    705/  1000, exp = train
train_iter_loss: 0.15268279612064362
train_iter_loss: 0.3514121174812317
train_iter_loss: 0.25880861282348633
train_iter_loss: 0.21719464659690857
train_iter_loss: 0.38718438148498535
train_iter_loss: 0.3786770701408386
train_iter_loss: 0.2737540602684021
train_iter_loss: 0.20287351310253143
train_iter_loss: 0.26843175292015076
train_iter_loss: 0.36533525586128235
train_iter_loss: 0.26933398842811584
train_iter_loss: 0.33226335048675537
train_iter_loss: 0.25436532497406006
train_iter_loss: 0.4234192371368408
train_iter_loss: 0.3475818932056427
train_iter_loss: 0.29625675082206726
train_iter_loss: 0.2302827090024948
train_iter_loss: 0.31645506620407104
train_iter_loss: 0.20697127282619476
train_iter_loss: 0.18436957895755768
train_iter_loss: 0.15195876359939575
train_iter_loss: 0.13516534864902496
train_iter_loss: 0.23878207802772522
train_iter_loss: 0.1919296830892563
train_iter_loss: 0.11111898720264435
train_iter_loss: 0.46443116664886475
train_iter_loss: 0.2381858229637146
train_iter_loss: 0.23159576952457428
train_iter_loss: 0.19841980934143066
train_iter_loss: 0.27954432368278503
train_iter_loss: 0.26741015911102295
train_iter_loss: 0.08129504323005676
train_iter_loss: 0.28511250019073486
train_iter_loss: 0.3223460912704468
train_iter_loss: 0.1418614685535431
train_iter_loss: 0.2060215175151825
train_iter_loss: 0.2617233693599701
train_iter_loss: 0.17766815423965454
train_iter_loss: 0.22814255952835083
train_iter_loss: 0.2587380111217499
train_iter_loss: 0.34718310832977295
train_iter_loss: 0.3848067820072174
train_iter_loss: 0.21927691996097565
train_iter_loss: 0.47442638874053955
train_iter_loss: 0.20618735253810883
train_iter_loss: 0.21891523897647858
train_iter_loss: 0.2467128485441208
train_iter_loss: 0.15442857146263123
train_iter_loss: 0.3215838670730591
train_iter_loss: 0.27342820167541504
train_iter_loss: 0.2822459042072296
train_iter_loss: 0.31567174196243286
train_iter_loss: 0.33579596877098083
train_iter_loss: 0.2603743076324463
train_iter_loss: 0.2798885703086853
train_iter_loss: 0.23730608820915222
train_iter_loss: 0.26649370789527893
train_iter_loss: 0.3498634696006775
train_iter_loss: 0.1373792290687561
train_iter_loss: 0.21778832376003265
train_iter_loss: 0.21641989052295685
train_iter_loss: 0.2426968663930893
train_iter_loss: 0.20396873354911804
train_iter_loss: 0.23891884088516235
train_iter_loss: 0.3693252503871918
train_iter_loss: 0.22585543990135193
train_iter_loss: 0.1962871104478836
train_iter_loss: 0.45787736773490906
train_iter_loss: 0.2684122323989868
train_iter_loss: 0.29411327838897705
train_iter_loss: 0.3391697406768799
train_iter_loss: 0.2717364430427551
train_iter_loss: 0.17455260455608368
train_iter_loss: 0.37439367175102234
train_iter_loss: 0.34451109170913696
train_iter_loss: 0.23911502957344055
train_iter_loss: 0.3165547251701355
train_iter_loss: 0.14054204523563385
train_iter_loss: 0.28390172123908997
train_iter_loss: 0.3204004466533661
train_iter_loss: 0.2669442296028137
train_iter_loss: 0.1975504457950592
train_iter_loss: 0.22042152285575867
train_iter_loss: 0.280943363904953
train_iter_loss: 0.3500262498855591
train_iter_loss: 0.08925135433673859
train_iter_loss: 0.2700361907482147
train_iter_loss: 0.3313465416431427
train_iter_loss: 0.19868776202201843
train_iter_loss: 0.23326551914215088
train_iter_loss: 0.314296692609787
train_iter_loss: 0.1977556347846985
train_iter_loss: 0.2084827870130539
train_iter_loss: 0.2049422711133957
train_iter_loss: 0.30609434843063354
train_iter_loss: 0.3174871802330017
train_iter_loss: 0.27998343110084534
train_iter_loss: 0.21234817802906036
train_iter_loss: 0.2802925109863281
train_iter_loss: 0.2744760513305664
train loss :0.2653
---------------------
Validation seg loss: 0.3640402792821164 at epoch 705
epoch =    706/  1000, exp = train
train_iter_loss: 0.28195521235466003
train_iter_loss: 0.04170108586549759
train_iter_loss: 0.28966668248176575
train_iter_loss: 0.3708280324935913
train_iter_loss: 0.24053817987442017
train_iter_loss: 0.2690528631210327
train_iter_loss: 0.17755316197872162
train_iter_loss: 0.41064390540122986
train_iter_loss: 0.23094162344932556
train_iter_loss: 0.2628140151500702
train_iter_loss: 0.14354167878627777
train_iter_loss: 0.2872486710548401
train_iter_loss: 0.25239327549934387
train_iter_loss: 0.22519293427467346
train_iter_loss: 0.3505646288394928
train_iter_loss: 0.24118570983409882
train_iter_loss: 0.35310032963752747
train_iter_loss: 0.22309868037700653
train_iter_loss: 0.2573012411594391
train_iter_loss: 0.2675613462924957
train_iter_loss: 0.2427489310503006
train_iter_loss: 0.44443655014038086
train_iter_loss: 0.2732502222061157
train_iter_loss: 0.26850569248199463
train_iter_loss: 0.3192811608314514
train_iter_loss: 0.2802228033542633
train_iter_loss: 0.26869454979896545
train_iter_loss: 0.23181571066379547
train_iter_loss: 0.2271762490272522
train_iter_loss: 0.3074999749660492
train_iter_loss: 0.1529729962348938
train_iter_loss: 0.18522682785987854
train_iter_loss: 0.20408765971660614
train_iter_loss: 0.29408037662506104
train_iter_loss: 0.1917920559644699
train_iter_loss: 0.2510833740234375
train_iter_loss: 0.1004214882850647
train_iter_loss: 0.31006184220314026
train_iter_loss: 0.1345517635345459
train_iter_loss: 0.20865753293037415
train_iter_loss: 0.172222301363945
train_iter_loss: 0.27603352069854736
train_iter_loss: 0.15280312299728394
train_iter_loss: 0.35196179151535034
train_iter_loss: 0.2525554895401001
train_iter_loss: 0.255603551864624
train_iter_loss: 0.3941020369529724
train_iter_loss: 0.2136460542678833
train_iter_loss: 0.19950257241725922
train_iter_loss: 0.299520879983902
train_iter_loss: 0.18466107547283173
train_iter_loss: 0.27203503251075745
train_iter_loss: 0.32845062017440796
train_iter_loss: 0.28373661637306213
train_iter_loss: 0.237390398979187
train_iter_loss: 0.28120139241218567
train_iter_loss: 0.31895193457603455
train_iter_loss: 0.20021937787532806
train_iter_loss: 0.2249341607093811
train_iter_loss: 0.343400776386261
train_iter_loss: 0.18796296417713165
train_iter_loss: 0.3168613612651825
train_iter_loss: 0.2902846336364746
train_iter_loss: 0.23073117434978485
train_iter_loss: 0.2336246222257614
train_iter_loss: 0.4216082990169525
train_iter_loss: 0.3863586187362671
train_iter_loss: 0.29131999611854553
train_iter_loss: 0.2726222276687622
train_iter_loss: 0.2506568431854248
train_iter_loss: 0.3256257176399231
train_iter_loss: 0.3269570767879486
train_iter_loss: 0.18903851509094238
train_iter_loss: 0.29354244470596313
train_iter_loss: 0.29991286993026733
train_iter_loss: 0.3829716444015503
train_iter_loss: 0.2516382336616516
train_iter_loss: 0.3395759165287018
train_iter_loss: 0.34143078327178955
train_iter_loss: 0.10338495671749115
train_iter_loss: 0.12620095908641815
train_iter_loss: 0.24038931727409363
train_iter_loss: 0.29719123244285583
train_iter_loss: 0.23396724462509155
train_iter_loss: 0.11163312196731567
train_iter_loss: 0.29708656668663025
train_iter_loss: 0.4162447452545166
train_iter_loss: 0.21439094841480255
train_iter_loss: 0.25268957018852234
train_iter_loss: 0.2144697904586792
train_iter_loss: 0.26379892230033875
train_iter_loss: 0.10410069674253464
train_iter_loss: 0.2305585741996765
train_iter_loss: 0.27103564143180847
train_iter_loss: 0.24416638910770416
train_iter_loss: 0.3721461594104767
train_iter_loss: 0.16171270608901978
train_iter_loss: 0.245114266872406
train_iter_loss: 0.12264468520879745
train_iter_loss: 0.24078483879566193
train loss :0.2598
---------------------
Validation seg loss: 0.3742083893025751 at epoch 706
epoch =    707/  1000, exp = train
train_iter_loss: 0.32348349690437317
train_iter_loss: 0.3429260849952698
train_iter_loss: 0.1762167364358902
train_iter_loss: 0.24779050052165985
train_iter_loss: 0.3020836412906647
train_iter_loss: 0.29444432258605957
train_iter_loss: 0.32095402479171753
train_iter_loss: 0.16876284778118134
train_iter_loss: 0.3398546278476715
train_iter_loss: 0.32867273688316345
train_iter_loss: 0.27779385447502136
train_iter_loss: 0.14405778050422668
train_iter_loss: 0.09511816501617432
train_iter_loss: 0.32590076327323914
train_iter_loss: 0.22619393467903137
train_iter_loss: 0.37569886445999146
train_iter_loss: 0.24619925022125244
train_iter_loss: 0.31770059466362
train_iter_loss: 0.22168192267417908
train_iter_loss: 0.03363991528749466
train_iter_loss: 0.16137786209583282
train_iter_loss: 0.2948911786079407
train_iter_loss: 0.1744653880596161
train_iter_loss: 0.3437075912952423
train_iter_loss: 0.29937416315078735
train_iter_loss: 0.2448997050523758
train_iter_loss: 0.18578800559043884
train_iter_loss: 0.3763526976108551
train_iter_loss: 0.2424667626619339
train_iter_loss: 0.17730316519737244
train_iter_loss: 0.259990930557251
train_iter_loss: 0.26984745264053345
train_iter_loss: 0.24004890024662018
train_iter_loss: 0.3027556836605072
train_iter_loss: 0.28866639733314514
train_iter_loss: 0.1392512172460556
train_iter_loss: 0.2900005877017975
train_iter_loss: 0.34665730595588684
train_iter_loss: 0.21006229519844055
train_iter_loss: 0.2726615369319916
train_iter_loss: 0.1251271814107895
train_iter_loss: 0.234030082821846
train_iter_loss: 0.1944083571434021
train_iter_loss: 0.26405084133148193
train_iter_loss: 0.4485316872596741
train_iter_loss: 0.29667791724205017
train_iter_loss: 0.3904089033603668
train_iter_loss: 0.17960341274738312
train_iter_loss: 0.24217559397220612
train_iter_loss: 0.12791147828102112
train_iter_loss: 0.3603258430957794
train_iter_loss: 0.13441652059555054
train_iter_loss: 0.32100486755371094
train_iter_loss: 0.12912635505199432
train_iter_loss: 0.3976649343967438
train_iter_loss: 0.22620238363742828
train_iter_loss: 0.2403956800699234
train_iter_loss: 0.18270337581634521
train_iter_loss: 0.34325823187828064
train_iter_loss: 0.27464473247528076
train_iter_loss: 0.13903583586215973
train_iter_loss: 0.32323673367500305
train_iter_loss: 0.38993343710899353
train_iter_loss: 0.3046744167804718
train_iter_loss: 0.4107268452644348
train_iter_loss: 0.3051077425479889
train_iter_loss: 0.21230222284793854
train_iter_loss: 0.16658137738704681
train_iter_loss: 0.2458801120519638
train_iter_loss: 0.26144593954086304
train_iter_loss: 0.31699618697166443
train_iter_loss: 0.3843183219432831
train_iter_loss: 0.1264951080083847
train_iter_loss: 0.31040507555007935
train_iter_loss: 0.34625986218452454
train_iter_loss: 0.2029534876346588
train_iter_loss: 0.4130805730819702
train_iter_loss: 0.22909416258335114
train_iter_loss: 0.23222018778324127
train_iter_loss: 0.22875000536441803
train_iter_loss: 0.3897192180156708
train_iter_loss: 0.16274012625217438
train_iter_loss: 0.2832813858985901
train_iter_loss: 0.39475134015083313
train_iter_loss: 0.29907476902008057
train_iter_loss: 0.32384049892425537
train_iter_loss: 0.2740752398967743
train_iter_loss: 0.40186962485313416
train_iter_loss: 0.12187685817480087
train_iter_loss: 0.19389057159423828
train_iter_loss: 0.32368937134742737
train_iter_loss: 0.251518577337265
train_iter_loss: 0.2680601179599762
train_iter_loss: 0.3366861343383789
train_iter_loss: 0.6511738896369934
train_iter_loss: 0.2845507264137268
train_iter_loss: 0.24252824485301971
train_iter_loss: 0.2744755446910858
train_iter_loss: 0.26554423570632935
train_iter_loss: 0.2140262871980667
train loss :0.2711
---------------------
Validation seg loss: 0.37034777942429875 at epoch 707
epoch =    708/  1000, exp = train
train_iter_loss: 0.1503746509552002
train_iter_loss: 0.39870497584342957
train_iter_loss: 0.3010042905807495
train_iter_loss: 0.14609582722187042
train_iter_loss: 0.2635055184364319
train_iter_loss: 0.3252875804901123
train_iter_loss: 0.24430681765079498
train_iter_loss: 0.22918039560317993
train_iter_loss: 0.30342185497283936
train_iter_loss: 0.310137540102005
train_iter_loss: 0.2864898145198822
train_iter_loss: 0.25008830428123474
train_iter_loss: 0.24514436721801758
train_iter_loss: 0.02660227194428444
train_iter_loss: 0.20772024989128113
train_iter_loss: 0.16663143038749695
train_iter_loss: 0.24630440771579742
train_iter_loss: 0.08810198307037354
train_iter_loss: 0.10161001980304718
train_iter_loss: 0.18931187689304352
train_iter_loss: 0.3346956968307495
train_iter_loss: 0.2633042335510254
train_iter_loss: 0.2650981843471527
train_iter_loss: 0.3575270473957062
train_iter_loss: 0.13486924767494202
train_iter_loss: 0.3640974760055542
train_iter_loss: 0.08902614563703537
train_iter_loss: 0.2047044038772583
train_iter_loss: 0.20369058847427368
train_iter_loss: 0.18575288355350494
train_iter_loss: 0.3509409725666046
train_iter_loss: 0.2286044955253601
train_iter_loss: 0.38907966017723083
train_iter_loss: 0.3708106577396393
train_iter_loss: 0.3575044274330139
train_iter_loss: 0.26328304409980774
train_iter_loss: 0.46113723516464233
train_iter_loss: 0.4350232779979706
train_iter_loss: 0.3333377242088318
train_iter_loss: 0.1975177675485611
train_iter_loss: 0.24140487611293793
train_iter_loss: 0.3008480966091156
train_iter_loss: 0.4927842915058136
train_iter_loss: 0.2725725471973419
train_iter_loss: 0.10556106269359589
train_iter_loss: 0.21282318234443665
train_iter_loss: 0.26457443833351135
train_iter_loss: 0.18411260843276978
train_iter_loss: 0.27523210644721985
train_iter_loss: 0.1808600276708603
train_iter_loss: 0.26645416021347046
train_iter_loss: 0.18515156209468842
train_iter_loss: 0.25136682391166687
train_iter_loss: 0.4421904385089874
train_iter_loss: 0.3475075662136078
train_iter_loss: 0.15335245430469513
train_iter_loss: 0.24457281827926636
train_iter_loss: 0.17892691493034363
train_iter_loss: 0.21995888650417328
train_iter_loss: 0.29326310753822327
train_iter_loss: 0.31362369656562805
train_iter_loss: 0.20432603359222412
train_iter_loss: 0.17052407562732697
train_iter_loss: 0.3401210308074951
train_iter_loss: 0.37531721591949463
train_iter_loss: 0.4320904016494751
train_iter_loss: 0.26121172308921814
train_iter_loss: 0.6316438317298889
train_iter_loss: 0.22521774470806122
train_iter_loss: 0.19158804416656494
train_iter_loss: 0.36312630772590637
train_iter_loss: 0.239852637052536
train_iter_loss: 0.24476200342178345
train_iter_loss: 0.18852704763412476
train_iter_loss: 0.269552618265152
train_iter_loss: 0.3942543864250183
train_iter_loss: 0.23813702166080475
train_iter_loss: 0.140329971909523
train_iter_loss: 0.2099744826555252
train_iter_loss: 0.29528650641441345
train_iter_loss: 0.3159056007862091
train_iter_loss: 0.2642621099948883
train_iter_loss: 0.25985896587371826
train_iter_loss: 0.17174594104290009
train_iter_loss: 0.14028562605381012
train_iter_loss: 0.40216073393821716
train_iter_loss: 0.19858340919017792
train_iter_loss: 0.2345743328332901
train_iter_loss: 0.3484356701374054
train_iter_loss: 0.34808406233787537
train_iter_loss: 0.25893282890319824
train_iter_loss: 0.2635228931903839
train_iter_loss: 0.2331525683403015
train_iter_loss: 0.2759581208229065
train_iter_loss: 0.1872945874929428
train_iter_loss: 0.18519262969493866
train_iter_loss: 0.18091849982738495
train_iter_loss: 0.32685890793800354
train_iter_loss: 0.20655716955661774
train_iter_loss: 0.1964622288942337
train loss :0.2638
---------------------
Validation seg loss: 0.3679660438770815 at epoch 708
epoch =    709/  1000, exp = train
train_iter_loss: 0.18388712406158447
train_iter_loss: 0.2118396759033203
train_iter_loss: 0.173305481672287
train_iter_loss: 0.3695184588432312
train_iter_loss: 0.2665741741657257
train_iter_loss: 0.1311495453119278
train_iter_loss: 0.17436185479164124
train_iter_loss: 0.3589858114719391
train_iter_loss: 0.3189544379711151
train_iter_loss: 0.2864305377006531
train_iter_loss: 0.21691542863845825
train_iter_loss: 0.19149021804332733
train_iter_loss: 0.2929418683052063
train_iter_loss: 0.1848859041929245
train_iter_loss: 0.11137184500694275
train_iter_loss: 0.22098776698112488
train_iter_loss: 0.174592062830925
train_iter_loss: 0.28982043266296387
train_iter_loss: 0.2890876233577728
train_iter_loss: 0.15686412155628204
train_iter_loss: 0.33010727167129517
train_iter_loss: 0.2826840877532959
train_iter_loss: 0.23131944239139557
train_iter_loss: 0.3806964159011841
train_iter_loss: 0.3957556486129761
train_iter_loss: 0.32631948590278625
train_iter_loss: 0.23314809799194336
train_iter_loss: 0.25775405764579773
train_iter_loss: 0.3386566638946533
train_iter_loss: 0.2766750752925873
train_iter_loss: 0.25832799077033997
train_iter_loss: 0.2568865418434143
train_iter_loss: 0.2986322045326233
train_iter_loss: 0.32060056924819946
train_iter_loss: 0.1799435168504715
train_iter_loss: 0.18984705209732056
train_iter_loss: 0.22696228325366974
train_iter_loss: 0.2844065725803375
train_iter_loss: 0.23984602093696594
train_iter_loss: 0.28457579016685486
train_iter_loss: 0.19294853508472443
train_iter_loss: 0.27193647623062134
train_iter_loss: 0.19190669059753418
train_iter_loss: 0.3082925081253052
train_iter_loss: 0.30494531989097595
train_iter_loss: 0.25589048862457275
train_iter_loss: 0.13729813694953918
train_iter_loss: 0.34398192167282104
train_iter_loss: 0.2800293266773224
train_iter_loss: 0.15335971117019653
train_iter_loss: 0.1697017252445221
train_iter_loss: 0.19759194552898407
train_iter_loss: 0.3349781334400177
train_iter_loss: 0.44961732625961304
train_iter_loss: 0.36927530169487
train_iter_loss: 0.32585108280181885
train_iter_loss: 0.1810557097196579
train_iter_loss: 0.2928580641746521
train_iter_loss: 0.27447929978370667
train_iter_loss: 0.22285336256027222
train_iter_loss: 0.13490161299705505
train_iter_loss: 0.20946522057056427
train_iter_loss: 0.13003306090831757
train_iter_loss: 0.2735399305820465
train_iter_loss: 0.18190845847129822
train_iter_loss: 0.5531577467918396
train_iter_loss: 0.3362099230289459
train_iter_loss: 0.19839972257614136
train_iter_loss: 0.22524280846118927
train_iter_loss: 0.3083745539188385
train_iter_loss: 0.25000184774398804
train_iter_loss: 0.2744258940219879
train_iter_loss: 0.49712806940078735
train_iter_loss: 0.4159383773803711
train_iter_loss: 0.1475811004638672
train_iter_loss: 0.19146893918514252
train_iter_loss: 0.2596159279346466
train_iter_loss: 0.14773577451705933
train_iter_loss: 0.156610906124115
train_iter_loss: 0.45079007744789124
train_iter_loss: 0.16291140019893646
train_iter_loss: 0.3670099675655365
train_iter_loss: 0.19957022368907928
train_iter_loss: 0.4475366771221161
train_iter_loss: 0.16266991198062897
train_iter_loss: 0.3857142925262451
train_iter_loss: 0.2475287765264511
train_iter_loss: 0.26029396057128906
train_iter_loss: 0.42890527844429016
train_iter_loss: 0.33633458614349365
train_iter_loss: 0.24686560034751892
train_iter_loss: 0.2777605354785919
train_iter_loss: 0.3047056496143341
train_iter_loss: 0.26375246047973633
train_iter_loss: 0.26860424876213074
train_iter_loss: 0.1468910127878189
train_iter_loss: 0.2631480097770691
train_iter_loss: 0.23209887742996216
train_iter_loss: 0.3256032466888428
train_iter_loss: 0.2694820463657379
train loss :0.2666
---------------------
Validation seg loss: 0.36690997810296294 at epoch 709
epoch =    710/  1000, exp = train
train_iter_loss: 0.17291951179504395
train_iter_loss: 0.16137413680553436
train_iter_loss: 0.3577636480331421
train_iter_loss: 0.21131542325019836
train_iter_loss: 0.2189178466796875
train_iter_loss: 0.2049838900566101
train_iter_loss: 0.1280069649219513
train_iter_loss: 0.26834920048713684
train_iter_loss: 0.28325316309928894
train_iter_loss: 0.1873234510421753
train_iter_loss: 0.21417692303657532
train_iter_loss: 0.24303384125232697
train_iter_loss: 0.2982679605484009
train_iter_loss: 0.2315998077392578
train_iter_loss: 0.1814437210559845
train_iter_loss: 0.21358375251293182
train_iter_loss: 0.23947066068649292
train_iter_loss: 0.20720526576042175
train_iter_loss: 0.20112363994121552
train_iter_loss: 0.2747144401073456
train_iter_loss: 0.10710889846086502
train_iter_loss: 0.11647644639015198
train_iter_loss: 0.21498514711856842
train_iter_loss: 0.22389273345470428
train_iter_loss: 0.1627647876739502
train_iter_loss: 0.19132277369499207
train_iter_loss: 0.19373439252376556
train_iter_loss: 0.3507811427116394
train_iter_loss: 0.42272886633872986
train_iter_loss: 0.2749025225639343
train_iter_loss: 0.2451828271150589
train_iter_loss: 0.2507505714893341
train_iter_loss: 0.16822978854179382
train_iter_loss: 0.3837719261646271
train_iter_loss: 0.21586762368679047
train_iter_loss: 0.3455367386341095
train_iter_loss: 0.25731170177459717
train_iter_loss: 0.09746260195970535
train_iter_loss: 0.5548887252807617
train_iter_loss: 0.36598339676856995
train_iter_loss: 0.250129759311676
train_iter_loss: 0.26779472827911377
train_iter_loss: 0.19538302719593048
train_iter_loss: 0.2727085053920746
train_iter_loss: 0.14740926027297974
train_iter_loss: 0.3797776401042938
train_iter_loss: 0.328005313873291
train_iter_loss: 0.26687705516815186
train_iter_loss: 0.3008985221385956
train_iter_loss: 0.29712000489234924
train_iter_loss: 0.3446485698223114
train_iter_loss: 0.21440212428569794
train_iter_loss: 0.23667508363723755
train_iter_loss: 0.320463627576828
train_iter_loss: 0.31066301465034485
train_iter_loss: 0.3250422179698944
train_iter_loss: 0.2975316345691681
train_iter_loss: 0.23038113117218018
train_iter_loss: 0.22639143466949463
train_iter_loss: 0.3064614534378052
train_iter_loss: 0.17044131457805634
train_iter_loss: 0.3076021373271942
train_iter_loss: 0.48380979895591736
train_iter_loss: 0.1379220336675644
train_iter_loss: 0.31700360774993896
train_iter_loss: 0.13557955622673035
train_iter_loss: 0.2772693634033203
train_iter_loss: 0.2719484865665436
train_iter_loss: 0.3449559509754181
train_iter_loss: 0.37197062373161316
train_iter_loss: 0.3270270526409149
train_iter_loss: 0.31761762499809265
train_iter_loss: 0.26397889852523804
train_iter_loss: 0.24921783804893494
train_iter_loss: 0.2148592472076416
train_iter_loss: 0.2641066014766693
train_iter_loss: 0.2029201239347458
train_iter_loss: 0.21645206212997437
train_iter_loss: 0.25906869769096375
train_iter_loss: 0.2755977511405945
train_iter_loss: 0.18042969703674316
train_iter_loss: 0.4007699489593506
train_iter_loss: 0.3356740474700928
train_iter_loss: 0.1601259708404541
train_iter_loss: 0.27426403760910034
train_iter_loss: 0.2991621494293213
train_iter_loss: 0.23390471935272217
train_iter_loss: 0.2581280767917633
train_iter_loss: 0.2781388759613037
train_iter_loss: 0.35958585143089294
train_iter_loss: 0.23009788990020752
train_iter_loss: 0.28772202134132385
train_iter_loss: 0.3052927851676941
train_iter_loss: 0.15213099122047424
train_iter_loss: 0.2978361248970032
train_iter_loss: 0.23694954812526703
train_iter_loss: 0.45947039127349854
train_iter_loss: 0.41399556398391724
train_iter_loss: 0.1090344786643982
train_iter_loss: 0.11296103149652481
train loss :0.2622
---------------------
Validation seg loss: 0.3574979335579248 at epoch 710
epoch =    711/  1000, exp = train
train_iter_loss: 0.4131764769554138
train_iter_loss: 0.2192486971616745
train_iter_loss: 0.1801522672176361
train_iter_loss: 0.2373020052909851
train_iter_loss: 0.2633989155292511
train_iter_loss: 0.21244452893733978
train_iter_loss: 0.2875586748123169
train_iter_loss: 0.28557631373405457
train_iter_loss: 0.2749669551849365
train_iter_loss: 0.34684908390045166
train_iter_loss: 0.44952255487442017
train_iter_loss: 0.09694857895374298
train_iter_loss: 0.2488541454076767
train_iter_loss: 0.1929158866405487
train_iter_loss: 0.2724616229534149
train_iter_loss: 0.0900622084736824
train_iter_loss: 0.19332638382911682
train_iter_loss: 0.29275792837142944
train_iter_loss: 0.19217674434185028
train_iter_loss: 0.20044350624084473
train_iter_loss: 0.22236141562461853
train_iter_loss: 0.31785836815834045
train_iter_loss: 0.2665749788284302
train_iter_loss: 0.23650063574314117
train_iter_loss: 0.41111502051353455
train_iter_loss: 0.21538662910461426
train_iter_loss: 0.3502902388572693
train_iter_loss: 0.2071499228477478
train_iter_loss: 0.17116349935531616
train_iter_loss: 0.271749883890152
train_iter_loss: 0.22829411923885345
train_iter_loss: 0.19823502004146576
train_iter_loss: 0.19737745821475983
train_iter_loss: 0.1308516561985016
train_iter_loss: 0.24866165220737457
train_iter_loss: 0.3161139488220215
train_iter_loss: 0.239082470536232
train_iter_loss: 0.35806700587272644
train_iter_loss: 0.2043749839067459
train_iter_loss: 0.37155553698539734
train_iter_loss: 0.39697694778442383
train_iter_loss: 0.2852795124053955
train_iter_loss: 0.2725416421890259
train_iter_loss: 0.3376055359840393
train_iter_loss: 0.31150195002555847
train_iter_loss: 0.35805273056030273
train_iter_loss: 0.30779141187667847
train_iter_loss: 0.3376447558403015
train_iter_loss: 0.32356026768684387
train_iter_loss: 0.34421542286872864
train_iter_loss: 0.15015733242034912
train_iter_loss: 0.21572144329547882
train_iter_loss: 0.18714556097984314
train_iter_loss: 0.21545124053955078
train_iter_loss: 0.276387095451355
train_iter_loss: 0.12766042351722717
train_iter_loss: 0.24765613675117493
train_iter_loss: 0.23124563694000244
train_iter_loss: 0.2167086899280548
train_iter_loss: 0.18104927241802216
train_iter_loss: 0.40892648696899414
train_iter_loss: 0.41419562697410583
train_iter_loss: 0.37201717495918274
train_iter_loss: 0.24156178534030914
train_iter_loss: 0.2841498851776123
train_iter_loss: 0.26927849650382996
train_iter_loss: 0.3761771619319916
train_iter_loss: 0.27066338062286377
train_iter_loss: 0.24815773963928223
train_iter_loss: 0.1905462145805359
train_iter_loss: 0.21374240517616272
train_iter_loss: 0.18568064272403717
train_iter_loss: 0.19997505843639374
train_iter_loss: 0.1758572906255722
train_iter_loss: 0.22792382538318634
train_iter_loss: 0.4900992810726166
train_iter_loss: 0.19680508971214294
train_iter_loss: 0.3979566991329193
train_iter_loss: 0.490646630525589
train_iter_loss: 0.11242569983005524
train_iter_loss: 0.057319629937410355
train_iter_loss: 0.33715373277664185
train_iter_loss: 0.22786030173301697
train_iter_loss: 0.20372851192951202
train_iter_loss: 0.17679888010025024
train_iter_loss: 0.1989511400461197
train_iter_loss: 0.24560189247131348
train_iter_loss: 0.21998928487300873
train_iter_loss: 0.2127605825662613
train_iter_loss: 0.40922999382019043
train_iter_loss: 0.3607757091522217
train_iter_loss: 0.31398409605026245
train_iter_loss: 0.31536296010017395
train_iter_loss: 0.13379929959774017
train_iter_loss: 0.19256101548671722
train_iter_loss: 0.1152174174785614
train_iter_loss: 0.26932451128959656
train_iter_loss: 0.3278886675834656
train_iter_loss: 0.16362088918685913
train_iter_loss: 0.2879025936126709
train loss :0.2624
---------------------
Validation seg loss: 0.36137754107684883 at epoch 711
epoch =    712/  1000, exp = train
train_iter_loss: 0.3331971764564514
train_iter_loss: 0.2559962570667267
train_iter_loss: 0.377603679895401
train_iter_loss: 0.20717820525169373
train_iter_loss: 0.21888555586338043
train_iter_loss: 0.14719600975513458
train_iter_loss: 0.20663918554782867
train_iter_loss: 0.3849506080150604
train_iter_loss: 0.1645773947238922
train_iter_loss: 0.19824276864528656
train_iter_loss: 0.21638697385787964
train_iter_loss: 0.20062069594860077
train_iter_loss: 0.23749329149723053
train_iter_loss: 0.2600622773170471
train_iter_loss: 0.20311862230300903
train_iter_loss: 0.2445881962776184
train_iter_loss: 0.07903150469064713
train_iter_loss: 0.2526857554912567
train_iter_loss: 0.19516310095787048
train_iter_loss: 0.3654541075229645
train_iter_loss: 0.3324410021305084
train_iter_loss: 0.23725493252277374
train_iter_loss: 0.3145013749599457
train_iter_loss: 0.19753292202949524
train_iter_loss: 0.3946630358695984
train_iter_loss: 0.3739638030529022
train_iter_loss: 0.2538447976112366
train_iter_loss: 0.08200719952583313
train_iter_loss: 0.10927584767341614
train_iter_loss: 0.3629039227962494
train_iter_loss: 0.14767232537269592
train_iter_loss: 0.2520301043987274
train_iter_loss: 0.25660818815231323
train_iter_loss: 0.183336079120636
train_iter_loss: 0.24351349472999573
train_iter_loss: 0.24535410106182098
train_iter_loss: 0.31212764978408813
train_iter_loss: 0.22130998969078064
train_iter_loss: 0.45063358545303345
train_iter_loss: 0.16641110181808472
train_iter_loss: 0.11094707250595093
train_iter_loss: 0.328633576631546
train_iter_loss: 0.3502427935600281
train_iter_loss: 0.10635319352149963
train_iter_loss: 0.2195551097393036
train_iter_loss: 0.21618317067623138
train_iter_loss: 0.1544722616672516
train_iter_loss: 0.38302305340766907
train_iter_loss: 0.1490703821182251
train_iter_loss: 0.31443580985069275
train_iter_loss: 0.16646388173103333
train_iter_loss: 0.3592623472213745
train_iter_loss: 0.16961216926574707
train_iter_loss: 0.13767072558403015
train_iter_loss: 0.2647276520729065
train_iter_loss: 0.1232166662812233
train_iter_loss: 0.21875876188278198
train_iter_loss: 0.3332049548625946
train_iter_loss: 0.33402299880981445
train_iter_loss: 0.25614386796951294
train_iter_loss: 0.20163211226463318
train_iter_loss: 0.3353504240512848
train_iter_loss: 0.23176568746566772
train_iter_loss: 0.3003295958042145
train_iter_loss: 0.32165491580963135
train_iter_loss: 0.16962598264217377
train_iter_loss: 0.250510036945343
train_iter_loss: 0.38726046681404114
train_iter_loss: 0.12368886172771454
train_iter_loss: 0.3076804578304291
train_iter_loss: 0.25792452692985535
train_iter_loss: 0.44689038395881653
train_iter_loss: 0.21301518380641937
train_iter_loss: 0.3345409333705902
train_iter_loss: 0.38792940974235535
train_iter_loss: 0.34337058663368225
train_iter_loss: 0.16521860659122467
train_iter_loss: 0.3320486545562744
train_iter_loss: 0.21912410855293274
train_iter_loss: 0.3838343024253845
train_iter_loss: 0.19604302942752838
train_iter_loss: 0.2951684296131134
train_iter_loss: 0.23359695076942444
train_iter_loss: 0.2870638370513916
train_iter_loss: 0.40310853719711304
train_iter_loss: 0.26079457998275757
train_iter_loss: 0.16385219991207123
train_iter_loss: 0.1662341207265854
train_iter_loss: 0.4370800256729126
train_iter_loss: 0.20764665305614471
train_iter_loss: 0.3646307587623596
train_iter_loss: 0.3784007132053375
train_iter_loss: 0.2513631284236908
train_iter_loss: 0.13339988887310028
train_iter_loss: 0.2077202945947647
train_iter_loss: 0.1411985605955124
train_iter_loss: 0.2431875765323639
train_iter_loss: 0.4815577566623688
train_iter_loss: 0.38292789459228516
train_iter_loss: 0.331564724445343
train loss :0.2612
---------------------
Validation seg loss: 0.3571280553884242 at epoch 712
epoch =    713/  1000, exp = train
train_iter_loss: 0.14362797141075134
train_iter_loss: 0.318459153175354
train_iter_loss: 0.47340530157089233
train_iter_loss: 0.2602550983428955
train_iter_loss: 0.3410813510417938
train_iter_loss: 0.454668253660202
train_iter_loss: 0.2552477717399597
train_iter_loss: 0.114952452480793
train_iter_loss: 0.24309058487415314
train_iter_loss: 0.1971062421798706
train_iter_loss: 0.1532336175441742
train_iter_loss: 0.21733398735523224
train_iter_loss: 0.1659851372241974
train_iter_loss: 0.31040406227111816
train_iter_loss: 0.18529994785785675
train_iter_loss: 0.2591201663017273
train_iter_loss: 0.3230840265750885
train_iter_loss: 0.12677066028118134
train_iter_loss: 0.1835961490869522
train_iter_loss: 0.24960874021053314
train_iter_loss: 0.4006464183330536
train_iter_loss: 0.4176666736602783
train_iter_loss: 0.2849039137363434
train_iter_loss: 0.12700626254081726
train_iter_loss: 0.1536075472831726
train_iter_loss: 0.4886210858821869
train_iter_loss: 0.3361356258392334
train_iter_loss: 0.27593961358070374
train_iter_loss: 0.2657853364944458
train_iter_loss: 0.40707963705062866
train_iter_loss: 0.34281691908836365
train_iter_loss: 0.40135741233825684
train_iter_loss: 0.17984184622764587
train_iter_loss: 0.29159650206565857
train_iter_loss: 0.29091060161590576
train_iter_loss: 0.3480969965457916
train_iter_loss: 0.32752084732055664
train_iter_loss: 0.2246662825345993
train_iter_loss: 0.2650615870952606
train_iter_loss: 0.30423620343208313
train_iter_loss: 0.28446057438850403
train_iter_loss: 0.2624771296977997
train_iter_loss: 0.3093208372592926
train_iter_loss: 0.27870529890060425
train_iter_loss: 0.24133142828941345
train_iter_loss: 0.22601726651191711
train_iter_loss: 0.2093784213066101
train_iter_loss: 0.19823257625102997
train_iter_loss: 0.3005177974700928
train_iter_loss: 0.1724957972764969
train_iter_loss: 0.14763608574867249
train_iter_loss: 0.09105908870697021
train_iter_loss: 0.21680572628974915
train_iter_loss: 0.24423624575138092
train_iter_loss: 0.16444514691829681
train_iter_loss: 0.2643435597419739
train_iter_loss: 0.23559430241584778
train_iter_loss: 0.21307498216629028
train_iter_loss: 0.24379988014698029
train_iter_loss: 0.2998148202896118
train_iter_loss: 0.2949361503124237
train_iter_loss: 0.2885648310184479
train_iter_loss: 0.43077629804611206
train_iter_loss: 0.2893851101398468
train_iter_loss: 0.4072379767894745
train_iter_loss: 0.29555532336235046
train_iter_loss: 0.18402020633220673
train_iter_loss: 0.21854551136493683
train_iter_loss: 0.31660279631614685
train_iter_loss: 0.42109423875808716
train_iter_loss: 0.11128628253936768
train_iter_loss: 0.26775482296943665
train_iter_loss: 0.18991412222385406
train_iter_loss: 0.1468297690153122
train_iter_loss: 0.24069656431674957
train_iter_loss: 0.2708517014980316
train_iter_loss: 0.3278290331363678
train_iter_loss: 0.3226352632045746
train_iter_loss: 0.47843480110168457
train_iter_loss: 0.23568101227283478
train_iter_loss: 0.183357372879982
train_iter_loss: 0.2156299650669098
train_iter_loss: 0.16534215211868286
train_iter_loss: 0.24069294333457947
train_iter_loss: 0.34903478622436523
train_iter_loss: 0.2297326922416687
train_iter_loss: 0.2546640932559967
train_iter_loss: 0.15619513392448425
train_iter_loss: 0.28844499588012695
train_iter_loss: 0.41672971844673157
train_iter_loss: 0.1878606230020523
train_iter_loss: 0.22333887219429016
train_iter_loss: 0.2676841616630554
train_iter_loss: 0.36184653639793396
train_iter_loss: 0.12334820628166199
train_iter_loss: 0.22702139616012573
train_iter_loss: 0.24461747705936432
train_iter_loss: 0.43983644247055054
train_iter_loss: 0.16317473351955414
train_iter_loss: 0.43278175592422485
train loss :0.2688
---------------------
Validation seg loss: 0.34845949763010414 at epoch 713
epoch =    714/  1000, exp = train
train_iter_loss: 0.24782110750675201
train_iter_loss: 0.39061686396598816
train_iter_loss: 0.1476733535528183
train_iter_loss: 0.27518075704574585
train_iter_loss: 0.17899654805660248
train_iter_loss: 0.2730644941329956
train_iter_loss: 0.25833696126937866
train_iter_loss: 0.18874236941337585
train_iter_loss: 0.31731370091438293
train_iter_loss: 0.17696619033813477
train_iter_loss: 0.37512996792793274
train_iter_loss: 0.3050338923931122
train_iter_loss: 0.17048174142837524
train_iter_loss: 0.2833455801010132
train_iter_loss: 0.3942056894302368
train_iter_loss: 0.22499527037143707
train_iter_loss: 0.14854028820991516
train_iter_loss: 0.2061944603919983
train_iter_loss: 0.37994101643562317
train_iter_loss: 0.11799315363168716
train_iter_loss: 0.40345409512519836
train_iter_loss: 0.2789202630519867
train_iter_loss: 0.4189143776893616
train_iter_loss: 0.17687110602855682
train_iter_loss: 0.14633335173130035
train_iter_loss: 0.2954592704772949
train_iter_loss: 0.22770537436008453
train_iter_loss: 0.20282551646232605
train_iter_loss: 0.19411756098270416
train_iter_loss: 0.18196606636047363
train_iter_loss: 0.19233688712120056
train_iter_loss: 0.2826691269874573
train_iter_loss: 0.2959313988685608
train_iter_loss: 0.2791304588317871
train_iter_loss: 0.18652506172657013
train_iter_loss: 0.2539532780647278
train_iter_loss: 0.25803428888320923
train_iter_loss: 0.2913329005241394
train_iter_loss: 0.20361903309822083
train_iter_loss: 0.2804599106311798
train_iter_loss: 0.3348842263221741
train_iter_loss: 0.16346731781959534
train_iter_loss: 0.4752059876918793
train_iter_loss: 0.16944441199302673
train_iter_loss: 0.16197936236858368
train_iter_loss: 0.25916481018066406
train_iter_loss: 0.18666434288024902
train_iter_loss: 0.315851628780365
train_iter_loss: 0.47681692242622375
train_iter_loss: 0.2507070004940033
train_iter_loss: 0.28379011154174805
train_iter_loss: 0.24438107013702393
train_iter_loss: 0.2369537353515625
train_iter_loss: 0.34559211134910583
train_iter_loss: 0.5026730298995972
train_iter_loss: 0.2947123944759369
train_iter_loss: 0.40016233921051025
train_iter_loss: 0.14609946310520172
train_iter_loss: 0.36556005477905273
train_iter_loss: 0.30288878083229065
train_iter_loss: 0.14979438483715057
train_iter_loss: 0.24051009118556976
train_iter_loss: 0.2949110269546509
train_iter_loss: 0.3555189073085785
train_iter_loss: 0.2164122313261032
train_iter_loss: 0.2460259199142456
train_iter_loss: 0.14563338458538055
train_iter_loss: 0.23239518702030182
train_iter_loss: 0.2211112678050995
train_iter_loss: 0.2933543622493744
train_iter_loss: 0.3306090831756592
train_iter_loss: 0.24799016118049622
train_iter_loss: 0.17455808818340302
train_iter_loss: 0.3983539342880249
train_iter_loss: 0.2730858325958252
train_iter_loss: 0.21887612342834473
train_iter_loss: 0.37202367186546326
train_iter_loss: 0.2148262858390808
train_iter_loss: 0.24565522372722626
train_iter_loss: 0.24482934176921844
train_iter_loss: 0.20499306917190552
train_iter_loss: 0.3393633961677551
train_iter_loss: 0.3419879078865051
train_iter_loss: 0.43233799934387207
train_iter_loss: 0.1827084720134735
train_iter_loss: 0.33366867899894714
train_iter_loss: 0.25740745663642883
train_iter_loss: 0.362051784992218
train_iter_loss: 0.11986655741930008
train_iter_loss: 0.13411293923854828
train_iter_loss: 0.2295822650194168
train_iter_loss: 0.3545377552509308
train_iter_loss: 0.21829961240291595
train_iter_loss: 0.4668995141983032
train_iter_loss: 0.1799798458814621
train_iter_loss: 0.19357454776763916
train_iter_loss: 0.27466100454330444
train_iter_loss: 0.2115938812494278
train_iter_loss: 0.1829073429107666
train_iter_loss: 0.227828711271286
train loss :0.2666
---------------------
Validation seg loss: 0.3741737673946498 at epoch 714
epoch =    715/  1000, exp = train
train_iter_loss: 0.1811523288488388
train_iter_loss: 0.2724730670452118
train_iter_loss: 0.2382737249135971
train_iter_loss: 0.1214529275894165
train_iter_loss: 0.19309936463832855
train_iter_loss: 0.19781053066253662
train_iter_loss: 0.18615007400512695
train_iter_loss: 0.24658606946468353
train_iter_loss: 0.2761864960193634
train_iter_loss: 0.2920398712158203
train_iter_loss: 0.25746145844459534
train_iter_loss: 0.2602884769439697
train_iter_loss: 0.41183170676231384
train_iter_loss: 0.15525713562965393
train_iter_loss: 0.18285872042179108
train_iter_loss: 0.2791643440723419
train_iter_loss: 0.1483403891324997
train_iter_loss: 0.15852776169776917
train_iter_loss: 0.22581222653388977
train_iter_loss: 0.2570420801639557
train_iter_loss: 0.38057947158813477
train_iter_loss: 0.16667047142982483
train_iter_loss: 0.25881853699684143
train_iter_loss: 0.3057546317577362
train_iter_loss: 0.2819331884384155
train_iter_loss: 0.3928654193878174
train_iter_loss: 0.13148394227027893
train_iter_loss: 0.17252501845359802
train_iter_loss: 0.4240953028202057
train_iter_loss: 0.30377110838890076
train_iter_loss: 0.2956054210662842
train_iter_loss: 0.17631970345973969
train_iter_loss: 0.3040914833545685
train_iter_loss: 0.3075897693634033
train_iter_loss: 0.21156638860702515
train_iter_loss: 0.19162610173225403
train_iter_loss: 0.23627278208732605
train_iter_loss: 0.21916982531547546
train_iter_loss: 0.3182494342327118
train_iter_loss: 0.2970024347305298
train_iter_loss: 0.2539708912372589
train_iter_loss: 0.38541916012763977
train_iter_loss: 0.3140190839767456
train_iter_loss: 0.27710315585136414
train_iter_loss: 0.3569409251213074
train_iter_loss: 0.13097083568572998
train_iter_loss: 0.17620433866977692
train_iter_loss: 0.1608179360628128
train_iter_loss: 0.13589058816432953
train_iter_loss: 0.24353033304214478
train_iter_loss: 0.11911582201719284
train_iter_loss: 0.37775397300720215
train_iter_loss: 0.22030849754810333
train_iter_loss: 0.36086300015449524
train_iter_loss: 0.304464191198349
train_iter_loss: 0.26237261295318604
train_iter_loss: 0.33410826325416565
train_iter_loss: 0.35814881324768066
train_iter_loss: 0.2815546691417694
train_iter_loss: 0.32058724761009216
train_iter_loss: 0.1097969263792038
train_iter_loss: 0.16722510755062103
train_iter_loss: 0.3232789635658264
train_iter_loss: 0.34405097365379333
train_iter_loss: 0.5067357420921326
train_iter_loss: 0.25396493077278137
train_iter_loss: 0.30770695209503174
train_iter_loss: 0.27574416995048523
train_iter_loss: 0.17719143629074097
train_iter_loss: 0.3854358494281769
train_iter_loss: 0.23405444622039795
train_iter_loss: 0.19903843104839325
train_iter_loss: 0.3008636236190796
train_iter_loss: 0.28937309980392456
train_iter_loss: 0.17678695917129517
train_iter_loss: 0.28078651428222656
train_iter_loss: 0.22226457297801971
train_iter_loss: 0.14902769029140472
train_iter_loss: 0.16501858830451965
train_iter_loss: 0.2771947979927063
train_iter_loss: 0.1546602100133896
train_iter_loss: 0.3276667892932892
train_iter_loss: 0.20956072211265564
train_iter_loss: 0.30539649724960327
train_iter_loss: 0.2826182246208191
train_iter_loss: 0.4254760146141052
train_iter_loss: 0.355521559715271
train_iter_loss: 0.37012535333633423
train_iter_loss: 0.3418550491333008
train_iter_loss: 0.2916971445083618
train_iter_loss: 0.30190563201904297
train_iter_loss: 0.3014131784439087
train_iter_loss: 0.21245349943637848
train_iter_loss: 0.2599131464958191
train_iter_loss: 0.24335752427577972
train_iter_loss: 0.48757684230804443
train_iter_loss: 0.2260906845331192
train_iter_loss: 0.1145397201180458
train_iter_loss: 0.2905099093914032
train_iter_loss: 0.3114432394504547
train loss :0.2652
---------------------
Validation seg loss: 0.34664692930912355 at epoch 715
epoch =    716/  1000, exp = train
train_iter_loss: 0.17615768313407898
train_iter_loss: 0.3456890881061554
train_iter_loss: 0.3017323613166809
train_iter_loss: 0.22123053669929504
train_iter_loss: 0.1868535727262497
train_iter_loss: 0.23684950172901154
train_iter_loss: 0.4212409555912018
train_iter_loss: 0.1393480747938156
train_iter_loss: 0.3644607961177826
train_iter_loss: 0.11146057397127151
train_iter_loss: 0.3646574318408966
train_iter_loss: 0.20600520074367523
train_iter_loss: 0.30327093601226807
train_iter_loss: 0.37772059440612793
train_iter_loss: 0.2525521218776703
train_iter_loss: 0.27059951424598694
train_iter_loss: 0.2503599226474762
train_iter_loss: 0.20928837358951569
train_iter_loss: 0.206577867269516
train_iter_loss: 0.3676219880580902
train_iter_loss: 0.20745326578617096
train_iter_loss: 0.25244513154029846
train_iter_loss: 0.4144059419631958
train_iter_loss: 0.2505478858947754
train_iter_loss: 0.441787987947464
train_iter_loss: 0.2376505583524704
train_iter_loss: 0.1842336505651474
train_iter_loss: 0.3120606541633606
train_iter_loss: 0.16809701919555664
train_iter_loss: 0.4157649874687195
train_iter_loss: 0.17375311255455017
train_iter_loss: 0.29071366786956787
train_iter_loss: 0.23909100890159607
train_iter_loss: 0.215249165892601
train_iter_loss: 0.07437366247177124
train_iter_loss: 0.23850156366825104
train_iter_loss: 0.19738991558551788
train_iter_loss: 0.24566949903964996
train_iter_loss: 0.13934014737606049
train_iter_loss: 0.19565372169017792
train_iter_loss: 0.19996090233325958
train_iter_loss: 0.23765838146209717
train_iter_loss: 0.323993980884552
train_iter_loss: 0.20091289281845093
train_iter_loss: 0.38620683550834656
train_iter_loss: 0.2584240436553955
train_iter_loss: 0.2172793745994568
train_iter_loss: 0.3674545884132385
train_iter_loss: 0.22206611931324005
train_iter_loss: 0.25218212604522705
train_iter_loss: 0.3049089312553406
train_iter_loss: 0.3243219256401062
train_iter_loss: 0.36767131090164185
train_iter_loss: 0.3875703513622284
train_iter_loss: 0.1937747597694397
train_iter_loss: 0.22817449271678925
train_iter_loss: 0.28980588912963867
train_iter_loss: 0.2862281799316406
train_iter_loss: 0.22771362960338593
train_iter_loss: 0.23327691853046417
train_iter_loss: 0.09045884758234024
train_iter_loss: 0.2740110754966736
train_iter_loss: 0.14439935982227325
train_iter_loss: 0.2284466177225113
train_iter_loss: 0.22460953891277313
train_iter_loss: 0.34402137994766235
train_iter_loss: 0.3142150640487671
train_iter_loss: 0.37710854411125183
train_iter_loss: 0.18478895723819733
train_iter_loss: 0.2864893972873688
train_iter_loss: 0.10434490442276001
train_iter_loss: 0.18736137449741364
train_iter_loss: 0.2815103232860565
train_iter_loss: 0.289029598236084
train_iter_loss: 0.1917734444141388
train_iter_loss: 0.3721327483654022
train_iter_loss: 0.21246661245822906
train_iter_loss: 0.41130921244621277
train_iter_loss: 0.2718329131603241
train_iter_loss: 0.3004242479801178
train_iter_loss: 0.14056938886642456
train_iter_loss: 0.31896156072616577
train_iter_loss: 0.24449685215950012
train_iter_loss: 0.3071748912334442
train_iter_loss: 0.09228646010160446
train_iter_loss: 0.269273966550827
train_iter_loss: 0.12119434773921967
train_iter_loss: 0.3582262098789215
train_iter_loss: 0.36867061257362366
train_iter_loss: 0.2137177288532257
train_iter_loss: 0.3654361069202423
train_iter_loss: 0.23350512981414795
train_iter_loss: 0.33840325474739075
train_iter_loss: 0.12080682814121246
train_iter_loss: 0.15707813203334808
train_iter_loss: 0.2121288925409317
train_iter_loss: 0.10638750344514847
train_iter_loss: 0.2759915292263031
train_iter_loss: 0.15958549082279205
train_iter_loss: 0.3368757367134094
train loss :0.2581
---------------------
Validation seg loss: 0.36465246598499845 at epoch 716
epoch =    717/  1000, exp = train
train_iter_loss: 0.23565053939819336
train_iter_loss: 0.34428685903549194
train_iter_loss: 0.18875502049922943
train_iter_loss: 0.3186439871788025
train_iter_loss: 0.16058175265789032
train_iter_loss: 0.3476314842700958
train_iter_loss: 0.23593677580356598
train_iter_loss: 0.22137734293937683
train_iter_loss: 0.28116586804389954
train_iter_loss: 0.06410759687423706
train_iter_loss: 0.28252556920051575
train_iter_loss: 0.31986942887306213
train_iter_loss: 0.22386431694030762
train_iter_loss: 0.23769326508045197
train_iter_loss: 0.30724188685417175
train_iter_loss: 0.28354883193969727
train_iter_loss: 0.38047268986701965
train_iter_loss: 0.38546210527420044
train_iter_loss: 0.3454483449459076
train_iter_loss: 0.3129531145095825
train_iter_loss: 0.21163034439086914
train_iter_loss: 0.23245231807231903
train_iter_loss: 0.24640622735023499
train_iter_loss: 0.45253250002861023
train_iter_loss: 0.15072773396968842
train_iter_loss: 0.3118867576122284
train_iter_loss: 0.3236130475997925
train_iter_loss: 0.27555376291275024
train_iter_loss: 0.3679283857345581
train_iter_loss: 0.10277224332094193
train_iter_loss: 0.20290689170360565
train_iter_loss: 0.36266109347343445
train_iter_loss: 0.30843016505241394
train_iter_loss: 0.2774602174758911
train_iter_loss: 0.24443063139915466
train_iter_loss: 0.26360955834388733
train_iter_loss: 0.2620336711406708
train_iter_loss: 0.25902655720710754
train_iter_loss: 0.3419005274772644
train_iter_loss: 0.21182134747505188
train_iter_loss: 0.17567594349384308
train_iter_loss: 0.2653760313987732
train_iter_loss: 0.3466900885105133
train_iter_loss: 0.20909665524959564
train_iter_loss: 0.34896305203437805
train_iter_loss: 0.10760635137557983
train_iter_loss: 0.15760770440101624
train_iter_loss: 0.09034644067287445
train_iter_loss: 0.21591784060001373
train_iter_loss: 0.08192761987447739
train_iter_loss: 0.12393881380558014
train_iter_loss: 0.428072988986969
train_iter_loss: 0.30536842346191406
train_iter_loss: 0.3294800817966461
train_iter_loss: 0.2679848372936249
train_iter_loss: 0.07027261704206467
train_iter_loss: 0.2591874301433563
train_iter_loss: 0.4158945083618164
train_iter_loss: 0.19658619165420532
train_iter_loss: 0.12039844691753387
train_iter_loss: 0.43767687678337097
train_iter_loss: 0.2653197646141052
train_iter_loss: 0.18791529536247253
train_iter_loss: 0.24218906462192535
train_iter_loss: 0.17467793822288513
train_iter_loss: 0.21138055622577667
train_iter_loss: 0.043500687927007675
train_iter_loss: 0.25859278440475464
train_iter_loss: 0.2123272716999054
train_iter_loss: 0.20907793939113617
train_iter_loss: 0.2891194820404053
train_iter_loss: 0.2300499677658081
train_iter_loss: 0.3291504979133606
train_iter_loss: 0.3877731263637543
train_iter_loss: 0.3917165994644165
train_iter_loss: 0.23297476768493652
train_iter_loss: 0.21301180124282837
train_iter_loss: 0.260201096534729
train_iter_loss: 0.23028865456581116
train_iter_loss: 0.2897615432739258
train_iter_loss: 0.3368825614452362
train_iter_loss: 0.4164546728134155
train_iter_loss: 0.3491944968700409
train_iter_loss: 0.2842458486557007
train_iter_loss: 0.09765142947435379
train_iter_loss: 0.29957282543182373
train_iter_loss: 0.07609673589468002
train_iter_loss: 0.3154822289943695
train_iter_loss: 0.2712595462799072
train_iter_loss: 0.27123406529426575
train_iter_loss: 0.2902253568172455
train_iter_loss: 0.24176271259784698
train_iter_loss: 0.34025338292121887
train_iter_loss: 0.2085018754005432
train_iter_loss: 0.1469825953245163
train_iter_loss: 0.24155759811401367
train_iter_loss: 0.3684997856616974
train_iter_loss: 0.21548984944820404
train_iter_loss: 0.16204774379730225
train_iter_loss: 0.0954490676522255
train loss :0.2579
---------------------
Validation seg loss: 0.34548759930503536 at epoch 717
epoch =    718/  1000, exp = train
train_iter_loss: 0.22630713880062103
train_iter_loss: 0.22243089973926544
train_iter_loss: 0.4122562110424042
train_iter_loss: 0.29749825596809387
train_iter_loss: 0.29456043243408203
train_iter_loss: 0.27996864914894104
train_iter_loss: 0.3345145881175995
train_iter_loss: 0.24807097017765045
train_iter_loss: 0.30772554874420166
train_iter_loss: 0.20084905624389648
train_iter_loss: 0.19609247148036957
train_iter_loss: 0.263036847114563
train_iter_loss: 0.22611992061138153
train_iter_loss: 0.26673653721809387
train_iter_loss: 0.24146296083927155
train_iter_loss: 0.3177764117717743
train_iter_loss: 0.24522583186626434
train_iter_loss: 0.1961653083562851
train_iter_loss: 0.3459945321083069
train_iter_loss: 0.33852124214172363
train_iter_loss: 0.2882585823535919
train_iter_loss: 0.36323463916778564
train_iter_loss: 0.2853918969631195
train_iter_loss: 0.2520073354244232
train_iter_loss: 0.19477884471416473
train_iter_loss: 0.34810835123062134
train_iter_loss: 0.21773812174797058
train_iter_loss: 0.3017209768295288
train_iter_loss: 0.2054937332868576
train_iter_loss: 0.38687917590141296
train_iter_loss: 0.14778956770896912
train_iter_loss: 0.23988521099090576
train_iter_loss: 0.16066361963748932
train_iter_loss: 0.16143977642059326
train_iter_loss: 0.1853300929069519
train_iter_loss: 0.19149868190288544
train_iter_loss: 0.22931574285030365
train_iter_loss: 0.18528765439987183
train_iter_loss: 0.3340740203857422
train_iter_loss: 0.3275885283946991
train_iter_loss: 0.2479007989168167
train_iter_loss: 0.1504811942577362
train_iter_loss: 0.17872507870197296
train_iter_loss: 0.2747045159339905
train_iter_loss: 0.37571394443511963
train_iter_loss: 0.30207401514053345
train_iter_loss: 0.14566735923290253
train_iter_loss: 0.3048326075077057
train_iter_loss: 0.27587488293647766
train_iter_loss: 0.2697369456291199
train_iter_loss: 0.27681243419647217
train_iter_loss: 0.28854265809059143
train_iter_loss: 0.15675510466098785
train_iter_loss: 0.4052485227584839
train_iter_loss: 0.404398649930954
train_iter_loss: 0.33151987195014954
train_iter_loss: 0.12167967855930328
train_iter_loss: 0.2962435185909271
train_iter_loss: 0.20257657766342163
train_iter_loss: 0.3977641463279724
train_iter_loss: 0.27569273114204407
train_iter_loss: 0.2186022251844406
train_iter_loss: 0.31700852513313293
train_iter_loss: 0.3097091019153595
train_iter_loss: 0.1866961121559143
train_iter_loss: 0.31924739480018616
train_iter_loss: 0.5325980186462402
train_iter_loss: 0.35492223501205444
train_iter_loss: 0.28180956840515137
train_iter_loss: 0.21671448647975922
train_iter_loss: 0.28800156712532043
train_iter_loss: 0.22368791699409485
train_iter_loss: 0.21920976042747498
train_iter_loss: 0.2935614585876465
train_iter_loss: 0.3044791519641876
train_iter_loss: 0.2682114243507385
train_iter_loss: 0.4366830289363861
train_iter_loss: 0.23814056813716888
train_iter_loss: 0.20503686368465424
train_iter_loss: 0.2535943388938904
train_iter_loss: 0.222381591796875
train_iter_loss: 0.20843668282032013
train_iter_loss: 0.18630987405776978
train_iter_loss: 0.3937220275402069
train_iter_loss: 0.19552232325077057
train_iter_loss: 0.3013353943824768
train_iter_loss: 0.221559539437294
train_iter_loss: 0.16149623692035675
train_iter_loss: 0.10307995975017548
train_iter_loss: 0.2197927087545395
train_iter_loss: 0.1520233154296875
train_iter_loss: 0.39413079619407654
train_iter_loss: 0.2051388919353485
train_iter_loss: 0.08213333785533905
train_iter_loss: 0.33936935663223267
train_iter_loss: 0.3030168414115906
train_iter_loss: 0.257559210062027
train_iter_loss: 0.33844462037086487
train_iter_loss: 0.07781055569648743
train_iter_loss: 0.2772037982940674
train loss :0.2652
---------------------
Validation seg loss: 0.3619343121272494 at epoch 718
epoch =    719/  1000, exp = train
train_iter_loss: 0.29265478253364563
train_iter_loss: 0.07099699974060059
train_iter_loss: 0.16116859018802643
train_iter_loss: 0.21198026835918427
train_iter_loss: 0.22207941114902496
train_iter_loss: 0.23240041732788086
train_iter_loss: 0.2316904515028
train_iter_loss: 0.15687209367752075
train_iter_loss: 0.3107585310935974
train_iter_loss: 0.3019998371601105
train_iter_loss: 0.2520691752433777
train_iter_loss: 0.18573741614818573
train_iter_loss: 0.24943527579307556
train_iter_loss: 0.25876346230506897
train_iter_loss: 0.3081529438495636
train_iter_loss: 0.1251853108406067
train_iter_loss: 0.1746397763490677
train_iter_loss: 0.4729062616825104
train_iter_loss: 0.28795552253723145
train_iter_loss: 0.2703810930252075
train_iter_loss: 0.2817341983318329
train_iter_loss: 0.2310948669910431
train_iter_loss: 0.30009767413139343
train_iter_loss: 0.18645329773426056
train_iter_loss: 0.18175646662712097
train_iter_loss: 0.16810016334056854
train_iter_loss: 0.22317349910736084
train_iter_loss: 0.28286251425743103
train_iter_loss: 0.2332467883825302
train_iter_loss: 0.25411367416381836
train_iter_loss: 0.2816157042980194
train_iter_loss: 0.279748797416687
train_iter_loss: 0.35835587978363037
train_iter_loss: 0.07514449954032898
train_iter_loss: 0.20110094547271729
train_iter_loss: 0.2694101333618164
train_iter_loss: 0.25637659430503845
train_iter_loss: 0.20293065905570984
train_iter_loss: 0.3514147996902466
train_iter_loss: 0.3379775285720825
train_iter_loss: 0.2470449060201645
train_iter_loss: 0.2045716941356659
train_iter_loss: 0.31945350766181946
train_iter_loss: 0.19385863840579987
train_iter_loss: 0.35715800523757935
train_iter_loss: 0.23035462200641632
train_iter_loss: 0.25856587290763855
train_iter_loss: 0.283008337020874
train_iter_loss: 0.2758385241031647
train_iter_loss: 0.3264347016811371
train_iter_loss: 0.24390465021133423
train_iter_loss: 0.34919077157974243
train_iter_loss: 0.42710989713668823
train_iter_loss: 0.09656615555286407
train_iter_loss: 0.25391685962677
train_iter_loss: 0.2804987132549286
train_iter_loss: 0.3356790244579315
train_iter_loss: 0.3275373578071594
train_iter_loss: 0.3128789961338043
train_iter_loss: 0.3450566530227661
train_iter_loss: 0.18133066594600677
train_iter_loss: 0.29263514280319214
train_iter_loss: 0.20730887353420258
train_iter_loss: 0.2550662159919739
train_iter_loss: 0.19821079075336456
train_iter_loss: 0.1348523646593094
train_iter_loss: 0.3658738136291504
train_iter_loss: 0.21108266711235046
train_iter_loss: 0.42001691460609436
train_iter_loss: 0.1783725768327713
train_iter_loss: 0.20775972306728363
train_iter_loss: 0.21212948858737946
train_iter_loss: 0.22962695360183716
train_iter_loss: 0.33768153190612793
train_iter_loss: 0.17585386335849762
train_iter_loss: 0.4048076272010803
train_iter_loss: 0.25928062200546265
train_iter_loss: 0.2399083971977234
train_iter_loss: 0.3495117425918579
train_iter_loss: 0.39494389295578003
train_iter_loss: 0.2811197340488434
train_iter_loss: 0.28119316697120667
train_iter_loss: 0.24953725934028625
train_iter_loss: 0.2949366569519043
train_iter_loss: 0.33216363191604614
train_iter_loss: 0.24693281948566437
train_iter_loss: 0.24476107954978943
train_iter_loss: 0.13794474303722382
train_iter_loss: 0.3057655394077301
train_iter_loss: 0.20707973837852478
train_iter_loss: 0.1462688148021698
train_iter_loss: 0.1941329687833786
train_iter_loss: 0.21066243946552277
train_iter_loss: 0.40113309025764465
train_iter_loss: 0.154466912150383
train_iter_loss: 0.2165880799293518
train_iter_loss: 0.23473423719406128
train_iter_loss: 0.3423078954219818
train_iter_loss: 0.13267074525356293
train_iter_loss: 0.20469801127910614
train loss :0.2574
---------------------
Validation seg loss: 0.3497371247899279 at epoch 719
epoch =    720/  1000, exp = train
train_iter_loss: 0.4242662489414215
train_iter_loss: 0.1491219848394394
train_iter_loss: 0.36487406492233276
train_iter_loss: 0.13516870141029358
train_iter_loss: 0.4061172306537628
train_iter_loss: 0.1477658599615097
train_iter_loss: 0.3382132649421692
train_iter_loss: 0.3481452763080597
train_iter_loss: 0.14510655403137207
train_iter_loss: 0.3478119969367981
train_iter_loss: 0.24361367523670197
train_iter_loss: 0.09271322935819626
train_iter_loss: 0.20557090640068054
train_iter_loss: 0.23632456362247467
train_iter_loss: 0.2846274971961975
train_iter_loss: 0.3461022675037384
train_iter_loss: 0.32455524802207947
train_iter_loss: 0.4200681746006012
train_iter_loss: 0.40491247177124023
train_iter_loss: 0.23881879448890686
train_iter_loss: 0.28559261560440063
train_iter_loss: 0.27389392256736755
train_iter_loss: 0.24787278473377228
train_iter_loss: 0.23199261724948883
train_iter_loss: 0.47863468527793884
train_iter_loss: 0.19895261526107788
train_iter_loss: 0.3855925500392914
train_iter_loss: 0.1466309130191803
train_iter_loss: 0.21110817790031433
train_iter_loss: 0.21699228882789612
train_iter_loss: 0.36154642701148987
train_iter_loss: 0.3613022267818451
train_iter_loss: 0.15538769960403442
train_iter_loss: 0.19256818294525146
train_iter_loss: 0.16504722833633423
train_iter_loss: 0.2814445197582245
train_iter_loss: 0.2136322557926178
train_iter_loss: 0.13044849038124084
train_iter_loss: 0.22941167652606964
train_iter_loss: 0.2990814447402954
train_iter_loss: 0.19919338822364807
train_iter_loss: 0.23816446959972382
train_iter_loss: 0.3509690761566162
train_iter_loss: 0.17913471162319183
train_iter_loss: 0.25033050775527954
train_iter_loss: 0.1252834051847458
train_iter_loss: 0.18985515832901
train_iter_loss: 0.17172782123088837
train_iter_loss: 0.2536435127258301
train_iter_loss: 0.1657734364271164
train_iter_loss: 0.07422902435064316
train_iter_loss: 0.42518162727355957
train_iter_loss: 0.21558476984500885
train_iter_loss: 0.23549659550189972
train_iter_loss: 0.25320151448249817
train_iter_loss: 0.2356528639793396
train_iter_loss: 0.2741425633430481
train_iter_loss: 0.6028832197189331
train_iter_loss: 0.19394466280937195
train_iter_loss: 0.36432960629463196
train_iter_loss: 0.3360704481601715
train_iter_loss: 0.4014810025691986
train_iter_loss: 0.16910748183727264
train_iter_loss: 0.2246418446302414
train_iter_loss: 0.250179260969162
train_iter_loss: 0.19187511503696442
train_iter_loss: 0.17730911076068878
train_iter_loss: 0.3137470781803131
train_iter_loss: 0.22652125358581543
train_iter_loss: 0.2646108567714691
train_iter_loss: 0.24281242489814758
train_iter_loss: 0.24880097806453705
train_iter_loss: 0.28815439343452454
train_iter_loss: 0.20487898588180542
train_iter_loss: 0.44129541516304016
train_iter_loss: 0.13182592391967773
train_iter_loss: 0.1805330514907837
train_iter_loss: 0.346416175365448
train_iter_loss: 0.31134629249572754
train_iter_loss: 0.20625542104244232
train_iter_loss: 0.31610292196273804
train_iter_loss: 0.18317987024784088
train_iter_loss: 0.3487710952758789
train_iter_loss: 0.3511013388633728
train_iter_loss: 0.22088956832885742
train_iter_loss: 0.194572314620018
train_iter_loss: 0.2756827473640442
train_iter_loss: 0.23520083725452423
train_iter_loss: 0.2569846212863922
train_iter_loss: 0.25847992300987244
train_iter_loss: 0.26103952527046204
train_iter_loss: 0.39667144417762756
train_iter_loss: 0.2702791392803192
train_iter_loss: 0.2734246253967285
train_iter_loss: 0.27532878518104553
train_iter_loss: 0.36084678769111633
train_iter_loss: 0.26435163617134094
train_iter_loss: 0.22151625156402588
train_iter_loss: 0.29556745290756226
train_iter_loss: 0.1590183526277542
train loss :0.2650
---------------------
Validation seg loss: 0.34532109938807926 at epoch 720
epoch =    721/  1000, exp = train
train_iter_loss: 0.16470925509929657
train_iter_loss: 0.3387795388698578
train_iter_loss: 0.31896495819091797
train_iter_loss: 0.1795702874660492
train_iter_loss: 0.3405228853225708
train_iter_loss: 0.2211361974477768
train_iter_loss: 0.08635263890028
train_iter_loss: 0.3634200692176819
train_iter_loss: 0.31545013189315796
train_iter_loss: 0.14931902289390564
train_iter_loss: 0.2887222468852997
train_iter_loss: 0.19682011008262634
train_iter_loss: 0.18393315374851227
train_iter_loss: 0.1964729130268097
train_iter_loss: 0.16878961026668549
train_iter_loss: 0.14765067398548126
train_iter_loss: 0.2496037781238556
train_iter_loss: 0.14693059027194977
train_iter_loss: 0.21225306391716003
train_iter_loss: 0.23613788187503815
train_iter_loss: 0.3968736529350281
train_iter_loss: 0.1500130593776703
train_iter_loss: 0.2972263693809509
train_iter_loss: 0.3451233208179474
train_iter_loss: 0.25828173756599426
train_iter_loss: 0.14405813813209534
train_iter_loss: 0.3103938698768616
train_iter_loss: 0.20957830548286438
train_iter_loss: 0.37465280294418335
train_iter_loss: 0.2634024918079376
train_iter_loss: 0.21407701075077057
train_iter_loss: 0.256699800491333
train_iter_loss: 0.3362552523612976
train_iter_loss: 0.20595373213291168
train_iter_loss: 0.1872037798166275
train_iter_loss: 0.1833961308002472
train_iter_loss: 0.17828573286533356
train_iter_loss: 0.24900081753730774
train_iter_loss: 0.2773887813091278
train_iter_loss: 0.22950704395771027
train_iter_loss: 0.3125796616077423
train_iter_loss: 0.3529655337333679
train_iter_loss: 0.30560562014579773
train_iter_loss: 0.4469650089740753
train_iter_loss: 0.1727849692106247
train_iter_loss: 0.2978718876838684
train_iter_loss: 0.2523554563522339
train_iter_loss: 0.3119962811470032
train_iter_loss: 0.25501346588134766
train_iter_loss: 0.31458017230033875
train_iter_loss: 0.31310364603996277
train_iter_loss: 0.25419217348098755
train_iter_loss: 0.31440284848213196
train_iter_loss: 0.21342237293720245
train_iter_loss: 0.22709700465202332
train_iter_loss: 0.21568571031093597
train_iter_loss: 0.2322535216808319
train_iter_loss: 0.23393598198890686
train_iter_loss: 0.28770068287849426
train_iter_loss: 0.3075312077999115
train_iter_loss: 0.08681023865938187
train_iter_loss: 0.2753535807132721
train_iter_loss: 0.338225781917572
train_iter_loss: 0.2865529954433441
train_iter_loss: 0.3508521616458893
train_iter_loss: 0.36611831188201904
train_iter_loss: 0.1788039654493332
train_iter_loss: 0.22805280983448029
train_iter_loss: 0.34518954157829285
train_iter_loss: 0.18451941013336182
train_iter_loss: 0.18853692710399628
train_iter_loss: 0.27815747261047363
train_iter_loss: 0.32452359795570374
train_iter_loss: 0.17563240230083466
train_iter_loss: 0.29359713196754456
train_iter_loss: 0.2879314422607422
train_iter_loss: 0.2717836797237396
train_iter_loss: 0.1905398666858673
train_iter_loss: 0.2453342229127884
train_iter_loss: 0.19137024879455566
train_iter_loss: 0.34004175662994385
train_iter_loss: 0.34341326355934143
train_iter_loss: 0.39792072772979736
train_iter_loss: 0.1977262794971466
train_iter_loss: 0.2839782238006592
train_iter_loss: 0.2635432481765747
train_iter_loss: 0.2682721018791199
train_iter_loss: 0.18407100439071655
train_iter_loss: 0.2541949450969696
train_iter_loss: 0.3473372161388397
train_iter_loss: 0.29381248354911804
train_iter_loss: 0.25028103590011597
train_iter_loss: 0.4506685733795166
train_iter_loss: 0.39478355646133423
train_iter_loss: 0.06876740604639053
train_iter_loss: 0.20183391869068146
train_iter_loss: 0.1715133786201477
train_iter_loss: 0.4609343111515045
train_iter_loss: 0.12614178657531738
train_iter_loss: 0.2522616386413574
train loss :0.2610
---------------------
Validation seg loss: 0.3521931678592667 at epoch 721
epoch =    722/  1000, exp = train
train_iter_loss: 0.40362098813056946
train_iter_loss: 0.3081328272819519
train_iter_loss: 0.13385187089443207
train_iter_loss: 0.3246183395385742
train_iter_loss: 0.13364294171333313
train_iter_loss: 0.34861451387405396
train_iter_loss: 0.40285196900367737
train_iter_loss: 0.3764321208000183
train_iter_loss: 0.16887609660625458
train_iter_loss: 0.25913652777671814
train_iter_loss: 0.07733175158500671
train_iter_loss: 0.17787913978099823
train_iter_loss: 0.2783259451389313
train_iter_loss: 0.4451861083507538
train_iter_loss: 0.24949656426906586
train_iter_loss: 0.26095694303512573
train_iter_loss: 0.3016546666622162
train_iter_loss: 0.30930766463279724
train_iter_loss: 0.311881959438324
train_iter_loss: 0.2620431184768677
train_iter_loss: 0.2401253879070282
train_iter_loss: 0.19536370038986206
train_iter_loss: 0.29195985198020935
train_iter_loss: 0.29652470350265503
train_iter_loss: 0.20872856676578522
train_iter_loss: 0.27068525552749634
train_iter_loss: 0.3204152286052704
train_iter_loss: 0.2930520176887512
train_iter_loss: 0.19028009474277496
train_iter_loss: 0.23542390763759613
train_iter_loss: 0.1701265275478363
train_iter_loss: 0.17649798095226288
train_iter_loss: 0.26410362124443054
train_iter_loss: 0.25753718614578247
train_iter_loss: 0.33919599652290344
train_iter_loss: 0.2155369520187378
train_iter_loss: 0.26348474621772766
train_iter_loss: 0.3237193822860718
train_iter_loss: 0.232738196849823
train_iter_loss: 0.33129847049713135
train_iter_loss: 0.25506216287612915
train_iter_loss: 0.2406143695116043
train_iter_loss: 0.25444304943084717
train_iter_loss: 0.293519526720047
train_iter_loss: 0.24784401059150696
train_iter_loss: 0.3013915717601776
train_iter_loss: 0.19656431674957275
train_iter_loss: 0.15956392884254456
train_iter_loss: 0.29338979721069336
train_iter_loss: 0.2510128319263458
train_iter_loss: 0.23094822466373444
train_iter_loss: 0.17743222415447235
train_iter_loss: 0.17960117757320404
train_iter_loss: 0.21782635152339935
train_iter_loss: 0.3082546889781952
train_iter_loss: 0.37644705176353455
train_iter_loss: 0.29329410195350647
train_iter_loss: 0.2650166153907776
train_iter_loss: 0.3351373076438904
train_iter_loss: 0.3812645673751831
train_iter_loss: 0.26371192932128906
train_iter_loss: 0.2269357591867447
train_iter_loss: 0.2345069944858551
train_iter_loss: 0.266489714384079
train_iter_loss: 0.27535954117774963
train_iter_loss: 0.19593507051467896
train_iter_loss: 0.3764074444770813
train_iter_loss: 0.07587369531393051
train_iter_loss: 0.12771980464458466
train_iter_loss: 0.13884568214416504
train_iter_loss: 0.23839615285396576
train_iter_loss: 0.1778024286031723
train_iter_loss: 0.2831973135471344
train_iter_loss: 0.4234830141067505
train_iter_loss: 0.2678263783454895
train_iter_loss: 0.1805763989686966
train_iter_loss: 0.3454521894454956
train_iter_loss: 0.27486997842788696
train_iter_loss: 0.34521812200546265
train_iter_loss: 0.3020983040332794
train_iter_loss: 0.4053349196910858
train_iter_loss: 0.29749730229377747
train_iter_loss: 0.15768197178840637
train_iter_loss: 0.14515316486358643
train_iter_loss: 0.30506062507629395
train_iter_loss: 0.09735307842493057
train_iter_loss: 0.1608329564332962
train_iter_loss: 0.14827348291873932
train_iter_loss: 0.33628082275390625
train_iter_loss: 0.13685300946235657
train_iter_loss: 0.24863237142562866
train_iter_loss: 0.34160640835762024
train_iter_loss: 0.23230458796024323
train_iter_loss: 0.3866806924343109
train_iter_loss: 0.2855060398578644
train_iter_loss: 0.25997471809387207
train_iter_loss: 0.12692947685718536
train_iter_loss: 0.27936097979545593
train_iter_loss: 0.19417396187782288
train_iter_loss: 0.37158262729644775
train loss :0.2611
---------------------
Validation seg loss: 0.3647837470953335 at epoch 722
epoch =    723/  1000, exp = train
train_iter_loss: 0.1681230515241623
train_iter_loss: 0.508143961429596
train_iter_loss: 0.2565092146396637
train_iter_loss: 0.038209445774555206
train_iter_loss: 0.30511289834976196
train_iter_loss: 0.19521598517894745
train_iter_loss: 0.10691361129283905
train_iter_loss: 0.1833830326795578
train_iter_loss: 0.18502913415431976
train_iter_loss: 0.28635668754577637
train_iter_loss: 0.18098799884319305
train_iter_loss: 0.1649583876132965
train_iter_loss: 0.3233299255371094
train_iter_loss: 0.17427116632461548
train_iter_loss: 0.03862347453832626
train_iter_loss: 0.285184770822525
train_iter_loss: 0.2902504503726959
train_iter_loss: 0.2618667185306549
train_iter_loss: 0.37078914046287537
train_iter_loss: 0.26339370012283325
train_iter_loss: 0.4506343603134155
train_iter_loss: 0.28428009152412415
train_iter_loss: 0.3240249752998352
train_iter_loss: 0.14322516322135925
train_iter_loss: 0.2529635429382324
train_iter_loss: 0.3304126262664795
train_iter_loss: 0.39471080899238586
train_iter_loss: 0.1450679451227188
train_iter_loss: 0.25614437460899353
train_iter_loss: 0.24680715799331665
train_iter_loss: 0.3199708163738251
train_iter_loss: 0.31066060066223145
train_iter_loss: 0.3133040964603424
train_iter_loss: 0.24842531979084015
train_iter_loss: 0.3433835208415985
train_iter_loss: 0.22068597376346588
train_iter_loss: 0.4303642511367798
train_iter_loss: 0.12513749301433563
train_iter_loss: 0.2093350738286972
train_iter_loss: 0.3779744505882263
train_iter_loss: 0.24749138951301575
train_iter_loss: 0.26385292410850525
train_iter_loss: 0.24871107935905457
train_iter_loss: 0.17072072625160217
train_iter_loss: 0.29958388209342957
train_iter_loss: 0.29490306973457336
train_iter_loss: 0.3420250117778778
train_iter_loss: 0.3745723366737366
train_iter_loss: 0.31870126724243164
train_iter_loss: 0.24604105949401855
train_iter_loss: 0.29414111375808716
train_iter_loss: 0.33417269587516785
train_iter_loss: 0.3644000291824341
train_iter_loss: 0.2497241199016571
train_iter_loss: 0.3977779448032379
train_iter_loss: 0.32877278327941895
train_iter_loss: 0.21533355116844177
train_iter_loss: 0.18512727320194244
train_iter_loss: 0.22339719533920288
train_iter_loss: 0.24806544184684753
train_iter_loss: 0.19367560744285583
train_iter_loss: 0.35313549637794495
train_iter_loss: 0.21734760701656342
train_iter_loss: 0.26448702812194824
train_iter_loss: 0.2614821195602417
train_iter_loss: 0.16958926618099213
train_iter_loss: 0.2099292129278183
train_iter_loss: 0.3122921586036682
train_iter_loss: 0.38270628452301025
train_iter_loss: 0.25389429926872253
train_iter_loss: 0.14508360624313354
train_iter_loss: 0.131944477558136
train_iter_loss: 0.3662715256214142
train_iter_loss: 0.24989520013332367
train_iter_loss: 0.26073741912841797
train_iter_loss: 0.27926552295684814
train_iter_loss: 0.4115126430988312
train_iter_loss: 0.39462652802467346
train_iter_loss: 0.2436913251876831
train_iter_loss: 0.21718758344650269
train_iter_loss: 0.2320381999015808
train_iter_loss: 0.28587615489959717
train_iter_loss: 0.29481250047683716
train_iter_loss: 0.2539973258972168
train_iter_loss: 0.28647205233573914
train_iter_loss: 0.3133816719055176
train_iter_loss: 0.3627549111843109
train_iter_loss: 0.11001551151275635
train_iter_loss: 0.2904808223247528
train_iter_loss: 0.2552177309989929
train_iter_loss: 0.14923827350139618
train_iter_loss: 0.32067781686782837
train_iter_loss: 0.2835511267185211
train_iter_loss: 0.14799708127975464
train_iter_loss: 0.2662471532821655
train_iter_loss: 0.24455644190311432
train_iter_loss: 0.30553483963012695
train_iter_loss: 0.3917812407016754
train_iter_loss: 0.25974956154823303
train_iter_loss: 0.24641485512256622
train loss :0.2684
---------------------
Validation seg loss: 0.36584837194356434 at epoch 723
epoch =    724/  1000, exp = train
train_iter_loss: 0.17464524507522583
train_iter_loss: 0.25043976306915283
train_iter_loss: 0.23457074165344238
train_iter_loss: 0.3006991744041443
train_iter_loss: 0.3162989914417267
train_iter_loss: 0.41227275133132935
train_iter_loss: 0.1173282191157341
train_iter_loss: 0.3247588276863098
train_iter_loss: 0.4016377329826355
train_iter_loss: 0.3092796504497528
train_iter_loss: 0.243067666888237
train_iter_loss: 0.14599421620368958
train_iter_loss: 0.14322486519813538
train_iter_loss: 0.17046701908111572
train_iter_loss: 0.3454528748989105
train_iter_loss: 0.2550467848777771
train_iter_loss: 0.3321179747581482
train_iter_loss: 0.3097500801086426
train_iter_loss: 0.3634360134601593
train_iter_loss: 0.27411508560180664
train_iter_loss: 0.35377374291419983
train_iter_loss: 0.19148197770118713
train_iter_loss: 0.21103933453559875
train_iter_loss: 0.16549159586429596
train_iter_loss: 0.12695223093032837
train_iter_loss: 0.28290802240371704
train_iter_loss: 0.43772441148757935
train_iter_loss: 0.15251202881336212
train_iter_loss: 0.30419686436653137
train_iter_loss: 0.3305082619190216
train_iter_loss: 0.22005407512187958
train_iter_loss: 0.18894881010055542
train_iter_loss: 0.25798171758651733
train_iter_loss: 0.3674662411212921
train_iter_loss: 0.1420517861843109
train_iter_loss: 0.2454001009464264
train_iter_loss: 0.19131053984165192
train_iter_loss: 0.13837073743343353
train_iter_loss: 0.34823474287986755
train_iter_loss: 0.3388007581233978
train_iter_loss: 0.15302440524101257
train_iter_loss: 0.308269739151001
train_iter_loss: 0.21218080818653107
train_iter_loss: 0.1556580513715744
train_iter_loss: 0.2206631898880005
train_iter_loss: 0.09022465348243713
train_iter_loss: 0.13830681145191193
train_iter_loss: 0.12527218461036682
train_iter_loss: 0.31073129177093506
train_iter_loss: 0.32950448989868164
train_iter_loss: 0.12911777198314667
train_iter_loss: 0.09586264193058014
train_iter_loss: 0.15898406505584717
train_iter_loss: 0.25872281193733215
train_iter_loss: 0.2529768943786621
train_iter_loss: 0.2618590295314789
train_iter_loss: 0.193778395652771
train_iter_loss: 0.3150486350059509
train_iter_loss: 0.45679378509521484
train_iter_loss: 0.25340697169303894
train_iter_loss: 0.3129340708255768
train_iter_loss: 0.2891805171966553
train_iter_loss: 0.33522215485572815
train_iter_loss: 0.28614115715026855
train_iter_loss: 0.34814897179603577
train_iter_loss: 0.21085917949676514
train_iter_loss: 0.373639315366745
train_iter_loss: 0.26193487644195557
train_iter_loss: 0.2814372777938843
train_iter_loss: 0.22944213449954987
train_iter_loss: 0.26795729994773865
train_iter_loss: 0.14656074345111847
train_iter_loss: 0.2673632800579071
train_iter_loss: 0.21183352172374725
train_iter_loss: 0.2621636390686035
train_iter_loss: 0.2607278823852539
train_iter_loss: 0.3262743055820465
train_iter_loss: 0.18969044089317322
train_iter_loss: 0.13870325684547424
train_iter_loss: 0.4220377504825592
train_iter_loss: 0.2958008050918579
train_iter_loss: 0.251048743724823
train_iter_loss: 0.3491968810558319
train_iter_loss: 0.4187197983264923
train_iter_loss: 0.19456005096435547
train_iter_loss: 0.25353771448135376
train_iter_loss: 0.26930564641952515
train_iter_loss: 0.24860204756259918
train_iter_loss: 0.20648297667503357
train_iter_loss: 0.23092204332351685
train_iter_loss: 0.2994590699672699
train_iter_loss: 0.2866271138191223
train_iter_loss: 0.3751310706138611
train_iter_loss: 0.19898803532123566
train_iter_loss: 0.269553005695343
train_iter_loss: 0.34264954924583435
train_iter_loss: 0.29885807633399963
train_iter_loss: 0.321100115776062
train_iter_loss: 0.2714219391345978
train_iter_loss: 0.37726345658302307
train loss :0.2629
---------------------
Validation seg loss: 0.36743160625392535 at epoch 724
epoch =    725/  1000, exp = train
train_iter_loss: 0.23684121668338776
train_iter_loss: 0.2861843705177307
train_iter_loss: 0.3662756681442261
train_iter_loss: 0.2876091003417969
train_iter_loss: 0.31327199935913086
train_iter_loss: 0.16484849154949188
train_iter_loss: 0.2230188548564911
train_iter_loss: 0.32988440990448
train_iter_loss: 0.7121988534927368
train_iter_loss: 0.22579605877399445
train_iter_loss: 0.28569483757019043
train_iter_loss: 0.27820056676864624
train_iter_loss: 0.13639412820339203
train_iter_loss: 0.3131566643714905
train_iter_loss: 0.3868257701396942
train_iter_loss: 0.19747135043144226
train_iter_loss: 0.17177847027778625
train_iter_loss: 0.23254135251045227
train_iter_loss: 0.129512220621109
train_iter_loss: 0.4942784607410431
train_iter_loss: 0.06798017770051956
train_iter_loss: 0.3822990953922272
train_iter_loss: 0.2410200536251068
train_iter_loss: 0.31568217277526855
train_iter_loss: 0.1960359364748001
train_iter_loss: 0.19522428512573242
train_iter_loss: 0.29855814576148987
train_iter_loss: 0.4059194028377533
train_iter_loss: 0.21497458219528198
train_iter_loss: 0.24744360148906708
train_iter_loss: 0.2668658494949341
train_iter_loss: 0.07330044358968735
train_iter_loss: 0.2660144567489624
train_iter_loss: 0.12689560651779175
train_iter_loss: 0.21185912191867828
train_iter_loss: 0.20443134009838104
train_iter_loss: 0.3817777633666992
train_iter_loss: 0.43283528089523315
train_iter_loss: 0.18909476697444916
train_iter_loss: 0.30771341919898987
train_iter_loss: 0.19152431190013885
train_iter_loss: 0.2529566287994385
train_iter_loss: 0.25070250034332275
train_iter_loss: 0.19274720549583435
train_iter_loss: 0.19643498957157135
train_iter_loss: 0.19026920199394226
train_iter_loss: 0.3141091763973236
train_iter_loss: 0.1776791512966156
train_iter_loss: 0.21649065613746643
train_iter_loss: 0.25155457854270935
train_iter_loss: 0.25147518515586853
train_iter_loss: 0.16501761972904205
train_iter_loss: 0.20020297169685364
train_iter_loss: 0.33135315775871277
train_iter_loss: 0.3823966383934021
train_iter_loss: 0.15555207431316376
train_iter_loss: 0.28157690167427063
train_iter_loss: 0.37298518419265747
train_iter_loss: 0.2726748287677765
train_iter_loss: 0.32225894927978516
train_iter_loss: 0.4566197693347931
train_iter_loss: 0.370437890291214
train_iter_loss: 0.140215665102005
train_iter_loss: 0.2086319923400879
train_iter_loss: 0.15777568519115448
train_iter_loss: 0.16347740590572357
train_iter_loss: 0.14996683597564697
train_iter_loss: 0.19677115976810455
train_iter_loss: 0.22671742737293243
train_iter_loss: 0.31979551911354065
train_iter_loss: 0.20316287875175476
train_iter_loss: 0.20964479446411133
train_iter_loss: 0.3553042709827423
train_iter_loss: 0.34921810030937195
train_iter_loss: 0.41657909750938416
train_iter_loss: 0.23291148245334625
train_iter_loss: 0.21164532005786896
train_iter_loss: 0.2275741547346115
train_iter_loss: 0.31914639472961426
train_iter_loss: 0.321670264005661
train_iter_loss: 0.09377267211675644
train_iter_loss: 0.3756193220615387
train_iter_loss: 0.28852665424346924
train_iter_loss: 0.3069327473640442
train_iter_loss: 0.3575358986854553
train_iter_loss: 0.18148629367351532
train_iter_loss: 0.28178292512893677
train_iter_loss: 0.17311938107013702
train_iter_loss: 0.32287514209747314
train_iter_loss: 0.29365450143814087
train_iter_loss: 0.25102299451828003
train_iter_loss: 0.3564746677875519
train_iter_loss: 0.2975519895553589
train_iter_loss: 0.2755144238471985
train_iter_loss: 0.40549445152282715
train_iter_loss: 0.2866930067539215
train_iter_loss: 0.36781397461891174
train_iter_loss: 0.263712078332901
train_iter_loss: 0.12214084714651108
train_iter_loss: 0.42275214195251465
train loss :0.2699
---------------------
Validation seg loss: 0.36578601110234576 at epoch 725
epoch =    726/  1000, exp = train
train_iter_loss: 0.284278005361557
train_iter_loss: 0.3033369481563568
train_iter_loss: 0.20104633271694183
train_iter_loss: 0.21138805150985718
train_iter_loss: 0.1522832065820694
train_iter_loss: 0.2300136685371399
train_iter_loss: 0.30987852811813354
train_iter_loss: 0.3183225691318512
train_iter_loss: 0.2143925279378891
train_iter_loss: 0.4071525037288666
train_iter_loss: 0.2664458453655243
train_iter_loss: 0.1624108850955963
train_iter_loss: 0.24745908379554749
train_iter_loss: 0.2793460488319397
train_iter_loss: 0.23172256350517273
train_iter_loss: 0.22454668581485748
train_iter_loss: 0.16917480528354645
train_iter_loss: 0.5806150436401367
train_iter_loss: 0.22322551906108856
train_iter_loss: 0.39075466990470886
train_iter_loss: 0.2598212659358978
train_iter_loss: 0.3447152078151703
train_iter_loss: 0.19239094853401184
train_iter_loss: 0.207447350025177
train_iter_loss: 0.2544791102409363
train_iter_loss: 0.2780713140964508
train_iter_loss: 0.28017017245292664
train_iter_loss: 0.22024306654930115
train_iter_loss: 0.3825819790363312
train_iter_loss: 0.3157590627670288
train_iter_loss: 0.25832200050354004
train_iter_loss: 0.32977622747421265
train_iter_loss: 0.38883358240127563
train_iter_loss: 0.3425436317920685
train_iter_loss: 0.17913930118083954
train_iter_loss: 0.30976006388664246
train_iter_loss: 0.3163946270942688
train_iter_loss: 0.22266380488872528
train_iter_loss: 0.039003439247608185
train_iter_loss: 0.26091158390045166
train_iter_loss: 0.26917803287506104
train_iter_loss: 0.23935317993164062
train_iter_loss: 0.3289141058921814
train_iter_loss: 0.28515109419822693
train_iter_loss: 0.5286208391189575
train_iter_loss: 0.22109101712703705
train_iter_loss: 0.24343308806419373
train_iter_loss: 0.2565149962902069
train_iter_loss: 0.23131313920021057
train_iter_loss: 0.4073837101459503
train_iter_loss: 0.16272620856761932
train_iter_loss: 0.3681063950061798
train_iter_loss: 0.2456546127796173
train_iter_loss: 0.24870994687080383
train_iter_loss: 0.44511735439300537
train_iter_loss: 0.24149060249328613
train_iter_loss: 0.14828312397003174
train_iter_loss: 0.1840776801109314
train_iter_loss: 0.14096184074878693
train_iter_loss: 0.2860870063304901
train_iter_loss: 0.23287071287631989
train_iter_loss: 0.3031407296657562
train_iter_loss: 0.4296603202819824
train_iter_loss: 0.2085118293762207
train_iter_loss: 0.51670902967453
train_iter_loss: 0.1406181901693344
train_iter_loss: 0.3830694854259491
train_iter_loss: 0.28850221633911133
train_iter_loss: 0.18797960877418518
train_iter_loss: 0.27138689160346985
train_iter_loss: 0.23847298324108124
train_iter_loss: 0.16580377519130707
train_iter_loss: 0.0366448275744915
train_iter_loss: 0.18578846752643585
train_iter_loss: 0.2244122177362442
train_iter_loss: 0.3781263828277588
train_iter_loss: 0.4615652561187744
train_iter_loss: 0.24733345210552216
train_iter_loss: 0.4619855284690857
train_iter_loss: 0.32894641160964966
train_iter_loss: 0.19806963205337524
train_iter_loss: 0.18210703134536743
train_iter_loss: 0.3272247612476349
train_iter_loss: 0.1550862044095993
train_iter_loss: 0.09005297720432281
train_iter_loss: 0.3175259232521057
train_iter_loss: 0.2186390608549118
train_iter_loss: 0.23054492473602295
train_iter_loss: 0.307910293340683
train_iter_loss: 0.30242258310317993
train_iter_loss: 0.22407492995262146
train_iter_loss: 0.2596935033798218
train_iter_loss: 0.1974678635597229
train_iter_loss: 0.15682536363601685
train_iter_loss: 0.32590076327323914
train_iter_loss: 0.25801822543144226
train_iter_loss: 0.27616751194000244
train_iter_loss: 0.09290286153554916
train_iter_loss: 0.16739116609096527
train_iter_loss: 0.2865009009838104
train loss :0.2681
---------------------
Validation seg loss: 0.3632196774223011 at epoch 726
epoch =    727/  1000, exp = train
train_iter_loss: 0.25503063201904297
train_iter_loss: 0.2176561802625656
train_iter_loss: 0.4332830607891083
train_iter_loss: 0.2564258575439453
train_iter_loss: 0.1797478348016739
train_iter_loss: 0.16772204637527466
train_iter_loss: 0.2631795406341553
train_iter_loss: 0.24435043334960938
train_iter_loss: 0.3163565695285797
train_iter_loss: 0.2917470335960388
train_iter_loss: 0.38494038581848145
train_iter_loss: 0.23532544076442719
train_iter_loss: 0.24085095524787903
train_iter_loss: 0.28310394287109375
train_iter_loss: 0.21871760487556458
train_iter_loss: 0.351487934589386
train_iter_loss: 0.2223667949438095
train_iter_loss: 0.33016085624694824
train_iter_loss: 0.26483312249183655
train_iter_loss: 0.3024629056453705
train_iter_loss: 0.2583460509777069
train_iter_loss: 0.1684252917766571
train_iter_loss: 0.2726403474807739
train_iter_loss: 0.16193069517612457
train_iter_loss: 0.420204222202301
train_iter_loss: 0.2300414741039276
train_iter_loss: 0.3090980648994446
train_iter_loss: 0.34409099817276
train_iter_loss: 0.20484399795532227
train_iter_loss: 0.2542276978492737
train_iter_loss: 0.308998703956604
train_iter_loss: 0.26194241642951965
train_iter_loss: 0.17561672627925873
train_iter_loss: 0.41921496391296387
train_iter_loss: 0.2821243107318878
train_iter_loss: 0.4410315752029419
train_iter_loss: 0.13003914058208466
train_iter_loss: 0.3514660596847534
train_iter_loss: 0.15590225160121918
train_iter_loss: 0.23559699952602386
train_iter_loss: 0.3986615836620331
train_iter_loss: 0.16356559097766876
train_iter_loss: 0.09766587615013123
train_iter_loss: 0.2308485358953476
train_iter_loss: 0.22256268560886383
train_iter_loss: 0.30141690373420715
train_iter_loss: 0.2674719989299774
train_iter_loss: 0.3469545245170593
train_iter_loss: 0.2666778564453125
train_iter_loss: 0.2909618318080902
train_iter_loss: 0.20822522044181824
train_iter_loss: 0.17578905820846558
train_iter_loss: 0.34586310386657715
train_iter_loss: 0.27936050295829773
train_iter_loss: 0.19893290102481842
train_iter_loss: 0.3154864013195038
train_iter_loss: 0.1891317218542099
train_iter_loss: 0.32111647725105286
train_iter_loss: 0.28292137384414673
train_iter_loss: 0.24941040575504303
train_iter_loss: 0.3754672706127167
train_iter_loss: 0.07126370072364807
train_iter_loss: 0.2633172869682312
train_iter_loss: 0.3271290361881256
train_iter_loss: 0.3119960129261017
train_iter_loss: 0.21445707976818085
train_iter_loss: 0.22037559747695923
train_iter_loss: 0.30209600925445557
train_iter_loss: 0.41730383038520813
train_iter_loss: 0.3442426323890686
train_iter_loss: 0.15572988986968994
train_iter_loss: 0.32758596539497375
train_iter_loss: 0.3228122591972351
train_iter_loss: 0.22820430994033813
train_iter_loss: 0.2659239172935486
train_iter_loss: 0.23464854061603546
train_iter_loss: 0.1803235411643982
train_iter_loss: 0.33890947699546814
train_iter_loss: 0.317551851272583
train_iter_loss: 0.15162375569343567
train_iter_loss: 0.3285297751426697
train_iter_loss: 0.16859599947929382
train_iter_loss: 0.2579183578491211
train_iter_loss: 0.163030207157135
train_iter_loss: 0.1796002984046936
train_iter_loss: 0.2604203224182129
train_iter_loss: 0.39132362604141235
train_iter_loss: 0.27945879101753235
train_iter_loss: 0.24358528852462769
train_iter_loss: 0.40114709734916687
train_iter_loss: 0.14962829649448395
train_iter_loss: 0.24124328792095184
train_iter_loss: 0.25811874866485596
train_iter_loss: 0.29381126165390015
train_iter_loss: 0.14804035425186157
train_iter_loss: 0.1977795511484146
train_iter_loss: 0.25915464758872986
train_iter_loss: 0.25940677523612976
train_iter_loss: 0.2921845316886902
train_iter_loss: 0.15926900506019592
train loss :0.2657
---------------------
Validation seg loss: 0.38097605866134027 at epoch 727
epoch =    728/  1000, exp = train
train_iter_loss: 0.40248361229896545
train_iter_loss: 0.30700162053108215
train_iter_loss: 0.14774388074874878
train_iter_loss: 0.33362239599227905
train_iter_loss: 0.21134114265441895
train_iter_loss: 0.41102293133735657
train_iter_loss: 0.2979314625263214
train_iter_loss: 0.13733133673667908
train_iter_loss: 0.2495073676109314
train_iter_loss: 0.17783209681510925
train_iter_loss: 0.4620042145252228
train_iter_loss: 0.11252349615097046
train_iter_loss: 0.3078245520591736
train_iter_loss: 0.16890546679496765
train_iter_loss: 0.31802108883857727
train_iter_loss: 0.40932244062423706
train_iter_loss: 0.3901473581790924
train_iter_loss: 0.268938273191452
train_iter_loss: 0.31487345695495605
train_iter_loss: 0.33797943592071533
train_iter_loss: 0.2494298219680786
train_iter_loss: 0.3751195967197418
train_iter_loss: 0.21290993690490723
train_iter_loss: 0.18866054713726044
train_iter_loss: 0.24416184425354004
train_iter_loss: 0.21791456639766693
train_iter_loss: 0.3133988380432129
train_iter_loss: 0.1609545201063156
train_iter_loss: 0.2984640896320343
train_iter_loss: 0.28289973735809326
train_iter_loss: 0.1237494945526123
train_iter_loss: 0.19983269274234772
train_iter_loss: 0.21727325022220612
train_iter_loss: 0.277180552482605
train_iter_loss: 0.26442810893058777
train_iter_loss: 0.2653728425502777
train_iter_loss: 0.0715670958161354
train_iter_loss: 0.15587778389453888
train_iter_loss: 0.2509770095348358
train_iter_loss: 0.25904470682144165
train_iter_loss: 0.3032517433166504
train_iter_loss: 0.24869957566261292
train_iter_loss: 0.3725537657737732
train_iter_loss: 0.28231745958328247
train_iter_loss: 0.22338329255580902
train_iter_loss: 0.2538955807685852
train_iter_loss: 0.3078443109989166
train_iter_loss: 0.3760533928871155
train_iter_loss: 0.2515381872653961
train_iter_loss: 0.23833686113357544
train_iter_loss: 0.23557724058628082
train_iter_loss: 0.3852411210536957
train_iter_loss: 0.14112383127212524
train_iter_loss: 0.42107564210891724
train_iter_loss: 0.2541503310203552
train_iter_loss: 0.16854950785636902
train_iter_loss: 0.27673372626304626
train_iter_loss: 0.34489214420318604
train_iter_loss: 0.3656429350376129
train_iter_loss: 0.12857729196548462
train_iter_loss: 0.2706795334815979
train_iter_loss: 0.3768281638622284
train_iter_loss: 0.3145814836025238
train_iter_loss: 0.07289024442434311
train_iter_loss: 0.18976496160030365
train_iter_loss: 0.17627990245819092
train_iter_loss: 0.40408846735954285
train_iter_loss: 0.37640446424484253
train_iter_loss: 0.2537839710712433
train_iter_loss: 0.22493094205856323
train_iter_loss: 0.2222355753183365
train_iter_loss: 0.2735469341278076
train_iter_loss: 0.23222286999225616
train_iter_loss: 0.11686084419488907
train_iter_loss: 0.07753975689411163
train_iter_loss: 0.199102982878685
train_iter_loss: 0.11953350156545639
train_iter_loss: 0.18291935324668884
train_iter_loss: 0.2576795220375061
train_iter_loss: 0.35095375776290894
train_iter_loss: 0.2841602563858032
train_iter_loss: 0.1944679319858551
train_iter_loss: 0.07487904280424118
train_iter_loss: 0.13684621453285217
train_iter_loss: 0.3198222517967224
train_iter_loss: 0.32626107335090637
train_iter_loss: 0.18911288678646088
train_iter_loss: 0.336789071559906
train_iter_loss: 0.25971487164497375
train_iter_loss: 0.23367521166801453
train_iter_loss: 0.17717301845550537
train_iter_loss: 0.15939106047153473
train_iter_loss: 0.36249327659606934
train_iter_loss: 0.4878152012825012
train_iter_loss: 0.19011613726615906
train_iter_loss: 0.2646281123161316
train_iter_loss: 0.31103944778442383
train_iter_loss: 0.030013488605618477
train_iter_loss: 0.22175626456737518
train_iter_loss: 0.26570773124694824
train loss :0.2576
---------------------
Validation seg loss: 0.3513572415565404 at epoch 728
epoch =    729/  1000, exp = train
train_iter_loss: 0.3927287459373474
train_iter_loss: 0.31987741589546204
train_iter_loss: 0.23947767913341522
train_iter_loss: 0.32526618242263794
train_iter_loss: 0.16736380755901337
train_iter_loss: 0.21793444454669952
train_iter_loss: 0.3885692358016968
train_iter_loss: 0.1620444804430008
train_iter_loss: 0.2813694477081299
train_iter_loss: 0.2198241502046585
train_iter_loss: 0.05817975476384163
train_iter_loss: 0.24556978046894073
train_iter_loss: 0.2908586263656616
train_iter_loss: 0.18080845475196838
train_iter_loss: 0.12087453901767731
train_iter_loss: 0.23097945749759674
train_iter_loss: 0.1624196171760559
train_iter_loss: 0.23282098770141602
train_iter_loss: 0.4192904233932495
train_iter_loss: 0.1742532104253769
train_iter_loss: 0.2420967072248459
train_iter_loss: 0.2590593099594116
train_iter_loss: 0.3283720910549164
train_iter_loss: 0.3543905019760132
train_iter_loss: 0.23719511926174164
train_iter_loss: 0.335329532623291
train_iter_loss: 0.2302185297012329
train_iter_loss: 0.11729650944471359
train_iter_loss: 0.2977580726146698
train_iter_loss: 0.35843420028686523
train_iter_loss: 0.14617624878883362
train_iter_loss: 0.24463902413845062
train_iter_loss: 0.3715789020061493
train_iter_loss: 0.32681822776794434
train_iter_loss: 0.3438679277896881
train_iter_loss: 0.48761045932769775
train_iter_loss: 0.43439608812332153
train_iter_loss: 0.38429614901542664
train_iter_loss: 0.2932942807674408
train_iter_loss: 0.38174277544021606
train_iter_loss: 0.3225228190422058
train_iter_loss: 0.22723719477653503
train_iter_loss: 0.2671254575252533
train_iter_loss: 0.2844814658164978
train_iter_loss: 0.29881468415260315
train_iter_loss: 0.23711232841014862
train_iter_loss: 0.3891218304634094
train_iter_loss: 0.2464131861925125
train_iter_loss: 0.5703129768371582
train_iter_loss: 0.2629222571849823
train_iter_loss: 0.31891030073165894
train_iter_loss: 0.2625611424446106
train_iter_loss: 0.2503233253955841
train_iter_loss: 0.1674894392490387
train_iter_loss: 0.37876877188682556
train_iter_loss: 0.2081388682126999
train_iter_loss: 0.3771461248397827
train_iter_loss: 0.2674160599708557
train_iter_loss: 0.3737352788448334
train_iter_loss: 0.20880034565925598
train_iter_loss: 0.31018534302711487
train_iter_loss: 0.2049262821674347
train_iter_loss: 0.2830372154712677
train_iter_loss: 0.24090690910816193
train_iter_loss: 0.26026540994644165
train_iter_loss: 0.2001909762620926
train_iter_loss: 0.4398277997970581
train_iter_loss: 0.24714019894599915
train_iter_loss: 0.33367159962654114
train_iter_loss: 0.2539355754852295
train_iter_loss: 0.29127606749534607
train_iter_loss: 0.2936142086982727
train_iter_loss: 0.24834662675857544
train_iter_loss: 0.2484579086303711
train_iter_loss: 0.20348474383354187
train_iter_loss: 0.2694138288497925
train_iter_loss: 0.204458549618721
train_iter_loss: 0.252227783203125
train_iter_loss: 0.2618979215621948
train_iter_loss: 0.18194693326950073
train_iter_loss: 0.28665584325790405
train_iter_loss: 0.2450953722000122
train_iter_loss: 0.1507202833890915
train_iter_loss: 0.33947208523750305
train_iter_loss: 0.2398805469274521
train_iter_loss: 0.24238434433937073
train_iter_loss: 0.21587957441806793
train_iter_loss: 0.36619192361831665
train_iter_loss: 0.25667744874954224
train_iter_loss: 0.35891014337539673
train_iter_loss: 0.3235216736793518
train_iter_loss: 0.2597501277923584
train_iter_loss: 0.17057622969150543
train_iter_loss: 0.2874222993850708
train_iter_loss: 0.3351253569126129
train_iter_loss: 0.2091653198003769
train_iter_loss: 0.13897360861301422
train_iter_loss: 0.16506341099739075
train_iter_loss: 0.256539523601532
train_iter_loss: 0.16270455718040466
train loss :0.2742
---------------------
Validation seg loss: 0.35164480361613043 at epoch 729
epoch =    730/  1000, exp = train
train_iter_loss: 0.22130334377288818
train_iter_loss: 0.31981831789016724
train_iter_loss: 0.3115498125553131
train_iter_loss: 0.2641122043132782
train_iter_loss: 0.33112242817878723
train_iter_loss: 0.4198134243488312
train_iter_loss: 0.31069064140319824
train_iter_loss: 0.14845994114875793
train_iter_loss: 0.32806289196014404
train_iter_loss: 0.5148925185203552
train_iter_loss: 0.2188144475221634
train_iter_loss: 0.17671924829483032
train_iter_loss: 0.20959709584712982
train_iter_loss: 0.3281247913837433
train_iter_loss: 0.11675257980823517
train_iter_loss: 0.28440237045288086
train_iter_loss: 0.17092503607273102
train_iter_loss: 0.24850282073020935
train_iter_loss: 0.27670419216156006
train_iter_loss: 0.2722850739955902
train_iter_loss: 0.17420537769794464
train_iter_loss: 0.2559095025062561
train_iter_loss: 0.1577540934085846
train_iter_loss: 0.26384416222572327
train_iter_loss: 0.21862128376960754
train_iter_loss: 0.24421052634716034
train_iter_loss: 0.13419939577579498
train_iter_loss: 0.30265384912490845
train_iter_loss: 0.37317901849746704
train_iter_loss: 0.2612006962299347
train_iter_loss: 0.25633910298347473
train_iter_loss: 0.2694447338581085
train_iter_loss: 0.34720468521118164
train_iter_loss: 0.15687933564186096
train_iter_loss: 0.09256114810705185
train_iter_loss: 0.12529678642749786
train_iter_loss: 0.2315366566181183
train_iter_loss: 0.34202682971954346
train_iter_loss: 0.2577737271785736
train_iter_loss: 0.2148779332637787
train_iter_loss: 0.2044036090373993
train_iter_loss: 0.166083425283432
train_iter_loss: 0.1842569261789322
train_iter_loss: 0.18344710767269135
train_iter_loss: 0.33453911542892456
train_iter_loss: 0.236506387591362
train_iter_loss: 0.3167438209056854
train_iter_loss: 0.2118532657623291
train_iter_loss: 0.3469334542751312
train_iter_loss: 0.2825693190097809
train_iter_loss: 0.21888768672943115
train_iter_loss: 0.18252454698085785
train_iter_loss: 0.21485626697540283
train_iter_loss: 0.3144376575946808
train_iter_loss: 0.2307112216949463
train_iter_loss: 0.22044236958026886
train_iter_loss: 0.247428297996521
train_iter_loss: 0.22613058984279633
train_iter_loss: 0.3082062005996704
train_iter_loss: 0.3404596447944641
train_iter_loss: 0.23084741830825806
train_iter_loss: 0.2926415801048279
train_iter_loss: 0.25586360692977905
train_iter_loss: 0.2338881641626358
train_iter_loss: 0.2737788259983063
train_iter_loss: 0.149506613612175
train_iter_loss: 0.2124987095594406
train_iter_loss: 0.3447345495223999
train_iter_loss: 0.42112424969673157
train_iter_loss: 0.26121804118156433
train_iter_loss: 0.23845984041690826
train_iter_loss: 0.29167237877845764
train_iter_loss: 0.3754664361476898
train_iter_loss: 0.2477865368127823
train_iter_loss: 0.20228493213653564
train_iter_loss: 0.4248599112033844
train_iter_loss: 0.19428294897079468
train_iter_loss: 0.2377934753894806
train_iter_loss: 0.22557201981544495
train_iter_loss: 0.1072888970375061
train_iter_loss: 0.3702031672000885
train_iter_loss: 0.34815049171447754
train_iter_loss: 0.314159095287323
train_iter_loss: 0.20659929513931274
train_iter_loss: 0.3670002222061157
train_iter_loss: 0.2591640055179596
train_iter_loss: 0.3083554804325104
train_iter_loss: 0.2436540722846985
train_iter_loss: 0.28606945276260376
train_iter_loss: 0.23888233304023743
train_iter_loss: 0.3354228138923645
train_iter_loss: 0.436055451631546
train_iter_loss: 0.26186808943748474
train_iter_loss: 0.1873064637184143
train_iter_loss: 0.29199734330177307
train_iter_loss: 0.3715813159942627
train_iter_loss: 0.132558673620224
train_iter_loss: 0.22067546844482422
train_iter_loss: 0.15928277373313904
train_iter_loss: 0.3307797312736511
train loss :0.2629
---------------------
Validation seg loss: 0.3542717947132604 at epoch 730
epoch =    731/  1000, exp = train
train_iter_loss: 0.2068677395582199
train_iter_loss: 0.32689782977104187
train_iter_loss: 0.32990145683288574
train_iter_loss: 0.2560182809829712
train_iter_loss: 0.3131461441516876
train_iter_loss: 0.35205891728401184
train_iter_loss: 0.20569074153900146
train_iter_loss: 0.20427259802818298
train_iter_loss: 0.31171566247940063
train_iter_loss: 0.21863409876823425
train_iter_loss: 0.2192913442850113
train_iter_loss: 0.15358750522136688
train_iter_loss: 0.2509966492652893
train_iter_loss: 0.21477366983890533
train_iter_loss: 0.4830687344074249
train_iter_loss: 0.2390730082988739
train_iter_loss: 0.2674483060836792
train_iter_loss: 0.2654464542865753
train_iter_loss: 0.2180299609899521
train_iter_loss: 0.41831305623054504
train_iter_loss: 0.33932211995124817
train_iter_loss: 0.09376068413257599
train_iter_loss: 0.3630635440349579
train_iter_loss: 0.29573649168014526
train_iter_loss: 0.31861820816993713
train_iter_loss: 0.301725298166275
train_iter_loss: 0.1783960461616516
train_iter_loss: 0.2069772332906723
train_iter_loss: 0.24052207171916962
train_iter_loss: 0.4468843936920166
train_iter_loss: 0.2554783523082733
train_iter_loss: 0.1974388062953949
train_iter_loss: 0.34266117215156555
train_iter_loss: 0.33770111203193665
train_iter_loss: 0.21559152007102966
train_iter_loss: 0.28764182329177856
train_iter_loss: 0.1931726187467575
train_iter_loss: 0.3134134113788605
train_iter_loss: 0.334553062915802
train_iter_loss: 0.33922603726387024
train_iter_loss: 0.20630092918872833
train_iter_loss: 0.27397480607032776
train_iter_loss: 0.18208996951580048
train_iter_loss: 0.17116741836071014
train_iter_loss: 0.12524257600307465
train_iter_loss: 0.2106994241476059
train_iter_loss: 0.17015090584754944
train_iter_loss: 0.2062300145626068
train_iter_loss: 0.2569904029369354
train_iter_loss: 0.17989405989646912
train_iter_loss: 0.3535579741001129
train_iter_loss: 0.24035103619098663
train_iter_loss: 0.317597895860672
train_iter_loss: 0.3361508548259735
train_iter_loss: 0.32588353753089905
train_iter_loss: 0.22876068949699402
train_iter_loss: 0.27685776352882385
train_iter_loss: 0.3446067273616791
train_iter_loss: 0.2928743362426758
train_iter_loss: 0.08007900416851044
train_iter_loss: 0.19795364141464233
train_iter_loss: 0.1611866056919098
train_iter_loss: 0.12572985887527466
train_iter_loss: 0.22753290832042694
train_iter_loss: 0.24306374788284302
train_iter_loss: 0.31605175137519836
train_iter_loss: 0.35579562187194824
train_iter_loss: 0.12259980291128159
train_iter_loss: 0.3429192304611206
train_iter_loss: 0.45256292819976807
train_iter_loss: 0.2190115600824356
train_iter_loss: 0.14031672477722168
train_iter_loss: 0.2664506137371063
train_iter_loss: 0.23500899970531464
train_iter_loss: 0.23037657141685486
train_iter_loss: 0.29227176308631897
train_iter_loss: 0.2713029086589813
train_iter_loss: 0.34264594316482544
train_iter_loss: 0.0852925106883049
train_iter_loss: 0.11519704759120941
train_iter_loss: 0.38108330965042114
train_iter_loss: 0.06604917347431183
train_iter_loss: 0.14886066317558289
train_iter_loss: 0.13736246526241302
train_iter_loss: 0.2544143795967102
train_iter_loss: 0.25716331601142883
train_iter_loss: 0.19650103151798248
train_iter_loss: 0.3143479824066162
train_iter_loss: 0.3965773284435272
train_iter_loss: 0.3234621286392212
train_iter_loss: 0.3581739068031311
train_iter_loss: 0.20396742224693298
train_iter_loss: 0.3027836084365845
train_iter_loss: 0.2290276139974594
train_iter_loss: 0.3847370743751526
train_iter_loss: 0.2422473281621933
train_iter_loss: 0.2648164629936218
train_iter_loss: 0.20060977339744568
train_iter_loss: 0.22271566092967987
train_iter_loss: 0.15105317533016205
train loss :0.2588
---------------------
Validation seg loss: 0.36198992268973085 at epoch 731
epoch =    732/  1000, exp = train
train_iter_loss: 0.23472720384597778
train_iter_loss: 0.3376493752002716
train_iter_loss: 0.20600178837776184
train_iter_loss: 0.33930447697639465
train_iter_loss: 0.25102874636650085
train_iter_loss: 0.24396346509456635
train_iter_loss: 0.25739604234695435
train_iter_loss: 0.30833134055137634
train_iter_loss: 0.3684311509132385
train_iter_loss: 0.18471787869930267
train_iter_loss: 0.3986903727054596
train_iter_loss: 0.27552342414855957
train_iter_loss: 0.31280776858329773
train_iter_loss: 0.30281737446784973
train_iter_loss: 0.23551371693611145
train_iter_loss: 0.25907275080680847
train_iter_loss: 0.2753244936466217
train_iter_loss: 0.31932777166366577
train_iter_loss: 0.22811125218868256
train_iter_loss: 0.04880037531256676
train_iter_loss: 0.38603755831718445
train_iter_loss: 0.26918068528175354
train_iter_loss: 0.21978320181369781
train_iter_loss: 0.23838719725608826
train_iter_loss: 0.26592740416526794
train_iter_loss: 0.3080342411994934
train_iter_loss: 0.38024821877479553
train_iter_loss: 0.2086116522550583
train_iter_loss: 0.16408498585224152
train_iter_loss: 0.277247816324234
train_iter_loss: 0.3380967974662781
train_iter_loss: 0.3516255021095276
train_iter_loss: 0.35906457901000977
train_iter_loss: 0.22869853675365448
train_iter_loss: 0.09773364663124084
train_iter_loss: 0.3674490451812744
train_iter_loss: 0.28523772954940796
train_iter_loss: 0.26575711369514465
train_iter_loss: 0.22101756930351257
train_iter_loss: 0.1327672153711319
train_iter_loss: 0.2114114761352539
train_iter_loss: 0.13680890202522278
train_iter_loss: 0.3254876136779785
train_iter_loss: 0.5045027136802673
train_iter_loss: 0.36414042115211487
train_iter_loss: 0.19595985114574432
train_iter_loss: 0.306155264377594
train_iter_loss: 0.24904637038707733
train_iter_loss: 0.16826516389846802
train_iter_loss: 0.4365578591823578
train_iter_loss: 0.31998682022094727
train_iter_loss: 0.2183133214712143
train_iter_loss: 0.11808865517377853
train_iter_loss: 0.4693978726863861
train_iter_loss: 0.20806637406349182
train_iter_loss: 0.3443123400211334
train_iter_loss: 0.19353418052196503
train_iter_loss: 0.19567106664180756
train_iter_loss: 0.3492424488067627
train_iter_loss: 0.19703561067581177
train_iter_loss: 0.1227540597319603
train_iter_loss: 0.3146827816963196
train_iter_loss: 0.08481358736753464
train_iter_loss: 0.21733663976192474
train_iter_loss: 0.3655935227870941
train_iter_loss: 0.18300263583660126
train_iter_loss: 0.3406617343425751
train_iter_loss: 0.3631753623485565
train_iter_loss: 0.4108872711658478
train_iter_loss: 0.28354668617248535
train_iter_loss: 0.1518581062555313
train_iter_loss: 0.2523053288459778
train_iter_loss: 0.31173714995384216
train_iter_loss: 0.2742176949977875
train_iter_loss: 0.3733733594417572
train_iter_loss: 0.17206132411956787
train_iter_loss: 0.22932545840740204
train_iter_loss: 0.28011423349380493
train_iter_loss: 0.32390788197517395
train_iter_loss: 0.2556142508983612
train_iter_loss: 0.21368496119976044
train_iter_loss: 0.14889593422412872
train_iter_loss: 0.2597532868385315
train_iter_loss: 0.1984793096780777
train_iter_loss: 0.26118773221969604
train_iter_loss: 0.42939063906669617
train_iter_loss: 0.2425837218761444
train_iter_loss: 0.2758876383304596
train_iter_loss: 0.38293978571891785
train_iter_loss: 0.23800095915794373
train_iter_loss: 0.2915428578853607
train_iter_loss: 0.16466066241264343
train_iter_loss: 0.3027423620223999
train_iter_loss: 0.20000013709068298
train_iter_loss: 0.24209162592887878
train_iter_loss: 0.23611894249916077
train_iter_loss: 0.26866042613983154
train_iter_loss: 0.24385003745555878
train_iter_loss: 0.15574252605438232
train_iter_loss: 0.3546896278858185
train loss :0.2695
---------------------
Validation seg loss: 0.35044192253910705 at epoch 732
epoch =    733/  1000, exp = train
train_iter_loss: 0.3562850058078766
train_iter_loss: 0.28212422132492065
train_iter_loss: 0.28869885206222534
train_iter_loss: 0.2678329050540924
train_iter_loss: 0.23547066748142242
train_iter_loss: 0.1403186321258545
train_iter_loss: 0.3237718641757965
train_iter_loss: 0.2775516211986542
train_iter_loss: 0.38881757855415344
train_iter_loss: 0.22653646767139435
train_iter_loss: 0.1852060854434967
train_iter_loss: 0.1616237759590149
train_iter_loss: 0.2028919905424118
train_iter_loss: 0.3658462166786194
train_iter_loss: 0.37110278010368347
train_iter_loss: 0.13756871223449707
train_iter_loss: 0.14382801949977875
train_iter_loss: 0.28114914894104004
train_iter_loss: 0.25046831369400024
train_iter_loss: 0.18414326012134552
train_iter_loss: 0.4428650140762329
train_iter_loss: 0.2319120317697525
train_iter_loss: 0.2510429918766022
train_iter_loss: 0.33039775490760803
train_iter_loss: 0.2878205180168152
train_iter_loss: 0.22005920112133026
train_iter_loss: 0.13826845586299896
train_iter_loss: 0.18452687561511993
train_iter_loss: 0.3047923445701599
train_iter_loss: 0.31645965576171875
train_iter_loss: 0.24778631329536438
train_iter_loss: 0.18512362241744995
train_iter_loss: 0.2505146861076355
train_iter_loss: 0.35735267400741577
train_iter_loss: 0.18149951100349426
train_iter_loss: 0.37146228551864624
train_iter_loss: 0.3453291058540344
train_iter_loss: 0.3324570059776306
train_iter_loss: 0.2101615071296692
train_iter_loss: 0.17192687094211578
train_iter_loss: 0.2711356282234192
train_iter_loss: 0.06484157592058182
train_iter_loss: 0.39815032482147217
train_iter_loss: 0.36457303166389465
train_iter_loss: 0.41361045837402344
train_iter_loss: 0.27260473370552063
train_iter_loss: 0.36186841130256653
train_iter_loss: 0.3784298598766327
train_iter_loss: 0.3261082172393799
train_iter_loss: 0.2647831439971924
train_iter_loss: 0.3143756091594696
train_iter_loss: 0.25085344910621643
train_iter_loss: 0.12698952853679657
train_iter_loss: 0.20438814163208008
train_iter_loss: 0.22942893207073212
train_iter_loss: 0.2602732479572296
train_iter_loss: 0.31312787532806396
train_iter_loss: 0.26936858892440796
train_iter_loss: 0.2322406768798828
train_iter_loss: 0.1266145408153534
train_iter_loss: 0.3752233386039734
train_iter_loss: 0.22524163126945496
train_iter_loss: 0.1961905062198639
train_iter_loss: 0.23306024074554443
train_iter_loss: 0.4612377882003784
train_iter_loss: 0.2919212579727173
train_iter_loss: 0.22317448258399963
train_iter_loss: 0.22029341757297516
train_iter_loss: 0.35816362500190735
train_iter_loss: 0.3179253935813904
train_iter_loss: 0.2585134506225586
train_iter_loss: 0.29946428537368774
train_iter_loss: 0.32189735770225525
train_iter_loss: 0.14201834797859192
train_iter_loss: 0.28374674916267395
train_iter_loss: 0.20623965561389923
train_iter_loss: 0.08791213482618332
train_iter_loss: 0.2443166971206665
train_iter_loss: 0.3208582103252411
train_iter_loss: 0.3167903423309326
train_iter_loss: 0.34825873374938965
train_iter_loss: 0.31483975052833557
train_iter_loss: 0.15381474792957306
train_iter_loss: 0.32597336173057556
train_iter_loss: 0.32018154859542847
train_iter_loss: 0.14571818709373474
train_iter_loss: 0.3700025677680969
train_iter_loss: 0.19488652050495148
train_iter_loss: 0.25939416885375977
train_iter_loss: 0.17567956447601318
train_iter_loss: 0.237829327583313
train_iter_loss: 0.16387112438678741
train_iter_loss: 0.298401802778244
train_iter_loss: 0.24535512924194336
train_iter_loss: 0.19699373841285706
train_iter_loss: 0.2971855401992798
train_iter_loss: 0.30275580286979675
train_iter_loss: 0.26329708099365234
train_iter_loss: 0.22014300525188446
train_iter_loss: 0.3435288667678833
train loss :0.2671
---------------------
Validation seg loss: 0.35806380296653173 at epoch 733
epoch =    734/  1000, exp = train
train_iter_loss: 0.2606651484966278
train_iter_loss: 0.3849113881587982
train_iter_loss: 0.3371303975582123
train_iter_loss: 0.2631577253341675
train_iter_loss: 0.2739996910095215
train_iter_loss: 0.3229764699935913
train_iter_loss: 0.24294817447662354
train_iter_loss: 0.300199419260025
train_iter_loss: 0.2922687530517578
train_iter_loss: 0.3705720603466034
train_iter_loss: 0.18008340895175934
train_iter_loss: 0.26071569323539734
train_iter_loss: 0.1896013468503952
train_iter_loss: 0.38033658266067505
train_iter_loss: 0.342603862285614
train_iter_loss: 0.1392115205526352
train_iter_loss: 0.33107101917266846
train_iter_loss: 0.14770616590976715
train_iter_loss: 0.4304184019565582
train_iter_loss: 0.18828479945659637
train_iter_loss: 0.10925941914319992
train_iter_loss: 0.28499066829681396
train_iter_loss: 0.2113463431596756
train_iter_loss: 0.2007959932088852
train_iter_loss: 0.28859633207321167
train_iter_loss: 0.10283638536930084
train_iter_loss: 0.3954668343067169
train_iter_loss: 0.2810129225254059
train_iter_loss: 0.25382116436958313
train_iter_loss: 0.14290697872638702
train_iter_loss: 0.2774675786495209
train_iter_loss: 0.303844153881073
train_iter_loss: 0.4050849974155426
train_iter_loss: 0.2494145780801773
train_iter_loss: 0.22238066792488098
train_iter_loss: 0.4447949230670929
train_iter_loss: 0.3839103579521179
train_iter_loss: 0.3370625674724579
train_iter_loss: 0.1294696033000946
train_iter_loss: 0.17640048265457153
train_iter_loss: 0.33591943979263306
train_iter_loss: 0.2480897456407547
train_iter_loss: 0.4171331524848938
train_iter_loss: 0.36276400089263916
train_iter_loss: 0.11867047101259232
train_iter_loss: 0.4996539354324341
train_iter_loss: 0.26617875695228577
train_iter_loss: 0.2550417482852936
train_iter_loss: 0.3034636080265045
train_iter_loss: 0.2988552451133728
train_iter_loss: 0.16641847789287567
train_iter_loss: 0.33070018887519836
train_iter_loss: 0.23584698140621185
train_iter_loss: 0.3232016861438751
train_iter_loss: 0.3727230429649353
train_iter_loss: 0.18328198790550232
train_iter_loss: 0.30314844846725464
train_iter_loss: 0.3653728663921356
train_iter_loss: 0.2592586874961853
train_iter_loss: 0.1626465618610382
train_iter_loss: 0.017566608265042305
train_iter_loss: 0.24702952802181244
train_iter_loss: 0.19820551574230194
train_iter_loss: 0.11359522491693497
train_iter_loss: 0.31184977293014526
train_iter_loss: 0.3545718491077423
train_iter_loss: 0.15775883197784424
train_iter_loss: 0.3702669143676758
train_iter_loss: 0.1643405258655548
train_iter_loss: 0.28988105058670044
train_iter_loss: 0.257532000541687
train_iter_loss: 0.19254028797149658
train_iter_loss: 0.2193107306957245
train_iter_loss: 0.14840134978294373
train_iter_loss: 0.27819451689720154
train_iter_loss: 0.15930768847465515
train_iter_loss: 0.14862270653247833
train_iter_loss: 0.24184919893741608
train_iter_loss: 0.25154176354408264
train_iter_loss: 0.2591000497341156
train_iter_loss: 0.16935746371746063
train_iter_loss: 0.32281625270843506
train_iter_loss: 0.3892531991004944
train_iter_loss: 0.33328279852867126
train_iter_loss: 0.26256588101387024
train_iter_loss: 0.20817367732524872
train_iter_loss: 0.2573827803134918
train_iter_loss: 0.29374393820762634
train_iter_loss: 0.3692868947982788
train_iter_loss: 0.2640555202960968
train_iter_loss: 0.2892412841320038
train_iter_loss: 0.17121371626853943
train_iter_loss: 0.28633543848991394
train_iter_loss: 0.04723870009183884
train_iter_loss: 0.28709545731544495
train_iter_loss: 0.23725298047065735
train_iter_loss: 0.1633034348487854
train_iter_loss: 0.2458800971508026
train_iter_loss: 0.27450406551361084
train_iter_loss: 0.1403694897890091
train loss :0.2631
---------------------
Validation seg loss: 0.36690034171267644 at epoch 734
epoch =    735/  1000, exp = train
train_iter_loss: 0.259386271238327
train_iter_loss: 0.18905410170555115
train_iter_loss: 0.4162217974662781
train_iter_loss: 0.22765564918518066
train_iter_loss: 0.19425010681152344
train_iter_loss: 0.3422100841999054
train_iter_loss: 0.1974286437034607
train_iter_loss: 0.3163018822669983
train_iter_loss: 0.34083762764930725
train_iter_loss: 0.3070661723613739
train_iter_loss: 0.2142055481672287
train_iter_loss: 0.2712269723415375
train_iter_loss: 0.19473962485790253
train_iter_loss: 0.3568553328514099
train_iter_loss: 0.19970780611038208
train_iter_loss: 0.33389565348625183
train_iter_loss: 0.25334879755973816
train_iter_loss: 0.24352873861789703
train_iter_loss: 0.297639399766922
train_iter_loss: 0.3288956582546234
train_iter_loss: 0.31698012351989746
train_iter_loss: 0.08391604572534561
train_iter_loss: 0.29435399174690247
train_iter_loss: 0.41281798481941223
train_iter_loss: 0.22697705030441284
train_iter_loss: 0.28680795431137085
train_iter_loss: 0.2880426049232483
train_iter_loss: 0.17626188695430756
train_iter_loss: 0.2132941484451294
train_iter_loss: 0.22083809971809387
train_iter_loss: 0.45826736092567444
train_iter_loss: 0.2591143846511841
train_iter_loss: 0.07885894179344177
train_iter_loss: 0.1689903736114502
train_iter_loss: 0.4382806420326233
train_iter_loss: 0.13246916234493256
train_iter_loss: 0.29502108693122864
train_iter_loss: 0.29463547468185425
train_iter_loss: 0.24453143775463104
train_iter_loss: 0.25906315445899963
train_iter_loss: 0.31882423162460327
train_iter_loss: 0.22516368329524994
train_iter_loss: 0.2197219729423523
train_iter_loss: 0.3261120319366455
train_iter_loss: 0.21867544949054718
train_iter_loss: 0.343910276889801
train_iter_loss: 0.1338529884815216
train_iter_loss: 0.19275078177452087
train_iter_loss: 0.2771701514720917
train_iter_loss: 0.3154517710208893
train_iter_loss: 0.2677423059940338
train_iter_loss: 0.2582733631134033
train_iter_loss: 0.30709052085876465
train_iter_loss: 0.06678277254104614
train_iter_loss: 0.12628677487373352
train_iter_loss: 0.24757206439971924
train_iter_loss: 0.2111857831478119
train_iter_loss: 0.29205402731895447
train_iter_loss: 0.2390294224023819
train_iter_loss: 0.24195583164691925
train_iter_loss: 0.175025075674057
train_iter_loss: 0.30955934524536133
train_iter_loss: 0.18512125313282013
train_iter_loss: 0.15092945098876953
train_iter_loss: 0.14335957169532776
train_iter_loss: 0.26962876319885254
train_iter_loss: 0.2573336362838745
train_iter_loss: 0.3666366934776306
train_iter_loss: 0.11531035602092743
train_iter_loss: 0.28456878662109375
train_iter_loss: 0.20187342166900635
train_iter_loss: 0.35219064354896545
train_iter_loss: 0.2611326277256012
train_iter_loss: 0.21106095612049103
train_iter_loss: 0.34799158573150635
train_iter_loss: 0.21899326145648956
train_iter_loss: 0.15805841982364655
train_iter_loss: 0.18968729674816132
train_iter_loss: 0.37821999192237854
train_iter_loss: 0.32310065627098083
train_iter_loss: 0.4724384546279907
train_iter_loss: 0.36237630248069763
train_iter_loss: 0.36306124925613403
train_iter_loss: 0.28491395711898804
train_iter_loss: 0.3271719813346863
train_iter_loss: 0.27650701999664307
train_iter_loss: 0.24032466113567352
train_iter_loss: 0.13737794756889343
train_iter_loss: 0.2244621068239212
train_iter_loss: 0.2979002296924591
train_iter_loss: 0.10048089176416397
train_iter_loss: 0.05987193062901497
train_iter_loss: 0.2634989321231842
train_iter_loss: 0.21480126678943634
train_iter_loss: 0.26559463143348694
train_iter_loss: 0.3000311851501465
train_iter_loss: 0.2223433405160904
train_iter_loss: 0.21989329159259796
train_iter_loss: 0.23304656147956848
train_iter_loss: 0.23423261940479279
train loss :0.2573
---------------------
Validation seg loss: 0.3640689359854837 at epoch 735
epoch =    736/  1000, exp = train
train_iter_loss: 0.19060908257961273
train_iter_loss: 0.22625158727169037
train_iter_loss: 0.22652412950992584
train_iter_loss: 0.3149329125881195
train_iter_loss: 0.22062452137470245
train_iter_loss: 0.17800939083099365
train_iter_loss: 0.21607959270477295
train_iter_loss: 0.26224085688591003
train_iter_loss: 0.39718663692474365
train_iter_loss: 0.31933262944221497
train_iter_loss: 0.29908525943756104
train_iter_loss: 0.27358025312423706
train_iter_loss: 0.26947924494743347
train_iter_loss: 0.1993160843849182
train_iter_loss: 0.2738759219646454
train_iter_loss: 0.09704326838254929
train_iter_loss: 0.3045210838317871
train_iter_loss: 0.2565551698207855
train_iter_loss: 0.14603199064731598
train_iter_loss: 0.23367761075496674
train_iter_loss: 0.3766487240791321
train_iter_loss: 0.24890634417533875
train_iter_loss: 0.32942330837249756
train_iter_loss: 0.24289023876190186
train_iter_loss: 0.3780342638492584
train_iter_loss: 0.46097686886787415
train_iter_loss: 0.1563202142715454
train_iter_loss: 0.3006766736507416
train_iter_loss: 0.25272417068481445
train_iter_loss: 0.25035810470581055
train_iter_loss: 0.21572574973106384
train_iter_loss: 0.303373247385025
train_iter_loss: 0.18361106514930725
train_iter_loss: 0.3342739939689636
train_iter_loss: 0.2608403265476227
train_iter_loss: 0.21433953940868378
train_iter_loss: 0.29982173442840576
train_iter_loss: 0.2695810794830322
train_iter_loss: 0.20583942532539368
train_iter_loss: 0.2682360112667084
train_iter_loss: 0.34734466671943665
train_iter_loss: 0.19287198781967163
train_iter_loss: 0.32866689562797546
train_iter_loss: 0.20993654429912567
train_iter_loss: 0.3581725060939789
train_iter_loss: 0.35185277462005615
train_iter_loss: 0.10043556988239288
train_iter_loss: 0.225569948554039
train_iter_loss: 0.2835135757923126
train_iter_loss: 0.24085880815982819
train_iter_loss: 0.2214692085981369
train_iter_loss: 0.22867341339588165
train_iter_loss: 0.2552299201488495
train_iter_loss: 0.239162415266037
train_iter_loss: 0.3114011287689209
train_iter_loss: 0.22995032370090485
train_iter_loss: 0.13006846606731415
train_iter_loss: 0.42146846652030945
train_iter_loss: 0.22027763724327087
train_iter_loss: 0.3557735085487366
train_iter_loss: 0.09278017282485962
train_iter_loss: 0.31539762020111084
train_iter_loss: 0.12871529161930084
train_iter_loss: 0.28075137734413147
train_iter_loss: 0.17220120131969452
train_iter_loss: 0.3383066952228546
train_iter_loss: 0.315102756023407
train_iter_loss: 0.21254467964172363
train_iter_loss: 0.11775350570678711
train_iter_loss: 0.3100915849208832
train_iter_loss: 0.20868268609046936
train_iter_loss: 0.3211080729961395
train_iter_loss: 0.13132326304912567
train_iter_loss: 0.30414503812789917
train_iter_loss: 0.08827739208936691
train_iter_loss: 0.23374806344509125
train_iter_loss: 0.2838062047958374
train_iter_loss: 0.2202450931072235
train_iter_loss: 0.20957350730895996
train_iter_loss: 0.22074583172798157
train_iter_loss: 0.2136024832725525
train_iter_loss: 0.27349644899368286
train_iter_loss: 0.20121851563453674
train_iter_loss: 0.26037532091140747
train_iter_loss: 0.29835045337677
train_iter_loss: 0.20647159218788147
train_iter_loss: 0.23776647448539734
train_iter_loss: 0.23554812371730804
train_iter_loss: 0.06563319265842438
train_iter_loss: 0.3646961748600006
train_iter_loss: 0.34615880250930786
train_iter_loss: 0.1458486020565033
train_iter_loss: 0.5012445449829102
train_iter_loss: 0.23182368278503418
train_iter_loss: 0.43148404359817505
train_iter_loss: 0.4281740188598633
train_iter_loss: 0.4505780339241028
train_iter_loss: 0.45999857783317566
train_iter_loss: 0.36574986577033997
train_iter_loss: 0.21052876114845276
train loss :0.2640
---------------------
Validation seg loss: 0.34980876505691205 at epoch 736
epoch =    737/  1000, exp = train
train_iter_loss: 0.19214209914207458
train_iter_loss: 0.24659056961536407
train_iter_loss: 0.2582215666770935
train_iter_loss: 0.2464541494846344
train_iter_loss: 0.17784462869167328
train_iter_loss: 0.17662619054317474
train_iter_loss: 0.4090995192527771
train_iter_loss: 0.37605997920036316
train_iter_loss: 0.34191304445266724
train_iter_loss: 0.26490092277526855
train_iter_loss: 0.2945256233215332
train_iter_loss: 0.13796201348304749
train_iter_loss: 0.34739449620246887
train_iter_loss: 0.157345250248909
train_iter_loss: 0.09397480636835098
train_iter_loss: 0.021881530061364174
train_iter_loss: 0.17594285309314728
train_iter_loss: 0.437673956155777
train_iter_loss: 0.28140559792518616
train_iter_loss: 0.22500355541706085
train_iter_loss: 0.3016926050186157
train_iter_loss: 0.3429591953754425
train_iter_loss: 0.2999227046966553
train_iter_loss: 0.12348980456590652
train_iter_loss: 0.2914147675037384
train_iter_loss: 0.1411091536283493
train_iter_loss: 0.36638954281806946
train_iter_loss: 0.4357250928878784
train_iter_loss: 0.39416950941085815
train_iter_loss: 0.18021118640899658
train_iter_loss: 0.2940581440925598
train_iter_loss: 0.21661296486854553
train_iter_loss: 0.22724513709545135
train_iter_loss: 0.3254915177822113
train_iter_loss: 0.22768308222293854
train_iter_loss: 0.3216801583766937
train_iter_loss: 0.16455501317977905
train_iter_loss: 0.17835630476474762
train_iter_loss: 0.27079376578330994
train_iter_loss: 0.26290836930274963
train_iter_loss: 0.22659818828105927
train_iter_loss: 0.22387950122356415
train_iter_loss: 0.35748234391212463
train_iter_loss: 0.17932403087615967
train_iter_loss: 0.3320945203304291
train_iter_loss: 0.2998436391353607
train_iter_loss: 0.28063642978668213
train_iter_loss: 0.3821057975292206
train_iter_loss: 0.17733877897262573
train_iter_loss: 0.31077995896339417
train_iter_loss: 0.35022464394569397
train_iter_loss: 0.43077948689460754
train_iter_loss: 0.32878410816192627
train_iter_loss: 0.2041272521018982
train_iter_loss: 0.2611273229122162
train_iter_loss: 0.44782188534736633
train_iter_loss: 0.25460436940193176
train_iter_loss: 0.3011287450790405
train_iter_loss: 0.17170250415802002
train_iter_loss: 0.1328677535057068
train_iter_loss: 0.3890736401081085
train_iter_loss: 0.13171245157718658
train_iter_loss: 0.23055361211299896
train_iter_loss: 0.1840977817773819
train_iter_loss: 0.2405918389558792
train_iter_loss: 0.21220551431179047
train_iter_loss: 0.1464131772518158
train_iter_loss: 0.15035463869571686
train_iter_loss: 0.14851118624210358
train_iter_loss: 0.3046630024909973
train_iter_loss: 0.19471760094165802
train_iter_loss: 0.36592140793800354
train_iter_loss: 0.26742860674858093
train_iter_loss: 0.35882726311683655
train_iter_loss: 0.23377260565757751
train_iter_loss: 0.26579582691192627
train_iter_loss: 0.3344052731990814
train_iter_loss: 0.41676437854766846
train_iter_loss: 0.1754089742898941
train_iter_loss: 0.2894287109375
train_iter_loss: 0.11963807791471481
train_iter_loss: 0.4504154324531555
train_iter_loss: 0.1718902438879013
train_iter_loss: 0.24623659253120422
train_iter_loss: 0.14723077416419983
train_iter_loss: 0.2181808054447174
train_iter_loss: 0.3003027141094208
train_iter_loss: 0.13388574123382568
train_iter_loss: 0.3408808410167694
train_iter_loss: 0.26502472162246704
train_iter_loss: 0.25364938378334045
train_iter_loss: 0.20975768566131592
train_iter_loss: 0.048528559505939484
train_iter_loss: 0.35490182042121887
train_iter_loss: 0.271175354719162
train_iter_loss: 0.42144498229026794
train_iter_loss: 0.3853565454483032
train_iter_loss: 0.4129515290260315
train_iter_loss: 0.22064858675003052
train_iter_loss: 0.14020676910877228
train loss :0.2630
---------------------
Validation seg loss: 0.36823204795847525 at epoch 737
epoch =    738/  1000, exp = train
train_iter_loss: 0.2279440313577652
train_iter_loss: 0.3178291320800781
train_iter_loss: 0.14209280908107758
train_iter_loss: 0.24029681086540222
train_iter_loss: 0.1792636215686798
train_iter_loss: 0.24707470834255219
train_iter_loss: 0.2405305951833725
train_iter_loss: 0.4278276860713959
train_iter_loss: 0.2350478172302246
train_iter_loss: 0.18767911195755005
train_iter_loss: 0.22924460470676422
train_iter_loss: 0.3418845534324646
train_iter_loss: 0.12700724601745605
train_iter_loss: 0.15927812457084656
train_iter_loss: 0.2672657370567322
train_iter_loss: 0.19893570244312286
train_iter_loss: 0.21872684359550476
train_iter_loss: 0.30567461252212524
train_iter_loss: 0.33917564153671265
train_iter_loss: 0.3310187757015228
train_iter_loss: 0.19204583764076233
train_iter_loss: 0.22889399528503418
train_iter_loss: 0.3788909912109375
train_iter_loss: 0.39993998408317566
train_iter_loss: 0.22346711158752441
train_iter_loss: 0.5388320088386536
train_iter_loss: 0.28161895275115967
train_iter_loss: 0.3073383867740631
train_iter_loss: 0.22014795243740082
train_iter_loss: 0.3477167785167694
train_iter_loss: 0.30802974104881287
train_iter_loss: 0.04247315227985382
train_iter_loss: 0.29642847180366516
train_iter_loss: 0.3722110688686371
train_iter_loss: 0.12348221987485886
train_iter_loss: 0.2809644937515259
train_iter_loss: 0.31926068663597107
train_iter_loss: 0.3092503547668457
train_iter_loss: 0.2424117624759674
train_iter_loss: 0.25954514741897583
train_iter_loss: 0.2968796193599701
train_iter_loss: 0.11069687455892563
train_iter_loss: 0.34249773621559143
train_iter_loss: 0.2596965730190277
train_iter_loss: 0.3110072910785675
train_iter_loss: 0.16953563690185547
train_iter_loss: 0.14697980880737305
train_iter_loss: 0.1134057566523552
train_iter_loss: 0.31649070978164673
train_iter_loss: 0.27527862787246704
train_iter_loss: 0.14061188697814941
train_iter_loss: 0.2626454532146454
train_iter_loss: 0.14534854888916016
train_iter_loss: 0.12137726694345474
train_iter_loss: 0.2567409873008728
train_iter_loss: 0.3489331007003784
train_iter_loss: 0.32530876994132996
train_iter_loss: 0.3764518201351166
train_iter_loss: 0.3787848949432373
train_iter_loss: 0.5138174891471863
train_iter_loss: 0.18368816375732422
train_iter_loss: 0.2469223588705063
train_iter_loss: 0.20490968227386475
train_iter_loss: 0.22964079678058624
train_iter_loss: 0.2981514036655426
train_iter_loss: 0.2666904032230377
train_iter_loss: 0.16626323759555817
train_iter_loss: 0.198459193110466
train_iter_loss: 0.34219932556152344
train_iter_loss: 0.23275919258594513
train_iter_loss: 0.22779624164104462
train_iter_loss: 0.4652971625328064
train_iter_loss: 0.2229938954114914
train_iter_loss: 0.28760311007499695
train_iter_loss: 0.421970009803772
train_iter_loss: 0.10331843793392181
train_iter_loss: 0.3037896454334259
train_iter_loss: 0.3528234362602234
train_iter_loss: 0.24687518179416656
train_iter_loss: 0.23692765831947327
train_iter_loss: 0.09153363853693008
train_iter_loss: 0.22118908166885376
train_iter_loss: 0.15315403044223785
train_iter_loss: 0.1916016936302185
train_iter_loss: 0.30176231265068054
train_iter_loss: 0.2557311952114105
train_iter_loss: 0.20288914442062378
train_iter_loss: 0.29706498980522156
train_iter_loss: 0.1898552030324936
train_iter_loss: 0.24201929569244385
train_iter_loss: 0.3866904377937317
train_iter_loss: 0.25924697518348694
train_iter_loss: 0.22160762548446655
train_iter_loss: 0.31469470262527466
train_iter_loss: 0.3057034909725189
train_iter_loss: 0.1909606009721756
train_iter_loss: 0.387311726808548
train_iter_loss: 0.20662575960159302
train_iter_loss: 0.20748208463191986
train_iter_loss: 0.17725755274295807
train loss :0.2616
---------------------
Validation seg loss: 0.35826464314540885 at epoch 738
epoch =    739/  1000, exp = train
train_iter_loss: 0.30465832352638245
train_iter_loss: 0.27267760038375854
train_iter_loss: 0.3558495342731476
train_iter_loss: 0.37598279118537903
train_iter_loss: 0.27973881363868713
train_iter_loss: 0.1220841109752655
train_iter_loss: 0.22870950400829315
train_iter_loss: 0.2858108580112457
train_iter_loss: 0.4275541305541992
train_iter_loss: 0.19900858402252197
train_iter_loss: 0.2674998641014099
train_iter_loss: 0.2640474736690521
train_iter_loss: 0.17214539647102356
train_iter_loss: 0.3184863328933716
train_iter_loss: 0.17942123115062714
train_iter_loss: 0.16061997413635254
train_iter_loss: 0.2967281937599182
train_iter_loss: 0.17919903993606567
train_iter_loss: 0.31406471133232117
train_iter_loss: 0.12126441299915314
train_iter_loss: 0.4047296345233917
train_iter_loss: 0.32434460520744324
train_iter_loss: 0.1372528374195099
train_iter_loss: 0.18456271290779114
train_iter_loss: 0.3056413233280182
train_iter_loss: 0.17919425666332245
train_iter_loss: 0.21793456375598907
train_iter_loss: 0.028698856011033058
train_iter_loss: 0.25733011960983276
train_iter_loss: 0.27999982237815857
train_iter_loss: 0.25816506147384644
train_iter_loss: 0.33319908380508423
train_iter_loss: 0.2483992725610733
train_iter_loss: 0.42931878566741943
train_iter_loss: 0.24786120653152466
train_iter_loss: 0.3425693213939667
train_iter_loss: 0.26625528931617737
train_iter_loss: 0.16913890838623047
train_iter_loss: 0.4627475142478943
train_iter_loss: 0.2278464436531067
train_iter_loss: 0.33869585394859314
train_iter_loss: 0.24783556163311005
train_iter_loss: 0.20077824592590332
train_iter_loss: 0.2683234214782715
train_iter_loss: 0.30381765961647034
train_iter_loss: 0.2522372007369995
train_iter_loss: 0.4332233965396881
train_iter_loss: 0.2653784155845642
train_iter_loss: 0.22862635552883148
train_iter_loss: 0.20625291764736176
train_iter_loss: 0.31064194440841675
train_iter_loss: 0.3059861958026886
train_iter_loss: 0.36099904775619507
train_iter_loss: 0.1733640730381012
train_iter_loss: 0.2407195121049881
train_iter_loss: 0.3932480216026306
train_iter_loss: 0.1706099808216095
train_iter_loss: 0.18927541375160217
train_iter_loss: 0.2362687736749649
train_iter_loss: 0.44267383217811584
train_iter_loss: 0.3602674901485443
train_iter_loss: 0.18837210536003113
train_iter_loss: 0.13174761831760406
train_iter_loss: 0.28861546516418457
train_iter_loss: 0.26696500182151794
train_iter_loss: 0.3800745904445648
train_iter_loss: 0.31526172161102295
train_iter_loss: 0.3799136281013489
train_iter_loss: 0.22620557248592377
train_iter_loss: 0.34303018450737
train_iter_loss: 0.22769051790237427
train_iter_loss: 0.189511239528656
train_iter_loss: 0.4108119606971741
train_iter_loss: 0.3418042063713074
train_iter_loss: 0.336588978767395
train_iter_loss: 0.10067251324653625
train_iter_loss: 0.2515316903591156
train_iter_loss: 0.41290464997291565
train_iter_loss: 0.2873959243297577
train_iter_loss: 0.2184952348470688
train_iter_loss: 0.21017585694789886
train_iter_loss: 0.1585378795862198
train_iter_loss: 0.24244245886802673
train_iter_loss: 0.25959041714668274
train_iter_loss: 0.3660036623477936
train_iter_loss: 0.17298202216625214
train_iter_loss: 0.42188552021980286
train_iter_loss: 0.30439987778663635
train_iter_loss: 0.2571605443954468
train_iter_loss: 0.17316286265850067
train_iter_loss: 0.23046663403511047
train_iter_loss: 0.14925578236579895
train_iter_loss: 0.22401116788387299
train_iter_loss: 0.31705549359321594
train_iter_loss: 0.26300933957099915
train_iter_loss: 0.2969056963920593
train_iter_loss: 0.1649569720029831
train_iter_loss: 0.30476659536361694
train_iter_loss: 0.36473050713539124
train_iter_loss: 0.3591925799846649
train loss :0.2716
---------------------
Validation seg loss: 0.34866192172070576 at epoch 739
epoch =    740/  1000, exp = train
train_iter_loss: 0.31131917238235474
train_iter_loss: 0.21541620790958405
train_iter_loss: 0.40254536271095276
train_iter_loss: 0.2536570131778717
train_iter_loss: 0.19045497477054596
train_iter_loss: 0.2840135991573334
train_iter_loss: 0.19522178173065186
train_iter_loss: 0.29878056049346924
train_iter_loss: 0.3345569372177124
train_iter_loss: 0.28659918904304504
train_iter_loss: 0.43295902013778687
train_iter_loss: 0.21510106325149536
train_iter_loss: 0.23371659219264984
train_iter_loss: 0.24168573319911957
train_iter_loss: 0.2931216061115265
train_iter_loss: 0.5375223755836487
train_iter_loss: 0.3523521423339844
train_iter_loss: 0.370401531457901
train_iter_loss: 0.26490074396133423
train_iter_loss: 0.29661113023757935
train_iter_loss: 0.2725497782230377
train_iter_loss: 0.27424731850624084
train_iter_loss: 0.07730074226856232
train_iter_loss: 0.3257475197315216
train_iter_loss: 0.21883997321128845
train_iter_loss: 0.18836958706378937
train_iter_loss: 0.23613406717777252
train_iter_loss: 0.20286941528320312
train_iter_loss: 0.11773993074893951
train_iter_loss: 0.28224825859069824
train_iter_loss: 0.25718435645103455
train_iter_loss: 0.3442305326461792
train_iter_loss: 0.2744261920452118
train_iter_loss: 0.37572795152664185
train_iter_loss: 0.3800991475582123
train_iter_loss: 0.29435205459594727
train_iter_loss: 0.25087878108024597
train_iter_loss: 0.27822527289390564
train_iter_loss: 0.3179177939891815
train_iter_loss: 0.2364269196987152
train_iter_loss: 0.2691759467124939
train_iter_loss: 0.303121954202652
train_iter_loss: 0.22375281155109406
train_iter_loss: 0.28469038009643555
train_iter_loss: 0.40854209661483765
train_iter_loss: 0.24077162146568298
train_iter_loss: 0.29366037249565125
train_iter_loss: 0.20945236086845398
train_iter_loss: 0.17356005311012268
train_iter_loss: 0.26431819796562195
train_iter_loss: 0.2560754120349884
train_iter_loss: 0.15669777989387512
train_iter_loss: 0.42880478501319885
train_iter_loss: 0.40117546916007996
train_iter_loss: 0.2866741120815277
train_iter_loss: 0.3106832206249237
train_iter_loss: 0.21398669481277466
train_iter_loss: 0.18315136432647705
train_iter_loss: 0.3853612542152405
train_iter_loss: 0.2115010768175125
train_iter_loss: 0.23568300902843475
train_iter_loss: 0.37332189083099365
train_iter_loss: 0.10152070969343185
train_iter_loss: 0.19616375863552094
train_iter_loss: 0.27486640214920044
train_iter_loss: 0.2572648525238037
train_iter_loss: 0.20692938566207886
train_iter_loss: 0.3029097020626068
train_iter_loss: 0.23440413177013397
train_iter_loss: 0.3101073205471039
train_iter_loss: 0.18051543831825256
train_iter_loss: 0.13189226388931274
train_iter_loss: 0.28311535716056824
train_iter_loss: 0.24795538187026978
train_iter_loss: 0.3158489167690277
train_iter_loss: 0.21027882397174835
train_iter_loss: 0.25297221541404724
train_iter_loss: 0.24459651112556458
train_iter_loss: 0.29161345958709717
train_iter_loss: 0.25416648387908936
train_iter_loss: 0.3709971010684967
train_iter_loss: 0.32185959815979004
train_iter_loss: 0.2569621205329895
train_iter_loss: 0.18448351323604584
train_iter_loss: 0.1108209490776062
train_iter_loss: 0.17841903865337372
train_iter_loss: 0.25327542424201965
train_iter_loss: 0.24568453431129456
train_iter_loss: 0.23056992888450623
train_iter_loss: 0.3790593147277832
train_iter_loss: 0.20550313591957092
train_iter_loss: 0.2788480520248413
train_iter_loss: 0.15913887321949005
train_iter_loss: 0.22299662232398987
train_iter_loss: 0.23209668695926666
train_iter_loss: 0.17840246856212616
train_iter_loss: 0.24519787728786469
train_iter_loss: 0.21856804192066193
train_iter_loss: 0.17821839451789856
train_iter_loss: 0.2192877233028412
train loss :0.2657
---------------------
Validation seg loss: 0.3469733218398859 at epoch 740
epoch =    741/  1000, exp = train
train_iter_loss: 0.16362927854061127
train_iter_loss: 0.30310213565826416
train_iter_loss: 0.31956055760383606
train_iter_loss: 0.27376940846443176
train_iter_loss: 0.22362227737903595
train_iter_loss: 0.2811465263366699
train_iter_loss: 0.3666607737541199
train_iter_loss: 0.3209088146686554
train_iter_loss: 0.2885599732398987
train_iter_loss: 0.14028826355934143
train_iter_loss: 0.2490161955356598
train_iter_loss: 0.30112478137016296
train_iter_loss: 0.20564910769462585
train_iter_loss: 0.3197577893733978
train_iter_loss: 0.18755146861076355
train_iter_loss: 0.24861764907836914
train_iter_loss: 0.21470701694488525
train_iter_loss: 0.21203191578388214
train_iter_loss: 0.273183137178421
train_iter_loss: 0.16097098588943481
train_iter_loss: 0.08424922078847885
train_iter_loss: 0.2169797569513321
train_iter_loss: 0.07727198302745819
train_iter_loss: 0.24133837223052979
train_iter_loss: 0.08432787656784058
train_iter_loss: 0.3025522232055664
train_iter_loss: 0.3186192810535431
train_iter_loss: 0.335772842168808
train_iter_loss: 0.3250035345554352
train_iter_loss: 0.4594968259334564
train_iter_loss: 0.3615516126155853
train_iter_loss: 0.20613998174667358
train_iter_loss: 0.2645898759365082
train_iter_loss: 0.286348432302475
train_iter_loss: 0.2815300524234772
train_iter_loss: 0.2476954162120819
train_iter_loss: 0.4780013859272003
train_iter_loss: 0.10575221478939056
train_iter_loss: 0.308878093957901
train_iter_loss: 0.2207627296447754
train_iter_loss: 0.22956079244613647
train_iter_loss: 0.30559882521629333
train_iter_loss: 0.1359962821006775
train_iter_loss: 0.17012529075145721
train_iter_loss: 0.28081104159355164
train_iter_loss: 0.2475576102733612
train_iter_loss: 0.2210509330034256
train_iter_loss: 0.37073957920074463
train_iter_loss: 0.3065650165081024
train_iter_loss: 0.0628957748413086
train_iter_loss: 0.1992858499288559
train_iter_loss: 0.21980199217796326
train_iter_loss: 0.26268789172172546
train_iter_loss: 0.3849845826625824
train_iter_loss: 0.323996901512146
train_iter_loss: 0.27205580472946167
train_iter_loss: 0.34378549456596375
train_iter_loss: 0.24706919491291046
train_iter_loss: 0.15629178285598755
train_iter_loss: 0.18269145488739014
train_iter_loss: 0.13588249683380127
train_iter_loss: 0.33673539757728577
train_iter_loss: 0.37689098715782166
train_iter_loss: 0.3157629072666168
train_iter_loss: 0.2315392941236496
train_iter_loss: 0.38750535249710083
train_iter_loss: 0.2810402810573578
train_iter_loss: 0.5319223403930664
train_iter_loss: 0.18133732676506042
train_iter_loss: 0.3128044903278351
train_iter_loss: 0.23534756898880005
train_iter_loss: 0.18001922965049744
train_iter_loss: 0.12299418449401855
train_iter_loss: 0.10508294403553009
train_iter_loss: 0.28565657138824463
train_iter_loss: 0.3450849652290344
train_iter_loss: 0.3419078588485718
train_iter_loss: 0.21108809113502502
train_iter_loss: 0.2237057238817215
train_iter_loss: 0.2742610573768616
train_iter_loss: 0.15738432109355927
train_iter_loss: 0.24697135388851166
train_iter_loss: 0.35021913051605225
train_iter_loss: 0.2916126847267151
train_iter_loss: 0.22389569878578186
train_iter_loss: 0.24157820641994476
train_iter_loss: 0.31421419978141785
train_iter_loss: 0.20155544579029083
train_iter_loss: 0.28436794877052307
train_iter_loss: 0.22977539896965027
train_iter_loss: 0.2814221680164337
train_iter_loss: 0.2837245762348175
train_iter_loss: 0.363326758146286
train_iter_loss: 0.31842389702796936
train_iter_loss: 0.031248655170202255
train_iter_loss: 0.12292339652776718
train_iter_loss: 0.35117602348327637
train_iter_loss: 0.24651631712913513
train_iter_loss: 0.20269811153411865
train_iter_loss: 0.22095496952533722
train loss :0.2583
---------------------
Validation seg loss: 0.3796167532113855 at epoch 741
epoch =    742/  1000, exp = train
train_iter_loss: 0.24731075763702393
train_iter_loss: 0.49734488129615784
train_iter_loss: 0.30163463950157166
train_iter_loss: 0.3666619062423706
train_iter_loss: 0.17286747694015503
train_iter_loss: 0.20436830818653107
train_iter_loss: 0.19822630286216736
train_iter_loss: 0.1447586864233017
train_iter_loss: 0.2455441653728485
train_iter_loss: 0.4725547134876251
train_iter_loss: 0.27181482315063477
train_iter_loss: 0.11609907448291779
train_iter_loss: 0.28571245074272156
train_iter_loss: 0.23675890266895294
train_iter_loss: 0.337097704410553
train_iter_loss: 0.14698491990566254
train_iter_loss: 0.08495118468999863
train_iter_loss: 0.2660432755947113
train_iter_loss: 0.25324827432632446
train_iter_loss: 0.27825257182121277
train_iter_loss: 0.1496666669845581
train_iter_loss: 0.15788541734218597
train_iter_loss: 0.3238013684749603
train_iter_loss: 0.19290859997272491
train_iter_loss: 0.409890741109848
train_iter_loss: 0.1333971917629242
train_iter_loss: 0.3196212649345398
train_iter_loss: 0.2098657637834549
train_iter_loss: 0.17598631978034973
train_iter_loss: 0.38824254274368286
train_iter_loss: 0.14562252163887024
train_iter_loss: 0.21799516677856445
train_iter_loss: 0.14943474531173706
train_iter_loss: 0.18057936429977417
train_iter_loss: 0.2689220607280731
train_iter_loss: 0.4378112256526947
train_iter_loss: 0.2850545346736908
train_iter_loss: 0.16291460394859314
train_iter_loss: 0.1634177714586258
train_iter_loss: 0.12648531794548035
train_iter_loss: 0.23688410222530365
train_iter_loss: 0.12915313243865967
train_iter_loss: 0.4605359733104706
train_iter_loss: 0.3176969885826111
train_iter_loss: 0.3619491159915924
train_iter_loss: 0.34149083495140076
train_iter_loss: 0.24203959107398987
train_iter_loss: 0.36421823501586914
train_iter_loss: 0.2944759130477905
train_iter_loss: 0.28068655729293823
train_iter_loss: 0.3315601050853729
train_iter_loss: 0.36948060989379883
train_iter_loss: 0.21041789650917053
train_iter_loss: 0.26738241314888
train_iter_loss: 0.32587599754333496
train_iter_loss: 0.24024555087089539
train_iter_loss: 0.19741269946098328
train_iter_loss: 0.1819734126329422
train_iter_loss: 0.203561931848526
train_iter_loss: 0.30957573652267456
train_iter_loss: 0.38222938776016235
train_iter_loss: 0.24187906086444855
train_iter_loss: 0.24745525419712067
train_iter_loss: 0.16880513727664948
train_iter_loss: 0.19704453647136688
train_iter_loss: 0.26873618364334106
train_iter_loss: 0.359824001789093
train_iter_loss: 0.24109137058258057
train_iter_loss: 0.2366456538438797
train_iter_loss: 0.32093706727027893
train_iter_loss: 0.22784343361854553
train_iter_loss: 0.3886986970901489
train_iter_loss: 0.2768041789531708
train_iter_loss: 0.18968623876571655
train_iter_loss: 0.1586534082889557
train_iter_loss: 0.2538779675960541
train_iter_loss: 0.3619290888309479
train_iter_loss: 0.17738975584506989
train_iter_loss: 0.26576098799705505
train_iter_loss: 0.37374648451805115
train_iter_loss: 0.3199773132801056
train_iter_loss: 0.23864510655403137
train_iter_loss: 0.15624038875102997
train_iter_loss: 0.22862659394741058
train_iter_loss: 0.39125490188598633
train_iter_loss: 0.2080908864736557
train_iter_loss: 0.22499363124370575
train_iter_loss: 0.25607848167419434
train_iter_loss: 0.23676122725009918
train_iter_loss: 0.22677713632583618
train_iter_loss: 0.4917047321796417
train_iter_loss: 0.22652076184749603
train_iter_loss: 0.23081174492835999
train_iter_loss: 0.23377864062786102
train_iter_loss: 0.2814091742038727
train_iter_loss: 0.40169408917427063
train_iter_loss: 0.2470550835132599
train_iter_loss: 0.2194392830133438
train_iter_loss: 0.42794927954673767
train_iter_loss: 0.23048283159732819
train loss :0.2647
---------------------
Validation seg loss: 0.3546450804954149 at epoch 742
epoch =    743/  1000, exp = train
train_iter_loss: 0.2485128790140152
train_iter_loss: 0.13449028134346008
train_iter_loss: 0.21085818111896515
train_iter_loss: 0.4077201783657074
train_iter_loss: 0.21246536076068878
train_iter_loss: 0.3610970377922058
train_iter_loss: 0.19230669736862183
train_iter_loss: 0.15251868963241577
train_iter_loss: 0.3791857361793518
train_iter_loss: 0.32798901200294495
train_iter_loss: 0.40329140424728394
train_iter_loss: 0.36094236373901367
train_iter_loss: 0.19272983074188232
train_iter_loss: 0.19107867777347565
train_iter_loss: 0.2649746239185333
train_iter_loss: 0.322765588760376
train_iter_loss: 0.3221767842769623
train_iter_loss: 0.3991486430168152
train_iter_loss: 0.23492825031280518
train_iter_loss: 0.1628958284854889
train_iter_loss: 0.12404832243919373
train_iter_loss: 0.306573748588562
train_iter_loss: 0.1527758240699768
train_iter_loss: 0.18119165301322937
train_iter_loss: 0.2245936244726181
train_iter_loss: 0.1327870935201645
train_iter_loss: 0.15261715650558472
train_iter_loss: 0.36193859577178955
train_iter_loss: 0.2694449722766876
train_iter_loss: 0.12644509971141815
train_iter_loss: 0.40512678027153015
train_iter_loss: 0.13284613192081451
train_iter_loss: 0.31325650215148926
train_iter_loss: 0.16990339756011963
train_iter_loss: 0.33612409234046936
train_iter_loss: 0.15234152972698212
train_iter_loss: 0.26429542899131775
train_iter_loss: 0.19738152623176575
train_iter_loss: 0.23368927836418152
train_iter_loss: 0.26354411244392395
train_iter_loss: 0.20677800476551056
train_iter_loss: 0.4035092890262604
train_iter_loss: 0.3578077256679535
train_iter_loss: 0.25455841422080994
train_iter_loss: 0.2810588777065277
train_iter_loss: 0.3189637362957001
train_iter_loss: 0.11731799691915512
train_iter_loss: 0.31681013107299805
train_iter_loss: 0.2659144699573517
train_iter_loss: 0.3136248290538788
train_iter_loss: 0.3104487657546997
train_iter_loss: 0.30771130323410034
train_iter_loss: 0.4145970642566681
train_iter_loss: 0.26670053601264954
train_iter_loss: 0.36275529861450195
train_iter_loss: 0.24844111502170563
train_iter_loss: 0.4035567045211792
train_iter_loss: 0.2443358302116394
train_iter_loss: 0.2198283076286316
train_iter_loss: 0.45844173431396484
train_iter_loss: 0.14964011311531067
train_iter_loss: 0.2900969982147217
train_iter_loss: 0.46467649936676025
train_iter_loss: 0.22133715450763702
train_iter_loss: 0.23394498229026794
train_iter_loss: 0.2265203297138214
train_iter_loss: 0.24758343398571014
train_iter_loss: 0.25962215662002563
train_iter_loss: 0.12337733805179596
train_iter_loss: 0.19941440224647522
train_iter_loss: 0.21153217554092407
train_iter_loss: 0.2949605882167816
train_iter_loss: 0.39404669404029846
train_iter_loss: 0.30191949009895325
train_iter_loss: 0.23031802475452423
train_iter_loss: 0.13184893131256104
train_iter_loss: 0.257844477891922
train_iter_loss: 0.26831746101379395
train_iter_loss: 0.21708077192306519
train_iter_loss: 0.2857268452644348
train_iter_loss: 0.23734967410564423
train_iter_loss: 0.1861954778432846
train_iter_loss: 0.37631955742836
train_iter_loss: 0.24131760001182556
train_iter_loss: 0.23751331865787506
train_iter_loss: 0.2061755210161209
train_iter_loss: 0.20943064987659454
train_iter_loss: 0.1847732663154602
train_iter_loss: 0.2803662121295929
train_iter_loss: 0.28708416223526
train_iter_loss: 0.1628541648387909
train_iter_loss: 0.2565746009349823
train_iter_loss: 0.3028453588485718
train_iter_loss: 0.3724409341812134
train_iter_loss: 0.31749799847602844
train_iter_loss: 0.4289770722389221
train_iter_loss: 0.17268642783164978
train_iter_loss: 0.12335312366485596
train_iter_loss: 0.13724014163017273
train_iter_loss: 0.24546539783477783
train loss :0.2630
---------------------
Validation seg loss: 0.3582534937249532 at epoch 743
epoch =    744/  1000, exp = train
train_iter_loss: 0.15856462717056274
train_iter_loss: 0.33241721987724304
train_iter_loss: 0.20920276641845703
train_iter_loss: 0.383327454328537
train_iter_loss: 0.30384859442710876
train_iter_loss: 0.19267578423023224
train_iter_loss: 0.29700109362602234
train_iter_loss: 0.2282881885766983
train_iter_loss: 0.23245741426944733
train_iter_loss: 0.2802339792251587
train_iter_loss: 0.2653410732746124
train_iter_loss: 0.17090003192424774
train_iter_loss: 0.34167319536209106
train_iter_loss: 0.24728122353553772
train_iter_loss: 0.3227837085723877
train_iter_loss: 0.10408273339271545
train_iter_loss: 0.3087908625602722
train_iter_loss: 0.2431211769580841
train_iter_loss: 0.35610607266426086
train_iter_loss: 0.31381097435951233
train_iter_loss: 0.24942533671855927
train_iter_loss: 0.18918102979660034
train_iter_loss: 0.15937575697898865
train_iter_loss: 0.45294567942619324
train_iter_loss: 0.20040775835514069
train_iter_loss: 0.30663594603538513
train_iter_loss: 0.114572674036026
train_iter_loss: 0.1861318200826645
train_iter_loss: 0.19218242168426514
train_iter_loss: 0.271254301071167
train_iter_loss: 0.22040782868862152
train_iter_loss: 0.324049711227417
train_iter_loss: 0.36376410722732544
train_iter_loss: 0.29711267352104187
train_iter_loss: 0.2383861243724823
train_iter_loss: 0.2743251323699951
train_iter_loss: 0.4113694131374359
train_iter_loss: 0.19296590983867645
train_iter_loss: 0.24066556990146637
train_iter_loss: 0.20858584344387054
train_iter_loss: 0.3381168842315674
train_iter_loss: 0.24440822005271912
train_iter_loss: 0.32015398144721985
train_iter_loss: 0.3562118411064148
train_iter_loss: 0.3003869652748108
train_iter_loss: 0.21557356417179108
train_iter_loss: 0.27113038301467896
train_iter_loss: 0.21120381355285645
train_iter_loss: 0.2898126244544983
train_iter_loss: 0.18950822949409485
train_iter_loss: 0.3510500192642212
train_iter_loss: 0.31265023350715637
train_iter_loss: 0.232924222946167
train_iter_loss: 0.18887567520141602
train_iter_loss: 0.20543818175792694
train_iter_loss: 0.20942741632461548
train_iter_loss: 0.1939326822757721
train_iter_loss: 0.2594154477119446
train_iter_loss: 0.17512007057666779
train_iter_loss: 0.1966731995344162
train_iter_loss: 0.17205610871315002
train_iter_loss: 0.11791634559631348
train_iter_loss: 0.4353125989437103
train_iter_loss: 0.37665557861328125
train_iter_loss: 0.22461360692977905
train_iter_loss: 0.24910584092140198
train_iter_loss: 0.20843864977359772
train_iter_loss: 0.2175305187702179
train_iter_loss: 0.25015226006507874
train_iter_loss: 0.47597429156303406
train_iter_loss: 0.14750830829143524
train_iter_loss: 0.18340381979942322
train_iter_loss: 0.110505111515522
train_iter_loss: 0.22282657027244568
train_iter_loss: 0.20848698914051056
train_iter_loss: 0.6726126074790955
train_iter_loss: 0.22486108541488647
train_iter_loss: 0.25415581464767456
train_iter_loss: 0.25635701417922974
train_iter_loss: 0.2229154407978058
train_iter_loss: 0.2006506323814392
train_iter_loss: 0.3184448778629303
train_iter_loss: 0.28322750329971313
train_iter_loss: 0.39304792881011963
train_iter_loss: 0.2619985044002533
train_iter_loss: 0.1473168134689331
train_iter_loss: 0.24468669295310974
train_iter_loss: 0.3043840229511261
train_iter_loss: 0.2191535383462906
train_iter_loss: 0.17844124138355255
train_iter_loss: 0.6141061782836914
train_iter_loss: 0.489928275346756
train_iter_loss: 0.2172398865222931
train_iter_loss: 0.33424195647239685
train_iter_loss: 0.2685468792915344
train_iter_loss: 0.23998863995075226
train_iter_loss: 0.16658103466033936
train_iter_loss: 0.29118818044662476
train_iter_loss: 0.2878945767879486
train_iter_loss: 0.352967232465744
train loss :0.2676
---------------------
Validation seg loss: 0.3857281769643415 at epoch 744
epoch =    745/  1000, exp = train
train_iter_loss: 0.3579750955104828
train_iter_loss: 0.324695348739624
train_iter_loss: 0.225140780210495
train_iter_loss: 0.3608885109424591
train_iter_loss: 0.32058361172676086
train_iter_loss: 0.13802964985370636
train_iter_loss: 0.27205517888069153
train_iter_loss: 0.24233293533325195
train_iter_loss: 0.3632330596446991
train_iter_loss: 0.07969222217798233
train_iter_loss: 0.2053481638431549
train_iter_loss: 0.2950582802295685
train_iter_loss: 0.4374518096446991
train_iter_loss: 0.21219471096992493
train_iter_loss: 0.31610599160194397
train_iter_loss: 0.16824251413345337
train_iter_loss: 0.28124144673347473
train_iter_loss: 0.3270268738269806
train_iter_loss: 0.2850651741027832
train_iter_loss: 0.3438601493835449
train_iter_loss: 0.36238038539886475
train_iter_loss: 0.25960424542427063
train_iter_loss: 0.1442529559135437
train_iter_loss: 0.3083594739437103
train_iter_loss: 0.18077737092971802
train_iter_loss: 0.2825224995613098
train_iter_loss: 0.17674914002418518
train_iter_loss: 0.21905677020549774
train_iter_loss: 0.26007628440856934
train_iter_loss: 0.3080964684486389
train_iter_loss: 0.24282416701316833
train_iter_loss: 0.1645219326019287
train_iter_loss: 0.2423628270626068
train_iter_loss: 0.21017469465732574
train_iter_loss: 0.2932437062263489
train_iter_loss: 0.30610865354537964
train_iter_loss: 0.37582847476005554
train_iter_loss: 0.18814915418624878
train_iter_loss: 0.2047015130519867
train_iter_loss: 0.35301241278648376
train_iter_loss: 0.18375669419765472
train_iter_loss: 0.20289044082164764
train_iter_loss: 0.20443497598171234
train_iter_loss: 0.33270543813705444
train_iter_loss: 0.2002866417169571
train_iter_loss: 0.19784662127494812
train_iter_loss: 0.16863004863262177
train_iter_loss: 0.21160368621349335
train_iter_loss: 0.40243858098983765
train_iter_loss: 0.36075082421302795
train_iter_loss: 0.25283151865005493
train_iter_loss: 0.4588715136051178
train_iter_loss: 0.07077892124652863
train_iter_loss: 0.24287430942058563
train_iter_loss: 0.17330510914325714
train_iter_loss: 0.2637525200843811
train_iter_loss: 0.20262713730335236
train_iter_loss: 0.4324461817741394
train_iter_loss: 0.2179311364889145
train_iter_loss: 0.22766155004501343
train_iter_loss: 0.12763315439224243
train_iter_loss: 0.21980716288089752
train_iter_loss: 0.1901617795228958
train_iter_loss: 0.5204514861106873
train_iter_loss: 0.24453236162662506
train_iter_loss: 0.4547801911830902
train_iter_loss: 0.24780157208442688
train_iter_loss: 0.25138401985168457
train_iter_loss: 0.3087252378463745
train_iter_loss: 0.31118592619895935
train_iter_loss: 0.26939451694488525
train_iter_loss: 0.23187977075576782
train_iter_loss: 0.36309295892715454
train_iter_loss: 0.3181527853012085
train_iter_loss: 0.39948388934135437
train_iter_loss: 0.2125394195318222
train_iter_loss: 0.18502146005630493
train_iter_loss: 0.33278170228004456
train_iter_loss: 0.36920949816703796
train_iter_loss: 0.1579211801290512
train_iter_loss: 0.24618761241436005
train_iter_loss: 0.1453493982553482
train_iter_loss: 0.18951573967933655
train_iter_loss: 0.3585892617702484
train_iter_loss: 0.30625030398368835
train_iter_loss: 0.2878744602203369
train_iter_loss: 0.0880599394440651
train_iter_loss: 0.3649439215660095
train_iter_loss: 0.16596923768520355
train_iter_loss: 0.24864666163921356
train_iter_loss: 0.24294529855251312
train_iter_loss: 0.2729831635951996
train_iter_loss: 0.36787673830986023
train_iter_loss: 0.18089008331298828
train_iter_loss: 0.1728457659482956
train_iter_loss: 0.3743945062160492
train_iter_loss: 0.07733109593391418
train_iter_loss: 0.20268851518630981
train_iter_loss: 0.29210132360458374
train_iter_loss: 0.36166587471961975
train loss :0.2658
---------------------
Validation seg loss: 0.3714880388634244 at epoch 745
epoch =    746/  1000, exp = train
train_iter_loss: 0.34609729051589966
train_iter_loss: 0.3401714861392975
train_iter_loss: 0.12266182899475098
train_iter_loss: 0.28114262223243713
train_iter_loss: 0.21018067002296448
train_iter_loss: 0.22999398410320282
train_iter_loss: 0.3439483642578125
train_iter_loss: 0.3017198145389557
train_iter_loss: 0.22321011126041412
train_iter_loss: 0.18373429775238037
train_iter_loss: 0.18671926856040955
train_iter_loss: 0.24772116541862488
train_iter_loss: 0.27192845940589905
train_iter_loss: 0.4837224781513214
train_iter_loss: 0.23592674732208252
train_iter_loss: 0.28447288274765015
train_iter_loss: 0.20121681690216064
train_iter_loss: 0.2640322148799896
train_iter_loss: 0.33167508244514465
train_iter_loss: 0.15707039833068848
train_iter_loss: 0.20867560803890228
train_iter_loss: 0.26436224579811096
train_iter_loss: 0.3521508574485779
train_iter_loss: 0.21092039346694946
train_iter_loss: 0.41424015164375305
train_iter_loss: 0.1558496356010437
train_iter_loss: 0.3386351466178894
train_iter_loss: 0.13943970203399658
train_iter_loss: 0.22520682215690613
train_iter_loss: 0.14838603138923645
train_iter_loss: 0.2549302279949188
train_iter_loss: 0.14998671412467957
train_iter_loss: 0.24751029908657074
train_iter_loss: 0.25659120082855225
train_iter_loss: 0.10950907319784164
train_iter_loss: 0.2028791457414627
train_iter_loss: 0.4207760691642761
train_iter_loss: 0.32800453901290894
train_iter_loss: 0.33094480633735657
train_iter_loss: 0.27642276883125305
train_iter_loss: 0.11470608413219452
train_iter_loss: 0.4305121600627899
train_iter_loss: 0.30039316415786743
train_iter_loss: 0.26166290044784546
train_iter_loss: 0.3000359833240509
train_iter_loss: 0.2496747225522995
train_iter_loss: 0.36838069558143616
train_iter_loss: 0.18331791460514069
train_iter_loss: 0.3404404819011688
train_iter_loss: 0.2695069909095764
train_iter_loss: 0.19648981094360352
train_iter_loss: 0.26125621795654297
train_iter_loss: 0.3773725628852844
train_iter_loss: 0.3281950354576111
train_iter_loss: 0.3391241133213043
train_iter_loss: 0.29528263211250305
train_iter_loss: 0.22265800833702087
train_iter_loss: 0.0348268523812294
train_iter_loss: 0.3602054715156555
train_iter_loss: 0.2171778678894043
train_iter_loss: 0.30256375670433044
train_iter_loss: 0.3656037151813507
train_iter_loss: 0.22076010704040527
train_iter_loss: 0.2734310030937195
train_iter_loss: 0.21246734261512756
train_iter_loss: 0.25062358379364014
train_iter_loss: 0.2385271042585373
train_iter_loss: 0.1593228131532669
train_iter_loss: 0.19783435761928558
train_iter_loss: 0.21045906841754913
train_iter_loss: 0.32625719904899597
train_iter_loss: 0.3059658110141754
train_iter_loss: 0.2468142956495285
train_iter_loss: 0.2484133094549179
train_iter_loss: 0.3456774950027466
train_iter_loss: 0.2560603618621826
train_iter_loss: 0.4637605547904968
train_iter_loss: 0.3103739321231842
train_iter_loss: 0.1019245907664299
train_iter_loss: 0.2498418539762497
train_iter_loss: 0.26067638397216797
train_iter_loss: 0.22238637506961823
train_iter_loss: 0.36863476037979126
train_iter_loss: 0.2828310430049896
train_iter_loss: 0.20715144276618958
train_iter_loss: 0.2638978958129883
train_iter_loss: 0.20012857019901276
train_iter_loss: 0.34586745500564575
train_iter_loss: 0.26580846309661865
train_iter_loss: 0.3016688823699951
train_iter_loss: 0.2958288788795471
train_iter_loss: 0.26288798451423645
train_iter_loss: 0.2811459004878998
train_iter_loss: 0.2052188366651535
train_iter_loss: 0.22894874215126038
train_iter_loss: 0.2936858832836151
train_iter_loss: 0.31850889325141907
train_iter_loss: 0.1610196977853775
train_iter_loss: 0.17688053846359253
train_iter_loss: 0.2377920150756836
train loss :0.2647
---------------------
Validation seg loss: 0.364341196731471 at epoch 746
epoch =    747/  1000, exp = train
train_iter_loss: 0.341096967458725
train_iter_loss: 0.14386439323425293
train_iter_loss: 0.35863155126571655
train_iter_loss: 0.2519424557685852
train_iter_loss: 0.09410247951745987
train_iter_loss: 0.17347422242164612
train_iter_loss: 0.190152108669281
train_iter_loss: 0.37278106808662415
train_iter_loss: 0.2646428346633911
train_iter_loss: 0.16302338242530823
train_iter_loss: 0.2013501673936844
train_iter_loss: 0.22032713890075684
train_iter_loss: 0.2757619619369507
train_iter_loss: 0.20732557773590088
train_iter_loss: 0.3700726330280304
train_iter_loss: 0.1978125423192978
train_iter_loss: 0.31596896052360535
train_iter_loss: 0.5043357014656067
train_iter_loss: 0.29466041922569275
train_iter_loss: 0.3462172746658325
train_iter_loss: 0.2666657865047455
train_iter_loss: 0.25651922821998596
train_iter_loss: 0.23723188042640686
train_iter_loss: 0.18328700959682465
train_iter_loss: 0.2077515572309494
train_iter_loss: 0.17408835887908936
train_iter_loss: 0.37015029788017273
train_iter_loss: 0.29885074496269226
train_iter_loss: 0.2987823486328125
train_iter_loss: 0.3640000522136688
train_iter_loss: 0.17701052129268646
train_iter_loss: 0.20998984575271606
train_iter_loss: 0.1448153555393219
train_iter_loss: 0.2875624895095825
train_iter_loss: 0.31435972452163696
train_iter_loss: 0.2851411998271942
train_iter_loss: 0.2296193242073059
train_iter_loss: 0.149607315659523
train_iter_loss: 0.35497355461120605
train_iter_loss: 0.3242756426334381
train_iter_loss: 0.2954561114311218
train_iter_loss: 0.26020869612693787
train_iter_loss: 0.20815299451351166
train_iter_loss: 0.34314268827438354
train_iter_loss: 0.22397220134735107
train_iter_loss: 0.4059477150440216
train_iter_loss: 0.15201601386070251
train_iter_loss: 0.19696681201457977
train_iter_loss: 0.23573091626167297
train_iter_loss: 0.1608356535434723
train_iter_loss: 0.26197582483291626
train_iter_loss: 0.2149321287870407
train_iter_loss: 0.42545849084854126
train_iter_loss: 0.32582879066467285
train_iter_loss: 0.35982561111450195
train_iter_loss: 0.20315289497375488
train_iter_loss: 0.2903386056423187
train_iter_loss: 0.2408638894557953
train_iter_loss: 0.33710822463035583
train_iter_loss: 0.34496361017227173
train_iter_loss: 0.21876639127731323
train_iter_loss: 0.13490594923496246
train_iter_loss: 0.22798039019107819
train_iter_loss: 0.28600025177001953
train_iter_loss: 0.3958338499069214
train_iter_loss: 0.1370953917503357
train_iter_loss: 0.3761214315891266
train_iter_loss: 0.20149078965187073
train_iter_loss: 0.2669128179550171
train_iter_loss: 0.25890597701072693
train_iter_loss: 0.27562445402145386
train_iter_loss: 0.2774229347705841
train_iter_loss: 0.2802329957485199
train_iter_loss: 0.06476784497499466
train_iter_loss: 0.2587485611438751
train_iter_loss: 0.23360401391983032
train_iter_loss: 0.2743968665599823
train_iter_loss: 0.3213816285133362
train_iter_loss: 0.24284470081329346
train_iter_loss: 0.4198392927646637
train_iter_loss: 0.3272247314453125
train_iter_loss: 0.38197997212409973
train_iter_loss: 0.22178339958190918
train_iter_loss: 0.23854835331439972
train_iter_loss: 0.34281933307647705
train_iter_loss: 0.17050956189632416
train_iter_loss: 0.15126024186611176
train_iter_loss: 0.20883020758628845
train_iter_loss: 0.3257639408111572
train_iter_loss: 0.2597086727619171
train_iter_loss: 0.27533861994743347
train_iter_loss: 0.35707882046699524
train_iter_loss: 0.2687195837497711
train_iter_loss: 0.3230116665363312
train_iter_loss: 0.10100787878036499
train_iter_loss: 0.2505147457122803
train_iter_loss: 0.16132347285747528
train_iter_loss: 0.46975788474082947
train_iter_loss: 0.3332272469997406
train_iter_loss: 0.2949766218662262
train loss :0.2679
---------------------
Validation seg loss: 0.38268137951645087 at epoch 747
epoch =    748/  1000, exp = train
train_iter_loss: 0.34541600942611694
train_iter_loss: 0.15984906256198883
train_iter_loss: 0.3080984950065613
train_iter_loss: 0.21535497903823853
train_iter_loss: 0.14412736892700195
train_iter_loss: 0.1641494482755661
train_iter_loss: 0.4128318130970001
train_iter_loss: 0.2795931398868561
train_iter_loss: 0.22991035878658295
train_iter_loss: 0.30123934149742126
train_iter_loss: 0.2949943542480469
train_iter_loss: 0.15676671266555786
train_iter_loss: 0.4031219482421875
train_iter_loss: 0.19391293823719025
train_iter_loss: 0.2172164022922516
train_iter_loss: 0.3650801181793213
train_iter_loss: 0.19652059674263
train_iter_loss: 0.20622499287128448
train_iter_loss: 0.33146485686302185
train_iter_loss: 0.4731334149837494
train_iter_loss: 0.2208525538444519
train_iter_loss: 0.4159582257270813
train_iter_loss: 0.27089011669158936
train_iter_loss: 0.30678921937942505
train_iter_loss: 0.3254174292087555
train_iter_loss: 0.2038998156785965
train_iter_loss: 0.09311976283788681
train_iter_loss: 0.37843501567840576
train_iter_loss: 0.36771321296691895
train_iter_loss: 0.2702817916870117
train_iter_loss: 0.3129955232143402
train_iter_loss: 0.268525630235672
train_iter_loss: 0.24684375524520874
train_iter_loss: 0.12970224022865295
train_iter_loss: 0.3192490041255951
train_iter_loss: 0.13109253346920013
train_iter_loss: 0.2535865902900696
train_iter_loss: 0.3258192241191864
train_iter_loss: 0.30559495091438293
train_iter_loss: 0.08764014393091202
train_iter_loss: 0.23354282975196838
train_iter_loss: 0.3135339319705963
train_iter_loss: 0.28184884786605835
train_iter_loss: 0.45752015709877014
train_iter_loss: 0.2094947248697281
train_iter_loss: 0.2634434103965759
train_iter_loss: 0.24433490633964539
train_iter_loss: 0.1266932636499405
train_iter_loss: 0.224400594830513
train_iter_loss: 0.26451995968818665
train_iter_loss: 0.3249693810939789
train_iter_loss: 0.18085432052612305
train_iter_loss: 0.3861197829246521
train_iter_loss: 0.21159033477306366
train_iter_loss: 0.32098475098609924
train_iter_loss: 0.18529273569583893
train_iter_loss: 0.33300548791885376
train_iter_loss: 0.29878222942352295
train_iter_loss: 0.3222447335720062
train_iter_loss: 0.2614508867263794
train_iter_loss: 0.28215041756629944
train_iter_loss: 0.2842794954776764
train_iter_loss: 0.3209667205810547
train_iter_loss: 0.27601104974746704
train_iter_loss: 0.13278579711914062
train_iter_loss: 0.17863988876342773
train_iter_loss: 0.3820500373840332
train_iter_loss: 0.22378724813461304
train_iter_loss: 0.111945740878582
train_iter_loss: 0.3292867839336395
train_iter_loss: 0.42295870184898376
train_iter_loss: 0.19017867743968964
train_iter_loss: 0.1384022831916809
train_iter_loss: 0.33208152651786804
train_iter_loss: 0.23518472909927368
train_iter_loss: 0.17718422412872314
train_iter_loss: 0.26609107851982117
train_iter_loss: 0.20966050028800964
train_iter_loss: 0.2766328752040863
train_iter_loss: 0.11970166862010956
train_iter_loss: 0.33004340529441833
train_iter_loss: 0.2813570201396942
train_iter_loss: 0.3698793351650238
train_iter_loss: 0.19325681030750275
train_iter_loss: 0.3510681688785553
train_iter_loss: 0.33509302139282227
train_iter_loss: 0.22987401485443115
train_iter_loss: 0.24882283806800842
train_iter_loss: 0.21279524266719818
train_iter_loss: 0.2302420735359192
train_iter_loss: 0.15449437499046326
train_iter_loss: 0.16571871936321259
train_iter_loss: 0.4695011079311371
train_iter_loss: 0.1735296994447708
train_iter_loss: 0.12756173312664032
train_iter_loss: 0.23046179115772247
train_iter_loss: 0.3156677186489105
train_iter_loss: 0.22143937647342682
train_iter_loss: 0.29728731513023376
train_iter_loss: 0.36677443981170654
train loss :0.2654
---------------------
Validation seg loss: 0.37894736109125726 at epoch 748
epoch =    749/  1000, exp = train
train_iter_loss: 0.20314455032348633
train_iter_loss: 0.18645580112934113
train_iter_loss: 0.11138168722391129
train_iter_loss: 0.20692098140716553
train_iter_loss: 0.20334786176681519
train_iter_loss: 0.29086539149284363
train_iter_loss: 0.19168256223201752
train_iter_loss: 0.274114191532135
train_iter_loss: 0.3400563597679138
train_iter_loss: 0.2589525878429413
train_iter_loss: 0.23932716250419617
train_iter_loss: 0.24074000120162964
train_iter_loss: 0.13876265287399292
train_iter_loss: 0.16705335676670074
train_iter_loss: 0.2864796221256256
train_iter_loss: 0.3404424786567688
train_iter_loss: 0.3808117210865021
train_iter_loss: 0.15787118673324585
train_iter_loss: 0.23345480859279633
train_iter_loss: 0.19529865682125092
train_iter_loss: 0.3360970616340637
train_iter_loss: 0.30476608872413635
train_iter_loss: 0.4366011619567871
train_iter_loss: 0.2461521327495575
train_iter_loss: 0.40887215733528137
train_iter_loss: 0.35420361161231995
train_iter_loss: 0.26836124062538147
train_iter_loss: 0.3312453329563141
train_iter_loss: 0.2579452395439148
train_iter_loss: 0.19818493723869324
train_iter_loss: 0.1390753835439682
train_iter_loss: 0.24423716962337494
train_iter_loss: 0.39922499656677246
train_iter_loss: 0.20952633023262024
train_iter_loss: 0.2494240254163742
train_iter_loss: 0.2980100214481354
train_iter_loss: 0.30907920002937317
train_iter_loss: 0.22758154571056366
train_iter_loss: 0.2898482382297516
train_iter_loss: 0.3236924111843109
train_iter_loss: 0.24827940762043
train_iter_loss: 0.27560189366340637
train_iter_loss: 0.2758277356624603
train_iter_loss: 0.32265526056289673
train_iter_loss: 0.2328411340713501
train_iter_loss: 0.22413772344589233
train_iter_loss: 0.286583811044693
train_iter_loss: 0.2423832267522812
train_iter_loss: 0.3683796226978302
train_iter_loss: 0.22274082899093628
train_iter_loss: 0.22898808121681213
train_iter_loss: 0.4034533202648163
train_iter_loss: 0.12167458236217499
train_iter_loss: 0.26264023780822754
train_iter_loss: 0.33604276180267334
train_iter_loss: 0.3107154667377472
train_iter_loss: 0.19359344244003296
train_iter_loss: 0.13730183243751526
train_iter_loss: 0.2662915885448456
train_iter_loss: 0.4283507764339447
train_iter_loss: 0.3084953725337982
train_iter_loss: 0.1441885232925415
train_iter_loss: 0.32386985421180725
train_iter_loss: 0.34235018491744995
train_iter_loss: 0.2530733346939087
train_iter_loss: 0.2108125239610672
train_iter_loss: 0.2257649004459381
train_iter_loss: 0.26737767457962036
train_iter_loss: 0.2686421275138855
train_iter_loss: 0.2427135705947876
train_iter_loss: 0.13183504343032837
train_iter_loss: 0.21162548661231995
train_iter_loss: 0.13323210179805756
train_iter_loss: 0.23155708611011505
train_iter_loss: 0.27504763007164
train_iter_loss: 0.26936760544776917
train_iter_loss: 0.18972180783748627
train_iter_loss: 0.19053028523921967
train_iter_loss: 0.11102670431137085
train_iter_loss: 0.2815198004245758
train_iter_loss: 0.34968486428260803
train_iter_loss: 0.3039902448654175
train_iter_loss: 0.2587260901927948
train_iter_loss: 0.30151528120040894
train_iter_loss: 0.26115772128105164
train_iter_loss: 0.31860288977622986
train_iter_loss: 0.1405945122241974
train_iter_loss: 0.18777605891227722
train_iter_loss: 0.3975297808647156
train_iter_loss: 0.3113269805908203
train_iter_loss: 0.2802150249481201
train_iter_loss: 0.288911372423172
train_iter_loss: 0.23895275592803955
train_iter_loss: 0.171035036444664
train_iter_loss: 0.32655298709869385
train_iter_loss: 0.285214364528656
train_iter_loss: 0.29703426361083984
train_iter_loss: 0.34987738728523254
train_iter_loss: 0.24760830402374268
train_iter_loss: 0.23332469165325165
train loss :0.2630
---------------------
Validation seg loss: 0.36622767597612627 at epoch 749
epoch =    750/  1000, exp = train
train_iter_loss: 0.3267068862915039
train_iter_loss: 0.47172030806541443
train_iter_loss: 0.2733524739742279
train_iter_loss: 0.2214200347661972
train_iter_loss: 0.3594399094581604
train_iter_loss: 0.32837173342704773
train_iter_loss: 0.36092692613601685
train_iter_loss: 0.30773553252220154
train_iter_loss: 0.10918143391609192
train_iter_loss: 0.5169923305511475
train_iter_loss: 0.15160731971263885
train_iter_loss: 0.4352264404296875
train_iter_loss: 0.21364107728004456
train_iter_loss: 0.22965474426746368
train_iter_loss: 0.16477343440055847
train_iter_loss: 0.29417210817337036
train_iter_loss: 0.12155900150537491
train_iter_loss: 0.34613555669784546
train_iter_loss: 0.43762198090553284
train_iter_loss: 0.39414653182029724
train_iter_loss: 0.10066041350364685
train_iter_loss: 0.42091497778892517
train_iter_loss: 0.36156266927719116
train_iter_loss: 0.1661583036184311
train_iter_loss: 0.12790998816490173
train_iter_loss: 0.18364344537258148
train_iter_loss: 0.17150932550430298
train_iter_loss: 0.22451937198638916
train_iter_loss: 0.30630284547805786
train_iter_loss: 0.20197634398937225
train_iter_loss: 0.43271249532699585
train_iter_loss: 0.14439740777015686
train_iter_loss: 0.2438783347606659
train_iter_loss: 0.2290901243686676
train_iter_loss: 0.23868966102600098
train_iter_loss: 0.2832423150539398
train_iter_loss: 0.31100034713745117
train_iter_loss: 0.27804771065711975
train_iter_loss: 0.3859344720840454
train_iter_loss: 0.2475942075252533
train_iter_loss: 0.15831269323825836
train_iter_loss: 0.2142687439918518
train_iter_loss: 0.25775831937789917
train_iter_loss: 0.20088183879852295
train_iter_loss: 0.1784888505935669
train_iter_loss: 0.25716862082481384
train_iter_loss: 0.2549815773963928
train_iter_loss: 0.18732650578022003
train_iter_loss: 0.2596495747566223
train_iter_loss: 0.15021711587905884
train_iter_loss: 0.2541952133178711
train_iter_loss: 0.3771360218524933
train_iter_loss: 0.17525853216648102
train_iter_loss: 0.16368108987808228
train_iter_loss: 0.22377148270606995
train_iter_loss: 0.31995588541030884
train_iter_loss: 0.24282418191432953
train_iter_loss: 0.25999489426612854
train_iter_loss: 0.22801969945430756
train_iter_loss: 0.20537497103214264
train_iter_loss: 0.16849006712436676
train_iter_loss: 0.273196816444397
train_iter_loss: 0.30937659740448
train_iter_loss: 0.08023152500391006
train_iter_loss: 0.17185422778129578
train_iter_loss: 0.2538776397705078
train_iter_loss: 0.3111964166164398
train_iter_loss: 0.21243561804294586
train_iter_loss: 0.3252355456352234
train_iter_loss: 0.2514704167842865
train_iter_loss: 0.4051958918571472
train_iter_loss: 0.4057404696941376
train_iter_loss: 0.2187729924917221
train_iter_loss: 0.2818388342857361
train_iter_loss: 0.2279900461435318
train_iter_loss: 0.23800913989543915
train_iter_loss: 0.1729588508605957
train_iter_loss: 0.22448380291461945
train_iter_loss: 0.42107969522476196
train_iter_loss: 0.3032390773296356
train_iter_loss: 0.44812509417533875
train_iter_loss: 0.08955416083335876
train_iter_loss: 0.360250324010849
train_iter_loss: 0.24285003542900085
train_iter_loss: 0.471875935792923
train_iter_loss: 0.21309229731559753
train_iter_loss: 0.33820879459381104
train_iter_loss: 0.3050493597984314
train_iter_loss: 0.18427617847919464
train_iter_loss: 0.06911661475896835
train_iter_loss: 0.2736952304840088
train_iter_loss: 0.17578274011611938
train_iter_loss: 0.2390878051519394
train_iter_loss: 0.23086336255073547
train_iter_loss: 0.37112128734588623
train_iter_loss: 0.2130044549703598
train_iter_loss: 0.2774621844291687
train_iter_loss: 0.15950845181941986
train_iter_loss: 0.29881158471107483
train_iter_loss: 0.16908162832260132
train loss :0.2634
---------------------
Validation seg loss: 0.3678554940819389 at epoch 750
epoch =    751/  1000, exp = train
train_iter_loss: 0.1597786247730255
train_iter_loss: 0.289221853017807
train_iter_loss: 0.2604007124900818
train_iter_loss: 0.46235665678977966
train_iter_loss: 0.387570321559906
train_iter_loss: 0.20011021196842194
train_iter_loss: 0.21469910442829132
train_iter_loss: 0.28344962000846863
train_iter_loss: 0.23250563442707062
train_iter_loss: 0.18404541909694672
train_iter_loss: 0.18986117839813232
train_iter_loss: 0.31804507970809937
train_iter_loss: 0.2724311053752899
train_iter_loss: 0.3792646825313568
train_iter_loss: 0.2376427948474884
train_iter_loss: 0.22569793462753296
train_iter_loss: 0.36453235149383545
train_iter_loss: 0.05205289646983147
train_iter_loss: 0.23267191648483276
train_iter_loss: 0.21545541286468506
train_iter_loss: 0.26244762539863586
train_iter_loss: 0.2838578522205353
train_iter_loss: 0.20846785604953766
train_iter_loss: 0.32636258006095886
train_iter_loss: 0.2240857034921646
train_iter_loss: 0.3764145076274872
train_iter_loss: 0.33722156286239624
train_iter_loss: 0.18382321298122406
train_iter_loss: 0.36685964465141296
train_iter_loss: 0.16372762620449066
train_iter_loss: 0.255109965801239
train_iter_loss: 0.3169277608394623
train_iter_loss: 0.28573888540267944
train_iter_loss: 0.19770754873752594
train_iter_loss: 0.8137186169624329
train_iter_loss: 0.2754638195037842
train_iter_loss: 0.2699621021747589
train_iter_loss: 0.25842559337615967
train_iter_loss: 0.2099739909172058
train_iter_loss: 0.08409750461578369
train_iter_loss: 0.3285534083843231
train_iter_loss: 0.3069215416908264
train_iter_loss: 0.27134278416633606
train_iter_loss: 0.17431476712226868
train_iter_loss: 0.11916318535804749
train_iter_loss: 0.2497699111700058
train_iter_loss: 0.2932990789413452
train_iter_loss: 0.3461536467075348
train_iter_loss: 0.20294228196144104
train_iter_loss: 0.2971782982349396
train_iter_loss: 0.30668002367019653
train_iter_loss: 0.3015163540840149
train_iter_loss: 0.17821653187274933
train_iter_loss: 0.2391311228275299
train_iter_loss: 0.30025845766067505
train_iter_loss: 0.45098158717155457
train_iter_loss: 0.2942771911621094
train_iter_loss: 0.37352943420410156
train_iter_loss: 0.142238587141037
train_iter_loss: 0.35671940445899963
train_iter_loss: 0.37625548243522644
train_iter_loss: 0.11708381772041321
train_iter_loss: 0.4291397035121918
train_iter_loss: 0.06344983726739883
train_iter_loss: 0.31436648964881897
train_iter_loss: 0.13690975308418274
train_iter_loss: 0.33421751856803894
train_iter_loss: 0.23192822933197021
train_iter_loss: 0.31941986083984375
train_iter_loss: 0.32140570878982544
train_iter_loss: 0.15939822793006897
train_iter_loss: 0.2222721427679062
train_iter_loss: 0.2795197665691376
train_iter_loss: 0.1911957561969757
train_iter_loss: 0.29843881726264954
train_iter_loss: 0.265107125043869
train_iter_loss: 0.0936465784907341
train_iter_loss: 0.13028517365455627
train_iter_loss: 0.19465219974517822
train_iter_loss: 0.3830246329307556
train_iter_loss: 0.3306742012500763
train_iter_loss: 0.335122674703598
train_iter_loss: 0.3373752236366272
train_iter_loss: 0.3863375782966614
train_iter_loss: 0.42074811458587646
train_iter_loss: 0.30409133434295654
train_iter_loss: 0.4874098002910614
train_iter_loss: 0.2931925058364868
train_iter_loss: 0.3876287341117859
train_iter_loss: 0.20718735456466675
train_iter_loss: 0.4507785439491272
train_iter_loss: 0.22923271358013153
train_iter_loss: 0.2466626912355423
train_iter_loss: 0.24178513884544373
train_iter_loss: 0.21409998834133148
train_iter_loss: 0.25010231137275696
train_iter_loss: 0.2680087685585022
train_iter_loss: 0.23279735445976257
train_iter_loss: 0.3318488597869873
train_iter_loss: 0.2162322998046875
train loss :0.2769
---------------------
Validation seg loss: 0.3583315441314623 at epoch 751
epoch =    752/  1000, exp = train
train_iter_loss: 0.4455477297306061
train_iter_loss: 0.36118122935295105
train_iter_loss: 0.2626074254512787
train_iter_loss: 0.3197384178638458
train_iter_loss: 0.32613953948020935
train_iter_loss: 0.3771047592163086
train_iter_loss: 0.39153239130973816
train_iter_loss: 0.30087152123451233
train_iter_loss: 0.29411616921424866
train_iter_loss: 0.260306715965271
train_iter_loss: 0.34231114387512207
train_iter_loss: 0.21419782936573029
train_iter_loss: 0.4587293565273285
train_iter_loss: 0.4234515130519867
train_iter_loss: 0.4659859538078308
train_iter_loss: 0.16247300803661346
train_iter_loss: 0.3673146069049835
train_iter_loss: 0.39882904291152954
train_iter_loss: 0.23206309974193573
train_iter_loss: 0.26179125905036926
train_iter_loss: 0.2370031625032425
train_iter_loss: 0.1688007116317749
train_iter_loss: 0.18412254750728607
train_iter_loss: 0.14430536329746246
train_iter_loss: 0.08025307953357697
train_iter_loss: 0.31484556198120117
train_iter_loss: 0.16055113077163696
train_iter_loss: 0.2747553884983063
train_iter_loss: 0.21419765055179596
train_iter_loss: 0.15977820754051208
train_iter_loss: 0.17188620567321777
train_iter_loss: 0.1640993356704712
train_iter_loss: 0.12537875771522522
train_iter_loss: 0.3069753348827362
train_iter_loss: 0.26292699575424194
train_iter_loss: 0.2758108675479889
train_iter_loss: 0.3239160180091858
train_iter_loss: 0.2815534770488739
train_iter_loss: 0.39865919947624207
train_iter_loss: 0.20581567287445068
train_iter_loss: 0.19008006155490875
train_iter_loss: 0.271697998046875
train_iter_loss: 0.28325796127319336
train_iter_loss: 0.16136550903320312
train_iter_loss: 0.19853892922401428
train_iter_loss: 0.2611995339393616
train_iter_loss: 0.48284250497817993
train_iter_loss: 0.21234388649463654
train_iter_loss: 0.3143998384475708
train_iter_loss: 0.3182086646556854
train_iter_loss: 0.21261821687221527
train_iter_loss: 0.310596764087677
train_iter_loss: 0.09068809449672699
train_iter_loss: 0.10564713925123215
train_iter_loss: 0.18747468292713165
train_iter_loss: 0.2884913384914398
train_iter_loss: 0.2650126516819
train_iter_loss: 0.18721775710582733
train_iter_loss: 0.11846031993627548
train_iter_loss: 0.36519816517829895
train_iter_loss: 0.2782818675041199
train_iter_loss: 0.14749473333358765
train_iter_loss: 0.2050718367099762
train_iter_loss: 0.132038876414299
train_iter_loss: 0.18840062618255615
train_iter_loss: 0.2551519572734833
train_iter_loss: 0.3087170124053955
train_iter_loss: 0.17677193880081177
train_iter_loss: 0.21849974989891052
train_iter_loss: 0.4340662360191345
train_iter_loss: 0.34650057554244995
train_iter_loss: 0.23610876500606537
train_iter_loss: 0.17480237782001495
train_iter_loss: 0.4057019054889679
train_iter_loss: 0.40440115332603455
train_iter_loss: 0.2159295380115509
train_iter_loss: 0.3696839213371277
train_iter_loss: 0.10102345794439316
train_iter_loss: 0.16828154027462006
train_iter_loss: 0.3213093876838684
train_iter_loss: 0.1637081503868103
train_iter_loss: 0.1630631387233734
train_iter_loss: 0.33367210626602173
train_iter_loss: 0.3665856122970581
train_iter_loss: 0.16130568087100983
train_iter_loss: 0.2998030483722687
train_iter_loss: 0.1639500856399536
train_iter_loss: 0.1573958545923233
train_iter_loss: 0.2998122572898865
train_iter_loss: 0.12264321744441986
train_iter_loss: 0.25593259930610657
train_iter_loss: 0.25256747007369995
train_iter_loss: 0.3506461977958679
train_iter_loss: 0.22929373383522034
train_iter_loss: 0.3078947365283966
train_iter_loss: 0.252806693315506
train_iter_loss: 0.2941826581954956
train_iter_loss: 0.34820252656936646
train_iter_loss: 0.23568418622016907
train_iter_loss: 0.22053290903568268
train loss :0.2629
---------------------
Validation seg loss: 0.35152910111190855 at epoch 752
epoch =    753/  1000, exp = train
train_iter_loss: 0.12143547832965851
train_iter_loss: 0.1926116645336151
train_iter_loss: 0.2637997567653656
train_iter_loss: 0.27161967754364014
train_iter_loss: 0.13128846883773804
train_iter_loss: 0.15350475907325745
train_iter_loss: 0.1891130805015564
train_iter_loss: 0.1977621465921402
train_iter_loss: 0.1573069989681244
train_iter_loss: 0.24833635985851288
train_iter_loss: 0.12194953858852386
train_iter_loss: 0.27330172061920166
train_iter_loss: 0.3834422826766968
train_iter_loss: 0.14503878355026245
train_iter_loss: 0.1417727917432785
train_iter_loss: 0.23113907873630524
train_iter_loss: 0.045501355081796646
train_iter_loss: 0.15828156471252441
train_iter_loss: 0.24533121287822723
train_iter_loss: 0.2823048532009125
train_iter_loss: 0.3626213073730469
train_iter_loss: 0.2309916764497757
train_iter_loss: 0.2705408036708832
train_iter_loss: 0.32824358344078064
train_iter_loss: 0.2189374715089798
train_iter_loss: 0.26295891404151917
train_iter_loss: 0.16131283342838287
train_iter_loss: 0.2296273410320282
train_iter_loss: 0.20866110920906067
train_iter_loss: 0.2175883799791336
train_iter_loss: 0.45114773511886597
train_iter_loss: 0.16354288160800934
train_iter_loss: 0.32625749707221985
train_iter_loss: 0.31992846727371216
train_iter_loss: 0.35781940817832947
train_iter_loss: 0.3077501058578491
train_iter_loss: 0.21797582507133484
train_iter_loss: 0.3717952370643616
train_iter_loss: 0.23019443452358246
train_iter_loss: 0.17669522762298584
train_iter_loss: 0.351809561252594
train_iter_loss: 0.2789836823940277
train_iter_loss: 0.2335902601480484
train_iter_loss: 0.3146684765815735
train_iter_loss: 0.20640979707241058
train_iter_loss: 0.12789225578308105
train_iter_loss: 0.2350810468196869
train_iter_loss: 0.4086548686027527
train_iter_loss: 0.2794210612773895
train_iter_loss: 0.3195682466030121
train_iter_loss: 0.4205382764339447
train_iter_loss: 0.29624035954475403
train_iter_loss: 0.3102071285247803
train_iter_loss: 0.3200796842575073
train_iter_loss: 0.30039775371551514
train_iter_loss: 0.2693195044994354
train_iter_loss: 0.26019299030303955
train_iter_loss: 0.3250136375427246
train_iter_loss: 0.3018699586391449
train_iter_loss: 0.22576723992824554
train_iter_loss: 0.3777717649936676
train_iter_loss: 0.16840149462223053
train_iter_loss: 0.21600781381130219
train_iter_loss: 0.32266348600387573
train_iter_loss: 0.3102758228778839
train_iter_loss: 0.32016149163246155
train_iter_loss: 0.0748460665345192
train_iter_loss: 0.2929282486438751
train_iter_loss: 0.18097200989723206
train_iter_loss: 0.20634418725967407
train_iter_loss: 0.3349437117576599
train_iter_loss: 0.3112662732601166
train_iter_loss: 0.14382128417491913
train_iter_loss: 0.2625526785850525
train_iter_loss: 0.2804092466831207
train_iter_loss: 0.3078661561012268
train_iter_loss: 0.25416597723960876
train_iter_loss: 0.19533877074718475
train_iter_loss: 0.16286790370941162
train_iter_loss: 0.2747229039669037
train_iter_loss: 0.3661577105522156
train_iter_loss: 0.17158742249011993
train_iter_loss: 0.21847620606422424
train_iter_loss: 0.4265964925289154
train_iter_loss: 0.3228283226490021
train_iter_loss: 0.18532973527908325
train_iter_loss: 0.4021300971508026
train_iter_loss: 0.2459123581647873
train_iter_loss: 0.15795180201530457
train_iter_loss: 0.265531450510025
train_iter_loss: 0.4107719957828522
train_iter_loss: 0.44082340598106384
train_iter_loss: 0.20099957287311554
train_iter_loss: 0.19491034746170044
train_iter_loss: 0.2609747052192688
train_iter_loss: 0.2839547395706177
train_iter_loss: 0.3766920566558838
train_iter_loss: 0.2524513304233551
train_iter_loss: 0.1796753853559494
train_iter_loss: 0.35640430450439453
train loss :0.2614
---------------------
Validation seg loss: 0.34641172236956236 at epoch 753
epoch =    754/  1000, exp = train
train_iter_loss: 0.3630075454711914
train_iter_loss: 0.21748816967010498
train_iter_loss: 0.29042118787765503
train_iter_loss: 0.30153152346611023
train_iter_loss: 0.25524306297302246
train_iter_loss: 0.28921717405319214
train_iter_loss: 0.1348399966955185
train_iter_loss: 0.27763083577156067
train_iter_loss: 0.26329392194747925
train_iter_loss: 0.2644764184951782
train_iter_loss: 0.22833947837352753
train_iter_loss: 0.30823224782943726
train_iter_loss: 0.23229672014713287
train_iter_loss: 0.17167282104492188
train_iter_loss: 0.2346373051404953
train_iter_loss: 0.2989455461502075
train_iter_loss: 0.2140740007162094
train_iter_loss: 0.29372167587280273
train_iter_loss: 0.21659693121910095
train_iter_loss: 0.351729154586792
train_iter_loss: 0.31249508261680603
train_iter_loss: 0.23591448366641998
train_iter_loss: 0.38424986600875854
train_iter_loss: 0.1075611561536789
train_iter_loss: 0.4105113744735718
train_iter_loss: 0.2223116010427475
train_iter_loss: 0.17668935656547546
train_iter_loss: 0.16213834285736084
train_iter_loss: 0.2547059953212738
train_iter_loss: 0.20920196175575256
train_iter_loss: 0.2897210419178009
train_iter_loss: 0.1533001810312271
train_iter_loss: 0.07244791090488434
train_iter_loss: 0.22868500649929047
train_iter_loss: 0.30697211623191833
train_iter_loss: 0.30979496240615845
train_iter_loss: 0.19693569839000702
train_iter_loss: 0.4001466631889343
train_iter_loss: 0.29015448689460754
train_iter_loss: 0.27668845653533936
train_iter_loss: 0.43463197350502014
train_iter_loss: 0.28751498460769653
train_iter_loss: 0.2652895450592041
train_iter_loss: 0.30170872807502747
train_iter_loss: 0.34669235348701477
train_iter_loss: 0.3610112965106964
train_iter_loss: 0.3958936929702759
train_iter_loss: 0.1638917624950409
train_iter_loss: 0.11527184396982193
train_iter_loss: 0.21180079877376556
train_iter_loss: 0.16128170490264893
train_iter_loss: 0.2610104978084564
train_iter_loss: 0.1419849991798401
train_iter_loss: 0.3002443015575409
train_iter_loss: 0.3648291826248169
train_iter_loss: 0.282137930393219
train_iter_loss: 0.16555359959602356
train_iter_loss: 0.19712091982364655
train_iter_loss: 0.21286718547344208
train_iter_loss: 0.2207726538181305
train_iter_loss: 0.2952108085155487
train_iter_loss: 0.31164464354515076
train_iter_loss: 0.22484926879405975
train_iter_loss: 0.31284070014953613
train_iter_loss: 0.3058816194534302
train_iter_loss: 0.18951763212680817
train_iter_loss: 0.1842195987701416
train_iter_loss: 0.35469508171081543
train_iter_loss: 0.3277025520801544
train_iter_loss: 0.27670788764953613
train_iter_loss: 0.23002764582633972
train_iter_loss: 0.18199783563613892
train_iter_loss: 0.181846022605896
train_iter_loss: 0.19034068286418915
train_iter_loss: 0.2330857366323471
train_iter_loss: 0.3154636323451996
train_iter_loss: 0.3720720410346985
train_iter_loss: 0.4914324879646301
train_iter_loss: 0.24600347876548767
train_iter_loss: 0.39109307527542114
train_iter_loss: 0.2552277445793152
train_iter_loss: 0.29215580224990845
train_iter_loss: 0.3690855801105499
train_iter_loss: 0.19044561684131622
train_iter_loss: 0.36149728298187256
train_iter_loss: 0.22868052124977112
train_iter_loss: 0.14444077014923096
train_iter_loss: 0.19789278507232666
train_iter_loss: 0.21462486684322357
train_iter_loss: 0.35335370898246765
train_iter_loss: 0.21855026483535767
train_iter_loss: 0.31203415989875793
train_iter_loss: 0.12625329196453094
train_iter_loss: 0.27296772599220276
train_iter_loss: 0.2234862744808197
train_iter_loss: 0.21922536194324493
train_iter_loss: 0.23887403309345245
train_iter_loss: 0.16329394280910492
train_iter_loss: 0.2953678369522095
train_iter_loss: 0.33816972374916077
train loss :0.2629
---------------------
Validation seg loss: 0.34800921016337594 at epoch 754
epoch =    755/  1000, exp = train
train_iter_loss: 0.2259930670261383
train_iter_loss: 0.21221788227558136
train_iter_loss: 0.1720665693283081
train_iter_loss: 0.3004390597343445
train_iter_loss: 0.21704895794391632
train_iter_loss: 0.3094945549964905
train_iter_loss: 0.22649936378002167
train_iter_loss: 0.2216671258211136
train_iter_loss: 0.30765730142593384
train_iter_loss: 0.3287539482116699
train_iter_loss: 0.3018455505371094
train_iter_loss: 0.2259979397058487
train_iter_loss: 0.41412755846977234
train_iter_loss: 0.20146967470645905
train_iter_loss: 0.25565236806869507
train_iter_loss: 0.1698889285326004
train_iter_loss: 0.3336842954158783
train_iter_loss: 0.2003995031118393
train_iter_loss: 0.12355276197195053
train_iter_loss: 0.16981294751167297
train_iter_loss: 0.28332990407943726
train_iter_loss: 0.2628617286682129
train_iter_loss: 0.36402449011802673
train_iter_loss: 0.1966090351343155
train_iter_loss: 0.18070350587368011
train_iter_loss: 0.24380716681480408
train_iter_loss: 0.1095036119222641
train_iter_loss: 0.2866193652153015
train_iter_loss: 0.1877947300672531
train_iter_loss: 0.26317667961120605
train_iter_loss: 0.368500679731369
train_iter_loss: 0.3031960725784302
train_iter_loss: 0.3272383511066437
train_iter_loss: 0.34875473380088806
train_iter_loss: 0.303681880235672
train_iter_loss: 0.21124623715877533
train_iter_loss: 0.303867369890213
train_iter_loss: 0.23387016355991364
train_iter_loss: 0.04219658672809601
train_iter_loss: 0.3159226179122925
train_iter_loss: 0.140708789229393
train_iter_loss: 0.19782237708568573
train_iter_loss: 0.18156252801418304
train_iter_loss: 0.13761863112449646
train_iter_loss: 0.2494097799062729
train_iter_loss: 0.26166462898254395
train_iter_loss: 0.22797426581382751
train_iter_loss: 0.2249300628900528
train_iter_loss: 0.24367213249206543
train_iter_loss: 0.3678564727306366
train_iter_loss: 0.20987485349178314
train_iter_loss: 0.25135472416877747
train_iter_loss: 0.16861967742443085
train_iter_loss: 0.31234100461006165
train_iter_loss: 0.26365891098976135
train_iter_loss: 0.22265565395355225
train_iter_loss: 0.3856128752231598
train_iter_loss: 0.09124774485826492
train_iter_loss: 0.10413594543933868
train_iter_loss: 0.251682311296463
train_iter_loss: 0.3024251163005829
train_iter_loss: 0.2724372148513794
train_iter_loss: 0.31779494881629944
train_iter_loss: 0.40703728795051575
train_iter_loss: 0.24297606945037842
train_iter_loss: 0.06812915205955505
train_iter_loss: 0.21819299459457397
train_iter_loss: 0.3943658173084259
train_iter_loss: 0.524562656879425
train_iter_loss: 0.19041618704795837
train_iter_loss: 0.5701647996902466
train_iter_loss: 0.21117016673088074
train_iter_loss: 0.38376206159591675
train_iter_loss: 0.49761322140693665
train_iter_loss: 0.28751131892204285
train_iter_loss: 0.2894039750099182
train_iter_loss: 0.27859845757484436
train_iter_loss: 0.19021211564540863
train_iter_loss: 0.33931034803390503
train_iter_loss: 0.3173829913139343
train_iter_loss: 0.3214617967605591
train_iter_loss: 0.11262211203575134
train_iter_loss: 0.40194958448410034
train_iter_loss: 0.269580215215683
train_iter_loss: 0.24912938475608826
train_iter_loss: 0.38834714889526367
train_iter_loss: 0.24237601459026337
train_iter_loss: 0.39237481355667114
train_iter_loss: 0.20546311140060425
train_iter_loss: 0.26638534665107727
train_iter_loss: 0.1445484161376953
train_iter_loss: 0.31564435362815857
train_iter_loss: 0.12606531381607056
train_iter_loss: 0.2277391403913498
train_iter_loss: 0.28251543641090393
train_iter_loss: 0.22181981801986694
train_iter_loss: 0.26766103506088257
train_iter_loss: 0.3527103364467621
train_iter_loss: 0.19474366307258606
train_iter_loss: 0.3206922709941864
train loss :0.2642
---------------------
Validation seg loss: 0.34937613462273664 at epoch 755
epoch =    756/  1000, exp = train
train_iter_loss: 0.22307981550693512
train_iter_loss: 0.22783172130584717
train_iter_loss: 0.2936934530735016
train_iter_loss: 0.15315623581409454
train_iter_loss: 0.43766170740127563
train_iter_loss: 0.15196943283081055
train_iter_loss: 0.2996116578578949
train_iter_loss: 0.2601938545703888
train_iter_loss: 0.30923306941986084
train_iter_loss: 0.24323534965515137
train_iter_loss: 0.11818517744541168
train_iter_loss: 0.2664112150669098
train_iter_loss: 0.16067180037498474
train_iter_loss: 0.37369751930236816
train_iter_loss: 0.18836691975593567
train_iter_loss: 0.20009443163871765
train_iter_loss: 0.26673033833503723
train_iter_loss: 0.19217833876609802
train_iter_loss: 0.2085641771554947
train_iter_loss: 0.17975562810897827
train_iter_loss: 0.1773787885904312
train_iter_loss: 0.22286592423915863
train_iter_loss: 0.1819230020046234
train_iter_loss: 0.3894675672054291
train_iter_loss: 0.2358955442905426
train_iter_loss: 0.222623810172081
train_iter_loss: 0.24188287556171417
train_iter_loss: 0.2299289107322693
train_iter_loss: 0.3729332387447357
train_iter_loss: 0.34783414006233215
train_iter_loss: 0.04871510714292526
train_iter_loss: 0.3523768186569214
train_iter_loss: 0.18680721521377563
train_iter_loss: 0.260532021522522
train_iter_loss: 0.22229933738708496
train_iter_loss: 0.3970193862915039
train_iter_loss: 0.3111981451511383
train_iter_loss: 0.0690528005361557
train_iter_loss: 0.3493492901325226
train_iter_loss: 0.12109816074371338
train_iter_loss: 0.23894453048706055
train_iter_loss: 0.21497485041618347
train_iter_loss: 0.1964714527130127
train_iter_loss: 0.33926013112068176
train_iter_loss: 0.31964996457099915
train_iter_loss: 0.3477492034435272
train_iter_loss: 0.19512929022312164
train_iter_loss: 0.24399837851524353
train_iter_loss: 0.2372451275587082
train_iter_loss: 0.15814894437789917
train_iter_loss: 0.1892962008714676
train_iter_loss: 0.18467289209365845
train_iter_loss: 0.13868393003940582
train_iter_loss: 0.3552324175834656
train_iter_loss: 0.3390287756919861
train_iter_loss: 0.35537073016166687
train_iter_loss: 0.23385637998580933
train_iter_loss: 0.18913643062114716
train_iter_loss: 0.3444257378578186
train_iter_loss: 0.22417181730270386
train_iter_loss: 0.30046480894088745
train_iter_loss: 0.2564265727996826
train_iter_loss: 0.24441950023174286
train_iter_loss: 0.43907666206359863
train_iter_loss: 0.3151843845844269
train_iter_loss: 0.32450613379478455
train_iter_loss: 0.40196695923805237
train_iter_loss: 0.12572117149829865
train_iter_loss: 0.23263594508171082
train_iter_loss: 0.30545637011528015
train_iter_loss: 0.24401991069316864
train_iter_loss: 0.3677453398704529
train_iter_loss: 0.33879074454307556
train_iter_loss: 0.28063440322875977
train_iter_loss: 0.3121538460254669
train_iter_loss: 0.36377575993537903
train_iter_loss: 0.16526541113853455
train_iter_loss: 0.17649690806865692
train_iter_loss: 0.36441347002983093
train_iter_loss: 0.13210546970367432
train_iter_loss: 0.21766826510429382
train_iter_loss: 0.21354305744171143
train_iter_loss: 0.41711753606796265
train_iter_loss: 0.15496614575386047
train_iter_loss: 0.24275349080562592
train_iter_loss: 0.16773293912410736
train_iter_loss: 0.18644435703754425
train_iter_loss: 0.29191434383392334
train_iter_loss: 0.3370744585990906
train_iter_loss: 0.37123024463653564
train_iter_loss: 0.4301891326904297
train_iter_loss: 0.2811920940876007
train_iter_loss: 0.16486646234989166
train_iter_loss: 0.34152284264564514
train_iter_loss: 0.27458810806274414
train_iter_loss: 0.22974617779254913
train_iter_loss: 0.31816259026527405
train_iter_loss: 0.2721099853515625
train_iter_loss: 0.3308989405632019
train_iter_loss: 0.30799755454063416
train loss :0.2625
---------------------
Validation seg loss: 0.3635970638441098 at epoch 756
epoch =    757/  1000, exp = train
train_iter_loss: 0.42582350969314575
train_iter_loss: 0.21706824004650116
train_iter_loss: 0.27587950229644775
train_iter_loss: 0.39564085006713867
train_iter_loss: 0.18025824427604675
train_iter_loss: 0.2356519103050232
train_iter_loss: 0.1160077229142189
train_iter_loss: 0.46148189902305603
train_iter_loss: 0.32520726323127747
train_iter_loss: 0.08253131806850433
train_iter_loss: 0.19559639692306519
train_iter_loss: 0.19263699650764465
train_iter_loss: 0.39736804366111755
train_iter_loss: 0.2852655053138733
train_iter_loss: 0.5582075119018555
train_iter_loss: 0.3949207663536072
train_iter_loss: 0.18504974246025085
train_iter_loss: 0.21465903520584106
train_iter_loss: 0.24987778067588806
train_iter_loss: 0.2854190766811371
train_iter_loss: 0.354885458946228
train_iter_loss: 0.2723759114742279
train_iter_loss: 0.15739931166172028
train_iter_loss: 0.2539866864681244
train_iter_loss: 0.21395668387413025
train_iter_loss: 0.12283894419670105
train_iter_loss: 0.22470234334468842
train_iter_loss: 0.343237042427063
train_iter_loss: 0.24661456048488617
train_iter_loss: 0.13521699607372284
train_iter_loss: 0.4111471176147461
train_iter_loss: 0.21984708309173584
train_iter_loss: 0.20944739878177643
train_iter_loss: 0.229486346244812
train_iter_loss: 0.18097543716430664
train_iter_loss: 0.4811733365058899
train_iter_loss: 0.3398274779319763
train_iter_loss: 0.2634006440639496
train_iter_loss: 0.34081727266311646
train_iter_loss: 0.24923014640808105
train_iter_loss: 0.4010477662086487
train_iter_loss: 0.3897170424461365
train_iter_loss: 0.26764675974845886
train_iter_loss: 0.24353134632110596
train_iter_loss: 0.4451194405555725
train_iter_loss: 0.17675082385540009
train_iter_loss: 0.37873733043670654
train_iter_loss: 0.18505322933197021
train_iter_loss: 0.2837235927581787
train_iter_loss: 0.18267877399921417
train_iter_loss: 0.3091084063053131
train_iter_loss: 0.23331187665462494
train_iter_loss: 0.23176075518131256
train_iter_loss: 0.24865029752254486
train_iter_loss: 0.18898850679397583
train_iter_loss: 0.46406203508377075
train_iter_loss: 0.2089204043149948
train_iter_loss: 0.16349801421165466
train_iter_loss: 0.166179820895195
train_iter_loss: 0.2585010826587677
train_iter_loss: 0.27538323402404785
train_iter_loss: 0.32847535610198975
train_iter_loss: 0.1853836178779602
train_iter_loss: 0.2803356945514679
train_iter_loss: 0.2592443823814392
train_iter_loss: 0.3988415002822876
train_iter_loss: 0.3239253759384155
train_iter_loss: 0.26155105233192444
train_iter_loss: 0.2690580189228058
train_iter_loss: 0.16932865977287292
train_iter_loss: 0.24410516023635864
train_iter_loss: 0.18921349942684174
train_iter_loss: 0.1418217122554779
train_iter_loss: 0.34010225534439087
train_iter_loss: 0.5092273950576782
train_iter_loss: 0.12728160619735718
train_iter_loss: 0.28918036818504333
train_iter_loss: 0.14227738976478577
train_iter_loss: 0.1739044040441513
train_iter_loss: 0.27143269777297974
train_iter_loss: 0.27183303236961365
train_iter_loss: 0.2916490435600281
train_iter_loss: 0.2727286219596863
train_iter_loss: 0.26251280307769775
train_iter_loss: 0.27344217896461487
train_iter_loss: 0.2035035490989685
train_iter_loss: 0.4385552704334259
train_iter_loss: 0.13779357075691223
train_iter_loss: 0.2526133954524994
train_iter_loss: 0.22441720962524414
train_iter_loss: 0.11685622483491898
train_iter_loss: 0.4330955743789673
train_iter_loss: 0.31600943207740784
train_iter_loss: 0.15115907788276672
train_iter_loss: 0.21990831196308136
train_iter_loss: 0.249065101146698
train_iter_loss: 0.38110455870628357
train_iter_loss: 0.39457571506500244
train_iter_loss: 0.13089269399642944
train_iter_loss: 0.3042285442352295
train loss :0.2712
---------------------
Validation seg loss: 0.3634142682641604 at epoch 757
epoch =    758/  1000, exp = train
train_iter_loss: 0.21520912647247314
train_iter_loss: 0.2473447173833847
train_iter_loss: 0.3546256721019745
train_iter_loss: 0.32058724761009216
train_iter_loss: 0.4433567225933075
train_iter_loss: 0.17264769971370697
train_iter_loss: 0.11251100897789001
train_iter_loss: 0.19128938019275665
train_iter_loss: 0.3507072627544403
train_iter_loss: 0.29612186551094055
train_iter_loss: 0.17262707650661469
train_iter_loss: 0.22751674056053162
train_iter_loss: 0.20480979979038239
train_iter_loss: 0.08482467383146286
train_iter_loss: 0.27426016330718994
train_iter_loss: 0.16888460516929626
train_iter_loss: 0.20469054579734802
train_iter_loss: 0.35094940662384033
train_iter_loss: 0.32059651613235474
train_iter_loss: 0.3460460603237152
train_iter_loss: 0.3253976106643677
train_iter_loss: 0.21844592690467834
train_iter_loss: 0.1954604834318161
train_iter_loss: 0.3826431334018707
train_iter_loss: 0.13070295751094818
train_iter_loss: 0.3396066427230835
train_iter_loss: 0.32552558183670044
train_iter_loss: 0.2556058466434479
train_iter_loss: 0.36912885308265686
train_iter_loss: 0.33343493938446045
train_iter_loss: 0.1456567496061325
train_iter_loss: 0.07102030515670776
train_iter_loss: 0.2278960794210434
train_iter_loss: 0.23997287452220917
train_iter_loss: 0.2229042947292328
train_iter_loss: 0.14680977165699005
train_iter_loss: 0.2759566903114319
train_iter_loss: 0.29761722683906555
train_iter_loss: 0.23354825377464294
train_iter_loss: 0.33030104637145996
train_iter_loss: 0.31413447856903076
train_iter_loss: 0.2914365231990814
train_iter_loss: 0.16506452858448029
train_iter_loss: 0.17613883316516876
train_iter_loss: 0.1777670532464981
train_iter_loss: 0.17871394753456116
train_iter_loss: 0.2019578218460083
train_iter_loss: 0.26190605759620667
train_iter_loss: 0.3290453553199768
train_iter_loss: 0.37197014689445496
train_iter_loss: 0.2703627049922943
train_iter_loss: 0.2965139150619507
train_iter_loss: 0.39922186732292175
train_iter_loss: 0.2591801881790161
train_iter_loss: 0.11256326735019684
train_iter_loss: 0.3239467740058899
train_iter_loss: 0.20124337077140808
train_iter_loss: 0.32661008834838867
train_iter_loss: 0.23028796911239624
train_iter_loss: 0.200852632522583
train_iter_loss: 0.538311243057251
train_iter_loss: 0.20749543607234955
train_iter_loss: 0.3895902931690216
train_iter_loss: 0.4389781355857849
train_iter_loss: 0.2235504686832428
train_iter_loss: 0.18536800146102905
train_iter_loss: 0.19375155866146088
train_iter_loss: 0.2723813056945801
train_iter_loss: 0.297653466463089
train_iter_loss: 0.3577309250831604
train_iter_loss: 0.10113010555505753
train_iter_loss: 0.3149271607398987
train_iter_loss: 0.5620861053466797
train_iter_loss: 0.19036690890789032
train_iter_loss: 0.29158496856689453
train_iter_loss: 0.11446646600961685
train_iter_loss: 0.17767281830310822
train_iter_loss: 0.17237015068531036
train_iter_loss: 0.27889665961265564
train_iter_loss: 0.31847020983695984
train_iter_loss: 0.35602810978889465
train_iter_loss: 0.21411225199699402
train_iter_loss: 0.41977137327194214
train_iter_loss: 0.21087191998958588
train_iter_loss: 0.23756860196590424
train_iter_loss: 0.28168272972106934
train_iter_loss: 0.27463218569755554
train_iter_loss: 0.2730254530906677
train_iter_loss: 0.3076814115047455
train_iter_loss: 0.43822553753852844
train_iter_loss: 0.12055611610412598
train_iter_loss: 0.21277564764022827
train_iter_loss: 0.3379836082458496
train_iter_loss: 0.22460319101810455
train_iter_loss: 0.12206338346004486
train_iter_loss: 0.29883360862731934
train_iter_loss: 0.21918077766895294
train_iter_loss: 0.28520265221595764
train_iter_loss: 0.35654646158218384
train_iter_loss: 0.0801587700843811
train loss :0.2639
---------------------
Validation seg loss: 0.35328025995085965 at epoch 758
epoch =    759/  1000, exp = train
train_iter_loss: 0.13251259922981262
train_iter_loss: 0.24750520288944244
train_iter_loss: 0.4420502781867981
train_iter_loss: 0.17968405783176422
train_iter_loss: 0.22793538868427277
train_iter_loss: 0.26155292987823486
train_iter_loss: 0.5996224880218506
train_iter_loss: 0.2876633107662201
train_iter_loss: 0.2386251837015152
train_iter_loss: 0.18315455317497253
train_iter_loss: 0.30027204751968384
train_iter_loss: 0.2689001262187958
train_iter_loss: 0.2974993586540222
train_iter_loss: 0.3351403474807739
train_iter_loss: 0.20630411803722382
train_iter_loss: 0.26084065437316895
train_iter_loss: 0.35243290662765503
train_iter_loss: 0.2768799066543579
train_iter_loss: 0.0629749670624733
train_iter_loss: 0.2342299073934555
train_iter_loss: 0.33019623160362244
train_iter_loss: 0.22690542042255402
train_iter_loss: 0.1032414436340332
train_iter_loss: 0.3256937265396118
train_iter_loss: 0.2813633680343628
train_iter_loss: 0.15162822604179382
train_iter_loss: 0.16684238612651825
train_iter_loss: 0.5067004561424255
train_iter_loss: 0.15748710930347443
train_iter_loss: 0.2923799753189087
train_iter_loss: 0.08510449528694153
train_iter_loss: 0.25157588720321655
train_iter_loss: 0.4557075500488281
train_iter_loss: 0.20306532084941864
train_iter_loss: 0.2158445119857788
train_iter_loss: 0.30835068225860596
train_iter_loss: 0.15691949427127838
train_iter_loss: 0.2879853844642639
train_iter_loss: 0.34264352917671204
train_iter_loss: 0.24744881689548492
train_iter_loss: 0.3035319745540619
train_iter_loss: 0.3731459975242615
train_iter_loss: 0.311890184879303
train_iter_loss: 0.1280738115310669
train_iter_loss: 0.20319965481758118
train_iter_loss: 0.28763583302497864
train_iter_loss: 0.16414731740951538
train_iter_loss: 0.24869289994239807
train_iter_loss: 0.3022996783256531
train_iter_loss: 0.1833743304014206
train_iter_loss: 0.16218145191669464
train_iter_loss: 0.38586705923080444
train_iter_loss: 0.25744396448135376
train_iter_loss: 0.32379618287086487
train_iter_loss: 0.08258611708879471
train_iter_loss: 0.0982883870601654
train_iter_loss: 0.33181530237197876
train_iter_loss: 0.26411402225494385
train_iter_loss: 0.23470774292945862
train_iter_loss: 0.29676827788352966
train_iter_loss: 0.33266928791999817
train_iter_loss: 0.1854255497455597
train_iter_loss: 0.19587363302707672
train_iter_loss: 0.3942345082759857
train_iter_loss: 0.16326086223125458
train_iter_loss: 0.3106062412261963
train_iter_loss: 0.08076338469982147
train_iter_loss: 0.3380526304244995
train_iter_loss: 0.3935360908508301
train_iter_loss: 0.1965978443622589
train_iter_loss: 0.2747317850589752
train_iter_loss: 0.3362281322479248
train_iter_loss: 0.24516229331493378
train_iter_loss: 0.20938166975975037
train_iter_loss: 0.3038488030433655
train_iter_loss: 0.29809194803237915
train_iter_loss: 0.1240568682551384
train_iter_loss: 0.2952612638473511
train_iter_loss: 0.2880016565322876
train_iter_loss: 0.2536570429801941
train_iter_loss: 0.2786386013031006
train_iter_loss: 0.33011019229888916
train_iter_loss: 0.29631760716438293
train_iter_loss: 0.21174179017543793
train_iter_loss: 0.1875545084476471
train_iter_loss: 0.42153337597846985
train_iter_loss: 0.34733086824417114
train_iter_loss: 0.39643150568008423
train_iter_loss: 0.24121218919754028
train_iter_loss: 0.3639804422855377
train_iter_loss: 0.26781365275382996
train_iter_loss: 0.22613407671451569
train_iter_loss: 0.17827776074409485
train_iter_loss: 0.19942361116409302
train_iter_loss: 0.26619046926498413
train_iter_loss: 0.260680228471756
train_iter_loss: 0.1524619311094284
train_iter_loss: 0.3406631052494049
train_iter_loss: 0.22412896156311035
train_iter_loss: 0.15823203325271606
train loss :0.2627
---------------------
Validation seg loss: 0.3458925297149931 at epoch 759
epoch =    760/  1000, exp = train
train_iter_loss: 0.16888201236724854
train_iter_loss: 0.24711407721042633
train_iter_loss: 0.24353869259357452
train_iter_loss: 0.28250205516815186
train_iter_loss: 0.18899385631084442
train_iter_loss: 0.3017411231994629
train_iter_loss: 0.21808937191963196
train_iter_loss: 0.23685500025749207
train_iter_loss: 0.31209596991539
train_iter_loss: 0.2750941216945648
train_iter_loss: 0.1067889854311943
train_iter_loss: 0.2888597548007965
train_iter_loss: 0.3887631297111511
train_iter_loss: 0.27589622139930725
train_iter_loss: 0.2525447607040405
train_iter_loss: 0.306549072265625
train_iter_loss: 0.27705010771751404
train_iter_loss: 0.30369576811790466
train_iter_loss: 0.3335561156272888
train_iter_loss: 0.28712597489356995
train_iter_loss: 0.3829422891139984
train_iter_loss: 0.18848861753940582
train_iter_loss: 0.3046995997428894
train_iter_loss: 0.0947306826710701
train_iter_loss: 0.2761072814464569
train_iter_loss: 0.16985714435577393
train_iter_loss: 0.4184139370918274
train_iter_loss: 0.2535709738731384
train_iter_loss: 0.32492461800575256
train_iter_loss: 0.2934724688529968
train_iter_loss: 0.21076180040836334
train_iter_loss: 0.22149936854839325
train_iter_loss: 0.2256251573562622
train_iter_loss: 0.20892655849456787
train_iter_loss: 0.20766597986221313
train_iter_loss: 0.2986420691013336
train_iter_loss: 0.4070938527584076
train_iter_loss: 0.2731173038482666
train_iter_loss: 0.24833615124225616
train_iter_loss: 0.39207011461257935
train_iter_loss: 0.16521552205085754
train_iter_loss: 0.43586355447769165
train_iter_loss: 0.18261589109897614
train_iter_loss: 0.1862211972475052
train_iter_loss: 0.3429924249649048
train_iter_loss: 0.286490261554718
train_iter_loss: 0.2109505534172058
train_iter_loss: 0.25268906354904175
train_iter_loss: 0.44467899203300476
train_iter_loss: 0.107162706553936
train_iter_loss: 0.3298327326774597
train_iter_loss: 0.33280837535858154
train_iter_loss: 0.3511725962162018
train_iter_loss: 0.29614493250846863
train_iter_loss: 0.2135506570339203
train_iter_loss: 0.23782190680503845
train_iter_loss: 0.26506486535072327
train_iter_loss: 0.25698214769363403
train_iter_loss: 0.17007045447826385
train_iter_loss: 0.20515796542167664
train_iter_loss: 0.2457040697336197
train_iter_loss: 0.17710386216640472
train_iter_loss: 0.4560181200504303
train_iter_loss: 0.12823380529880524
train_iter_loss: 0.1310383528470993
train_iter_loss: 0.3377818167209625
train_iter_loss: 0.32911497354507446
train_iter_loss: 0.2107321172952652
train_iter_loss: 0.29183122515678406
train_iter_loss: 0.16173863410949707
train_iter_loss: 0.4966094195842743
train_iter_loss: 0.28993645310401917
train_iter_loss: 0.2719596326351166
train_iter_loss: 0.35124072432518005
train_iter_loss: 0.12132962793111801
train_iter_loss: 0.06231987848877907
train_iter_loss: 0.05977742001414299
train_iter_loss: 0.08516406267881393
train_iter_loss: 0.48222118616104126
train_iter_loss: 0.3477139472961426
train_iter_loss: 0.2874513864517212
train_iter_loss: 0.22595785558223724
train_iter_loss: 0.24369169771671295
train_iter_loss: 0.25642791390419006
train_iter_loss: 0.35853949189186096
train_iter_loss: 0.1964142918586731
train_iter_loss: 0.08170601725578308
train_iter_loss: 0.3153734803199768
train_iter_loss: 0.05549619719386101
train_iter_loss: 0.1581580936908722
train_iter_loss: 0.17909125983715057
train_iter_loss: 0.34754854440689087
train_iter_loss: 0.19254496693611145
train_iter_loss: 0.1471661776304245
train_iter_loss: 0.35267406702041626
train_iter_loss: 0.36724019050598145
train_iter_loss: 0.17604701220989227
train_iter_loss: 0.26646968722343445
train_iter_loss: 0.19902288913726807
train_iter_loss: 0.283171683549881
train loss :0.2596
---------------------
Validation seg loss: 0.3828446709348838 at epoch 760
epoch =    761/  1000, exp = train
train_iter_loss: 0.3339124917984009
train_iter_loss: 0.193613663315773
train_iter_loss: 0.190042644739151
train_iter_loss: 0.24666035175323486
train_iter_loss: 0.3389846980571747
train_iter_loss: 0.1571725755929947
train_iter_loss: 0.2118164598941803
train_iter_loss: 0.3237147927284241
train_iter_loss: 0.27700453996658325
train_iter_loss: 0.3209325671195984
train_iter_loss: 0.2859480679035187
train_iter_loss: 0.175344780087471
train_iter_loss: 0.19517117738723755
train_iter_loss: 0.15177075564861298
train_iter_loss: 0.24491295218467712
train_iter_loss: 0.26966044306755066
train_iter_loss: 0.2093134969472885
train_iter_loss: 0.3665458858013153
train_iter_loss: 0.3637455999851227
train_iter_loss: 0.2836688756942749
train_iter_loss: 0.09312541037797928
train_iter_loss: 0.2556972801685333
train_iter_loss: 0.32291659712791443
train_iter_loss: 0.3367191553115845
train_iter_loss: 0.4409032166004181
train_iter_loss: 0.12365512549877167
train_iter_loss: 0.3462304174900055
train_iter_loss: 0.23462581634521484
train_iter_loss: 0.20621247589588165
train_iter_loss: 0.28193190693855286
train_iter_loss: 0.12499605119228363
train_iter_loss: 0.34168657660484314
train_iter_loss: 0.15722036361694336
train_iter_loss: 0.16791829466819763
train_iter_loss: 0.2532784342765808
train_iter_loss: 0.29403984546661377
train_iter_loss: 0.34287387132644653
train_iter_loss: 0.024171583354473114
train_iter_loss: 0.28764134645462036
train_iter_loss: 0.22502002120018005
train_iter_loss: 0.09844697266817093
train_iter_loss: 0.2901637554168701
train_iter_loss: 0.12278188019990921
train_iter_loss: 0.4734462797641754
train_iter_loss: 0.1578626185655594
train_iter_loss: 0.2594050168991089
train_iter_loss: 0.11135659366846085
train_iter_loss: 0.29093578457832336
train_iter_loss: 0.19030627608299255
train_iter_loss: 0.3212878704071045
train_iter_loss: 0.3941226899623871
train_iter_loss: 0.23241673409938812
train_iter_loss: 0.24738822877407074
train_iter_loss: 0.5762487649917603
train_iter_loss: 0.31341004371643066
train_iter_loss: 0.2874349057674408
train_iter_loss: 0.2962590157985687
train_iter_loss: 0.28204450011253357
train_iter_loss: 0.20414240658283234
train_iter_loss: 0.2574180066585541
train_iter_loss: 0.3616161048412323
train_iter_loss: 0.2531229257583618
train_iter_loss: 0.2671703100204468
train_iter_loss: 0.08141434192657471
train_iter_loss: 0.20616646111011505
train_iter_loss: 0.2467428296804428
train_iter_loss: 0.22474662959575653
train_iter_loss: 0.21893900632858276
train_iter_loss: 0.3866015374660492
train_iter_loss: 0.2475300431251526
train_iter_loss: 0.22650496661663055
train_iter_loss: 0.37735262513160706
train_iter_loss: 0.09756628423929214
train_iter_loss: 0.25989842414855957
train_iter_loss: 0.35541120171546936
train_iter_loss: 0.39742523431777954
train_iter_loss: 0.02358393371105194
train_iter_loss: 0.35943907499313354
train_iter_loss: 0.3500746190547943
train_iter_loss: 0.4734461009502411
train_iter_loss: 0.37323373556137085
train_iter_loss: 0.29061567783355713
train_iter_loss: 0.28665927052497864
train_iter_loss: 0.3546806871891022
train_iter_loss: 0.10649968683719635
train_iter_loss: 0.26232102513313293
train_iter_loss: 0.2028246968984604
train_iter_loss: 0.2900419235229492
train_iter_loss: 0.39951851963996887
train_iter_loss: 0.23592975735664368
train_iter_loss: 0.31449100375175476
train_iter_loss: 0.20509664714336395
train_iter_loss: 0.08263563364744186
train_iter_loss: 0.17457197606563568
train_iter_loss: 0.29383569955825806
train_iter_loss: 0.37633517384529114
train_iter_loss: 0.3780559301376343
train_iter_loss: 0.32794010639190674
train_iter_loss: 0.3800698518753052
train_iter_loss: 0.21755927801132202
train loss :0.2663
---------------------
Validation seg loss: 0.363517683210238 at epoch 761
epoch =    762/  1000, exp = train
train_iter_loss: 0.18217188119888306
train_iter_loss: 0.2601275146007538
train_iter_loss: 0.3980036973953247
train_iter_loss: 0.1643725484609604
train_iter_loss: 0.4032602310180664
train_iter_loss: 0.20266865193843842
train_iter_loss: 0.22516675293445587
train_iter_loss: 0.2984606921672821
train_iter_loss: 0.22688980400562286
train_iter_loss: 0.2703188955783844
train_iter_loss: 0.19546397030353546
train_iter_loss: 0.27252820134162903
train_iter_loss: 0.26157045364379883
train_iter_loss: 0.2498864382505417
train_iter_loss: 0.21344777941703796
train_iter_loss: 0.3166502118110657
train_iter_loss: 0.33927345275878906
train_iter_loss: 0.20387451350688934
train_iter_loss: 0.29014697670936584
train_iter_loss: 0.16208533942699432
train_iter_loss: 0.3258252739906311
train_iter_loss: 0.20893403887748718
train_iter_loss: 0.3308289349079132
train_iter_loss: 0.2943040430545807
train_iter_loss: 0.2996467351913452
train_iter_loss: 0.27364200353622437
train_iter_loss: 0.28742489218711853
train_iter_loss: 0.13754059374332428
train_iter_loss: 0.5117732882499695
train_iter_loss: 0.20086827874183655
train_iter_loss: 0.2556728720664978
train_iter_loss: 0.3353399336338043
train_iter_loss: 0.2082468718290329
train_iter_loss: 0.3888077139854431
train_iter_loss: 0.2246810644865036
train_iter_loss: 0.26680782437324524
train_iter_loss: 0.20546825230121613
train_iter_loss: 0.3001438081264496
train_iter_loss: 0.3568664491176605
train_iter_loss: 0.19557684659957886
train_iter_loss: 0.3900676667690277
train_iter_loss: 0.13135544955730438
train_iter_loss: 0.29908236861228943
train_iter_loss: 0.1807841956615448
train_iter_loss: 0.12616337835788727
train_iter_loss: 0.23661528527736664
train_iter_loss: 0.2601069509983063
train_iter_loss: 0.1622946858406067
train_iter_loss: 0.2135985940694809
train_iter_loss: 0.18772973120212555
train_iter_loss: 0.1940641552209854
train_iter_loss: 0.29185473918914795
train_iter_loss: 0.26811376214027405
train_iter_loss: 0.20649860799312592
train_iter_loss: 0.3936125338077545
train_iter_loss: 0.18149229884147644
train_iter_loss: 0.08587510138750076
train_iter_loss: 0.33182576298713684
train_iter_loss: 0.13136760890483856
train_iter_loss: 0.2247689962387085
train_iter_loss: 0.40553832054138184
train_iter_loss: 0.36104726791381836
train_iter_loss: 0.43065235018730164
train_iter_loss: 0.3347488045692444
train_iter_loss: 0.20524227619171143
train_iter_loss: 0.20136083662509918
train_iter_loss: 0.30260297656059265
train_iter_loss: 0.17630508542060852
train_iter_loss: 0.25186607241630554
train_iter_loss: 0.301682710647583
train_iter_loss: 0.1520327925682068
train_iter_loss: 0.36473333835601807
train_iter_loss: 0.4527524411678314
train_iter_loss: 0.25807318091392517
train_iter_loss: 0.2317306101322174
train_iter_loss: 0.10284256190061569
train_iter_loss: 0.18682646751403809
train_iter_loss: 0.26080700755119324
train_iter_loss: 0.3505191206932068
train_iter_loss: 0.26443055272102356
train_iter_loss: 0.25653964281082153
train_iter_loss: 0.29684486985206604
train_iter_loss: 0.16243939101696014
train_iter_loss: 0.40625375509262085
train_iter_loss: 0.27275311946868896
train_iter_loss: 0.312009334564209
train_iter_loss: 0.38058337569236755
train_iter_loss: 0.2888534367084503
train_iter_loss: 0.2584441602230072
train_iter_loss: 0.23898424208164215
train_iter_loss: 0.2483135312795639
train_iter_loss: 0.33195066452026367
train_iter_loss: 0.26528552174568176
train_iter_loss: 0.3173743784427643
train_iter_loss: 0.28132209181785583
train_iter_loss: 0.3102910816669464
train_iter_loss: 0.30486005544662476
train_iter_loss: 0.30463486909866333
train_iter_loss: 0.28786978125572205
train_iter_loss: 0.2586088478565216
train loss :0.2689
---------------------
Validation seg loss: 0.3518174751222415 at epoch 762
epoch =    763/  1000, exp = train
train_iter_loss: 0.32074761390686035
train_iter_loss: 0.27255865931510925
train_iter_loss: 0.23294074833393097
train_iter_loss: 0.14766228199005127
train_iter_loss: 0.17735768854618073
train_iter_loss: 0.3113178610801697
train_iter_loss: 0.27684351801872253
train_iter_loss: 0.1691620796918869
train_iter_loss: 0.1690329760313034
train_iter_loss: 0.3027797043323517
train_iter_loss: 0.2431708127260208
train_iter_loss: 0.2549380660057068
train_iter_loss: 0.2966705858707428
train_iter_loss: 0.1647741049528122
train_iter_loss: 0.24671082198619843
train_iter_loss: 0.4404538571834564
train_iter_loss: 0.18105214834213257
train_iter_loss: 0.36104118824005127
train_iter_loss: 0.254058837890625
train_iter_loss: 0.32621005177497864
train_iter_loss: 0.40104103088378906
train_iter_loss: 0.17165829241275787
train_iter_loss: 0.16783881187438965
train_iter_loss: 0.20197793841362
train_iter_loss: 0.26692458987236023
train_iter_loss: 0.33516740798950195
train_iter_loss: 0.11019057035446167
train_iter_loss: 0.2280116230249405
train_iter_loss: 0.24303153157234192
train_iter_loss: 0.38537168502807617
train_iter_loss: 0.24310681223869324
train_iter_loss: 0.24195870757102966
train_iter_loss: 0.20636244118213654
train_iter_loss: 0.3122379779815674
train_iter_loss: 0.1995903104543686
train_iter_loss: 0.20182405412197113
train_iter_loss: 0.23291721940040588
train_iter_loss: 0.3114299774169922
train_iter_loss: 0.32820579409599304
train_iter_loss: 0.30602824687957764
train_iter_loss: 0.1869984120130539
train_iter_loss: 0.20121267437934875
train_iter_loss: 0.12813599407672882
train_iter_loss: 0.2261706292629242
train_iter_loss: 0.32135722041130066
train_iter_loss: 0.22403262555599213
train_iter_loss: 0.3145621120929718
train_iter_loss: 0.42594942450523376
train_iter_loss: 0.24267151951789856
train_iter_loss: 0.17594558000564575
train_iter_loss: 0.20227332413196564
train_iter_loss: 0.4146958589553833
train_iter_loss: 0.2719014286994934
train_iter_loss: 0.16480906307697296
train_iter_loss: 0.2823215425014496
train_iter_loss: 0.2895215153694153
train_iter_loss: 0.15110361576080322
train_iter_loss: 0.23640316724777222
train_iter_loss: 0.32836559414863586
train_iter_loss: 0.2271023541688919
train_iter_loss: 0.15533365309238434
train_iter_loss: 0.3339841365814209
train_iter_loss: 0.16639992594718933
train_iter_loss: 0.249542698264122
train_iter_loss: 0.21332518756389618
train_iter_loss: 0.14548896253108978
train_iter_loss: 0.21564723551273346
train_iter_loss: 0.23348639905452728
train_iter_loss: 0.26373663544654846
train_iter_loss: 0.16569145023822784
train_iter_loss: 0.31509608030319214
train_iter_loss: 0.29565125703811646
train_iter_loss: 0.3737848401069641
train_iter_loss: 0.303808331489563
train_iter_loss: 0.33190402388572693
train_iter_loss: 0.17540094256401062
train_iter_loss: 0.29403236508369446
train_iter_loss: 0.3661234676837921
train_iter_loss: 0.3397924602031708
train_iter_loss: 0.16804970800876617
train_iter_loss: 0.2847268581390381
train_iter_loss: 0.17419081926345825
train_iter_loss: 0.2825640141963959
train_iter_loss: 0.23205846548080444
train_iter_loss: 0.11422060430049896
train_iter_loss: 0.23438973724842072
train_iter_loss: 0.440108984708786
train_iter_loss: 0.2084600031375885
train_iter_loss: 0.3565759062767029
train_iter_loss: 0.22477911412715912
train_iter_loss: 0.29594922065734863
train_iter_loss: 0.4205746352672577
train_iter_loss: 0.18633496761322021
train_iter_loss: 0.4470617175102234
train_iter_loss: 0.3107835352420807
train_iter_loss: 0.2964269518852234
train_iter_loss: 0.2275906652212143
train_iter_loss: 0.0894569456577301
train_iter_loss: 0.20148904621601105
train_iter_loss: 0.35413941740989685
train loss :0.2602
---------------------
Validation seg loss: 0.3801276441575643 at epoch 763
epoch =    764/  1000, exp = train
train_iter_loss: 0.3398170471191406
train_iter_loss: 0.376702219247818
train_iter_loss: 0.28653737902641296
train_iter_loss: 0.09265749901533127
train_iter_loss: 0.36107301712036133
train_iter_loss: 0.30993619561195374
train_iter_loss: 0.251149982213974
train_iter_loss: 0.0874558761715889
train_iter_loss: 0.36419573426246643
train_iter_loss: 0.28416451811790466
train_iter_loss: 0.3240680396556854
train_iter_loss: 0.30891016125679016
train_iter_loss: 0.15748542547225952
train_iter_loss: 0.1698271483182907
train_iter_loss: 0.29388657212257385
train_iter_loss: 0.1350311040878296
train_iter_loss: 0.16026201844215393
train_iter_loss: 0.3011796772480011
train_iter_loss: 0.350252240896225
train_iter_loss: 0.21350018680095673
train_iter_loss: 0.2401866763830185
train_iter_loss: 0.36905404925346375
train_iter_loss: 0.2520669400691986
train_iter_loss: 0.20622383058071136
train_iter_loss: 0.06881792843341827
train_iter_loss: 0.19128352403640747
train_iter_loss: 0.35435134172439575
train_iter_loss: 0.09215258061885834
train_iter_loss: 0.2871018350124359
train_iter_loss: 0.2287425547838211
train_iter_loss: 0.1742769181728363
train_iter_loss: 0.18143920600414276
train_iter_loss: 0.4140210449695587
train_iter_loss: 0.5072433352470398
train_iter_loss: 0.12857024371623993
train_iter_loss: 0.26824474334716797
train_iter_loss: 0.27788448333740234
train_iter_loss: 0.21874171495437622
train_iter_loss: 0.2063562273979187
train_iter_loss: 0.23372621834278107
train_iter_loss: 0.14375649392604828
train_iter_loss: 0.19953453540802002
train_iter_loss: 0.2993297874927521
train_iter_loss: 0.12413351237773895
train_iter_loss: 0.2580700218677521
train_iter_loss: 0.033858466893434525
train_iter_loss: 0.13674381375312805
train_iter_loss: 0.1709914654493332
train_iter_loss: 0.26874515414237976
train_iter_loss: 0.16725637018680573
train_iter_loss: 0.2041182816028595
train_iter_loss: 0.24050100147724152
train_iter_loss: 0.276530921459198
train_iter_loss: 0.2914302349090576
train_iter_loss: 0.17234613001346588
train_iter_loss: 0.30834004282951355
train_iter_loss: 0.29375773668289185
train_iter_loss: 0.23763015866279602
train_iter_loss: 0.5596024990081787
train_iter_loss: 0.2737746238708496
train_iter_loss: 0.2003149539232254
train_iter_loss: 0.29623186588287354
train_iter_loss: 0.3489192724227905
train_iter_loss: 0.3235073387622833
train_iter_loss: 0.20184829831123352
train_iter_loss: 0.404177725315094
train_iter_loss: 0.1306619942188263
train_iter_loss: 0.2397942841053009
train_iter_loss: 0.377343088388443
train_iter_loss: 0.18295757472515106
train_iter_loss: 0.333037793636322
train_iter_loss: 0.32359954714775085
train_iter_loss: 0.18265408277511597
train_iter_loss: 0.3444587290287018
train_iter_loss: 0.34496161341667175
train_iter_loss: 0.26318955421447754
train_iter_loss: 0.2323591560125351
train_iter_loss: 0.24095647037029266
train_iter_loss: 0.08422372490167618
train_iter_loss: 0.20930220186710358
train_iter_loss: 0.3232923448085785
train_iter_loss: 0.20746169984340668
train_iter_loss: 0.40058469772338867
train_iter_loss: 0.25755617022514343
train_iter_loss: 0.2865700125694275
train_iter_loss: 0.2622579336166382
train_iter_loss: 0.3257438838481903
train_iter_loss: 0.09538078308105469
train_iter_loss: 0.182742640376091
train_iter_loss: 0.24920856952667236
train_iter_loss: 0.36313289403915405
train_iter_loss: 0.3046388626098633
train_iter_loss: 0.21619875729084015
train_iter_loss: 0.19795924425125122
train_iter_loss: 0.4402003586292267
train_iter_loss: 0.32971158623695374
train_iter_loss: 0.3637741208076477
train_iter_loss: 0.13132311403751373
train_iter_loss: 0.11713021993637085
train_iter_loss: 0.30237486958503723
train loss :0.2561
---------------------
Validation seg loss: 0.3827835316506197 at epoch 764
epoch =    765/  1000, exp = train
train_iter_loss: 0.24862068891525269
train_iter_loss: 0.1635597199201584
train_iter_loss: 0.0990777388215065
train_iter_loss: 0.3044987916946411
train_iter_loss: 0.44580796360969543
train_iter_loss: 0.49763163924217224
train_iter_loss: 0.12277811020612717
train_iter_loss: 0.21351683139801025
train_iter_loss: 0.39425143599510193
train_iter_loss: 0.19942103326320648
train_iter_loss: 0.1821170300245285
train_iter_loss: 0.2230590283870697
train_iter_loss: 0.1810351014137268
train_iter_loss: 0.37519311904907227
train_iter_loss: 0.3677109479904175
train_iter_loss: 0.32303279638290405
train_iter_loss: 0.19530074298381805
train_iter_loss: 0.2899458110332489
train_iter_loss: 0.22911259531974792
train_iter_loss: 0.24350371956825256
train_iter_loss: 0.3998035490512848
train_iter_loss: 0.1743956208229065
train_iter_loss: 0.30618202686309814
train_iter_loss: 0.19618640840053558
train_iter_loss: 0.2557131350040436
train_iter_loss: 0.22465094923973083
train_iter_loss: 0.37032678723335266
train_iter_loss: 0.20874156057834625
train_iter_loss: 0.29239314794540405
train_iter_loss: 0.17121383547782898
train_iter_loss: 0.26035791635513306
train_iter_loss: 0.3525536358356476
train_iter_loss: 0.25562772154808044
train_iter_loss: 0.3774886429309845
train_iter_loss: 0.2944718897342682
train_iter_loss: 0.25040119886398315
train_iter_loss: 0.4126475751399994
train_iter_loss: 0.2489784061908722
train_iter_loss: 0.15511518716812134
train_iter_loss: 0.09066101908683777
train_iter_loss: 0.27313196659088135
train_iter_loss: 0.2135883867740631
train_iter_loss: 0.2736521065235138
train_iter_loss: 0.13592101633548737
train_iter_loss: 0.2676783800125122
train_iter_loss: 0.38326767086982727
train_iter_loss: 0.1410079449415207
train_iter_loss: 0.4067954123020172
train_iter_loss: 0.21506305038928986
train_iter_loss: 0.3337448239326477
train_iter_loss: 0.2877868115901947
train_iter_loss: 0.23605813086032867
train_iter_loss: 0.22848115861415863
train_iter_loss: 0.2918761372566223
train_iter_loss: 0.21201004087924957
train_iter_loss: 0.321382611989975
train_iter_loss: 0.1110590249300003
train_iter_loss: 0.17916682362556458
train_iter_loss: 0.19607070088386536
train_iter_loss: 0.27722740173339844
train_iter_loss: 0.2685134708881378
train_iter_loss: 0.13045519590377808
train_iter_loss: 0.26359400153160095
train_iter_loss: 0.3005070090293884
train_iter_loss: 0.2246539145708084
train_iter_loss: 0.3813338279724121
train_iter_loss: 0.2178264707326889
train_iter_loss: 0.21052272617816925
train_iter_loss: 0.13135926425457
train_iter_loss: 0.3479653298854828
train_iter_loss: 0.2669970393180847
train_iter_loss: 0.2921879291534424
train_iter_loss: 0.17103222012519836
train_iter_loss: 0.30644580721855164
train_iter_loss: 0.24187368154525757
train_iter_loss: 0.2978972792625427
train_iter_loss: 0.3097895085811615
train_iter_loss: 0.33450332283973694
train_iter_loss: 0.3566584885120392
train_iter_loss: 0.16387729346752167
train_iter_loss: 0.11656329780817032
train_iter_loss: 0.29898497462272644
train_iter_loss: 0.1365421563386917
train_iter_loss: 0.24683789908885956
train_iter_loss: 0.3191338777542114
train_iter_loss: 0.3867185711860657
train_iter_loss: 0.29871687293052673
train_iter_loss: 0.2531394064426422
train_iter_loss: 0.1786223202943802
train_iter_loss: 0.29018571972846985
train_iter_loss: 0.37430456280708313
train_iter_loss: 0.3200388550758362
train_iter_loss: 0.3021494150161743
train_iter_loss: 0.26514872908592224
train_iter_loss: 0.27013397216796875
train_iter_loss: 0.23174166679382324
train_iter_loss: 0.2594562768936157
train_iter_loss: 0.14354448020458221
train_iter_loss: 0.29957738518714905
train_iter_loss: 0.3110272288322449
train loss :0.2636
---------------------
Validation seg loss: 0.3760815246891722 at epoch 765
epoch =    766/  1000, exp = train
train_iter_loss: 0.3202325105667114
train_iter_loss: 0.24326196312904358
train_iter_loss: 0.1776430755853653
train_iter_loss: 0.2481880635023117
train_iter_loss: 0.17405614256858826
train_iter_loss: 0.386205792427063
train_iter_loss: 0.3510640263557434
train_iter_loss: 0.1788628101348877
train_iter_loss: 0.27963733673095703
train_iter_loss: 0.24664853513240814
train_iter_loss: 0.25070157647132874
train_iter_loss: 0.3405638039112091
train_iter_loss: 0.10204176604747772
train_iter_loss: 0.19794733822345734
train_iter_loss: 0.28918129205703735
train_iter_loss: 0.2705133855342865
train_iter_loss: 0.16380925476551056
train_iter_loss: 0.33035191893577576
train_iter_loss: 0.2671352028846741
train_iter_loss: 0.253519207239151
train_iter_loss: 0.22410579025745392
train_iter_loss: 0.1939619779586792
train_iter_loss: 0.21323800086975098
train_iter_loss: 0.27631086111068726
train_iter_loss: 0.33794286847114563
train_iter_loss: 0.3060486614704132
train_iter_loss: 0.36403796076774597
train_iter_loss: 0.1988086998462677
train_iter_loss: 0.21474361419677734
train_iter_loss: 0.33373501896858215
train_iter_loss: 0.2899891436100006
train_iter_loss: 0.2800820469856262
train_iter_loss: 0.1687464416027069
train_iter_loss: 0.2209109663963318
train_iter_loss: 0.17433826625347137
train_iter_loss: 0.156265988945961
train_iter_loss: 0.17844824492931366
train_iter_loss: 0.32036784291267395
train_iter_loss: 0.24565649032592773
train_iter_loss: 0.15852929651737213
train_iter_loss: 0.3679945766925812
train_iter_loss: 0.21485812962055206
train_iter_loss: 0.3657555878162384
train_iter_loss: 0.2235482782125473
train_iter_loss: 0.3958353102207184
train_iter_loss: 0.23965594172477722
train_iter_loss: 0.1521655172109604
train_iter_loss: 0.23612509667873383
train_iter_loss: 0.06789203733205795
train_iter_loss: 0.5042416453361511
train_iter_loss: 0.2889409363269806
train_iter_loss: 0.21395690739154816
train_iter_loss: 0.2398137003183365
train_iter_loss: 0.23351389169692993
train_iter_loss: 0.20438255369663239
train_iter_loss: 0.24679754674434662
train_iter_loss: 0.15954817831516266
train_iter_loss: 0.22302524745464325
train_iter_loss: 0.2226227968931198
train_iter_loss: 0.18360763788223267
train_iter_loss: 0.20350521802902222
train_iter_loss: 0.15139690041542053
train_iter_loss: 0.17470504343509674
train_iter_loss: 0.31689852476119995
train_iter_loss: 0.101008340716362
train_iter_loss: 0.26266777515411377
train_iter_loss: 0.28804340958595276
train_iter_loss: 0.12925752997398376
train_iter_loss: 0.24559715390205383
train_iter_loss: 0.19232666492462158
train_iter_loss: 0.14583443105220795
train_iter_loss: 0.3590991199016571
train_iter_loss: 0.26890555024147034
train_iter_loss: 0.23036576807498932
train_iter_loss: 0.32867431640625
train_iter_loss: 0.21216651797294617
train_iter_loss: 0.262629896402359
train_iter_loss: 0.31165358424186707
train_iter_loss: 0.21803484857082367
train_iter_loss: 0.33691826462745667
train_iter_loss: 0.2293323129415512
train_iter_loss: 0.3594890236854553
train_iter_loss: 0.3737195134162903
train_iter_loss: 0.34835997223854065
train_iter_loss: 0.30413147807121277
train_iter_loss: 0.28877899050712585
train_iter_loss: 0.3423101007938385
train_iter_loss: 0.202922984957695
train_iter_loss: 0.3400926887989044
train_iter_loss: 0.1984512060880661
train_iter_loss: 0.385484516620636
train_iter_loss: 0.2558375298976898
train_iter_loss: 0.2864651083946228
train_iter_loss: 0.21210527420043945
train_iter_loss: 0.35006701946258545
train_iter_loss: 0.35604438185691833
train_iter_loss: 0.30133694410324097
train_iter_loss: 0.19045448303222656
train_iter_loss: 0.3585769534111023
train_iter_loss: 0.2186339646577835
train loss :0.2582
---------------------
Validation seg loss: 0.3614114731866515 at epoch 766
epoch =    767/  1000, exp = train
train_iter_loss: 0.2919657230377197
train_iter_loss: 0.2407931238412857
train_iter_loss: 0.2068847119808197
train_iter_loss: 0.36008909344673157
train_iter_loss: 0.19957894086837769
train_iter_loss: 0.1535644233226776
train_iter_loss: 0.27805542945861816
train_iter_loss: 0.145772323012352
train_iter_loss: 0.27336904406547546
train_iter_loss: 0.30427175760269165
train_iter_loss: 0.19228526949882507
train_iter_loss: 0.17937257885932922
train_iter_loss: 0.39138421416282654
train_iter_loss: 0.29718688130378723
train_iter_loss: 0.15859107673168182
train_iter_loss: 0.4431762397289276
train_iter_loss: 0.3669586479663849
train_iter_loss: 0.13221150636672974
train_iter_loss: 0.18999813497066498
train_iter_loss: 0.26808226108551025
train_iter_loss: 0.24281787872314453
train_iter_loss: 0.3842187225818634
train_iter_loss: 0.27593913674354553
train_iter_loss: 0.17929504811763763
train_iter_loss: 0.1651291400194168
train_iter_loss: 0.09679489582777023
train_iter_loss: 0.21883384883403778
train_iter_loss: 0.2032247632741928
train_iter_loss: 0.2770210802555084
train_iter_loss: 0.2647870182991028
train_iter_loss: 0.27310705184936523
train_iter_loss: 0.29737773537635803
train_iter_loss: 0.13212424516677856
train_iter_loss: 0.3925275206565857
train_iter_loss: 0.23845164477825165
train_iter_loss: 0.18976998329162598
train_iter_loss: 0.247353658080101
train_iter_loss: 0.15623962879180908
train_iter_loss: 0.09954486042261124
train_iter_loss: 0.22544945776462555
train_iter_loss: 0.25579091906547546
train_iter_loss: 0.4036903977394104
train_iter_loss: 0.20533472299575806
train_iter_loss: 0.27663880586624146
train_iter_loss: 0.2546336352825165
train_iter_loss: 0.24240681529045105
train_iter_loss: 0.2536159157752991
train_iter_loss: 0.19645407795906067
train_iter_loss: 0.34993064403533936
train_iter_loss: 0.3092908561229706
train_iter_loss: 0.156271830201149
train_iter_loss: 0.17015114426612854
train_iter_loss: 0.20207729935646057
train_iter_loss: 0.26349854469299316
train_iter_loss: 0.37716513872146606
train_iter_loss: 0.29603826999664307
train_iter_loss: 0.3714580237865448
train_iter_loss: 0.37065279483795166
train_iter_loss: 0.3026980757713318
train_iter_loss: 0.23913317918777466
train_iter_loss: 0.18378284573554993
train_iter_loss: 0.5415584444999695
train_iter_loss: 0.23251071572303772
train_iter_loss: 0.3909342586994171
train_iter_loss: 0.23857374489307404
train_iter_loss: 0.2358464151620865
train_iter_loss: 0.2380494922399521
train_iter_loss: 0.3807585537433624
train_iter_loss: 0.36023950576782227
train_iter_loss: 0.25876086950302124
train_iter_loss: 0.1782924383878708
train_iter_loss: 0.18039347231388092
train_iter_loss: 0.2965885400772095
train_iter_loss: 0.4379604756832123
train_iter_loss: 0.07790825515985489
train_iter_loss: 0.32511526346206665
train_iter_loss: 0.22342221438884735
train_iter_loss: 0.37519967555999756
train_iter_loss: 0.1872059553861618
train_iter_loss: 0.2310631275177002
train_iter_loss: 0.2357175350189209
train_iter_loss: 0.17368441820144653
train_iter_loss: 0.22300288081169128
train_iter_loss: 0.2231108397245407
train_iter_loss: 0.2611843943595886
train_iter_loss: 0.08134908229112625
train_iter_loss: 0.32248789072036743
train_iter_loss: 0.0925564393401146
train_iter_loss: 0.2975940704345703
train_iter_loss: 0.41157495975494385
train_iter_loss: 0.4916670024394989
train_iter_loss: 0.3220852017402649
train_iter_loss: 0.21895144879817963
train_iter_loss: 0.31641703844070435
train_iter_loss: 0.13833749294281006
train_iter_loss: 0.2973417639732361
train_iter_loss: 0.11813726276159286
train_iter_loss: 0.06856725364923477
train_iter_loss: 0.2712652385234833
train_iter_loss: 0.257220983505249
train loss :0.2579
---------------------
Validation seg loss: 0.3734261367474419 at epoch 767
epoch =    768/  1000, exp = train
train_iter_loss: 0.27017325162887573
train_iter_loss: 0.3091023564338684
train_iter_loss: 0.22287875413894653
train_iter_loss: 0.12686674296855927
train_iter_loss: 0.22749464213848114
train_iter_loss: 0.2073851376771927
train_iter_loss: 0.40692785382270813
train_iter_loss: 0.37264931201934814
train_iter_loss: 0.23863910138607025
train_iter_loss: 0.26698628067970276
train_iter_loss: 0.31444576382637024
train_iter_loss: 0.2540845274925232
train_iter_loss: 0.4245789051055908
train_iter_loss: 0.2429390698671341
train_iter_loss: 0.2509330213069916
train_iter_loss: 0.1722109317779541
train_iter_loss: 0.20073217153549194
train_iter_loss: 0.2753758728504181
train_iter_loss: 0.29913419485092163
train_iter_loss: 0.20679742097854614
train_iter_loss: 0.18437780439853668
train_iter_loss: 0.37774381041526794
train_iter_loss: 0.37208348512649536
train_iter_loss: 0.3405066430568695
train_iter_loss: 0.3478134870529175
train_iter_loss: 0.1789412498474121
train_iter_loss: 0.21321552991867065
train_iter_loss: 0.11664915084838867
train_iter_loss: 0.2949836850166321
train_iter_loss: 0.1753971427679062
train_iter_loss: 0.3635159432888031
train_iter_loss: 0.24502849578857422
train_iter_loss: 0.13858069479465485
train_iter_loss: 0.27501997351646423
train_iter_loss: 0.2253078818321228
train_iter_loss: 0.23567621409893036
train_iter_loss: 0.34062659740448
train_iter_loss: 0.3923669159412384
train_iter_loss: 0.3088361918926239
train_iter_loss: 0.19367453455924988
train_iter_loss: 0.4737551212310791
train_iter_loss: 0.1801602691411972
train_iter_loss: 0.34475886821746826
train_iter_loss: 0.14400966465473175
train_iter_loss: 0.2121417373418808
train_iter_loss: 0.38423243165016174
train_iter_loss: 0.5262987017631531
train_iter_loss: 0.2737639844417572
train_iter_loss: 0.18204553425312042
train_iter_loss: 0.2467348277568817
train_iter_loss: 0.31151139736175537
train_iter_loss: 0.3320814073085785
train_iter_loss: 0.33298027515411377
train_iter_loss: 0.2831226885318756
train_iter_loss: 0.2840081751346588
train_iter_loss: 0.1091887578368187
train_iter_loss: 0.23526927828788757
train_iter_loss: 0.21205338835716248
train_iter_loss: 0.29123401641845703
train_iter_loss: 0.22894689440727234
train_iter_loss: 0.28120824694633484
train_iter_loss: 0.4192810654640198
train_iter_loss: 0.26681816577911377
train_iter_loss: 0.2320815920829773
train_iter_loss: 0.1356535404920578
train_iter_loss: 0.1645907759666443
train_iter_loss: 0.242092102766037
train_iter_loss: 0.24029602110385895
train_iter_loss: 0.303555428981781
train_iter_loss: 0.14168806374073029
train_iter_loss: 0.28274261951446533
train_iter_loss: 0.18409986793994904
train_iter_loss: 0.07983756065368652
train_iter_loss: 0.13815125823020935
train_iter_loss: 0.3397652208805084
train_iter_loss: 0.1316068321466446
train_iter_loss: 0.3398340046405792
train_iter_loss: 0.18743489682674408
train_iter_loss: 0.18728944659233093
train_iter_loss: 0.3415190577507019
train_iter_loss: 0.2063274383544922
train_iter_loss: 0.3802397847175598
train_iter_loss: 0.2547619044780731
train_iter_loss: 0.13489636778831482
train_iter_loss: 0.2637152075767517
train_iter_loss: 0.2280695140361786
train_iter_loss: 0.2363700270652771
train_iter_loss: 0.37146368622779846
train_iter_loss: 0.20957855880260468
train_iter_loss: 0.3025749623775482
train_iter_loss: 0.26696544885635376
train_iter_loss: 0.2827756702899933
train_iter_loss: 0.28475135564804077
train_iter_loss: 0.2512476146221161
train_iter_loss: 0.3393549621105194
train_iter_loss: 0.37349268794059753
train_iter_loss: 0.22412507236003876
train_iter_loss: 0.275727242231369
train_iter_loss: 0.10751436650753021
train_iter_loss: 0.33593547344207764
train loss :0.2643
---------------------
Validation seg loss: 0.3788072548516727 at epoch 768
epoch =    769/  1000, exp = train
train_iter_loss: 0.4163225591182709
train_iter_loss: 0.0889243483543396
train_iter_loss: 0.06963972002267838
train_iter_loss: 0.16607123613357544
train_iter_loss: 0.5604898929595947
train_iter_loss: 0.17071014642715454
train_iter_loss: 0.24637067317962646
train_iter_loss: 0.2441301792860031
train_iter_loss: 0.22248251736164093
train_iter_loss: 0.27475887537002563
train_iter_loss: 0.19997254014015198
train_iter_loss: 0.25629952549934387
train_iter_loss: 0.3462735712528229
train_iter_loss: 0.24858702719211578
train_iter_loss: 0.16090227663516998
train_iter_loss: 0.34214404225349426
train_iter_loss: 0.24607065320014954
train_iter_loss: 0.19846870005130768
train_iter_loss: 0.2209354192018509
train_iter_loss: 0.3515281677246094
train_iter_loss: 0.4278559684753418
train_iter_loss: 0.2858186364173889
train_iter_loss: 0.33608534932136536
train_iter_loss: 0.3573637008666992
train_iter_loss: 0.2649903893470764
train_iter_loss: 0.23921793699264526
train_iter_loss: 0.30661633610725403
train_iter_loss: 0.2346923053264618
train_iter_loss: 0.14894238114356995
train_iter_loss: 0.18518559634685516
train_iter_loss: 0.22726620733737946
train_iter_loss: 0.1314978003501892
train_iter_loss: 0.18626472353935242
train_iter_loss: 0.2999204695224762
train_iter_loss: 0.28525683283805847
train_iter_loss: 0.35327064990997314
train_iter_loss: 0.2542996108531952
train_iter_loss: 0.25982749462127686
train_iter_loss: 0.20874501764774323
train_iter_loss: 0.27626487612724304
train_iter_loss: 0.17170579731464386
train_iter_loss: 0.2556707561016083
train_iter_loss: 0.283957302570343
train_iter_loss: 0.30464258790016174
train_iter_loss: 0.3634554147720337
train_iter_loss: 0.29316824674606323
train_iter_loss: 0.2851383090019226
train_iter_loss: 0.19100500643253326
train_iter_loss: 0.4436616003513336
train_iter_loss: 0.08060918748378754
train_iter_loss: 0.24157562851905823
train_iter_loss: 0.2899230718612671
train_iter_loss: 0.2416616827249527
train_iter_loss: 0.20877505838871002
train_iter_loss: 0.2122602015733719
train_iter_loss: 0.2845821976661682
train_iter_loss: 0.32169291377067566
train_iter_loss: 0.2512305676937103
train_iter_loss: 0.42056775093078613
train_iter_loss: 0.2727956771850586
train_iter_loss: 0.29910627007484436
train_iter_loss: 0.2916092574596405
train_iter_loss: 0.2972511053085327
train_iter_loss: 0.24529509246349335
train_iter_loss: 0.2999587059020996
train_iter_loss: 0.17182789742946625
train_iter_loss: 0.3285365700721741
train_iter_loss: 0.24240219593048096
train_iter_loss: 0.1598597913980484
train_iter_loss: 0.40184327960014343
train_iter_loss: 0.32719722390174866
train_iter_loss: 0.3689137101173401
train_iter_loss: 0.2683100700378418
train_iter_loss: 0.22153885662555695
train_iter_loss: 0.14934146404266357
train_iter_loss: 0.29185307025909424
train_iter_loss: 0.19915327429771423
train_iter_loss: 0.21678967773914337
train_iter_loss: 0.27849406003952026
train_iter_loss: 0.19899949431419373
train_iter_loss: 0.3637421131134033
train_iter_loss: 0.25197839736938477
train_iter_loss: 0.19534191489219666
train_iter_loss: 0.1550748497247696
train_iter_loss: 0.22870658338069916
train_iter_loss: 0.3098793029785156
train_iter_loss: 0.3027878701686859
train_iter_loss: 0.3082633912563324
train_iter_loss: 0.25184643268585205
train_iter_loss: 0.15419608354568481
train_iter_loss: 0.2523585855960846
train_iter_loss: 0.2519384026527405
train_iter_loss: 0.3867930769920349
train_iter_loss: 0.2022232562303543
train_iter_loss: 0.1807359904050827
train_iter_loss: 0.11081662774085999
train_iter_loss: 0.18892516195774078
train_iter_loss: 0.19181287288665771
train_iter_loss: 0.15158823132514954
train_iter_loss: 0.07169177383184433
train loss :0.2575
---------------------
Validation seg loss: 0.3636957277842569 at epoch 769
epoch =    770/  1000, exp = train
train_iter_loss: 0.16702282428741455
train_iter_loss: 0.19864337146282196
train_iter_loss: 0.4008711278438568
train_iter_loss: 0.11581655591726303
train_iter_loss: 0.31560149788856506
train_iter_loss: 0.2053632289171219
train_iter_loss: 0.3707406222820282
train_iter_loss: 0.27677786350250244
train_iter_loss: 0.13824772834777832
train_iter_loss: 0.1920621544122696
train_iter_loss: 0.1628606915473938
train_iter_loss: 0.18216651678085327
train_iter_loss: 0.04958786442875862
train_iter_loss: 0.3312743306159973
train_iter_loss: 0.25496670603752136
train_iter_loss: 0.39006364345550537
train_iter_loss: 0.23797327280044556
train_iter_loss: 0.23207096755504608
train_iter_loss: 0.18662448227405548
train_iter_loss: 0.3128388524055481
train_iter_loss: 0.3006235957145691
train_iter_loss: 0.2545160949230194
train_iter_loss: 0.13396333158016205
train_iter_loss: 0.2459525465965271
train_iter_loss: 0.18993625044822693
train_iter_loss: 0.13551189005374908
train_iter_loss: 0.29754915833473206
train_iter_loss: 0.11473054438829422
train_iter_loss: 0.2575685977935791
train_iter_loss: 0.22483383119106293
train_iter_loss: 0.3659580647945404
train_iter_loss: 0.23703204095363617
train_iter_loss: 0.3420429229736328
train_iter_loss: 0.3458847999572754
train_iter_loss: 0.2899297773838043
train_iter_loss: 0.24255099892616272
train_iter_loss: 0.3138561546802521
train_iter_loss: 0.37616783380508423
train_iter_loss: 0.3171420097351074
train_iter_loss: 0.12386977672576904
train_iter_loss: 0.11319076269865036
train_iter_loss: 0.2579175531864166
train_iter_loss: 0.3947822153568268
train_iter_loss: 0.10750418156385422
train_iter_loss: 0.3282574415206909
train_iter_loss: 0.25022831559181213
train_iter_loss: 0.21779392659664154
train_iter_loss: 0.33346858620643616
train_iter_loss: 0.3470178246498108
train_iter_loss: 0.38243624567985535
train_iter_loss: 0.2590257525444031
train_iter_loss: 0.2052464485168457
train_iter_loss: 0.2938244938850403
train_iter_loss: 0.17555034160614014
train_iter_loss: 0.4342111051082611
train_iter_loss: 0.14960028231143951
train_iter_loss: 0.3202657997608185
train_iter_loss: 0.19957323372364044
train_iter_loss: 0.17813819646835327
train_iter_loss: 0.20142851769924164
train_iter_loss: 0.22650979459285736
train_iter_loss: 0.32292720675468445
train_iter_loss: 0.22478535771369934
train_iter_loss: 0.18947963416576385
train_iter_loss: 0.331052303314209
train_iter_loss: 0.1822396069765091
train_iter_loss: 0.1302396059036255
train_iter_loss: 0.2073332965373993
train_iter_loss: 0.21419738233089447
train_iter_loss: 0.36551862955093384
train_iter_loss: 0.1982201784849167
train_iter_loss: 0.19812631607055664
train_iter_loss: 0.10911299288272858
train_iter_loss: 0.2927159070968628
train_iter_loss: 0.1917332410812378
train_iter_loss: 0.27924859523773193
train_iter_loss: 0.175834059715271
train_iter_loss: 0.18238140642642975
train_iter_loss: 0.353899747133255
train_iter_loss: 0.15683379769325256
train_iter_loss: 0.31869786977767944
train_iter_loss: 0.2162262499332428
train_iter_loss: 0.14678668975830078
train_iter_loss: 0.22520510852336884
train_iter_loss: 0.3063775599002838
train_iter_loss: 0.422448992729187
train_iter_loss: 0.29808253049850464
train_iter_loss: 0.2582114338874817
train_iter_loss: 0.2196342796087265
train_iter_loss: 0.2678150236606598
train_iter_loss: 0.3872891664505005
train_iter_loss: 0.29029160737991333
train_iter_loss: 0.3482215106487274
train_iter_loss: 0.34288299083709717
train_iter_loss: 0.39151695370674133
train_iter_loss: 0.17907050251960754
train_iter_loss: 0.2709900438785553
train_iter_loss: 0.26398423314094543
train_iter_loss: 0.35528579354286194
train_iter_loss: 0.23860876262187958
train loss :0.2552
---------------------
Validation seg loss: 0.3576810442104514 at epoch 770
epoch =    771/  1000, exp = train
train_iter_loss: 0.2803053855895996
train_iter_loss: 0.20669610798358917
train_iter_loss: 0.19137141108512878
train_iter_loss: 0.2926561236381531
train_iter_loss: 0.20050761103630066
train_iter_loss: 0.28685951232910156
train_iter_loss: 0.15525241196155548
train_iter_loss: 0.3772179186344147
train_iter_loss: 0.24393513798713684
train_iter_loss: 0.3013007640838623
train_iter_loss: 0.31348225474357605
train_iter_loss: 0.28530043363571167
train_iter_loss: 0.11682486534118652
train_iter_loss: 0.17294283211231232
train_iter_loss: 0.3105226159095764
train_iter_loss: 0.207040473818779
train_iter_loss: 0.17971134185791016
train_iter_loss: 0.384600967168808
train_iter_loss: 0.3146303594112396
train_iter_loss: 0.18720343708992004
train_iter_loss: 0.25627362728118896
train_iter_loss: 0.33843836188316345
train_iter_loss: 0.16598521173000336
train_iter_loss: 0.4261668920516968
train_iter_loss: 0.35246360301971436
train_iter_loss: 0.3323846757411957
train_iter_loss: 0.21878443658351898
train_iter_loss: 0.26924920082092285
train_iter_loss: 0.33810868859291077
train_iter_loss: 0.283030241727829
train_iter_loss: 0.2367914766073227
train_iter_loss: 0.30716821551322937
train_iter_loss: 0.17181329429149628
train_iter_loss: 0.44779664278030396
train_iter_loss: 0.3338431715965271
train_iter_loss: 0.3048100769519806
train_iter_loss: 0.3857422471046448
train_iter_loss: 0.23892219364643097
train_iter_loss: 0.2026827186346054
train_iter_loss: 0.39984992146492004
train_iter_loss: 0.29617840051651
train_iter_loss: 0.18954797089099884
train_iter_loss: 0.17779254913330078
train_iter_loss: 0.16656775772571564
train_iter_loss: 0.5488618612289429
train_iter_loss: 0.1250310242176056
train_iter_loss: 0.3195820748806
train_iter_loss: 0.29641783237457275
train_iter_loss: 0.199912890791893
train_iter_loss: 0.34018778800964355
train_iter_loss: 0.25051695108413696
train_iter_loss: 0.2902853786945343
train_iter_loss: 0.2865771949291229
train_iter_loss: 0.19679096341133118
train_iter_loss: 0.2563610374927521
train_iter_loss: 0.15216030180454254
train_iter_loss: 0.21580104529857635
train_iter_loss: 0.16152136027812958
train_iter_loss: 0.2556819021701813
train_iter_loss: 0.1233748346567154
train_iter_loss: 0.22171229124069214
train_iter_loss: 0.2263404130935669
train_iter_loss: 0.28123652935028076
train_iter_loss: 0.3748568296432495
train_iter_loss: 0.3218904137611389
train_iter_loss: 0.32938650250434875
train_iter_loss: 0.2227153480052948
train_iter_loss: 0.2629423141479492
train_iter_loss: 0.2707391381263733
train_iter_loss: 0.21722511947155
train_iter_loss: 0.28279271721839905
train_iter_loss: 0.25288742780685425
train_iter_loss: 0.5515314340591431
train_iter_loss: 0.1229502409696579
train_iter_loss: 0.2674151360988617
train_iter_loss: 0.31938332319259644
train_iter_loss: 0.32251662015914917
train_iter_loss: 0.3262227475643158
train_iter_loss: 0.16385062038898468
train_iter_loss: 0.296042263507843
train_iter_loss: 0.24029454588890076
train_iter_loss: 0.1676156371831894
train_iter_loss: 0.3870614469051361
train_iter_loss: 0.1233561784029007
train_iter_loss: 0.2049788534641266
train_iter_loss: 0.3279933035373688
train_iter_loss: 0.3037054240703583
train_iter_loss: 0.08504360914230347
train_iter_loss: 0.19674986600875854
train_iter_loss: 0.21097654104232788
train_iter_loss: 0.30543893575668335
train_iter_loss: 0.12198848277330399
train_iter_loss: 0.3081856966018677
train_iter_loss: 0.37830397486686707
train_iter_loss: 0.21891961991786957
train_iter_loss: 0.33673572540283203
train_iter_loss: 0.24323247373104095
train_iter_loss: 0.24454505741596222
train_iter_loss: 0.15775959193706512
train_iter_loss: 0.1714470386505127
train loss :0.2649
---------------------
Validation seg loss: 0.3452647302985051 at epoch 771
epoch =    772/  1000, exp = train
train_iter_loss: 0.267924427986145
train_iter_loss: 0.18919895589351654
train_iter_loss: 0.47865456342697144
train_iter_loss: 0.268762469291687
train_iter_loss: 0.32975250482559204
train_iter_loss: 0.2662099301815033
train_iter_loss: 0.19935311377048492
train_iter_loss: 0.2405177652835846
train_iter_loss: 0.25508949160575867
train_iter_loss: 0.29411453008651733
train_iter_loss: 0.32211950421333313
train_iter_loss: 0.32108092308044434
train_iter_loss: 0.32207512855529785
train_iter_loss: 0.2912294268608093
train_iter_loss: 0.2831976115703583
train_iter_loss: 0.36620935797691345
train_iter_loss: 0.19154343008995056
train_iter_loss: 0.04920095205307007
train_iter_loss: 0.33420005440711975
train_iter_loss: 0.19068777561187744
train_iter_loss: 0.3357662558555603
train_iter_loss: 0.15120407938957214
train_iter_loss: 0.41492870450019836
train_iter_loss: 0.38887548446655273
train_iter_loss: 0.309105783700943
train_iter_loss: 0.3882448971271515
train_iter_loss: 0.39061325788497925
train_iter_loss: 0.32039734721183777
train_iter_loss: 0.33678561449050903
train_iter_loss: 0.2883414030075073
train_iter_loss: 0.2607460916042328
train_iter_loss: 0.17933689057826996
train_iter_loss: 0.1771237850189209
train_iter_loss: 0.3089539110660553
train_iter_loss: 0.23511160910129547
train_iter_loss: 0.3289273679256439
train_iter_loss: 0.12976358830928802
train_iter_loss: 0.2874395251274109
train_iter_loss: 0.25058722496032715
train_iter_loss: 0.25289276242256165
train_iter_loss: 0.34434735774993896
train_iter_loss: 0.14345869421958923
train_iter_loss: 0.2892056703567505
train_iter_loss: 0.21669501066207886
train_iter_loss: 0.32299280166625977
train_iter_loss: 0.2663983702659607
train_iter_loss: 0.36894476413726807
train_iter_loss: 0.23799492418766022
train_iter_loss: 0.1589965671300888
train_iter_loss: 0.19202665984630585
train_iter_loss: 0.24343439936637878
train_iter_loss: 0.2896736264228821
train_iter_loss: 0.10618860274553299
train_iter_loss: 0.2793325185775757
train_iter_loss: 0.26867982745170593
train_iter_loss: 0.13375206291675568
train_iter_loss: 0.3673873245716095
train_iter_loss: 0.2528792917728424
train_iter_loss: 0.48395222425460815
train_iter_loss: 0.2535838484764099
train_iter_loss: 0.3840194642543793
train_iter_loss: 0.2747960388660431
train_iter_loss: 0.2239789515733719
train_iter_loss: 0.28050366044044495
train_iter_loss: 0.26500505208969116
train_iter_loss: 0.28650471568107605
train_iter_loss: 0.3088659644126892
train_iter_loss: 0.23531316220760345
train_iter_loss: 0.06649313867092133
train_iter_loss: 0.5024653077125549
train_iter_loss: 0.22047963738441467
train_iter_loss: 0.21980158984661102
train_iter_loss: 0.05289100483059883
train_iter_loss: 0.3377895653247833
train_iter_loss: 0.3965814709663391
train_iter_loss: 0.24065887928009033
train_iter_loss: 0.21959666907787323
train_iter_loss: 0.22317124903202057
train_iter_loss: 0.29948997497558594
train_iter_loss: 0.18412618339061737
train_iter_loss: 0.16178378462791443
train_iter_loss: 0.26577699184417725
train_iter_loss: 0.20277662575244904
train_iter_loss: 0.3376716375350952
train_iter_loss: 0.13803155720233917
train_iter_loss: 0.17657752335071564
train_iter_loss: 0.3231123387813568
train_iter_loss: 0.13482680916786194
train_iter_loss: 0.16239793598651886
train_iter_loss: 0.3669014275074005
train_iter_loss: 0.23778854310512543
train_iter_loss: 0.23506788909435272
train_iter_loss: 0.15923748910427094
train_iter_loss: 0.38602733612060547
train_iter_loss: 0.16793382167816162
train_iter_loss: 0.312407523393631
train_iter_loss: 0.22695720195770264
train_iter_loss: 0.19373781979084015
train_iter_loss: 0.18595978617668152
train_iter_loss: 0.27345696091651917
train loss :0.2655
---------------------
Validation seg loss: 0.3596016368828714 at epoch 772
epoch =    773/  1000, exp = train
train_iter_loss: 0.09072793275117874
train_iter_loss: 0.306794673204422
train_iter_loss: 0.25891542434692383
train_iter_loss: 0.16572587192058563
train_iter_loss: 0.10441087186336517
train_iter_loss: 0.2939833402633667
train_iter_loss: 0.23472437262535095
train_iter_loss: 0.316646546125412
train_iter_loss: 0.17198972404003143
train_iter_loss: 0.23678016662597656
train_iter_loss: 0.09108640998601913
train_iter_loss: 0.30173951387405396
train_iter_loss: 0.11414821445941925
train_iter_loss: 0.30631223320961
train_iter_loss: 0.3055247366428375
train_iter_loss: 0.2880304157733917
train_iter_loss: 0.23759296536445618
train_iter_loss: 0.42864349484443665
train_iter_loss: 0.2705222964286804
train_iter_loss: 0.17903262376785278
train_iter_loss: 0.07631911337375641
train_iter_loss: 0.312360942363739
train_iter_loss: 0.3662923276424408
train_iter_loss: 0.16024863719940186
train_iter_loss: 0.39979854226112366
train_iter_loss: 0.36997532844543457
train_iter_loss: 0.3972126543521881
train_iter_loss: 0.3903311789035797
train_iter_loss: 0.29484474658966064
train_iter_loss: 0.13908487558364868
train_iter_loss: 0.17243675887584686
train_iter_loss: 0.1803346425294876
train_iter_loss: 0.08942310512065887
train_iter_loss: 0.17211578786373138
train_iter_loss: 0.38275742530822754
train_iter_loss: 0.34384778141975403
train_iter_loss: 0.1975456029176712
train_iter_loss: 0.22389324009418488
train_iter_loss: 0.2777612507343292
train_iter_loss: 0.20848919451236725
train_iter_loss: 0.3123399019241333
train_iter_loss: 0.2705689072608948
train_iter_loss: 0.4168505370616913
train_iter_loss: 0.20110638439655304
train_iter_loss: 0.22346067428588867
train_iter_loss: 0.18004311621189117
train_iter_loss: 0.4086161255836487
train_iter_loss: 0.2542346715927124
train_iter_loss: 0.3165438175201416
train_iter_loss: 0.30441534519195557
train_iter_loss: 0.08611168712377548
train_iter_loss: 0.373553991317749
train_iter_loss: 0.22536924481391907
train_iter_loss: 0.31676599383354187
train_iter_loss: 0.2436000108718872
train_iter_loss: 0.30499476194381714
train_iter_loss: 0.19274580478668213
train_iter_loss: 0.25752219557762146
train_iter_loss: 0.2530454099178314
train_iter_loss: 0.24282130599021912
train_iter_loss: 0.1609870344400406
train_iter_loss: 0.22163988649845123
train_iter_loss: 0.20879389345645905
train_iter_loss: 0.4552842676639557
train_iter_loss: 0.32913291454315186
train_iter_loss: 0.32799142599105835
train_iter_loss: 0.32010212540626526
train_iter_loss: 0.2906096875667572
train_iter_loss: 0.18595187366008759
train_iter_loss: 0.3606107234954834
train_iter_loss: 0.2556123435497284
train_iter_loss: 0.2130483239889145
train_iter_loss: 0.1882704198360443
train_iter_loss: 0.2644849121570587
train_iter_loss: 0.12256141006946564
train_iter_loss: 0.23942691087722778
train_iter_loss: 0.28833717107772827
train_iter_loss: 0.32335084676742554
train_iter_loss: 0.3523615002632141
train_iter_loss: 0.15295635163784027
train_iter_loss: 0.47913020849227905
train_iter_loss: 0.2409249097108841
train_iter_loss: 0.18806599080562592
train_iter_loss: 0.37621670961380005
train_iter_loss: 0.17111286520957947
train_iter_loss: 0.2548973560333252
train_iter_loss: 0.269584596157074
train_iter_loss: 0.19344206154346466
train_iter_loss: 0.3112604022026062
train_iter_loss: 0.32680267095565796
train_iter_loss: 0.31697380542755127
train_iter_loss: 0.29228517413139343
train_iter_loss: 0.28925102949142456
train_iter_loss: 0.23296408355236053
train_iter_loss: 0.33425524830818176
train_iter_loss: 0.16295550763607025
train_iter_loss: 0.25015878677368164
train_iter_loss: 0.2375795841217041
train_iter_loss: 0.4299672842025757
train_iter_loss: 0.036736488342285156
train loss :0.2616
---------------------
Validation seg loss: 0.3497463009618926 at epoch 773
epoch =    774/  1000, exp = train
train_iter_loss: 0.15387490391731262
train_iter_loss: 0.13580456376075745
train_iter_loss: 0.2773047387599945
train_iter_loss: 0.2031126171350479
train_iter_loss: 0.24557094275951385
train_iter_loss: 0.23061120510101318
train_iter_loss: 0.3692516088485718
train_iter_loss: 0.25596025586128235
train_iter_loss: 0.23550891876220703
train_iter_loss: 0.14736875891685486
train_iter_loss: 0.2310812771320343
train_iter_loss: 0.14922450482845306
train_iter_loss: 0.20061762630939484
train_iter_loss: 0.37352293729782104
train_iter_loss: 0.22794318199157715
train_iter_loss: 0.148757204413414
train_iter_loss: 0.2683415710926056
train_iter_loss: 0.20234394073486328
train_iter_loss: 0.21990014612674713
train_iter_loss: 0.29037633538246155
train_iter_loss: 0.2226712852716446
train_iter_loss: 0.21733376383781433
train_iter_loss: 0.4125806987285614
train_iter_loss: 0.2654047906398773
train_iter_loss: 0.24362313747406006
train_iter_loss: 0.3591199815273285
train_iter_loss: 0.10623856633901596
train_iter_loss: 0.32873862981796265
train_iter_loss: 0.12782883644104004
train_iter_loss: 0.3239729106426239
train_iter_loss: 0.2042500078678131
train_iter_loss: 0.2145698070526123
train_iter_loss: 0.363562673330307
train_iter_loss: 0.22547365725040436
train_iter_loss: 0.2622981071472168
train_iter_loss: 0.27765125036239624
train_iter_loss: 0.22619883716106415
train_iter_loss: 0.30878496170043945
train_iter_loss: 0.3572149872779846
train_iter_loss: 0.1359076201915741
train_iter_loss: 0.31961122155189514
train_iter_loss: 0.4270170331001282
train_iter_loss: 0.34107303619384766
train_iter_loss: 0.282896488904953
train_iter_loss: 0.4764990508556366
train_iter_loss: 0.15322406589984894
train_iter_loss: 0.2462998777627945
train_iter_loss: 0.2018045037984848
train_iter_loss: 0.25490301847457886
train_iter_loss: 0.30690455436706543
train_iter_loss: 0.18636487424373627
train_iter_loss: 0.30803439021110535
train_iter_loss: 0.22337587177753448
train_iter_loss: 0.14388440549373627
train_iter_loss: 0.19098252058029175
train_iter_loss: 0.2805861830711365
train_iter_loss: 0.2993573546409607
train_iter_loss: 0.20086486637592316
train_iter_loss: 0.28188881278038025
train_iter_loss: 0.10494983941316605
train_iter_loss: 0.30491846799850464
train_iter_loss: 0.13979728519916534
train_iter_loss: 0.28439679741859436
train_iter_loss: 0.14614522457122803
train_iter_loss: 0.30645880103111267
train_iter_loss: 0.3607325851917267
train_iter_loss: 0.34388381242752075
train_iter_loss: 0.24263454973697662
train_iter_loss: 0.19446012377738953
train_iter_loss: 0.19126734137535095
train_iter_loss: 0.22088047862052917
train_iter_loss: 0.2584412097930908
train_iter_loss: 0.2407267689704895
train_iter_loss: 0.330486536026001
train_iter_loss: 0.39751482009887695
train_iter_loss: 0.15361297130584717
train_iter_loss: 0.24375683069229126
train_iter_loss: 0.3622126877307892
train_iter_loss: 0.46522486209869385
train_iter_loss: 0.32304710149765015
train_iter_loss: 0.23915275931358337
train_iter_loss: 0.25318410992622375
train_iter_loss: 0.22578103840351105
train_iter_loss: 0.3777123689651489
train_iter_loss: 0.2740621268749237
train_iter_loss: 0.1991874724626541
train_iter_loss: 0.28490570187568665
train_iter_loss: 0.26979631185531616
train_iter_loss: 0.3298289477825165
train_iter_loss: 0.2184973806142807
train_iter_loss: 0.268235981464386
train_iter_loss: 0.36366933584213257
train_iter_loss: 0.18155521154403687
train_iter_loss: 0.2439972460269928
train_iter_loss: 0.39735233783721924
train_iter_loss: 0.2138194739818573
train_iter_loss: 0.16179664433002472
train_iter_loss: 0.2958676517009735
train_iter_loss: 0.3223797678947449
train_iter_loss: 0.2447587102651596
train loss :0.2612
---------------------
Validation seg loss: 0.3949554106747767 at epoch 774
epoch =    775/  1000, exp = train
train_iter_loss: 0.2268160730600357
train_iter_loss: 0.13031050562858582
train_iter_loss: 0.3684217631816864
train_iter_loss: 0.26789015531539917
train_iter_loss: 0.28319308161735535
train_iter_loss: 0.22427226603031158
train_iter_loss: 0.1448184847831726
train_iter_loss: 0.15583699941635132
train_iter_loss: 0.1526734083890915
train_iter_loss: 0.28111380338668823
train_iter_loss: 0.21858271956443787
train_iter_loss: 0.3151259124279022
train_iter_loss: 0.29409754276275635
train_iter_loss: 0.1644306182861328
train_iter_loss: 0.04370630532503128
train_iter_loss: 0.18492987751960754
train_iter_loss: 0.17388731241226196
train_iter_loss: 0.37334948778152466
train_iter_loss: 0.3476625680923462
train_iter_loss: 0.2240094691514969
train_iter_loss: 0.20189562439918518
train_iter_loss: 0.26603126525878906
train_iter_loss: 0.20218762755393982
train_iter_loss: 0.16396769881248474
train_iter_loss: 0.35977667570114136
train_iter_loss: 0.28278154134750366
train_iter_loss: 0.19759567081928253
train_iter_loss: 0.29011818766593933
train_iter_loss: 0.20771118998527527
train_iter_loss: 0.2410532385110855
train_iter_loss: 0.4400167167186737
train_iter_loss: 0.1552395224571228
train_iter_loss: 0.25314491987228394
train_iter_loss: 0.15641850233078003
train_iter_loss: 0.42394718527793884
train_iter_loss: 0.14375506341457367
train_iter_loss: 0.298647403717041
train_iter_loss: 0.16736437380313873
train_iter_loss: 0.15265634655952454
train_iter_loss: 0.2329588383436203
train_iter_loss: 0.3320603668689728
train_iter_loss: 0.3092086613178253
train_iter_loss: 0.3343003988265991
train_iter_loss: 0.3216532766819
train_iter_loss: 0.1832980215549469
train_iter_loss: 0.1999148279428482
train_iter_loss: 0.21486537158489227
train_iter_loss: 0.19398394227027893
train_iter_loss: 0.3382219076156616
train_iter_loss: 0.24340616166591644
train_iter_loss: 0.06133810803294182
train_iter_loss: 0.3594663143157959
train_iter_loss: 0.4195663332939148
train_iter_loss: 0.28135716915130615
train_iter_loss: 0.3523813784122467
train_iter_loss: 0.3259422481060028
train_iter_loss: 0.22945338487625122
train_iter_loss: 0.1865551471710205
train_iter_loss: 0.3736741244792938
train_iter_loss: 0.31143906712532043
train_iter_loss: 0.36141061782836914
train_iter_loss: 0.3558245301246643
train_iter_loss: 0.23061765730381012
train_iter_loss: 0.27106642723083496
train_iter_loss: 0.23483890295028687
train_iter_loss: 0.3500804007053375
train_iter_loss: 0.3867538273334503
train_iter_loss: 0.21377664804458618
train_iter_loss: 0.31454893946647644
train_iter_loss: 0.3167608380317688
train_iter_loss: 0.15850815176963806
train_iter_loss: 0.36508050560951233
train_iter_loss: 0.2350168228149414
train_iter_loss: 0.2207508385181427
train_iter_loss: 0.2080671489238739
train_iter_loss: 0.19107316434383392
train_iter_loss: 0.32111501693725586
train_iter_loss: 0.3038868308067322
train_iter_loss: 0.2923312485218048
train_iter_loss: 0.2557065188884735
train_iter_loss: 0.2456331104040146
train_iter_loss: 0.14799922704696655
train_iter_loss: 0.1266864687204361
train_iter_loss: 0.2434249371290207
train_iter_loss: 0.10709407180547714
train_iter_loss: 0.23598478734493256
train_iter_loss: 0.1986228972673416
train_iter_loss: 0.3255331814289093
train_iter_loss: 0.38413673639297485
train_iter_loss: 0.1967676281929016
train_iter_loss: 0.24777504801750183
train_iter_loss: 0.254427045583725
train_iter_loss: 0.2511592507362366
train_iter_loss: 0.10662981867790222
train_iter_loss: 0.2809234857559204
train_iter_loss: 0.35689252614974976
train_iter_loss: 0.2749832570552826
train_iter_loss: 0.32820188999176025
train_iter_loss: 0.33888182044029236
train_iter_loss: 0.28233835101127625
train loss :0.2577
---------------------
Validation seg loss: 0.34400908687626414 at epoch 775
epoch =    776/  1000, exp = train
train_iter_loss: 0.20017044246196747
train_iter_loss: 0.2790290117263794
train_iter_loss: 0.2972247898578644
train_iter_loss: 0.18845640122890472
train_iter_loss: 0.3469027280807495
train_iter_loss: 0.24006816744804382
train_iter_loss: 0.2457592636346817
train_iter_loss: 0.2334287017583847
train_iter_loss: 0.19566282629966736
train_iter_loss: 0.1752059906721115
train_iter_loss: 0.16240020096302032
train_iter_loss: 0.3014121949672699
train_iter_loss: 0.3028057813644409
train_iter_loss: 0.23070009052753448
train_iter_loss: 0.37807878851890564
train_iter_loss: 0.25373750925064087
train_iter_loss: 0.19963982701301575
train_iter_loss: 0.09042935818433762
train_iter_loss: 0.5051541924476624
train_iter_loss: 0.28012022376060486
train_iter_loss: 0.4522568881511688
train_iter_loss: 0.11437055468559265
train_iter_loss: 0.21584075689315796
train_iter_loss: 0.24135833978652954
train_iter_loss: 0.19304366409778595
train_iter_loss: 0.2621566951274872
train_iter_loss: 0.4703156054019928
train_iter_loss: 0.26960593461990356
train_iter_loss: 0.15014727413654327
train_iter_loss: 0.3568480610847473
train_iter_loss: 0.3095775842666626
train_iter_loss: 0.26123857498168945
train_iter_loss: 0.32051652669906616
train_iter_loss: 0.174658864736557
train_iter_loss: 0.5174742341041565
train_iter_loss: 0.19699667394161224
train_iter_loss: 0.24135547876358032
train_iter_loss: 0.25089597702026367
train_iter_loss: 0.27250897884368896
train_iter_loss: 0.3351867198944092
train_iter_loss: 0.20370455086231232
train_iter_loss: 0.30977803468704224
train_iter_loss: 0.1885245144367218
train_iter_loss: 0.1224098652601242
train_iter_loss: 0.2808387279510498
train_iter_loss: 0.2963705062866211
train_iter_loss: 0.31066885590553284
train_iter_loss: 0.17368720471858978
train_iter_loss: 0.23517145216464996
train_iter_loss: 0.2862641513347626
train_iter_loss: 0.2761555314064026
train_iter_loss: 0.37101510167121887
train_iter_loss: 0.25509151816368103
train_iter_loss: 0.19486355781555176
train_iter_loss: 0.24803338944911957
train_iter_loss: 0.24212780594825745
train_iter_loss: 0.31127700209617615
train_iter_loss: 0.26010772585868835
train_iter_loss: 0.22615870833396912
train_iter_loss: 0.11883754283189774
train_iter_loss: 0.23021769523620605
train_iter_loss: 0.27264490723609924
train_iter_loss: 0.13003942370414734
train_iter_loss: 0.35757023096084595
train_iter_loss: 0.2785155773162842
train_iter_loss: 0.3751518726348877
train_iter_loss: 0.24950407445430756
train_iter_loss: 0.30173033475875854
train_iter_loss: 0.3303319215774536
train_iter_loss: 0.21123747527599335
train_iter_loss: 0.22797547280788422
train_iter_loss: 0.23498539626598358
train_iter_loss: 0.2824316620826721
train_iter_loss: 0.15037967264652252
train_iter_loss: 0.1916968673467636
train_iter_loss: 0.2821469306945801
train_iter_loss: 0.22604972124099731
train_iter_loss: 0.1552795022726059
train_iter_loss: 0.2996838092803955
train_iter_loss: 0.3874804973602295
train_iter_loss: 0.25183919072151184
train_iter_loss: 0.3658246695995331
train_iter_loss: 0.29562491178512573
train_iter_loss: 0.2026108205318451
train_iter_loss: 0.13104796409606934
train_iter_loss: 0.29137927293777466
train_iter_loss: 0.1900639533996582
train_iter_loss: 0.16897600889205933
train_iter_loss: 0.3643043637275696
train_iter_loss: 0.30671989917755127
train_iter_loss: 0.38655155897140503
train_iter_loss: 0.2460951805114746
train_iter_loss: 0.397424578666687
train_iter_loss: 0.31416311860084534
train_iter_loss: 0.38337424397468567
train_iter_loss: 0.3488825857639313
train_iter_loss: 0.15884988009929657
train_iter_loss: 0.11571940034627914
train_iter_loss: 0.43160414695739746
train_iter_loss: 0.345193475484848
train loss :0.2675
---------------------
Validation seg loss: 0.35008466338633365 at epoch 776
epoch =    777/  1000, exp = train
train_iter_loss: 0.332226425409317
train_iter_loss: 0.1645425260066986
train_iter_loss: 0.22647759318351746
train_iter_loss: 0.25181159377098083
train_iter_loss: 0.11900413036346436
train_iter_loss: 0.34670233726501465
train_iter_loss: 0.26459091901779175
train_iter_loss: 0.2310587465763092
train_iter_loss: 0.17190739512443542
train_iter_loss: 0.2765548825263977
train_iter_loss: 0.26289471983909607
train_iter_loss: 0.13187934458255768
train_iter_loss: 0.2781689763069153
train_iter_loss: 0.17004628479480743
train_iter_loss: 0.28898531198501587
train_iter_loss: 0.36449000239372253
train_iter_loss: 0.3365013897418976
train_iter_loss: 0.41965678334236145
train_iter_loss: 0.28030210733413696
train_iter_loss: 0.28131306171417236
train_iter_loss: 0.19796282052993774
train_iter_loss: 0.19537775218486786
train_iter_loss: 0.23176641762256622
train_iter_loss: 0.2637873589992523
train_iter_loss: 0.4729035198688507
train_iter_loss: 0.3351365327835083
train_iter_loss: 0.4197152853012085
train_iter_loss: 0.41535136103630066
train_iter_loss: 0.37314391136169434
train_iter_loss: 0.30306902527809143
train_iter_loss: 0.3103817105293274
train_iter_loss: 0.324719101190567
train_iter_loss: 0.2967619299888611
train_iter_loss: 0.19702409207820892
train_iter_loss: 0.22863827645778656
train_iter_loss: 0.08665492385625839
train_iter_loss: 0.18904970586299896
train_iter_loss: 0.20372773706912994
train_iter_loss: 0.16265568137168884
train_iter_loss: 0.299306720495224
train_iter_loss: 0.32178643345832825
train_iter_loss: 0.1787586212158203
train_iter_loss: 0.16700948774814606
train_iter_loss: 0.36809295415878296
train_iter_loss: 0.20523537695407867
train_iter_loss: 0.25371405482292175
train_iter_loss: 0.2507120966911316
train_iter_loss: 0.1454515904188156
train_iter_loss: 0.10999184846878052
train_iter_loss: 0.26956596970558167
train_iter_loss: 0.355030357837677
train_iter_loss: 0.3983435034751892
train_iter_loss: 0.20164722204208374
train_iter_loss: 0.3347983956336975
train_iter_loss: 0.21271033585071564
train_iter_loss: 0.39122748374938965
train_iter_loss: 0.28598812222480774
train_iter_loss: 0.18179097771644592
train_iter_loss: 0.191237211227417
train_iter_loss: 0.2905985414981842
train_iter_loss: 0.27348819375038147
train_iter_loss: 0.3707389235496521
train_iter_loss: 0.3030118942260742
train_iter_loss: 0.076853908598423
train_iter_loss: 0.12585659325122833
train_iter_loss: 0.21888045966625214
train_iter_loss: 0.23895487189292908
train_iter_loss: 0.13964983820915222
train_iter_loss: 0.18932674825191498
train_iter_loss: 0.3093419075012207
train_iter_loss: 0.273170530796051
train_iter_loss: 0.3378433883190155
train_iter_loss: 0.2763034999370575
train_iter_loss: 0.298281192779541
train_iter_loss: 0.3440607488155365
train_iter_loss: 0.27998408675193787
train_iter_loss: 0.3158986270427704
train_iter_loss: 0.2676350772380829
train_iter_loss: 0.15792249143123627
train_iter_loss: 0.2278037965297699
train_iter_loss: 0.1882689893245697
train_iter_loss: 0.19939377903938293
train_iter_loss: 0.2989794611930847
train_iter_loss: 0.14704419672489166
train_iter_loss: 0.28564178943634033
train_iter_loss: 0.08113878965377808
train_iter_loss: 0.24609653651714325
train_iter_loss: 0.25740543007850647
train_iter_loss: 0.24355483055114746
train_iter_loss: 0.14587810635566711
train_iter_loss: 0.39422380924224854
train_iter_loss: 0.15364108979701996
train_iter_loss: 0.2216455489397049
train_iter_loss: 0.26089027523994446
train_iter_loss: 0.43615129590034485
train_iter_loss: 0.2987663149833679
train_iter_loss: 0.2321588546037674
train_iter_loss: 0.2972836494445801
train_iter_loss: 0.2489609718322754
train_iter_loss: 0.21674908697605133
train loss :0.2597
---------------------
Validation seg loss: 0.3447108718935611 at epoch 777
epoch =    778/  1000, exp = train
train_iter_loss: 0.15737022459506989
train_iter_loss: 0.17145639657974243
train_iter_loss: 0.4303739368915558
train_iter_loss: 0.2805711627006531
train_iter_loss: 0.2202603816986084
train_iter_loss: 0.2480459213256836
train_iter_loss: 0.1813909262418747
train_iter_loss: 0.2747211456298828
train_iter_loss: 0.22917619347572327
train_iter_loss: 0.3309084177017212
train_iter_loss: 0.1677899807691574
train_iter_loss: 0.22310207784175873
train_iter_loss: 0.42556557059288025
train_iter_loss: 0.32449832558631897
train_iter_loss: 0.35749736428260803
train_iter_loss: 0.2023853063583374
train_iter_loss: 0.26199355721473694
train_iter_loss: 0.2001103162765503
train_iter_loss: 0.3467558026313782
train_iter_loss: 0.32687628269195557
train_iter_loss: 0.2682751417160034
train_iter_loss: 0.2959790825843811
train_iter_loss: 0.12520354986190796
train_iter_loss: 0.20315615832805634
train_iter_loss: 0.4522966742515564
train_iter_loss: 0.05278794467449188
train_iter_loss: 0.11462781578302383
train_iter_loss: 0.22802229225635529
train_iter_loss: 0.1963759809732437
train_iter_loss: 0.2817719578742981
train_iter_loss: 0.1878497302532196
train_iter_loss: 0.29746347665786743
train_iter_loss: 0.48563152551651
train_iter_loss: 0.257565975189209
train_iter_loss: 0.21933816373348236
train_iter_loss: 0.40512388944625854
train_iter_loss: 0.3221200108528137
train_iter_loss: 0.3551666736602783
train_iter_loss: 0.22094134986400604
train_iter_loss: 0.296073317527771
train_iter_loss: 0.0999298095703125
train_iter_loss: 0.15091808140277863
train_iter_loss: 0.13836035132408142
train_iter_loss: 0.25439634919166565
train_iter_loss: 0.14752261340618134
train_iter_loss: 0.36769622564315796
train_iter_loss: 0.22922393679618835
train_iter_loss: 0.46678614616394043
train_iter_loss: 0.22798268496990204
train_iter_loss: 0.3566928505897522
train_iter_loss: 0.40332797169685364
train_iter_loss: 0.20655569434165955
train_iter_loss: 0.340350866317749
train_iter_loss: 0.3574320077896118
train_iter_loss: 0.12048186361789703
train_iter_loss: 0.3654724955558777
train_iter_loss: 0.3264940679073334
train_iter_loss: 0.3352433443069458
train_iter_loss: 0.22552433609962463
train_iter_loss: 0.20865698158740997
train_iter_loss: 0.25129544734954834
train_iter_loss: 0.3705032765865326
train_iter_loss: 0.19856667518615723
train_iter_loss: 0.06878145784139633
train_iter_loss: 0.3295195400714874
train_iter_loss: 0.15418380498886108
train_iter_loss: 0.23924465477466583
train_iter_loss: 0.3600965142250061
train_iter_loss: 0.16845671832561493
train_iter_loss: 0.028551984578371048
train_iter_loss: 0.21649892628192902
train_iter_loss: 0.3454848527908325
train_iter_loss: 0.2836648225784302
train_iter_loss: 0.4065444767475128
train_iter_loss: 0.3055398166179657
train_iter_loss: 0.27949532866477966
train_iter_loss: 0.30576327443122864
train_iter_loss: 0.2693065404891968
train_iter_loss: 0.1366177499294281
train_iter_loss: 0.27467575669288635
train_iter_loss: 0.2632145583629608
train_iter_loss: 0.11538274586200714
train_iter_loss: 0.38985675573349
train_iter_loss: 0.25333088636398315
train_iter_loss: 0.3153851628303528
train_iter_loss: 0.295723021030426
train_iter_loss: 0.17072923481464386
train_iter_loss: 0.2060534954071045
train_iter_loss: 0.22813425958156586
train_iter_loss: 0.2629006505012512
train_iter_loss: 0.2228539139032364
train_iter_loss: 0.16899026930332184
train_iter_loss: 0.24725361168384552
train_iter_loss: 0.144717276096344
train_iter_loss: 0.25193244218826294
train_iter_loss: 0.2678351402282715
train_iter_loss: 0.1798923909664154
train_iter_loss: 0.1146143227815628
train_iter_loss: 0.22927284240722656
train_iter_loss: 0.3064171075820923
train loss :0.2584
---------------------
Validation seg loss: 0.35003457926565185 at epoch 778
epoch =    779/  1000, exp = train
train_iter_loss: 0.4062480330467224
train_iter_loss: 0.18991759419441223
train_iter_loss: 0.17502059042453766
train_iter_loss: 0.23214410245418549
train_iter_loss: 0.3482514023780823
train_iter_loss: 0.1348446160554886
train_iter_loss: 0.22919468581676483
train_iter_loss: 0.3412505090236664
train_iter_loss: 0.3882036805152893
train_iter_loss: 0.22671329975128174
train_iter_loss: 0.309806227684021
train_iter_loss: 0.2436998188495636
train_iter_loss: 0.2250891476869583
train_iter_loss: 0.2908647656440735
train_iter_loss: 0.26188698410987854
train_iter_loss: 0.21303944289684296
train_iter_loss: 0.2607777416706085
train_iter_loss: 0.2723126709461212
train_iter_loss: 0.3092123866081238
train_iter_loss: 0.34958136081695557
train_iter_loss: 0.24284155666828156
train_iter_loss: 0.314614474773407
train_iter_loss: 0.22041189670562744
train_iter_loss: 0.5709794163703918
train_iter_loss: 0.26383498311042786
train_iter_loss: 0.364522784948349
train_iter_loss: 0.42617475986480713
train_iter_loss: 0.21250039339065552
train_iter_loss: 0.3226328194141388
train_iter_loss: 0.278506875038147
train_iter_loss: 0.34453147649765015
train_iter_loss: 0.21004056930541992
train_iter_loss: 0.26784345507621765
train_iter_loss: 0.33337289094924927
train_iter_loss: 0.3600195348262787
train_iter_loss: 0.2514517307281494
train_iter_loss: 0.08783665299415588
train_iter_loss: 0.11921947449445724
train_iter_loss: 0.16259443759918213
train_iter_loss: 0.16794703900814056
train_iter_loss: 0.294913113117218
train_iter_loss: 0.2515222132205963
train_iter_loss: 0.2530808448791504
train_iter_loss: 0.19745220243930817
train_iter_loss: 0.2324356883764267
train_iter_loss: 0.424062579870224
train_iter_loss: 0.31094783544540405
train_iter_loss: 0.3537745475769043
train_iter_loss: 0.3056095540523529
train_iter_loss: 0.28239503502845764
train_iter_loss: 0.16953890025615692
train_iter_loss: 0.4264138340950012
train_iter_loss: 0.2901046574115753
train_iter_loss: 0.27064248919487
train_iter_loss: 0.34269922971725464
train_iter_loss: 0.3079659342765808
train_iter_loss: 0.20726267993450165
train_iter_loss: 0.3804070055484772
train_iter_loss: 0.25813162326812744
train_iter_loss: 0.21587328612804413
train_iter_loss: 0.15252390503883362
train_iter_loss: 0.2255280464887619
train_iter_loss: 0.22514063119888306
train_iter_loss: 0.3164755403995514
train_iter_loss: 0.328435480594635
train_iter_loss: 0.2889389395713806
train_iter_loss: 0.19100111722946167
train_iter_loss: 0.1338137686252594
train_iter_loss: 0.12567494809627533
train_iter_loss: 0.22298987209796906
train_iter_loss: 0.31357452273368835
train_iter_loss: 0.2674514055252075
train_iter_loss: 0.14478996396064758
train_iter_loss: 0.32340264320373535
train_iter_loss: 0.19509267807006836
train_iter_loss: 0.10481946170330048
train_iter_loss: 0.21172456443309784
train_iter_loss: 0.19549240171909332
train_iter_loss: 0.3315489590167999
train_iter_loss: 0.21302050352096558
train_iter_loss: 0.3087430000305176
train_iter_loss: 0.17998844385147095
train_iter_loss: 0.30267244577407837
train_iter_loss: 0.2848546504974365
train_iter_loss: 0.22728189826011658
train_iter_loss: 0.30697938799858093
train_iter_loss: 0.39291876554489136
train_iter_loss: 0.24823087453842163
train_iter_loss: 0.24307414889335632
train_iter_loss: 0.23744547367095947
train_iter_loss: 0.22952577471733093
train_iter_loss: 0.3201755881309509
train_iter_loss: 0.18372373282909393
train_iter_loss: 0.37451326847076416
train_iter_loss: 0.13626575469970703
train_iter_loss: 0.3816583752632141
train_iter_loss: 0.10163228958845139
train_iter_loss: 0.20519109070301056
train_iter_loss: 0.288818895816803
train_iter_loss: 0.16536162793636322
train loss :0.2659
---------------------
Validation seg loss: 0.3573485297625357 at epoch 779
epoch =    780/  1000, exp = train
train_iter_loss: 0.2427939474582672
train_iter_loss: 0.15192504227161407
train_iter_loss: 0.4550632834434509
train_iter_loss: 0.3152831792831421
train_iter_loss: 0.2970801591873169
train_iter_loss: 0.1793958991765976
train_iter_loss: 0.19120463728904724
train_iter_loss: 0.379890501499176
train_iter_loss: 0.30770576000213623
train_iter_loss: 0.1636209785938263
train_iter_loss: 0.27491194009780884
train_iter_loss: 0.22318878769874573
train_iter_loss: 0.31615573167800903
train_iter_loss: 0.23857836425304413
train_iter_loss: 0.3001461327075958
train_iter_loss: 0.2295253723859787
train_iter_loss: 0.1389322280883789
train_iter_loss: 0.31048834323883057
train_iter_loss: 0.2410210371017456
train_iter_loss: 0.2497185468673706
train_iter_loss: 0.31822195649147034
train_iter_loss: 0.25478553771972656
train_iter_loss: 0.2825852036476135
train_iter_loss: 0.2404864877462387
train_iter_loss: 0.24911169707775116
train_iter_loss: 0.23389627039432526
train_iter_loss: 0.21017108857631683
train_iter_loss: 0.3056153953075409
train_iter_loss: 0.2841019630432129
train_iter_loss: 0.27740854024887085
train_iter_loss: 0.26247286796569824
train_iter_loss: 0.4288303256034851
train_iter_loss: 0.13359184563159943
train_iter_loss: 0.24487613141536713
train_iter_loss: 0.28356432914733887
train_iter_loss: 0.09985331445932388
train_iter_loss: 0.35097354650497437
train_iter_loss: 0.2050229161977768
train_iter_loss: 0.3080659508705139
train_iter_loss: 0.2402336299419403
train_iter_loss: 0.298991858959198
train_iter_loss: 0.13873495161533356
train_iter_loss: 0.1664370894432068
train_iter_loss: 0.30569449067115784
train_iter_loss: 0.26396965980529785
train_iter_loss: 0.28559431433677673
train_iter_loss: 0.20465563237667084
train_iter_loss: 0.29455554485321045
train_iter_loss: 0.37124305963516235
train_iter_loss: 0.22261367738246918
train_iter_loss: 0.2535703182220459
train_iter_loss: 0.4982718825340271
train_iter_loss: 0.23665951192378998
train_iter_loss: 0.2657163441181183
train_iter_loss: 0.2084265500307083
train_iter_loss: 0.2053251415491104
train_iter_loss: 0.18634791672229767
train_iter_loss: 0.3435409665107727
train_iter_loss: 0.2169773429632187
train_iter_loss: 0.31339719891548157
train_iter_loss: 0.13692283630371094
train_iter_loss: 0.20361573994159698
train_iter_loss: 0.22957108914852142
train_iter_loss: 0.2566433846950531
train_iter_loss: 0.3083908259868622
train_iter_loss: 0.3283078670501709
train_iter_loss: 0.1359979510307312
train_iter_loss: 0.25816792249679565
train_iter_loss: 0.33196255564689636
train_iter_loss: 0.10068204998970032
train_iter_loss: 0.21024569869041443
train_iter_loss: 0.3956824541091919
train_iter_loss: 0.30674418807029724
train_iter_loss: 0.20180639624595642
train_iter_loss: 0.3343846797943115
train_iter_loss: 0.18862284719944
train_iter_loss: 0.15939240157604218
train_iter_loss: 0.2945321500301361
train_iter_loss: 0.11252845823764801
train_iter_loss: 0.22550803422927856
train_iter_loss: 0.1720961481332779
train_iter_loss: 0.22383102774620056
train_iter_loss: 0.2693859040737152
train_iter_loss: 0.14606742560863495
train_iter_loss: 0.14421235024929047
train_iter_loss: 0.33629006147384644
train_iter_loss: 0.2870664894580841
train_iter_loss: 0.18664805591106415
train_iter_loss: 0.33906471729278564
train_iter_loss: 0.27911680936813354
train_iter_loss: 0.33607766032218933
train_iter_loss: 0.41284048557281494
train_iter_loss: 0.2988395094871521
train_iter_loss: 0.3359385132789612
train_iter_loss: 0.2630961835384369
train_iter_loss: 0.2293817698955536
train_iter_loss: 0.19545024633407593
train_iter_loss: 0.2891748249530792
train_iter_loss: 0.29325416684150696
train_iter_loss: 0.19081637263298035
train loss :0.2592
---------------------
Validation seg loss: 0.3708056655914505 at epoch 780
epoch =    781/  1000, exp = train
train_iter_loss: 0.3800172209739685
train_iter_loss: 0.19834335148334503
train_iter_loss: 0.10391865670681
train_iter_loss: 0.1487213522195816
train_iter_loss: 0.315181165933609
train_iter_loss: 0.21508991718292236
train_iter_loss: 0.42729613184928894
train_iter_loss: 0.32537901401519775
train_iter_loss: 0.16419288516044617
train_iter_loss: 0.24803145229816437
train_iter_loss: 0.37325841188430786
train_iter_loss: 0.19447524845600128
train_iter_loss: 0.23650924861431122
train_iter_loss: 0.16529302299022675
train_iter_loss: 0.19567157328128815
train_iter_loss: 0.23588460683822632
train_iter_loss: 0.3784234821796417
train_iter_loss: 0.19700665771961212
train_iter_loss: 0.39822256565093994
train_iter_loss: 0.24931535124778748
train_iter_loss: 0.29635369777679443
train_iter_loss: 0.1544373333454132
train_iter_loss: 0.25280335545539856
train_iter_loss: 0.30593520402908325
train_iter_loss: 0.24286974966526031
train_iter_loss: 0.14356949925422668
train_iter_loss: 0.265849232673645
train_iter_loss: 0.2205876260995865
train_iter_loss: 0.32804015278816223
train_iter_loss: 0.2957065999507904
train_iter_loss: 0.23569725453853607
train_iter_loss: 0.2563694715499878
train_iter_loss: 0.38716864585876465
train_iter_loss: 0.2283123880624771
train_iter_loss: 0.24906419217586517
train_iter_loss: 0.1348903328180313
train_iter_loss: 0.2631717920303345
train_iter_loss: 0.30284175276756287
train_iter_loss: 0.1968628615140915
train_iter_loss: 0.24092203378677368
train_iter_loss: 0.22602714598178864
train_iter_loss: 0.20124700665473938
train_iter_loss: 0.3615112602710724
train_iter_loss: 0.3411575257778168
train_iter_loss: 0.2275444120168686
train_iter_loss: 0.29113486409187317
train_iter_loss: 0.3649025857448578
train_iter_loss: 0.2606904208660126
train_iter_loss: 0.18890340626239777
train_iter_loss: 0.3181760013103485
train_iter_loss: 0.19112615287303925
train_iter_loss: 0.20724347233772278
train_iter_loss: 0.2663112282752991
train_iter_loss: 0.20223543047904968
train_iter_loss: 0.22208157181739807
train_iter_loss: 0.2836115062236786
train_iter_loss: 0.2034502923488617
train_iter_loss: 0.2458115518093109
train_iter_loss: 0.3608294427394867
train_iter_loss: 0.17649142444133759
train_iter_loss: 0.2897002398967743
train_iter_loss: 0.3081304728984833
train_iter_loss: 0.26957911252975464
train_iter_loss: 0.2969464063644409
train_iter_loss: 0.2409059852361679
train_iter_loss: 0.24119268357753754
train_iter_loss: 0.3176901936531067
train_iter_loss: 0.3092561960220337
train_iter_loss: 0.28085923194885254
train_iter_loss: 0.34675392508506775
train_iter_loss: 0.15501774847507477
train_iter_loss: 0.1309524029493332
train_iter_loss: 0.27899131178855896
train_iter_loss: 0.22632083296775818
train_iter_loss: 0.3125832676887512
train_iter_loss: 0.2366802841424942
train_iter_loss: 0.23147903382778168
train_iter_loss: 0.23619066178798676
train_iter_loss: 0.36791226267814636
train_iter_loss: 0.2038830667734146
train_iter_loss: 0.2117270529270172
train_iter_loss: 0.24808068573474884
train_iter_loss: 0.4271043837070465
train_iter_loss: 0.1994454711675644
train_iter_loss: 0.273990273475647
train_iter_loss: 0.18061119318008423
train_iter_loss: 0.35870927572250366
train_iter_loss: 0.1968788504600525
train_iter_loss: 0.3319862484931946
train_iter_loss: 0.19835080206394196
train_iter_loss: 0.3007623851299286
train_iter_loss: 0.27619674801826477
train_iter_loss: 0.2055503875017166
train_iter_loss: 0.27230104804039
train_iter_loss: 0.2663570046424866
train_iter_loss: 0.32502222061157227
train_iter_loss: 0.16575905680656433
train_iter_loss: 0.1553996503353119
train_iter_loss: 0.2001742571592331
train_iter_loss: 0.3197784125804901
train loss :0.2595
---------------------
Validation seg loss: 0.3448532345124854 at epoch 781
epoch =    782/  1000, exp = train
train_iter_loss: 0.4442991316318512
train_iter_loss: 0.22482243180274963
train_iter_loss: 0.21675047278404236
train_iter_loss: 0.2814093828201294
train_iter_loss: 0.1964254230260849
train_iter_loss: 0.21665655076503754
train_iter_loss: 0.22490879893302917
train_iter_loss: 0.2786495089530945
train_iter_loss: 0.4382207691669464
train_iter_loss: 0.23629716038703918
train_iter_loss: 0.23729820549488068
train_iter_loss: 0.20954135060310364
train_iter_loss: 0.20828984677791595
train_iter_loss: 0.31004950404167175
train_iter_loss: 0.34975454211235046
train_iter_loss: 0.23534227907657623
train_iter_loss: 0.19436796009540558
train_iter_loss: 0.1363653689622879
train_iter_loss: 0.42746618390083313
train_iter_loss: 0.3568331301212311
train_iter_loss: 0.2740129232406616
train_iter_loss: 0.16884367167949677
train_iter_loss: 0.10074719041585922
train_iter_loss: 0.3497582674026489
train_iter_loss: 0.20064349472522736
train_iter_loss: 0.32529738545417786
train_iter_loss: 0.1081693172454834
train_iter_loss: 0.23276276886463165
train_iter_loss: 0.1944333016872406
train_iter_loss: 0.2778061330318451
train_iter_loss: 0.14205795526504517
train_iter_loss: 0.16105708479881287
train_iter_loss: 0.317024827003479
train_iter_loss: 0.18597760796546936
train_iter_loss: 0.167818084359169
train_iter_loss: 0.14084084331989288
train_iter_loss: 0.11145797371864319
train_iter_loss: 0.13581423461437225
train_iter_loss: 0.2511464059352875
train_iter_loss: 0.34789013862609863
train_iter_loss: 0.23040889203548431
train_iter_loss: 0.3496146500110626
train_iter_loss: 0.3288278579711914
train_iter_loss: 0.2803729474544525
train_iter_loss: 0.24504795670509338
train_iter_loss: 0.39345285296440125
train_iter_loss: 0.23814848065376282
train_iter_loss: 0.2641626000404358
train_iter_loss: 0.1789897084236145
train_iter_loss: 0.25212034583091736
train_iter_loss: 0.20546500384807587
train_iter_loss: 0.24889661371707916
train_iter_loss: 0.18882308900356293
train_iter_loss: 0.3676810562610626
train_iter_loss: 0.1058962419629097
train_iter_loss: 0.32101619243621826
train_iter_loss: 0.13676762580871582
train_iter_loss: 0.26214897632598877
train_iter_loss: 0.47093865275382996
train_iter_loss: 0.15767642855644226
train_iter_loss: 0.14227987825870514
train_iter_loss: 0.2635461390018463
train_iter_loss: 0.32705995440483093
train_iter_loss: 0.23930995166301727
train_iter_loss: 0.18595881760120392
train_iter_loss: 0.3603590428829193
train_iter_loss: 0.48317962884902954
train_iter_loss: 0.25982147455215454
train_iter_loss: 0.28881558775901794
train_iter_loss: 0.3913009464740753
train_iter_loss: 0.26762300729751587
train_iter_loss: 0.22279678285121918
train_iter_loss: 0.5316872000694275
train_iter_loss: 0.30167636275291443
train_iter_loss: 0.2817894518375397
train_iter_loss: 0.15118089318275452
train_iter_loss: 0.33549293875694275
train_iter_loss: 0.3212537169456482
train_iter_loss: 0.2084130048751831
train_iter_loss: 0.33406439423561096
train_iter_loss: 0.08386807143688202
train_iter_loss: 0.33513545989990234
train_iter_loss: 0.25584089756011963
train_iter_loss: 0.3859540522098541
train_iter_loss: 0.3110644221305847
train_iter_loss: 0.3812348544597626
train_iter_loss: 0.23855331540107727
train_iter_loss: 0.1634080410003662
train_iter_loss: 0.26931315660476685
train_iter_loss: 0.7556581497192383
train_iter_loss: 0.2018294483423233
train_iter_loss: 0.22394675016403198
train_iter_loss: 0.28757497668266296
train_iter_loss: 0.18794125318527222
train_iter_loss: 0.3048413395881653
train_iter_loss: 0.17079418897628784
train_iter_loss: 0.2350032776594162
train_iter_loss: 0.27032461762428284
train_iter_loss: 0.1899738609790802
train_iter_loss: 0.21819347143173218
train loss :0.2647
---------------------
Validation seg loss: 0.3689068658365253 at epoch 782
epoch =    783/  1000, exp = train
train_iter_loss: 0.17269445955753326
train_iter_loss: 0.37598779797554016
train_iter_loss: 0.26468199491500854
train_iter_loss: 0.16022728383541107
train_iter_loss: 0.20270545780658722
train_iter_loss: 0.22654826939105988
train_iter_loss: 0.2874746322631836
train_iter_loss: 0.31605473160743713
train_iter_loss: 0.17930816113948822
train_iter_loss: 0.28763890266418457
train_iter_loss: 0.13565532863140106
train_iter_loss: 0.23800715804100037
train_iter_loss: 0.13354241847991943
train_iter_loss: 0.312009334564209
train_iter_loss: 0.22798748314380646
train_iter_loss: 0.2063617706298828
train_iter_loss: 0.26557213068008423
train_iter_loss: 0.2776065170764923
train_iter_loss: 0.44323641061782837
train_iter_loss: 0.348597913980484
train_iter_loss: 0.3293389081954956
train_iter_loss: 0.3555482029914856
train_iter_loss: 0.221537247300148
train_iter_loss: 0.2138603776693344
train_iter_loss: 0.2841288149356842
train_iter_loss: 0.34091880917549133
train_iter_loss: 0.33620685338974
train_iter_loss: 0.33779364824295044
train_iter_loss: 0.21073628962039948
train_iter_loss: 0.24733148515224457
train_iter_loss: 0.19656960666179657
train_iter_loss: 0.23504766821861267
train_iter_loss: 0.3000662922859192
train_iter_loss: 0.25658300518989563
train_iter_loss: 0.07866872102022171
train_iter_loss: 0.2662935256958008
train_iter_loss: 0.22884181141853333
train_iter_loss: 0.31517037749290466
train_iter_loss: 0.44906511902809143
train_iter_loss: 0.3122212290763855
train_iter_loss: 0.2015630602836609
train_iter_loss: 0.16087885200977325
train_iter_loss: 0.3371102213859558
train_iter_loss: 0.2970666289329529
train_iter_loss: 0.1948351413011551
train_iter_loss: 0.4236438274383545
train_iter_loss: 0.17064477503299713
train_iter_loss: 0.08184932172298431
train_iter_loss: 0.3341836631298065
train_iter_loss: 0.26018741726875305
train_iter_loss: 0.37594181299209595
train_iter_loss: 0.2878466248512268
train_iter_loss: 0.31829142570495605
train_iter_loss: 0.3550260365009308
train_iter_loss: 0.22451835870742798
train_iter_loss: 0.21026645600795746
train_iter_loss: 0.5101953744888306
train_iter_loss: 0.31595978140830994
train_iter_loss: 0.23074518144130707
train_iter_loss: 0.294253945350647
train_iter_loss: 0.23648865520954132
train_iter_loss: 0.14928172528743744
train_iter_loss: 0.31106463074684143
train_iter_loss: 0.12324197590351105
train_iter_loss: 0.19928666949272156
train_iter_loss: 0.21544095873832703
train_iter_loss: 0.24008400738239288
train_iter_loss: 0.20842666923999786
train_iter_loss: 0.23270833492279053
train_iter_loss: 0.24812555313110352
train_iter_loss: 0.2549534738063812
train_iter_loss: 0.1974501758813858
train_iter_loss: 0.28198710083961487
train_iter_loss: 0.3065386116504669
train_iter_loss: 0.2390771508216858
train_iter_loss: 0.3287450075149536
train_iter_loss: 0.33057931065559387
train_iter_loss: 0.11308157444000244
train_iter_loss: 0.29781267046928406
train_iter_loss: 0.3585059344768524
train_iter_loss: 0.3198758363723755
train_iter_loss: 0.03843744099140167
train_iter_loss: 0.21351470053195953
train_iter_loss: 0.3653002679347992
train_iter_loss: 0.3279436528682709
train_iter_loss: 0.35335689783096313
train_iter_loss: 0.42636293172836304
train_iter_loss: 0.2900680899620056
train_iter_loss: 0.2667997181415558
train_iter_loss: 0.3546549081802368
train_iter_loss: 0.19554349780082703
train_iter_loss: 0.3570810556411743
train_iter_loss: 0.28987693786621094
train_iter_loss: 0.12509816884994507
train_iter_loss: 0.3581472337245941
train_iter_loss: 0.21031029522418976
train_iter_loss: 0.3131203055381775
train_iter_loss: 0.1641159951686859
train_iter_loss: 0.09257758408784866
train_iter_loss: 0.2125903218984604
train loss :0.2658
---------------------
Validation seg loss: 0.34714103370623767 at epoch 783
epoch =    784/  1000, exp = train
train_iter_loss: 0.337948203086853
train_iter_loss: 0.13664041459560394
train_iter_loss: 0.1645030379295349
train_iter_loss: 0.2522026598453522
train_iter_loss: 0.23589354753494263
train_iter_loss: 0.29294049739837646
train_iter_loss: 0.26072755455970764
train_iter_loss: 0.3158756494522095
train_iter_loss: 0.2313116490840912
train_iter_loss: 0.2596120834350586
train_iter_loss: 0.21702027320861816
train_iter_loss: 0.2017427384853363
train_iter_loss: 0.18359971046447754
train_iter_loss: 0.25892874598503113
train_iter_loss: 0.3317764699459076
train_iter_loss: 0.3280034065246582
train_iter_loss: 0.23700164258480072
train_iter_loss: 0.3325079083442688
train_iter_loss: 0.2793668210506439
train_iter_loss: 0.17190371453762054
train_iter_loss: 0.30642151832580566
train_iter_loss: 0.1489425003528595
train_iter_loss: 0.27245032787323
train_iter_loss: 0.11828813701868057
train_iter_loss: 0.2713310718536377
train_iter_loss: 0.21528592705726624
train_iter_loss: 0.3411347270011902
train_iter_loss: 0.23030038177967072
train_iter_loss: 0.44747769832611084
train_iter_loss: 0.272413969039917
train_iter_loss: 0.20997628569602966
train_iter_loss: 0.2617616653442383
train_iter_loss: 0.3360300064086914
train_iter_loss: 0.28990092873573303
train_iter_loss: 0.34416869282722473
train_iter_loss: 0.3326917290687561
train_iter_loss: 0.32576248049736023
train_iter_loss: 0.46209874749183655
train_iter_loss: 0.1926165372133255
train_iter_loss: 0.25488045811653137
train_iter_loss: 0.2968348264694214
train_iter_loss: 0.17232738435268402
train_iter_loss: 0.22268104553222656
train_iter_loss: 0.31559962034225464
train_iter_loss: 0.14443770051002502
train_iter_loss: 0.38792189955711365
train_iter_loss: 0.29141750931739807
train_iter_loss: 0.26717132329940796
train_iter_loss: 0.36099007725715637
train_iter_loss: 0.2104233205318451
train_iter_loss: 0.16019438207149506
train_iter_loss: 0.2051820456981659
train_iter_loss: 0.24735060334205627
train_iter_loss: 0.2842436134815216
train_iter_loss: 0.31005293130874634
train_iter_loss: 0.14433877170085907
train_iter_loss: 0.27964499592781067
train_iter_loss: 0.2341334968805313
train_iter_loss: 0.3410460650920868
train_iter_loss: 0.2105569988489151
train_iter_loss: 0.17241844534873962
train_iter_loss: 0.0839746966958046
train_iter_loss: 0.1993168741464615
train_iter_loss: 0.218724325299263
train_iter_loss: 0.16773943603038788
train_iter_loss: 0.34127405285835266
train_iter_loss: 0.2636467516422272
train_iter_loss: 0.3250578045845032
train_iter_loss: 0.10722579061985016
train_iter_loss: 0.3759118318557739
train_iter_loss: 0.4066459536552429
train_iter_loss: 0.2703079283237457
train_iter_loss: 0.3831314146518707
train_iter_loss: 0.23061895370483398
train_iter_loss: 0.36745256185531616
train_iter_loss: 0.1838054358959198
train_iter_loss: 0.21538998186588287
train_iter_loss: 0.40185317397117615
train_iter_loss: 0.286459743976593
train_iter_loss: 0.13572636246681213
train_iter_loss: 0.15838724374771118
train_iter_loss: 0.22565127909183502
train_iter_loss: 0.29886671900749207
train_iter_loss: 0.15880437195301056
train_iter_loss: 0.33884501457214355
train_iter_loss: 0.2370629608631134
train_iter_loss: 0.20825155079364777
train_iter_loss: 0.22067828476428986
train_iter_loss: 0.11598043143749237
train_iter_loss: 0.33969980478286743
train_iter_loss: 0.2153530865907669
train_iter_loss: 0.33944419026374817
train_iter_loss: 0.27010801434516907
train_iter_loss: 0.16511313617229462
train_iter_loss: 0.48931241035461426
train_iter_loss: 0.32704541087150574
train_iter_loss: 0.2098308950662613
train_iter_loss: 0.2882072925567627
train_iter_loss: 0.3121265172958374
train_iter_loss: 0.33016806840896606
train loss :0.2640
---------------------
Validation seg loss: 0.345664555365044 at epoch 784
epoch =    785/  1000, exp = train
train_iter_loss: 0.20365220308303833
train_iter_loss: 0.3303928077220917
train_iter_loss: 0.2502887547016144
train_iter_loss: 0.15308012068271637
train_iter_loss: 0.04128333181142807
train_iter_loss: 0.28258660435676575
train_iter_loss: 0.24601541459560394
train_iter_loss: 0.2851291000843048
train_iter_loss: 0.22383306920528412
train_iter_loss: 0.2289898544549942
train_iter_loss: 0.34042888879776
train_iter_loss: 0.32610395550727844
train_iter_loss: 0.232717826962471
train_iter_loss: 0.23631276190280914
train_iter_loss: 0.3204540014266968
train_iter_loss: 0.39884355664253235
train_iter_loss: 0.11880945414304733
train_iter_loss: 0.19399985671043396
train_iter_loss: 0.17863532900810242
train_iter_loss: 0.17903576791286469
train_iter_loss: 0.1998099684715271
train_iter_loss: 0.37506136298179626
train_iter_loss: 0.19364537298679352
train_iter_loss: 0.32310473918914795
train_iter_loss: 0.18491099774837494
train_iter_loss: 0.2948608696460724
train_iter_loss: 0.18827173113822937
train_iter_loss: 0.2439734935760498
train_iter_loss: 0.2393534779548645
train_iter_loss: 0.21526868641376495
train_iter_loss: 0.307699978351593
train_iter_loss: 0.06894692778587341
train_iter_loss: 0.3709154725074768
train_iter_loss: 0.2925049662590027
train_iter_loss: 0.2360548973083496
train_iter_loss: 0.25026267766952515
train_iter_loss: 0.26293596625328064
train_iter_loss: 0.16152849793434143
train_iter_loss: 0.18659795820713043
train_iter_loss: 0.33799779415130615
train_iter_loss: 0.19301998615264893
train_iter_loss: 0.4142272174358368
train_iter_loss: 0.4513903260231018
train_iter_loss: 0.2594205141067505
train_iter_loss: 0.25691384077072144
train_iter_loss: 0.20893888175487518
train_iter_loss: 0.19170132279396057
train_iter_loss: 0.3375939130783081
train_iter_loss: 0.30118992924690247
train_iter_loss: 0.2343197762966156
train_iter_loss: 0.20169754326343536
train_iter_loss: 0.07335533201694489
train_iter_loss: 0.1957727074623108
train_iter_loss: 0.49222028255462646
train_iter_loss: 0.17760862410068512
train_iter_loss: 0.2748095989227295
train_iter_loss: 0.1976565718650818
train_iter_loss: 0.3459222614765167
train_iter_loss: 0.3911648392677307
train_iter_loss: 0.19416582584381104
train_iter_loss: 0.26832497119903564
train_iter_loss: 0.3488312363624573
train_iter_loss: 0.31172651052474976
train_iter_loss: 0.2926280200481415
train_iter_loss: 0.48612692952156067
train_iter_loss: 0.2673858404159546
train_iter_loss: 0.2759920358657837
train_iter_loss: 0.2582240700721741
train_iter_loss: 0.28844380378723145
train_iter_loss: 0.3510955274105072
train_iter_loss: 0.18360719084739685
train_iter_loss: 0.3903000056743622
train_iter_loss: 0.290460467338562
train_iter_loss: 0.22656308114528656
train_iter_loss: 0.31916001439094543
train_iter_loss: 0.30999478697776794
train_iter_loss: 0.3030717372894287
train_iter_loss: 0.20274710655212402
train_iter_loss: 0.16527429223060608
train_iter_loss: 0.1506289690732956
train_iter_loss: 0.20972086489200592
train_iter_loss: 0.353786438703537
train_iter_loss: 0.1586204320192337
train_iter_loss: 0.2889723777770996
train_iter_loss: 0.39548131823539734
train_iter_loss: 0.14792150259017944
train_iter_loss: 0.10320346802473068
train_iter_loss: 0.4223134517669678
train_iter_loss: 0.2836226224899292
train_iter_loss: 0.14368879795074463
train_iter_loss: 0.25477856397628784
train_iter_loss: 0.31992897391319275
train_iter_loss: 0.28089389204978943
train_iter_loss: 0.33983826637268066
train_iter_loss: 0.23333600163459778
train_iter_loss: 0.264123797416687
train_iter_loss: 0.22894465923309326
train_iter_loss: 0.25317510962486267
train_iter_loss: 0.27354639768600464
train_iter_loss: 0.3724953234195709
train loss :0.2638
---------------------
Validation seg loss: 0.35510853901154027 at epoch 785
epoch =    786/  1000, exp = train
train_iter_loss: 0.18451550602912903
train_iter_loss: 0.262322336435318
train_iter_loss: 0.31824594736099243
train_iter_loss: 0.3752487599849701
train_iter_loss: 0.3204907178878784
train_iter_loss: 0.09863431006669998
train_iter_loss: 0.3140350878238678
train_iter_loss: 0.27888643741607666
train_iter_loss: 0.21697884798049927
train_iter_loss: 0.1554846465587616
train_iter_loss: 0.17430295050144196
train_iter_loss: 0.30457422137260437
train_iter_loss: 0.29704299569129944
train_iter_loss: 0.32305580377578735
train_iter_loss: 0.12260404974222183
train_iter_loss: 0.3645104765892029
train_iter_loss: 0.2958250045776367
train_iter_loss: 0.3084317147731781
train_iter_loss: 0.18362699449062347
train_iter_loss: 0.35845640301704407
train_iter_loss: 0.31845277547836304
train_iter_loss: 0.22726711630821228
train_iter_loss: 0.22156678140163422
train_iter_loss: 0.2831708788871765
train_iter_loss: 0.27864280343055725
train_iter_loss: 0.2388283610343933
train_iter_loss: 0.35676002502441406
train_iter_loss: 0.1407010406255722
train_iter_loss: 0.30396178364753723
train_iter_loss: 0.2110830694437027
train_iter_loss: 0.3048582375049591
train_iter_loss: 0.32575586438179016
train_iter_loss: 0.1964387148618698
train_iter_loss: 0.3598582148551941
train_iter_loss: 0.190357506275177
train_iter_loss: 0.3828265964984894
train_iter_loss: 0.11267019808292389
train_iter_loss: 0.22122754156589508
train_iter_loss: 0.17528538405895233
train_iter_loss: 0.4665871858596802
train_iter_loss: 0.11301319301128387
train_iter_loss: 0.2884742319583893
train_iter_loss: 0.30002352595329285
train_iter_loss: 0.13969457149505615
train_iter_loss: 0.2826959490776062
train_iter_loss: 0.29491716623306274
train_iter_loss: 0.26381373405456543
train_iter_loss: 0.28556543588638306
train_iter_loss: 0.23633135855197906
train_iter_loss: 0.2695005238056183
train_iter_loss: 0.11618822813034058
train_iter_loss: 0.27372607588768005
train_iter_loss: 0.3499704897403717
train_iter_loss: 0.3172867000102997
train_iter_loss: 0.24964267015457153
train_iter_loss: 0.26480579376220703
train_iter_loss: 0.15396754443645477
train_iter_loss: 0.3179779052734375
train_iter_loss: 0.33596673607826233
train_iter_loss: 0.11030996590852737
train_iter_loss: 0.19040074944496155
train_iter_loss: 0.23964853584766388
train_iter_loss: 0.32984381914138794
train_iter_loss: 0.2245190143585205
train_iter_loss: 0.3095700740814209
train_iter_loss: 0.25093162059783936
train_iter_loss: 0.24796970188617706
train_iter_loss: 0.1766635775566101
train_iter_loss: 0.16897955536842346
train_iter_loss: 0.21959182620048523
train_iter_loss: 0.09253991395235062
train_iter_loss: 0.29472535848617554
train_iter_loss: 0.23377321660518646
train_iter_loss: 0.2765938341617584
train_iter_loss: 0.33274441957473755
train_iter_loss: 0.24233272671699524
train_iter_loss: 0.27319732308387756
train_iter_loss: 0.2981477677822113
train_iter_loss: 0.3488275110721588
train_iter_loss: 0.23994535207748413
train_iter_loss: 0.31794893741607666
train_iter_loss: 0.17939460277557373
train_iter_loss: 0.42827457189559937
train_iter_loss: 0.15341192483901978
train_iter_loss: 0.3735840916633606
train_iter_loss: 0.3184160888195038
train_iter_loss: 0.16769066452980042
train_iter_loss: 0.35404062271118164
train_iter_loss: 0.5270790457725525
train_iter_loss: 0.2006392478942871
train_iter_loss: 0.3594604432582855
train_iter_loss: 0.18322071433067322
train_iter_loss: 0.25383806228637695
train_iter_loss: 0.1239420548081398
train_iter_loss: 0.3220362961292267
train_iter_loss: 0.23951144516468048
train_iter_loss: 0.19097931683063507
train_iter_loss: 0.33631768822669983
train_iter_loss: 0.2165490984916687
train_iter_loss: 0.259868323802948
train loss :0.2630
---------------------
Validation seg loss: 0.36476786216756085 at epoch 786
epoch =    787/  1000, exp = train
train_iter_loss: 0.19452771544456482
train_iter_loss: 0.3798695504665375
train_iter_loss: 0.22278271615505219
train_iter_loss: 0.2988121509552002
train_iter_loss: 0.12334798276424408
train_iter_loss: 0.11461802572011948
train_iter_loss: 0.1646486222743988
train_iter_loss: 0.2671881318092346
train_iter_loss: 0.3897833228111267
train_iter_loss: 0.32675808668136597
train_iter_loss: 0.323306679725647
train_iter_loss: 0.21659882366657257
train_iter_loss: 0.1655646413564682
train_iter_loss: 0.4297369420528412
train_iter_loss: 0.28637248277664185
train_iter_loss: 0.26101866364479065
train_iter_loss: 0.20202720165252686
train_iter_loss: 0.2458617091178894
train_iter_loss: 0.3006611168384552
train_iter_loss: 0.31895047426223755
train_iter_loss: 0.17461377382278442
train_iter_loss: 0.33535853028297424
train_iter_loss: 0.26417675614356995
train_iter_loss: 0.28777799010276794
train_iter_loss: 0.18114714324474335
train_iter_loss: 0.23304061591625214
train_iter_loss: 0.3852459788322449
train_iter_loss: 0.2509477436542511
train_iter_loss: 0.14808012545108795
train_iter_loss: 0.2356482297182083
train_iter_loss: 0.19783541560173035
train_iter_loss: 0.19241677224636078
train_iter_loss: 0.15931232273578644
train_iter_loss: 0.2555926442146301
train_iter_loss: 0.25433990359306335
train_iter_loss: 0.24112945795059204
train_iter_loss: 0.2657059133052826
train_iter_loss: 0.29010459780693054
train_iter_loss: 0.3431156873703003
train_iter_loss: 0.17987528443336487
train_iter_loss: 0.3284790515899658
train_iter_loss: 0.26337578892707825
train_iter_loss: 0.1921737641096115
train_iter_loss: 0.392532616853714
train_iter_loss: 0.10150020569562912
train_iter_loss: 0.22979581356048584
train_iter_loss: 0.27199679613113403
train_iter_loss: 0.3013918697834015
train_iter_loss: 0.377932071685791
train_iter_loss: 0.30189642310142517
train_iter_loss: 0.31439560651779175
train_iter_loss: 0.127134308218956
train_iter_loss: 0.1946626901626587
train_iter_loss: 0.15600912272930145
train_iter_loss: 0.4245660603046417
train_iter_loss: 0.24833834171295166
train_iter_loss: 0.18024174869060516
train_iter_loss: 0.1361227184534073
train_iter_loss: 0.39751699566841125
train_iter_loss: 0.2234649807214737
train_iter_loss: 0.49398061633110046
train_iter_loss: 0.155457004904747
train_iter_loss: 0.2415797859430313
train_iter_loss: 0.20842117071151733
train_iter_loss: 0.18056486546993256
train_iter_loss: 0.4831274151802063
train_iter_loss: 0.2884354889392853
train_iter_loss: 0.10928831249475479
train_iter_loss: 0.20458121597766876
train_iter_loss: 0.24027331173419952
train_iter_loss: 0.31640690565109253
train_iter_loss: 0.22302789986133575
train_iter_loss: 0.21548612415790558
train_iter_loss: 0.34601008892059326
train_iter_loss: 0.47679242491722107
train_iter_loss: 0.3876645267009735
train_iter_loss: 0.2552781403064728
train_iter_loss: 0.2336219698190689
train_iter_loss: 0.41803115606307983
train_iter_loss: 0.24580813944339752
train_iter_loss: 0.2908046245574951
train_iter_loss: 0.26987308263778687
train_iter_loss: 0.3417198061943054
train_iter_loss: 0.3917396068572998
train_iter_loss: 0.24438026547431946
train_iter_loss: 0.1874830573797226
train_iter_loss: 0.20572017133235931
train_iter_loss: 0.2264886349439621
train_iter_loss: 0.20929157733917236
train_iter_loss: 0.4406995475292206
train_iter_loss: 0.10698805004358292
train_iter_loss: 0.2669750452041626
train_iter_loss: 0.15549619495868683
train_iter_loss: 0.2664587199687958
train_iter_loss: 0.5609514117240906
train_iter_loss: 0.35749000310897827
train_iter_loss: 0.14449244737625122
train_iter_loss: 0.21295103430747986
train_iter_loss: 0.2067323923110962
train_iter_loss: 0.17930814623832703
train loss :0.2652
---------------------
Validation seg loss: 0.3554851844388429 at epoch 787
epoch =    788/  1000, exp = train
train_iter_loss: 0.31188496947288513
train_iter_loss: 0.26081302762031555
train_iter_loss: 0.3288384675979614
train_iter_loss: 0.31216350197792053
train_iter_loss: 0.2788945138454437
train_iter_loss: 0.12712809443473816
train_iter_loss: 0.29375889897346497
train_iter_loss: 0.23496969044208527
train_iter_loss: 0.10765040665864944
train_iter_loss: 0.3418399393558502
train_iter_loss: 0.2977030575275421
train_iter_loss: 0.30847859382629395
train_iter_loss: 0.1378876119852066
train_iter_loss: 0.15784986317157745
train_iter_loss: 0.34255144000053406
train_iter_loss: 0.33057624101638794
train_iter_loss: 0.2570631504058838
train_iter_loss: 0.2697947025299072
train_iter_loss: 0.1581585705280304
train_iter_loss: 0.22245264053344727
train_iter_loss: 0.2553795874118805
train_iter_loss: 0.11012818664312363
train_iter_loss: 0.31441953778266907
train_iter_loss: 0.18203099071979523
train_iter_loss: 0.20279596745967865
train_iter_loss: 0.375222772359848
train_iter_loss: 0.2759460210800171
train_iter_loss: 0.29761016368865967
train_iter_loss: 0.33146071434020996
train_iter_loss: 0.4538556635379791
train_iter_loss: 0.20491524040699005
train_iter_loss: 0.20088663697242737
train_iter_loss: 0.19565023481845856
train_iter_loss: 0.19018681347370148
train_iter_loss: 0.2914547622203827
train_iter_loss: 0.2546967566013336
train_iter_loss: 0.36220410466194153
train_iter_loss: 0.29406335949897766
train_iter_loss: 0.11431059241294861
train_iter_loss: 0.3153047263622284
train_iter_loss: 0.1913800984621048
train_iter_loss: 0.24178779125213623
train_iter_loss: 0.13198187947273254
train_iter_loss: 0.17676420509815216
train_iter_loss: 0.2590605318546295
train_iter_loss: 0.28126776218414307
train_iter_loss: 0.29470348358154297
train_iter_loss: 0.1165003553032875
train_iter_loss: 0.22535693645477295
train_iter_loss: 0.41858911514282227
train_iter_loss: 0.20757842063903809
train_iter_loss: 0.11310317367315292
train_iter_loss: 0.2086460292339325
train_iter_loss: 0.2036990076303482
train_iter_loss: 0.29242008924484253
train_iter_loss: 0.283676415681839
train_iter_loss: 0.24608182907104492
train_iter_loss: 0.2778241038322449
train_iter_loss: 0.30706918239593506
train_iter_loss: 0.3315945267677307
train_iter_loss: 0.29388973116874695
train_iter_loss: 0.3120695948600769
train_iter_loss: 0.23597612977027893
train_iter_loss: 0.18953630328178406
train_iter_loss: 0.23973822593688965
train_iter_loss: 0.09215433895587921
train_iter_loss: 0.46184948086738586
train_iter_loss: 0.35837283730506897
train_iter_loss: 0.3424624800682068
train_iter_loss: 0.32463812828063965
train_iter_loss: 0.42160725593566895
train_iter_loss: 0.08622358739376068
train_iter_loss: 0.21964164078235626
train_iter_loss: 0.301973432302475
train_iter_loss: 0.2676665782928467
train_iter_loss: 0.3514188826084137
train_iter_loss: 0.13798600435256958
train_iter_loss: 0.14118875563144684
train_iter_loss: 0.20200121402740479
train_iter_loss: 0.1912185102701187
train_iter_loss: 0.24370546638965607
train_iter_loss: 0.3235548436641693
train_iter_loss: 0.30110692977905273
train_iter_loss: 0.13720768690109253
train_iter_loss: 0.25150954723358154
train_iter_loss: 0.112459696829319
train_iter_loss: 0.23122742772102356
train_iter_loss: 0.44104596972465515
train_iter_loss: 0.2459099292755127
train_iter_loss: 0.27248647809028625
train_iter_loss: 0.22595809400081635
train_iter_loss: 0.20279642939567566
train_iter_loss: 0.2395099401473999
train_iter_loss: 0.16753746569156647
train_iter_loss: 0.2578884959220886
train_iter_loss: 0.20820558071136475
train_iter_loss: 0.388669490814209
train_iter_loss: 0.3241724967956543
train_iter_loss: 0.23231840133666992
train_iter_loss: 0.2625177502632141
train loss :0.2562
---------------------
Validation seg loss: 0.3469046257383559 at epoch 788
epoch =    789/  1000, exp = train
train_iter_loss: 0.25622865557670593
train_iter_loss: 0.3051374852657318
train_iter_loss: 0.26324987411499023
train_iter_loss: 0.2747490704059601
train_iter_loss: 0.3947908878326416
train_iter_loss: 0.2033475935459137
train_iter_loss: 0.24348041415214539
train_iter_loss: 0.3192225992679596
train_iter_loss: 0.24876512587070465
train_iter_loss: 0.32735341787338257
train_iter_loss: 0.3821551501750946
train_iter_loss: 0.21329258382320404
train_iter_loss: 0.2712506353855133
train_iter_loss: 0.17544467747211456
train_iter_loss: 0.29223671555519104
train_iter_loss: 0.16659823060035706
train_iter_loss: 0.30660948157310486
train_iter_loss: 0.14067339897155762
train_iter_loss: 0.17400296032428741
train_iter_loss: 0.25537189841270447
train_iter_loss: 0.2668885886669159
train_iter_loss: 0.19131404161453247
train_iter_loss: 0.17180542647838593
train_iter_loss: 0.3240028917789459
train_iter_loss: 0.3111710846424103
train_iter_loss: 0.1901499480009079
train_iter_loss: 0.08124642074108124
train_iter_loss: 0.17166569828987122
train_iter_loss: 0.20657092332839966
train_iter_loss: 0.2941414415836334
train_iter_loss: 0.44497403502464294
train_iter_loss: 0.31228575110435486
train_iter_loss: 0.22753961384296417
train_iter_loss: 0.37189266085624695
train_iter_loss: 0.530025839805603
train_iter_loss: 0.16182851791381836
train_iter_loss: 0.232487753033638
train_iter_loss: 0.26310858130455017
train_iter_loss: 0.2870290279388428
train_iter_loss: 0.22268791496753693
train_iter_loss: 0.20104274153709412
train_iter_loss: 0.22411535680294037
train_iter_loss: 0.32636502385139465
train_iter_loss: 0.1646035611629486
train_iter_loss: 0.11120613664388657
train_iter_loss: 0.15946033596992493
train_iter_loss: 0.2576710283756256
train_iter_loss: 0.12440400570631027
train_iter_loss: 0.3688053786754608
train_iter_loss: 0.25961026549339294
train_iter_loss: 0.26133421063423157
train_iter_loss: 0.28687846660614014
train_iter_loss: 0.3735139071941376
train_iter_loss: 0.34111520648002625
train_iter_loss: 0.19860967993736267
train_iter_loss: 0.3242567479610443
train_iter_loss: 0.25423458218574524
train_iter_loss: 0.22489477694034576
train_iter_loss: 0.1704680472612381
train_iter_loss: 0.17247089743614197
train_iter_loss: 0.2586747407913208
train_iter_loss: 0.24510543048381805
train_iter_loss: 0.16453498601913452
train_iter_loss: 0.3330715298652649
train_iter_loss: 0.279418021440506
train_iter_loss: 0.2013031542301178
train_iter_loss: 0.28005868196487427
train_iter_loss: 0.24798376858234406
train_iter_loss: 0.32827523350715637
train_iter_loss: 0.17449253797531128
train_iter_loss: 0.20436784625053406
train_iter_loss: 0.3408646285533905
train_iter_loss: 0.4848708212375641
train_iter_loss: 0.22950910031795502
train_iter_loss: 0.3265010416507721
train_iter_loss: 0.24617932736873627
train_iter_loss: 0.12071061134338379
train_iter_loss: 0.21305324137210846
train_iter_loss: 0.27346691489219666
train_iter_loss: 0.28788018226623535
train_iter_loss: 0.28591832518577576
train_iter_loss: 0.40849751234054565
train_iter_loss: 0.25698140263557434
train_iter_loss: 0.30419662594795227
train_iter_loss: 0.22076009213924408
train_iter_loss: 0.26531118154525757
train_iter_loss: 0.3187711238861084
train_iter_loss: 0.27700114250183105
train_iter_loss: 0.08402706682682037
train_iter_loss: 0.23860682547092438
train_iter_loss: 0.33556830883026123
train_iter_loss: 0.3555421233177185
train_iter_loss: 0.24907250702381134
train_iter_loss: 0.221201553940773
train_iter_loss: 0.31273359060287476
train_iter_loss: 0.29487481713294983
train_iter_loss: 0.33396631479263306
train_iter_loss: 0.21198652684688568
train_iter_loss: 0.1635887324810028
train_iter_loss: 0.29410678148269653
train loss :0.2621
---------------------
Validation seg loss: 0.3654332561945578 at epoch 789
epoch =    790/  1000, exp = train
train_iter_loss: 0.44998520612716675
train_iter_loss: 0.34784090518951416
train_iter_loss: 0.3514721691608429
train_iter_loss: 0.321047842502594
train_iter_loss: 0.34803104400634766
train_iter_loss: 0.21270345151424408
train_iter_loss: 0.2887604832649231
train_iter_loss: 0.1922246813774109
train_iter_loss: 0.4220774173736572
train_iter_loss: 0.18471494317054749
train_iter_loss: 0.19073401391506195
train_iter_loss: 0.3253921866416931
train_iter_loss: 0.4447266161441803
train_iter_loss: 0.2396213859319687
train_iter_loss: 0.27456796169281006
train_iter_loss: 0.21993690729141235
train_iter_loss: 0.15470078587532043
train_iter_loss: 0.2805081009864807
train_iter_loss: 0.17953091859817505
train_iter_loss: 0.3457290232181549
train_iter_loss: 0.25926241278648376
train_iter_loss: 0.26682475209236145
train_iter_loss: 0.22396132349967957
train_iter_loss: 0.20614886283874512
train_iter_loss: 0.3651041090488434
train_iter_loss: 0.19462718069553375
train_iter_loss: 0.36237308382987976
train_iter_loss: 0.22154736518859863
train_iter_loss: 0.1059124693274498
train_iter_loss: 0.17052528262138367
train_iter_loss: 0.23967097699642181
train_iter_loss: 0.18991006910800934
train_iter_loss: 0.3409765660762787
train_iter_loss: 0.29290318489074707
train_iter_loss: 0.2954216003417969
train_iter_loss: 0.21383732557296753
train_iter_loss: 0.44226905703544617
train_iter_loss: 0.38682010769844055
train_iter_loss: 0.36272409558296204
train_iter_loss: 0.2576073706150055
train_iter_loss: 0.33145254850387573
train_iter_loss: 0.3692408800125122
train_iter_loss: 0.24441517889499664
train_iter_loss: 0.20978441834449768
train_iter_loss: 0.35893043875694275
train_iter_loss: 0.4444993734359741
train_iter_loss: 0.25832629203796387
train_iter_loss: 0.30643415451049805
train_iter_loss: 0.29218730330467224
train_iter_loss: 0.0725913792848587
train_iter_loss: 0.2196080982685089
train_iter_loss: 0.14468206465244293
train_iter_loss: 0.3753206431865692
train_iter_loss: 0.09747129678726196
train_iter_loss: 0.10444550961256027
train_iter_loss: 0.38640129566192627
train_iter_loss: 0.1749761551618576
train_iter_loss: 0.29967403411865234
train_iter_loss: 0.27594229578971863
train_iter_loss: 0.19785934686660767
train_iter_loss: 0.12041828036308289
train_iter_loss: 0.18723833560943604
train_iter_loss: 0.3544903099536896
train_iter_loss: 0.23971030116081238
train_iter_loss: 0.17506523430347443
train_iter_loss: 0.18114031851291656
train_iter_loss: 0.20497854053974152
train_iter_loss: 0.28191637992858887
train_iter_loss: 0.387090265750885
train_iter_loss: 0.2648787200450897
train_iter_loss: 0.22307991981506348
train_iter_loss: 0.23026533424854279
train_iter_loss: 0.19034633040428162
train_iter_loss: 0.22027789056301117
train_iter_loss: 0.25740310549736023
train_iter_loss: 0.27730506658554077
train_iter_loss: 0.11141250282526016
train_iter_loss: 0.21661129593849182
train_iter_loss: 0.22452816367149353
train_iter_loss: 0.3311603367328644
train_iter_loss: 0.20739537477493286
train_iter_loss: 0.20655500888824463
train_iter_loss: 0.2898638844490051
train_iter_loss: 0.41340675950050354
train_iter_loss: 0.2269945740699768
train_iter_loss: 0.3506046533584595
train_iter_loss: 0.26254111528396606
train_iter_loss: 0.2228022664785385
train_iter_loss: 0.22141622006893158
train_iter_loss: 0.2749244272708893
train_iter_loss: 0.42690831422805786
train_iter_loss: 0.1749931275844574
train_iter_loss: 0.251436322927475
train_iter_loss: 0.22568532824516296
train_iter_loss: 0.3028605282306671
train_iter_loss: 0.08052381128072739
train_iter_loss: 0.3135460913181305
train_iter_loss: 0.3120845854282379
train_iter_loss: 0.17604580521583557
train_iter_loss: 0.18938499689102173
train loss :0.2641
---------------------
Validation seg loss: 0.3421459768037751 at epoch 790
********************
best_val_epoch_loss:  0.3421459768037751
MODEL UPDATED
epoch =    791/  1000, exp = train
train_iter_loss: 0.14308105409145355
train_iter_loss: 0.3963867425918579
train_iter_loss: 0.28755179047584534
train_iter_loss: 0.183494433760643
train_iter_loss: 0.09380917251110077
train_iter_loss: 0.359824538230896
train_iter_loss: 0.4512292444705963
train_iter_loss: 0.30650460720062256
train_iter_loss: 0.29104605317115784
train_iter_loss: 0.315039724111557
train_iter_loss: 0.18183690309524536
train_iter_loss: 0.3148341774940491
train_iter_loss: 0.24329255521297455
train_iter_loss: 0.1252620965242386
train_iter_loss: 0.24778799712657928
train_iter_loss: 0.22875641286373138
train_iter_loss: 0.30111318826675415
train_iter_loss: 0.24862030148506165
train_iter_loss: 0.269810289144516
train_iter_loss: 0.21980991959571838
train_iter_loss: 0.36624592542648315
train_iter_loss: 0.3732694983482361
train_iter_loss: 0.2435609996318817
train_iter_loss: 0.17478135228157043
train_iter_loss: 0.3086687922477722
train_iter_loss: 0.1907256543636322
train_iter_loss: 0.11708618700504303
train_iter_loss: 0.4523307681083679
train_iter_loss: 0.40137165784835815
train_iter_loss: 0.188262939453125
train_iter_loss: 0.32424792647361755
train_iter_loss: 0.20751512050628662
train_iter_loss: 0.2379516065120697
train_iter_loss: 0.35429278016090393
train_iter_loss: 0.2546474039554596
train_iter_loss: 0.14770683646202087
train_iter_loss: 0.12361516058444977
train_iter_loss: 0.3638463616371155
train_iter_loss: 0.3057061433792114
train_iter_loss: 0.2653439939022064
train_iter_loss: 0.3794151246547699
train_iter_loss: 0.4079713225364685
train_iter_loss: 0.18701525032520294
train_iter_loss: 0.25862962007522583
train_iter_loss: 0.23864416778087616
train_iter_loss: 0.25399887561798096
train_iter_loss: 0.3431980311870575
train_iter_loss: 0.3122350573539734
train_iter_loss: 0.24590778350830078
train_iter_loss: 0.21165232360363007
train_iter_loss: 0.13826876878738403
train_iter_loss: 0.3129333257675171
train_iter_loss: 0.20865650475025177
train_iter_loss: 0.17875352501869202
train_iter_loss: 0.32152748107910156
train_iter_loss: 0.15921442210674286
train_iter_loss: 0.1732044368982315
train_iter_loss: 0.2822844088077545
train_iter_loss: 0.19582781195640564
train_iter_loss: 0.15375083684921265
train_iter_loss: 0.1538013517856598
train_iter_loss: 0.24341179430484772
train_iter_loss: 0.22112739086151123
train_iter_loss: 0.2705805003643036
train_iter_loss: 0.27496573328971863
train_iter_loss: 0.3174240291118622
train_iter_loss: 0.301899254322052
train_iter_loss: 0.3767054080963135
train_iter_loss: 0.15167662501335144
train_iter_loss: 0.36100202798843384
train_iter_loss: 0.22695501148700714
train_iter_loss: 0.2942623496055603
train_iter_loss: 0.020904920995235443
train_iter_loss: 0.13837946951389313
train_iter_loss: 0.14990481734275818
train_iter_loss: 0.3131585419178009
train_iter_loss: 0.34108078479766846
train_iter_loss: 0.34571921825408936
train_iter_loss: 0.23449920117855072
train_iter_loss: 0.36378785967826843
train_iter_loss: 0.33245015144348145
train_iter_loss: 0.12066887319087982
train_iter_loss: 0.3163806200027466
train_iter_loss: 0.3230605125427246
train_iter_loss: 0.32606393098831177
train_iter_loss: 0.26328274607658386
train_iter_loss: 0.22209665179252625
train_iter_loss: 0.3478296101093292
train_iter_loss: 0.19320186972618103
train_iter_loss: 0.20456889271736145
train_iter_loss: 0.28980839252471924
train_iter_loss: 0.19421906769275665
train_iter_loss: 0.20663562417030334
train_iter_loss: 0.26538988947868347
train_iter_loss: 0.14374294877052307
train_iter_loss: 0.32432833313941956
train_iter_loss: 0.13369262218475342
train_iter_loss: 0.2539415657520294
train_iter_loss: 0.3909856677055359
train_iter_loss: 0.2881005108356476
train loss :0.2608
---------------------
Validation seg loss: 0.3778513463407333 at epoch 791
epoch =    792/  1000, exp = train
train_iter_loss: 0.1875801384449005
train_iter_loss: 0.1568029820919037
train_iter_loss: 0.29070180654525757
train_iter_loss: 0.15395237505435944
train_iter_loss: 0.22622160613536835
train_iter_loss: 0.1714448183774948
train_iter_loss: 0.5940149426460266
train_iter_loss: 0.3045766353607178
train_iter_loss: 0.22040040791034698
train_iter_loss: 0.16733106970787048
train_iter_loss: 0.16105403006076813
train_iter_loss: 0.22187161445617676
train_iter_loss: 0.2900114357471466
train_iter_loss: 0.1113324835896492
train_iter_loss: 0.3424672484397888
train_iter_loss: 0.3015000820159912
train_iter_loss: 0.2593246400356293
train_iter_loss: 0.1811281442642212
train_iter_loss: 0.2958088219165802
train_iter_loss: 0.13738131523132324
train_iter_loss: 0.09643345326185226
train_iter_loss: 0.21727824211120605
train_iter_loss: 0.046692896634340286
train_iter_loss: 0.2890418469905853
train_iter_loss: 0.14013996720314026
train_iter_loss: 0.23769506812095642
train_iter_loss: 0.29661697149276733
train_iter_loss: 0.14935900270938873
train_iter_loss: 0.28701314330101013
train_iter_loss: 0.3256998062133789
train_iter_loss: 0.25736913084983826
train_iter_loss: 0.20840731263160706
train_iter_loss: 0.3527577817440033
train_iter_loss: 0.3154361844062805
train_iter_loss: 0.41212746500968933
train_iter_loss: 0.36800146102905273
train_iter_loss: 0.2371341437101364
train_iter_loss: 0.19255521893501282
train_iter_loss: 0.3529888391494751
train_iter_loss: 0.26777970790863037
train_iter_loss: 0.31172236800193787
train_iter_loss: 0.24627992510795593
train_iter_loss: 0.32315295934677124
train_iter_loss: 0.24931080639362335
train_iter_loss: 0.2285444140434265
train_iter_loss: 0.44928908348083496
train_iter_loss: 0.18013450503349304
train_iter_loss: 0.3582632839679718
train_iter_loss: 0.22755470871925354
train_iter_loss: 0.258597731590271
train_iter_loss: 0.2548587918281555
train_iter_loss: 0.32440662384033203
train_iter_loss: 0.13022631406784058
train_iter_loss: 0.1185014545917511
train_iter_loss: 0.15035010874271393
train_iter_loss: 0.218723863363266
train_iter_loss: 0.3383482098579407
train_iter_loss: 0.3652679920196533
train_iter_loss: 0.19396720826625824
train_iter_loss: 0.2869585156440735
train_iter_loss: 0.200062558054924
train_iter_loss: 0.24245953559875488
train_iter_loss: 0.29581817984580994
train_iter_loss: 0.24469639360904694
train_iter_loss: 0.13584236800670624
train_iter_loss: 0.3199019432067871
train_iter_loss: 0.28595325350761414
train_iter_loss: 0.21647629141807556
train_iter_loss: 0.3409695029258728
train_iter_loss: 0.27625784277915955
train_iter_loss: 0.3056471347808838
train_iter_loss: 0.4073968231678009
train_iter_loss: 0.19314447045326233
train_iter_loss: 0.11727321892976761
train_iter_loss: 0.2800474762916565
train_iter_loss: 0.2908869683742523
train_iter_loss: 0.11718355864286423
train_iter_loss: 0.28073638677597046
train_iter_loss: 0.27888044714927673
train_iter_loss: 0.2698100209236145
train_iter_loss: 0.22333890199661255
train_iter_loss: 0.4080738425254822
train_iter_loss: 0.22205545008182526
train_iter_loss: 0.17331258952617645
train_iter_loss: 0.29400634765625
train_iter_loss: 0.2448616623878479
train_iter_loss: 0.40365853905677795
train_iter_loss: 0.27321141958236694
train_iter_loss: 0.18794503808021545
train_iter_loss: 0.2202848345041275
train_iter_loss: 0.3413812220096588
train_iter_loss: 0.190731480717659
train_iter_loss: 0.1396304965019226
train_iter_loss: 0.4693600535392761
train_iter_loss: 0.16155186295509338
train_iter_loss: 0.17322245240211487
train_iter_loss: 0.1627693623304367
train_iter_loss: 0.2758411765098572
train_iter_loss: 0.34970617294311523
train_iter_loss: 0.26667889952659607
train loss :0.2558
---------------------
Validation seg loss: 0.3539591618497276 at epoch 792
epoch =    793/  1000, exp = train
train_iter_loss: 0.26951873302459717
train_iter_loss: 0.11541218310594559
train_iter_loss: 0.2855467200279236
train_iter_loss: 0.24877402186393738
train_iter_loss: 0.3144341707229614
train_iter_loss: 0.12970301508903503
train_iter_loss: 0.28416603803634644
train_iter_loss: 0.3476405143737793
train_iter_loss: 0.19026178121566772
train_iter_loss: 0.2683606743812561
train_iter_loss: 0.15270550549030304
train_iter_loss: 0.1629064381122589
train_iter_loss: 0.24346472322940826
train_iter_loss: 0.20456932485103607
train_iter_loss: 0.1342902034521103
train_iter_loss: 0.20251373946666718
train_iter_loss: 0.41661277413368225
train_iter_loss: 0.32658952474594116
train_iter_loss: 0.27495017647743225
train_iter_loss: 0.1997983455657959
train_iter_loss: 0.2739067077636719
train_iter_loss: 0.1886950582265854
train_iter_loss: 0.45890164375305176
train_iter_loss: 0.31509318947792053
train_iter_loss: 0.192881241440773
train_iter_loss: 0.26496538519859314
train_iter_loss: 0.23032468557357788
train_iter_loss: 0.23318804800510406
train_iter_loss: 0.2225455939769745
train_iter_loss: 0.2051774263381958
train_iter_loss: 0.35659363865852356
train_iter_loss: 0.08875329047441483
train_iter_loss: 0.267591267824173
train_iter_loss: 0.19140274822711945
train_iter_loss: 0.3377271890640259
train_iter_loss: 0.1524754911661148
train_iter_loss: 0.18690481781959534
train_iter_loss: 0.31367039680480957
train_iter_loss: 0.38339945673942566
train_iter_loss: 0.23819191753864288
train_iter_loss: 0.17828039824962616
train_iter_loss: 0.3521711528301239
train_iter_loss: 0.19896936416625977
train_iter_loss: 0.32733580470085144
train_iter_loss: 0.3024088442325592
train_iter_loss: 0.22388054430484772
train_iter_loss: 0.2688125669956207
train_iter_loss: 0.14861203730106354
train_iter_loss: 0.13912376761436462
train_iter_loss: 0.3416002690792084
train_iter_loss: 0.26827114820480347
train_iter_loss: 0.26380518078804016
train_iter_loss: 0.3146129548549652
train_iter_loss: 0.3442569077014923
train_iter_loss: 0.38848593831062317
train_iter_loss: 0.2973521053791046
train_iter_loss: 0.20649635791778564
train_iter_loss: 0.26696646213531494
train_iter_loss: 0.3704227805137634
train_iter_loss: 0.387703001499176
train_iter_loss: 0.3571767807006836
train_iter_loss: 0.0846584215760231
train_iter_loss: 0.24070066213607788
train_iter_loss: 0.38413697481155396
train_iter_loss: 0.28968557715415955
train_iter_loss: 0.4352356195449829
train_iter_loss: 0.25550365447998047
train_iter_loss: 0.2186993658542633
train_iter_loss: 0.3231019079685211
train_iter_loss: 0.28344953060150146
train_iter_loss: 0.26079079508781433
train_iter_loss: 0.3539411127567291
train_iter_loss: 0.2080610990524292
train_iter_loss: 0.2905837893486023
train_iter_loss: 0.1603720635175705
train_iter_loss: 0.20780248939990997
train_iter_loss: 0.2953057885169983
train_iter_loss: 0.2605157196521759
train_iter_loss: 0.11988143622875214
train_iter_loss: 0.48308050632476807
train_iter_loss: 0.4398115873336792
train_iter_loss: 0.2238580882549286
train_iter_loss: 0.13024502992630005
train_iter_loss: 0.25584402680397034
train_iter_loss: 0.30903738737106323
train_iter_loss: 0.215431347489357
train_iter_loss: 0.2889525890350342
train_iter_loss: 0.2582537531852722
train_iter_loss: 0.20489294826984406
train_iter_loss: 0.2749467194080353
train_iter_loss: 0.23637160658836365
train_iter_loss: 0.21188364923000336
train_iter_loss: 0.3392537832260132
train_iter_loss: 0.3133127987384796
train_iter_loss: 0.15874585509300232
train_iter_loss: 0.21551203727722168
train_iter_loss: 0.23049584031105042
train_iter_loss: 0.24115635454654694
train_iter_loss: 0.3673063814640045
train_iter_loss: 0.16137026250362396
train loss :0.2631
---------------------
Validation seg loss: 0.3751281354014041 at epoch 793
epoch =    794/  1000, exp = train
train_iter_loss: 0.21915574371814728
train_iter_loss: 0.40429046750068665
train_iter_loss: 0.22063836455345154
train_iter_loss: 0.2222135066986084
train_iter_loss: 0.16864988207817078
train_iter_loss: 0.38787442445755005
train_iter_loss: 0.2921556234359741
train_iter_loss: 0.19859963655471802
train_iter_loss: 0.33833426237106323
train_iter_loss: 0.18750299513339996
train_iter_loss: 0.2505851686000824
train_iter_loss: 0.22152678668498993
train_iter_loss: 0.35987287759780884
train_iter_loss: 0.21263371407985687
train_iter_loss: 0.16509202122688293
train_iter_loss: 0.17364086210727692
train_iter_loss: 0.2373880296945572
train_iter_loss: 0.13104823231697083
train_iter_loss: 0.4538172483444214
train_iter_loss: 0.29573893547058105
train_iter_loss: 0.09206532686948776
train_iter_loss: 0.22130398452281952
train_iter_loss: 0.16782546043395996
train_iter_loss: 0.31215211749076843
train_iter_loss: 0.25183722376823425
train_iter_loss: 0.1562635898590088
train_iter_loss: 0.3696204721927643
train_iter_loss: 0.18738794326782227
train_iter_loss: 0.29937177896499634
train_iter_loss: 0.25638994574546814
train_iter_loss: 0.5073466300964355
train_iter_loss: 0.27484315633773804
train_iter_loss: 0.16622567176818848
train_iter_loss: 0.21105709671974182
train_iter_loss: 0.35476839542388916
train_iter_loss: 0.23179131746292114
train_iter_loss: 0.14816541969776154
train_iter_loss: 0.21859115362167358
train_iter_loss: 0.08487701416015625
train_iter_loss: 0.22525958716869354
train_iter_loss: 0.3274620473384857
train_iter_loss: 0.21252788603305817
train_iter_loss: 0.14931584894657135
train_iter_loss: 0.23440779745578766
train_iter_loss: 0.4383886754512787
train_iter_loss: 0.3532152771949768
train_iter_loss: 0.2034018486738205
train_iter_loss: 0.2592643201351166
train_iter_loss: 0.2268194556236267
train_iter_loss: 0.37486910820007324
train_iter_loss: 0.4602172374725342
train_iter_loss: 0.39572277665138245
train_iter_loss: 0.29218408465385437
train_iter_loss: 0.3165498673915863
train_iter_loss: 0.2581736743450165
train_iter_loss: 0.32318225502967834
train_iter_loss: 0.21963797509670258
train_iter_loss: 0.19270876049995422
train_iter_loss: 0.16105777025222778
train_iter_loss: 0.26974162459373474
train_iter_loss: 0.28486374020576477
train_iter_loss: 0.22772473096847534
train_iter_loss: 0.2606220543384552
train_iter_loss: 0.2660818099975586
train_iter_loss: 0.2966423034667969
train_iter_loss: 0.20894373953342438
train_iter_loss: 0.16117650270462036
train_iter_loss: 0.488361120223999
train_iter_loss: 0.39715203642845154
train_iter_loss: 0.25775396823883057
train_iter_loss: 0.17128100991249084
train_iter_loss: 0.16641230881214142
train_iter_loss: 0.2521071135997772
train_iter_loss: 0.2077239602804184
train_iter_loss: 0.21764814853668213
train_iter_loss: 0.24899519979953766
train_iter_loss: 0.14215408265590668
train_iter_loss: 0.18041299283504486
train_iter_loss: 0.2862516939640045
train_iter_loss: 0.21779805421829224
train_iter_loss: 0.3033964931964874
train_iter_loss: 0.3120502233505249
train_iter_loss: 0.3869600296020508
train_iter_loss: 0.3141302168369293
train_iter_loss: 0.23473770916461945
train_iter_loss: 0.23273396492004395
train_iter_loss: 0.3554041087627411
train_iter_loss: 0.2488490343093872
train_iter_loss: 0.35581329464912415
train_iter_loss: 0.20970940589904785
train_iter_loss: 0.3414655923843384
train_iter_loss: 0.16930517554283142
train_iter_loss: 0.2455516755580902
train_iter_loss: 0.19563201069831848
train_iter_loss: 0.29244154691696167
train_iter_loss: 0.24359753727912903
train_iter_loss: 0.28635910153388977
train_iter_loss: 0.16881610453128815
train_iter_loss: 0.21388418972492218
train_iter_loss: 0.15198923647403717
train loss :0.2601
---------------------
Validation seg loss: 0.37028413389827003 at epoch 794
epoch =    795/  1000, exp = train
train_iter_loss: 0.22263675928115845
train_iter_loss: 0.2427990585565567
train_iter_loss: 0.12832018733024597
train_iter_loss: 0.25690028071403503
train_iter_loss: 0.25277334451675415
train_iter_loss: 0.2446625828742981
train_iter_loss: 0.3660023510456085
train_iter_loss: 0.07964449375867844
train_iter_loss: 0.29558640718460083
train_iter_loss: 0.40043580532073975
train_iter_loss: 0.3001721203327179
train_iter_loss: 0.249896839261055
train_iter_loss: 0.2432546466588974
train_iter_loss: 0.2293320596218109
train_iter_loss: 0.2515595257282257
train_iter_loss: 0.2534651458263397
train_iter_loss: 0.2735772132873535
train_iter_loss: 0.2897282838821411
train_iter_loss: 0.11984953284263611
train_iter_loss: 0.30956271290779114
train_iter_loss: 0.1994192749261856
train_iter_loss: 0.26746323704719543
train_iter_loss: 0.35011228919029236
train_iter_loss: 0.20623824000358582
train_iter_loss: 0.23576809465885162
train_iter_loss: 0.4153215289115906
train_iter_loss: 0.240853950381279
train_iter_loss: 0.47797393798828125
train_iter_loss: 0.18548980355262756
train_iter_loss: 0.2475680112838745
train_iter_loss: 0.1990303099155426
train_iter_loss: 0.3726463317871094
train_iter_loss: 0.24155160784721375
train_iter_loss: 0.10965686291456223
train_iter_loss: 0.23760755360126495
train_iter_loss: 0.19464826583862305
train_iter_loss: 0.42400848865509033
train_iter_loss: 0.2673935890197754
train_iter_loss: 0.1549803763628006
train_iter_loss: 0.24500416219234467
train_iter_loss: 0.1528744101524353
train_iter_loss: 0.2971910834312439
train_iter_loss: 0.31307849287986755
train_iter_loss: 0.32918810844421387
train_iter_loss: 0.1801767200231552
train_iter_loss: 0.28072482347488403
train_iter_loss: 0.35948675870895386
train_iter_loss: 0.3039061427116394
train_iter_loss: 0.33369842171669006
train_iter_loss: 0.2825148105621338
train_iter_loss: 0.11916498094797134
train_iter_loss: 0.21227271854877472
train_iter_loss: 0.2948234975337982
train_iter_loss: 0.20969945192337036
train_iter_loss: 0.3310520648956299
train_iter_loss: 0.23807454109191895
train_iter_loss: 0.31989309191703796
train_iter_loss: 0.5159531235694885
train_iter_loss: 0.2993212640285492
train_iter_loss: 0.25030216574668884
train_iter_loss: 0.10536783188581467
train_iter_loss: 0.231769859790802
train_iter_loss: 0.10685368627309799
train_iter_loss: 0.22901907563209534
train_iter_loss: 0.32580769062042236
train_iter_loss: 0.17043912410736084
train_iter_loss: 0.10171419382095337
train_iter_loss: 0.2557862102985382
train_iter_loss: 0.3337523639202118
train_iter_loss: 0.24043728411197662
train_iter_loss: 0.3582708537578583
train_iter_loss: 0.25781551003456116
train_iter_loss: 0.46491724252700806
train_iter_loss: 0.31855520606040955
train_iter_loss: 0.20392273366451263
train_iter_loss: 0.19059230387210846
train_iter_loss: 0.29096415638923645
train_iter_loss: 0.2611095905303955
train_iter_loss: 0.30869048833847046
train_iter_loss: 0.41506436467170715
train_iter_loss: 0.2537901997566223
train_iter_loss: 0.3431694209575653
train_iter_loss: 0.13236111402511597
train_iter_loss: 0.1734085977077484
train_iter_loss: 0.26907864212989807
train_iter_loss: 0.19244715571403503
train_iter_loss: 0.2684214413166046
train_iter_loss: 0.10895351320505142
train_iter_loss: 0.1854059398174286
train_iter_loss: 0.22271759808063507
train_iter_loss: 0.31614363193511963
train_iter_loss: 0.29930976033210754
train_iter_loss: 0.24510426819324493
train_iter_loss: 0.3874654471874237
train_iter_loss: 0.42777562141418457
train_iter_loss: 0.43106696009635925
train_iter_loss: 0.3004930913448334
train_iter_loss: 0.1439172923564911
train_iter_loss: 0.1793934553861618
train_iter_loss: 0.2761765122413635
train loss :0.2649
---------------------
Validation seg loss: 0.35202655479979683 at epoch 795
epoch =    796/  1000, exp = train
train_iter_loss: 0.24486111104488373
train_iter_loss: 0.2133752405643463
train_iter_loss: 0.342340886592865
train_iter_loss: 0.25233516097068787
train_iter_loss: 0.2972700595855713
train_iter_loss: 0.2609746754169464
train_iter_loss: 0.2972015142440796
train_iter_loss: 0.2858104109764099
train_iter_loss: 0.2532176971435547
train_iter_loss: 0.24776624143123627
train_iter_loss: 0.37726131081581116
train_iter_loss: 0.18923330307006836
train_iter_loss: 0.22828923165798187
train_iter_loss: 0.26951029896736145
train_iter_loss: 0.3392629325389862
train_iter_loss: 0.3345353901386261
train_iter_loss: 0.29056668281555176
train_iter_loss: 0.2649215757846832
train_iter_loss: 0.20314930379390717
train_iter_loss: 0.15337902307510376
train_iter_loss: 0.265142023563385
train_iter_loss: 0.2596096396446228
train_iter_loss: 0.1686198115348816
train_iter_loss: 0.18369843065738678
train_iter_loss: 0.14158113300800323
train_iter_loss: 0.34030866622924805
train_iter_loss: 0.16974762082099915
train_iter_loss: 0.3152870237827301
train_iter_loss: 0.30828917026519775
train_iter_loss: 0.25323525071144104
train_iter_loss: 0.09030379354953766
train_iter_loss: 0.2417086809873581
train_iter_loss: 0.1454705446958542
train_iter_loss: 0.23839351534843445
train_iter_loss: 0.3379170000553131
train_iter_loss: 0.2820821702480316
train_iter_loss: 0.42386534810066223
train_iter_loss: 0.37920552492141724
train_iter_loss: 0.24409343302249908
train_iter_loss: 0.25544407963752747
train_iter_loss: 0.27411288022994995
train_iter_loss: 0.19498127698898315
train_iter_loss: 0.12658387422561646
train_iter_loss: 0.21331024169921875
train_iter_loss: 0.11520802974700928
train_iter_loss: 0.3202402889728546
train_iter_loss: 0.16779328882694244
train_iter_loss: 0.17453792691230774
train_iter_loss: 0.28202909231185913
train_iter_loss: 0.23510180413722992
train_iter_loss: 0.26557672023773193
train_iter_loss: 0.3845284581184387
train_iter_loss: 0.22200971841812134
train_iter_loss: 0.4222352206707001
train_iter_loss: 0.46823081374168396
train_iter_loss: 0.16001473367214203
train_iter_loss: 0.3329238295555115
train_iter_loss: 0.09222736954689026
train_iter_loss: 0.32794389128685
train_iter_loss: 0.34647244215011597
train_iter_loss: 0.07768598943948746
train_iter_loss: 0.31097593903541565
train_iter_loss: 0.293680876493454
train_iter_loss: 0.11990005522966385
train_iter_loss: 0.2288137674331665
train_iter_loss: 0.39372017979621887
train_iter_loss: 0.31963223218917847
train_iter_loss: 0.3313489258289337
train_iter_loss: 0.2032078206539154
train_iter_loss: 0.4869268834590912
train_iter_loss: 0.22834590077400208
train_iter_loss: 0.1388036012649536
train_iter_loss: 0.34739768505096436
train_iter_loss: 0.3317323923110962
train_iter_loss: 0.2687802314758301
train_iter_loss: 0.17248816788196564
train_iter_loss: 0.31053096055984497
train_iter_loss: 0.2940341532230377
train_iter_loss: 0.24249185621738434
train_iter_loss: 0.25030913949012756
train_iter_loss: 0.25132638216018677
train_iter_loss: 0.14119990170001984
train_iter_loss: 0.24860221147537231
train_iter_loss: 0.19890262186527252
train_iter_loss: 0.11963380873203278
train_iter_loss: 0.24019166827201843
train_iter_loss: 0.38613781332969666
train_iter_loss: 0.28326624631881714
train_iter_loss: 0.3110388517379761
train_iter_loss: 0.29456934332847595
train_iter_loss: 0.3702475130558014
train_iter_loss: 0.21883490681648254
train_iter_loss: 0.1466154307126999
train_iter_loss: 0.3529799282550812
train_iter_loss: 0.2317150980234146
train_iter_loss: 0.07148905843496323
train_iter_loss: 0.1697966456413269
train_iter_loss: 0.25838732719421387
train_iter_loss: 0.46602663397789
train_iter_loss: 0.17282067239284515
train loss :0.2606
---------------------
Validation seg loss: 0.3541376724628345 at epoch 796
epoch =    797/  1000, exp = train
train_iter_loss: 0.26131972670555115
train_iter_loss: 0.22143276035785675
train_iter_loss: 0.19318796694278717
train_iter_loss: 0.31112250685691833
train_iter_loss: 0.31425219774246216
train_iter_loss: 0.3237747550010681
train_iter_loss: 0.21303528547286987
train_iter_loss: 0.27795350551605225
train_iter_loss: 0.12596774101257324
train_iter_loss: 0.20052774250507355
train_iter_loss: 0.28891849517822266
train_iter_loss: 0.4235307574272156
train_iter_loss: 0.3074913024902344
train_iter_loss: 0.3139912188053131
train_iter_loss: 0.18477380275726318
train_iter_loss: 0.19512104988098145
train_iter_loss: 0.14652866125106812
train_iter_loss: 0.2875303328037262
train_iter_loss: 0.28210482001304626
train_iter_loss: 0.3224494755268097
train_iter_loss: 0.31428074836730957
train_iter_loss: 0.20675070583820343
train_iter_loss: 0.1309940069913864
train_iter_loss: 0.38186904788017273
train_iter_loss: 0.2135782390832901
train_iter_loss: 0.14392518997192383
train_iter_loss: 0.307816743850708
train_iter_loss: 0.28094521164894104
train_iter_loss: 0.25699055194854736
train_iter_loss: 0.19009467959403992
train_iter_loss: 0.2602637708187103
train_iter_loss: 0.13885198533535004
train_iter_loss: 0.4239892363548279
train_iter_loss: 0.1634141057729721
train_iter_loss: 0.3304097354412079
train_iter_loss: 0.19233089685440063
train_iter_loss: 0.38258373737335205
train_iter_loss: 0.19240717589855194
train_iter_loss: 0.2509857416152954
train_iter_loss: 0.2855665683746338
train_iter_loss: 0.15925367176532745
train_iter_loss: 0.22523778676986694
train_iter_loss: 0.24693801999092102
train_iter_loss: 0.29312434792518616
train_iter_loss: 0.3464517295360565
train_iter_loss: 0.18916131556034088
train_iter_loss: 0.40124163031578064
train_iter_loss: 0.2464292198419571
train_iter_loss: 0.23878741264343262
train_iter_loss: 0.24392475187778473
train_iter_loss: 0.2720048427581787
train_iter_loss: 0.28936880826950073
train_iter_loss: 0.32552263140678406
train_iter_loss: 0.2535324692726135
train_iter_loss: 0.23019711673259735
train_iter_loss: 0.35765889286994934
train_iter_loss: 0.2994033694267273
train_iter_loss: 0.2613099217414856
train_iter_loss: 0.2989327311515808
train_iter_loss: 0.32383498549461365
train_iter_loss: 0.32001784443855286
train_iter_loss: 0.3060929775238037
train_iter_loss: 0.14622238278388977
train_iter_loss: 0.1789020299911499
train_iter_loss: 0.3316269814968109
train_iter_loss: 0.13057999312877655
train_iter_loss: 0.3073546886444092
train_iter_loss: 0.447490930557251
train_iter_loss: 0.29034337401390076
train_iter_loss: 0.2581159174442291
train_iter_loss: 0.3606623113155365
train_iter_loss: 0.21202802658081055
train_iter_loss: 0.24721671640872955
train_iter_loss: 0.3103226125240326
train_iter_loss: 0.20935437083244324
train_iter_loss: 0.09128009527921677
train_iter_loss: 0.28501096367836
train_iter_loss: 0.14595027267932892
train_iter_loss: 0.3259904682636261
train_iter_loss: 0.3341718912124634
train_iter_loss: 0.3790450692176819
train_iter_loss: 0.25269150733947754
train_iter_loss: 0.38885316252708435
train_iter_loss: 0.15950922667980194
train_iter_loss: 0.21658456325531006
train_iter_loss: 0.09789293259382248
train_iter_loss: 0.24220678210258484
train_iter_loss: 0.25206753611564636
train_iter_loss: 0.1992114782333374
train_iter_loss: 0.19137132167816162
train_iter_loss: 0.33839350938796997
train_iter_loss: 0.13679373264312744
train_iter_loss: 0.3667449653148651
train_iter_loss: 0.27068212628364563
train_iter_loss: 0.3745025098323822
train_iter_loss: 0.22380612790584564
train_iter_loss: 0.26764950156211853
train_iter_loss: 0.2240145206451416
train_iter_loss: 0.19825716316699982
train_iter_loss: 0.32588744163513184
train loss :0.2635
---------------------
Validation seg loss: 0.3474520536903995 at epoch 797
epoch =    798/  1000, exp = train
train_iter_loss: 0.22420723736286163
train_iter_loss: 0.19163952767848969
train_iter_loss: 0.1275070458650589
train_iter_loss: 0.2590174973011017
train_iter_loss: 0.24063464999198914
train_iter_loss: 0.19539032876491547
train_iter_loss: 0.2374894767999649
train_iter_loss: 0.1804000735282898
train_iter_loss: 0.24267186224460602
train_iter_loss: 0.30028384923934937
train_iter_loss: 0.34771159291267395
train_iter_loss: 0.34625762701034546
train_iter_loss: 0.264591246843338
train_iter_loss: 0.1830883026123047
train_iter_loss: 0.13164940476417542
train_iter_loss: 0.24912723898887634
train_iter_loss: 0.28072577714920044
train_iter_loss: 0.18278680741786957
train_iter_loss: 0.09679373353719711
train_iter_loss: 0.3356800079345703
train_iter_loss: 0.25758811831474304
train_iter_loss: 0.24612993001937866
train_iter_loss: 0.279762327671051
train_iter_loss: 0.15991640090942383
train_iter_loss: 0.30627429485321045
train_iter_loss: 0.2915838360786438
train_iter_loss: 0.34250226616859436
train_iter_loss: 0.4018072187900543
train_iter_loss: 0.16011352837085724
train_iter_loss: 0.2031388282775879
train_iter_loss: 0.3196606934070587
train_iter_loss: 0.2883385717868805
train_iter_loss: 0.2530929446220398
train_iter_loss: 0.4607826769351959
train_iter_loss: 0.2448480874300003
train_iter_loss: 0.33821865916252136
train_iter_loss: 0.27013805508613586
train_iter_loss: 0.1998295933008194
train_iter_loss: 0.2079424113035202
train_iter_loss: 0.25215065479278564
train_iter_loss: 0.1978752315044403
train_iter_loss: 0.31132394075393677
train_iter_loss: 0.28365546464920044
train_iter_loss: 0.36118069291114807
train_iter_loss: 0.1439245343208313
train_iter_loss: 0.1738671064376831
train_iter_loss: 0.2189805805683136
train_iter_loss: 0.28862541913986206
train_iter_loss: 0.2675425410270691
train_iter_loss: 0.25490590929985046
train_iter_loss: 0.3714037537574768
train_iter_loss: 0.3211056888103485
train_iter_loss: 0.15888924896717072
train_iter_loss: 0.3338591456413269
train_iter_loss: 0.31479454040527344
train_iter_loss: 0.24947907030582428
train_iter_loss: 0.2615363597869873
train_iter_loss: 0.23259955644607544
train_iter_loss: 0.18948926031589508
train_iter_loss: 0.20601005852222443
train_iter_loss: 0.3480033278465271
train_iter_loss: 0.19117508828639984
train_iter_loss: 0.2737596333026886
train_iter_loss: 0.2270207554101944
train_iter_loss: 0.11856172233819962
train_iter_loss: 0.06985355913639069
train_iter_loss: 0.2930351495742798
train_iter_loss: 0.3732859790325165
train_iter_loss: 0.23877008259296417
train_iter_loss: 0.14312228560447693
train_iter_loss: 0.22297196090221405
train_iter_loss: 0.19176062941551208
train_iter_loss: 0.43384605646133423
train_iter_loss: 0.15705710649490356
train_iter_loss: 0.2434433400630951
train_iter_loss: 0.1932336539030075
train_iter_loss: 0.2206907421350479
train_iter_loss: 0.25449255108833313
train_iter_loss: 0.2821284830570221
train_iter_loss: 0.4013890027999878
train_iter_loss: 0.28058862686157227
train_iter_loss: 0.518202543258667
train_iter_loss: 0.22369368374347687
train_iter_loss: 0.3615608811378479
train_iter_loss: 0.2598809599876404
train_iter_loss: 0.34853672981262207
train_iter_loss: 0.18785303831100464
train_iter_loss: 0.2434692680835724
train_iter_loss: 0.16152893006801605
train_iter_loss: 0.33509671688079834
train_iter_loss: 0.3337855637073517
train_iter_loss: 0.36523619294166565
train_iter_loss: 0.39203566312789917
train_iter_loss: 0.17906098067760468
train_iter_loss: 0.23146192729473114
train_iter_loss: 0.2990739643573761
train_iter_loss: 0.16383782029151917
train_iter_loss: 0.1765388399362564
train_iter_loss: 0.34696725010871887
train_iter_loss: 0.2724311351776123
train loss :0.2606
---------------------
Validation seg loss: 0.34580828441869255 at epoch 798
epoch =    799/  1000, exp = train
train_iter_loss: 0.25140753388404846
train_iter_loss: 0.18216829001903534
train_iter_loss: 0.3197244107723236
train_iter_loss: 0.2783587872982025
train_iter_loss: 0.15900669991970062
train_iter_loss: 0.1964006870985031
train_iter_loss: 0.20345830917358398
train_iter_loss: 0.26981282234191895
train_iter_loss: 0.34510233998298645
train_iter_loss: 0.48082131147384644
train_iter_loss: 0.24415630102157593
train_iter_loss: 0.3405350148677826
train_iter_loss: 0.2788560092449188
train_iter_loss: 0.38377630710601807
train_iter_loss: 0.09188859164714813
train_iter_loss: 0.2863258123397827
train_iter_loss: 0.3909982740879059
train_iter_loss: 0.320010781288147
train_iter_loss: 0.24151742458343506
train_iter_loss: 0.1423838883638382
train_iter_loss: 0.1435246616601944
train_iter_loss: 0.17491815984249115
train_iter_loss: 0.29895031452178955
train_iter_loss: 0.40594804286956787
train_iter_loss: 0.32381659746170044
train_iter_loss: 0.29392993450164795
train_iter_loss: 0.30035528540611267
train_iter_loss: 0.23541314899921417
train_iter_loss: 0.27551525831222534
train_iter_loss: 0.15089282393455505
train_iter_loss: 0.21346227824687958
train_iter_loss: 0.2365526258945465
train_iter_loss: 0.2944525182247162
train_iter_loss: 0.2896730303764343
train_iter_loss: 0.29280075430870056
train_iter_loss: 0.1569942682981491
train_iter_loss: 0.3271465599536896
train_iter_loss: 0.12099948525428772
train_iter_loss: 0.20774802565574646
train_iter_loss: 0.27360525727272034
train_iter_loss: 0.215630441904068
train_iter_loss: 0.259568989276886
train_iter_loss: 0.3466249108314514
train_iter_loss: 0.2457796335220337
train_iter_loss: 0.09987995028495789
train_iter_loss: 0.2151644378900528
train_iter_loss: 0.28245630860328674
train_iter_loss: 0.4107430875301361
train_iter_loss: 0.27307620644569397
train_iter_loss: 0.31400251388549805
train_iter_loss: 0.2552693784236908
train_iter_loss: 0.3309289216995239
train_iter_loss: 0.1875731647014618
train_iter_loss: 0.17382562160491943
train_iter_loss: 0.39563441276550293
train_iter_loss: 0.14167281985282898
train_iter_loss: 0.18510954082012177
train_iter_loss: 0.3477875590324402
train_iter_loss: 0.2279922068119049
train_iter_loss: 0.2723694443702698
train_iter_loss: 0.35791322588920593
train_iter_loss: 0.3395698070526123
train_iter_loss: 0.2709588408470154
train_iter_loss: 0.2403426617383957
train_iter_loss: 0.21058513224124908
train_iter_loss: 0.2101566195487976
train_iter_loss: 0.24116650223731995
train_iter_loss: 0.2506256103515625
train_iter_loss: 0.1946970820426941
train_iter_loss: 0.26480239629745483
train_iter_loss: 0.2488010972738266
train_iter_loss: 0.25754135847091675
train_iter_loss: 0.32342758774757385
train_iter_loss: 0.21226996183395386
train_iter_loss: 0.28410133719444275
train_iter_loss: 0.19988375902175903
train_iter_loss: 0.16938775777816772
train_iter_loss: 0.18906114995479584
train_iter_loss: 0.16998177766799927
train_iter_loss: 0.24561108648777008
train_iter_loss: 0.2963787615299225
train_iter_loss: 0.31679609417915344
train_iter_loss: 0.17356163263320923
train_iter_loss: 0.15043558180332184
train_iter_loss: 0.24411557614803314
train_iter_loss: 0.2563645839691162
train_iter_loss: 0.3499191403388977
train_iter_loss: 0.24685822427272797
train_iter_loss: 0.30186012387275696
train_iter_loss: 0.1816752403974533
train_iter_loss: 0.25536003708839417
train_iter_loss: 0.1901683509349823
train_iter_loss: 0.24830493330955505
train_iter_loss: 0.1934472918510437
train_iter_loss: 0.1649801880121231
train_iter_loss: 0.16666868329048157
train_iter_loss: 0.25735363364219666
train_iter_loss: 0.2558414340019226
train_iter_loss: 0.3384074568748474
train_iter_loss: 0.17836013436317444
train loss :0.2551
---------------------
Validation seg loss: 0.37357794034804376 at epoch 799
epoch =    800/  1000, exp = train
train_iter_loss: 0.31346094608306885
train_iter_loss: 0.23093903064727783
train_iter_loss: 0.27422162890434265
train_iter_loss: 0.18900300562381744
train_iter_loss: 0.13857270777225494
train_iter_loss: 0.27487918734550476
train_iter_loss: 0.3509591519832611
train_iter_loss: 0.3335703909397125
train_iter_loss: 0.20461712777614594
train_iter_loss: 0.21217451989650726
train_iter_loss: 0.2296316921710968
train_iter_loss: 0.23112852871418
train_iter_loss: 0.16686835885047913
train_iter_loss: 0.1432783007621765
train_iter_loss: 0.2704518437385559
train_iter_loss: 0.45155760645866394
train_iter_loss: 0.2777499854564667
train_iter_loss: 0.16717201471328735
train_iter_loss: 0.12782709300518036
train_iter_loss: 0.27456676959991455
train_iter_loss: 0.3043614625930786
train_iter_loss: 0.4152262210845947
train_iter_loss: 0.22895877063274384
train_iter_loss: 0.08844789117574692
train_iter_loss: 0.2908071279525757
train_iter_loss: 0.2979923188686371
train_iter_loss: 0.36170390248298645
train_iter_loss: 0.13727620244026184
train_iter_loss: 0.32987117767333984
train_iter_loss: 0.23294393718242645
train_iter_loss: 0.23206481337547302
train_iter_loss: 0.2425023764371872
train_iter_loss: 0.09515842795372009
train_iter_loss: 0.313993364572525
train_iter_loss: 0.4008558988571167
train_iter_loss: 0.17806939780712128
train_iter_loss: 0.33288562297821045
train_iter_loss: 0.12749309837818146
train_iter_loss: 0.2621864378452301
train_iter_loss: 0.2590121924877167
train_iter_loss: 0.31042250990867615
train_iter_loss: 0.2778691351413727
train_iter_loss: 0.310180127620697
train_iter_loss: 0.2474130541086197
train_iter_loss: 0.2940630614757538
train_iter_loss: 0.06529274582862854
train_iter_loss: 0.3221622109413147
train_iter_loss: 0.09520432353019714
train_iter_loss: 0.26212358474731445
train_iter_loss: 0.36312136054039
train_iter_loss: 0.3088858127593994
train_iter_loss: 0.32303112745285034
train_iter_loss: 0.16664807498455048
train_iter_loss: 0.42552173137664795
train_iter_loss: 0.3169873058795929
train_iter_loss: 0.24093449115753174
train_iter_loss: 0.13845184445381165
train_iter_loss: 0.3011407256126404
train_iter_loss: 0.40415284037590027
train_iter_loss: 0.2806369662284851
train_iter_loss: 0.28261905908584595
train_iter_loss: 0.1878068447113037
train_iter_loss: 0.15165524184703827
train_iter_loss: 0.14697153866291046
train_iter_loss: 0.21727745234966278
train_iter_loss: 0.28955185413360596
train_iter_loss: 0.26658928394317627
train_iter_loss: 0.29252535104751587
train_iter_loss: 0.36183997988700867
train_iter_loss: 0.38619962334632874
train_iter_loss: 0.2342248409986496
train_iter_loss: 0.33021044731140137
train_iter_loss: 0.4054153263568878
train_iter_loss: 0.21770216524600983
train_iter_loss: 0.22459453344345093
train_iter_loss: 0.4021807610988617
train_iter_loss: 0.3371817171573639
train_iter_loss: 0.2055317759513855
train_iter_loss: 0.05587060749530792
train_iter_loss: 0.25758135318756104
train_iter_loss: 0.21648377180099487
train_iter_loss: 0.13274894654750824
train_iter_loss: 0.12247300148010254
train_iter_loss: 0.2312639206647873
train_iter_loss: 0.2274211347103119
train_iter_loss: 0.26659828424453735
train_iter_loss: 0.19762568175792694
train_iter_loss: 0.27711281180381775
train_iter_loss: 0.2497236281633377
train_iter_loss: 0.2628892958164215
train_iter_loss: 0.32108384370803833
train_iter_loss: 0.22623391449451447
train_iter_loss: 0.16375114023685455
train_iter_loss: 0.3251146078109741
train_iter_loss: 0.489468514919281
train_iter_loss: 0.29547494649887085
train_iter_loss: 0.058299701660871506
train_iter_loss: 0.3193078935146332
train_iter_loss: 0.15565092861652374
train_iter_loss: 0.2816154956817627
train loss :0.2578
---------------------
Validation seg loss: 0.3486309206104433 at epoch 800
epoch =    801/  1000, exp = train
train_iter_loss: 0.35814258456230164
train_iter_loss: 0.1988174170255661
train_iter_loss: 0.26214030385017395
train_iter_loss: 0.26258185505867004
train_iter_loss: 0.27161356806755066
train_iter_loss: 0.2866380512714386
train_iter_loss: 0.19179706275463104
train_iter_loss: 0.27025946974754333
train_iter_loss: 0.28447026014328003
train_iter_loss: 0.24792234599590302
train_iter_loss: 0.23412787914276123
train_iter_loss: 0.2845364809036255
train_iter_loss: 0.2568958103656769
train_iter_loss: 0.27904000878334045
train_iter_loss: 0.2503267526626587
train_iter_loss: 0.22768107056617737
train_iter_loss: 0.27443474531173706
train_iter_loss: 0.2948475182056427
train_iter_loss: 0.20659105479717255
train_iter_loss: 0.14325375854969025
train_iter_loss: 0.15917588770389557
train_iter_loss: 0.3251780569553375
train_iter_loss: 0.372651070356369
train_iter_loss: 0.12576505541801453
train_iter_loss: 0.3380585312843323
train_iter_loss: 0.3578070402145386
train_iter_loss: 0.2807886302471161
train_iter_loss: 0.35894492268562317
train_iter_loss: 0.3535514175891876
train_iter_loss: 0.1170831173658371
train_iter_loss: 0.346184641122818
train_iter_loss: 0.1309291571378708
train_iter_loss: 0.18064019083976746
train_iter_loss: 0.22533132135868073
train_iter_loss: 0.23668141663074493
train_iter_loss: 0.34337174892425537
train_iter_loss: 0.28237980604171753
train_iter_loss: 0.2692239284515381
train_iter_loss: 0.20699825882911682
train_iter_loss: 0.19257096946239471
train_iter_loss: 0.14771635830402374
train_iter_loss: 0.23336313664913177
train_iter_loss: 0.27696529030799866
train_iter_loss: 0.2549366056919098
train_iter_loss: 0.4004989266395569
train_iter_loss: 0.19339397549629211
train_iter_loss: 0.33049511909484863
train_iter_loss: 0.3669850826263428
train_iter_loss: 0.17664436995983124
train_iter_loss: 0.1570572406053543
train_iter_loss: 0.22392350435256958
train_iter_loss: 0.3454071581363678
train_iter_loss: 0.25690388679504395
train_iter_loss: 0.332109659910202
train_iter_loss: 0.4500340223312378
train_iter_loss: 0.24749340116977692
train_iter_loss: 0.30663126707077026
train_iter_loss: 0.17564672231674194
train_iter_loss: 0.24352358281612396
train_iter_loss: 0.5998222231864929
train_iter_loss: 0.3898624777793884
train_iter_loss: 0.39741525053977966
train_iter_loss: 0.18099965155124664
train_iter_loss: 0.1300993412733078
train_iter_loss: 0.1282755583524704
train_iter_loss: 0.18947575986385345
train_iter_loss: 0.20253264904022217
train_iter_loss: 0.4454954266548157
train_iter_loss: 0.18533842265605927
train_iter_loss: 0.11985284090042114
train_iter_loss: 0.164203479886055
train_iter_loss: 0.2847273349761963
train_iter_loss: 0.3038431704044342
train_iter_loss: 0.40689176321029663
train_iter_loss: 0.2197154015302658
train_iter_loss: 0.24821968376636505
train_iter_loss: 0.4350188076496124
train_iter_loss: 0.26325467228889465
train_iter_loss: 0.23178036510944366
train_iter_loss: 0.16340355575084686
train_iter_loss: 0.42072850465774536
train_iter_loss: 0.19641779363155365
train_iter_loss: 0.40099725127220154
train_iter_loss: 0.32405713200569153
train_iter_loss: 0.25956782698631287
train_iter_loss: 0.16208681464195251
train_iter_loss: 0.24851608276367188
train_iter_loss: 0.16018453240394592
train_iter_loss: 0.37146610021591187
train_iter_loss: 0.1523127257823944
train_iter_loss: 0.32551100850105286
train_iter_loss: 0.2310294359922409
train_iter_loss: 0.4387586712837219
train_iter_loss: 0.24612757563591003
train_iter_loss: 0.12586630880832672
train_iter_loss: 0.3296910524368286
train_iter_loss: 0.16272076964378357
train_iter_loss: 0.1693459153175354
train_iter_loss: 0.3647373616695404
train_iter_loss: 0.3185924291610718
train loss :0.2677
---------------------
Validation seg loss: 0.3650939467732074 at epoch 801
epoch =    802/  1000, exp = train
train_iter_loss: 0.21778638660907745
train_iter_loss: 0.3735157251358032
train_iter_loss: 0.20812354981899261
train_iter_loss: 0.224130317568779
train_iter_loss: 0.20424650609493256
train_iter_loss: 0.22899025678634644
train_iter_loss: 0.2902945578098297
train_iter_loss: 0.2195451259613037
train_iter_loss: 0.2515970766544342
train_iter_loss: 0.30297228693962097
train_iter_loss: 0.3029608726501465
train_iter_loss: 0.34952259063720703
train_iter_loss: 0.28774237632751465
train_iter_loss: 0.30157163739204407
train_iter_loss: 0.16182830929756165
train_iter_loss: 0.20172937214374542
train_iter_loss: 0.3874014616012573
train_iter_loss: 0.1381675750017166
train_iter_loss: 0.426674485206604
train_iter_loss: 0.030834248289465904
train_iter_loss: 0.23128941655158997
train_iter_loss: 0.22004729509353638
train_iter_loss: 0.17294982075691223
train_iter_loss: 0.1870008409023285
train_iter_loss: 0.1823117882013321
train_iter_loss: 0.48201286792755127
train_iter_loss: 0.1309954822063446
train_iter_loss: 0.20778875052928925
train_iter_loss: 0.37545526027679443
train_iter_loss: 0.48411333560943604
train_iter_loss: 0.2794966399669647
train_iter_loss: 0.13793115317821503
train_iter_loss: 0.23741839826107025
train_iter_loss: 0.41436952352523804
train_iter_loss: 0.05377841368317604
train_iter_loss: 0.16892315447330475
train_iter_loss: 0.4166319668292999
train_iter_loss: 0.23308061063289642
train_iter_loss: 0.2607633173465729
train_iter_loss: 0.26143091917037964
train_iter_loss: 0.39633336663246155
train_iter_loss: 0.25538623332977295
train_iter_loss: 0.3417181074619293
train_iter_loss: 0.3381084203720093
train_iter_loss: 0.2848113179206848
train_iter_loss: 0.2674216628074646
train_iter_loss: 0.4354316294193268
train_iter_loss: 0.3713619112968445
train_iter_loss: 0.24686981737613678
train_iter_loss: 0.27811330556869507
train_iter_loss: 0.38391172885894775
train_iter_loss: 0.23752842843532562
train_iter_loss: 0.2119060605764389
train_iter_loss: 0.12766996026039124
train_iter_loss: 0.3814283311367035
train_iter_loss: 0.3339232802391052
train_iter_loss: 0.13299904763698578
train_iter_loss: 0.35835450887680054
train_iter_loss: 0.21647700667381287
train_iter_loss: 0.3419352173805237
train_iter_loss: 0.2633330523967743
train_iter_loss: 0.30941078066825867
train_iter_loss: 0.3488655388355255
train_iter_loss: 0.27404293417930603
train_iter_loss: 0.33241382241249084
train_iter_loss: 0.29782670736312866
train_iter_loss: 0.22660601139068604
train_iter_loss: 0.2226865440607071
train_iter_loss: 0.15720055997371674
train_iter_loss: 0.2721489369869232
train_iter_loss: 0.1621233969926834
train_iter_loss: 0.4451169967651367
train_iter_loss: 0.25907984375953674
train_iter_loss: 0.20277489721775055
train_iter_loss: 0.2097754329442978
train_iter_loss: 0.14647197723388672
train_iter_loss: 0.40575140714645386
train_iter_loss: 0.3795803189277649
train_iter_loss: 0.25909167528152466
train_iter_loss: 0.3073256313800812
train_iter_loss: 0.22746159136295319
train_iter_loss: 0.19263456761837006
train_iter_loss: 0.24485811591148376
train_iter_loss: 0.44180166721343994
train_iter_loss: 0.18841645121574402
train_iter_loss: 0.32069453597068787
train_iter_loss: 0.2911609709262848
train_iter_loss: 0.3797381520271301
train_iter_loss: 0.2520157992839813
train_iter_loss: 0.226084366440773
train_iter_loss: 0.2604926824569702
train_iter_loss: 0.24084004759788513
train_iter_loss: 0.1134243980050087
train_iter_loss: 0.17606361210346222
train_iter_loss: 0.21891848742961884
train_iter_loss: 0.4162038266658783
train_iter_loss: 0.2907853126525879
train_iter_loss: 0.2562994360923767
train_iter_loss: 0.5102563500404358
train_iter_loss: 0.21518415212631226
train loss :0.2740
---------------------
Validation seg loss: 0.38951826056922384 at epoch 802
epoch =    803/  1000, exp = train
train_iter_loss: 0.33343517780303955
train_iter_loss: 0.28684258460998535
train_iter_loss: 0.248311385512352
train_iter_loss: 0.3544832468032837
train_iter_loss: 0.17605428397655487
train_iter_loss: 0.16671539843082428
train_iter_loss: 0.37473535537719727
train_iter_loss: 0.233419269323349
train_iter_loss: 0.43688130378723145
train_iter_loss: 0.2635500431060791
train_iter_loss: 0.17053872346878052
train_iter_loss: 0.14801111817359924
train_iter_loss: 0.07311379164457321
train_iter_loss: 0.3408721089363098
train_iter_loss: 0.2013375610113144
train_iter_loss: 0.1891227513551712
train_iter_loss: 0.17105145752429962
train_iter_loss: 0.4132508337497711
train_iter_loss: 0.3028641939163208
train_iter_loss: 0.2620287835597992
train_iter_loss: 0.14927296340465546
train_iter_loss: 0.17460593581199646
train_iter_loss: 0.31361326575279236
train_iter_loss: 0.43615496158599854
train_iter_loss: 0.3106943964958191
train_iter_loss: 0.06460815668106079
train_iter_loss: 0.2660048305988312
train_iter_loss: 0.3257041573524475
train_iter_loss: 0.27421340346336365
train_iter_loss: 0.30869874358177185
train_iter_loss: 0.19196803867816925
train_iter_loss: 0.28582051396369934
train_iter_loss: 0.18420177698135376
train_iter_loss: 0.21667762100696564
train_iter_loss: 0.3583986461162567
train_iter_loss: 0.20617258548736572
train_iter_loss: 0.17036567628383636
train_iter_loss: 0.23874807357788086
train_iter_loss: 0.1868281066417694
train_iter_loss: 0.2159537971019745
train_iter_loss: 0.5613663196563721
train_iter_loss: 0.23745854198932648
train_iter_loss: 0.2331506609916687
train_iter_loss: 0.11865335702896118
train_iter_loss: 0.24887119233608246
train_iter_loss: 0.2547931671142578
train_iter_loss: 0.20500002801418304
train_iter_loss: 0.3956339657306671
train_iter_loss: 0.319682776927948
train_iter_loss: 0.25461599230766296
train_iter_loss: 0.3543500006198883
train_iter_loss: 0.3433922231197357
train_iter_loss: 0.3415401875972748
train_iter_loss: 0.17532460391521454
train_iter_loss: 0.29075345396995544
train_iter_loss: 0.340402752161026
train_iter_loss: 0.3009592890739441
train_iter_loss: 0.4058164060115814
train_iter_loss: 0.21149204671382904
train_iter_loss: 0.19611483812332153
train_iter_loss: 0.12315157800912857
train_iter_loss: 0.3004421591758728
train_iter_loss: 0.37527284026145935
train_iter_loss: 0.3666943609714508
train_iter_loss: 0.2353232353925705
train_iter_loss: 0.27395692467689514
train_iter_loss: 0.2105923742055893
train_iter_loss: 0.1654924750328064
train_iter_loss: 0.1829654574394226
train_iter_loss: 0.2957604229450226
train_iter_loss: 0.39209872484207153
train_iter_loss: 0.218143031001091
train_iter_loss: 0.18220113217830658
train_iter_loss: 0.38878223299980164
train_iter_loss: 0.18756750226020813
train_iter_loss: 0.21877436339855194
train_iter_loss: 0.3162264823913574
train_iter_loss: 0.2946197986602783
train_iter_loss: 0.3554326891899109
train_iter_loss: 0.38132354617118835
train_iter_loss: 0.23605482280254364
train_iter_loss: 0.12643052637577057
train_iter_loss: 0.27279675006866455
train_iter_loss: 0.2112744003534317
train_iter_loss: 0.18408460915088654
train_iter_loss: 0.24125780165195465
train_iter_loss: 0.14701926708221436
train_iter_loss: 0.11712907254695892
train_iter_loss: 0.2726280987262726
train_iter_loss: 0.21933503448963165
train_iter_loss: 0.45603713393211365
train_iter_loss: 0.3772852122783661
train_iter_loss: 0.32478266954421997
train_iter_loss: 0.2924654483795166
train_iter_loss: 0.29855239391326904
train_iter_loss: 0.08232633024454117
train_iter_loss: 0.2682856023311615
train_iter_loss: 0.3222629427909851
train_iter_loss: 0.21838918328285217
train_iter_loss: 0.24177385866641998
train loss :0.2646
---------------------
Validation seg loss: 0.3568776135403171 at epoch 803
epoch =    804/  1000, exp = train
train_iter_loss: 0.23608051240444183
train_iter_loss: 0.4110959470272064
train_iter_loss: 0.22650493681430817
train_iter_loss: 0.35038185119628906
train_iter_loss: 0.3182750642299652
train_iter_loss: 0.2586216628551483
train_iter_loss: 0.24923866987228394
train_iter_loss: 0.15608477592468262
train_iter_loss: 0.28030967712402344
train_iter_loss: 0.18630042672157288
train_iter_loss: 0.3552727997303009
train_iter_loss: 0.20145867764949799
train_iter_loss: 0.12314873188734055
train_iter_loss: 0.2620238661766052
train_iter_loss: 0.31644701957702637
train_iter_loss: 0.3943609297275543
train_iter_loss: 0.16807399690151215
train_iter_loss: 0.17592878639698029
train_iter_loss: 0.16779831051826477
train_iter_loss: 0.4005548655986786
train_iter_loss: 0.09151414036750793
train_iter_loss: 0.32845339179039
train_iter_loss: 0.3003752529621124
train_iter_loss: 0.3138803243637085
train_iter_loss: 0.2926003336906433
train_iter_loss: 0.27469581365585327
train_iter_loss: 0.34306439757347107
train_iter_loss: 0.23971007764339447
train_iter_loss: 0.3153494596481323
train_iter_loss: 0.23000986874103546
train_iter_loss: 0.22281229496002197
train_iter_loss: 0.43615832924842834
train_iter_loss: 0.21360933780670166
train_iter_loss: 0.07745496928691864
train_iter_loss: 0.12846966087818146
train_iter_loss: 0.19415944814682007
train_iter_loss: 0.2929017245769501
train_iter_loss: 0.272000253200531
train_iter_loss: 0.37529516220092773
train_iter_loss: 0.46454909443855286
train_iter_loss: 0.28680020570755005
train_iter_loss: 0.10863730311393738
train_iter_loss: 0.31131380796432495
train_iter_loss: 0.12674126029014587
train_iter_loss: 0.24059104919433594
train_iter_loss: 0.30439314246177673
train_iter_loss: 0.24168148636817932
train_iter_loss: 0.12459180504083633
train_iter_loss: 0.15484477579593658
train_iter_loss: 0.28922829031944275
train_iter_loss: 0.26947149634361267
train_iter_loss: 0.21701501309871674
train_iter_loss: 0.2914954125881195
train_iter_loss: 0.2982321083545685
train_iter_loss: 0.32088789343833923
train_iter_loss: 0.31623831391334534
train_iter_loss: 0.23808379471302032
train_iter_loss: 0.17341238260269165
train_iter_loss: 0.2874823212623596
train_iter_loss: 0.24135182797908783
train_iter_loss: 0.19940371811389923
train_iter_loss: 0.17178300023078918
train_iter_loss: 0.265511155128479
train_iter_loss: 0.3084726333618164
train_iter_loss: 0.16994892060756683
train_iter_loss: 0.3404734134674072
train_iter_loss: 0.28211960196495056
train_iter_loss: 0.3924594521522522
train_iter_loss: 0.35884180665016174
train_iter_loss: 0.10967522859573364
train_iter_loss: 0.22810038924217224
train_iter_loss: 0.15428905189037323
train_iter_loss: 0.25569236278533936
train_iter_loss: 0.3403201103210449
train_iter_loss: 0.16477462649345398
train_iter_loss: 0.2662063539028168
train_iter_loss: 0.3112666606903076
train_iter_loss: 0.32192283868789673
train_iter_loss: 0.2268245369195938
train_iter_loss: 0.2134290188550949
train_iter_loss: 0.34064528346061707
train_iter_loss: 0.24972733855247498
train_iter_loss: 0.18514639139175415
train_iter_loss: 0.21033750474452972
train_iter_loss: 0.38669484853744507
train_iter_loss: 0.3260268568992615
train_iter_loss: 0.2853168547153473
train_iter_loss: 0.12489689141511917
train_iter_loss: 0.26147568225860596
train_iter_loss: 0.08601567149162292
train_iter_loss: 0.47011759877204895
train_iter_loss: 0.3142637312412262
train_iter_loss: 0.3060303032398224
train_iter_loss: 0.14082352817058563
train_iter_loss: 0.23632942140102386
train_iter_loss: 0.3303845524787903
train_iter_loss: 0.3008711636066437
train_iter_loss: 0.4170522689819336
train_iter_loss: 0.25209397077560425
train_iter_loss: 0.2798503041267395
train loss :0.2634
---------------------
Validation seg loss: 0.36375793612579693 at epoch 804
epoch =    805/  1000, exp = train
train_iter_loss: 0.24628880620002747
train_iter_loss: 0.29180601239204407
train_iter_loss: 0.17221598327159882
train_iter_loss: 0.13485626876354218
train_iter_loss: 0.29654446244239807
train_iter_loss: 0.3092004060745239
train_iter_loss: 0.3499755859375
train_iter_loss: 0.42036738991737366
train_iter_loss: 0.2521112859249115
train_iter_loss: 0.16053611040115356
train_iter_loss: 0.3770601153373718
train_iter_loss: 0.20373588800430298
train_iter_loss: 0.15250557661056519
train_iter_loss: 0.28252989053726196
train_iter_loss: 0.37992754578590393
train_iter_loss: 0.24608775973320007
train_iter_loss: 0.25870972871780396
train_iter_loss: 0.2580516040325165
train_iter_loss: 0.36640748381614685
train_iter_loss: 0.15849515795707703
train_iter_loss: 0.3861958980560303
train_iter_loss: 0.2528805732727051
train_iter_loss: 0.33045539259910583
train_iter_loss: 0.23847374320030212
train_iter_loss: 0.039317786693573
train_iter_loss: 0.23790383338928223
train_iter_loss: 0.39668866991996765
train_iter_loss: 0.0710880234837532
train_iter_loss: 0.4398351311683655
train_iter_loss: 0.16778026521205902
train_iter_loss: 0.20937542617321014
train_iter_loss: 0.33103302121162415
train_iter_loss: 0.24534542858600616
train_iter_loss: 0.18560166656970978
train_iter_loss: 0.2217094600200653
train_iter_loss: 0.35714292526245117
train_iter_loss: 0.180320605635643
train_iter_loss: 0.10233625769615173
train_iter_loss: 0.31922873854637146
train_iter_loss: 0.09119468927383423
train_iter_loss: 0.40968021750450134
train_iter_loss: 0.5101027488708496
train_iter_loss: 0.25823846459388733
train_iter_loss: 0.24801062047481537
train_iter_loss: 0.2803471088409424
train_iter_loss: 0.3192536532878876
train_iter_loss: 0.23867261409759521
train_iter_loss: 0.30491605401039124
train_iter_loss: 0.3026100993156433
train_iter_loss: 0.26068300008773804
train_iter_loss: 0.37072765827178955
train_iter_loss: 0.24643327295780182
train_iter_loss: 0.1381927728652954
train_iter_loss: 0.26760947704315186
train_iter_loss: 0.2804214060306549
train_iter_loss: 0.3015233874320984
train_iter_loss: 0.23337413370609283
train_iter_loss: 0.28589528799057007
train_iter_loss: 0.20736631751060486
train_iter_loss: 0.30886393785476685
train_iter_loss: 0.3055436313152313
train_iter_loss: 0.41708189249038696
train_iter_loss: 0.3707961142063141
train_iter_loss: 0.05266224965453148
train_iter_loss: 0.2089812159538269
train_iter_loss: 0.18867269158363342
train_iter_loss: 0.36979132890701294
train_iter_loss: 0.16320276260375977
train_iter_loss: 0.22589242458343506
train_iter_loss: 0.20796626806259155
train_iter_loss: 0.1592663675546646
train_iter_loss: 0.3410792350769043
train_iter_loss: 0.30431801080703735
train_iter_loss: 0.30206239223480225
train_iter_loss: 0.2859635651111603
train_iter_loss: 0.2931695282459259
train_iter_loss: 0.21021680533885956
train_iter_loss: 0.267233669757843
train_iter_loss: 0.2184639722108841
train_iter_loss: 0.32683154940605164
train_iter_loss: 0.47208255529403687
train_iter_loss: 0.2590920329093933
train_iter_loss: 0.2222374826669693
train_iter_loss: 0.19211168587207794
train_iter_loss: 0.3289920687675476
train_iter_loss: 0.19741760194301605
train_iter_loss: 0.2874271869659424
train_iter_loss: 0.1657482534646988
train_iter_loss: 0.19770009815692902
train_iter_loss: 0.22186651825904846
train_iter_loss: 0.3785219192504883
train_iter_loss: 0.14422041177749634
train_iter_loss: 0.23847998678684235
train_iter_loss: 0.15300171077251434
train_iter_loss: 0.17927168309688568
train_iter_loss: 0.285664826631546
train_iter_loss: 0.33735185861587524
train_iter_loss: 0.21960648894309998
train_iter_loss: 0.19696222245693207
train_iter_loss: 0.22050490975379944
train loss :0.2628
---------------------
Validation seg loss: 0.3419261365988345 at epoch 805
********************
best_val_epoch_loss:  0.3419261365988345
MODEL UPDATED
epoch =    806/  1000, exp = train
train_iter_loss: 0.21174509823322296
train_iter_loss: 0.17045947909355164
train_iter_loss: 0.21068762242794037
train_iter_loss: 0.20703205466270447
train_iter_loss: 0.32850444316864014
train_iter_loss: 0.20862838625907898
train_iter_loss: 0.30815988779067993
train_iter_loss: 0.3123198449611664
train_iter_loss: 0.3838399052619934
train_iter_loss: 0.31291159987449646
train_iter_loss: 0.20436756312847137
train_iter_loss: 0.20105396211147308
train_iter_loss: 0.13055865466594696
train_iter_loss: 0.1924736201763153
train_iter_loss: 0.2689759135246277
train_iter_loss: 0.23868365585803986
train_iter_loss: 0.2562147378921509
train_iter_loss: 0.17742452025413513
train_iter_loss: 0.17665891349315643
train_iter_loss: 0.24567900598049164
train_iter_loss: 0.263641357421875
train_iter_loss: 0.27550575137138367
train_iter_loss: 0.23167987167835236
train_iter_loss: 0.3265348970890045
train_iter_loss: 0.020293740555644035
train_iter_loss: 0.2851226329803467
train_iter_loss: 0.2717686593532562
train_iter_loss: 0.2873852252960205
train_iter_loss: 0.11796735227108002
train_iter_loss: 0.190360888838768
train_iter_loss: 0.3282943367958069
train_iter_loss: 0.2149907946586609
train_iter_loss: 0.4433608055114746
train_iter_loss: 0.4100373685359955
train_iter_loss: 0.3699614405632019
train_iter_loss: 0.2506957948207855
train_iter_loss: 0.13963153958320618
train_iter_loss: 0.25915658473968506
train_iter_loss: 0.3207572102546692
train_iter_loss: 0.16788233816623688
train_iter_loss: 0.04552239924669266
train_iter_loss: 0.3840729892253876
train_iter_loss: 0.1356269121170044
train_iter_loss: 0.24105729162693024
train_iter_loss: 0.11410399526357651
train_iter_loss: 0.16554823517799377
train_iter_loss: 0.19152501225471497
train_iter_loss: 0.1333925873041153
train_iter_loss: 0.2030622363090515
train_iter_loss: 0.2978702783584595
train_iter_loss: 0.24716174602508545
train_iter_loss: 0.20528584718704224
train_iter_loss: 0.2944627106189728
train_iter_loss: 0.24371841549873352
train_iter_loss: 0.260831356048584
train_iter_loss: 0.42358914017677307
train_iter_loss: 0.28431156277656555
train_iter_loss: 0.3262823820114136
train_iter_loss: 0.27801647782325745
train_iter_loss: 0.24906504154205322
train_iter_loss: 0.22272948920726776
train_iter_loss: 0.2809310257434845
train_iter_loss: 0.3424733579158783
train_iter_loss: 0.17771708965301514
train_iter_loss: 0.17794035375118256
train_iter_loss: 0.20299969613552094
train_iter_loss: 0.33439934253692627
train_iter_loss: 0.25163841247558594
train_iter_loss: 0.21152392029762268
train_iter_loss: 0.42409250140190125
train_iter_loss: 0.4139229655265808
train_iter_loss: 0.32924821972846985
train_iter_loss: 0.2169513702392578
train_iter_loss: 0.34940123558044434
train_iter_loss: 0.2789402902126312
train_iter_loss: 0.3360539972782135
train_iter_loss: 0.17735131084918976
train_iter_loss: 0.31692829728126526
train_iter_loss: 0.6060592532157898
train_iter_loss: 0.20198167860507965
train_iter_loss: 0.4790579676628113
train_iter_loss: 0.19718453288078308
train_iter_loss: 0.041666530072689056
train_iter_loss: 0.3058507442474365
train_iter_loss: 0.32404887676239014
train_iter_loss: 0.40284138917922974
train_iter_loss: 0.2646327614784241
train_iter_loss: 0.3105812072753906
train_iter_loss: 0.19969332218170166
train_iter_loss: 0.19949714839458466
train_iter_loss: 0.3301272392272949
train_iter_loss: 0.17981477081775665
train_iter_loss: 0.3338261544704437
train_iter_loss: 0.31593188643455505
train_iter_loss: 0.27527445554733276
train_iter_loss: 0.22007012367248535
train_iter_loss: 0.47267162799835205
train_iter_loss: 0.30162307620048523
train_iter_loss: 0.2996392846107483
train_iter_loss: 0.11215351521968842
train loss :0.2633
---------------------
Validation seg loss: 0.3480974632438342 at epoch 806
epoch =    807/  1000, exp = train
train_iter_loss: 0.23970185220241547
train_iter_loss: 0.24929940700531006
train_iter_loss: 0.31321465969085693
train_iter_loss: 0.34891802072525024
train_iter_loss: 0.3144230544567108
train_iter_loss: 0.1716148853302002
train_iter_loss: 0.32279443740844727
train_iter_loss: 0.31024864315986633
train_iter_loss: 0.32609647512435913
train_iter_loss: 0.34846776723861694
train_iter_loss: 0.2744210362434387
train_iter_loss: 0.2636331617832184
train_iter_loss: 0.1853988617658615
train_iter_loss: 0.18900565803050995
train_iter_loss: 0.3529420793056488
train_iter_loss: 0.1164235919713974
train_iter_loss: 0.21580898761749268
train_iter_loss: 0.35125449299812317
train_iter_loss: 0.24027124047279358
train_iter_loss: 0.1552751213312149
train_iter_loss: 0.3637324273586273
train_iter_loss: 0.22931474447250366
train_iter_loss: 0.2404833287000656
train_iter_loss: 0.14982165396213531
train_iter_loss: 0.3256145715713501
train_iter_loss: 0.24720941483974457
train_iter_loss: 0.08655061572790146
train_iter_loss: 0.5933799147605896
train_iter_loss: 0.19216090440750122
train_iter_loss: 0.16821369528770447
train_iter_loss: 0.4014444649219513
train_iter_loss: 0.1783536970615387
train_iter_loss: 0.22250162065029144
train_iter_loss: 0.21792304515838623
train_iter_loss: 0.2685171067714691
train_iter_loss: 0.22461912035942078
train_iter_loss: 0.2303069829940796
train_iter_loss: 0.2748640775680542
train_iter_loss: 0.14316050708293915
train_iter_loss: 0.2511771321296692
train_iter_loss: 0.2322172075510025
train_iter_loss: 0.09551897644996643
train_iter_loss: 0.16740280389785767
train_iter_loss: 0.19812707602977753
train_iter_loss: 0.2743200659751892
train_iter_loss: 0.3211319148540497
train_iter_loss: 0.24492524564266205
train_iter_loss: 0.3950320780277252
train_iter_loss: 0.2781154215335846
train_iter_loss: 0.1320953220129013
train_iter_loss: 0.29828348755836487
train_iter_loss: 0.2591778635978699
train_iter_loss: 0.2676459848880768
train_iter_loss: 0.3441235423088074
train_iter_loss: 0.35333746671676636
train_iter_loss: 0.17264531552791595
train_iter_loss: 0.2561938762664795
train_iter_loss: 0.2919534742832184
train_iter_loss: 0.14650611579418182
train_iter_loss: 0.30008435249328613
train_iter_loss: 0.46575018763542175
train_iter_loss: 0.28263357281684875
train_iter_loss: 0.21152879297733307
train_iter_loss: 0.19397898018360138
train_iter_loss: 0.19827811419963837
train_iter_loss: 0.30647510290145874
train_iter_loss: 0.4138706624507904
train_iter_loss: 0.30739539861679077
train_iter_loss: 0.3446309566497803
train_iter_loss: 0.18006673455238342
train_iter_loss: 0.1035555899143219
train_iter_loss: 0.2514795660972595
train_iter_loss: 0.3188554048538208
train_iter_loss: 0.2339540421962738
train_iter_loss: 0.2999051511287689
train_iter_loss: 0.13986478745937347
train_iter_loss: 0.17722366750240326
train_iter_loss: 0.3039010763168335
train_iter_loss: 0.08826470375061035
train_iter_loss: 0.19020815193653107
train_iter_loss: 0.13715243339538574
train_iter_loss: 0.5325399041175842
train_iter_loss: 0.19485609233379364
train_iter_loss: 0.2883296310901642
train_iter_loss: 0.28682154417037964
train_iter_loss: 0.25738221406936646
train_iter_loss: 0.19536840915679932
train_iter_loss: 0.38333457708358765
train_iter_loss: 0.27599775791168213
train_iter_loss: 0.2724703848361969
train_iter_loss: 0.19441558420658112
train_iter_loss: 0.16799429059028625
train_iter_loss: 0.15222808718681335
train_iter_loss: 0.47957825660705566
train_iter_loss: 0.3981764614582062
train_iter_loss: 0.18635685741901398
train_iter_loss: 0.0808931514620781
train_iter_loss: 0.3397607207298279
train_iter_loss: 0.36190706491470337
train_iter_loss: 0.08814043551683426
train loss :0.2588
---------------------
Validation seg loss: 0.4233340576624955 at epoch 807
epoch =    808/  1000, exp = train
train_iter_loss: 0.34352731704711914
train_iter_loss: 0.275942862033844
train_iter_loss: 0.2535499930381775
train_iter_loss: 0.16979433596134186
train_iter_loss: 0.3559095859527588
train_iter_loss: 0.2934061884880066
train_iter_loss: 0.3239763081073761
train_iter_loss: 0.24678687751293182
train_iter_loss: 0.2161288857460022
train_iter_loss: 0.24487727880477905
train_iter_loss: 0.22770538926124573
train_iter_loss: 0.13803015649318695
train_iter_loss: 0.2902679443359375
train_iter_loss: 0.14918744564056396
train_iter_loss: 0.16002967953681946
train_iter_loss: 0.23433354496955872
train_iter_loss: 0.28224313259124756
train_iter_loss: 0.24386528134346008
train_iter_loss: 0.26331478357315063
train_iter_loss: 0.02403470128774643
train_iter_loss: 0.31301963329315186
train_iter_loss: 0.22302104532718658
train_iter_loss: 0.33147382736206055
train_iter_loss: 0.20524732768535614
train_iter_loss: 0.3049822747707367
train_iter_loss: 0.24594993889331818
train_iter_loss: 0.24281571805477142
train_iter_loss: 0.21564285457134247
train_iter_loss: 0.21495328843593597
train_iter_loss: 0.0858924388885498
train_iter_loss: 0.3222716450691223
train_iter_loss: 0.28325894474983215
train_iter_loss: 0.3604457676410675
train_iter_loss: 0.24643908441066742
train_iter_loss: 0.19717521965503693
train_iter_loss: 0.2060222178697586
train_iter_loss: 0.2935517132282257
train_iter_loss: 0.2452123761177063
train_iter_loss: 0.21182659268379211
train_iter_loss: 0.29296043515205383
train_iter_loss: 0.2797262668609619
train_iter_loss: 0.24275647103786469
train_iter_loss: 0.39412981271743774
train_iter_loss: 0.2838405966758728
train_iter_loss: 0.19532588124275208
train_iter_loss: 0.3665256202220917
train_iter_loss: 0.2723487317562103
train_iter_loss: 0.5402460694313049
train_iter_loss: 0.24652248620986938
train_iter_loss: 0.1905212104320526
train_iter_loss: 0.29141202569007874
train_iter_loss: 0.24986843764781952
train_iter_loss: 0.22999978065490723
train_iter_loss: 0.250159353017807
train_iter_loss: 0.2294127494096756
train_iter_loss: 0.1748303920030594
train_iter_loss: 0.17919957637786865
train_iter_loss: 0.3872382342815399
train_iter_loss: 0.13426856696605682
train_iter_loss: 0.17579244077205658
train_iter_loss: 0.18776054680347443
train_iter_loss: 0.28894898295402527
train_iter_loss: 0.26284119486808777
train_iter_loss: 0.23318429291248322
train_iter_loss: 0.2526993155479431
train_iter_loss: 0.30192404985427856
train_iter_loss: 0.22047875821590424
train_iter_loss: 0.5293973088264465
train_iter_loss: 0.2989721894264221
train_iter_loss: 0.22904744744300842
train_iter_loss: 0.45853254199028015
train_iter_loss: 0.13353805243968964
train_iter_loss: 0.18975549936294556
train_iter_loss: 0.2667044997215271
train_iter_loss: 0.1652652770280838
train_iter_loss: 0.3664567470550537
train_iter_loss: 0.20111224055290222
train_iter_loss: 0.35732442140579224
train_iter_loss: 0.20230576395988464
train_iter_loss: 0.23393391072750092
train_iter_loss: 0.26340410113334656
train_iter_loss: 0.25694528222084045
train_iter_loss: 0.32438477873802185
train_iter_loss: 0.32722634077072144
train_iter_loss: 0.28051304817199707
train_iter_loss: 0.27546072006225586
train_iter_loss: 0.1119837835431099
train_iter_loss: 0.149583637714386
train_iter_loss: 0.3450578451156616
train_iter_loss: 0.13945427536964417
train_iter_loss: 0.3008466958999634
train_iter_loss: 0.16802047193050385
train_iter_loss: 0.24845877289772034
train_iter_loss: 0.3172006607055664
train_iter_loss: 0.40961942076683044
train_iter_loss: 0.2772194743156433
train_iter_loss: 0.23183530569076538
train_iter_loss: 0.2822630703449249
train_iter_loss: 0.21641553938388824
train_iter_loss: 0.21705347299575806
train loss :0.2588
---------------------
Validation seg loss: 0.3548356077440505 at epoch 808
epoch =    809/  1000, exp = train
train_iter_loss: 0.22699588537216187
train_iter_loss: 0.23565900325775146
train_iter_loss: 0.20507778227329254
train_iter_loss: 0.3636913001537323
train_iter_loss: 0.1859275847673416
train_iter_loss: 0.32851970195770264
train_iter_loss: 0.25066453218460083
train_iter_loss: 0.22075030207633972
train_iter_loss: 0.24072763323783875
train_iter_loss: 0.24394625425338745
train_iter_loss: 0.33867815136909485
train_iter_loss: 0.22975626587867737
train_iter_loss: 0.21147459745407104
train_iter_loss: 0.2131025791168213
train_iter_loss: 0.18428905308246613
train_iter_loss: 0.312691330909729
train_iter_loss: 0.1844903826713562
train_iter_loss: 0.22473925352096558
train_iter_loss: 0.2954135537147522
train_iter_loss: 0.16644343733787537
train_iter_loss: 0.20904666185379028
train_iter_loss: 0.3402820825576782
train_iter_loss: 0.18796774744987488
train_iter_loss: 0.29707440733909607
train_iter_loss: 0.2920932471752167
train_iter_loss: 0.25752463936805725
train_iter_loss: 0.3633732795715332
train_iter_loss: 0.2596442699432373
train_iter_loss: 0.1608162522315979
train_iter_loss: 0.16116124391555786
train_iter_loss: 0.30754461884498596
train_iter_loss: 0.16603362560272217
train_iter_loss: 0.328777015209198
train_iter_loss: 0.37354394793510437
train_iter_loss: 0.2313293069601059
train_iter_loss: 0.08354634046554565
train_iter_loss: 0.221173495054245
train_iter_loss: 0.1854463666677475
train_iter_loss: 0.31328681111335754
train_iter_loss: 0.27202266454696655
train_iter_loss: 0.3169950246810913
train_iter_loss: 0.2998402416706085
train_iter_loss: 0.13253486156463623
train_iter_loss: 0.18514961004257202
train_iter_loss: 0.42825451493263245
train_iter_loss: 0.2723965048789978
train_iter_loss: 0.022019121795892715
train_iter_loss: 0.39459699392318726
train_iter_loss: 0.2411939799785614
train_iter_loss: 0.27046528458595276
train_iter_loss: 0.2223595529794693
train_iter_loss: 0.19681091606616974
train_iter_loss: 0.21044401824474335
train_iter_loss: 0.109991155564785
train_iter_loss: 0.18357785046100616
train_iter_loss: 0.21017122268676758
train_iter_loss: 0.16499805450439453
train_iter_loss: 0.29540514945983887
train_iter_loss: 0.358413964509964
train_iter_loss: 0.24806497991085052
train_iter_loss: 0.2943994104862213
train_iter_loss: 0.2518789768218994
train_iter_loss: 0.2729826271533966
train_iter_loss: 0.16594330966472626
train_iter_loss: 0.34340113401412964
train_iter_loss: 0.2015276849269867
train_iter_loss: 0.2400095909833908
train_iter_loss: 0.1820354014635086
train_iter_loss: 0.24856989085674286
train_iter_loss: 0.24347110092639923
train_iter_loss: 0.23077525198459625
train_iter_loss: 0.3425865173339844
train_iter_loss: 0.4406318664550781
train_iter_loss: 0.12484408915042877
train_iter_loss: 0.28351980447769165
train_iter_loss: 0.21145665645599365
train_iter_loss: 0.34793469309806824
train_iter_loss: 0.2365780919790268
train_iter_loss: 0.3166451156139374
train_iter_loss: 0.11512889713048935
train_iter_loss: 0.2054763287305832
train_iter_loss: 0.22877542674541473
train_iter_loss: 0.40233314037323
train_iter_loss: 0.39751893281936646
train_iter_loss: 0.24032920598983765
train_iter_loss: 0.11246862262487411
train_iter_loss: 0.3485884666442871
train_iter_loss: 0.37407562136650085
train_iter_loss: 0.37370356917381287
train_iter_loss: 0.3254879117012024
train_iter_loss: 0.2768344581127167
train_iter_loss: 0.311710000038147
train_iter_loss: 0.26050975918769836
train_iter_loss: 0.2518007457256317
train_iter_loss: 0.15506315231323242
train_iter_loss: 0.4146416485309601
train_iter_loss: 0.3072800934314728
train_iter_loss: 0.25751519203186035
train_iter_loss: 0.2993868589401245
train_iter_loss: 0.18694813549518585
train loss :0.2572
---------------------
Validation seg loss: 0.3693905762563688 at epoch 809
epoch =    810/  1000, exp = train
train_iter_loss: 0.35001125931739807
train_iter_loss: 0.21632987260818481
train_iter_loss: 0.11208341270685196
train_iter_loss: 0.34300240874290466
train_iter_loss: 0.3279862403869629
train_iter_loss: 0.26510798931121826
train_iter_loss: 0.31669220328330994
train_iter_loss: 0.2416459321975708
train_iter_loss: 0.04088924452662468
train_iter_loss: 0.2605271339416504
train_iter_loss: 0.3205384314060211
train_iter_loss: 0.26172763109207153
train_iter_loss: 0.29964330792427063
train_iter_loss: 0.24405445158481598
train_iter_loss: 0.28947213292121887
train_iter_loss: 0.29180556535720825
train_iter_loss: 0.23443593084812164
train_iter_loss: 0.19795402884483337
train_iter_loss: 0.290006548166275
train_iter_loss: 0.23530781269073486
train_iter_loss: 0.14937083423137665
train_iter_loss: 0.2668613791465759
train_iter_loss: 0.27978816628456116
train_iter_loss: 0.19561052322387695
train_iter_loss: 0.14093813300132751
train_iter_loss: 0.205551877617836
train_iter_loss: 0.3017077147960663
train_iter_loss: 0.14398549497127533
train_iter_loss: 0.2336353361606598
train_iter_loss: 0.17189012467861176
train_iter_loss: 0.24970011413097382
train_iter_loss: 0.23460406064987183
train_iter_loss: 0.3487721383571625
train_iter_loss: 0.4647669494152069
train_iter_loss: 0.09179426729679108
train_iter_loss: 0.24891310930252075
train_iter_loss: 0.23393119871616364
train_iter_loss: 0.28519803285598755
train_iter_loss: 0.4949994683265686
train_iter_loss: 0.4042139947414398
train_iter_loss: 0.21033960580825806
train_iter_loss: 0.30661991238594055
train_iter_loss: 0.25561556220054626
train_iter_loss: 0.21295884251594543
train_iter_loss: 0.11604934930801392
train_iter_loss: 0.16178734600543976
train_iter_loss: 0.2079828530550003
train_iter_loss: 0.19140228629112244
train_iter_loss: 0.2915343642234802
train_iter_loss: 0.390799880027771
train_iter_loss: 0.46551790833473206
train_iter_loss: 0.09370746463537216
train_iter_loss: 0.28204599022865295
train_iter_loss: 0.20291483402252197
train_iter_loss: 0.24583512544631958
train_iter_loss: 0.27506667375564575
train_iter_loss: 0.2807005047798157
train_iter_loss: 0.33962172269821167
train_iter_loss: 0.255971223115921
train_iter_loss: 0.3016052544116974
train_iter_loss: 0.32162362337112427
train_iter_loss: 0.3026965856552124
train_iter_loss: 0.21772147715091705
train_iter_loss: 0.18745207786560059
train_iter_loss: 0.3390158712863922
train_iter_loss: 0.24652281403541565
train_iter_loss: 0.22736810147762299
train_iter_loss: 0.24857750535011292
train_iter_loss: 0.22942620515823364
train_iter_loss: 0.3215217888355255
train_iter_loss: 0.34181100130081177
train_iter_loss: 0.17061454057693481
train_iter_loss: 0.2624916434288025
train_iter_loss: 0.17776347696781158
train_iter_loss: 0.23484858870506287
train_iter_loss: 0.20136801898479462
train_iter_loss: 0.27790331840515137
train_iter_loss: 0.19485093653202057
train_iter_loss: 0.3101472556591034
train_iter_loss: 0.2762508988380432
train_iter_loss: 0.16008906066417694
train_iter_loss: 0.3167666792869568
train_iter_loss: 0.3077827990055084
train_iter_loss: 0.30090802907943726
train_iter_loss: 0.2733386754989624
train_iter_loss: 0.18500903248786926
train_iter_loss: 0.2549590766429901
train_iter_loss: 0.3261071443557739
train_iter_loss: 0.19300900399684906
train_iter_loss: 0.3030938506126404
train_iter_loss: 0.19996075332164764
train_iter_loss: 0.31894415616989136
train_iter_loss: 0.24153605103492737
train_iter_loss: 0.1882992833852768
train_iter_loss: 0.1804129183292389
train_iter_loss: 0.25676876306533813
train_iter_loss: 0.2601858973503113
train_iter_loss: 0.24157528579235077
train_iter_loss: 0.20342668890953064
train_iter_loss: 0.2923513650894165
train loss :0.2573
---------------------
Validation seg loss: 0.3507330214850745 at epoch 810
epoch =    811/  1000, exp = train
train_iter_loss: 0.3352264165878296
train_iter_loss: 0.13106998801231384
train_iter_loss: 0.2236139178276062
train_iter_loss: 0.1819022297859192
train_iter_loss: 0.18096895515918732
train_iter_loss: 0.2976725697517395
train_iter_loss: 0.3660050928592682
train_iter_loss: 0.17367492616176605
train_iter_loss: 0.23500698804855347
train_iter_loss: 0.14366793632507324
train_iter_loss: 0.202725350856781
train_iter_loss: 0.3102351725101471
train_iter_loss: 0.3867625296115875
train_iter_loss: 0.3862803876399994
train_iter_loss: 0.43309032917022705
train_iter_loss: 0.29667046666145325
train_iter_loss: 0.3153316080570221
train_iter_loss: 0.3110071122646332
train_iter_loss: 0.2443724423646927
train_iter_loss: 0.27323290705680847
train_iter_loss: 0.21558716893196106
train_iter_loss: 0.29481959342956543
train_iter_loss: 0.21574822068214417
train_iter_loss: 0.24392420053482056
train_iter_loss: 0.31379640102386475
train_iter_loss: 0.22321341931819916
train_iter_loss: 0.10644584149122238
train_iter_loss: 0.2951951026916504
train_iter_loss: 0.24173355102539062
train_iter_loss: 0.18303793668746948
train_iter_loss: 0.4299375116825104
train_iter_loss: 0.41612622141838074
train_iter_loss: 0.27683597803115845
train_iter_loss: 0.3622192144393921
train_iter_loss: 0.2184564769268036
train_iter_loss: 0.2376975268125534
train_iter_loss: 0.24524004757404327
train_iter_loss: 0.21386182308197021
train_iter_loss: 0.23480898141860962
train_iter_loss: 0.31147468090057373
train_iter_loss: 0.12858200073242188
train_iter_loss: 0.2391863763332367
train_iter_loss: 0.36856865882873535
train_iter_loss: 0.18351784348487854
train_iter_loss: 0.23468907177448273
train_iter_loss: 0.34572455286979675
train_iter_loss: 0.34000903367996216
train_iter_loss: 0.25846952199935913
train_iter_loss: 0.2669418156147003
train_iter_loss: 0.21432465314865112
train_iter_loss: 0.0929432138800621
train_iter_loss: 0.32346922159194946
train_iter_loss: 0.5498296618461609
train_iter_loss: 0.15996630489826202
train_iter_loss: 0.23316222429275513
train_iter_loss: 0.2707563042640686
train_iter_loss: 0.35052207112312317
train_iter_loss: 0.2784576416015625
train_iter_loss: 0.16560770571231842
train_iter_loss: 0.22087639570236206
train_iter_loss: 0.3025735318660736
train_iter_loss: 0.2610994577407837
train_iter_loss: 0.2865574359893799
train_iter_loss: 0.18371936678886414
train_iter_loss: 0.32005804777145386
train_iter_loss: 0.18885438144207
train_iter_loss: 0.2489946335554123
train_iter_loss: 0.1937370002269745
train_iter_loss: 0.3331281840801239
train_iter_loss: 0.19210107624530792
train_iter_loss: 0.16914178431034088
train_iter_loss: 0.30459582805633545
train_iter_loss: 0.09732646495103836
train_iter_loss: 0.26635897159576416
train_iter_loss: 0.17084254324436188
train_iter_loss: 0.21349790692329407
train_iter_loss: 0.33391228318214417
train_iter_loss: 0.29235315322875977
train_iter_loss: 0.21067185699939728
train_iter_loss: 0.17379401624202728
train_iter_loss: 0.3452052175998688
train_iter_loss: 0.29029369354248047
train_iter_loss: 0.1321818232536316
train_iter_loss: 0.19341139495372772
train_iter_loss: 0.25347962975502014
train_iter_loss: 0.2552244961261749
train_iter_loss: 0.34487512707710266
train_iter_loss: 0.398006409406662
train_iter_loss: 0.24963350594043732
train_iter_loss: 0.23475463688373566
train_iter_loss: 0.14897611737251282
train_iter_loss: 0.18079160153865814
train_iter_loss: 0.23073932528495789
train_iter_loss: 0.16496410965919495
train_iter_loss: 0.11972928792238235
train_iter_loss: 0.2766057848930359
train_iter_loss: 0.24926362931728363
train_iter_loss: 0.28277692198753357
train_iter_loss: 0.2984705865383148
train_iter_loss: 0.30919501185417175
train loss :0.2589
---------------------
Validation seg loss: 0.3629350366536528 at epoch 811
epoch =    812/  1000, exp = train
train_iter_loss: 0.28545576333999634
train_iter_loss: 0.2625764310359955
train_iter_loss: 0.20726081728935242
train_iter_loss: 0.18099786341190338
train_iter_loss: 0.284414678812027
train_iter_loss: 0.22986453771591187
train_iter_loss: 0.23746070265769958
train_iter_loss: 0.4975642263889313
train_iter_loss: 0.05339187756180763
train_iter_loss: 0.3325258493423462
train_iter_loss: 0.26806390285491943
train_iter_loss: 0.12269740551710129
train_iter_loss: 0.13343049585819244
train_iter_loss: 0.2679835855960846
train_iter_loss: 0.19042547047138214
train_iter_loss: 0.2317487746477127
train_iter_loss: 0.35984566807746887
train_iter_loss: 0.2196248322725296
train_iter_loss: 0.28377190232276917
train_iter_loss: 0.29320889711380005
train_iter_loss: 0.2595902681350708
train_iter_loss: 0.20177538692951202
train_iter_loss: 0.13944929838180542
train_iter_loss: 0.26758521795272827
train_iter_loss: 0.3108341693878174
train_iter_loss: 0.18290752172470093
train_iter_loss: 0.3384092152118683
train_iter_loss: 0.2569761872291565
train_iter_loss: 0.15925827622413635
train_iter_loss: 0.17336338758468628
train_iter_loss: 0.23349012434482574
train_iter_loss: 0.13967086374759674
train_iter_loss: 0.27676835656166077
train_iter_loss: 0.09880878031253815
train_iter_loss: 0.24038837850093842
train_iter_loss: 0.20384326577186584
train_iter_loss: 0.2390366494655609
train_iter_loss: 0.27357712388038635
train_iter_loss: 0.17222990095615387
train_iter_loss: 0.3375398814678192
train_iter_loss: 0.3126438558101654
train_iter_loss: 0.2334001362323761
train_iter_loss: 0.32149437069892883
train_iter_loss: 0.2467581033706665
train_iter_loss: 0.24099856615066528
train_iter_loss: 0.08300276845693588
train_iter_loss: 0.2953951954841614
train_iter_loss: 0.20749397575855255
train_iter_loss: 0.2601388394832611
train_iter_loss: 0.22530439496040344
train_iter_loss: 0.25359949469566345
train_iter_loss: 0.38297325372695923
train_iter_loss: 0.4267594516277313
train_iter_loss: 0.38063156604766846
train_iter_loss: 0.24268588423728943
train_iter_loss: 0.2973270118236542
train_iter_loss: 0.21060854196548462
train_iter_loss: 0.33320891857147217
train_iter_loss: 0.11984938383102417
train_iter_loss: 0.22901464998722076
train_iter_loss: 0.08245446532964706
train_iter_loss: 0.34689733386039734
train_iter_loss: 0.15753529965877533
train_iter_loss: 0.18583472073078156
train_iter_loss: 0.2993309497833252
train_iter_loss: 0.2311452031135559
train_iter_loss: 0.17615196108818054
train_iter_loss: 0.30139368772506714
train_iter_loss: 0.11489872634410858
train_iter_loss: 0.31351304054260254
train_iter_loss: 0.3209981322288513
train_iter_loss: 0.15731920301914215
train_iter_loss: 0.4125017523765564
train_iter_loss: 0.12912246584892273
train_iter_loss: 0.22218750417232513
train_iter_loss: 0.2320798933506012
train_iter_loss: 0.18516500294208527
train_iter_loss: 0.1703978329896927
train_iter_loss: 0.30694669485092163
train_iter_loss: 0.3239641785621643
train_iter_loss: 0.24489635229110718
train_iter_loss: 0.2151481658220291
train_iter_loss: 0.6897239685058594
train_iter_loss: 0.2315129190683365
train_iter_loss: 0.28917208313941956
train_iter_loss: 0.21128886938095093
train_iter_loss: 0.36626991629600525
train_iter_loss: 0.33171361684799194
train_iter_loss: 0.13990525901317596
train_iter_loss: 0.2111009657382965
train_iter_loss: 0.4167076349258423
train_iter_loss: 0.34315142035484314
train_iter_loss: 0.27392375469207764
train_iter_loss: 0.3540489971637726
train_iter_loss: 0.21006612479686737
train_iter_loss: 0.4655035734176636
train_iter_loss: 0.23893696069717407
train_iter_loss: 0.26622244715690613
train_iter_loss: 0.18268872797489166
train_iter_loss: 0.23196128010749817
train loss :0.2559
---------------------
Validation seg loss: 0.3537425101901633 at epoch 812
epoch =    813/  1000, exp = train
train_iter_loss: 0.28164952993392944
train_iter_loss: 0.27268609404563904
train_iter_loss: 0.28360241651535034
train_iter_loss: 0.2811250686645508
train_iter_loss: 0.17539703845977783
train_iter_loss: 0.3248193562030792
train_iter_loss: 0.2324092835187912
train_iter_loss: 0.27776479721069336
train_iter_loss: 0.3293255865573883
train_iter_loss: 0.29940757155418396
train_iter_loss: 0.19385379552841187
train_iter_loss: 0.15111374855041504
train_iter_loss: 0.27748870849609375
train_iter_loss: 0.35622063279151917
train_iter_loss: 0.16082917153835297
train_iter_loss: 0.2512127161026001
train_iter_loss: 0.1479194164276123
train_iter_loss: 0.3094385266304016
train_iter_loss: 0.18848854303359985
train_iter_loss: 0.1799139380455017
train_iter_loss: 0.2501402497291565
train_iter_loss: 0.22961771488189697
train_iter_loss: 0.4083435833454132
train_iter_loss: 0.3378125727176666
train_iter_loss: 0.25116199254989624
train_iter_loss: 0.2790459394454956
train_iter_loss: 0.19783875346183777
train_iter_loss: 0.20735666155815125
train_iter_loss: 0.24823565781116486
train_iter_loss: 0.19553743302822113
train_iter_loss: 0.48732656240463257
train_iter_loss: 0.4151226580142975
train_iter_loss: 0.33906450867652893
train_iter_loss: 0.13823583722114563
train_iter_loss: 0.23684094846248627
train_iter_loss: 0.14365211129188538
train_iter_loss: 0.23899000883102417
train_iter_loss: 0.2959136962890625
train_iter_loss: 0.23158295452594757
train_iter_loss: 0.19130565226078033
train_iter_loss: 0.6136319041252136
train_iter_loss: 0.22697246074676514
train_iter_loss: 0.1598319411277771
train_iter_loss: 0.2601049542427063
train_iter_loss: 0.272616982460022
train_iter_loss: 0.27929288148880005
train_iter_loss: 0.3190276622772217
train_iter_loss: 0.14690695703029633
train_iter_loss: 0.35071152448654175
train_iter_loss: 0.2128136157989502
train_iter_loss: 0.29942214488983154
train_iter_loss: 0.10995277017354965
train_iter_loss: 0.15881961584091187
train_iter_loss: 0.02644094079732895
train_iter_loss: 0.48402684926986694
train_iter_loss: 0.20891043543815613
train_iter_loss: 0.08986803889274597
train_iter_loss: 0.1816358119249344
train_iter_loss: 0.4465704560279846
train_iter_loss: 0.2793435752391815
train_iter_loss: 0.22062353789806366
train_iter_loss: 0.21769239008426666
train_iter_loss: 0.15612706542015076
train_iter_loss: 0.20922289788722992
train_iter_loss: 0.36749693751335144
train_iter_loss: 0.20471058785915375
train_iter_loss: 0.5667245984077454
train_iter_loss: 0.15643049776554108
train_iter_loss: 0.22598320245742798
train_iter_loss: 0.24256113171577454
train_iter_loss: 0.34157323837280273
train_iter_loss: 0.2677764594554901
train_iter_loss: 0.24673397839069366
train_iter_loss: 0.3576010763645172
train_iter_loss: 0.24425412714481354
train_iter_loss: 0.23061513900756836
train_iter_loss: 0.23355230689048767
train_iter_loss: 0.0834183394908905
train_iter_loss: 0.08473251014947891
train_iter_loss: 0.3081129193305969
train_iter_loss: 0.22546342015266418
train_iter_loss: 0.4305453896522522
train_iter_loss: 0.2262738049030304
train_iter_loss: 0.22938349843025208
train_iter_loss: 0.42807358503341675
train_iter_loss: 0.23025144636631012
train_iter_loss: 0.30231237411499023
train_iter_loss: 0.36257296800613403
train_iter_loss: 0.1651654690504074
train_iter_loss: 0.26928237080574036
train_iter_loss: 0.30427610874176025
train_iter_loss: 0.2554084360599518
train_iter_loss: 0.18854624032974243
train_iter_loss: 0.29688021540641785
train_iter_loss: 0.33579879999160767
train_iter_loss: 0.26434457302093506
train_iter_loss: 0.2752465009689331
train_iter_loss: 0.3037121891975403
train_iter_loss: 0.36598458886146545
train_iter_loss: 0.36340388655662537
train loss :0.2647
---------------------
Validation seg loss: 0.3719468939970335 at epoch 813
epoch =    814/  1000, exp = train
train_iter_loss: 0.4258216321468353
train_iter_loss: 0.21801185607910156
train_iter_loss: 0.3219677209854126
train_iter_loss: 0.33616527915000916
train_iter_loss: 0.2611772119998932
train_iter_loss: 0.21646329760551453
train_iter_loss: 0.2784130573272705
train_iter_loss: 0.1779891848564148
train_iter_loss: 0.22988635301589966
train_iter_loss: 0.0577654168009758
train_iter_loss: 0.18567843735218048
train_iter_loss: 0.36840948462486267
train_iter_loss: 0.18959258496761322
train_iter_loss: 0.3027433753013611
train_iter_loss: 0.24067731201648712
train_iter_loss: 0.23112736642360687
train_iter_loss: 0.23652777075767517
train_iter_loss: 0.3506503403186798
train_iter_loss: 0.2869512140750885
train_iter_loss: 0.1906491070985794
train_iter_loss: 0.23332086205482483
train_iter_loss: 0.24148888885974884
train_iter_loss: 0.3010390102863312
train_iter_loss: 0.25121310353279114
train_iter_loss: 0.35900983214378357
train_iter_loss: 0.15807726979255676
train_iter_loss: 0.1461666226387024
train_iter_loss: 0.20583407580852509
train_iter_loss: 0.13241256773471832
train_iter_loss: 0.2662001848220825
train_iter_loss: 0.3220999538898468
train_iter_loss: 0.2552148997783661
train_iter_loss: 0.1584903597831726
train_iter_loss: 0.2038862109184265
train_iter_loss: 0.1640479862689972
train_iter_loss: 0.3133009672164917
train_iter_loss: 0.40739086270332336
train_iter_loss: 0.3239964544773102
train_iter_loss: 0.26390203833580017
train_iter_loss: 0.30024659633636475
train_iter_loss: 0.3361700475215912
train_iter_loss: 0.5558426976203918
train_iter_loss: 0.07095768302679062
train_iter_loss: 0.23733094334602356
train_iter_loss: 0.32967451214790344
train_iter_loss: 0.38527151942253113
train_iter_loss: 0.284802109003067
train_iter_loss: 0.19361327588558197
train_iter_loss: 0.3957812786102295
train_iter_loss: 0.15243542194366455
train_iter_loss: 0.317854642868042
train_iter_loss: 0.3023667335510254
train_iter_loss: 0.18868669867515564
train_iter_loss: 0.40273913741111755
train_iter_loss: 0.18776778876781464
train_iter_loss: 0.21787023544311523
train_iter_loss: 0.4200674593448639
train_iter_loss: 0.22587989270687103
train_iter_loss: 0.1922782063484192
train_iter_loss: 0.26632410287857056
train_iter_loss: 0.2933601140975952
train_iter_loss: 0.29451584815979004
train_iter_loss: 0.2606993317604065
train_iter_loss: 0.36159491539001465
train_iter_loss: 0.2137940526008606
train_iter_loss: 0.16404829919338226
train_iter_loss: 0.1795378029346466
train_iter_loss: 0.25826460123062134
train_iter_loss: 0.3665141761302948
train_iter_loss: 0.4250663220882416
train_iter_loss: 0.28626948595046997
train_iter_loss: 0.43553251028060913
train_iter_loss: 0.3481872081756592
train_iter_loss: 0.27573904395103455
train_iter_loss: 0.17208388447761536
train_iter_loss: 0.27444303035736084
train_iter_loss: 0.18845167756080627
train_iter_loss: 0.22492575645446777
train_iter_loss: 0.17983552813529968
train_iter_loss: 0.18680691719055176
train_iter_loss: 0.3579123914241791
train_iter_loss: 0.2651135325431824
train_iter_loss: 0.25649335980415344
train_iter_loss: 0.14291127026081085
train_iter_loss: 0.29439273476600647
train_iter_loss: 0.22376346588134766
train_iter_loss: 0.23812532424926758
train_iter_loss: 0.18183162808418274
train_iter_loss: 0.24141499400138855
train_iter_loss: 0.2768506109714508
train_iter_loss: 0.11574660986661911
train_iter_loss: 0.3070546090602875
train_iter_loss: 0.2279655933380127
train_iter_loss: 0.3058016300201416
train_iter_loss: 0.19449017941951752
train_iter_loss: 0.19726212322711945
train_iter_loss: 0.27671992778778076
train_iter_loss: 0.23789533972740173
train_iter_loss: 0.1152883768081665
train_iter_loss: 0.2550518810749054
train loss :0.2614
---------------------
Validation seg loss: 0.35491908816532847 at epoch 814
epoch =    815/  1000, exp = train
train_iter_loss: 0.3537925183773041
train_iter_loss: 0.4128458797931671
train_iter_loss: 0.17326019704341888
train_iter_loss: 0.2875431180000305
train_iter_loss: 0.323984295129776
train_iter_loss: 0.3106432855129242
train_iter_loss: 0.31741565465927124
train_iter_loss: 0.3432241678237915
train_iter_loss: 0.34176382422447205
train_iter_loss: 0.2620212435722351
train_iter_loss: 0.21342188119888306
train_iter_loss: 0.3571214973926544
train_iter_loss: 0.308039128780365
train_iter_loss: 0.3204852044582367
train_iter_loss: 0.32327210903167725
train_iter_loss: 0.2421412169933319
train_iter_loss: 0.23596593737602234
train_iter_loss: 0.34358862042427063
train_iter_loss: 0.15461793541908264
train_iter_loss: 0.0622452050447464
train_iter_loss: 0.3974474370479584
train_iter_loss: 0.23789097368717194
train_iter_loss: 0.1457550823688507
train_iter_loss: 0.1882934868335724
train_iter_loss: 0.10659635812044144
train_iter_loss: 0.3373684287071228
train_iter_loss: 0.3212302327156067
train_iter_loss: 0.27458786964416504
train_iter_loss: 0.3671773076057434
train_iter_loss: 0.13049720227718353
train_iter_loss: 0.2437683343887329
train_iter_loss: 0.26195228099823
train_iter_loss: 0.20189474523067474
train_iter_loss: 0.1636890470981598
train_iter_loss: 0.26947054266929626
train_iter_loss: 0.25650542974472046
train_iter_loss: 0.12961317598819733
train_iter_loss: 0.23577286303043365
train_iter_loss: 0.12872205674648285
train_iter_loss: 0.37460190057754517
train_iter_loss: 0.2428417205810547
train_iter_loss: 0.28192293643951416
train_iter_loss: 0.31061407923698425
train_iter_loss: 0.3903796076774597
train_iter_loss: 0.2636549472808838
train_iter_loss: 0.39626315236091614
train_iter_loss: 0.20481055974960327
train_iter_loss: 0.12619756162166595
train_iter_loss: 0.23828881978988647
train_iter_loss: 0.2006937861442566
train_iter_loss: 0.2616386413574219
train_iter_loss: 0.18088096380233765
train_iter_loss: 0.3010977804660797
train_iter_loss: 0.21605634689331055
train_iter_loss: 0.24341407418251038
train_iter_loss: 0.21436062455177307
train_iter_loss: 0.28876158595085144
train_iter_loss: 0.16523006558418274
train_iter_loss: 0.3713354170322418
train_iter_loss: 0.29293036460876465
train_iter_loss: 0.15682287514209747
train_iter_loss: 0.2667514383792877
train_iter_loss: 0.5286006331443787
train_iter_loss: 0.39034023880958557
train_iter_loss: 0.2755851447582245
train_iter_loss: 0.20711298286914825
train_iter_loss: 0.2442258596420288
train_iter_loss: 0.20106665790081024
train_iter_loss: 0.42168331146240234
train_iter_loss: 0.20219549536705017
train_iter_loss: 0.2407463788986206
train_iter_loss: 0.18688680231571198
train_iter_loss: 0.16814903914928436
train_iter_loss: 0.29282569885253906
train_iter_loss: 0.27277955412864685
train_iter_loss: 0.3655298054218292
train_iter_loss: 0.08390773832798004
train_iter_loss: 0.17155145108699799
train_iter_loss: 0.28676721453666687
train_iter_loss: 0.2753539979457855
train_iter_loss: 0.3286716043949127
train_iter_loss: 0.20449046790599823
train_iter_loss: 0.21958637237548828
train_iter_loss: 0.3820827901363373
train_iter_loss: 0.2342059314250946
train_iter_loss: 0.1870938390493393
train_iter_loss: 0.4138801693916321
train_iter_loss: 0.16294889152050018
train_iter_loss: 0.2717055380344391
train_iter_loss: 0.3393975794315338
train_iter_loss: 0.14041316509246826
train_iter_loss: 0.2652273178100586
train_iter_loss: 0.24247503280639648
train_iter_loss: 0.3069674074649811
train_iter_loss: 0.3450544774532318
train_iter_loss: 0.20857420563697815
train_iter_loss: 0.18032337725162506
train_iter_loss: 0.19783931970596313
train_iter_loss: 0.24191686511039734
train_iter_loss: 0.22796909511089325
train loss :0.2625
---------------------
Validation seg loss: 0.36105118494674143 at epoch 815
epoch =    816/  1000, exp = train
train_iter_loss: 0.23149487376213074
train_iter_loss: 0.3290368616580963
train_iter_loss: 0.21478308737277985
train_iter_loss: 0.2862498462200165
train_iter_loss: 0.39521437883377075
train_iter_loss: 0.3038397431373596
train_iter_loss: 0.10877412557601929
train_iter_loss: 0.15237829089164734
train_iter_loss: 0.2976173758506775
train_iter_loss: 0.32307055592536926
train_iter_loss: 0.1699662208557129
train_iter_loss: 0.3398449420928955
train_iter_loss: 0.4472217857837677
train_iter_loss: 0.3266477882862091
train_iter_loss: 0.11243131756782532
train_iter_loss: 0.2946758568286896
train_iter_loss: 0.22373288869857788
train_iter_loss: 0.25813597440719604
train_iter_loss: 0.22586534917354584
train_iter_loss: 0.2473001480102539
train_iter_loss: 0.1362612098455429
train_iter_loss: 0.10766292363405228
train_iter_loss: 0.2517351806163788
train_iter_loss: 0.2499675303697586
train_iter_loss: 0.3419440686702728
train_iter_loss: 0.40462586283683777
train_iter_loss: 0.3596492409706116
train_iter_loss: 0.3056854009628296
train_iter_loss: 0.16739489138126373
train_iter_loss: 0.4269721508026123
train_iter_loss: 0.1532129943370819
train_iter_loss: 0.20582343637943268
train_iter_loss: 0.3481229841709137
train_iter_loss: 0.07248277962207794
train_iter_loss: 0.21822327375411987
train_iter_loss: 0.1925831437110901
train_iter_loss: 0.20489133894443512
train_iter_loss: 0.24991481006145477
train_iter_loss: 0.1976938247680664
train_iter_loss: 0.2636348307132721
train_iter_loss: 0.2006991058588028
train_iter_loss: 0.4690987765789032
train_iter_loss: 0.10945114493370056
train_iter_loss: 0.16541507840156555
train_iter_loss: 0.48813289403915405
train_iter_loss: 0.3085106909275055
train_iter_loss: 0.32515713572502136
train_iter_loss: 0.3920811116695404
train_iter_loss: 0.23865161836147308
train_iter_loss: 0.3305405080318451
train_iter_loss: 0.20864932239055634
train_iter_loss: 0.26738396286964417
train_iter_loss: 0.3466362953186035
train_iter_loss: 0.29335227608680725
train_iter_loss: 0.1508064866065979
train_iter_loss: 0.25767526030540466
train_iter_loss: 0.3480064570903778
train_iter_loss: 0.24230289459228516
train_iter_loss: 0.14288051426410675
train_iter_loss: 0.3109552562236786
train_iter_loss: 0.26679661870002747
train_iter_loss: 0.12604595720767975
train_iter_loss: 0.24444158375263214
train_iter_loss: 0.21454952657222748
train_iter_loss: 0.2114534229040146
train_iter_loss: 0.24131102859973907
train_iter_loss: 0.11018247157335281
train_iter_loss: 0.1847776174545288
train_iter_loss: 0.20560871064662933
train_iter_loss: 0.34856846928596497
train_iter_loss: 0.21502186357975006
train_iter_loss: 0.33850303292274475
train_iter_loss: 0.24485263228416443
train_iter_loss: 0.3170527219772339
train_iter_loss: 0.3055040240287781
train_iter_loss: 0.29173997044563293
train_iter_loss: 0.24245430529117584
train_iter_loss: 0.3953147828578949
train_iter_loss: 0.422804057598114
train_iter_loss: 0.3032023310661316
train_iter_loss: 0.2980157136917114
train_iter_loss: 0.27509018778800964
train_iter_loss: 0.1977860927581787
train_iter_loss: 0.16550980508327484
train_iter_loss: 0.26898518204689026
train_iter_loss: 0.17310313880443573
train_iter_loss: 0.37926438450813293
train_iter_loss: 0.23244717717170715
train_iter_loss: 0.15569724142551422
train_iter_loss: 0.4119231700897217
train_iter_loss: 0.2494993954896927
train_iter_loss: 0.224284827709198
train_iter_loss: 0.27667155861854553
train_iter_loss: 0.23625947535037994
train_iter_loss: 0.3598157465457916
train_iter_loss: 0.2232619971036911
train_iter_loss: 0.3424118161201477
train_iter_loss: 0.12802748382091522
train_iter_loss: 0.28919529914855957
train_iter_loss: 0.32386109232902527
train loss :0.2645
---------------------
Validation seg loss: 0.36809782538000707 at epoch 816
epoch =    817/  1000, exp = train
train_iter_loss: 0.16116388142108917
train_iter_loss: 0.38006478548049927
train_iter_loss: 0.10772781819105148
train_iter_loss: 0.3632119297981262
train_iter_loss: 0.24461887776851654
train_iter_loss: 0.22756658494472504
train_iter_loss: 0.2240181565284729
train_iter_loss: 0.36362946033477783
train_iter_loss: 0.10713469237089157
train_iter_loss: 0.18329966068267822
train_iter_loss: 0.25683170557022095
train_iter_loss: 0.34342703223228455
train_iter_loss: 0.20925912261009216
train_iter_loss: 0.23793208599090576
train_iter_loss: 0.26539215445518494
train_iter_loss: 0.25685471296310425
train_iter_loss: 0.27246859669685364
train_iter_loss: 0.312951922416687
train_iter_loss: 0.2820000648498535
train_iter_loss: 0.1365867555141449
train_iter_loss: 0.31428003311157227
train_iter_loss: 0.2939334809780121
train_iter_loss: 0.24483723938465118
train_iter_loss: 0.3640132546424866
train_iter_loss: 0.1407473087310791
train_iter_loss: 0.24334312975406647
train_iter_loss: 0.22639025747776031
train_iter_loss: 0.3534546196460724
train_iter_loss: 0.1790364533662796
train_iter_loss: 0.27622562646865845
train_iter_loss: 0.2990787625312805
train_iter_loss: 0.31277647614479065
train_iter_loss: 0.37396708130836487
train_iter_loss: 0.2959778308868408
train_iter_loss: 0.315218985080719
train_iter_loss: 0.24050980806350708
train_iter_loss: 0.11168418824672699
train_iter_loss: 0.2425246685743332
train_iter_loss: 0.36456969380378723
train_iter_loss: 0.2918318212032318
train_iter_loss: 0.25037500262260437
train_iter_loss: 0.19892995059490204
train_iter_loss: 0.23339252173900604
train_iter_loss: 0.2717282772064209
train_iter_loss: 0.21393507719039917
train_iter_loss: 0.3286704123020172
train_iter_loss: 0.29459112882614136
train_iter_loss: 0.27837780117988586
train_iter_loss: 0.23137891292572021
train_iter_loss: 0.23173244297504425
train_iter_loss: 0.18373672664165497
train_iter_loss: 0.2981860041618347
train_iter_loss: 0.3027587831020355
train_iter_loss: 0.12822559475898743
train_iter_loss: 0.45837974548339844
train_iter_loss: 0.27051928639411926
train_iter_loss: 0.46784356236457825
train_iter_loss: 0.19843174517154694
train_iter_loss: 0.3676292300224304
train_iter_loss: 0.28589770197868347
train_iter_loss: 0.1420343518257141
train_iter_loss: 0.27033334970474243
train_iter_loss: 0.2523316740989685
train_iter_loss: 0.14110830426216125
train_iter_loss: 0.2617313265800476
train_iter_loss: 0.3134625256061554
train_iter_loss: 0.16277268528938293
train_iter_loss: 0.10469324886798859
train_iter_loss: 0.21584104001522064
train_iter_loss: 0.350341796875
train_iter_loss: 0.1903252899646759
train_iter_loss: 0.25771564245224
train_iter_loss: 0.5094561576843262
train_iter_loss: 0.14122672379016876
train_iter_loss: 0.2719068229198456
train_iter_loss: 0.17177346348762512
train_iter_loss: 0.29383352398872375
train_iter_loss: 0.30893421173095703
train_iter_loss: 0.31903183460235596
train_iter_loss: 0.3110646903514862
train_iter_loss: 0.12289392203092575
train_iter_loss: 0.20060449838638306
train_iter_loss: 0.2594820559024811
train_iter_loss: 0.2867806851863861
train_iter_loss: 0.3667408525943756
train_iter_loss: 0.2668084502220154
train_iter_loss: 0.2763032019138336
train_iter_loss: 0.23951971530914307
train_iter_loss: 0.29871636629104614
train_iter_loss: 0.22206170856952667
train_iter_loss: 0.23959670960903168
train_iter_loss: 0.2368829995393753
train_iter_loss: 0.2655198872089386
train_iter_loss: 0.27840572595596313
train_iter_loss: 0.20496945083141327
train_iter_loss: 0.14844359457492828
train_iter_loss: 0.31873056292533875
train_iter_loss: 0.19449536502361298
train_iter_loss: 0.18757770955562592
train_iter_loss: 0.2321726679801941
train loss :0.2602
---------------------
Validation seg loss: 0.3472477306769985 at epoch 817
epoch =    818/  1000, exp = train
train_iter_loss: 0.31595751643180847
train_iter_loss: 0.37721046805381775
train_iter_loss: 0.34993648529052734
train_iter_loss: 0.3045768737792969
train_iter_loss: 0.2742478549480438
train_iter_loss: 0.4132535457611084
train_iter_loss: 0.30693310499191284
train_iter_loss: 0.30418893694877625
train_iter_loss: 0.13370832800865173
train_iter_loss: 0.3761310279369354
train_iter_loss: 0.14584240317344666
train_iter_loss: 0.3535763621330261
train_iter_loss: 0.3990912139415741
train_iter_loss: 0.2394753396511078
train_iter_loss: 0.23686589300632477
train_iter_loss: 0.2466028332710266
train_iter_loss: 0.3538760244846344
train_iter_loss: 0.1637534648180008
train_iter_loss: 0.3087182343006134
train_iter_loss: 0.102271668612957
train_iter_loss: 0.2946825921535492
train_iter_loss: 0.05865954980254173
train_iter_loss: 0.30405193567276
train_iter_loss: 0.1482178270816803
train_iter_loss: 0.22980071604251862
train_iter_loss: 0.39213210344314575
train_iter_loss: 0.30238795280456543
train_iter_loss: 0.3859982490539551
train_iter_loss: 0.2954244315624237
train_iter_loss: 0.1830993890762329
train_iter_loss: 0.31893882155418396
train_iter_loss: 0.254900187253952
train_iter_loss: 0.15386813879013062
train_iter_loss: 0.3712504506111145
train_iter_loss: 0.23417863249778748
train_iter_loss: 0.20440928637981415
train_iter_loss: 0.3372272551059723
train_iter_loss: 0.2910045385360718
train_iter_loss: 0.2697700262069702
train_iter_loss: 0.23736874759197235
train_iter_loss: 0.2553700804710388
train_iter_loss: 0.3597354292869568
train_iter_loss: 0.35075318813323975
train_iter_loss: 0.15354469418525696
train_iter_loss: 0.16863179206848145
train_iter_loss: 0.5074323415756226
train_iter_loss: 0.33201801776885986
train_iter_loss: 0.2494010627269745
train_iter_loss: 0.32371675968170166
train_iter_loss: 0.2235945761203766
train_iter_loss: 0.1927676647901535
train_iter_loss: 0.2781122922897339
train_iter_loss: 0.2040794938802719
train_iter_loss: 0.13173332810401917
train_iter_loss: 0.26121509075164795
train_iter_loss: 0.2523847818374634
train_iter_loss: 0.17050108313560486
train_iter_loss: 0.43440574407577515
train_iter_loss: 0.2541957199573517
train_iter_loss: 0.48873916268348694
train_iter_loss: 0.23732095956802368
train_iter_loss: 0.2940347194671631
train_iter_loss: 0.18215696513652802
train_iter_loss: 0.38894206285476685
train_iter_loss: 0.24372057616710663
train_iter_loss: 0.36791515350341797
train_iter_loss: 0.2792989909648895
train_iter_loss: 0.18795688450336456
train_iter_loss: 0.34509849548339844
train_iter_loss: 0.37939944863319397
train_iter_loss: 0.3613929748535156
train_iter_loss: 0.2244049608707428
train_iter_loss: 0.227552130818367
train_iter_loss: 0.06447125226259232
train_iter_loss: 0.2505406439304352
train_iter_loss: 0.34347274899482727
train_iter_loss: 0.190240740776062
train_iter_loss: 0.2924610674381256
train_iter_loss: 0.21610260009765625
train_iter_loss: 0.25834977626800537
train_iter_loss: 0.17734065651893616
train_iter_loss: 0.3535878658294678
train_iter_loss: 0.20522400736808777
train_iter_loss: 0.4429740607738495
train_iter_loss: 0.2506310045719147
train_iter_loss: 0.36181676387786865
train_iter_loss: 0.13146530091762543
train_iter_loss: 0.2827237546443939
train_iter_loss: 0.2008347362279892
train_iter_loss: 0.12744392454624176
train_iter_loss: 0.10006792843341827
train_iter_loss: 0.25297659635543823
train_iter_loss: 0.211562842130661
train_iter_loss: 0.3160610795021057
train_iter_loss: 0.2990356683731079
train_iter_loss: 0.30208325386047363
train_iter_loss: 0.18316982686519623
train_iter_loss: 0.2390577644109726
train_iter_loss: 0.3112565875053406
train_iter_loss: 0.08430671691894531
train loss :0.2699
---------------------
Validation seg loss: 0.35133160629643584 at epoch 818
epoch =    819/  1000, exp = train
train_iter_loss: 0.14729265868663788
train_iter_loss: 0.2206965833902359
train_iter_loss: 0.2921394109725952
train_iter_loss: 0.20635508000850677
train_iter_loss: 0.305364727973938
train_iter_loss: 0.3698625862598419
train_iter_loss: 0.2082524299621582
train_iter_loss: 0.30221864581108093
train_iter_loss: 0.21327194571495056
train_iter_loss: 0.13018611073493958
train_iter_loss: 0.30199339985847473
train_iter_loss: 0.24121849238872528
train_iter_loss: 0.33783894777297974
train_iter_loss: 0.2989480495452881
train_iter_loss: 0.2570669949054718
train_iter_loss: 0.11560626327991486
train_iter_loss: 0.5317082405090332
train_iter_loss: 0.4132901132106781
train_iter_loss: 0.19868610799312592
train_iter_loss: 0.3038735091686249
train_iter_loss: 0.24064068496227264
train_iter_loss: 0.3158668279647827
train_iter_loss: 0.3174232840538025
train_iter_loss: 0.2573625445365906
train_iter_loss: 0.13855284452438354
train_iter_loss: 0.29349860548973083
train_iter_loss: 0.31473129987716675
train_iter_loss: 0.393993079662323
train_iter_loss: 0.31429940462112427
train_iter_loss: 0.2927844226360321
train_iter_loss: 0.19985464215278625
train_iter_loss: 0.35723555088043213
train_iter_loss: 0.28535422682762146
train_iter_loss: 0.18286629021167755
train_iter_loss: 0.2254095822572708
train_iter_loss: 0.31263935565948486
train_iter_loss: 0.4052254855632782
train_iter_loss: 0.26532983779907227
train_iter_loss: 0.24244602024555206
train_iter_loss: 0.24751923978328705
train_iter_loss: 0.29192671179771423
train_iter_loss: 0.27049991488456726
train_iter_loss: 0.29599428176879883
train_iter_loss: 0.22654308378696442
train_iter_loss: 0.4896426200866699
train_iter_loss: 0.1975531429052353
train_iter_loss: 0.2170144021511078
train_iter_loss: 0.3548769950866699
train_iter_loss: 0.276353120803833
train_iter_loss: 0.3114164471626282
train_iter_loss: 0.06162929907441139
train_iter_loss: 0.25649410486221313
train_iter_loss: 0.20202569663524628
train_iter_loss: 0.22887012362480164
train_iter_loss: 0.34721922874450684
train_iter_loss: 0.2807632386684418
train_iter_loss: 0.34777551889419556
train_iter_loss: 0.24717681109905243
train_iter_loss: 0.18725630640983582
train_iter_loss: 0.21167141199111938
train_iter_loss: 0.41180944442749023
train_iter_loss: 0.22153331339359283
train_iter_loss: 0.25281116366386414
train_iter_loss: 0.2267398238182068
train_iter_loss: 0.1480485051870346
train_iter_loss: 0.3688035309314728
train_iter_loss: 0.26923051476478577
train_iter_loss: 0.18526621162891388
train_iter_loss: 0.27993595600128174
train_iter_loss: 0.09853333979845047
train_iter_loss: 0.2183367908000946
train_iter_loss: 0.17552441358566284
train_iter_loss: 0.2677419185638428
train_iter_loss: 0.3183857798576355
train_iter_loss: 0.22029593586921692
train_iter_loss: 0.17124788463115692
train_iter_loss: 0.2719497084617615
train_iter_loss: 0.2873132526874542
train_iter_loss: 0.16736875474452972
train_iter_loss: 0.21290786564350128
train_iter_loss: 0.17229604721069336
train_iter_loss: 0.15035755932331085
train_iter_loss: 0.19543713331222534
train_iter_loss: 0.22657117247581482
train_iter_loss: 0.21232151985168457
train_iter_loss: 0.18415845930576324
train_iter_loss: 0.17182742059230804
train_iter_loss: 0.25973081588745117
train_iter_loss: 0.35655415058135986
train_iter_loss: 0.3179544508457184
train_iter_loss: 0.29138433933258057
train_iter_loss: 0.20400631427764893
train_iter_loss: 0.2693866491317749
train_iter_loss: 0.026532435789704323
train_iter_loss: 0.2882803976535797
train_iter_loss: 0.36981895565986633
train_iter_loss: 0.1732708215713501
train_iter_loss: 0.22692254185676575
train_iter_loss: 0.3163313865661621
train_iter_loss: 0.32335591316223145
train loss :0.2608
---------------------
Validation seg loss: 0.3565051805157706 at epoch 819
epoch =    820/  1000, exp = train
train_iter_loss: 0.16514192521572113
train_iter_loss: 0.17462806403636932
train_iter_loss: 0.26885390281677246
train_iter_loss: 0.1448291540145874
train_iter_loss: 0.048744022846221924
train_iter_loss: 0.28551533818244934
train_iter_loss: 0.36505305767059326
train_iter_loss: 0.364738404750824
train_iter_loss: 0.37081554532051086
train_iter_loss: 0.26664605736732483
train_iter_loss: 0.2498912215232849
train_iter_loss: 0.22774340212345123
train_iter_loss: 0.31410643458366394
train_iter_loss: 0.16598810255527496
train_iter_loss: 0.127095565199852
train_iter_loss: 0.15520691871643066
train_iter_loss: 0.278945654630661
train_iter_loss: 0.34356164932250977
train_iter_loss: 0.33940988779067993
train_iter_loss: 0.1890854835510254
train_iter_loss: 0.37550920248031616
train_iter_loss: 0.15417282283306122
train_iter_loss: 0.2761336863040924
train_iter_loss: 0.1340264081954956
train_iter_loss: 0.1252351552248001
train_iter_loss: 0.2514532208442688
train_iter_loss: 0.3503727316856384
train_iter_loss: 0.28540799021720886
train_iter_loss: 0.19980217516422272
train_iter_loss: 0.17596910893917084
train_iter_loss: 0.33964353799819946
train_iter_loss: 0.354481041431427
train_iter_loss: 0.24236856400966644
train_iter_loss: 0.234796941280365
train_iter_loss: 0.23377861082553864
train_iter_loss: 0.3995334804058075
train_iter_loss: 0.23276478052139282
train_iter_loss: 0.11059575527906418
train_iter_loss: 0.3314821124076843
train_iter_loss: 0.27226579189300537
train_iter_loss: 0.33803215622901917
train_iter_loss: 0.30698245763778687
train_iter_loss: 0.20746950805187225
train_iter_loss: 0.23240883648395538
train_iter_loss: 0.20907023549079895
train_iter_loss: 0.20635369420051575
train_iter_loss: 0.2677127420902252
train_iter_loss: 0.23532699048519135
train_iter_loss: 0.36808541417121887
train_iter_loss: 0.20909668505191803
train_iter_loss: 0.3090115785598755
train_iter_loss: 0.3329772353172302
train_iter_loss: 0.07761549949645996
train_iter_loss: 0.3094401955604553
train_iter_loss: 0.210093691945076
train_iter_loss: 0.2674425542354584
train_iter_loss: 0.1598658710718155
train_iter_loss: 0.19559691846370697
train_iter_loss: 0.4136474132537842
train_iter_loss: 0.33252429962158203
train_iter_loss: 0.38023319840431213
train_iter_loss: 0.3700467348098755
train_iter_loss: 0.2539418935775757
train_iter_loss: 0.3751690983772278
train_iter_loss: 0.4050804674625397
train_iter_loss: 0.3592798113822937
train_iter_loss: 0.25563645362854004
train_iter_loss: 0.3416685461997986
train_iter_loss: 0.2702732980251312
train_iter_loss: 0.1430545449256897
train_iter_loss: 0.2884618937969208
train_iter_loss: 0.2900063097476959
train_iter_loss: 0.2661295533180237
train_iter_loss: 0.22753874957561493
train_iter_loss: 0.28814300894737244
train_iter_loss: 0.29178163409233093
train_iter_loss: 0.3071117401123047
train_iter_loss: 0.2501925528049469
train_iter_loss: 0.24109020829200745
train_iter_loss: 0.2635382115840912
train_iter_loss: 0.307573527097702
train_iter_loss: 0.3625507056713104
train_iter_loss: 0.1817006766796112
train_iter_loss: 0.15325914323329926
train_iter_loss: 0.18764573335647583
train_iter_loss: 0.3027210235595703
train_iter_loss: 0.07519585639238358
train_iter_loss: 0.327504962682724
train_iter_loss: 0.27875810861587524
train_iter_loss: 0.43305060267448425
train_iter_loss: 0.23997344076633453
train_iter_loss: 0.17455850541591644
train_iter_loss: 0.26037701964378357
train_iter_loss: 0.1698540598154068
train_iter_loss: 0.10189525038003922
train_iter_loss: 0.2777670621871948
train_iter_loss: 0.2070748656988144
train_iter_loss: 0.31058254837989807
train_iter_loss: 0.34694504737854004
train_iter_loss: 0.11012525856494904
train loss :0.2605
---------------------
Validation seg loss: 0.3529388343801124 at epoch 820
epoch =    821/  1000, exp = train
train_iter_loss: 0.47578439116477966
train_iter_loss: 0.022910816594958305
train_iter_loss: 0.33121058344841003
train_iter_loss: 0.1808258444070816
train_iter_loss: 0.23571264743804932
train_iter_loss: 0.13137845695018768
train_iter_loss: 0.198329895734787
train_iter_loss: 0.4168829619884491
train_iter_loss: 0.23083800077438354
train_iter_loss: 0.19411601126194
train_iter_loss: 0.2590267062187195
train_iter_loss: 0.338787317276001
train_iter_loss: 0.22033903002738953
train_iter_loss: 0.15481343865394592
train_iter_loss: 0.37762272357940674
train_iter_loss: 0.11209066957235336
train_iter_loss: 0.2981940805912018
train_iter_loss: 0.2639216184616089
train_iter_loss: 0.08931051939725876
train_iter_loss: 0.21576055884361267
train_iter_loss: 0.19896674156188965
train_iter_loss: 0.17420300841331482
train_iter_loss: 0.14631275832653046
train_iter_loss: 0.2840566635131836
train_iter_loss: 0.30073872208595276
train_iter_loss: 0.2942740321159363
train_iter_loss: 0.34351420402526855
train_iter_loss: 0.1444934457540512
train_iter_loss: 0.3916217088699341
train_iter_loss: 0.2026522010564804
train_iter_loss: 0.2844460904598236
train_iter_loss: 0.2772480249404907
train_iter_loss: 0.35148102045059204
train_iter_loss: 0.16230140626430511
train_iter_loss: 0.18206913769245148
train_iter_loss: 0.13564489781856537
train_iter_loss: 0.23388460278511047
train_iter_loss: 0.3729172646999359
train_iter_loss: 0.4214628040790558
train_iter_loss: 0.3443586230278015
train_iter_loss: 0.29708197712898254
train_iter_loss: 0.1867743879556656
train_iter_loss: 0.1904136687517166
train_iter_loss: 0.31544122099876404
train_iter_loss: 0.32232922315597534
train_iter_loss: 0.2736164927482605
train_iter_loss: 0.352947860956192
train_iter_loss: 0.19938881695270538
train_iter_loss: 0.25497493147850037
train_iter_loss: 0.41345155239105225
train_iter_loss: 0.22613874077796936
train_iter_loss: 0.23039092123508453
train_iter_loss: 0.20458750426769257
train_iter_loss: 0.3612349331378937
train_iter_loss: 0.3629465699195862
train_iter_loss: 0.19629456102848053
train_iter_loss: 0.13348929584026337
train_iter_loss: 0.30016347765922546
train_iter_loss: 0.27857211232185364
train_iter_loss: 0.35188084840774536
train_iter_loss: 0.14203062653541565
train_iter_loss: 0.2205009162425995
train_iter_loss: 0.26055076718330383
train_iter_loss: 0.14636686444282532
train_iter_loss: 0.2368142008781433
train_iter_loss: 0.10274676978588104
train_iter_loss: 0.2274923175573349
train_iter_loss: 0.374041348695755
train_iter_loss: 0.19780178368091583
train_iter_loss: 0.29357022047042847
train_iter_loss: 0.41928574442863464
train_iter_loss: 0.271907240152359
train_iter_loss: 0.27947741746902466
train_iter_loss: 0.21199189126491547
train_iter_loss: 0.24722598493099213
train_iter_loss: 0.2569359242916107
train_iter_loss: 0.33693793416023254
train_iter_loss: 0.33294060826301575
train_iter_loss: 0.18140926957130432
train_iter_loss: 0.5009076595306396
train_iter_loss: 0.3033602237701416
train_iter_loss: 0.26008233428001404
train_iter_loss: 0.13300101459026337
train_iter_loss: 0.19477325677871704
train_iter_loss: 0.2776358723640442
train_iter_loss: 0.27951961755752563
train_iter_loss: 0.15103742480278015
train_iter_loss: 0.1848076730966568
train_iter_loss: 0.44629690051078796
train_iter_loss: 0.2590595483779907
train_iter_loss: 0.46311163902282715
train_iter_loss: 0.16087844967842102
train_iter_loss: 0.2731378674507141
train_iter_loss: 0.2089633345603943
train_iter_loss: 0.2291993945837021
train_iter_loss: 0.3499130308628082
train_iter_loss: 0.23141947388648987
train_iter_loss: 0.16980642080307007
train_iter_loss: 0.303660124540329
train_iter_loss: 0.14715346693992615
train loss :0.2597
---------------------
Validation seg loss: 0.36616268706560695 at epoch 821
epoch =    822/  1000, exp = train

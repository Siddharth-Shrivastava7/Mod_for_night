===============================
2376893.pbshpc
vsky009.hpc.iitd.ac.in
===============================
/home/cse/phd/anz208849/Mod_for_night
wait
['so_run_btad_2.py']
Snapshot stored in: ../scratch/saved_models/acdc/dannet/train
                     note : train          
                    model : deeplab        
                    train : 1              
                 multigpu : 0              
                    fixbn : 0              
                 fix_seed : 1              
            learning_rate : 7.5e-05        
                num_steps : 5000           
                   epochs : 1000           
             weight_decay : 0.0005         
                 momentum : 0.9            
                    power : 0.9            
                    round : 6              
               print_freq : 372            
                save_freq : 372            
              tensorboard : 1              
                  neptune : 0              
                   screen : 1              
                      val : 1              
                 val_freq : 5              
                   source : acdc_train_rf_tensor
                   target : acdc_val_rf_tensor
                   worker : 4              
               batch_size : 2              
              num_classes : 2              
                input_src : 720            
                input_tgt : 720            
                 crop_src : 600            
                 crop_tgt : 600            
                   mirror : 1              
                scale_min : 0.5            
                scale_max : 1.5            
                      rec : 0              
              init_weight : ./save/model410_city_deeplabv2.pth
             restore_from : None           
                 snapshot : ../scratch/saved_models/acdc/dannet/train
                   result : ./miou_result/ 
                      log : ./log/         
                   plabel : ./plabel       
                       tb : ./log/train    

Mode --> Train
Lets ride...
epoch =      0/  1000, exp = train
train_iter_loss: 0.7891377806663513
train_iter_loss: 0.7710544466972351
train_iter_loss: 0.7434272170066833
train_iter_loss: 0.7830803990364075
train_iter_loss: 0.7533856630325317
train_iter_loss: 0.7647235989570618
train_iter_loss: 0.7269695401191711
train_iter_loss: 0.7370830178260803
train_iter_loss: 0.7663946151733398
train_iter_loss: 0.7739381194114685
train_iter_loss: 0.7433757781982422
train_iter_loss: 0.6974069476127625
train_iter_loss: 0.7307256460189819
train_iter_loss: 0.7005720138549805
train_iter_loss: 0.7076200842857361
train_iter_loss: 0.6851050853729248
train_iter_loss: 0.719805121421814
train_iter_loss: 0.6709028482437134
train_iter_loss: 0.7080504298210144
train_iter_loss: 0.7093616127967834
train_iter_loss: 0.682666540145874
train_iter_loss: 0.6429831981658936
train_iter_loss: 0.6492220759391785
train_iter_loss: 0.683724045753479
train_iter_loss: 0.7012643814086914
train_iter_loss: 0.6678435206413269
train_iter_loss: 0.6189623475074768
train_iter_loss: 0.7185155153274536
train_iter_loss: 0.6861247420310974
train_iter_loss: 0.710379421710968
train_iter_loss: 0.6237457394599915
train_iter_loss: 0.6596927046775818
train_iter_loss: 0.6452205777168274
train_iter_loss: 0.6471046805381775
train_iter_loss: 0.6645433306694031
train_iter_loss: 0.6800805926322937
train_iter_loss: 0.6926548480987549
train_iter_loss: 0.5942675471305847
train_iter_loss: 0.608245849609375
train_iter_loss: 0.6027271747589111
train_iter_loss: 0.5724011063575745
train_iter_loss: 0.6553471088409424
train_iter_loss: 0.6652076840400696
train_iter_loss: 0.7227064967155457
train_iter_loss: 0.5396127104759216
train_iter_loss: 0.6860776543617249
train_iter_loss: 0.7037680149078369
train_iter_loss: 0.7190098762512207
train_iter_loss: 0.6508646607398987
train_iter_loss: 0.6806129813194275
train_iter_loss: 0.6218745708465576
train_iter_loss: 0.6898081302642822
train_iter_loss: 0.6648104786872864
train_iter_loss: 0.7141156196594238
train_iter_loss: 0.6616340279579163
train_iter_loss: 0.643215000629425
train_iter_loss: 0.6345054507255554
train_iter_loss: 0.6635540127754211
train_iter_loss: 0.6802759170532227
train_iter_loss: 0.6997935771942139
train_iter_loss: 0.6339688897132874
train_iter_loss: 0.6859594583511353
train_iter_loss: 0.6280211210250854
train_iter_loss: 0.5913406014442444
train_iter_loss: 0.6306756734848022
train_iter_loss: 0.6220790147781372
train_iter_loss: 0.7016578912734985
train_iter_loss: 0.6355026364326477
train_iter_loss: 0.6277801394462585
train_iter_loss: 0.6436104774475098
train_iter_loss: 0.6143394708633423
train_iter_loss: 0.6728267669677734
train_iter_loss: 0.5870620608329773
train_iter_loss: 0.6076335906982422
train_iter_loss: 0.6467733979225159
train_iter_loss: 0.6715782880783081
train_iter_loss: 0.5924621820449829
train_iter_loss: 0.5860205292701721
train_iter_loss: 0.6596748232841492
train_iter_loss: 0.6534350514411926
train_iter_loss: 0.5607712268829346
train_iter_loss: 0.6095226407051086
train_iter_loss: 0.6575927734375
train_iter_loss: 0.6340251564979553
train_iter_loss: 0.5575104355812073
train_iter_loss: 0.6612110137939453
train_iter_loss: 0.5974321365356445
train_iter_loss: 0.6243160367012024
train_iter_loss: 0.6440996527671814
train_iter_loss: 0.6077760457992554
train_iter_loss: 0.6438203454017639
train_iter_loss: 0.5817103385925293
train_iter_loss: 0.6008382439613342
train_iter_loss: 0.6740309596061707
train_iter_loss: 0.6170794367790222
train_iter_loss: 0.6141438484191895
train_iter_loss: 0.5155543088912964
train_iter_loss: 0.5418702960014343
train_iter_loss: 0.5834543704986572
train_iter_loss: 0.5790122151374817
train_iter_loss: 0.7126557230949402
train_iter_loss: 0.6743993759155273
train_iter_loss: 0.6456155776977539
train_iter_loss: 0.6530106663703918
train_iter_loss: 0.6513620018959045
train_iter_loss: 0.5925623774528503
train_iter_loss: 0.5649354457855225
train_iter_loss: 0.5444352626800537
train_iter_loss: 0.697401225566864
train_iter_loss: 0.5981169939041138
train_iter_loss: 0.6440320014953613
train_iter_loss: 0.5513675212860107
train_iter_loss: 0.6928759813308716
train_iter_loss: 0.6981958746910095
train_iter_loss: 0.5955230593681335
train_iter_loss: 0.6004355549812317
train_iter_loss: 0.6423863172531128
train_iter_loss: 0.6925399899482727
train_iter_loss: 0.6106386780738831
train_iter_loss: 0.6418889164924622
train_iter_loss: 0.7002370357513428
train_iter_loss: 0.5971177220344543
train_iter_loss: 0.5487571954727173
train_iter_loss: 0.5632823705673218
train_iter_loss: 0.5553827285766602
train_iter_loss: 0.6263668537139893
train_iter_loss: 0.6155492663383484
train_iter_loss: 0.5856900811195374
train_iter_loss: 0.6239703893661499
train_iter_loss: 0.5819582939147949
train_iter_loss: 0.5648514628410339
train_iter_loss: 0.6203559041023254
train_iter_loss: 0.6363601088523865
train_iter_loss: 0.5863081812858582
train_iter_loss: 0.5972460508346558
train_iter_loss: 0.6492430567741394
train_iter_loss: 0.6194981932640076
train_iter_loss: 0.6336345672607422
train_iter_loss: 0.592354416847229
train_iter_loss: 0.6624891757965088
train_iter_loss: 0.6291831135749817
train_iter_loss: 0.6239035129547119
train_iter_loss: 0.6873059868812561
train_iter_loss: 0.5721157193183899
train_iter_loss: 0.6194292902946472
train_iter_loss: 0.6924742460250854
train_iter_loss: 0.5330852270126343
train_iter_loss: 0.5135930180549622
train_iter_loss: 0.5829508304595947
train_iter_loss: 0.5328987240791321
train_iter_loss: 0.6247841715812683
train_iter_loss: 0.558551013469696
train_iter_loss: 0.6591207981109619
train_iter_loss: 0.552920401096344
train_iter_loss: 0.6622589230537415
train_iter_loss: 0.57296222448349
train_iter_loss: 0.5465715527534485
train_iter_loss: 0.5864837765693665
train_iter_loss: 0.5363190770149231
train_iter_loss: 0.5965113639831543
train_iter_loss: 0.655205488204956
train_iter_loss: 0.599227249622345
train_iter_loss: 0.6168637275695801
train_iter_loss: 0.6196007132530212
train_iter_loss: 0.5929628014564514
train_iter_loss: 0.5754473209381104
train_iter_loss: 0.5881582498550415
train_iter_loss: 0.6337496638298035
train_iter_loss: 0.6439502835273743
train_iter_loss: 0.5614302754402161
train_iter_loss: 0.593962550163269
train_iter_loss: 0.6507906317710876
train_iter_loss: 0.5653362274169922
train_iter_loss: 0.6151260137557983
train_iter_loss: 0.6210075616836548
train_iter_loss: 0.5948950052261353
train_iter_loss: 0.6052822470664978
train_iter_loss: 0.6254613399505615
train_iter_loss: 0.6422386765480042
train_iter_loss: 0.6467852592468262
train_iter_loss: 0.6067068576812744
train_iter_loss: 0.7084055542945862
train_iter_loss: 0.5642955899238586
train_iter_loss: 0.5720967054367065
train_iter_loss: 0.6349806785583496
train_iter_loss: 0.5743563175201416
train_iter_loss: 0.6035217642784119
train_iter_loss: 0.6226484775543213
train_iter_loss: 0.5983028411865234
train_iter_loss: 0.6162843704223633
train_iter_loss: 0.6218669414520264
train_iter_loss: 0.570503830909729
train_iter_loss: 0.6194101572036743
train_iter_loss: 0.615272581577301
train_iter_loss: 0.5693612098693848
train_iter_loss: 0.5829878449440002
train_iter_loss: 0.5809770226478577
train_iter_loss: 0.5682042837142944
train_iter_loss: 0.6225041151046753
train_iter_loss: 0.5060924887657166
train loss :0.6344
---------------------
Validation seg loss: 0.6104424165667228 at epoch 0
********************
best_val_epoch_loss:  0.6104424165667228
MODEL UPDATED
epoch =      1/  1000, exp = train
train_iter_loss: 0.49915915727615356
train_iter_loss: 0.5755258798599243
train_iter_loss: 0.5684743523597717
train_iter_loss: 0.6170495748519897
train_iter_loss: 0.5527051091194153
train_iter_loss: 0.6174588799476624
train_iter_loss: 0.629814624786377
train_iter_loss: 0.574317216873169
train_iter_loss: 0.5462508201599121
train_iter_loss: 0.651107907295227
train_iter_loss: 0.5633699893951416
train_iter_loss: 0.5521504282951355
train_iter_loss: 0.5760486721992493
train_iter_loss: 0.6253111958503723
train_iter_loss: 0.6157483458518982
train_iter_loss: 0.6286262273788452
train_iter_loss: 0.5632379651069641
train_iter_loss: 0.578559160232544
train_iter_loss: 0.6390472650527954
train_iter_loss: 0.5774381160736084
train_iter_loss: 0.554123044013977
train_iter_loss: 0.567248523235321
train_iter_loss: 0.5485827922821045
train_iter_loss: 0.5271689891815186
train_iter_loss: 0.5900788903236389
train_iter_loss: 0.5831574201583862
train_iter_loss: 0.5055622458457947
train_iter_loss: 0.5267727971076965
train_iter_loss: 0.5181731581687927
train_iter_loss: 0.4892890155315399
train_iter_loss: 0.5631944537162781
train_iter_loss: 0.6437406539916992
train_iter_loss: 0.6191117167472839
train_iter_loss: 0.572205126285553
train_iter_loss: 0.6337860226631165
train_iter_loss: 0.6255842447280884
train_iter_loss: 0.5982624292373657
train_iter_loss: 0.5701708793640137
train_iter_loss: 0.586520791053772
train_iter_loss: 0.5851894021034241
train_iter_loss: 0.5631434321403503
train_iter_loss: 0.570064127445221
train_iter_loss: 0.5709637403488159
train_iter_loss: 0.5534068942070007
train_iter_loss: 0.5202891230583191
train_iter_loss: 0.6368700265884399
train_iter_loss: 0.48626911640167236
train_iter_loss: 0.5219235420227051
train_iter_loss: 0.6044237613677979
train_iter_loss: 0.5448884963989258
train_iter_loss: 0.585638701915741
train_iter_loss: 0.5547110438346863
train_iter_loss: 0.5387372374534607
train_iter_loss: 0.5824041962623596
train_iter_loss: 0.5635919570922852
train_iter_loss: 0.6460595726966858
train_iter_loss: 0.6066858768463135
train_iter_loss: 0.6656522154808044
train_iter_loss: 0.6046233177185059
train_iter_loss: 0.5761080980300903
train_iter_loss: 0.5423220992088318
train_iter_loss: 0.5467490553855896
train_iter_loss: 0.6035056710243225
train_iter_loss: 0.590226948261261
train_iter_loss: 0.6145961284637451
train_iter_loss: 0.560001790523529
train_iter_loss: 0.5954037308692932
train_iter_loss: 0.5663967132568359
train_iter_loss: 0.5751452445983887
train_iter_loss: 0.5908529758453369
train_iter_loss: 0.5876318216323853
train_iter_loss: 0.5398206114768982
train_iter_loss: 0.5812774300575256
train_iter_loss: 0.5126299262046814
train_iter_loss: 0.6370083093643188
train_iter_loss: 0.5742784738540649
train_iter_loss: 0.5904321074485779
train_iter_loss: 0.5953147411346436
train_iter_loss: 0.5981630682945251
train_iter_loss: 0.557040274143219
train_iter_loss: 0.5173973441123962
train_iter_loss: 0.6029388904571533
train_iter_loss: 0.6359256505966187
train_iter_loss: 0.5942385196685791
train_iter_loss: 0.5651463270187378
train_iter_loss: 0.5597309470176697
train_iter_loss: 0.5401380062103271
train_iter_loss: 0.6139869689941406
train_iter_loss: 0.6119557619094849
train_iter_loss: 0.6310171484947205
train_iter_loss: 0.5860114693641663
train_iter_loss: 0.5862075686454773
train_iter_loss: 0.664717435836792
train_iter_loss: 0.5493589043617249
train_iter_loss: 0.5337153077125549
train_iter_loss: 0.5960931181907654
train_iter_loss: 0.5545278191566467
train_iter_loss: 0.5498324632644653
train_iter_loss: 0.532622218132019
train_iter_loss: 0.48550087213516235
train_iter_loss: 0.6583141684532166
train_iter_loss: 0.5978479981422424
train_iter_loss: 0.5539023876190186
train_iter_loss: 0.5978190898895264
train_iter_loss: 0.478659451007843
train_iter_loss: 0.6338949203491211
train_iter_loss: 0.5160712599754333
train_iter_loss: 0.5349525213241577
train_iter_loss: 0.5968539714813232
train_iter_loss: 0.6089334487915039
train_iter_loss: 0.4946564733982086
train_iter_loss: 0.5892158150672913
train_iter_loss: 0.5624602437019348
train_iter_loss: 0.5019239187240601
train_iter_loss: 0.5585513114929199
train_iter_loss: 0.5887749195098877
train_iter_loss: 0.5748934745788574
train_iter_loss: 0.5122510194778442
train_iter_loss: 0.4826379716396332
train_iter_loss: 0.6019342541694641
train_iter_loss: 0.5923669338226318
train_iter_loss: 0.5263470411300659
train_iter_loss: 0.6173253655433655
train_iter_loss: 0.5973562598228455
train_iter_loss: 0.5565135478973389
train_iter_loss: 0.5639309287071228
train_iter_loss: 0.5821138024330139
train_iter_loss: 0.48455098271369934
train_iter_loss: 0.49992164969444275
train_iter_loss: 0.5323566198348999
train_iter_loss: 0.5395553708076477
train_iter_loss: 0.5794901251792908
train_iter_loss: 0.6197639107704163
train_iter_loss: 0.5313665270805359
train_iter_loss: 0.5230486989021301
train_iter_loss: 0.6155549883842468
train_iter_loss: 0.6246882081031799
train_iter_loss: 0.4723011553287506
train_iter_loss: 0.5509048104286194
train_iter_loss: 0.5014370083808899
train_iter_loss: 0.5667027831077576
train_iter_loss: 0.5207380652427673
train_iter_loss: 0.5933260321617126
train_iter_loss: 0.5351836085319519
train_iter_loss: 0.551099956035614
train_iter_loss: 0.545135498046875
train_iter_loss: 0.6078301668167114
train_iter_loss: 0.5744053721427917
train_iter_loss: 0.5625911951065063
train_iter_loss: 0.5947378277778625
train_iter_loss: 0.533455491065979
train_iter_loss: 0.5208205580711365
train_iter_loss: 0.5549783110618591
train_iter_loss: 0.5511096715927124
train_iter_loss: 0.4744294285774231
train_iter_loss: 0.547390341758728
train_iter_loss: 0.5382470488548279
train_iter_loss: 0.6000679731369019
train_iter_loss: 0.584018886089325
train_iter_loss: 0.5195095539093018
train_iter_loss: 0.6247224807739258
train_iter_loss: 0.5772626399993896
train_iter_loss: 0.5940418839454651
train_iter_loss: 0.565992534160614
train_iter_loss: 0.6128636598587036
train_iter_loss: 0.5387678742408752
train_iter_loss: 0.5330158472061157
train_iter_loss: 0.5148544907569885
train_iter_loss: 0.5944772958755493
train_iter_loss: 0.5145425796508789
train_iter_loss: 0.49630025029182434
train_iter_loss: 0.590603232383728
train_iter_loss: 0.5705108642578125
train_iter_loss: 0.5819993019104004
train_iter_loss: 0.49677178263664246
train_iter_loss: 0.5194748044013977
train_iter_loss: 0.5509786009788513
train_iter_loss: 0.5543064475059509
train_iter_loss: 0.5500748157501221
train_iter_loss: 0.5778180360794067
train_iter_loss: 0.5465726852416992
train_iter_loss: 0.5254992246627808
train_iter_loss: 0.5451729893684387
train_iter_loss: 0.535584032535553
train_iter_loss: 0.5392846465110779
train_iter_loss: 0.4998636841773987
train_iter_loss: 0.5358490943908691
train_iter_loss: 0.5379744172096252
train_iter_loss: 0.5402135252952576
train_iter_loss: 0.5748830437660217
train_iter_loss: 0.539798378944397
train_iter_loss: 0.5749497413635254
train_iter_loss: 0.6235718131065369
train_iter_loss: 0.5738303661346436
train_iter_loss: 0.4558221697807312
train_iter_loss: 0.5677862167358398
train_iter_loss: 0.5478963255882263
train_iter_loss: 0.5817251205444336
train_iter_loss: 0.5644307732582092
train_iter_loss: 0.5465489029884338
train loss :0.5695
---------------------
Validation seg loss: 0.5637947287199632 at epoch 1
********************
best_val_epoch_loss:  0.5637947287199632
MODEL UPDATED
epoch =      2/  1000, exp = train
train_iter_loss: 0.5516772270202637
train_iter_loss: 0.6137903928756714
train_iter_loss: 0.5755231976509094
train_iter_loss: 0.5466445088386536
train_iter_loss: 0.612078070640564
train_iter_loss: 0.5439555048942566
train_iter_loss: 0.5044100880622864
train_iter_loss: 0.5101388096809387
train_iter_loss: 0.5341556072235107
train_iter_loss: 0.5742523670196533
train_iter_loss: 0.4700528681278229
train_iter_loss: 0.5693494081497192
train_iter_loss: 0.535793125629425
train_iter_loss: 0.5774692893028259
train_iter_loss: 0.5555673837661743
train_iter_loss: 0.6195380687713623
train_iter_loss: 0.5286611914634705
train_iter_loss: 0.46718382835388184
train_iter_loss: 0.6102270483970642
train_iter_loss: 0.5484922528266907
train_iter_loss: 0.5547621846199036
train_iter_loss: 0.5236555933952332
train_iter_loss: 0.4653838574886322
train_iter_loss: 0.5519319176673889
train_iter_loss: 0.5725535154342651
train_iter_loss: 0.5739130973815918
train_iter_loss: 0.5848443508148193
train_iter_loss: 0.5697875618934631
train_iter_loss: 0.5584784746170044
train_iter_loss: 0.49760469794273376
train_iter_loss: 0.4896330237388611
train_iter_loss: 0.583137571811676
train_iter_loss: 0.5367157459259033
train_iter_loss: 0.524674654006958
train_iter_loss: 0.44266995787620544
train_iter_loss: 0.5493623614311218
train_iter_loss: 0.4890587031841278
train_iter_loss: 0.5728992819786072
train_iter_loss: 0.5297906398773193
train_iter_loss: 0.5386475920677185
train_iter_loss: 0.5014163255691528
train_iter_loss: 0.5348993539810181
train_iter_loss: 0.5588696599006653
train_iter_loss: 0.5179270505905151
train_iter_loss: 0.5698398947715759
train_iter_loss: 0.5356739163398743
train_iter_loss: 0.49514511227607727
train_iter_loss: 0.5840463638305664
train_iter_loss: 0.5282292366027832
train_iter_loss: 0.4878596067428589
train_iter_loss: 0.5501702427864075
train_iter_loss: 0.534084141254425
train_iter_loss: 0.4981183409690857
train_iter_loss: 0.5843744277954102
train_iter_loss: 0.5890278816223145
train_iter_loss: 0.5481864213943481
train_iter_loss: 0.567410945892334
train_iter_loss: 0.5413748025894165
train_iter_loss: 0.5075010061264038
train_iter_loss: 0.5526825189590454
train_iter_loss: 0.5026068091392517
train_iter_loss: 0.5693881511688232
train_iter_loss: 0.524448573589325
train_iter_loss: 0.5549291968345642
train_iter_loss: 0.5973336696624756
train_iter_loss: 0.564281165599823
train_iter_loss: 0.5444217920303345
train_iter_loss: 0.527300238609314
train_iter_loss: 0.5550900101661682
train_iter_loss: 0.5568299889564514
train_iter_loss: 0.4942810833454132
train_iter_loss: 0.51567542552948
train_iter_loss: 0.5662552714347839
train_iter_loss: 0.5262248516082764
train_iter_loss: 0.4907125234603882
train_iter_loss: 0.5615018010139465
train_iter_loss: 0.48859402537345886
train_iter_loss: 0.5691540241241455
train_iter_loss: 0.5155673623085022
train_iter_loss: 0.4914725422859192
train_iter_loss: 0.45658203959465027
train_iter_loss: 0.5134211182594299
train_iter_loss: 0.4773806929588318
train_iter_loss: 0.5252782702445984
train_iter_loss: 0.5146617889404297
train_iter_loss: 0.5958451628684998
train_iter_loss: 0.557756781578064
train_iter_loss: 0.5402204394340515
train_iter_loss: 0.5408100485801697
train_iter_loss: 0.5605947971343994
train_iter_loss: 0.5502879023551941
train_iter_loss: 0.438286691904068
train_iter_loss: 0.49077722430229187
train_iter_loss: 0.40989619493484497
train_iter_loss: 0.5680570602416992
train_iter_loss: 0.5379458665847778
train_iter_loss: 0.5647100210189819
train_iter_loss: 0.5586039423942566
train_iter_loss: 0.594703733921051
train_iter_loss: 0.5614763498306274
train_iter_loss: 0.4714071750640869
train_iter_loss: 0.5307483673095703
train_iter_loss: 0.5142883658409119
train_iter_loss: 0.5340431928634644
train_iter_loss: 0.5234870314598083
train_iter_loss: 0.5660061836242676
train_iter_loss: 0.5083879232406616
train_iter_loss: 0.5134215354919434
train_iter_loss: 0.5434600710868835
train_iter_loss: 0.5422113537788391
train_iter_loss: 0.5422771573066711
train_iter_loss: 0.4520103931427002
train_iter_loss: 0.5502721667289734
train_iter_loss: 0.5423638224601746
train_iter_loss: 0.47921913862228394
train_iter_loss: 0.49411922693252563
train_iter_loss: 0.5682252645492554
train_iter_loss: 0.5584858059883118
train_iter_loss: 0.5054447054862976
train_iter_loss: 0.519504189491272
train_iter_loss: 0.582347571849823
train_iter_loss: 0.4991634488105774
train_iter_loss: 0.46732938289642334
train_iter_loss: 0.5067662000656128
train_iter_loss: 0.5014544129371643
train_iter_loss: 0.5656271576881409
train_iter_loss: 0.5449463725090027
train_iter_loss: 0.49210694432258606
train_iter_loss: 0.5516994595527649
train_iter_loss: 0.5058801770210266
train_iter_loss: 0.5201297402381897
train_iter_loss: 0.4994586408138275
train_iter_loss: 0.4997728765010834
train_iter_loss: 0.5044727325439453
train_iter_loss: 0.5692598819732666
train_iter_loss: 0.5060056447982788
train_iter_loss: 0.539770781993866
train_iter_loss: 0.5241946578025818
train_iter_loss: 0.5315229296684265
train_iter_loss: 0.5715777277946472
train_iter_loss: 0.5343887805938721
train_iter_loss: 0.4842659533023834
train_iter_loss: 0.5529127717018127
train_iter_loss: 0.5390772819519043
train_iter_loss: 0.47464483976364136
train_iter_loss: 0.5528358817100525
train_iter_loss: 0.506412923336029
train_iter_loss: 0.5460377335548401
train_iter_loss: 0.47699055075645447
train_iter_loss: 0.569758951663971
train_iter_loss: 0.47635218501091003
train_iter_loss: 0.4712975323200226
train_iter_loss: 0.4699641764163971
train_iter_loss: 0.539326548576355
train_iter_loss: 0.5458033680915833
train_iter_loss: 0.5421005487442017
train_iter_loss: 0.5354279279708862
train_iter_loss: 0.5332943201065063
train_iter_loss: 0.5056338906288147
train_iter_loss: 0.43405571579933167
train_iter_loss: 0.5254742503166199
train_iter_loss: 0.43916574120521545
train_iter_loss: 0.5195684432983398
train_iter_loss: 0.5201635360717773
train_iter_loss: 0.49746260046958923
train_iter_loss: 0.49093762040138245
train_iter_loss: 0.5134069919586182
train_iter_loss: 0.538058340549469
train_iter_loss: 0.52793949842453
train_iter_loss: 0.5005371570587158
train_iter_loss: 0.48582059144973755
train_iter_loss: 0.5056635737419128
train_iter_loss: 0.6107202768325806
train_iter_loss: 0.4541061222553253
train_iter_loss: 0.5226506590843201
train_iter_loss: 0.4947584867477417
train_iter_loss: 0.5071097016334534
train_iter_loss: 0.5497321486473083
train_iter_loss: 0.48134154081344604
train_iter_loss: 0.5035499930381775
train_iter_loss: 0.5251510739326477
train_iter_loss: 0.5152024626731873
train_iter_loss: 0.4891258776187897
train_iter_loss: 0.41994789242744446
train_iter_loss: 0.5044912099838257
train_iter_loss: 0.5007166862487793
train_iter_loss: 0.45058730244636536
train_iter_loss: 0.5365025997161865
train_iter_loss: 0.547302782535553
train_iter_loss: 0.42079663276672363
train_iter_loss: 0.5772420763969421
train_iter_loss: 0.5190744996070862
train_iter_loss: 0.45139551162719727
train_iter_loss: 0.46631473302841187
train_iter_loss: 0.5752149224281311
train_iter_loss: 0.5055862069129944
train_iter_loss: 0.4633640944957733
train_iter_loss: 0.45003676414489746
train_iter_loss: 0.4737008810043335
train_iter_loss: 0.4916444420814514
train loss :0.5285
---------------------
Validation seg loss: 0.5221758878455972 at epoch 2
********************
best_val_epoch_loss:  0.5221758878455972
MODEL UPDATED
epoch =      3/  1000, exp = train
train_iter_loss: 0.523986279964447
train_iter_loss: 0.46987006068229675
train_iter_loss: 0.561617910861969
train_iter_loss: 0.5267292857170105
train_iter_loss: 0.48337289690971375
train_iter_loss: 0.49166181683540344
train_iter_loss: 0.5199403762817383
train_iter_loss: 0.6193587779998779
train_iter_loss: 0.4901066720485687
train_iter_loss: 0.5004161596298218
train_iter_loss: 0.5574216842651367
train_iter_loss: 0.49785637855529785
train_iter_loss: 0.49906060099601746
train_iter_loss: 0.5289958119392395
train_iter_loss: 0.5080207586288452
train_iter_loss: 0.49362340569496155
train_iter_loss: 0.4908844530582428
train_iter_loss: 0.5054611563682556
train_iter_loss: 0.4593254029750824
train_iter_loss: 0.49336162209510803
train_iter_loss: 0.5283348560333252
train_iter_loss: 0.45226913690567017
train_iter_loss: 0.5124136209487915
train_iter_loss: 0.528978705406189
train_iter_loss: 0.5327765345573425
train_iter_loss: 0.5421410202980042
train_iter_loss: 0.46480435132980347
train_iter_loss: 0.5275679230690002
train_iter_loss: 0.49870532751083374
train_iter_loss: 0.5002581477165222
train_iter_loss: 0.4379732608795166
train_iter_loss: 0.4906398355960846
train_iter_loss: 0.45330697298049927
train_iter_loss: 0.47770166397094727
train_iter_loss: 0.4813293218612671
train_iter_loss: 0.5146855115890503
train_iter_loss: 0.4886939227581024
train_iter_loss: 0.49165141582489014
train_iter_loss: 0.48378875851631165
train_iter_loss: 0.5222446322441101
train_iter_loss: 0.4697880148887634
train_iter_loss: 0.5119439959526062
train_iter_loss: 0.501552164554596
train_iter_loss: 0.47097888588905334
train_iter_loss: 0.5059877634048462
train_iter_loss: 0.43088892102241516
train_iter_loss: 0.44186559319496155
train_iter_loss: 0.5034944415092468
train_iter_loss: 0.5208653211593628
train_iter_loss: 0.4477251172065735
train_iter_loss: 0.5107719302177429
train_iter_loss: 0.5608636736869812
train_iter_loss: 0.5434906482696533
train_iter_loss: 0.504747748374939
train_iter_loss: 0.5047577023506165
train_iter_loss: 0.46919795870780945
train_iter_loss: 0.5266270041465759
train_iter_loss: 0.4328450858592987
train_iter_loss: 0.4591785669326782
train_iter_loss: 0.4451915919780731
train_iter_loss: 0.5201008915901184
train_iter_loss: 0.4585200548171997
train_iter_loss: 0.4232780933380127
train_iter_loss: 0.4562353193759918
train_iter_loss: 0.48809686303138733
train_iter_loss: 0.4974890947341919
train_iter_loss: 0.44890543818473816
train_iter_loss: 0.469696968793869
train_iter_loss: 0.5155479311943054
train_iter_loss: 0.47257399559020996
train_iter_loss: 0.5155821442604065
train_iter_loss: 0.4284663796424866
train_iter_loss: 0.49464720487594604
train_iter_loss: 0.575045645236969
train_iter_loss: 0.4620054364204407
train_iter_loss: 0.49290353059768677
train_iter_loss: 0.5151193737983704
train_iter_loss: 0.6314934492111206
train_iter_loss: 0.5113971829414368
train_iter_loss: 0.5029125809669495
train_iter_loss: 0.49696847796440125
train_iter_loss: 0.422575443983078
train_iter_loss: 0.5637745261192322
train_iter_loss: 0.5134004950523376
train_iter_loss: 0.5252882242202759
train_iter_loss: 0.5166066288948059
train_iter_loss: 0.5117352604866028
train_iter_loss: 0.4973290264606476
train_iter_loss: 0.4938155710697174
train_iter_loss: 0.5000091791152954
train_iter_loss: 0.4328940808773041
train_iter_loss: 0.5051512122154236
train_iter_loss: 0.5155901908874512
train_iter_loss: 0.5105106830596924
train_iter_loss: 0.457713782787323
train_iter_loss: 0.519324779510498
train_iter_loss: 0.5215902924537659
train_iter_loss: 0.5346047878265381
train_iter_loss: 0.4834451377391815
train_iter_loss: 0.4422009289264679
train_iter_loss: 0.4751170873641968
train_iter_loss: 0.5692286491394043
train_iter_loss: 0.47099822759628296
train_iter_loss: 0.5092835426330566
train_iter_loss: 0.3922254741191864
train_iter_loss: 0.5383884906768799
train_iter_loss: 0.43991991877555847
train_iter_loss: 0.467481791973114
train_iter_loss: 0.4667235314846039
train_iter_loss: 0.4759540855884552
train_iter_loss: 0.49192073941230774
train_iter_loss: 0.5230790972709656
train_iter_loss: 0.50074702501297
train_iter_loss: 0.5141059756278992
train_iter_loss: 0.43956664204597473
train_iter_loss: 0.49990782141685486
train_iter_loss: 0.4809815585613251
train_iter_loss: 0.4602896571159363
train_iter_loss: 0.5178389549255371
train_iter_loss: 0.5320125818252563
train_iter_loss: 0.4811917543411255
train_iter_loss: 0.47596275806427
train_iter_loss: 0.46170106530189514
train_iter_loss: 0.5167771577835083
train_iter_loss: 0.461849570274353
train_iter_loss: 0.5271217823028564
train_iter_loss: 0.44246238470077515
train_iter_loss: 0.5539413094520569
train_iter_loss: 0.5092089176177979
train_iter_loss: 0.4995926320552826
train_iter_loss: 0.5363977551460266
train_iter_loss: 0.5205262899398804
train_iter_loss: 0.4892605245113373
train_iter_loss: 0.4649515450000763
train_iter_loss: 0.5255739688873291
train_iter_loss: 0.47640934586524963
train_iter_loss: 0.5387912392616272
train_iter_loss: 0.5064086318016052
train_iter_loss: 0.4721734821796417
train_iter_loss: 0.4518784284591675
train_iter_loss: 0.5428221225738525
train_iter_loss: 0.4461928904056549
train_iter_loss: 0.4646114110946655
train_iter_loss: 0.4922108054161072
train_iter_loss: 0.46731793880462646
train_iter_loss: 0.4780198633670807
train_iter_loss: 0.4436565041542053
train_iter_loss: 0.4702690839767456
train_iter_loss: 0.4206826090812683
train_iter_loss: 0.4890006184577942
train_iter_loss: 0.550827145576477
train_iter_loss: 0.5034617185592651
train_iter_loss: 0.5123536586761475
train_iter_loss: 0.49450165033340454
train_iter_loss: 0.43549326062202454
train_iter_loss: 0.5109848976135254
train_iter_loss: 0.48147210478782654
train_iter_loss: 0.4650130569934845
train_iter_loss: 0.5330055356025696
train_iter_loss: 0.5199451446533203
train_iter_loss: 0.47808149456977844
train_iter_loss: 0.500086784362793
train_iter_loss: 0.4822015166282654
train_iter_loss: 0.4761732220649719
train_iter_loss: 0.5278598666191101
train_iter_loss: 0.46274280548095703
train_iter_loss: 0.4386822581291199
train_iter_loss: 0.44979679584503174
train_iter_loss: 0.47095778584480286
train_iter_loss: 0.46427613496780396
train_iter_loss: 0.49910634756088257
train_iter_loss: 0.47105297446250916
train_iter_loss: 0.4769991636276245
train_iter_loss: 0.49550744891166687
train_iter_loss: 0.48355790972709656
train_iter_loss: 0.5043827295303345
train_iter_loss: 0.45841503143310547
train_iter_loss: 0.49655601382255554
train_iter_loss: 0.4922296702861786
train_iter_loss: 0.47659358382225037
train_iter_loss: 0.4652749001979828
train_iter_loss: 0.4989634156227112
train_iter_loss: 0.4898408055305481
train_iter_loss: 0.4300929009914398
train_iter_loss: 0.5099741816520691
train_iter_loss: 0.45701834559440613
train_iter_loss: 0.49371054768562317
train_iter_loss: 0.5270124077796936
train_iter_loss: 0.5094640851020813
train_iter_loss: 0.49828460812568665
train_iter_loss: 0.39420026540756226
train_iter_loss: 0.4548577666282654
train_iter_loss: 0.514434278011322
train_iter_loss: 0.5003272891044617
train_iter_loss: 0.4893357753753662
train_iter_loss: 0.5020250678062439
train_iter_loss: 0.42937642335891724
train_iter_loss: 0.49884647130966187
train_iter_loss: 0.5200474262237549
train_iter_loss: 0.4964667856693268
train loss :0.4949
---------------------
Validation seg loss: 0.5190735985085649 at epoch 3
********************
best_val_epoch_loss:  0.5190735985085649
MODEL UPDATED
epoch =      4/  1000, exp = train
train_iter_loss: 0.47953274846076965
train_iter_loss: 0.47814539074897766
train_iter_loss: 0.5251741409301758
train_iter_loss: 0.4544590413570404
train_iter_loss: 0.4665033519268036
train_iter_loss: 0.49132317304611206
train_iter_loss: 0.5444851517677307
train_iter_loss: 0.4559255838394165
train_iter_loss: 0.5024157762527466
train_iter_loss: 0.4356538653373718
train_iter_loss: 0.4581848382949829
train_iter_loss: 0.47480443120002747
train_iter_loss: 0.452644407749176
train_iter_loss: 0.43815210461616516
train_iter_loss: 0.3666621148586273
train_iter_loss: 0.4345410168170929
train_iter_loss: 0.460803359746933
train_iter_loss: 0.47801485657691956
train_iter_loss: 0.4476795196533203
train_iter_loss: 0.4941501021385193
train_iter_loss: 0.4303663969039917
train_iter_loss: 0.48656779527664185
train_iter_loss: 0.4748409390449524
train_iter_loss: 0.43008723855018616
train_iter_loss: 0.5071097016334534
train_iter_loss: 0.5201930999755859
train_iter_loss: 0.5232138633728027
train_iter_loss: 0.4989643692970276
train_iter_loss: 0.4549662172794342
train_iter_loss: 0.42824289202690125
train_iter_loss: 0.5231047868728638
train_iter_loss: 0.5148360729217529
train_iter_loss: 0.5092859864234924
train_iter_loss: 0.4699191153049469
train_iter_loss: 0.43338361382484436
train_iter_loss: 0.4914916753768921
train_iter_loss: 0.45572027564048767
train_iter_loss: 0.4743834435939789
train_iter_loss: 0.4658842086791992
train_iter_loss: 0.4975432753562927
train_iter_loss: 0.4480445086956024
train_iter_loss: 0.46669450402259827
train_iter_loss: 0.46460017561912537
train_iter_loss: 0.4890630841255188
train_iter_loss: 0.45741838216781616
train_iter_loss: 0.45874300599098206
train_iter_loss: 0.4818838834762573
train_iter_loss: 0.5058574676513672
train_iter_loss: 0.4476141929626465
train_iter_loss: 0.4778476655483246
train_iter_loss: 0.46638980507850647
train_iter_loss: 0.453843891620636
train_iter_loss: 0.47281500697135925
train_iter_loss: 0.46102461218833923
train_iter_loss: 0.4391065239906311
train_iter_loss: 0.4360608756542206
train_iter_loss: 0.4590657651424408
train_iter_loss: 0.5161034464836121
train_iter_loss: 0.4667523503303528
train_iter_loss: 0.48034441471099854
train_iter_loss: 0.4924532473087311
train_iter_loss: 0.5308064222335815
train_iter_loss: 0.46916964650154114
train_iter_loss: 0.4919997751712799
train_iter_loss: 0.42798081040382385
train_iter_loss: 0.4712899327278137
train_iter_loss: 0.46052902936935425
train_iter_loss: 0.44453078508377075
train_iter_loss: 0.5001178979873657
train_iter_loss: 0.4514933228492737
train_iter_loss: 0.4132368564605713
train_iter_loss: 0.4899563491344452
train_iter_loss: 0.4638720154762268
train_iter_loss: 0.45334166288375854
train_iter_loss: 0.47773414850234985
train_iter_loss: 0.46627405285835266
train_iter_loss: 0.47463664412498474
train_iter_loss: 0.4521418511867523
train_iter_loss: 0.40112540125846863
train_iter_loss: 0.47380298376083374
train_iter_loss: 0.4673106372356415
train_iter_loss: 0.48011016845703125
train_iter_loss: 0.452950656414032
train_iter_loss: 0.4533762037754059
train_iter_loss: 0.5090819001197815
train_iter_loss: 0.4260128140449524
train_iter_loss: 0.43261539936065674
train_iter_loss: 0.4737783670425415
train_iter_loss: 0.46692556142807007
train_iter_loss: 0.45349815487861633
train_iter_loss: 0.43320730328559875
train_iter_loss: 0.4968370497226715
train_iter_loss: 0.45506197214126587
train_iter_loss: 0.4799621105194092
train_iter_loss: 0.3620865046977997
train_iter_loss: 0.47818663716316223
train_iter_loss: 0.4872346818447113
train_iter_loss: 0.5102629661560059
train_iter_loss: 0.45871222019195557
train_iter_loss: 0.5201497077941895
train_iter_loss: 0.49448850750923157
train_iter_loss: 0.41516634821891785
train_iter_loss: 0.457694947719574
train_iter_loss: 0.47930851578712463
train_iter_loss: 0.48196303844451904
train_iter_loss: 0.44440940022468567
train_iter_loss: 0.4795309603214264
train_iter_loss: 0.4995613694190979
train_iter_loss: 0.4657244384288788
train_iter_loss: 0.5212641954421997
train_iter_loss: 0.49621662497520447
train_iter_loss: 0.421087384223938
train_iter_loss: 0.4291595220565796
train_iter_loss: 0.43950870633125305
train_iter_loss: 0.5156571865081787
train_iter_loss: 0.3814699947834015
train_iter_loss: 0.5102468132972717
train_iter_loss: 0.48362213373184204
train_iter_loss: 0.42279931902885437
train_iter_loss: 0.46575555205345154
train_iter_loss: 0.4769270420074463
train_iter_loss: 0.5085163116455078
train_iter_loss: 0.3963228464126587
train_iter_loss: 0.47400930523872375
train_iter_loss: 0.4642139673233032
train_iter_loss: 0.4395322799682617
train_iter_loss: 0.45443597435951233
train_iter_loss: 0.4841753840446472
train_iter_loss: 0.4236195385456085
train_iter_loss: 0.42922842502593994
train_iter_loss: 0.41935035586357117
train_iter_loss: 0.4080353081226349
train_iter_loss: 0.4461890459060669
train_iter_loss: 0.4746852219104767
train_iter_loss: 0.45164334774017334
train_iter_loss: 0.51878821849823
train_iter_loss: 0.4575386047363281
train_iter_loss: 0.4337772727012634
train_iter_loss: 0.40100154280662537
train_iter_loss: 0.44227391481399536
train_iter_loss: 0.47424444556236267
train_iter_loss: 0.4751715064048767
train_iter_loss: 0.4924872815608978
train_iter_loss: 0.5082478523254395
train_iter_loss: 0.44263672828674316
train_iter_loss: 0.4785979092121124
train_iter_loss: 0.402850478887558
train_iter_loss: 0.4680953621864319
train_iter_loss: 0.575355052947998
train_iter_loss: 0.4714154303073883
train_iter_loss: 0.4301891028881073
train_iter_loss: 0.4410058557987213
train_iter_loss: 0.4760344922542572
train_iter_loss: 0.4968368113040924
train_iter_loss: 0.43269890546798706
train_iter_loss: 0.4428033232688904
train_iter_loss: 0.4360138177871704
train_iter_loss: 0.46431079506874084
train_iter_loss: 0.44647887349128723
train_iter_loss: 0.46990966796875
train_iter_loss: 0.43349602818489075
train_iter_loss: 0.4659581482410431
train_iter_loss: 0.48772215843200684
train_iter_loss: 0.4663965702056885
train_iter_loss: 0.45465487241744995
train_iter_loss: 0.42401182651519775
train_iter_loss: 0.45488253235816956
train_iter_loss: 0.4527789056301117
train_iter_loss: 0.4046633243560791
train_iter_loss: 0.4096039831638336
train_iter_loss: 0.41098594665527344
train_iter_loss: 0.5001333951950073
train_iter_loss: 0.48979130387306213
train_iter_loss: 0.469095915555954
train_iter_loss: 0.44706740975379944
train_iter_loss: 0.4268178939819336
train_iter_loss: 0.43363097310066223
train_iter_loss: 0.4577910602092743
train_iter_loss: 0.4799574911594391
train_iter_loss: 0.4394536018371582
train_iter_loss: 0.4950493276119232
train_iter_loss: 0.4358954131603241
train_iter_loss: 0.5163827538490295
train_iter_loss: 0.5021785497665405
train_iter_loss: 0.4509328305721283
train_iter_loss: 0.4575177729129791
train_iter_loss: 0.44710150361061096
train_iter_loss: 0.45385193824768066
train_iter_loss: 0.47462648153305054
train_iter_loss: 0.43968310952186584
train_iter_loss: 0.5690165758132935
train_iter_loss: 0.46472039818763733
train_iter_loss: 0.420400470495224
train_iter_loss: 0.4022238850593567
train_iter_loss: 0.5013771653175354
train_iter_loss: 0.49932998418807983
train_iter_loss: 0.4489871859550476
train_iter_loss: 0.4113030433654785
train_iter_loss: 0.47516483068466187
train_iter_loss: 0.4612724781036377
train loss :0.4667
---------------------
Validation seg loss: 0.48310189989377866 at epoch 4
********************
best_val_epoch_loss:  0.48310189989377866
MODEL UPDATED
epoch =      5/  1000, exp = train
train_iter_loss: 0.4250343143939972
train_iter_loss: 0.4797668755054474
train_iter_loss: 0.47802215814590454
train_iter_loss: 0.46909815073013306
train_iter_loss: 0.42500898241996765
train_iter_loss: 0.48394766449928284
train_iter_loss: 0.5086849331855774
train_iter_loss: 0.4645099639892578
train_iter_loss: 0.43313249945640564
train_iter_loss: 0.4461973011493683
train_iter_loss: 0.43067216873168945
train_iter_loss: 0.44398564100265503
train_iter_loss: 0.4285745322704315
train_iter_loss: 0.40696609020233154
train_iter_loss: 0.48533597588539124
train_iter_loss: 0.4384964108467102
train_iter_loss: 0.434334933757782
train_iter_loss: 0.43693193793296814
train_iter_loss: 0.4716935157775879
train_iter_loss: 0.4828961193561554
train_iter_loss: 0.5272589921951294
train_iter_loss: 0.4832374155521393
train_iter_loss: 0.4462878406047821
train_iter_loss: 0.39613673090934753
train_iter_loss: 0.4796939194202423
train_iter_loss: 0.44126802682876587
train_iter_loss: 0.4536788761615753
train_iter_loss: 0.4449697434902191
train_iter_loss: 0.3974386751651764
train_iter_loss: 0.46419334411621094
train_iter_loss: 0.4136180281639099
train_iter_loss: 0.43851765990257263
train_iter_loss: 0.4643357992172241
train_iter_loss: 0.45688825845718384
train_iter_loss: 0.4445720314979553
train_iter_loss: 0.4882284104824066
train_iter_loss: 0.5020092725753784
train_iter_loss: 0.44195377826690674
train_iter_loss: 0.4813310205936432
train_iter_loss: 0.43355387449264526
train_iter_loss: 0.428266704082489
train_iter_loss: 0.4461095929145813
train_iter_loss: 0.44120296835899353
train_iter_loss: 0.4286501705646515
train_iter_loss: 0.3780174255371094
train_iter_loss: 0.4565770626068115
train_iter_loss: 0.44857439398765564
train_iter_loss: 0.4918414354324341
train_iter_loss: 0.4482690989971161
train_iter_loss: 0.41959407925605774
train_iter_loss: 0.40116098523139954
train_iter_loss: 0.4705697298049927
train_iter_loss: 0.44468361139297485
train_iter_loss: 0.41552311182022095
train_iter_loss: 0.4637109935283661
train_iter_loss: 0.42367982864379883
train_iter_loss: 0.42278608679771423
train_iter_loss: 0.47292810678482056
train_iter_loss: 0.4419777989387512
train_iter_loss: 0.4889337420463562
train_iter_loss: 0.45527130365371704
train_iter_loss: 0.448781281709671
train_iter_loss: 0.44516220688819885
train_iter_loss: 0.4208546280860901
train_iter_loss: 0.4350150227546692
train_iter_loss: 0.43928825855255127
train_iter_loss: 0.4357675015926361
train_iter_loss: 0.4493371248245239
train_iter_loss: 0.4786326587200165
train_iter_loss: 0.41033104062080383
train_iter_loss: 0.38661107420921326
train_iter_loss: 0.49463924765586853
train_iter_loss: 0.43773317337036133
train_iter_loss: 0.4345906376838684
train_iter_loss: 0.4599705636501312
train_iter_loss: 0.43951666355133057
train_iter_loss: 0.45706281065940857
train_iter_loss: 0.4546065032482147
train_iter_loss: 0.4531043767929077
train_iter_loss: 0.4466475546360016
train_iter_loss: 0.3832109272480011
train_iter_loss: 0.3980541527271271
train_iter_loss: 0.43163833022117615
train_iter_loss: 0.39197972416877747
train_iter_loss: 0.4478304088115692
train_iter_loss: 0.4638344645500183
train_iter_loss: 0.44798681139945984
train_iter_loss: 0.41498783230781555
train_iter_loss: 0.46189579367637634
train_iter_loss: 0.4419133961200714
train_iter_loss: 0.39001888036727905
train_iter_loss: 0.4012749493122101
train_iter_loss: 0.4241381287574768
train_iter_loss: 0.4409758746623993
train_iter_loss: 0.4244980216026306
train_iter_loss: 0.4376487731933594
train_iter_loss: 0.4248339831829071
train_iter_loss: 0.4350533187389374
train_iter_loss: 0.42133158445358276
train_iter_loss: 0.44176244735717773
train_iter_loss: 0.46449369192123413
train_iter_loss: 0.4354384243488312
train_iter_loss: 0.44158026576042175
train_iter_loss: 0.41117650270462036
train_iter_loss: 0.40336886048316956
train_iter_loss: 0.3785564601421356
train_iter_loss: 0.4204122722148895
train_iter_loss: 0.46286532282829285
train_iter_loss: 0.4683423936367035
train_iter_loss: 0.4413183331489563
train_iter_loss: 0.4144076704978943
train_iter_loss: 0.42263171076774597
train_iter_loss: 0.4209716320037842
train_iter_loss: 0.4361538887023926
train_iter_loss: 0.4069775938987732
train_iter_loss: 0.4463760256767273
train_iter_loss: 0.37939542531967163
train_iter_loss: 0.467422217130661
train_iter_loss: 0.45494574308395386
train_iter_loss: 0.4354502558708191
train_iter_loss: 0.4543081223964691
train_iter_loss: 0.4246055781841278
train_iter_loss: 0.445412278175354
train_iter_loss: 0.5065276026725769
train_iter_loss: 0.48773548007011414
train_iter_loss: 0.42183372378349304
train_iter_loss: 0.44768592715263367
train_iter_loss: 0.4120182991027832
train_iter_loss: 0.451181560754776
train_iter_loss: 0.44841641187667847
train_iter_loss: 0.42759495973587036
train_iter_loss: 0.41134539246559143
train_iter_loss: 0.44764024019241333
train_iter_loss: 0.49599871039390564
train_iter_loss: 0.4637323319911957
train_iter_loss: 0.39642783999443054
train_iter_loss: 0.4532204866409302
train_iter_loss: 0.43691104650497437
train_iter_loss: 0.48431214690208435
train_iter_loss: 0.4408690929412842
train_iter_loss: 0.43264609575271606
train_iter_loss: 0.4506714344024658
train_iter_loss: 0.43624821305274963
train_iter_loss: 0.44543638825416565
train_iter_loss: 0.366351842880249
train_iter_loss: 0.3995491862297058
train_iter_loss: 0.4005756676197052
train_iter_loss: 0.4870273172855377
train_iter_loss: 0.4124458134174347
train_iter_loss: 0.42186591029167175
train_iter_loss: 0.5101510882377625
train_iter_loss: 0.43991619348526
train_iter_loss: 0.43985238671302795
train_iter_loss: 0.3853677809238434
train_iter_loss: 0.4226755201816559
train_iter_loss: 0.46564775705337524
train_iter_loss: 0.4410693943500519
train_iter_loss: 0.3861193060874939
train_iter_loss: 0.3989063501358032
train_iter_loss: 0.4335840344429016
train_iter_loss: 0.4111727774143219
train_iter_loss: 0.4413413405418396
train_iter_loss: 0.43840235471725464
train_iter_loss: 0.41649994254112244
train_iter_loss: 0.4335947036743164
train_iter_loss: 0.45756128430366516
train_iter_loss: 0.43010079860687256
train_iter_loss: 0.40839552879333496
train_iter_loss: 0.42032867670059204
train_iter_loss: 0.4102299213409424
train_iter_loss: 0.48927557468414307
train_iter_loss: 0.4512576758861542
train_iter_loss: 0.4426894187927246
train_iter_loss: 0.4491933286190033
train_iter_loss: 0.3854662775993347
train_iter_loss: 0.49932861328125
train_iter_loss: 0.3648717999458313
train_iter_loss: 0.4173460304737091
train_iter_loss: 0.38763198256492615
train_iter_loss: 0.41737931966781616
train_iter_loss: 0.39184728264808655
train_iter_loss: 0.48866012692451477
train_iter_loss: 0.4394129812717438
train_iter_loss: 0.42110878229141235
train_iter_loss: 0.4554729163646698
train_iter_loss: 0.4565780460834503
train_iter_loss: 0.3673926293849945
train_iter_loss: 0.4373324513435364
train_iter_loss: 0.479200154542923
train_iter_loss: 0.4084254205226898
train_iter_loss: 0.5164058208465576
train_iter_loss: 0.5112646222114563
train_iter_loss: 0.4420328736305237
train_iter_loss: 0.4443998634815216
train_iter_loss: 0.4196929931640625
train_iter_loss: 0.4523198902606964
train_iter_loss: 0.43880486488342285
train_iter_loss: 0.4530414044857025
train_iter_loss: 0.44754356145858765
train_iter_loss: 0.47314876317977905
train loss :0.4427
---------------------
Validation seg loss: 0.47502917866661865 at epoch 5
********************
best_val_epoch_loss:  0.47502917866661865
MODEL UPDATED
epoch =      6/  1000, exp = train
